 \pdfoutput=1

\documentclass[11pt]{article}
\usepackage{float}

\usepackage{acl}

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{dsfont}

\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\usepackage{tikzscale}
\usepackage{amsmath}

\usepackage{multirow, colortbl}
\usepackage{color}
\usepackage{array}
\usepackage{multirow}
\usepackage{CJKutf8}
% \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
% Define your own color
\usepackage{tabularx,booktabs}
\usepgfplotslibrary{groupplots}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{placeins}

\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}} % Check mark
\newcommand{\xmark}{\ding{55}} % X mark

\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

\definecolor{ablation6}{HTML}{fcefed}
\definecolor{ablation_tie}{HTML}{fce3e1}

\definecolor{ablation5}{HTML}{fcd8d4}
\definecolor{ablation4}{HTML}{FBC3BC}
\definecolor{ablation3}{HTML}{F7A399}
\definecolor{ablation2}{HTML}{F38375}
\definecolor{ablation1}{HTML}{EF6351}

% \definecolor{ablation1}{HTML}{e3faf8}
% \definecolor{ablation2}{HTML}{C4FFF9}
% \definecolor{ablation3}{HTML}{9CEAEF}
% \definecolor{ablation4}{HTML}{68D8D6}
% \definecolor{ablation5}{HTML}{3DCCC7}
% \definecolor{ablation6}{HTML}{07BEB8}



\usepackage[]{algpseudocode}
\usepackage[]{algorithm}
\usepackage{float}
\algtext*{EndFor}%
\algtext*{EndProcedure}%

\usepackage{booktabs}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{adjustbox}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\newcommand{\russian}[1]{{\fontencoding{T2A}\selectfont\foreignlanguage{russian}{#1}}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{scalerel,xparse}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
%\usepackage[dvipsnames]{xcolor}

\renewcommand{\floatpagefraction}{.8}%
\renewcommand{\textfraction}{.1}%
\setcounter{totalnumber}{5}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{cleveref}
\usepackage{microtype}
\usepackage{enumitem}\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{dsfont}

\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\usepackage{tikzscale}
\usepackage{amsmath}
\newcommand{\probP}{\text{I\kern-0.15em P}}

\usepackage{multirow, colortbl}
\usepackage{color}
\usepackage{array}
\usepackage{multirow}
\usepackage{CJKutf8}
% \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
% Define your own color
\usepackage{tabularx,booktabs}
\usepgfplotslibrary{groupplots}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{placeins}

\usepackage{soul}
\definecolor{light blue}{RGB}{215, 242, 252}
\definecolor{light purple}{RGB}{247, 215, 252}
\definecolor{light orange}{rgb}{0.9961, 0.875, 0.7188}
\sethlcolor{light blue}
\newcommand{\hlpurple}[1]{\sethlcolor{light purple}\hl{#1}\sethlcolor{light blue}}
\newcommand{\hlorange}[1]{\sethlcolor{light orange}\hl{#1}\sethlcolor{light blue}}

\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

\definecolor{ablation6}{HTML}{fcefed}
\definecolor{ablation_tie}{HTML}{fce3e1}

\definecolor{ablation5}{HTML}{fcd8d4}
\definecolor{ablation4}{HTML}{FBC3BC}
\definecolor{ablation3}{HTML}{F7A399}
\definecolor{ablation2}{HTML}{F38375}
\definecolor{ablation1}{HTML}{EF6351}

% \definecolor{ablation1}{HTML}{e3faf8}
% \definecolor{ablation2}{HTML}{C4FFF9}
% \definecolor{ablation3}{HTML}{9CEAEF}
% \definecolor{ablation4}{HTML}{68D8D6}
% \definecolor{ablation5}{HTML}{3DCCC7}
% \definecolor{ablation6}{HTML}{07BEB8}



\usepackage[]{algpseudocode}
\usepackage[]{algorithm}
\usepackage{float}
\algtext*{EndFor}%
\algtext*{EndProcedure}%


% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{adjustbox}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{scalerel,xparse}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
%\usepackage[dvipsnames]{xcolor}

\renewcommand{\floatpagefraction}{.8}%
\renewcommand{\textfraction}{.1}%
\setcounter{totalnumber}{5}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{cleveref}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{placeins}

\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}

\newcommand\mymathop[1]{\mathop{\operatorname{#1}}}


% -------------- prompt box setup ---------------- %
% \definecolor{bggray}{rgb}{0.95, 0.95, 0.95}
% \usepackage[%
%     framemethod=tikz,
%     skipbelow=\topskip,
%     skipabove=\topskip
% ]{mdframed}
% \DeclareUnicodeCharacter{2212}{−}
% \usepgfplotslibrary{groupplots,dateplot}
% \usetikzlibrary{patterns,shapes.arrows}
% \pgfplotsset{compat=newest}


\input{xling-write}

% \title{Rewritten Inputs, Refined Outputs: Enhancing Machine Translation through Source Text Rewrites}
% \title{Does Rewriting Inputs Improve Translations from Large Language Models?}
% \title{Rewriting Inputs for Translation with Large Language Models}
% \title{\textit{Rewritten} Inputs, \textit{Refined} Outputs: \\
% Does Rewriting with Language Models Improve Translation?}
\title{Automatic Input Rewriting Improves Translation \\ with Large Language Models}

% \mc{or should we mention simplification?}
% \zk{I think current title is okay without mentioning simplification}

\author{Dayeon Ki \\
  University of Maryland \\
  \texttt{dayeonki@umd.edu} \\\And
  Marine Carpuat \\
  University of Maryland \\
  \texttt{marine@cs.umd.edu} \\}


\begin{document}
\maketitle


% \mdfsetup{%
%     leftmargin=0pt,
%     rightmargin=0pt,
%     backgroundcolor=bggray,
%     middlelinecolor=black,
%     roundcorner=3
% }
% \newtcolorbox[list inside=prompt,auto counter,number within=section]{prompt}[1][]{
%     colbacktitle=black!60,
%     fonttitle=\small,
%     coltitle=white,
%     fontupper=\footnotesize,
%     boxsep=4pt,
%     left=0pt,
%     % right=0pt,
%     top=0pt,
%     bottom=0pt,
%     boxrule=1pt,
%     #1,
% }


\begin{abstract}
Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.\footnote{We release our code and dataset at \url{https://github.com/dayeonki/rewrite_mt}.}


% using the \textsc{Tower} LLM \citep{alves2024tower},\mc{Need to update to reflect experiments actually included in the paper}

% \mc{The single most important part of the human evaluation is meaning preservation since it is harder to assess automatically, so the take-away needs to be mentioned here, and in all the other relevant spots in the paper (intro, conclusion, etc.).} 


% \mc{X tasks}

% \mc{Is the 21 number still correct?}
% \zk{Yes, I counted and it totals 21 methods.}

% We first consider stylistic rewrites that are agnostic to MT, and tailor them to the MT task by assessing input translatability with quality estimation.

%sign  according to MT-agnostic style dimensions, an stylisti
%Rewriting inputs is a common strategy to enhance translation quality exploited by end users and in dedicated machine translation (MT) architectures.

%how effective is this approach when translating with Large Language Models (LLMs)? 

%We conduct an empirical study using the Tower multilingual LLM, primarily trained for translation-related tasks, across two distinct rewriting methods: \textbf{MT-Agnostic} (without translation-related knowledge) and \textbf{MT-Aware} (with translation signal), yielding three key findings. First, MT-agnostic style rewrites do not uniformly improve translations but re-ranking and fine-tuning to guide rewrites with reference-free quality estimation improves translation quality according to both automatic and human evaluations. Second, when a rewrite is easier to translate, it often fails to preserve the original meaning, which poses a challenge of Pareto optimization. Third, rewriting inputs complements post-editing outputs, demonstrating the effectiveness of combining both strategies to further improve translation quality\footnote{We will release all models, datasets, and code.}.

% \footnote{We release our code and dataset at \url{https://github.com/dayeonki/rewrite_mt}.}.


% \mc{TODOs in preparation for author response:
% (1) do a read through post deadline to catch any remaining issues
% (2) manual evaluation
% (3) possibly try other LLMs with the winning strategies (Aya? larger Tower?)
% (4) possibly try other held-out test sets that are not out-of-En lang pairs
% (5) possibly optimize simplification+MT prompts to maximize translatability and/or reference-based metrics on a devset through DSPy
% }

\end{abstract}

\input{page/00_introduction}
\input{page/01_method}
\input{page/02_experimental_setup}
\input{page/03_results}
\input{page/04_analysis}
\input{page/05_related_works}
\input{page/06_conclusion}
\input{page/07_limitation}
% \input{page/08 societal_concern}
\input{page/09_acknowledgement}


\bibliography{custom,inputrewrite}

\appendix


\section{Model and Experiment Details}
\subsection{Prompt Templates}
\label{appendix:prompt_templates}
In Tables \ref{tab:prompting_template} and \ref{tab:training_template}, we describe the prompt templates used for prompting and fine-tuning experiments, respectively. For stylistic rewriting, we use the same prompts as those used to train the \textsc{CoEdIT-XL} model. During prompting, we provide the original source as the input, while for fine-tuning, we provide the positive rewrite along with the source.


\subsection{Training Setup}
\label{appendix:parameters}
All models are trained using one NVIDIA RTX A5000 GPU. In practice, we find that fine-tuning converges in around 3 hours. We use a 90/10 train/validation data split and adopt QLoRA \citep{dettmers2023qlora}, a quantized version of LoRA \citep{hu2021lora}, for parameter-efficient training. We train \textsc{Tower-Instruct 7B} with 8-bit quantization, a LoRA rank of 16, a scaling parameter ($\alpha$) of 32, and a dropout probability of 0.05 for layers. We train for 10 epochs. All unspecified hyperparameters are set to default values.


\subsection{Decoding Strategy}
We use greedy decoding (no sampling) when generating rewrites for prompting experiments. We fix the temperature value to 0 throughout the experiments in order to eliminate sampling variations.


\input{table/huggingface_names}


\subsection{Dataset Details}
\label{appendix:dataset_details}
We provide detailed statistics of our training ($\mathcal{D}_{pos}$) and test dataset in Table \ref{tab:dataset_details}. For $\mathcal{D}_{pos}$, we only use rewrites where the \textsc{xCOMET}$(s', t')$ score is higher than the original \textsc{xCOMET}$(s, t)$ score. We further conduct a two-step pre-processing procedure: \textbf{1)} Remove duplicate instances and \textbf{2)} Remove lengthy instances where the upper threshold is set as Q$3 + 1.5 \times \text{IQR}$.


\section{Detailed Results}
\subsection{Full Results}
\label{appendix:detailed results}
In Tables \ref{tab:detailed_results_ende} to \ref{tab:detailed_results_enzh}, we present the detailed numerical results for all tested variations. Most rewrites yield higher \textsc{xCOMET}$(s,t)$ scores, indicating better translatability compared to the original baseline. For stylistic rewrites with \textsc{CoEdIT}, prompting to make the text easier to understand (Understandable) achieves the highest translatability score, while prompting to rewrite the text more formally (Formal) results in the highest translation quality. The Coherent prompt achieves the highest meaning preservation score but this is because most rewrites are merely copies of the original source (Appendix \ref{appendix:direct_copy}). Overall, we demonstrate that translatability-based selection method remains the most effective method, even outperforming scores from our fine-tuned LLMs.


\subsection{Impact of LLM}
\label{appendix:impact of llm}
Among the three LLMs used for prompting, \textsc{Tower-Instruct} performs the best in terms of the combined metric \textsc{xCOMET}$(s,t,r)$. Although it lags behind \textsc{LLaMA-2} and \textsc{LLaMA-3} in translatability, its meaning preservation score deteriorates the least, resulting in the highest overall score. \textsc{LLaMA-3} performs the best in terms of translatability, likely due to its more multilingual training data, with over 5\% of its pre-training dataset consisting of high-quality non-English data.\footnote{\url{https://ai.meta.com/blog/meta-LLaMA-3/}} This suggests that the amount of multilingual data in the pre-training phase may enhance the model's ability to generate more translatable rewrites. However, this advantage does not extend when comparing the \textsc{LLaMA} models to \textsc{Tower-Instruct}. Despite being inherently multilingual primarily trained on translation-related tasks, \textsc{Tower-Instruct} performs lower than the \textsc{LLaMA} models in translatability. This discrepancy can be attributed to \textsc{Tower-Instruct} not being specifically trained on rewriting tasks to improve MT quality, highlighting the importance of introducing translation-related knowledge for effective rewriting.


We further compare the results with off-the-shelf paraphrasing (\textsc{DIPPER}) and text-editing (\textsc{CoEdIT-XL}) tools. Despite being specifically trained for rewriting tasks, their rewrites are not as translatable as those generated by the prompted LLMs. For \textsc{DIPPER}, this may be due to its primary focus on paraphrasing, which has been shown to be less effective (\S \ref{simplification best}). In the case of \textsc{CoEdIT}, we attribute the lower performance to the model's smaller size (3B) compared to the 7B LLMs used for prompting.


\subsection{Same LLM vs. Different LLM}
\label{appendix:same llm}
We distinguish whether the LLM being prompted is the same as the one used as the MT system. Initially, we expected the highest improvements when prompting \textsc{Tower-Instruct}, which may incur self-preference bias, where the LLM favors its own outputs due to recognition \citep{panickssery2024llm}. However, our results indicate that prompting \textsc{Tower-Instruct} does not yield the most translatable rewrites. Instead, the LLaMA series models consistently outperform in this aspect. Interestingly, \textsc{Tower-Instruct} consistently produces rewrites that are more meaning-preserving compared to \textsc{LLaMA-2} or \textsc{LLaMA-3}, resulting in higher \textsc{xCOMET}$(s,t,r)$ scores overall. We conclude that prompting the same LLM used for the MT system is not helpful in generating more translatable rewrites, but these rewrites are better at preserving the intended meaning.


% \section{Re-ranking Details}
% \subsection{Top-1 Rewrites}
% A natural question that arises from using a re-ranker is whether the higher-ranked rewrites are consistent across language pairs. To find this, we track the source of the Top-1 rewrites. We increment a count for each rewriting method if the Top-1 rewrite originates from that method. If the Top-1 rewrite is coming from multiple rewriting methods, we count them towards both. As illustrated in Table \ref{tab:top1_rewrites}, Top-3 rewrites are coming from the same rewriting methods across tested language pairs are: simplification \small(\textsc{LLaMA-3})\normalsize, paraphrase \small(\textsc{LLaMA-3})\normalsize, and stylistic \small(\textsc{CoEdIT} Understandable)\normalsize. Thus, the most effective prompting rewriting methods are generalizable across different languages.

% \input{table/top1_rewrites}


% \subsection{Top-$k$ Rewrites}
% \label{appendix:top k rewrites}
% Our default re-ranker considers all 13 MT-agnostic rewriting methods. However, in practical scenarios, generating a large number of rewrites incurs significant costs. To simulate this, we limit our re-ranker to consider only the Top-$k$ rewrites, as outlined in Table \ref{tab:top-k rerank}. Figure \ref{fig:topk_rerank} illustrates the impact of varying $k$ on the \textsc{xCOMET}$(s,t)$ score. We observe that increasing the number of rewriting methods (higher $k$) improves translatability, as evidenced by a consistent increase in the \textsc{xCOMET}$(s,t)$ score. However, this improvement diminishes gradually, particularly for the \textsc{En-De} pair (0.027 → 0.013 → 0.006), where the score converges around Top-3. Overall, this underscores that re-ranking using a small subset of the best rewriting methods is sufficient.

% \input{figures/topk_rerank}


\section{Qualitative Evaluation}
\label{appendix: qualitative eval details}
\subsection{Copying Behavior}
\label{appendix:direct_copy}
To prevent LLMs from directly copying the original source, we explicitly state in the prompt to ``\textit{avoid directly copying the source}'' (Appendix \ref{appendix:prompt_templates}). However, we still observe some rewrites that are identical to the source sentence. We count the occurrences and compute the percentage per language pair in Table \ref{tab:direct_copy}. Note that we do not consider Translatability-Aware Selection rewrite method here since this involves selecting whether to keep the original source or use the rewrite based on translatability scores. The highest occurrence appears for stylistic rewrites using the \textsc{CoEdIT-XL} Coherent prompt, where the source is copied most of the time (82.2\%, 91.9\%, 93.2\% for \textsc{En-De}, \textsc{En-Ru}, and \textsc{En-Zh}, respectively).

\input{figures/success}

\subsection{What makes a Good Rewrite for MT?}
Qualitatively examining translation outputs reveals several common patterns, which motivate us to conduct a detailed qualitative analysis. Here, we aim to identify the properties that lead to meaning-equivalent rewrites that are easier to translate. We examine 200 data instances where each rewrite is the highest performing rewrite based on the \textsc{xCOMET}$(s,t)$ score. To focus on successful rewrites, we filter instances where \textsc{xCOMET}$(s',t')$ $>$ \textsc{xCOMET}$(s,t)$. Each rewrite is annotated with the following labels: (1) \textbf{Simplified}: Replaces complex words with simpler ones or reduces structural complexity; (2) \textbf{Detailed}: Adds information for better context; (3) \textbf{Fluency}: Restructures the sentence for better flow and readability. 
Examples of rewrites for each annotation label are in  Table \ref{tab:success_types}. 

As shown in Figure \ref{fig:success}, most successful rewrites are labeled as \textbf{Simplified}. This highlights the effectiveness of simplification, which has been consistently effective even in the context of LLMs. Notably, many simplified rewrites involve changing complex words to simpler, more conventional alternatives (e.g., ``Derry City \textit{emerged victorious} in the President's Cup as they \textit{ran out} 2-0 \textit{winners} over Shamrock Rovers.'' → ``Derry City \textit{won} the President's Cup title by \textit{defeating} Shamrock Rovers 2-0.''). This finding aligns with our conclusions from MT-Agnostic rewriting methods (\S \ref{3.1 mt-agnostic}), where simplification emerged as the best rewrite method among the prompting variations.


\section{Additional Results}
\subsection{Additional LLM Baselines}
\label{appendix:more_llms}
\paragraph{LLMs for Rewriting.}
Our initial experiments consist of 21 input rewriting methods across 3 LLMs (\textsc{LLaMA-2 7B}, \textsc{LLaMA-3 8B}, and \textsc{Tower-Instruct 7B}). In Table \ref{tab:more_rewrite_llms}, we present extended experiment results by applying simplification rewriting with two additional LLMs: \textsc{Aya-23 8B} \citep{aryabumi2024aya23openweight} and \textsc{Tower-Instruct 13B} \citep{alves2024tower}. The results confirms that simplification rewriting improves translation quality measured by \textsc{xCOMET}$(s,t,r)$ compared to the original baseline.

\paragraph{LLMs for MT.}
Furthermore, we initially relied on \textsc{Tower-Instruct 7B} as our MT system for all our experiments since it is specifically trained for translation-related tasks and has demonstrated superior MT performance (\S \ref{3 method}). However, we extend our analysis by comparing the original baseline and our winning strategy (simplification with \textsc{Tower-Instruct 7B}) using two additional LLMs as the MT system. As shown in Table \ref{tab:more_mt_llms}, our method outperforms the original baseline in terms of both the translation quality (\textsc{xCOMET}$(s,t,r)$) and \textsc{MetricX}$(s,t)$, regardless of the LLM used as the MT system.


\subsection{Additional Language Pairs}
\label{appendix:more_lang_pairs}
To assess the generalizability to other source languages, we test two of our winning strategies (simplification with \textsc{Tower-Instruct 7B} and inference-time selection) on seven additional into-English and non-English language pairs from the WMT-23 General MT task test set.\footnote{\url{https://www2.statmt.org/wmt23/translation-task.html}} As shown in Table \ref{tab:more_lang_pairs}, while translatability scores (\textsc{xCOMET}$(s,t)$) improve across all language pairs, translation quality (\textsc{xCOMET}$(s,t,r)$) improvements are less pronounced compared to out-of-English pairs. Notably, gains in translation quality are observed only for German-English (\textsc{De-En}) and Chinese-English (\textsc{Zh-En}) pairs. These results highlight the importance of input rewrites' quality, which is currently higher for high-resource source languages. This motivates further work to strengthen input rewriting for broader range of source languages.



% \section{Direct Preference Optimization}
% \label{appendix:dpo}
% \subsection{Preference Dataset}
% Further motivated by recent efforts to utilize preference learning strategies \citep{rafailov2023direct, xu2024contrastive, xu2024advancing, he2024improving}, we collect triplets of dispreferred (negative) and preferred (positive) rewrites for each source sentence via pairwise comparisons. We then use this signal with Direct Preference Optimization (\citet{rafailov2023direct}, DPO) to further align the initial fine-tuned model with an MT-based objective. Direct Preference Optimization (\citet{rafailov2023direct}, DPO) needs input $x$ and preferred/dispreferred outputs $y_w$/$y_l$, where $y_w > y_l$ amongst preference pair. We set $x$ as the above prompt templates with corresponding inputs from $\mathcal{D}_{ft}$. For each source sentence $s$, we collect a triplet of source, positive (preferred), and negative (dispreferred) rewrite $(s, s'_p, s'_n)$ where we define positive rewrites as rewrites having higher \textsc{xCOMET}$(s,t)$ score and negative rewrites as those having lower score than the original. We consider all combinations of positive and negative rewrites, thus there can be multiple triplets for the same source sentence. We filter to only use unique triplet combination in our final preference dataset $\mathcal{D}_{dpo}$. We conduct the same 2-step pre-processing procedure taken for the fine-tuning dataset $\mathcal{D}_{pos}$: 1) Remove duplicated instances; and 2) Remove lengthy instances where the upper threshold is set as Q$3 + 1.5 \times \text{IQR}$. We show the number of instances in Table \ref{tab:dpo_dataset_details}.

% \input{table/dpo_dataset_details}

% With this data, we update our initial model $\pi_0$  to align better rewrite model $\pi_{dpo}$ with DPO. We explore the same prompt template variants as in the fine-tuning setting (Appendix \ref{appendix:prompt_templates}).



% \subsection{Training Setup}
% We find that DPO converges in approximately 10 hours. We use a 90/10 train/validation data split and start with a supervised fine-tuned model as our initial model, $\pi_0$. We perform DPO with a training batch size of 4, a beta value of 0.1, a learning rate of 2e-4, a warm-up ratio of 0.05, a cosine learning rate scheduler, a maximum gradient norm clipping value of 0.3, a maximum prompt length of 1024 tokens, and 10 training epochs. All unspecified hyperparameters are set to their default values.

% \subsection{Results}
% In Tables \ref{tab:detailed_results_ende} to \ref{tab:detailed_results_enzh}, we compare DPO results to other rewriting methods. While previous works \citep{rafailov2023direct, wang2023making, tunstall2023zephyr} have demonstrated the effectiveness of DPO in aligning LLMs with preference datasets, we find that alignment-based learning is less effective for generating better rewrites for MT. Although \textsc{xCOMET}$(s,t)$ scores improve by +0.59 and +0.71 for \textsc{En-De} and \textsc{En-Ru}, respectively, \textsc{xCOMET}$(s,r)$ scores decrease by even wider margins. Among the three prompt template variations (Basic, MT, Ref), providing both the source and MT (MT) is most effective. We attribute this to the use of both preferred and dispreferred rewrites in DPO, suggesting that providing the MT as context for comparison may be helpful. Overall, DPO outperforms the baseline, but translatability-based selection remains the best rewriting method.


\section{Human Annotation Details}
\label{appendix:annotation details}
We use Qualtrics\footnote{\url{https://www.qualtrics.com}} to design our survey and Prolific\footnote{\url{https://www.prolific.com}} to recruit human annotators fluent in the tested target language.

\subsection{Original MT vs. Rewrite MT Details}
\label{appendix:mt_details}
We randomize the order of the two sentences (original MT and rewrite MT) to mitigate position bias. Annotators evaluate which sentence is better across four dimensions: fluency, understandability, level of detail, and meaning preservation. The entire survey is estimated to take approximately 20 minutes to complete. We recruit a total of 9 annotators and provide a compensation of 5 US dollars per survey (15 US dollars/hr), totaling 45 US dollars.


\subsection{Original vs. Rewrite Details}
\label{appendix:details}
Each annotator is tasked to judge how well the rewritten sentence preserves the meaning of the original source sentence. The survey is estimated to take approximately 30 minutes to complete. We recruit a total of 3 annotators. We offer a compensation of 7.5 US dollars per survey (15 US dollars/hr), totaling 22.5 US dollars.


\subsection{Annotator Instructions}
In Figures \ref{fig:human_intro} to \ref{fig:human_example_3}, we present the instructions and survey content provided to annotators. For the Original MT vs. Rewrite MT evaluation, each annotator reviews 20 sets of examples. Each question consists of two parts: \textbf{1)} comparing the two sentences based on fluency, understandability, and level of detail, and \textbf{2)} selecting which sentence better preserves the meaning of the reference translation. For the Original vs. Rewrite evaluation, each annotator reviews 30 sets of examples. Additionally, a free-form text box is provided alongside each example for annotators to offer feedback or suggestions.


\section{Time \& Computational Efficiency}
\label{appendix:inference_cost}
We show that on average, rewriting with our winning strategy is not a resource-intensive option for downstream applications in terms of both time and computation. For approximately 1.5K sentences, the rewrite and MT pipeline using our winning strategy (simplification with \textsc{Tower-Instruct 7b} takes 1 hour, compared to 30 minutes for the MT process alone. All variants of our prompting experiments are conducted using a single NVIDIA RTX958 A5000 GPU. In terms of efficiency compared to automatic post-editing (\S \ref{res:post-editing}), both approaches remains equivalent in time and computational requirements since the rewriting or post-editing process only differs in its position within the pipeline. Input rewriting modifies the source before the MT system, while output post-editing adjusts the translation after the MT system.


% \subsection{Annotator Feedback Details}
% \label{appendix:further comments}
% In our survey, we include an optional question for each example to collect additional feedback or comments from the annotators. We present some of this feedback in Table \ref{tab:annotator_feedback}. Annotators commented that they prefer the translations of rewrites over the original translations because 1) they are easier to understand; 2) they contain words that better suit the target language context; and 3) they are more precise. These comments align with the higher winning rates in the fluency, understandability, and detail dimensions.


\input{table/prompting_template}
\input{table/training_template}
\clearpage

% \input{table/topk_rerank}
\input{table/dataset_details}
\input{table/detailed_results_ende}
\clearpage

\input{table/detailed_results_enru}
\input{table/detailed_results_enzh}
\clearpage

\input{table/direct_copy}

% \input{table/annotator_feedback}
% \clearpage

% \input{table/good_rewrites_ende}
% \input{table/good_rewrites_enru}
% \input{table/good_rewrites_enzh}
% \clearpage

\input{table/success_types}
\clearpage
% \input{table/inputvsoutput}

\input{table/readability_ende}
\input{table/readability_enru}
\input{table/readability_enzh}
\clearpage

\input{table/more_rewrite_llms}
\input{table/more_mt_llms}
\input{table/more_lang_pairs}

\input{figures/human_intro}
\input{figures/human_example_1}
\input{figures/human_example_2}
\input{figures/human_example_3}

\end{document}
