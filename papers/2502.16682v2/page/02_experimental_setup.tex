\section{Experimental Setup}


\input{table/main_results}


\subsection{Model \& Data}

\paragraph{MT System.} We use \textsc{Tower-Instruct} 7B as our MT system for all our experiments since it is specifically trained for translation-related tasks and has demonstrated superior MT performance compared to other LLMs \citep{alves2024tower}.

\paragraph{Rewriting Models.} For prompting experiments, we use 7B variant of three open-weight LLMs in zero-shot setting: \textsc{LLaMA-2} \citep{touvron2023llama} \---\ the base model for \textsc{Tower-Instruct}, \textsc{LLaMA-3} \citep{grattafiori2024llama3herdmodels} \---\ more recent multilingual model compared to \textsc{LLaMA-2}, and \textsc{Tower-Instruct} \citep{alves2024tower} \---\ the same LLM as used for our MT system.\footnote{The HuggingFace model names are detailed in Appendix Table \ref{tab:huggingface_api}.} For supervised fine-tuning, we draw training samples from the English-German and English-Russian subset from WMT-20, 21, and 22 General MT task datasets \citep{freitag2021experts}\footnote{We do not consider English-Chinese pair here since this language pair is not supported in the dataset.}, and provide detailed parameter settings in Appendix~\ref{appendix:parameters}.


%We first prompt LLMs using MT-agnostic rewriting methods to generate rewrites $s'$ for each $s$. Then, we translate the original source and rewrites using Tower-Instruct 7B to get MT outputs $t$ and $t'$. We use \textsc{xCOMET} to compute scores between $s'$ and $t'$, which estimates translation quality for each rewrite. Our goal of fine-tuning is to generate better rewrites than the original source. Thus, we create $\mathcal{D}_{pos}$, a subset of $\mathcal{D}_{ft}$, which only contains the rewrites where \textsc{xCOMET}$(s', t')$ > \textsc{xCOMET}$(s,t)$. 
%For prompt template, we compare three variants:
%\begin{itemize}[leftmargin=*, itemsep=2pt, parsep=-1pt]
% \item \textbf{Basic}: Given $s$, generate $s'$.
% \item \textbf{MT}: Given $s$ and $t$, generate $s'$.
% \item \textbf{Reference}: Given $s$ and $r$ (reference translation), generate $s'$.
%\end{itemize}
%Specific prompt templates are outlined in Appendix \ref{appendix:prompt_templates}.


\paragraph{Test Data.}
We use the WMT-23 General MT task\footnote{\url{https://www2.statmt.org/wmt23/translation-task.html}} from the \textsc{TowerEval} dataset\footnote{\url{https://huggingface.co/datasets/Unbabel/TowerEval-Data-v0.1}} to guarantee that it was held out from the various training stages. We focus on translation from English into German (\textsc{En-De}), Russian (\textsc{En-Ru}) and Chinese (\textsc{En-Zh}) for an extensive empirical comparison, and then test whether the most promising approaches generalize to translation from English into Czech (\textsc{En-Cs}), Hebrew (\textsc{En-He}) and Japanese (\textsc{En-Ja}). % We use combination of dev and test split from the original dataset\footnote{We translate each source sentence using our MT system and do not use the original NLLB 3B \citep{nllbteam2022language} translations.}. Reference translations are human-produced directly based on the source sentences.
See Appendix Table~\ref{tab:dataset_details} for data statistics.

% \mc{the key quesiton is how? directly based on the source? or are the reference translations generated by asking people to post-edit NLLB output?}
% \zk{I checked again the Tower paper, TowerEval huggingface page and the dataset I used -- the datasets that TowerEval name as "WMT23 Automatic Post-Edition" does not corresponds to actual WMT23 Automatic Post-edition (only has En-Marathi pair). The dataset here are source and reference from WMT23 general mt task with translations from nllb model. So I'll reframe it as "we combine development and test sets of WMT23 General MT task" and also get rid of en-ru, en-zh results as held-out test sets.}
% \mc{okay}

% \mc{How were the references created? Are they translatinos of the source from scratch or are they created by post-editing the NLLB output? if it is the latter it might create a weird bias.}

\subsection{Evaluation Metrics}
We use \textsc{xCOMET} \citep{guerreiro2023xcomet} and \textsc{MetricX} \citep{juraska-etal-2023-metricx} to evaluate different aspects of rewrite quality. Specifically, we use \textsc{xCOMET-XL}\footnote{\url{https://huggingface.co/Unbabel/XCOMET-XL}} and \textsc{MetricX-23-XL}.\footnote{\url{https://huggingface.co/google/metricx-23-xl-v2p0}} Higher scores indicate better performance for \textsc{xCOMET}, while lower scores are better with \textsc{MetricX}.


\paragraph{Translatability.}
%Generic \textit{translatability} has been defined as ``a measurement of the time and effort it takes to translate a text'' \citep{kumhyr-etal-1994-internationalization}. Here, we define translatability as a measure of how well a given source sentence can be translated by a particular MT system. More translatable inputs yield better MT outputs \citep{uchimoto-etal-2005-automatic}, so 
We quantify translatability with the quality estimation score for a specific input--output pair (\textsc{xCOMET}$(s', t')$ or \textsc{MetricX-QE}$(s',t')$). A rewrite $s'$ of the original input $s$ is considered easier to translate if \textsc{xCOMET}$(s', t')$ is higher than \textsc{xCOMET}$(s, t)$.

\paragraph{Meaning Preservation.} We do not want rewrites that are easier to translate at the expense of changing the original meaning. Our meaning preservation metric evaluates how well the rewrite maintains the intended meaning of the translation as represented by the reference \citep{Graham2015CanMT}. We use a reference-based metric as opposed to using the semantic similarity between $s$ and $s'$ because it abstracts the meaning away from the specific formulation of $s$, reducing overfitting. We compute \textsc{xCOMET} scores between the rewrites and reference translations (\textsc{xCOMET}$(s',r)$). The desired behavior is to minimize the deterioration in \textsc{xCOMET}$(s',r)$ compared to \textsc{xCOMET}$(s,r)$.


\paragraph{Translation Quality.} We additionally report the combined evaluation metric, \textsc{xCOMET}$(s',t',r)$ to take into account of the trade-off between the two above metrics, and \textsc{MetricX}$(t',r)$ which also assesses translation quality of the rewrite but is not informed by the updated source $s'$.

%\paragraph{\textsc{MetricX}.} In addition to \textsc{xCOMET}, we consider \textsc{MetricX-23-XL} \citep{juraska-etal-2023-metricx} as a secondary evaluation metric. We use two score variants: \textsc{MetricX}$(s',t')$ for quality estimation and \textsc{MetricX}$(t',r)$ as a reference-based score.