\section{Analysis}
\label{5 analysis}

\subsection{Simplifying Inputs Improves MT Readability}
\label{readability}

Simplification as an input rewriting strategy can balance translatability and meaning preservation, leading to overall improvements in translation quality. We also examine whether this enhances the readability of both inputs and, subsequently, translation outputs. In Table~\ref{tab:readability}, we present the Flesch Reading Ease score\footnote{\url{https://en.wikipedia.org/wiki/Flesch-Kincaid_readability_tests}} and Gunning Fog index\footnote{\url{https://en.wikipedia.org/wiki/Gunning_fog_index}} to measure input readability, and the Vienna formula (WSTF) \citep{zowalla2023readability} and the Russian version of Flesch Readability test \citep{inbook} to assess output readability for \textsc{En-De} and \textsc{En-Ru}, respectively.

As expected, input readability improves across all simplification methods, whether used in MT-Agnostic (\textsc{LLaMA-2}, \textsc{LLaMA-3}, and \textsc{Tower-Instruct} in Table~\ref{tab:readability}) or Translatability-Aware (Selection in Table~\ref{tab:readability}) manner. Interestingly, simplification not only leads to more readable input but also more readable outputs, with gains of up to 0.22 WSTF scores for \textsc{En-De} and 0.95 Flesch scores for \textsc{En-Ru}. We provide several qualitative examples in Appendix Tables \ref{tab:readability_ende} to \ref{tab:readability_enzh} that illustrate how simplification rewrites can lead to varying degrees of readability improvements in both inputs and translation outputs.


\input{table/readability}
\input{table/complementary}


\input{figures/human_results}


\subsection{Input Rewriting outperforms Post-Editing}
\label{res:post-editing}

The symmetric task to input rewriting is post-editing, which focuses on improving and correcting errors in translation outputs. Can post-editing alone achieve the same improvements, or are both strategies \textit{complementary}? To explore this, we compare input rewriting to post-editing by prompting \textsc{Tower-Instruct}\footnote{We focus on \textsc{Tower-Instruct} as it is a multilingual LLM capable of rewriting in non-English target languages.} to simplify either inputs or outputs. As shown in Table~\ref{tab:complementary}, rewriting inputs (\textbf{I}) offers a notable advantage over post-editing outputs (\textbf{Owo}), even when post-editing is guided by the input sentence (\textbf{Ow}). Combining input rewriting and post-editing (\textbf{I+O}) yields the highest translation quality, though the difference compared to input rewriting alone is not statistically significant. This confirms that rewriting text for better translatability before translation plays a more decisive role than post-editing the output.\footnote{We compare time and computational efficiency for input rewriting and output post-editing in Appendix \ref{appendix:inference_cost}.}




\subsection{Human Evaluation}
\label{human evaluation}


\paragraph{Original MT vs. Rewrite MT.}
We conduct a manual evaluation to determine whether bilingual human annotators rate translations generated using our winning rewrite method (simplification with \textsc{Tower-Instruct}) as superior to the original translations. For each language pairs (\textsc{En-De}, \textsc{En-Ru}, \textsc{En-Zh}), we randomly select 20 pairs of instances, resulting in a total of 180 annotations from three annotators per pair. Inter-annotator agreement, measured by Fleiss' Kappa\footnote{\url{https://en.wikipedia.org/wiki/Fleiss_kappa}}, is moderate, with values of 0.43, 0.39, and 0.51 for \textsc{En-De}, \textsc{En-Ru}, and \textsc{En-Zh}, respectively. For each instance, annotators are first provided with two translations and asked to evaluate on three axes: \textbf{1)} Fluency, \textbf{2)} Understandability, and \textbf{3)} Level of detail. Subsequently, we provide the reference translation, and annotators are asked to assess \textbf{4)} Meaning preservation. Annotators are also given the option to provide free form comments. Further details on the annotation set-up are available in Appendix \ref{appendix:mt_details}.

As illustrated in Figure~\ref{fig:human_results}, the human evaluation results confirm that translations from simplified inputs are rated as more fluent, understandable, and better at preserving the meaning of the reference translation. While this improvement is clear for the \textsc{En-De} and \textsc{En-Zh} pair, for \textsc{En-Ru} pair, annotators rate original MT as more fluent and more faithful to the original meaning.\footnote{Note that the Fleiss's Kappa scores indicate that there is more disagreement between annotators for \textsc{En-Ru} pair.} Some \textsc{En-Ru} annotators who preferred the original MT noted that it often retained a more accurate sense of the words in the reference. In contrast, those who favored the simplified rewrite MT highlighted that translations are more contextually appropriate, easier to read, and more comprehensible than the original MT.


\paragraph{Original vs. Rewrite.}
Our automatic meaning preservation metric evaluates the extent to which the original meaning is retained in the rewrite by comparing the rewritten source to the reference translation, rather than to the original source \citep{Graham2015CanMT}. Comparing to the original source is in the same language, but introduces a bias toward the original wording. On the other hand, comparing to the reference involves a cross-lingual comparison and is affected by unstable quality of references \citep{kocmi-etal-2022-findings}, but is less biased toward the original wording of the source.

To complement our automatic metric, we conduct a manual evaluation to assess how well the rewrites from simplification with \textsc{Tower-Instruct} preserve the meaning of the original source. We randomly sample 30 pairs of instances and collect three annotations per pair, totaling 90 annotations. Annotators are presented with both the original and rewritten sources and asked to evaluate how well the rewrite captures the meaning of the original source using a 4-point Likert scale (1: Does not capture meaning, 2: Partially, 3: Mostly, 4: Fully). Inter-annotator agreement by Fleiss' Kappa is 0.45. Of the 90 annotations, 55 were rated as 4, 27 as 3, 7 as 2, and 1 as 1, resulting an average score of 3.51. These results indicate that simplified rewrites generated by \textsc{Tower-Instruct}, although compared against the original source, still largely preserve the original meaning. Further details are provided in Appendix \ref{appendix:details}.