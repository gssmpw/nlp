\section{Related Work}

\paragraph{Rewriting with LLMs.} 
Recent advances in LLMs have demonstrated impressive zero-shot capabilities in rewriting textual input based on user requirements \citep{shu2023rewritelm}. Most LLM-assisted rewriting tasks focus on query rewriting \citep{efthimiadis1996query}, which aims to reformulate text-based queries to enhance their representativeness and improve recall with retrieval-augmented LLMs \citep{mao-etal-2023-search, Zhu_2024}. Rewriting methods include prompting LLMs both as rewriters and rewrite editors \citep{ye-etal-2023-enhancing, kunilovskaya-etal-2024-mitigating}, and training LLMs as rewriters using feedback alignment learning \citep{ma-etal-2023-query, mao2024rafe}. Another line of work focuses on style transfer, where the goal is to rewrite textual input into a specified style \citep{wordcraft, hallinan2023steer}. Our research aligns with efforts to rewrite texts with LLM assistance; however, unlike these works, we focus on rewriting source inputs to enhance MT quality.

\paragraph{Quality Estimation Metrics.}
% These metrics use complex neural networks to estimate the quality of MT outputs more effectively. 
The discrepancy between lexical-based metrics (e.g., \textsc{BLEU} \citep{papineni-etal-2002-bleu}, \textsc{chrF} \citep{popovic-2015-chrf}) and human judgments \citep{ma-etal-2019-results} has led to research in \textit{neural} metrics. Particularly, quality estimation (QE) metrics, which compute a quality score for the translation conditioned only on the source sentence, have demonstrated benefits in improving MT quality. QE metrics are used for various purposes, including filtering out low-quality translations during training \citep{tomani2024qualityaware}, applying to post-editing workflows \citep{bechara2021role}, and providing feedback to users of MT systems \citep{mehandru2023physician}. In our experiments, we use \textsc{xCOMET} as our main evaluation metric, as it shows the best correlation with human judgments \citep{agrawal2024automatic}. We primarily use \textsc{xCOMET} as a QE metric to compute translatability, further providing this information as knowledge to LLMs to improve MT quality.

\paragraph{Rewriting MT Outputs.} 
The symmetric task of post-editing MT outputs has received significantly more attention than rewriting MT inputs. Most recent work relies on LLMs to automatically detect and correct errors in MT outputs using their internal knowledge \citep{raunak-etal-2023-leveraging, zeng2024improving, chen2024iterative}, with the help of external feedback \citep{ki2024guiding, xu2024llmrefine} or through fine-tuning \citep{treviso2024xtowermultilingualllmexplaining}. In contrast, the task of rewriting MT inputs to make them more suitable for translation has been relatively underexplored with LLMs. While there have been some efforts in query rewriting and style transfer to improve retrieval \citep{mao-etal-2023-search, Zhu_2024} and stylistic coherence \citep{ye-etal-2023-enhancing, hallinan2023steer}, the specific application of LLMs to rewrite inputs for the purpose of enhancing MT quality is still emerging. Our research addresses this gap by focusing on the potential of LLM-assisted input rewriting to improve the translatability and quality of the resulting translations.

% Traditional automatic metrics for MT evaluation rely on lexical-based approaches, calculating the evaluation score based on lexical overlap between a candidate translation and a reference translation (ex. \textsc{BLEU} \citep{papineni-etal-2002-bleu}, \textsc{METEOR} \citep{banerjee-lavie-2005-meteor}, and \textsc{chrF} \citep{popovic-2015-chrf}). However, evidence indicate that these lexical metrics do not consistently correlate with human judgments \citep{ma-etal-2019-results}. This discrepancy led to

% \mc{This paragraph is a little weak --- generic overview of QE methods. It would be much stronger to focus instead on discussing how QE has been sued to improve MT quality in the past.}

% \paragraph{Preference Alignment Learning.}
% An increasing body of work seeks to align LLMs with preference datasets, either collected from humans (RLHF, \citet{ouyang2022training}) or AI (RLAIF, \citet{bai2022constitutional}). However, RLHF procedure is complex, where we need to first fit a reward model over human preferences, and then use reinforcement learning (RL) algorithms such as Proximal Policy Optimization (\citet{schulman2017proximal}, PPO) to find a policy that maximizes the learned reward \citep{ziegler2020finetuning, ouyang2022training}. In contrast, reward-free methods offer simpler training procedure by directly training LLMs on preference without the need of reward modeling or RL \citep{yuan2023rrhf}. Among these, Direct Preference Optimization (\citet{rafailov2023direct}, DPO) has shown strong performance while being simple, thus we adopt DPO for our preference alignment learning.