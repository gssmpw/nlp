\definecolor{light blue}{RGB}{215, 242, 252}
\definecolor{light purple}{RGB}{247, 215, 252}
\definecolor{light orange}{rgb}{0.9961, 0.875, 0.7188}



\section{Results}
\label{4 results}

We first extensively compare rewrite strategies focusing on the overall translation quality achieved by MT-Agnostic rewrites (\S \ref{simplification best}) and Translatability-Aware rewrites (\S \ref{input selection}). To understand how rewrites change translations, we then analyze the trade-offs between translatability and meaning preservation (\S \ref{pareto optimality}). Finally, we test whether the best-performing methods identified so far generalize to new language pairs (\S \ref{sec:newlanguages}).


\subsection{Simplifying Inputs Works Best}
\label{simplification best}
We first compare the \hl{MT Agnostic} rewriting methods: simplification, paraphrasing, and stylistic edits. Due to space limits, we show the best and worst performing variations for each input rewriting method based on the overall translation quality metric \textsc{xCOMET}$(s,t,r)$ for each language pair in Table~\ref{tab:main_results}. Full results are available in Appendix \ref{appendix:detailed results}.

Results show that all rewriting strategies improve translatability, but only \textbf{simplification} also improves the overall translation quality. Even the lowest performing rewrites reach higher translatability than the original baseline. Each method surpasses the baseline by up to 0.056 and 0.027 \textsc{xCOMET}$(s,t)$ average scores for \textsc{En-De}, up to 0.058 and 0.036 average scores for \textsc{En-Ru}, and up to 0.054 and 0.028 average scores for \textsc{En-Zh} pair. Trends are consistent with \textsc{MetricX}$(s,t)$. However, making inputs easier to translate often degrades quality when comparing against references $r$. Simplification with \textsc{Tower-Instruct} distinguishes itself by improving translation quality based on \textsc{xCOMET}$(s,t,r)$ scores and maintaining it according to the \textsc{MetricX}$(t,r)$ scores \---\ a harder metric to improve since the reference might be biased toward the original wording of the source.

%However, as detailed in Appendix Table~\ref{tab:detailed_results_ende} to \ref{tab:detailed_results_enzh}, this improvement comes at the cost of lower meaning preservation score, resulting in lower overall translation quality \textsc{xCOMET}$(s,t,r)$ scores for most methods. A notable exception is simplification, which outperforms the the original MT in the \textsc{xCOMET}$(s,t,r)$ metric.

Among the three LLMs used for simplification, \textsc{Tower-Instruct} achieves the best translation quality, while \textsc{LLaMA-3} excels in translatability at the expense of meaning preservation. Interestingly, there is no benefit to using a separate LLM, even one fine-tuned specifically on paraphrasing or style edits such as \textsc{DIPPER} or \textsc{CoEdIT}. Overall, the best performing method for MT-agnostic rewrites is simplification with \textsc{Tower-Instruct}, the same model we use as our MT system. We attribute this to \textsc{Tower-Instruct} being instruction fine-tuned on translation related tasks (but not simplification) and having more domain knowledge of the WMT dataset used in our evaluation.\footnote{\url{https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1}} 


% Further details are provided in Appendix \ref{appendix:impact of llm} and \ref{appendix:same llm}.

As shown in Table~\ref{tab:main_results}, simplifying with \textsc{Tower-Instruct} still holds the top spot when compared to \hlpurple{Task-Aware} rewriting methods, as indicated by higher \textsc{xCOMET}$(s,t,r)$ scores. This suggests that injecting knowledge about the end-task (MT) to LLMs is less effective than simplifying inputs to improve translation quality.

Overall, these results confirm the intuition that simpler text is easier to translate, but establish that rewrites are not uniformly helpful for translation quality, motivating the need for more selective input rewriting strategies.

%We observe similar trends with our secondary evaluation metric, \textsc{MetricX}. Simplification with \textsc{Tower-Instruct} consistently improves both \textsc{MetricX}$(s,t)$ and \textsc{MetricX}$(t,r)$ scores over the baseline; -0.534 and -0.015 for \textsc{En-De}, -1.4 and -0.107 for \textsc{En-Ru}, and -1.924 and -0.055 for \textsc{En-Zh}. However, note that the latter score may be inherently biased toward the original MT outputs since it does not use the source as part of input.

\input{figures/pareto_frontier}

\subsection{Selection via Translatability Improves MT}
\label{input selection}

We evaluate the impact of inference-time selection based on \hlorange{translatability} scores (\textit{Selection} in Table~\ref{tab:main_results}), and compare it further with the more expensive supervised fine-tuning strategy (\textit{Fine-tune}). 

All language pairs consistently benefit from selection. Translation quality improves significantly, with average \textsc{xCOMET}$(s,t,r)$ gains of 0.024 for \textsc{En-De}, 0.031 for \textsc{En-Ru}, and 0.025 for \textsc{En-Zh}, marking the best performance among all variants. \textsc{MetricX}$(t,r)$ scores confirm this trend, showing average improvements of 0.073 for \textsc{En-De}, 0.198 for \textsc{En-Ru}, and 0.076 for \textsc{En-Zh}. At the segment level, rewrites are preferred to original inputs in 1197/1557 cases for \textsc{En-De}, 1610/2074 cases for \textsc{En-Ru}, and 2163/3074 cases for \textsc{En-Zh}. Fine-tuning shows smaller gains compared to MT-Agnostic or Task-Aware methods, both in terms of translatability and translation quality, despite being more resource-intensive.

In summary, the results suggest that inference-time selection of inputs based on translatability scores is a promising strategy, outperforming MT-agnostic rewrites and rewrites obtained via a more expensive fine-tuning process.
% \footnote{We also found no benefit from adding preference learning with Direct Preference Optimization \citep{rafailov2023direct}. More details are provided in Appendix \ref{appendix:dpo}.}

\subsection{Input Rewriting Trades Off Translatability and Meaning Preservation}
\label{pareto optimality}

We observe a moderate negative correlation between translatability and meaning preservation scores, with Pearson coefficients of -0.48, -0.66, and -0.52 for \textsc{En-De}, \textsc{En-Ru}, and \textsc{En-Zh}, respectively. This trade-off between the two metrics poses a Pareto optimization challenge: when a rewrite is easier to translate, it often results in lower meaning preservation. Therefore, we aim to find Pareto optimal solutions, which balance these trade-offs on a Pareto frontier \citep{huang-etal-2023-towards}.\footnote{In Pareto optimization, Pareto optimal solutions are those where no single solution outperforms another in all tasks \citep{pareto}. The set of Pareto optimal solutions forms the Pareto frontier.}

In Figure~\ref{fig:pareto_frontier}, we visualize our two objectives, translatability and meaning preservation, on each axis and identify the Pareto frontier. The results are consistent with the overall translation quality metric, \textsc{xCOMET}$(s,t,r)$, where the scores for rewriting methods on the Pareto frontier are consistently the same as or on par with the original baseline. This also aligns with our earlier findings from comparing MT-Agnostic and Task-Aware rewrites (\S \ref{simplification best}), where simplification with \textsc{Tower-Instruct} lies on the Pareto frontier for \textsc{En-De} and \textsc{En-Ru}. Even for \textsc{En-Zh}, although this does not lie on the frontier, it has a higher \textsc{xCOMET}$(s,t,r)$ score (0.802) than the original baseline (0.794). Furthermore, the best rewriting method according to \textsc{xCOMET}$(s,t,r)$, translatability-based selection (\S \ref{input selection}), always lies on the Pareto frontier across all language pairs.




\input{table/heldout}

\subsection{Best Input Rewriting Strategy Improves MT on Held-out Test sets}
\label{sec:newlanguages}

We evaluate whether the top methods that have emerged from the controlled empirical comparison conducted so far generalize to further test settings. As shown in Table~\ref{tab:heldout}, we test both simplification with \textsc{Tower-Instruct} (\textit{Simplification}) and translatability-based input selection (\textit{Selection}) on new test sets from the  WMT-23 General MT task, English-Czech (\textsc{En-Cs}), English-Hebrew (\textsc{En-He}), and English-Japanese (\textsc{En-Ja}) to assess generalization to lower-resource target languages.

Both simplification and translatability-based selection lead to progressive improvements in translation quality, as measured by \textsc{xCOMET}$(s,t,r)$. Notably, the selection strategy tends to excel in language pairs with lower-resource target languages, showing translation quality gains of 0.064, 0.043, 0.051 scores for \textsc{En-Cs}, \textsc{En-He}, \textsc{En-Ja}, respectively, compared to increases of 0.017, 0.031, and 0.025 for \textsc{En-De}, \textsc{En-Ru} and \textsc{En-Zh}. At the segment level, rewrites are also more preferred over original inputs, selected in 1395/2074 cases for \textsc{En-Cs}, 1309/2074 for \textsc{En-He}, and 1411/2074 for \textsc{En-Ja}. \textsc{MetricX} trends are consistent.

In sum, our findings generalize well to held-out test sets, further validating the effectiveness of the translatability-based selection strategy. This approach offers a practical and scalable solution for input rewriting across a broader range of domains and language pairs, though there are many other dimensions that remain unexplored. We have conducted initial experiments with additional LLMs and source languages, shown in Appendix \ref{appendix:more_llms} and \ref{appendix:more_lang_pairs}, which confirms our previous findings that simplification rewriting enhances translation quality. We leave a more comprehensive exploration of this direction for future work.
