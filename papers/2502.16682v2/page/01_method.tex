\definecolor{light blue}{RGB}{215, 242, 252}
\definecolor{light purple}{RGB}{247, 215, 252}
\definecolor{light orange}{rgb}{0.9961, 0.875, 0.7188}

\section{Input Rewriting Methods}

\label{3 method}
Within the process of source rewriting, the goal of a rewrite model is to rewrite the original source sentence $s$ into another form that is easier to translate while preserving its intended meaning. For \textbf{MT-Agnostic} rewriting methods (\S \ref{3.1 mt-agnostic}), which lacks translation-related knowledge, the rewrite model $\mathcal{M}_{\theta}$ can rewrite $s$ into $s'$:
\begin{equation}
    s' = \mathcal{M}_{\theta}(s)
\end{equation}

On the contrary, both \textbf{Task-Aware} (\S \ref{3.2 task-aware}) and \textbf{Translatability-Aware} (\S \ref{3.3 translatability-aware}) rewriting methods incorporate some translation signal. For Task-Aware, $\mathcal{M}_{\theta}$ rewrites $s$ with the information of the end-task (MT):

\begin{equation}
    s' = \mathcal{M}_{\theta}(s, \text{MT task})
\end{equation}

For Translatability-Aware method, it rewrites with the knowledge of segment level quality estimation scores between source and the output of a specific MT system MT($t$):

\begin{equation}
    s' = \mathcal{M}_{\theta}(s, \text{\textsc{xCOMET}}(s,\text{MT}(t)))
\end{equation}
Figure~\ref{fig:main_figure} shows the overview of our proposed rewriting pipeline. To find the most effective $\mathcal{M}_{\theta}$, we test a total of 21 input rewriting methods.


\subsection{\fcolorbox{white}{light blue}{\raisebox{-0.2em}{\includegraphics[height=1em]{figures/logos/agnostic.png}} MT-Agnostic} Rewriting}
\label{3.1 mt-agnostic}
MT-agnostic rewriting methods reflect various a priori assumptions on what makes text easier to translate. They do not take as input any signal of translatability or knowledge about the end-task. We consider three prompting variants here, all inspired by prior works on source rewriting \citep{mirkin-etal-2009-source, mirkin-etal-2013-sort, stajner-popovic-2016-text}.

\paragraph{Simplification.}
Simplification includes replacing complex words with simpler ones, rephrasing complex syntactic structures, and shortening sentences \citep{article, Feng2008}. Prior works show that simplified inputs are more conducive to MT, and particularly improve the fluency of MT outputs \citep{stajner-popovic-2019-automated}.

\paragraph{Paraphrase.}
Paraphrases are alternative ways of expressing the same information within one language, which can help resolve unknown or complex words \citep{callison-burch-etal-2006-improved}. Paraphrasing with LLMs might benefit MT by normalizing inputs using language patterns that are more frequent in LLM training data. Further, some LLMs, such as \textsc{Tower} \citep{alves2024tower}, are fine-tuned on both paraphrasing and MT tasks, and might thus produce paraphrases that are useful for MT.

\paragraph{Stylistic.}
We employ an off-the-shelf text editing tool \textsc{CoEdIT-XL} \citep{raheja-etal-2023-coedit} to rewrite inputs according to diverse style specifications:
\begin{itemize}[leftmargin=*, itemsep=2pt, parsep=-1pt]
 \item \textbf{Grammar}: Fix the grammar.
 \item \textbf{Coherent}: Make the text more coherent.
 \item \textbf{Understandable}: Make it easier to understand.
 \item \textbf{Formal}: Rewrite the text more formally.
\end{itemize}
These operationalize the assumption that well-formed text is easier to translate.
%In addition to translation-related tasks, Tower-Instruct is trained on Grammatical Error Correction (GEC) task, which motivates us to consider the Grammar prompt. The Coherent and Understandable prompts function similarly to the Simplification rewrites, making the text easier to translate by using a dedicated text editing tool. Further, we explore the impact of increasing formality on MT quality \citep{lee-etal-2023-improving-formality} with Formal prompt.
All prompt templates are shown in Appendix Table~\ref{tab:prompting_template}.

\subsection{\fcolorbox{white}{light purple}{\raisebox{-0.2em}{\includegraphics[height=1em]{figures/logos/task.png}} Task-Aware} Rewriting}
\label{3.2 task-aware}

For task-aware rewriting methods, we design prompts that account for the fact that rewrites are aimed at MT. 
Prior work has shown that LLMs can post-edit errors in MT outputs \citep{ki2024guiding, zeng2024improving, treviso-etal-2024-xtower, xu2024llmrefine, briakou-etal-2024-translating}, raising the question of whether this ability can be extended to rewriting inputs to enhance translatability. Additionally, \textsc{Tower-Instruct} has been jointly trained on paraphrasing, grammatical error correction (GEC), and translation tasks, suggesting it may be well-suited for performing translatability rewrites in a zero-shot fashion. We consider two prompting strategies (Refer to Appendix Table~\ref{tab:prompting_template} for exact templates):

\paragraph{Easy Translation.} We prompt LLMs to rewrite inputs in a way that specifically facilitates translation into the target language.

\paragraph{Chain of Thought Rewrite+Translate.} We use a Chain of Thought (\citet{wei2023chainofthought}, CoT) style prompt where LLMs are prompted to handle the entire rewriting and translation process in one sequence of CoT instructions within a single model.



\subsection{\fcolorbox{white}{light orange}{\raisebox{-0.2em}{\includegraphics[height=1em]{figures/logos/translatability.png}} Translatability-Aware} Rewriting}
\label{3.3 translatability-aware}
We propose to use quality estimation scores for a given input and output pair to assess the translatability of inputs at the segment level. This makes it possible to inject translatability signals at inference or training time. We introduce a lightweight inference-time selection strategy, and contrast it against a more expensive fine-tuning approach.

\paragraph{Inference-Time Selection.}
Input segments might not benefit from rewriting uniformly, since the quality of the original inputs and of their rewrites might vary. We thus propose to use translatability scores to decide whether or not to replace the original input with a rewrite  at inference time. We use the state-of-the-art \textsc{xCOMET} quality estimation tool \citep{guerreiro2023xcomet} to assess how good the translation $t'$ of a rewrite $s'$ is: \textsc{xCOMET}$(s',t')$. We compare this score with the estimated quality of the translation $t$ of the original source $s$, choosing to use the rewrite if \textsc{xCOMET}$(s',t')$ > \textsc{xCOMET}$(s,t)$, and keeping the original source otherwise. This straightforward approach allows us incorporate translatability signals at inference time, with little additional cost.

% we consider the best-performing rewrite among MT-Agnostic and Task-Aware methods. 

\paragraph{Supervised Fine-tuning.}
The translatability-based selection process described above for inference could also be used to gather examples of good rewrites and enable instruction fine-tuning of models to rewrite text for improved translation.
%
%
%Quantifying whether a given rewrite improves translatability at the segment level as above makes it possible to training data, and thus fine-tuning
%We explore fine-tuning LLMs to enhance their capability in rewriting the source for improved translation.
While designing an optimal approach for this task is out of scope for this work,  we wish to compare our inference-time selection strategy with a straightforward training strategy. We construct a fine-tuning dataset of positive rewrite examples $\mathcal{D}_{pos}$, as follows: for a given input $s$, we generate rewrites using all MT-agnostic methods. We add to our training set the rewrites that improve translatability as measured by \textsc{xCOMET}$(s', t')$ > \textsc{xCOMET}$(s,t)$. The base LLM is then instruction fine-tuned based to rewrite input $s$ so that it is better translated, using $s'$ as supervision. Detailed prompt templates are shown in Appendix \ref{appendix:prompt_templates}.

%The original data is drawn from the English-German and English-Russian subset from WMT-20, 21, and 22 General MT task datasets \citep{freitag2021experts}\footnote{We do not consider English-Chinese pair here since this language pair is not supported in the dataset.}. We first prompt LLMs using MT-agnostic rewriting methods to generate rewrites $s'$ for each $s$. Then, we translate the original source and rewrites using Tower-Instruct 7B to get MT outputs $t$ and $t'$. We use \textsc{xCOMET} to compute scores between $s'$ and $t'$, which estimates translation quality for each rewrite. Our goal of fine-tuning is to generate better rewrites than the original source. Thus, we create $\mathcal{D}_{pos}$, a subset of $\mathcal{D}_{ft}$, which only contains the rewrites where \textsc{xCOMET}$(s', t')$ > \textsc{xCOMET}$(s,t)$. 

%For prompt template, we compare three variants:
%\begin{itemize}[leftmargin=*, itemsep=2pt, parsep=-1pt]
% \item \textbf{Basic}: Given $s$, generate $s'$.
% \item \textbf{MT}: Given $s$ and $t$, generate $s'$.
% \item \textbf{Reference}: Given $s$ and $r$ (reference translation), generate $s'$.
%\end{itemize}
%Specific prompt templates are outlined in Appendix \ref{appendix:prompt_templates}. All parameters are listed in Appendix \ref{appendix:parameters}.
