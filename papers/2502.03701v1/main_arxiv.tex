%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\crefname{equation}{}{}
\crefname{figure}{Figure}{Figures}
\creflabelformat{equation}{\textup{(#2#1#3)}}
\crefname{assumption}{Assumption}{Assumptions}
\crefname{condition}{Condition}{Conditions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\usepackage{enumitem}
%\usepackage{paralist}
%% The following can be uncommented instead of using paralist package...but cannot use together.
%\newlist{compactenum}{enumerate}{4}
%\setlist[compactenum,1]{nolistsep} 
\setlist[enumerate]{leftmargin=1.3em, itemindent=0em, itemsep=0em, topsep=0em, label = {\bfseries \arabic*.}} 
\setlist[itemize]{leftmargin=1.3em,itemindent=0em, itemsep=0em, topsep=0em} 

%\input{packages}
\input{newcmnds_icml}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{First-ish Order Methods: Hessian-aware Scalings of Gradient Descent}

% \setlength\parindent{0pt}
% \setlength{\leftmargini}{2em} % Adjust the indentation
% \setlength{\leftmarginii}{3em} % Adjust the indentation

% Before ICML format
%\title{First-ish Order Methods: Hessian-aware Scalings of Gradient Descent}
%\author{%
%{Oscar Smee \thanks{School of Mathematics and Physics, University of Queensland, Australia. Email: \texttt{o.smee@uq.edu.au}}} \hspace{10mm}
%{Fred Roosta \thanks{School of Mathematics and Physics, University of Queensland, Australia. Email: \texttt{fred.roosta@uq.edu.au}}}\hspace{10mm}
%{Stephen J. Wright \thanks{Computer Sciences Department, University of Wisconsin-Madison, USA. Email: \texttt{swright@cs.wisc.edu}}} 
%}
%\date{\today}


\begin{document}
%\maketitle

\twocolumn[
\icmltitle{First-ish Order Methods: Hessian-aware Scalings of Gradient Descent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Oscar Smee}{uq}
\icmlauthor{Fred Roosta}{uq,cires}
\icmlauthor{Stephen J. Wright}{uow}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{uq}{School of Mathematics and Physics, University of Queensland, Brisbane Australia.}
\icmlaffiliation{cires}{ARC Training Centre for Information Resilience (CIRES),
Brisbane, Australia}
\icmlaffiliation{uow}{Computer Sciences Department, University of Wisconsin-Madison, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Oscar Smee}{o.smee@uq.edu.au}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, Optimization, Gradient Descent, Hessian, Inexact Hessian, Local Convergence, Global Convergence, Hessian scaling, Nonconvex}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Gradient descent is the primary workhorse for optimizing large-scale problems in machine learning. However, its performance is highly sensitive to the choice of the learning rate.
A key limitation of gradient descent is its lack of natural scaling, which often necessitates expensive line searches or heuristic tuning to determine an appropriate step size.
%
In this paper, we address this limitation by incorporating Hessian information to scale the gradient direction. By accounting for the curvature of the function along the gradient, our adaptive, Hessian-aware scaling method ensures a local unit step size guarantee, even in nonconvex settings. 
Near a local minimum that satisfies the second-order sufficient conditions, our approach achieves linear convergence with a unit step size.
%
We show that our method converges globally under a significantly weaker version of the standard Lipschitz gradient smoothness assumption. 
Even when Hessian information is inexact, the local unit step size guarantee and global convergence properties remain valid under mild conditions.
%
Finally, we validate our theoretical results empirically on a range of convex and nonconvex machine learning tasks, showcasing the effectiveness of the approach.
\end{abstract}

\section{Introduction}
Consider the optimization problem 
\begin{align}
\label{eqn:optimisation problem}
    \min_{\xx \in \real^d} f(\xx), 
\end{align}
where $f:\real^{d} \to \real$ is twice continuously differentiable, bounded below, and possibly nonconvex. 
Arguably, the simplest and most widely used workhorse for solving such problems in large-scale machine learning is gradient descent (GD) and its stochastic variants \cite{lanFirstorderStochasticOptimization2020}. 
Recall that an iteration of GD takes the form 
\begin{align*}
    \xx_{k+1} = \xx_k - \alpha_k \bgg_k,
\end{align*}
where $\bggk \defeq \bgg(\xxk) = \nabla f(\xxk)$ is the gradient of $f$ at $\xxk$. 

Gradient descent is favored for its simplicity, low per-iteration cost, and convergence properties in nonconvex settings. With an arbitrary initialization $\xxo$, the only hyperparameter is the step size, $\alpha$, which may vary across iterations. Choosing this parameter has been the focus of significant research.
Most analyses assume Lipschitz smoothness of the gradient, equivalent to a uniform upper bound on the Hessian spectrum for twice continuously differentiable functions. However, this approach often ignores \textit{local} curvature, leading to conservative step sizes. 
Additionally, since the gradient's Lipschitz constant is typically unknown, the step size is often determined through backtracking line search, without an a priori known effective initialization, or via simple trial and error, significantly increasing computational overhead.


In contrast to first-order methods, most second-order methods incorporate local curvature information to construct properly scaled directions, ensuring that a unit step size ($\alpha = 1$) achieves a sufficient decrease in the function value\footnote{This brings to mind the following quote about the significance of a unit step size: \textit{“Any optimization algorithm for which the unit step length works has some wisdom. It is too much of a fluke if the unit step length [accidentally] works.”} - J. Nocedal (\href{https://www.stat.berkeley.edu/~mmahoney/talks/second_order_ml_sum17.pdf}{Source}.)} near a local solution \cite{boydConvexOptimization2004,nocedalNumericalOptimization2006}. This property allows $\alpha=1$ to serve as an initial guess in backtracking line search, as it will eventually meet the acceptance criteria. By leveraging local curvature, second-order methods often converge in significantly fewer iterations than first-order methods \citep{martensDeepLearningHessianfree2010}. However, this advantage comes at the cost of higher per-iteration computational expense.


These facts lead us to the motivating question for our paper: \emph{Can local curvature information be used to properly, and efficiently, scale the gradient--without altering its direction--to attain certain desirable properties of both first and second-order methods?}
%
To that end, we propose our ``first-ish order'' method, which employs updates of the form:
\begin{align}
    \label{eq:method}
    \xx_{k+1} = \xx_k + \alphak \pp_k, \quad \pp_k = -s_k \bgg_k,
\end{align}
where $s_k > 0$ is an adaptive, \textit{Hessian-aware} scaling and $\alphak > 0$ is the step size, chosen to ensure global convergence. We distinguish between $s_k$ and $\alphak$ to emphasize that the scaling is applied, prior to, and independently of, the step size selection strategy (e.g., line search). 


Remarkably, we show that certain Hessian-aware scalings, which can be calculated using only a single Hessian-vector product, can endow GD with a property typically restricted to second-order methods, namely, a local unit step size guarantee. 
The key to our results is the fact that the scaled gradient direction, $\ppk$, satisfies the following \textit{second-order descent} condition\footnote{Note that \cref{eqn:second-order descent} has also been called ``2.5\th order descent'' as it is a stronger condition than the typical reduction in the local quadratic approximation to the objective.}:
 \begin{align}
    \label{eqn:second-order descent}
    \dotprod{\bggk, \ppk} + \dotprod{\ppk, \HHk \ppk} \leq 0,
\end{align}
where $\HHk \defeq \HH(\xxk) = \nabla^{2} f(\xxk)$ is the Hessian of $f$ at $\xxk$.
%
This property \cref{eqn:second-order descent} is typically satisfied by search directions in second-order methods. When there is positive curvature along $\pp$, \cref{eqn:second-order descent} strengthens the standard first-order descent condition $\dotprod{\bgg, \pp} < 0$. For instance, the Newton direction $\pp_N = -\HH^{-1}\bgg$, as well as directions produced by inexact methods like Newton-MR and Newton-CG, satisfy \cref{eqn:second-order descent} as long as a non-positive curvature (NPC) is not detected (see, e.g., \citep[Lemma 13]{liuNewtonMRAlgorithmComplexity2023} and \citep[Chapter~5]{nocedalNumericalOptimization2006}).


The descent condition \cref{eqn:second-order descent} eliminates the need to bound curvature terms, enabling the use of analytical tools typically associated with second-order methods,  without prior knowledge of problem constants. Our key upper bound (see \cref{eqn:function value upper bound}) incorporates local curvature information and is derived from Lipschitz Hessian smoothness (\cref{ass:Hessian smoothness condition}), despite the step \emph{direction} relying solely on first-order information.

\paragraph{Contributions.} Our contributions are outlined as follows:
\begin{enumerate}
    \item In \cref{sec:global}, we show that, when the gradient is small, the Armijo line search applied to the rescaled gradient direction will accept a unit step size, even for nonconvex $f$. We establish global convergence under relaxed smoothness conditions, which generalize Lipschitz continuity of the gradient and Hessian.
    \item In \cref{sec:local}, we analyze the local convergence properties of our method. We show that near a minimum, under certain conditions, the unit step size is accepted at each iteration, leading to local linear convergence of the function value with a rate that may improve upon typical local gradient descent rates. For a specific scaling, we also establish local linear convergence in the gradient norm. 
    \item In \cref{sec:inexact-hessian}, we examine the case in which the Hessian is inexact. 
    Under certain conditions on the inexactness, we show that a unit step size and global convergence guarantee similar to the exact case applies.
    \item Finally, in \cref{sec:numerical results}, we validate our results numerically on large scale, nonconvex objectives arising from problems in data science.
\end{enumerate}

\section{Our Approach} \label{sec:approach}
We now introduce and motivate our Hessian-aware scalings and  highlight their key properties.

\subsection{Hessian-aware Scaling}
\label{sec:hess_aware}
To determine the scaling $s_k$ in \cref{eq:method} required for the update direction, $\ppk$, to achieve \emph{natural scaling} (i.e., eventual unit step length), we draw inspiration from the Newton's method in one dimension. We also account for varying curvature along the gradient direction, considering cases of strongly positive, limited positive, and negative curvature.


\paragraph{Strong Positive Curvature ($\SPC$).}

For strongly convex problems, the Newton direction minimizes the local quadratic approximation: $\min_{\pp} \dotprod{\pp,\bgg} + \dotprod{\pp, \HH \pp}/2$. Restricting to $\pp = -s\bgg$, and assuming $\gHg > 0$, the optimal scaling is given by
\begin{align}
    \label{eqn:CG scaling defn}
    \sCG = \frac{ \vnorm{\bgg}^2}{\langle \bgg, \HH \bgg \rangle}.
\end{align}
The notation $\sCG$ reflects a connection to the conjugate gradient (CG) method, discussed in \cref{sec:related_work}.
%
An alternative motivation for Newton's method is solving the optimization problem $\min_{\pp} \vnorm{\HH \pp + \bgg}^{2}$. By restricting the search to $\pp = -s\bgg$, the unique minimizing scaling is given by
\begin{align}
    \label{eqn:MR scaling defn}
    \sMR = \frac{\dotprod{\bgg, \HH \bgg} }{\vnorm{\HH \bgg}^2}.
\end{align}
The notation $\sMR$ highlights a connection to the minimum residual (MINRES) method, as discussed in \cref{sec:related_work}. 

From these two fundamental scalings, additional factors like the geometric mean of CG and MR scalings can be derived:
\begin{align}
    \label{eqn:GM scaling defn}
    \sGM = \sqrt{\sMR\sCG} = \frac{\vnorm{\bgg}}{\vnorm{\HH \bgg}}.
\end{align}
The CG, GM, and MR scalings each reflect an aspect of the inverse Hessian restricted to one dimension. For example, the CG scaling in \cref{eqn:CG scaling defn} is an inverse Rayleigh quotient of the Hessian, $\HH$, with respect to $\bgg$, while the MR scaling in \cref{eqn:MR scaling defn} corresponds to a Rayleigh quotient of the Hessian pseudo-inverse, $\HH^\dagger$, with respect to $\HH\bgg$ (using $\HH \HH^\dagger \HH = \HH$).
%
In the univariate case, all scalings reduce to the inverse of the second derivative, making the update in \cref{eq:method} the exact Newton step. Thus, these scalings are most effective under $\SPC$ along the gradient direction, i.e., $\gHg > \sigma \vnorm{\bgg}^2$ for some $\sigma > 0$. The $\SPC$ condition can be interpreted as a strong convexity condition \textit{restricted to the gradient direction}.


\paragraph{Negative Curvature ($\NC$).}
In nonconvex settings, the indefiniteness of the Hessian can make the gradient a $\NC$ direction, i.e., $\gHg < 0$. Since $-\bgg$ is already a descent direction, both the first and second directional derivatives 
along $-\bgg$ are negative, regardless of scaling choice. Previous theoretical results \cite{gouldExploitingNegativeCurvature2000,curtisExploitingNegativeCurvature2019,liuNewtonMRAlgorithmComplexity2023} and practical experience suggest that substantial progress can be made with large steps along negative curvature directions. 

\paragraph{Limited Positive Curvature ($\LPC$).}
The $\LPC$ case, where $0 \leq \gHg \leq \sigma \vnorm{\bgg}^2$, represents a middle ground between $\SPC$ and $\NC$, characterized by small but non-negative curvature along $\bgg$. Here, $\sigma$ acts as a gradient Lipschitz constant, constraining the second-order term in the Taylor expansion along $-\bgg$. Since second-order information is unreliable in this regime, we revert to gradient descent with a step size independent of second-order information. Thanks to the curvature bound $\sigma$, scalings satisfying $s \leq 1/\sigma$ maintain second-order descent properties akin to those in the $\SPC$ and $\NC$ cases. 

The above discussions are summarized in \cref{alg:scaling selection}.
\begin{algorithm}[ht]
    \begin{algorithmic}[1]
        \STATE \textbf{Inputs}: Gradient $\bgg$, Hessian $\HH$, and $\SPC$ scaling tolerance $\sigma > 0$.
        \vspace{1mm} \STATE Set the range for $\LPC$ scaling as $\sLPC_{\tmin} \in (0, 1/\sigma)$.
        \vspace{1mm} \STATE Set the range for $\NC$ scaling as $ 0 < \sNC_{\tmin} \leq \sNC_{\tmax} < \infty$.
        \IF{ $\dotprod{ \bgg, \HH \bgg}  > \sigma \vnorm{\bgg}^2$ }
            \vspace{1mm} \STATE  choose $\sSPC \in \{ \sMR, \sCG, \sGM \}$, set $\pp = -\sSPC \bgg$, set $\texttt{FLAG} = \SPC$.
        \vspace{1mm} \ELSIF{ $0 \leq \dotprod{ \bgg, \HH \bgg} \leq \sigma \vnorm{\bgg}^2$ }
            \vspace{1mm} \STATE choose $\sLPC \in [\sLPC_{\tmin}, 1/\sigma]$, set $\pp = -\sLPC \bgg$, set $\texttt{FLAG} = \LPC$.
        \vspace{1mm} \ELSE 
            \vspace{1mm} \STATE choose $\sNC \in [\sNC_{\tmin}, \sNC_{\tmax}]$, set $\pp = - \sNC \bgg$, set $\texttt{FLAG} = \NC$.
        \vspace{1mm} \ENDIF  
        \STATE \textbf{Return} $\pp$, $\texttt{FLAG}$.
    \end{algorithmic}
    \caption{Hessian-aware Scaling Selection}
    \label{alg:scaling selection}
\end{algorithm} 

As shown in \cref{prop:scaling upper bounds}, the constant $1/\sigma$ bounds the step size in both the $\SPC$ and $\LPC$ cases, so $\sigma$ should generally be chosen small ($\sigma \ll 1$). For fixed steps, $\sLPC_{\tmin}$ can be set to $1/\sigma$, and $\sNC_{\tmin} = \sNC_{\tmax}$. While $\sNC_{\tmax}$ theoretically allows large scalings due to negative curvature, in practice it can also be set to $1/\sigma$.

\begin{remark}
Computationally, \cref{alg:scaling selection} requires an additional Hessian-vector product, $\HH \bgg$, for curvature testing and scaling computation. This product can be efficiently evaluated without computing the full Hessian, using techniques like automatic differentiation \cite{pearlmutterFastExactMultiplication1994, baydinAutomaticDifferentiationMachine2018}, which requires only an extra forward and backward pass through the computational graph. Although this incurs additional cost, our numerical experiments show that scaled gradient descent can have lower overall computational costs compared to alternatives, as it eliminates the need for multiple backtracking line search evaluations or trial-and-error step size selection. This efficiency stems from the local unit step size guarantee (see \cref{prop:unit step size acceptance}), avoiding costly function evaluations during line search.
\end{remark}


\subsection{Basic Properties}
\label{sec:properties}

We collect some basic properties of the scalings from \cref{alg:scaling selection}. We relegate all proofs to \cref{apx:proof-basic-properties}. 

\begin{proposition}[Scaling Upper Bounds] \label{prop:scaling upper bounds}
    In the $\SPC$ case, $0< \sMR \leq \sGM \leq  \sCG \leq 1/\sigma$, while for the $\LPC$ case, $0 < \sLPC \leq 1/\sigma$. Finally, in the $\NC$ case, $0 < \sNC \leq \sNC_{\max}$.
\end{proposition}
The proof of this result demonstrates the importance of verifying strong positive curvature, in order to obtain an upper bound in the $\SPC$ and $\LPC$ cases. 

\begin{proposition}[Second-order Descent]\label{prop:second-order descent}
    Suppose $\bgg \neq 0$. The direction $\pp$ returned by \cref{alg:scaling selection} satisfies both the first-order descent condition $\dotprod{\bgg, \pp} < 0$ and the second-order descent condition \cref{eqn:second-order descent}.
\end{proposition}


\begin{remark}
Our selection of scalings satisfying second-order descent is not exhaustive. For instance, if $f$ has an $\Lg$-Lipschitz gradient, $s = 1/\Lg$ ensures \cref{eqn:second-order descent} by conservatively controlling curvature. However, this choice is not adaptive to local curvature, and $\Lg$ may be unknown.
\end{remark}

A well-known property of classical Newton's method is its invariance under affine transformations \citep[Chapter 9.5]{boydConvexOptimization2004}. While directions parallel to gradient cannot achieve full affine invariance for general objectives, \cref{prop:scalar invariance} shows that our Hessian-aware scalings lead to invariance to scalar transformations.
\begin{proposition}[Scalar Invariance] 
\label{prop:scalar invariance} 
Consider \cref{eq:method} with $s_k = \sSPC_k$ for all $k$, applied to $f(\xx)$ and its scalar reparameterization $f(\yy)$ where $\yy =  \xx/c$ for any $c \neq 0$. Then $\yy_{k} = \xxk/c$ for all $k$.
\end{proposition}
Although understandably limited compared to the full affine invariance of Newton's method, this invariance property distinguishes our approach from standard gradient descent, where the step size is highly sensitive to changes in the coordinate system. 


\subsection{Related Works}
\label{sec:related_work}
\paragraph{Quadratic Problems.} 
The scalings \cref{eqn:CG scaling defn,eqn:MR scaling defn} are well-studied for strongly convex quadratic functions; see \cite{gonzagaSteepestDescentAlgorithm2016,macdonaldFamilyRelaxedGradient2024} and references therein. These correspond to exact line search methods along the gradient direction for $f$ and $\vnorm{\bgg}^2$, known as steepest descent and minimal gradient methods, respectively\footnote{Beyond quadratics, there is no correspondence to exact line search, so we avoid labeling \cref{eqn:CG scaling defn,eqn:MR scaling defn} as such.}. The GM scaling \cref{eqn:GM scaling defn} has been studied for such problems by \cite{daiNewGradientMethod2006}, showing that for $\alpha > 0$, $\sGM$ estimates an inverse gradient Lipschitz constant.

\vspace{-1mm}
\paragraph{Barzilai-Borwein Methods.} 
The Barzilai-Borwein (BB) method \cite{barzilaiTwopointStepSize1988, fletcherBarzilaiBorweinMethod2005, daiFamilySpectralGradient2019} is also based on scalar minimization of a second-order approximation. For quadratics, the long and short BB step sizes align with our CG and MR scalings, shifted by one iteration, due to their equivalence to steepest descent and minimal gradient methods. While BB achieves R-linear convergence for strongly convex quadratics, its convergence for non-quadratic objectives cannot be guaranteed.

\vspace{-1mm}
\paragraph{Inexact Newton Methods.} 
The scalings $\sCG$ and $\sMR$ are deeply connected to Krylov subspace-based inexact Newton methods. The Newton-CG method \cite{nocedalNumericalOptimization2006} uses CG to solve the Newton system. The $t\th$ iteration of CG amounts to solving  
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \frac{1}{2}\dotprod{\dd, \HH \dd} + \dotprod{\bgg, \dd},
\end{align*}
where $\sK_t(\HH, \bgg) = \Span \{ \bgg, \HH\bgg, \dots, \HH^{t-1} \bgg \}$ is the Krylov subspace of degree $t$. For $t = 1$, $\sK_1(\HH, \bgg) = \{ s \bgg : s \in \mathbb{R} \}$, and the solution is $-\sCG \bgg$. Similarly, the Newton-MR method \cite{roostaNewtonMRInexactNewton2022, liuNewtonMRAlgorithmComplexity2023,smeeInexactNewtontypeMethods2024,limComplexityGuaranteesNonconvex2024}, based on MINRES \cite{paige1975solution}, computes
the approximate Newton direction as a solution to
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \vnorm{\HH \dd + \bgg},
\end{align*}
yielding $-\sMR \bgg$ for $t = 1$. These connections to inexact second-order methods motivated the study of CG and MR scaled gradient methods. The treatment of limited positive curvature in \cite{smeeInexactNewtontypeMethods2024, limComplexityGuaranteesNonconvex2024} inspired our curvature validation procedure.

\vspace{-1mm}
\paragraph{Smoothness and Adaptivity.} 
The global convergence analysis in this paper does not rely on the conventional Lipschitz gradient smoothness assumption, which has come under increasing scrutiny in recent works. Many machine learning objectives, such as feedforward or recurrent neural networks, fail to satisfy this condition \cite{patelGlobalConvergenceStability2022}. Recent studies suggest that this assumption may not hold even along the optimization trajectory \cite{cohenGradientDescentNeural2022}. Instead, \citet[Remark 5]{ahnUnderstandingUnstableConvergence2022} argue that {\em Lipschitz Hessian smoothness} is more appropriate; our work adopts a weaker form of this assumption (\cref{ass:Hessian gradient smoothness condition}). Additionally, \citet{patelGradientDescentAbsence2023} study gradient descent with diminishing step sizes under local Lipschitz smoothness, a weaker assumption than the standard one.

Our method avoids globally conservative step sizes, instead using adaptive local curvature information to guide step size selection. Recent studies suggest that conservative step sizes can hinder convergence. \citet{grimmerProvablyFasterGradient2024} show that larger step sizes can improve rates, while \citet{altschulerAccelerationStepsizeHedging2023} demonstrate that combining long and short steps enhances convergence in convex settings. Dynamic step size selection incorporates local information, e.g., the Polyak step size \citep{polyakIntroductionOptimization1987} and its variants \citep{loizouStochasticPolyakStepsize2021,orvieto2022dynamics,oikonomou2024stochastic}. \citet{malitskyAdaptiveGradientDescent2020,malitskyAdaptiveProximalGradient2024} adaptively set step sizes using local Lipschitz estimates and control sequences, ensuring global convergence under convexity and locally Lipschitz gradients. \citet{mishkinDirectionalSmoothnessGradient2024} explores adaptive step sizes via directional smoothness, closely related to our use of gradient curvature. \citet{berahasNonUniformSmoothnessGradient2023} investigates local first-order smoothness oracles, while methods like D-adaptation \cite{defazioLearningRateFreeLearningDAdaptation2023,mishchenkoProdigyExpeditiouslyAdaptive2024} and Do(W)G \cite{ivgiDoGSGDsBest2023,khaledDoWGUnleashedEfficient2024} achieve parameter-free convergence by adaptively estimating problem constants.

\vspace{-1mm}
\paragraph{Quadratic Model Scaling } 

Utilizing a quadratic model to rescale search directions is not new. The KFAC method \citep{martensOptimizingNeuralNetworks2020} rescales directions using a quadratic model based on the Fisher information matrix. Similarly,  \citet{rouletSteppingEdgeCurvature2024} adaptively set the step size using a quadratic model of the objective along a search direction, akin to our CG scaling. \citet{rouletSteppingEdgeCurvature2024} show that curvature-aware scaling can align optimization dynamics with the edge of stability \citep{cohenGradientDescentNeural2022}, though they do not exploit negative curvature or provide theoretical guarantees. \citet{casteraSecondOrderStepSizeTuning2022} use a quadratic model (similar to our CG scaling) to verify curvature, enabling BB step sizes in nonconvex and stochastic settings, and also find that large step sizes are viable with negative curvature. \citet{degournay2022adaptivescalinglearningrate} rescale step directions using directional second-order information, applying a moving average of directional curvatures, but focus on practical implementation without convergence theory.

\section{Convergence Analyses} \label{sec:convergence-analysis}
In \cref{sec:global}, we consider the global properties of \cref{eq:method} with  \cref{alg:scaling selection} and the classical Armijo line-search strategy. 
In \cref{sec:local}, we consider the local convergence properties.
Finally, in \cref{sec:inexact-hessian}, we investigate the case of inexact Hessian. 
We defer all proofs to \cref{apx:proof-global-convergence}.

\subsection{Global Convergence}
\label{sec:global}

To globalize the iteration in \cref{eq:method}, we select a step size via the Armijo condition, which requires $\alpha$ to satisfy
\begin{align}
    \label{eqn:armijo condition} 
    f(\xx + \alpha \pp) \leq f(\xx) + \rho \alpha \dotprod{\pp, \bgg}, 
\end{align}
for some constant $\rho \in (0,1/2)$.
The resulting method is depicted in \cref{alg:scaled gradient}, where backtracking (\cref{alg:back tracking line search} in \cref{sec:appendix:linesearch_algs}) and forward tracking (\cref{alg:forward tracking line search} in \cref{sec:appendix:linesearch_algs}) strategies are used to find $\alphak$ satisfying \cref{eqn:armijo condition}. 
%
\begin{algorithm}[ht]
    \begin{algorithmic}[1]
        \STATE \textbf{Input}: Line search parameter $\rho < 1/2$, termination tolerance $\eg > 0$.
        \vspace{1mm}\WHILE{$\vnorm{\bgg_k} \geq \eg$}
        \vspace{1mm}\STATE $[\ppk, \texttt{FLAG}]  \leftarrow$ Call \cref{alg:scaling selection} with $\HHk$, $\bggk$ and $\sigma$
        \vspace{1mm}\STATE For $\texttt{FLAG} = \SPC/\LPC$, use \cref{alg:back tracking line search} to find $\alpha_k \in (0,1]$ satisfying \cref{eqn:armijo condition}. For $\texttt{FLAG} = \NC$, use \cref{alg:forward tracking line search} to find $\alpha_k \in (0,\infty)$ satisfying \cref{eqn:armijo condition}. 
        \vspace{1mm}\STATE $\xx_{k+1} = \xx_k +\alpha_k \pp_k$
        \ENDWHILE
    \end{algorithmic}
    \caption{Scaled Gradient Descent With Line Search}
    \label{alg:scaled gradient}
\end{algorithm}

Our analysis relies on a weakened version of the typical Lipschitz Hessian smoothness condition, requiring smoothness to hold only along the negative-gradient direction.
%
\begin{assumption}[Hessian Directional Smoothness] \label{ass:Hessian smoothness condition}
    There exists, $0 \leq L_2 < \infty$ such that, $\forall \xx \in \real^d$ and $\forall t\geq 0$  
    \begin{align}
        \vnorm{\HH\left(\xx - t\bgg(\xx)\right) - \HH(\xx) } \leq t L_2 \vnorm{\bgg(\xx)}.
    \end{align}
\end{assumption}
While \cref{ass:Hessian smoothness condition} may be difficult to verify directly, it is implied by Hessian Lipschitz continuity. If $f$ satisfies the latter with constant $L_H$, then $L_2 \leq L_H$. However, $L_2$ can be much smaller than $L_H$, as \cref{ass:Hessian smoothness condition} applies only along a single direction at each $\xx$.

The following result states that, local to a critical point, the unit step size $\alpha =1 $ ensures sufficient decrease for the scaled gradient direction, $\pp$.

\begin{proposition}[Sufficient Condition for Acceptance of Unit Step Size] \label{prop:unit step size acceptance}
    Consider \cref{ass:Hessian smoothness condition} and let $\pp$ be the direction selected by \cref{alg:scaling selection}. The Armijo line search \cref{eqn:armijo condition} is satisfied with $\alpha=1$ if 
    \begin{align}
        \label{eqn:gradient line search termination}
        \vnorm{\bgg}  \leq \min\left\{\frac{6\sigma^2(1/2 - \rho)}{L_2}, \frac{6(1-\rho)}{L_2 (\sNC_{\tmax})^2} \right\}.
    \end{align}
\end{proposition}
This unit-step property is characteristic of Newton-type methods \citep[Chapter~9]{boydConvexOptimization2004}. A key consequence of \cref{prop:unit step size acceptance} is that our approach introduces a natural scaling to the gradient direction, enabling the backtracking line search to be safely initialized at the unit step size. Moreover, as shown in \cref{thm:local convergence}, under certain regularity conditions, the unit step size will satisfy the line search condition \cref{eqn:armijo condition} for all iterations near a local minimum. In practice, as demonstrated in \cref{sec:numerical results}, the line search typically accepts $\alpha = 1$ for most iterations throughout the algorithm's runtime. This stands in sharp contrast to other first-order algorithms, where the lack of such a mechanism necessitates imposing arbitrary step size ranges. 

Once the line search condition \cref{eqn:armijo condition} is met, the function value decreases by at least $\rho \alpha\dotprod{\pp, \bgg} = -\rho \alpha\vnorm{\pp}\vnorm{\bgg}$. Additionally, our analysis in \cref{prop:unit step size acceptance} provides a lower bound on the largest step size satisfying \cref{eqn:armijo condition} at each step. These results ensure global convergence, provided the scaling factor $s$ in \cref{eq:method} is bounded below. This condition holds automatically for the $\NC$ and $\LPC$ scalings. For the $\SPC$ case, where the scaling is adaptive, we require the following regularity condition on the Hessian.

\begin{assumption}[Hessian-gradient Directional Smoothness] 
    \label{ass:Hessian gradient smoothness condition} There exists $0 \leq L_1 < \infty$ such that
    for all $\xx \in \real^d$, if $\dotprod{\bgg(\xx), \HH(\xx) \bgg(\xx)} > 0$, then $ \vnorm{\HH(\xx) \bgg(\xx)} \leq L_1 \vnorm{\bgg(\xx)}$.
\end{assumption}

\begin{remark}
For twice continuously differentiable functions, \cref{ass:Hessian gradient smoothness condition} relaxes the standard $\Lg$-Lipschitz gradient smoothness assumption by requiring regularity only along $\bgg$. Clearly, $L_1 \leq \Lg$. The concept of {\em moral smoothness}, introduced in \citet[Assumption 2]{roostaNewtonMRInexactNewton2022}, is strictly weaker than Lipschitz gradient and Hessian assumptions on any sublevel set of the gradient norm, yet it still implies \cref{ass:Hessian gradient smoothness condition}; see  \citet[Lemma 2]{roostaNewtonMRInexactNewton2022}.  
\end{remark}


\begin{theorem}[Global Convergence] \label{thm:global convergence}
    Consider \cref{ass:Hessian gradient smoothness condition,ass:Hessian smoothness condition} and suppose $f$ is lower bounded. For any $0 < \eg < 1$, after at most $K \in \bigO{\eg^{-2}}$  iterations of \cref{alg:scaled gradient}, we have $\vnorm{\bgg_k} \leq \varepsilon_{\bgg}$.
\end{theorem}
The detailed complexity bound in \cref{thm:global convergence}, including all underlying constants, is provided in \cref{apx:proof-global-convergence}.
%
Unsurprisingly, the rate in \cref{thm:global convergence} matches that of gradient descent under the Lipschitz gradient smoothness condition with a line search or fixed step size \cite{cartisEvaluationComplexityAlgorithms2022,nesterovIntroductoryLecturesConvex2004}. The novelty of \cref{thm:global convergence} lies in achieving this complexity under the weaker smoothness conditions, i.e.,  \cref{ass:Hessian gradient smoothness condition,ass:Hessian smoothness condition}. 
 

\subsection{Local Convergence}
\label{sec:local}
 
Let $\xx^{\star}$ be a local minima satisfying the second-order sufficient condition $\bgg(\xx^{\star}) = 0$ and $\HH(\xx^{\star}) \succ 0$. 
By the continuity of the Hessian, there exists a ball of radius $r$ around $\xx^{\star}$, denoted by $\sB_r^\star$, such that  
\begin{align} 
\label{eqn:second-order sufficient conditions}
    0 \hspace{-0.5mm} < \hspace{-0.5mm} \mu \hspace{-0.5mm}\defeq \hspace{-0.5mm}\min_{\xx \in \sB_r^\star} \lambda_{\tmin} (\HH) \hspace{-0.5mm} \leq \hspace{-0.5mm} \max_{\xx \in \sB_r^\star} \lambda_{\tmax}(\HH) \hspace{-0.5mm} \defeq \hspace{-0.5mm} M \hspace{-0.5mm}< \hspace{-0.5mm} \infty. 
\end{align}
Our next result demonstrates that, near $\xx^\star$, the unit step size is acceptable to line search for all iterations, leading to a linear decrease in the sub-optimality of the objective value.
\begin{theorem} \label{thm:local convergence}
    Consider \cref{ass:Hessian smoothness condition} and suppose $\xx^{\star}$ is a local minimum satisfying the second order sufficient conditions. If $\xx_0$ is sufficiently close to $\xx^{\star}$, then for all iteration of \cref{alg:scaled gradient}, the unit step size $\alphak=1$ satisfies the Armijo condition \cref {eqn:armijo condition} and  we have 
    \begin{align*}
        f(\xx_{k+1}) - f(\xx^{\star}) \leq (1-\tau)(f(\xx_k) - f(\xx^{\star})),
    \end{align*}
    where $\tau \defeq 2 \rho \mu \max\{1/M, \sLPC_{\tmin}\} \in (0, 1]$ if $\sigma \geq \mu$, and $\tau \defeq 2\rho \mu/M \in (0, 1]$ otherwise.
\end{theorem}

\begin{remark}
    Suppose $\xx\in \sB_r^\star$ where $\sB_r^\star$ is as in \cref{eqn:second-order sufficient conditions}. Since $\dotprod{ \bgg, \HH\bgg} \leq  M \vnorm{\bgg}^2$, the $\SPC$ case only arises on $\sB_r^\star$ if $\sigma$ is chosen such that $\sigma \leq M$; otherwise \cref{alg:scaling selection} always returns the $\LPC$ scaling. In this case, since $\sLPC_{\tmin} < 1/\sigma < 1/M$, and $\rho<1/2$ we always have $0 < \tau \leq 1$ in \cref{thm:local convergence}.
\end{remark}
\cref{thm:local convergence} suggests that the rate of convergence for the scaled gradient descent has a similar dependence on the local condition number $\kappa \defeq M/\mu$ as that of the typical gradient descent.  However, as the proof of \cref{thm:local convergence} reveals, this worst-case analysis overlooks the potential for scaled gradient methods to exploit larger scalings by adapting to local geometry. For instance, if $\sigma < \mu$ then $\sSPC \leq 1/\mu$. Therefore, in ``best-case'' iterations where large scalings (close to $1/\mu$) pass the line search with unit step size, the linear rate can be as small as $1 - 2\rho$ (cf. \cref{eqn:local function sub-optimality recursion}). This rate eliminates dependence on the local condition number, resembling the problem-independent convergence rates of many Newton-type methods \cite{roosta2019subsampledNewton, roostaNewtonMRInexactNewton2022}. In \cref{sec:numerical results}, we demonstrate numerically that scaled gradient methods often produce large scalings that pass the line search with unit step size, leading to rapid convergence. Conversely, if $\sigma \gg \mu$, both $\LPC$ and $\SPC$ scalings are upper bounded by $1/\sigma \ll 1/\mu$, limiting the local rate. These observations suggest that $\sigma$ should be set relatively small.

Our next result states that the MR scaling \cref{eqn:MR scaling defn} can give rise to linear convergence in the gradient norm. 
Intuitively, this result arises because the MR scaling minimizes the norm of the residual of the Newton system, which can also be viewed as the norm of the linearized gradient.

\begin{theorem} \label{thm:local convergence MR second-order sufficient}
    Consider \cref{ass:Hessian smoothness condition} and suppose $\xx^{\star}$ is a local minimum satisfying the second order sufficient conditions. If $\xx_0$ is sufficiently close to $\xx^{\star}$, then the iterations of the form $\xx_{k+1} = \xx_k - \sMR_{k} \bgg_k$ converge linearly in the gradient norm.
\end{theorem}
See \cref{apx:convergence in gradient norm} for a proof of this result, which extends beyond second-order sufficient conditions to a more general setting.

An immediate implication of \cref{thm:local convergence MR second-order sufficient} is that the gradient norm serves as a ``secondary objective'' for the MR scaling. This is notable given recent work on gradient norm regularization, which biases gradient descent toward regions with small gradient norms, often linked to ``flat'' regions and better generalization in machine learning \cite{barrettImplicitGradientRegularization2022,smithOriginImplicitRegularization2021,zhaoPenalizingGradientNorm2022,karakidaUnderstandingGradientRegularization2023,hochreiterFlatMinima1997,keskarLargeBatchTrainingDeep2017}. Unlike explicit regularization, which requires additional hyperparameter tuning, MR scaling implicitly achieves this bias without additional considerations. 


\subsection{Inexact Hessian} \label{sec:inexact-hessian}

We now consider the case where the Hessian can only be accessed through an inexact estimate $\HHt$. 
We show that under certain conditions on the inexactness tolerance, we can attain similar unit step size and convergence guarantees as in the exact case. 
\begin{assumption}[Hessian Error Bound] \label{ass:Hessian inexactness bound}
    For a given $\Delta_H > 0$ and $\xx$, we can produce $\HHt$ such that
    \begin{align*}
        \abs{\dotprod{\bgg(\xx), (\HH(\xx) - \HHt(\xx))\bgg(\xx)}} \leq \Delta_H \vnorm{\bgg(\xx)}^2. \tageq\label{eqn:Hessian error bound}
    \end{align*}
\end{assumption}
This condition requires only that the inexactness is bounded along the gradient direction, a weaker requirement than a direct bound on the difference, $\HH(\xx) - \HHt(\xx)$.
%
For a specific case where \cref{ass:Hessian inexactness bound} applies, consider  the finite-sum objective where  $f(\xx) \defeq \sum_{i=1}^n f_i(\xx)/n$. This formulation arises often in machine learning as part of  empirical risk minimization framework \cite{murphyMachineLearningProbabilistic2012}. In the ``big data'' regime, where $n \gg 1$, optimization algorithms often use subsampling to estimate gradients or Hessians. Suppose we estimate the Hessian via
\begin{align*}
    \HHt(\xx) = \frac{1}{|\sI_H|} \sum_{i \in \sI_H} \HH_i(\xx), \tageq\label{eqn:Hessian subsampling}
\end{align*}
where $\sI_H \subseteq \{1, \ldots, n\}$ is a minibatch of indices uniformly, with replacement. In this case, \cref{ass:Hessian inexactness bound} holds with high probability if the sample size is sufficiently large and the individual Hessians, $\HH_i$, are uniformly bounded along the gradient direction (see \cref{sec:subsampled hessian} for details). 

Note that when the estimated Hessian $\HHt$ replaces $\HH$ in \cref{alg:scaling selection}, the curvature tests and resulting scalings depend on $\HHt$ rather than $\HH$ (see \cref{apx:proofs for inexact Hessian}).
Our first result guarantees a unit step size when $\Delta_H$ is sufficiently small.

\begin{proposition} \label{prop:inexact unit step size acceptance}
    Consider \cref{ass:Hessian smoothness condition} and \cref{ass:Hessian inexactness bound} with $\Delta_H \leq 2 \min\left\{ \sigma\left( {1}/{2} - \rho\right), {(1-\rho)}/{\sNC_{\tmax}}\right\}$. 
    If 
    \begin{align*}
        \vnorm{\bgg} \leq  6 \min\left\{\frac{\sigma^2 \left(\frac{1}{2} - \rho - \frac{\Delta_H}{2\sigma} \right)}{L_2 },   \frac{\left( 1 - \rho - \frac{\sNC_{\tmax}\Delta_H}{2} \right) }{L_2 (\sNC_{\tmax})^2}\right\},
    \end{align*}
    then the Armijo condition \cref{eqn:armijo condition} is satisfied with $\alpha=1$.
\end{proposition}
Using a similar analysis to the exact case, we can also derive a global convergence guarantee for the inexact case, but we require an additional assumption.
% 
\begin{assumption}[Inexact Hessian-gradient Smoothness] 
    \label{ass:inexact Hessian gradient smoothness condition}
    There exists $0 \leq \Lt_1< \infty$ such that for all $\xx \in \real^d$, we have $\|\HHt(\xx) \bgg(\xx)\| \leq \Lt_1 \vnorm{\bgg(\xx)}$.
\end{assumption}
Much like the exact Hessian case, \cref{ass:inexact Hessian gradient smoothness condition} relaxes the typical Lipschitz gradient assumption used for global convergence in gradient descent. For subsampled Hessians, \cref{ass:inexact Hessian gradient smoothness condition} clearly holds if the individual Hessians, $\HH_i$, are uniformly bounded along the gradient direction. 

The following result shows that, regardless of the Hessian estimation inaccuracy $\Delta_H$, the headline convergence rate for the inexact Hessian case matches that of the exact case.
\begin{proposition} \label{prop:inexact global convergence}
    Consider \cref{ass:Hessian smoothness condition}, \cref{ass:Hessian inexactness bound} for any $\Delta_H \geq 0$ and \cref{ass:inexact Hessian gradient smoothness condition}, and suppose $f$ is lower bounded. For any $0<\eg < 1$, after at most $K \in \bigO{\eg^{-2}}$  iterations of \cref{alg:scaled gradient}, with $\HH$ replaced with $\HHt$, we have $\vnorm{\bgg_k} \leq \varepsilon_{\bgg}$.
\end{proposition}

Much like \cref{thm:global convergence}, the rate obtained in \cref{prop:inexact global convergence} matches the worst case rate for gradient descent on a function with Lipschitz smooth gradients. Interestingly, unlike \cref{prop:inexact unit step size acceptance}, \cref{prop:inexact global convergence} imposes no specific requirement on $\Delta_H$. This is because small step sizes (i.e., $\alpha<1$) can mitigate noise when $\Delta_H$ is large. In \cref{apx:proofs for inexact Hessian}, we discuss how bounding $\Delta_H$ can improve the per-iteration decrease beyond the worst-case rate for certain steps. 


\section{Numerical Results} \label{sec:numerical results}

We now proceed to validate our results by comparing our scaled gradient method against multiple variants of GD including fixed step size, line search, Nesterov acceleration \citep{nesterovIntroductoryLecturesConvex2004}, heavy ball momentum \citep{wrightOptimizationDataAnalysis2022}, and Adam \citep{kingma2014adam}. 
We consider each of the CG, MR and GM scalings in our numerical results and, following promising results from \citet{daiAlternateMinimizationGradient2003}, in the quadratic case\footnote{In fact, for two dimensional quadratics, \citet{daiAlternateMinimizationGradient2003} demonstrate that super-linear convergence can be obtained by alternating steepest descent and minimal gradient steps.}, we also consider various combinations of alternating scalings. The alternating scaling procedure, denoted in our results by a concatenated string of the two scalings, e.g. ``MRCG'', simply consists of alternating between two scalings, in order, on each $\SPC$ iteration\footnote{If an $\LPC/\NC$ arises, we proceed based on the last $\SPC$ step.}. 
In the main body, we report the best performing scaling, relegating performance comparison between scalings to \cref{apx:additional-numerical-results}. 
Where available, we apply theoretically convergent hyperparameter values for our competitor methods, otherwise we manually tune hyperparameter settings.
To fairly compare methods with differing per-iteration cost, we plot the objective value against the number of \textit{oracle calls}, i.e., the number of equivalent function evaluations, see \cref{apx:numerical-results-oracle-calls} for details.
However, for completeness we also include wall-clock time results in \cref{apx:additional-numerical-results}.  
The setup for each problem and optimizer is described in more detail in \cref{apx:additional-numerical-results}. All methods are implemented in PyTorch \citep{pytorch}.


\paragraph{Multi-class Logistic Regression.}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{logistic_regression_CIFAR10.pdf}
    \caption{Multi-class logistic regression on CIFAR10. (a) The objective value. (b) The scaling utilized by the MRCG method. (c) The step size selected by line search, for applicable methods.}
    \label{fig:logistic_regression_cifar10}
\end{figure*}

In \cref{fig:logistic_regression_cifar10}, we present multi-class logistic regression with $\ell_2$ regularization on the CIFAR10 dataset \citep{Krizhevsky2009CIFAR10}. This $\mu$-strongly convex problem provides a well-behaved, yet challenging, baseline for comparison. 
We observe that ``MRCG'' is competitive with Adam and accelerated methods, while significantly outperforming line search and fixed step size approaches. This is achieved without requiring any tuning or prior knowledge of problem constants. Notably, the scalings produced by our method oscillate between large (close to $1/\mu$) and small values throughout the optimization trajectory\footnote{As shown in \cref{apx:numerical-results-logistic}, this ``large scaling'' effect is most pronounced for alternating scaling.}. 
%
Despite the magnitude of these scaling values, the line search accepts the unit step length at \textit{all iterations}, confirming that \cref{prop:unit step size acceptance} holds over the entire optimization trajectory. In light of the local convergence property in \cref{thm:local convergence}, we believe these large scaling values may contribute to the rapid convergence of our method.

\paragraph{Multilayer Perceptron (MLP).} 
In \cref{fig:mlp_fashionMNIST}, we examine a nonconvex, small MLP on the FashionMNIST dataset \citep{xiao2017fashionmnist}. The results show that ``CGMR'' scaled gradient is again competitive with Adam and significantly outperforms all other methods, all without requiring hyperparameter tuning. 
%
Notably, similar to the logistic regression experiment, the unit step size is accepted by the line search at every iteration, except for one where $\NC$ is detected and exploited using forward tracking search. Meanwhile, $\LPC$ does not occur at any point during the optimization trajectory. The scaling chosen by CGMR oscillates between large and small values, mirroring the behavior observed in the convex logistic regression experiment.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{mlp_FashionMNIST.pdf}
    \caption{MLP on the FashionMNIST. (a) The objective value (b) The scaling utilized by the CGMR method (c) The step size selected by line search, for applicable methods. Crosses indicate iterations where negative curvature is detected.}
    \label{fig:mlp_fashionMNIST}
\end{figure*}

\paragraph{Resnet.} 
In \cref{fig:resnet_Imagenette}, we examine an over-parameterized, nonconvex ResNet18 architecture \citep{Kaiming2016ResNet} on the Imagenette dataset \citep{Howard2019Imagenette}. For this experiment, we run our scaling methods with a fixed unit step size instead of line search. The results show that the scaled gradient method MRCG reduces the function value monotonically throughout the optimization trajectory, with no $\NC$ or $\LPC$ directions detected during the iterations.
%
Our method significantly outperforms the only other adaptive method (line search) but is surpassed by hyperparameter-tuned approaches. In contrast to scaled gradient, the tuned methods exhibit notable instability in the early iterations, followed by stabilization and convergence. This ``unstable convergence'' effect has been observed to be beneficial in many large-scale, nonconvex models \citep{cohenGradientDescentNeural2022,ahnUnderstandingUnstableConvergence2022}. 
%
Given this, it appears that for scaled gradient to be competitive on large-scale problems, some degree of non-monotonicity must be introduced into the iterations. Fortunately, \citet{rouletSteppingEdgeCurvature2024} demonstrate that by using CG-style scaling with large step sizes (e.g., $\alpha \geq 2$), unstable convergence can be induced in a principled\footnote{Specifically, a step size of $\alpha = 2$ with CG scaling implies \emph{non-decrease} in the local quadratic model along the search direction.} manner. A similar approach could be applied to our methods to design \emph{principled} step size schedules, such as annealing from $\alpha \geq 2$ (unstable) to $\alpha = 1$ (stable), to leverage the benefits of the unstable regime. We leave this exploration for future work.

Finally, we note that the MR scaling uniformly decreases the gradient with the unit step size across our examples, consistent with \cref{thm:local convergence MR second-order sufficient}; see \cref{apx:additional-numerical-results} .

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{resnet_Imagenette.pdf}
    \caption{ResNet18 on Imagenette. (a) The objective value. (b) The scaling utilized by the MRCG method.}
    \label{fig:resnet_Imagenette}
\end{figure}

\section{Conclusions and Future Directions}
We developed a framework based on (weakened) Lipschitz Hessian smoothness for analyzing Hessian-aware scaling of  gradient directions. Under this framework, we show that Hessian curvature information can be used to enhance vanilla GD with a unit step size guarantee. We show this guarantee holds for all iterations near minima satisfying certain regularity conditions. Furthermore, with a weakened Lipschitz gradient smoothness, we prove global convergence. 
%
Numerically, we observe that the unit step size guarantee holds across most of the optimization trajectory. We also demonstrate that alternating scalings can achieve fast monotonic convergence on medium-scale problems. 

A limitation of our results is the performance on large-scale nonconvex models. However, principled step size schedules, scaling for momentum and adaptive gradient methods, and stochastic gradient variations can be viewed as natural extensions of our analysis, offering potential avenues for future research to address this limitation.

\section*{Acknowledgments}
Roosta was partially supported by the Australian Research Council through an Industrial Transformation Training Centre for Information Resilience (IC200100022). 
Wright was partially supported by the US National Science Foundation under grants CCF 2224213 and DMS 2023239.

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{bibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Material for \cref{sec:approach}} \label{apx:proof-basic-properties}

\paragraph{Proof of \cref{prop:scaling upper bounds}}
\begin{proof}
    In the $\LPC$ and $\NC$ cases, the results follow from the definition of $\sLPC$ and $\sNC$ in \cref{alg:scaling selection}. The $\SPC$ condition excludes the case where $\bgg = 0$, furthermore, it is clear that $\dotprod{\bgg, \HH \bgg} > 0$ implies $\HH\bgg \neq 0$, and hence all of the scalings are well defined. The Cauchy-Schwarz inequality implies $\gHg \leq \vnorm{\bgg} \vnorm{\HH\bgg}$ and $\vnorm{\HH\bgg} \geq \dotprod{\bgg, \HH \bgg}/\vnorm{\bgg}$. Applying these two inequalities subsequently to $\sMR$ we have 
    \begin{align*}
        0 < \sMR = \frac{\gHg}{\vnorm{\HH \bgg}^2} \leq \frac{\vnorm{\bgg}}{ \vnorm{\HH \bgg }} \leq \frac{\vnorm{\bgg}^2}{\gHg},
    \end{align*}
    which captures the three left inequalities in this case. The right-most inequality follows from $\dotprod{\bgg, \HH \bgg } > \sigma \vnorm{\bgg}^2$ and the definition of \cref{eqn:CG scaling defn}. 
\end{proof}

\paragraph{Proof of \cref{prop:second-order descent}}

\begin{proof}
    The first-order descent trivially holds. 
    In the $\NC$ case, the result immediately follows from $\dotprod{\pp, \HH \pp} < 0$ and the first-order descent property. In the $\LPC$ case, 
    \begin{align*}
        \dotprod{\bgg, \pp} + \dotprod{\pp, \HH \pp} &= -\sLPC \vnorm{\bgg}^2 + (\sLPC)^2\dotprod{\bgg, \HH \bgg} \\
        &\leq -\sLPC \vnorm{\bgg}^2 +  (\sLPC)^2 \sigma \vnorm{\bgg}^2 \\
        &\leq -\sLPC \vnorm{\bgg}^2 + \sLPC \vnorm{\bgg}^2 = 0,
    \end{align*}
    where the second to last line follows from the $\LPC$ condition and the last line follows from $\sLPC \leq 1/\sigma$ in \cref{prop:scaling upper bounds}.  

    For the $\SPC$ case, we consider each scaling successively. For the CG scaling, we have 
    \begin{align*}
        \dotprod{ \pCG, \bgg} + \dotprod{\pCG, \HH \pCG} &= -\sCG \vnorm{\bgg}^2 + (\sCG)^2 \dotprod{\bgg, \HH \bgg} \\
        &= -\frac{\vnorm{ \bgg}^4}{\dotprod{\bgg, \HH \bgg}} + \frac{\vnorm{ \bgg}^4}{(\dotprod{\bgg, \HH \bgg})^2} \dotprod{\bgg, \HH \bgg} \\
        &= 0.
    \end{align*}
    For the MR scaling, we have
    \begin{align*}
        \dotprod{\pMR, \bgg} + \dotprod{\pMR, \HH \pMR} 
        &= -\sMR \vnorm{\bgg}^2 + (\sMR)^2 \dotprod{\bgg, \HH \bgg} \\
        &= -\frac{\langle \bgg, \HH \bgg \rangle }{\| \HH \bgg \|^2} \| \bgg\|^2 + \frac{(\dotprod{\bgg, \HH \bgg} )^2}{ \| \HH \bgg \|^4}\dotprod{\bgg, \HH \bgg} \\
        &\leq -\frac{\dotprod{\bgg, \HH \bgg} }{\| \HH \bgg \|^2} \vnorm{\bgg}^2 + \frac{ \vnorm{\bgg}^2}{ \| \HH \bgg \|^2} \dotprod{\bgg, \HH \bgg} \\
        &= 0,
    \end{align*}
    where the third line follows from the Cauchy-Schwarz inequality. Finally, for the geometric mean scaling, we have
    \begin{align*}
        \dotprod{\pGM, \bgg} + \dotprod{\pGM, \HH \pGM} &= -\sGM \vnorm{\bgg}^2 + (\sGM)^2\dotprod{\bgg, \HH \bgg} \\
        &= -\frac{\vnorm{\bgg}^3}{\vnorm{\HH \bgg}} + \frac{\vnorm{\bgg}^2}{\vnorm{\HH \bgg}^2} \dotprod{\bgg, \HH \bgg} \\
        &\leq  -\frac{\vnorm{\bgg}^3}{\vnorm{\HH \bgg }} + \frac{\vnorm{\bgg}^2}{\vnorm{\HH \bgg}^2} \vnorm{\bgg} \vnorm{\HH \bgg} \\
        &=  -\frac{\vnorm{\bgg}^3}{\vnorm{\HH \bgg }} + \frac{\vnorm{\bgg}^3}{\vnorm{\HH \bgg}} = 0,
    \end{align*}
    where again the third line follows from the Cauchy-Schwarz inequality.

\end{proof}

\paragraph{Proof of \cref{prop:scalar invariance}}
\begin{proof}
     Consider a scalar reparameterization of the original variables $\xx$ as $\yy = \xx/c$, for some $c \neq 0$. Our objective over the new coordinates is $\bar{f}(\yy) = f(c \yy) $ so that
     \begin{align*}
         \grad_\yy \bar{f}(\yy) = c \grad f(\xx), \quad \grad^2_\yy \bar{f}(\yy) = c^2 \grad^2 f(\xx).
     \end{align*}
    Therefore, $\SPC$ scalings computed in the transformed coordinates satisfy
    \begin{align*}
        s(\yy) = \frac{s(\xx)}{c^2}.
    \end{align*}
    For example, for the MR scaling we have 
    \begin{align*}
        s^\text{MR}(\yy) = \frac{\dotprod{ \grad_\yy \bar{f}(\yy), \grad_\yy^2 \bar{f}(\yy) \grad_\yy \bar{f}(\yy)}}{\vnorm{\grad_\yy^2 \bar{f}(\yy) \grad_\yy \bar{f}(\yy)}^2} 
        = \frac{\dotprod{ c \grad f(\xx), (c^2 \grad^2 f(\xx)) c \grad f(\xx)}}{\vnorm{c^2 \grad^2 f(\xx) c \grad f(\xx)}^2} = \frac{1}{c^2} \sMR(\xx).
    \end{align*}
    Now this implies that, for the $\SPC$ scaled gradient directions, the update computed with respect to the new coordinates preserves the reparameterization. Indeed, given $\yy_k = \xx_k/c$, the scaled gradient descent update at $\yy_k$ with some fixed step size $\alpha > 0$ is 
    \begin{align*}
        \yy_{k+1} &= \yy_k - \alpha s(\yy_k)\grad_\yy \bar{f}(\yy_k) = \frac{1}{c}  \left(\xx_k - \alpha s(\xx_k) \grad f(\xx_k)\right)  = \frac{\xx_{k+1}}{c},
    \end{align*}
    where $\xx_{k+1} - \xx_k - \alpha s(\xx_k) \grad f(\xx_k)$ is the update in the original coordinates. That is, the same relationship between $\xx$ and $\yy$ holds at the following iteration.
\end{proof}

\section{Additional Material for \cref{sec:convergence-analysis}}  \label{apx:proof-global-convergence}

\subsection{Line Search Algorithms}
\label{sec:appendix:linesearch_algs}

\begin{algorithm}[htbp]
    \begin{algorithmic}[1]
        \STATE \textbf{input}: Initial step size $\alpha_0 = 1$, Scaling parameter $0 < \theta < 1$.
        \vspace{1mm}
        \STATE $\alpha \gets \alpha_0$.
        \vspace{1mm}
        \WHILE{\cref{eqn:armijo condition} is not satisfied}
        \vspace{1mm}
            \STATE $\alpha \gets \theta\alpha$.
            \vspace{1mm}
        \ENDWHILE
        \vspace{1mm}
        \STATE \textbf{return} $\alpha$.
        \vspace{1mm}
    \end{algorithmic}
    \caption{Backward Tracking Line Search.}
    \label{alg:back tracking line search}
\end{algorithm}

\begin{algorithm}[htbp]
    \begin{algorithmic}[1]
        \STATE \textbf{input}: Initial step size $\alpha_0 = 1$, Scaling parameter $0 < \theta < 1$.
        \vspace{1mm}
        \STATE $\alpha \gets \alpha_0$.
        \vspace{1mm}
        \IF {\cref{eqn:armijo condition} is not satisfied} 
		\vspace{1mm}
		\STATE \text{Call \cref{alg:back tracking line search}}
		\vspace{1mm}
		\ELSE
		\vspace{1mm}
		\WHILE {\cref{eqn:armijo condition} is satisfied}
		\vspace{1mm}
		\STATE $ \alpha = \alpha/\theta .$
		\vspace{1mm}
		\ENDWHILE
        \vspace{1mm}
		\STATE \textbf{return} $\theta\alpha$.
		\vspace{1mm}
        \ENDIF
    \end{algorithmic}
    \caption{Forward/Backward Tracking Line Search}
    \label{alg:forward tracking line search}
\end{algorithm}


\subsection{Derivation and Proof of \cref{prop:unit step size acceptance}} \label{apx:proof-unit-step-size}

Throughout the following we routinely make use of the facts, due to collinearity of $\bgg$ and $\pp$, that $\dotprod{\pp, \bgg} = - \vnorm{\pp} \vnorm{\bgg}$, $\vnorm{\pp} = s \vnorm{\bgg}$, and the fact that $\dotprod{\bgg, \HH \bgg}$ and $\dotprod{\pp, \HH \pp}$ have the same sign. Also, letting $\pp = -s \bgg$, $s \geq 0$, \cref{ass:Hessian smoothness condition} and twice continuous differentiability of $f$ imply
\begin{align*}
    \vnorm{\bgg(\xx + \pp) - \bgg (\xx) - \HH(\xx)\pp}  &\leq \frac{L_2}{2}\vnorm{\pp}^2, \tageq\label{eqn:gradient upper bound}\\
    \abs{ f(\xx + \pp) - f(\xx) - \dotprod{\bgg, \pp} - \frac{1}{2} \dotprod{\pp, \HH\pp}} &\leq \frac{L_2}{6} \vnorm{\pp}^3. \tageq\label{eqn:function value upper bound} 
\end{align*}
These bounds can be derived similarly to the general Lipschitz Hessian upper bounds in, e.g., \citet[Lemma 1]{nesterovCubicRegularizationNewton2006}. Our analysis is based on applying the second order descent condition \cref{eqn:second-order descent} to the cubic upper bound in \cref{eqn:function value upper bound}. We consider the non-negative and negative curvature cases separately.

\paragraph{Non-negative Curvature Case.} Suppose the curvature along the gradient direction is nonnegative, i.e., $\gHg \geq 0$. In this case, \cref{alg:scaling selection} selects either the $\SPC$ or $\LPC$ scalings.  By \cref{prop:scaling upper bounds,prop:second-order descent}, we have $s \leq 1/\sigma$ and \cref{eqn:second-order descent}. Considering \cref{eqn:function value upper bound} and applying $\alpha \leq 1 $ and \cref{eqn:second-order descent}, we have
\begin{align*}
    f(\xx + \alpha \pp) &\leq f(\xx) + \alpha\dotprod{\bgg, \pp}  + \frac{\alpha^2}{2} \dotprod{\pp , \HH \pp} + \frac{L_2 \alpha^3 }{6} \vnorm{\pp}^3 \\ 
    &\leq f(\xx) + \frac{\alpha}{2}\dotprod{\bgg, \pp}  + \frac{\alpha}{2} \left(\dotprod{\bgg, \pp} + \dotprod{\pp , \HH \pp}\right) + \frac{L_2 \alpha^3 }{6} \vnorm{\pp}^3 \\
    &\leq f(\xx) + \frac{\alpha}{2}\dotprod{\bgg, \pp} + \frac{L_2 \alpha^3 }{6} \vnorm{\pp}^3. 
\end{align*}
Subtracting $f(\xx) + \alpha \rho \dotprod{\pp, \bgg}$ from both sides yields 
\begin{align*}
    f(\xx + \alpha \pp) - f(\xx) - \alpha \rho \dotprod{\pp, \bgg} &\leq \alpha\left(\frac{1}{2} - \rho \right)\dotprod{\pp, \bgg} + \frac{L_2 \alpha^3}{6} \vnorm{\pp}^3 \\
    &= -\alpha \left(\frac{1}{2} - \rho \right)\vnorm{\bgg}\vnorm{\pp} + \frac{L_2 \alpha^3 }{6} s \vnorm{\bgg} \vnorm{\pp}^2 \\
    &= \left( \rho - \frac{1}{2} + \frac{L_2 \alpha^2 s \vnorm{\pp} }{6}  \right) \alpha \vnorm{\pp}\vnorm{\bgg} \\
    &\leq \left( \rho - \frac{1}{2} + \frac{L_2 \alpha^2 \vnorm{\pp} }{6\sigma }  \right) \alpha \vnorm{\pp}\vnorm{\bgg},
\end{align*}
where the last inequality follows from $s \leq 1/\sigma$. This implies \cref{eqn:armijo condition} holds if  
\begin{align}
    \label{eqn:positive curvature line search termination}
    \alpha  \leq \min\left\{1,\sqrt{\frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\pp}}}\right\}.
\end{align}

\paragraph{Negative Curvature Case.} We now consider the case where $\dotprod{\bgg, \HH \bgg} < 0$, in which case \cref{alg:scaling selection} selects the $\NC$ scaling. As a result, dropping the negative term in \cref{eqn:function value upper bound} gives
\begin{align*}
    f(\xx + \alpha \pp) &\leq f(\xx) + \alpha\dotprod{\pp, \bgg} + \frac{\alpha^2}{2} \dotprod{\pp, \HH \pp} + \frac{L_2 \alpha^3}{6} \vnorm{\pp}^3 \\
    &\leq f(\xx) + \alpha\dotprod{\pp, \bgg} + \frac{L_2 \alpha^3}{6} \vnorm{\pp}^3.
\end{align*}
Similarly to the previous case, subtracting $f(\xx) + \alpha \rho \dotprod{\pp, \bgg}$ yields 
\begin{align*}
    f(\xx + \alpha \pp ) - f(\xx) - \alpha  \rho \dotprod{\pp, \bgg} &\leq \alpha (1 - \rho)\dotprod{\pp, \bgg} + \frac{L_2 \alpha^3}{6} \vnorm{\pp}^3 \\
    &\leq -\alpha (1 - \rho)\vnorm{\bgg}\vnorm{\pp} + \frac{L_2 \alpha^3 s}{6} \vnorm{\bgg}\vnorm{\pp}^2 \\
    &\leq \alpha \vnorm{\pp} \vnorm{\bgg} \left(- (1 - \rho) + \frac{L_2 \alpha^2 (\sNC_{\tmax})}{6} \vnorm{\pp}\right),
\end{align*}
which implies \cref{eqn:armijo condition} if  
\begin{align}    \label{eqn:NC case line search termination}
    \alpha  \leq \sqrt{\frac{6(1-\rho)}{L_2\sNC_{\tmax}\vnorm{\pp}}}, 
\end{align}

This analysis demonstrates the importance of separating the nonnegative and negative cases. In particular, in the case of the former, the condition $\alpha \leq 1$ is necessary, while in the latter, the second-order term is omitted entirely, allowing for unrestricted step sizes. This distinction explains why large step sizes (e.g., from forward tracking line search) are feasible in the negative curvature case. 

The bounds in \cref{eqn:positive curvature line search termination,eqn:NC case line search termination} ensure that the line search condition is satisfied by small enough step sizes. While these bounds are typically utilized for proving global convergence, we instead establish a \textit{local unit step size} result for our Hessian-aware scaled gradient descent. Specifically, these bounds imply that, for $\vnorm{\pp}$ sufficiently small, the line search condition \cref{eqn:armijo condition} is satisfied with $\alpha = 1$. Through \cref{prop:scaling upper bounds}, this can be linked to a condition on the gradient, which forms the proof of \cref{prop:unit step size acceptance}.

\begin{proof}[Proof of \cref{prop:unit step size acceptance}]
    Suppose $\gHg \geq 0$. Since $s \leq 1/\sigma$, we have $\vnorm{\pp} \leq \vnorm{\bgg}/\sigma$ so that if \cref{eqn:gradient line search termination} holds then
    \begin{align*}
        \vnorm{\pp} \leq \frac{\vnorm{\bgg}}{\sigma} \leq \frac{6\sigma(1/2 - \rho)}{L_2},
    \end{align*}
    which implies \cref{eqn:positive curvature line search termination} holds with $\alpha=1$. When $\gHg < 0$, we have $s \leq \sNC_{\tmax}$, and hence if \cref{eqn:gradient line search termination} then
    \begin{align*}
        \vnorm{\pp} \leq \sNC_{\tmax}\vnorm{\bgg} \leq\frac{6(1-\rho)}{L_2\sNC_{\tmax}},
    \end{align*}
    which implies \cref{eqn:NC case line search termination} with $\alpha=1$.
\end{proof}

\subsection{Proof of \cref{thm:global convergence}}
Let us first restate \cref{thm:global convergence} with all the details and underlying constants.
\begin{theorem}[Restatement of \cref{thm:global convergence}]
    \label{thm:global convergence_detailed}
    Consider \cref{ass:Hessian gradient smoothness condition,ass:Hessian smoothness condition} and suppose $- \infty < f^{\star} = \min_{\xx \in \real^{d}} f(\xx)$. For any $0 < \eg < 1$, after at most  
    \begin{align*}
        K = \left\lceil\frac{f(\xx_0) - f^{\star}}{\min\left\{ \cSPC, \cLPC, \cNC \right\} \eg^2}\right\rceil,
    \end{align*}
    iterations of \cref{alg:scaled gradient}, we have $\vnorm{\bgg_k} \leq \varepsilon_{\bgg}$, where $\cSPC$, $\cLPC$ and $\cNC$ are defined as follows:
    \begin{align*}
        \cSPC &\defeq \sigma\rho\min\left\{ \left(\frac{6\sigma(\tfrac12 - \rho)}{L_2} \right)^2,  \frac{1}{L_1^2} \right\}\\
        \cLPC &\defeq\rho\min\left\{\sLPC_{\tmin}, \sqrt{\frac{6\sigma(\tfrac12 - \rho)\sLPC_{\tmin}}{L_2}} \right\}, \\
        \cNC &\defeq \rho\sqrt{\frac{6(1-\rho) \sNC_{\tmin}}{L_2 \sNC_{\tmax}}}.
    \end{align*}
\end{theorem}
Before providing the proof, we note that, as discussed earlier, $L_1$ and $L_2$ are lower bounds for the global Lipschitz constants of the problem (if such constants exist), so the factor based on $L_1$ and $L_2$ could be significantly smaller than those based on their global counterparts. The constant factor in \cref{thm:global convergence_detailed} depends quadratically on both $L_{1}$ and $L_{2}$, but the dependence on $L_{1}$ can be improved to linear if the MR scaling is not used; see \cref{remark:scaling lower bound}. This would align with the dependence on the Lipschitz gradient constant for standard gradient descent in the nonconvex setting.

To prove \cref{thm:global convergence_detailed}, we first consider a lemma which analyzes the worst case \emph{per-iteration} descent for the \texttt{NC}, \texttt{LPC}, and \texttt{SPC} separately. 

\begin{lemma}[Per-iteration Descent]\label{lemma:per iteration decrease}
    Consider \cref{ass:Hessian smoothness condition}, and let $\pp$ and $\alpha > 0$ be generated by \cref{alg:scaled gradient}. If $\gHg < 0$ ($\NC$), then
        \begin{align*}
            f(\xx + \alpha \pp) \leq f(\xx) - \rho\sqrt{\frac{6(1-\rho) \sNC_{\tmin}}{L_2 \sNC_{\tmax}}} \vnorm{\bgg}^{3/2}.
        \end{align*}
    If $0 \leq \gHg \leq \sigma \vnorm{\bgg}^2$ ($\LPC$), then
    \begin{align*}
        f(\xx + \alpha \pp) \leq f(\xx) - \rho\min\left\{\sLPC_{\tmin} \vnorm{\bgg}^2, \sqrt{\frac{6\sigma(1/2 - \rho)\sLPC_{\tmin}}{L_2}} \vnorm{\bgg}^{3/2} \right\}.
    \end{align*}
    Suppose $\gHg > \sigma \vnorm{\bgg}^2$ ($\SPC$). If
    \begin{align}
        \label{eqn:SPC step norm lower bound}
        \vnorm{\pp} \geq \frac{6 \sigma (1/2 - \rho)}{L_2},
    \end{align}
    then
    \begin{align*}
        f(\xx + \alpha \pp) \leq f(\xx) -\rho \sigma \left(\frac{6\sigma(1/2 - \rho)}{L_2}\right)^2.
    \end{align*}
    Otherwise, we have $\alpha =1$ and 
    \begin{align*}
        f(\xx + \pp) \leq  f(\xx) -\rho \vnorm{\pp}\vnorm{\bgg}.
    \end{align*}
\end{lemma}
\begin{proof}
    ($\NC$) From \cref{eqn:NC case line search termination}, it follows that the largest step size satisfying \cref{eqn:armijo condition} must satisfy
    \begin{align*}
        \alpha \geq \sqrt{\frac{6(1-\rho)}{L_2 \sNC_{\tmax} \vnorm{\pp}}}.
    \end{align*}
    Hence, 
    \begin{align*}
        f(\xx + \alpha \pp) - f(\xx) &\leq  \rho \alpha \dotprod{\pp, \bgg} = -\rho \alpha \vnorm{\pp}\vnorm{\bgg}\leq - \rho \sqrt{\frac{6(1-\rho) \vnorm{\pp}}{L_2 \sNC_{\tmax}}} \vnorm{\bgg}.
    \end{align*}
    Applying $\vnorm{\pp} \geq \sNC_{\tmin} \vnorm{\bgg}$ gives the result. 
    
    ($\LPC$) Similarly, \cref{eqn:positive curvature line search termination} implies that the largest step size $\alpha \in (0,1]$ satisfying \cref{eqn:armijo condition} must satisfy 
    \begin{align*}
        \alpha \geq \min\left\{1, \sqrt{\frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\pp}}} \right\}.
    \end{align*}
    Now, from \cref{eqn:armijo condition} we obtain 
    \begin{align*}
        f(\xx + \alpha \pp) - f(\xx) &\leq \rho \alpha \dotprod{\pp, \bgg} = -\rho \alpha \vnorm{\pp} \vnorm{\bgg} \\
        &\leq -\rho\min\left\{\vnorm{\pp} \vnorm{\bgg}, \sqrt{\frac{6\sigma(1/2 - \rho)\vnorm{\pp}}{L_2}}\vnorm{\bgg} \right\}.
    \end{align*}
    Since $\sLPC \geq \sLPC_{\tmin}$, $\vnorm{\pp} \geq \sLPC_{\tmin} \vnorm{\bgg}$, which yields the result.
    
    ($\SPC$) Similar to the $\LPC$ case, \cref{eqn:positive curvature line search termination} implies that the largest step size $\alpha \in (0,1]$ satisfying \cref{eqn:armijo condition} must satisfy 
    \begin{align*}
        \min\left\{1, \sqrt{\frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\pp}}} \right\} \leq \alpha \leq 1.
    \end{align*}
    Suppose  \cref{eqn:SPC step norm lower bound} holds, which implies  
    \begin{align*}
        \alpha \geq \sqrt{\frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\pp}}}.
    \end{align*}
    In light of \cref{eqn:armijo condition,eqn:second-order descent} and the fact that  $\dotprod{\pp, \HH \pp} > \sigma \vnorm{\pp}^2$, we have
    \begin{align*}
        f(\xx + \alpha \pp) - f(\xx) &\leq \rho \alpha \dotprod{\pp, \bgg} \leq - \rho \alpha \dotprod{\pp, \HH \pp} 
        \leq - \rho \alpha \sigma \vnorm{\pp}^2 \leq -\rho \sigma \left(\frac{6\sigma(1/2 - \rho)}{L_2}\right)^2,
    \end{align*}
    where the final inequality follows from \cref{eqn:SPC step norm lower bound}. If \cref{eqn:SPC step norm lower bound} does not hold, then $\alpha=1$ and hence 
    \begin{align*}
        f(\xx + \pp) - f(\xx) &\leq \rho \dotprod{\pp, \bgg} = - \rho \vnorm{\bgg}\vnorm{\pp}.
    \end{align*}
\end{proof}

By inspecting the proof of \cref{lemma:per iteration decrease}, we see that the per iteration decrease is independent of the scaling magnitude in all cases except $\SPC$, $\alpha=1$. As discussed in the main body, we handle this case with a regularity condition on the Hessian-gradient product, \cref{ass:Hessian gradient smoothness condition}. In particular, Combining \cref{eqn:MR scaling defn}, $\sigma \vnorm{\bgg}^2 < \dotprod{\bgg, \HH \bgg }$, and $ \vnorm{\HH \bgg} \leq L_1 \vnorm{\bgg}$ from \cref{ass:Hessian gradient smoothness condition}, we can obtain a lower bound on the $\SPC$ scalings. 

\begin{lemma}[Scaling Lower Bounds] \label{lemma:scaling lower bounds}
    Consider \cref{ass:Hessian gradient smoothness condition} and let $\sigma \leq L_1$. We have
    \begin{align}
    \label{eqn:MR scaling lower bound}
        \frac{\sigma}{L_1^2} &\leq \sMR \leq \sGM \leq \sCG.
    \end{align}
\end{lemma}

\begin{remark} \label{remark:scaling lower bound}
    Note that since $\dotprod{ \bgg, \HH\bgg} \leq \vnorm{\HH \bgg} \vnorm{\bgg} \leq L_1 \vnorm{\bgg}^2$, the $\SPC$ case is detected only when $\sigma$ is chosen such that $\sigma \leq L_1$; otherwise \cref{alg:scaling selection} always returns either of $\LPC$ or $\NC$ scalings. Additionally, by applying $\vnorm{\HH\bgg} \leq L_1 \vnorm{\bgg}$ directly to $\sGM$ we can sharpen the lower bound for the CG and GM scalings to $1/L_1 \leq \sGM \leq \sCG$.
\end{remark}

With \cref{lemma:per iteration decrease} and \cref{lemma:scaling lower bounds} in hand, the proof of \cref{thm:global convergence} proceeds by combining the worst case analysis of each case and \cref{lemma:scaling lower bounds}. 

\begin{proof}[Proof of \cref{thm:global convergence_detailed}]
    Suppose that $\vnorm{\bgg_k} \leq \eg$ fails to hold for $k=0,\ldots,K-1$. We divide the iterations $\sK = \{0, \ldots,K-1\}$ into a disjoint union $\sK = \sK_{\text{NC}} \cup \sK_{\text{LPC}} \cup \sK_{\text{SPC}} $ where 
    \begin{align*} 
        \sK_{\text{NC}} &= \left\{ k \in \sK \mid \dotprod{\bgg_k, \HH_k \bgg_k} < 0 \right\} \\
        \sK_{\text{LPC}} &= \left\{ k \in \sK \mid 0 \leq \dotprod{\bgg_k, \HH_k \bgg_k} \leq \sigma \vnorm{\bgg_k}^2 \right\} \\
        \sK_{\text{SPC}} &= \left\{ k \in \sK \mid \dotprod{\bgg_k, \HH_k \bgg_k} > \sigma \vnorm{\bgg_k}^2 \right\}.
    \end{align*}
    For $k \in \sK_{\text{NC}}$, \cref{lemma:per iteration decrease}, $\vnorm{\bgg_k} > \eg$ and $\eg < 1$ give
    \begin{align*}
         f(\xx_k) - f(\xx_{k+1}) >  \rho\sqrt{\frac{6(1-\rho) \sNC_{\tmin}}{L_2 \sNC_{\tmax}}} \eg^{3/2} \geq \cNC \eg^2,
    \end{align*}
    Similarly, for $k \in \sK_{\text{LPC}}$, \cref{lemma:per iteration decrease}, $\vnorm{\bgg_k} > \eg$, and $\eg < 1$ imply
    \begin{align*}
         f(\xx_k) -f(\xx_{k+1}) &> \rho\min\left\{\sLPC_{\tmin} \vnorm{\bgg_k}^2, \sqrt{\frac{6\sigma(1/2 - \rho)\sLPC_{\tmin}}{L_2}} \vnorm{\bgg_k}^{3/2} \right\} \\
        &> \rho\min\left\{\sLPC_{\tmin} \eg^2, \sqrt{\frac{6\sigma(1/2 - \rho)\sLPC_{\tmin}}{L_2}}\eg^{3/2} \right\}  \\
        &\geq \cLPC \eg^2,
    \end{align*}
    For $k \in \sK_{\text{SPC}}$, if $\cref{eqn:SPC step norm lower bound}$ does not hold, then \cref{lemma:scaling lower bounds,lemma:per iteration decrease} give
    \begin{align*}
        f(\xx_k) - f(\xx_{k+1}) &\geq \rho \vnorm{\pp_k} \vnorm{\bgg_k} 
        \geq \frac{\sigma \rho}{L_1^2} \vnorm{\bgg_k}^2 
        > \frac{\sigma \rho}{L_1^2} \eg^2.
    \end{align*}
    We can combine this with the case where $\cref{eqn:SPC step norm lower bound}$ is satisfied to obtain 
    \begin{align*}
        f(\xx_k) - f(\xx_{k+1}) &> \rho\min\left\{ \sigma \left(\frac{6\sigma^2(1/2 - \rho)}{L_2} \right)^2,  \frac{\sigma}{L_1^2} \eg^2\right\} \\
        &\geq  \sigma\rho\min\left\{ \left(\frac{6\sigma(1/2 - \rho)}{L_2} \right)^2,  \frac{1}{L_1^2} \right\}\eg^2 \\
        &= \cSPC \eg^2,
    \end{align*}
    where in the second line we applied $\eg<1$. Finally, we apply a telescoping sum over each case
    \begin{align*}
        f(\xx_0) - f(\xx_K) &= \sum_{i=0}^{K-1} f(\xx_k) - f(\xx_{k+1}) \\
        &= \sum_{k\in\sK_{\text{NC}}} f(\xx_k) - f(\xx_{k+1}) + \sum_{k\in\sK_{\text{LPC}}} f(\xx_k) - f(\xx_{k+1}) + \sum_{k\in\sK_{\text{SPC}}} f(\xx_k) - f(\xx_{k+1}) \\
        &> |\sK_{\text{NC}} | \cNC \eg^2 + |\sK_{\text{LPC}}|\cLPC \eg^2 + |\sK_{\text{SPC}}| \cSPC \eg^2 \\
        &\geq (|\sK_{\text{NC}} | + |\sK_{\text{LPC}} | + |\sK_{\text{SPC}}|)\min\left\{ \cNC, \cLPC, \cSPC  \right\}\eg^2 \\
        &= K\min\left\{ \cNC, \cLPC, \cSPC \right\}\eg^2 \\
        &\geq f(\xx_0) - f^{\star},
    \end{align*}
    which implies $f^{\star} > f(\xx_K)$, leading to a contradiction.
\end{proof}



\subsection{Proof of \cref{thm:local convergence}}
\label{sec:appendix:proofs}

 We begin with a lemma, which bounds on the objective function sub-optimality and the gradient norm in a region local to a second order sufficient minima.     
\begin{lemma}
    \label{lem: mu and M local}
    Suppose that $\xx^\star$ satisfies the second order sufficient conditions. Furthermore, suppose that $r$ is chosen such that \cref{eqn:second-order sufficient conditions} holds for any $\xx \in \sB^*_r$, then
    \begin{align*}
        \frac{\mu}{2} \vnorm{\xx - \xx^{\star}}^2 \leq f(\xx) - f(\xx^{\star}) &\leq \frac{M}{2} \vnorm{\xx - \xx^{\star}}^2, \tageq\label{eqn:function local bound} \\
        f(\xx) - f(\xx^{\star}) &\leq \frac{1}{2\mu} \vnorm{\bgg}^2, \tageq\label{eqn:local PL inequality}\\
        \mu \vnorm{\xx - \xx^{\star}} \leq \vnorm{\bgg(\xx)} &\leq M\vnorm{\xx - \xx^{\star}}. \tageq\label{eqn:gradient local bound}
    \end{align*}
\end{lemma}
\begin{proof}
    For any $\xx, \yy \in \real^d$, the mean value theorem for twice differentiable functions implies
    \begin{align*}
        f(\yy) = f(\xx) + \dotprod{\bgg(\xx), \yy - \xx} +\frac{1}{2}\dotprod{\yy - \xx, \HH(\xx + t(\yy - \xx)(\yy - \xx)},
    \end{align*}
    for some $t \in (0,1)$. If $\xx, \yy \in \sB^*_r$, then $\xx + t(\yy - \xx) \in \sB^*_r$, and so 
    \begin{align*}
        \frac{\mu}{2}\vnorm{\yy -\xx}^2 \leq f(\yy) - f(\xx) - \dotprod{\bgg(\xx), \yy - \xx} \leq \frac{M}{2}\vnorm{\yy -\xx}^2. 
    \end{align*}
    By setting $\xx = \xx^{\star}$ and $\yy = \xx \in \sB^*_r$ we obtain \cref{eqn:function local bound}. Now, by rearranging we obtain
    \begin{align*}
        f(\yy) \geq f(\xx) + \dotprod{\bgg(\xx), \yy - x} + \frac{\mu}{2} \vnorm{\yy -\xx}^2. 
    \end{align*}
    Minimizing both sides over $\sB_r^\star$ gives \cref{eqn:local PL inequality} since
    \begin{align*}
        f(\xx^{\star}) = \min_{\yy \in \sB_r^\star} f(\yy) &\geq \min_{\yy \in \sB_r^\star} f(\xx) + \dotprod{\bgg(\xx), \yy - x} + \frac{\mu}{2} \vnorm{\yy -\xx}^2 \\
        &\geq \min_{\yy \in \real^d}f(\xx) + \dotprod{\bgg(\xx), \yy - \xx} + \frac{\mu}{2} \vnorm{\yy -\xx}^2 \\
        &= f(\xx) - \frac{1}{2\mu} \vnorm{\bgg(\xx)}^2.
    \end{align*}
    Finally, recall that for twice continuously differentiable functions we have
    \begin{align*}
        \bgg(\xx) = \int_0^1 \HH(\xx^{\star} + t(\xx - \xx^{\star}))(\xx - \xx^{\star}) \, dt.
    \end{align*}
    Hence, for $\xx \in \sB_r^\star$, we get
    \begin{align*}
        \vnorm{\bgg(\xx)}^2 =\dotprod{\bgg(\xx), \bgg(\xx)} &= \dotprod{\int_0^1 \HH(\xx^{\star} + t(\xx - \xx^{\star}))(\xx - \xx^{\star}) \, dt, \int_0^1 \HH(\xx^{\star} + s(\xx - \xx^{\star}))(\xx - \xx^{\star}) \, ds} \\
        &= \int_0^1 \int_0^1 \dotprod{ \HH(\xx^{\star} + t(\xx - \xx^{\star}))(\xx - \xx^{\star}) , \HH(\xx^{\star} + s(\xx - \xx^{\star}))(\xx - \xx^{\star}) }\, ds \, dt \\
        &= \int_0^1 \int_0^1 \dotprod{\xx - \xx^{\star} , \HH(\xx^{\star} + t(\xx - \xx^{\star}))\HH(\xx^{\star} + s(\xx - \xx^{\star}))(\xx - \xx^{\star}) }\, ds \, dt .
    \end{align*}
    For positive definite matrices $\AA$ and $\BB$, we have $\lambda_{\tmin}(\AA)\lambda_{\tmin}(\BB) \leq \lambda_{\tmin}(\AA\BB)$ and $\lambda_{\tmax}(\AA\BB) \leq \lambda_{\tmax}(\AA)\lambda_{\tmax}(\BB)$. Now since $t, s \in [0, 1]$ we have
    \begin{align*}
        \mu^2\eye \preceq \HH(\xx^{\star} + t(\xx - \xx^{\star}))\HH(\xx^{\star} + s(\xx - \xx^{\star})) \preceq M^2\eye,
    \end{align*}
    which gives the bounds in \cref{eqn:gradient local bound}.
\end{proof}

With \cref{lem: mu and M local} in hand we prove \cref{thm:local convergence}.

\begin{proof}[Proof of \cref{thm:local convergence}]
    Since $\bgg(\xx^{\star}) = 0$ and the gradient is continuous, we can choose $r' \leq r$ small enough that that \cref{eqn:gradient line search termination} and \cref{eqn:second-order sufficient conditions} hold on $\xx \in \sB_{r'}^\star$.
    Now suppose that $\xx_k \in \sB_{r'}^\star$, \cref{prop:unit step size acceptance,eqn:armijo condition,eqn:local PL inequality} imply
    \begin{align*}
        f(\xx_{k+1}) &\leq f(\xx_k) + \rho \dotprod{\pp_k, \bgg_k} = f(\xx_k) - s_k \rho \vnorm{\bgg_k}^2 \leq f(\xx_k) - 2 s_k  \mu \rho\left( f(\xx_k) - f(\xx^{\star}) \right),
    \end{align*}
    which implies
    \begin{align}
    \label{eqn:local function sub-optimality recursion}
    f(\xx_{k+1}) - f(\xx^{\star}) &\leq \left( 1 -  2\mu\rho s_k \right)(f(\xx_k) - f(\xx^{\star})).
    \end{align}
    Since $\HH_k \succ 0$, we only need to consider $\LPC$ and $\SPC$ scalings. If $\bgg_k$ is a $\SPC$ direction, then following a similar line of reasoning as that in  \cref{lemma:scaling lower bounds}, with $M$ taking the place of $L_1$, we get $s_{k} = \sSPC \geq 1/M$. Note that for the MR scaling, this bound follows from $\mu \eye \preceq \HH_k \preceq M\eye$ and \cref{lemma:tighter MR lower bound}. Hence, \cref{eqn:local function sub-optimality recursion} gives
    \begin{align}
        f(\xx_{k+1}) - f(\xx^{\star}) &\leq \left( 1 -  \frac{2\mu\rho}{M}\right)(f(\xx_k) - f(\xx^{\star})). \label{eqn:local function sub-optimality recursion SPC CASE}
    \end{align}
    If $\sigma < \mu$ then clearly $\bgg_k$ cannot be an $\LPC$ direction and so \cref{eqn:local function sub-optimality recursion SPC CASE} yields the desired recursion in function sub-optimality. On the other hand, if $\sigma \geq \mu$ and $\bgg_k$ is an $\LPC$ direction, then we simply apply $\sLPC_{\tmin} \leq \sLPC$ to \cref{eqn:local function sub-optimality recursion} to obtain
    \begin{align}
        f(\xx_{k+1}) - f(\xx^{\star}) &\leq \left( 1 -  2\mu\sLPC_{\tmin}\rho \right)(f(\xx_k) - f(\xx^{\star})). \label{eqn:local function sub-optimality recursion LPC CASE}
    \end{align}
    The desired recursion follows from combining \cref{eqn:local function sub-optimality recursion SPC CASE} and \cref{eqn:local function sub-optimality recursion LPC CASE}.
    
    It remains to show that the iterates remain in $\sB_{r'}^\star$. To this end, define the set
    \begin{align*}
        \sF = \left\{ x \mid f(\xx) - f(\xx^{\star}) \leq \frac{\mu \sigma^{2} r^2}{2\left(\sigma + M\right)^2} \right\}.
    \end{align*}
    Now suppose that an iterate $\xx_k$ is sufficiently close to $\xx^{\star}$ such that $\xx_k \in B_{r'}^\star \cap \sF$. Such  $\xx_k$ exists, as implied by the upper bound in \cref{eqn:function local bound}. We now show that all subsequent iterates remain in $B_{r'}^\star \cap \sF$. Indeed, from the earlier part of the proof, we get $f(\xx_{k+1}) - f(\xx^{\star}) \leq (1-\tau)(f(\xx_k) - f(\xx^{\star}))$, which implies $ \xx_{k+1} \in \sF$. On the other hand, from $\xx_{k+1} = \xx_k - s_k \bgg_k$ (since $\alpha=1$ is accepted by the line search), $s \leq 1/\sigma$ (\cref{prop:scaling upper bounds}), the upper bound in \cref{eqn:gradient local bound}, and the lower bound in $\cref{eqn:function local bound}$, it follows that 
    \begin{align*}
        \vnorm{\xx_{k+1} - \xx^{\star}} &\leq \vnorm{\xx_k - \xx^{\star}} + s_k\vnorm{\bgg_k} \leq \left(1 + \frac{M}{\sigma} \right)\vnorm{\xx_k - \xx^{\star}} \\
        &\leq \left(1 + \frac{M}{\sigma}\right) \sqrt{\frac{2}{\mu} (f(\xx_k) - f(\xx^{\star}))} \leq r,
    \end{align*}
    which gives $\xx_{k+1} \in \sB_{r'}^\star$, and hence $\xx_{k+1} \in \sB_{r'}^\star \cap \sF$.
\end{proof}

\subsection{Convergence in Gradient Norm and Proof of \cref{thm:local convergence MR second-order sufficient}} 
\label{apx:convergence in gradient norm}

We prove \cref{thm:local convergence MR second-order sufficient} by considering a more general result, \cref{prop:gradient norm recursion}, to which \cref{thm:local convergence MR second-order sufficient} is a corollary. Our analysis begins by utilizing \cref{ass:Hessian smoothness condition} and a bound on the residual to obtain a recursion in the gradient norm. 

\begin{lemma} \label{lemma:gradient norm recursion}
    Considering \cref{ass:Hessian smoothness condition} and supposing $\vnorm{\HH\bgg} \neq 0$ and $\vnorm{\bgg} \neq 0$ we have the following recursion for the gradient with the step $\pp=-\sMR\bgg$
    \begin{align*}
        \vnorm{\bgg(\xx + \pp)} &\leq \frac{L_2 }{2}\vnorm{\pp}^2 + \sqrt{1 - \cos^2{\theta}}\vnorm{\bgg}, \tageq\label{eqn:MR case upper bound}
    \end{align*}
    where $\theta \defeq \measuredangle(\bgg, \HH \bgg)$ is the angle between $\bgg$ and $\HH \bgg$.
\end{lemma}
\begin{proof}
    Adding and subtracting appropriate values and applying the triangle inequality we have
    \begin{align*}
    \vnorm{\bgg(\xx + \pp)} &= \vnorm{\bgg(\xx + \pp) + \bgg + \HH \pp - \bgg - \HH \pp } \\
    &\leq \vnorm{\bgg(\xx + \pp) - \bgg - \HH \pp} + \vnorm{\bgg + \HH \pp} \\
    &= \vnorm{\bgg(\xx + \pp) - \bgg - \HH \pp} + \vnorm{\rr}.
    \end{align*}
    Now, \cref{ass:Hessian smoothness condition,eqn:gradient upper bound} gives
    \begin{align*}
        \vnorm{\bgg(\xx + \pp)} \leq \frac{L_2}{2}\vnorm{\pp}^2 + \vnorm{\rr}.
    \end{align*}
    We can handle the residual term using the properties of MR scaling. Indeed, applying the definition of $\sMR$ for $\HH\bgg \neq 0$ we have
    \begin{align*}
        \vnorm{\rr}^2 = \vnorm{\bgg - \sMR \HH \bgg}^2 &= \vnorm{\bgg}^2 - 2\sMR \dotprod{\bgg, \HH \bgg} + (\sMR)^2\vnorm{\HH \bgg}^2 \\
        &= \vnorm{\bgg}^2 - 2\frac{\dotprod{\bgg, \HH \bgg}^2 }{\vnorm{\HH \bgg}^2 } + \frac{\dotprod{\bgg, \HH \bgg}^2 }{ \vnorm{\HH \bgg}^2 } \\
        &= \vnorm{\bgg}^2\left( 1 - \frac{\dotprod{\bgg, \HH \bgg}^2 }{\vnorm{\HH \bgg}^2 \vnorm{\bgg}^2 } \right) \\
        &= \vnorm{\bgg}^2\left( 1 - \cos^2{\theta} \right),
    \end{align*}
    Putting this all together yields
    \begin{align*}
        \vnorm{\bgg(\xx + \pp)} &\leq \frac{L_2}{2}\vnorm{\pp}^2 + \sqrt{1 - \cos^2{\theta}} \vnorm{\bgg}.
    \end{align*}
    which is \cref{eqn:MR case upper bound}.
\end{proof}

To apply \cref{eqn:MR case upper bound} for convergence in gradient norm, the magnitude of $(\sMR)^2$ needs to be bounded from above and $\cos{\theta}$ must remain bounded away from zero. To that end, we introduce \cref{ass:MR local assumption}, which excludes cases where the MR scaling becomes unbounded and $\theta \approx \pi/2$, meaning $\HH\bgg$ and $\bgg$ become nearly orthogonal.

\begin{assumption}\label{ass:MR local assumption}
Given $\xx_0$, there exists constants $\nu_{0} > 0$ and $\mu_{0} > 0$, possibly depending on $\xx_0$, such that for all the iterates of the form $\xx_{k+1} = \xx_k - \sMR_{k} \bgg_k$, $k \geq 0$, with $\bgg_k \neq 0$ we have 
\begin{align*}
    \mu_0\vnorm{\bggk}^2 &\leq | \dotprod{ \bggk, \HHk \bggk} |, \tageq\label{eqn:MR curvature absolute lower bound} \\
    \nu_{0} &\leq \cos^2{\theta_{k}}. \tageq\label{eqn:cosine lower bound}
\end{align*} 
\end{assumption}
 
\begin{remark}
    The conditions in \cref{ass:MR local assumption} are general and somewhat unconventional. However, as we shall see they are a weakening of more typical conditions. 
    Indeed, any smooth and strongly convex function on the set containing the iterates satisfies \cref{ass:MR local assumption}. 
    Furthermore, as we shall see in the proof of \cref{thm:local convergence MR second-order sufficient}, the second order sufficient conditions subsume \cref{ass:MR local assumption} for $\xx_0$ close enough to a minima. 
    The gradient curvature condition, \cref{eqn:MR curvature absolute lower bound}, is a weakening of strong convexity, as the curvature along gradient direction need only be uniformly bounded away from zero (instead of uniformly positive). 
    Applying the Cauchy-Schwarz inequality to \cref{eqn:MR curvature absolute lower bound} we see that $\vnorm{\bgg_k} > 0$ implies $\vnorm{\HH_k \bgg_k} > 0$, which ensures that $\cos{\theta_k}$ and $\sMR_k$ remain well defined along the path of the iterates.
    On the other hand, the cosine condition \cref{eqn:cosine lower bound} is implied by \cref{eqn:MR curvature absolute lower bound} along with a bound of the form $\vnorm{\HH_k \bgg_k} \leq L \vnorm{\bgg_k}$.
    However, \cref{eqn:cosine lower bound} is also more generally applicable. For example, when $d=1$, \cref{eqn:cosine lower bound} holds with $\cos^2{\theta}=1$, so long as $\HH \neq 0$. Another example, is $f(\xx) = \vnorm{\xx}^3/6$ where, again, one can show $\cos^2{\theta}=1$. 
\end{remark}

An immediate consequence of \cref{eqn:MR curvature absolute lower bound} and $\vnorm{\HH_k\bgg_k} \geq \abs{\dotprod{\bgg_k, \HH_k \bgg_k}}/\vnorm{\bgg_k}$ is that
\begin{align*}
    (\sMR_k)^2 = \frac{(\dotprod{\bgg_k, \HH_k \bgg_k})^2 }{ \vnorm{\HH_k \bgg_k}^4} \leq \frac{\vnorm{\bgg_k}^4}{(\dotprod{\bgg_k, \HH_k \bgg_k})^2} \leq \frac{1}{\mu_0^2}.
\end{align*}
Therefore by applying \cref{ass:MR local assumption,eqn:MR case upper bound}, we get  
\begin{align*}
    \vnorm{\bgg(\xx + \pp)} \leq \left(\frac{L_2}{2\mu_{0}^2}\vnorm{\bgg} + \sqrt{ 1 - \nu_{0}} \right)\vnorm{\bgg}. \tageq\label{eqn:MR case gradient recursion}
\end{align*}
The recursion in the gradient norm, \cref{eqn:MR case gradient recursion}, is linear-quadratic in a manner similar to the results obtained for Newton-MR method applied to invex problems; see \cite{roostaNewtonMRInexactNewton2022}. 
%
It follows directly from \cref{eqn:MR case gradient recursion} that, when the gradient is small enough, i.e., 
\begin{align*}
    \vnorm{\bgg} < G_{0} \defeq 2 \mu_{0}^2\left(1 - \sqrt{1 - \nu_{0}} \right)/L_2, \tageq\label{eqn:G_0}
\end{align*}
the gradient norm decreases monotonically. Naturally, once the gradient hits this monotonic phase, every subsequent iterate will also satisfy $\vnorm{\bgg}\leq G_{0}$. These properties are all formalized to a rate in \cref{prop:gradient norm recursion}.


\begin{proposition}[MR Scaling Convergence Gradient Norm] \label{prop:gradient norm recursion}
    Consider \cref{ass:Hessian smoothness condition}, and iterations of the form $\xx_{k+1} = \xx_k - \sMR_{k} \bgg_k$ starting from $\xx_0$ for which $\vnorm{\bgg(\xx_0)} < G_{0}$ and \cref{ass:MR local assumption} holds. For all $k \geq 1$, we have  $\vnorm{\bgg_k} < \vnorm{\bgg_{k-1}}$ and 
    \begin{align*}
        \vnorm{\bgg_k} \leq \frac{G_{0} }{G_{0} - \vnorm{\bgg_0}} \left( 1 -  \frac{1 - \sqrt{1 - \nu_{0}}}{2 - \sqrt{1 - \nu_{0}}} \right)^k \vnorm{\bgg_0}.
    \end{align*}
\end{proposition}
\begin{proof}
Since \cref{ass:MR local assumption} holds with $\xx_{0}$ and $\vnorm{\bgg(\xx_0)}< G_{0}$, \cref{eqn:MR case gradient recursion} implies that $\vnorm{\bgg_k} \leq \vnorm{\bgg_{k-1}} < G_{0}$ for $k \geq 1$. To obtain a quantitative rate, we follow a similar line of reasoning as \citep[Theorem 1.2.4]{nesterovIntroductoryLecturesConvex2004}. Let $a_k = \vnorm{\bgg_k} L_2/(2\mu_{0}^2) $ and $q = 1 - \sqrt{1 - \nu_{0}}$. We have $a_k < q$ and
\begin{align*}
    a_{k+1} \leq a_k^2 + \sqrt{1- \nu_{0}} a_k = (a_k + \sqrt{1 - \nu_{0}}) a_k = (1 + a_k - q) a_k.
\end{align*}
Also, since $\alpha_k - q < 0$ by \cref{eqn:G_0},
\begin{align*}
    a_k(1 + a_k - q)  = \left(\frac{(1 - (a_k - q))(1+(a_k-q))}{(1 - (a_k -q))}\right) a_k  = \left(\frac{(1 - (a_k -q)^2)}{1 - (a_k - q)}\right) a_k  \leq \frac{a_k}{1+ q - a_k},
\end{align*}
where the inequality follows from $(1 - (a_k -q)^2)\leq 1$. Hence,  
\begin{align*}
    \frac{q}{a_{k+1}} - 1 \geq (1 + q) \left(\frac{q}{a_k} - 1 \right) .
\end{align*}
Applying this iteratively gives
\begin{align*}
    \frac{q}{a_k} - 1 \geq (1+q)^k \left(\frac{q}{a_0} - 1\right) = (1+q)^k \left(\frac{2\mu_{0}^2(1 - \sqrt{1-\nu_{0}})}{L_2\vnorm{\bgg_0}} - 1\right) = (1 + q)^k \left( \frac{G_{0}}{\vnorm{\bgg_0}} - 1 \right).
\end{align*}
Rearranging, we obtain 
\begin{align*}
    a_k &\leq \frac{q \vnorm{\bgg_0} }{(1 + q)^k(G_{0} - \vnorm{\bgg_0}) + \vnorm{\bgg_0}}.
\end{align*}
Finally, by substituting in the definition of $a_k$ and $q$, we obtain
\begin{align*}
    \vnorm{\bgg_k} &\leq  \frac{G_{0} \vnorm{\bgg_0}}{(2 - \sqrt{1 - \nu_{0}})^k(G_{0} - \vnorm{\bgg_0})} = \frac{G_{0} }{G_{0} - \vnorm{\bgg_0}} \left( 1 -  \frac{1 - \sqrt{1 - \nu_{0}}}{2 - \sqrt{1 - \nu_{0}}} \right)^k \vnorm{\bgg_0}.
\end{align*}
\end{proof}

Finally, we prove \cref{thm:local convergence MR second-order sufficient} by specializing to the case where the second order sufficient conditions hold.

\begin{proof}[Proof of \cref{thm:local convergence MR second-order sufficient}]
    It suffices to find a region where the conditions of \cref{prop:gradient norm recursion} are satisfied, that the iterates will not leave. Similar to the proof of \cref{thm:local convergence}, when the second order sufficient conditions hold, there exists a ball $\sB_r^\star$ such that \cref{eqn:second-order sufficient conditions} holds. This in turn implies \cref{eqn:gradient local bound}, as well as \cref{eqn:MR curvature absolute lower bound,eqn:cosine lower bound} with $\mu_{0} = \mu$ and $\nu_{0} = \mu^2/M^2$, respectively, for any  $\xx \in \sB_r^\star$. With these $\mu_{0}$ and $\nu_{0}$ in hand, consider $G_{0}$ in \cref{eqn:G_0} and define  
    \begin{align*}
        \sG = \left\{ \xx \mid 0 < \vnorm{\bgg(\xx)} \leq \min\left\{G_{0}, \frac{r \mu}{M \mu + 1} \right\} \right\}.
    \end{align*}
    Suppose $\xx_k \in \sB_r^\star \cap \sG$ (such an $\xx_k$ exists by the RHS of \cref{eqn:gradient local bound}). From \cref{eqn:MR case gradient recursion}, we get $\vnorm{\bgg_{k+1}} < \vnorm{\bgg_k}$, which in turn implies $\xx_{k+1} \in \sG$. Applying the upper bound in \cref{eqn:gradient local bound}, $\sMR_{k} \leq 1/\mu$ and $\xx_k\in \sG$, gives 
    \begin{align*}
        \vnorm{\xx_{k+1} - \xx^{\star}} &\leq \vnorm{\xx_k - \xx^{\star}} + \abs{\sMR_{k}}\vnorm{\bgg_k} 
        \leq \left(M + \frac{1}{\mu} \right) \vnorm{\bgg_k} 
        \leq r.
    \end{align*}
    That is, $\xx_{k+1} \in \sB_r^\star \cap \sG$. Hence, \cref{ass:MR local assumption} holds for all of the iterates if $\xx_0 \in \sB_r^\star \cap \sG$. \cref{prop:gradient norm recursion} can be applied to obtain the convergence rate. 
\end{proof}

Finally, we remark that the iteration $\xx_{k+1} = \xx_k - \sMR_k\bgg_k$, as applied in \cref{thm:local convergence MR second-order sufficient}, can occur as a special case of \cref{alg:scaled gradient}. In particular, if $\sigma$ is chosen $\sigma < \mu $ then the $\SPC$ scaling (in particular, the MR scaling) will always be selected. Moreover, if $\xx_0$ is chosen sufficiently close to $\xx^\star$, \cref{eqn:gradient line search termination} will hold; implying that $\alpha=1$ will be acceptable to the line search. Indeed, \cref{eqn:gradient line search termination} will be satisfied for all future iterations by monotonicity of the gradient norm.


\subsection{Inexact Hessian Scaling and Proof of \cref{prop:inexact unit step size acceptance} and \cref{prop:inexact global convergence}} \label{apx:proofs for inexact Hessian}

When running \cref{alg:scaling selection} with $\HHt$ in place of $\HH$ the curvature tolerance tests as well as the scalings (in the $\SPC$ case) now depend on $\HHt$. Fortunately, many of the key properties from the deterministic case still hold for inexact Hessian. Indeed, $\SPC$ steps satisfy $\sigma \vnorm{\bgg}^2 < \dotprod{\bgg, \HHt \bgg}$, $\LPC$ steps satisfy $0 \leq \dotprod{\bgg, \HHt \bgg} \leq \sigma \vnorm{\bgg}^2$ and $\NC$ steps satisfy $\dotprod{\bgg, \HHt \bgg} < 0$. The second order descent condition \cref{eqn:second-order descent} from \cref{prop:second-order descent} continues to hold with respect to $\HHt$, that is, 
\begin{align*}
    \dotprod{\bgg, \ppt} + \dotprod{\ppt, \HHt \ppt} \leq 0, \tageq\label{eqn:inexact second order descent}
\end{align*}
Moreover, the upper bounds from \cref{prop:scaling upper bounds} continue to hold, as they arise due to the curvature test. On the other hand, if \cref{ass:inexact Hessian gradient smoothness condition} holds then we have a lower bound (compare with \cref{lemma:scaling lower bounds}) on the scaling magnitudes
\begin{align*}
    \frac{\sigma}{\Lt_1^2} \leq \sMR \leq \sGM \leq \sCG. \tageq\label{eqn:inexact step size lower bound}
\end{align*}
The search direction, $\ppt$, also remains collinear to to the gradient, so that, $\dotprod{\bgg, \ppt} = - \vnorm{\bgg} \vnorm{\ppt}$ and \cref{eqn:function value upper bound} can be applied along $\ppt$. The Armijo condition for the line search is given by 
\begin{align}
    \label{eqn:inexact line search}
    f(\xx + \alpha \ppt) \leq f(\xx) + \alpha \rho \dotprod{\bgg, \ppt},
\end{align}
for $\rho \in (0, 1/2)$. With these ideas in hand we can derive the results for \cref{sec:inexact-hessian}. Starting with the proof of \cref{prop:inexact unit step size acceptance}.

\paragraph{Proof of \cref{prop:inexact unit step size acceptance}}
\begin{proof}[Proof of \cref{prop:inexact unit step size acceptance}]
We begin by adding and subtracting $\dotprod{\ppt, \HHt\ppt}$ to the upper bound in \cref{eqn:function value upper bound} and applying \cref{ass:Hessian inexactness bound}
\begin{align*}
    f(\xx + \alpha \pp) - f(\xx) 
    &\leq \alpha \dotprod{\bgg, \ppt} + \frac{\alpha^2}{2}\dotprod{\ppt, (\HH - \HHt)\ppt} + \frac{\alpha^2}{2}\dotprod{\ppt, \HHt \ppt} + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3  \\
    &\leq \alpha \dotprod{\bgg, \ppt} + \frac{\alpha^2\Delta_H }{2} \vnorm{\ppt}^2 + \frac{\alpha^2}{2}\dotprod{\ppt, \HHt \ppt} + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3. \tageq\label{eqn:inexact function value upper bound}
\end{align*}
The upper bound in \cref{eqn:inexact function value upper bound} now involves the curvature of the inexact Hessian, $\dotprod{\ppt, \HHt \ppt}$, which we can control with the curvature tests and second order descent condition.
Starting with non-negative curvature case (encompassing $\SPC$ and $\LPC$ scalings) with $\dotprod{\bgg, \HHt \bgg} \geq 0$. Taking \cref{eqn:inexact function value upper bound}, subtracting $\rho \alpha \dotprod{\bgg, \ppt}$ and applying $\alpha \leq 1$ and \cref{eqn:inexact second order descent} we obtain
\begin{align*}
     f(\xx + \alpha \pp) - f(\xx) - \rho \alpha \dotprod{\bgg, \ppt} 
     &\leq \alpha\left(\frac{1}{2} - \rho \right) \dotprod{\bgg, \ppt} + \frac{\alpha}{2} \left(\dotprod{\bgg, \ppt} + \dotprod{\ppt, \HHt \ppt}\right) + \frac{\alpha^2}{2} \Delta_H \vnorm{\ppt}^2  + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3 \\
     &\leq -\alpha \left(\frac{1}{2} - \rho\right)\vnorm{\bgg} \vnorm{\ppt} + \frac{\alpha^2}{2} \Delta_H \vnorm{\ppt}^2  + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3 \\
     &=\left(-\left(\frac{1}{2} - \rho\right) + \frac{\alpha \st \Delta_H}{2}   + \frac{L_2 \alpha^2 \st }{6} \vnorm{\ppt} \right) \alpha \vnorm{\ppt} \vnorm{\bgg} \\
     &\leq \left(-\left(\frac{1}{2} - \rho\right) + \frac{\alpha \Delta_H}{2\sigma}   + \frac{L_2 \alpha^2  }{6 \sigma } \vnorm{\ppt} \right) \alpha \vnorm{\ppt} \vnorm{\bgg}. \tageq\label{eqn:inexact case positive function upper bound}
\end{align*}
In the final line we apply $\st \leq 1/\sigma$, which holds for both the $\LPC$ and $\SPC$ scalings. The line search criteria \cref{eqn:inexact line search} is clearly satisfied if the upper bound in \cref{eqn:inexact case positive function upper bound} is negative. That is,
\begin{align*}
    \vnorm{\ppt} \leq \frac{6 \sigma}{L_2 \alpha^2}\left(\left(\frac{1}{2} - \rho \right) - \frac{\alpha \Delta_H}{2\sigma} \right).
\end{align*}
This bound is nontrivial if 
\begin{align*}
    \Delta_H \leq \frac{2\sigma}{\alpha}\left( \frac{1}{2} - \rho\right).
\end{align*}
Now we consider the negative curvature case, where$\dotprod{\bgg, \HHt \bgg} <0$. Applying the negative curvature condition to \cref{eqn:inexact function value upper bound} and subtracting $\alpha \rho \dotprod{\bgg, \ppt}$
\begin{align*}
     f(\xx + \alpha \pp) - f(\xx) -\alpha \rho \dotprod{\bgg, \ppt} &\leq \alpha(1-\rho)\dotprod{\bgg, \ppt} + \frac{\alpha^2 \Delta_H }{2}\vnorm{\ppt}^2 + \frac{\alpha^2}{2}\dotprod{\ppt, \HHt \ppt} + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3 \\
     &\leq -\alpha(1-\rho)\vnorm{\bgg}\vnorm{\ppt} + \frac{\alpha^2 \Delta_H }{2}\vnorm{\ppt}^2 + \frac{L_2 \alpha^3}{6} \vnorm{\ppt}^3 \\
     &= \left(-(1-\rho)  + \frac{\alpha\sNCt}{2} \Delta_H  + \frac{L_2 \alpha^2\sNCt}{6} \vnorm{\ppt} \right) \alpha \vnorm{\bgg}\vnorm{\ppt} \\
     &\leq \left(-(1-\rho)  + \frac{\alpha\sNC_{\tmax}}{2} \Delta_H  + \frac{L_2 \alpha^2\sNC_{\tmax}}{6} \vnorm{\ppt} \right) \alpha \vnorm{\bgg}\vnorm{\ppt}, \tageq\label{eqn:inexact case negative function upper bound}
\end{align*}
where in the final line we we apply $\sNCt \leq \sNC_{\tmax}$. Again, if the upper bound in \cref{eqn:inexact case negative function upper bound} is negative \cref{eqn:inexact line search} is clearly satisfied. In particular, \cref{eqn:inexact case negative function upper bound} is negative if
\begin{align*}
    \vnorm{\ppt} \leq  \frac{6}{L_2 \sNC_{\tmax} \alpha^2}\left( 1 - \rho - \frac{\alpha \sNC_{\tmax}\Delta_H}{2} \right).
\end{align*}
This bound is nontrivial if 
\begin{align*}
    \Delta_H \leq \frac{2(1-\rho)}{\alpha \sNC_{\tmax}}.
\end{align*}
The result follows from combining the two tolerances on $\Delta_H$, setting $\alpha=1$ and using the fact that $\vnorm{\ppt} \leq \vnorm{\bgg}/\sigma$ in the non-negative curvature case and $\vnorm{\ppt} \leq \sNC_{\tmax} \vnorm{\bgg}$ in the negative curvature case.
\end{proof}

Next we prove the global convergence result in \cref{prop:inexact global convergence}.

\paragraph{Proof of \cref{prop:inexact global convergence}}
To establish a global convergence rate we need a technical lemma
\begin{lemma} \label{lemma:technical lower bound}
    For $A \geq 0$, $x \geq0 $ and $B > 0$ we have 
    \begin{align*}
        -A + \sqrt{A^2 + Bx } \geq \frac{B}{A + \sqrt{A^2 + B}} \min\{x, 1\}.
    \end{align*}
\end{lemma}
\begin{proof}
The $x=0$ case is trivial, so take $x >0$. Multiplying the numerator and denominator by a conjugate
\begin{align*}
    -A + \sqrt{A^2 + Bx } &= \frac{(-A + \sqrt{A^2 + Bx})(A + \sqrt{A^2 + Bx})}{A + \sqrt{A^2 + Bx}} \\
    &= \frac{-A^2 + A^2 + Bx}{A + \sqrt{A^2 + Bx}} \\
    &= \frac{Bx}{A + \sqrt{A^2 + Bx}}.
\end{align*}
Now consider two cases. If $x < 1$ then $A + \sqrt{A^2 + Bx} < A + \sqrt{A^2 + B}$ so that 
\begin{align*}
     -A + \sqrt{A^2 + Bx} = \frac{Bx}{A + \sqrt{A^2 + Bx}} \geq \frac{B}{A + \sqrt{A^2 + B}}x.
\end{align*}
On the other hand, if $x \geq 1$, we rearrange to obtain
\begin{align*}
    -A + \sqrt{A^2 + Bx} = \frac{Bx}{A + \sqrt{A^2 + Bx}} = \frac{B}{A/x + \sqrt{A^2/x^2 + B/x}}.
\end{align*}
The denominator of this expression is decreasing in $x$, indeed, we clearly have $A/x + \sqrt{A^2/x^2 + B/x} \leq A + \sqrt{A^2 + B}$ so that
\begin{align*}
    -A + \sqrt{A^2 + B} \geq \frac{B}{A + \sqrt{A^2 + B}}.
\end{align*}
\end{proof}

\begin{proof}[Proof of \cref{prop:inexact global convergence}] 

We begin by re-examining \cref{eqn:inexact case positive function upper bound,eqn:inexact case negative function upper bound} from the proof of \cref{prop:inexact unit step size acceptance}, in terms of $\alpha$, to demonstrate that there is a positive step size for which \cref{eqn:inexact line search} certainly holds. 
We assume that \cref{alg:scaled gradient} has not terminated, i.e., $\vnorm{\bgg} > \eg$.
Starting with the negative curvature case. Consider the upper bound in \cref{eqn:inexact case negative function upper bound}. The line search condition \cref{eqn:inexact line search} is certainly satisfied if
\begin{align*}
    C_2 \alpha^2 + C_1 \alpha + C_0 \leq 0,
\end{align*}
where 
\begin{align*}
    C_2 \defeq \frac{L_2 \sNC_{\tmax} \vnorm{\ppt}}{6}, \quad C_1 \defeq \frac{\sNC_{\tmax} \Delta_H}{2}, \quad C_0 \defeq -(1-\rho).
\end{align*}
This quadratic has two real roots, one positive and one negative. The positive root is given by 
\begin{align*}
    \alpha^\star_1 &= -\frac{C_1}{2C_2} + \sqrt{\frac{C_1^2}{4C_2^2} - \frac{C_0}{C_2}} \\
    &= -\frac{3\Delta_H }{2L_2 \vnorm{\ppt}} + \sqrt{\frac{9 \Delta_H^2}{4 L_2^2 \vnorm{\ppt}^2} + \frac{6(1-\rho)}{L_2 \sNC_{\tmax} \vnorm{\ppt}}},
\end{align*}
which implies that the largest step size for which \cref{eqn:inexact line search} is satisfied also must satisfy $\alpha \geq \alpha^\star_1$. Therefore, when \cref{eqn:inexact line search} is satisfied, we have
\begin{align*}
    f(\xx + \alpha \ppt) - f(\xx) 
    &\leq -\alpha\rho\vnorm{\ppt}\vnorm{\bgg} \\
    &\leq -\rho \left(-\frac{3\Delta_H }{2L_2} + \sqrt{\frac{9 \Delta_H^2}{4 L_2^2} + \frac{6(1-\rho)\vnorm{\ppt}} {L_2 \sNC_{\tmax}}} \right) \vnorm{\bgg} \\
    &< -\rho \left(- \frac{3\Delta_H}{2L_2}  + \sqrt{ \frac{9 \Delta_H^2}{4L_2^2} + \frac{6(1-\rho)\sNC_{\tmin}\eg} {L_2\sNC_{\tmax}} } \right) \eg \\
    &\leq -\rho \left( \frac{ 6(1-\rho)\sNC_{\tmin}/(L_2\sNC_{\tmax}) } {\frac{3\Delta_H}{2L_2}  + \sqrt{\frac{9 \Delta_H^2}{4L_2^2}  + \frac{6(1-\rho)\sNC_{\tmin}} {L_2\sNC_{\tmax}}} } \right) \eg^2 \\
    &= - \cNCt \eg^2.
\end{align*}
On the third line we apply $\vnorm{\ppt} \geq \sNC_{\tmin} \vnorm{\bgg}$ and $\vnorm{\bgg} > \eg$, whereas on the fourth line we apply \cref{lemma:technical lower bound} and $\eg < 1$. In the final line we define 
\begin{align*}
    \cNCt \defeq \rho\frac{12(1- \rho) \sNC_{\tmin}/\sNC_{\tmax}} {3\Delta_H + \sqrt{9 \Delta_H^2 + 24(1-\rho)L_2\sNC_{\tmin}/\sNC_{\tmax}}}.
\end{align*}

Next we come to the positive curvature case. Considering \cref{eqn:inexact case positive function upper bound} we see that the line search condition \cref{eqn:inexact line search} is satisfied if
\begin{align*}
    D_2 \alpha^2 + D_1 \alpha + D_0 \leq 0,
\end{align*}
where 
\begin{align*}
    D_2 \defeq \frac{L_2 \vnorm{\ppt}}{6\sigma}, \quad D_1 \defeq \frac{\Delta_H}{2\sigma}, \quad D_0 \defeq -\left(\frac{1}{2} - \rho\right).
\end{align*}
Again this quadratic has a positive and negative root. The positive root is given by 
\begin{align*}
    \alpha^\star_2 &\defeq - \frac{D_1}{2D_2} + \sqrt{\frac{D_1^2}{4D_2^2} - \frac{D_0}{D_2}} \\
    &= -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{\frac{9\Delta_H^2}{4L_2^2 \vnorm{\ppt}^2} + \frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\ppt}}}.
\end{align*}
Therefore, the largest step size for which \cref{eqn:inexact line search} holds must satisfy $\alpha \geq \min\{ 1, \alpha^\star_2 \}$. We now isolate the $\LPC$ and $\SPC$ cases. For the $\LPC$ case, by combining the line search criteria \cref{eqn:inexact line search} and $ \alpha \geq \min\{1, \alpha^\star_2\} $ we obtain
\begin{align*}
    f(\xx + \alpha \ppt) - f(\xx) &\leq - \rho \alpha \vnorm{\bgg}\vnorm{\ppt} \\
    &\leq - \rho \min\left\{ \alpha^\star_2\vnorm{\ppt} , \vnorm{\ppt} \right\}\vnorm{\bgg} \\
    &= - \rho \min\left\{ -\frac{3 \Delta_H }{2 L_2} + \sqrt{\frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma(1/2 - \rho) \vnorm{\ppt} }{L_2} }, \vnorm{\ppt} \right\}\vnorm{\bgg} \\
    &< - \rho \min\left\{  -\frac{3 \Delta_H }{2 L_2} + \sqrt{\frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma(1/2 - \rho)\sLPC_{\tmin} \eg } {L_2} }, \sLPC_{\tmin}\eg \right\}\eg \\
    &\leq - \rho \min\left\{ \frac{6\sigma(1/2 - \rho)\sLPC_{\tmin}/L_2}{ \frac{3 \Delta_H }{2 L_2} + \sqrt{\frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma(1/2 - \rho)\sLPC_{\tmin}}{L_2}}}, \sLPC_{\tmin} \right\}\eg^2 \\
    &= -\cLPCt \eg^2.
\end{align*}
On the fourth line we applied $\vnorm{\ppt} \geq \sLPC_{\tmin} \vnorm{\bgg}$ and $\vnorm{\bgg}> \eg$, while in the fifth line we applied \cref{lemma:technical lower bound} and $\eg < 1$. In the final line we define 
\begin{align*}
    \cLPCt \defeq \rho \min \left\{ \frac{12\sigma(1/2 - \rho)\sLPC_{\tmin}} {3\Delta_H + \sqrt{9\Delta_H^2 + 24\sigma(1/2 - \rho)L_2\sLPC_{\tmin}}},  \sLPC_{\tmin} \right\}.
\end{align*}

Finally, for the SPC case the line search condition \cref{eqn:inexact line search} and $\alpha \geq \min\{ 1, \alpha^\star_2 \}$ yields
\begin{align*}
    f(\xx + \alpha \ppt) - f(\xx) &\leq - \rho \alpha \vnorm{\bgg}\vnorm{\ppt} \\
    &\leq - \rho \min\left\{ \alpha^\star_2\vnorm{\ppt} , \vnorm{\ppt} \right\}\vnorm{\bgg} \\
    &= - \rho \min\left\{ - \frac{3 \Delta_H}{2L_2} + \sqrt{ \frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma(1/2 - \rho)\vnorm{\ppt}}{L_2}  }, \vnorm{\ppt} \right\}\vnorm{\bgg} \\
    &< - \rho \min\left\{ - \frac{3 \Delta_H}{2L_2} + \sqrt{ \frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma^2(1/2 - \rho) \eg}{L_2 \Lt_1^2}  }, \frac{\sigma}{\Lt_1^2} \eg \right\}\eg \\
    &\leq - \rho \min\left\{ \frac{ 6\sigma^2(1/2 - \rho)/ (L_2 \Lt_1^2)} {\frac{3 \Delta_H} {2L_2} + \sqrt{\frac{9 \Delta_H^2} {4 L_2^2} + \frac{6\sigma^2(1/2 - \rho)} {L_2 \Lt_1^2}}}, \frac{\sigma}{\Lt_1^2} \right\}\eg^2 \\
    &= -\cSPCt \eg^2.
\end{align*}
We apply $\vnorm{\ppt} \geq (\sigma/\Lt_1^2) \vnorm{\bgg}$, which follows from \cref{eqn:inexact step size lower bound}, and $\vnorm{\bgg} > \eg$ in the fourth line, while in the fifth line we apply \cref{lemma:technical lower bound}. In the final line we define
\begin{align*}
    \cSPCt \defeq \rho\min\left\{ \frac{ 12\sigma^2(1/2 - \rho)/\Lt_1^2} { 3 \Delta_H + \sqrt{ 9 \Delta_H^2 + 24\sigma^2(1/2 - \rho)L_2/\Lt_1^2}}, \frac{\sigma}{\Lt_1^2} \right\} .
\end{align*}
The proof now proceeds similarly to the deterministic case. Let 
\begin{align*}
    K \defeq \left\lceil\frac{f(\xx_0) - f^\star}{\min\left\{ \cNCt, \cLPCt, \cSPCt \right\}\eg^2} \right\rceil.
\end{align*}


Suppose that $\vnorm{\bgg_k} \leq \eg$ fails to hold for $k=0,\ldots,K-1$. We divide the iterations $\sK = \{0, \ldots,K-1\}$ into a disjoint union $\sK = \sK_{\text{NC}} \cup \sK_{\text{LPC}} \cup \sK_{\text{SPC}} $ where 
\begin{align*} 
    \sK_{\text{NC}} &= \left\{ k \in \sK \mid \dotprod{\bgg_k, \HHt_k \bgg_k} < 0 \right\} \\
    \sK_{\text{LPC}} &= \left\{ k \in \sK \mid 0 \leq \dotprod{\bgg_k, \HHt_k \bgg_k} \leq \sigma \vnorm{\bgg_k}^2 \right\} \\
    \sK_{\text{SPC}} &= \left\{ k \in \sK \mid \dotprod{\bgg_k, \HHt_k \bgg_k} > \sigma \vnorm{\bgg_k}^2 \right\}.
\end{align*}
Applying a telescoping sum and the lower bounds on function decrease in each case, we have
\begin{align*}
    f(\xx_0) - f(\xx_K) &= \sum_{i=0}^{K-1} f(\xx_k) - f(\xx_{k+1}) \\
    &= \sum_{k\in\sK_{\text{NC}}} f(\xx_k) - f(\xx_{k+1}) + \sum_{k\in\sK_{\text{LPC}}} f(\xx_k) - f(\xx_{k+1}) + \sum_{k\in\sK_{\text{SPC}}} f(\xx_k) - f(\xx_{k+1}) \\
    &> |\sK_{\text{NC}} | \cNCt \eg^2 + |\sK_{\text{LPC}}|\cLPCt \eg^2 + |\sK_{\text{SPC}}| \cSPCt \eg^2 \\
    &\geq (|\sK_{\text{NC}} | + |\sK_{\text{LPC}} | + |\sK_{\text{SPC}}|)\min\left\{ \cNCt, \cLPCt, \cSPCt  \right\}\eg^2 \\
    &= K\min\left\{ \cNCt, \cLPCt, \cSPCt \right\}\eg^2 \\
    &\geq f(\xx_0) - f^{\star},
\end{align*}
which implies $f(\xx_K) < f^\star$, a contradiction.
\end{proof}

\paragraph{Remark on \cref{prop:inexact global convergence}}

Notably, \cref{prop:inexact global convergence} does not require an explicit bound on the Hessian error, $\Delta_H$. However, as we now demonstrate, the per iteration dependence on $\eg$ can be improved (relative to the worst case) for $\SPC$ steps if we impose a bound on $\Delta_H$. In particular, we stipulate that, for $\theta \in [0,1)$, the Hessian error tolerance satisfies
\begin{align*}
    \Delta_H \leq 2\theta\sigma(1/2 - \rho). \tageq\label{eqn:inexact tolerance better per iteration}
\end{align*}
Consider then a ``large $\ppt$'' setting (similar to \cref{prop:unit step size acceptance}) defined by
\begin{align*}
    \vnorm{\ppt} \geq \frac{6\sigma(1/2 - \rho) - 3 \Delta_H}{L_2} = \frac{6\sigma(1/2 - \rho)}{L_2} - \frac{3\Delta_H}{L_2}.
\end{align*}
Note that the lower bound on $\ppt$ is nontrivial because of the bound in \cref{eqn:inexact tolerance better per iteration}. Rearranging and subbing to the step size acceptance tolerance for the $\SPC$ case, $\alpha^\star_2$, we have 
\begin{align*}
    \alpha_2^\star &= -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{\frac{9\Delta_H^2}{4L_2^2 \vnorm{\ppt}^2} + \frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\ppt}}} \\ 
    &\leq -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{\frac{9\Delta_H^2}{4L_2^2 \vnorm{\ppt}^2} + \frac{\vnorm{\ppt} + \frac{3\Delta_H}{L_2}}{\vnorm{\ppt}}} \\ 
    &= -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{1 + \frac{9\Delta_H^2}{4L_2^2 \vnorm{\ppt}^2} + \frac{3\Delta_H}{L_2 \vnorm{\ppt}}} \\ 
    &= -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{\left( 1 + \frac{3\Delta_H}{2L_2 \vnorm{\ppt}} \right)^2} \\ 
    &= 1.
\end{align*}
Therefore, for large $\ppt$ the largest step size that satisfies \cref{eqn:inexact line search}, must also satisfy $\alpha \geq \alpha_2^\star$. Additionally, applying \cref{eqn:inexact tolerance better per iteration} to the lower bound on $\vnorm{\ppt}$ we obtain
\begin{align*}
    \vnorm{\ppt} \geq \frac{6\sigma(1/2 - \rho) - 3 \Delta_H }{L_2} \geq \frac{6\sigma(1 - \theta)(1/2 - \rho)}{L_2}.
\end{align*}
By combining the line search condition \cref{eqn:inexact line search}, $\alpha \geq \alpha_2^\star$, \cref{eqn:inexact second order descent}, the $\SPC$ curvature test and the lower bound on $\vnorm{\ppt}$ we have
\begin{align*}
    f(\xx + \alpha \ppt) - f(\xx) &\leq  \alpha \rho \dotprod{\ppt, \bgg}  \\ 
    &\leq - \alpha^\star_2 \rho \dotprod{\ppt, \HHt \ppt} \\
    &\leq - \rho \left( -\frac{3 \Delta_H }{2 L_2 \vnorm{\ppt}} + \sqrt{\frac{9\Delta_H^2}{4L_2^2 \vnorm{\ppt}^2} + \frac{6\sigma(1/2 - \rho)}{L_2 \vnorm{\ppt}}} \right) \sigma \vnorm{\ppt}^2 \\ 
    &\leq - \sigma \rho \left( -\frac{3 \Delta_H }{2 L_2}  + \sqrt{\frac{9\Delta_H^2}{4L_2^2} + \frac{6\sigma(1/2 - \rho)\vnorm{\ppt}}{L_2}} \right) \vnorm{\ppt} \\ 
    &\leq - \sigma \rho \left( -\frac{3 \Delta_H }{2 L_2}  + \sqrt{\frac{9\Delta_H^2}{4L_2^2} + \frac{36\sigma^2(1/2 - \rho)^2(1-\theta)}{L_2^2}} \right)\left(\frac{6\sigma(1 - \theta)(1/2 - \rho)}{L_2} \right) \\
    &= - \sigma \rho \left( \frac{ 72\sigma^2(1/2 - \rho)^2(1-\theta)/L_2} { 3 \Delta_H + \sqrt{ 9\Delta_H^2 + 144\sigma^2(1/2 - \rho)^2(1-\theta)}} \right)\left(\frac{6\sigma(1 - \theta)(1/2 - \rho)}{L_2} \right) \\
    &= - \frac{\sigma \rho}{L_2^2} \left( \frac{ 432\sigma^3(1/2 - \rho)^3(1-\theta)^2} { 3 \Delta_H + \sqrt{ 9\Delta_H^2 + 144\sigma^2(1/2 - \rho)^2(1-\theta)}} \right). \tageq\label{eqn:inexact SPC larger p function decrease}
\end{align*}
This bound suggests that, analogously to the ``large $\pp$'' case for the exact Hessian, a better dependence on $\eg$ is obtained when $\ppt$ is large and $\Delta_H$ is controlled. In fact, when $\theta=0$ we have $\Delta_H=0$ and \cref{eqn:inexact SPC larger p function decrease} matches the ``large $\pp$'' exact case. In conclusion, despite the worst case headline rate, in some cases a better dependence on $\eg$ may be obtained for certain steps if $\Delta_H$ is controlled. 




\subsection{Sub-sampled Hessian Error Bound} \label{sec:subsampled hessian}

\begin{lemma}[Hessian Sub-sampling] \label{lemma:hessian subsampling}

Consider the finite sum objective 
\begin{align*}
    f(\xx) = \frac{1}{n} \sum_{i=1}^n f_i(\xx).
\end{align*}
Suppose that there exists $0 \leq \Lmax_1 < \infty$ such that for all $\xx \in \real^d$ we have $\max_{i=1,\ldots,n} \vnorm{\HH_i \bgg} \leq \Lmax_1 \vnorm{\bgg}$. Let $\delta \in (0,1)$ and $\HHt$ be as in \cref{eqn:Hessian subsampling}. Then for any $\Delta_H \geq 0$ and $\xx \in \real^d$ we have 
\begin{align*}
    \Pr\left(\vnorm{(\HHt - \HH)\bgg} \leq \Delta_H \vnorm{\bgg} \right) \geq 1-\delta,
\end{align*}
if 
\begin{align*}
    |\sI_H| \geq \frac{ (\Lmax_1)^2 \left(1 + \sqrt{8\log(1/\delta)}\right)^2} {\Delta_H^2 }.
\end{align*}
    
\end{lemma}

\begin{proof}
    The result proceeds similarly to \citet[Lemma 3]{roosta2019subsampledNewton}. First we write $\HH\bgg = \AA \BB$ where
    \begin{align*}
        \AA = [\HH_1\bgg, \ldots, \HH_n \bgg] \in \real^{d \times n}, \quad \BB = [1/n, \ldots, 1/n]^\transpose \in \real^n.
    \end{align*}
    We would like to relate this matrix product to the subsampled alternative. To do this, take the minibatch, $\sI_H$, and form $\tilde{\AA} \in \real^{d \times |\sI_H| }$, using the columns of $\AA$ corresponding to $\sI_H$, rescaled by $\sqrt{n/|\sI_H|}$. Similarly, form $\tilde{\BB} \in \real^{|\sI_H|}$, using the rows of $\BB$ corresponding to $\sI_H$, rescaled by $\sqrt{n/|\sI_H|}$. Clearly,
    \begin{align*}
        \tilde{\AA} \tilde{\BB}= \frac{n}{|\sI_H|} \sum_{i \in \sI_H} \frac{1}{n} \HH_i \bgg = \HHt \bgg.
    \end{align*}
    Applying \citet[Lemma 11]{drineas2006FastMonteCarlo} allows us to relate $\AA\BB$ to $\tilde{\AA}\tilde{\BB}$. In particular, with probability at least $1- \delta$, we have
    \begin{align*}
        \vnorm{\HH\bgg - \HHt\bgg} = \vnorm{\AA \BB - \tilde{\AA}\tilde{\BB}} &\leq \sqrt{\frac{n}{|\sI_H|} \sum_{i=1}^n \vnorm{\AA_i}^2 |\BB_i|^2} + \frac{n}{\sqrt{|\sI_H|}} \sqrt{8 \log(1/\delta)} \max_{i=1, \ldots, n}\vnorm{\AA_i}|\BB_i| \\
        &\leq \sqrt{\frac{1}{n|\sI_H|} \sum_{i=1}^n \vnorm{\HH_i \bgg}^2  } + \frac{n}{\sqrt{|\sI_H|}} \sqrt{8 \log(1/\delta)}\max_{i=1, \ldots, n} \vnorm{\HH_i\bgg}/n \\
        &\leq \sqrt{\frac{1}{|\sI_H|} (\Lmax_1)^2\vnorm{\bgg}^2 } + \sqrt{\frac{8 \log(1/\delta)}{|\sI_H|}} \Lmax_1 \vnorm{\bgg} \\
        &\leq \frac{1}{\sqrt{|\sI_H|}}\left( 1 + \sqrt{8 \log(1/\delta)} \right) \Lmax_1\vnorm{\bgg}.
    \end{align*}
    Finally, we see that 
    \begin{align*}
        |\sI_H| \geq \frac{ (\Lmax_1)^2 \left(1 + \sqrt{8\log(1/\delta)}\right)^2} {\Delta_H^2 } \implies \frac{1}{\sqrt{|\sI_H|}}\left( 1 + \sqrt{8 \log(1/\delta)} \right)\Lmax_1 \leq \Delta_H.
    \end{align*}
\end{proof}

As a result of \cref{lemma:hessian subsampling} it is clear that \cref{ass:Hessian inexactness bound} is satisfied with high probability as, by the Cauchy-Schwarz inequality, we have 
\begin{align*}
    \abs{\dotprod{\bgg, (\HH - \HHt)\bgg}} \leq \vnorm{\bgg} \vnorm{(\HH - \HHt)\bgg}.
\end{align*}

\section{Miscellaneous Scaling Properties }


\paragraph{MR Scaling Tighter Lower Bound} Recall from \cref{lemma:scaling lower bounds} that if $\vnorm{\HH\bgg} \leq L_1 \vnorm{\bgg}$ for some $L_1$ (see \cref{ass:Hessian gradient smoothness condition}) then then CG, MR and GM scalings satisfy a lower bound in terms of the gradient norm. Notably, the MR scaling lower bound is weaker by a factor of $\sigma/L_1$, in comparison with the CG and GM. In the following lemma we show that this lower bound can be tightened to match the CG and GM case if the Hessian spectrum is positive and bounded.

 

\begin{lemma}\label{lemma:tighter MR lower bound}
    If the Hessian satisfies $0 \preceq \HH \preceq M \eye$ and $\vnorm{\HH \bgg} >0$ then 
    \begin{align*}
        \sMR \geq 1/M.
    \end{align*}
\end{lemma}
\begin{proof}
    We utilise the fact that $\sMR$ is a Rayleigh quotient of $\HH^\dagger$. Suppose that $\HH$ has $\psi_+$ nonzero eigenvalues ($\vnorm{\HH \bgg} > 0$ implies $\psi_+ > 0$) given by $0 < \lambda_1 < \lambda_2 \leq \ldots \leq \lambda_{\psi_+} \leq M$. The corresponding nonzero eigenvalues of $\HH^\dagger$ are given by $1/\lambda_{\psi_+} \leq \ldots \leq 1/\lambda_1$. Since $\HH\bgg \in \range{(\HH)}$ and is therefore orthogonal to $\Null{(\HH)}$ we have
    \begin{align*}
        \sMR = \frac{\langle \bgg, \HH \bgg \rangle}{\vnorm{\HH \bgg}^2}  = \frac{\langle \HH \bgg, \HH^\dagger \HH \bgg \rangle}{\| \HH \bgg \|^2} \geq \frac{1}{\lambda_{\psi^+}} \frac{\vnorm{\HH \bgg}^2}{\vnorm{\HH \bgg}^2} \geq \frac{1}{M}.
    \end{align*}
\end{proof}



The requirement that $\vnorm{\HH \bgg} > 0$ is not stringent since the positive curvature test $\dotprod{\bgg, \HH \bgg} > 0$ implies $\HH\bgg \neq 0$.


\section{Additional Numerical Results} \label{apx:additional-numerical-results}

\subsection{Oracle Calls as Complexity Measure} \label{apx:numerical-results-oracle-calls}

Following the typical convection in the optimization literature, in all our experiments, we plot the objective value against the total number of oracle calls for function, gradient, and Hessian-vector product evaluations, which allows for a fair comparison between methods with a differing per-iteration computational costs.
We adopt this approach because the measurement of ``wall-clock'' time can be heavily dependent on specific implementation details and computational platform. 
In contrast, counting the number of equivalent function evaluations, as an implementation and system independent unit of complexity is more appropriate and fair. 
More specifically, upon evaluating the function, computing its gradient is equivalent to one additional function evaluation, and computing a Hessian-vector product requires two additional function evaluations compared to a gradient evaluation \cite{pearlmutterFastExactMultiplication1994}. 
For example, in neural networks, for given data at the input layer, evaluation of network's output, i.e., function evaluation, involves one forward propagation. The corresponding gradient is computed by performing one additional backward propagation. After computing the gradient, an additional forward followed by a backward propagation give the corresponding Hessian-vector product \cite{goodfellow2016deep,blondel2024elementsdifferentiableprogramming}. 


\subsection{Multi-class Logistic Regression} \label{apx:numerical-results-logistic}

Consider a set of data items $\sD = \{\aa_i, b_i \}_{i=1}^n \subset \real^d \times \{1, \ldots C\}$. Denote the weights of each class as $\xx_1, \ldots, \xx_{C}$ and define $\xx = [ \xx_1, \ldots, \xx_{C-1}]$. We are free to take $\xx_C = \zero$ as class $C$ is identifiable from the weights of the other classes. The objective, $f$, is given by 
\begin{align*}
    f(\xx) = \frac{1}{n} \sum_{i=1}^n  \sum_{c=1}^{C-1} -\one(b_i = c) \log{(\text{softmax}(\xx_c, \aa_i))} + \frac{\lambda}{2} \vnorm{\xx}^2, \tageq\label{eqn:multinomial regression objective}
\end{align*}
where $\one(\cdot)$ is the indicator function and 
\begin{align*}
    \text{softmax}(\xx_c, \aa_i) = \frac{\exp{(\langle \xx_c, \aa_i \rangle)}}{\sum_{c=1}^C \exp{(\langle \xx_c, \aa_i \rangle)}}.
\end{align*}
In our experiments a bias parameter is included in the weights for each class. We set the regularization parameter to $\lambda=10^{-3}$ and initialize by setting $\xx_0 = 0$. We run all methods until a maximum of $10^5$ oracle calls or until $\vnorm{\bgg_k} \leq 10^{-4}$.

Note that the spectrum of the Hessian of \cref{eqn:multinomial regression objective} can be bounded as 
\begin{align*}
    \lambda \eye \preceq \grad^2 f(\xx) \preceq \left(\frac{C-1}{4n} \vnorm{\AA}^2 + \lambda \right) \eye, \tageq\label{eqn:multinomial regression Hessian bound}
\end{align*}
which allows us to estimate the strong convexity parameter and Lipschitz constant of \cref{eqn:multinomial regression objective} as 
\begin{align*}
    \mu_{\text{approx}} \defeq \lambda, \quad L_{\text{approx}} \defeq \frac{C-1}{4n} \vnorm{\AA}^2 + \lambda.
\end{align*}

\paragraph{Parameter Settings}

For the scaled gradient methods, we compare between CG, MR, GM, MRCG, CGMR, GMCG, GMMR and CGMR scalings. We set $\sigma=0$ as \cref{eqn:multinomial regression objective} is $\mu$-strongly convex (i.e., curvature along the gradient is already lower bounded). For the line search, we apply a backtracking procedure (\cref{alg:back tracking line search}) with $\theta=0.5$ and $\rho=10^{-4}$, which is a common choice in the literature.

For line search gradient descent, we apply a backtracking procedure (\cref{alg:back tracking line search}) based on the Amijo condition \cref{eqn:armijo condition} with $\pp= -\bgg$, $\theta=0.5$, $\alpha_0=1$ and $\rho=10^{-4}$. For fixed step size gradient descent we use a step size of $\alpha=1/L_\text{approx}$. 
In terms of the momentum methods, we consider updates of the form 
\begin{align*}
    \begin{cases}
        \vv_{k+1} = \beta \vv_k -\alpha \bgg_{k} \\
        \xx_{k+1} = \begin{cases}
        \xx_k + \vv_{k+1} &\quad \text{if Heavy ball.} \\
        \xx_k -\alpha \bgg_k + \beta \vv_{k+1} &\quad \text{if Nesterov.}
    \end{cases}
    \end{cases} \tageq\label{eqn:momentum methods}
\end{align*}
where $\alpha$ is a `step size' parameter and $\beta$ is a momentum parameter. For $\mu$-strongly convex, $\Lg$-smooth problems the parameters for the Heavy ball momentum \citep{polyakIntroductionOptimization1987} can be set as
\begin{align*}
    \alpha = \frac{4}{(\sqrt{\Lg} + \sqrt{\mu})^2}, \quad \beta = \left(\frac{\sqrt{\Lg} - \sqrt{\mu}}{\sqrt{\Lg} + \sqrt{\mu}} \right)^2.
\end{align*}
While for Nesterov acceleration \citep{nesterovIntroductoryLecturesConvex2004} the step size and momentum parameter can be set as
\begin{align*}
    \alpha = \frac{1}{\Lg}, \quad \beta = \frac{1 - \sqrt{\Lg/\mu}}{1 + \sqrt{\Lg/\mu}}.
\end{align*}
In our experiments we use these settings with $\mu_{\text{approx}}$ and $L_{\text{approx}}$ taking the place of $\mu$ and $\Lg$, respectively. 
Finally, for Adam \citep{kingma2014adam} we use standard values of $\beta_1 = 0.9$, $\beta_2=0.999$ and $\epsilon =10^{-8}$ and then tune the step size parameter by selecting the largest step size in power of 10 increments with stable convergence along the optimization trajectory. This procedure resulted in a step size of $\alpha=10^{-3}$. 

\paragraph{Additional Results}

We now present some additional numerical results for the multi-class logistic regression problem. In \cref{fig:logistic_regression_cifar10_scalings} we evaluate the performance of each of the scaled gradient methods. We see that CGMR and MRCG (which differ by whether they start on the CG or MR scaling, respectively) perform quite similarly outside of the initial convergence phase and handily outperform the other methods. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{logistic_regression_CIFAR10_scaled.pdf}
    \caption{Comparison between scaling method performance for multiclass logistic regression on CIFAR10.}
    \label{fig:logistic_regression_cifar10_scalings}
\end{figure}

In \cref{fig:logistic_regression_cifar10_CG} and \cref{fig:logistic_regression_cifar10_MR} we see a breakdown of the performance of the ``CG'' and ``MR'' scalings, respectively. Firstly, in panel (c) for each method, we see that the unit step size is accepted by the line search at each iteration, similarly to MRCG.
For the MR scaling (\cref{fig:logistic_regression_cifar10_MR}) we plot the gradient norm in the (a) panel, from which we see that the MR method produces a monotonic decrease in the gradient norm at each iteration with a unit step size; indicating the results from \cref{thm:local convergence MR second-order sufficient} can hold over a wide portion of the optimization landscape. This is despite the line search targeting the function value, rather than the gradient norm. 
In panel (b) of \cref{fig:logistic_regression_cifar10_CG} and \cref{fig:logistic_regression_cifar10_MR} we see that the oscillating behavior is observed in the scaling values. 
However, comparing with \cref{fig:logistic_regression_cifar10}, it is clear that this oscillation is over a smaller range of values (particularly for MR scaling) than the alternating MRCG scaling. This indicates that the alternation between MR and CG scalings is key to obtaining large scaling values and, in light of \cref{thm:local convergence MR second-order sufficient}, corresponding rapid convergence.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{logistic_regression_CIFAR10_cg.pdf}
    \caption{Performance of the CG scaling on the multi-class logistic regression problem on CIFAR10. (a) The objective function. (b) The scaling selected by the CG method. (c) The step size selected by line search.}
    \label{fig:logistic_regression_cifar10_CG}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{logistic_regression_CIFAR10_mr_gradient.pdf}
    \caption{Performance of the MR scaling on the multi-class logistic regression problem on CIFAR10. (a) The gradient norm. (b) The scaling selected by the MR method. (c) The step size selected by line search.}
    \label{fig:logistic_regression_cifar10_MR}
\end{figure}

Finally, in \cref{fig:logistic_regression_cifar10_time} we plot the objective value against wall-clock time we see that results largely conform with oracle calls. 

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{logistic_regression_CIFAR10_time.pdf}
    \caption{Wall-clock time for multi-class logistic regression on the CIFAR10 dataset. $10^{-4}$ has been added to all times for plotting purposes.}
    \label{fig:logistic_regression_cifar10_time}
\end{figure}

\subsection{MLP} \label{apx:numerical-results-mlp}

Let $\hh(\xx; \cdot)$ denote a two layer MLP with 100 hidden units per layer with GeLU \citep{hendrycksGaussianErrorLinear2023} activations and $C$ output layers, with parameters $\xx$. The objective for our experiment is the function
\begin{align*}
    f(\xx) = \frac{1}{n} \sum_{i=1}\text{CrossEntropy}(\hh(\xx; \aa_i), b_i) + \frac{\lambda}{2} \vnorm{\xx}^2, \tageq\label{eqn:mlp_problem} 
\end{align*}
where $\text{CrossEntropy}(\cdot, \cdot)$ is the cross-entropy loss between the predictions of the MLP and the true labels and $\lambda$ is a $\ell_2$ regularization (or weight decay) parameter. In our experiments the regularization parameter was set to $\lambda=10^{-3}$ and the parameters were initialised with the PyTorch default, that is, the weights for each layer are drawn from $U(-\sqrt{k}, \sqrt{k})$ where $k = 1/(\#\text{num inputs to the layer})$. We run all methods until a maximum of $10^5$ oracle calls or until $\vnorm{\bgg_k} \leq 10^{-4}$.

\paragraph{Parameter Settings}

For the scaled methods we utilize the same set of scalings at the logistic regression case. We set $\sigma=10^{-6}$ and utilize fixed scalings given by $\sLPC = \sNC = 1$. For the line search, we set $\rho = 10^{-4}$ and we use $\theta=0.5$ as the scale factor for back and forward-tracking. For vanilla gradient descent with line search, we apply a the same parameters as for the logistic regression experiment. Since \cref{eqn:mlp_problem} is nonconvex and there is no closed expression for the gradient Lipschitz constant, we set the parameters for the remaining methods by a tuning procedure. Specifically, for fixed step size gradient descent we tune the step size parameter by reducing the step size (with power of 10 increments) until stable convergence is achieved (i.e. no divergence or large scale oscillation) along optimization trajectory. For momentum methods \cref{eqn:momentum methods} we fix $\beta=0.9$ and tune the step size parameter by a similar procedure. For Adam we use $\beta_1 = 0.9$, $\beta_2=0.999$ and $\epsilon =10^{-8}$ and, again, tune the step size parameter in a similar manner to the fixed step size case. The resulting learning rate parameters are summarized in \cref{tab:mlp_fashionMNIST_learning_rates}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Method & Learning Rate \\ \hline
        Fixed & $10^{-2}$  \\
        Adam & $10^{-5}$ \\
        HB & $10^{-3}$ \\
        NES & $10^{-3}$ \\
        \hline
    \end{tabular}
    \caption{Tuned learning rates for MLP model on FashionMNIST}
    \label{tab:mlp_fashionMNIST_learning_rates}
\end{table}


\paragraph{Additional Results} We report some additional results for the MLP model on FashionMNIST dataset. In \cref{fig:mlp_fashionMNIST_scalings} we compare the performance of each of the scalings on the problem. Again, we see that the alternating scalings ``CGMR'' and ``MRCG'' perform the best.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{mlp_FashionMNIST_scaled.pdf}
    \caption{Comparison between scaling methods for MLP on FashionMNIST dataset.}
    \label{fig:mlp_fashionMNIST_scalings}
\end{figure}

In \cref{fig:mlp_fashionMNIST_cg} and \cref{fig:mlp_fashionMNIST_mr} we consider the performance of the CG and MR methods, respectively. For the MR scaling (\cref{fig:mlp_fashionMNIST_mr}) we see that the unit step length is accepted at each iteration except for the iteration where $\NC$ is detected. At this iteration the forward tracking line search kicks in and a larger step size is selected. Similar, to the logistic regression case, in panel (a) of \cref{fig:mlp_fashionMNIST_mr} we see that the gradient norm is monotonic, except for iteration where $\NC$ is detected, reinforcing the results in \cref{thm:local convergence MR second-order sufficient}. On the other hand, for the CG scaling (\cref{fig:mlp_fashionMNIST_cg}) $\NC$ is detected at a single iteration, where the forward tracking line search selects a larger step size. Also a handful of $\SPC$ iterations require a small adjustment to the step size from the line search, however in the terminal phase of the algorithm the unit step size is acceptable, consistent with \cref{prop:unit step size acceptance}. No $\LPC$ directions are detected for either the MR or CG scaling. 


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{mlp_FashionMNIST_cg.pdf}
    \caption{Performance of the CG scaling for the MLP on the FashionMNIST dataset. (a) The objective function. (b) The scaling selected by the CG method. (c) The step size selected by line search. Crosses indicate iterations where negative curvature is detected.}
    \label{fig:mlp_fashionMNIST_cg}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{mlp_FashionMNIST_mr_gradient.pdf}
    \caption{Performance of the MR scaling for the MLP on the FashionMNIST dataset. (a) The gradient norm. (b) The scaling selected by the MR method. (c) The step size selected by line search. Crosses indicate iterations where negative curvature is detected.}
    \label{fig:mlp_fashionMNIST_mr}
\end{figure}

In \cref{fig:mlp_fashionMNIST_time} we report our results from \cref{fig:mlp_fashionMNIST} in terms of wall-clock time. We see that the results are largely unchanged.

\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{mlp_FashionMNIST_time.pdf}
    \caption{Wall-clock time results for MLP on the FashionMNIST dataset \citep{xiao2017fashionmnist}. $10^{-3}$ has been added to all times for plotting purposes.}
    \label{fig:mlp_fashionMNIST_time}
\end{figure}


\subsection{Resnet} \label{apx:numerical-results-resnet}

Let $\hh(\xx, \cdot)$ denote a depth 18 ResNet \citep{Kaiming2016ResNet} with parameters $\xx$. The objective for our experiment is the function
\begin{align*}
    f(\xx) = \frac{1}{n} \sum_{i=1}\text{CrossEntropy}(\hh(\xx; \aa_i), b_i) + \frac{\lambda}{2} \vnorm{\xx}^2, \tageq\label{eqn:resnet_problem} 
\end{align*}
where $\text{CrossEntropy}(\cdot, \cdot)$ is the cross-entropy loss between the predictions of the network and the true labels and $\lambda$ is an $\ell_2$ regularization parameter. We specifically consider off-the-shelf ResNet18 implementation from PyTorch\footnote{Available \href{https://pytorch.org/vision/main/models/resnet.html}{here}.} with two modifications. Firstly, we replace all $\text{ReLU}$ activations with $\text{GeLU}$ \citep{hendrycksGaussianErrorLinear2023} to ensure twice differentiability. The second modification is replacing the $\text{BatchNorm}$ \citep{Ioffe2015BatchNorm} layers with $\text{LayerNorm}$ \citep{ba2016layernormalization}. This modification has previously been considered in \citep{wu2018groupnormalization}. We utilize the ``160px'' version of the Imagenette dataset \citep{Howard2019Imagenette}, which is available from PyTorch \citep{pytorch}. The Imagenette is a 10 class, subset of the full Imagenet dataset \citep{deng2009imagenet}. We process the data by normalizing per channel using Imagenet \citep{deng2009imagenet} statistics followed by resizing the images to $32 \times 32$.

In our experiments we set $\lambda = 10^{-5}$ and use the default initialization for the PyTorch ResNet model. All experiments are run until a maximum of $10^4$ oracle calls or $\vnorm{\bgg_k} \leq 10^{-4}$.

\paragraph{Parameter Settings}

Given the results of the previous experiments, we chose to focus on the MRCG, MR and CG as our scaling methods in this experiment. Additionally, as noted in the main body we disabled the line search and simply used a fixed step size of $\alpha=1$. We set $\sigma=10^{-6}$ and $\sNC = \sLPC = 1$. Although, we never encounter $\NC$ or $\LPC$ directions throughout iterations, it could be beneficial to retain the line search for $\NC$ and $\LPC$ directions as the scaling selections for these cases are arbitrary.

For vanilla gradient descent with line search, we apply a backtracking search procedure (\cref{alg:back tracking line search}) based on the Amijo condition \cref{eqn:armijo condition} with $\pp = -\bgg$, $\theta=0.5$, $\alpha_0=1$ and $\rho=0$. This choice of $\rho$ was made for numerical stability purposes as it simply forces monotonicity of the function value, rather than ``sufficient decrease''. We set the learning rates for all other methods by tuning over a grid of candidate values (unlike the MLP and logistic regression experiments, larger learning weren't necessarily better). We give the search grid as well as the selected learning rate in \cref{tab:resnet_Imagenette_learning_rates}. All other hyperparameters we left the same as the MLP setup; see \cref{apx:numerical-results-mlp}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Method & Grid & Learning Rate \\ \hline
        Fixed & $\{10^0, 10^{-1}, 10^{-2}, 10^{-3} \}$ & $10^{-1}$ \\
        Adam & $\{ 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5} \}$ & $10^{-3}$ \\
        HB &  $\{ 10^{-1}, 10^{-2}, 10^{-3}\}$ & $10^{-2}$ \\
        NES &  $\{ 10^{-1}, 10^{-2}, 10^{-3}\}$ &  $10^{-2}$ \\
        \hline
    \end{tabular}
    \caption{Learning rate tuning for ResNet18 model on Imagenette.}
    \label{tab:resnet_Imagenette_learning_rates}
\end{table}


\paragraph{Additional Results}

We now collect the additional results from our ResNet18 experiments. In \cref{fig:resnet_Imagenette_scaled} we see that, similar to the previous two experiments, alternating MRCG scaling performs significantly better than either MR or CG on their own.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{resnet_Imagenette_scaled.pdf}
    \caption{Comparison of scaling method performance on for the ResNet18 model on the Imagenette dataset}
    \label{fig:resnet_Imagenette_scaled}
\end{figure}

In \cref{fig:resnet_Imagenette_cg} and \cref{fig:resnet_Imagenette_mr_gnorm} we consider the CG and MR scalings in particular. Inspecting panel (b) of both figures and comparing with \cref{fig:resnet_Imagenette}, we see that the alternating MRCG scaling is capable of producing significantly larger scaling values, which could explain the faster convergence of the alternating scaling. In panel (a) of \cref{fig:resnet_Imagenette_mr_gnorm} we plot the gradient norm of the objective, we see that, similar to the MLP and logistic regression example, the MR scaling produces monotonic decrease in the gradient norm with the unit step size. This result is consistent with \cref{thm:local convergence MR second-order sufficient}. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{resnet_Imagenette_cg_f.pdf}
        \caption{Performance of the CG scaling for the ResNet18 model on Imagenette dataset. (a) The objective function. (b) The scaling selected by the CG method.}
    \label{fig:resnet_Imagenette_cg}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{resnet_Imagenette_mr_gnorm.pdf}
    \caption{Performance of the MR scaling for the ResNet18 model on Imagenette dataset. (a) The objective function. (b) The scaling selected by the MR method.}
    \label{fig:resnet_Imagenette_mr_gnorm}
\end{figure}

In \cref{fig:resnet_Imagenette_time} we plot the results from \cref{fig:resnet_Imagenette} against wall-clock time. We see that the results roughly conform with the oracle call results.  

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\linewidth]{resnet_Imagenette_time.pdf}
    \caption{Wall-clock time results for the ResNet18 model on Imagenette dataset. $10^{-2}$ has been added to all times for plotting purposes.}
    \label{fig:resnet_Imagenette_time}
\end{figure}

\end{document}