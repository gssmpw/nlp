\section{Related Works}
\label{sec:related_work}
\paragraph{Quadratic Problems.} 
The scalings \cref{eqn:CG scaling defn,eqn:MR scaling defn} are well-studied for strongly convex quadratic functions; see \cite{gonzagaSteepestDescentAlgorithm2016,macdonaldFamilyRelaxedGradient2024} and references therein. These correspond to exact line search methods along the gradient direction for $f$ and $\vnorm{\bgg}^2$, known as steepest descent and minimal gradient methods, respectively\footnote{Beyond quadratics, there is no correspondence to exact line search, so we avoid labeling \cref{eqn:CG scaling defn,eqn:MR scaling defn} as such.}. The GM scaling \cref{eqn:GM scaling defn} has been studied for such problems by \cite{daiNewGradientMethod2006}, showing that for $\alpha > 0$, $\sGM$ estimates an inverse gradient Lipschitz constant.

\vspace{-1mm}
\paragraph{Barzilai-Borwein Methods.} 
The Barzilai-Borwein (BB) method \cite{barzilaiTwopointStepSize1988, fletcherBarzilaiBorweinMethod2005, daiFamilySpectralGradient2019} is also based on scalar minimization of a second-order approximation. For quadratics, the long and short BB step sizes align with our CG and MR scalings, shifted by one iteration, due to their equivalence to steepest descent and minimal gradient methods. While BB achieves R-linear convergence for strongly convex quadratics, its convergence for non-quadratic objectives cannot be guaranteed.

\vspace{-1mm}
\paragraph{Inexact Newton Methods.} 
The scalings $\sCG$ and $\sMR$ are deeply connected to Krylov subspace-based inexact Newton methods. The Newton-CG method \cite{nocedalNumericalOptimization2006} uses CG to solve the Newton system. The $t\th$ iteration of CG amounts to solving  
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \frac{1}{2}\dotprod{\dd, \HH \dd} + \dotprod{\bgg, \dd},
\end{align*}
where $\sK_t(\HH, \bgg) = \Span \{ \bgg, \HH\bgg, \dots, \HH^{t-1} \bgg \}$ is the Krylov subspace of degree $t$. For $t = 1$, $\sK_1(\HH, \bgg) = \{ s \bgg : s \in \mathbb{R} \}$, and the solution is $-\sCG \bgg$. Similarly, the Newton-MR method \cite{roostaNewtonMRInexactNewton2022, liuNewtonMRAlgorithmComplexity2023,smeeInexactNewtontypeMethods2024,limComplexityGuaranteesNonconvex2024}, based on MINRES \cite{paige1975solution}, computes
the approximate Newton direction as a solution to
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \vnorm{\HH \dd + \bgg},
\end{align*}
yielding $-\sMR \bgg$ for $t = 1$. These connections to inexact second-order methods motivated the study of CG and MR scaled gradient methods. The treatment of limited positive curvature in \cite{smeeInexactNewtontypeMethods2024, limComplexityGuaranteesNonconvex2024} inspired our curvature validation procedure.

\vspace{-1mm}
\paragraph{Smoothness and Adaptivity.} 
The global convergence analysis in this paper does not rely on the conventional Lipschitz gradient smoothness assumption, which has come under increasing scrutiny in recent works. Many machine learning objectives, such as feedforward or recurrent neural networks, fail to satisfy this condition \cite{patelGlobalConvergenceStability2022}. Recent studies suggest that this assumption may not hold even along the optimization trajectory \cite{cohenGradientDescentNeural2022}. Instead, \citet[Remark 5]{ahnUnderstandingUnstableConvergence2022} argue that {\em Lipschitz Hessian smoothness} is more appropriate; our work adopts a weaker form of this assumption (\cref{ass:Hessian gradient smoothness condition}). Additionally, \citet{patelGradientDescentAbsence2023} study gradient descent with diminishing step sizes under local Lipschitz smoothness, a weaker assumption than the standard one.

Our method avoids globally conservative step sizes, instead using adaptive local curvature information to guide step size selection. Recent studies suggest that conservative step sizes can hinder convergence. \citet{grimmerProvablyFasterGradient2024} show that larger step sizes can improve rates, while \citet{altschulerAccelerationStepsizeHedging2023} demonstrate that combining long and short steps enhances convergence in convex settings. Dynamic step size selection incorporates local information, e.g., the Polyak step size \citep{polyakIntroductionOptimization1987} and its variants \citep{loizouStochasticPolyakStepsize2021,orvieto2022dynamics,oikonomou2024stochastic}. \citet{malitskyAdaptiveGradientDescent2020,malitskyAdaptiveProximalGradient2024} adaptively set step sizes using local Lipschitz estimates and control sequences, ensuring global convergence under convexity and locally Lipschitz gradients. \citet{mishkinDirectionalSmoothnessGradient2024} explores adaptive step sizes via directional smoothness, closely related to our use of gradient curvature. \citet{berahasNonUniformSmoothnessGradient2023} investigates local first-order smoothness oracles, while methods like D-adaptation \cite{defazioLearningRateFreeLearningDAdaptation2023,mishchenkoProdigyExpeditiouslyAdaptive2024} and Do(W)G \cite{ivgiDoGSGDsBest2023,khaledDoWGUnleashedEfficient2024} achieve parameter-free convergence by adaptively estimating problem constants.

\vspace{-1mm}
\paragraph{Quadratic Model Scaling } 

Utilizing a quadratic model to rescale search directions is not new. The KFAC method \citep{martensOptimizingNeuralNetworks2020} rescales directions using a quadratic model based on the Fisher information matrix. Similarly,  \citet{rouletSteppingEdgeCurvature2024} adaptively set the step size using a quadratic model of the objective along a search direction, akin to our CG scaling. \citet{rouletSteppingEdgeCurvature2024} show that curvature-aware scaling can align optimization dynamics with the edge of stability \citep{cohenGradientDescentNeural2022}, though they do not exploit negative curvature or provide theoretical guarantees. \citet{casteraSecondOrderStepSizeTuning2022} use a quadratic model (similar to our CG scaling) to verify curvature, enabling BB step sizes in nonconvex and stochastic settings, and also find that large step sizes are viable with negative curvature. \citet{degournay2022adaptivescalinglearningrate} rescale step directions using directional second-order information, applying a moving average of directional curvatures, but focus on practical implementation without convergence theory.