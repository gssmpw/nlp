\section{Related Works}
\label{sec:related_work}
\paragraph{Quadratic Problems.} 
The scalings \cref{eqn:CG scaling defn,eqn:MR scaling defn} are well-studied for strongly convex quadratic functions; see ____ and references therein. These correspond to exact line search methods along the gradient direction for $f$ and $\vnorm{\bgg}^2$, known as steepest descent and minimal gradient methods, respectively\footnote{Beyond quadratics, there is no correspondence to exact line search, so we avoid labeling \cref{eqn:CG scaling defn,eqn:MR scaling defn} as such.}. The GM scaling \cref{eqn:GM scaling defn} has been studied for such problems by ____, showing that for $\alpha > 0$, $\sGM$ estimates an inverse gradient Lipschitz constant.

\vspace{-1mm}
\paragraph{Barzilai-Borwein Methods.} 
The Barzilai-Borwein (BB) method ____ is also based on scalar minimization of a second-order approximation. For quadratics, the long and short BB step sizes align with our CG and MR scalings, shifted by one iteration, due to their equivalence to steepest descent and minimal gradient methods. While BB achieves R-linear convergence for strongly convex quadratics, its convergence for non-quadratic objectives cannot be guaranteed.

\vspace{-1mm}
\paragraph{Inexact Newton Methods.} 
The scalings $\sCG$ and $\sMR$ are deeply connected to Krylov subspace-based inexact Newton methods. The Newton-CG method ____ uses CG to solve the Newton system. The $t\th$ iteration of CG amounts to solving  
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \frac{1}{2}\dotprod{\dd, \HH \dd} + \dotprod{\bgg, \dd},
\end{align*}
where $\sK_t(\HH, \bgg) = \Span \{ \bgg, \HH\bgg, \dots, \HH^{t-1} \bgg \}$ is the Krylov subspace of degree $t$. For $t = 1$, $\sK_1(\HH, \bgg) = \{ s \bgg : s \in \mathbb{R} \}$, and the solution is $-\sCG \bgg$. Similarly, the Newton-MR method ____, based on MINRES ____, computes
the approximate Newton direction as a solution to
\begin{align*}
    \min_{\dd \in \sK_t(\HH, \bgg)} \vnorm{\HH \dd + \bgg},
\end{align*}
yielding $-\sMR \bgg$ for $t = 1$. These connections to inexact second-order methods motivated the study of CG and MR scaled gradient methods. The treatment of limited positive curvature in ____ inspired our curvature validation procedure.

\vspace{-1mm}
\paragraph{Smoothness and Adaptivity.} 
The global convergence analysis in this paper does not rely on the conventional Lipschitz gradient smoothness assumption, which has come under increasing scrutiny in recent works. Many machine learning objectives, such as feedforward or recurrent neural networks, fail to satisfy this condition ____. Recent studies suggest that this assumption may not hold even along the optimization trajectory ____. Instead, ____ argue that {\em Lipschitz Hessian smoothness} is more appropriate; our work adopts a weaker form of this assumption (\cref{ass:Hessian gradient smoothness condition}). Additionally, ____ study gradient descent with diminishing step sizes under local Lipschitz smoothness, a weaker assumption than the standard one.

Our method avoids globally conservative step sizes, instead using adaptive local curvature information to guide step size selection. Recent studies suggest that conservative step sizes can hinder convergence. ____ show that larger step sizes can improve rates, while ____ demonstrate that combining long and short steps enhances convergence in convex settings. Dynamic step size selection incorporates local information, e.g., the Polyak step size ____ and its variants ____. ____ adaptively set step sizes using local Lipschitz estimates and control sequences, ensuring global convergence under convexity and locally Lipschitz gradients. ____ explores adaptive step sizes via directional smoothness, closely related to our use of gradient curvature. ____ investigates local first-order smoothness oracles, while methods like D-adaptation ____ and Do(W)G ____ achieve parameter-free convergence by adaptively estimating problem constants.

\vspace{-1mm}
\paragraph{Quadratic Model Scaling } 

Utilizing a quadratic model to rescale search directions is not new. The KFAC method ____ rescales directions using a quadratic model based on the Fisher information matrix. Similarly,  ____ adaptively set the step size using a quadratic model of the objective along a search direction, akin to our CG scaling. ____ show that curvature-aware scaling can align optimization dynamics with the edge of stability ____, though they do not exploit negative curvature or provide theoretical guarantees. ____ use a quadratic model (similar to our CG scaling) to verify curvature, enabling BB step sizes in nonconvex and stochastic settings, and also find that large step sizes are viable with negative curvature. ____ rescale step directions using directional second-order information, applying a moving average of directional curvatures, but focus on practical implementation without convergence theory.