\section{Introduction}
\label{sec:introduction}


% The rise of Large Language Models (LLMs) with advanced capabilities has transformed the technology landscape. However, the most performant LLMs on general tasks are often closed-source or black-box models~\citep{achiam2023gpt, bai2022training}. These models provide access to generated text in response to input prompts but do not reveal the underlying model weights or the data used for training. This black-box nature limits the level of customization and transparency available to end users. 

% Despite these limitations, closed-source LLMs continue to power a broad range of generative AI applications. Developers leverage these black-box models by optimizing input prompts to achieve reasonable performance for downstream, domain-specific use cases. Examples range from travel itinerary creators to tax experts. These applications demonstrate the value of generative AI in diverse domains, even when the underlying models remain inaccessible. However, the black-box nature of these LLMs poses significant challenges when it comes to domain-specific or enterprise-specific tasks. For instance, consider a content writer for a brand who seeks to generate product descriptions that align with the brand's tone, style, and identity. Black-box LLMs trained on broad datasets may not produce text that fully matches such specific requirements. With limited access to only the generated tokens, the writer's primary option is prompt engineering, using either zero-shot~\citep{kojima2022large} or few-shot~\citep{song2023llm} examples, to guide the model toward brand-aligned content. If the LLM weights were accessible, techniques like Parameter-Efficient Fine-Tuning (PEFT), such as LoRA~\citep{hu2021lora}, QLoRA~\citep{dettmers2024qlora}, prefix tuning~\citep{li-liang-2021-prefix}, or adapters~\citep{hu-etal-2023-llm}, could be employed for better alignment. Unfortunately, due to intellectual property concerns and the significant resources invested in developing these models, most high-performing LLMs remain closed-source. While some organizations offer API access for fine-tuning, many developers are reluctant to upload proprietary data, fearing privacy and security risks.

The rise of Large Language Models (LLMs) has revolutionized generative AI, yet the most capable models are often closed-source  or black-box~\citep{achiam2023gpt, bai2022training}. These models generate text based on input prompts but keep their internal weights and training data undisclosed, limiting transparency and customization. Despite these constraints, closed-source LLMs are widely adopted across applications ranging from travel itinerary generation to tax advice, with developers largely relying on prompt optimization to achieve domain-specific outputs. 

However, this reliance on prompt engineering is insufficient for specialized tasks, e.g., those requiring brand-specific tone or style. Consider a content writer aiming to generate product descriptions that reflect a brand’s unique identity. Black-box LLMs, trained on broad datasets, often fail to meet such nuanced requirements. With access limited to generated tokens, developers resort to zero-shot~\citep{kojima2022large} or few-shot~\citep{song2023llm} prompting techniques. However, if model weights were accessible, advanced techniques like Parameter-Efficient Fine-Tuning (PEFT) using LoRA~\citep{hu2021lora}, QLoRA~\citep{dettmers2024qlora}, prefix tuning~\citep{li-liang-2021-prefix}, or adapters~\citep{hu-etal-2023-llm} could be employed for fine-tuning. Yet, due to intellectual property concerns and the high costs of development, most commercial LLMs remain closed-source, and even with API-based fine-tuning options, concerns over data privacy discourage developers from sharing proprietary data.

% In this paper, we propose a middle ground between the creators of general-purpose LLMs and developers seeking application-specific alignment. We argue and demonstrate that providing access to logits or token probabilities, in addition to generated text, could enable more effective customization of LLM outputs for downstream tasks. Specifically, building upon the analogy between LLMs and supervised classification models, we observe that next-token prediction in LLMs can be framed as a classification problem, where the number of classes corresponds to the size of the vocabulary. Decoder-only models are trained in a similar fashion, using the context of preceding tokens to predict the next token. Aligning black-box LLMs to application- or enterprise-specific data can thus be interpreted as addressing a label noise setup in supervised classification. 

% In this analogy, the training data for black-box LLMs, derived from broad world knowledge, acts as a source of proxy labels, while the application-specific data represents the true labels relevant to the downstream task. Classic distribution shift setups naturally arise in this context. For example, in a label shift scenario~\citep{lipton2018detecting}, certain tokens may occur more frequently in application-specific data than in the LLM's training corpus. Similarly, in class-dependent or independent label noise setups~\citep{patrini2017making}, synonymous phrases or stylistic variations prevalent in application-specific data may differ from those seen during LLM training. 

% We take inspiration from loss correction approaches developed for supervised classification in label noise setups, particularly the framework introduced by~\citep{patrini2017making}. Their approach estimates a transition matrix to model class-dependent label noise and uses it to correct the loss during model training. However, our setting introduces critical differences: we lack access to the LLM's original training data and cannot retrain the model. Instead, we estimate an autoregressive transition matrix directly from application-specific data to align the LLM’s output. 

% This autoregressive extension is novel, as it accounts for dependencies on previously generated tokens when reweighting probabilities for the next token. Our work formulates the problem by adapting the insights from label noise correction to autoregressive setups for language modeling. By doing so, we provide a practical solution to align black-box LLMs to task-specific data using only logits and without requiring access to the original training data or model weights.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{icml2025/images/plugin_fig.png}
    % \vspace{-8mm}
    \caption{Inference phase of the \textit{Plugin} model. The token probabilities are a product of the probabilities from the black-box model and a reweighting model that denotes label transitioning.}
    % \vspace{-8mm}
    \label{fig:plugin_fig}
\end{figure}


In this paper, we propose a middle ground between general-purpose LLM creators and developers seeking application-specific alignment. We argue that providing access to token logits, in addition to generated text, would enable more effective customization for downstream tasks. Viewing next-token prediction as a classification problem, we draw an analogy between LLMs and supervised classification models. Since decoder-only LLMs are trained to predict the next token given preceding tokens, aligning black-box LLMs to domain-specific data can be reframed as a label noise correction problem in supervised classification. In this analogy, the LLM’s broad training data serves as proxy labels, while application-specific data represents true labels. This can be interpreted as a distribution shift scenario. For example, in ``label shift''~\citep{lipton2018detecting}, certain tokens may appear more frequently in application-specific data than in the LLM’s original corpus. In ``class-dependent or independent label noise''~\citep{patrini2017making}, synonymous expressions or stylistic variations in application data may diverge from those seen during model training.

Inspired by label noise correction methods~\citep{patrini2017making}, which estimate a transition matrix to adjust for class-dependent noise, we adapt this approach to black-box LLM alignment. Unlike prior methods that modify loss functions during training, we lack access to the LLM’s original training data and cannot retrain the model. Instead, we estimate an autoregressive transition matrix from application-specific data to reweight token probabilities at inference.

This autoregressive extension is novel, as it accounts for dependencies on previously generated tokens when adjusting logits for the next token. By adapting label noise correction techniques to autoregressive language modeling, we present a practical method to align black-box LLMs using only logits—without requiring access to model weights or original training data.

Our contributions are summarized as follows:
% \vspace{-2mm}
% \begin{enumerate}
%     \item We formulate the problem of adapting black-box LLMs to application-specific content generation as a loss correction approach, requiring access only to logits at each token generation step. This formulation bridges concepts from label noise correction in supervised classification with autoregressive setups in language modeling (Sections~\ref{sec:preliminaries} and~\ref{sec:robustness}).
    
%     \item We propose an autoregressive probability reweighting framework that enables post-shifting of probability estimates from the black-box LLM at each token generation step. The model learned through this framework, termed the ``plugin decoding", reweights the probabilities autoregressively during inference to align with application-specific data (Section~\ref{sec:method}).

%     \item Under mild assumptions, we provide theoretical guarantees for the proposed framework. We show that, given a sufficient number of application-specific samples, the plugin model consistently aligns probability estimates with the target data distribution. To the best of our knowledge, this is the first work to establish such consistency in an autoregressive setting (Section~\ref{sec:theory}).
%     %
%     % In our setting the sequence of auto-regressive loss functions is not i.i.d. So our proof technique differs from that of \citet{frostig2015competing, chaudhuri2015convergence} which assumes the loss functions are i.i.d. The work of \citet{mukherjee2022chernoff} studies a different setting for active regression and does not consider the cross-entropy loss or transition dynamics of the label noise. 

%     \item We conduct extensive experiments to evaluate the effectiveness of our approach. Results across four language generation datasets, three black-box LLMs, and multiple ablation studies demonstrate that the plugin model achieves superior performance compared to state-of-the-art baselines in adapting black-box LLMs for application-specific content generation (Section~\ref{sec:experiments}). 

%     \item Finally, we advocate for publishing logits alongside token outputs in closed-source LLMs. Our findings underscore the utility of logits in enabling developers to adapt black-box LLMs to task-specific requirements without compromising the model's intellectual property.
% \end{enumerate}

\begin{enumerate}
    \item We formulate the problem of adapting black-box LLMs for application-specific content generation as a loss correction approach, requiring only token logits at each generation step. This bridges label noise correction in supervised classification with autoregressive language modeling (Sections~\ref{sec:preliminaries} and~\ref{sec:robustness}).
    % \vspace{-1mm}
    \item We propose an autoregressive probability reweighting framework, enabling token-level probability adjustment during inference. The resulting \textit{Plugin} model dynamically reweights logits to align generation with task-specific data (Section~\ref{sec:method}).
    % \vspace{-1mm}
    \item We provide theoretical guarantees, showing that under mild assumptions, the \textit{Plugin} model consistently aligns probability estimates with the target distribution given sufficient application-specific samples. To our knowledge, this is the first work to establish such consistency in an autoregressive label noise setting (Section~\ref{sec:theory}).
    % \vspace{-1mm}
    \item We conduct extensive experiments across four language generation datasets and three black-box LLMs. Our results, supported by multiple ablations, demonstrate that the \textit{Plugin} model outperforms baselines in adapting black-box LLMs for domain-specific content generation (Section~\ref{sec:experiments}). Based on our results, we advocate for publishing token logits alongside outputs in closed-source LLMs. 
\end{enumerate}

