\section*{Impact Statement}
The ability to adapt closed-source LLMs without modifying their weights has significant implications for both research and industry. 
Our proposed method, which leverages token logits for task-specific alignment, offers a practical solution for developers constrained by black-box APIs. 
This approach enhances customization, allowing models to generate more domain-relevant and controlled content while preserving the privacy and security of proprietary data. 
Furthermore, by advocating for broader access to token logits, this work fosters greater transparency and flexibility in commercial LLMs. 
The findings also highlight the importance of mitigating biases in black-box models, contributing to more equitable and context-aware language generation across diverse applications.

While \textit{Plugin} effectively adapts black-box LLMs, it has some limitations, too. Since it only reweights token probabilities without modifying internal representations or embeddings, it may struggle with tasks requiring deep structural adaptations, such as executing complex reasoning. Further research on this aspect is needed. Additionally, although \textit{Plugin} avoids full fine-tuning, training a separate reweighting model introduces computational overhead compared to prompt tuning or in-context learning, with efficiency depending on the complexity of the reweighting model and the availability of task-specific data.


\section*{Acknowledgement}
HW acknowledges support by Fonds de recherche du Québec – Nature et technologies (FRQNT) and Borealis AI.
SK acknowledges support by NSF 2046795 and 2205329, IES R305C240046, the MacArthur Foundation, Stanford HAI, OpenAI, and Google.