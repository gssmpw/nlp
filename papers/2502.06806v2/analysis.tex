\section{Theoretical Analysis}
\label{sec:theory}

% \todom{Mention the end results, technical novelty and contribution upfront}

% \todom{Provide an intuition about technical statement including assumptions.}


% In this section, we discuss the convergence guarantees for the \emph{plugin} model. We show that after training for $t$ tokens the plugin model achieves a good estimation of the autoregressive noise transition matrix. We formulate the problem such that the transition matrix depends on the unknown parameter $\theta_*$ and after training on a sequence of auto regressive losses the plugin model is able to estimate $\theta_*$ with high probability. 

% Denote the history of selected tokens from time $1,2,\ldots,t-1$ as $\F^{t-1}$. Assume that there exists an unknown parameter $\btheta_* \in\bTheta$ that governs the transition dynamics of flipping the label between any pair of tokens. Let $T_t(\btheta_*|\F^{t-1})$ denote the transition matrix that governs the transition between any pair of tokens at time $t$, and depends on all the previous tokens seen till time $t-1$. 

We establish the convergence properties of the \textit{Plugin} model, demonstrating that after training for $t$ tokens, it accurately estimates the autoregressive noise transition matrix. We model the transition matrix as a function of an unknown parameter $\btheta_*$ and show that, after optimizing the autoregressive loss over a sequence of tokens, the \textit{Plugin} model estimates $\btheta_*$ with high probability.

Let $\F^{t-1}$ denote the history of selected tokens up to time $t-1$. Let an unknown parameter $\btheta_* \in \bTheta \subseteq \mathbb{R}^d$ governs the transition dynamics of label flipping between token pairs. The transition matrix at time $t$, denoted as $T_t(\btheta_* | \F^{t-1})$, depends on $\btheta_*$ and all previously observed tokens. Before proving our main result, we first make a few assumptions. 


%
% We follow the setup of \citet{wu2022precise}. At time $t$, learner makes a prediction of the class $Y_t\in [0,|V|]$ and the true label of the class is determined by the function $f_{I_t}(\btheta_*; x_i, x_j, \F^{t-1})\in \{0,\ldots,|V|\}$. 


\begin{assumption}
\label{assm:transition-matrix}
    %
    %
    Let $T_t(\btheta_* ; x_i, x_j, \F^{t-1})$ denote the $(i,j)$-th component of the transition matrix, and let  $f_{I_t}(\btheta_* ; x_i, x_j,  \F^{t-1})$ be the transition function that determines the transition from $x_i$ to $x_j$, where $I_t$ is the $x_i$ token selected at time $t$. Let $x_i, x_j \!\in\! \mathbb{R}^d$.
    %
    We assume that $\nabla f_{I_t}(\btheta_* ; x_i, x_j, \F^{t-1})\!< \!\lambda_0$ and $\nabla^2 f_{I_t}(\btheta_* ; x_i, x_j,  \F^{t-1}) \!<\! \lambda_1$ for some constant $\lambda_0 \!>\! 0$, $\lambda_1 > 0$ and for all steps $t$.
    %
\end{assumption}
\Cref{assm:transition-matrix} ensures that the transition matrix depends on the generalized history-dependent function $f_{I_t}(\cdot)$ which has a bounded gradient and hessian. Similar assumptions for other deep models have been made in~\cite{singh2023hessian, zhang2024transformers}.
%
% Moreover, \Cref{assm:transition-matrix} ensures that estimating the transition matrix requires estimating the correct $\btheta_*$ with high probability.
%
% Let the clipped cross entropy token loss at time $t$ following \eqref{eq:ce_batch_loss} be 
% \begin{align}
%     \ell^{cl-\log}_{t} = -\sum_{j=1}^{|V|} \log(\max(\epsilon, \bp_i))\odot \be_j. \label{eq:clipped-log-loss}
% \end{align}
% for some $\epsilon > 0$ where the $j$-th token appears at the $i$-th place in sequence $t$.


\begin{assumption}
\label{assm:bound-ce}
    We assume the cross-entropy loss~\eqref{eq:ce_batch_loss} is clipped by $\epsilon > 0$ and upper bounded as  
 $\ell^{clipped}_{t} \!\!\leq\!\! C|V|^2(Y_t - f_{I_t}(\btheta_*; x_i, x_j, \F^{t-1}))^2$ for any time $t$, where $Y_t$ is the predicted token class, $f_{I_t}$ determines the true class and satisfies \Cref{assm:transition-matrix}, and $C \!>\! 0$ is a constant.
    %
    % so that the $|V|$ way clipped log loss $\ell^{cl-\log}_{s}$ at time $s$ is upper bounded by $(1+\frac{1}{\epsilon})|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$, where $Y_s$ is the predicted class of the token and $f_{I_s}$ determines the true class and satisfies \Cref{assm:transition-matrix}. WLOG we denote $C=(1+\frac{1}{\epsilon}) > 0$ as a constant and so $\ell^{cl-\log}_{s} \leq C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ for any time $s$.  
\end{assumption}


\cref{assm:bound-ce} ensures that the clipped log loss is upper bounded by a smoother squared loss. For the remaining of this section we refer to this squared loss at time $t$ as $\ell_t(\btheta)$.
%
Let the \textit{Plugin} model minimize the loss $\ell_{1}(\btheta), \ell_{2}(\btheta), \cdots, \ell_{t}(\btheta)$ over $t$ iterations. Let $\twtheta_t =\argmin_{\btheta \in \bTheta} \sum_{s = 1}^t \ell_{s}(\btheta)$. 
% Finally define the estimated transition loss as $\wT_t(x_i, x_j; \twtheta_t)$ that governs the transition of $x_i$ to $x_j$. Under \eqref{assm:transition-matrix} we have that
% \begin{align*}
%     \wT_t(x_i, x_j; \twtheta_t) = vec(x_i x_j^\top)^\top vec(\twtheta_t \twtheta_t^\top)b(x_i)b(x_j)
% \end{align*}
%
At every iteration $t$, the \textit{Plugin} algorithm looks into the history $\F^{t-1}$ and samples a token $\xbm_t\sim \pbm_{\hat \theta_t}=\bbm_t \odot \rbm_{\hat \theta_t}$. 
% following the probability distribution $\bp_{\twtheta_t}\in\mathbb{R}^{|V|}$. 
% %%% Commented %%%%%
% At time t, the plugin works like this:
% \begin{align*}
% x_t \sim \mathbf{p}_{\twtheta_{t-1}} \implies x_t \sim (T_t(\twtheta_t))^{-1}\mathbf{p}_{{\twtheta}_{t-1}}.
% \end{align*}

Let $\wLcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t \ell_{s}(\btheta)$ and its expectation $\Lcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t\E_{x_s\sim \mathbf{p}_{\twtheta_{s-1}}}[\ell_s(\btheta)|\F^{s-1}]$.
%
We impose regularity and smoothness assumptions on the loss function $\ell_t(\btheta)$ as stated in \Cref{assm:thm} (\Cref{app:theory}). We are now ready to prove the main theoretical result of the paper.
% The following theorem provides an upper bound on the difference between the estimated and true average loss functions, showing that this gap decreases as the number of training tokens increases.


% \todom{Mention that it is bounded by squared loss. Refer to appendix.}

% \todom{Complete and check the details in the appendix.}
% \todom{Make line 278 to 298 concise.}
%
%
% Now in the following proof we will upper bound $\ell^{cl-\log}_{s} \leq C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $Y_s$ is the predicted class at time $s$ and $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ is the function that determines the true class. Also, it follows from \Cref{assm:transition-matrix} that the gradient $\nabla f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ and $\nabla^2 f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$   is bounded. We will denote $\ell_{s} = C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $C>0$ is the constant defined in \cref{assm:bound-ce}. 





% Note that the clipped log loss is always bounded by squared loss because squared loss grows faster in all regions, especially when clipping is applied to the log loss.
% %
% Using \cref{assm:bound-ce} we can upper bound the clipped log loss with a squared loss. Now in the following proof we will upper bound $\ell^{cl-\log}_{s} \leq C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $Y_s$ is the predicted class at time $s$ and $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ is the function that determines the true class. Also, it follows from \Cref{assm:transition-matrix} that the gradient $\nabla f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ and $\nabla^2 f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$   is bounded. We will denote $\ell_{s} = C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $C>0$ is the constant defined in \cref{assm:bound-ce}. 


\begin{customtheorem}{1}
\label{thm:main}
Suppose $\ell_{1}(\btheta), \cdots, \ell_{t}(\btheta): \mathbb{R}^{|V|} \!\!\rightarrow \!\!\mathbb{R}$ are loss functions from a distribution that satisfies Assumptions \ref{assm:transition-matrix}, \ref{assm:bound-ce}, and \ref{assm:thm}. Define 
% \begin{align*}
    $\Lcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t\E_{x_s\sim \mathbf{p}_{\twtheta_{s-1}}}[\ell_s(\btheta)|\F^{s-1}]$
% \end{align*}
where, $\twtheta_t =\argmin_{\btheta \in \bTheta} \sum_{s = 1}^t \ell_{s}(\btheta)$. If $t$ is large enough such that $ \frac{\gamma\log(dt)}{t}\leq c^{\prime} \min \left\{\frac{1}{C_{1}C_{2} |V|^4 }, \frac{\max\limits_{\btheta \in \bTheta}\left(\!\Lcal_{t}(\btheta)\!-\!\Lcal_{t}\left(\btheta_{*}\!\right)\right)}{C_{2}}\right\}$
then for a constant $\gamma \geq 2$,  universal constants $C_1,C_2,c'$,  we have that 
\begin{align*}
\left(1-\rho_{t}\right) \frac{\sigma_t^2}{t}- \frac{C_1^2}{t^{\gamma / 2}} 
&\leq \E\left[\Lcal_t(\twtheta_t)-\Lcal_t\left(\btheta_{*}\right)\right] \\
%%%%%%%
&\leq \left(1+\rho_{t}\right) \frac{\sigma_t^2}{t}\!+\!\frac{\max\limits_{\btheta \in \bTheta}\left(\!\Lcal_{t}(\btheta)\!-\!\Lcal_{t}\left(\btheta_{*}\!\right)\right)}{t^{\gamma}},
\end{align*}
where 
$\sigma^{2}_t \coloneqq \E_{}\left[\frac{1}{2}\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right)^{-1}}^{2}\right]$, 
and $\rho_t \coloneqq \left(C_1C_2 + 2\eta^2\lambda_1^2\right)\sqrt{\frac{\gamma\log(dt)}{t}}$.
\end{customtheorem}

% \Cref{thm:main} shows that after the plugin model is trained on $t$ tokens then in expectation over the sequence of tokens chosen the estimated average loss function $\Lcal_t(\twtheta_t)$ is closer to true average loss function $\Lcal_t(\btheta_{*})$. 
\Cref{thm:main} bounds the difference between the estimated and true average loss functions, showing that this gap diminishes as the number of training tokens increases. Since $\twtheta_t = \argmin_{\btheta \in \bTheta} \sum_{s = 1}^t \ell_{s}(\btheta)$, the \textit{Plugin} model progressively refines its estimation of the unknown parameter $\btheta_*$. As the transition matrix $T_t(\btheta_*; x_i, x_j, \F^{t-1})$ is derived from $f_{I_t}(\btheta_*; x_i, x_j, \F^{t-1})$, which depends on $\btheta_*$, training on sufficiently many tokens ensures an accurate estimation of each component of $T_t(\btheta_* | \F^{t-1})$.


% The proof relies on first converting our setting into a sequential hypothesis testing setting to estimate the average loss function $\Lcal_t(\twtheta_t)$ using the sequence of losses $\ell_1(\btheta),  \cdots, \ell_t(\btheta)$~\cite{naghshvar2013active, lattimore2020bandit}. 
% Note that the auto-regressive loss at time $t$ is dependent on all the prior losses and is not an i.i.d. draw. Therefore the proof technique differs from that of \citet{frostig2015competing, chaudhuri2015convergence} which assume the loss functions are i.i.d. \cite{mukherjee2022chernoff} study a different setting for active regression and does not consider cross entropy loss or transition noise matrix like from~\cite{patrini2017making}. We give a brief overview of the proof technique in \Cref{app:remark} in \Cref{app:theory}.
Our proof reformulates the problem as a sequential hypothesis testing setting to estimate the average loss function $\Lcal_t(\twtheta_t)$ using the sequence of losses $\ell_1(\btheta), \dots, \ell_t(\btheta)$~\citep{naghshvar2013active, lattimore2020bandit}. Unlike prior work~\citep{frostig2015competing, chaudhuri2015convergence}, which assumes i.i.d. losses, the loss at time $t$ in our setting  depends on all previous losses. Additionally, \citet{mukherjee2022chernoff} study a different active regression setting without considering cross-entropy loss or transition noise matrices as in~\citep{patrini2017making}. We provide a brief overview of the proof technique in \Cref{app:remark} (\Cref{app:theory}), highlighting key novelties.



% The proof relies on several steps. In the first step we relate $\nabla^2 \wLcal_t(\btheta)$ to $\nabla^2\Lcal_t(\btheta_*)$ for any $\btheta$ in a ball $\mathcal{B}$ around $\btheta_*$. The ball $\mathcal{B}$ is assumed in \Cref{assm:thm} to be a neighborhood where $\nabla^2 \ell_s(\btheta)$ satisfies a Lipschitz property. \Cref{assm:thm} in \Cref{app:theory} are standard and have also been made by \citet{frostig2015competing, chaudhuri2015convergence, mukherjee2020generalized}. 
% Using \Cref{assm:transition-matrix} and \Cref{assm:thm}, we can show that for a large enough sequences of tokens $t$ stated in \Cref{thm:main} we have the following: (1) $\nabla^2 \Lcal_t(\btheta_*)$ lies between in the positive semidefinite order by scaled multiples of $\nabla^2 \wLcal_t(\btheta)$ for any $\btheta \in \mathcal{B}$, and (2) the empirical error minimizing $\twtheta_t$ is in the ball $\mathcal{B}$ with probability $1 - 1/t^\gamma$, which is the good event $\mathcal{E}$. Then we use a Taylor series expansion around $\twtheta_t$ and using the fact that $\nabla \wLcal_t(\wtheta(t)) = 0$ along with the relation between $\nabla^2 \wLcal_t(\btheta)$ and $\nabla^2 \Lcal_t(\btheta_*)$, we can obtain an upper bound to $\lVert \wtheta(t) - \btheta_*\rVert_{\nabla^2 \Lcal_t(\btheta_*)}$ in terms of $\lVert \nabla \wLcal_t(\btheta_*) \rVert_{(\nabla^2 \Lcal_t(\btheta_*))^{-1}}$ that can be shown to be decreasing with $t$. 
% Further, $\lVert \wtheta(t) - \btheta_*\rVert_{\nabla^2 \Lcal_t(\btheta_*)}$ can also be used to obtain an upper bound to $\Lcal_t(\wtheta(t)) - \Lcal_t(\btheta_*)$ using a Taylor series expansion. 
% %
% Finally we can bound $\E[\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})] =\E[(\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})) I(\mathcal{E})]+\E[(\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})) I(\mathcal{E}^\complement)]$ where $I(\cdot)$ is the indicator. Since $\Pb(\mathcal{E}^\complement) \leq 1/t^\gamma$, the second term can be bounded as $\max_{\btheta \in \bTheta}\left(\Lcal_{t}(\btheta)-\Lcal_{t}\left(\btheta^{*}\right)\right)/t^{\gamma}$, while the first term simplifies to $(1 + \rho_t)\sigma_t^2/t$. 
% The full proof is in  \Cref{app:theory}.


