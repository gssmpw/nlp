
\section{Preliminaries}
\label{sec:preliminaries}

We begin by establishing the notation. The index set is denoted as $[c] = \{1, \dots, c\}$ for any positive integer $c$. Vectors are represented in boldface, for example, $\bm{v}$, while matrices are denoted using uppercase letters, such as $V$. The coordinates of a vector are indicated with subscripts, for instance, $v_j$. 
% , whereas the rows and columns of a matrix are denoted as $V_{j\cdot}$ and $V_{\cdot j}$, respectively. 
The all-ones vector is denoted by $\mathbf{1}$, with its size being clear from the context. The $c$-dimensional simplex is represented as $\Delta^{c-1} \subset [0,1]^c$. Finally, a sequence $(x_{t}, x_{t-1}, \dots, x_1)$ of size $t$ is denoted by $x_{t:1}$. 

We assume access to language data for the target task, while the black-box LLM, trained on broad world knowledge, is treated as having learned from a noisy version of this data. Our objective is to adapt the black-box model to align with the task-specific distribution. To formalize this, we extend the label-noise framework from supervised classification~\citep{patrini2017making} to the decoder-only language modeling setting.


Decoder-only models are trained using a next-token prediction objective. At each step, this setup resembles a supervised classification problem with $|V|$ classes, where $V$ is the vocabulary of tokens.   Formally, the label space at step $t$ is $\Xcal_t = \{\ebm^i : i \in [|V|]\}$, where $\ebm^i$ denotes the $i$-th standard canonical vector in $\mathbb R^{|V|}$, i.e., $\ebm^i \in \{0,1\}^{|V|}, \mathbf{1}^T\ebm^i=1$. The task at each step $t$ is to predict the next token 
$\xbm_t$ (denoted as one-hot vector) 
given a sequence of tokens $\xbm_{t-1:1}$. 

% One observes (clean) examples $(\xbm_t, \xbm_{t-1:1})$ from an unknown distribution $p^*(\xbm_t, \xbm_{t-1:1}) = p^*(\xbm_t|\xbm_{t-1:1})p^*(\xbm_{t-1:1})$ over $V \times V^{[t-1]}$. We denote expectations over $p^*(\xbm_t, \xbm_{t-1:1})$ by $E^*_{\xbm_t,\xbm_{t-1:1}}$. The loss often used is the standard cross-entropy loss over the vocabulary tokens. We assume we have access to logits, and thus the softmax output, from the black-box (closed source) LLMs. It is the softmax output from the black-box model, which can be interpreted as a vector approximating the class-conditional probabilities $p^*(\xbm_t|\xbm_{t-1:1})$; we denote it by $b(\xbm_t|\xbm_{t-1:1})\in \Delta^{|V|-1}$.

One observes examples $(\xbm_t, \xbm_{t-1:1})$ drawn from an unknown distribution $p^*(\xbm_t, \xbm_{t-1:1}) = p^*(\xbm_t|\xbm_{t-1:1})p^*(\xbm_{t-1:1})$ over $V \times V^{[t-1]}$, with expectations denoted by $E^*_{\xbm_t,\xbm_{t-1:1}}$. The standard cross-entropy loss is typically used for training over the vocabulary tokens. Assuming access to token logits, and thus the softmax outputs, from the black-box LLM, we interpret the softmax output as a vector approximating the class-conditional probabilities $p^*(\xbm_t|\xbm_{t-1:1})$, denoted as $b(\xbm_t|\xbm_{t-1:1}) \in \Delta^{|V|-1}$.



% Next, we measure the discrepancy between label $\xbm_t = \ebm^i$ at step $t$ and model output by a loss function $\ell: |V| \times \Delta^{|V|-1} \rightarrow \mathbb R$, for example by means of cross-entropy which is often used in next token prediction tasks:
% \begin{align*}
%     \label{eq:loss}
%     \ell(\ebm^i, b(\xbm_t | {\xbm_{t-1:1}})) &= -(\ebm^i)^T \log b(\xbm_t|\xbm_{t-1:1}) \\
%     &= -\log b(\xbm_t = \ebm^i | \xbm_{t-1:1}). 
%     \numberthis 
% \end{align*}

To quantify the discrepancy between the target label $\xbm_t = \ebm^i$ at step $t$ and the modelâ€™s predicted output, we define a loss function $\ell: |V| \times \Delta^{|V|-1} \rightarrow \mathbb{R}$. A common choice in next-token prediction tasks is the cross-entropy loss:
\begin{align*}
    \label{eq:loss}
    \ell(\ebm^i, b(\xbm_t | {\xbm_{t-1:1}})) &= -(\ebm^i)^T \log b(\xbm_t|\xbm_{t-1:1}) \\
    &= -\log b(\xbm_t = \ebm^i | \xbm_{t-1:1}). 
    \numberthis 
\end{align*}


With some abuse of notation, the loss in vector form $\bm\ell: \Delta^{|V|-1}\rightarrow\mathbb{R}^{|V|}$, computed on every possible label is $\bm\ell(b(\xbm_t|\xbm_{t-1:1})
% \begin{align*}
=\Big(\ell(\ebm^1, b(\xbm_t|\xbm_{t-1:1})), \dots, \ell(\ebm^{|V|}, b(\xbm_t|\xbm_{t-1:1})) \Big)^T.$
% \end{align*}