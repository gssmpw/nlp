\section{Related Work}
\label{sec:relatedwork}
% \vspace{-1mm}
\paragraph{Parameter-Efficient Fine-Tuning (PEFT).}  
PEFT methods adapt LLMs to downstream tasks while minimizing computational overhead. LoRA~\citep{hu2021lora} and QLoRA~\citep{dettmers2024qlora} introduce low-rank updates and quantization for efficient fine-tuning, while prefix tuning~\citep{li-liang-2021-prefix}, adapters~\citep{hu2023llm}, and soft prompting~\citep{lester2021power} modify task-specific representations through trainable layers or embeddings. However, these methods require access to model weights, gradients, or architecture details, making them unsuitable for closed-source LLMs and inapplicable as baselines in our setup. In contrast, our approach operates solely on token logits, enabling adaptation without modifying the underlying model.
% \vspace{-2mm}

 

% \paragraph{Steering and Aligning LLMs:} Steering and aligning LLMs typically involve reinforcement learning or instruction tuning to modify model behavior. Reinforcement learning from human feedback (RLHF)~\citep{christiano2017deep, ouyang2022training} fine-tunes models using human preference data to enhance alignment, but it requires extensive annotated feedback and substantial computational resources. Direct Preference Optimization (DPO)~\citep{rafailov2024direct} simplifies RLHF by optimizing preference scores without reward modeling. Constitutional AI~\citep{bai2022constitutional} leverages self-generated principles to improve alignment while instruction tuning~\citep{weifinetuned2021, sanhmultitask2022} adapts models through task-specific demonstrations. However, these methods require access to model weights and training data, which is not feasible for closed-source models. In contrast, our approach aligns black-box LLMs using token-level probability reweighting, requiring only logits rather than model modifications or extensive labeled data, making it a lightweight and practical alternative for enterprise-specific adaptation.

\paragraph{Steering and Aligning LLMs.}  
LLM alignment methods primarily use reinforcement learning or instruction tuning. RLHF and DPO~\citep{christiano2017deep, ouyang2022training, rafailov2024direct} optimize model behavior via human preferences, with DPO eliminating reward modeling. Constitutional AI~\citep{bai2022constitutional} aligns models using self-generated principles, while instruction tuning~\citep{weifinetuned2021, sanhmultitask2022} adapts them via task-specific demonstrations. Unlike our approach, these methods require model weights and training data, limiting their applicability as baselines in our setup. 
% \vspace{-2mm}
% to closed-source models. In contrast, our method reweights token probabilities at inference using logits alone, enabling efficient adaptation without modifying the model.



% \paragraph{Calibration of LLMs:} Calibration in large language models (LLMs) aims to align model confidence with predictive accuracy, ensuring that generated outputs reflect appropriate uncertainty. Recent studies have introduced various approaches for improving LLM calibration. APRICOT~\cite{ulmer2024calibrating} calibrates LLMs using only their generations by training an auxiliary model to predict confidence levels. Huang et al.~\citeyear{huang2024calibrating} propose a framework that treats both correctness and confidence as distributions, addressing calibration in long-form generation tasks. Thermometer~\citep{shenthermometer} learns an auxiliary model for universal LLM calibration across multiple tasks. Calibration-tuning~\citep{kapoor2024calibration} focuses on refining LLM calibration during fine-tuning, particularly in scenarios where models must recognize uncertainty. Additionally, a systematic study by~\citep{zhu2023calibration} investigates calibration across different alignment stages, examining how LLMs handle factuality and response confidence. However, these approaches primarily focus on adjusting confidence scores without altering token predictions. In contrast, our method directly modifies token probabilities at inference via reweighting, adapting black-box LLMs without requiring access to model weights, fine-tuning, or auxiliary calibration models.

% \paragraph{Calibration of LLMs.}  
% LLM calibration methods aim to align model confidence with predictive accuracy. APRICOT~\citep{ulmer2024calibrating} and Thermometer~\citep{shenthermometer} use auxiliary models for confidence calibration, while Huang et al.~\citep{huang2024calibrating} and Calibration-Tuning~\citep{kapoor2024calibration} address calibration during fine-tuning and long-form generation. Zhu et al.~\citep{zhu2023calibration} examine calibration across model alignment stages. These approaches adjust confidence scores but do not alter token predictions. In contrast, our method reweights token probabilities at inference, enabling adaptation of black-box LLMs without modifying the model or requiring fine-tuning.
% \vspace{-2mm}

\paragraph{Calibration of LLMs.}  
LLM calibration methods aim to align model confidence with predictive accuracy and adjust confidence scores but do not alter token predictions~\citep{ulmer2024calibrating, shenthermometer, huang2024calibrating, kapoor2024calibration, zhu2023calibration}. In contrast, our method reweights token probabilities at inference, enabling adaptation of black-box LLMs without modifying the model or requiring fine-tuning.
% \vspace{-3mm}

\paragraph{Black-box LLMs.}  
Prior work explores various approaches for adapting black-box LLMs without fine-tuning, though they differ fundamentally from our method. \cite{gao2024aligning} infer user preferences through interactive edits but do not adapt models based on past language data. Diffusion-LM~\citep{li2022diffusion} formulates text generation as a non-autoregressive denoising process, whereas our approach reweights token probabilities autoregressively without requiring black-box model weights. Discriminator-based methods~\citep{dathathriplug, mireshghallah2022mix, yang2021fudge, krause2021gedi} control generation based on predefined attributes, contrasting with our method, which enables free-form text adaptation. DExperts~\citep{liu2021dexperts} combines expert and anti-expert probabilities; we incorporate a similar probability combining strategy in a modified baseline without a de-expert component. In-context learning~\citep{long2023adapt, dong2024survey} offers a common adaptation technique for black-box models and serves as a baseline in our setup.
% \vspace{-3mm}