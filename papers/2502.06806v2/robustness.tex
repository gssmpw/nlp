
\section{Loss Robustness}
\label{sec:robustness}

% We extend the concept of label noise to the autoregressive language modeling domain, focusing on asymmetric or class-conditional noise. Specifically, at each step $t$, the label $\xbm_t$ in the training data of the black-box model is flipped to  $\tilde \xbm_t \in V$ with probability $p^*(\tilde \xbm_t|\xbm_t)$, while the feature vectors or preceding tokens $(\xbm_{t-1:1})$ remain unchanged. Consequently, the black-box model observes samples from a noisy distribution given by  $p^*(\tilde \xbm_t, \xbm_{t-1:1}) = \sum_{\xbm_t}p^*(\tilde \xbm_t | \xbm_t)p^*(\xbm_t|\xbm_{t-1:1})p^*(\xbm_{t-1:1})$.

% Denote by $T_t  \in [0, 1]^{|V|\times |V|}$, the noise transition matrix at step $t$ specifying the probability of one label being flipped to another, so that $\forall i, j \;\; T_{t_{ij}}=p^*(\tilde \xbm_t = \ebm^j | \xbm_t = \ebm^i)$. The matrix is row-stochastic and not necessarily symmetric across the classes. 

% To address asymmetric label noise, we modify the loss $\bm{\ell}$ to ensure robustness. Initially, assuming the noise transition matrix $T_t$ is known, we apply a loss correction inspired by prior work~\citep{patrini2017making, sukhbaatar2015training}. Subsequently, we relax this assumption and estimate $T_t$ directly, forming the foundation of our plugin model approach.

We extend label noise modeling to the autoregressive language setting, focusing on asymmetric or class-conditional noise. At each step $t$, the label $\xbm_t$ in the black-box model’s training data is flipped to $\tilde \xbm_t \in V$ with probability $p^*(\tilde \xbm_t|\xbm_t)$, while preceding tokens $(\xbm_{t-1:1})$ remain unchanged. As a result, the black-box model observes samples from a noisy distribution: $p^*(\tilde \xbm_t, \xbm_{t-1:1}) = \sum_{\xbm_t} p^*(\tilde \xbm_t | \xbm_t) p^*(\xbm_t|\xbm_{t-1:1}) p^*(\xbm_{t-1:1}).$

We define the noise transition matrix $T_t \in [0,1]^{|V|\times |V|}$ at step $t$, where each entry $T_{t_{ij}} = p^*(\tilde \xbm_t = \ebm^j | \xbm_t = \ebm^i)$ represents the probability of label flipping. This matrix is row-stochastic but not necessarily symmetric.

To handle asymmetric label noise, we modify the loss $\bm{\ell}$ for robustness. Initially, assuming a known $T_t$, we apply a loss correction inspired by~\citep{patrini2017making, sukhbaatar2015training}. We then relax this assumption by estimating $T_t$ directly, forming the basis of our \textit{Plugin} model approach.

We observe that a language model trained with no loss correction would result in a predictor for noisy labels $b(\tilde \xbm_t | \xbm_{t-1:1})$. We can make explicit the dependence on $T_t$. For example, with cross-entropy we have:

\begin{align*}
&\ell(\ebm^i, b(\tilde \xbm_t | \xbm_{t-1:1})) = -\log b(\tilde\xbm_t = \ebm^i | \xbm_{t-1:1}) \\
&= -\log \sum_{j=1}^{|V|} p^*(\tilde\xbm_t = \ebm^i | \xbm_t = \ebm^j) b(\xbm_t = \ebm^j | \xbm_{t-1:1}) \\ 
&= -\log \sum_{j=1}^{|V|} T_{t_{ji}} {b}(\xbm_t = \ebm^j | \xbm_{t-1:1}), \numberthis
\label{eq:fc}
\end{align*}
or in matrix form
\begin{equation}
    \label{eq:fc-mat}
    \bm{\ell}(b(\tilde \xbm_t|\xbm_{t-1:1})) = -\log T_t^\top b(\xbm_t|\xbm_{t-1:1}).
\end{equation}

% This loss compares the noisy label $\tilde \xbm_t$ to the noisy predictions averaged using the transition matrix $T_t$ at step $t$. Note that the cross-entropy loss is commonly employed for next-token prediction tasks. Cross-entropy is a \emph{proper composite loss} with the softmax function as its \emph{inverse link function}~\citep{patrini2017making}. Consequently, from Theorem 2 of~\citep{patrini2017making}, the minimizer of the \emph{forwardly-corrected} loss in Equation~\eqref{eq:fc-mat} for noisy data corresponds to the minimizer of the actual loss for clean data. Formally, this can be expressed as:

% \begin{align*}
%     \label{eq:loss-minimizers}
%     & \argmin_{w} E^*_{\tilde \xbm_t,\xbm_{t-1:1}}\Big[\bm{\ell}(\xbm_t, T_t^T b(\xbm_t|\xbm_{t-1:1})) \Big] \\ &= 
%     \argmin_{w} E^*_{\xbm_t,\xbm_{t-1:1}}\Big[\bm{\ell}(\xbm_t, b(\xbm_t|\xbm_{t-1:1})) \Big],
% \end{align*}
% where $w$ are the weights of the language model, and their dependence is implicitly embedded in the definition of the softmax output $b$ from the black-box language model. This result indicates that if the transition matrix $T_t$ were known, we could transform the softmax output $b(\bm{x}_t \mid \bm{x}_{t-1:1})$ using $T_t^T$, use the transformed predictions as the final outputs, and re-train the black-box model accordingly with the corrected loss. However, the transition matrix $T_t$ is not known a priori, and we do not have access to the training data. Thus, estimating $T_t$ from clean data becomes a crucial step in our approach.

This loss compares the noisy label $\tilde \xbm_t$ to the noisy predictions averaged via the transition matrix $T_t$ at step $t$. Cross-entropy loss, commonly used for next-token prediction, is a \emph{proper composite loss} with the softmax function as its \emph{inverse link function}~\citep{patrini2017making}. Consequently, from Theorem 2 of~\citet{patrini2017making}, the minimizer of the \emph{forwardly-corrected} loss in Equation~\eqref{eq:fc-mat} on noisy data aligns with the minimizer of the true loss on clean data, i.e., 
\begin{align*}
    \label{eq:loss-minimizers}
    & \argmin_{w} E^*_{\tilde \xbm_t,\xbm_{t-1:1}}\Big[\bm{\ell}(\xbm_t, T_t^\top b(\xbm_t|\xbm_{t-1:1})) \Big] \\ &= 
    \argmin_{w} E^*_{\xbm_t,\xbm_{t-1:1}}\Big[\bm{\ell}(\xbm_t, b(\xbm_t|\xbm_{t-1:1})) \Big],
\end{align*}
where $w$ are the language model’s weights, implicitly embedded in the softmax output $b$ from the black-box model. This result suggests that if $T_t$ were known, we could transform the softmax output $b(\xbm_t \mid \xbm_{t-1:1})$ using $T_t^T$, use the transformed predictions as final outputs, and retrain the model accordingly. However, since $T_t$ is unknown and training data is inaccessible, estimating $T_t$ from clean data is essential to our approach.


\subsection{Estimation of Transition Matrix}
\label{ssec:estimatingT}

% In our problem setup, we assume access to a small amount of clean language data for the task. Under the assumption that the black-box model is expressive enough to model $p^*(\tilde{\bm{x}}_t \mid \bm{x}_{t-1:1})$ (Assumption (2) in Theorem 3 of~\citep{patrini2017making}), the transition matrix $T_t$ can be estimated using this clean data. Considering the supervised classification problem at step $t$, let $\mathcal{X}_t^i$ denote all samples in the clean data where $\bm{x}_t = \bm{e}^i$ and the preceding tokens are $(\bm{x}_{t-1}, \dots, \bm{x}_1)$. A naive estimate of the transition matrix can be computed as follows:

We assume access to a small amount of target language data for the task. Given that the black-box model is expressive enough to approximate $p^*(\tilde{\xbm}_t \mid \xbm_{t-1:1})$ (Assumption (2) in Theorem 3 of~\citet{patrini2017making}), the transition matrix $T_t$ can be estimated from this target data. Considering the supervised classification setting at step $t$, let $\mathcal{X}_t^i$ represent all target data samples where $\xbm_t = \ebm^i$ and the preceding tokens are $(\xbm_{t-1:1})$. A naive estimate of the transition matrix is: $\hat T_{t_{ij}}=b(\tilde \xbm_t = \ebm^j|\xbm_t=\ebm^i)=\frac{1}{|\mathcal{X}_t^i|}\sum_{x\in\mathcal{X}_t^i}b(\tilde \xbm_t = \ebm^j|\xbm_{t-1:1})$.


While this setup works for a single step $t$, there are two key challenges in extending it across all steps in the token prediction task:

\begin{enumerate}[leftmargin=0.4cm]
    \item \textbf{Limited sample availability:} The number of samples where $\bm{x}_t = \bm{e}^i$ and the preceding tokens $(\bm{x}_{t-1}, \dots, \bm{x}_1)$ match exactly is limited in the clean data, especially with large vocabulary sizes (e.g., $|V| = O(100K)$ for LLaMA~\citep{dubey2024llama}). This necessitates modeling the transition matrix as a function of features derived from $\bm{x}_{t-1:1}$, akin to text-based autoregressive models.
    \item \textbf{Large parameter space:} With a vocabulary size of $|V| = O(100K)$, the transition matrix $T_t$ at step $t$ contains approximately 10 billion parameters. This scale may exceed the size of the closed-source LLM and cannot be effectively learned from limited target data. Therefore, structural restrictions must be imposed on $T_t$ to reduce its complexity.
\end{enumerate}

To address these challenges, we impose the restriction that the transition matrix $T_t$ is diagonal. While various constraints could be applied to simplify the problem, assuming $T_t$ is diagonal offers two key advantages. First, it allows the transition matrix—effectively a vector in this case—to be modeled using standard autoregressive language models, such as a \emph{GPT-2 model with $k$ transformer blocks}, a \emph{LLaMA model with $d$-dimensional embeddings}, or a fine-tuned \emph{GPT-2-small} model. These architectures can be adjusted based on the size of the target data. Second, a diagonal transition matrix corresponds to a symmetric or class-independent label noise setup, where $\xbm_t = \ebm^i$ flips to any other class with equal probability in the training data. This assumption, while simplifying, remains realistic within the framework of label noise models.

By enforcing this diagonal structure, we ensure efficient estimation of the transition matrix while maintaining practical applicability within our framework. In the next section, we outline our approach for adapting closed-source language models to target data.












