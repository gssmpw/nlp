\section{Related Work}
\label{sec:relatedwork}
% \vspace{-1mm}
\paragraph{Parameter-Efficient Fine-Tuning (PEFT).}  
PEFT methods adapt LLMs to downstream tasks while minimizing computational overhead. LoRA____ and QLoRA____ introduce low-rank updates and quantization for efficient fine-tuning, while prefix tuning____, adapters____, and soft prompting____ modify task-specific representations through trainable layers or embeddings. However, these methods require access to model weights, gradients, or architecture details, making them unsuitable for closed-source LLMs and inapplicable as baselines in our setup. In contrast, our approach operates solely on token logits, enabling adaptation without modifying the underlying model.
% \vspace{-2mm}

 

% \paragraph{Steering and Aligning LLMs:} Steering and aligning LLMs typically involve reinforcement learning or instruction tuning to modify model behavior. Reinforcement learning from human feedback (RLHF)____ fine-tunes models using human preference data to enhance alignment, but it requires extensive annotated feedback and substantial computational resources. Direct Preference Optimization (DPO)____ simplifies RLHF by optimizing preference scores without reward modeling. Constitutional AI____ leverages self-generated principles to improve alignment while instruction tuning____ adapts models through task-specific demonstrations. However, these methods require access to model weights and training data, which is not feasible for closed-source models. In contrast, our approach aligns black-box LLMs using token-level probability reweighting, requiring only logits rather than model modifications or extensive labeled data, making it a lightweight and practical alternative for enterprise-specific adaptation.

\paragraph{Steering and Aligning LLMs.}  
LLM alignment methods primarily use reinforcement learning or instruction tuning. RLHF and DPO____ optimize model behavior via human preferences, with DPO eliminating reward modeling. Constitutional AI____ aligns models using self-generated principles, while instruction tuning____ adapts them via task-specific demonstrations. Unlike our approach, these methods require model weights and training data, limiting their applicability as baselines in our setup. 
% \vspace{-2mm}
% to closed-source models. In contrast, our method reweights token probabilities at inference using logits alone, enabling efficient adaptation without modifying the model.



% \paragraph{Calibration of LLMs:} Calibration in large language models (LLMs) aims to align model confidence with predictive accuracy, ensuring that generated outputs reflect appropriate uncertainty. Recent studies have introduced various approaches for improving LLM calibration. APRICOT____ calibrates LLMs using only their generations by training an auxiliary model to predict confidence levels. Huang et al.____ propose a framework that treats both correctness and confidence as distributions, addressing calibration in long-form generation tasks. Thermometer____ learns an auxiliary model for universal LLM calibration across multiple tasks. Calibration-tuning____ focuses on refining LLM calibration during fine-tuning, particularly in scenarios where models must recognize uncertainty. Additionally, a systematic study by____ investigates calibration across different alignment stages, examining how LLMs handle factuality and response confidence. However, these approaches primarily focus on adjusting confidence scores without altering token predictions. In contrast, our method directly modifies token probabilities at inference via reweighting, adapting black-box LLMs without requiring access to model weights, fine-tuning, or auxiliary calibration models.

% \paragraph{Calibration of LLMs.}  
% LLM calibration methods aim to align model confidence with predictive accuracy. APRICOT____ and Thermometer____ use auxiliary models for confidence calibration, while Huang et al.____ and Calibration-Tuning____ address calibration during fine-tuning and long-form generation. Zhu et al.____ examine calibration across model alignment stages. These approaches adjust confidence scores but do not alter token predictions. In contrast, our method reweights token probabilities at inference, enabling adaptation of black-box LLMs without modifying the model or requiring fine-tuning.
% \vspace{-2mm}

\paragraph{Calibration of LLMs.}  
LLM calibration methods aim to align model confidence with predictive accuracy and adjust confidence scores but do not alter token predictions____. In contrast, our method reweights token probabilities at inference, enabling adaptation of black-box LLMs without modifying the model or requiring fine-tuning.
% \vspace{-3mm}

\paragraph{Black-box LLMs.}  
Prior work explores various approaches for adapting black-box LLMs without fine-tuning, though they differ fundamentally from our method. ____ infer user preferences through interactive edits but do not adapt models based on past language data. Diffusion-LM____ formulates text generation as a non-autoregressive denoising process, whereas our approach reweights token probabilities autoregressively without requiring black-box model weights. Discriminator-based methods____ control generation based on predefined attributes, contrasting with our method, which enables free-form text adaptation. DExperts____ combines expert and anti-expert probabilities; we incorporate a similar probability combining strategy in a modified baseline without a de-expert component. In-context learning____ offers a common adaptation technique for black-box models and serves as a baseline in our setup.
% \vspace{-3mm}