% \vspace{-1mm}
\section{Experiments}
\label{sec:experiments}
% \vspace{-1mm}
\begin{table*}[t]
    \centering
    \caption{Performance comparison on E2E NLG dataset. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:e2e_final_results}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        
        GPT2-XL & Zeroshot & 0.0562 & 0.4013 & 0.1636& 0.2862 & 0.3697 & 0.0187 & 0.5338 \\
        GPT2-XL & ICL-1 & 0.0686$_{\pm0.032}$ & 0.4016$_{\pm0.042}$ & 0.1404$_{\pm0.052}$ & 0.2745$_{\pm0.025}$ & 0.3503$_{\pm0.019}$ & 0.0353$_{\pm0.015}$ & 0.7944$_{\pm0.067}$ \\
        GPT2-XL & ICL-3 & 0.0980$_{\pm0.035}$ & 0.4188$_{\pm0.040}$ & 0.1923$_{\pm0.046}$ & 0.2912$_{\pm0.031}$ & 0.3925$_{\pm0.027}$ & 0.0250$_{\pm0.017}$ & 0.9390$_{\pm0.054}$ \\ 
        GPT2-XL & NewModel & \underline{0.2377}$_{\pm0.011}$ & \underline{0.5049}$_{\pm0.014}$ & \underline{0.2742}$_{\pm0.013}$ & \underline{0.3902}$_{\pm0.006}$ & \underline{0.4521}$_{\pm0.016}$ & \underline{0.3938}$_{\pm0.019}$ & \underline{1.1927}$_{\pm0.069}$ \\
        GPT2-XL & WeightedComb & 0.1184$_{\pm0.010}$ & 0.4237$_{\pm0.016}$ & 0.1858$_{\pm0.012}$ & 0.3004$_{\pm0.010}$ & 0.3776$_{\pm0.016}$ & 0.1818$_{\pm0.023}$ & 1.0261$_{\pm0.057}$ \\
        GPT2-XL & \textbf{Plugin (Ours)}  & \textbf{0.2470}$_{\pm0.009}$ & \textbf{0.5536}$_{\pm0.007}$ & \textbf{0.3084}$_{\pm0.007}$ & \textbf{0.4213}$_{\pm0.008}$ & \textbf{0.5057}$_{\pm0.009}$ & \textbf{0.5455}$_{\pm0.013}$ & \textbf{1.2736}$_{\pm0.051}$ \\
        \hline
        
        LLaMA-3.1-8B & Zeroshot & 0.3226 & 0.6917 & 0.4050 & 0.5004 & 0.6041 & 0.9764 &  1.1310 \\
        LLaMA-3.1-8B & ICL-1 & 0.3301$_{\pm0.037}$ & 0.6914$_{\pm0.027}$ & 0.4126$_{\pm0.026}$ & 0.5023$_{\pm0.018}$ & 0.6037$_{\pm0.015}$ & 0.9715$_{\pm0.057}$ & 1.1735$_{\pm0.066}$\\
        LLaMA-3.1-8B & ICL-3 & \underline{0.3527}$_{\pm0.033}$ & 0.6936$_{\pm0.036}$ & 0.4217$_{\pm0.017}$ & 0.5127$_{\pm0.017}$ & 0.6202$_{\pm0.009}$ & 0.9927$_{\pm0.018}$ & 1.1672$_{\pm0.047}$\\
        LLaMA-3.1-8B & NewModel & 0.2452$_{\pm0.008}$ & 0.5347$_{\pm0.005}$ & 0.2905$_{\pm0.006}$ & 0.4097$_{\pm0.005}$ & 0.4812$_{\pm0.009}$ & 0.4571$_{\pm0.021}$ & \textbf{1.2281}$_{\pm0.041}$\\
        LLaMA-3.1-8B & WeightedComb & 0.3517$_{\pm0.004}$ & \underline{0.7040}$_{\pm0.004}$ & \underline{0.4249}$_{\pm0.004}$ & \underline{0.5181}$_{\pm0.003}$ & \underline{0.6206}$_{\pm0.002}$ & \underline{1.0947}$_{\pm0.010}$ & 1.1737$_{\pm0.015}$\\
        % LLaMA-3.1-8B & Calibration & \\
        LLaMA-3.1-8B & \textbf{Plugin (Ours)} & \textbf{0.3691}$_{\pm0.013}$ & \textbf{0.7113}$_{\pm0.002}$ & \textbf{0.4374}$_{\pm0.004}$ & \textbf{0.5247}$_{\pm0.002}$ & \textbf{0.6392}$_{\pm0.009}$ & \textbf{1.1441}$_{\pm0.030}$ & \underline{1.1749}$_{\pm0.034}$\\
        
        \hline
    \end{tabular}
    }
    \vspace{-3mm}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Performance comparison on Web NLG dataset. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:web_final_results}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-XL & Zeroshot & 0.0317 & 0.2992 & 0.1321 & 0.2417 & 0.1969 & 0.0491 & 0.1826\\
        GPT2-XL & ICL-1 & 0.0510$_{\pm0.024}$ & 0.3223$_{\pm0.026}$ & 0.1526$_{\pm0.016}$ & 0.2562$_{\pm0.031}$ & 0.2591$_{\pm0.009}$ & 0.1336$_{\pm0.029}$ & 0.2235$_{\pm0.033}$\\
        GPT2-XL & ICL-3 & 0.0744$_{\pm0.016}$ & 0.3383$_{\pm0.036}$ & \underline{0.1682}$_{\pm0.016}$ & 0.2651$_{\pm0.028}$ & \underline{0.3071}$_{\pm0.014}$ & 0.1675$_{\pm0.024}$ & 0.2550$_{\pm0.021}$\\
        GPT2-XL & NewModel & \underline{0.1071}$_{\pm0.005}$ & 0.3260$_{\pm0.010}$ & 0.1496$_{\pm0.014}$ & 0.2724$_{\pm0.013}$ & 0.2642$_{\pm0.008}$ & \underline{0.4327}$_{\pm0.023}$ & \underline{0.2916}$_{\pm0.031}$ \\
        GPT2-XL & WeightedComb & 0.0636$_{\pm0.006}$ & \underline{0.3453}$_{\pm0.007}$ & 0.1666$_{\pm0.003}$ & \underline{0.2782}$_{\pm0.005}$ & 0.2871$_{\pm0.006}$ & 0.2460$_{\pm0.005}$ & \textbf{0.2981}$_{\pm0.018}$\\
        GPT2-XL & \textbf{Plugin (Ours)} & \textbf{0.1673}$_{\pm0.004}$ & \textbf{0.4616}$_{\pm0.007}$ & \textbf{0.2527}$_{\pm0.007}$ & \textbf{0.3757}$_{\pm0.008}$ & \textbf{0.3895}$_{\pm0.007}$ & \textbf{0.8987}$_{\pm0.013}$ & 0.2646$_{\pm0.003}$ \\
        \hline
        
        LLaMA-3.1-8B & Zeroshot & 0.1453 & 0.5278 & 0.3030 & 0.3982 & 0.4314 & 0.6991 & \underline{0.2684}\\
        LLaMA-3.1-8B & ICL-1 & \underline{0.2166}$_{\pm0.031}$ & 0.5944$_{\pm0.027}$ & 0.3706$_{\pm0.025}$ & \underline{0.4667}$_{\pm0.013}$ & 0.5651$_{\pm0.045}$ & \underline{1.5719}$_{\pm0.024}$ & 0.2462$_{\pm0.038}$\\
        LLaMA-3.1-8B & ICL-3 & 0.2031$_{\pm0.027}$ & 0.5937$_{\pm0.019}$ & \underline{0.3821}$_{\pm0.015}$ & 0.4653$_{\pm0.024}$ & \underline{0.5682}$_{\pm0.046}$ & 1.3826$_{\pm0.051}$ & 0.2469$_{\pm0.045}$\\
        LLaMA-3.1-8B & NewModel & 0.1284$_{\pm0.005}$ & 0.3506$_{\pm0.009}$ & 0.1673$_{\pm0.007}$ & 0.2879$_{\pm0.009}$ & 0.2921$_{\pm0.008}$ & 0.4999$_{\pm0.030}$ & \textbf{0.2973}$_{\pm0.008}$\\
        LLaMA-3.1-8B & WeightedComb & 0.1922$_{\pm0.012}$ & \underline{0.5986}$_{\pm0.019}$ & 0.3612$_{\pm0.012}$ & 0.4659$_{\pm0.008}$ & 0.4470$_{\pm0.030}$ & 1.1855$_{\pm0.075}$ & 0.2575$_{\pm0.020}$\\
        LLaMA-3.1-8B & \textbf{Plugin (Ours)} & \textbf{0.2542}$_{\pm0.004}$ & \textbf{0.6375}$_{\pm0.005}$ & \textbf{0.3873}$_{\pm0.005}$ & \textbf{0.4869}$_{\pm0.007}$ & \textbf{0.5724}$_{\pm0.004}$ & \textbf{1.5911}$_{\pm0.046}$ & 0.2590$_{\pm0.003}$\\
        \hline
    \end{tabular}
    }
        \vspace{-3mm}

\end{table*}

\begin{table*}[t]
    \centering
    \caption{Performance comparison on CommonGen dataset. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:common_final_results}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-XL & Zeroshot & 0.0317 & 0.2992 & 0.1321 & 0.2417 & 0.1969 & 0.0491 & 0.1826\\
        GPT2-XL & ICL-1 & 0.0508$_{\pm0.023}$ & 0.3201$_{\pm0.035}$ & 0.1526$_{\pm0.097}$ & 0.2562$_{\pm0.103}$ & 0.2591$_{\pm0.089}$ & 0.1336$_{\pm0.092}$ & 0.2235$_{\pm0.069}$\\
        GPT2-XL & ICL-3 & 0.0744$_{\pm0.011}$ & 0.3383$_{\pm0.014}$ & 0.1682$_{\pm0.030}$ & 0.2651$_{\pm0.072}$ & 0.3071$_{\pm0.073}$ & 0.1675$_{\pm0.066}$ & 0.2550$_{\pm0.047}$\\ 
        GPT2-XL & NewModel & \underline{0.1260}$_{\pm0.007}$ & \underline{0.4106}$_{\pm0.016}$ & \underline{0.1683}$_{\pm0.013}$ & \underline{0.3740}$_{\pm0.009}$ & \underline{0.3600}$_{\pm0.024}$ & \underline{0.4570}$_{\pm0.058}$ & \textbf{0.7113}$_{\pm0.025}$\\
        GPT2-XL & WeightedComb & 0.0614$_{\pm0.020}$ & 0.3364$_{\pm0.024}$ & 0.1347$_{\pm0.009}$ & 0.2969$_{\pm0.019}$ & 0.2921$_{\pm0.018}$ & 0.2763$_{\pm0.010}$ & 0.3352$_{\pm0.051}$ \\
        GPT2-XL & \textbf{Plugin (Ours)} & \textbf{0.1791}$_{\pm0.014}$ & \textbf{0.4932}$_{\pm0.007}$ & \textbf{0.2288}$_{\pm0.004}$ & \textbf{0.4347}$_{\pm0.007}$ & \textbf{0.4702}$_{\pm0.006}$ & \textbf{0.7283}$_{\pm0.012}$ & \underline{0.6554}$_{\pm0.038}$\\
        
        \hline
        LLaMA-3.1-8B & Zeroshot & 0.0643 & 0.2776 & 0.1181 & 0.2488 & 0.3857 & 0.3155 & 0.3347\\
        LLaMA-3.1-8B & ICL-1 & 0.0615$_{\pm0.027}$ & 0.2697$_{\pm0.033}$ & 0.1158$_{\pm0.062}$ & 0.2469$_{\pm0.087}$ & 0.3822$_{\pm0.069}$ & 0.3005$_{\pm0.072}$ & 0.3059$_{\pm0.094}$\\
        LLaMA-3.1-8B & ICL-3 & 0.0635$_{\pm0.016}$ & 0.2748$_{\pm0.024}$ & 0.1225$_{\pm0.018}$ & 0.3120$_{\pm0.047}$ & \underline{0.4012}$_{\pm0.029}$ & 0.3250$_{\pm0.022}$ & 0.3794$_{\pm0.034}$\\
        LLaMA-3.1-8B & NewModel & 0.0753$_{\pm0.004}$ & \underline{0.3716}$_{\pm0.005}$ & 0.1122$_{\pm0.003}$ & \underline{0.3404}$_{\pm0.004}$ & 0.2665$_{\pm0.006}$ & 0.1919$_{\pm0.015}$ & \underline{0.6900}$_{\pm0.046}$\\
        LLaMA-3.1-8B & WeightedComb & \underline{0.1789}$_{\pm0.005}$ & 0.3485$_{\pm0.012}$ & \underline{0.1797}$_{\pm0.008}$ & 0.2981$_{\pm0.012}$ & 0.3637$_{\pm0.011}$ & \underline{0.5503}$_{\pm0.046}$ & 0.5450$_{\pm0.020}$\\
        LLaMA-3.1-8B & \textbf{Plugin (Ours)} & \textbf{0.2665}$_{\pm0.010}$ & \textbf{0.5800}$_{\pm0.002}$ & \textbf{0.3139}$_{\pm0.005}$ & \textbf{0.5037}$_{\pm0.004}$ & \textbf{0.5829}$_{\pm0.003}$ & \textbf{1.0876}$_{\pm0.020}$ & \textbf{0.7031}$_{\pm0.007}$\\
        \hline
    \end{tabular}
    }
        \vspace{-3mm}

\end{table*}




\begin{table*}[t]
    \centering
    \caption{Performance comparison on Adidas dataset. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:adidas_final_results}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline

        GPT2-XL & Zeroshot & 0.0075 & 0.2309 & 0.0278 & 0.1438 & 0.1487 & 0.0184 & 0.4956\\
        GPT2-XL & ICL-1 & 0.0109$_{\pm0.039}$ & 0.2567$_{\pm0.082}$ & 0.0265$_{\pm0.054}$ & 0.1519$_{\pm0.038}$ & 0.1649$_{\pm0.052}$ & 0.0318$_{\pm0.171}$ & 0.5133$_{\pm0.162}$\\
        GPT2-XL & ICL-3 & 0.0295$_{\pm0.037}$ & 0.2509$_{\pm0.071}$ & 0.0395$_{\pm0.043}$ & 0.1536$_{\pm0.039}$ & 0.1658$_{\pm0.041}$ & 0.0321$_{\pm0.109}$ & 0.5176$_{\pm0.116}$\\ 
        GPT2-XL & NewModel & 0.0515$_{\pm0.016}$ & \underline{0.2690}$_{\pm0.014}$ & 0.0637$_{\pm0.014}$ & \underline{0.1697}$_{\pm0.008}$ & \underline{0.1918}$_{\pm0.013}$ & \underline{0.0550}$_{\pm0.086}$ & \textbf{0.6682}$_{\pm0.047}$\\
        GPT2-XL & WeightedComb & \underline{0.0567}$_{\pm0.016}$ & 0.2210$_{\pm0.027}$ & \underline{0.0714}$_{\pm0.015}$ & 0.1550$_{\pm0.024}$ & 0.1674$_{\pm0.017}$ & 0.0183$_{\pm0.117}$ & 0.4105$_{\pm0.109}$\\
        GPT2-XL & \textbf{Plugin (Ours)} & \textbf{0.0600}$_{\pm0.017}$ & \textbf{0.2710}$_{\pm0.025}$ & \textbf{0.0722}$_{\pm0.018}$ & \textbf{0.1725}$_{\pm0.017}$ & \textbf{0.1995}$_{\pm0.018}$ & \textbf{0.1195}$_{\pm0.138}$ & \underline{0.6375}$_{\pm0.120}$
\\
        \hline
        LLaMA-3.1-8B & Zeroshot & 0.0120 & 0.2470 & 0.0318 & 0.1493 & 0.1526 & 0.0424 & 0.5285\\
        LLaMA-3.1-8B & ICL-1 & 0.0220$_{\pm0.044}$ & 0.2472$_{\pm0.072}$ & 0.0405$_{\pm0.068}$ & 0.1434$_{\pm0.057}$ & 0.1686$_{\pm0.041}$ & 0.0555$_{\pm0.133}$ & 0.5078$_{\pm0.142}$\\
        LLaMA-3.1-8B & ICL-3 & 0.0177$_{\pm0.041}$ & 0.2385$_{\pm0.065}$ & 0.0364$_{\pm0.071}$ & 0.1408$_{\pm0.030}$ & 0.1712$_{\pm0.029}$ & 0.0587$_{\pm0.102}$ & 0.5775$_{\pm0.145}$\\
        LLaMA-3.1-8B & NewModel & \underline{0.0506}$_{\pm0.011}$ & \underline{0.2700}$_{\pm0.011}$ & 0.0634$_{\pm0.006}$ & \underline{0.1749}$_{\pm0.006}$ & \textbf{0.1995}$_{\pm0.009}$ & 0.0575$_{\pm0.051}$ & \textbf{0.6570}$_{\pm0.072}$\\
        LLaMA-3.1-8B & WeightedComb & 0.0357$_{\pm0.017}$ & 0.2583$_{\pm0.014}$ & \underline{0.0661}$_{\pm0.015}$ & 0.1560$_{\pm0.011}$ & 0.1706$_{\pm0.016}$ & \underline{0.0745}$_{\pm0.086}$ & 0.5927$_{\pm0.077}$\\
        LLaMA-3.1-8B & \textbf{Plugin (Ours)} & \textbf{0.0611}$_{\pm0.018}$ & \textbf{0.2714}$_{\pm0.029}$ & \textbf{0.0742}$_{\pm0.020}$ & \textbf{0.1759}$_{\pm0.019}$ & \underline{0.1990}$_{\pm0.020}$ & \textbf{0.1293}$_{\pm0.152}$ & \underline{0.6361}$_{\pm0.134}$\\
        \hline
    \end{tabular}
    }
        \vspace{-3mm}

\end{table*}

We divide this section into three parts. 
\Cref{ssec:exp_textgen} evaluates \textit{Plugin} on four text generation datasets across three black-box language models. Since the \textit{Plugin} model is trained on top of black-box models, we refer to black-box models interchangeably as \emph{base models}. 
\Cref{ssec:ablation} presents ablation studies analyzing the impact of black-box model quality, \textit{Plugin}'s complexity, and architecture choices.
Section~\ref{ssec:qualitative} shows qualitative analysis and case studies.

% We evaluate \textit{Plugin} on four text generation benchmarks: (a) E2E NLG~\cite{duvsek2020evaluating}, (b) Web NLG~\cite{gardent2017creating}, (c) CommonGen~\cite{lin2020commongen}, and (d) Adidas product description dataset~\cite{adidasdataset}. 
% We use the standard train, validation, and test split for the first three datasets from the Transformers library~\cite{wolf2020transformers}. 
% Furthermore, to create distribution shift setting, the training data in Web NLG is filtered to contain description of only infrastructures; whereas, the validation and test data contains description of persons. 
% Similarly, the training data in CommonGen dataset is filtered such that the samples contain \textit{man} based descriptions; whereas, validation and test data are kept as it is.
% Details of this approach are demonstrated in Section~\ref{ssec:qualitative}.
% We break the Adidas dataset into two parts: validation and test set. 
% The data statistics are provided in Table~\ref{tab:dataset_statistics}, Appendix~\ref{sec:data_statistics}. 

We evaluate \textit{Plugin} on four text generation benchmarks: (a) E2E NLG~\citep{duvsek2020evaluating}, (b) Web NLG~\citep{gardent2017creating}, (c) CommonGen~\citep{lin2020commongen}, and (d) the Adidas product description dataset~\citep{adidasdataset}. For the first three datasets, we use the standard train-validation-test splits from the Transformers library~\citep{wolf2020transformers}. To introduce distribution shifts, we filter Web NLG's training data to include only \emph{infrastructure} descriptions, while validation and test sets retain \emph{person} descriptions. Similarly, CommonGen’s training set is restricted to samples containing \textit{man}, while validation and test sets remain unchanged. Details of this setup are in \Cref{ssec:qualitative}. The Adidas dataset is split into validation and test sets. Dataset statistics are provided in \Cref{tab:dataset_statistics}, Appendix~\ref{sec:data_statistics}.
% \vspace{-1mm}
\subsection{Text Generation Performance Comparison}
\label{ssec:exp_textgen}
% \vspace{-1mm}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{icml2025/images/plugin_effect.pdf}
    \vspace{-3mm}
    \caption{\textit{Plugin} with increasingly fine-tuned GPT2-M models on the E2E NLG dataset. Results demonstrate that as the quality of the base model improves, the performance of the \textit{Plugin} improves.}
    \label{fig:plugin_effect}
    \vspace{-2mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{icml2025/images/plugin_complexity.pdf}
    \vspace{-3mm}
    \caption{Performance of GPT2-M with varying reweighting model complexities on E2E NLG (BLEU, ROUGE-L). A single-layer reweighting model yields significant gains, while additional layers degrade performance due to overfitting. Initializing with GPT2-Small as the reweighting model improves performance, demonstrating the benefits of leveraging small pretrained models.}
    \vspace{-2mm}
    \label{fig:plugin_complexity}
\end{figure}


We evaluate \textit{Plugin} on the text generation task using only the validation and test splits of all four datasets, reserving the train split for ablation studies (\Cref{ssec:ablation}). \textit{Plugin} and baseline models are trained on the small validation set, with performance measured on the test set. Additionally, we allocate 40\% of the validation data as \textit{hyper-validation} for cross-validation of hyperparameters.

Performance is reported using seven standard natural language generation metrics: (a) BLEU~\citep{papineni2002bleu}, (b) ROUGE-1~\citep{lin2004rouge}, (c) ROUGE-2~\citep{lin2004rouge}, (d) ROUGE-L~\citep{lin2004automatic}, (e) METEOR~\citep{banerjee2005meteor}, (f) CIDEr~\citep{vedantam2015cider}, and (g) NIST~\citep{doddington2002automatic}. All experiments are repeated over five random seeds, and we report the mean and standard deviation for each metric.


% We compare our \textit{Plugin} with the following baselines: (a) \textbf{Zeroshot}: The black-box model is used complete the text generation task, (b) 
% \textbf{ICL-1}~\cite{long2023adapt} 1 sample from the validation set is chosen at random and provided as in-context example to complete the generation task, (c) \textbf{ICL-3}~\cite{long2023adapt} 3 samples from the validation set are chosen at random and used as in-context examples, (d) \textbf{NewModel}: A new language model is trained using the validation data, and (e) \textbf{WeightedComb}~\cite{liu2021dexperts}: A new language model is trained in conjunction with the black-box model. The token probabilities are given by: $\alpha \nbm + (1-\alpha)\bbm$, where $\nbm$ denotes the probabilities from the new model, and $\alpha$ is cross-validated from the set $\{0.25, 0.50, 0.75\}$,
% Note that, wherever prompts are used, the same prompts are used for every baseline method. 
% The prompts are provided in Appendix~\ref{ssec:prompts_app}. 
% For all the methods, we use greedy decoding on token probabilities to complete the text generation. 
% We vary the choice of black-box models across GPT2-M~\cite{radford2019language}, GPT2-XL~\cite{radford2019language}, and LLaMA-3.1-8B~\cite{dubey2024llama}. 

% \textit{NewModel}, \textit{WeightedComb}, and the reweighting model in \textit{Plugin} use the same architecture in the experiments. In case of GPT models, these models use 1-hidden layer in the Transformer encoder. 
% Rest of the config uses default parameters. In case of LLaMA model, these models use 1-hidden layer in the Transformer encoder with 256 hidden size, 1024 intermediate size, and 1 attention head. The learning rate and weight decay for these models are cross-validated across \{1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3\} and \{0.01, 0.1, 1, 10\}, respectively.
% We use AdamW for training with a warmup phase, followed by linear decay. Early stopping is applied if the loss on \textit{hyper-validation} does not decrease for five consecutive epochs. Recall that we assume that we do have access to the weights of the black-box model; hence, any fine-tuning based approach is not applicable in our setting.
 
We compare \textit{Plugin} with the following baselines:  
(a) \textbf{Zeroshot}: The black-box model directly performs text generation without additional adaptation.  
(b) \textbf{ICL-1}~\citep{long2023adapt}: One randomly selected validation sample is used as an in-context example.  
(c) \textbf{ICL-3}~\citep{long2023adapt}: Three randomly selected validation samples are used as in-context examples.  
(d) \textbf{NewModel}: A new language model is trained using the validation data.  
(e) \textbf{WeightedComb}~\citep{liu2021dexperts}: A new model is trained alongside the black-box model, with token probabilities computed as $\alpha \nbm + (1-\alpha)\bbm$, where $\nbm$ represents the probabilities from the new model and $\alpha$ is cross-validated in $\{0.25, 0.50, 0.75\}$. Since the black-box model weights are inaccessible, fine-tuning-based approaches are not applicable in our setting.

All methods use the same prompts where applicable (Appendix~\ref{ssec:prompts_app}) and employ greedy decoding. The base (black-box) models used are GPT2-M~\citep{radford2019language}, GPT2-XL~\citep{radford2019language}, and LLaMA-3.1-8B~\citep{dubey2024llama}. \textit{NewModel}, \textit{WeightedComb}, and the reweighting model in \textit{Plugin} share the same architecture. For GPT-based models, these use a Transformer encoder with one hidden layer and default configurations. For LLaMA-based models, the architecture consists of a Transformer encoder with one hidden layer, 256 hidden size, 1024 intermediate size, and one attention head. Learning rate and weight decay are cross-validated over $\{1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3\}$ and $\{0.01, 0.1, 1, 10\}$, respectively. Models are trained using AdamW with warmup followed by linear decay, and early stopping is applied if the \textit{hyper-validation} loss does not decrease for five consecutive epochs. 

% \vspace{-1mm}
Due to space constraints, results for GPT2-M as the base model are presented in Appendix~\ref{ssec:additional_results}. As shown in Tables~\ref{tab:e2e_final_results}–\ref{tab:adidas_final_results} and their counterparts in Appendix~\ref{ssec:additional_results} (the best is bold, the second best is underlined), \textit{Plugin} outperforms baselines across nearly all datasets, black-box models, and evaluation metrics.  \textit{NewModel} occasionally achieves higher NIST scores due to increased repetition of less-frequent input tokens, but this comes at the cost of coherence, as reflected by other metrics. \textit{WeightedComb} does not perform well,  indicating one combination for all tokens is not a good modeling choice. We note that the absolute numbers may not appear competitive with state-of-the-art results, because (a) we restrict to greedy decoding (\Cref{ssec:inference}), and (b) Web NLG and CommonGen use distribution-shifted subsets.

We also conduct a human evaluation on 100 Adidas dataset samples, where three subjects compare outputs from \textit{Plugin} and \textit{ICL-3} using LLaMA-3.1 as the base model. Evaluators select the prediction closest to the ground truth, with \textit{Plugin} preferred in 81\% of cases. Details are in Appendix~\ref{appendix:adidas_case_study}.
% \vspace{-2mm}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{icml2025/images/adidas_histogram.png}
    \vspace{-6mm}
    \caption{Comparison of the adaptation ability between the base model and \textit{Plugin} on Adidas dataset. \textit{Plugin}, enhanced with a reweighting model, generates text that better aligns with the ``\textit{Adidas domain}''. The bottom row illustrates token probabilities for key Adidas-related words at different decoding steps, showing how the reweighting model influences token selection.}
    \label{fig:adidas_decoding}
    % \vspace{-5mm}
\end{figure*}


\subsection{Ablation Study}
\label{ssec:ablation}
% \vspace{-2mm}
 We now show ablation studies that reflect various aspects of the \textit{Plugin} model. We display the results using GPT2-M as base model on the E2E NLG dataset. The observation is similar on other base models and datasets (Appendix~\ref{appendix:more_ablation}).
% \vspace{-4mm}

% \vspace{-1mm}
\paragraph{Impact of Base Model Quality.}  
We fine-tune GPT2-M  for varying epochs, denoted as 1FT (one epoch), 2FT (two epochs), and 5FT (five epochs), and train a \textit{Plugin} model for each. Figure~\ref{fig:plugin_effect} shows that as the base model's task-specific quality improves, the \textit{Plugin's} performance improves.
% \vspace{-2.5mm}

% \vspace{-1mm}
\paragraph{Complexity of the Reweighting Model in \textit{Plugin}.}  
We train \textit{Plugin} models with reweighting architectures varying from 1 to 12 transformer layers while keeping other configurations unchanged. Additionally, we train a variant where the reweighting model is initialized with GPT2-Small. As shown in Figure~\ref{fig:plugin_complexity}, a single-layer reweighting model yields significant improvements over the base GPT2-M model, while additional layers (e.g., 2, 4, 8, 12) offer diminishing returns and slight performance decline due to overfitting on the small validation set of E2E NLG. This suggests that more data is required for learning complex reweighting models. Notably, initializing with a pretrained GPT2-Small substantially improves performance, underscoring the advantage of using small pretrained models for reweighting due to their inherent autoregressive properties.


% \begin{table}[t]
%     \centering
%     \caption{Comparison of Output Distribution on Web NLG and CommonGen. The results highlight biases in model performance due to differences in data distribution across categories.}
%     \setlength{\tabcolsep}{10pt}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{lcccc}
%         \toprule
%         & \multicolumn{2}{c}{\textbf{WebNLG}} & \multicolumn{2}{c}{\textbf{CommonGen}} \\
%         & \makecell{Human} & \makecell{Building} 
%         & \makecell{Woman} & \makecell{Man} \\

%         \midrule
%         Ground-Truth (Reference) & 100.00\% & 0.00\% & 16.55\% & 16.55\% \\
%         Fine-Tuned Base Model & 7.86\% & 6.43\% & 5.24\% & 4.98\% \\
%         Plugin Model (Ours) & 71.34\% & 68.92\% & 31.92\% & 30.47\% \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:adaptation_results}
% \end{table}




% \vspace{-2mm}
\subsection{Qualitative Analysis and Case Study}
\label{ssec:qualitative}

\paragraph{\textit{Plugin} adapting to distribution shift.}  
We evaluate \textit{Plugin} on distribution-shifted Web NLG and CommonGen using LLaMA-3.1-8B as the base model. Web NLG training data contains only \textit{Infrastructure} concepts, while validation and test sets include \textit{Person} concepts. Similarly, CommonGen training data features \textit{man}, whereas validation and test sets contain both \textit{man} and \textit{woman}. The base model is fine-tuned on training data, and \textit{Plugin} is trained on validation data using the fine-tuned model as the base, ensuring that \textit{Plugin} corrects biases learned from training.  

Using GPT-4o~\citep{hurst2024gpt} as an evaluator, the fine-tuned Web NLG model generates only 17.99\% \textit{Person}-related sentences, while \textit{Plugin} increases this to 71.34\%. 
On CommonGen, the fine-tuned model generates 10.37\% \textit{Woman}-related sentences, whereas \textit{Plugin} improves this to 31.92\%. These results highlight \textit{Plugin}'s ability to adapt under distribution shift and mitigate biases in the base model.

% \vspace{-4mm}
\paragraph{Case study: \textit{Plugin} adapting to domain.}  
We examine token probabilities during inference for LLaMA-3.1-8B and \textit{Plugin} to assess domain adaptation in the Adidas dataset. Removing stopwords, we extract the top-50 most frequent words, defining the ``\textit{Adidas domain}''.
Figure~\ref{fig:adidas_decoding} illustrates this adaptation: the first row shows product attributes and ground-truth references; the second row compares outputs from the base model (left) and \textit{Plugin} (right); the third row visualizes model probabilities for ``\textit{Adidas domain}'' words at three decoding steps.

As seen in Figure~\ref{fig:adidas_decoding}, \textit{Plugin} dynamically reweights probabilities to align with domain-specific language. At step 23, ``keep'' is significantly upweighted. At step 48, ``comfortable'' and ``dry'' gain prominence over ``fit,'' which the base model favors. 
At step 51, ``ambition'' is preferred by \textit{Plugin}, aligning with the ground truth, while the base model favors ``look'' and ``products''. This demonstrates that \textit{Plugin} effectively steers generation toward domain-specific terminology, whereas the base model, trained on broad corpora, lacks inherent domain preference.








%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
% Below is run by Gaurush
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%

% \newpage
% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2-M & 0.0247 & 0.3539 & 0.1003 & 0.2250 & 0.3015 & 0.0156 & 0.6133 \\
%         GPT2-Plugin (5e-4,10.0) & 0.1886 & 0.5406 & 0.2805 & 0.3916 & 0.4951 & 0.4325 & 1.0976\\
%         GPT2-1FT (5e-4,0.01)  & 0.2869 & 0.6239 &0.3611 &0.4581 &0.6217 &0.6835 &1.4117 \\
%         GPT2-1FT-Plugin (5e-5,10)  &0.3323 &0.6705 & 0.4064& 0.5087& 0.6135&0.9225 & 1.127\\
%         GPT2-2FT (5e-4,0.01) & 0.2277 & 0.5688 & 0.318 & 0.4063 & 0.6203 & 0.3648 & 1.2415 \\
%         GPT2-2FT-Plugin (5e-5,10) & 0.3229& 0.6648&0.4071 &0.5096 &0.5912 &0.9056 &1.018 \\
%         GPT2-5FT (5e-4,0.01) & 0.1618& 0.4894& 0.2548 & 0.3300& 0.6089& 0.0042 & 0.9929\\
%         GPT2-5FT-Plugin (5e-5,10) &0.3169 & 0.6672& 0.4128&0.5144 &0.5853 & 0.8549&0.9894 \\
%         \\
%         \\
%         GPT2-1FT (5e-5,0.01) & 0.2192 & 0.5539& 0.3000 & 0.3872& 0.5790& 0.3292 & 1.2447\\
%         GPT2-1FT-Plugin (5e-4,10) &0.3008 & 0.6311 & 0.3706 & 0.4680 & 0.5711 &0.8008 & 1.1337\\
%         GPT2-2FT (5e-5,0.01) & 0.2199 & 0.5616 & 0.3055 & 0.3987 & 0.5997 & 0.3287 & 1.2791\\
%         GPT2-2FT-Plugin (5e-3,10) & 0.3123 &0.6386 &0.3744 &0.4724 &0.5980 &0.8225 & 1.2274\\
%         GPT2-5FT (5e-5,0.01) & 0.2481 & 0.5922 & 0.3314 & 0.4248 & 0.6160 & 0.4825 & 1.3424 \\
%         GPT2-5FT-Plugin (5e-5,10)& 0.3079 & 0.6519 & 0.3870 &0.4869 & 0.6081 & 0.8493 & 1.1561 \\
%         \\ \\
%         GPT2-Plugin-Small (5e-4,1)& 0.2912 & 0.6259 & 0.3593 &0.4663 & 0.5733 & 0.7153 & 1.1206 \\
%         GPT2-Plugin-12layer (5e-3,10) & 0.1638 & 0.5339 & 0.2661 & 0.3781 & 0.4914 & 0.3601 & 1.1072 \\
%         \\ \\
%         NewModel-GPT2-1layer (5e-4,1) (baseline) & 0.2551 & 0.5330 & 0.3033 &0.4189 & 0.4599 & 0.4579 & 1.1482\\
%         WeightedComb-GPT2-1layer (5e-4,1, 0.75) (baseline) & 0.0859 & 0.4223 & 0.1636 & 0.3151 & 0.3056 & 0.2611 & 0.8811\\
%         GPT2-1ICL (baseline) & 0.0543 & 0.3431 & 0.1299 & 0.2280 & 0.3434 & 0.0260 & 0.7767\\
%         GPT2-3ICL (baseline) & 0.0750 & 0.3955& 0.1676 & 0.2649 & 0.3977 & 0.0252 & 0.8993\\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt. Plugin trained on val and hyperval. \todog{NewModel is a surprising result. It is just the language model with same architecture as plugin but is trained independent of the black box model.}}
%     \label{tab:e2e_gpt2final}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2XL (zero shot) & 0.0562 & 0.4013 & 0.1636& 0.2862 & 0.3697 & 0.0187 & 0.5338\\
%         GPT2XL-Plugin-1-layer (5e-4, 10) & 0.2365 & 0.5524 & 0.3054 & 0.4201 &0.5033 & 0.5550& 1.2073\\
%         NM-GPT2XL-1-layer  (5e-4, 1) & 0.2278 & 0.5016 & 0.2625 & 0.3886 & 0.4385 & 0.4346 & 1.2516\\
%         WC-GPT2XL-1-layer (5-4, 1, 0.75)  &  0.107 & 0.4295 & 0.1588 & 0.3116 & 0.3214 & 0.2370 & 1.1487 \\
%         GPT2XL-IC-1  & 0.0686 & 0.4016 & 0.1404 & 0.2745 & 0.3503 & 0.0353 & 0.7944 \\
%         GPT2XL-IC-3 & 0.0980 & 0.4188 & 0.1923 & 0.2912 & 0.3925 & 0.0250 & 0.9390 \\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt for GPT2-XL. Plugin trained on val and hyperval.}
%     \label{tab:e2e_gpt2xlfinal}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model (epoch, lr, wd) & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST \\ 
%         \hline
%         LLaMA (zero shot) & 0.3226 & 0.6917 & 0.4050 & 0.5004 & 0.6041 & 0.9764 &  1.1310 \\
%         LLaMA-Plugin-1-layer-1-Atthead-256-hs-1024-is (5e-4, 1) & 0.3701 & 0.7148 & 0.4407 & 0.5282 & 0.638 &  1.1611 &  1.1586 \\
%         LLaMA-Plugin-1-layer-1-Atthead (5e-5, 10)& 0.3769 & 0.7101 & 0.4359 & 0.5201 & 0.6435 & 1.1373  & 1.1955  \\
%         LLaMA-Plugin-1-layer-4-Atthead (80, 5e-5,10) & 0.3760 & 0.7103 & 0.4363 & 0.5212 & 0.6450 & 1.1362 & 1.2002 \\
%         LLaMA-Plugin-2-layer-4-Atthead (80, 5e-3, 10) & 0.3653 & 0.7067 & 0.4295 & 0.5169 & 0.6317 & 1.0986 & 1.1794\\
%         \\ \\
%         NewModel-LLaMA-1layer  (5e-4, 0.1) (baseline) & 0.2400 & 0.5263 & 0.2738 & 0.3959 & 0.4686 & 0.4059 & 1.1700\\
%         WeightedComb-LLaMA-1layer  (5e-4, 1, 0.25) (baseline) &  0.3195  & 0.6782 & 0.3929 & 0.4872 & 0.5905 & 0.9427 & 1.1755\\
%         LLaMA-1ICL (baseline) &  0.2912  & 0.6741 & 0.3650 & 0.4818 & 0.6018 & 0.9504 & 1.1858 \\
%         LLaMA-3ICL (baseline) &  0.3063  & 0.6479 & 0.3792 & 0.4618 & 0.5759 & 0.9545 & 1.2209 \\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with final good prompt for LLaMA (no fine-tuning). Pluging trained on val and hyperval.}
%     \label{tab:e2e_LLaMA_w_prompt}
% \end{table*}




% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2-M & 0.0213 & 0.2765  & 0.1014 &  0.1872 & 0.2111 & 0.0479 & 0.234 \\
%         GPT2-Plugin (5e-4,10.0) & 0.1284 & 0.4568 & 0.2239 & 0.3509 & 0.3838 & 0.7223& 0.2968\\
%         GPT2-1FT (5e-4, 0.01) & 0.1193 &0.4877  & 0.2251 & 0.3509 & 0.4201 & 0.6944 & 0.3343 \\
%         GPT2-1FT-Plugin (5e-5, 10) &0.1642 & 0.5371 & 0.2826 & 0.4156 & 0.4715 & 1.2148 & 0.2892\\
%         GPT2-2FT (5e-4, 0.001) & 0.142 & 0.521 & 0.272 & 0.391 & 0.4547 &0.9883  & 0.2925 \\
%         GPT2-2FT-Plugin (5e-5, 10) & 0.1620 & 0.5499 & 0.2991 & 0.4204 & 0.4909  & 1.2176 & 0.2892 \\
%         GPT2-5FT (5e-4, 0.01) & 0.0857 & 0.4627 & 0.2133 & 0.3083 &  0.4991& 0.4002  & 0.373 \\
%         GPT2-5FT-Plugin (5e-5, 10) & 0.1582 & 0.5503 & 0.2957 & 0.4176 & 0.5006 & 1.1348 & 0.2951 \\
%         \\ \\
%         GPT2-Plugin (5e-4, 10) - IC 1 & 0.1439 & 0.4780 & 0.2387 & 0.3619 & 0.4355 & 0.8128 & 0.3036 \\
%         GPT2-Plugin (5e-4, 10) - IC 3 & 0.0859 & 0.3983 & 0.1765 & 0.2740 & 0.4591 & 0.1998 & 0.4033 \\
%         GPT2-1FT-Plugin (5e-5, 10) - IC 1 & 0.1324 & 0.4693  & 0.2346 & 0.3681 & 0.4123 & 0.8613 & 0.2879 \\
%         GPT2-1FT-Plugin (5e-5, 10) - IC 3 & 0.1408 & 0.4885 & 0.2382 & 0.3650 & 0.4559 & 0.7001 & 0.3636 \\
%         \\ \\ 
%         NewModel-GPT2-1layer (5e-4,0.01) (baseline) & 0.1119  & 0.3329 & 0.1477 & 0.2779 & 0.2702 & 0.4348 & 0.3024 \\
%         WC-GPT2-1layer (5e-4, 0.1, 0.25) (baseline) &  0.0483 & 0.3121 & 0.1279 & 0.2436 & 0.2087 & 0.1070 & 0.2903 \\
%         IC-GPT2-1 (baseline) & 0.0317  & 0.3388 & 0.1318 & 0.2346 & 0.2876 & 0.0732  & 0.2715 \\
%         IC-GPT2-3 (baseline) & 0.0461  & 0.3388 & 0.1378 & 0.2291 & 0.3408 &  0.0748 & 0.3283 \\

%         \hline
%     \end{tabular}
%     \caption{Performance on Web NLG dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt. Plugin trained on val and hyperval.}
%     \label{tab:webnlg_gpt2final}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model (epoch, lr, wd) & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST \\ 
%         \hline
%         LLaMA (zero shot) & 0.1453 & 0.5278 & 0.3030 & 0.3982 & 0.4314 & 0.6991 &  0.2684 \\
%         WC-LLaMA-1-Att-256-hs-1024-is (5e-4, 0.1, 0.25) (baseline) & 0.1444  & 0.5277 & 0.3029 & 0.3979 & 0.4318 & 0.7142  & 0.2703   \\
%         NewM-LLaMA-1-Att-256-hs-1024-is (5e-3, 1)  (baseline) &  0.1124 & 0.3339 & 0.1512 & 0.2769 &  0.2729& 0.4274  & 0.2861   \\
%         IC-LLaMA-1 (baseline) & 0.1333  & 0.4423 & 0.2611 & 0.3496 & 0.4952 & 0.3265  &  0.2568  \\
%         IC-LLaMA-3 (baseline) & 0.0496  &  0.1177&  0.0719 & 0.0919 & 0.1341 &0.0837   &  0.2321  \\
%         LLaMA-Plugin-1-Att-256-hs-1024-is (5e-4, 0.01) &  0.2560 & 0.6269 &  0.3821& 0.4925 &  0.5339& 1.5925  & 0.2561   \\
%         LLaMA-Plugin-1-Att-256-hs-1024-is (5e-4, 0.01) - IC 1 &  0.2166 & 0.5944 & 0.3706 & 0.4667 & 0.5651 & 1.5719  &  0.2462  \\
%         LLaMA-Plugin-1-Att-256-hs-1024-is (5e-4, 0.01) - IC 3 &  0.0818 & 0.2685 & 0.1697 & 0.2099 & 0.2629 & 0.8070  &  0.2169  \\
%         \hline
%     \end{tabular}
%     \caption{Performance on Web NLG dataset with input-output pre-concatenated (passing, mr, hr) with final good prompt for LLaMA (no fine-tuning). Pluging trained on val and hyperval. \todog{Need to check in-context cases, many predictions are null for LLaMA model.}}
%     \label{tab:webnlg_LLaMA_w_prompt}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2XL (zero shot) &0.0317 & 0.2992 &  0.1321 & 0.2417 & 0.1969 & 0.0491 & 0.1826\\
%         GPT2XL-Plugin-1-layer (5e-4, 10)  & 0.1606 & 0.4578 & 0.2507 &0.3716 & 0.3778 &0.8472 & 0.2528\\
%         \\ \\
%         NewM-GPT2XL-1-layer (5e-3, 1) & 0.1142 & 0.3294 &0.1464 & 0.2729 & 0.2700& 0.4020& 0.3095\\
%         WC-GPT2XL-1-layer (5e-4, 0.1, 0.25) & 0.0459 & 0.3068 & 0.1397 & 0.2520 & 0.1828 & 0.1463 & 0.1810\\
%         IC-GPT2XL-1  & 0.0510 & 0.3223 &0.1526 & 0.2562 & 0.2591 & 0.1336 & 0.2235 \\
%         IC-GPT2XL-3  & 0.0744 &0.3383 & 0.1682 & 0.2651 & 0.3071 & 0.1675 & 0.255\\
%         \hline
%     \end{tabular}
%     \caption{Performance on Web NLG dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt for GPT2-XL. Plugin trained on val and hyperval.}
%     \label{tab:webnlg_gpt2xlfinal}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2-M & 0.0130 & 0.2250  & 0.0409 &  0.1540 & 0.2848 & 0.0000 & 0.3686 \\
%         GPT2-Plugin (5e-4, 10) & 0.1567 & 0.4549  & 0.1937& 0.3988  &0.4187  & 0.5759 & 0.6959\\
%         GPT2-1FT (5e-4, 0.01) & 0.0226 & 0.2752 & 0.0662 & 0.1853 & 0.3438 & 0.0100 & 0.5038\\
%         GPT2-1FT-Plugin (5e-5, 10) &0.1618 & 0.4710& 0.1987 & 0.4087 & 0.4358 & 0.6575 & 0.6513 \\
%         GPT2-2FT (5e-4, 0.01) & 0.1246 & 0.4501& 0.1878 & 0.3854 &0.4474  & 0.5848 & 0.6925\\
%         GPT2-2FT-Plugin (5e-4, 10) & 0.1932 & 0.5157 & 0.2364 & 0.4453 & 0.4839 & 0.8256 & 0.5911 \\
%         GPT2-5FT (5e-4, 0.01) & 0.1557 & 0.4785 &0.2040 &0.4106 & 0.4746 & 0.6826& 0.7257\\
%         GPT2-5FT-Plugin (5e-4, 10) & 0.2335 &0.5382 &0.2627 & 0.4692 & 0.5168 & 0.8852 & 0.5541\\
%         \\ \\
%         NM-GPT2-1layer (5e-3, 1) (baseline) & 0.1260 & 0.4106 & 0.1683 & 0.3740 & 0.3600& 0.4570& 0.7113\\
%         WC-GPT2-1layer (5e-4, 10, 0.25) (baseline) & 0.0417 & 0.3464 & 0.1013 & 0.2951 & 0.2671 & 0.2148 & 0.4452\\
%         GPT2-IC-1  &0.0114 &0.217 &0.0362 & 0.1388& 0.2771& 0.0222& 0.3504\\
%         GPT2-IC-3  &0.0587 &0.367 &0.1248 & 0.2680& 0.4079& 0.1366& 0.5340\\
%         \hline
%     \end{tabular}
%     \caption{Performance on Common Generation dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt. Plugin trained on val and hyperval. Train contains man data, validation and test contain all genders.}
%     \label{tab:cg_gpt2final}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         LLaMA (zero shot) &  0.0643 & 0.3643  &0.1684 & 0.3063  & 0.4157 & 0.3155  & 0.3730\\
%         LLaMA-Plugin-1-Att-256-hs-1024-is (5e-3, 1) &0.2611  &  0.5860 & 0.3016&  0.5008 & 0.5581 & 1.0299 & 0.5153\\
%         \\ \\
%         NewM-LLaMA-1-Att-256-hs-1024-is (5e-3, 1)&0.0084 &0.3048 &0.0589 &0.2891&0.1689 & 0.0096&0.2921\\
%         WC-LLaMA-1-Att-256-hs-1024-is (5e-4, 0.1, 0.25) & 0.2084 & 0.4615 & 0.2380&0.3969 & 0.4437&0.7460 &0.4125\\
%         IC-LLaMA-1 &0.0495 & 0.2466& 0.1025& 0.2094&0.3611 & 0.0003&0.2794\\
%         IC-LLaMA-3 &0.0615 & 0.2697& 0.1258& 0.2469& 0.3722&0.0005 & 0.3059\\
%         \hline
%     \end{tabular}
%     \caption{Performance on Common Generation dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt for LLaMA. Plugin trained on val and hyperval. Train contains man data, validation and test contain all genders.}
%     \label{tab:cg_LLaMAfinal}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
%         \hline
%         GPT2XL (zero shot) & 0.0204 & 0.2247 & 0.0567 &0.1918 & 0.2779 & 0.0001 & 0.2890\\
%         GPT2XL-Plugin-1-layer (5e-3, 10)  & 0.1729 &0.4815 &0.2230 & 0.4223 & 0.4608 & 0.7136 & 0.6992\\
%         \\ \\
%         NewM-GPT2XL-1-layer (5e-3, 1)  & 0.1227 & 0.4143 &0.1596  & 0.3775 & 0.3397 & 0.3897 & 0.6881\\
%         WC-GPT2XL-1-layer (5e-4, 1, 0.25) & 0.0247 & 0.2226 & 0.0847 & 0.1987 & 0.1718& 0.1975& 0.1881\\
%         IC-GPT2XL-1  & 0.0210 &0.2369 &0.0562 & 0.1887 & 0.2471 & 0.0006 &0.2519 \\
%         IC-GPT2XL-3  & 0.0323 &0.2347 & 0.0808 & 0.1991 & 0.3004 & 0.0002& 0.3243\\
%         \hline
%     \end{tabular}
%     \caption{Performance on Common Generation dataset with input-output pre-concatenated (passing, mr, hr) with good final prompt for GPT2-XL. Plugin trained on val and hyperval. Train contains man data, validation and test contain all genders.}
%     \label{tab:cg_gpt2xlfinal}
% \end{table*}