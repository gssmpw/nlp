
@InProceedings{pmlr-v80-kerdreux18a,
  title = 	 {Frank-{W}olfe with Subsampling Oracle},
  author =       {Kerdreux, Thomas and Pedregosa, Fabian and d'Aspremont, Alexandre},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2591--2600},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},  
}

@article{ZhaoRenbo2023AotF,
title = {Analysis of the Frank–Wolfe method for convex composite optimization involving a logarithmically-homogeneous barrier},
author = {Zhao, Renbo and Freund, Robert M.},
doi = {10.1007/s10107-022-01820-9},
issn = {0025-5610},
journal = {Mathematical programming},
keywords = {Calculus of Variations and Optimal Control; Optimization;Combinatorics;Full Length Paper;Mathematical and Computational Physics;Mathematical Methods in Physics;Mathematics;Mathematics and Statistics;Mathematics of Computing;Numerical Analysis;Theoretical},
language = {eng},
number = {1-2},
pages = {123 - 163},
publisher = {Springer Berlin Heidelberg},
volume = {199},
year = {2023},
}





@inproceedings{mahmood2014weighted,
  title={Weighted importance sampling for off-policy learning with linear function approximation.},
  author={Mahmood, Ashique Rupam and Van Hasselt, Hado and Sutton, Richard S},
  booktitle={NIPS},
  pages={3014--3022},
  year={2014}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016},
  organization={PMLR}
}

@inproceedings{hanna2017data,
  title={Data-efficient policy evaluation through behavior policy search},
  author={Hanna, Josiah P and Thomas, Philip S and Stone, Peter and Niekum, Scott},
  booktitle={International Conference on Machine Learning},
  pages={1394--1403},
  year={2017},
  organization={PMLR}
}

@inproceedings{hanna2019importance,
  title={Importance sampling policy evaluation with an estimated behavior policy},
  author={Hanna, Josiah and Niekum, Scott and Stone, Peter},
  booktitle={International Conference on Machine Learning},
  pages={2605--2613},
  year={2019},
  organization={PMLR}
}

@inproceedings{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle={International Conference on Machine Learning},
  pages={3703--3712},
  year={2019},
  organization={PMLR}
}

@inproceedings{antos2008active,
  title={Active learning in multi-armed bandits},
  author={Antos, Andr{\'a}s and Grover, Varun and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={287--302},
  year={2008},
  organization={Springer}
}

@inproceedings{carpentier2011finite,
  title={Finite-time analysis of stratified sampling for monte carlo},
  author={Carpentier, Alexandra and Munos, R{\'e}mi},
  booktitle={NIPS-Twenty-Fifth Annual Conference on Neural Information Processing Systems},
  year={2011}
}

@article{talebi2019learning,
  title={Learning Multiple Markov Chains via Adaptive Allocation},
  author={Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  journal={arXiv preprint arXiv:1905.11128},
  year={2019}
}

@inproceedings{riquelme2017active,
  title={Active learning for accurate estimation of linear models},
  author={Riquelme, Carlos and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
  booktitle={International Conference on Machine Learning},
  pages={2931--2939},
  year={2017},
  organization={PMLR}
}

@article{wagenmaker2021beyond,
  title={Beyond No Regret: Instance-Dependent PAC Reinforcement Learning},
  author={Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2108.02717},
  year={2021}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}


@inproceedings{carpentier2012minimax,
  title={Minimax number of strata for online stratified sampling given noisy samples},
  author={Carpentier, Alexandra and Munos, R{\'e}mi},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={229--244},
  year={2012},
  organization={Springer}
}

@article{carpentier2015adaptive,
  title={Adaptive strategy for stratified Monte Carlo sampling.},
  author={Carpentier, Alexandra and Munos, Remi and Antos, Andr{\'a}s},
  journal={J. Mach. Learn. Res.},
  volume={16},
  pages={2231--2271},
  year={2015}
}

@inproceedings{neufeld2014adaptive,
  title={Adaptive Monte Carlo via bandit allocation},
  author={Neufeld, James and Gyorgy, Andras and Szepesv{\'a}ri, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={1944--1952},
  year={2014},
  organization={PMLR}
}

%%%% Exploration in RL

@incollection{barto2013intrinsic,
    title={Intrinsic motivation and reinforcement learning},
    author={Barto, Andrew G},
    booktitle={Intrinsically motivated learning in natural and artificial systems},
    pages={17--47},
    year={2013},
    publisher={Springer}
}

@inproceedings{bellemare2016unifying,
    title={Unifying count-based exploration and intrinsic motivation},
    author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
    booktitle={Advances in Neural Information Processing Systems},
    pages={1471--1479},
    year={2016}
}

@inproceedings{pathak2017curiosity,
    title={Curiosity-driven exploration by self-supervised prediction},
    author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
    booktitle={International Conference on Machine Learning},
    volume={2017},
    year={2017}
}

@article{burda2018exploration,
    title={Exploration by random network distillation},
    author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
    journal={arXiv preprint arXiv:1810.12894},
    year={2018}
}

@inproceedings{schmidhuber1991possibility,
    title={A possibility for implementing curiosity and boredom in model-building neural controllers},
    author={Schmidhuber, J{\"u}rgen},
    booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
    pages={222--227},
    year={1991}
}

%%%%% Policy evaluation

@book{rubinstein2013cross,
  title={The cross-entropy method: a unified approach to combinatorial optimization, {M}onte {C}arlo simulation and machine learning},
  author={Rubinstein, Reuven Y. and Kroese, Dirk P.},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{rasmussen2003bayesian,
  title={Bayesian monte carlo},
  author={Rasmussen, Carl Edward and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  pages={505--512},
  year={2003},
  publisher={MIT; 1998}
}

@article{ohagan1987monte,
  title={Monte Carlo is fundamentally unsound},
  author={O'Hagan, Anthony},
  journal={The Statistician},
  pages={247--249},
  year={1987},
  publisher={JSTOR}
}

@inproceedings{frank2008reinforcement,
  title={Reinforcement learning in the presence of rare events},
  author={Frank, Jordan and Mannor, Shie and Precup, Doina},
  booktitle={Proceedings of the 25th International Conference on Machine Learning},
  pages={336--343},
  year={2008},
  organization={ACM},
  annote={Assume known rare-events and a simulator which directly controls the probability of rare-events.}
}

@inproceedings{ciosek2017offer,
  title={O{F}{F}{E}{R}: Off-Environment Reinforcement Learning},
  author={Ciosek, Kamil and Whiteson, Shimon},
  year={2017},
  booktitle={Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)}
}

@article{bouchard2016online,
  title={Online learning to sample},
  author={Bouchard, Guillaume and Trouillon, Th{\'e}o and Perez, Julien and Gaidon, Adrien},
  journal={arXiv preprint arXiv:1506.09016},
  year={2016}
}

@article{zanette2019almost,
  title={Almost horizon-free structure-aware best policy identification with a generative model},
  author={Zanette, Andrea and Kochenderfer, Mykel J and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@book{resnick2019probability,
  title={A probability path},
  author={Resnick, Sidney},
  year={2019},
  publisher={Springer}
}

@book{massart2007concentration,
  title={Concentration inequalities and model selection: Ecole d'Et{\'e} de Probabilit{\'e}s de Saint-Flour XXXIII-2003},
  author={Massart, Pascal},
  year={2007},
  publisher={Springer}
}

@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  year={2019}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@article{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{o2018uncertainty,
  title={The uncertainty bellman equation and exploration},
  author={O’Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
  booktitle={International Conference on Machine Learning},
  pages={3836--3845},
  year={2018}
}

@article{audibert2009exploration,
  title={Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
  author={Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={410},
  number={19},
  pages={1876--1902},
  year={2009},
  publisher={Elsevier}
}

@article{maurer2009empirical,
  title={Empirical bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}

@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}

@inproceedings{pavse2020reducing,
  title={Reducing sampling error in batch temporal difference learning},
  author={Pavse, Brahma and Durugkar, Ishan and Hanna, Josiah and Stone, Peter},
  booktitle={International Conference on Machine Learning},
  pages={7543--7552},
  year={2020},
  organization={PMLR}
}

@article{ma2021minimax,
  title={Minimax Off-Policy Evaluation for Multi-Armed Bandits},
  author={Ma, Cong and Zhu, Banghua and Jiao, Jiantao and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2101.07781},
  year={2021}
}


@article{mukherjee2022revar,
  title={ReVar: Strengthening Policy Evaluation via Reduced Variance Sampling},
  author={Mukherjee, Subhojyoti and Hanna, Josiah P and Nowak, Robert},
  journal={arXiv preprint arXiv:2203.04510},
  year={2022}
}


@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}


@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}


@article{wagenmaker2021first,
  title={First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach},
  author={Wagenmaker, Andrew and Chen, Yifang and Simchowitz, Max and Du, Simon S and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2112.03432},
  year={2021}
}


@inproceedings{katz2021improved,
  title={Improved algorithms for agnostic pool-based active classification},
  author={Katz-Samuels, Julian and Zhang, Jifan and Jain, Lalit and Jamieson, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={5334--5344},
  year={2021},
  organization={PMLR}
}


@article{katz2020empirical,
  title={An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits},
  author={Katz-Samuels, Julian and Jain, Lalit and Jamieson, Kevin G and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10371--10382},
  year={2020}
}

@article{mukherjee2020generalized,
  title={Generalized Chernoff Sampling for Active Testing, Active Regression and Structured Bandit Algorithms},
  author={Mukherjee, Subhojyoti and Tripathy, Ardhendu and Nowak, Robert},
  journal={arXiv preprint arXiv:2012.08073},
  year={2020}
}

@article{mason2021nearly,
  title={Nearly Optimal Algorithms for Level Set Estimation},
  author={Mason, Blake and Camilleri, Romain and Mukherjee, Subhojyoti and Jamieson, Kevin and Nowak, Robert and Jain, Lalit},
  journal={arXiv preprint arXiv:2111.01768},
  year={2021}
}


@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}


@article{kiefer1960equivalence,
  title={The equivalence of two extremum problems},
  author={Kiefer, Jack and Wolfowitz, Jacob},
  journal={Canadian Journal of Mathematics},
  volume={12},
  pages={363--366},
  year={1960},
  publisher={Cambridge University Press}
}

@book{pukelsheim2006optimal,
  title={Optimal design of experiments},
  author={Pukelsheim, Friedrich},
  year={2006},
  publisher={SIAM}
}


@article{rigollet2015high,
  title={High dimensional statistics},
  author={Rigollet, Phillippe and H{\"u}tter, Jan-Christian},
  journal={Lecture notes for course 18S997},
  volume={813},
  number={814},
  pages={46},
  year={2015}
}


@misc{greene2002000,
  title={000. Econometric analysis},
  author={Greene, William H},
  year={2002},
  publisher={Prentice-Hall, Inc}
}

@inproceedings{chaudhuri2017active,
  title={Active heteroscedastic regression},
  author={Chaudhuri, Kamalika and Jain, Prateek and Natarajan, Nagarajan},
  booktitle={International Conference on Machine Learning},
  pages={694--702},
  year={2017},
  organization={PMLR}
}


@inproceedings{strens2000bayesian,
	title = {A {Bayesian} framework for reinforcement learning},
	abstract = {The reinforcement learning problem can be decomposed into two parallel types of inference: (i) estimating the parameters of a model for the underlying process; (ii) determining behavior which maximizes return under the estimated model. Following Dearden, Friedman and Andre (1999), it is proposed that the learning process estimates online the full posterior distribution over models. To determine behavior, a hypothesis is sampled from this distribution and the greedy policy with respect to the hypothesis is obtained by dynamic programming. By using a different hypothesis for each trial appropriate exploratory and exploitative behavior is obtained. This Bayesian method always converges to the optimal policy for a stationary process with discrete states. 1.},
	booktitle = {In {Proceedings} of the {Seventeenth} {International} {Conference} on {Machine} {Learning}},
	publisher = {ICML},
	author = {Strens, Malcolm},
	year = {2000},
	pages = {943--950},
}

@article{auer2002finite-time,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	volume = {47},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1013689704352},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	language = {en},
	number = {2},
	urldate = {2022-05-13},
	journal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
	month = may,
	year = {2002},
	keywords = {adaptive allocation rules, bandit problems, finite horizon regret},
	pages = {235--256},
}

@article{lai1985asymptotically,
	title = {Asymptotically efficient adaptive allocation rules},
	volume = {6},
	issn = {0196-8858},
	url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
	doi = {10.1016/0196-8858(85)90002-8},
	abstract = {Animals, humans, and organizations are known to adjust how (much) they explore complex environments that exceed their information processing capacity, rather than relentlessly search for the optimal action. The adjusted depth of exploration is supposed to depend on the aspiration level internal to the agent. This action selection tendency is known as satisficing. The Risk-sensitive Satisficing (RS) model implements satisficing in the reinforcement learning framework through conversion of action values into gains (or losses) relative to the aspiration level. The risk-sensitive evaluation of action values by RS has been shown to be effective in reinforcement learning. In this paper, first we analyze RS in comparison with UCB and Thompson sampling algorithms. We also show that RS shows differential risk-attitudes considering the risks. Then we propose the Softsatisficing policy that is a stochastic equivalent of RS and further analyze the exploratory behavior of risk-sensitive satisficing that RS and Softsatisficing implement. We emphasize that Softsatisficing has the potential of modeling risk-sensitive foraging and other decision-making behaviors by humans, animals, and organizations.
In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.},
	language = {en},
	number = {1},
	urldate = {2022-05-13},
	journal = {Advances in Applied Mathematics},
	author = {Lai, T. L and Robbins, Herbert},
	month = mar,
	year = {1985},
	pages = {4--22},
}

@article{thompson1933likelihood,
	title = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
	volume = {25},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/25.3-4.285},
	doi = {10.1093/biomet/25.3-4.285},
	number = {3-4},
	urldate = {2022-05-13},
	journal = {Biometrika},
	author = {Thompson, William R.},
	month = dec,
	year = {1933},
	pages = {285--294},
}


@article{hanna2017data-efficient,
	title = {Data-{Efficient} {Policy} {Evaluation} {Through} {Behavior} {Policy} {Search}},
	url = {http://arxiv.org/abs/1706.03469},
	abstract = {We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic expression for the optimal behavior policy --- the behavior policy that minimizes the mean squared error of the resulting estimates. Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error. We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.},
	urldate = {2022-02-18},
	journal = {arXiv:1706.03469 [cs]},
	author = {Hanna, Josiah P. and Thomas, Philip S. and Stone, Peter and Niekum, Scott},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03469},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ICML 2017; Extended version; 15 pages},
}

@article{oosterhuis2020taking,
	title = {Taking the {Counterfactual} {Online}: {Efficient} and {Unbiased} {Online} {Evaluation} for {Ranking}},
	shorttitle = {Taking the {Counterfactual} {Online}},
	url = {http://arxiv.org/abs/2007.12719},
	doi = {10.1145/3409256.3409820},
	abstract = {Counterfactual evaluation can estimate Click-Through-Rate (CTR) differences between ranking systems based on historical interaction data, while mitigating the effect of position bias and item-selection bias. We introduce the novel Logging-Policy Optimization Algorithm (LogOpt), which optimizes the policy for logging data so that the counterfactual estimate has minimal variance. As minimizing variance leads to faster convergence, LogOpt increases the data-efficiency of counterfactual estimation. LogOpt turns the counterfactual approach - which is indifferent to the logging policy - into an online approach, where the algorithm decides what rankings to display. We prove that, as an online evaluation method, LogOpt is unbiased w.r.t. position and item-selection bias, unlike existing interleaving methods. Furthermore, we perform large-scale experiments by simulating comparisons between thousands of rankers. Our results show that while interleaving methods make systematic errors, LogOpt is as efficient as interleaving without being biased.},
	urldate = {2022-03-14},
	journal = {Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval},
	author = {Oosterhuis, Harrie and de Rijke, Maarten},
	month = sep,
	year = {2020},
	note = {arXiv: 2007.12719},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {137--144},
	annote = {Comment: ICTIR 2020},
	annote = {Optimize the logging policy for counterfactual evaluation of ranking policies. Find policy that gives minimal variance evaluation.},
}

@article{tucker2022variance-optimal,
	title = {Variance-{Optimal} {Augmentation} {Logging} for {Counterfactual} {Evaluation} in {Contextual} {Bandits}},
	url = {http://arxiv.org/abs/2202.01721},
	abstract = {Methods for offline A/B testing and counterfactual learning are seeing rapid adoption in search and recommender systems, since they allow efficient reuse of existing log data. However, there are fundamental limits to using existing log data alone, since the counterfactual estimators that are commonly used in these methods can have large bias and large variance when the logging policy is very different from the target policy being evaluated. To overcome this limitation, we explore the question of how to design data-gathering policies that most effectively augment an existing dataset of bandit feedback with additional observations for both learning and evaluation. To this effect, this paper introduces Minimum Variance Augmentation Logging (MVAL), a method for constructing logging policies that minimize the variance of the downstream evaluation or learning problem. We explore multiple approaches to computing MVAL policies efficiently, and find that they can be substantially more effective in decreasing the variance of an estimator than na{\textbackslash}"ive approaches.},
	urldate = {2022-03-14},
	journal = {arXiv:2202.01721 [cs]},
	author = {Tucker, Aaron David and Joachims, Thorsten},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.01721},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	annote = {Comment: 19 pages, 4 figures, in submission},
	annote = {Contextual bandit setting. Study how to augment logged data for minimal variance OPE using the balanced IPS estimator (i.e., denominator is mixture between logging and augmentatio policy).},
}

@article{uesato2018rigorous,
	title = {Rigorous {Agent} {Evaluation}: {An} {Adversarial} {Approach} to {Uncover} {Catastrophic} {Failures}},
	shorttitle = {Rigorous {Agent} {Evaluation}},
	url = {http://arxiv.org/abs/1812.01647},
	abstract = {This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.},
	urldate = {2022-03-17},
	journal = {arXiv:1812.01647 [cs, stat]},
	author = {Uesato, Jonathan and Kumar, Ananya and Szepesvari, Csaba and Erez, Tom and Ruderman, Avraham and Anderson, Keith and Krishmamurthy and Dvijotham and Heess, Nicolas and Kohli, Pushmeet},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01647},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
}


@article{rusmevichientong2010linearly,
  title={Linearly parameterized bandits},
  author={Rusmevichientong, Paat and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={35},
  number={2},
  pages={395--411},
  year={2010},
  publisher={INFORMS}
}


@article{jamieson2022interactive,
  title={Interactive Machine Learning},
  author={Jamieson, Kevin and Jain, Lalit},
  year={2022}
}


@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{agrawal2012analysis,
  title={Analysis of thompson sampling for the multi-armed bandit problem},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={Conference on learning theory},
  pages={39--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}


@inproceedings{kirschner2018information,
  title={Information directed sampling and bandits with heteroscedastic noise},
  author={Kirschner, Johannes and Krause, Andreas},
  booktitle={Conference On Learning Theory},
  pages={358--384},
  year={2018},
  organization={PMLR}
}


@article{whittle1958multivariate,
  title={A multivariate generalization of Tchebichev's inequality},
  author={Whittle, P},
  journal={The Quarterly Journal of Mathematics},
  volume={9},
  number={1},
  pages={232--240},
  year={1958},
  publisher={Oxford University Press}
}


@article{fang1994inequalities,
  title={Inequalities for the trace of matrix product},
  author={Fang, Yuguang and Loparo, Kenneth A and Feng, Xiangbo},
  journal={IEEE Transactions on Automatic Control},
  volume={39},
  number={12},
  pages={2489--2490},
  year={1994},
  publisher={IEEE}
}

@book{fedorov2013theory,
  title={Theory of optimal experiments},
  author={Fedorov, Valerii Vadimovich},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{fontaine2021online,
  title={Online a-optimal design and active linear regression},
  author={Fontaine, Xavier and Perrault, Pierre and Valko, Michal and Perchet, Vianney},
  booktitle={International Conference on Machine Learning},
  pages={3374--3383},
  year={2021},
  organization={PMLR}
}


@inproceedings{mukherjee2022chernoff,
  title={Chernoff Sampling for Active Testing and Extension to Active Regression},
  author={Mukherjee, Subhojyoti and Tripathy, Ardhendu S and Nowak, Robert},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7384--7432},
  year={2022},
  organization={PMLR}
}


@article{petersen2008matrix,
  title={The matrix cookbook},
  author={Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  journal={Technical University of Denmark},
  volume={7},
  number={15},
  pages={510},
  year={2008}
}

@article{berthet2017fast,
  title={Fast rates for bandit optimization with upper-confidence Frank-Wolfe},
  author={Berthet, Quentin and Perchet, Vianney},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{lacoste2013affine,
  title={An affine invariant linear convergence analysis for Frank-Wolfe algorithms},
  author={Lacoste-Julien, Simon and Jaggi, Martin},
  journal={arXiv preprint arXiv:1312.7864},
  year={2013}
}

@article{zhao2022bandit,
  title={Bandit Learning with General Function Classes: Heteroscedastic Noise and Variance-dependent Regret Bounds},
  author={Zhao, Heyang and Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2202.13603},
  year={2022}
}



@book{tsybakov2008introduction,
  title={Introduction to nonparametric estimation},
  author={Tsybakov, Alexandre B},
  year={2008},
  publisher={Springer Science \& Business Media}
}


@inproceedings{huang2017structured,
  author    = {Ruitong Huang and
               Mohammad M. Ajallooeian and
               Csaba Szepesv{\'{a}}ri and
               Martin M{\"{u}}ller},
  editor    = {Steve Hanneke and
               Lev Reyzin},
  title     = {Structured Best Arm Identification with Fixed Confidence},
  booktitle = {International Conference on Algorithmic Learning Theory, {ALT} 2017,
               15-17 October 2017, Kyoto University, Kyoto, Japan},
  series    = {Proceedings of Machine Learning Research},
  volume    = {76},
  pages     = {593--616},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v76/huang17a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:23 +0200},
  biburl    = {https://dblp.org/rec/conf/alt/HuangAS017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{wan2022safe,
  title={Safe Exploration for Efficient Policy Evaluation and Comparison},
  author={Wan, Runzhe and Kveton, Branislav and Song, Rui},
  journal={arXiv preprint arXiv:2202.13234},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%% OPE Motivations %%%%%%%%%%%%%%%


@article{bottou2013counterfactual,
  title={Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising.},
  author={Bottou, L{\'e}on and Peters, Jonas and Qui{\~n}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={11},
  year={2013}
}

@inproceedings{li2011unbiased,
  title={Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms},
  author={Li, Lihong and Chu, Wei and Langford, John and Wang, Xuanhui},
  booktitle={Proceedings of the fourth ACM international conference on Web search and data mining},
  pages={297--306},
  year={2011}
}


@article{zhou2017residual,
  title={Residual weighted learning for estimating individualized treatment rules},
  author={Zhou, Xin and Mayer-Hamblett, Nicole and Khan, Umer and Kosorok, Michael R},
  journal={Journal of the American Statistical Association},
  volume={112},
  number={517},
  pages={169--187},
  year={2017},
  publisher={Taylor \& Francis}
}


@article{dudik2014doubly,
  title={Doubly robust policy evaluation and optimization},
  author={Dud{\'\i}k, Miroslav and Erhan, Dumitru and Langford, John and Li, Lihong},
  year={2014}
}

@inproceedings{li2015toward,
  title={Toward minimax off-policy value estimation},
  author={Li, Lihong and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  booktitle={Artificial Intelligence and Statistics},
  pages={608--616},
  year={2015},
  organization={PMLR}
}


@article{swaminathan2017off,
  title={Off-policy evaluation for slate recommendation},
  author={Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dudik, Miro and Langford, John and Jose, Damien and Zitouni, Imed},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{wang2017optimal,
  title={Optimal and adaptive off-policy evaluation in contextual bandits},
  author={Wang, Yu-Xiang and Agarwal, Alekh and Dud{\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={3589--3597},
  year={2017},
  organization={PMLR}
}

@inproceedings{su2020doubly,
  title={Doubly robust off-policy evaluation with shrinkage},
  author={Su, Yi and Dimakopoulou, Maria and Krishnamurthy, Akshay and Dud{\'\i}k, Miroslav},
  booktitle={International Conference on Machine Learning},
  pages={9167--9176},
  year={2020},
  organization={PMLR}
}

@inproceedings{kallus2021optimal,
  title={Optimal off-policy evaluation from multiple logging policies},
  author={Kallus, Nathan and Saito, Yuta and Uehara, Masatoshi},
  booktitle={International Conference on Machine Learning},
  pages={5247--5256},
  year={2021},
  organization={PMLR}
}


@article{cai2021deep,
  title={Deep jump learning for off-policy evaluation in continuous treatment settings},
  author={Cai, Hengrui and Shi, Chengchun and Song, Rui and Lu, Wenbin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15285--15300},
  year={2021}
}


@inproceedings{sachdeva2020off,
  title={Off-policy bandits with deficient support},
  author={Sachdeva, Noveen and Su, Yi and Joachims, Thorsten},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={965--975},
  year={2020}
}

@article{tran2021combining,
  title={Combining Online Learning and Offline Learning for Contextual Bandits with Deficient Support},
  author={Tran-The, Hung and Gupta, Sunil and Nguyen-Tang, Thanh and Rana, Santu and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:2107.11533},
  year={2021}
}

@inproceedings{wu2016conservative,
  title={Conservative bandits},
  author={Wu, Yifan and Shariff, Roshan and Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning},
  pages={1254--1262},
  year={2016},
  organization={PMLR}
}


@InProceedings{pmlr-v151-zhu22a,
  title = 	 { Safe Optimal Design with Applications in Off-Policy Learning },
  author =       {Zhu, Ruihao and Kveton, Branislav},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2436--2447},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/zhu22a/zhu22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/zhu22a.html},
  abstract = 	 { Motivated by practical needs in online experimentation and off-policy learning, we study the problem of safe optimal design, where we develop a data logging policy that efficiently explores while achieving competitive rewards with a baseline production policy. We first show, perhaps surprisingly, that a common practice of mixing the production policy with uniform exploration, despite being safe, is sub-optimal in maximizing information gain. Then we propose a safe optimal logging policy for the case when no side information about the actions’ expected rewards is available. We improve upon this design by considering side information and also extend both approaches to a large number of actions with a linear reward model. We analyze how our data logging policies impact errors in off-policy learning. Finally, we empirically validate the benefit of our designs by conducting extensive experiments. }
}


@article{zhang2021improved,
  title={Improved variance-aware confidence sets for linear bandits and linear mixture mdp},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4342--4355},
  year={2021}
}


@article{zhou2022computationally,
  title={Computationally efficient horizon-free reinforcement learning for linear mixture mdps},
  author={Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2205.11507},
  year={2022}
}


@misc{ movielens,
  author = "Shyong Lam and Jon Herlocker",
  title = "{MovieLens Dataset}",
  howpublished = "http://grouplens.org/datasets/movielens/",
  year = "2016"
}



@article{cortez2009modeling,
  title={Modeling wine preferences by data mining from physicochemical properties},
  author={Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  journal={Decision support systems},
  volume={47},
  number={4},
  pages={547--553},
  year={2009},
  publisher={Elsevier}
}

@article{de2008field,
  title={On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario},
  author={De Vito, Saverio and Massera, Ettore and Piga, Marco and Martinotto, Luca and Di Francia, Girolamo},
  journal={Sensors and Actuators B: Chemical},
  volume={129},
  number={2},
  pages={750--757},
  year={2008},
  publisher={Elsevier}
}

@article{antos2010active,
  title={Active learning in heteroscedastic noise},
  author={Antos, Andr{\'a}s and Grover, Varun and Szepesv{\'a}ri, Csaba},
  journal={Theoretical Computer Science},
  volume={411},
  number={29-30},
  pages={2712--2728},
  year={2010},
  publisher={Elsevier}
}

@article{khosla2022neural,
  title={Neural Active Learning on Heteroskedastic Distributions},
  author={Khosla, Savya and Whye, Chew Kin and Ash, Jordan T and Zhang, Cyril and Kawaguchi, Kenji and Lamb, Alex},
  journal={arXiv preprint arXiv:2211.00928},
  year={2022}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@article{zhu2021safe,
  title={Safe Data Collection for Offline and Online Policy Learning},
  author={Zhu, Ruihao and Kveton, Branislav},
  journal={arXiv preprint arXiv:2111.04835},
  year={2021}
}

@article{kohavi2017online,
  title={Online Controlled Experiments and A/B Testing.},
  author={Kohavi, Ron and Longbotham, Roger},
  journal={Encyclopedia of machine learning and data mining},
  volume={7},
  number={8},
  pages={922--929},
  year={2017}
}

@misc{jamieson2020some,
  title={Some notes on multi-armed bandits},
  author={Jamieson, Kevin},
  year={2020}
}


%%%%%%%%%%% RLHF theory %%%%%%%%%%%%%

@article{zhu2023principled,
  title={Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
  author={Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11270},
  year={2023}
}


@article{wu2023making,
  title={Making RL with Preference-based Feedback Efficient via Randomization},
  author={Wu, Runzhe and Sun, Wen},
  journal={arXiv preprint arXiv:2310.14554},
  year={2023}
}


%%%%%%%%%%%%% RLHF for LLM %%%%%%%%%

@article{yang2023rlcd,
  title={Rlcd: Reinforcement learning from contrast distillation for language model alignment},
  author={Yang, Kevin and Klein, Dan and Celikyilmaz, Asli and Peng, Nanyun and Tian, Yuandong},
  journal={arXiv preprint arXiv:2307.12950},
  year={2023}
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}


%%%%%%% Duelling RL %%%%%%%

@inproceedings{saha2023dueling,
  title={Dueling RL: Reinforcement Learning with Trajectory Preferences},
  author={Saha, Aadirupa and Pacchiano, Aldo and Lee, Jonathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6263--6289},
  year={2023},
  organization={PMLR}
}

%%%%%%%%%%%%% Pref model %%%%%%%%%%%%

@article{plackett1975analysis,
  title={The analysis of permutations},
  author={Plackett, Robin L},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={24},
  number={2},
  pages={193--202},
  year={1975},
  publisher={Oxford University Press}
}

@book{luce2012individual,
  title={Individual choice behavior: A theoretical analysis},
  author={Luce, R Duncan},
  year={2012},
  publisher={Courier Corporation}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}


%%%%%%%%%%%%% Fixed budget %%%%%%%%%%%
@inproceedings{audibert2010best,
  title={Best arm identification in multi-armed bandits.},
  author={Audibert, Jean-Yves and Bubeck, S{\'e}bastien and Munos, R{\'e}mi},
  booktitle={COLT},
  pages={41--53},
  year={2010}
}

@inproceedings{lalitha2023fixed,
  title={Fixed-budget best-arm identification with heterogeneous reward variances},
  author={Lalitha, Anusha Lalitha and Kalantari, Kousha and Ma, Yifei and Deoras, Anoop and Kveton, Branislav},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1164--1173},
  year={2023},
  organization={PMLR}
}


@article{yang2021minimax,
  title={Minimax Optimal Fixed-Budget Best Arm Identification in Linear Bandits},
  author={Yang, Junwen and Tan, Vincent YF},
  journal={arXiv preprint arXiv:2105.13017},
  year={2021}
}

%%%%%%%%%% Fixed confidence %%%%%%%%%%%%%

@article{even2006action,
  title={Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems.},
  author={Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay and Mahadevan, Sridhar},
  journal={Journal of machine learning research},
  volume={7},
  number={6},
  year={2006}
}

@inproceedings{aziz2018pure,
  title={Pure exploration in infinitely-armed bandit models with fixed-confidence},
  author={Aziz, Maryam and Anderton, Jesse and Kaufmann, Emilie and Aslam, Javed},
  booktitle={Algorithmic Learning Theory},
  pages={3--24},
  year={2018},
  organization={PMLR}
}

@inproceedings{degenne2020gamification,
  title={Gamification of pure exploration for linear bandits},
  author={Degenne, R{\'e}my and M{\'e}nard, Pierre and Shang, Xuedong and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={2432--2442},
  year={2020},
  organization={PMLR}
}

@inproceedings{li2017provably,
  title={Provably optimal algorithms for generalized linear contextual bandits},
  author={Li, Lihong and Lu, Yu and Zhou, Dengyong},
  booktitle={International Conference on Machine Learning},
  pages={2071--2080},
  year={2017},
  organization={PMLR}
}

%%%%%%%%%% Expt %%%%%%%%%%
@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness \& Harmlessness with RLAIF},
    url = {},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}

@inproceedings{INSTRUCTOR,
  title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
  author={Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and  Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A. and  Zettlemoyer, Luke and Yu, Tao},
  url={https://arxiv.org/abs/2212.09741},
  year={2022},
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{ radlinski08learning,
  author = "Filip Radlinski and Robert Kleinberg and Thorsten Joachims",
  title = "Learning Diverse Rankings with Multi-Armed Bandits",
  booktitle = "Proceedings of the 25th International Conference on Machine Learning",
  pages = "784-791",
  year = "2008"
}

@inproceedings{ kveton15cascading,
  author = "Branislav Kveton and Csaba Szepesvari and Zheng Wen and Azin Ashkan",
  title = "Cascading Bandits: Learning to Rank in the Cascade Model",
  booktitle = "Proceedings of the 32nd International Conference on Machine Learning",
  year = "2015"
}

@article{opoku2023randomized,
  title={Randomized Controlled Trials via Reinforcement Learning from Human Feedback},
  author={Opoku-Agyemang, Kweku A},
  year={2023}
}

@article{xiong2023gibbs,
  title={Gibbs Sampling from Human Feedback: A Provable KL-constrained Framework for RLHF},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Zhong, Han and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2312.11456},
  year={2023}
}


@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@inproceedings{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL? A Theoretical Perspective},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{fiez2019sequential,
  title={Sequential experimental design for transductive linear bandits},
  author={Fiez, Tanner and Jain, Lalit and Jamieson, Kevin G and Ratliff, Lillian},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{mukherjee2023speed,
  title={SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits},
  author={Mukherjee, Subhojyoti and Xie, Qiaomin and Hanna, Josiah and Nowak, Robert},
  journal={arXiv preprint arXiv:2301.12357},
  year={2023}
}



%%%%%%%%%%%% Pref learning

@inproceedings{furnkranz2003pairwise,
  title={Pairwise preference learning and ranking},
  author={F{\"u}rnkranz, Johannes and H{\"u}llermeier, Eyke},
  booktitle={European conference on machine learning},
  pages={145--156},
  year={2003},
  organization={Springer}
}


@article{houlsby2011bayesian,
  title={Bayesian active learning for classification and preference learning},
  author={Houlsby, Neil and Husz{\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
  journal={arXiv preprint arXiv:1112.5745},
  year={2011}
}

@misc{glass2006explaining,
  title={Explaining Preference Learning},
  author={Glass, Alyssa},
  year={2006},
  publisher={CiteSeerx}
}

@inproceedings{ermis2020learning,
  title={Learning to rank in the position based model with bandit feedback},
  author={Ermis, Beyza and Ernst, Patrick and Stein, Yannik and Zappella, Giovanni},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={2405--2412},
  year={2020}
}

@inproceedings{kveton2015cascading,
  title={Cascading bandits: Learning to rank in the cascade model},
  author={Kveton, Branislav and Szepesvari, Csaba and Wen, Zheng and Ashkan, Azin},
  booktitle={International conference on machine learning},
  pages={767--776},
  year={2015},
  organization={PMLR}
}

@article{bengs2021preference,
  title={Preference-based online learning with dueling bandits: A survey},
  author={Bengs, Viktor and Busa-Fekete, R{\'o}bert and El Mesaoudi-Paul, Adil and H{\"u}llermeier, Eyke},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={278--385},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{zoghi2017online,
  title={Online learning to rank in stochastic click models},
  author={Zoghi, Masrour and Tunys, Tomas and Ghavamzadeh, Mohammad and Kveton, Branislav and Szepesvari, Csaba and Wen, Zheng},
  booktitle={International conference on machine learning},
  pages={4199--4208},
  year={2017},
  organization={PMLR}
}

@article{yue2012k,
  title={The k-armed dueling bandits problem},
  author={Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten},
  journal={Journal of Computer and System Sciences},
  volume={78},
  number={5},
  pages={1538--1556},
  year={2012},
  publisher={Elsevier}
}

@article{saha2022versatile,
  title={Versatile dueling bandits: Best-of-both-world analyses for online learning from preferences},
  author={Saha, Aadirupa and Gaillard, Pierre},
  journal={arXiv preprint arXiv:2202.06694},
  year={2022}
}

@article{lekang2019simple,
  title={Simple Algorithms for Dueling Bandits},
  author={Lekang, Tyler and Lamperski, Andrew},
  journal={arXiv preprint arXiv:1906.07611},
  year={2019}
}

@article{zhou2023bandit,
  title={Bandit Learning to Rank with Position-Based Click Models: Personalized and Equal Treatments},
  author={Zhou, Tianchen and Liu, Jia and Jiao, Yang and Dong, Chaosheng and Chen, Yetian and Gao, Yan and Sun, Yi},
  journal={arXiv preprint arXiv:2311.04528},
  year={2023}
}

@article{lagree2016multiple,
  title={Multiple-play bandits in the position-based model},
  author={Lagr{\'e}e, Paul and Vernade, Claire and Cappe, Olivier},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}


@article{zong2016cascading,
  title={Cascading bandits for large-scale recommendation problems},
  author={Zong, Shi and Ni, Hao and Sung, Kenny and Ke, Nan Rosemary and Wen, Zheng and Kveton, Branislav},
  journal={arXiv preprint arXiv:1603.05359},
  year={2016}
}

@article{zhong2021thompson,
  title={Thompson sampling algorithms for cascading bandits},
  author={Zhong, Zixin and Chueng, Wang Chi and Tan, Vincent YF},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={9915--9980},
  year={2021},
  publisher={JMLRORG}
}


@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1029--1038},
  year={2020},
  organization={PMLR}
}


@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017},
  publisher={Journal of Machine Learning Research/Massachusetts Institute of Technology~…}
}

@article{hejna2023inverse,
  title={Inverse Preference Learning: Preference-based RL without a Reward Function},
  author={Hejna, Joey and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2305.15363},
  year={2023}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}



@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lee2021b,
  title={B-pref: Benchmarking preference-based reinforcement learning},
  author={Lee, Kimin and Smith, Laura and Dragan, Anca and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2111.03026},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{liu2009learning,
  title={Learning to rank for information retrieval},
  author={Liu, Tie-Yan and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={3},
  pages={225--331},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{xia2008listwise,
  title={Listwise approach to learning to rank: theory and algorithm},
  author={Xia, Fen and Liu, Tie-Yan and Wang, Jue and Zhang, Wensheng and Li, Hang},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1192--1199},
  year={2008}
}

@inproceedings{brown2019extrapolating,
  title={Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations},
  author={Brown, Daniel and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
  booktitle={International conference on machine learning},
  pages={783--792},
  year={2019},
  organization={PMLR}
}

@article{shin2023benchmarks,
  title={Benchmarks and algorithms for offline preference-based reward learning},
  author={Shin, Daniel and Dragan, Anca D and Brown, Daniel S},
  journal={arXiv preprint arXiv:2301.01392},
  year={2023}
}


@article{busa2014preference,
  title={Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm},
  author={Busa-Fekete, R{\'o}bert and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Weng, Paul and Cheng, Weiwei and H{\"u}llermeier, Eyke},
  journal={Machine learning},
  volume={97},
  pages={327--351},
  year={2014},
  publisher={Springer}
}

@inproceedings{ramachandran2007bayesian,
  title={Bayesian Inverse Reinforcement Learning.},
  author={Ramachandran, Deepak and Amir, Eyal},
  booktitle={IJCAI},
  volume={7},
  pages={2586--2591},
  year={2007}
}


@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}



@article{neu2009training,
  title={Training parsers by inverse reinforcement learning},
  author={Neu, Gergely and Szepesv{\'a}ri, Csaba},
  journal={Machine learning},
  volume={77},
  pages={303--337},
  year={2009},
  publisher={Springer}
}


@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@inproceedings{florence2022implicit,
  title={Implicit behavioral cloning},
  author={Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar A and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  booktitle={Conference on Robot Learning},
  pages={158--168},
  year={2022},
  organization={PMLR}
}


@article{hussein2017imitation,
  title={Imitation learning: A survey of learning methods},
  author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={2},
  pages={1--35},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}

@article{li2022pessimism,
  title={Pessimism for Offline Linear Contextual Bandits using $\\backslash \\ell\\_p $ Confidence Sets},
  author={Li, Gene and Ma, Cong and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20974--20987},
  year={2022}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@inproceedings{zanette2023realizability,
  title={When is realizability sufficient for off-policy reinforcement learning?},
  author={Zanette, Andrea},
  booktitle={International Conference on Machine Learning},
  pages={40637--40668},
  year={2023},
  organization={PMLR}
}

@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}

@article{xu2022provably,
  title={Provably efficient offline reinforcement learning with trajectory-wise reward},
  author={Xu, Tengyu and Wang, Yue and Zou, Shaofeng and Liang, Yingbin},
  journal={arXiv preprint arXiv:2206.06426},
  year={2022}
}


@book{chuklin2022click,
  title={Click models for web search},
  author={Chuklin, Aleksandr and Markov, Ilya and De Rijke, Maarten},
  year={2022},
  publisher={Springer Nature}
}

@article{ji2023provable,
  title={Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems},
  author={Ji, Xiang and Wang, Huazheng and Chen, Minshuo and Zhao, Tuo and Wang, Mengdi},
  journal={arXiv preprint arXiv:2307.12975},
  year={2023}
}


@article{dadkhahi2018alternating,
  title={Alternating linear bandits for online matrix-factorization recommendation},
  author={Dadkhahi, Hamid and Negahban, Sahand},
  journal={arXiv preprint arXiv:1810.09401},
  year={2018}
}

@inproceedings{gampa2021banditrank,
  title={BanditRank: Learning to rank using contextual bandits},
  author={Gampa, Phanideep and Fujita, Sumio},
  booktitle={Pacific-Asia conference on knowledge discovery and data mining},
  pages={259--271},
  year={2021},
  organization={Springer}
}

%%%%%%%%%%%%%%%%%%%%%%%%%% Deep AL papers

@article{ash2019deep,
  title={Deep batch active learning by diverse, uncertain gradient lower bounds},
  author={Ash, Jordan T and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
  journal={arXiv preprint arXiv:1906.03671},
  year={2019}
}


@article{yuan2020cold,
  title={Cold-start active learning through self-supervised language modeling},
  author={Yuan, Michelle and Lin, Hsuan-Tien and Boyd-Graber, Jordan},
  journal={arXiv preprint arXiv:2010.09535},
  year={2020}
}


@article{ash2021gone,
  title={Gone fishing: Neural active learning with fisher embeddings},
  author={Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Kakade, Sham},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{ren2021survey,
  title={A survey of deep active learning},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang and Wang, Xin},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={9},
  pages={1--40},
  year={2021},
  publisher={ACM New York, NY}
}

@techreport{ burges10ranknet,
  author = "Christopher Burges",
  title = "From {RankNet} to {LambdaRank} to {LambdaMART}: An Overview",
  institution = "Microsoft Research",
  number = "MSR-TR-2010-82",
  year = "2010"
}

@inproceedings{ lattimore18toprank,
  author = "Tor Lattimore and Branislav Kveton and Shuai Li and Csaba Szepesvari",
  title = "{TopRank}: A Practical Algorithm for Online Stochastic Ranking",
  booktitle = "Advances in Neural Information Processing Systems 31",
  pages = "3949-3958",
  year = "2018"
}


@inproceedings{sui2018advancements,
  title={Advancements in Dueling Bandits.},
  author={Sui, Yanan and Zoghi, Masrour and Hofmann, Katja and Yue, Yisong},
  booktitle={IJCAI},
  pages={5502--5510},
  year={2018}
}

@article{szorenyi2015online,
  title={Online rank elicitation for plackett-luce: A dueling bandits approach},
  author={Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Busa-Fekete, R{\'o}bert and Paul, Adil and H{\"u}llermeier, Eyke},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{ dudik14doubly,
  author = "Miroslav Dudik and Dumitru Erhan and John Langford and Lihong Li",
  title = "Doubly Robust Policy Evaluation and Optimization",
  journal = "Statistical Science",
  volume = "29",
  number = "4",
  pages = "485-511",
  year = "2014"
}

@inproceedings{ swaminathan15counterfactual,
  author = "Adith Swaminathan and Thorsten Joachims",
  title = "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
  booktitle = "Proceedings of the 32nd International Conference on Machine Learning",
  pages = "814-823",
  year = "2015"
}

@inproceedings{jamieson2014best,
  title={Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting},
  author={Jamieson, Kevin and Nowak, Robert},
  booktitle={2014 48th Annual Conference on Information Sciences and Systems (CISS)},
  pages={1--6},
  year={2014},
  organization={IEEE}
}

@article{diamond2016cvxpy,
  author  = {Steven Diamond and Stephen Boyd},
  title   = {{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {83},
  pages   = {1--5},
}

@article{kendall1948rank,
  title={Rank correlation methods.},
  author={Kendall, Maurice George},
  year={1948},
  publisher={Griffin}
}


@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@inproceedings{frostig2015competing,
  title={Competing with the empirical risk minimizer in a single pass},
  author={Frostig, Roy and Ge, Rong and Kakade, Sham M and Sidford, Aaron},
  booktitle={Conference on learning theory},
  pages={728--763},
  year={2015},
  organization={PMLR}
}

@article{chaudhuri2015convergence,
  title={Convergence rates of active learning for maximum likelihood estimation},
  author={Chaudhuri, Kamalika and Kakade, Sham M and Netrapalli, Praneeth and Sanghavi, Sujay},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{naghshvar2013active,
  title={Active sequential hypothesis testing},
  author={Naghshvar, Mohammad and Javidi, Tara},
  year={2013}
}

@article{wu2022precise,
  title={Precise regret bounds for log-loss via a truncated bayesian algorithm},
  author={Wu, Changlong and Heidari, Mohsen and Grama, Ananth and Szpankowski, Wojciech},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26903--26914},
  year={2022}
}

@article{singh2023hessian,
  title={The hessian perspective into the nature of convolutional neural networks},
  author={Singh, Sidak Pal and Hofmann, Thomas and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2305.09088},
  year={2023}
}

@article{zhang2024transformers,
  title={Why transformers need adam: A hessian perspective},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}