@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{sukhbaatar2015training,
  title={Training convolutional networks with noisy labels},
  author={Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{narasimhan2015consistent,
  title={Consistent multiclass algorithms for complex performance measures},
  author={Narasimhan, Harikrishna and Ramaswamy, Harish and Saha, Aadirupa and Agarwal, Shivani},
  booktitle={International Conference on Machine Learning},
  pages={2398--2407},
  year={2015},
  organization={PMLR}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@inproceedings{song2023llm,
  title={Llm-planner: Few-shot grounded planning for embodied agents with large language models},
  author={Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2998--3009},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2021",
    pages = "4582--4597"
}

@inproceedings{hu-etal-2023-llm,
    title = "{LLM}-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    author = "Hu, Zhiqiang  and
      Wang, Lei  and
      Lan, Yihuai  and
      Xu, Wanyu  and
      Lim, Ee-Peng  and
      Bing, Lidong  and
      Xu, Xing  and
      Poria, Soujanya  and
      Lee, Roy",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",

    pages = "5254--5276"
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{lin2004automatic,
  title={Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics},
  author={Lin, Chin-Yew and Och, Franz Josef},
  booktitle={Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04)},
  pages={605--612},
  year={2004}
}


@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@inproceedings{doddington2002automatic,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  booktitle={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.03771},
  year={2020}
}

@article{duvsek2020evaluating,
  title={Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge},
  author={Du{\v{s}}ek, Ond{\v{r}}ej and Novikova, Jekaterina and Rieser, Verena},
  journal={Computer Speech \& Language},
  volume={59},
  pages={123--156},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{gardent2017creating,
  title={Creating training corpora for nlg micro-planning},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={55th Annual Meeting of the Association for Computational Linguistics, ACL 2017},
  pages={179--188},
  year={2017},
  organization={Association for Computational Linguistics (ACL)}
}

@inproceedings{lin2020commongen,
  title={CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},
  author={Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1823--1840},
  year={2020}
}

@article{adidasdataset,
    title = {Adidas US Retail Products Dataset},
    journal = {Kaggle},
    url = {https://www.kaggle.com/datasets/whenamancodes/adidas-us-retail-products-dataset},
    year = {2023}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@article{gao2024aligning,
  title={Aligning llm agents by learning latent preference from user edits},
  author={Gao, Ge and Taymanov, Alexey and Salinas, Eduardo and Mineiro, Paul and Misra, Dipendra},
  journal={arXiv preprint arXiv:2404.15269},
  year={2024}
}

@inproceedings{hu2023llm,
  title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5254--5276},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{weifinetuned2021,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{sanhmultitask2022,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4328--4343},
  year={2022}
}

@inproceedings{mireshghallah2022mix,
  title={Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models},
  author={Mireshghallah, Fatemehsadat and Goyal, Kartik and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={401--415},
  year={2022}
}

@inproceedings{yang2021fudge,
  title={FUDGE: Controlled Text Generation With Future Discriminators},
  author={Yang, Kevin and Klein, Dan},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3511--3535},
  year={2021}
}

@inproceedings{krause2021gedi,
  title={GeDi: Generative Discriminator Guided Sequence Generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={4929--4952},
  year={2021}
}

@inproceedings{dathathriplug,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{liu2021dexperts,
  title={DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021}
}

@inproceedings{long2023adapt,
  title={Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning},
  author={Long, Quanyu and Wang, Wenya and Pan, Sinno},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={6525--6542},
  year={2023}
}

@inproceedings{dong2024survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={1107--1128},
  year={2024}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{muller2019does,
  title={When does label smoothing help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{kuleshov2018accurate,
  title={Accurate uncertainties for deep learning using calibrated regression},
  author={Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2796--2804},
  year={2018},
  organization={PMLR}
}

@article{jiang2018trust,
  title={To trust or not to trust a classifier},
  author={Jiang, Heinrich and Kim, Been and Guan, Melody and Gupta, Maya},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{ulmer2024calibrating,
  title={Calibrating Large Language Models Using Their Generations Only},
  author={Ulmer, Dennis and Gubri, Martin and Lee, Hwaran and Yun, Sangdoo and Oh, Seong Joon},
  journal={arXiv preprint arXiv:2403.05973},
  year={2024}
}

@article{huang2024calibrating,
  title={Calibrating Long-form Generations from Large Language Models},
  author={Huang, Yukun and Liu, Yixin and Thirukovalluru, Raghuveer and Cohan, Arman and Dhingra, Bhuwan},
  journal={arXiv preprint arXiv:2402.06544},
  year={2024}
}

@inproceedings{shenthermometer,
  title={Thermometer: Towards Universal Calibration for Large Language Models},
  author={Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W and Ghosh, Soumya},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{kapoor2024calibration,
  title={Calibration-Tuning: Teaching Large Language Models to Know What They Don’t Know},
  author={Kapoor, Sanyam and Gruver, Nate and Roberts, Manley and Pal, Arka and Dooley, Samuel and Goldblum, Micah and Wilson, Andrew},
  booktitle={Proceedings of the 1st Workshop on Uncertainty-Aware NLP (UncertaiNLP 2024)},
  pages={1--14},
  year={2024}
}

@inproceedings{zhu2023calibration,
  title={On the Calibration of Large Language Models and Alignment},
  author={Zhu, Chiwei and Xu, Benfeng and Wang, Quan and Zhang, Yongdong and Mao, Zhendong},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={9778--9795},
  year={2023}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}