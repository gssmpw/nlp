%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithm Details}
\label{sec:alg_app}
We provide summarized form of the training and inference algorithm for the \textit{Plugin} model below.

\begin{algorithm}[H]
\caption{Training and Inference for the Plugin Model}
\label{alg:plugin_model}
\textbf{Input:} Black-box model $B$, reweighting model $R$, clean training data $\mathcal{D}$, vocabulary $V$ \\
\textbf{Output:} Plugin model predictions $\bm{x}_{1:T}$ for a given sequence
\begin{algorithmic}[1]
\STATE \textbf{Training Phase:}
\FOR{each sequence $s \in \mathcal{D}$}
    \STATE Compute token probabilities $\{\bm{b}_1, \bm{b}_2, \dots, \bm{b}_m\}$ using $B$.
    \STATE Compute token probabilities $\{\bm{r}_1, \bm{r}_2, \dots, \bm{r}_m\}$ using $R$.
    \STATE Combine probabilities: ${\bm{p}}_i = \frac{\bm{b}_i \odot \bm{r}_i}{\|\bm{b}_i \odot \bm{r}_i\|_1}$ for $i \in [m]$.
    \STATE Compute sequence loss $\ell_s = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{|V|} \log({\bm{p}}_i) \odot \bm{e}_j$.
    \STATE Update parameters of $R$ using back-propagation. Freeze $B$.
\ENDFOR
\STATE \textbf{Inference Phase:}
\STATE Initialize sequence $\bm{x}_{1:T} = \{\}$.
\FOR{each token position $t = 1$ to $T$}
    \STATE Compute token probabilities $\bm{b}_t$ using $B$.
    \STATE Compute token probabilities $\bm{r}_t$ using $R$.
    \STATE Combine probabilities: ${\bm{p}}_t = \frac{\bm{b}_t \odot \bm{r}_t}{\|\bm{b}_t \odot \bm{r}_t\|_1}$.
    \STATE Predict token: $\bm{x}_t = \argmax_{V} ({\bm{p}}_t)$.
    \STATE Append $\bm{x}_t$ to $\bm{x}_{1:T}$.
\ENDFOR
\STATE \textbf{Return:} $\bm{x}_{1:T}$
\end{algorithmic}
\end{algorithm}

\section{Proof of Main Convergence Theorem}
\label{app:theory}
\input{theory}

\section{Experimental Details}


\subsection{Dataset Statistics}
\label{sec:data_statistics}

We provide the processed data statistics in Table~\ref{tab:dataset_statistics}.
We would like to highlight that due to the black-box assumption of the base model, the training set is merely used for ablation and qualitative analysis in Section~\ref{ssec:ablation} and Section~\ref{ssec:qualitative}.


\begin{table}[h]
    \centering
    \caption{Processed Dataset Statistics. Training set is only used for ablation and qualitative analysis due to the black-box model assumption.}
    \resizebox{0.7\textwidth}{!}{ 
    \begin{tabular}{lccc}
        \toprule
        \textbf{Dataset} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
        \midrule
        E2E NLG & 33,525 & 4,299 & 4,693 \\
         Web NLG & 2,732 (filtered by categories) & 844 & 720 \\
        CommonGen & 1,476 (filtered for ``man'') & 2,026 & 1,992 \\
       
        Adidas & --- & 745 & 100\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:dataset_statistics}
\end{table}


\subsection{Prompts}
\label{ssec:prompts_app}

We now describe the prompts we used for the four datasets and three models.

\paragraph{E2E NLG Dataset}
\begin{itemize}[noitemsep,topsep=0pt]
    \item For the \textbf{GPT2-M} model, we use the prompt:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Given the following aspects of a restaurant, [attributes], a natural language sentence describing the restaurant is:}
    \end{mdframed}
    
    \item For the \textbf{GPT2-XL} model, the prompt is:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Imagine you are writing a one-sentence description for a restaurant, given the following aspects: [attributes], a human-readable natural language sentence describing the restaurant is:}
    \end{mdframed}
    
    \item For the \textbf{LLaMA-3.1-8B} model, we use:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Please convert the following attributes into a coherent sentence. Do not provide an explanation.}
    \end{mdframed}
\end{itemize}


\paragraph{Web NLG Dataset} 
\begin{itemize}[noitemsep,topsep=0pt]
    \item For the \textbf{GPT2-M} model, we use the prompt:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Convert the following facts into a coherent sentence: Facts: [facts] Sentence:} 
    \end{mdframed}
    
    \item For the \textbf{GPT2-XL} model, the prompt is:
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{You are given the following facts. Facts: [facts] A short, coherent sentence summarizing the facts is:} 
    \end{mdframed}
    
    \item For the \textbf{LLaMA-3.1-8B} model, we use:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Do not provide an explanation or follow-up. Just convert the following facts of an entity into a coherent sentence. Facts: [facts] Sentence:}  
    \end{mdframed}
\end{itemize}

\paragraph{CommonGen Dataset} 
\begin{itemize}[noitemsep,topsep=0pt]
    \item For the \textbf{GPT2-M} and \textbf{GPT2-XL} models, we use the same prompt:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{One coherent sentence that uses all the following concepts: [concepts], is:}  
    \end{mdframed}
    
    \item For the \textbf{LLaMA-3.1-8B} model, we use:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Please write a coherent sentence that uses all the following concepts. Concepts: [concepts] Sentence:}  
    \end{mdframed}
\end{itemize}

\paragraph{Adidas Dataset} 
\begin{itemize}[noitemsep,topsep=0pt]
    \item For the \textbf{GPT2-M} and \textbf{GPT2-XL} models, we use the same prompt:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Given the following attributes of a product, write a description. Attributes: [attributes] Description:} 
    \end{mdframed}
    
    \item For the \textbf{LLaMA-3.1-8B} model, we use:  
    \begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
    \texttt{Please write a description of this product given the following attributes. Attributes: [attributes] Description:}  
    \end{mdframed}
\end{itemize}

For \textbf{in-context learning}, we simply add a sentence at the beginning of the prompt before offering the samples:  
\colorbox{gray!20}{\texttt{Below are a list of demonstrations:}}.

For the qualitative analysis on the distribution shift in Section~\ref{ssec:qualitative}, we ask GPT-4o with the following prompt:\\
For Web NLG dataset:
\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
\colorbox{gray!20}{\texttt{Focus on all the samples, how much percentage is related to ``Person''?}}
\end{mdframed}

For CommonGen dataset:
\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
\texttt{Focus on those samples whose target is related to gender, how much percentage is related to ``woman''?}
\end{mdframed}

\begin{table*}[t]
    \centering
    \caption{Performance comparison on E2E NLG dataset. The base model is GPT2-M. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:e2e_final_results_gpt2m}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-M & Zeroshot & 0.0247 & 0.3539 & 0.1003 & 0.2250 & 0.3015 & 0.0156 & 0.6133 \\
        GPT2-M & ICL-1 & 0.0543$_{\pm0.026}$ & 0.3431$_{\pm0.048}$ & 0.1299$_{\pm0.033}$ & 0.2280$_{\pm0.047}$ & 0.3434$_{\pm0.051}$ & 0.0260$_{\pm0.042}$ & 0.7767$_{\pm0.060}$ \\
        GPT2-M & ICL-3 & 0.0750$_{\pm0.035}$ & 0.3955$_{\pm0.028}$ & 0.1676$_{\pm0.020}$ & 0.2649$_{\pm0.052}$ & 0.3977$_{\pm0.063}$ & 0.0252$_{\pm0.049}$ & 0.8993$_{\pm0.076}$ \\
        GPT2-M & NewModel & \textbf{0.2377}$_{\pm0.011}$ & \underline{0.5049}$_{\pm0.014}$ & \textbf{0.2742}$_{\pm0.013}$ & \textbf{0.3902}$_{\pm0.006}$ & \underline{0.4521}$_{\pm0.016}$ & \textbf{0.3938}$_{\pm0.019}$ & \underline{1.1927}$_{\pm0.069}$ \\
        GPT2-M & WeightedComb & 0.1709$_{\pm0.008}$ & 0.4817$_{\pm0.020}$ & 0.2447$_{\pm0.011}$ & 0.3720$_{\pm0.014}$ & 0.4071$_{\pm0.025}$ & 0.3329$_{\pm0.027}$ & 1.0864$_{\pm0.002}$\\
        GPT2-M & \textbf{Plugin (Ours)} & \underline{0.1863}$_{\pm0.010}$ & \textbf{0.5227}$_{\pm0.011}$ & \underline{0.2612}$_{\pm0.013}$ & \underline{0.3728}$_{\pm0.003}$ & \textbf{0.4857}$_{\pm0.012}$ & \underline{0.3544}$_{\pm0.013}$ & \textbf{1.1241}$_{\pm0.009}$\\
        \hline
    \end{tabular}
    }
\end{table*}

\begin{table*}[t]
    \centering
    \caption{Performance comparison on Web NLG dataset. The base model is GPT2-M. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:web_final_results_gpt2m}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-M & Zeroshot & 0.0213 & 0.2765 & 0.1014 & 0.1872 & 0.2111 & 0.0479 & 0.2340\\
        GPT2-M & ICL-1 & 0.0317$_{\pm0.013}$ & 0.3388$_{\pm0.021}$ & 0.1318$_{\pm0.013}$ & 0.2346$_{\pm0.019}$ & 0.2876$_{\pm0.042}$ & 0.0732$_{\pm0.053}$ & 0.2715$_{\pm0.042}$\\
        GPT2-M & ICL-3 & 0.0461$_{\pm0.014}$ & 0.3388$_{\pm0.018}$ & 0.1378$_{\pm0.016}$ & 0.2291$_{\pm0.010}$ & \underline{0.3408}$_{\pm0.027}$ & 0.0748$_{\pm0.031}$ & \textbf{0.3283}$_{\pm0.037}$\\
        GPT2-M & NewModel & \underline{0.1071}$_{\pm0.005}$ & 0.3260$_{\pm0.010}$ & 0.1496$_{\pm0.014}$ & 0.2724$_{\pm0.013}$ & 0.2642$_{\pm0.008}$ & \underline{0.4327}$_{\pm0.023}$ & 0.2916$_{\pm0.031}$ \\
        GPT2-M & WeightedComb & 0.0692$_{\pm0.007}$ & \underline{0.3593}$_{\pm0.010}$ & \underline{0.1568}$_{\pm0.008}$ & \underline{0.2834}$_{\pm0.015}$ & 0.2379$_{\pm0.030}$ & 0.1916$_{\pm0.028}$ & 0.2996$_{\pm0.037}$ \\
        GPT2-M & \textbf{Plugin (Ours)} & \textbf{0.1280}$_{\pm0.007}$ & \textbf{0.4590}$_{\pm0.005}$ & \textbf{0.2226}$_{\pm0.005}$ & \textbf{0.3515}$_{\pm0.006}$ & \textbf{0.3832}$_{\pm0.010}$ & \textbf{0.7280}$_{\pm0.039}$ & \underline{0.3060}$_{\pm0.017}$ \\
        \hline
    \end{tabular}
    }
\end{table*}




\begin{table*}[t]
    \centering
    \caption{Performance comparison on CommonGen dataset. The base model is GPT2-M. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:common_final_results_gpt2m}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-M & Zeroshot & 0.0153 & 0.2216 & 0.0409 & 0.1527 & 0.2848 & 0.0001 & 0.3686\\
        GPT2-M & ICL-1 & 0.0157$_{\pm0.013}$ & 0.2580$_{\pm0.024}$ & 0.0362$_{\pm0.096}$ & 0.1388$_{\pm0.102}$ & 0.2871$_{\pm0.107}$ & 0.0222$_{\pm0.076}$ & 0.3704$_{\pm0.101}$\\
        GPT2-M & ICL-3 & 0.0552$_{\pm0.010}$ & 0.3610$_{\pm0.019}$ & 0.1248$_{\pm0.045}$ & 0.2680$_{\pm0.089}$ & \underline{0.4079}$_{\pm0.133}$ & 0.1366$_{\pm0.125}$ & 0.5340$_{\pm0.087}$ \\
        GPT2-M & NewModel & \underline{0.1260}$_{\pm0.007}$ & \underline{0.4106}$_{\pm0.016}$ & \underline{0.1683}$_{\pm0.013}$ & \underline{0.3740}$_{\pm0.009}$ & 0.3600$_{\pm0.024}$ & \underline{0.4570}$_{\pm0.058}$ & \textbf{0.7113}$_{\pm0.025}$\\
        GPT2-M & WeightedComb & 0.0567$_{\pm0.005}$ & 0.3918$_{\pm0.010}$ & 0.1353$_{\pm0.005}$ & 0.3280$_{\pm0.010}$ & 0.2929$_{\pm0.016}$ & 0.2623$_{\pm0.042}$ & 0.4353$_{\pm0.028}$\\
        GPT2-M & \textbf{Plugin (Ours)} & \textbf{0.1366}$_{\pm0.003}$ & \textbf{0.4533}$_{\pm0.007}$ & \textbf{0.1878}$_{\pm0.003}$ & \textbf{0.3934}$_{\pm0.006}$ & \textbf{0.4095}$_{\pm0.011}$ & \textbf{0.5572}$_{\pm0.022}$ & \underline{0.6395}$_{\pm0.061}$\\
        \hline
    \end{tabular}
    }
\end{table*}




\begin{table*}[t]
    \centering
    \caption{Performance comparison on Adidas dataset. The base model is GPT2-M. We show mean and standard deviation of the metrics over five seeds.}
    \vspace{1mm}
    \label{tab:adidas_final_results_gpt2m}
    \resizebox{1.0\textwidth}{!}{ 
    \begin{tabular}{|llccccccc|}
    \hline
        Model & Method & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & CIDEr & NIST\\ 
        \hline
        GPT2-M & Zeroshot & 0.0046 & 0.2488 & 0.0189 & 0.1353 & 0.1653 & 0.0312 & 0.6860 \\
        GPT2-M & ICL-1 & 0.0088$_{\pm0.054}$ & 0.2667$_{\pm0.047}$ & 0.0247$_{\pm0.66}$ & 0.1358$_{\pm0.041}$ & 0.1762$_{\pm0.028}$ & 0.0464$_{\pm0.089}$ & 0.6793$_{\pm0.078}$\\
        GPT2-M & ICL-3 & 0.0121$_{\pm0.047}$ & 0.2693$_{\pm0.028}$ & 0.0262$_{\pm0.054}$ & 0.1470$_{\pm0.020}$ & 0.1806$_{\pm0.030}$ & 0.0415$_{\pm0.104}$ & 0.7037$_{\pm0.081}$\\
        GPT2-M & NewModel & \underline{0.0515}$_{\pm0.016}$ & \underline{0.2690}$_{\pm0.014}$ & \textbf{0.0637}$_{\pm0.014}$ & \textbf{0.1697}$_{\pm0.008}$ & 0.1918$_{\pm0.013}$ & 0.0550$_{\pm0.086}$ & \underline{0.6682}$_{\pm0.047}$\\
        GPT2-M & WeightedComb & \textbf{0.0565}$_{\pm0.014}$ & 0.2630$_{\pm0.028}$ & 0.0495$_{\pm0.018}$ & 0.1565$_{\pm0.015}$ & \underline{0.1938}$_{\pm0.019}$ & \underline{0.0585}$_{\pm0.088}$ & 0.6456$_{\pm0.156}$\\
        GPT2-M & \textbf{Plugin (Ours)} & 0.0486$_{\pm0.006}$ & \textbf{0.2766}$_{\pm0.002}$ & \underline{0.0515}$_{\pm0.007}$ & \underline{0.1684}$_{\pm0.005}$ & \textbf{0.1994}$_{\pm0.004}$ & \textbf{0.0626}$_{\pm0.017}$ & \textbf{0.7919}$_{\pm0.024}$\\
        \hline
    \end{tabular}
    }
\end{table*}


\subsection{Metrics}
\label{ssec:metrics_app}

We report performance using seven standard metrics often used in the natural language generation tasks. These are: (a) BLEU~\cite{papineni2002bleu} (measures n-gram overlap between the generated and reference texts, emphasizing precision), (b) ROUGE-1~\cite{lin2004rouge} (computes unigram recall to measure the overlap between generated and reference texts), (c) ROUGE-2~\cite{lin2004rouge} (extends ROUGE-1 to bigrams, measuring the recall of two-word sequences), (d) ROUGE-L~\cite{lin2004automatic} (uses the longest common subsequence to evaluate recall), (e) METEOR~\cite{banerjee2005meteor} (combines unigram precision, recall, and semantic matching to assess similarity),  (f) CIDEr~\cite{vedantam2015cider} (measures consensus in n-gram usage across multiple references, with tf-idf weighting), and (g) NIST~\cite{doddington2002automatic} (similar to BLEU but weights n-grams by their informativeness, favoring less frequent and meaningful phrases).


\subsection{Additional Results when using GPT2-M as the Base Model}
\label{ssec:additional_results}
Due to page limit, we move the results of using the base model as GPT2-M here in Appendix.
In Table~\ref{tab:e2e_final_results_gpt2m}, ~\ref{tab:web_final_results_gpt2m}, ~\ref{tab:common_final_results_gpt2m}, and ~\ref{tab:adidas_final_results_gpt2m}, we observe the similar trend that \textit{Plugin} generally performs the best.
Only on E2E NLG that NewModel performs better, likely due to the GPT2-M model on this dataset providing too noisy predictions, and it is better to learn a new language model. 







\subsection{Further Quantitative Analysis and Ablation}
\label{appendix:more_ablation}
Following Section~\ref{ssec:ablation}, we now display the same analysis of GPT2-M on the other three datasets.

As shown in Figure~\ref{fig:plugin_effect_other3}, we observe the similar trend as that in Figure~\ref{fig:plugin_effect}, that the \textit{Plugin} model consistently improves across all settings as the base model becomes stronger with additional fine-tuning, approving the robustness and versatility of our approach.

As shown in Figure~\ref{fig:plugin_complexity_other3}, we also observe the similar trend as that in Figure~\ref{fig:plugin_complexity}.
A single-layer reweighting model achieves the best performance, while adding more layers leads to overfitting, causing a decline in performance. 
Consistently, initializing the reweighting model with a pretrained GPT2-Small significantly enhances performance.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{icml2025/images/plugin_effect_other3.pdf}
    \caption{Performance of applying a single-layer reweighting model across increasingly fine-tuned GPT2-M models on the three datasets. Results demonstrate consistent improvements introduced by our method regardless of the strength of the base model.}
    \label{fig:plugin_effect_other3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{icml2025/images/plugin_complexity_other3.pdf}
    \caption{Performance of GPT2-M with varying reweighting model complexities on the three datasets, measured by BLEU and Rouge-L. Results demonstrate that a single reweighting layer achieves significant improvements, while increasing the number of layers beyond this leads to performance degradation, likely due to overfitting.
    Using a pretrained GPT2-Small as the reweighting model largely boosts the performance, highlighting the benefits of leveraging pretrained models.}
    \label{fig:plugin_complexity_other3}
\end{figure}

\subsection{Influence of the architecture of the reweighting model in \textit{Plugin}}
We ablate the choice of the reweighting model architecture. 
We find that a causal transformer layer identical to those used in the base model performs best, as it can leverage the base model's logits and aggregate contextual information from prior tokens to better adapt the base model to the new data distribution.
% Given that the inference cost of a single additional layer is relatively negligible, using a full transformer layer is generally optimal for task performance, as it can leverage the base model's logits and aggregate contextual information from prior tokens to enhance calibration.
This conclusion is reinforced by Figure~\ref{fig:plugin_architecture}, where the transformer architecture consistently outperforms both the MLP (two layer with ReLU activation) and linear layers across all metrics, as indicated by higher means and narrower standard deviation bands. 
These results highlight the importance of leveraging the architectural capacity of transformers to effectively adapt the logits of the base black-box model.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{icml2025/images/plugin_architecture.pdf}
    \vspace{-5mm}
    \caption{Performance comparison of the weighting model architecture in \textit{Plugin}. The transformer layer achieves the best performance with consistently higher means and narrower standard deviations. Shaded bands represent the standard deviation around the mean.}
    \label{fig:plugin_architecture}
\end{figure}


\subsection{Details for Adidas Qualitative Studies}
\label{appendix:adidas_case_study}

\paragraph{Human Evaluation.}
We finally conduct a human evaluation on 100 test passages from the Adidas product dataset, comparing outputs generated with and without applying the reweighting model, using LLaMA-3.1-8B as the base model. 
Three human evaluators are presented with a ground-truth Adidas product description and two randomly ordered descriptions: one generated with the reweighting layer and one without (i.e., we use the base model with ICL-3 as a much stronger baseline due to the low quality of the zero-shot). 
Evaluators are prompted to select the prediction closest to the ground truth.
Results show that the output generated with the reweighting model is preferred on an average of 80.7 out of all 100 cases.
The output descriptions from the base model without the reweighting are generally short and general; see details in Appendix~\ref{appendix:adidas_case_study}.
This demonstrates that our approach effectively adapts a closed model to the unique style of the given dataset. 


In this section, we display some details for the qualitative analysis on the Adidas product description dataset.


\paragraph{Details of Extracting Adidas Style Words.}
We discuss the details on extracting the most frequent 50 words in the Adidas product description dataset as the ``Adidas style'' words.
We argue that there does not exist a gold-standard way to define the ``style'' words for a dataset.
We extract these style words through a minimal preprocessing pipeline: converting text to lowercase, removing special characters and numbers, and filtering out common English stopwords. 
We deliberately preserve the original word forms without lemmatization or stemming to maintain distinct style markers (e.g., keeping ``comfortable'' distinct from ``comfort'', ``running'' distinct from ``run'').
After tokenization using NLTK's word tokenizer, we count word frequencies across all product descriptions and select the top 50 most frequent words.
This approach captures the exact vocabulary used in Adidas' product descriptions, including specific product features.


A statistics of the frequency of these top-50 words is shown in Figure~\ref{fig:adidas_style_statistics}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{icml2025/images/adidas_style_statistics.pdf}
    \caption{Word Frequency of the Top-50 Words in Adidas Dataset.}
    \label{fig:adidas_style_statistics}
\end{figure}

\paragraph{Case Studies for Human Side-by-side Evaluation.}
Below we provide more case studies on the Adidas product description datasets. 
We display the given product attributes, base model's zero-shot outputs, base model with ICL-3's outputs, \textit{Plugin} model's outputs, and the ground-truth references for each case.


\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
$\clubsuit$ \textbf{Case 1}\\
\textbf{Given Product Attributes:} 
\begin{itemize}[noitemsep,topsep=0pt,label=-]  
    \item name [Cushioned Angle Stripe Low-Cut Socks 6 Pairs]
    \item category [Accessories]
    \item price [13]
    \item color [Multicolor]
\end{itemize}

\textbf{Base Model (Zero-shot):} Cushioned Angle Stripe Low-Cut Socks 6 Pairs, Multicolor.

\textbf{Base Model (ICL-3):} These adidas socks are made with a cushioned sole for extra comfort and support. The low-cut design is perfect for everyday wear, whether you're running errands or hitting the gym.

\textbf{Plugin Model:} Cushioned Angle Stripe Low-Cut Socks 6 Pairs are the perfect blend of style and comfort. The low-cut design gives you a sleek, modern look while still providing plenty of support for your feet. Made with high-quality materials, these socks will keep your feet feeling comfortable and looking chic all day long.

\textbf{Ground-truth Reference:} With lots of cushioning in the heel and toe and a soft, stretchy feel, these kids' socks are big on comfort. Whether they're bouncing around between classes, practice or after-school activities, their little feet will stay dry thanks to moisture-wicking yarn and a mesh upper. An adidas Badge of Sport and 3-Stripes give the socks a classic adidas feel.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
$\spadesuit$ \textbf{Case 2}\\
\textbf{Given Product Attributes:} 
\begin{itemize}[noitemsep,topsep=0pt,label=-]  
    \item name [Badge of Sport Tee]
    \item category [Clothing]
    \item price [21]
    \item color [Grey]
\end{itemize}


\textbf{Base Model (Zero-shot):} A white t-shirt with a badge of the sport you love on it.

\textbf{Base Model (ICL-3):} The adidas Badge of Sport tee is a classic. It's made with soft cotton and features the iconic 3-Stripes on the shoulders. The crew neck has a ribbed hem for a comfortable fit.

\textbf{Plugin Model:} There's nothing more classic than a white tee with your favorite team's logo. This adidas Badge of Sport Tee is an everyday essential that can be worn to the gym or on game days. It's made with recycled materials, so you're making a sustainable choice. The soft cotton jersey is lightweight, breathable, and comfortable.

\textbf{Ground-truth Reference:} Put your brand pride on full display. A simple yet versatile look, this long sleeve t-shirt keeps you comfortable while on the move or lounging at home. A contrast adidas Badge of Sport stands out on the chest. This product is made with Primegreen, a series of high-performance recycled materials.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
$\vardiamond$ \textbf{Case 3}\\
\textbf{Given Product Attributes:} 
\begin{itemize}[noitemsep,topsep=0pt,label=-]  
    \item name [Stretch Cotton Trunk Briefs 3 Pairs]
    \item category [Clothing]
    \item price [21]
    \item color [Grey]
\end{itemize}


\textbf{Base Model (Zero-shot):} Stretch Cotton Trunk Briefs 3 Pairs, Grey.

\textbf{Base Model (ICL-3):} These adidas men's trunks are made from a soft cotton blend that feels great against the skin. The stretchy fabric moves with you, while the flatlock seams prevent chafing and irritation. A drawcord waistband allows for an adjustable fit.

\textbf{Plugin Model:} Stretch Cotton Trunk Briefs 3 Pairs. The stretch cotton fabric is comfortable to wear. These briefs are made with a classic design in mind, so you can feel confident and comfortable. They have been designed with a low waist, which gives them a flattering fit and makes them easy to get on. 

\textbf{Ground-truth Reference:} Just look at these adidas boxer briefs. They're made with soft, quick-dry fabric that keeps you feeling fresh and a stay-put design that keeps you covered from your first rep to your last. Get comfortable and give it your all. Our cotton products support sustainable cotton farming. This is part of our ambition to end plastic waste.
\end{mdframed}

\begin{mdframed}[backgroundcolor=gray!20, linewidth=0pt]
$\varheart$ \textbf{Case 4}\\
\textbf{Given Product Attributes:} 
\begin{itemize}[noitemsep,topsep=0pt,label=-]  
    \item name [adidas x Zoe Saldana Long Sleeve Tee]
    \item category [Clothing]
    \item price [28]
    \item color [Black]
\end{itemize}


\textbf{Base Model (Zero-shot):} This is a long sleeve tee with the adidas logo on the front and Zoe Saldana's name on the back. It has a crew neck, short sleeves, and a straight hem. The fabric is made of 100\% cotton and is machine washable.

\textbf{Base Model (ICL-3):} This adidas x Zoe Saldana long sleeve tee is made with 100\% recycled polyester. The soft, lightweight fabric has a smooth finish and features a crew neck and short sleeves. It's finished with an adidas logo on the chest.

\textbf{Plugin Model:} This is not your average training t-shirt. A modern, slim fit and the softest fabric make it one of our most comfortable tops to date. The lightweight feel makes this tee perfect for all-day wear. Pair it with a pair of leggings or joggers for that ultimate athleisure look you're always striving for ambition and comfort.

\textbf{Ground-truth Reference:} Your growth, your goals and the challenges you face along the way are all part of your journey. This adidas tee was created in collaboration with Zoe Saldana to inspire you to move with confidence, no matter where you are on your path. Smooth material lays softly on your body so you can feel comfortable in your own skin. A solid color offers styling versatility. This product is made with recycled content as part of our ambition to end plastic waste.
\end{mdframed}


% base gpt2-medium epoch 1
% Best hyperparameters: learning\_rate=5e-4, weight\_decay=0.01 with validation\_loss=1.1292061805725098

% base gpt2-medium epoch 2
% Best hyperparameters: learning\_rate=5e-4, weight\_decay=0.01 with validation\_loss=1.0625724792480469


% base gpt2-medium epoch 5
% Best hyperparameters: learning rate=5e-4, weight decay=0.01 with validation loss=1.0323151350021362 (edited) 


% plugin 1 layer on gpt2-medium
% Best hyperparameters: learning rate=5e-4, weight decay=10 with validation loss=1.4012659788131714


% plugin 1 layer on gpt2-medium 1 epoch
% Best hyperparameters: learning rate=5e-5, weight decay=10 with validation loss=1.1128274202346802

% plugin 1 layer on gpt2-medium 2 epoch
% Best hyperparameters: learning rate=5e-5, weight decay=10 with validation loss=1.0761886835098267

% plugin 1 layer on gpt2-medium 5 epoch
% Best hyperparameters: learning rate=5e-5, weight decay=0.1 with validation loss=1.0527310371398926

% plugin LLaMA 1 layer
% Best hyperparameters: learning rate=5e-5, weight decay=10 with validation loss=missing

% plugin LLaMA 2 layer
% Best hyperparameters: learning rate=5e-5, weight decay=1 with validation loss=1.855312705039978

% %###### OLD TABLES

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|ccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L \\ 
%         \hline
%         GPT2 & 0.0315 & 0.2592 & 0.0759 & 0.1817 \\
%         GPT2-Plugin & 0.0997 & 0.3972 & 0.1765 & 0.2584\\
%         FT-GPT2-5e-5 & 0.1444 & 0.4567 &  0.2306 & 0.3015  \\
%         FT-GPT2-5e-5-Plugin & 0.1480 &0.4620 &  0.2384 & 0.3092\\
%         FT-GPT2-5e-4 & 0.1630 &  0.4783 & 0.2567 & 0.3263 \\
%         FT-GPT2-5e-4-Plugin & 0.1627 & 0.4796 & 0.2564 & 0.3260\\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output concatenated (padding, mr, padding, lr) without prompt.}
%     \label{tab:e2e_wo_prompt}
% \end{table}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|ccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L \\ 
%         \hline
%         GPT2 & 0.0247 & 0.3540 & 0.1003 & 0.2249 \\
%         GPT2-Plugin & 0.0982 & 0.4185 & 0.1874 & 0.2559\\
%         FT-GPT2-5e-5 & 0.1517 & 0.467 & 0.2431  & 0.3156  \\
%         FT-GPT2-5e-5-Plugin & 0.1552 &  0.4664 & 0.2489 & 0.3186 \\
%         FT-GPT2-5e-4 & 0.1577  & 0.4727  & 0.2479 & 0.3214 \\
%         FT-GPT2-5e-4-Plugin & 0.1566 & 0.4695 & 0.2491 & 0.3205 \\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with prompt.}
%     \label{tab:e2e}
% \end{table}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & NIST & CIDEr\\ 
%         \hline
%         GPT2 & 0.0247 & 0.3540 & 0.1003 & 0.2249 & & & \\
%         GPT2-Plugin & 0.0972 & 0.4105 & 0.1840 & 0.2471 & & & \\
%         FT-GPT2-5e-5 & 0.1517 & 0.4672 & 0.2433  & 0.3157 & 0.6058 & 0.9734 & 0.0037  \\
%         FT-GPT2-5e-5-Plugin & 0.1560 & 0.4671  & 0.2480 & 0.3171 & 0.6108 & 0.9602 & 0.0042 \\
%         FT-GPT2-5e-4 & 0.1577  & 0.4727  & 0.2479 & 0.3214 & & & \\
%         FT-GPT2-5e-4-Plugin & 0.1569 & 0.4797 & 0.2510 & 0.3249 & & & \\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with prompt. Full validation data without early stopping.}
%     \label{tab:e2e_fullvalid}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{|cccccccc|}
%     \hline
%         Model (epoch, lr, wd) & BLEU & Rouge-1 & Rouge-2 & Rouge-L & METEOR & NIST & CIDEr\\ 
%         \hline
%         LLaMA (zero shot) & 0.0775 & 0.0847 & 0.0436 & 0.0631 & 0.1163 & 0.5547 & 0.0569 \\
%         LLaMA-Plugin-1-layer (50, 5e-4,0.1) & 0.1151 & 0.4108 & 0.235 & 0.308 & 0.535 & 0.7184 & 0.1447 \\
%         LLaMA-Plugin-2-layer (80, 5e-4, 0.1) & 0.0930 & 0.3569 & 0.1927 & 0.2706 & 0.4845 & 0.6197 & 0.1403 \\
%         LLaMA-Plugin-1-layer (train-25 epochs) &  &  &  &  &  &  & \\
%         \hline
%     \end{tabular}
%     \caption{Performance on E2E NLG dataset with input-output pre-concatenated (passing, mr, hr) with prompt for LLaMA (no fine-tuning). Pluging trained on val and hyperval.}
%     \label{tab:old_e2e_LLaMA_w_prompt}
% \end{table*}


