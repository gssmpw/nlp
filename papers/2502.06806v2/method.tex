\section{Proposed Method: The Plugin Approach}
\label{sec:method}

% To estimate the autoregressive transition vector, we train an autoregressive language model on the clean data. This model operates alongside the black-box model during inference, serving as an autoregressive reweighting mechanism for the token probabilities produced by the black-box model. The combined model, which integrates the probabilities from the black-box and reweighting models, is referred to as the ``plugin" model. The term ``plugin" is inspired by classification literature, where plugin methods adapt to distribution shifts by reweighting probabilities~\citep{koyejo2014consistent, narasimhan2014statistical, narasimhan2015consistent, hiranandani2021optimizing}. We now explain the training and inference phases in detail. These are summarized in Algorithm~\ref{alg:plugin_model} in Appendix~\ref{sec:alg_app}. Please see Figure~\ref{fig:plugin_fig} for an illustration. 

To estimate the autoregressive transition vector, we train an autoregressive language model on target data, which operates alongside the black-box model during inference. This model acts as an autoregressive reweighting mechanism, adjusting the token probabilities produced by the black-box model. The combined approach, integrating probabilities from the black-box and reweighting models, is referred to as the \textit{Plugin} model. The term \textit{Plugin} is inspired by classification literature, where plugin methods reweight probabilities to adapt to distribution shifts~\citep{koyejo2014consistent, narasimhan2014statistical, narasimhan2015consistent, hiranandani2021optimizing}. We now detail the training and inference phases, summarized in Algorithm~\ref{alg:plugin_model} (Appendix~\ref{sec:alg_app}) and illustrated in Figure~\ref{fig:plugin_fig}.


\subsection{Training the Plugin Model}
\label{ssec:training}

% Consider a single sequence $s$ of $m$ tokens during one iteration of training the reweighting model. First, we pass the sequence through the black-box model to obtain token probabilities $\{\bm{b}_1, \bm{b}_2, \dots, \bm{b}_m\}$, where each $\bm{b}_i \in \Delta^{|V|-1}$ for $i \in [m]$. Next, we pass the same sequence through the current state of the reweighting model to produce token probabilities $\{\bm{r}_1, \bm{r}_2, \dots, \bm{r}_m\}$, where each $\bm{r}_i \in \Delta^{|V|-1}$ for $i \in [m]$. Let $\odot$ denote element-wise multiplication. The final token probability from the plugin model is obtained by normalizing the element-wise product of these probabilities using the 1-norm:
% \begin{equation}
%     \label{eq:final_prob}
%     {\bm{p}}_i = \frac{\bm{b}_i \odot \bm{r}_i}{\|\bm{b}_i \odot \bm{r}_i\|_1}.
% \end{equation}

% The cross-entropy loss for the sequence $s$ is defined as:
% \begin{equation}
%     \label{eq:ce_batch_loss}
%     \ell_s = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{|V|} \log({\bm{p}}_i) \odot \bm{e}_j,
% \end{equation}
% where the $j$-th token appears at the $i$-th position in the sequence $s$.

% During back-propagation, we update the parameters of the reweighting model, while keeping the black-box model frozen, as it only provides access to token logits. This loss formulation extends naturally to a batch of sequences. Over training iterations, the refined estimates $\bm{r}_i$ converge towards the transition vector governing label shifts in the clean data. 

During each training iteration, a sequence $s$ of $m$ tokens is passed through both the black-box model and the reweighting model to obtain token probabilities $\{\bm{b}_1, \bm{b}_2, \dots, \bm{b}_m\}$ and $\{\bm{r}_1, \bm{r}_2, \dots, \bm{r}_m\}$, respectively, where each $\bm{b}_i, \bm{r}_i \in \Delta^{|V|-1}$. The final token probability from the \textit{Plugin} model is computed by normalizing the element-wise product of these probabilities:
\begin{equation}
    \label{eq:final_prob}
    {\bm{p}}_i = \frac{\bm{b}_i \odot \bm{r}_i}{\|\bm{b}_i \odot \bm{r}_i\|_1}.
\end{equation}

The sequence-level cross-entropy loss is given by:
\begin{equation}
    \label{eq:ce_batch_loss}
    \ell_s = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{|V|} \log({\bm{p}}_i) \odot \bm{e}_j,
\end{equation}
where the $j$-th token appears at the $i$-th position in the sequence $s$. During backpropagation, only the reweighting model parameters are updated, while the black-box model remains frozen. This formulation extends naturally to batch training, refining $\bm{r}_i$ over iterations to approximate the transition vector governing label shifts in the target data.



\subsection{Inference from the Plugin Model}
\label{ssec:inference}

% Suppose the reweighting model has been fully trained and the black-box model is available. To generate the first token, the black-box model produces probabilities $\bm{b}_1$, and the reweighting model produces probabilities $\bm{r}_1$. The plugin model predicts the first token as:
% $
% \bm{x}_1 = \argmax_{V} (\bm{b}_1 \odot \bm{r}_1).
% $

% For generating the token at the $t$-th step, given the tokens $\bm{x}_{t-1:1}$ generated so far by the plugin model, we compute probabilities $\bm{b}_t$ from the black-box model and $\bm{r}_t$ from the reweighting model. The plugin model predicts the token at step $t$ as:
% \[
% \bm{x}_t = \argmax_{V} (\bm{b}_t \odot \bm{r}_t).
% \]

Given a fully trained reweighting model and access to the black-box model, token generation proceeds autoregressively. At the first step, the black-box model produces token probabilities $\bm{b}_1$, while the reweighting model outputs $\bm{r}_1$. The \textit{Plugin} model selects the first token as $\bm{x}_1 = \argmax_{V} (\bm{b}_1 \odot \bm{r}_1).$ For subsequent steps, given the previously generated tokens $\bm{x}_{t-1:1}$, we obtain probabilities $\bm{b}_t$ from the black-box model and $\bm{r}_t$ from the reweighting model. The \textit{Plugin} model then predicts the next token as: $\bm{x}_t = \argmax_{V} (\bm{b}_t \odot \bm{r}_t)$.

The process continues until a stopping criterion is met. Note that, this manuscript focuses on greedy decoding for inference. Other decoding strategies, such as temperature scaling, top-$p$ sampling, or beam search, can be incorporated by normalizing the element-wise product of probabilities and using them as the final token distribution, as in~\eqref{eq:final_prob}.