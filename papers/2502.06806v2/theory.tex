
% \subsection{Main Theorem}

% The proof mainly follows the work of  \citet{frostig2015competing, chaudhuri2015convergence, mukherjee2022chernoff}.
%
% Assume that there exists an unknown parameter $\btheta_* \in\bTheta$ that governs the dynamics. Let $T_t(\btheta_* ; x_i, x_j)$ denote the transition matrix that governs the transition of $x_i$ to $x_j$ at time $t$. 
% %
% We follow the setup of \citet{wu2022precise}. The learner makes a prediction of the class $Y_s\in [0,|V|]$ and the true label of the class is given by $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\in \{0,|V|\}$. 


% \begin{assumption}
% \label{assm:transition-matrix}
%     % Let the token $x_{i}$ at time $t$ gets flipped to $x_{j}$. 
%     %
%     The matrix $T_t(\btheta_* | \F^{t-1})\in \mathbb{R}^{|V|\times |V|}$ is the transition matrix that governs that transition between any pair of tokens $x_i$ and $x_j$ at time $t$.
%     %
%     Let $T_t(\btheta_* ; x_i, x_j)$ denote the $(i,j)$-th component of that matrix and the transition function $f_{I_t}(\btheta_* ; x_i, x_j)$ determines the transition from $x_i$ to $x_j$ where $I_t$ is the $x_i$ token selected at time $t$.
%     %
%     We assume that $\nabla f_{I_t}(\btheta_* ; x_i, x_j)< \lambda_0$ and $\nabla^2 f_{I_t}(\btheta_* ; x_i, x_j) < \lambda_1$ for some constant $\lambda_0$, $\lambda_1$ and for all time instant $t$.
%     %
% \end{assumption}
% With this \Cref{assm:transition-matrix} we have reduced the problem of estimating the transition matrix to a hypothesis testing problem \citet{naghshvar2013active} such that estimating the correct $\btheta_*$ with high probability will also lead to the correct estimation of the transition matrix.


% Let the plugin model minimizes the log loss (cross entropy loss) $\ell_{1}(\btheta), \ell_{2}(\btheta), \cdots, \ell_{t}(\btheta)$ over $t$ iterations. Let $\twtheta_t =\argmin_{\btheta \in \bTheta} \sum_{s = 1}^t \ell_{s}(\btheta)$. 
% %
% % Finally define the estimated transition loss as $\wT_t(x_i, x_j; \twtheta_t)$ that governs the transition of $x_i$ to $x_j$. Under \eqref{assm:transition-matrix} we have that
% % \begin{align*}
% %     \wT_t(x_i, x_j; \twtheta_t) = vec(x_i x_j^\top)^\top vec(\twtheta_t \twtheta_t^\top)b(x_i)b(x_j)
% % \end{align*}
% %
% At every iteration $t$, the plugin algorithm looks into the history $\F^{t-1}$ and selects an example (token) $x_t$ following the probability distribution $\bp_{\twtheta_t}\in\mathbb{R}^{|V|}$. At time t, the plugin works like this:
% \begin{align*}
% x_t \sim \mathbf{p}_{\twtheta_{t-1}} \implies x_t \sim (T_t(\twtheta_t))^{-1}\mathbf{p}_{{\twtheta}_{t-1}}
% \end{align*}

% Let $\wLcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t \ell_{s}(\btheta)$ and its expectation $\Lcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t\E_{x_s\sim \mathbf{p}_{\twtheta_{s-1}}}[\ell_s(\btheta)|\F^{s-1}]$.

We define the following assumption on the smoothness and regularity of the loss function. 
\begin{assumption}
\label{assm:thm}
We assume the following assumptions hold with probability $1$:
\begin{enumerate}
    \item \textbf{(Convexity of $\ell_{s}$):} The loss function $\ell_{s}$ is convex for all time $s\in[t]$.
    \item \textbf{(Smoothness of $\ell_{s}$):} The $\ell_{s}$ is smooth such that the first, second, and third derivatives exist at all interior points in $\bTheta$.
    \item \textbf{(Regularity Conditions):} \begin{enumerate}
        \item $\bTheta$ is compact and $\ell_{s}(\btheta)$ is bounded for all $\btheta\in\bTheta$ and for all $s\in[t]$.
        \item $\btheta_*$ is an interior point in $\bTheta$.
        \item $\nabla^2 \ell_{s}(\btheta_*)$ is positive definite, for all $s\in[t]$ .
        \item There exists a neighborhood $\mathcal{B}$ of $\btheta_*$ and a constant $C_{1}$, such that $\nabla^{2} \ell_{s}(\btheta)$ is $C_{1}$ -Lipschitz. Hence, we have that $\left\|\nabla^{2} \ell_{s}(\btheta)-\nabla^{2} \ell_{s}\left(\btheta^{\prime}\right)\right\|_{*} \leq C_{1}\left\|\btheta-\btheta^{\prime}\right\|_{\nabla^{2} \Lcal_s\left(\btheta_{*}\right)}$, for $\btheta, \btheta^{\prime}$ in this neighborhood.
    \end{enumerate}
    \item \textbf{(Concentration at $\btheta_{*}$):} We further assume that $\left\|\nabla \ell_{s}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_s\left(\btheta_{*}\right)\right)^{-1}} \leq C_{2}$ hold with probability one.
\end{enumerate}
\end{assumption}

% Let the clipped cross entropy token loss at time $s$ following \cref{eq:ce_batch_loss} be defined as
% \begin{align}
%     \ell^{cl-\log}_{s} = -\sum_{j=1}^{|V|} \log(\max(\epsilon, \bp_i))\odot \be_j. \label{eq:clipped-log-loss}
% \end{align}
% for some $\epsilon > 0$ where the $j$-th token appears at the $i$-th place in sequence $s$.

% %
% \begin{assumption}
% \label{assm:bound-ce}
%     We assume that the cross entropy loss used in training is clipped log loss defined in \eqref{eq:clipped-log-loss} so that the $|V|$ way clipped log loss $\ell^{cl-\log}_{s}$ at time $s$ is upper bounded by $(1+\frac{1}{\epsilon})|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$, where $Y_s$ is the predicted class of the token and $f_{I_s}$ determines the true class and satisfies \Cref{assm:transition-matrix}. WLOG we denote $C=(1+\frac{1}{\epsilon}) > 0$ as a constant and so $\ell^{cl-\log}_{s} \leq C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ for any time $s$.  
% \end{assumption}
% Note that the clipped log loss is always bounded by squared loss because squared loss grows faster in all regions, especially when clipping is applied to the log loss.


% Using \cref{assm:bound-ce} we can upper bound the clipped log loss with a squared loss. Now in the following proof we will upper bound $\ell^{cl-\log}_{s} \leq C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $Y_s$ is the predicted class at time $s$ and $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ is the function that determines true class. Also, it follows from \Cref{assm:transition-matrix} that the gradient $\nabla f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$ and $\nabla^2 f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$   is bounded. We will denote $\ell_{s} = C|V|^2(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$ where $C>0$ is the constant defined in \cref{assm:bound-ce}. 

% 
\begin{lemma}\textbf{(Proposition 2 of \citep{hsu2012tail})}
\label{lemma:vector-martingale}
Let $\mathbf{u}_{1}, \ldots, \mathbf{u}_{n}$ be a martingale difference vector sequence (i.e., $\mathbb{E}\left[\mathbf{u}_{i} \mid \mathbf{u}_{1}, \ldots, \mathbf{u}_{i-1}\right]=$ 0 for all $i=1, \ldots, n$ ) such that
$$
\sum_{i=1}^{n} \mathbb{E}\left[\left\|\mathbf{u}_{i}\right\|^{2} \mid \mathbf{u}_{1}, \ldots, \mathbf{u}_{i-1}\right] \leq v \quad \text { and } \quad\left\|\mathbf{u}_{i}\right\| \leq b
$$
for all $i=1, \ldots, n,$ almost surely. For all $t>0$
$$
\operatorname{Pr}\left[\left\|\sum_{i=1}^{n} \mathbf{u}_{i}\right\|>\sqrt{v}+\sqrt{8 v t}+(4 / 3) b t\right] \leq e^{-t}
$$
\end{lemma}

\begin{lemma}
\label{lemma:vector-conc}
  The probability that $\|\nabla \wLcal_t(\btheta_*)\|_{\left(\nabla^{2} \Lcal\left(\btheta_{*}\right)\right)^{-1}} $ crosses the threshold $\sqrt{\dfrac{c\gamma\log (dt)}{t}} > 0$ is bounded by
 \begin{align*}
     \Pb\left(\|\nabla \wLcal_t(\btheta_*)\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}\geq C_2\sqrt{\dfrac{c\gamma\log (dt)}{t}}\right) \leq \frac{1}{t^{c\gamma}}.
 \end{align*}
\end{lemma}

\begin{proof}
Define $\mathbf{u_s} \coloneqq \nabla (Y_s -f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2$. Then we have $\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{t}$ as random vectors such that
\begin{align*}
    & \mathbb{E}\left[\left\|\sum_{s=1}^t \mathbf{u_s}\right\|_{\left(\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right)^{-1}}^{2} \bigg | \mathbf{u}_{1}, \ldots, \mathbf{u}_{s-1}\right] = \mathbb{E}\left[\sum_{s=1}^t \mathbf{u_s}^{\top}\left(\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right)^{-1}\mathbf{u_s} \mid \mathbf{u}_{1}, \ldots, \mathbf{u}_{s-1}\right] \leq t C^2_2
\end{align*}
Also we have that $\|\mathbf{u_s}\| \leq C_2$. Finally we have that 
\begin{align*}
    \E[\nabla_{\btheta = \btheta_*}\mathbf{u_s}] = -2\sum_{s=1}^{t} p_{\twtheta_{s-1}}(f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\nabla_{\btheta = \btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}) = 0.
\end{align*}
Then following \Cref{lemma:vector-martingale} and by setting $\epsilon = c\gamma\log(dt)$ we can show that
\begin{align*}
    \Pb&\left(\|\frac{1}{t}\sum_{s=1}^t \mathbf{u_s}\|^2_{_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}} - \E\left[\|\frac{1}{t}\sum_{s=1}^t \mathbf{u_s}\|^2_{_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}}\right] >  \frac{1}{t}\sqrt{8tC_2^2 \epsilon} + \dfrac{4 C_2 }{3\epsilon}   \right) \\
    %%%%%%%%%%%%%%%%%%%%%%%%
    &= \Pb\left(\|\frac{1}{t}\sum_{s=1}^t \mathbf{u_s}\|^2_{_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}} >  C_1^2 +  C_2\sqrt{\frac{8\epsilon}{t}} + \dfrac{4 C_2 }{3\epsilon}   \right)\\
    %%%%%%%%%%%%%%%%%%%%%%%%%
    &\leq \Pb\left(\|\sum_{s=1}^t \mathbf{u_s}\|^2_{_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}} >  C_2\sqrt{\frac{8\epsilon}{t}}  \right) = \Pb\left(\|\sum_{s=1}^t \mathbf{u_s}\|^2_{_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}} >  4C_2\sqrt{\dfrac{  c\gamma \log(dt)}{t}}  \right)\\
    &\leq \exp(- c\gamma\log (dt)) = \left(\frac{1}{dt}\right)^{c\gamma} \leq \frac{1}{t^{c\gamma}}
\end{align*}
The claim of the lemma follows.
\end{proof}

\begin{lemma}
\label{lemma:support-lemma1}
Let the $j$-th row and $k$-th column entry in the Hessian matrix $\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))$ be denoted as $[\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))]_{jk}$. Then we have that
\begin{align*}
    [\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))]_{jk} = 2 \dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2\left(f_{I_s}(\btheta; x_i, x_j, \F^{s-1}) - Y_s\right)\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}.
\end{align*}
\end{lemma}

\begin{proof}
This lemma follows from \citet{ frostig2015competing, mukherjee2020generalized} adapted to our setting for the squared loss, and transition function $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$. 
%
We want to evaluate the Hessian $\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))$ at any $\btheta^{\prime}\in\bTheta$. We denote the $j$-th row and $k$-th column entry in the Hessian matrix as $[\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))]_{jk}$. Then we can show that
\begin{align*}
    [\nabla^2_{\btheta = \btheta^{\prime}}(\ell_{s}(\btheta))]_{jk}&\coloneqq \frac{\partial }{\partial \btheta_j} \left[\frac{\partial (f_{I_s}(\btheta; x_i, x_j, \F^{s-1}) - Y_s)^2}{\partial \btheta_k}\right] = \frac{\partial }{\partial \btheta_j}\left[2(f_{I_s}(\btheta; x_i, x_j, \F^{s-1}) - Y_s) \frac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right]\\
    %%%%%%%%%%%%%%%%%%%%%
    &= \frac{\partial }{\partial \btheta_j}\left[ 2f_{I_s}(\btheta; x_i, x_j, \F^{s-1})\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} - 2Y_s\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right]\\
    %%%%%%%%%%%%%%%%%%%%%%
    &= 2 \dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k} \\
    %%%%%
    &\qquad - 2 Y_s\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k} - 2 \dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial Y_s}{\partial \btheta_k}\\
    %%%%%%%%%%%%%%%%%%%%%%%
    &= 2 \dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2\left(f_{I_s}(\btheta; x_i, x_j, \F^{s-1}) - Y_s\right)\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}
\end{align*}
The claim of the lemma follows.
\end{proof}


\begin{lemma}
\label{lemma:support-lemma2}
Let the $j$-th row and $k$-th column entry in the Hessian matrix $\nabla^2_{\btheta = \btheta^{\prime}}(\E[\ell_{s}(\btheta)|\F^{s-1}])$ be denoted as $[\nabla^2_{\btheta = \btheta^{\prime}}(\E[\ell_{s}(\btheta)|\F^{s-1}])]_{jk}$. Then we have that
\begin{align*}
     \left[\nabla^2_{\btheta = \btheta^{\prime}}  \E[\ell_{s}(\btheta)|\F^{s-1}]\right]_{jk} &=  2\sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\left( \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right.\\
     %%%%%%
     &\qquad\left. + 2\left(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}\right).
\end{align*}
\end{lemma}

\begin{proof}
This lemma follows from \citet{ frostig2015competing, mukherjee2020generalized} adapted to our setting for the squared loss,  transition function $f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})$, and the sampling distribution $\bp_{\twtheta_{s-1}}$. We show it here for completeness. Now we want to evaluate the Hessian $\nabla^2_{\btheta = \btheta^{\prime}}(\E[\ell_{s}(\btheta)|\F^{s-1}])$ at any $\btheta^{\prime}\in\bTheta$. We denote the $j$-th row and $k$-th column entry in the Hessian matrix as $[\nabla^2_{\btheta = \btheta^{\prime}}(\E[\ell_{s}(\btheta)|\F^{s-1}])]_{jk}$. Then we can show that
\begin{align}
     &\nabla^2_{\btheta = \btheta^{\prime}}  \E[\ell_{s}(\btheta)|\F^{s-1}]
    %%%%%%%%%%%%%%%%%%%
    = \nabla^2_{\btheta = \btheta^{\prime}} \left(f^2_{I_s}(\btheta; x_i, x_j, \F^{s-1}) + \E[Y^2_s|\F^{s-1}] - 2\E[Y_s|\F^{s-1}]f_{I_s}(\btheta; x_i, x_j, \F^{s-1})\right)\nonumber\\
    %%%%%%%%%%%%%%%%%%%%
    &= \nabla^2_{\btheta = \btheta^{\prime}} \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\left(f^2_{i}(\btheta ; x_i, x_j, \F^{s-1}) + f^2_{i}(\btheta^{\prime} ; x_i, x_j, \F^{s-1}) + \frac{1}{2} - 2f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\right)\nonumber\\
    %%%%%%%%%%%%%%%%%%%%
    &=  \nabla^2_{\btheta = \btheta^{\prime}} \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\left(\left(f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\right)^2 + \frac{1}{2} \right)\nonumber\\
    %%%%%%%%%%%%%%%%%%%%
    &=  \nabla^2_{\btheta = \btheta^{\prime}}\sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\left(\left(f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\right)^2  \right)\label{eq:Hessian-expectation}
\end{align}
We now denote the $j$-th row and $k$-th column entry of the Hessian Matrix $\nabla^2_{\btheta = \btheta^{\prime}}((f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_i(\btheta_* ; x_i, x_j, \F^{s-1}))^2)$ as $\big[\nabla^2_{\btheta = \btheta^{\prime}}((f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2)\big]_{jk}$. Then we can show that
\begin{align*}
    &\big[\nabla^2_{\btheta = \btheta_*}((f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2)\big]_{jk} \\
    %%%%%%%%%
    &\coloneqq \frac{\partial }{\partial \btheta_j} \left[\frac{\partial (f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1}))^2}{\partial \btheta_k}\right] \\
    %%%%%%%%%%%
    &= \frac{\partial }{\partial \btheta_j}\left[2(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})) \frac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right]\\
    %%%%%%%%%%%%%%%%%%%%%
    &= \frac{\partial }{\partial \btheta_j}\left[ 2f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} - 2f_i(\btheta_*)\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right]\\
    %%%%%%%%%%%%%%%%%%%%%%
    &= 2 \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\btheta_k} \\
    %%%%%%%%%%%%%%%%%%%%%
    &- 2 f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\btheta_k} - 2 \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\\
    %%%%%%%%%%%%%%%%%%%%%%%
    &= 2 \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2\left(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}
\end{align*}
Plugging this back in \cref{eq:Hessian-expectation} we get that
\begin{align*}
    \left[\nabla^2_{\btheta = \btheta^{\prime}}  \E[\ell_{s}(\btheta)|\F^{s-1}]\right]_{jk} &= 2\sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\left( \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\right. \\
    %%%%%
    &\qquad \left.+ 2\left(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}\right).
\end{align*}
\end{proof}



\begin{lemma}
\label{lemma:support-lemma3}
The sum of the difference of the Hessians $\sum_{s=1}^{t} \nabla_{\btheta = \btheta'}^{2} \ell_{s}\left(\btheta^{}\right)-\E\left[\nabla_{\btheta=\btheta'}^{2} \ell_{s}\left(\btheta^{}\right) \mid \F^{s-1}\right]$ is given by 
\begin{align*}
    \sum_{s=1}^{t}\nabla_{\btheta = \btheta'}^{2} \ell_{s}\left(\btheta^{}\right)-\E\left[\nabla_{\btheta=\btheta'}^{2} \ell_{s}\left(\btheta^{}\right) \mid \F^{s-1}\right] \!\! &=\!\! \sum_{s=1}^t\bigg( -2(Y_s - f_{I_s}(\btheta; x_i, x_j, \F^{s-1}))\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k} \\
    %%%%%%%%
    &\qquad+ 2\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} \\
    %%%%%%%%%%%%%%%%%%%%%%%
    &\qquad- 2 \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\bigg).
\end{align*}
\end{lemma}

\begin{proof}
This lemma directly follows from \Cref{lemma:support-lemma1} and \Cref{lemma:support-lemma2}.
%
First note that the difference $\nabla_{\btheta = \btheta'}^{2} \ell_{s}\left(\btheta^{}\right)-\E\left[\nabla_{\btheta=\btheta'}^{2} \ell_{s}\left(\btheta^{}\right) \mid \F^{s-1}\right]_{jk}$ is given by
\begin{align}
   &\nabla_{\btheta = \btheta'}^{2} \ell_{s}\left(\btheta^{}\right)-\E\left[\nabla_{\btheta=\btheta'}^{2} \ell_{s}\left(\btheta^{}\right) \mid \F^{s-1}\right] \nonumber\\
   %%%%%%%%%%%%%%%%%
   &\overset{(a)}{=}  2\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k} + 2\left(f_{I_s}(\btheta; x_i, x_j, \F^{s-1}) - Y_s\right)\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}\nonumber\\
    %%%%%%%%%%%%%%%%%%%%
    &\qquad- 2 \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\bigg( \dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} - \left(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\cdot\nonumber\\
    %%%%%%%%%
    &\qquad\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}\bigg)\nonumber\\
    %%%%%%%%%%%%%%%%%%%%%
    = & -2(Y_s - f_{I_s}(\btheta; x_i, x_j, \F^{s-1}))\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k} + 2\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\nonumber\\
    %%%%%%%%%%%%%%%%
    &\qquad - 2 \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} \label{eq:support-equality}
\end{align}
where, $(a)$ follows from \Cref{lemma:support-lemma1} and \Cref{lemma:support-lemma2}. Plugging this equality in \Cref{eq:support-equality} below we get
\begin{align*}
    &\sum_{s=1}^{t} \nabla_{\btheta = \btheta'}^{2} \ell_{s}\left(\btheta^{}\right)-\E\left[\nabla_{\btheta=\btheta'}^{2} \ell_{s}\left(\btheta^{}\right) \mid \F^{s-1}\right]  \\
    %%%%%%%%%%%
    &= \sum_{s=1}^t \bigg(-2(Y_s - f_{I_s}(\btheta; x_i, x_j, \F^{s-1}))\dfrac{\partial^2 f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k} + 2\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta; x_i, x_j, \F^{s-1})}{\partial \btheta_k}\\
    %%%%%%%%%%%%%%%%%%%%%%%
    &\qquad  - 2 \sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\bigg(\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j}\dfrac{\partial f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_k} - 2\left(f_{I_s}(\btheta ; x_i, x_j, \F^{s-1}) - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\cdot\\
    %%%%%
    &\qquad\dfrac{\partial^2 f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})}{\partial \btheta_j\partial\btheta_k}\bigg)\bigg).
\end{align*}
The claim of the lemma follows.
\end{proof}


\begin{lemma}
\label{lemma:matrix-conc3}
Let $\wLcal_t(\btheta_*) = \frac{1}{t}\sum_{s=1}^t \ell_{s}(\btheta_*)$ and $\nabla^2 \Lcal_t(\btheta_*) = \frac{1}{t}\sum_{s=1}^t \nabla^2\E[\ell_{s}(\btheta_*)|\F^{s-1}]$. Then we can bound the 
\begin{align*}
    \Pb&\left(\lambda_{\max}(\nabla^2\wLcal_t(\btheta_*) - \nabla^2 \Lcal_t(\theta^*)) > \sqrt{\dfrac{8C|V|^2\eta^2\lambda^2_1c \gamma \log(dt)}{t}}\right) \leq \dfrac{2}{(dt)^{\gamma}},
\end{align*}
where $c > 0$ is a constant.
\end{lemma}


\begin{proof}
This lemma is different than \citet{frostig2015competing, mukherjee2020generalized} as it requires a different concentration bound to take into account the squared loss \Cref{assm:bound-ce} and the vocabulary size.
%
Recall that $ \wLcal_t(\btheta_*) = \frac{1}{t}\sum_{s=1}^t \ell_{s}(\btheta_*)$ and $\nabla^2 \Lcal_s(\theta^*) = \nabla^2\E[\ell_{s}(\btheta_*)|\F^{s-1}]$. We define $\nabla^2 \Lcal_t(\btheta_*) = \frac{1}{t}\sum_{s=1}^t\nabla^2\E[\ell_{s}(\btheta_*)|\F^{s-1}]$. Denote,
% \begin{align*}
    $\mathbf{V}_s = 2\nabla_{\btheta = \btheta_*}f_{I_s}(\btheta; x_i, x_j, \F^{s-1})\nabla_{\btheta = \btheta_*}f_{I_s}(\btheta; x_i, x_j, \F^{s-1})^\top - 2\sum_{i=1}^{|V|} p_{\twtheta_{s-1}}(i)\nabla_{\btheta = \btheta_*}f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})\nabla_{\btheta = \btheta_*}f_{I_s}(\btheta ; x_i, x_j, \F^{s-1})^\top$.
% \end{align*}
Then we can show that,
\begin{align}
    &\Pb\left(\lambda_{\max}(\nabla^2\wLcal_t(\btheta_*) - \nabla^2 \Lcal_t(\theta^*)) > \sqrt{\dfrac{8C^2|V|^4\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
    %%%%%%%%%%%%%%%%%
    &= \Pb\left(\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}\frac{1}{t}\sum_{s=1}^t \ell_{s}(\btheta) - \frac{1}{t}\sum_{s=1}^t\nabla^2_{\btheta=\btheta_*} \E[\ell_{s}(\btheta)|\F^{s-1}]\right) > \sqrt{\dfrac{8C^2|V|^4\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
    %%%%%%%%%%%%%%%%
    &= \Pb\left(\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}\frac{1}{t}\sum_{s=1}^t\left( \ell_{s}(\btheta) - \nabla^2_{\btheta=\btheta_*} \E[\ell_{s}(\btheta)|\F^{s-1}]\right)\right) > \sqrt{\dfrac{8C^2|V|^4\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
% \end{align}
% \begin{align}
    %%%%%%%%%%%%%%%%%%
    &\overset{(a)}{\leq} \Pb\left(\lambda_{\max}\left(\frac{C|V|^2}{t}\sum_{s=1}^t\left(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right.\right. \nonumber\\
    %%%%
    &\qquad\left.\left. + \frac{C|V|^2}{t}\sum_{s=1}^t \mathbf{V}_s\right) > \sqrt{\dfrac{8C^2|V|^4\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
    %%%%%%%%%%%%%%%%%
    &\leq \Pb\left(\lambda_{\max}\left(\frac{1}{t}\sum_{s=1}^t-2\left(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right) > \frac{1}{2}\sqrt{\dfrac{8\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
    %%%%%%%%%%%%%%%%
    &\qquad + \Pb\left( \lambda_{\max}\left(\frac{1}{t}\sum_{s=1}^t \mathbf{V}_s\right) > \frac{1}{2}\sqrt{\dfrac{8\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)  \nonumber\\
    %%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%
    &\overset{(b)}{\leq} \Pb\left(\frac{1}{t}\sum_{s=1}^t-2\left(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right) > \frac{1}{2}\sqrt{\dfrac{8\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\
    %%%%%%%%%%%%%%%%%%%%%%%%%
    &\qquad + \Pb\left( \frac{1}{t}\sum_{s=1}^t \lambda_{\max}\left(\mathbf{V}_s\right) > \frac{1}{2}\sqrt{\dfrac{8\eta^2\lambda^2_1c\gamma\log(dt)}{t}}\right)\nonumber\\%\label{eq:regression-conc}
    %%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%
    &\overset{(c)}{\leq} 2\exp\left(- \dfrac{t^2 8\eta^2\lambda_1^2c\gamma\log(dt)}{4t}\cdot\dfrac{1}{2tc\eta^2\lambda_1^2}\right) \overset{(d)}{\leq} 2\left(\dfrac{1}{dt}\right)^{\gamma}.
    % \nonumber %\leq 2\delta.
    \label{eq:regression-conc}
\end{align}
where, $(a)$ follows from substituting the value of $\nabla^2_{\btheta=\btheta_*} \ell_{s}(\btheta) - \nabla^2_{\btheta=\btheta_*} \E[\ell_{s}(\btheta)|\F^{s-1}]$ from \Cref{lemma:support-lemma3}, and $(b)$ follows by triangle inequality, $(c)$ follows by using two concentration inequalities stated below, and $(d)$ follows by simplifying the equations. 
% for a constant $c_1$ subsuming the constants $\nicefrac{c}{4c'}$.

% substituting the value of $\nabla^2_{\btheta=\btheta_*} \ell_{s}(\btheta) - \nabla^2_{\btheta=\btheta_*} \E[\ell_{s}(\btheta)|\F^{s-1}]$ from \Cref{lemma:support-lemma3}, and $(c)$ follows from \Cref{lemma:matrix-conc2}.

Denote $Q_s = -2\left(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)$. Also note that $\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right) \leq \lambda_1$ for all time $s$ using \Cref{assm:thm}.
\begin{align*} 
\Pb(\sum_{s=1}^t-2&\left(Y_s - f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right)\lambda_{\max}\left(\nabla^2_{\btheta=\btheta_*}f_{I_s}(\btheta_*; x_i, x_j, \F^{s-1})\right) \geq  \epsilon) =\Pb\left(-\sum_{s=1}^{t} Q_{s} \geq \epsilon\right) \\
&=\Pb\left(e^{-\lambda \sum_{s=1}^{t} Q_{s}} \geq e^{\lambda \epsilon}\right) \overset{(a)}{\leq} e^{-\lambda \epsilon} \E\left[e^{-\lambda \sum_{s=1}^{t} Q_{s}}\right] 
%%%%%%%%%%%%%%%%%%%
=  e^{-\lambda \epsilon} \E\left[\E\left[e^{-\lambda \sum_{s=1}^{t} Q_{s}}\big|\twtheta_{t-1}\right] \right]\\
%%%%%%%%%%%%%%%%%%%%
&\overset{(b)}{=} e^{-\lambda \epsilon} \E\left[\E\left[e^{-\lambda  Q_{t}}|\twtheta_{t-1}\right]\E\left[e^{-\lambda \sum_{s=1}^{t-1} Q_{s}} \big|\twtheta_{t-1}\right]  \right]\\
%%%%%%%%%%%%%%%%%%
&\leq e^{-\lambda \epsilon} \E\left[\exp\left(2\lambda^2\lambda_1^2\eta^2\right)\E\left[e^{-\lambda \sum_{s=1}^{t-1} Q_{s}}\big |\twtheta_{t-1}\right]  \right]\\
%%%%%%%%%%%%%%%%%%
& \overset{}{=} e^{-\lambda \epsilon} e^{2\lambda^{2} \eta^{2}\lambda_1^2} \mathbb{E}\left[e^{-\lambda \sum_{s=1}^{t-1} Q_{s}}\right] \\ 
& \vdots \\ 
& \overset{(c)}{\leq} e^{-\lambda \epsilon} e^{2\lambda^{2} t \eta^{2}\lambda^2_1} 
\overset{(d)}{\leq} \exp\left(-\dfrac{2\epsilon^2}{t\eta^2\lambda_1^2}\right).
\end{align*}
where $(a)$ follows by Markov's inequality, $(b)$ follows as $Q_s$ is conditionally independent given $\twtheta_{s-1}$, $(c)$ follows by unpacking the term for $t$ times and $(d)$  follows by taking $\lambda= \epsilon / 4t\lambda_1^2\eta^2$ where $\lambda_1$ is defined in \Cref{assm:transition-matrix}. Next we bound the second term of \eqref{eq:regression-conc} below.


\begin{align*} 
\Pb(\sum_{s=1}^t \lambda_{\max}\left(\mathbf{V}_s\right) \geq  \epsilon) &=\Pb\left(\lambda\sum_{s=1}^{t} \lambda_{\max}\left(\mathbf{V}_s\right) \geq \lambda\epsilon\right) 
=\Pb\left(e^{\lambda \sum_{s=1}^{t} \lambda_{\max}\left(\mathbf{V}_s\right)} \geq e^{\lambda \epsilon}\right) \overset{(a)}{\leq} e^{-\lambda \epsilon} \E\left[e^{\lambda \sum_{s=1}^{t} \lambda_{\max}\left(\mathbf{V}_s\right)}\right] \\
%%%%%%%%%%%%%%%%%%%%%%%
&=  e^{-\lambda \epsilon} \E\left[\E\left[e^{\lambda \sum_{s=1}^{t} \lambda_{\max}\left(\mathbf{V}_s\right)}\big|\twtheta_{t-1}\right] \right]\\
%%%%%%%%%%%%%%%%%%
&\overset{(b)}{=} e^{-\lambda \epsilon} \E\left[\E\left[e^{\lambda  \lambda_{\max}(\mathbf{V}_{t})}|\twtheta_{t-1}\right]\E\left[e^{\lambda \sum_{s=1}^{t-1} \lambda_{\max}\left(\mathbf{V}_s\right)} \big|\twtheta_{t-1}\right]  \right]\\
%%%%%%%%%%%%%%%%%%%%%%
&\overset{(c)}{\leq} e^{-\lambda \epsilon} \E\left[\exp\left(2c\lambda^2\lambda^2_1\eta^2\right)\E\left[e^{\lambda \sum_{s=1}^{t-1} \lambda_{\max}\left(\mathbf{V}_s\right)}\big |\twtheta_{t-1}\right]  \right]\\
%%%%%%%%%%%%%%%%%%%%%%%
& \overset{}{=} e^{-\lambda \epsilon} e^{2c\lambda^{2} \eta^{2}\lambda_1^2} \mathbb{E}\left[e^{\lambda \sum_{s=1}^{t-1} \lambda_{\max}\left(\mathbf{V}_s\right)}\right] \\ 
& \vdots \\ 
& \overset{(d)}{\leq} e^{-\lambda \epsilon} e^{2c\lambda^{2} t \eta^{2}\lambda^2_1} \overset{(e)}{\leq} \exp\left(-\dfrac{2\epsilon^2}{tc\eta^2\lambda_1^2}\right)
\end{align*}

where $(a)$ follows by Markov's inequality, $(b)$ follows as $\lambda_{\max}(\mathbf{V}_s)$ is conditionally independent given $\twtheta_{s-1}$. In the inequality $(c)$ using the always valid upper bound of $2\lambda_1$, we have that $\E[\lambda_{\max}(\mathbf{V}_t)] \leq 2\lambda_1$. So the term in inequality $(c)$ will become 
$e^{-\lambda \epsilon} e^{2\lambda^2 t\eta^2 \lambda_1^t + 4t\lambda \lambda_1}$. Hence, we can upper bound the inequality $(c)$ by a constant $c > 0$ such that we have $\E[e^{\lambda \lambda_{\max}(V_t)} \mid \twtheta_{t-1}] \leq e^{2\lambda^2\lambda_1^2\eta^2}e^{2\lambda \times 2\lambda_1} = \exp(2\lambda^2\lambda_1^2\eta^2 + 4 \lambda \lambda_1) \leq  \exp(2c\lambda^2\lambda_1^2\eta^2)$. The inequality $(d)$ follows by unpacking the term for $t$ times and $(e)$  follows by taking $\lambda= \epsilon / 4tc\lambda_1^2\eta^2$ and $\lambda_1$ defined in \Cref{assm:transition-matrix}.
\end{proof}


\begin{lemma}
\label{lemma:inequality}
Let $\twtheta_t - \btheta_* = \left(\nabla^2 \wLcal_t(\ttheta_t)\right)^{-1}\nabla \wLcal_t(\btheta_*)$ where $\ttheta_t$ is between $\twtheta_{t}$ and $\btheta_{*}$. Then we can show that
\begin{align*}
    \left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)} \leq \left\|\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\right\|\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}} .
\end{align*}
\end{lemma}

\begin{proof}
We begin with the definition of $\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}$ as follows:
\begin{align*}
\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)} &\overset{(a)}{=} \sqrt{(\twtheta_{t}-\btheta_{*})^T\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)(\twtheta_{t}-\btheta_{*})}\\
%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(b)}{=} \sqrt{\left(\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1} \nabla \wLcal_{t}\left(\btheta_{*}\right)\right)^T\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\left(\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1} \nabla \wLcal_{t}\left(\btheta_{*}\right)\right)}\\
%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(c)}{\leq}  \left\|\nabla^2 \Lcal_{t}\left(\btheta_{*}\right)^{1/2} \left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}\nabla^2 \Lcal_{t}\left(\btheta_{*}\right)^{1/2}\right\| \sqrt{\left(\nabla^{} \wLcal_{t}\left(\btheta_{*}\right)^T\left(\nabla^{2} \Lcal_{t}(\btheta_*)\right)^{-1} \nabla \wLcal_{t}\left(\btheta_{*}\right)\right)}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
& = \left\|\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\right\|\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}} .
\end{align*}
where, $(a)$ follows as $\|x\|_{M} = \sqrt{x^T M x}$, $(b)$ follows as $\|\twtheta_t - \btheta_*\|_{\nabla^2 \Lcal_t(\btheta_*)} = \left(\nabla^2 \wLcal_t(\ttheta)\right)^{-1}\nabla \wLcal_t(\btheta_*)$, and $(c)$ follows from Cauchy Schwarz inequality.
%
The claim of the lemma follows.
\end{proof}

\begin{remark}
\label{app:remark}
The proof of \Cref{thm:main} consists of several steps. In the first step we relate $\nabla^2 \wLcal_t(\btheta)$ to $\nabla^2\Lcal_t(\btheta_*)$ for any $\btheta$ in a ball $\mathcal{B}$ around $\btheta_*$. The ball $\mathcal{B}$ is assumed in \Cref{assm:thm} to be a neighborhood where $\nabla^2 \ell_s(\btheta)$ satisfies a Lipschitz property. \Cref{assm:thm} in \Cref{app:theory} are standard and have also been made by \citet{frostig2015competing, chaudhuri2015convergence, mukherjee2020generalized}. 
Using \Cref{assm:transition-matrix} and \Cref{assm:thm}, we can show that for a large enough sequences of tokens $t$ stated in \Cref{thm:main} we have the following: (1) $\nabla^2 \Lcal_t(\btheta_*)$ lies between in the positive semidefinite order by scaled multiples of $\nabla^2 \wLcal_t(\btheta)$ for any $\btheta \in \mathcal{B}$, and (2) the empirical error minimizing $\twtheta_t$ is in the ball $\mathcal{B}$ with probability $1 - 1/t^\gamma$, which is the good event $\mathcal{E}$. Then we use a Taylor series expansion around $\twtheta_t$ and using the fact that $\nabla \wLcal_t(\wtheta(t)) = 0$ along with the relation between $\nabla^2 \wLcal_t(\btheta)$ and $\nabla^2 \Lcal_t(\btheta_*)$, we can obtain an upper bound to $\lVert \wtheta(t) - \btheta_*\rVert_{\nabla^2 \Lcal_t(\btheta_*)}$ in terms of $\lVert \nabla \wLcal_t(\btheta_*) \rVert_{(\nabla^2 \Lcal_t(\btheta_*))^{-1}}$ that can be shown to be decreasing with $t$. 
Further, $\lVert \wtheta(t) - \btheta_*\rVert_{\nabla^2 \Lcal_t(\btheta_*)}$ can also be used to obtain an upper bound to $\Lcal_t(\wtheta(t)) - \Lcal_t(\btheta_*)$ using a Taylor series expansion. 
%
Finally we can bound $\E[\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})] =\E[(\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})) I(\mathcal{E})]+\E[(\Lcal_{t}(\wtheta_{t})-\Lcal_{t}(\btheta^{*})) I(\mathcal{E}^\complement)]$ where $I(\cdot)$ is the indicator. Since $\Pb(\mathcal{E}^\complement) \leq 1/t^\gamma$, the second term can be bounded as $\max_{\btheta \in \bTheta}\left(\Lcal_{t}(\btheta)-\Lcal_{t}\left(\btheta^{*}\right)\right)/t^{\gamma}$, while the first term simplifies to $(1 + \rho_t)\sigma_t^2/t$. 
% The full proof is in  \Cref{app:theory}.
\end{remark}

\begin{customtheorem}{1}\textbf{(Restatement of main theorem)}
Suppose $\ell_{1}(\btheta), \ell_{2}(\btheta), \cdots, \ell_{t}(\btheta): \mathbb{R}^{|V|} \rightarrow \mathbb{R}$ are loss functions from a distribution that satisfies Assumptions \ref{assm:transition-matrix} , \ref{assm:bound-ce}, and \ref{assm:thm}. Define 
% \begin{align*}
    $\Lcal_t(\btheta) = \frac{1}{t}\sum_{s=1}^t\E_{x_s\sim \mathbf{p}_{\twtheta_{s-1}}}[\ell_s(\btheta)|\F^{s-1}]$
% \end{align*}
where, $\twtheta_t =\argmin_{\btheta \in \bTheta} \sum_{s = 1}^t \ell_{s}(\btheta)$. If $t$ is large enough such that $ \frac{\gamma\log(dt)}{t}\leq c^{\prime} \min \left\{\frac{1}{C_{1}C_{2} |V|^4 }, \frac{\max\limits_{\btheta \in \bTheta}\left(\!\Lcal_{t}(\btheta)\!-\!\Lcal_{t}\left(\btheta_{*}\!\right)\right)}{C_{2}}\right\}$
then for a constant $\gamma \geq 2$, universal constants $C_1,C_2,c'$,  we can show that 
\begin{align*}
\left(1-\rho_{t}\right) \frac{\sigma_t^2}{t}- \frac{C_1^2}{t^{\gamma / 2}} 
&\leq \E\left[\Lcal_t(\twtheta_t)-\Lcal_t\left(\btheta_{*}\right)\right] \leq \left(1+\rho_{t}\right) \frac{\sigma_t^2}{t}\!+\!\frac{\max\limits_{\btheta \in \bTheta}\left(\!\Lcal_{t}(\btheta)\!-\!\Lcal_{t}\left(\btheta_{*}\!\right)\right)}{t^{\gamma}},
\end{align*}
where 
$\sigma^{2}_t \coloneqq \E_{}\left[\frac{1}{2}\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right)^{-1}}^{2}\right]$, 
and $\rho_t \coloneqq \left(C_1C_2 + 2\eta^2\lambda_1^2\right)\sqrt{\frac{\gamma\log(dt)}{t}}$.
\end{customtheorem}



\begin{proof}
\textbf{Step 1:} We first bound the $\left\|\nabla^{2} \wLcal_{t}(\btheta)-\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right\|_{*}$ as follows
\begin{align}
\left\|\nabla^{2} \wLcal_{t}(\btheta)-\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right\|_{*} & \overset{(a)}{\leq}\left\|\nabla^{2} \wLcal_{t}(\btheta)-\nabla^{2} \wLcal_{t}\left(\btheta_{*}\right)\right\|_{*}+\left\|\nabla^{2} \wLcal_{t}\left(\btheta_{*}\right)-\nabla^{2} \Lcal_t\left(\btheta_{*}\right)\right\|_{*} \nonumber\\
%%%%%%%%%%%%%%%%%%%%%%%%%
& \overset{(b)}{\leq} C_{1}\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_t\left(\btheta_{*}\right)}+ \sqrt{\dfrac{8C^2|V|^4\eta^2\lambda_1^2c \gamma\log(dt)}{t}}\label{eq:1}
\end{align}
where, $(a)$ follows from triangle inequality, and $(b)$ is due to \Cref{assm:thm}.3.d and \Cref{lemma:matrix-conc3}.

\textbf{Step 2 (Approximation of $\nabla^{2} \Lcal_t\left(\btheta_{*}\right)$):} By choosing a sufficiently smaller ball $\mathcal{B}_{1}$ of radius of $\min \left\{1 /\left(10 C_{1}\right), \right.$ diameter $\left.(\mathcal{B})\right\}$ ), the first term in \eqref{eq:1} can be made small for $\btheta \in \mathcal{B}_{1}$. Also, for sufficiently large $t$, the second term in \eqref{eq:1} can be made arbitrarily small (smaller than $1 / 10$ ), which occurs if $\sqrt{\frac{\gamma \log (dt)}{t}} \leq \frac{c^{\prime}}{\sqrt{2C^2|V|^4\eta^2\lambda_1^2}}$. Hence for large $t$ and $\btheta\in \mathcal{B}_1$ we have 
\begin{align}
    \frac{1}{2} \nabla^{2} \wLcal_{t}(\btheta) \preceq \nabla^{2} \Lcal_t\left(\btheta_{*}\right) \preceq 2 \nabla^{2} \wLcal_{t}(\btheta) \label{eq:relation}
\end{align}

\textbf{Step 3 (Show $\twtheta_t$ in $\mathcal{B}_1$):} Fix a $\ttheta$ between $\btheta$ and $\btheta_*$ in $\mathcal{B}_1$. Apply Taylor's series approximation
\begin{align*}
    \wLcal_{t}(\btheta)=\wLcal_{t}\left(\btheta_{*}\right)+\nabla \wLcal_{t}\left(\btheta_{*}\right)^{\top}\left(\btheta-\btheta_{*}\right)+\frac{1}{2}\left(\btheta-\btheta_{*}\right)^{\top} \nabla^{2} \wLcal_{t}(\ttheta)\left(\btheta-\btheta_{*}\right)
\end{align*}
We can further reduce this as follows:
\begin{align}
\wLcal_{t}(\btheta)-\wLcal_{t}\left(\btheta_{*}\right) &\overset{(a)}{=}\nabla \wLcal_{t}\left(\btheta_{*}\right)^{\top}\left(\btheta-\btheta_{*}\right)+\frac{1}{2}\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \wLcal_t(\ttheta)}^{2} \nonumber\\
%%%%%%%%%%%%%%%%%%%%%
& \overset{(b)}{\geq} \nabla \wLcal_{t}\left(\btheta_{*}\right)^{\top}\left(\btheta-\btheta_{*}\right)+\frac{1}{4}\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}^{2} \nonumber\\
%%%%%%%%%%%%%%%%%%
&\geq -\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}} + \frac{1}{4}\left(\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}\right)^{\top}\left(\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}\right)\nonumber\\
%%%%%%%%%%%%%%%%%%
& =\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}\left(-\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}+\frac{1}{4}\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}\right) \label{eq:2}
\end{align}
where, $(a)$ follows as $\left\|\btheta-\btheta_{*}\right\|_{\nabla^{2} \wLcal_t(\ttheta)}^{2}\coloneqq \left(\btheta-\btheta_{*}\right)^{\top} \nabla^{2} \wLcal_{t}(\ttheta)\left(\btheta-\btheta_{*}\right)$, and $(b)$ follows as $\ttheta$ is in between $\btheta$ and $\btheta_*$ and then using \eqref{eq:relation}. 
Note that in \eqref{eq:2} if the right hand side is positive for some $\btheta \in \mathcal{B}_{1}$, then $\btheta$ is not a local minimum. Also, since $\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\| \rightarrow 0,$ for a sufficiently small value of $\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|,$ all points on the boundary of $\mathcal{B}_{1}$ will have values greater than that of $\btheta_{*} .$ Hence, we must have a local minimum of $\wLcal_{t}(\btheta)$ that is strictly inside $\mathcal{B}_{1}$ (for $t$ large enough). We can ensure this local minimum condition is achieved by choosing an $t$ large enough so that $\sqrt{\frac{\gamma \log (dt)}{t}} \leq c^{\prime} \min \left\{\frac{1}{C_{1}C_{2} }, \frac{\operatorname{diameter}(\mathcal{B})}{C_{2}}\right\},$ using \Cref{lemma:vector-conc} (and
our bound on the diameter of $\mathcal{B}_{1}$ ). By convexity, we have that this is the global minimum, $\twtheta_{t},$ and so $\twtheta_{t} \in \mathcal{B}_{1}$ for $t$ large enough. We will assume now that $t$ is this large from here on.



\textbf{Step 4 (Bound $\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_t\left(\btheta_{*}\right)}$):} For the $\twtheta(t)$ that minimizes the sum of squared errors, $0=\nabla \wLcal_{t}(\twtheta_{t})$. Again, using Taylor's theorem if $\twtheta_{t}$ is an interior point, we have:
\begin{align}
0=\nabla \wLcal_{t}(\twtheta_{t})=\nabla \wLcal_{t}\left(\btheta_{*}\right)+\nabla^{2} \wLcal_{t}(\ttheta_{t})\left(\twtheta_{t}-\btheta_{*}\right)\label{eq:taylor}
\end{align}
for some $\ttheta_{t}$ between $\btheta_{*}$ and $\twtheta_{t}$. Now observe that $\ttheta_{t}$ is in $B_{1}$ (since, for $t$ large enough, $\twtheta_{t} \in \mathcal{B}_{1}$ ). Thus it follows from \eqref{eq:taylor} that,
\begin{align}
\twtheta_{t} - \btheta_{*}=\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1} \nabla \wLcal_{t}\left(\btheta_{*}\right)    \label{eq:erm}
\end{align}
where the invertibility is guaranteed by \eqref{eq:relation} and the positive definiteness of $\nabla^2 \Lcal_{t}\left(\btheta_{*}\right)$ (by \Cref{assm:thm} (3c)). We finally derive the upper bound to $\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}$ as follows
\begin{align}
\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(a)}{\leq} \left\|\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\right\|\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}} \nonumber\\
%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(b)}{\leq} c C_{2} \sqrt{\frac{\gamma \log (dt)}{t}} \label{eq:3}
\end{align}
where $(a)$ follows from \Cref{lemma:inequality}, and $(b)$ from \Cref{lemma:vector-conc}, \eqref{eq:2}, and $c$ is some universal constant.

\textbf{Step 5 (Introducing $\tz$):} Fix a $\tz_t$ between $\btheta_*$ and $\twtheta_t$. Apply Taylor's series 
\begin{align}
    \Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)=\frac{1}{2}\left(\twtheta_{t}-\btheta_{*}\right)^{\top} \nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\left(\twtheta_{t}-\btheta_{*}\right) \label{eq:Pt-z}
\end{align}
Now note that both $\ttheta_{t}$ and $\tz_{t}$ are between $\twtheta_{t}$ and $\btheta_{*},$ which implies $\ttheta_{t} \rightarrow \btheta_{*}$ and $\tz_{t} \rightarrow \btheta_{*}$ since $\twtheta_{t} \rightarrow \btheta_{*}$. By \eqref{eq:1} and \eqref{eq:3} and applying the concentration inequalities give us
\begin{align}
&\left\|\nabla^{2} \wLcal_{t}(\ttheta_{t})-\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right\|_{*} \leq \rho_t \label{eq:theta-ttheta}\\
%%%%%%%%%%%%%%%%%%%
&\left\|\nabla^{2} \Lcal_{t}\left(\tz_{t}\right)-\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right\|_{*} \leq C_{1}\left\|\tz_{t} - \btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)} \leq \rho_t \label{eq:theta-tz}
\end{align}
where $\rho_t=c\left(C_{1}C_{2} + 2\eta^2\lambda_1^2\right) \sqrt{\frac{\gamma \log (dt)}{t}}$.



\textbf{Step 6 (Define $\bM_{1, t}$ and $\bM_{2, t}$):} It follows from the inequality \eqref{eq:theta-ttheta} that 
\begin{align*}
&\nabla^{2} \wLcal_{t}(\ttheta_{t}) \preceq\left(1+\rho_t\right) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)
\implies \nabla^{2} \wLcal_{t}(\ttheta_{t}) - \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right) \preceq \rho_t \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right) \\
&\implies \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)^{-1/2}(\wLcal_{t}(\ttheta_{t}) - \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)^{-1/2} \preceq \rho_t I
\\
&\implies \lVert \nabla^2\wLcal_{t}(\ttheta_{t}) - \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right) \rVert_* \leq \rho_t.
\end{align*}
Then we can use the inequalities \eqref{eq:theta-ttheta} and \eqref{eq:theta-tz} to show that
\begin{align*}
&\left(1-\rho_t\right) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right) \preceq \nabla^{2} \wLcal_{t}(\ttheta_{t}) \preceq\left(1+\rho_t\right) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\\
%%%%%%%%%%%%%%
&\left(1-\rho_t\right) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right) \preceq \nabla^{2} \Lcal_{t}\left(\tz_{t}\right) \preceq\left(1+\rho_t\right) \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right).
\end{align*}
Now we define the two quantities $\bM_{1, t}$ and $\bM_{2, t}$ as follows:
\begin{align*}
\bM_{1, t} &:= \left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2}\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{1 / 2} \\
\bM_{2, t} &:= \left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1 / 2} \nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1 / 2}.
\end{align*}


\textbf{Step 7 (Lower bound $\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)$):} Now for the lower bound it follows from \Cref{eq:Pt-z} that
\begin{align*}
    \Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right) & = \frac{1}{2}\left(\twtheta_{t}-\btheta_{*}\right)^{\top} \nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\left(\twtheta_{t}-\btheta_{*}\right)\\
    %%%%%%%%%%%%%%%%%%%%%%%%
    % &=\frac{1}{2}\left(\twtheta_{t}-\btheta_{*}\right)^{\top} \nabla^2 \Lcal_t(\btheta_*) \nabla^2 \Lcal_t(\btheta_*)^{-1}\nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\left(\twtheta_{t}-\btheta_{*}\right)\\
    % %%%%%%%%%%%%%%%%%%%%%%%%%
    % &= \frac{1}{2}\left(\twtheta_{t}-\btheta_{*}\right)^{\top}\nabla^2 \Lcal_t(\btheta_*)^{-1}\nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\nabla^2 \Lcal_t(\btheta_*)\left(\twtheta_{t}-\btheta_{*}\right)\\
    %%%%%%%%%%%%%%%%%%%%%%%
    &= \frac{1}{2}\left(\twtheta_{t}-\btheta_{*}\right)^{\top}\nabla^2 \Lcal_t(\btheta_*)^{\frac{1}{2}}\nabla^2 \Lcal_t(\btheta_*)^{-\frac{1}{2}}\nabla^{2} \Lcal_{t}\left(\tz_{t}\right)\nabla^2\Lcal_t(\btheta_*)^{-\frac{1}{2}} \nabla^2 \Lcal_t(\btheta_*)^{\frac{1}{2}}\left(\twtheta_{t}-\btheta_{*}\right)\\
    %%%%%%%%%%%%%
    &\overset{(a)}{=} \frac{1}{2} \mathbf{u}^T \mathbf{M}_{2,t} \mathbf{u}
\end{align*}
where, in $(a)$ we define the vector $\mathbf{u} := \left(\twtheta_{t}-\btheta_{*}\right)^{\top}\nabla^2 \Lcal_t(\btheta_*)^{\frac{1}{2}}$. Now observe from the definition of and then using the min-max theorem we can show that
\begin{align*}
\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right) & \geq \frac{1}{2} \lambda_{\min }\left(\bM_{2, t}\right) \mathbf{u}^T\mathbf{u}\\
%%%%%%%%%%%%%%%%%%%%%%%%%
& = \frac{1}{2} \lambda_{\min }\left(\bM_{2, t}\right)\left\|\twtheta_{t}-\btheta_{*}\right\|_{\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)}^{2} \\
%%%%%%%%%%%%%%%%%%%%%%%
&\overset{}{=}\frac{1}{2} \lambda_{\min }\left(\bM_{2, t}\right)\left\|\nabla^{2} \wLcal_{t}(\ttheta_{t})\left(\twtheta_{t}-\btheta_{*}\right)\right\|_{\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1} \nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\left(\nabla^{2} \wLcal_{t}(\ttheta_{t})\right)^{-1}}^{2} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%
& \geq \frac{1}{2}\left(\lambda_{\min }\left(\bM_{1, t}\right)\right)^{2} \lambda_{\min }\left(\bM_{2, t}\right)\left\|\nabla^{2} \wLcal_{t}(\ttheta_{t})\left(\twtheta_{t}-\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{(a)}{=}\frac{1}{2}\left(\lambda_{\min }\left(\bM_{1, t}\right)\right)^{2} \lambda_{\min }\left(\bM_{2, t}\right)\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2}
\end{align*}
where, in $(a)$ we use the \cref{eq:erm}.
%\smnote{talk about minmax theorem from reference fr 1st inequality}

\textbf{Step 8:} Define $I(\mathcal{E})$ as the indicator that the desired previous events hold, which we can ensure with probability greater than $1-2\left(\dfrac{1}{dt}\right)^{\gamma}$. Then we can show that:

\begin{align*}
 \E\left[\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right] 
\geq & \E\left[\left(\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right) I(\mathcal{E})\right] \\
%%%%%%%%%%%%%%%%%%%%%%%
\geq & \frac{1}{2} \E\left[\left(\lambda_{\min }\left(\bM_{1, t}\right)\right)^{2} \lambda_{\min }\left(\bM_{2, t}\right)\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\mathcal{E})\right] \\
%%%%%%%%%%%%%%%%%%%%%%%%%
\geq &\left(1-c^{\prime} \rho_t\right) \frac{1}{2} \E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\mathcal{E})\right] \\
%%%%%%%%%%%%%%%%%%%%%%%%%%
=&\left(1-c^{\prime} \rho_t\right) \frac{1}{2} \E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2}(1-I(\operatorname{not} \mathcal{E}))\right] \\
%%%%%%%%%%%%%%%%%%%%%%%%%
\overset{(a)}{=}&\left(1-c^{\prime} \rho_t\right)\left(\sigma^{2}_t-\frac{1}{2} \E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\operatorname{not} \mathcal{E})\right]\right) \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geq &\left(1-c^{\prime} \rho_t\right) \sigma^{2}_t-\E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\operatorname{not} \mathcal{E})\right]
\end{align*}
where, in $(a)$ we have $\sigma^2_t:= \left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2}$, and $c'$ is an universal constant.

\textbf{Step 9:} Define the random variable $Z=\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}$. With a failure event probability of less than $2\left(\dfrac{1}{dt}\right)^{\gamma}$ for any $z_{0},$ we have:
\begin{align*}
\mathbb{E}\left[Z^{2} I(\operatorname{not} \mathcal{E})\right] &=\E\left[Z^{2} I(\operatorname{not} \mathcal{E}) I\left(Z^{2} < z_{0}\right)\right]+\E\left[Z^{2} I(\operatorname{not} \mathcal{E}) I\left(Z^{2} \geq z_{0}\right)\right] \\
& \leq z_{0} \E[I(\operatorname{not} \mathcal{E})]+\E\left[Z^{2} I\left(Z^{2} \geq z_{0}\right)\right] \\
& \leq \frac{z_{0}}{2 t^{\gamma}}+\E\left[Z^{2} \frac{Z^{2}}{z_{0}}\right] \\
& \leq \frac{z_{0}}{2 t^{\gamma}}+\frac{\E\left[Z^{4}\right]}{z_{0}} \\
& \leq \frac{\sqrt{\E\left[Z^{4}\right]}}{t^{\gamma / 2}}
\end{align*}
where $z_{0}=t^{\gamma / 2} \sqrt{\mathbb{E}\left[Z^{4}\right]}$.

\textbf{Step 10 (Upper Bound): } For an upper bound we have that:
\begin{align*}
\E\left[\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right] &=\E\left[\left(\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right) I(\mathcal{E})\right]+\E\left[\left(\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right) I(\operatorname{not} \mathcal{E})\right] \\
& \leq \E\left[\left(\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right) I(\mathcal{E})\right]+\frac{\max_{\btheta \in \bTheta}\left(\Lcal_{t}(\btheta)-\Lcal_{t}\left(\btheta_{*}\right)\right)}{t^{\gamma}}
\end{align*}
since the probability of not $\mathcal{E}$ is less than $\dfrac{1}{t^{\gamma}}$. Now for an upper bound of the first term, observe that
\begin{align*}
\E\left[\left(\Lcal_{t}(\twtheta_{t})-\Lcal_{t}\left(\btheta_{*}\right)\right) I(\mathcal{E})\right] 
\leq & \frac{1}{2} \E\left[\left(\lambda_{\max }\left(\bM_{1, t}\right)\right)^{2} \lambda_{\max }\left(\bM_{2, t}\right)\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\mathcal{E})\right] \\
\leq &\left(1+c^{\prime} \rho_t\right) \frac{1}{2} \E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2} I(\mathcal{E})\right] \\
\leq &\left(1+c^{\prime} \rho_t\right) \frac{1}{2} \E\left[\left\|\nabla \wLcal_{t}\left(\btheta_{*}\right)\right\|_{\left(\nabla^{2} \Lcal_{t}\left(\btheta_{*}\right)\right)^{-1}}^{2}\right] \\
=&\left(1+c^{\prime} \rho_t\right) \frac{\sigma^{2}_t}{t}
\end{align*}
where, $c'$ is another universal constant.
\end{proof}