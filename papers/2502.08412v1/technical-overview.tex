\section{Technical Overview}\label{sec:technical overview}
In this section, we overview the main technical contributions of this paper.
Our mechanism \mechname (detailed in \Cref{alg:mechanism}) ensures strong performance by imposing threats of \textit{future punishments}. Its deployment requires estimating each agent's fair share, that is the probability of winning had all agents reported truthfully, in an {online} manner, which we resolve using \textit{flags}. Besides these algorithmic components, we also develop novel analysis techniques to circumvent correlations between agent strategic reports and the estimation procedure.

\paragraph{Future Punishment.}
Both dominant frameworks for non-monetary resource allocations in the literature -- artificial currencies \citep{gorokh2021monetary} and future promises \citep{balseiro2019multiagent,blanchard2024near} -- involve calculations using agents' utility distributions. It is unclear how to estimate these from strategic reports while preserving a reasonable social welfare performance.
A workaround proposed by \citet{gorokh2019remarkable} and adopted by follow-up works \citep{banerjee2023robust,fikioris2023online} is to focus on another metric called \textit{$\beta$-Utopia}: given any \textit{fixed a-priori} $\bm \alpha=(\alpha_i)_{i\in [K]}$, the utility of every agent is at least $\beta$ fraction their best-possible utility under the constraint that any agent $i$ must be allocated \textit{w.p.} $\alpha_i$. In practice, however, selecting $\bm\alpha$ to maximize social welfare is highly non-trivial since no choice of $\bm \alpha$ ensures sub-linear regret for every possible set of utility distributions.
Therefore, we instead directly focus on the social welfare regret.

Intuitively, we use audits to impose threats of future punishments. Precisely, if the winner in round $t\in[T]$ is caught lying ($v_{t,i}\neq u_{t,i}$) we eliminate this agent permanently, hence they receive zero future utility.
\citet{yin2022online} also used this idea in the special case where agents all share the same utility distribution (albeit unknown), in which case unilateral deviations are identified easily as outliers (see \Cref{sec:related work}).
Unfortunately, this idea does not carry over to the general case. Our workaround is an \textit{adaptive probabilistic audit scheme} that incentivizes trustworthiness, which (roughly speaking) audits the winner $i\in[K]$ \textit{w.p.} $(\E[\text{future gain of }i])^{-1}$. Under this scheme, if an agent lies, their potential loss $(\E[\text{future gain of }i])^{-1}\times (\E[\text{future gain of }i])\gtrsim 1$ would be larger than their immediate gain; meanwhile, $\sum_t (\E[\text{future gain of }i])^{-1}\approx \sum_t (T-t)^{-1}=\O(\log T)$, hence the expected number of audits is logarithmic. A more detailed exposition is given in \Cref{sec:description of check prob}.

\paragraph{Estimating Fair Shares Online via Flagging.}
Without agents' distributions, one cannot directly compute $\E[\text{future gain of }i]$ but needs to estimate it, which is closely related to the \textit{fair share} of agents $\mu_i=\E[\text{gain of }i\text{ for one round when all agents are truthful}]$. In the hypothetical case where all agents were truthful, $\O(\log T)$ audits for agent $i$ would give a multiplicatively close estimate $\hat \mu_i\in [\frac 12 \mu_i,2\mu_i]$ \textit{w.p.} $1-\frac{1}{\text{poly}(T)}$. Unfortunately, the failure probability $\frac{1}{\text{poly}(T)}$ can be significantly amplified due to strategic agents: the estimation $\hat \mu_i$ might change drastically due to other agents' manipulated reports; moreover, even without direct manipulation, agents can still anticipate whether $\hat \mu_i$ would be inaccurate by computing the failure probability \emph{conditioned on the history}. Knowing these, agents may plan ahead for possible estimation errors $\hat \mu_i\not \in [\frac 12\mu_i,2\mu_i]$ earlier in the game. Therefore, the estimate $\hat \mu_i$ is much more likely to be biased compared to the non-strategic case.

To resolve the coupling between agents' strategic behaviors and planner's online estimations, we add an additional algorithmic component that allows any agent to \textit{flag} a generated estimate when they observe biases. If every agent flags selflessly (for example, whenever $\hat\mu_i\notin[\frac{1}{2}\mu_i,2\mu_i]$), we can avoid {any} estimation error and consequently ensure incentive-compatibility. Fortunately, as we shall elaborate in \Cref{sec:description of est and flag}, our mechanism is designed so that adopting such a well-behaved flagging behavior is perfectly aligned with each agent's individual incentive.

\paragraph{Reduction to Auxiliary Game.}
In addition to the algorithmic innovations described above, we also introduce proof techniques that may be of independent interest. Previous works on non-monetary mechanism design with distributional information focused on showing that truthful reporting is an equilibrium \citep{balseiro2019multiagent,blanchard2024near} or at least, an approximate equilibrium \citep{gorokh2021monetary}. In light of the revelation principle, this is in fact without loss of generality. However, without priors on utility distributions, one needs to design a mechanism that would achieve high social welfare for a large class of agent distributions. This precludes using the revelation principle, which is also the reason why one cannot always ensure that truthful reporting is an equilibrium. For a concrete and simple example, consider the case of two agents with fixed utility $\frac 13$ and $\frac 23$ respectively, but the mechanism does not know which has utility $\frac 23$. Under truthful reporting, the agent with utility $\frac 13$ receives $0$ utility throughout the game, while they could hope to win at least once by sending reports as if their true utility were $\frac 23$, the same as the other agent.

To characterize the equilibrium, we instead reduce the allocation problem to an auxiliary game where each agent is only allowed to report false utilities near the end of the game and are immediately eliminated if they win by doing so. Denoting by $\bm \pi^\ast$ the PBE in this auxiliary game, the proof structure for bounding the regret of our mechanism is as follows:
\begin{enumerate}
\item \textbf{Correspondence between Games.} Every joint strategy in this auxiliary game corresponds to a joint strategy in the original game which has the same utility profile for every agent (\Cref{lem:equiv between actual and no-flagging}).
\item \textbf{$\bm \pi^\ast$ Ensures High Social Welfare.} The strategic actions allowed in the auxiliary game (dishonest behaviors adopted by $\bm \pi^\ast$) do not significantly affect social welfare (\Cref{lem:u lower and upper bound}).
\item \textbf{$\bm \pi^\ast$ is also a PBE in Original Game.} Agents' unilateral deviation from $\bm\pi^\ast$ to strategic actions that were prohibited in the auxiliary game, does not improve their gain (see \Cref{lem:min report with u is good}).
\end{enumerate}

This general strategy, as well as omitted proof details, may be of independent interest for other mechanism design problems in which the revelation principle cannot be directly applied. As a side note, while we show that the strategic behaviors in $\bm \pi^\ast$ do not significantly impact the regret, these may increase the expected number of audits. These are bounded separately by carefully analyzing each agents' incentives in the original game. More details are given in \Cref{sec:sketch varying-p}.
