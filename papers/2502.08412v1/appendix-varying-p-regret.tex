\section{Characterization of a PBE for \mechname (\Cref{thm:equivalence of PBE})}\label{sec:appendix varying-p}

Towards proving our main result \Cref{thm:deterministic main thm}, we first aim to characterize a PBE for agent joint strategies in the game induced by \mechname. Following the proof structure described in \Cref{sec:sketch varying-p} (Part I), in this section we prove \Cref{thm:equivalence of PBE}.

\subsection{Additional Notations}\label{sec:appendix additional notations varying-p}

For convenience, we define a few additional notations for agent strategies.

\begin{definition}[Additional Notations for Agents' Strategies]
For any agent $i\in [K]$, in analog to $\bm \pi_i=(\pi_{t,i})_{t\in [T]}$, notation $\bm \pi_{-i}=(\pi_{t,j})_{t\in [T],j\ne i}$ contains the strategies of all agents other than $i$.

Given $\bm \pi_i^1=(\pi_{t,i}^1)_{t\in [T]}$ and $\bm \pi_{-i}^2=(\pi_{t,j}^2)_{t\in [T],j\ne i}$, we use $\bm \pi_i^1 \circ \bm \pi_{-i}^2$ to denote the corresponding joint strategy obtained by concatenation, that is, for every round $t\in [T]$, agent $i$ follows $\pi_{t,i}^1$ while agent $j\ne i$ follows $\pi_{t,j}^2$.

Given joint strategies $\bm \pi^1$ and $\bm \pi^2$, we use $\bm \pi^1\diamond_t \bm \pi^2$ to denote the joint strategy of following $\bm \pi^1$ up to round $t$ and following $\bm \pi^2$ afterwards, i.e., $\bm \pi^1\diamond_t \bm \pi^2=(\pi_{\tau,i})_{\tau\in [T],i\in [K]}$ where $\pi_{\tau,i}=\pi_{\tau,i}^1$ for $\tau\leq t$ and $\pi_{\tau,i}=\pi_{\tau,i}^2$ for $\tau>t$.
\end{definition}

Next, we recall the definition of the fair allocation probability $q_{t,i}$ for a round $t\in[T]$ and an agent $i\in[K]$, which we complement with the fair share $\mu_{t,i}$. By definition, for any round $t\in [T]$ and agent $i\in [K]$, we define
\begin{align}
\mu_{t,i}&= \E_{\bm u_t\sim \mV}\bigg [u_{t,i}\mathbbm{1}[i\in \mA_t] \1[u_{t,i}\geq c]\mathbbm{1}[u_{t,i}>u_{t,j},\forall j\in \mA_t\setminus \{i\}]\bigg ] \nonumber\\
q_{t,i}&=\Pr_{\bm u_t\sim \mV}\bigg \{(i\in \mA_t)\wedge (u_{t,i}\geq c) \wedge (u_{t,i}>u_{t,j},\forall j\in \mA_t\setminus \{i\})\bigg \}.\label{eq:fair share}
\end{align}
Although we write them as $\mu_{t,i}$ and $q_{t,i}$, they in fact both depend on the specific realization of $\mA_t$ during the execution of \Cref{alg:mechanism}, rather than only depending on the round number $t$.

We recall that the mechanism \mechname is formally described in \Cref{alg:mechanism}. For any strategy $\bm \pi=(\bm \pi^r,\bm \pi^f)$ under the mechanism \mechname, we can also rewrite the V-function defined in \Cref{eq:V-func general} through its \textit{Bellman form}: For any eliminated agent $i\not \in \mA_t$, $V_i^{\bm \pi}(\mH_t)=0$. For any alive agent $i\in \mA_t$, its V-function under $\bm \pi$ starting from history $\mH_t$ is
\begin{align}
V_i^{\bm \pi}(\mH_t)=\E\left [\sum_{\tau=t}^T u_{\tau,i}\mathbbm{1}[i_{\tau}=i]\middle \vert \mH_t\right ]=\E\big [u_{t,i}\mathbbm{1}[v_{t,i}>v_{t,j},\forall j\in \mA_t\setminus \{i\}]+V_i^{\bm \pi}(\mH_{t+1})~\big \vert ~ \mH_t\big ],\label{eq:V-func recursive}
\end{align}
where the randomness lies in the generation of utilities $\bm u_t\sim \mV$, reports $\bm v_t\sim \bm \pi_t^r$, flags $\bm f_t\sim \bm \pi_t^f$, $(i_t,o_t)$, and the corresponding next-round history $\mH_{t+1}$ generated according to \Cref{alg:mechanism}.

\subsection{Formal Setup for the Auxiliary Game (\Cref{def:auxiliary game})}\label{sec:appendix auxiliary game}
We recall that the auxiliary game was defined in \Cref{def:auxiliary game} as follows. We also include a formal pseudo-code of this mechanism in \Cref{alg:auxiliary game}. 

We specialize our definition of agents' strategies in \Cref{def:agent_strat_mechanism} to the auxiliary game as follows, where we recall that we defined $p_{t,i}:= \frac{2K^2}{(T-t)q_{t,i} c} $ for $t\in[T]$ and $i\in[K]$ within \Cref{def:auxiliary game}.
\begin{definition}[Agents' Strategies in Auxiliary Game]
We call $\tilde \mH_t:= (t,\mA_t)$ the simplified history at the beginning of round $t$, and $\tilde H_t:=\{t\}\times 2^{[K]}$ the corresponding simplified history space.
A {report strategy} in the auxiliary game \Cref{def:auxiliary game} for agent $i\in[K]$ is a sequence of measurable functions $\bm\pi_i^r:=(\pi^r_{t,i})_{t\in[T]}$ where
\begin{equation*}
\pi^r_{t,i}:[0,1]\times \tilde H_t\times \Xi \to \begin{cases}
[0,u_{t,i}],&\quad \text{if }p_{t,i}\leq 1\\
[0,1],&\quad \text{if }p_{t,i}> 1
\end{cases}.
\end{equation*}
Their report in round $t\in[T]$ is $v_{t,i}:=\pi^r_{t,i}(u_{t,i},\mH_t^i,\xi^r_{t,i})$, where $\xi^r_{t,i}\sim\mD_\xi$ is sampled independently from other random variables. There is no answers in this game, i.e., $F=\{0\}$.
A {joint strategy} for agents is a collection of all agent's report strategies $\bm\pi^r:=(\bm \pi_{i}^r)_{i\in[K]}$.
\end{definition}

\begin{algorithm}[!t]
\caption{Auxiliary Game}
\label{alg:auxiliary game}
\begin{algorithmic}[1]
\Require{Number of rounds $T$, number of agents $K$}
\Ensure{Allocations $i_1,i_2,\ldots,i_T\in \{0\}\cup [K]$}
\State Initialize the set of alive agents $\mA_1=[K]$.
\For{$t=1,2,\ldots,T$}
\State Collect reports $v_{t,i}\in [0,1]$ from $i\in \mA_t$ and allocate to agent $i_t=\argmax_{i\in \mA} v_{t,i}$.
\If{$v_{t,i_t}>u_{t,i_t}$}
\State Eliminate agent $i_t$ permanently by updating $\mA_{t+1}\gets \mA_t\setminus \{i_t\}$.
\Else
\State Everyone stays alive, that is, set $\mA_{t+1}\gets \mA_t$.
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Throughout the proof, to highlight the difference between the auxiliary game (\Cref{def:auxiliary game}) and the original game, we write down the following three restrictions that we described in the main text:
\begin{enumerate}
\item \textbf{Reports only depend on $(t,\mA_t)$.} Agents are forced to decide their reports $v_{t,i}$ only depending on their own private utilities $u_{t,i}$, the current round number $t$, and the set of alive agents $\mA_t$. \label{item:only simplified history formal}
\item \textbf{No Mark Up Unless $p_{t,i}>1$.} When $p_{t,i}=\frac{2K^2}{(T-t)q_{t,i} c} \leq 1$ (\textit{cf.} $\hat p_{t,i}=\min\big (\frac{8K^2}{(T-t) \hat q_i c},1\big )$ defined in \Cref{alg:mechanism}), agents must have their reports no larger than actual utilities, \textit{i.e.}, $v_{t,i}\le u_{t,i}$. \label{item:no mark up unless p>=1 formal}
\item \textbf{Eliminate Immediately on Mark Up.} In case an agent $i$ wins by marking up, \textit{i.e.}, $i_t=i$ and their report $v_{t,i}>u_{t,i}$, they are eliminated for all future rounds. In comparison, in the original game, agent $i$ would be eliminated with probability $\hat p_{t,i}\le 1$ (see \Cref{line:randomly_check} of \Cref{alg:mechanism}).\label{item:eliminate immediately formal}
\end{enumerate}

In the auxiliary game, the V-function only depends on the simplified history $\tilde \mH_t$ since agents' and the mechanism' actions only depend on $\tilde \mH_t$. Therefore, for a joint strategy ${\bm \pi}^r$, the agents' V-functions under ${\bm \pi}^r$ starting from simplified history $\tilde \mH_t$ can be written as
\begin{equation*}
\tilde V_i^{{\bm \pi}^r}(\tilde \mH_t)=\E\left [\sum_{\tau=t}^T u_{t,i}\mathbbm{1}[i_t=i]\middle \vert \tilde \mH_t\right ],
\end{equation*}
where agents utilities $\bm u_t,\bm u_{t+1},\ldots,\bm u_T\overset{\text{i.i.d.}}{\sim}\mV$, reports $\bm v_t\sim {\bm \pi}^r_t,\bm v_{t+1}\sim {\bm \pi}^r_{t+1},\ldots,\bm v_T\sim {\bm \pi}^r_T$, and histories $\tilde \mH_{t+1}=(t+1,\mA_{t+1}),\ldots,\tilde \mH_T=(T,\mA_T)$ are computed according to \Cref{def:auxiliary game}.

\subsection{Proof of Correspondence between Auxiliary Game and Original Game (\Cref{lem:equiv between actual and no-flagging})}\label{sec:equiv between actual and no-flagging formal}
For the ease of presentation, we restate the following definition that appeared as \Cref{def:well-behaved flagging}:
\begin{definition}[Well-Behaved Answer Strategy]\label{def:well-behaved flagging formal}
For an agent $i\in [K]$, define ${\bm \pi}_i^{\ast,f}=(\pi_{t,i}^{\ast,f})_{t\in [T]}$ as the strategy of agent $i$ that flags $f_{t,i}=1$ if and only if either of the following happens:
\begin{itemize}
\item $i_t\neq i$ and $\hat q_{t+1,i_t}>4q_{t+1,i_t}$ (the $\hat q_{t+1,i_t}$ defined in \Cref{line:estimate winning prob} can be computed via agent $i$'s observable history $\mH_t^i$ and is thus measurable),\footnote{As flagging only happens when nobody is eliminated in this round, we do not need to distinguish between $q_{t+1,i_t}$ and $q_{t,i_t}$ here.} or
\item $i_t=i$ and $\hat q_{t+1,i_t}<q_{t+1,i_t}/4$.
\end{itemize}
The well-behaved answer strategy ${\bm \pi}^{\ast,f}=({\bm \pi}_i^{\ast,f})_{i\in [K]}$ is the joint strategy where every agent $i\in [K]$ follows ${\bm \pi}_i^{\ast,f}$.
\end{definition}

We now present and prove the formal version of \Cref{lem:equiv between actual and no-flagging}:
\begin{lemma}[Correspondence between Auxiliary Game and Actual Game]\label{lem:equiv between actual and no-flagging formal}
Let ${\bm \pi}^r$ be a joint strategy for the auxiliary game from \Cref{def:auxiliary game}.
Consider the joint strategy $\bm \pi=({\bm \pi}^r,{\bm \pi}^{\ast,f})$ in the actual game that uses ${\bm \pi}^r$ as the reporting policy and ${\bm \pi}^{\ast,f}$ (see \Cref{def:well-behaved flagging formal}) as the flagging policy.

For every history $\mH_t$ such that $\hat q_{t,j}\le 4q_{t,j}$ for all $j\in [K]$ (which is a natural consequence of following ${\bm \pi}^{\ast,f}$ in the past) and the corresponding simplified history $\tilde \mH_t=(t,\mA_t)$,
\begin{equation}\label{eq:equiv between actual and no-flagging}
V_i^{\bm \pi}(\mH_{t})=\tilde V_i^{{\bm \pi}^r}(\tilde \mH_t),\quad \forall i\in [K].
\end{equation}
Furthermore, the allocation sequences $\{i_{\tau}\}_{\tau=t}^T$ in the actual and auxiliary games are the same.\footnote{By ``the same'', we mean there exists a coupling between the realization of $\{i_{\tau}\}_{\tau=t}^T$ in the actual game under joint strategy $\bm \pi$ and starting from $\mH_t$, and that in the auxiliary game under ${\bm \pi}^r$ and starting from $\tilde \mH_t$.}
\end{lemma}
\begin{proof}
We prove by backward induction. Let $t\in[T]$ and fix a common history $\mH_t$. Suppose that the claim holds for all possible next-round histories $\{\mH_{t+1}\}$ resulted from $\bm \pi$.
As the reporting policies and allocation mechanism are identical for both games, the reports $\bm v_t$ and winning agent $i_t$ can be coupled to be the same for both games.
To finish the proof, it suffices to show that the next-round history $\mH_{t+1}$ generated by $\bm \pi$ in the original game (LHS) has the same simplified history as the next-round simplified history $\tilde\mH_{t+1}$ generated by ${\bm \pi}^r$ in the auxiliary game (RHS).

Since $\hat q_{t,j}\le 4q_{t,j}$ for all $j\in [K]$, we have ($\hat p_{t,j}$ is defined in \Cref{line:check prob} and $p_{t,j}$ is in \Cref{def:auxiliary game})
\begin{equation*}
\hat p_{t,j}=\min\left (\frac{8K^2}{(T-t) \hat q_{t,j} c},1\right )\ge \min\left (\frac{8K^2}{(T-t) 4 q_{t,j} c},1\right )=\min(p_{t,j},1),\quad \forall j\in [K].
\end{equation*}

We now discuss whether the winner $i_t$ (the same in both games) marked up, that is $v_{t,i_t}>u_{t,i_t}$.
\begin{itemize}
\item If so, they are eliminated immediately in the RHS according to \Cref{item:eliminate immediately}. Thus, the next-round simplified history generated by ${\bm \pi}^r$ in the auxiliary game is $\tilde \mH_{t+1}=(t+1,\mA_t\setminus \{i_t\})$.

For the LHS, $i_t$ marked up using $\pi^r_{t,i_t}$, \Cref{item:no mark up unless p>=1 formal} implies that $p_{t,i_t}=\frac{2K^2}{(T-t) q_{t,i_t} c}\ge 1$ and thus $\hat p_{t,i_t}\ge \min(p_{t,i_t},1)=1$. Therefore, $i_t$ is audited and eliminated with probability 1 in the original game as well (\Cref{line:elimination} of \Cref{alg:mechanism}). The next-round history in the actual game $\mH_{t+1}$ hence corresponds to the same simplified history $\tilde \mH_{t+1}$ \textit{a.s.}
\item Otherwise, $i_t$ does not mark up. Thus, the next-round simplified history in the auxiliary game is $\tilde\mH_{t+1}=(t+1,\mA_t)$. Meanwhile, regardless of whether they are audited in the original game, $i_t$ stays alive because $v_{t,i_t}\le u_{t,i_t}$. Thus the next-round auxiliary histories are again identical.
\end{itemize}

In summary, there is a coupling in which both games generate the same round-$t$ allocation $i_t$ and next-round simplified history $\tilde \mH_{t+1}$.
The final step is to argue that the new $\mH_{t+1}$ always ensures $\hat q_{t+1,j}\le 4q_{t+1,j}$, $\forall j\in [K]$ so that the induction hypothesis is applicable:
\begin{itemize}
\item If someone is eliminated, then $\hat q_{t+1,j}=0$, which is no more than $4q_{t+1,j}$ for all $j$.
\item If no new $\hat q_{t+1,i_t}$ is generated or the new $\hat q_{t+1,i_t}\le 4q_{t+1,i_t}$, then the property follows from the condition that $\hat q_{t,j}\le 4q_{t,j}$ for all $j\in [K]$.
\item If a new $\hat q_{t+1,i_t}$ is generated and $\hat q_{t+1,i_t}>4q_{t+1,i_t}$, then ${\bm \pi}_{t,-i_t}^{\ast,f}$ would flag them. In this case $\hat q_{t+1,i_t}$ is replaced with 0 and the property also holds.
\end{itemize}
Hence, our claim follows from the backward induction on $t$.
\end{proof}

\subsection{Proof of V-Function Lower and Upper Bounds (\Cref{lem:u lower and upper bound})}
\begin{lemma}[Upper and Lower Bounds when Following $\bm \pi^\ast$]\label{lem:u lower and upper bound formal}
Fix any round $t\in [T]$ and any history $\mH_t$ generated via following $\bm \pi^\ast$ in the past. The following inequalities hold:
\begin{align}
(T-t+1) \mu_{t,i}-K &\leq V_i^{{\bm \pi}^\ast}(\mH_t)\leq (T-t+1) \mu_{t,i}+K^2,&&\quad \forall t\in [T],\text{history }\mH_t,i\in \mA_t,\tag{\ref{eq:for alive}}\\
(T-t+1) \mu_{1,i}-K &\leq V_i^{{\bm \pi}^\ast}(\mH_t)=0,&&\quad \forall t\in [T],\text{history }\mH_t,i\notin \mA_t, \tag{\ref{eq:for dead}}
\end{align}
where the definitions of $\mu_{t,i}$ and $\mu_{1,i}$ can be found in \Cref{eq:fair share}.
\end{lemma}
\begin{proof}
As $\mH_t$ is generated via following $\bm \pi^\ast$ in the past, we must have $\hat q_{t,j}\le 4q_{t,j}$ for all $j\in [K]$.
Applying \Cref{lem:equiv between actual and no-flagging formal} to the joint strategy $\bm \pi^\ast=({\bm \pi}^{\ast,r},{\bm \pi}^{\ast,f})$ and history $\mH_t$, we have
\begin{equation*}
V_i^{\bm\pi^\ast}(\mH_t) = \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_t),
\end{equation*}
where $\tilde \mH_t=(t,\mA_t)$ is the simplified history corresponding to $\mH_t$.

\paragraph{Lower Bounds for Alive Agents.} Fix an alive agent $i\in\mA_t$. We start by proving a lower bound on the utility of agent $i$ in the auxiliary game (\Cref{def:auxiliary game}). Consider the truthful reporting strategy $\truth_i$ for agent $i$ that always reports $v_{\tau,i}=u_{\tau,i}$ for any round $\tau\in[T]$. Since ${\bm \pi}^{\ast,r}$ is a PBE in the auxiliary game and $\truth_i\circ {\bm \pi}^{\ast,r}_{-i}$ is a valid joint strategy therein, we have
\begin{equation}\label{eq:pbe_for_lower_bound}
V_i^{\bm\pi^\ast}(\mH_t) = \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_t)\ge \tilde V_i^{\truth_i\circ {\bm \pi}^{\ast,r}_{-i}}(\tilde \mH_{t}).
\end{equation}

We now lower bound the RHS where agent $i$ always reports truthfully. If in any round $\tau\in\{t,\ldots,T\}$ agent $i$ would have won had other agents reported truthfully (\textit{i.e.}, agent $i$ had the largest true utility $u_{\tau,i}=\max_{j\in \mA_{\tau}} u_{\tau,j}$ and $u_{\tau,i}\ge c$), either agent $i$ won so that $i_{\tau}=i$, or the winner $i_{\tau}$ marked up and was eliminated. Since each agent can only be eliminated once and agent $i$ is never eliminated, the latter case occurs for at most $K-1$ rounds. Making it formal, we get
\begin{align*}
    &\quad \tilde V_i^{\truth_i\circ {\bm \pi}^{\ast,r}_{-i}}(\tilde \mH_{t}) = \E_{\truth_i\circ {\bm \pi}^{\ast,r}_{-i}}\left [ \sum_{\tau=t}^T u_{\tau,i} \1[i_{\tau}=i] \middle \vert \tilde \mH_t\right ]\\
    &\geq \E_{\truth_i\circ {\bm \pi}^{\ast,r}_{-i}} \sqb{ \sum_{\tau=t}^T u_{\tau,i} \paren{\1\sqb{u_{\tau,i}=\max_{j\in \mA_{\tau}}u_{\tau,j}} \1[u_{\tau,i}\ge c]  - \1[i_t\neq i] \1\sqb{v_{t,i_t}>u_{t,i_t}}} \1\sqb{i\in \mA_{\tau}}}\\
    &\overset{(a)}{\geq} (T-t+1)\mu_{t,i} + \E_{\truth_i\circ {\bm \pi}^{\ast,r}_{-i}} \sqb{ \sum_{t=1}^T |\mA_t|-|\mA_{t+1}|} \geq (T-t+1)\mu_{t,i} - (K-1).
\end{align*}
where (a) uses the fact that agent $i$ is always alive when adopting $\truth_i$ and that if $\mA_{\tau}\subseteq \mA_t$ and $i\in \mA_{\tau}$, then $\mu_{\tau,i}\ge \mu_{t,i}$.
Together with \Cref{eq:pbe_for_lower_bound}, this proves $V_i^{\bm\pi^\ast}(\mH_t)\ge (T-t+1)\mu_{t,i}-K$ for all alive agents $i\in\mA_t$, showing the lower bound of \Cref{eq:for alive}.

\paragraph{Lower Bounds for Eliminated Agents.}
Before proving the lower bound, we make the following claim: For any round $s\in[T]$, simplified history $\tilde\mH_s$, and alive agent $i\in\mA_s$ such that $(T-s)\mu_{s,i} > K$, agent $i$ remains alive (that is, $i\in \mA_{s+1}$) with probability $1$ under the joint strategy ${\bm \pi}^{\ast,r}$.

To prove this claim, we construct an alternative policy $\pi_{s,i}$ that never results in an elimination, and utilize the fact that ${\bm \pi}^{\ast,r}$ is a PBE in the auxiliary game. Specifically, define alternative policy
\begin{equation*}
\pi_{s,i}(u_{s,i};\tilde \mH_s)=\min(u_{s,i},v_{s,i}),\quad \text{where }v_{s,i}\sim \tilde{\pi}_{s,i}^\ast (u_{s,i};\tilde \mH_s),
\end{equation*}
that is, capping the reports suggested by $\pi_{s,i}^{\ast,r}$ at the true utility $u_{s,i}$.

As ${\bm \pi}^{\ast,r}$ is a PBE, agent $i$'s unilateral deviation to $\pi_{s,i}$ in round $s$ does not increase its gain:
\begin{equation*}
\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_s)\ge \tilde V_i^{(\pi_{s,i}\circ {\bm \pi}_{s,-i}^{\ast,r})\diamond_s {\bm \pi}^{\ast,r}}(\tilde \mH_s),
\end{equation*}
where we recall the notation $\bm \pi_1 \diamond_t \bm \pi_2$ means following $\bm \pi_1$ up to round $t$ and adopting $\bm \pi_2$ after that.

The only case where the two strategies yield different outcomes is when \textit{1)} agent $i$ marks up under $\pi^r_{s,i}$, \textit{i.e.}, $v_{s,i}>u_{s,i}$, and \textit{2)} agent $i$ wins at time $s$ by reporting $v_{s,i}$ but not by reporting $u_{s,i}$. Denote by $\mE_{s,i}$ this event. Under $\mE_{s,i}$, if agent $i$ followed $\pi_{s,i}^{\ast,r}$ by reporting $v_{s,i}$, its total utility is $u_{s,i}\leq 1$ since they are automatically eliminated at time $s$ (\Cref{item:eliminate immediately formal}). On the other hand, if agent $i$ followed $\pi_{s,i}$, they would report $u_{s,i}$ and obtain utility
\begin{equation*}
\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_{s+1})\geq (T-s)\mu_{s+1,i}-(K-1)\ge (T-s)\mu_{s,i}-(K-1),
\end{equation*}
where the first inequality is because $i\in \mA_{s+1}$ and the lower bound \Cref{eq:for alive} for those alive agents, and the second inequality is because $\mA_{s+1}\subseteq \mA_s$ and hence $\mu_{s+1,i}\ge \mu_{s,i}$. In summary,
\begin{equation*}
0\geq \tilde V_i^{(\pi_{s,i}\circ {\bm \pi}_{s,-i}^{\ast,r})\diamond_s {\bm \pi}^{\ast,r}}(\tilde \mH_s) - \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_{s}) \geq ((T-s)\mu_{s,i}-K)\Pr \{\mE_{s,i}\},
\end{equation*}
where the first inequality uses the fact that ${\bm \pi}^{\ast,r}$ is a PBE in the auxiliary game. As a result, $\Pr\{\mE_{s,i}\}=0$ and hence almost surely agent $i$ does not die in round $s$.

We now prove \Cref{eq:for dead} using this claim. Since $\tilde\mH_t$ has non-zero probability in the auxiliary game, the claim indicates that for all eliminated agents $i\notin \mA_t$ we have $(T-t+1)\mu_{1,i}\leq K$ because $\mu_{1,i}\le \mu_{s,i}$ for all $s\in [T]$. Therefore, $V_i^{\bm \pi^\ast}(\mH_t) = \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_t) = 0\ge (T-t)\mu_{1,i}-K$.

\paragraph{Upper Bounds for Alive Agents.}
Conditioning on $\mH_t$, only those agents in $\mA_t$ may receive allocations during rounds $\tau=t,t+1,\ldots,T$. Hence, the sum of V-functions of all agents can never exceed the mechanism that allocates the resource to the alive agent with the highest utility. That is,
\begin{align*}
\sum_{i=1}^K V_i^{{\bm \pi}^\ast}(\mH_{t}) &\le \sum_{i\in\mA_t} (T-t+1)\E_{\bm u_t\sim \mV} \bigg[u_{t,i}\mathbbm{1}[i\in \mA_t] \mathbbm{1}[u_{t,i}>u_{t,j},\forall j\in \mA_t\setminus \{i\}] \bigg]\\
&\le (T-t+1)\left (\sum_{i\in\mA_t} \mu_{t,i} + c\Pr_{\bm u\sim \mV}\{V_i<c,\forall i\in\mA_t\}\right ),
\end{align*}
where the second inequality is because the definition of $\mu_{t,i}$ in \Cref{eq:fair share} contains an indicator $\1[u_{t,i}\ge c]$.
We recall that by \Cref{assumption:min report}, there exists some (albeit unknown) agent $i_0\in [K]$ that ensures $\Pr_{u_{i_0}\sim\mu_{i_0}} \{u_{i_0}\geq c\}=1$. Thus, if $i_0\in \mA_t$ is still alive, we already proved the desired upper bound since $\Pr_{\bm u\sim \mV}\{V_i<c,\forall i\in\mA_t\}=0$. We now consider the case when $i_0\notin\mA_t$. Notice that
\begin{equation*}
    \sum_{i\notin\mA_t}\mu_{1,i}=\E_{\bm u\sim \mV}\sqb{\max_{i\in [K] u_{i}}\1\sqb{\argmax_{i\in [K]} V_i \notin \mA_t}}\ge c\Pr_{\bm u\sim \mV}\{V_i<c,\forall i\in\mA_t\},
\end{equation*}
where the inequality is because $i_0\notin \mA_t$ and \Cref{assumption:min report}.
As a result, we obtained
\begin{equation}\label{eq:upper_bound_max_welfare}
    \sum_{i=1}^K V_i^{{\bm \pi}^\ast}(\mH_{t}) \leq (T-t+1)\left (\sum_{i\in\mA_t} \mu_{t,i}+\sum_{i\notin\mA_t} \mu_{1,i}\right ) \leq (T-t+1)\sum_{i\in\mA_t} \mu_{t,i} + K\sum_{i=1}^K \1[i\notin \mA_t],
\end{equation}
where we used the proved claim that $(T-t+1)\mV_i\leq K$ for all $i\notin \mA_t$ (\Cref{eq:for dead}).
Utilizing the lower bound for those alive agents in \Cref{eq:for alive}, we know
\begin{equation*}
\sum_{j\in\mA_t\setminus\{i\}} u_{j}^{{\bm \pi}^\ast}(\mH_{t})\ge \sum_{j\in\mA_t\setminus\{i\}} \left ((T-t+1)\mu_{t,j}-K\right ),\quad \forall i\in \mA_t,
\end{equation*}
which consequently gives
\begin{align*}
V_i^{{\bm \pi}^\ast}(\mH_{t})&\le (T-t+1)\mu_{t,i}+K \sum_{j=1}^K (\1[j\notin \mA_t]+\1[j\in \mA_t\setminus \{i\}])\\&\le (T-t+1)\mu_{t,i}+K^2,\quad \forall i\in \mA_t.
\end{align*}
The upper bound involving $\mu_{1,i}$ follows from $\mu_{t,i}\geq \mu_{1,i}$.
\end{proof}

\subsection{Proof of Marking Up is Worse than Honesty (\Cref{lem:min report with u is good})}
\begin{lemma}[Marking Up is Worse than Honesty]\label{lem:min report with u is good formal}
Fix round $t\in[T]$ and history $\mH_t$ generated by following $\bm \pi^\ast$ in the past. Consider an alive agent $i\in\mA_t$ such that $p_{t,i}=\frac{2K^2}{(T-t)q_{t,i} c} \leq 1$. For any of its round-$t$ reporting strategy $\pi^r_{t,i}$ in the auxiliary game, let $\pi_{t,i}^{r,\prime}$ be the reporting strategy of
\begin{equation*}
\pi_{t,i}^{r,\prime}(u_{t,i};\tilde \mH_t)=\min(v_{t,i},u_{t,i}),\quad \text{where }v_{t,i}\sim \pi^r_{t,i}(u_{t,i};\tilde \mH_t),
\end{equation*}
which caps the reports suggested by $\pi^r_{t,i}$ with its true utility $u_{t,i}$.

Let $\pi_{t,i}=(\pi^r_{t,i},{\pi}_{t,i}^{\ast,f})$ and $\pi_{t,i}'=(\pi_{t,i}^{r,\prime},{\pi}_{t,i}^{\ast,f})$ be the corresponding round-$t$ strategies in the actual game.
When the opponents are following $\bm \pi_{t,-i}^\ast$, replacing $\pi_{t,i}$ with $\pi^r_{t,i}$ is better for agent $i$:
\begin{equation*}
V_i^{(\pi_{t,i}\circ \bm\pi^\ast_{t,-i})\diamond_t \bm\pi^\ast}(\mH_t) \geq V_i^{(\pi'_{t,i}\circ \bm\pi^\ast_{t,-i})\diamond_t \bm\pi^\ast}(\mH_t).
\end{equation*}
\end{lemma}
\begin{proof}
Throughout the proof, we denote by $u_{t,i}$ the utility of agent $i$ and by $v_{t,i}$ the report suggested by $\pi_{t,i}$ (both can be stochastic).
Since the opponents are following $\bm \pi_{t,-i}^\ast$, the flagging strategies ${\bm \pi}_{t,-i}^{\ast,f}$ are well-behaved (recall \Cref{def:well-behaved flagging}) and the next-round history $\mH_{t+1}$ ensures $\hat q_{t,j}\le 4q_{t,j}$, $\forall j\in [K]$. Applying \Cref{lem:equiv between actual and no-flagging formal} to any possible next-round history $\mH_{t+1}$ and $\bm \pi^\ast=({\bm \pi}^{\ast,r},{\bm \pi}^{\ast,f})$ thus gives $V_i^{\bm \pi^\ast}(\mH_{t+1})=\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_{t+1})$ --- that is, the gain of agent $i$ in the future only depends on $\tilde \mH_{t+1}$, and we only need to care about $i_t$ and $\tilde \mH_{t+1}$ when comparing the LHS and RHS.

Comparing $\pi_{t,i}$ and $\pi_{t,i}'$, the only event such that the allocation $i_t$ or the next-round simplified history $\tilde\mH_{t+1}$ can differ is that $v_{t,i}>u_{t,i}$. Denote this event by $\mE_{t,i}$.
We now assume $\mE_{t,i}$ holds and analyze the utility obtained by using $\pi_{t,i}$ or $\pi'_{t,i}$, conditioning on $u_{t,i}$ and $v_{t,i}$. 

We start with $\pi_{t,i}$. Since agent $i$ reports $v_{t,i}$, they win (so $i_t=i$) and are audited with probability $\hat p_{t,i}=\min(\frac{8K^2}{(T-t)\hat q_{t,i} c},1)$ (recall \Cref{line:check prob} from \Cref{alg:mechanism}). If audited, they are eliminated immediately because $v_{t,i}>u_{t,i}$. Otherwise, we move to a next-round simplified history $\tilde \mH_{t+1}=(t+1,\mA_{t+1})$ with $\mA_{t+1}=\mA_t$, and thus the V-function upper bound in \Cref{eq:for alive} ensures the utility is at most
\begin{equation*}
u_{t,i}+(1-\hat p_{i,t}) V_i^{\bm \pi^\ast}({\mH}_{t+1})\le 1 + (1-\hat p_{i,t})\paren{(T-t) \mu_{t,i}+K^2} := u_1.
\end{equation*}

Alternatively, if agent $i$ uses strategy $\pi'_{t,i}$ and reports $u_{t,i}$ instead, they remain alive as $i\in\mA_{t+1}$. Now applying the lower bound part in \Cref{eq:for alive} to the next-round history $\mH_{t+1}$ shows that
\begin{equation*}
u_{t,i}\mathbbm{1}[i_t=i]+V_i^{{\bm \pi}^\ast}({\mH}_{t+1})\ge (T-t) \mu_{t,i}-K:=u_2.
\end{equation*}

It suffices to show that $u_2\geq u_1$ for the desired conclusion.
Since $\hat q_{t,i}\leq 4q_{t,i}$, we have $\hat p_{t,i} \geq \min(p_{t,i},1)=p_{t,i}=\frac{2K^2}{(T-t)q_{t,i} c}$. As \Cref{assumption:min report} further implies $\mu_{t,i}\geq q_{t,i} c$, we obtain
\begin{equation*}
    u_2-u_1 \geq \hat p_{t,i}(T-t)\mu_{t,i} - K^2 - K - 1 \geq 2K^2 - K^2-K-1 \geq 0.
\end{equation*}
Therefore, under event $\mE_{t,i}$, the gain of $\pi_{t,i}'$ is no better than the that of $\pi_{t,i}$. Since they give the same allocations $i_t$ and next-round auxiliary histories $\tilde \mH_{t+1}$ under $\neg \mE_{t,i}$, our conclusion follows.
\end{proof}

\subsection{Proof of \Cref{thm:equivalence of PBE}}\label{sec:equivalence of PBE formal}

We are now ready to prove \Cref{thm:equivalence of PBE}, which characterizes a PBE under \mechname.

\begin{proof}[Proof of \Cref{thm:equivalence of PBE}]
Fix an agent $i\in[K]$. We aim to show that its unilateral deviation from $\bm \pi_i^\ast$ does not increase its utility. Formally, let ${\bm \pi}_i=({\bm \pi}_i^r,{\bm \pi}_i^f)$ be an alternative strategy for agent $i$.

\paragraph{Step 1. Isolating Deviations in Different Rounds.}
We apply the performance difference lemma \citep{kakade2002approximately} between the two joint strategies $\bm \pi_i\circ \bm \pi_{-i}^\ast$ and $\bm \pi^\ast$:
\begin{align}
&\quad V_i^{\bm \pi_i\circ \bm \pi_{-i}^\ast}(\mH_1)-V_i^{\bm \pi^\ast}(\mH_1) \nonumber\\
&=\sum_{t=1}^T \E_{\mH_t\sim \bm \pi_i\circ \bm \pi_{-i}^\ast}\left [\E_{u_{t,i}\sim \mV_i,v_{t,i}\sim \pi_{t,i}^r(u_{t,i};\mH_t)}\left [Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^f)\right ]-V_i^{\bm \pi^\ast}(\mH_t)\right ],\label{eq:performance difference lemma}
\end{align}
where the Q-function (state-action utility function) for a joint strategy $\bm \pi$ in the actual game is defined similarly to V-function characterized in \Cref{eq:V-func recursive}, except that we fix agent $i$'s current-round utility $u_{t,i}$, report $v_{t,i}$, and flagging policy $\pi_{t,i}^f\colon (u_{t,i},\bm v_t,i_t;\mH_t)\mapsto \Delta(\{0,1\})$:
\begin{align}
Q_i^{\bm \pi}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^f)=\E\left [u_{t,i}\mathbbm{1}[v_{t,i}>v_{t,j},\forall j\in \mA_t\setminus \{i\}]+V_i^{\bm \pi}(\mH_{t+1})\middle \vert \vphantom{\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_{t+1})} \mH_t\right ],\label{eq:Q-func recursive}
\end{align}
where in the expectation, we fix agent $i$'s utility and report as $u_{t,i}$ and $v_{t,i}$, and sample others' utilities and reports as $\bm u_{t,-i}\sim \mV_{-i}$, $\bm v_{t,-i}\sim \bm \pi_{t,-i}^r$. We calculate agent $i$'s flag as $f_{t,i}\sim \pi_{t,i}^f$ and others' as $\bm f_{t,-i}\sim \bm \pi_{t,-i}^f$. After that, the next-round history $\mH_{t+1}$ is generated according to \Cref{alg:mechanism}.

We now focus on a single history $\mH_t$ which is sampled from $\bm \pi_i\circ \bm \pi_{-i}^\ast$. Recalling the definition of well-behaved flagging policy ${\bm \pi}^{\ast,f}$ from \Cref{def:well-behaved flagging}, we immediately have $\hat q_{t,j}\le 4q_{t,j}$ for all $j\in [K]$ at $\mH_t$ since $\bm \pi_{-i}^\ast=({\bm \pi}_{-i}^{\ast,r},{\bm \pi}_{-i}^{\ast,f})$. In the rest of the proof, we show the following claim:
\begin{equation}\label{eq:obj after performance difference lemma}
\E_{u_{t,i}\sim \mV_i,v_{t,i}\sim \pi_{t,i}^r(u_{t,i};\mH_t)}\left [Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^f)\right ]\le V_i^{\bm \pi^\ast}(\mH_t),\quad \forall \mH_t\text{ s.t. }\hat q_{t,j}\le 4q_{t,j}~\forall  j\in [K].
\end{equation}

That is, instead of considering the $T$-round deviation to $\bm \pi_i$, we now only need to focus on a single round deviation to $(\pi_{t,i}^r,\pi_{t,i}^f)$ and assume that the future joint strategy ($\bm \pi$ in the $V_i^{\bm \pi}(\mH_{t+1})$ terms in \Cref{eq:V-func recursive,eq:Q-func recursive}) is the well-behaved $\bm \pi^\ast=({\bm \pi}^{\ast,r},{\bm \pi}^{\ast,f})$, instead of the deviated $\bm \pi_i\circ \bm \pi_{-i}^\ast$.
Using the $\diamond_t$ notation for concatenating one policy to another, \Cref{eq:obj after performance difference lemma} can be rewritten as
\begin{equation*}
\E_{u_{t,i}\sim \mV_i,v_{t,i}\sim \pi_{t,i}^r(u_{t,i};\mH_t)}\left [Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^f)\right ]=V_i^{(\bm \pi_{t,i}\circ \bm \pi_{t,-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t)\le V_i^{\bm \pi^\ast}(\mH_t).
\end{equation*}

\paragraph{Step 2. Eliminating the Deviation in Flagging Policies.}
We now eliminate the effect of replacing ${\pi}_{t,i}^f$ with $\pi_{t,i}^{\ast,f}$.
As the future joint strategy $\bm \pi^\ast$ is constructed from the joint strategy ${\bm \pi}^{\ast,r}$ in the auxiliary game (recall the construction in \Cref{lem:equiv between actual and no-flagging formal} and the definition of $\bm \pi^\ast$ in \Cref{thm:equivalence of PBE}),
\begin{equation*}
V_i^{\bm \pi^\ast}(\mH_{t+1})=\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde{\mH}_{t+1})\text{ as long as }\mH_{t+1}\text{ ensures }\hat q_{t+1,j}\le 4q_{t+1,j}\text{ for all } j\in [K].
\end{equation*}

Similar to the arguments when proving \Cref{lem:equiv between actual and no-flagging formal}, the condition $\hat q_{t+1,j}\le 4q_{t+1,j}$ for all $j\in [K]$ is always true under either $\pi_{t,i}^f$ or ${\pi}_{t,i}^{\ast,f}$, thanks to ${\bm \pi}_{t,-i}^{\ast,f}$: if someone dies, then all $\hat q_{t+1,j}$'s are zero; if $\hat q_{t+1,i_t}\le 4q_{t+1,i_t}$, then we are good; otherwise, due to ${\bm \pi}_{t,-i}^{\ast,f}$, such $\hat q_{t+1,i_t}$ would be reset to $0$.

Furthermore, ${\pi}_{t,i}^f$ and $\pi_{t,i}^{\ast,f}$ give the same next-round simplified history $\tilde \mH_{t+1}$ because flags only happen if $\mA_{t+1}=\mA_t$, which already indicates $\tilde \mH_{t+1}=(t+1,\mA_t)$. Therefore,
\begin{equation}\label{eq:eliminate_flag}
Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^f) = Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^{\ast,f}).
\end{equation}

\paragraph{Step 3. Eliminating the Deviation to Marking Up.}
We next introduce a reporting policy $\bar\pi^{r,1}_{t,i}$ that complies with \Cref{item:no mark up unless p>=1 formal}. If $p_{t,i}>1$, we simply use $\bar\pi^{r,1}_{t,i}=\bar\pi^{r}_{t,i}$. Otherwise, for given any utility $u_{t,i}$, we define an alternative strategy that caps the reports at the true utility, as in \Cref{lem:min report with u is good formal}:
\begin{equation*}
\pi_{t,i}^{r,1}(u_{t,i};\mH_t)=\min(v_{t,i},u_{t,i}),\quad \text{where }v_{t,i}\sim \pi_{t,i}^r (u_{t,i};\mH_t).
\end{equation*}
\Cref{lem:min report with u is good formal} then implies that
\begin{equation}\label{eq:obj after removing marking up}
V_i^{(({\color{red}\pi_{t,i}^{r,1}},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t)  \geq V_i^{((\pi_{t,i}^{r},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t) = V_i^{(\bm \pi_{t,i}\circ \bm \pi_{t,-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t), 
\end{equation}
where the equality comes from \Cref{eq:eliminate_flag}.

\paragraph{Step 4. Eliminating the History Dependency.}
We further remove the dependency on the full history $\mH_t$ in $\pi_{t,i}^{r,1}$ and get yet another intermediate policy $\pi_{t,i}^{r,2}$ that further complies with \Cref{item:only simplified history formal}.

To do so, since $\pi_{t,i}^{r,1}$ complies with \Cref{item:no mark up unless p>=1 formal}, we know
\begin{align*}
V_i^{(\pi_{t,i}^{r,1},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t)&=\E_{u_{t,i}\sim \mV_i}\left [\E_{v_{t,i}\sim \pi_{t,i}^{r,1}(u_{t,i};\mH_t)}\left [Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^{\ast,f})\right ]\right ]\\
&\le \E_{u_{t,i}\sim \mV_i}\left [\max_{v_{t,i}\in \text{Range}_{t,i}(u_{t,i})}\left [Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},\pi_{t,i}^{\ast,f})\right ]\right ],
\end{align*}
where $\text{Range}_{t,i}(u_{t,i})$ is defined as $[0,u_{t,i}]$ if $\frac{2K^2}{(T-t) q_i c}\leq 1$ and $[0,1]$ otherwise. We define $\pi_{t,i}^{r,2}$ as the strategy attaining the RHS, and then justify that $\pi_{t,i}^{r,2}$ can be implemented only using $\tilde \mH_t$. That is, let
\begin{equation*}
\pi_{t,i}^{r,2}(u_{t,i};\mH_t)=\argmax_{v_{t,i}\in \text{Range}_{t,i}(u_{t,i})} Q_i^{{\bm \pi}^\ast}(\mH_t,u_{t,i},v_{t,i},{\pi}_{t,i}^{\ast,f}),
\end{equation*}
and we show that the $\argmax$ over $v_{t,i}\in \text{Range}_{t,i}(u_{t,i})$ can be evaluated only using $\tilde H_t$.\footnote{The statement is not true if we change $v_{t,i}\in \text{Range}_{t,i}(u_{t,i})$ to $v_{t,i}\in [0,1]$ because reporting $v_{t,i}>u_{t,i}$ when $p_{t,i}<1$ results in a different elimination probability from the auxiliary game \Cref{def:auxiliary game}. Therefore, we need Step 3 to generate a $\pi_{t,i}^{r,1}$ satisfying \Cref{item:no mark up unless p>=1 formal}.}
Recall the definition of the Q-function from \Cref{eq:Q-func recursive}:
\begin{align*}
Q_i^{\bm \pi^\ast}(\mH_t,u_{t,i},v_{t,i},{\pi_{t,i}}^{\ast,f})&=\E\bigg [u_{t,i}\mathbbm{1}[v_{t,i}>v_{t,j},\forall j\in \mA_t\setminus\{i\}]+ V_i^{{\bm \pi}^\ast}(\mH_{t+1})\bigg \vert\\
&\quad \qquad \bm u_{t,-i}\sim \mV_{-i}, \bm v_{t,-i}\sim {\bm \pi}_{t,-i}^{\ast,r},\mH_{t+1}\text{ from \Cref{alg:mechanism}}\bigg ]\bigg ].
\end{align*}

The indicator whether $v_{t,i}>v_{t,j}$ for all $j\in \mA_t\setminus\{i\}$ only depends on current-round reports $\bm v_t$ and the set of alive agents $\mA_t$. Meanwhile, thanks to \Cref{lem:equiv between actual and no-flagging formal}, $V_i^{\bm \pi^\ast}(\mH_{t+1})=\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_{t+1})$ only depends on $\tilde \mH_{t+1}=(t+1,\mA_{t+1})$. 
Thanks to the correspondence result in \Cref{lem:equiv between actual and no-flagging formal}, even in the actual game, whether agent $j\in \mA_t$ stays alive in the next round when reporting $v_{t,j}\in \text{Range}_{t,j}(u_{t,j})$ is also history-independent. Furthermore, opponents' strategies ${\bm \pi}_{t,-i}^{\ast,r}$ and $\text{Range}_{t,j}(u_{t,j})$ only depend on $\tilde \mH_t$. Hence, $\pi_{t,i}^{r,2}$ complies with \Cref{item:only simplified history formal}. Furthering the bound from \Cref{eq:obj after removing marking up},
\begin{equation}\label{eq:obj after removing history-dependency}
V_i^{(\bm \pi_{t,i}\circ \bm \pi_{t,-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t)\le V_i^{((\pi_{t,i}^{r,1},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t) \leq V_i^{(({\color{red}\pi_{t,i}^{r,2}},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t).
\end{equation}

\paragraph{Step 5. Utilizing ${\bm \pi}^{\ast,r}$ is a PBE.}
Because of Steps 2 -- 4, the joint strategy $(\pi_{t,i}^{r,2}\circ {\bm \pi}_{-i}^{\ast,r})\diamond_t {\bm \pi}^{\ast,r}$ is a valid in the auxiliary game (\Cref{def:auxiliary game}). Therefore, utilizing the equivalence lemma (\Cref{lem:equiv between actual and no-flagging formal}) and that ${\bm \pi}^{\ast,r}$ is a PBE in the auxiliary game, the second inequality in \Cref{eq:obj after removing history-dependency} is true:
\begin{equation*}
V_i^{((\pi_{t,i}^{r,2},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_t)\overset{(a)}{=}\tilde V_i^{(\pi_{t,i}^{r,2}\circ {\bm \pi}_{-i}^{\ast,r})\diamond_t {\bm \pi}^{\ast,r}}(\tilde \mH_t)\overset{(b)}{\le} \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde \mH_t)\overset{(c)}{=}V_i^{\bm \pi^\ast}(\mH_t),
\end{equation*}
where (a) applies \Cref{lem:equiv between actual and no-flagging formal} to $((\pi_{t,i}^{r,2},{\pi}_{t,i}^{\ast,f})\circ {\bm \pi}_{-i}^\ast)\diamond_t \bm \pi^\ast$ with respect to $\mH_t$, (b) uses the definition ${\bm \pi}^{\ast,r}$ is a PBE in the auxiliary game, and (c) applies \Cref{lem:equiv between actual and no-flagging formal} again to $\bm \pi^\ast$.

Together with \Cref{eq:obj after removing history-dependency}, this proves \Cref{eq:obj after performance difference lemma}. Last, plugging this into the performance difference lemma \Cref{eq:performance difference lemma} concludes the proof that any unilateral deviation $\bm \pi_i\circ {\bm \pi}_{-i}^\ast$ does not strictly improve over $\bm \pi^\ast$ for an arbitrary agent $i\in [K]$. Hence, $\bm \pi^\ast$ is a PBE in the actual game.
\end{proof}

