\section{Main Results}


\begin{figure}[tb]
    \centering
    \begin{tikzpicture}[scale=0.7]

    \draw (10,0) node {$\begin{matrix}\textbf{Expected} \\  \textbf{\#audits} \end{matrix}$};
    \draw (-3,3) node[rotate=90]{\textbf{Regret}};
    
    \draw[thick] (8,0.1) -- (8,-0.1) node[below]{$T^{\vphantom{2}}$};
    \draw[thick] (0.1,6) -- (-0.1,6) node[left]{$T$};


    \draw[thick] (0.7,0.1) -- (0.7,-0.1) node[below]{$\Omega(1)^{\vphantom{2}}$};
    \draw[thick] (2,0.1) -- (2,-0.1) node[below]{$K^2$};
    \draw[thick] (3,0.1) -- (3,-0.1);
    \draw (3.7,-0.1)  node[below]{$K^3\log T$};
    \draw[thick] (0.1,4.5) -- (-0.1,4.5) node[left]{$\sqrt {\frac{T}{\log T}}$};
    \draw[thick] (0.1,1) -- (-0.1,1) node[left]{$K$};
    \draw[thick] (0.1,2) -- (-0.1,2) node[left]{$K^2$};

    \draw[thick,draw=none,pattern=north west lines,pattern color=red!50] (0,0) -- (8,0) -- (8,1) -- (0.7,1) -- (0.7,4.5) -- (0,4.5);
    \draw[thick,draw=none,pattern=north west lines,pattern color=green] (2,6) -- (3,6-2/3) -- (3,2) -- (8,2) -- (8,6);

    \draw (2,6) -- (3,6-2/3);
    \draw[dashed] (3,6-2/3)-- (8,2);
    %\draw[dotted] (2,0)--(2,6);
    \filldraw (3,2) circle (3pt) node[below]{Thm. \ref{thm:deterministic main thm}};
    \draw (3,6-2/3) -- (3,2) -- (8,2);
    \draw (0,4.5) -- (0.7,4.5) -- (0.7,1) -- (8,1);

    \draw[line width=1pt,black] (0,0) rectangle (8,6);

    \draw (2,0.5) node {Thm. \ref{thm:lower_bounds}};
    \draw (5,5) node{Thm. \ref{thm:simple mech main theorem}};

    \end{tikzpicture}
    
    \caption{Known trade-offs between regret and expected number of audits. The dashed green (resp. red) region depicts known achievable (resp. unachievable) regions.}
    \label{fig:main_plot}
\end{figure}

In this section, we summarize our results and give some intuitions about our main mechanism. We introduce the following assumption on utility distributions.

\begin{assumption}[An Agent's Utility is Away from Zero]\label{assumption:min report}
There is an agent $i_0\in[K]$ whose utility is at least $c$ almost surely, \textit{i.e.}, $\mV_{i_0}$ is supported on $[c,1]$, where $c>0$ is a publicly known constant.
\end{assumption}

Note that we only require \textit{one} of the agents $i_0$ to have their utility distribution bounded away from $0$, and their identity is unrevealed. \Cref{assumption:min report} is weak in the sense that it is automatically satisfied, for example, if the central planner has a fixed cost $c>0$ for allocating the resource. 

Our main contribution is the design and analysis of our mechanism \mechname (\textbf{Ada}ptive \textbf{Audit}ing; \Cref{alg:mechanism}). As we will introduce in \Cref{sec:description of check prob,sec:description of est and flag}, it incorporates the future punishment and estimation via flagging techniques in \Cref{sec:technical overview}.
It enjoys the following performance guarantee, which means a constant regret and logarithmic expected number of audits in $T$. The proof of \Cref{thm:deterministic main thm} is sketched in \Cref{sec:sketch varying-p} and formally included in \Cref{sec:appendix varying-p,sec:appendix varying-p-audits}.

\begin{theorem}[Main Theorem for \mechname]\label{thm:deterministic main thm}
Under the mechanism \mechname from \Cref{alg:mechanism}, for any utility distributions $\{\mV_i\}_{i\in[K]}$ satisfying \Cref{assumption:min report}, there exists a PBE of agents' strategies $\bm \pi^\ast$ such that $\mathcal R_T(\bm \pi^\ast,\mechname)\leq K^2$ and $\mathcal B_T(\bm \pi^\ast,\mechname)= \O\left (\frac{K^3\log T}{c}\right )$.
\end{theorem}

As a motivation, we could first consider the simple mechanism $\bm M^0(p)$ which always allocates to the agent with highest report (recall that ties in utilities are broken by lexicographical order as discussed in \Cref{footnote:break ties}), audits them with a pre-determined probability $p\in (0,1]$, and eliminates them if the audit shows that they lied. A formal description of the mechanism $\bm M^0(p)$ is given in \Cref{alg:simple_mechanism} from \Cref{sec:appendix fixed-p}. This simple mechanism enjoys the following guarantee.

\begin{theorem}[Simple Mechanism Auditing with Fixed Probability]\label{thm:simple mech main theorem}
Fix $p\in(0,1]$. Under the mechanism $\bm M^0(p)$ defined in \Cref{alg:simple_mechanism}, for any utility distributions $\{\mV_i\}_{i\in[K]}$, any corresponding PBE of agents' joint strategy $\bm \pi$ ensures $\mathcal R_T(\bm \pi,\bm M^0(p))\leq \frac{K(K-1)}{p}$ and $\mathcal B_T(\bm \pi,\bm M^0(p))=pT$.
\end{theorem}

The guarantee for $\bm M^0(p)$ holds without \Cref{assumption:min report} and for any PBE, but the trade-off achieved in terms of expected regret and number of audits is much weaker than that of \mechname, as depicted in \Cref{fig:main_plot}.
To complement the trade-off between regret and number of audits, in \Cref{sec:lower bounds}, we give hardness results saying that \textit{i)} one cannot make the regret independent to $K$, and \textit{ii)} to avoid $\text{poly}(T)$ regret, the expected number of audits must be $\Omega(1)$. We remark that these lower bounds are technically non-trivial due to the prohibition of revelation principle (specifically, if we assumed that under mechanism $\bm M$, truthfully reporting is a PBE for some $\{\mV_i\}_{i\in [K]}$, this cannot hold for other $\{\mV_i'\}_{i\in [K]}$ which prohibits standard information-theoretic tools).

\begin{theorem}[Hardness Results]
\label{thm:lower_bounds}
Fix $K\geq 2$. There exists a choice of utility distributions $\mV=\{\mV_i\}_{i\in [K]}$ satisfying \Cref{assumption:min report} with $c=\frac{1}{3}$, such that for any central planner mechanism $\bm M$ and any corresponding PBE of agents' strategies $\bm \pi$, the following hold, $\mathcal R_T(\bm \pi,\bm M) =\Omega(K)$. Furthermore, even if $K=2$, there exists some $\{\mV_i\}_{i\in [K]}$ and a constant $c>0$ such that if $\mathcal B_T(\bm \pi,\bm M) \leq c$ then $\mathcal R_T(\bm \pi,\bm M) =\Omega(\sqrt {T/\log T})$.
\end{theorem}
As a final remark, these lower bounds are stronger than the upper bounds from \Cref{thm:deterministic main thm,thm:simple mech main theorem} since they hold even if the central planner had full knowledge of the agents' utility distributions.

\subsection{Incentivizing Trustworthiness when Fair Share is Available}\label{sec:description of check prob}

\input{pseudo-code}

To explain the high-level idea behind our \mechname mechanism, we first compare it with the simple mechanism $\bm M^0(p)$ that always audits the winner with a fixed probability $p\in (0,1]$ (whose pseudo-code is in \Cref{alg:simple_mechanism} and guarantee is in \Cref{thm:simple mech main theorem}). The idea of $\bm M^0(p)$ is ensuring that every winning agent has some probability of being checked and thus they hesitate to lie. However, this uniform auditing approach is wasteful since agents may have different levels of willingness to lie. More precisely, the larger the total utility an agent $i\in[K]$ can secure in the future had they remain alive, the less willing they would be to lie by reporting $v_{t,i}>u_{t,i}$. On the other extreme, an agent $i\in[K]$ which has almost 0 utility (\textit{e.g.}, very small $\mV_i$) will manipulate their report $v_{t,i}$ as much as possible in order to win once. 
\mechname therefore aims to adapt the probability of auditing to each agent $i\in[K]$ and also to $t\in[T]$ while still incentivizing the agents to be trustworthy through the threat of elimination (\Cref{line:randomly_check,line:elimination}). We refer to this as the future punishment component. 

In this section, we show how to adapt the probability of auditing while incentivizing trustworthiness. For simplicity, let us first ignore the estimation and flagging component (\Cref{line:estimate winning prob,line:flagging}) by pretending that the central planner has full information about agents' utility distributions $\{\mV_i\}_{i\in [K]}$. As suggested above, a critical parameter dictating the behavior of agents is their \emph{fair share}, defined as the expected per-round gain when all agents are truthful:
\begin{equation*}
\mu_i=\E_{\bm u\sim \mV}\big [u_{i}\mathbbm 1[u_i>u_{j},\forall j\ne i]\big ],\quad \forall i\in [K],
\end{equation*}
where ties are always broken via agents' indices (see \Cref{footnote:break ties}). Due to possible eliminations, the fair share $\mu_i$ is time-varying (\textit{i.e.}, the indicator is in fact $\mathbbm{1}[u_i>u_j,\forall j\in \mA_t\setminus \{i\}]$). This also applies to the fair winning probability $q_i$ defined later. We omit this issue for ease of presentation. We now introduce the \emph{ideal mechanism} $\bm M^\ast$ which at each time $t\in[T]$ allocates to alive agent $i_t$ with highest report, audits the winner with probability $p_{t,i_t}:=\min\set{\frac{1}{(T-t)\mu_{i_t}},1} $, and eliminates the winner if the audits shows that they were lying ($o_t=1$ and $o_t u_{t,i_t}\neq v_{t,i_t}$).
We claim that $\bm M^\ast$ ensures both low regret and few audits.

\paragraph{$\bm M^\ast$ Ensures $\text{poly}(K)$ Regret.}
Consider an agent $i\in [K]$ deciding their current-round strategy $\pi_{t,i}$, assuming \textit{i)} in any future round $t'>t$, the strategy of any agent $j\in [K]$ is some known $\pi_{t',j}$ (slightly abusing notation $\pi$), and \textit{ii)} in this round, the strategy of any opponent $j\ne i$ is also known as $\pi_{t,j}$.
Given agent $i$'s private utility $u_{t,i}$, they can calculate the expected gain of any report $v_{t,i}$ as
\begin{equation*}
V_{t,i}^{\bm \pi}(v_{t,i};u_{t,i},\mH_t^i):=\E_{\bm u_{t,-i}\sim \mu_{-i},\bm v_{t,-i}\sim \bm \pi_{t,-i},(i_t,o_t)\sim M_t^\ast}\left [u_{t,i}\mathbbm{1}[i_t=i]+V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)\right ],
\end{equation*}
where the usage of notation $V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)$ is valid because all agents' strategies in the future are fixed.
For now, let us pretend that $\E[V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)]$ is independent of $v_{t,i}$ (this dependency is mild as formalized in \Cref{lem:u lower and upper bound}).
Now we consider the following two choices for agent $i$ at round $t$:
\begin{enumerate}
\item Lie with $v_{t,i}=1$. This increases their probability of winning in round $t$, which for simplicity we consider becomes $1$. However, they are audited and eliminated \textit{w.p.} $p_{t,i}$. Hence, since $u_{t,i}\le 1$,
\begin{equation*}
V_{t,i}^{\bm \pi}(1;u_{t,i},\mH_t^i)\le 1+(1-p_{t,i})\E\left [V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)\right ].
\end{equation*}
\item Truthfully report $v_{t,i}=u_{t,i}$. Under this report, agent $i$ may lose in the current round (thus receiving 0 gain at that round), but at least remains alive for future rounds. Hence,
\begin{equation*}
V_{t,i}^{\bm \pi}(u_{t,i};u_{t,i},\mH_t^i)\ge \E\left [V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)\right ].
\end{equation*}
\end{enumerate}

Therefore, provided that $p_{t,i}\ge (\E[V_{i}^{\bm \pi}(\mH_{t+1};\bm M^\ast)])^{-1}$, agent $i$ will voluntarily report truthfully. Furthermore, as we shall formalize in \Cref{lem:u lower and upper bound}, the expectation future value is approximately $(T-t)\mu_i$, \textit{i.e.}, the expected gain when everyone is honest. Hence, the definition of the audit probability $p_{t,i}=\min\set{\frac{1}{(T-t)\mu_i},1}$ from $\bm M^\ast$ ensures that agent $i$ reports truthfully when $(T-t)\mu_i<1$. Otherwise, when $(T-t)\mu_i\ge 1$ and agent $i$ wins by lying in round $t$, we have $p_{t,i}=1$ and thus they are always audited and eliminated. In summary, report manipulations of agent $i$ can only cause a constant regret overhead. Note that as we explained in \Cref{sec:technical overview}, without distributional information, one cannot always ensure that agents are truthful.

\paragraph{$\bm M^\ast$ Ensures Logarithmic Number of Audits.}
Even better, provided all the agents are (almost) always truthful, the choice of $p_{t,i}$ from $\bm M^\ast$ ensures a logarithmic number of audits in expectation:
\begin{equation*}
\mathbb{E}\left [\sum_{t=1}^T o_t\right ]=
\mathbb{E}\left [\sum_{t=1}^T \sum_{i=1}^K \mathbbm{1}[i_t=i] p_{t,i}\right ]\lesssim \sum_{t=1}^{T-1} \sum_{i=1}^K q_{i} \frac{1}{(T-t) \mu_i}=\O\left (\frac{K\log T}{c}\right ),
\end{equation*}
where $q_{i}:=\Pr_{\bm u\sim \mV}\{u_i>u_{j},\forall j\ne i\}$ is called the \textit{fair winning probability} of agent $i\in [K]$. The second equality is because \Cref{assumption:min report} infers $q_i\ge \frac{\mu_i}{c}$.

Formalizing the previous arguments requires a careful analysis which is deferred to \Cref{sec:sketch varying-p}. Still, beyond technical details, the mechanism $\bm M^\ast$ requires knowing the fair shares $\bm\mu$. In practice, these must be \emph{learned} online. As discussed in \Cref{sec:technical overview} this estimation could be significantly biaised by agents' strategic behaviors. We detail our approach to tackle this issue in the following section.

\subsection{Estimating Fair Shares Online via Flagging}\label{sec:description of est and flag}

To implement the ideas of $\bm M^\ast$ without having access to agents' utility distributions, \mechname estimates the fair winning probability $q_i$ for each agent $i\in[K]$ in \Cref{line:estimate winning prob}. After the estimation, \mechname sets audit probability $\hat p_{t,i}\approx \frac{1}{(T-t)c\hat q_i}$ similarly as in $\bm M^\ast$ (from \Cref{assumption:min report}, $c q_i\leq \mu_i$). 
If all agents reported truthfully during this estimation phase, the gap between two consecutive wins of agent $i\in[K]$ is geometrically distributed with parameter $q_i$. Therefore, after $\O(\log T)$ wins of agent $i$, we can expect the mean of these gaps to be within a multiplicative factor of $q_i^{-1}$ \textit{w.p.} $\frac{1}{\text{poly}(T)}$.

However, as discussed in \Cref{sec:technical overview}, agents can strategically manipulate their reports to affect the estimation process. Agents can anticipate events when the estimates are off, conditionally on their available history. If they anticipate that such an event disfavors their individual gain, they may decide to manipulate the estimation early. This is especially true as some of the estimations only span a logarithmic number of rounds but affect all future rounds through the audit probabilities. 

To tackle this issue, we propose a workaround that allows agents to \textit{flag} agents who have a biased estimate $\hat q_i$ (see \Cref{line:flagging}).
As we saw in \Cref{sec:description of check prob}, agent $i$ reports truthfully when $\hat p_{t,i}\gtrsim \frac{1}{(T-t)\mu_i}$, which holds when $\hat q_i\lesssim q_i$.
As a result, when $\hat q_i\gg q_i$, agent $i$ may lie more and hurt other agents' gains. \Cref{line:flagging} gives the victims $j\ne i$ a possibility to flag such events (who indeed have incentives to do so).
On the other hand, if $\hat q_i\ll q_i$, agent $i$ themselves are penalized because their audit probability would be higher. In this case, agent $i$ may be interested in flagging themselves to get a fair re-estimation.
In summary, the flagging component aligns agents' individual incentives with accurately estimating $(q_i)_{i\in[K]}$. The above intuitions are formalized in \Cref{sec:sketch varying-p}.
