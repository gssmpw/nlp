%!TEX root=main.tex

\section{Preliminaries}\label{sec:setup}

\paragraph{Repeated Resource Allocation with Audits.} We consider a $T$-round sequential game in which a central planner repeatedly allocates a single resource within $K\geq 2$ strategic agents. Private utilities of each agent $i\in[K]$ are assumed to be sampled \textit{i.i.d.} from a fixed utility distribution $\mV_i$ in each round. Without loss of generality, we assume these distributions have support on $[0,1]$. In each round, the central planner receives reports $v_{t,i}$ from all agents $i\in[K]$ and decides on who to allocate the resource to (potentially no one). The central planner can then potentially request an {audit} of the winner, revealing their true utility. Formally, each round $t\in[T]$ proceeds as follows.
\begin{enumerate}
    \item Each agent $i\in[K]$ observes their private utility $u_{t,i}\overset{i.i.d.}{\sim} \mV_i$ sampled independently from the history and between agents. We abbreviate this process as $\bm u_t\sim \mV$.
    \item Each agent $i\in[K]$ reports a report $v_{t,i}\in [0,1]$ to the central planner based on the history and their own utility $u_{t,i}$ only (they do not know their exact future utilities $u_{t',i}$ for $t'>t$).
    \item The central planner allocates the resource to an agent $i_t\in[K]\cup\{0\}$ based on the history and reports $\bm v_t=\{v_{t,i}\}_{i\in[K]}$ only. Choosing $i_t=0$ corresponds to not allocating the resource.
    \item After the allocation, the central planner may decide to perform an {audit} or not, by choosing a binary variable $o_t\in\{0,1\}$. If $o_t=1$, the central planner observes as feedback $u_{t,i_t}$. \label{item:possible_audit}
    \item All public information is revealed to agents $[K]$, that is, $\bm v_t$, $i_t$, $o_t$ and the feedback $o_t u_{t,i_t}$. The central planner may request further information from agents to which they answer strategically.
    \label{item:additional_questions}
\end{enumerate}
The last step~\ref{item:additional_questions} gives the opportunity to the central planner to further interact with agents beyond their reports. We denote by $f_{t,i}$ the answer of agent $i\in[K]$ in round $t$ and assume answers must lie in a fixed measurable space $F$ (where $F=\{0\}$ corresponds to not asking further questions).

\paragraph{No Prior Distributional Information for the Central Planner.}
We assume the central planner does not have any prior knowledge on the distributions $\{\mV_i\}_{i\in[K]}$. Any information on these must be learned by requesting audits, which is the only difference between our setup and standard repeated allocation.
To further introduce information asymmetry, a common challenge in real-world resource allocations, we assume the utility distributions $\{\mV_i\}_{i\in [K]}$ are public knowledge among agents. This is in fact necessary to formally define a Bayesian equilibrium for agents. An equilibrium can only be reached if agents have knowledge of all the agents' utility distributions (or at least some Bayesian prior).

\paragraph{Game-Theoretic Definitions.}
We start by formally defining histories at the beginning of a round $t\in[T]$. The {public history} $\mH_t^0$ contains all agents' previous reports, allocations, audit decisions, potential feedback, and agent answers, {i.e.}, $\mH_t^0:=\set{(\bm v_\tau,i_\tau,o_\tau,o_\tau u_{\tau,i_\tau}, \bm f_\tau)}_{\tau<t}$. For any agent $i\in[K]$, their {observable history} $\mH_t^i$ additionally contains their private utilities, {i.e.}, $\mH_t^i:=\set{(u_{\tau,i},\bm v_\tau,i_\tau,o_\tau,o_\tau u_{\tau,i_\tau},\bm f_\tau)}_{\tau<t}$. Last, we define the {complete history} at round $t$ via $\mH_t:=\set{(\bm u_{\tau},\bm v_\tau,i_\tau,o_\tau,o_\tau u_{\tau,i_\tau}, \bm f_\tau)}_{\tau<t}$. Note that the complete history is not available to any player and will only be used for proof purposes. For convenience, we define the space of public histories via $H_t^0:=([0,1]^K \times([K]\cup\{0\})\times \{0,1\}\times [0,1]\times F^K)^{t-1}$, and similarly the space of agent observable histories $H_t^i$ for $i\in[K]$ and the space of full histories $H_t$. We now define player strategies. To allow for randomized strategies, we further fix a measurable space $\Xi$ and a distribution $\mD_\xi$ on $\Xi$.

\begin{definition}[Central Planner Mechanism]
    A {central planner mechanism} is a sequence of measurable functions $\bm M:=(M_t)_{t\in[T]}$ where $M_t:[0,1]^K\times H_t^0\times \Xi \to ([K]\cup\{0\}) \times \{0,1\}$. The allocation $i_t$ and audit decision $o_t$ in round $t\in[T]$ are given by $(i_t,o_t):=M_t(\bm v_t,\mH_t^0,\xi_{t,0})$, where $\xi_{t,0}\sim\mD_\xi$ is sampled independently from all other random variables.
\end{definition}

\begin{definition}[Agent Strategies]
\label{def:agent_strat_mechanism}
    A {report strategy} for agent $i\in[K]$ is a sequence of measurable functions $\bm\pi_i^r:=(\pi^r_{t,i})_{t\in[T]}$ where $\pi^r_{t,i}:[0,1]\times H_t^i\times \Xi \to [0,1]$. Their report in round $t\in[T]$ is $v_{t,i}:=\pi^r_{t,i}(u_{t,i},\mH_t^i,\xi^r_{t,i})$, where $\xi^r_{t,i}\sim\mD_\xi$ is sampled independently from other random variables.

    An {answer strategy} (or flagging strategy, which we use interchangably) for agent $i$ is a sequence of measurable functions $\bm\pi^f_i:=(\pi^f_{t,i})_{t\in[T]}$ where $\pi^f_{t,i}:[0,1]\times [0,1]^K\times ([K]\cup\{0\})\times \{0,1\}\times [0,1]\times H_t^i\times \Xi \to F$. Their answer in round $t\in[T]$ (Step~\ref{item:additional_questions}) is $f_{t,i}:=\pi^f_{t,i}(u_{t,i},\bm v_t,i_t,o_t,o_t u_{t,i_t}, \mH_t^i,\xi^f_{t,i})$, where $\xi^f_{t,i}\sim\mD_\xi$ is sampled independently from all other random variables.

    A {strategy} for agent $i$ is composed of a report and answer strategy $\bm\pi_i:=(\bm\pi^r_i,\bm\pi_i^f)$.

    A {joint strategy} for agents is a collection of all agent strategies $\bm\pi:=(\bm \pi_{i})_{i\in[K]}$.
\end{definition}

For a given central planner mechanism $\bm M$ and joint strategy for agents $\bm\pi$, we define the value function (V-function) of any agent $i\in[K]$ from any complete history $\mH_t$ as 
\begin{equation}\label{eq:V-func general}
V_i^{\bm \pi}(\mH_t;\bm M):=\E\left [\sum_{\tau=t}^T u_{\tau,i} \mathbbm{1}[i_{\tau}=i]\middle \vert \mH_t\right ],\quad \forall i\in [K], \mH_t\in H_t.
\end{equation}
Because they are strategic, the agents' objective is to maximize their own utility function when deciding their strategies. Formally, the resulting joint strategy forms a Perfect Bayesian Equilibrium.

\begin{definition}[Perfect Bayesian Equilibrium (PBE)]
A joint strategy $\bm \pi$ is a Perfect Bayesian Equilibrium (PBE) under a central planner mechanism $\bm M$ if any unilateral deviation of any agent cannot increase their utility function. That is, for any alternative strategy $\bm \pi_i'$ of agent $i\in [K]$, let $\bm \pi'$ be the joint strategy where agent $i$ follows $\bm \pi_i'$ and any other agent $j\ne i$ follows $\bm \pi_j$, then we must have
\begin{equation*}
V_i^{\bm \pi'}(\mH_t;\bm M)\le 
V_i^{\bm \pi}(\mH_t;\bm M),\quad \forall t\in[T],\mH_t\in H_t.
\end{equation*}
\end{definition}


\paragraph{Objectives of the Central Planner.} The central planner aims to design a mechanism that both (in expectation) minimizes the number of audits requested and maximizes the social welfare $\sum_{t=1}^T u_{t,i_t}$, at a corresponding PBE. Here, we defined $u_{t,0}=0$ for all $t\in [T]$ for simplicity. The latter objective is equivalent to minimizing the regret \textit{w.r.t.} optimal allocations in hindsight, as defined below.

\begin{definition}[Regret and Expected Number of Audits]
For a central planner mechanism $\bm M$ and an agents' joint strategy $\bm\pi$, the (social welfare) regret and the expected number of audits are
\begin{align*}
\mathcal R_T(\bm \pi,\bm M)&= \E_{\substack{\bm u_{t}\sim \mV,\bm v_t\sim \bm \pi,\\(i_t,o_t)\sim \bm M,\\ \bm f_t\sim\bm\pi}}\sqb{\sum_{t=1}^T \left (\max_{i\in [K]} u_{t,i}-u_{t,i_t}\right )} \quad \text{and}\quad
\mathcal B_T(\bm \pi,\bm M)= \E_{\substack{\bm u_{t}\sim \mV,\bm v_t\sim \bm \pi,\\(i_t,o_t)\sim \bm M,\\ \bm f_t\sim\bm\pi}}\sqb{\sum_{t=1}^T o_t}.
\end{align*}
\end{definition}
