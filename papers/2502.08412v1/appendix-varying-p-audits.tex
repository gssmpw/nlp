\section{Guarantees on the Regret and Number of Audits for \mechname (\Cref{thm:deterministic main thm})}\label{sec:appendix varying-p-audits}

In this section, we prove our main result \Cref{thm:deterministic main thm} which bounds the regret and expected number of audits of the mechanism \mechname. We prove each bound separately in \Cref{subsec:proof_regret,subsec:proof_audits}.

\subsection{Proof of Regret Guarantee}\label{subsec:proof_regret}

Using \Cref{lem:u lower and upper bound formal}, we directly prove the following bound on the regret of \mechname under the PBE $\bm\pi^\ast$.

\begin{theorem}[Regret Guarantee of \mechname]\label{thm:regret main theorem formal}
Under the mechanism \mechname defined in \Cref{alg:mechanism}, there exists a PBE of agents' strategies, namely $\bm \pi^\ast=(\pi_{t,i}^\ast)_{t\in [T],i\in [K]}$, such that
$\mathcal R_T(\bm \pi^\ast,\mechname)\le K^2$.
\end{theorem}
\begin{proof}
From \Cref{thm:equivalence of PBE}, the $\bm \pi^\ast$ defined using ${\bm \pi}^{\ast,r}$ (the PBE in the auxiliary game \Cref{def:auxiliary game}) and ${\bm \pi}^{\ast,f}$ (the well-behaved flagging policy; \Cref{def:well-behaved flagging formal}) is a PBE.
Under ${\bm \pi}^\ast$, using the lower bound \Cref{eq:for alive} from \Cref{lem:u lower and upper bound formal}, the social welfare is at least
\begin{equation*}
\sum_{i=1}^K V_i^{{\bm \pi}^\ast}(\mH_1)\ge \sum_{i=1}^K (T\mu_{1,i}-K)=T\sum_{i=1}^K \mu_{1,i}-K^2,
\end{equation*}
where we used the fact that all agents are alive in the very beginning (that is, $\mA_1=[K]$).

Therefore, since the maximum possible social welfare is exactly $T\sum_{i=1}^K \mu_{1,i}$,
\begin{align*}
\mathcal R_T(\bm \pi^
\ast,\mechname)&=\E \left [\sum_{t=1}^T \left (\max_{i\in [K]} u_{t,i}-u_{t,i_t}\right )\right ]\\
&=\sum_{i=1}^K \E\left [\sum_{t=1}^T u_{t,i}\mathbbm{1}[u_{t,i}>u_{t,j},\forall j\ne i]\right ]-\sum_{i=1}^K V_i^{{\bm \pi}^\ast}(\mH_1)\\
&\overset{(a)}{=}\sum_{i=1}^K \E\left [\sum_{t=1}^T u_{t,i} \mathbbm{1}[u_{t,i}\ge c] \mathbbm{1}[u_{t,i}>u_{t,j},\forall j\ne i]\right ]-\sum_{i=1}^K V_i^{{\bm \pi}^\ast}(\mH_1)\\
&\le T\sum_{i=1}^K \mu_{1,i}-\left (T\sum_{i=1}^K \mu_{1,i}-K^2\right )=K^2,
\end{align*}
where (a) uses \Cref{assumption:min report}. The regret upper bound hence follows.
\end{proof}

\subsection{Proof of Number of Audits Guarantee}\label{subsec:proof_audits}

We next turn to proving the guarantee on the expected number of audits of \mechname from \Cref{thm:deterministic main thm}. This requires a few additional notations.

\begin{definition}[Estimation Epochs]
Let $1=t_1,t_2,\ldots$ be the rounds in which we eliminate everyone and reset all empirical estimations $\hat q_{t,i}$'s to $0$ (see \Cref{line:elimination}). Formally, we write
\begin{equation*}
    t_\ell=\min \set{t\in[T]\mid \lvert \mA_t\rvert =T-i+1}\cup\{+\infty\}, \quad \ell\in[K],
\end{equation*}
where $t_\ell=+\infty$ indicates that there were at most $\ell-1$ restarts of the estimation process. The $\ell$-th estimation epoch $E_\ell$ is all the rounds $E_\ell=\{t_\ell,t_\ell+1,\ldots,t_{\ell+1}-1\}$.
\end{definition}

For each agent $i\in[K]$ and each estimation epoch $\ell\in[K]$ in which agent $i$ is alive, we decompose the rounds in which agent $i$ wins on two phases. The first \emph{estimation} phase corresponds to the rounds where $\hat q_{t,i}=0$, and the second \emph{incentive-compatible} phase is when $\hat q_{t,i}>0$ (recall from \Cref{alg:mechanism} that $\hat q_{t,i}$ means the estimation $\hat q_i$ at the beginning of round $t$). To avoid confusions, we denote by $\hat q_{\ell,i}$ the final estimation $\hat q_{t,i}$ in epoch $\ell$ that no agent answer with $f_{t,i}=1$; this $\hat q_{\ell,i}$ is then fixed throughout the $\ell$-th incentive-compatible phase. In the special case where there is no incentive-compatible phase during the $\ell$-th epoch, we write $\hat q_{\ell,i}=0$.

As introduced in \Cref{sec:sketch varying-p} (Part II), bounding the number of audits requires bounding the number of events where agents voluntarily mark down. We prove the following complete version of \Cref{lem:number of marking down bound}.

\begin{lemma}\label{lem:number of marking down bound formal}
For every round $t\in [T]$ and agent $i\in [K]$, define indicator $D_{t,i}$ for the event that they are alive in round $t$, do not win, but would have won had they reported honestly, i.e.,
\begin{equation*}
D_{t,i}=\mathbbm{1}[i\in \mA_t]\mathbbm{1}[i_t\ne i]\mathbbm{1}[u_{t,i}\geq c]\mathbbm{1}[u_{t,i}>v_{t,j},\forall j\in \mA_t\setminus\{i\}].
\end{equation*}
Now we consider any history $\mH_{t_\ell}$ corresponding to the start of a new estimation epoch $E_\ell$. We have
\begin{equation*}
    \E\sqb{\sum_{i\in\mA_{t_\ell}}\sum_{t\in E_\ell} D_{t,i}\middle \vert\mH_{t_\ell}} \leq \frac{2K^2}{c}.
\end{equation*}
\end{lemma}

\begin{proof}
Fix $t\geq t_\ell$ and consider any history $\mH_{t}$ within $E_\ell$. Throughout this proof, the probabilities will always be taken over the allocation process starting from history $\mH_{t}$. We aim to show that
\begin{equation}\label{eq:lower_bound_proba_die}
    \Pr \{i_t\text{ eliminated at round } t\mid\mH_t \} \geq \frac{c}{2K^2} \E\sqb{\sum_{i\in\mA_t} D_{t,i} \middle \vert \mH_t}.
\end{equation}

To prove \Cref{eq:lower_bound_proba_die}, we consider an alternative policy $\pi_{t,i}$ for agent $i$ at time $t$:
\begin{equation*}
\pi_{t,i}=\max(v_{t,i},u_{t,i}),\quad \text{where }v_{t,i}\sim \pi_{t,i}^\ast(u_{t,i};\tilde \mH_{t}),
\end{equation*}
which removes all mark-downs suggested by $\bm \pi_i^\ast$. We now compare the utilities collected by agent $i$ starting from $\mH_{t}$ by $\bm\pi_i^\ast$ and $\pi_{t,i} \diamond_t \bm\pi_{i}^\ast$. Note that the only event making these two policies have different allocations $i_{t}$ or different next-round auxiliary histories $\tilde \mH_{t+1}$ is when $D_{t,i}=1$.

Suppose that $D_{t,i}=1$. If agent $i$ followed $\pi_{t,i} \diamond_t \bm\pi_{i}^\ast$, then they report $u_{t,i}$ in round $t$ and the next-round auxiliary history is $\tilde\mH_{t+1}^0:=(t+1,\mA_t)$. Using \Cref{lem:equiv between actual and no-flagging}, their utility is
\begin{equation*}
    V_{1,i}:=u_{t,i} + \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^0) \geq c + \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^0),
\end{equation*}
where we used the definition of $D_{t,i}=1$ to get $u_{t,i}\geq c$.

On the other hand, if agent $i$ used strategy $\bm \pi_{i}^\ast$, they report $v_{t,i}$ and lose in round $t$. There are two possibilities for the next-round auxiliary history: either the winner $i_{t}$ is eliminated which results in $\tilde\mH_{t+1}^1:=(t+1,\mA_t\setminus\{i_{t}\})$, or the winner remains alive and gives $\tilde\mH_{t+1}^0$. Again using \Cref{lem:equiv between actual and no-flagging}, the utility obtained is
\begin{align*}
    V_{2,i}&:=\tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^0) + \1[i_{t}\text{ eliminated}] \paren{ \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^1) - \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^0) },
\end{align*}
where the inequality comes from the upper and lower bounds in \Cref{lem:u lower and upper bound}. For completeness, when $D_{t,i}=0$, let us pose $V_{1,i}:=V_{2,i}:=0$.
Altogether, we obtained
\begin{align*}
    V_i^{\bm\pi^\ast}(\mH_{t}) &- V_i^{(\pi_{t,i}\circ \bm \pi_{t,-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_{t}) = \E[(V_{2,i}-V_{1,i})D_{t,i} \mid \mH_t]\\
    &\leq \E\sqb{D_{t,i}\1[i_t \text{ eliminated}] \paren{ \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^1) - \tilde V_i^{{\bm \pi}^{\ast,r}}(\tilde\mH_{t+1}^0) }\middle \vert \mH_t} - c\Pr\{D_{t,i}=1\mid\mH_t\}.
\end{align*}
Since $\bm\pi^\ast$ is a PBE, the left-hand side above is non-negative for all $i\in\mA_t$. Also, note that the auxiliary histories $\tilde\mH_{t+1}^0$ and $\tilde\mH_{t+1}^1$ only depends on $i_t$. In particular, if we fix any two realizable histories $\mH_{t+1}^0$ and $\mH_{t+1}^1$ such that their auxiliary histories are $\tilde\mH_{t+1}^0=(t+1,\mA_t)$ and $\tilde\mH_{t+1}^1=(t+1,\mA_t\setminus\{i_t\})$ respectively, the sum of the previous equations gives
\begin{align}
    0&\leq \sum_{i\in\mA_t} \left (V_i^{\bm\pi^\ast}(\mH_{t}) - V_i^{(\pi_{t,i}\circ \bm \pi_{t,-i}^\ast)\diamond_t \bm \pi^\ast}(\mH_{t})\right ) \notag \\
    &= \E\sqb{\1[i_t \text{ eliminated}]\sum_{i\in \mA_t}  D_{t,i} \paren{   V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) -  V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^0) }\middle \vert\mH_t} - c\E\sqb{\sum_{i\in\mA_t}D_{t,i}\middle \vert\mH_t} .\label{eq:summed_PBE_equations}
\end{align}
Now fix a specific realization for the reports at time $t$ such that $i_t$ was eliminated at that round (when all agents followed $\bm\pi^\ast$ at round $t$). We introduce the set $S:=\{i\in\mA_t\mid D_{t,i}=1\}$ and control the sum $\sum_{i\in \mA_t}D_{t,i}(V_i^{\bm \pi^\ast}(\mH_{t+1}^1)-V_i^{\bm \pi^\ast}(\mH_{t+1}^0))=\sum_{i\in S}(V_i^{\bm \pi^\ast}(\mH_{t+1}^1)-V_i^{\bm \pi^\ast}(\mH_{t+1}^0))$ in \Cref{eq:summed_PBE_equations}.

For $\sum_{i\in S} V_i^{\bm \pi^\ast}(\mH_{t+1}^0)$, we utilize the lower bound on V-functions for those alive agents from \Cref{lem:u lower and upper bound formal} (which is applicable to all $i\in S\subseteq \mA_{t+1}$) and get
\begin{equation}\label{eq:lower_bound_sum_values}
    \sum_{i\in S} V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^0) \geq (T-t)\sum_{i\in S}\mu_{t+1,i}^0 - K|S| = (T-t)\sum_{i\in S}\mu_{t,i} - K|S|.
\end{equation}
where by $\mu_{t+1,i}^0$ we denote the fair share of agent $i$ corresponding to the auxiliary history $\tilde\mH_{t+1}^0$. In the last equality, we use the fact that $\tilde \mH_{t+1}^0=(t+1,\mA_t)$ and thus $\mu_{t+1,i}^0=\mu_{t,i}$.

For $\sum_{i\in S}V_i^{\bm \pi^\ast}(\mH_{t+1}^1)$, we rewrite it as $\sum_{i\in \mA_{t+1}}V_i^{\bm \pi^\ast}(\mH_{t+1}^1)-\sum_{i\in \mA_{t+1}\setminus S}V_i^{\bm \pi^\ast}(\mH_{t+1}^1)$. For the first term, using the upper bounds on V-functions (specifically, \Cref{eq:upper_bound_max_welfare} in the proof of \Cref{lem:u lower and upper bound formal}),
\begin{equation*}
    \sum_{i\in \mA_{t+1}} V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1)  \leq (T-t)\sum_{i\in\mA_{t+1}}\mu_{t+1,i}^1 + K^2.
\end{equation*}
Again, $\mu_{t+1,i}^1$ denotes the fair share corresponding to the auxiliary history $\tilde\mH_{t+1}^1$ which corresponds to the set of alive agents $\mA_t\setminus\{i_t\}$. Meanwhile, via the lower bound on V-functions in \Cref{lem:u lower and upper bound formal},
\begin{equation*}
     \sum_{i\in\mA_{t+1}\setminus S} V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) \geq (T-t)\sum_{i\in\mA_{t+1}\setminus S}\mu_{t+1,i}^1 -K|\mA_{t+1}\setminus S|.
\end{equation*}
Note that because agents in $S$ marked down at round $t$, they are not eliminated, that is $S\subset\mA_{t+1}$. Then, combining the two previous equations implies
\begin{align}
    \sum_{i\in S}  V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) &\leq (T-t)\sum_{i\in S}\mu_{t+1,i}^1 + K^2 + K(|\mA_t|-|S|-1)\notag\\
    &\leq (T-t)\paren{\sum_{i\in S}\mu_{t,i} + \mu_{t,i_t}} + K^2 + K(|\mA_t|-|S|-1)\label{eq:new_upper_bound_sum_S}
\end{align}
In the last inequality, we used the fact that the only difference between the fair shares $\mu_{t,i}$ and $\mu_{t+1,i}$ are that in the second one, agent $i_t$ is not alive anymore and as a result,
\begin{align*}
    \sum_{i\in S}\mu_{t+1,i}^1 
    &= \E_{\bm u\sim \mV}\sqb{\max_{i\in S}u_i \cdot \1\sqb{\max_{i\in S}u_i> \max_{i\in \mA_t\setminus(S\cup\{i_t\})}u_i}}\\
    &= \E_{\bm u\sim \mV}\sqb{\max_{i\in S}u_i \cdot \paren{ \1\sqb{\max_{i\in S}u_i> \max_{i\in \mA_t\setminus S}u_i} + \1\sqb{u_{i_t}>\max_{i\in S}u_i> \max_{i\in \mA_t\setminus S}u_i }}} \\
    &\leq \sum_{i\in S}\mu_{t,i} + \E_{\bm u}\sqb{u_{i_t} \cdot \1\sqb{u_{i_t}>\max_{i\in S}u_i> \max_{i\in \mA_t\setminus (S\cup\{i_t\})}u_i }} \leq \sum_{i\in S}\mu_{t,i} + \mu_{t,i_t}.
\end{align*} 
Next, the proof of \Cref{eq:for dead} in \Cref{lem:u lower and upper bound formal} showed that for all $i\in\mA_t$, if we had $(T-t)\mu_{t,i}>K$ then we would have $i\in\mA_{t+1}$ \textit{a.s.} As a result, with probability one, $i_t\notin\mA_{t+1}$ (\textit{i.e.}, $i_t$ is eliminated) implies $(T-t)\mu_{t,i_t}\leq K$. Therefore, with probability one, \Cref{eq:new_upper_bound_sum_S} gives
\begin{equation*}
    \1[i_t\text{ eliminated}] \sum_{i\in S}  V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) \leq \1[i_t\text{ eliminated}]\paren{ (T-t)\sum_{i\in S}\mu_{t,i} + K^2 + K(|\mA_t|-|S|)}.
\end{equation*}

Together with \Cref{eq:lower_bound_sum_values}, this shows that with probability one
\begin{align*}
    &\quad \1[i_t\text{ eliminated}]\sum_{i\in \mA_t} D_{t,i}\paren{  V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) - V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^0)} \\
    &=\1[i_t\text{ eliminated}]\sum_{i\in S} \left (V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^1) -  V_i^{{\bm \pi}^{\ast}}(\mH_{t+1}^0)\right )\leq 2K^2 \1[i_t\text{ eliminated}].
\end{align*}
Plugging this identity within \Cref{eq:summed_PBE_equations} exactly proves \Cref{eq:lower_bound_proba_die}.

As a result of \Cref{eq:lower_bound_proba_die}, the sequence formed by
\begin{equation*}
    \sum_{\tau=t_\ell}^{t} \sum_{i\in\mA_{\tau}}D_{\tau,i} - \frac{2K^2}{c} (|\mA_{t_\ell}| - |\mA_{t+1}|)
\end{equation*}
for $t\geq t_\ell$ is a supermartingale with increments bounded by $1$ in absolute utility. Then, we can apply Doob's stopping theorem to the stopping time $\min(t_{\ell+1}-1,T)$ and conclude
\begin{equation*}
    \E\sqb{\sum_{t\in E_\ell} \sum_{i\in\mA_t}D_{t,i} \middle \vert \mH_{t_l}} \leq \frac{2K^2}{c}.
\end{equation*}
Recalling that for all $t\geq t_\ell$ we have $\mA_t\subseteq\mA_{t_\ell}$ and that $D_{t,i}\leq \1[i\in\mA_t]$ ends the proof.
\end{proof}

We are now ready to bound the expected number of audits of \mechname under the PBE $\bm\pi^\ast$.

\begin{theorem}[Number of Audits Guarantee of \mechname]\label{thm:audit main theorem formal}
Under the same conditions as in \Cref{thm:regret main theorem formal}, the expected number of audits made by \mechname under the same PBE $\bm \pi^\ast$ ensures
\begin{equation*}
\mathcal B_T(\bm \pi^\ast,\mechname)=\O\paren{\frac{K^3}{c} \log T}.
\end{equation*}
\end{theorem}
\begin{proof}
We control the number of audits that we spend on every agent $i\in [K]$, every estimation epoch $\ell\in [K]$, and every corresponding estimation and incentive-compatible phase separately.

\paragraph{Estimation Phase.}
We use the same notation $D_{t,i}$ as introduced in \Cref{lem:number of marking down bound formal}. When agent $i$ wins in some round $t\in E_\ell$ during its estimation phase, at least one of these scenarios holds:
\begin{itemize}
\item Agent $i$ marked up. In this case, $t_{\ell+1}=t+1$ and $i$ is eliminated. This happens at most once.
\item Agent $i$ would have won had \emph{all} agents reported truthfully, which formalizes as $u_{t,i}>u_{t,j}$, $\forall j\in \mA_{t_\ell}\setminus \{i\}$. Denote by $F_{t,i}$ the indicator of this event.
\item Some other alive agent $j\in \mA_{t_\ell}\setminus \{i\}$ would have won if they reported truthfully but marked down. This corresponds to the case where $D_{t,j}=1$.
\end{itemize}
Therefore, we may upper bound the number of wins of agent $i$ from $t_\ell$ and until some $\tau\ge t_\ell$ as
\begin{align}
    \abs{\set{ t\in[t_\ell,\tau]\mid i_{t}=i }}
    &\leq \sum_{t=t_\ell}^{\tau} F_{t,i} + \underbrace{1+\sum_{t\in E_\ell} \1[i_{t}=i] \left (\bigvee_{j\in\mA_{t_\ell}\setminus\{i\}} D_{t,j} \right )}_{:=M_{\ell,i}}.\label{eq:upper_bound_nb_wins}
\end{align}
In words, the number of rounds that agent $i$ actually wins (LHS) is at most its fair winning rounds (those $t$'s with $F_{t,i}=1$) plus those other agents miss ($t$'s where any other $j\ne i$ has $D_{t,j}=1$).

On the other hand, we lower bound the same quantity in another way. In a round $t\in E_\ell$ where agent $i$ would have won if \emph{all} agents reported truthfully (so $F_{t,i}=1$), one of these scenarios holds
\begin{itemize}
    \item Agent $i$ indeed wins, \textit{i.e.}, $i_t=i$.
    \item Agent $i$ did not win because itself is marking down. That is, $v_{t,i}<\max_{j\in \mA_{t_\ell}\setminus \{i\}}v_{t,j}<u_{t,i}$. This corresponds to the case where $D_{t,i}=1$.
    \item The agent $i_t$ won by marking up. This means $t_{\ell+1}=t+1$ and $i_t$ is eliminated at that time. This happens at most once during $E_\ell$.
\end{itemize}
Therefore, for any $\tau\ge t_\ell$, we also have
\begin{equation}\label{eq:lower_bound_nb_wins}
    \abs{\set{ t\in[t_\ell,\tau]\mid i_{t}=i }} \geq \sum_{t=t_\ell}^{\tau} F_{t,i} - \underbrace{\paren{\sum_{t\in E_\ell}D_{t,i} + 1}}_{:=N_{\ell,i}}.
\end{equation}
In words, the number of rounds that agent $i$ actually wins (LHS) is at least its fair winning rounds minus the rounds that itself misses.

Last, we define ${\mathfrak t}_{\ell,i}$ as the first round $t$ such that agent $i$ wins for at least $\max(1,4M_{\ell,i}, 3N_{\ell,i})$ times during epoch $E_\ell$ and that
\begin{equation}\label{eq:constraint_F}
    \frac{1}{t-t_\ell+1}\sum_{\tau=t_\ell}^t F_{t,i} \in\sqb{\frac{q_{\ell,i}}{3}, 3q_{\ell,i}},
\end{equation}
which roughly means the estimation $\hat q_{t,i}$ generated using outcomes of rounds $t_\ell,t_\ell+1,\ldots,t$ is close to the actual $q_{t,i}$.
If such $t$ does not exist, we define ${\mathfrak t}_{\ell,i}=t_{\ell+1}-1$ as the last round of epoch $E_\ell$.
We formalize the intuition for \Cref{eq:constraint_F} as the following claim:

\begin{claim}
The estimation phase of agent $i\in [K]$ in epoch $\ell\in [K]$ ends on or before $\mathfrak t_{\ell,i}$.
\end{claim}
\begin{proof}
From \Cref{line:estimate winning prob} of \Cref{alg:mechanism}, the estimation phase ends in some round $\tau \in E_\ell$ once no agent flags the estimated $\hat q_{\tau,i}$. Recalling the well-behaved flagging policy ${\bm\pi}^{\ast,f}$ from \Cref{def:well-behaved flagging}, this corresponds to the first time that $\hat q_{\tau,i} \in [ q_{\tau,i}/4, 4 q_{\tau,i}]=[q_{\ell,i}/4,4q_{\ell,i}]$. We now show this happens on or before $\mathfrak t_{\ell,i}$.

The claim is immediate if ${\mathfrak t}_{\ell,i}$ is the last time of epoch $E_\ell$.
Otherwise, because agent $i$ won at least $4M_{\ell,i}$ times, we have
\begin{equation*}
    \sum_{\tau=t_\ell}^{{\mathfrak t}_{\ell,i}} F_{t,i}\geq \abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }}-M_{i,j} \geq \frac{3\abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }}}{4},
\end{equation*}
where we used \Cref{eq:upper_bound_nb_wins} in the first inequality. This implies
\begin{equation*}
    \frac{ \abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }} }{{\mathfrak t}_{\ell,i}-t_\ell+1} \leq \frac{4}{3({\mathfrak t}_{\ell,i}-t_\ell+1)} \sum_{\tau=t_\ell}^{{\mathfrak t}_{\ell,i}} F_{t,i} \leq 4q_{\ell,i}.
\end{equation*}
In the last inequality, we used the definition of $\mathfrak t_{\ell,i}$ in \Cref{eq:constraint_F}. On the other hand, because at time $\mathfrak t_{\ell,i}$ agent $i$ won at least $3N_{\ell,i}$ times, \Cref{eq:lower_bound_nb_wins} implies
\begin{equation*}
    \sum_{\tau=t_\ell}^{{\mathfrak t}_{\ell,i}} F_{t,i} \leq \abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }} + N_{\ell,i}\leq \frac{4}{3}\abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }}.
\end{equation*}
Therefore,
\begin{equation*}
    \frac{ \abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }} }{{\mathfrak t}_{\ell,i}-t_\ell+1} \geq \frac{3}{4({\mathfrak t}_{\ell,i}-t_\ell+1)} \sum_{\tau=t_\ell}^{{\mathfrak t}_{\ell,i}} F_{t,i} \geq \frac{q_{\ell,i}}{4}.
\end{equation*}
Again, the last inequality used \Cref{eq:constraint_F}. As a result, the estimation phase of agent $i$ during epoch $\ell$ ends at or before $\mathfrak t_{\ell,i}$.
\end{proof}

Using this claim, the number of audits that we spend on agent $i$ \textit{in their estimation phase} during epoch $\ell$ then satisfies
\begin{align*}
    A_{\ell,i}:=\sum_{t\in E_\ell} o_t \1[i_t=i] \1[\hat q_{t,i}=0] 
    &\leq \abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }}.
\end{align*}

We make the following claim regarding the expectation of all alive agents' $A_{\ell,i}$'s:
\begin{claim}
The $A_{\ell,i}$ above satisfies $\E[\sum_{i\in\mA_{t_\ell}} A_{\ell,i}\mid \mH_{t_\ell}]=\O(K+\E[\sum_{i\in\mA_{t_\ell}} \sum_{t\in E_\ell} D_{t,i}\mid \mH_{t_\ell}])$.
\end{claim}
\begin{proof}
For an alive agent $i\in \mA_{t_\ell}$, to control $\abs{\set{ \tau\in[t_\ell,{\mathfrak t}_{\ell,i}]: i_{\tau}=i }}$, we additionally define $\tilde {\mathfrak t}_{\ell,i}$ as the first round when agent $i$ wins for the $\max(1,4M_{\ell,i}, 3N_{\ell,i})$-th time. If no such $t$ exists, we set $\tilde {\mathfrak t}_{\ell,i}=t_{\ell+1}-1$. The definition of $\tilde{\mathfrak t}_{\ell,i}$ is identical to that of $\mathfrak t_{\ell,i}$ except for the removal of \Cref{eq:constraint_F}.

If $\tilde {\mathfrak t}_{\ell,i}$ already satisfied the constraint from \Cref{eq:constraint_F}, then $\mathfrak t_{\ell,i}=\tilde{\mathfrak t}_{\ell,i}$ and we obtain directly
\begin{equation}\label{eq:case_1}
    A_{\ell,i}\leq \abs{\set{ \tau\in[t_\ell,\tilde {\mathfrak t}_{\ell,i}]: i_{\tau}=i }} = \max(1,4M_{\ell,i}, 3N_{\ell,i}).
\end{equation}
We suppose from now on that this is not the case. Applying \Cref{eq:upper_bound_nb_wins}, the upper bound on $\lvert \{t\in [t_\ell,\tau]\mid i_t=i\}\rvert$ for any $\tau\in E_\ell$, to the $\mathfrak t_{\ell,i}$ defined in \Cref{eq:constraint_F}, we have
\begin{equation}\label{eq:case_2a}
A_{\ell,i}\leq M_{\ell,i} +\sum_{\tau=t_\ell}^{{\mathfrak t}_{\ell,i}} F_{t,i}  \leq   M_{\ell,i} + 3q_{\ell,i}(\mathfrak t_{\ell,i}-t_\ell+1).
\end{equation}

Let $k_0$ be the integer such that $2^{k_0}\leq \mathfrak t_{\ell,i}-t_\ell+1 <2^{k_0+1}$. Since $\tilde {\mathfrak t}_{\ell,i}$ does not satisfy \Cref{eq:constraint_F}, we either have
\begin{equation}\label{eq:upper_bound_power_2}
    \frac{1}{ 2^{k_0+1}}\sum_{\tau=t_\ell}^{t_\ell + 2^{k_0+1}-1} F_{t,i} \geq \frac{1}{2(\tilde{\mathfrak t}_{\ell,i} -t_\ell + 1)}\sum_{\tau=t_\ell}^{\tilde{\mathfrak t}_{\ell,i}} F_{t,i} > \frac{3}{2}q_{\ell,i},
\end{equation}
if $t_\ell+2^{k_0+1}-1\in E_l$, or otherwise,
\begin{equation}\label{eq:lower_bound_power_2}
    \frac{1}{ 2^{k_0}}\sum_{\tau=t_\ell}^{t_\ell + 2^{k_0}-1} F_{t,i} < \frac{2}{\tilde{\mathfrak t}_{\ell,i} -t_\ell + 1}\sum_{\tau=t_\ell}^{\tilde{\mathfrak t}_{\ell,i}} F_{t,i} < \frac{2}{3}q_{\ell,i}.
\end{equation}

Now define $k_{\ell,i}$ as the smallest integer such that for all $k\geq k_{\ell,i}$, if $t_\ell+2^k-1\in E_\ell$ then
\begin{equation}\label{eq:def_k_il}
    \frac{1}{2^k}\sum_{\tau=t_\ell}^{t_\ell+2^k-1} F_{t,i} \in \sqb{\frac{2}{3}q_{\ell,i}, \frac{3}{2}q_{\ell,i} }.
\end{equation}
\Cref{eq:upper_bound_power_2,eq:lower_bound_power_2} show that $k_{\ell,i}>k_0$. Hence, furthering \Cref{eq:case_2a} gives $A_{\ell,i} \leq M_{\ell,i} + 6 q_{\ell,i} 2^{k_{\ell,i}}$.
Together with \Cref{eq:case_1} this shows that in all cases one has
\begin{equation}\label{eq:full_case}
    A_{\ell,i} \leq 1 + 4M_{\ell,i} + 3N_{\ell,i} + 6 q_{\ell,i} 2^{k_{\ell,i}}.
\end{equation}





We now bound $\E[2^{k_{\ell,i}}\mid\mH_{t_\ell}]$.
Note that conditionally on the history $\mH_{t_\ell}$ at the beginning of epoch $E_\ell$, the sequence of $F_{t,i}$ is an independent sequence of Bernoulli $\text{Ber}(q_{\ell,i})$. Hence, for any $k\geq 3+\log_2(1/q_{\ell,i})$, Chernoff's bound gives
\begin{equation*}
    \Pr\{k_{\ell,i} \geq k\} \leq \sum_{k'\geq k}\Pr\left \{\frac{1}{2^{k'}}\sum_{t=t_\ell}^{t_\ell+2^{k'}-1} F_{t,i} \notin \sqb{\frac{2}{3}q_{\ell,i},\frac{3}{2}q_{\ell,i}}\right \} \leq 2\sum_{k'\geq k}e^{-2^{k'}q_{\ell,i}/18} \leq 6e^{-2^{k}q_{\ell,i}/18}.
\end{equation*}

As a result,
\begin{equation*}
    \E[2^{k_{\ell,i}}\mid\mH_{t_\ell}] \leq \frac{8}{q_{\ell,i}} + \sum_{k\geq 3+\log_2(1/q_{\ell,i})}2^k \Pr \{k_{\ell,i} = k\} \leq \frac{98}{q_{\ell,i}}.
\end{equation*}

Plugging in the definitions of $M_{\ell,i}=1+\sum_{t\in E_\ell}\mathbbm{1}[i_t=i] (\bigvee_{j\in \mA_{t_\ell}\setminus \{i\}} D_{t,j})$ (from \Cref{eq:upper_bound_nb_wins}) and $N_{\ell,i}=1+\sum_{t\in E_\ell} D_{\ell,i}$ (from \Cref{eq:lower_bound_nb_wins}) into \Cref{eq:full_case} and observing that
\begin{equation}\label{eq:useful_inequality}
    \sum_{i\in \mA_{t_\ell}} M_{\ell,i} \leq K + \sum_{t\in E_\ell} \max_{j\in\mA_{t_\ell}} D_{t,j}\paren{ \sum_{i\in \mA_{t_\ell}} \1[i_{\tau}=i] } \leq K+\sum_{t\in E_\ell} \sum_{i\in \mA_{t_\ell}} D_{t,i},
\end{equation}
we get
\begin{equation*}
    \E\sqb{\sum_{i\in\mA_{t_\ell}} A_{\ell,i} \middle \vert \mH_{t_\ell}} \leq 600 K + 7 \E\sqb{\sum_{i\in \mA_{t_\ell}}\sum_{t\in E_\ell} D_{t,i} \middle \vert \mH_{t_\ell}},
\end{equation*}
which completes the proof of this claim.
\end{proof}

Together with the upper bound on $\E[\sum_{i\in \mA_{t_\ell}} \sum_{t\in E_\ell} D_{t,i}\mid \mH_{t_\ell}]$ from \Cref{lem:number of marking down bound formal}, we get
\begin{align*}
\E\sqb{\sum_{t=1}^T o_t \1[\hat q_{t,i}=0] }=\E\sqb{\sum_{\ell=1}^K \sum_{i\in\mA_{t_\ell}} A_{\ell,i} \middle \vert \mH_{t_\ell}}&\leq 600K^2 + 14\frac{K^3}{c}=\O\paren{\frac{K^3}{c}}.
\end{align*}

\paragraph{Incentive-Compatible Phase.} We now turn to the second phase. As before, we fix an epoch $\ell\in[K]$, a start time for the epoch $t_\ell\in[T]$ and a history $\mH_{t_\ell}$. We fix $i\in\mA_{t_\ell}$ and consider the incentive-compatible phase in which the estimation process already constructed an estimate $\hat q_{t,i}=\hat q_{\ell,i}\leq 4q_{t,i}$, $\forall t\in E_\ell$. For convenience, let us denote by ${\mathfrak t}_{\ell,i}^{ic}$ the start time of the incentive-compatible phase for $i$. During this phase, if agent $i$ wins, they are audited with probability $\hat p_{t,i}=\min(\frac{8K^2}{(T-t)\hat q_{\ell,i}c},1)$. Thus, the expected number of audits we spend on agent $i$ during their $\ell$-th incentive-compatible phase is (all expectations below are taken conditionally on $\mH_{t_{\ell,i}^{ic}}$)
\begin{align*}
B_{\ell,i}&:=\E\sqb{\sum_{t\in E_\ell}o_t \1[i_t=i] \1[\hat q_{t,i}>0] } = \E\sqb{\sum_{t\in E_\ell,t\geq {\mathfrak t}_{\ell,i}^{ic}}\1[i_t=i] \hat p_{i,t} \middle \vert \mH_{{\mathfrak t}_{\ell,i}^{ic}}}\\
&\leq \E\sqb{\sum_{t\in E_\ell,t\geq {\mathfrak t}_{\ell,i}^{ic}}(F_{t,i} + (1-F_{t,i})\1[i_t=i]) \hat p_{i,t} }\overset{(a)}{\leq} \E\sqb{\sum_{t\in E_\ell,t\geq {\mathfrak t}_{\ell,i}^{ic}}F_{t,i}\hat p_{i,t} } + \E[M_{\ell,i} ]\\
&\overset{(b)}{\leq } \frac{8K^2 }{c}\E\sqb{\sum_{t\in E_\ell,t\geq {\mathfrak t}_{\ell,i}^{ic}}\frac{q_{\ell,i}}{(T-t)\hat q_{\ell,i}} } + \E[M_{\ell,i} ],
\end{align*}
where \textit{(a)} uses the same arguments as in \Cref{eq:upper_bound_nb_wins} to bound by $M_{\ell,i}$ the number of times $i$ wins when they would not have won had all agents reported truthfully, and \textit{(b)} uses the fact that $F_{t,i}$ for $t\in E_\ell$ form an \textit{i.i.d.} sequence of $\text{Ber}(q_{\ell,i})$ as well as the definition of $\hat p_{t,i}$ in \Cref{line:check prob} of \Cref{alg:mechanism}. 

Since agent $i$ uses the flagging strategy ${\bm\pi}^{\ast,f}$, they ensure $\hat q_i\geq q_i/4$. 
Summing over all epochs $\ell\in[K]$ and $i\in\mA_{t_\ell}$ then gives
\begin{align*}
&\quad \E\sqb{\sum_{t=1}^T o_t \1[\hat q_{t,i}>0]}=\E\sqb{\sum_{\ell=1}^K B_{\ell,i}} \\
&\le \frac{32K^3}{c} \E\sqb{\sum_{t=1}^T \frac{1}{T-t} } + \E\sqb{\sum_{\ell\in[K]}\sum_{i\in\mA_{t_\ell}}M_{\ell,i}} \\
& =\mathcal O\paren{\frac{K^3}{c}\log T}.
\end{align*}
Here, we used the same arguments as in the estimation phase to bound the term summing the quantities $M_{\ell,i}$, that is \Cref{eq:useful_inequality} and \Cref{lem:number of marking down bound formal}.

Summing up the expected number of audits in the estimation and incentive-compatible phases completes the proof.
\end{proof}
