\section{Lower Bounds on Regret and Number of Audits}\label{sec:lower bounds}

In this section, we prove the lower bounds from \Cref{thm:lower_bounds}. We start by proving the lower bound $\Omega(K)$ on the regret, then prove the lower bound $\Omega(1)$ on the number of audits for having low regret.

\begin{theorem}[Lower Bound on Regret]\label{thm:lower_bound_regret}
Let $K\geq 2$ and $T\geq K$. Then, there exists $K$ utility distributions $\mV_1,\mV_2,\ldots,\mV_K$ over $[0,1]$, such that for any central planner mechanism $\bm M$ and corresponding agents' joint strategy PBE $\bm \pi=(\pi_{t,i})_{t\in [T],i\in[K]}$, we must have
\begin{equation*}
\mathcal R_T(\bm \pi,\bm M) \geq \frac{K-1}{64}=\Omega(K).
\end{equation*}
\end{theorem}
\begin{proof}
    We consider the following distributions: $\mV_1 = \delta_{2/3}$, that is, $u_1=\frac{2}{3}$ almost surely, and for all $i\geq 2$, we have $\mV_i \sim \frac{1}{3} + \frac{2}{3}\text{Ber}(\frac{\alpha}{T})$ where $\alpha=\frac{1}{8}$, that is, $u_i=1$ with probability $\alpha/T$ and $u_i=\frac{1}{3}$ otherwise.
    Now fix any PBE $\bm\pi$ corresponding to these utility distributions. We fix an agent $i\geq 2$ and aim to show that
    \begin{equation}\label{eq:bound_error_i}
        R_i:=\E_{\bm\pi}\sqb{\sum_{t=1}^T \1[u_{t,i}=1]\1[i_t\neq i] + \1[u_{t,i}=1/3]\1[i_t=i]} \geq \frac{\alpha}{2}.
    \end{equation}
    
    Suppose by contradiction that \Cref{eq:bound_error_i} does not hold. We can then bound the expected gain of agent $i$ as follows:
    \begin{equation}\label{eq:upper_bound_gain_i}
        V_i^{\bm\pi}(\mH_1) \leq \E\sqb{\sum_{t=1}^T \frac{1}{3}\1[u_{t,i}=1/3]\1[i_t=i] + \1[u_{t,i}=1]} \leq \alpha+\frac{R_i}{3}\leq \frac{7\alpha}{6}.
    \end{equation}
    On the other hand, we can also bound the expected gain of agent $i$ conditional on the event $\mE_i\colon \exists t\in[T]\text{ s.t. } u_{t,i}=1$ which excludes the case when agent $i$ always has utility $1/3$.
    \begin{align*}
        \E\sqb{\paren{\sum_{t=1}^T u_{t,i}\1[i_t=i]}\1[\mE_i]} &\geq \E\sqb{\paren{\sum_{t=1}^T \1[u_{t,i}=1]}\1[\mE_i]}- R_i\\
        &= \E\sqb{\sum_{t=1}^T \1[u_{t,i}=1]}- R_i =\alpha-R_i \geq \frac{\alpha}{2}.
    \end{align*}
    As a result,
    \begin{equation*}
        \E\sqb{\sum_{t=1}^T u_{t,i}\1[i_t=i] \middle \vert \mE_i} \geq \frac{\alpha}{2\Pr\{\mE_i\}} = \frac{\alpha}{2(1-(1-\alpha/T)^T)}\geq \frac{1}{2}.
    \end{equation*}
    We now define a new strategy $\tilde{\bm \pi}_i$ for agent $i$ which proceeds as follows: sample a "fake" sequence $\tilde{\bm u}_i=(\tilde u_{t,i})_{t\in[T]}$ from the distribution $\mV_i^{\otimes T}\mid\mE_i$, then run strategy $\pi_i$ throughout the whole allocation process by bidding as if the true utility of agent $i$ as $\tilde{\bm u}$. Then,
    \begin{align*}
        V_i^{(\bm\pi_{-i},\tilde\pi_i)}(\mH_1) &\geq \frac{1}{3}\Pr_{(\bm\pi_{-i},\tilde\pi_i)}\{\exists t\in[T]\text{ s.t. }i_t=i\} \geq \frac{1}{3} \E_{\bm \pi}\sqb{\sum_{t=1}^T u_{t,i}\1[i_t=i] \middle \vert \mE_i} \geq \frac{1}{6} > V_i^{\bm\pi}(\mH_1),
    \end{align*}
    where in the last inequality we used \Cref{eq:upper_bound_gain_i} and the fact that $\alpha<1/7$.
    This contradicts the fact that $\bm\pi$ is a PBE. In summary, we proved that \Cref{eq:bound_error_i} holds for all $i\geq 2$.

    We can then bound the regret of the mechanism at the PBE $\bm\pi$. First, note that any time $t$ for which $u_{t,i_t}=1/3$ induces a regret $1/3$ since agent $1$ always had utility $1$. Second, if at time $t$, there is a unique agent $i\geq 2$ which has utility $u_{t,i}=1$, the mechanism also incurs a regret at least $1/3$ if it does not allocate to agent $i$, that is if $i_t\neq i$. Formally,
    \begin{align*}
        \mathcal R_T(\bm \pi,\bm M) &\geq \E_{\bm u,\bm\pi}\sqb{\sum_{t=1}^T \sum_{i=2}^K\frac{1}{3}\paren{\1[u_{t,i}=1/3]\1[i_t=i]  + \1[u_{t,i}=1]\1[i_t\neq i]\1[u_{j,t}=1/3,\forall j\notin\{1,i\}]} }\\
        &\geq \frac{1}{3}\sum_{i=2}^K R_i - \frac{1}{3}\E\sqb{\sum_{t=1}^T \paren{\sum_{i=2}^K\1[u_{t,i}=1]}\1\sqb{\sum_{i=2}^K\1[u_{t,i}=1] >1}}\\
        &\geq \frac{(K-1)\alpha}{6} - \frac{T}{3}\paren{\frac{K\alpha}{T}-K\frac{\alpha}{T}\paren{1-\frac{\alpha}{T}}^{K-1}}\\
        &\geq \frac{(K-1)\alpha}{6}\paren{1-2\alpha},
    \end{align*}
    where in the last inequality, we used the fact that $1-(1-\alpha/T)^{K-1}\leq 1- (1-\alpha/T)^{T-1} \leq 1-e^{-\alpha}\leq \alpha$. Plugging in the utility $\alpha=1/8$ gives the desired result.
\end{proof}

\begin{theorem}[Lower Bound on Tradeoff Between Regret and Number of Audits]\label{thm:lower_bound_nb_checks}
Let $K\geq 2$ and $T\geq K$. Then, there exists $K$ utility distributions $\mV_1,\mV_2,\ldots,\mV_K$ over $[0,1]$, such that for any central planner mechanism $\bm M$ and corresponding agents' joint strategy PBE $\bm \pi=(\pi_{t,i})_{t\in [T],i\in[K]}$,
\begin{equation*}
    \mathcal B_T(\bm \pi,\bm M)\leq c_1\Longrightarrow \mathcal R_T(\bm \pi,\bm M)\ge c_2\sqrt {\frac{T}{\log T}},
\end{equation*}
where $c_1,c_2>0$ are some universal constants.
\end{theorem}
\begin{proof}
    We consider similar distributions as in the proof of \Cref{thm:lower_bound_regret}, but using only the first two agents. Precisely, we set $\mV_1=\delta_{2/3}$,  $\mV_2\sim\delta_{\frac{1}{3}} + \frac{2}{3}\text{Ber}(p)$, where $p=\frac{1}{3}$, and $\mV_i=\delta_{1/4}$ for $i\geq 3$. That is, $u_1=\frac{2}{3}$ always, $u_2$ equals $\frac{1}{3}$ or $1$ with equal probability $\frac{1}{2}$, and $u_i=\frac{1}{4}$ for all $i\geq 3$ (so their fair shares are all 0 as $u_i<u_1$ and $u_i<u_2$ \textit{a.s.}). By the revelation principle, we assume without loss of generality that under this specific utility distributions setup $\{\mV_i\}_{i\in [K]}$ and mechanism $\bm M$ truthful reporting $\textbf{truth}$ is the considered PBE. 

    By design, the central planner incurs a constant regret $\frac 13$ whenever they do not allocate the resource to agent $2$ if they had utility $1$. Also, because agent $1$ always has utility $\frac 23$, we have
    \begin{equation}\label{eq:lower_bound_regret}
        \mathcal R_T(\textbf{truth},\bm M) = \frac{1}{3}\E\sqb{\sum_{t=1}^T \1[u_{t,2}=1]\1[i_t\neq 2] + \1\sqb{u_{t,2}=\frac{1}{3} }\1[i_t\neq 1]}.
    \end{equation}

    From now, we focus on the agent $i=2$. For simplicity, we abbreviate the expectation
    \begin{equation*}
    \E_{\text{generate \textit{i.i.d.} samples }\bm u_i=(u_{t,i})_{t\in [T]}\text{ from }\mV_i} \left [\E_{\text{agents follow joint strategy }\bm \pi}\left [X\middle \vert \text{agent $i$ has true utilities }f(\bm u_i)\right ]\right ]
    \end{equation*}
    as $\E_{f(\bm u_i),\bm \pi}[X]$. We similarly define $\Pr_{f(\bm u_i),\bm \pi}[X]$. For example, if agent $i$'s true utilities are \textit{i.i.d.} samples from $\mV_i$ and all agents follow $\textbf{truth}$, then we write $\E_{\bm u_i,\textbf{truth}}$ as the expectation operator.
    We start by upper bounding the gain of agent $i$ under truthful reporting as follows.
    \begin{align}
        V_i^{\textbf{truth}}(\mH_1;\bm M) &\leq \E_{\bm u_i,\textbf{truth}} \sqb{\sum_{t=1}^T \1[u_{t,i}=1] + \frac{1}{3} \1\sqb{u_{t,i}=\frac{1}{3}}\1[i_t=i] } \notag\\
        &= Tp + \frac{1}{3} \E_{\bm u_i,\textbf{truth}} \sqb{\sum_{t=1}^T  \1\sqb{u_{t,i}=\frac{1}{3}}\1[i_t=i] }. \label{eq:upper_bound_truth}
    \end{align}
    
    We next construct an alternative strategy $\bm\pi_i$ for agent $i$ as follows.     
    In round $t\in[T]$, after observing their utility $u_{t,i}$,
    \begin{itemize}
    \item if $u_{t,i}=1$, then report $v_{t,i}=1$ truthfully as in \textbf{truth},
    \item otherwise, \textit{w.p.} $\epsilon=\frac{1}{\sqrt{T\log T}}$ independently report $v_{t,i}=1$ (\textit{i.e.}, pretend that they had true utility $u_{t,i}=1$), and otherwise report $v_{t,i}=\frac 13$ as in \textbf{truth}. In particular, for $T$ sufficiently large, we have $p+\epsilon\leq \frac{1}{2}$.
    \end{itemize}
    As a side note, the resource allocation setup defined in \Cref{sec:setup} allows the central planner to ask further information from the agents than only their utility report (see Step~\ref{item:additional_questions}). To these questions at round $t\in[T]$, $\bm\pi$ answers identically as what the PBE strategy $\textbf{truth}$ would have replied had their true utility sequence been the reported $(v_{\tau, i})_{\tau\leq t}$, and for the same public history. 
    In summary, the strategy $\bm\pi_i$ corresponds to using the PBE strategy $\textbf{truth}$ for the biaised utilities $(v_{t,i})_{t\in[T]}$. Note that by construction, these form an i.i.d. sequence of distribution $\tilde {\mathcal U}_i\sim\frac{1}{3} + \frac{2}{3}\text{Ber}(p+\alpha\sqrt p)$. 
    
    Importantly, the allocation process is identical under the following two scenarios:
    \begin{enumerate}
        \item agent $i$ had true utilities $\bm u_i=(u_{t,i})_{t\in [T]}$ but used strategy $\bm\pi_i$ to report $\bm v_i=(v_{t,i})_{t\in [T]}$, while all other agents follow $\textbf{truth}_{-i}$ (this corresponds to expectation operator $\E_{\bm u_i,\bm \pi_i\circ \textbf{truth}_{-i}}$), and
        \item agent $i$ had true utilities as the aforementioned $\bm v_i=(v_{t,i})_{t\in [T]}$ and used the PBE strategy $\textbf{truth}_i$, while all other agents still follow $\textbf{truth}_{-i}$ (we denote this case by $\E_{\bm v_i,\textbf{truth}}$),
    \end{enumerate}
    unless the central planner audits $i$ in a round $t$ where $v_{t,i}\neq u_{t,i}$ in the first scenario. Formally, for round $t\in[T]$, we denote by $F_{t,i}=\mathbbm{1}[u_{t,i}=\frac 13,v_{t,i}=1]$ the indicator that agent $i$ flips their report from $\frac 13$ to $1$ under strategy $\pi_{t,i}$. The two scenarios collide if $F_{t,i} \1[i_t=i] o_t=0$ for all $t\in[T]$.
    
    Putting everything together, we obtain
    \begin{align}
        V_i^{\bm\pi_i\circ \textbf{truth}_{-i}}(\mH_1;\bm M) &= \E_{\bm u_i,\bm \pi_i\circ \textbf{truth}_{-i}} \sqb{\sum_{t=1}^T u_{t,i} \mathbbm{1}[i_t=i] } \notag \\
        &\geq \E_{\bm v_i,\textbf{truth}} \sqb{ \1\left [\bigwedge_{t\in [T]} F_{t,i}\1[i_t=i]o_t=0\right ]\sum_{t=1}^T u_{t,i} \mathbbm{1}[i_t=i] } \notag  \\
        &\geq \E_{\bm v_i,\textbf{truth}} \sqb{\sum_{t=1}^T u_{t,i} \mathbbm{1}[i_t=i] } - T\E_{\bm v_i,\textbf{truth}}\sqb{\sum_{t=1}^T F_{t,i}\1[i_t=i]o_t}. \label{eq:bound_V_i_deviation_1}
    \end{align}

    We further bound each term of the last inequality as follows:
    \begin{align}
        \E_{\bm v_i,\textbf{truth}} \sqb{\sum_{t=1}^T u_{t,i} \mathbbm{1}[i_t=i] } &\geq \E_{\bm v_i,\textbf{truth}} \sqb{\sum_{t=1}^T u_{t,i} \mathbbm{1}[v_{t,i}=1] - \mathbbm{1}[v_{t,i}=1]\1[i_t\neq i] } \notag \\
        &=Tp +\frac{\epsilon T}{3}  - \E_{\bm v_i,\textbf{truth}} \sqb{\sum_{t=1}^T  \mathbbm{1}[v_{t,i}=1]\1[i_t\neq i] }. \label{eq:bound_V_i_deviation_2}
    \end{align}
    To bound the second term, note that in a round $t\in[T]$ in which the central planner allocated the resource to $i_t=i$, when the central planner makes the decision to audit agent $i$ or not, they only have access to the past history and the reports at round $t$, and $i_t$. Conditioning on these, the value of $u_{t,i}$ is in fact only dependent on the report $v_{t,i}$ as
    \begin{equation*}
        \Pr_{\bm v_i,\textbf{truth}}\set{ u_{t,i}=\frac{1}{3} \middle \vert \mH_t,\bm v_t, i_t, o_t} = \Pr\set{ u_{t,i}=\frac{1}{3} \middle \vert  v_{t,i}} = \begin{cases}
            1 & v_{t,i}=\frac{1}{3},\\
            \frac{\epsilon}{p}  & v_{t,i}=1.
        \end{cases}
    \end{equation*}
    In the last inequality, we used the construction of the variable $v_{t,i}$. 
    As a result, 
    \begin{align}
         \E_{\bm v_i,\textbf{truth}}\sqb{\sum_{t=1}^T F_{t,i}\1[i_t=i]o_t} &= \E_{\bm v_i,\textbf{truth}}\sqb{\sum_{t=1}^T \1[v_{t,i}=1,i_t=i]o_t \1\sqb{u_{t,i}=\frac 13}} \notag \\
         &= \frac{\epsilon}{p} \E_{\bm v_i,\textbf{truth}}\sqb{\sum_{t=1}^T \1[v_{t,i}=1,i_t=i]o_t}. \label{eq:bounding_nb_checks_for_catch}
    \end{align}
    
    In the remaining, we compare the expectation operators $\E_{\bm u_i,\textbf{truth}}$ and $\E_{\bm v_i,\textbf{truth}}$ which only differ in the fact that the true utility of agent $i$ was sampled i.i.d. following $\mathcal U_i$ and $\tilde{\mathcal U}_i$ respectively. That is,
    \begin{align}
    \E_{\bm u_i,\textbf{truth}}[X] &= \sum_{\bm z\in\set{\frac{1}{3},1}^T} \Pr_{\bm u\sim {\color{blue}{\mathcal U}_i^{\otimes T}}} \set{\bm u = \bm z} \E_{\textbf{truth}}[X\mid \text{agent $i$ has true utility }\bm z], \nonumber \\
    \E_{\bm v_i,\textbf{truth}}[X] &= \sum_{\bm z\in\set{\frac{1}{3},1}^T} \Pr_{\bm u\sim {\color{blue}\tilde{\mathcal U}_i^{\otimes T}}} \set{\bm u = \bm z} \E_{\textbf{truth}}[X\mid \text{agent $i$ has true utility }\bm z].\label{eq:comparing_expectations}
    \end{align}
    For any fixed $\bm z \in \set{\frac 13,1}^T$, denoting by $k_{\bm z}$ the number of components equal to $1$ within $\bm z$, we have
    
     \begin{align*}
         \log \frac{\Pr_{\bm u\sim \tilde{\mathcal U}_i^{\otimes T}} \set{\bm u = \bm z} }{\Pr_{\bm u\sim {\mathcal U}_i^{\otimes T}} \set{\bm u = \bm z} } &= k_{\bm z} \log\paren{\frac{p+\epsilon}{p}} +(T-k_{\bm z})\log \paren{\frac{1-p}{1-p-\epsilon}}\\
         &= T d_{KL}(p+\epsilon\parallel p) + (k_{\bm z}-T(p+\epsilon)) \log\paren{\frac{(p+\epsilon)(1-p)}{p(1-p-\epsilon)}}\\
         &\overset{(i)}{\leq} 1 + (k_{\bm z}-T(p+\epsilon)) \frac{\epsilon}{p(1-p-\epsilon)} \overset{(ii)}{\leq} 1 + \frac{2\epsilon}{p}   (k_{\bm z}-T(p+\epsilon)),
     \end{align*}
     where $d_{KL}(a\parallel b):=a\ln\frac{a}{b} + (1-a)\ln\frac{1-a}{1-b}$ is the binary KL divergence. In $(i)$ we used the fact that because $p+\epsilon\leq \frac{1}{2}$, one has $d_{KL}(p+\epsilon \parallel p)\leq \frac{\epsilon^2}{p}$ \citep[Lemma 16]{blanchard2024tight} as well as $\log(1+x)\leq x$ for $x>-1$. In $(ii)$ we used $p+\epsilon\leq \frac{1}{2}$. Next, by the Hoeffding bound, we have,
     \begin{align}
        \Pr_{\bm z\sim \tilde{\mathcal U}_i^{\otimes T}}\set{ \frac{\Pr_{\bm u\sim \tilde{\mathcal U}_i^{\otimes T}} \set{\bm u = \bm z} }{\Pr_{\bm u\sim {\mathcal U}_i^{\otimes T}} \set{\bm u = \bm z} } \leq e^{5}} &\geq \Pr_{\bm z\sim \tilde{\mathcal U}_i^{\otimes T}}\set{ k_{\bm z} \leq T(p+\epsilon) + \frac{2p}{\epsilon} } \notag \\
        &\geq 1-\exp \paren{-\frac{8 p^2 T}{\epsilon^2}} = 1-\frac{1}{T^2}.\label{eq:large_proba_similar_likelihood}
    \end{align}
    
    We denote by $\mathcal E_i$ the corresponding event. Plugging our estimates in \Cref{eq:comparing_expectations} then gives (which is the relationship between expectation operators)
    \begin{equation}\label{eq:complete_comparison_expectations}
        \E_{\bm v_i,\textbf{truth}} \leq e^{5} \E_{\bm u_i,\textbf{truth}} + \E_{\bm v_i,\textbf{truth}}\1[\neg \mathcal E_i].
    \end{equation}
    Using this identity twice within \Cref{eq:bound_V_i_deviation_2,eq:bounding_nb_checks_for_catch} then plugging the results within \Cref{eq:bound_V_i_deviation_1} yields
    \begin{align*}
        V_i^{\bm\pi_i\circ \textbf{truth}_{-i}}(\mH_1;\bm M) &\geq Tp + \frac{\epsilon T}{3} -\paren{\frac{\epsilon T^2}{p}+T}\Pr\set{\neg \mathcal E_i}- \\
        &\quad e^{5} \paren{ \frac{\epsilon T}{p} \E_{\bm u_i,\textbf{truth}}\sqb{\sum_{t=1}^T \1[v_{t,i}=1,i_t=i]o_t} +  \E_{\bm u_i,\textbf{truth}} \sqb{\sum_{t=1}^T  \mathbbm{1}[v_{t,i}=1]\1[i_t\neq i] }}.
    \end{align*}
    
    Next, by assumption \textbf{truth} is a PBE. Therefore, we must have $V_i^{\bm\pi_i\circ \textbf{truth}_{-i}}(\mH_1;\bm M) \leq V_i^{\textbf{truth}}(\mH_1;\bm M)$. Together with the previous lower bound for $V_i^{\bm\pi_i\circ \textbf{truth}_{-i}}(\mH_1;\bm M)$ and the upper bound on $V_i^{\textbf{truth}}(\mH_1;\bm M)$ from \Cref{eq:upper_bound_truth}, we obtain
    \begin{align*}
        &0\leq  V_i^{\textbf{truth}}(\mH_1;\bm M) - V_i^{\bm\pi_i\circ \textbf{truth}_{-i}}(\mH_1;\bm M) \\
        &\overset{(i)}{\leq} e^{5} \paren{ \frac{\epsilon T}{p} \E_{\bm u_i,\textbf{truth}}\sqb{\sum_{t=1}^T \1[v_{t,i}=1,i_t=i]o_t} +  \E_{\bm u_i,\textbf{truth}} \sqb{\sum_{t=1}^T \1\sqb{u_{t,i}=\frac{1}{3}}\1[i_t=i] + \mathbbm{1}[v_{t,i}=1]\1[i_t\neq i]  }} +\\
        &\qquad \frac{\epsilon}{p}+\frac{1}{T}  - \frac{\epsilon T}{3}\\
        &\leq e^5\paren{\frac{\epsilon T}{p}\mathcal B_T(\textbf{truth},\bm M) + 3\mathcal R_T(\textbf{truth},\bm M)} + 2 - \frac{\epsilon T}{3}.
    \end{align*}
    In $(i)$, we also used \Cref{eq:large_proba_similar_likelihood}. In $(ii)$, we used \Cref{eq:lower_bound_regret} and the definition of $\mathcal B_T(\textbf{truth},\bm M)$.

    As a result, if we have $B_T(\textbf{truth},\bm M) \leq \frac{p}{6e^5}$, the previous inequality implies
    \begin{equation*}
        \mathcal R_T(\textbf{truth},\bm M) \geq \frac{\epsilon T}{3}-2\geq \frac{\epsilon T}{4} = \frac{1}{4}\sqrt{\frac{T}{\log T}},
    \end{equation*}
    for $T$ sufficiently large. This ends the proof.
\end{proof}
