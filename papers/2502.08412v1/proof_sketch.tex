\section{Analysis Outline of \mechname}\label{sec:sketch varying-p}
This section highlights the main steps of the analysis of \mechname. All formal claims and corresponding proof can be found in \Cref{sec:appendix varying-p,sec:appendix varying-p-audits}.

\subsection{Part I: Characterization of the PBE via an Auxiliary Game}
As described in \Cref{sec:technical overview}, the first step is to characterize the PBE of agents' strategies under \mechname by reducing the problem to an \textit{auxiliary game} and showing that a PBE therein yields a PBE in the original game. We define:
\begin{definition}[Auxiliary Game]\label{def:auxiliary game}
    Initially, $\mA_1=[K]$. In round $t\in [T]$, each alive agent $i\in \mA_t$ reports a $v_{t,i}\in [0,1]$ that depends only on its private utility $u_{t,i}$, round number $t$, and set of alive agents $\mA_t$. Further, if $p_{t,i}:= \frac{2K^2}{(T-t)q_{t,i} c} \leq 1$, they are not allowed to mark up, that is $v_{t,i}\le u_{t,i}$ must hold. The mechanism allocates the resource to the alive agent with highest report $i_t=\argmax_{i\in \mA_t} v_{t,i}$. If $v_{t,i_t}>u_{t,i_t}$, they are eliminated: $\mA_{t+1}=\mA_t\setminus \{i_t\}$. Otherwise, $\mA_{t+1}=\mA_t$.

    The $q_{t,i}$ in the definition of $p_{t,i}$ is defined as follows: For a set of alive agents $\mA\subseteq [K]$, the fair winning probability for any alive $i\in \mA$ is $q_i(\mA) = \Pr_{\bm u\sim\mV}\{u_{i}\ge c\wedge (u_{i}>u_{j},\forall j\in \mA\setminus \{i\})\}$. The fair winning probability for agent $i\in [K]$ in any round $t\in [T]$ is $q_{t,i}:=q_i(\mA_t) \mathbbm{1}[i\in \mA_t]$.
\end{definition}
We defer to \Cref{sec:appendix auxiliary game} the mechanism's pseudo-code and the formal definition of agents' strategies (in analog to \Cref{def:agent_strat_mechanism}). Compared to the game induced by \mechname, the auxiliary game does not involve any estimation or flagging and also have the following restrictions:
\begin{enumerate}
\item Reports only depend on $(t,\mA_t)$, while \Cref{def:agent_strat_mechanism} allows much more dependencies. \label{item:only simplified history}
\item Agents never mark up unless $p_{t,i}>1$, while \Cref{def:agent_strat_mechanism} does not impose similar restrictions. \label{item:no mark up unless p>=1}
\item Winners who mark up are eliminated immediately. In comparison, \mechname eliminates such agents with probability $\hat p_{t,i}\le 1$; see \Cref{line:randomly_check} of \Cref{alg:mechanism}.\label{item:eliminate immediately}
\end{enumerate}
We emphasize that the auxiliary game above is only for proof purposes -- in the actual resource allocation process, we do not impose \Cref{item:only simplified history,item:no mark up unless p>=1,item:eliminate immediately} to the agents. The main point is that provided agents adopt a \textit{well-behaved} flagging strategy, a PBE in the auxiliary game can be translated into a PBE for the original game. The well-behaved flagging strategy is defined as follows.
\begin{definition}[Well-Behaved Flagging Strategy]\label{def:well-behaved flagging}
The \textit{well-behaved flagging strategy} $\bm \pi^{\ast,f}=(\pi_{t,i}^{\ast,f})_{t\in [T],i\in [K]}$ is defined such that for any agent $i\in[K]$ and round $t\in[T]$, $\pi_{t,i}^{\ast,f}$ answers $f_{t,i}=1$ if and only if either \textit{i)} $i_t\neq i$ and $\hat q_{t+1,i_t}>4q_{t+1,i_t}$, or \textit{ii)} $i_t=i$ and $\hat q_{t+1,i_t}<q_{t+1,i_t}/4$.
\end{definition}

Intuitively, when agents follows this well-behaved flagging strategy, they together ensure that all generated estimations of the fair allocation probabilities $(q_{t,i})_{i\in \mA_t}$ is accurate up to a multiplicative factor of $4$.
Our main characterization of a PBE under \mechname is the following.
\begin{theorem}[PBE in Auxiliary Game is PBE in Original Game]\label{thm:equivalence of PBE}
Let ${\bm \pi}^{\ast,r}$ be a PBE in the auxiliary game from \Cref{def:auxiliary game} and $\bm \pi^{\ast,f}$ be defined in \Cref{def:well-behaved flagging}. Then, $\bm \pi^\ast=({\bm \pi}^{\ast,r},{\bm \pi}^{\ast,f})$ is a PBE for agents' joint strategy under \mechname for the original repeated resource allocation problem.
\end{theorem}

Now fix a PBE $\bm\pi^{\ast,r}$ in the auxiliary game.
To prove \Cref{thm:equivalence of PBE}, we show that any unilateral deviation from $\bm \pi^\ast=({\bm \pi}^{\ast,r},{\bm \pi}^{\ast,f})$ of an agent $i\in[K]$ to another reporting and flagging strategy (which violates \Cref{item:only simplified history,item:no mark up unless p>=1,item:eliminate immediately}) does not improve their gain. In a high-level, unilaterally deviating their flagging strategy does not change the fact that the fair probability allocations are accurate: other agents still follow the well-behaved flagging strategy, which is sufficient for a proper estimation. Details on this argument are deferred to the proof of \Cref{thm:equivalence of PBE} in \Cref{sec:equivalence of PBE formal}. We now only consider unilateral deviations for the reporting strategy, which goes in three steps:

\paragraph{Step 1. Correspondence between Games.} We first show that every strategy in the auxiliary game corresponds to another in the original game under which agents' utility profiles are the identical.
\begin{lemma}[Correspondence between Auxiliary and Actual Games; Informal \Cref{lem:equiv between actual and no-flagging formal}]\label{lem:equiv between actual and no-flagging}
Fix any joint strategy $\bm \pi^r$ in the auxiliary game. For any agent $i\in [K]$, round $t\in[T]$, and history $\mH_t$, the expected utility of $i$ under joint strategy $\bm \pi:=(\bm\pi^r,\bm\pi^{\ast,f})$ in the original game starting from $\mH_t$ (that is, the V-function $V_i^{\bm \pi}(\mH_t)$) is the same as that under $\bm \pi^r$ in the auxiliary game.
\end{lemma}
Essentially, this shows that under the well-behaved flagging strategy $\bm\pi^{\ast,f}$, \mechname has the same elimination policy as \Cref{item:eliminate immediately}. Indeed, agents only mark up if $p_{t,i}$ from \Cref{item:no mark up unless p>=1}, and since $\bm\pi^{\ast,f}$ ensures that $q_{t,i}$ was properly estimated we can show that the audit probability was $\hat p_{t,i}=1$.

\paragraph{Step 2. Bounding agent utilities under $\bm\pi^\ast:=(\bm\pi^{\ast,r},\bm\pi^{\ast,f})$.}
We then focus on the auxiliary game and lower bound the utility obtained by each agent under the PBE $\bm\pi^{\ast,r}$: we argue that they would have obtained large utility by reporting truthfully since in the auxiliary game, other agents can lie at most once. Using the correspondence from \Cref{lem:equiv between actual and no-flagging}, we obtain the following.
\begin{lemma}[Upper and Lower Bounds when Following $\bm \pi^\ast$; Informal \Cref{lem:u lower and upper bound formal}]\label{lem:u lower and upper bound}
Under $\bm \pi^\ast$,
\begin{align}
(T-t+1) \mu_{t,i}-K &\leq V_i^{{\bm \pi}^\ast}(\mH_t)\leq (T-t+1) \mu_{t,i}+K^2,&&\quad \forall t\in [T],\text{history }\mH_t, i\in \mA_t,\label{eq:for alive}\\
(T-t+1) \mu_{1,i}-K &\leq V_i^{{\bm \pi}^\ast}(\mH_t)=0,&&\quad \forall t\in [T],\text{history }\mH_t,i\notin \mA_t. \label{eq:for dead}
\end{align}
\end{lemma}
In particular, this lower bounds the social welfare achieved under $\mechname$ for the strategy $\bm\pi^\ast$.

\paragraph{Step 3. Marking up is not beneficial under $\bm\pi^\ast$.}
Another important consequence of \Cref{lem:u lower and upper bound} is promising each agent $i\in\mA_t$ with a large future gain $V_i^{\bm \pi^\ast}(\mH_{t+1})\ge (T-t)\mu_{t+1,i}-K$ provided they remain alive. As discussed in \Cref{sec:description of check prob}, because of the audit probability $\hat p_{t,i}$ in \mechname, the threat of potential elimination is hence sufficient to prevent agent $i$ from marking up.
\begin{lemma}[Marking Up is Worse than Honesty; Informal \Cref{lem:min report with u is good formal}]\label{lem:min report with u is good}
Fix any round $t\in [T]$, history $\mH_t$, and agent $i\in \mA_t$. Their unilateral deviation at round $t$ from $\pi^{\ast,r}_{t,i}$ to an arbitrary $\pi^r_{t,i}$ that complies with \Cref{item:only simplified history} but not \Cref{item:no mark up unless p>=1} is no better than the following reporting strategy
\begin{equation*}
\pi_{t,i}^{r,\prime}(u_{t,i};\tilde \mH_t)=\min(v_{t,i},u_{t,i}),\quad \text{where }v_{t,i}\sim \pi^r_{t,i}(u_{t,i};\tilde \mH_t).
\end{equation*}
\end{lemma}
This result together with the performance difference lemma \citep{kakade2002approximately} implies any unilateral deviation of agent $i$ from the reporting strategy $\bm\pi^\ast$ has no better utility than a reporting strategy complying with \Cref{item:only simplified history,item:no mark up unless p>=1}. \Cref{item:only simplified history} appears without loss of generality, because $\bm \pi^{\ast,r}$, a valid joint strategy in the auxiliary game \Cref{def:auxiliary game}, ensures that other agents' actions only depend on $(t,\mA_t)$ in round $t$. The same applies to the mechanism \mechname.
From the correspondence result, \Cref{lem:equiv between actual and no-flagging}, \Cref{item:eliminate immediately} is also consistent with the execution of \mechname under the joint strategy $\bm\pi^\ast$.
Therefore, $\bm \pi^\ast$ is a PBE in the actual game given that $\bm \pi^{r,\ast}$ is a PBE in the auxiliary game, proving \Cref{thm:equivalence of PBE}.

\subsection{Part II: Regret and Expected Number of Audits Guarantees}
Given the PBE result, we are ready to derive the performance guarantees of \mechname stated in \Cref{thm:deterministic main thm}.
As discussed in Step 2 of Part I, \Cref{lem:u lower and upper bound} already provides the desired expected regret guarantee: Indeed, the social welfare achieved by \mechname under $\bm\pi^\ast$ satisfies $\sum_{i\in[K]} V_i^{\bm\pi^\ast} (\mH_1) \geq T\sum_{i\in[K]}\mu_{1,i} - K^2$, where the term $T\sum_{i\in[K]}\mu_{1,i}$ is precisely the maximum expected social welfare that any mechanism may attain.

We next turn to bounding the expected number of audits, which requires additional care. For each agent $i\in[K]$, \mechname proceeds alternatively between the \textit{estimation phase} where $\hat q_{t,i}=0$ (\textit{i.e.}, the mechanism is estimating the fair winning probability $q_{t,i}$; during this phase, agent $i$ is audited with probability $1$ whenever they win), and the \textit{incentive-compatible} phase where the mechanism sticks to a $\hat q_{t,i}>0$ where no agent answered $f_{t,i}=1$ (during this phase, agent $i$ is audited with probability $\hat p_{t,i}\leq 1$ as defined in \Cref{line:check prob} of \Cref{alg:mechanism}). We control them differently.

The incentive-compatible phases are relatively straightforward, because agents under the PBE $\bm \pi^\ast$ always use the well-behaved flagging strategy (recall \Cref{def:well-behaved flagging}) and thus the $\hat q_{t,i}$ used to construct $\hat p_{t,i}$ therein are always approximately their true value $q_{t,i}$. Hence, we can say $\hat p_{t,i}\lesssim \frac{\poly(K)}{(T-t)\mu_{t,i}}$ during this phase. Similar arguments as for the ideal mechanism $\bm M^\ast$ in \Cref{sec:description of check prob} then control the expected number of audits during all incentive-compatible phases by $\O(\poly(K) \log T)$.

For the estimation phases, while $\bm\pi^\ast$ prohibits agents to mark up during this phase (\Cref{item:eliminate immediately} from the auxiliary game \Cref{def:auxiliary game}), it does allow agents to voluntarily mark down, that is, report a value $v_{t,i}<u_{t,i}$. This could in turn bias the estimates $(\hat q_{t,i})_{i\in[K]}$ for the fair allocation probabilities, because agents lose some rounds that are ``fair wins'' for them.
Fortunately, by carefully analyzing agents' incentives, we give the following result that says such events only happen rarely:
\begin{lemma}[Number of Mark-Downs; Informal \Cref{lem:number of marking down bound formal}]\label{lem:number of marking down bound}
For round $t\in [T]$ and agent $i\in [K]$, let indicator $D_{t,i}$ be the event that they are alive, do not win, but would have won had they reported honestly, i.e.,
$D_{t,i}=\mathbbm{1}[i\in \mA_t]\mathbbm{1}[i_t\ne i]\mathbbm{1}[u_{t,i}\geq c]\mathbbm{1}[u_{t,i}>v_{t,j},\forall j\in \mA_t\setminus\{i\}].$
Then,
\begin{equation*}
\E_{\bm\pi^\ast,\mechname}\sqb{\sum_{t=1}^T \sum_{i=1}^K D_{t,i}} \leq \frac{2K^3}{c}.
\end{equation*}
\end{lemma}

Intuitively, the proof argues that the only reason for an agent $i\in [K]$ to miss a win by marking down is because some other agent $j\in \mA_t\setminus \{i\}$ would win-by-lying with high probability, and the consequent situation of $\mA_{t+1}=\mA_t\setminus \{j\}$ is more favorable for agent $i$. However, due to the upper and lower bounds on V-functions in \Cref{lem:u lower and upper bound}, such events can only happen rarely. With the help of \Cref{lem:number of marking down bound}, we are able to control the expected number of audits of \mechname under $\bm\pi^\ast$.


\section*{Acknowledgements}

This work was partly funded by AFOSR grant FA9550-23-1-0182. MB was supported by a Columbia Data Science Institute postdoctoral fellowship.