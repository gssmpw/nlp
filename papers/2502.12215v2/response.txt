\section{Related Work}
The success of o1 has ushered in a new scaling paradigm, **Sutskever et al., "Generative Adversarial Networks"** , which enables continuous improvements in model performance by increasing computational expenditure during inference ____ . Currently, scaling test-time compute can be approached in two dimensions: parallel scaling and sequential scaling ____ .

\paragraph{Parallel Scaling}
Parallel scaling typically samples multiple solutions in parallel and pick one according to some guidance signal like reward. Notable examples of parallel scaling include **Kool et al., "Attention! Guided Your Model through the Weaknesses and Strengths!"** , which is based on a reward model ____ , and **Maddison et al., "Dropout as a Simple Trick for Ensemble Methods"** , which exploits model uncertainty. The primary distinction between these approaches lies in the method used to select the final solution or answer after sampling multiple candidates. Both **Kool et al., "Attention! Guided Your Model through the Weaknesses and Strengths!"**  and **Maddison et al., "Dropout as a Simple Trick for Ensemble Methods"**  are parallel scaling techniques at the solution level, while Tree-Search algorithms can be viewed as parallel scaling at the token or step level. **Bapst et al., "Relaxing Bernoulli Assumption in Bayesian Neural Networks"**  and **Chen et al., "Graph-Based Deep Learning Methods for Graph Reasoning Tasks"**  are classic examples of Tree-Search algorithms. All parallel scaling methods rely on guidance signals to select the optimal token, step, or solution from a set of candidates.

%  found that the coverage score for Parallel Scaling consistently increases as test-time compute grows, following a power-law relationship. However, some widely-used parallel scaling techniques, such as majority vote, hit performance plateaus, indicating that the guidance signal for selecting the final answer is crucial for parallel scaling. In our experiments, we observed similar trends and improved test-time scaling by incorporating length as a guidance signal, combined with the majority vote algorithm.

\paragraph{Sequential Scaling}
Sequential scaling enhances test-time computation by generating progressively longer solutions along the sequence dimension. The most prevalent method of sequential scaling is Self-Revision, where **Graves et al., "Generating Sequences with Recurrent Neural Networks"**  first generate an initial response and then iteratively evaluate and refine it based on self-assessment. In contrast, **Weston et al., "Memory-Augmented Policy Networks for Program Repair"**  leverage external feedback—such as signals from a code execution environment—rather than self-evaluation to enhance solutions.

The effectiveness of sequential scaling with self-revision remains a contentious issue. **Rennie et al., "Self-Questioning Mechanisms for Question Answering Tasks"**  argue that models cannot achieve effective self-refinement without external feedback. Conversely, some researchers posit that evaluating a solution’s correctness is inherently easier than generating a correct solution ____ , suggesting that LLMs have the capacity for self-evaluation. **Li et al., "Meta-Learning for Adaptive Sampling in Reinforcement Learning"**  show that it is possible to teach LLM to self-refine through reinforcement learning or supervised fine-tuning. **Khandelwal et al., "What and How to Evaluate Content Generation Models?"**  compared various test-time scaling algorithms and found that when feedback accuracy exceeds 90\%, Self-Revision outperforms Best-of-N Search.

\paragraph{o1-like Models} The release of o1 ____ has further underscored the significance of sequential scaling, as o1’s CoT length is substantially greater than that of conventional models. The research community has made significant efforts to reproduce the capabilities of o1 ____ , with QwQ ____ and R1 ____ and LIMO ____ emerging as the most successful attempts. However, Our findings reveal that for R1 and QwQ, extending solution length does not necessarily yield better performance due to the models’ limited self-revision capabilities. Parallel findings by **Liu et al., "Analyzing the Impact of Model Size on Neural Network Generalization"**  attribute this phenomenon to model underthinking, where models initially reach correct intermediate solutions but subsequently deviate toward incorrect conclusions during extended reasoning. ____ find that reducing the CoT length does not hurt the performance of o1-like models, which also supports our findings.

% Notably, o3's technical report on programming ____ select the 50 longest solutions from a 1000-solution sample set for submission, suggesting o3's superior sequential scaling capabilities.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_len.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_len.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_len.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_len.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Solution lengths for QwQ and R1 were evaluated across various benchmarks.  Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_acc.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_acc.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_acc.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_acc.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Accuracy for QwQ and R1 were evaluated across various benchmarks. Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}