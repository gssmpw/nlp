
\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumitem}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}  % 导入 booktabs 包

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{fancyhdr}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}  % 确保加载 multirow 包
\usepackage{tcolorbox}

\newtcolorbox{promptbox}[2][]{%
    colback=gray!5,
    colframe=gray!50,
    boxrule=0.5pt,
    arc=0mm,
    title=#2,
    fonttitle=\bfseries,
    #1
}

\title{Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities? }

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\colmfinalcopy

% \author{
% Author 1$^{1,2,}$\thanks{Equal contribution.} \hspace{.3em}
% Author 2$^{1,2,}$\protect\footnotemark[\value{footnote}] \hspace{.3em}
% Author 3$^{1,2}$ \hspace{.1em}
% Author 4$^{2}$
% \\
% \textbf{
% Author 5$^{1}$ \hspace{.1em}
% Author 6$^{1}$ \hspace{.1em}
% Author 7$^{1,2}$ \hspace{.1em}
% Author 8$^{1}$ \hspace{.1em}
% }
% \\
% \textbf{
% Author 9$^{2,}$\thanks{Corresponding author. 
%  Correspondence to: Author 1 \texttt{<email 1>} Author 2 \texttt{<email 2>} Author 3 \texttt{<email 3>}} \hspace{.2em}
% Author 10$^{1,}$\protect\footnotemark[\value{footnote}]
% }
% \\
% % \texttt{chengqy21@m.fudan.edu.cn} \\
% [1ex]
% $^{1}$Institution 1 \\
% $^{2}$Institution 2 \\
% }
\author{Zhiyuan Zeng\textsuperscript{1},\quad Qinyuan Cheng\textsuperscript{1},\quad Zhangyue Yin\textsuperscript{1},\quad Yunhua Zhou\textsuperscript{2}, \quad Xipeng Qiu\textsuperscript{1}\thanks{~Corresponding author} \\
$^1$School of Computer Science, Fudan University, Shanghai, China \\
$^2$Shanghai AI Laboratory \\
{cengzy23@m.fudan.edu.cn; xpqiu@fudan.edu.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\pagestyle{fancy}
\fancypagestyle{firstpage}{
  \lhead{\begin{picture}(0,0)\put(0,-6){\includegraphics[width=0.3\linewidth]{./logo/OpenMOSS.pdf}}\end{picture}}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\thispagestyle{firstpage}

\maketitle
\begin{abstract}
The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI’s o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose ``Shortest Majority Vote'', a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches. Our implementation is available at \url{https://github.com/ZhiYuanZeng/test-time-scaling-eval}.

% The release of OpenAI’s o1 series models marked a significant leap in the reasoning capabilities of large language models (LLMs), introducing test-time scaling that enhances model performance by allocating more computational resources during inference. Successors such as QwQ and Deepseek-R1 (R1) have demonstrated similar achievements, but the role of chain-of-thought (CoT) length in improving reasoning accuracy remains uncertain. In this study, we investigate the relationship between CoT length and reasoning performance in QwQ and R1, challenging the assumption that longer CoTs always lead to better results. Our findings reveal that, contrary to expectations, longer CoTs do not consistently enhance accuracy, and in some cases, they may introduce inefficiencies or errors. Furthermore, self-revision behavior within CoTs shows that longer chains often involve more revisions, with performance declining for certain models as reflection length increases. Additionally, both QwQ and R1 show limited success in converting incorrect answers to correct ones during revisions, raising concerns about the effectiveness of sequential scaling. In contrast, we demonstrate that parallel scaling provides better coverage and scalability. We propose a new method, Shortest Majority Vote, which enhances majority voting by considering solution length, outperforming traditional methods, especially in R1-Distill-1.5b and QwQ. Our results highlight the complexities of CoT length and revision strategies in LLMs and offer new insights into optimizing reasoning performance through parallel scaling.


\end{abstract}
\section{Introduction}
The release of the OpenAI o1 series models~\citep{o1_blog,o1_system_card} marked a pivotal advancement in the reasoning capabilities of Large Language Models (LLMs), introducing a novel scaling paradigm, test-time scaling, which allocates more compute resources during test time. The test-time scaling have two dimensions, sequential and parallel \citep{o1-roadmap}. Sequential scaling increase test-time compute by scaling the length of Chain-of-Thought (CoT)~\citep{wei2022chain}, while parallel scaling parallely samples multiple solutions and pick the best one. 

Following o1's success, models such as QwQ \citep{qwq}, Deepseek-R1 (R1)~\citep{deepseek-r1} and LIMO \citep{ye2025limoreasoning} have emerged as leading open-source successors, replicating o1's achievements and demonstrating comparable reasoning abilities. Although both QwQ, R1 and LIMO demonstrate strong reasoning capabilities and the ability to generate lengthy CoT at test time, the existence of \textbf{true test-time scaling where performance consistently improves with longer CoTs} remains to be verified for these models.

To explore this question, we systematically investigate the relationship between CoT length and reasoning performance in QwQ, R1 and LIMO, challenging the conventional assumption that extended reasoning chains inherently lead to improved accuracy. Contrary to expectations, our analysis reveals that longer CoTs do not consistently improve accuracy of these o1-like models.
Notably, we found that the average length of correct solutions is shorter than that of incorrect ones for the same questions, which is shown in Figure \ref{fig:overall-correct-vs-incorrect}. This counterintuitive finding underscores the need for a deeper understanding of the test-time scaling of o1-like models.

To understand why the longer CoTs do not lead to the better performance, we compared the difference between long CoTs and short CoTs, finding that long CoTs contain more self-revisions (``Wait'', ``Alternatively'') than the short CoTs, which is shown in Appendix \ref{app:revision-examples}. Inspired by that, we iteratively prompted QwQ, R1 and LIMO for more self-revisions. Our observations revealed that QwQ and R1-Distill-1.5b exhibited performance degradation as the length of reflection increased. In contrast, R1-Distill-14b, R1-Distill-32b, and LIMO demonstrated initial performance improvements during early revisions, followed by oscillatory behavior in subsequent iterations. To further understand the limitations of sequential scaling, we evaluated the models' capacity to revise incorrect answers. Our findings indicate that QwQ, R1 and LIMO all demonstrated limited ability to convert incorrect answers to correct ones during the revision process. Most revisions retained the original answers, and more concerning, both QwQ and R1-Distill-1.5b showed a higher propensity to change correct answers to incorrect ones rather than vice versa. These results reveal that \textbf{self-revision ability is a key factor in the effectiveness of sequential scaling for o1-like models}.

Given the limited effectiveness of sequential scaling, we explored an alternative test-time scaling strategie, parallel scaling. Our comparative analysis of sequential and parallel scaling revealed that parallel scaling not only achieves the better coverage (pass@k score) but also offers superior scalability compared to sequential scaling for QwQ and R1, which demonstrates that o1-like models have limited sequential-scaling capability, but strong parallel-scaling capability.

Building on these findings, we propose a novel test-time scaling method, \textbf{Shortest Majority Vote}, which incorporate parallel scaling approaches with our insight on sequential scaling. In particular, this method leverages the observation that shorter solutions tend to lead to better performance compared to longer ones. Shortest Majority Vote improves majority vote by prioritizing clusters that have both more solutions and shorter solution lengths. Experimental results demonstrate that Shortest Majority Vote substantially outperforms conventional Majority Vote, significantly improving the test-time scalability of both QwQ and R1 models.

Our contributions are as follows:
\begin{enumerate}[itemsep=0pt,parsep=0pt,label=\arabic*)]
    \item We systematically investigate the test-time scaling capabilities of o1-like models QwQ, R1 and LIMO, and find that their performance can not be continuously improved through increasing CoT length.
    \item We reveal that insufficient self-revision capability of o1-like models is the primary reason for their failure in sequential scaling.
    \item We find that parallel scaling achieves better coverage and scalability than sequential revision for o1-like models. 
    \item Based on our insights into sequential and parallel scaling, we propose Shortest Majority Vote, a test-time scaling method that enhances majority voting by considering solution length, significantly outperforming traditional methods.
\end{enumerate}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overall_correct_vs_incorrect.pdf}
    \caption{The average length of correct solutions versus incorrect solutions evaluated on the same questions. For each question, solution lengths were averaged separately for correct and incorrect responses, then averaged across all questions.}
    \label{fig:overall-correct-vs-incorrect}
\end{figure}
\section{Related Work}
The success of o1 has ushered in a new scaling paradigm, \textbf{test-time compute scaling}, which enables continuous improvements in model performance by increasing computational expenditure during inference \citep{o1_blog,o1_system_card}. Currently, scaling test-time compute can be approached in two dimensions: parallel scaling and sequential scaling \citep{Scaling_test_time_compute,o1-roadmap}. 
\paragraph{Parallel Scaling}
Parallel scaling typicallly samples multiple solutions in parallel and pick one according to some guidence signal like reward. Notable examples of parallel scaling include Best-of-N Search \citep{OpenAIMathVerifierORM,Speculative_bon,BoNBoN,Variational_bon,BOND}, which is based on a reward model \citep{OpenAIMathVerifierORM,VerifySbyS}, and Majority Vote \citep{SC}, which exploits model uncertainty. The primary distinction between these approaches lies in the method used to select the final solution or answer after sampling multiple candidates. Both Best-of-N Search and Majority Vote are parallel scaling techniques at the solution level, while Tree-Search algorithms can be viewed as parallel scaling at the token or step level. Beam-Search \citep{TreeBoN,OVM,Self_evaluation_guided_beam_search,stochastic_beam_search} and MCTS \citep{RAP,Alpha-zero-like,AlphaMath,MCTS_for_code_generation} are classic examples of Tree-Search algorithms. All parallel scaling methods rely on guidance signals to select the optimal token, step, or solution from a set of candidates. 

% \citet{Large_Language_Monkeys} found that the coverage score for Parallel Scaling consistently increases as test-time compute grows, following a power-law relationship. However, some widely-used parallel scaling techniques, such as majority vote, hit performance plateaus, indicating that the guidence signal for selecting the final answer is crucial for parallel scaling. In our experiments, we observed similar trends and improved test-time scaling by incorporating length as a guidence signal, combined with the majority vote algorithm.

\paragraph{Sequential Scaling}
Sequential scaling enhances test-time computation by generating progressively longer solutions along the sequence dimension. The most prevalent method of sequential scaling is Self-Revision, where \citet{Self_refine} first generate an initial response and then iteratively evaluate and refine it based on self-assessment. In contrast, \citet{self_debug,Critic} leverage external feedback—such as signals from a code execution environment—rather than self-evaluation to enhance solutions.

The effectiveness of sequential scaling with self-revision remains a contentious issue. \citet{LLM_cannot_self_correct,can-llm-self-revision} argue that models cannot achieve effective self-refinement without external feedback. Conversely, some researchers posit that evaluating a solution’s correctness is inherently easier than generating a correct solution \citep{janleike2022why_excited_about_AI_assisted_human_feedback}, suggesting that LLMs have the capacity for self-evaluation. \citet{SCoRE,teach-self-revision} show that it is possible to teach LLM to self-refine through reinforcement learning or supervised fine-tuning. \citet{Tree_search_vs_revisions} compared various test-time scaling algorithms and found that when feedback accuracy exceeds 90\%, Self-Revision outperforms Best-of-N Search.

\paragraph{o1-like Models} The release of o1 \citep{o1_blog,o1_system_card} has further underscored the significance of sequential scaling, as o1’s CoT length is substantially greater than that of conventional models. The research community has made significant efforts to reproduce the capabilities of o1 \citep{o1-journey1,o1-journey2,renda-1,renda-2,s1}, with QwQ \citep{qwq} and R1 \citep{deepseek-r1} and LIMO \citep{ye2025limoreasoning} emerging as the most successful attempts. However, Our findings reveal that for R1 and QwQ, extending solution length does not necessarily yield better performance due to the models’ limited self-revision capabilities. Parallel findings by \citet{o1-underthink} attribute this phenomenon to model underthinking, where models initially reach correct intermediate solutions but subsequently deviate toward incorrect conclusions during extended reasoning. \citet{o1-overthink,o1-pruner,o1-efficient} find that reducing the CoT length does not hurt the performance of o1-like models, which also supports our findings.

% Notably, o3's technical report on programming \citep{o3-programming} select the 50 longest solutions from a 1000-solution sample set for submission, suggesting o3's superior sequential scaling capabilities.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_len.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_len.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_len.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_len.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Solution lengths for QwQ and R1 were evaluated across various benchmarks.  Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_acc.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_acc.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_acc.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_acc.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Accuracy for QwQ and R1 were evaluated across various benchmarks. Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}

\section{Experiment Setting}
\paragraph{Models} Our experiments involved models from the QwQ \citep{qwq}, LIMO\citep{ye2025limoreasoning} and Deepseek-R1 series \citep{deepseek-r1}, including Deepseek-R1, Deepseek-R1-Distill-Qwen-32b, Deepseek-R1-Distill-Qwen-14b, and Deepseek-R1-Distill-Qwen-1.5b. For simplicy, we call these R1 models as R1-671b, R1-Distill-32b, R1-Distill-14b and R1-Distill-1.5b respectively. The models were run using SGLang framework \citep{sglang}, with the sampling temperature set to 0.7 and the maximum generation length set to 32k. We show the system prompt and instructions used for evaluation in Appendix \ref{app:prompt}.

\paragraph{Benchmark} We conducted comprehensive evaluations across four benchmarks: MATH-500 \citep{VerifySbyS}, AIME \citep{aime}, Omini-MATH \citep{omini-math}, and GPQA \citep{gpqa}. While MATH-500, AIME, and Omini-MATH focus on mathematical reasoning, GPQA encompasses broader scientific domains. For AIME evaluation, we utilized the AIMO validation set, comprising 90 questions from AIME 22, 23, and 24 \citep{aime}. Given the computational demands of evaluating the full Omini-MATH dataset (4.4K questions), we randomly sampled 500 questions to maintain efficiency. For GPQA, we focused on the diamond subset containing 198 questions. To ensure robust evaluation of answer correctness, we employed both the OpenCompass \citep{2023opencompass} and Qwen Math \citep{Qwen2.5-Math} evaluators, considering an answer correct if validated by either evaluator.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{figures/overall_len.pdf}
        \caption{Evaluation for Solution length.}
        \label{fig:overall-len}
    \end{subfigure}
    
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{figures/overall_acc.pdf}
        \caption{Evaluation for accuracy.}
        \label{fig:overall-acc}
    \end{subfigure}
    \caption{Solutions of QwQ and R1 were categorized into different groups according to their length and evaluated in terms of solution length (a) and accuracy (b). The categorization of solutions is progressed for each question independently, i.e., all groups of solutions are corresponding to the same questions.}
    
\end{figure*}
\section{The Failure of Sequential Scaling}
% Our analysis primarily revolves around the phenomenon that scaling of CoT length may be invalid. In Section \ref{}, we present this phenomenon and investigate its relationship with model size, dataset domain, and dataset difficulty. In Section \ref{}, we attempt to analyze the underlying causes of this phenomenon and find that it is closely related to the model's revision capability. In Section \ref{}, we further compare two inference-time scaling methods—sequential revision and parallel sampling—and demonstrate that parallel sampling is more effective. 
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_correct_vs_incorrect.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_correct_vs_incorrect.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_correct_vs_incorrect.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_correct_vs_incorrect.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{The length of correct solutions versus incorrect solutions evaluated on different benchmarks with different models.}
% \end{figure*}

\subsection{Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance} \label{sec:invalid-scaling}
To investigate whether the accuracy of QwQ, R1 and LIMO genuinely improves with increasing CoT length, we sampled each model five times on the same question and sorted the five solutions by length in ascending order. We grouped the solutions based on their rank in this sorted list, with the $i$-th ranked solutions forming a distinct group. For instance, all the longest solutions (rank 5) from different questions formed one group, while all the shortest solutions (rank 1) formed another, resulting in 5 comprehensive solution groups for analysis.

We present the average lengths of the five groups of solutions in Figure \ref{fig:overall-len}. Since the grouping of solutions is based on their lengths, the differences in length between the groups are pronounced. The average length of the longest solutions is approximately twice that of the shortest solutions. This indicates that long-chain-of-thought (CoT) models like QwQ,  R1 and LIMO exhibit a high diversity in the lengths of the solutions they sample. 

There is no clear correlation between the length of solutions and the model's size. For example, R1-Distill-1.5b produces the longest solutions while QwQ (32b) generates the shortest. A comparison of solution lengths across different datasets shows that solutions for simpler datasets, such as Math, are significantly shorter than those for more difficult datasets, like AIME. This suggests that the model adjusts the solution length based on the difficulty of the problem.

The accuracy of the five groups of solutions is presented in Figure \ref{fig:overall-acc}. Although there is a significant disparity in solution lengths across the groups, the differences in accuracy are much less pronounced. Notably, we do not observe a consistent improvement in accuracy for either QwQ or R1 as solution length increases. This trend holds true across all model variants as well as across all evaluated datasets. In some cases, we even observe an inverse scaling phenomenon, where accuracy decreases with increasing CoT length, especially on more difficult datasets like AIME and Omini-MATH. These findings cast doubt on the presumed test-time scaling capabilities of o1-like models, challenging the assumption that extended reasoning chains inherently yield superior problem-solving performance. 

To make the relationship between  CoT length and accuracy more clear, we compared the lengths of correct and incorrect solutions for the same question. First, we identified questions that had both correct and incorrect answers. For each of these questions, we calculated the average length of correct and incorrect solutions. We then averaged these values across all questions to determine the overall average length for correct and incorrect solutions. The results are shown in Figure \ref{fig:overall-correct-vs-incorrect}. We found that, for QwQ, R1 and LIMO, across all model sizes and datasets, the length of correct solutions is consistently shorter than that of incorrect solutions. This observation suggests that longer CoTs do not necessarily lead to better performance and may even be associated with lower accuracy. Moreover, we observed that for weaker models, such as QwQ and R1-Distill-1.5B, the gap in solution length between correct and incorrect solutions is significantly larger than for stronger models, such as R1-671b. This suggests that the invalid scaling phenomenon is more pronounced in the weaker models. 


\subsection{Explaining Invalid Scaling: The Key Factor is the Failure of Self-Revision}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/aime-token-limit.pdf}
        \caption{Max Token Limitation}
        \label{fig:max-token-limit}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/wait_count.pdf}
        \caption{Frequence of ``Wait''}
        \label{fig:wait-count}
    \end{subfigure}
    \caption{(a): The relationship between model accuracy and the generation parameter Max Token Limitation. (b): The relationship between solution length and the average number of ``wait'' occur in a solution.}
    \label{fig:enter-label}
\end{figure*}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/stick_old_answer.pdf}
%     \caption{The proportion of cases that the model stick to the original answer.}
%     \label{fig:enter-label}
% \end{figure}

In Section \ref{sec:invalid-scaling}, we observed the phenomenon that long solutions exhibit lower accuracy compared to short solutions. In this section, we investigate the underlying reasons for this phenomenon. We first analyzed how the maximum token limitation affects generation performance and confirmed that the observed invalid scaling phenomenon was not caused by constraints in the maximum token length. Next, we examined the differences between long and short solutions, finding that long solutions exhibit a higher frequency of self-revision. Moreover, our analysis suggests a strong correlation between self-revision, solution length, and accuracy.

\paragraph{Max Token Limitation}

The max token limitation parameter controls the maximum number of tokens a model can generate for a question, which plays a critical role in influencing model accuracy, especially when generating long solutions. To explore its impact, we tested several max token limitation values and compared the performance of QwQ, R1 and LIMO on the AIME benchmark. The results are shown in Figure \ref{fig:max-token-limit}, which revealed that 16k is a key threshold: when the max token limitation is below this value, it significantly affects the model performance. However, increasing the max token limitation beyond 16k leads to diminishing returns, particularly for QwQ. In our other experiments, we set the max token limitation to 32k, suggesting that this parameter is not the main cause of invalid scaling.

\paragraph{Difference between Short and Long CoT}
% 为了分析long solutions效果比short solutions差的原因, 我们比较了两者的区别.
To understand why long solutions of QwQ, R1 and LIMO is not better than short solutions, we analyzed their differences. We observed that QwQ, R1 and LIMO all primarily extend solution length through self-revision, characterized by markers such as ``Wait'' and ``Alternatively''. We show some examples of that in Appendix \ref{app:revision-examples}. To quantify this phenomenon, we counted the occurrences of ``wait'' in solutions of QwQ, R1 and LIMO in Figure \ref{fig:wait-count}. The results demonstrates a strong linear correlation between solution length and the frequency of self-correction markers for all models. This suggests that the mechanisms of self-revision may play a significant role in generating longer solutions. 
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/r1_32_and_14b_acc_after_seq_scale.pdf}
        \caption{Acc of R1-Distill-32b, -14b}
        \label{fig:32b-14b-acc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/qwq_r1_1_5b_acc_after_seq_scale.pdf}
        \caption{Acc of R1-Distill-1.5b, QwQ}
        \label{fig:1.5b-qwq-acc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/len_after_seq_scale.pdf}
        \caption{Solution lengths.}
        \label{fig:increased-len}
    \end{subfigure}
    \caption{(a): Accuracy of R1-Distill-32b, R1-Distill-14b and LIMO during sequential revisions. (b): Accuracy of R1-Distill-1.5b and QwQ during sequential revisions. (c) Solution length increased with the more revision steps.}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overall_revision_1x5.pdf}
    \caption{The ratio of turning an initial correct answer to incorrect one (correct to wrong) and an initial incorrect answer to a correct one (wrong to correct)  during sequential scaling.}
    \label{fig:revision}
\end{figure*}
\paragraph{Scaling Solution Length with Self-Revision}\label{sec:sequential-revision}
We have tried to investigate the revision behaviors inside the sampled solutions, however, it is difficult to extract the initial solution and the following revision exactly from QwQ, R1 and LIMO's solutions. Alternatively to that, we prompted the models to continue thinking based on their sampled solutions. 

QwQ, R1 and LIMO often conclude their solutions with phrases like ``final answer: ...'', and R1 additionally outputs a `</think>' tag followed by a final response. To facilitate smoother continuation of the reasoning process, we removed the ``final answer'' portion from the solutions. We then used the keyword ``Wait'' or ``Alternatively'' as the prompt to encourage self-revision. We calculated the probabilities of the model predicting the next token as ``Wait'' or ``Alternatively'' and selected the one with the higher probability as the prompt.

We prompted QwQ, R1 and LIMO to continue reasoning for 40 additional steps on the AIME benchmark. We show the results in Figure \ref{fig:increased-len}, from which we observe that the solution length increase almost linearly with additional steps. After 40 steps, the solution length of QwQ and R1 is almost third as their original length.

We show the accuracy after sequential revision in Figure \ref{fig:32b-14b-acc} and \ref{fig:1.5b-qwq-acc}. Our results reveal that the accuracy of QwQ and R1-Distill-1.5b decreases constantly as the number of reasoning steps increases, while the accuracy of R1-Distill-32b, R1-Distill-14b and LIMO initially improves and then oscillates with further reasoning steps. Further analysis in Appendix \ref{app:short-long-revision} reveal that the improvement on R1-Distill-32b, R1-Distill-14b and LIMO during revisions mainly comes from the revision on short solutions. These results corroborate our previous experimental findings, suggesting that longer solutions do not improve performance, especially for weaker models such as QwQ and R1-Distill-1.5b. These findings suggest that the reason why longer solutions do not consistently lead to better performance in QwQ, R1 and LIMO may lie in the failure of self-revision.

\paragraph{Investigating Self-Revision Behavior}
To further investigate the effectiveness of self-revision, we analyzed the proportion of cases where the model corrected an initial incorrect answer to a correct one versus changing an initial correct answer to an incorrect one during scaling solution length. We found that, the proportions of changing a incorrect answer to an correct one is extremely low, always below 10\%. Notably, for QwQ and R1-Distill-1.5b, the proportion of changing a correct answer to an incorrect one was even higher than that of correcting an incorrect answer to a correct one. This observation helps explain why prompting QwQ and R1-Distill-1.5b to continue reasoning led to a decrease in accuracy. For simplicty, we call the proportions of changing a incorrect answer to an correct one as the successful-revision rate, while the reverse as the failed-revision rate.

Although R1-Distill-32b, R1-Distill-14b and LIMO exhibit a higher successful-revision rate than failed-revision rate, the increase of successful-revision rate plateaus after approximately 10 steps, with further revisions providing no additional benefits. This observation explains why their accuracy during sequential scaling initially increases with multiple rounds of revision but later stabilizes with fluctuations.

\begin{table}[t]
    \centering
    \begin{tabular}{ccccl}
        \toprule
        R1-32b & R1-14b & R1-1.5b & QwQ  &LIMO\\
        \midrule
         72\% & 70\% & 58\% & 32\%  & 54\%\\
        \bottomrule
    \end{tabular}
    \caption{The proportion of the revisions that models sitck to the original wrong answers.}
    \label{tab:stick}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/coverage-parallel-vs-seq.pdf}
        \caption{Evaluation on Coverage.}
        \label{fig:coverage-seq-vs-parallel}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/Acc-parallel-vs-seq.pdf}
        \caption{Evaluation on Accuracy}
        \label{fig:acc-seq-vs-parallel}
    \end{subfigure}
    \caption{(a): the coverage of sequential scaling and parallel scaling on AIME. (b): the accuracy of squential revision and majority vote on AIME.}
\end{figure*}
The successful-revision rate of QwQ, R1 and LIMO are all below 10\%, what is the outcome of the model’s self-revision in unsuccessful cases? We hypothesize that, in most instances, the model simply keeps its original answer unchanged. To validate that, we computed the proportion of instances where the model persists with its original answer, even when it is incorrect, and the results were as expected. As shown in Figure \ref{fig:revision}, when the original answer is wrong, both R1-Distill-32b and R1-Distill-14b maintain the original answer in over 70\% of cases. Although retaining the original answer does not reduce accuracy, it also makes the scaling solution length ineffective. This phenomenon suggests that the model’s ability to early stop may also be a critical factor influencing whether its performance improves with an increasing solution length.

The above analysis indicates that the key factor determining whether o1-like models' performance improve with an increase in solution length is their ability to self-revise. The model's accuracy increases with the more incorrect answers revised to correct and vice versa.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overall_aime_short_majority_vote_1x5.pdf}
    \caption{Parallel-scaling performance of Majority Vote, Shortest and Shortest Majority Vote on AIME.}
    \label{fig:short-majority-vote}
\end{figure*}

\section{Sequential Scaling vs. Parallel Scaling}\label{sec:seq-vs-parallel}
Based on our experimental findings presented in Section \ref{sec:sequential-revision}, sequential scaling demonstrates limited effectiveness for QwQ, R1 and LIMO. An alternative approach to scaling test-time compute is parallel scaling, which generates multiple solutions in parallel and selects the best one as the final answer.  

We compared the performance of sequential scaling and parallel scaling in terms of the coverage (pass@k score) and accuracy of QwQ and R1, which are shown in Figure \ref{fig:coverage-seq-vs-parallel} and \ref{fig:acc-seq-vs-parallel} respectively. For sequential scaling, we iteratively prompt models to self-revise for 40 steps. While for parallel scaling, we parallely sample 10 solutions. The coverage is evaluated by counting the proportion of whether multiple candidate answers contain a correct one. In parallel scaling, coverage increases by one if at least one sampled solution is correct. Similarly, in sequential scaling, coverage increases by one if at least one revision iteration succeeds.

Our findings show that, for the same number of generated tokens, parallel scaling provides a significantly larger improvement in coverage compared to sequential scaling, for both R1-Distill-32b and QwQ. However, a practical parallel scaling method must select a final answer from a set of candidate answers. We implement parallel scaling using majority vote \citep{SC} and sequential scaling by taking the answer from the last revision as the final answer. Since majority voting requires at least three solutions to be effective, it does not provide any benefit when scaling the number of solutions from 1 to 2. In contrast, sequential revision is effective for R1-Distill-32b when scaling the number of tokens to 10k, but further scaling does not yield additional benefits. Additionally, because sequential scaling involves attention over a longer context, its computational cost is much higher than that of parallel scaling when generating the same number of tokens. 
% Furthermore, this experimental observation suggests that majority voting, when employed as a parallel scaling strategy, may not yield performance improvements under constrained inference computational resources.

% \cqy{An potentail alternative storyline: Due to insufficient sequential sacling abilities of current open-source reasoning LLMs, we advocate to incorperate parallel scaling techniques to current sequential scaling models xxx, and use our insights as heuristics xxx.} This insight inspires us to propose a new parallel scaling algorithm in Section \ref{Application of Our Findings: Shortest Majority Vote}: Shortest Majority Vote. 

\section{Application of Our Findings: Shortest Majority Vote}\label{sec:new-method}
\begin{table*}[t]
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tabular}{cccccccc}
    \toprule
 \multirow{2}{*}{Model} & \multirow{2}{*}{Solutions} & \multicolumn{3}{c}{AIME} & \multicolumn{3}{c}{GPQA}\\
        \cmidrule{3-8}
        & & MV& Shortest& Shortest MV& MV& Shortest&Shortest MV\\
        % R1-Distill-32b & Majority Vote
        % R1-Distill-14b
        % R1-Distill-1.5b
        \midrule
R1-Distill-32b&\multirow{5}{*}{2}& 59.77&\textbf{62.22}& \textbf{62.22}& 61.41& \textbf{62.52}&\textbf{62.52}\\
R1-Distill-14b & & 58.88&\textbf{60.44}& \textbf{60.44}& 51.21& \textbf{52.32}&\textbf{52.32}\\
R1-Distill-1.5b & & 24&\textbf{27.55}& \textbf{27.55} & 15.25& \textbf{15.35}&\textbf{15.35}\\
QwQ & & \textbf{41.77}&40.22& 40.22& \textbf{58.05}& 57.02&57.02\\
 LIMO& & 56.66& \textbf{60.88}& \textbf{60.88}& 50.46& \textbf{54.56}&\textbf{54.56}\\
\midrule
R1-Distill-32b & \multirow{5}{*}{16}& 72.88&61.99& \textbf{73.77}& 63.33& 61.21&\textbf{63.53}\\
R1-Distill-14b & & \textbf{71.77}&62.00& 71.55& 56.16& \textbf{56.66}&56.46\\
R1-Distill-1.5b & & 40.00&26.22& \textbf{42.22}& 29.59& 27.77&\textbf{30.20}\\
QwQ  & & \textbf{51.33}&40.88& 50.88& \textbf{62.25}& 56.82&\textbf{62.25}\\
 LIMO& & 68.88& 62.22& \textbf{70.00}& 55.58& 50.15&\textbf{55.89}\\
  \bottomrule
    \end{tabular}}
    \caption{Performance comparison between Majority Vote (MV), Shortest and Shortest Majority Vote (Shortest MV) on AIME and GPQA, when there are 2 and 16 solutions sampled.}
    \label{tab:shortest-mv}
\end{table*}
\label{Application of Our Findings: Shortest Majority Vote}
Given the limitation of sequential scaling of the current o1-like models, we turn to parallel scaling techniques and incorperate it with our insight on sequential scaling. Specifically, we propose a new Parallel Scaling algorithm: Shortest Majority Vote. Shortest Majority Vote is an extension of Majority Vote, but it accounts for the length of the solutions generated by the model. In the original Majority Vote, solutions with the same answer are grouped into a single category, and the number of solutions in each category is counted, with the answer corresponding to the category with the most solutions selected as the final answer. In contrast, Shortest Majority Vote not only counts the number of solutions in each category, but also computes the average length of the solutions in each category. Let the number of solutions in the $i$-th category be $c_i$ and the average solution length in that category be $l_i$. The score for category $i$ in Shortest Majority Vote is computed as:
\begin{equation}
    s_i=\frac{c_i}{\log{l_i}}
\end{equation}
and the final answer is chosen from the category with the highest score. The score $s_i$ is designed with the assumption that the correct answer is more likely to appear in categories with a larger number of solutions and shorter solution lengths. Shortest Majority Vote offers two key advantages: first, it is particularly effective for some o1-like models, where performance deteriorates with increasing solution length; second, it enables the use of solution length as a guidence signal for identifying superior solutions when candidate solutions are limited, especially in cases where conventional Majority Vote becomes ineffective due to having only two candidate solutions.

% We conducted experiments on the AIME and GPQA benchmark by sampling 16 solutions from R1 and QwQ, comparing the performance of Shortest Majority Vote and Majority Vote. As a simple baseline, we used the answer from the shortest solution as the final answer, which we denote as ``Shortest''. Our results are shown in Table \ref{tab:shortest-mv} and Figure \ref{fig:short-majority-vote}. Table \ref{tab:shortest-mv} shows that Shortest Majority Vote is significantly and constantly better than Majority Vote and Shortest especially on AIME. Figure \ref{} shows the parallel-scaling performance of the three methods, as the number of generated tokens increase, Shortest Majority Vote significantly and constantly outperforms Majority Vote and Shortest on AIME. The parallel-scaling performance on GPQA is shown in Appendix \ref{}. We further observed that Shortest also outperforms Majority Vote when there are only 2 solutions sampled, but in all other cases, it performs worse. These findings underscore the effectiveness of Shortest Majority Vote.

We evaluated the performance of Shortest Majority Vote and Majority Vote through experiments on the AIME and GPQA benchmarks, sampling 16 solutions from QwQ, R1 and LIMO models. We implemented a simple baseline approach, denoted as "Shortest," which selects the answer from the solution with the minimal length. The experimental results are presented in Table \ref{tab:shortest-mv} and Figure \ref{fig:short-majority-vote}. Table \ref{tab:shortest-mv} demonstrates that Shortest Majority Vote  significantly outperforms both Majority Vote and Shortest methods, particularly on the AIME benchmark. Figure \ref{fig:short-majority-vote} illustrates the parallel-scaling performance of these three methods, showing that as the number of generated tokens increases, Shortest Majority Vote maintains superior performance over both alternatives on AIME. The corresponding parallel-scaling results for GPQA are provided in Appendix \ref{app:gpqa-scale}. Notably, while Shortest performs better than Majority Vote when only two solutions are sampled, it exhibits inferior performance in all other scenarios. These empirical findings strongly support the effectiveness of the Shortest Majority Vote approach.

\section{Conclusion}
In this study, we challenged the assumption that o1-like models like QwQ and R1 models have test-time scaling capability. 
% Our analysis reveals
We found that shorter solutions often outperform longer ones, and that sequential scaling through self-revision has limited effectiveness. Based on these insights, we developed Shortest Majority Vote, a parallel scaling method that considers solution length, which significantly outperformed traditional majority vote.


\section*{Limitations}
\begin{enumerate}
    % \item There are some new o1-like models from community like LIMO and S1 not been evaluated, although they are less well-known than QwQ and R1.
    \item Given the considerable cost of R1-671b, evaluation on it was limited to the experiments in Figures 1 and 2, whereas distilled R1 was utilized for all subsequent.
    % \item To make the evaluation easier, we have not evaluated the test-scaling of o1-like models on challanging code tasks like Codeforce.
    \item Our experimental framework was limited to static model checkpoints. Future research should investigate test-time scaling behavior using dynamic checkpoints in reinforcement learning settings.

    \item While the proposed shortest majority method may have limited applicability for models with strong sequential-scaling capabilities, solution length remains a valuable guidance signal for candidate selection in parallel scaling scenarios. The method can be adapted to a Longest Majority Vote variant for such cases.
\end{enumerate}

\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}
\clearpage


\appendix
% \paragraph{Investigating Self-Revision Behavior}
% 既然QwQ和R1的long solutions包含了大量的Self-Revision，那么这些self-revision是否能有效地提升模型的性能呢？我们提取QwQ和R1的solutions中包含的所有"wait"和"alternatively"，并将它们作为断点中断模型的生成，直接询问模型final answer，以此来验证每次"wait"和"alternatively"的有效性。我们发现，不管是对于QwQ还是R1，经过多次Revision后的准确率都随着Revision次数的增加而上升，这个现象和图\ref{}中展示的结果一样，即提前终止生成反而会降低模型的效果。我们认为这是模型
\section{Is Invalid Scaling Phenomenon Conflict to Findings of R1 technique Report?}

% The training objective of R1 is to improve model accuracy, yet correct solutions are typically shorter than incorrect ones. Why, then, does R1's reinforcement learning (RL) training lead to longer solutions? 

% To explore this question, we sampled five solutions for each question and sorted them by length in ascending order to obtain five groups of solutions of different lengths. We then analyzed the distribution of correct solutions correct solutions across these groups, as shown in Figure \ref{fig:overall-distribution}.

% We observed that correct solutions are predominantly distributed within short-solution groups, especially in the AIME dataset, which is consistent with our previous findings. However, the tokens corresponding to correct solutions are primarily concentrated in long-solution groups. This is due to the fact that longer solutions inherently contain more tokens. In the case of R1-Distill-1.5, both the number of solutions and tokens are primarily concentrated in short-solution groups. This can be attributed to the significantly lower performance of long solutions in R1-Distill-1.5, where the sheer quantity of short solutions compensates for the token advantage of longer solutions.

% We hypothesize that this discrepancy explains why RL training tends to produce longer solutions: the training process may favor generating longer solutions, even if they are less accurate, because they contribute more tokens to the gradient. 

The training objective of R1 aims to improve model accuracy, yet we observe that correct solutions tend to be shorter than incorrect ones. This raises an intriguing question: Why does R1's reinforcement learning (RL) training consistently produce longer solutions?

To investigate this phenomenon, we analyzed five solutions per question, organizing them into groups by length in ascending order. Figure \ref{fig:overall-distribution} illustrates the distribution of correct solutions across these groups.

Our analysis revealed that correct solutions predominantly appear in shorter-length groups, particularly in the AIME dataset. However, when examining the token distribution, we found that correct solution tokens are concentrated in longer-solution groups. This apparent contradiction arises because the total token count is determined by both the number of solutions and the average tokens per solution. As shown in Figure \ref{fig:overall-len}, solutions in the longest group contain nearly twice as many tokens as those in the shortest group. This explains why, despite having fewer individual solutions, longer solutions account for a greater share of the total tokens.



% R1-Distill-1.5 presents an exception to this pattern, with both solutions and tokens concentrated in shorter groups. This deviation can be attributed to the model's notably poor performance on longer solutions, where the sheer volume of short correct solutions outweighs the token advantage of longer ones.

We hypothesize that this discrepancy explains why RL training tends to produce longer solutions: the training process may favor generating longer solutions, even if they are less accurate, because they contribute more tokens to the gradient. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overall_distribution.pdf}
    \caption{The number of correct solutions and tokens distributed across groups of different lengths.}
    \label{fig:overall-distribution}
\end{figure}


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/r1_32b_acc_after_seq_scale.pdf}
        \caption{R1-Distill-32b}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/r1_14b_acc_after_seq_scale.pdf}
        \caption{R1-Distill-14b}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/limo_acc_after_seq_scale.pdf}
        \caption{R1-Distill-14b}
    \end{subfigure}
    \caption{Accuracy of short solutions and long solutions of R1-Distill-14b (a) and R1-Distill-32b (b) during sequential revision.}
    \label{fig:short-long-revision}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overall_gpqa_short_majority_vote_1x5.pdf}
    \caption{Performance Comparison between Majority Vote and Shortest Majority Vote on GPQA.}
    \label{fig:short-majority-vote-gpqa}
\end{figure*}

\section{Further analysis on Sequential Scaling on R1-Distill-14b, R1-Distill-32b and LIMO}\label{app:short-long-revision}
In Section \ref{sec:sequential-revision}, we observed that R1-Distill-14b, R1-Distill-32b and LIMO demonstrated some performance improvements after multiple rounds of self-revision, followed by stabilization. Furthermore, in Section \ref{sec:invalid-scaling}, we found that the correct solutions generated by R1-Distill-14b, R1-Distill-32b and LIMO were generally shorter than incorrect solutions. To reconcile these seemingly contradictory findings and further analyze how R1-Distill-14b, R1-Distill-32b and LIMO benefit from self-revision, we conducted a detailed analysis of self-revision outcomes on both long and short solutions. Our methodology for collecting long and short solutions involved sampling five solutions for each question, ordering them by length, and then segregating the longest and shortest solutions into separate groups. The results of self-revision on both short and long solutions are presented in Figure \ref{fig:short-long-revision}. Our analysis reveals that short solutions exhibited significant performance improvements following self-revision, while this trend was less pronounced for long solutions. Therefore, the performance improvements we observed through self-revision in R1-Distill-14b, R1-Distill-32b and LIMO primarily stem from the self-revision on short solutions. This suggests that the relationship between accuracy and solution length for these models is complex, demonstrating neither a strictly positive nor negative correlation with length.

\section{Parallel Scaling of Shortest Majority Vote on GPQA}\label{app:gpqa-scale}
In Section \ref{sec:new-method}, we demonstrated that our proposed Shortest Majority Vote achieves superior test-time scaling performance compared to the other two methods on the AIME benchmark. In this section, we present the parallel-scaling results on GPQA in Figure \ref{fig:short-majority-vote-gpqa}. While Shortest Majority Vote consistently outperforms the Shortest method on GPQA, it does not exhibit significantly better parallel scaling performance compared to Majority Vote on this benchmark. This phenomenon might be attributed to the smaller performance gap between short and long solutions on GPQA compared to AIME, suggesting that solution length plays a less critical role in determining solution quality on the GPQA benchmark, which can be observed from Figure \ref{fig:overall-acc}

\section{Prompt}\label{app:prompt}

System prompt:
\begin{promptbox}{System prompt}
You are a helpful and harmless assistant. You should think step-by-step.
\end{promptbox}

Instruction for MATH-500, AIME and Omini-MATH:
\begin{promptbox}{Instruction}
Answer the question and enclose the final answer in boxed\{\}
\end{promptbox}

Instruction for GPQA:
\begin{promptbox}{Instruction}
Select the best answer from the following options. Output only the letter corresponding to the correct answer, enclosed in boxed\{\}.
\end{promptbox}
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_early_stop.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.23\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_early_stop.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \caption{The early-stop accuracy of QwQ and R1-Distill-32b.}
%     \label{fig:early-stop}
% \end{figure}
% \begin{table}[t]
%     \centering
%     \resizebox{0.5\textwidth}{!}{  % 将表格宽度缩放到页面宽度
%     \begin{tabular}{cccccc}
%         \toprule
%         \multirow{2}{*}{Model} & \multirow{2}{*}{Metric} & \multicolumn{2}{c}{AIME} & \multicolumn{2}{c}{GPQA} \\
%         \cmidrule{3-6}
%                                 &      & Normal & Short & Normal & Short \\
%         \midrule
%         \multirow{2}{*}{R1-32b} & Acc & 60.88 & 57.33 & 60.80 & 61.1 \\
%                                 & Length & 10227 & 8646 & 4872 & 5310 \\
%         \midrule
%         \multirow{2}{*}{QwQ} & Acc & 38.44 & 39.55 & 57.57 & 57.57 \\
%                                 & Length & 9112 & 9096 & 3676 & 2934 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{The performance and response length of QwQ and R1-32b models when prompted to generate a short solution.}
%     \label{tab:prompt-to-short}
% \end{table}

% \section{The Shorter the Better?}
% Based on the above analysis, correct solutions tend to be shorter than incorrect ones. Does this imply that it would be better to make the solutions shorter? To explore this, we applied two simple methods to shorten the solutions: one involved directly prompting the model to generate shorter responses, avoiding unnecessary revisions, and the other involved truncating the generated output and then asking the model for its final answer. The results are shown in Table \ref{tab:prompt-to-short} Figure \ref{fig:early-stop} respectively. We found that prompting the model to generate short solutions does not compromise model performance, but fails to significantly reduce the length of generated solutions. Early stopping led to a significant performance loss for both R1-Distll-32b and QwQ, which implies that it is not nontrival to shorten the model solution while making the performance better.


\section{Examples of self-revision}\label{app:revision-examples}
\begin{promptbox}{Examples}\label{fig:revision-example}
Wait, let me verify that again ...\\\\
Wait, but that seems straightforward, but let me check if I got the constants right ...\\\\
Wait, but let me verify this to ensure I didn't make a mistake ...\\\\
Wait, so is the answer 756? But let me check if this is consistent ...\\\\
Wait, but in 3D space, the centers might not be coplanar? ...\\\\
Alternatively, try to find a general formula ...\\\\
Alternatively, consider that m is such that m divides k where k is from 1 to 999 ...\\\\
Alternatively, maybe we can use modulo 8 to get constraints ...\\\\
Alternatively, perhaps there's a smarter approach ...\\\\
Alternatively, another way to think about this problem is to recognize that w and z are roots of unity ...
\end{promptbox}

\newpage

\end{document}
