\section{Related Work}
The success of o1 has ushered in a new scaling paradigm, \textbf{test-time compute scaling}, which enables continuous improvements in model performance by increasing computational expenditure during inference ____. Currently, scaling test-time compute can be approached in two dimensions: parallel scaling and sequential scaling ____. 
\paragraph{Parallel Scaling}
Parallel scaling typicallly samples multiple solutions in parallel and pick one according to some guidence signal like reward. Notable examples of parallel scaling include Best-of-N Search ____, which is based on a reward model ____, and Majority Vote ____, which exploits model uncertainty. The primary distinction between these approaches lies in the method used to select the final solution or answer after sampling multiple candidates. Both Best-of-N Search and Majority Vote are parallel scaling techniques at the solution level, while Tree-Search algorithms can be viewed as parallel scaling at the token or step level. Beam-Search ____ and MCTS ____ are classic examples of Tree-Search algorithms. All parallel scaling methods rely on guidance signals to select the optimal token, step, or solution from a set of candidates. 

% ____ found that the coverage score for Parallel Scaling consistently increases as test-time compute grows, following a power-law relationship. However, some widely-used parallel scaling techniques, such as majority vote, hit performance plateaus, indicating that the guidence signal for selecting the final answer is crucial for parallel scaling. In our experiments, we observed similar trends and improved test-time scaling by incorporating length as a guidence signal, combined with the majority vote algorithm.

\paragraph{Sequential Scaling}
Sequential scaling enhances test-time computation by generating progressively longer solutions along the sequence dimension. The most prevalent method of sequential scaling is Self-Revision, where ____ first generate an initial response and then iteratively evaluate and refine it based on self-assessment. In contrast, ____ leverage external feedback—such as signals from a code execution environment—rather than self-evaluation to enhance solutions.

The effectiveness of sequential scaling with self-revision remains a contentious issue. ____ argue that models cannot achieve effective self-refinement without external feedback. Conversely, some researchers posit that evaluating a solution’s correctness is inherently easier than generating a correct solution ____, suggesting that LLMs have the capacity for self-evaluation. ____ show that it is possible to teach LLM to self-refine through reinforcement learning or supervised fine-tuning. ____ compared various test-time scaling algorithms and found that when feedback accuracy exceeds 90\%, Self-Revision outperforms Best-of-N Search.

\paragraph{o1-like Models} The release of o1 ____ has further underscored the significance of sequential scaling, as o1’s CoT length is substantially greater than that of conventional models. The research community has made significant efforts to reproduce the capabilities of o1 ____, with QwQ ____ and R1 ____ and LIMO ____ emerging as the most successful attempts. However, Our findings reveal that for R1 and QwQ, extending solution length does not necessarily yield better performance due to the models’ limited self-revision capabilities. Parallel findings by ____ attribute this phenomenon to model underthinking, where models initially reach correct intermediate solutions but subsequently deviate toward incorrect conclusions during extended reasoning. ____ find that reducing the CoT length does not hurt the performance of o1-like models, which also supports our findings.

% Notably, o3's technical report on programming ____ select the 50 longest solutions from a 1000-solution sample set for submission, suggesting o3's superior sequential scaling capabilities.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_len.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_len.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_len.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_len.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Solution lengths for QwQ and R1 were evaluated across various benchmarks.  Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/math_acc.pdf}
%         % \caption{fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/aime_acc.pdf}
%         % \caption{fig2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/omini_acc.pdf}
%         % \caption{fig3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.24\textwidth}
%         \includegraphics[width=\textwidth]{figures/gpqa_acc.pdf}
%         % \caption{fig4}
%     \end{subfigure} 
%     \hfill
%     \caption{Accuracy for QwQ and R1 were evaluated across various benchmarks. Solutions were categorized into five groups based on length, with the i-th group containing solutions ranked in the i-th percentile.}
% \end{figure*}