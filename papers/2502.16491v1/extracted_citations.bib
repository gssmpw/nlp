@inproceedings{backdoor_huang-etal-2024-composite,
    title = "Composite Backdoor Attacks Against Large Language Models",
    author = "Huang, Hai  and
      Zhao, Zhengyu  and
      Backes, Michael  and
      Shen, Yun  and
      Zhang, Yang",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.94",
    doi = "10.18653/v1/2024.findings-naacl.94",
    pages = "1459--1472",
}

@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}

@misc{lv2024codechameleonpersonalizedencryptionframework,
      title={CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models}, 
      author={Huijie Lv and Xiao Wang and Yuansen Zhang and Caishuang Huang and Shihan Dou and Junjie Ye and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2024},
      eprint={2402.16717},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16717}, 
}

@inproceedings{pap_zeng-etal-2024-johnny,
    title = "How Johnny Can Persuade {LLM}s to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing {LLM}s",
    author = "Zeng, Yi  and
      Lin, Hongpeng  and
      Zhang, Jingwen  and
      Yang, Diyi  and
      Jia, Ruoxi  and
      Shi, Weiyan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.773",
    doi = "10.18653/v1/2024.acl-long.773",
    pages = "14322--14350",
}

@inproceedings{poison_xu-etal-2024-instructions,
    title = "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    author = "Xu, Jiashu  and
      Ma, Mingyu  and
      Wang, Fei  and
      Xiao, Chaowei  and
      Chen, Muhao",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.171",
    doi = "10.18653/v1/2024.naacl-long.171",
    pages = "3111--3126",
}

@misc{roleplay_jin2024quack,
title={Quack: Automatic Jailbreaking Large Language Models via Role-playing},
author={Haibo Jin and Ruoxi Chen and Jinyin Chen and Haohan Wang},
year={2024},
url={https://openreview.net/forum?id=1zt8GWZ9sc}
}

@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

