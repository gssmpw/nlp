\section{Related Work}
Research on vulnerabilities in Large Language Models (LLMs) has been increasing, particularly in the domains of jailbreak attacks and backdoor attacks. Based on the difficulty and time required to implement these attacks, they can be categorized into three types: single-round execution, iterative attempts, and model fine-tuning.

% \paragraph{Single-round Execution} Single-round execution methods involve direct interaction with the model in a single instance, with the goal of quickly inducing the model to output a specific response through scenario setup or role-playing. These methods typically do not require multiple adjustments or optimizations, making them the least time-consuming. For example, \textit{Role-playing} \cite{roleplay_jin2024quack} involve interacting with the model through specific roles to induce deviations from its original behavioral guidelines. \textit{Scenario crafting} \cite{li2023deepinception} involves creating carefully designed scenarios to directly guide the model's output. \textit{Rare encoding}, such as specialized encryption or code completion, enables CipherChat \cite{CipherChat_yuan2024gpt} to bypass LLM safety mechanisms with ciphered prompts, exposing limitations in handling non-natural language inputs. Similarly, CodeChameleon \cite{lv2024codechameleonpersonalizedencryptionframework} uses personalized encryption to transform queries into code tasks, bypassing security checks and executing encrypted queries. \textit{Multilingual encoding} \cite{deng2024multilingual} uses low-resource language encoding strategies to circumvent the model's language processing security mechanisms.

\paragraph{Single-round Execution} Single-round execution methods involve direct, one-time interaction with the model, aiming to quickly induce specific responses through role-play or scenario design. These methods are less time-consuming as they avoid multiple adjustments or optimizations. For instance, \textit{Role-playing} \cite{roleplay_jin2024quack} involves assuming specific roles to elicit deviations from the model's default behavior. \textit{Scenario crafting} \cite{li2023deepinception} creates tailored contexts to guide model output. Techniques like \textit{Rare encoding} enable tools like CipherChat \cite{CipherChat_yuan2024gpt} to bypass safety protocols with ciphered prompts, while CodeChameleon \cite{lv2024codechameleonpersonalizedencryptionframework} utilizes personalized encryption to bypass checks by converting queries into code tasks. \textit{Multilingual encoding} \cite{deng2024multilingual} leverages low-resource language strategies to circumvent language processing security measures.

% \paragraph{Iterative Attempts} Iterative attempts rely on algorithms and automated tools that repeatedly adjust and optimize prompts to explore and exploit weaknesses in the model. These methods often involve multiple rounds of testing and modification, typically making them more time-consuming. \textit{GCG} \cite{zou2023universal} uses model gradient information to precisely explore target vulnerabilities. \textit{AutoDAN} \cite{liu2024autodan} employs genetic algorithms to optimize prompts and continually search for more effective attack methods. \textit{GPTFUZZER} \cite{yu2023gptfuzzer} systematically generate various prompt variations to explore the model's responses and potential weaknesses. \textit{PAIR} \cite{pair_chao2024jailbreaking} iteratively refines adversarial prompts by leveraging the target language model’s responses as feedback, using these responses to guide the optimization process and increase the attack success rates. \textit{PAP} \cite{pap_zeng-etal-2024-johnny} employs natural language persuasion techniques to craft adversarial prompts that induce large language models to perform "jailbreak" operations, bypassing their safety and alignment mechanisms.

\paragraph{Iterative Attempts} Iterative attempts utilize algorithms and automated tools to repeatedly adjust prompts, optimizing them to exploit model weaknesses. These methods often require multiple testing rounds, making them time-consuming. \textit{GCG} \cite{zou2023universal} uses model gradients to explore vulnerabilities, while \textit{AutoDAN} \cite{liu2024autodan} leverages genetic algorithms to optimize prompts and search for effective attacks. \textit{GPTFUZZER} \cite{yu2023gptfuzzer} generates prompt variations to probe the model, and \textit{PAIR} \cite{pair_chao2024jailbreaking} refines adversarial prompts based on the model’s feedback. \textit{PAP} \cite{pap_zeng-etal-2024-johnny} uses natural language persuasion to induce "jailbreak" operations in language models.

\paragraph{Model Fine-tuning} Model fine-tuning involves inserting backdoors during the training process or using poisoned data for fine-tuning, causing the model’s security alignment mechanisms to fail. This method is the most time-consuming but also the most successful. \textit{Backdoor attacks} ] \cite{backdoor_huang-etal-2024-composite} in the form of composite backdoor attacks (CBA) distribute multiple trigger keys across different components of the prompt, only activating the backdoor when all trigger keys appear simultaneously. \textit{Poisoned data injections} \cite{poison_xu-etal-2024-instructions} allow attackers to control model behavior by injecting a very small number of malicious instructions. 
%\textit{Assistant model training} (Deng et al., 2023a) leverages an assistant model trained on template datasets, using success rates as a reward mechanism to continuously optimize the generation of prompts, thereby achieving more efficient attacks.