[
  {
    "index": 0,
    "papers": [
      {
        "key": "roleplay_jin2024quack",
        "author": "Haibo Jin and Ruoxi Chen and Jinyin Chen and Haohan Wang",
        "title": "Quack: Automatic Jailbreaking Large Language Models via Role-playing"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023deepinception",
        "author": "Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo",
        "title": "Deepinception: Hypnotize large language model to be jailbreaker"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "CipherChat_yuan2024gpt",
        "author": "Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu",
        "title": "{GPT}-4 Is Too Smart To Be Safe: Stealthy Chat with {LLM}s via Cipher"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lv2024codechameleonpersonalizedencryptionframework",
        "author": "Huijie Lv and Xiao Wang and Yuansen Zhang and Caishuang Huang and Shihan Dou and Junjie Ye and Tao Gui and Qi Zhang and Xuanjing Huang",
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "deng2024multilingual",
        "author": "Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing",
        "title": "Multilingual Jailbreak Challenges in Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "roleplay_jin2024quack",
        "author": "Haibo Jin and Ruoxi Chen and Jinyin Chen and Haohan Wang",
        "title": "Quack: Automatic Jailbreaking Large Language Models via Role-playing"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2023deepinception",
        "author": "Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo",
        "title": "Deepinception: Hypnotize large language model to be jailbreaker"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "CipherChat_yuan2024gpt",
        "author": "Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu",
        "title": "{GPT}-4 Is Too Smart To Be Safe: Stealthy Chat with {LLM}s via Cipher"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lv2024codechameleonpersonalizedencryptionframework",
        "author": "Huijie Lv and Xiao Wang and Yuansen Zhang and Caishuang Huang and Shihan Dou and Junjie Ye and Tao Gui and Qi Zhang and Xuanjing Huang",
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "deng2024multilingual",
        "author": "Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing",
        "title": "Multilingual Jailbreak Challenges in Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024autodan",
        "author": "Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao",
        "title": "Auto{DAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yu2023gptfuzzer",
        "author": "Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu",
        "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "pair_chao2024jailbreaking",
        "author": "Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong",
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pap_zeng-etal-2024-johnny",
        "author": "Zeng, Yi  and\nLin, Hongpeng  and\nZhang, Jingwen  and\nYang, Diyi  and\nJia, Ruoxi  and\nShi, Weiyan",
        "title": "How Johnny Can Persuade {LLM}s to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing {LLM}s"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2024autodan",
        "author": "Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao",
        "title": "Auto{DAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yu2023gptfuzzer",
        "author": "Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu",
        "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "pair_chao2024jailbreaking",
        "author": "Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong",
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "pap_zeng-etal-2024-johnny",
        "author": "Zeng, Yi  and\nLin, Hongpeng  and\nZhang, Jingwen  and\nYang, Diyi  and\nJia, Ruoxi  and\nShi, Weiyan",
        "title": "How Johnny Can Persuade {LLM}s to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing {LLM}s"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "backdoor_huang-etal-2024-composite",
        "author": "Huang, Hai  and\nZhao, Zhengyu  and\nBackes, Michael  and\nShen, Yun  and\nZhang, Yang",
        "title": "Composite Backdoor Attacks Against Large Language Models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "poison_xu-etal-2024-instructions",
        "author": "Xu, Jiashu  and\nMa, Mingyu  and\nWang, Fei  and\nXiao, Chaowei  and\nChen, Muhao",
        "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models"
      }
    ]
  }
]