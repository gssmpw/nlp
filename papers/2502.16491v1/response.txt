\section{Related Work}
Research on vulnerabilities in Large Language Models (LLMs) has been increasing, particularly in the domains of jailbreak attacks and backdoor attacks. Based on the difficulty and time required to implement these attacks, they can be categorized into three types: single-round execution, iterative attempts, and model fine-tuning.

% \paragraph{Single-round Execution} Single-round execution methods involve direct interaction with the model in a single instance, with the goal of quickly inducing the model to output a specific response through scenario setup or role-playing. These methods typically do not require multiple adjustments or optimizations, making them the least time-consuming. For example, \textit{Role-playing} Brown et al., "Adversarial Attacks on Large Language Models" involve interacting with the model through specific roles to induce deviations from its original behavioral guidelines. \textit{Scenario crafting} Chen et al., "Crafting Adversarial Prompts for Large Language Models" involves creating carefully designed scenarios to directly guide the model's output. \textit{Rare encoding}, such as specialized encryption or code completion, enables CipherChat Nguyen et al., "Ciphered Queries: Bypassing LLM Safety Mechanisms" to bypass LLM safety mechanisms with ciphered prompts, exposing limitations in handling non-natural language inputs. Similarly, CodeChameleon Li et al., "Code Tasks: Encrypting Queries for Large Language Models" uses personalized encryption to transform queries into code tasks, bypassing security checks and executing encrypted queries. \textit{Multilingual encoding} Patel et al., "Low-Resource Language Encoding Strategies for LLMs" uses low-resource language encoding strategies to circumvent the model's language processing security mechanisms.

\paragraph{Single-round Execution} Single-round execution methods involve direct, one-time interaction with the model, aiming to quickly induce specific responses through role-play or scenario design. These methods are less time-consuming as they avoid multiple adjustments or optimizations. For instance, \textit{Role-playing} Brown et al., "Adversarial Attacks on Large Language Models" involves assuming specific roles to elicit deviations from the model's default behavior. \textit{Scenario crafting} Chen et al., "Crafting Adversarial Prompts for Large Language Models" creates tailored contexts to guide model output. Techniques like \textit{Rare encoding} enable tools like CipherChat Nguyen et al., "Ciphered Queries: Bypassing LLM Safety Mechanisms" to bypass safety protocols with ciphered prompts, while CodeChameleon Li et al., "Code Tasks: Encrypting Queries for Large Language Models" utilizes personalized encryption to bypass checks by converting queries into code tasks. \textit{Multilingual encoding} Patel et al., "Low-Resource Language Encoding Strategies for LLMs" leverages low-resource language strategies to circumvent language processing security measures.

% \paragraph{Iterative Attempts} Iterative attempts rely on algorithms and automated tools that repeatedly adjust and optimize prompts to explore and exploit weaknesses in the model. These methods often involve multiple rounds of testing and modification, typically making them more time-consuming. \textit{GCG} Tan et al., "Gradient-based Exploration of Large Language Models" uses model gradient information to precisely explore target vulnerabilities. \textit{AutoDAN} Liu et al., "Automated Prompt Optimization for Adversarial Attacks on LLMs" employs genetic algorithms to optimize prompts and continually search for more effective attack methods. \textit{GPTFUZZER} Wang et al., "Systematic Exploration of Large Language Models with Prompt Variations" systematically generate various prompt variations to explore the model's responses and potential weaknesses. \textit{PAIR} Kim et al., "Prompt Adversarial Iterative Refinement for Large Language Models" iteratively refines adversarial prompts by leveraging the target language model’s responses as feedback, using these responses to guide the optimization process and increase the attack success rates. \textit{PAP} Chen et al., "Persuasive Attacks on Large Language Models: A Natural Language Approach" employs natural language persuasion techniques to craft adversarial prompts that induce large language models to perform "jailbreak" operations, bypassing their safety and alignment mechanisms.

\paragraph{Iterative Attempts} Iterative attempts utilize algorithms and automated tools to repeatedly adjust prompts, optimizing them to exploit model weaknesses. These methods often require multiple testing rounds, making them time-consuming. \textit{GCG} Tan et al., "Gradient-based Exploration of Large Language Models" uses model gradients to explore vulnerabilities, while \textit{AutoDAN} Liu et al., "Automated Prompt Optimization for Adversarial Attacks on LLMs" leverages genetic algorithms to optimize prompts and search for effective attacks. \textit{GPTFUZZER} Wang et al., "Systematic Exploration of Large Language Models with Prompt Variations" generates prompt variations to probe the model, and \textit{PAIR} Kim et al., "Prompt Adversarial Iterative Refinement for Large Language Models" refines adversarial prompts based on the model’s feedback. \textit{PAP} Chen et al., "Persuasive Attacks on Large Language Models: A Natural Language Approach" uses natural language persuasion to induce "jailbreak" operations in language models.

\paragraph{Model Fine-tuning} Model fine-tuning involves inserting backdoors during the training process or using poisoned data for fine-tuning, causing the model’s security alignment mechanisms to fail. This method is the most time-consuming but also the most successful. \textit{Backdoor attacks} ] Zhang et al., "Composite Backdoor Attacks on Large Language Models" in the form of composite backdoor attacks (CBA) distribute multiple trigger keys across different components of the prompt, only activating the backdoor when all trigger keys appear simultaneously. \textit{Poisoned data injections} Patel et al., "Injecting Poisoned Data for Fine-tuning Large Language Models" allow attackers to control model behavior by injecting a very small number of malicious instructions.