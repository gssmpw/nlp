\section{Results}
\label{results}

In this section, we only present the aggregate performance statistics of \methodname and \methodnamepfn compared to baselines for brevity and straightforward comparison. Please see Appendix \ref{full_results} for detailed results for each combination of models and datasets. We calculate the rank and z-score between the three methods for each dataset at each train sample size based on AUC. Then, we average the rank and z-score across datasets of a given sample size to illustrate the variation in relative performance between the three methods across training sample sizes.

Average rank is an intuitive metric that is common in the tabular domain. Naturally, a lower value on this metric is better. However, it is a coarse metric because some models may obtain similar performance across all datasets and yet have very different average ranks. On the other hand, averaging AUC across datasets conveys the magnitudes by which one model outperforms another, but this metric can be dominated by a small number of datasets where the performance across models has a high variance. The average z-score metric described below mitigates this effect. 

The z-score for a model on a single dataset is calculated as $z = \frac{a-\mu}{\sigma}$, the number of standard deviations a model's performance is away from the mean computed across all methods considered in that experience. A negative z-score implies that the given method's performance is below the mean of all methods. A higher positive z-score implies better performance. We then average a single model's z-scores over all datasets and obtain an average z-score for that model.
We include average rank results in Appendix \ref{summary_results}, and we instead focus on average AUC and average z-score in the main body.

Further, in our experiments comparing \methodname and \methodnamepfn with TabLLM \citep{hegselmann2023tabllm} and AutoGluon \citep{agtabular}, we find our methods perform better in a majority of the sample sizes tested. Also, note that TabLLM requires fine-tuning a large language model, whereas our method can even be run on CPU only and requires no fine-tuning.  We include these results in Appendices \ref{tabllm} and \ref{autogluon}, respectively.

\textit{Note:} Some of the datasets we use for our experiments have less than 250 samples. Therefore, results we show on dataset sizes larger than 250 are for a subset of these datasets, in some cases leading to curves that appear non-monotone. A detailed list of experiments for each dataset and size are included in the Appendix \ref{full_results}. 


\subsection{\methodnamepfn}

Figure \ref{fig:pfn} showcases the exceptionally strong performance of \methodnamepfn, which combines XGBoost with TabPFN, across all sample sizes. We obtain average z-score, rank, and AUC by taking the mean of the row-wise z-score, rank, and AUC across all datasets at each sample size shown in Table \ref{tab:xgb-tabpfn_full}, where each experiment (row) is averaged over 5 seeds and the standard errors are displayed. For the full-dataset experiments where the training set size is greater than 1000, we randomly select 1000 samples to input into TabPFN, which is the standard procedure from the original work. Further details are found in Appendix \ref{full_results}. As stated in Section \ref{experiments}, the model selection baseline is based on the validation performance of each model. All final XGBoost, stacking, and \methodnamepfn results are obtained after HPO.

 

\begin{figure}
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{data/pfn_z.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{data/pfn_auc.pdf}
    \end{minipage}
    \caption{\textbf{\methodnamepfn, combining TabPFN and XGBoost outperforms ensemble baselines and standalone models across dataset sizes.} Left: Average Z-score based on AUC performance across dataset sizes for \methodnamepfn and other ensemble baselines. Right: Average AUC across dataset sizes.}
    \label{fig:pfn}
\end{figure}



\subsection{\methodname}

Figure \ref{fig:qwen} showcases the performance of \methodname combining XGBoost with Qwen-2.5-72B-Instruct, utilizing average z-score and AUC for all datasets at each sample size.

\begin{figure}
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{data/qwen_z.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{data/qwen_auc.pdf}
    \end{minipage}
    \caption{\textbf{\methodname, combining Qwen-2.5-72B-Instruct and XGBoost, outperforms ensemble baselines and the standalone constituent models across small dataset sizes.} Left: Average z-score based on AUC performance across dataset sizes for \methodname and other ensemble baselines. Right: AUC performance across dataset sizes. \textit{Important Note:} For this experiment, we always compute the LLM scores using a 3-shot prompt.  Therefore, the LLM performance remains constant throughout all trainset sizes where the extra data is only used for GBDT training. The trough in LLM performance in the 100-500 trainset range is due to us using only a subset of datasets which have sufficient training samples, for these data points.}
    \label{fig:qwen}
\end{figure}

Our full results are given in Table \ref{tab:xgb-qwen_full} where each experiment is averaged over 5 seeds. Additionally, we conduct ablation experiments to determine the variation of \methodname performance with LLM size and the number of few shot examples in Appendix \ref{llm_ablations}.

\subsection{\methodnamepfn vs \methodname}

Figure \ref{fig:pfn_compare} presents a direct AUC comparison between boosted and non-boosted Qwen-2.5-72B-Instruct and TabPFN. Our experiments makes it clear that \methodnamepfn yields better results than \methodname except in the smallest dataset sizes. This result is expected as TabPFN is a much stronger standalone model that can use up to 1000 in-context examples, while the LLM can only use far fewer. Although using a stronger or fine-tuned LLM might result in better performance, we conclude that \methodnamepfn is better suited in instances where data sample size is not severely limited.

\begin{figure}[h]
    \centering
    {{\includegraphics[width=0.6\linewidth]{data/pfn_compare_new.pdf}}}%
    \caption{\textbf{\methodnamepfn outperforms \methodname on larger datasets.} Direct comparison of \methodname with XGB+Qwen-2.5-72B-Instruct and \methodnamepfn with XGB+TabPFN. We observe from this comparison that boosted TabPFN results are better except for on small dataset sizes. This is as expected as TabPFN itself is far superior to the standalone LLM on average. However, improved LLMs may in turn improve \methodname.} %
    \label{fig:pfn_compare}%
\end{figure}


\subsection{Other boosting combinations}
We perform further LLM-Boost experiments with XGBoost+Flan-T5-XXL, XGBoost+Llama-3-8B-Instruct and LightGBM+Flan-T5-XXL, in order to study sensitivity of \methodname to the choice of language model and GBDT algorithm. These results can be found in Figures \ref{fig:flan_ap}, \ref{fig:llama}, and \ref{fig:lgbm}, respectively. Although \methodname still outperforms baselines when used with smaller language models, we find it more difficult to design prompts that consistently yield the class label exactly, especially for Llama-3-8B. Therefore, the performance of the Llama-3 model is comparatively lower, leading to lower \methodname performance gains as well. The LightGBM experiments yield superior results compared to baselines in the small dataset sizes. However, the performance gain for \methodname is not as pronounced compared to the XGBoost experiments. 

\subsection{Ablating the value of column headers by shuffling them}
LLMs perform well in the few-shot tabular setting as they are able to make use of the column headers (column names), which are valuable metadata that traditional tabular models cannot parse. To investigate the importance of meaningful column headers for LLM-Boost, we conduct an experiment where we randomly shuffle the column headers between columns and compare performance degradation. Once the column headers are shuffled, all semantic meaning of a column disappears because it is no longer corresponding to the appropriate value. We conduct this experiment on the Adult dataset and we provide our boosted/standalone performance for both shuffled and direct column headers in Figure \ref{fig:shuffled}.  We see there that the column headers are especially useful when the dataset size is small, yet the LLM provides an advantage over XGBoost alone for very small dataset sizes, even when the column headers are shuffled. This could be due to the LLM inferring the column headers from the data itself, which is especially straightforward for categorical columns. As the dataset size grows, eventually all models perform comparably well.

\begin{figure}[h]
    \centering
    {{\includegraphics[width=0.6\linewidth]{data/shuffling_new.pdf}}}%
    \caption{\textbf{\methodname benefits from meaningful column headers.} To emulate datasets where column headers contain little or no semantic meaning we shuffle the column headers of the Adult Income Dataset and run LLM-Boost. We see that even with the shuffled column headers, the LLM does provide some performance improvement over XGBoost when the datasize is small. However, \methodname with meaningful column headers performs noticeably better.} %
    \label{fig:shuffled}
\end{figure}


