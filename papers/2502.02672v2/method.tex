\section{Method}
\label{method}

In this section, we detail our boosting framework. We depict the \methodname algorithm in Figure~\ref{fig:cat_dog} as \methodnamepfn consists of the same steps. Broadly, \methodname and \methodnamepfn first take a tabular dataset and extract transformer model scores, or logits, for each row of the table. We then augment a GBDT model by seeding it with the transformer's logits so that it learns the residuals. When we perform this procedure, we must carefully tune a scaling parameter so that the GBDT is not overly reliant on these transformer predictions but simultaneously does not ignore them. Our approach is equivalent to replacing the first tree of the GBDT ensemble with the static prediction of the transformer, which need to be pre-computed only once for inference and training.
We then fit the GBDT to the residuals and evaluate the combined model's classification performance. We detail our pipeline in the following sections.

\subsection{Extracting Transformer Scores}
\label{score_extract}
In the case of \methodname the first step is to extract the LLM predictions for each row of a given tabular dataset. We create simple natural language, few-shot prompts utilizing the prompt generation and serialization tools developed by \citet{tabletSlack23}. The prompts are designed so that an instruction-tuned LLM will output one of the classification labels for each row of data. An example prompt for the UCI adult income dataset is given in Figure \ref{example_prompt}.

We take the negative of the language modelling loss (mean reduced cross-entropy) of each classification label with the language model output as the language model's un-normalized prediction \textbf{score} for that class ($\text{SCORE}_{\text{LLM}}$). Note that each classification label can contain multiple words. For example, `Greater than 50K' and `Less than or equal to 50K' for the Adult dataset. Thus, the loss calculation can be over a different number of tokens for each class, which is why we use mean reduction. The exact loss extraction process will differ slightly depending on whether it is a Masked LLM or a Causal LLM. Finally, we center these raw scores around zero by subtracting the mean of the scores.  When implementing \methodnamepfn, it is straight-forward to extract TabPFN scores as the model directly outputs un-normalized raw prediction scores.


\subsection{Boosting Transformer Scores}
Once we have the transformer (LLM or TabPFN) scores, we use them to kickstart a GBDT. GBDT algorithms sequentially construct weak decision trees, where each tree is optimized to fit the residual error of the preceding ensemble. The input to the first tree is usually a constant value for all classes \citep{chen2016xgboost}. Our approach is simply to replace this constant value with the transformer scores so that the GBDT algorithm learns the residual of the transformer prediction. This method can also be thought of as replacing the first tree in the ensemble with the transformer, during both training and inference. See Figure \ref{fig:cat_dog} for a representation of this procedure.

\begin{tcolorbox}[colback=gray!10!white, colframe=black, title=Example prompt for the adult income dataset]
 
Given information about a person, you must predict if their income exceeds \$50K/yr. Answer with one of the following: 
greater than 50K $\vert$ less than or equal to 50K.\\

\textbf{Example 1 -} \\
\textbf{workclass}: Private , \textbf{hours per week}: 20, \textbf{sex}: Male, \textbf{age}: 17, \textbf{occupation}: Other-service, \textbf{capital loss}: 0, \textbf{education}: 10th, \textbf{capital gain}: 0, \textbf{marital status}: Never-married, \textbf{relationship}: Own-child, \textbf{Answer}: less than or equal to 50K \\

\textbf{Example 2 -} ..... \\

\textbf{Workclass}: Private, \textbf{hours per week}: 40, \textbf{sex}: Female, \textbf{age}: 24, \textbf{occupation}: Sales, \textbf{capital loss}: 0, \textbf{education}: Some-college, \textbf{capital gain}: 0, \textbf{marital status}: Never-married, \textbf{relationship}: Own-child, \textbf{Answer}:
\end{tcolorbox}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{\textbf{An few-shot prompt for the UCI adult income dataset} designed to extract the LLM prediction scores required for \methodname.}
\label{example_prompt}
\end{minipage}

\subsection{The Scaling Parameter}
We use a scaling parameter $s$ to scale the transformer scores before passing them on to the GBDT algorithm. By setting the scaling parameter to zero, our method is equivalent to the standalone GBDT; by making the scaling parameter very large, our method outputs predictions arbitrarily close to those of the transformer. In our experiments, we tune this hyperparameter using Optuna \citep{optuna_2019}. We find that for intermediate values of this hyperparameter, we can often achieve performance that exceeds both the GBDT and the transformer. Refer to Appendix \ref{scaling} for an example.

The predictions of the ensemble consisting of the first $i$ trees are now 
\[ pred_{(0,i)} = pred_{(1,i)} + s*\text{SCORE}_{\text{Transformer}} + C,\]

where $pred_{(a,b)}$ is the sum of the predictions of all the  trees from $a$ to $b$; $s$ denotes the aforementioned scaling parameter that can take values $[0, \infty)$; $\text{SCORE}_{\text{Transformer}}$ denotes the raw prediction of the LLM (or TabPFN) which is a vector in the case of classification (See \ref{fig:cat_dog}); and $C$ denotes a constant which can be added to make $\text{SCORE}_{\text{Transformer}}$ centered around 0 for numerical stability. 
Each tree i is progressively optimized so that $pred_{(0,i)}$ minimized following the standard gradient boosting procedure.

