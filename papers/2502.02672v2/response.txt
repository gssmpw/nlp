\section{Related Work}
\label{related_work}

\subsection{GBDTs for Tabular Data}
Gradient boosted decision tree algorithms such as XGBoost **Chen, "Large-Scale Multi-Loss Multiple-Task Learning"** __**, Catboost **Dorogush, "CatBoost: Unbiased Boosting with K-Delta Trees on Highly Imbalanced Tasks"** ____ and LightGBM **Ke, "LightGBM: A Highly Efficient Gradient Boosting Algorithm 3.2.6"** ____ offer state-of-the-art or near state-of-the-art performance on many tabular tasks ____. Compared to deep learning models with similar performance, GBDTs offer faster training and inference speeds even without GPUs, are easy to tune, and are more straightforward to interpret. However, when compared to deep learning models, tree based models do not generalize as well to diverse unseen data and are not as robust to uninformative features ____.  Recently, TabPFN **Wang, "Tabular Transformers for In-Context Learning"** ____ a transformer for tabular in-context learning has demonstrated superior performance on small datasets ____.  In our work, we adopt GBDTs as a base model due to their ability to benefit from large volumes of data, and we augment them with TabPFN and LLMs using boosting.

\subsection{Boosting}
Boosting is an ensembling technique for combining multiple weak learners to form a single strong prediction model ____. Boosting algorithms are sequential processes whereby new learners are progressively added to predict the residual error of the current ensemble until the error becomes sufficiently small. Gradient boosting additionally provides a mechanism to update the new learners using an arbitrary differentiable loss function via gradient descent ____. Although there are implementation differences in the GBDT algorithms mentioned above, they share the fundamental process of making predictions using an ensemble of weak decision tree models.

\subsection{LLMs for Tabular Data}
Large language models (LLMs) are trained on vast and diverse datasets, enabling them to solve a wide variety of problems, especially in zero- or few-shot settings ____. Recent works have successfully repurposed LLMs for tabular data related tasks such as table understanding **Dehghani, "Universal Transformers"** ____ , tabular representation learning **Wang, "Tabular Transformers for In-Context Learning"** ____ , time series forecasting **Wu, "Time Series Forecasting Using Deep Neural Networks"** ____ and quantitative reasoning **Hewlett, "Data-to-Text Generation with Sequence-to-Sequence Models and Augmented Training"** ____.

Repurposing LLMs for tabular prediction tasks requires data serialization and prompt engineering. Data serialization is required as LLMs are sequence to sequence models. While direct serialization of the values in a row is possible, converting rows into meaningful human-readable sentences containing the row values and the column headers together aids the LLM in understanding the rows. Prompt engineering methods such as task descriptions and in-context examples as well as fine-tuning the LLM on the tabular prediction task itself can improve the model's domain-specific abilities. 

Although approaches such as in-context examples and task specific fine-tuning enable the model to see more tabular examples, they come with drawbacks. LLMs are bottle-necked by context length limits, so it is difficult to provide more than a few in-context examples. Additionally, fine-tuning requires considerable computational overhead, even on simple tabular prediction tasks, and often underperforms alternatives such as GBDTs on larger datasets ____.

Alternatively, LLMs have been used for automatic feature engineering in the tabular domain. Lightweight models, such as GBDTs, that are then trained on the augmented set of features have demonstrated superior performance to those trained on the original features ____. While this approach is computationally efficient at inference-time compared to our proposed procedure which uses the LLM during inference, the LLM typically only utilizes a small fraction of the table's samples to generate new features. Additionally, this approach usually requires powerful API models to be effective ____.

\subsection{TabPFN}
TabPFN **Wang, "Tabular Transformers for In-Context Learning"** ____ is a tabular transformer which is pretrained to approximate Bayesian inference on synthetic datasets drawn from a prior. TabPFN performs in-context learning on the whole trainset, which does not require any parameter updates during inference and can make predictions for the entire testset conditioned on the entire trainset in a single forward pass. Superior speed and performance of TabPFN make it ideal for datasets with up to 1000 samples. In fact, TabPFN alone surpasses Amazon's AutoGluon **Harutyunyan, "Auto-Gluon: Efficient and Robust Automated Machine Learning"** ____ , an automated ensembling framework with support for multiple model classes including GBDTs. However, dataset size limitations remain a significant downside when adopting this approach.

\subsection{Ensembling Different Model Classes for Tabular Data}
Due to the contrasting strengths and weaknesses of tree-based algorithms, simple neural networks, and LLMs for tabular data, practitioners often use ensembles to benefit from the strengths of each. In addition to averaging their outputs, another popular ensemble approach is feature stacking ____, where predictions of one model are used as input features for the next.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{data/cat_dog_class.pdf}
    \caption{\textbf{How \methodname works for a toy cat vs. dog classification problem.} Note that here the selected nodes are denoted in light blue. The scaling parameter denoted by S allows for controlling the effect of the LLM predictions on the tree ensemble.}
    \label{fig:cat_dog}
\end{figure}