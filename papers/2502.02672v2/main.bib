@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{hegselmann2023tabllm,
  title={Tabllm: Few-shot classification of tabular data with large language models},
  author={Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5549--5581},
  year={2023},
  organization={PMLR}
}

@article{mcelfresh2024neural,
  title={When do neural nets outperform boosted trees on tabular data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{borisov2022deep,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{tabletSlack23,
title={TABLET: Learning From Instructions For Tabular Data},
author={Dylan Slack and Sameer Singh},
year={2023},
journal={arXiv},
}

@inproceedings{
grinsztajn2022why,
title={Why do tree-based models still outperform deep learning on typical tabular data?},
author={Leo Grinsztajn and Edouard Oyallon and Gael Varoquaux},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=Fp7__phQszn}
}

@article{10.1214/aos/1013203451,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
}

@article{roberts2022t5x,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  journal={arXiv preprint arXiv:2203.17189},
  year = {2022},
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@misc{flant5,
  doi = {10.48550/ARXIV.2210.11416},
  url = {https://arxiv.org/abs/2210.11416},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Instruction-Finetuned Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{
  hollmann2023tabpfn,
  title={Tab{PFN}: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Noah Hollmann and Samuel M{\"u}ller and Katharina Eggensperger and Frank Hutter},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=cp5PvcI6w8_}
}

@article{tap4llmsui,
  publtype={informal},
  author={Yuan Sui and Jiaru Zou and Mengyu Zhou and Xinyi He and Lun Du and Shi Han and Dongmei Zhang},
  title={TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
  year={2023},
  cdate={1672531200000},
  journal={CoRR},
  volume={abs/2312.09039},
  url={https://doi.org/10.48550/arXiv.2312.09039}
}

@inproceedings{
gruver2023large,
title={Large Language Models Are Zero-Shot Time Series Forecasters},
author={Nate Gruver and Marc Anton Finzi and Shikai Qiu and Andrew Gordon Wilson},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=md68e8iZK1}
}



@inproceedings{fewshotreason,
  title={Large Language Models are few (1)-shot Table Reasoners},
  author={Chen, Wenhu},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
  pages={1120--1130},
  year={2023}
}

@inproceedings{
levin2023transfer,
title={Transfer Learning with Deep Tabular Models},
author={Roman Levin and Valeriia Cherepanova and Avi Schwarzschild and Arpit Bansal and C. Bayan Bruss and Tom Goldstein and Andrew Gordon Wilson and Micah Goldblum},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=b0RuGUYo8pA}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@article{OpenML2013,
author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
title = {OpenML: Networked Science in Machine Learning},
journal = {SIGKDD Explorations},
volume = {15},
number = {2},
year = {2013},
pages = {49--60},
url = {http://doi.acm.org/10.1145/2641190.2641198},
doi = {10.1145/2641190.2641198},
publisher = {ACM},
address = {New York, NY, USA},
}

@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}

@misc{Dua:2019,
  author = {Dua, Dheeru and Graff, Casey},
  year = {2017},
  title = {UCI Machine Learning Repository},
  url = {http://archive.ics.uci.edu/ml},
  institution = {University of California, Irvine, School of Information and Computer Sciences}
}

@article{somepalli2021saint,
  title={SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

@inproceedings{
gorishniy2021revisiting,
title={Revisiting Deep Learning Models for Tabular Data},
author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=i_Q1yrOegLY}
}

@article{Popov2019NeuralOD,
  title={Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
  author={Sergei Popov and Stanislav Morozov and Artem Babenko},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.06312},
  url={https://api.semanticscholar.org/CorpusID:202573030}
}

@article{tabnet,
  author       = {Sercan {\"{O}}mer Arik and
                  Tomas Pfister},
  title        = {TabNet: Attentive Interpretable Tabular Learning},
  journal      = {CoRR},
  volume       = {abs/1908.07442},
  year         = {2019},
  url          = {http://arxiv.org/abs/1908.07442},
  eprinttype    = {arXiv},
  eprint       = {1908.07442},
  timestamp    = {Mon, 26 Aug 2019 13:20:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1908-07442.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
dinh2022lift,
title={{LIFT}: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks},
author={Tuan Dinh and Yuchen Zeng and Ruisu Zhang and Ziqian Lin and Michael Gira and Shashank Rajput and Jy-yong Sohn and Dimitris Papailiopoulos and Kangwook Lee},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=s_PJMEGIUfa}
}

@article{agtabular,
  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}

@inproceedings{NEURIPS2023_hytrel,
 author = {Chen, Pei and Sarkar, Soumajyoti and Lausen, Leonard and Srinivasan, Balasubramaniam and Zha, Sheng and Huang, Ruihong and Karypis, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {32173--32193},
 publisher = {Curran Associates, Inc.},
 title = {HyTrel: Hypergraph-enhanced  Tabular Data Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/66178beae8f12fcd48699de95acc1152-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{iida-etal-2021-tabbie,
    title = "{TABBIE}: Pretrained Representations of Tabular Data",
    author = "Iida, Hiroshi  and
      Thai, Dung  and
      Manjunatha, Varun  and
      Iyyer, Mohit",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.270",
    doi = "10.18653/v1/2021.naacl-main.270",
    pages = "3446--3456",
    abstract = "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.",
}

@inproceedings{NEURIPS2023_caafe,
 author = {Hollmann, Noah and M\"{u}ller, Samuel and Hutter, Frank},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {44753--44775},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8c2df4c35cdbee764ebb9e9d0acd5197-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{nam2024optimizedfeaturegenerationtabular,
      title={Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning}, 
      author={Jaehyun Nam and Kyuyoung Kim and Seunghyuk Oh and Jihoon Tack and Jaehyung Kim and Jinwoo Shin},
      year={2024},
      eprint={2406.08527},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08527}, 
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
      journal={arXiv preprint arXiv:2407.10671},
      year={2024}
}

@article{erickson2020autogluon,
  title={Autogluon-tabular: Robust and accurate automl for structured data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}