\section{Discussion and Conclusion}
\label{discussion}

In this paper, we show how to combine the benefits of powerful tabular transformer models and GDBTs for data-efficient predictions on tabular datasets. \methodname and \methodnamepfn outperform the transformers and GDBTs individually as well as ensemble baselines, adaptively focusing on the strengths of each method.  In this final section, we close by discussing the limitations of our work as well as promising directions for future research.

\textbf{When to use \methodname?} \methodname showcases state-of-the-art performance on classification datasets with upto 100 samples. In order to optimally benefit from fusing a language model to decision trees, we need semantically meaningful column headers.  When such column headers are unavailable or the dataset size is larger, our variant \methodnamepfn which combines TabPFN with GBDTs is highly effective.

\textbf{Limitations.} While we present promising performance on a slice of tabular datasets, we enumerate several limitations:
\begin{itemize}
\item The biggest drawback that restricts \methodname is the necessity for interpretable text descriptors as column headers, namely column headers from which LLMs can extract meaning.  Accordingly, some datasets may require prompt engineering.
\item Language models are big in parameter count and slow, and they require GPUs for large-scale use.  During training, LLM outputs can be pre-computed and re-used across all GBDT training runs with various hyperparameter configurations.  After this one-time cost, training is no more expensive than GBDT training.  For very large datasets, pre-computing LLM outputs may become a non-trivial cost. However, the costs of TabPFN are negligible compared to LLM costs.
\item Common GBDT libraries are implemented in C++, accompanied by APIs in other languages such as Python.  Maintaining high-speed training while simultaneously modifying the code for \methodname and \methodnamepfn may require implementing it in the original C++.
\end{itemize} 

\textbf{Future work.}  We finally present several promising directions for future research:
\begin{itemize}
\item Data scientists interact with tabular datasets, analyzing variable names, for example to engineer new features, and employing tools such as gradient-boosted decision trees.  Our work is a first step towards automating this predictive modeling pipeline.  A next step is to expand the capabilities of LLMs, for a full stack of data science functionality such as data visualization, hypothesis testing, and even suggesting valuable features to use.
\item We only use three-shot prompting for the language model, but long-context methods may unlock the ability to feed far more training samples into the LLM.  This possibility raises the question, will we still need XGBoost as LLMs gain the capability to ingest more data?
\item Our boosting mechanism is model agnostic and may be expanded with other high performing tabular architectures such as Tabnet \citep{tabnet} SAINT \citep{somepalli2021saint}, NODE \citep{Popov2019NeuralOD} and FT-Transformer \citep{gorishniy2021revisiting} in addition to LLMs.
\end{itemize} 
