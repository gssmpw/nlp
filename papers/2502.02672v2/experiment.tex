\section{Experiments and Ablations}
\label{experiments}

Our primary experiments focus on boosting XGBoost~\citep{chen2016xgboost} with TabPFN ~\citep{hollmann2023tabpfn} and Qwen-2.5-72B-Instruct ~\citep{qwen2.5} predictions. Qwen-2.5-72B-Instruct is a powerful and recent open source LLM with instruction tuning.

We additionally perform ablations on different GBDT and LLM model combinations as our boosting framework is agnostic with respect to both the precise LLM and GBDT.  In addition to Qwen-2.5-72B-Instruct, we also include Flan-T5-XXL \citep{flant5} with approximately 11 billion parameters and the 8 billion parameter Llama-3-8B-Instruct \citep{llama3modelcard} model as a drop-in replacement. We show that \methodname outperforms baselines even when seeded with these smaller LLMs.
Together, we conduct ablations including the GBDTs XGBoost~\citep{chen2016xgboost} and LightGBM~\citep{ke2017lightgbm}, and including seeding mechanisms Qwen-2.5, Flan-T5-XXL, Llama-3-8B, and TabPFN.

\subsection{Datasets and Data Preparation}

For our experiments, we adopt the UCI \citep{Dua:2019} datasets used by \citet{tabletSlack23} together with the public tabular datasets used by \citet{hegselmann2023tabllm} (TabLLM). We filter out the datasets which have more than 5 classes from the UCI datasets as few shot LLM performance is generally poor when the number of classes are high. The final 16 datasets used after filtering are listed in Table \ref{tab:datasets}. As described in section \ref{score_extract} We prepare the data for few-shot (in-context) inference utilizing the tools developed by \citet{tabletSlack23}. We sub-sample our datasets to much smaller sizes so that we have sufficient granularity to bridge the few-shot regime where LLMs/TabPFN excel at and the large dataset regime where GBDTs are better. We chose the sample sizes 10, 25, 50, 100, 200 and 500 for applicable datasets in addition to running the experiments on the full dataset.

\begin{table}[H]
\centering
\caption{\textbf{The 16 datasets used in our experiments}}
\label{tab:datasets}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
UCI                                  & TabLLM           \\
\midrule
Abalone                              & Bank      \\
Adult (Also used for TabLLM)               & Blood  \\
Breast Cancer Wisconsin - Diagnostic & California \\
Churn                                & Car                      \\
Heart Disease                        & Credit-g                      \\
Shark Tank                           & Diabetes                      \\
Statlog - Australian Credit Approval & Heart                      \\
Wine                                 & Jungle                      \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Hyperparameter Optimization}
One of the benefits of using GBDT based methods is the ability to perform many rounds of hyperparameter optimization (HPO) with a low computation budget. HPO is well-known to increase the performance of tabular models \citep{gorishniy2021revisiting} and is often included as part of the GBDT pipeline. We perform HPO using Optuna \citep{optuna_2019}. We use separate validation folds so that test data is new used for HPO trials. The hyperparameter search spaces used for our GBDT experiments are listed in Appendix \ref{hyper-parameter}.

For best results, we find that the scaling hyperparameter $s$ should be independently tuned after tuning the standard GBDT hyperparameters. We find that this makes the tuning process more stable and guarantees improvement in validation loss when including scaling. We tune the GBDT hyperparameters for 100 Optuna trials and tune the scaling parameter for an additional 30 trials. Importantly, for our other ensembling baselines described in Section \ref{baselines}, we tune the GBDT hyperparameters for 130 Optuna trials to keep the total HPO trials consistent.


\subsection{Compute Resources}
\label{compute}
A major advantage of \methodname is its lightweight overhead. The computational resources required for our boosting process, disregarding LLM inference costs, is the same as that required for running HPO on GBDTs. Specifically, the boosting process can be performed on CPU. Full hyperparameter tuning only takes up to 4 hours for the largest datasets on CPU.  For few-shot LLM inference (Flan-T5-XXL and Llama-3-8B-Instruct) we use 4 RTX A4000 GPUs. Inference on the largest datasets we tested takes up to 18 hours to precompute. Importantly, this significantly less resource intensive compared to supervised fine-tuning of LLMs for tabular tasks.  Further, \methodnamepfn is very lightweight as TabPFN inference itself is very fast, even on CPU. As a result, the full \methodnamepfn pipeline is only slightly slower than running HPO for the standalone GBDT itself. 

\subsection{Baselines}
\label{baselines}
To validate our method, we first consider selecting the raw {\bf transformer (LLM/TabPFN)} and {\bf GBDT} models as baselines. However, on average \methodname and \methodnamepfn performs much better than either the GBDT or the transformer model alone. Therefore, we utilize two strong and widely-used ensembling baselines and compare our approach against them. The first baseline is {\bf Selection}, i.e., selecting the best performing model out of the GBDT and transformer based on validation performance. The other is {\bf Stacking}, i.e., appending transformer scores as additional features for GBDTs. Additionally, we use TabLLM \citep{hegselmann2023tabllm}, a LLM finetuning framework, as a baseline for LLM-based tabular predictors and Amazon's AutoGluon \citep{agtabular}, a widely-used lightweight ensembling algorithm, as a baseline for tabular ensemble methods.

