\section{Introduction}

Tabular data, or spreadsheets, constitute a large portion of real-world machine learning problems \citep{borisov2022deep}.  Tabular data comprise (a) columns, each containing a different feature or label; (b) rows, each containing an individual data sample; and (c) column headers describing the content of each column, often in the form of text.

Gradient-boosted decision trees (GBDTs), such as XGBoost \citep{chen2016xgboost}, LightGBM \citep{ke2017lightgbm}, and CatBoost \citep{prokhorenkova2018catboost}, have remained the de facto machine learning algorithms for analyzing tabular data over the past decade \citep{mcelfresh2024neural}.  They are efficient to train even on a CPU, they achieve competitive performance on a wide variety of datasets and sample sizes, and they can be deployed via user-friendly packages accessible to non-experts.  However, gradient-boosted decision trees have two major drawbacks:  (a) They only ingest the row features in a table and not the column headers, which may contain useful text descriptions.  For example, one may not need training data to anticipate that a hospital patientâ€™s weight is useful for predicting occurrences of heart disease.  Instead of leveraging column headers, from which a human might intuit relationships between columns, GBDTs have to learn these relationships from scratch from the feature values themselves.  (b) GBDTs are trained from scratch on each dataset, instead of benefiting from vast prior experience on other datasets, a staple of foundation models.

In contrast to gradient-boosted decision trees, large language models (LLMs) can parse and extract meaning from column headers, enabling them to achieve performance superior to GBDTs on very small tabular datasets with interpretable headers \citep{hegselmann2023tabllm}.  LLMs can even make accurate zero-shot predictions solely by applying natural language understanding to column headers without in-context training samples at all \citep{hegselmann2023tabllm}.  Despite their ability to parse column headers, LLMs are severely limited by their limited context length and high fine-tuning costs. 

TabPFN \citep{hollmann2023tabpfn} is a tabular transformer, pretrained on a vast number of synthetic tables, that can simultaneously perform in-context learning on an entire trainset and make predictions for the entire testset all in a single forward pass. Similarly to LLMs, TabPFN performance is very strong on small datasets, but it suffers from context-length limitations and can only handle datasets with up to 1000 samples. Therefore, LLMs and TabPFN cannot easily make use of large sample sizes, whereas GBDTs scale well to massive datasets.

In this paper, we combine the strengths of gradient-boosted decision trees and recent transformers to build models that simultaneously benefit from pretraining and textual column headers while scaling to much larger tabular datasets than LLMs and TabPFN could alone.  Our method \methodname, uses LLM predictions as a starting point for GBDTs, and then learns the residuals from the LLM predictions to the label.  This technique allows us to not only use the column headers for a strong prior but also benefits from the inductive bias and scalability of decision tree algorithms. In our experiments, \methodname showcases state-of-the-art performance, outcompeting strong baselines including both single models and other ensemble approaches, across a large range of dataset sizes.  \methodname excels at small and medium sized datasets that are too large for LLMs yet not large enough that column headers are not beneficial.  Motivated by the strong performance of TabPFN, we apply the same boosting approach swapping out LLMs for TabPFN. Importantly, we find that our boosted TabPFN combination, \methodnamepfn, achieves the top performance among all methods we consider outside of the very small dataset regime where our boosted LLMs reign supreme.  We summarize our contributions as follows.
\begin{itemize}[topsep=2pt, itemsep=2pt, parsep=0pt, leftmargin=5mm]
\item We propose \methodname: a novel yet simple and easy-to-implement boosting mechanism that combines LLMs, which ingest semantic column headers, with GBDTs that can scale to massive datasets.
\item We further propose \methodnamepfn, where we instead fuse TabPFN and GBDTs for performance gains over GBDTs alone across dataset sizes without using column headers.
\item We conduct thorough experiments across numerous datasets and sample sizes, comparing to strong baselines. \methodname and \methodnamepfn demonstrate consistently strong performance.
\end{itemize}