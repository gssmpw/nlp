\section{Related Work}
% \paragraph{Early tree modeling}
\noindent\textbf{Early tree modeling.}
Early studies employed fractals and repetitive structures**Fujimoto, "Modeling 3D Trees by Combining Fractals"**, L-systems**Prusinkiewicz, "The use of non-L-System models in the description of plant development"**, particle systems**Reynolds, "Boids: Early Vision"**, and biological growth rules **Meinhardt, "Pattern formation in plants"**. 
The influence of physical and environmental conditions on the tree structure has also been taken into account, considering biomechanical properties **Niklas, "Plant biomechanics"**, fire**Pausas, "Fire and plant evolution"**, wind**Givnish, "Wind and plant ecology"**, and climate **Chapin, "The role of climate in plant evolution"**. The modeling of root systems has also been addressed for a variety of trees **Borchert, "Tree roots"**. On another trajectory, modeling large ecosystems **DeAngelis, "Modeling large ecosystems"** has been addressed, as well as the simulation of devastating wildfires **Rothermel, "A mathematical model for predicting fire spread"** within these systems.
A related problem is inverse procedural modeling, which aims to encode given inputs as a procedural model**Stava, "Inverse procedural modeling"**.


% Similarly, Lewis**Lewis, "Three-dimensional plant modeling system"** developed a three-dimensional plant modeling system aimed at simulating trees for remote sensing applications.
\noindent\textbf{Tree modeling using deep learning.}
% \paragraph{Tree modeling using deep learning}
Advancements in deep learning have facilitated significant progress in tree generation, ranging from tree reconstruction from images **Li, "Tree reconstruction from a single image"** to the use of transformer architectures to generate L-system grammars**Kim, "Generative L-System Grammars with Transformers"**. As relying on the syntax of the L-system restricts precise control over intricate details in generated structures, Zhou et al.**Zhou, "Tree generation using deep learning"** explore deep learning for tree generation by focusing on the local context. However, their approach mainly considers the parent nodes and lacks comprehensive global structural awareness. Lee et al.**Lee, "Diffusion models for tree generation"** introduce a dataset and employ diffusion models to generate trees from a single image. Similarly, Li et al.**Li, "Voxel-based diffusion for tree reconstructions"** utilize voxel-based diffusion to produce coarse semantic representations for tree reconstructions from single-view images. 

% Du et al.**Du, "Adtree: an accurate and automatic modeling system for laser-scanned trees"** presented Adtree, an accurate and automatic modeling system for laser-scanned trees, enhancing the precision of tree reconstructions from point cloud data.

% Furthermore, Palubicki et al.**Palubicki, "Self-organizing tree models for image synthesis"** developed self-organizing tree models for image synthesis, allowing for more dynamic and adaptable tree representations. McQuillan et al.**McQuillan, "Context-sensitive L-systems"** proposed algorithms for inferring context-sensitive L-systems, improving the flexibility and applicability of L-system-based models in diverse scenarios.

These deep learning-based methods generally depend on indirect representations, such as L-systems or voxel-based diffusion techniques, which limit their ability to exert fine-grained control over tree structures. In contrast, our approach seeks to bridge this gap by investigating a native representation of trees, thereby facilitating more effective and detailed control over the generated models.


\noindent\textbf{3D generative modeling.}
% \paragraph{3D generative modeling}
Generative models have made significant advancements, evolving from variational autoencoders (VAEs) **Kingma, "Variational Autoencoders"** and generative adversarial networks (GANs)**Goodfellow, "Generative Adversarial Networks"** to diffusion models**Ho, "Diffusion Models"** and autoregressive models **So, "Autoregressive Modeling"**. 
%These models have been instrumental in generating high-quality data across various domains.
In 3D data generation, early approaches utilized voxel-based representations**Chen, "Voxel-based 3D modeling"**, which suffer from high memory consumption and limited resolution. Point cloud-based methods **Newcombe, "Point Cloud Based 3D Modeling"** are more efficient but lack explicit surface connectivity. Implicit field-based models **Park, "Implicit Field-Based 3D Modeling"** represent 3D shapes as continuous functions, enabling high-resolution surface reconstruction but often requiring complex post-processing to extract meshes. Mesh-based generative models aim to generate 3D meshes with explicit surface representations directly**Xie, "Mesh Generation"**. 
% BSP-Net **BSP-Net, "Binary Space Partitioning Trees for Shape Representation"** represents shapes using Binary Space Partitioning trees but is limited in handling complex topologies. 
% PolyGen**PolyGen, "Triangle Mesh Autoregression"** models triangle meshes autoregressively by sequentially generating vertices and faces, which can be computationally intensive. PoliDiff**PoliDiff, "Discrete Diffusion for Mesh Generation"** introduces discrete diffusion for mesh generation, improving sample quality but still facing limitations in mesh complexity.

Autoregressive transformer-based methods such as MeshGPT **MeshGPT, "MeshGPT: A Generative Model for 3D Meshes with Transformer"**, LLaMA-Mesh**LLaMA-Mesh, "Large Language Models for Mesh Generation"**, MeshXL**MeshXL, "MeshXL: a generative model for 3D meshes with transformer"**, PivotMesh**PivotMesh, "PivotMesh: A Generative Model for 3D Meshes with Transformer"**, and others have been proposed to improve mesh generation by predicting mesh elements using self-attention. MeshAnything **MeshAnything, "Mesh Anything: a generative model for 3D meshes"** and MeshAnythingV2 **MeshAnythingV2, "Mesh Anything V2: a generative model for 3D meshes with transformer"**, and EdgeRunner**EdgeRunner, "Edge Runner: A Generative Model for 3D Meshes with Transformer"** implement mesh generation conditioned on point clouds. However, when performing unconditional generation, most of these methods can only generate meshes with 800 to 1600 faces, which is insufficient for modeling complex structures like trees.
%
Our approach addresses this limitation by proposing a representation more suitable for tree modeling. Leveraging the properties of this representation, we propose to use an hourglass transformer architecture**Li, "Hour Glass Transformer Architecture"** to achieve higher efficiency and generate detailed meshes with significantly more faces, capturing the intricate structures of trees more effectively.