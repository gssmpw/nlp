\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{IEEEtrantools}

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
% 设置插入网址的包
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=green, linkcolor=red, urlcolor=green]{hyperref}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{makecell}   % 支持 makecell
\usepackage{graphicx}   % 支持 resizebox
% \usepackage{ulem}  % 引入 ulem 包

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}



\begin{document}
% \title{A Weakly Supervised Network based on Multi-level Information and Multi-scale Constraints for Thyroid Nodule Ultrasound Images Segmentation}
% \title{MLMC: Multi-Level Information Extraction and Multi-Scale Constraints for Weakly Supervised Segmentation of Medical Images}
% \title{MLMC: Enhanced Weakly Supervised Segmentation of Medical Images through Multi-Level Information Extraction and Multi-Scale Constraints}
% \title{MLMC: A Novel Approach for Weakly Supervised Medical Image Segmentation Using Multi-Level Information Extraction and Multi-Scale Constraints}
% \title{MLMC: A Novel Approach for Weakly Supervised Medical Image Segmentation Using Multi-Level Information Extraction and Multi-Scale Constraints}MLMC: Weakly Supervised Medical Image Segmentation with Multi-Level Information and Multi-Scale Constraints

% \title{Enhanced Multi-Level Weakly Supervised Segmentation of Medical Images via Instance Prototype Consistency and Cross-Data Contrastive Learning}

% \title{MLMC: Enhanced Multi-Level Weakly Supervised Segmentation for Ultrasound Images via Muti-scale Constraints}
\title{Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses}

% \author{
% \IEEEauthorblockN{Jianning Chi\textsuperscript{1}*, Zelan Li\textsuperscript{1}, Geng Lin\textsuperscript{1}, MingYang Sun\textsuperscript{1}, Ying Huang\textsuperscript{2}}
% \IEEEauthorblockA{\textsuperscript{1}\textit{Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China}}
% % \IEEEauthorblockA{\textsuperscript{2}\textit{Division of Biomedical Engineering, University of Saskatchewan, Saskatoon, Canada}}
% \IEEEauthorblockA{\textsuperscript{3}\textit{Department of Ultrasound, Shengjing Hospital of China Medical University, Shenyang, China}}
% }

% \author{Jianning Chi, Zelan Li, Geng Lin, and MingYang Sun, Xiaosheng Yu
% \thanks{Jianning Chi is with the Falculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: chijianning@mail.neu.edu.cn). }
% \thanks{Zelan Li is with the Falculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2410844@stu.neu.edu.cn).}
% \thanks{Geng Lin is with the Falculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2202048@stu.neu.edu.cn).}
% \thanks{MingYang sun is with the Falculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2302161@stu.neu.edu.cn).}
% \thanks{Jianning Chi, Zelan Li, Geng Lin, and MingYang Sun, and Xiaosheng Yu are with the Falculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: chijianning@mail.neu.edu.cn, 2410844@stu.neu.edu.cn, 2202048@stu.neu.edu.cn, 2302161@stu.neu.edu.cn, yuxiaosheng@mail.neu.edu.cn). }
% }
\author{Jianning Chi, \IEEEmembership{Member, IEEE}, Zelan Li, Geng Lin, MingYang Sun, and Xiaosheng Yu 
\thanks{Manuscript submitted on February 26, 2025. (Corresponding author:Jianning Chi.)}
\thanks{Jianning Chi is with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: chijianning@mail.neu.edu.cn). }
\thanks{Zelan Li is with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2410844@stu.neu.edu.cn).}
\thanks{Geng Lin is with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2202048@stu.neu.edu.cn).}
\thanks{Mingyang sun is with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: 2302161@stu.neu.edu.cn).}
\thanks{Xiaosheng Yu is with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang, 110169, China (e-mail: yuxiaosheng@mail.neu.edu.cn).}
}

\maketitle

\begin{abstract}
% type 1
% Our method consists of two main components: the extraction of multi-level information from weakly supervised labels and the use of multi-scale constraints to learn feature representations of multi-level information. Specifically, our approach adopts geometric transformations with topology priors and the Medical Segment Anything Model (MedSAM) prediction with anatomy certainty to generate convincing multi-level labels, which include low-level spatial information and high-level semantics information, with box labels of spatial indications and high-confidence foreground/background labels representing area distinction prototypes. Once these multi-level labels are obtained, a region-scale alignment loss is employed to assess the axis alignment between box labels and predictions for spatial constraints, along with a pixel-scale consistency loss to ensure that adjacent pixels in the pure foreground area share the same classification. Additionally, we introduce patch-scale contrastive loss and region-scale prototype correlation loss between the pure foreground and background areas to learn discriminative features for shape constraints. Experimental results demonstrate that our method achieves state-of-the-art performance on the publicly available dataset TN3K and DDTI dataset, as well as a self-collected dataset.
% Segmentation of thyroid nodule regions plays a crucial role in the diagnosis and surgical treatment of thyroid diseases. Weakly supervised segmentation (WSS) algorithms, which utilize four points annotated by clinical experts alleviate the conflict between annotation costs and model performance. 
% Precise thyroid nodule segmentation provides morphological features for diagnosis. Leveraging the nature of clinical labels, that four-point annotations are marked by clinicians to indicate thyroid nodules, weakly supervised segmentation algorithms utilizing such sparse annotations offer a more adaptable and efficient solution for thyroid nodule segmentation. However, typical weakly supervised segmentation algorithms rely on low-confidence pseudo-labels based on topological prior will introduces label noise, and they adopt the single-level learning strategy they adopt ignores multi-level discriminative information from the foreground and background, both leading to inaccurate nodule shape inaccurate. 
% Weakly supervised segmentation algorithms can use four-point annotations in clinical for adaptable and efficient lesion segmentation. However, conventional weakly supervised segmentation methods often rely on low-confidence pseudo-labels based on topological priors, introducing significant label noise. 
% type 2
% While weakly supervised segmentation algorithms can utilize four-point annotations in clinical practice for adaptable and efficient lesion segmentation, conventional methods predominantly rely on low-confidence pseudo-labels based on topological priors, introducing significant label noise. Additionally, the low-rationality learning strategy employed by these algorithms, which uses fixed-shape masks for segmentation results learning, fails to capture multi-level discriminative information necessary for thyroid nodules with diverse and complex shapes. To address the issues of label noise for supervision accuracy and learning strategies' inadequate performance with thyroid nodule complexity, we propose a novel framework based on high-confidence labels generation and high-rationality losses learning strategy.
% To address challenges related to supervision accuracy and feature learning strategy, we propose a novel weakly supervised segmentation framework based on generating high-confidence labels, which contain low-level spatial and high-level semantic information, and using high-rationality losses, which are used to learn discriminative location-level, region-level, and edge-level feature information jointly.
% Multi-level High-rationality Constraints (MHLMHC) learning. 
% These labels encompass former weak labels with low-level spatial information and high-confidence foreground/background containing high-level semantic information. 
% Specifically, our approach integrates topological priors from geometric transformations and anatomical information provided by the Medical Segment Anything Model (MedSAM) model to create high-confidence supervisory labels, which contain low-level spatial and high-level semantic information. Once these high-confidence labels are obtained, we designed a high-rationality multi-level loss function system comprising alignment loss, contrastive loss, and prototype correlation loss. The alignment loss optimizes the location-level consistency between predictions and box labels; the contrastive loss aims to learn region-level feature representations to reduce intra-class differences and increase inter-class distinctions; while the prototype correlation loss enhances edge segmentation accuracy by comparing discriminative feature correlation referring to foreground and background feature prototypes. Experimental results demonstrate that our method achieves state-of-the-art performance on the publicly available TN3K and DDTI datasets. The code is publicly available at \href{https://github.com/bluehenglee/MLI-MSC}{HCL-HRL}.
% we introduce a high-rationality multi-level learning strategy, which consists of alignment loss, contrast loss, and prototype correlation loss. The alignment loss aligns predictions with box labels for location. Contrastive loss learns region-level feature representation learning to reduce intra-class differences and increase inter-class differences. Prototype correlation loss obtains foreground and background feature prototypes to learn discriminative feature consistency and enforce edge-level segmentation shape. Experimental results demonstrate that our method achieves state-of-the-art performance on the publicly available TN3K and DDTI datasets. The code is publicly available at \href{https://github.com/bluehenglee/MLI-MSC}{HCL-HRL}.
Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that simply follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these problems, we clarify the objective together with required references of weakly supervised ultrasound image segmentation, and present the framework with high-confidence pseudo-labels to represent topological and anatomical information, and high-rationality losses to learn multi-level discriminative information. Specifically, we fuse geometric transformations of four-point annotations and results from the MedSAM model prompted by certain annotations to generate high-confidence box, foreground, and background labels. We design a high-rationality learning strategy comprising: 1) Alignment loss that measures the spatial projection consistency between the segmentation and box label, and the topological continuity of the segmentation within foreground label, guiding the network to perceive the location arrangement of nodule features; 2) Contrastive loss that pulls features sampled from labeled foreground regions, while pushing features sampled from labeled foreground and background regions, guiding the network to capture the regional distribution of nodule and background features; 3) Prototype correlation loss that measures the consistency between correlation maps derived by comparing features with foreground and background prototypes respectively, shrinking the uncertain regions to precise nodule edge delineation. Experimental results demonstrate that our method achieves state-of-the-art performance on the publicly available TN3K and DDTI datasets. The code is publicly available at \href{https://github.com/bluehenglee/MLI-MSC}{HCL-HRL}.
\end{abstract}

\begin{IEEEkeywords}
Medical image segmentation; Thyroid nodule; Deep learning; Weakly supervised segmentation.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{hyroid} nodule segmentation in the ultrasound image is critical for accurate thyroid disease diagnosis~\cite{chen2020review, inan2024multi}, but suffers from the blurred structures of anatomy with speckle noise, making it highly dependent on the expertise of the radiologist~\cite{khor2022ultrasound}.
%However, ultrasound images often suffer from low quality and speckle noise~\cite{khor2022ultrasound}, which blurs and distorts the appearance of organ tissues. 
Employing deep learning algorithms~\cite{chen2022mtnnet, chi2023htunet, ozcan2024enhancedtransunet, xiang2025federated} for thyroid nodule segmentation can significantly enhance diagnostic efficiency for healthcare professionals. While fully supervised algorithms~\cite{ronneberger2015unet,cai2020denseunet,SETR,Swin-unet,tao2022cenet} achieve promising performance on specific datasets 
where precise ground truth masks are available for training, acquiring a large number of delicate annotations remains resource-intensive and time-consuming~\cite{liu2024procns}.
% Fully supervised algorithms~\cite{ronneberger2015unet,cai2020Denseunet,SETR,Swin-unet,tao2022cenet} depend on precise label training, yielding satisfactory results on specific datasets. Nevertheless, acquiring a substantial number of precise labels is often challenging, as it is both time-consuming and labor-intensive. 

Weakly supervised segmentation (WSS) algorithms offer attractive alternatives by utilizing coarse annotations such as bounding boxes~\cite{zhang2020scrf,mahani2022uncrf,tian2021BoxInst,chen2024region,zhao2024ultrasound}, points~\cite{zhao2020weakly,li2023wsdac,zhao2024IDMPS}, or scribbles~\cite{wang2023s2me,han2024dmsps,li2024scribformer} to achieve accurate segmentation results. These approaches are particularly suitable for thyroid nodule segmentation in clinical practice, since they can utilize simple four-point annotations provided by clinicians as training supervision. However, existing weakly supervised methods still face the following challenges: 1) they typically generate low-confidence pseudo-labels based on topological geometric priors only~\cite{zhang2020scrf, mahani2022uncrf, lei2023uncertainty2, fan2024uncertainty,zhao2024IDMPS,tian2021BoxInst}, introducing label noise and potentially misleading training according to these uncertain or ambiguous conditions; 2) they primarily adopt rigid learning strategies such as comparing the segmentation with fixed-shape labels or pseudo-labels~\cite{zhang2020scrf, mahani2022uncrf, du2023weakly3D, zhai2023paseg}, severely limiting their flexibility and adaptability in handling diverse and complex nodule variations.
%related to label noise for supervision reference and inadequate learning of discriminative information. Typically, WSS algorithms~\cite{zhang2020scrf, mahani2022uncrf, lei2023uncertainty2, fan2024uncertainty,zhao2024IDMPS,tian2021BoxInst} generate low-confidence pseudo-labels based on topological geometric priors. While this approach can guide the learning process, it may introduce label noise and potentially mislead training by relying on uncertain or ambiguous annotations. For example, Zhang et al.~\cite{zhang2020scrf} and Mahani et al.~\cite{mahani2022uncrf} directly exploit box annotations into single-level pseudo-labels with Excessive uncertainty. Zhao et al.~\cite{zhao2024IDMPS} generate basic geometric shapes as Radical Labels and Conservative Labels for asymmetric
%learning and Li et al.~\cite{li2023wsdac} generate octagon from points annotation as initial contour to iteratively match thyroid nodule boundary, improving the accuracy of pseudo-mask but still introduce partial label noise. Additionally, some existing WSS methods~\cite{zhang2020scrf, mahani2022uncrf, du2023weakly3D, zhai2023paseg} primarily adopt pixel-to-pixel learning strategies that rely heavily on fixed-shape masks, which severely limits their flexibility and adaptability in handling diverse and complex object variations. For instance, Zhai et al.~\cite{zhai2023paseg} use pseudo-labels generated by geodesic distance transform for pixel-level noisy learning, lacking the ability to dynamically adjust for incorrect shapes. Du et al.~\cite{du2023weakly3D} proposed an algorithm that learns the location and geometric prior of organs mainly relying on the region of interest (ROI) feature, neglecting the multi-level discriminative information from both the foreground and background. 

% Weakly supervised segmentation (WSS) algorithms utilize coarse labels such as bounding boxes~\cite{zhang2020scrf,mahani2022uncrf,tian2021BoxInst,chen2024region,zhao2024ultrasound}, points~\cite{zhao2020weakly,li2023wsdac,zhao2024IDMPS}, or scribbles~\cite{wang2023s2me,han2024dmsps,li2024scribformer} to obtain accurate segmentation results, which is especially adaptable and efficient for thyroid nodule segmentation in clinical by using coarse four-point annotations marked by clinicians as supervision. However, existing weakly supervised methods still exist challenges in supervision accuracy and representation feature learning strategy. Firstly, typical WSS algorithms~\cite{zhang2020scrf, mahani2022uncrf, lei2023uncertainty2, fan2024uncertainty,zhao2024IDMPS,tian2021BoxInst} rely on low-confidence pseudo-labels based on topological geometric prior introduce label noise, leading to wrong guidance for training. For example, Zhang et al.~\cite{zhang2020scrf} and Mahani et al.~\cite{mahani2022uncrf} directly exploit box annotations into single-level pseudo-labels with Excessive uncertainty. Zhao et al.~\cite{zhao2024IDMPS} generate basic geometric shapes as Radical Labels and Conservative Labels for asymmetric learning and Li et al.~\cite{li2023wsdac} generate octagon from points annotation as initial contour to iteratively match thyroid nodule boundary, improving the accuracy of pseudo-mask but still introduce partial label noise. Additionally, some existing weakly supervised segmentation (WSS) methods~\cite{zhang2020scrf, mahani2022uncrf, du2023weakly3D, zhai2023paseg} primarily adopt learning strategies that rely heavily on fixed-shape masks, which severely limits their flexibility and adaptability in handling diverse and complex object variations. For instance, Zhai et al.~\cite{zhai2023paseg} use pseudo-labels generated by geodesic distance transform for pixel-level noisy learning, lacking the ability to dynamically adjust for incorrect shapes. Du et al.~\cite{du2023weakly3D} proposed an algorithm that learns the location and geometric prior of organs mainly relying on the region of interest (ROI) feature, neglecting the multi-level discriminative information from both the foreground and background. 

% which is useful for organ segmentation with fixed shapes but not suitable for thyroid nodules with diverse and complex shapes. 
% This approach neglects the multi-level discriminative information from both the foreground and background, 
 
% Typical methods focus on directly exploiting sparse annotations into single-level pseudo-labels~\cite{liu2024procns}. 
% Typical methods focus on exploiting anatomy and topology priors to directly expand sparse annotations into pseudo-labels by integrating weak supervision with additional deep-learning modules~\cite{liu2024procns}. 
% Some approaches~\cite{zhang2020scrf, mahani2022uncrf, lei2023uncertainty2, fan2024uncertainty} generate pseudo-masks by incorporating conditional probability modeling, such as conditional random fields (CRF) and uncertainty estimates, into the training process using weakly supervised labels. Others enhance the network architecture~\cite{wang2023s2me, zhao2024IDMPS, wang2023WSL-MIS, fan2024uncertainty} by introducing dual or multi-branch designs that help the model learn the consistency of pseudo-labels during training, thus improving the balance in pseudo-mask generation. Despite these advancements, these single-level learning by inaccurate pseudo-labels can misguide the model during training, leading to degraded feature learning and increased errors in boundary delineation.

% Recently, BoxInst~\cite{tian2021BoxInst} employs box annotations to localize segmentation targets, combining color similarity with graph neural networks to delineate segmentation boundaries. Nevertheless, for thyroid ultrasound images with low contrast and blurred boundaries, low-level color and texture information in the image cannot fully indicate the segmentation of the thyroid nodule's boundary. Inspired by BoxInst, we proposed a WSS framework by learning segmentation results from a multi-level learning strategy.
% To address the challenges outlined above, we propose an effective WSS algorithm that leverages four clinical point annotations for thyroid nodule segmentation based on generating \textbf{H}igh-\textbf{C}onfidence \textbf{L}abels integrating multi-level information and using \textbf{H}igh-\textbf{R}ationality \textbf{L}osses (HCL-HRL) to learn discriminative multi-level information.
% , which effectively addresses challenges by generating high-confidence pseudo-labels based on multi-level information integration and achieves high-rationality feature perception based on multi-level goal learning.
To address the aforementioned challenges, we propose a weakly supervised segmentation framework that leverages clinical four-point annotations to generate high-confidence pseudo-labels with both topological and anatomical information, and learns location-level, region-level, and edge-level discriminative information through high-rationality losses. Specifically, we fuse geometric transformations of four-point annotations and results from the MedSAM prompted by certain annotations to generate high-confidence box, foreground, and background labels. We then design a high-rationality multi-level learning strategy consisting of: 1) Alignment loss that measures the spatial projection consistency between the segmentation and the box label, and topological continuity of the segmentation within the foreground label, guiding the network to perceive the location arrangement of nodule features; 2) Contrastive loss that reduces the distances between features sampled from the labeled foreground regions, while increasing distances between features sampled from the labeled foreground and background regions, guiding the network to capture the regional distribution of nodule and background features; 3) Prototype correlation loss that measures the consistency between correlation maps derived by comparing deep features with foreground and background prototypes respectively, so that the uncertain regions are gradually evolved to precise nodule edge delineation.
%we present a novel WSS algorithm that 
%effectively utilizes four clinical point annotations for accurate thyroid nodule segmentation. Our approach 
%leverages clinical four-point annotations to generate high-confidence labels by integrating multi-level information and employing high-rationality losses (HCL-HRL) to enhance the learning of discriminative features across different levels.
%High-confidence labels are generated by combining geometric transformations of point annotations with confidence maps provided by MedSAM. The high-confidence box labels contain thyroid location information, while the high-confidence foreground/background labels indicate specific partitions of images. Subsequently, high-rationality losses are introduced, jointly learning the segmentation location and feature distribution. The alignment loss compares predictions with box labels to learn nodule location without misleading shapes. The contrastive loss reduces intra-class differences and increases inter-class differences to learn region-level feature representation implicitly. Prototype correlation loss compares foreground and background feature correlation to learn the edge of the thyroid nodule explicitly.
In summary, the contributions of our work are as follows:
\begin{enumerate}
% We propose a WSS framework generating high-confidence labels integrating multi-level information and using high-rationality losses to learn discriminative multi-level information jointly (HCL-HRL). 
    % \item We propose a WSS framework named HCL-HRL that combines geometry transformation with topology priors and MedSAM prediction with anatomical information to generate high-confidence labels for thyroid nodule segmentation. The generated labels integrate low-level spatial information and high-level semantic information, leading to more precise supervision for thyroid nodules learning.
    % \item We introduce a series of high-rationality losses for multi-level thyroid nodule location and shape feature learning. 
    % Alignment loss aligns predictions with box labels for location. Contrastive loss learns region-level feature representation learning to reduce intra-class differences and increase inter-class differences. Prototype correlation loss utilizes foreground and background feature prototypes to learn discriminative feature consistency and enforce edge-level segmentation shape. 
    % The alignment loss learns location without shape constrain. The contrastive loss implicitly learns discriminative features from both the foreground and background for region-level shape, and prototype correlation loss nodule edge by ensuring consistency of foreground and background explicitly.
    \item We propose a high-confidence pseudo label generation method that fuses geometric transformations of point annotations and segmentation provided by the MedSAM using prompts derived from point annotations, preventing the misleading conditions from label noise during network training.
    %generate high-confidence pseudo-labels by fusing geometric transformations of point annotations with segmentation predicted by the MedSAM using prompts derived from the point annotations. The integration of high-confidence labels with multi-level information ensures robust and discriminative feature learning. 
    % These labels not only capture the thyroid location information but also provide specific partitions for pixel classification, ensuring accurate and reliable segmentation. 
    \item We introduce a series of high-rationality losses, including alignment, contrastive, and prototype correlation loss. These losses guide the network to capture multi-level discriminative information of thyroid nodules' locations and shapes, significantly enhancing the reliability of the training process.
    %for multi-level thyroid nodule location and shape feature learning. Our innovative loss design, including alignment, contrastive, and prototype correlation losses, significantly enhances the precision and reliability of the segmentation process.
    % The alignment loss compares predictions with box labels to learn nodule location without misleading shapes. Contrastive loss learns region-level feature representation learning and prototype correlation loss learns edge-level discriminative features and enforces segmentation shape. 
    \item Extensive experiments show that our method achieves state-of-the-art on the publicly available thyroid nodule ultrasound dataset TN3K~\cite{gong2021tn3k} and DDTI~\cite{pedraza2015DDTI}. Additionally, the code for this paper is publicly available at \href{https://github.com/bluehenglee/MLI-MSC}{HCL-HRL}.
\end{enumerate}


\section{Related Work}
\subsection{Medical image Segmentation}
Medical image segmentation is a fundamental task in radiology and pathology, enabling automated analysis of medical images to assist in diagnosis and treatment planning~\cite{rayed2024deepreview, obuchowicz2024review}. In recent years, deep learning technology has achieved significant progress in the field of medical image segmentation~\cite{rayed2024deepreview, das2024nodulereview}. Models based on convolutional neural networks (CNN)~\cite{ronneberger2015unet, tao2022cenet, zhou2018unet++, isensee2021nnunet}, such as U-Net~\cite{ronneberger2015unet} and its variant networks~\cite{tao2022cenet, zhou2018unet++, isensee2021nnunet}, adopt an encoder-decoder architecture that enables them to maintain high resolution while extracting multi-scale feature information. On the other hand, Vision Transformer-based networks~\cite{Swin-unet, ozcan2024enhancedtransunet, zhou2024Thyroid-DETR, bi2023bpat, CHENTransUNet}, like TransUnet~\cite{CHENTransUNet} and SWin-Unet~\cite{Swin-unet}, utilize attention mechanisms during encoding or decoding processes to capture both local and global features of images through transformers, thereby learning more precise results for medical image segmentation. Moreover, models based on Mamba~\cite{chen2024masam, wang2024mamba-unet, xing2024segmamba, ruan2024vm-unet, liu2024Swin-UMamba}, such as SegMamba~\cite{xing2024segmamba} and VM-UNet~\cite{ruan2024vm-unet}, effectively capture long-range dependencies in full-scale features across various scales using state space models, achieving competitive performance in medical image segmentation tasks.

% Compared to other imaging technologies like CT and MRI, ultrasound images suffer from issues such as speckle noise and low contrast, which increase the difficulty of weakly supervised segmentation~\cite{das2024nodulereview}. 
In recent years, algorithms for thyroid nodule segmentation based on fully supervised precise labels~\cite{chi2023htunet, chen2024mlmseg, wu2024medsegdiff, li2023novelthyroid, gong2023thyroid, ma2024tnseg} have been extensively studied. Chi et al.~\cite{chi2023htunet} employed transformer attention mechanisms to extract intra-frame and inter-frame contextual features within thyroid nodule regions, achieving competitive segmentation results. Chen et al.~\cite{chen2024mlmseg} developed a multi-view learning model, which introduced deep convolutional neural networks to encode local view features and a cross-layer graph convolution module to learn the correlations between high-level and low-level features for superior segmentation performance. Wu et al.~\cite{wu2024medsegdiff} introduced dynamic conditional encoding and a feature frequency parser based on the diffusion probabilistic model, achieving excellent results in thyroid nodule segmentation on ultrasound images. 

Despite their competitive performance in medical image segmentation, these deep learning methods require extensive annotated data, which demands significant efforts and time in data collection and management, making them impractical for clinical settings.

\subsection{Weakly Supervised Segmentation Methods}
% Weakly Supervised Segmentation Methods aim to alleviate the burden of obtaining fully annotated pixel-level masks by leveraging sparse annotations.
Weakly supervised learning represents an emerging learning paradigm that requires only a small amount of coarse-grained annotation information for model training~\cite{guo2024weaklysup, lin2024weaklysup}. This approach significantly reduces the annotation workload while maintaining promising segmentation accuracy~\cite{roth2021weaklysegmentation}.

% Typical methods focus on directly exploiting sparse annotations or inaccurate geometric shapes into pseudo-labels~\cite{liu2024procns} for segmentation region learning. For example, Some approaches incorporating conditional probability modeling, such as conditional random fields (CRF)~\cite{zhang2020scrf, mahani2022uncrf} and uncertainty estimates~\cite{lei2023uncertainty2, fan2024uncertainty}, into the training process directly using weakly supervised labels. Others generate pseudo-masks relying on topological geometric transforms~\cite{wang2023s2me, zhao2024IDMPS, wang2023WSL-MIS, li2023wsdac}. For instance, Zhao et al.~\cite{zhao2024IDMPS} using quadrilateral as conservative labels and irregular ellipse as radical labels, and introducing dual-branch designs to help the model learn the consistency of pseudo-labels during training, thus improving the balance in prediction. Li et al.~\cite{li2023wsdac} generate octagon from points annotation as initial contour to iteratively match thyroid nodule boundary by active contours learning. 
Typical methods focus on directly exploiting sparse annotations or inaccurate geometric shapes to generate pseudo-labels~\cite{liu2024procns} for pixel-to-pixel region learning. For instance, some approaches incorporate conditional probability modeling techniques, such as conditional random fields (CRF)~\cite{zhang2020scrf, mahani2022uncrf}, and uncertainty estimates~\cite{lei2023uncertainty2, fan2024uncertainty} into the training process directly using weakly supervised labels to learn predictions. Other methods generate pseudo-masks based on topological geometric transforms~\cite{wang2023s2me, zhao2024IDMPS, wang2023WSL-MIS, li2023wsdac}. For example, Zhao et al.~\cite{zhao2024IDMPS} employ quadrilaterals as conservative labels and irregular ellipses as radical labels while introducing dual-branch designs to improve the consistency of pseudo-labels during training, thereby enhancing prediction accuracy. Similarly, Li et al.~\cite{li2023wsdac} propose a method that generates octagons from point annotations to serve as initial contours for iteratively refining thyroid nodule boundaries through active contour learning.

Recently, BoxInst~\cite{tian2021BoxInst} employs box annotations to localize segmentation targets, combining color similarity with graph neural networks to delineate segmentation boundaries. Nevertheless, for thyroid ultrasound images with low contrast and blurred boundaries, color similarity cannot fully indicate the thyroid nodule's boundary. Inspired by BoxInst, Du et al.~\cite{du2023weakly3D} proposed an algorithm that learns the location and geometric prior of organs mainly relying on the region of interest (ROI) feature, which is useful for organ segmentation with fixed prior shapes but not suitable for thyroid nodules with diverse and complex shapes.

% Although recent advancements in weakly supervised semantic segmentation (WSS) have yielded promising results, several challenges persist. First, the reliance on low-confidence pseudo-labels introduces noise into the training process, potentially compromising model performance. Second, the adoption of rigid learning strategies hinders the ability to learn generalizable feature representations.
Although recent advancements in Weakly supervised segmentation have yielded promising results, challenges such as pseudo-label noise from dependency on low-confidence pseudo-labels and the adoption of rigid learning strategies that compare the segmentation with fixed-shape labels or pseudo-labels hinder delicate segmentation learning.

% A prominent strategy of WSS is using pseudo-proposal and consistency learning to generate single-level pseudo-labels to bridge the gap between weak annotations and precise segmentation~\cite{zhang2020scrf,mahani2022uncrf,wang2023s2me,zhao2024IDMPS}. Zhang et al.~\cite{zhang2020scrf} combined coarse segmentation information with conditional random fields (CRF) to produce pseudo-labels, introducing SCRF. Mahani et al.~\cite{mahani2022uncrf} introduced UNCRF, which refines these pseudo-labels by incorporating uncertainty estimation to enhance segmentation accuracy. Zhao et al.~\cite{zhao2024IDMPS} proposed IDMPS, which employs a dual-network training approach to balance the contribution of both aggressive and implicit pseudo-labels

% The other alternative WSS strategy involves decomposing the segmentation process into multiple stages, which can help mitigate the noise in pseudo-labels~\cite{tian2021BoxInst,li2023wsdac,du2023weakly3D,liu2024procns}. Tian et al.~\cite{tian2021BoxInst} designed BoxInst, which utilizes box annotations to localize the target object and refines the segmentation boundaries using graph neural networks (GNNs) and color similarity. Li et al.~\cite{li2023wsdac} proposed WSDAC, which uses topological information to generate an initial mask and then applies gradient-based similarity and statistical offsets to refine segmentation shape. Du et al.~\cite{du2023weakly3D} proposed an algorithm that learns the location from box labels and uses the geometric prior of organs to learn segmentation shape.

% Despite these advancements, these single-level learning by inaccurate pseudo-labels can misguide the model during training, leading to degraded feature learning and increased errors in boundary delineation.

% Our proposed method explores multi-level high-confidence labels and multi-level learning strategies to learn discrepancy features.

% \subsection{Segment Anything Model for Weakly Supervised Segmentation}
% The Segment Anything Model (SAM)~\cite{kirillov2023SAM} is a foundation model that is designed as a promotable model capable of performing general-purpose segmentation tasks across a wide variety of object categories. The model is pre-trained on an extensive dataset using prompt-based learning, where a user provides interactive cues, such as points, bounding boxes, or masks, to guide the segmentation. 

% Ma et al.~\cite{ma2024MED-SAM} present MedSAM developed on a large-scale medical image dataset with 1,570,263 image-mask pairs. It provides high performance in the zero-shot learning regime of medical image segmentation. Comprehensive experimental studies adopted by Mazurowski et al.~\cite{mazurowski2023SAM_exprience} show that when given points or box prompts, SAM can perform convincing results in various medical imaging datasets. 

% Although fine-tuning a large model (MedSAM) can lead to high performance that is better adapted to a new domain, fine-tuning a large model typically requires significant computational resources. Additionally, The interactive nature of using SAM as a feature extractor by prompts may present limitations in fully automated settings or where human intervention is impractical.

% % In contrast, direct inference requires much less computational power. 

% % To simultaneously leverage the high generalization capability of MedSAM in providing anatomical segmentation results when given prompts~\cite{zhao2024sam}, while avoiding the heavy resource demands of using MedSAM as a feature extractor and the need for prompts, we use the results inferred from box labels as anatomical confidence, which is combined with initial foreground/background labels obtained through geometric transformations to generate more accurate multi-level labels.

% \subsection{Contrastive Learning for Feature Representation}

% Contrastive learning aims to attract the positive sample pairs and repulse the negative sample pairs through contrastive loss. In segmentation tasks, contrastive learning is used to learn discriminative representations by constructing sample queues from different regions of an image~\cite{oord2018representation}. Sample construction typically occurs at the pixel-scale~\cite{wang2021ContrastiveSeg,zhao2021contrastive-pixel,wang2022contrastmask,wen2022slotCon,du2023weakly3D} and patch-scale~\cite{yun2022patch,zhang2023ADCLR,wu2024voco}. Wang et al.~\cite{wang2021ContrastiveSeg} and Zhao et al.~\cite{zhao2021contrastive-pixel} both proposed pixel-level comparison algorithms that rely on fully supervised masks to sample pixels of different categories. Chaitanya et al.~\cite{chaitanya2023localcontrastive} and Du et al.~\cite{du2023weakly3D} applied pixel-level contrastive loss on pseudo-labels of unlabeled data in Semi-supervised and weakly-supervised segmentation, respectively. Meanwhile, Yun et al.~\cite{yun2022patch} devised patch-level contrastive approaches to enforce invariance against each patch and its neighbors. Zhang et al.~\cite{zhang2023ADCLR} adopted image augmentation for two input views and introduced a cross-view query-based patch-level Contrasting Paradigm for Self-supervised representation learning (SSL). Wu et al.~\cite{wu2024voco} exploited the contextual position priors of 3D medical images to generate base crops as prototypes, enforcing feature distinction across different regions.

% Although these studies highlight the potential of contrastive learning in feature representation, they primarily focus on pixel-level or patch-level methods, failing to fully leverage multi-scale information to learn feature representations with stronger discriminative power.

% Contrastive learning aims to attract the positive sample pairs and repulse the negative sample pairs through contrastive loss. Initially, it was primarily applied to image recognition tasks~\cite{wu2018InstDisc,ye2019Invaspread,tian2020cmc}. Wu et al. introduced InstDISC~\cite{wu2018InstDisc}, framing instance-level discrimination as a metric learning problem by comparing the similarity between image features and those stored in a memory bank. Subsequently, algorithms such as SimCLR~\cite{chen2020simCLR} and MoCo~S\cite{he2020moco} demonstrated strong performance in image classification tasks. Using data augmentation techniques, these methods generate different views of images and then construct positive and negative pairs through various sample strategies~\cite{grill2020BYOL,chen2021simsiam} to distinguish between different image classes.

% % While these studies highlight the potential of contrastive learning for feature representation, they primarily focus on either pixel-scale or patch-scale approaches. 
% % Our proposed MLMC framework leverages multi-scale contrastive constraints to capture multi-scale context across the training data. In contrast to existing WSS methods, which primarily focus on local context by analyzing pixel dependencies within individual images, our approach integrates both local and global contextual information.

% \subsection{Prototype Learning for Feature Representation}
% Prototype learning aims to classify and make decisions by selecting or learning representative samples, widely applied in semi-supervised learning, few-shot learning, and unsupervised learning~\cite{wang2024boundary-prototype, huang2024prototype-graph, zhao2024cpnet, ouyang2022self}. Huang et al.~\cite{huang2024prototype-graph} propose a novel prototype-guided graph reasoning network (PGRNet) to explicitly explore potential contextual relationships in structured query images. Recently, some weakly supervised algorithms have incorporated prototype learning to improve the feature representations extracted by networks. Du et al.~\cite{du2022weakly-prototype} conducted pixel-prototype contrastive learning in the feature space. Liu et al.~\cite{liu2024procns} proposed Progressive Prototype Calibration focuses solely on the feature prototypes of the foreground. 

% Although these studies can help segmentation network to learn more essential feature representation, the target prototypes directily generated from sparse labels may lack semantic representativeness. Directly or indirectly using them for prediction improvements might lead to overconfident errors. and neglecting the complementary consistency between background and foreground prototypes, leaving uncertainties in the boundaries of segmentation.

\section{Method}
% Number equations consecutively with equation numbers in parentheses flush with the right margin, as appears in \eqref{eq}. Refer to ``\eqref{eq},'' not ``Eq. \eqref{eq}'' 
% or ``equation \eqref{eq},'' except at the beginning of a sentence: ``Equation \eqref{eq} is $\ldots$ .'' To make your equations more compact, you may use the solidus (~/~), the exp function, or appropriate exponents. Use parentheses to avoid ambiguities in denominators. Punctuate equations when they are part of a sentence, as in
% \begin{equation}E=mc^2.\label{eq}\end{equation}

\begin{figure*}[h]
\centering
% \vspace{-0.2in}
\includegraphics[width=0.98\textwidth]{framework_193.png}
\caption{\textbf{Overview of the proposed HCL-HRL framework.} 1) High-confidence multi-level labels are generated by combining MedSAM results with geometric transformation outputs. 2) High-rationality multi-level learning strategy: The upper branch processes deep features through the segmentation head to generate segmentation predictions and applies alignment loss for location-level learning. The lower branch refines feature representations by calculating contrastive loss for region-level learning and the prototype correlation loss for edge-level learning.
% the prototype correlation loss constrains the complementary consistency of feature correlations between foreground and background, reducing uncertainty in edge-level learning.
}
% \vspace{-0.2in}
\label{fig:framework}
\end{figure*}

\subsection{Overall Framework}
As shown in Fig.~\ref{fig:framework}, we propose a novel Weakly supervised segmentation (WSS) framework for thyroid nodule segmentation. The framework consists of high-confidence multi-level labels generation flow and high-rationality multi-level learning strategy branches. In labels generation flow, we fuse MedSAM results with geometric transformations of four-point annotations to generate high-confidence multi-level labels. In multi-level learning branches, we use high-rationality losses consisting of alignment loss, contrastive loss, and prototype correlation loss to learn precise segmentation location and delicate shape jointly.  

% In labels generation flow, The input images and point annotations are passed through the pre-trained MedSAM model to generate a segmentation result, which containing anatomical information serves as a distribution-weighted map. Simultaneously, geometric transformations are applied to the point annotations to generate preliminary multi-level information. By fusion this initial information with the distribution-weighted map, high-confidence foreground/background regions can be obtained. In multi-level constraints learning branches, deep features are first extracted from the backbone network, then the segmentation branch and the feature representation branch using multi-level constraints are used to learn thyroid spatial and shape jointly.

% In the following paragraphs, we will elaborate on the technical details of the proposed algorithm. These include multi-level labels generation and multi-level constraints learning strategy.

% \subsection{Theory insight}
% Instead of defining segmentation through dense, pixel-level delineations as in traditional approaches, our method leverages macro-level object location and shape characteristics to construct the segmentation mask.

% We reconsider the components of the foreground subset $R_{\text{foreground}}$ consist of location elements $Loc(x_a, y_b, w_f, h_f)$ and shape elements $S$, the formula presentation can be illustrated as: 
% We construct the semantic segmentation mask as multi-level learning, the formula presentation can be illustrated as:
% \begin{eqnarray}
%     S_{\text{fore}} &= \left\{ (x_a, y_b, w, h) \in I, \, \theta \in (0^\circ, 360^\circ) \,\middle| \right. \nonumber \\
%     & \left. \quad y((x_a, y_b, w, h), f(\theta)) \sim \mathcal{D}(F_1, F_2) \right\}. \label{image segmentation define now}
% \end{eqnarray}

% \begin{eqnarray}
% R_{\text{foreground}} &=& \mathcal{R}(Loc(x_a, y_b, w_f, h_f), S),\\
% R_{\text{background}} &=& I - R_{\text{fore}},
% \label{image segmentation define now}
% \end{eqnarray}
% where $(x_a, y_b, w_f, h_f)$ describe the foreground rectangle region location information—specifically, the width $w_f\leq w$ and height $h_f\leq h$ in the upper left corner point $(x_a, y_b) \in I$ where $w$ and $h$ represent the width and height of the input image, respectively, and $I$ denotes the input image. Here, $S$ represents the foreground shape.  The background subset is denoted as $R_{\text{background}}$. The feature distribution of the image can be described as:
% \begin{eqnarray}
% \mathcal{D}(R_{\text{foreground}}, R_{\text{background}})\ \sim \mathcal{D}(F_1, F_2),
% \label{image segmentation define now}
% \end{eqnarray}
% where $\mathcal{D}(R_{\text{foreground}}, R_{\text{background}})$ represents the distinction of segmentation and should conform to the feature distribution $\mathcal{D}(F_1, F_2)$. Here, $F_1$ and $F_2$ denote the feature prototypes of the foreground and background, respectively.

% Based on this formulation, the segmentation results can be learned by providing only coarse spatial labels for location and prototype labels for the foreground and background shapes. Specifically, the training methodology involves teaching the network to predict segmentation areas by jointly learning spatial locations and semantic shapes from the feature distributions of relevant regions.
% According to this theory, the segmentation results can be learned by providing only coarse spatial labels for location and foreground and background prototype labels for shape. Based on the needed labels, the training methodology for the network to obtain segmentation areas by learning spatial locations and semantic shapes from feature distributions of relevant regions jointly can lead to segmentation results. 

% To achieve this, we employ high-confidence multi-level labels to provide precise location information (via location constraints) and foreground/background distribution details (through shape constraints). This multi-level learning strategy incorporates distinct loss functions for location and shape prediction, derived from our segmentation definition. Such an approach is particularly advantageous in weakly supervised segmentation tasks with vague or incomplete constraints, as it enhances the model's robustness against minor inaccuracies in macro-level inputs. Consequently, the model becomes more reliable and effective in real-world applications.
% We use high-confidence multi-level labels to provide needed location information and foreground/background distribution information for location constraint and shape constraint learning. The multi-level learning strategy of learning location and shape by different losses derived from this segmentation definition is particularly advantageous for weakly supervised segmentation tasks with vague constraints, as it enhances the model’s robustness against minor inaccuracies in macro.

% This definition enables a clear distinction between foreground and background elements within the image, providing a structured framework for subsequent segmentation analysis.  
% This strategy is particularly advantageous for weakly supervised segmentation tasks with vague constraints, as it enhances the model’s robustness against minor inaccuracies in the introduced information.
\subsection{Objective}
\label{sec:objective}
% Image segmentation can be viewed as identifying regions with accurate location and shape. In fully supervised segmentation, the ground truth (G) provides clear location and shape information, enabling the network to learn feature distributions by computing the difference between predicted and actual regions. Unlike fully supervised methods, in weakly supervised segmentation for thyroid ultrasound images, annotations only offer rough location constraints without precise shape information. Consequently, traditional optimization objectives, which measure the difference between predicted and ground truth regions, are insufficient. Instead, we focus on directly evaluating the rationality of feature distribution across the image space.
Image segmentation aims to locate and delineate regions precisely. Fully supervised methods can use ground truth labels with clear location $Loc$ and shape$S$ to learn feature distributions through pixel-to-pixel comparison. However, weakly supervised approaches for thyroid ultrasound images can only refer to coarse localization cues without precise shape details. 
% While fully supervised methods use ground truth labels with clear location $Loc$ and shape information $S$ to learn feature distributions through pixel-to-pixel comparison, weakly supervised approaches for thyroid ultrasound images depend solely on coarse localization cues, lacking precise shape details. 
This limitation renders traditional optimization objectives for shape $S$ learning, prompting a shift toward directly assessing the rationality of feature distributions across the image space.
% Fully supervised segmentation can learn feature distributions by comparing predicted and ground truth (G) labels which provide clear location $Loc$ and shape information $S$. In contrast, weakly supervised methods for thyroid ultrasound images rely solely on rough location constraints without precise shape details, that render their traditional optimization objectives inadequate in $S$ learning, prompting a shift toward directly evaluating the rationality of feature distributions across the image space.

Let $I$ denote the image, with $R_f$ as the foreground region and $R_b$ as the background region. Sample sets within these regions are denoted by $X_f$ and $X_b$. The segmentation network is represented by $F(x; \theta)$, where $x$ is the input of the network and $\theta$ are the parameters. Beside the basic conditions: 1) the union of $R_f$ and $R_b$ equals the entire image ($R_f \cup R_b = I$); 2) their intersection is empty ($R_f \cap R_b = \emptyset$); 3) both regions of $R_f$ and $R_b$ are fully connected, weakly supervised algorithms to identify regions $R_f$ and $R_b$ should also satisfy:

\textbf{Objective 1}: $R_f$ should lie within a predefined location range $Loc$, while $R_b$ should be outside this range.
\begin{align}
R_f \subseteq Loc \And R_b \cap Loc = \emptyset
\label{condition1}
\end{align}

\textbf{Objective 2}: The prototype of the predicted foreground regions $P(F(X_f; a))$ should closely match the reference foreground prototype $P_f$, while the predicted background regions prototype $P(F(X_b; a))$ should align with the reference background prototype $P_b$, as shown below:
\begin{equation}
\begin{aligned}
% &||P(F(X \in X_f; a)) - P(F(X_f; a))||_2 < \epsilon_f,\\
% &||P(F(X \in X_b; a)) - P(F(X_b; a))||_2 < \epsilon_b,
&\mathcal{D}(P(F(X_f; \theta)), P_f) < \epsilon_f,\\
&\mathcal{D}(P(F(X_b; \theta)), P_b) < \epsilon_b,
\label{condition2}
\end{aligned}
\end{equation}
where $\mathcal{D}(\cdot, \cdot)$ denotes the distance between features prototype, and $\epsilon$ are small thresholds ensuring proximity of segmentation prototypes to reference prototypes.

\textbf{Objective 3}: The sampled feature distribution in the predicted foreground $F(X\in X_f; a))$ should align with the foreground prototype $P(F(X_f; a))$, and those in the predicted background $F(X\in X_b; a)$ should match the background prototype $P(F(X_b; a))$, as shown below:
\begin{equation}
\begin{aligned}
% &D_{\text{KL}}(P(F(X \in X_f; a)) || P(F(X_f; a))) < \delta_f, \\
% &D_{\text{KL}}(P(F(X \in X_b; a)) || P(F(X_b; a))) < \delta_b,
% &||F(X \in X_f; a) - P(F(X_f; a))||_2 < \delta_f,\\
% &||F(X \in X_b; a) - P(F(X_b; a))||_2 < \delta_b,
&\mathcal{D}(F(X \in X_f; a), P(F(X_f; a))) < \delta_f,\\
&\mathcal{D}(F(X \in X_b; a), P(F(X_b; a))) < \delta_b,
\label{condition3}
\end{aligned}
\end{equation}
where $\delta$ are thresholds ensuring similarity between sampled feature distribution and reference prototypes.

To achieve these objectives, high-confidence references used in the optimal process are essential as follows: 

\textbf{Reference 1}: A correct range $G_{box}$ must be provided to guide the segmentation location process $Loc$.

\textbf{Reference 2}: High-confidence labels $G_f$ and $G_b$ are necessary to define precise foreground and background references $P_f$ and $P_b$.

\subsection{High-confidence Multi-level labels Generation}
% In clinical examinations, doctors commonly use points for annotation, which comprises a major axis and a minor axis that are perpendicular to each other \cite{zhao2024IDMPS}. 
According to Reference 1 and 2 discussed in \ref{sec:objective}, weakly supervised segmentation requires spatial labels for location learning and region distribution labels for shape learning. In this section, we integrate geometric transformations on point annotations and segmentation from MedSAM to generate high-confidence box labels $G_{box}$ for location $Loc$ learning~\eqref{condition1}, as well as high-confidence foreground labels $G_f$ and background labels $G_b$ for shape $S$ learning~\eqref{condition2} and ~\eqref{condition3}.

% In this section, we integrate geometric transformations obtained by a series of geometric transform on point annotations and segmentation from MedSAM using prompts derived from point annotations, to generate high-confidence box labels $G_{box}$ for location $Loc$ learning, as well as high-confidence foreground labels $G_f$ and background labels $G_b$ for shape $S$ learning.

\begin{table}[htbp!]
    \centering
    \caption{The precision of different labels as region prototype. Box represents using bounding box as label, MedSAM denotes using MedSAM result as label, HC f/b represents our proposed high-confidence foreground/background labels.}
    \begin{tabular}{l|c|c|c|c}
        \hline
        \multirow{2}{*}{\textbf{Labels}} & \multicolumn{2}{c|}{\textbf{TN3K}} & \multicolumn{2}{c}{\textbf{DDTI}} \\
        \cline{2-5}
        & \textbf{Foreground} & \textbf{Background} & \textbf{Foreground} & \textbf{Background} \\
        \hline
        Box & $66.14\%$ & $99.98\%$ & $73.64\%$ & $99.98\%$ \\
            & $\pm 7.08\%$ & $\pm 0.75\%$ & $\pm 5.59\%$ & $\pm 0.56\%$ \\
        \hline
        MedSAM & $92.36\%$ & $95.85\%$ & $93.76\%$ & $96.97\%$ \\
            & $\pm 4.44\%$ & $\pm 3.21\%$ & $\pm 3.66\%$ & $\pm 2.38\%$ \\
        \hline
        HC f/b & $99.66\%$ & $99.99\%$ & $99.79\%$ & $99.99\%$ \\
            & $\pm 2.82\%$ & $\pm 0.50\%$ & $\pm 0.88\%$ & $\pm 0.01\%$ \\
        \hline
    \end{tabular}
    \label{tab:label_precision}
\end{table}

% we generate high-confidence labels containing topological prior to capture diverse topological information about the thyroid nodule. 
Specifically, as illustrated in Fig.~\ref{fig:framework}, we derive three geometric transformations representing low-level location information from clinical annotations:
\begin{itemize}
    \item Connecting the endpoints of the annotations along each axis to form quadrilateral regions.
    \item Identifying and filling the minimum bounding box enclosing these four points per target to create box regions encompassing all foreground pixels
    \item Negating the bounding box regions results into obtain background-only regions
\end{itemize}
% 1. By connecting the endpoints of the annotations along each axis, we form quadrilateral regions.

% 2. By identifying and filling the minimum bounding box enclosing these four points per target, we create box regions encompassing all foreground pixels.

% 3. Transforming the bounding box regions results in background-only regions.

The regions represent high-level semantic information are generated by MedSAM:
\begin{itemize}
    \item Using MedSAM with prompts computed from point annotations to obtain segmentation masks that reflecting anatomical distributions from input images.
\end{itemize}
% 4. The input image is processed by MedSAM, where point annotations are used to compute prompts for generating segmentation masks that reflect anatomical distributions. 
% The training input image is fed into MedSAM, and the corresponding point annotations are utilized to compute prompts for MedSAM. The irregular segmentation of MedSAM reflecting anatomical distributions are then used as confidence map.
% The input image is fed into MedSAM, where the corresponding point annotations are utilized to compute prompts for MedSAM, to generate irregular foreground masks reflecting anatomical distributions.

Finally, we fuse the geometric transformations of four-point annotations and results from MedSAM to generate high-confidence box, foreground, and background labels. 
% These labels effectively combine topological priors with anatomical knowledge to achieve precise region definitions for downstream learning tasks.
% , providing a robust foundation for downstream learning processes.

% While the regions derived solely from topological priors or anatomical knowledge, may lack sufficient accuracy, we enhance their precision by incorporating MedSAM predictions as confidence factors. Consequently, the refined box, foreground, and background labels better represent high-precision regions.
% The above-mentioned initial labels based on only topological prior or anatomical knowledge are still not accurate enough, so we use the prediction of MedSAM as a confidence factor, the refined box, foreground, and background labels are then referring regions with higher precision.

% so we use prediction of medical foundation models to introduce anatomical knowledge to help us obtain high-confidence multi-level labels. 

% refined to obtain high-confidence labels. 
% A weighted foreground label generation progress is shown in Fig.~\ref{fig:labels}. 

% 做融合的结果
% \begin{table}[htbp]
%     \centering
%     \caption{The precision of different labels for shape learning. Box represents using bounding box as labels, MedSAM denotes using MedSAM results as labels, HC f/b represents our proposed high-confidence foreground/background labels.}
%         \begin{tabular}{l|c|c|c|c}
%             \hline
%             \multirow{2}{*}{labels} & \multicolumn{2}{c|}{TN3K} & \multicolumn{2}{c}{DDTI} \\
%             \cline{2-5}
%             & foreground & background & foreground & background \\
%             \hline
%             % Initial f/b & 96.17\% & 99.98\% & 99.65\% & 99.98\%\\
%             Box & $66.14\% \pm 7.08\%$ & $99.98\% \pm 0.75\%$ & $73.64\% \pm 5.59\%$ & $99.98\% \pm 0.56\%$ \\
%             MedSAM & $82.907\% \pm 10.01\%$ & $92.70\% \pm 4.57\%$ & $89.13\% \pm 6.40\%$ & $95.18\% \pm 2.71\%$ \\
%             HC f/b & $99.66\% \pm 2.82\%$ & $99.99\% \pm 0.50\%$ & $99.79\% \pm 0.88\%$ & $99.99\% \pm 0.01\%$ \\
%             \hline
%         \end{tabular}
%     \label{tab:ablation_results}
% \end{table}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.48\textwidth]{LaTeX/pic/labels.png}
% \caption{{\textbf{Generation of pure foreground distribution.} The initial foreground label has a slight misalignment (red arrow) and MedSAM shows overfitting. By weighting the initial label with MedSAM, we obtain a refined high-confidence foreground label.}}
% \label{fig:labels}
% \vspace{-0.2in}
% \end{figure}

As illustrated in Table~\ref{tab:label_precision}, our generated high-confidence foreground and background labels achieve precision exceeding 99.5\% across domain distributions, guaranteeing that precise foreground and background reference, as outlined in \eqref{condition2} and \eqref{condition3}, can be learned from these label regions.
% This level of accuracy underscores the robust guidance provided by these labels in effectively facilitating the learning process of shapes, as outlined in \eqref{condition2} and \eqref{condition3}.

\subsection{High-rationality Multi-level Learning Strategy}
% After obtaining high-confidence multi-level labels, the network needs to learn the position and shape information contained in these multi-level labels through a reasonable learning strategy. Therefore, according to theory insight, this paper proposes a highly reasonable multi-level learning strategy. During the network training process, we use the alignment loss to learn the location-level information of thyroid nodules, the contrastive loss to learn the region-level shape information of thyroid nodules implicitly, and prototype correlation loss to learn the edge-level shape information of thyroid nodules explicitly.
According to the Objective 1, 2, and 3 discussed we proposed in \ref{sec:objective}, we design a high-rationality learning strategy that guides the network to learn location and shape features to satisfy all conditions for weakly supervised segmentation tasks, which consisting of the following three losses: alignment loss, contrastive loss and prototype correlation loss. 
% alignment loss to reach the requirement of \eqref{condition1}, contrastive Loss to reach the requirement of \eqref{condition2}, and prototype correlation loss to reach the requirement of \eqref{condition3}.
% to effectively address the challenges related to fully exploiting and utilizing multi-level annotation information, this study proposes a comprehensive learning framework. This framework integrates multiple loss functions to fully leverage the rich information in high-confidence multi-level annotations:
% To effectively exploit the rich information embedded in these high-confidence multi-level annotations, we propose a comprehensive learning framework. Our approach integrates multiple loss terms to capture different levels of anatomical details:

% 1. Alignment Loss: To capture the location-level characteristics $Loc$ of thyroid nodules.

% 2. Contrastive Loss: To learn the region-level shape feature representation of thyroid nodules $S$.

% 3. Prototype Correlation Loss: To supervise the feature distribution in edge-level shape representations $S$.

% Together, these components enable the network to simultaneously attend to precise positioning and diverse morphological features.

% \subsubsection{Alignment Loss for Location-level Spatial Learning}
\subsubsection{Alignment Loss for Location Learning}
% We incorporate location information of thyroid nodules through an alignment loss, which comprises two key components: one evaluates the alignment between the projected segmentation results and the bounding box labels, while the other ensures the consistency of adjacent pixels within high-confidence foreground regions of the segmentation results. This approach enables rough weak supervision labels to effectively constrain the segmentation localization while preventing the network from being overly constrained by a single segmentation shape. 
To learn the location information as described in \eqref{condition1}, the alignment loss consists of two components: 1) alignment of projected segmentation results with bounding box labels, and 2) topological continuity loss in high-confidence foreground regions. 

For a predicted result $m_i$ and bounding box label $G_{box}$ derived from the points annotations, we define the projection of each box label mask as:
\begin{equation}
\begin{aligned}
% \operatorname{proj}_{x}(Y_{i}^{box}) &= \max_{col}(Y_{i}^{box}) = l_{x},\\
% \operatorname{proj}_{y}(Y_{i}^{box}) &= \max_{row}(Y_{i}^{box}) = l_{y},
p_{x} = \max_{col}(G_{box})&, p_{y} = \max_{row}(G_{box}),&\\
\tilde{p_x} = \max_{col}(m_i)&, \tilde{p_y} = \max_{row}(m_i),
\label{proj}
\end{aligned}
\end{equation}
% where $proj_x: {\mathbb{R}}^{H \times W} \to {\mathbb{R}}^W$ and $proj_y: {\mathbb{R}}^{H \times W} \to {\mathbb{R}}^H$ represent the operations of projecting the mask onto the x-axis and y-axis, respectively. 
% This projection operation is achieved by computing the maximum value of the mask along each axis. Specifically, $\max_{col}$ and $\max_{row}$ refer to the maximum operations along the columns and rows, respectively. 
% The projections $l_x$ and $l_y$ are derived from the box labels $m_{box}$, while the projections $\tilde{l_x}$ and $\tilde{l_y}$ are from the predicted mask $m_i$. The first part of alignment loss $L_{\text{align1}}$ between the box labels and the predicted mask is then computed as follows:
where $p_x$ and $p_y$ denote projections derived from the box label $G_{box}$ onto the x-axis and y-axis, respectively, $\tilde{p_x}$ and $\tilde{p_y}$ are the projections of predicted result $m_i$ onto the x and y axixes. 

The first part $L_{ali-proj}$  of alignment loss between the predicted mask and the box label is then computed as follows:
% \begin{equation}
\begin{align}
L_{ali-proj} = \frac{2 \times |\tilde{p_x} \cap p_x|}{|\tilde{p_x}| + |p_x|} + \frac{2 \times |\tilde{p_y} \cap p_y|}{|\tilde{p_y}| + |p_y|}.
\end{align}
% \end{equation}

This projection loss provides region localization constraints with high feasibility and efficiency. 

To prevent extreme predictions on coordinate projections, we introduce topological continuity loss as the second component of the alignment loss.
% The loss between projections of prediction and box labels significantly improves both computational efficiency and feasibility for region localization. However, it lacks sufficient regularization in local optimization that the network intends to predict the coordinate axes. To prevent the projection loss from leading to extreme coordinate axis projections being misinterpreted as valid localization during training, We introudced adjacent pixels consistency loss as the second part of alignment loss.
Given the high-confidence foreground labels $G_{f}$ and the prediction result ${m_i}$, target regions in the high-confidence foreground are calculated as:
\begin{align}
m_{i}' = {m_i} \cdot G_{f},
\end{align}
where $m_{i}'$ represents the predicted areas in high-confidence foreground areas. The topological continuity loss $L_{ali-topo}$ is defined as:
% \begin{equation}
\begin{align}
L_{ali-topo} = -\left[ m_{i}' \log (G_{f})+(1 - m_{i}') \log (1 - G_{f}) \right].
\label{consistency}
\end{align}
% \end{equation}

% \end{equation}

This loss ensures topological continuity within predicted foreground regions covered by the high-confidence foreground. Despite variations in the shapes of thyroid nodules, pixels within these high-confidence regions achieve classification accuracy of over $99.66\%$. This specific loss function is exclusively utilized to identify foreground regions to prevent local optima in coordinate axis-based optimization.

% This loss effectively avoids significant noise during the learning process while maintaining coherent segmentation and preventing local optima in coordinate axis-based optimization.

% Although thyroid nodules vary in shape, pixels in high-confidence regions have an over $99.66\%$ probability of being foreground. Thus, the $L_{ali-topo}$ loss ensures topological continuity between high-confidence foreground masked predicted regions will not introduce significant noise for learning, just maintaining coherent segmentation and preventing local optima in coordinate axis learning.

% Although thyroid nodules vary in shape, pixels in the high-confidence foreground region have a $98.5\% \pm 0.82\%$ probability of belonging to the foreground category. Therefore, the $L_{align2}$ loss enforces the region label consistency between predicted pixels and corresponding masked high-confidence foreground, ensuring that the predicted pixels align with the high-confidence foreground. This clearly indicates that the segmentation is a coherent region, preventing the network from getting stuck in a local optimum when learning position loss on the predicted coordinate axes.

By combining loss $L_{ali-proj}$ and loss $L_{ali-topo}$, we obtain the final alignment loss $L_{align}$ as follows:
% \begin{eqnarray}
\begin{align}
L_{align} = L_{ali-proj} + L_{ali-topo}.
% \end{eqnarray}
\end{align}

This loss allows weak supervision labels to constrain segmentation localization effectively while avoiding over-fitting to a single shape. 

% \subsubsection{Contrastive Loss for Region-level Semantic Feature Learning}
\subsubsection{Contrastive Loss for Region-level Shape Feature Learning}
Due to the absence of ground truth masks in weak supervision, as outlined in \ref{sec:objective}, we propose to learn the segmentation shape by refining the feature representation of foreground and background regions to achieve~\eqref{condition2}. 

% using \textbf{high-confidence} foreground/background labels, thyroid images are partitioned into \textbf{high-confidence} foreground, \textbf{high-confidence} background, and \textbf{uncertainty} regions, 
The proposed contrastive loss function aims to learn region-level discriminative feature representations from high-confidence foreground areas $G_f$ and background areas $G_b$, the generic contrastive loss with a sampling scale size $k \times k$, denoted as $L_{con}^k$, which is formally defined as:
\begin{align}
% L_{con}^k = \frac{1}{n} \sum_{q_k^{+} \in Q_{p}} ( -\frac{q_k \cdot q_k^{+}}{\tau}+\log ( e^{\frac{q_k \cdot q_k^{+}}{\tau}} + \sum_{q_k^{-} \in Q_{n}} e^{\frac{q_k \cdot q_k^{-}}{\tau}})),
L_{con}^k = \frac{1}{n} \sum_{q_k^{+} \in Q_{p}} -\log \left( \frac{e^{\frac{q_k \cdot q_k^{+}}{\tau}}}{e^{\frac{q_k \cdot q_k^{+}}{\tau}} + \sum_{q_k^{-} \in Q_{n}} e^{\frac{q_k \cdot q_k^{-}}{\tau}}} \right),
\end{align}
where $Q_{p}$ and $Q_{n}$ represent the sets of positive and negative feature embedding queues, respectively. $q_k^{+}$ denotes a foreground feature sample of size $k\times k$, while $q_k^{-}$ represents corresponding background feature samples. The anchor feature queue $q$ is selected from high-confidence foreground regions $G_f$. The temperature parameter $\tau > 0$ controls the slope of the loss function and its smoothness. Empirically, we evaluate the contrastive loss at scale sizes $k=3$ and $k=1$, with $\tau = 0.07$.

The contrastive loss is designed to minimize the distance between intra-class features (i.e., either both foregrounds or both backgrounds) in the embedding space and maximize the distance between inter-class features (i.e., foreground and background). Through this learning mechanism, the obtained feature representations effectively sharpen the classification boundaries between foreground and background regions. This enhancement directly improves the network's ability to distinguish between these regions, ultimately refining its predicted prototypes for both categories to align closely with reference standards.

% Through contrastive learning, the obtained feature representations sharpen the classification boundaries between point and patch feature vectors in the feature space. This enhances the network's ability to distinguish foreground from background during segmentation. By leveraging these highly discriminative features, the network implicitly transfers foreground and background information to the segmentation module, resulting in more accurate region segmentation.
% \begin{eqnarray}
% L_{cnt}^k = \frac{1}{n} \sum_{q_k^{+} \in Q_{p}} -\log (\frac{e^{\frac{q_k \cdot q_k^{+}}{\tau}}}{e^{\frac{q_k \cdot q_k^{+}}{\tau}} + \sum_{q_k^{-} \in Q_{n}} e^{\frac{q_k \cdot q_k^{-}}{\tau}}}),
% L_{\text{cnt}}^k = \frac{1}{n} \sum_{q_k^{+} \in Q_p} -\log \left( \frac{e^{\frac{q_k \cdot q_k^{+}}{\tau}}}{e^{\frac{q_k \cdot q_k^{+}}{\tau}} + \sum_{q_k^{-} \in Q_n} e^{\frac{q_k \cdot q_k^{-}}{\tau}}} \right)
% \end{eqnarray}
% The feature representations obtained through contrastive learning make the classification boundaries between points and patch feature vectors in the feature space more distinct, thereby enhancing the network's ability to segment the foreground and background. By utilizing the highly discriminative features learned through contrastive learning, the network implicitly transfers foreground and background information to the segmentation part, resulting in a more accurate segmentation of the region.

\subsubsection{Prototype Correlation Loss for Edge-level Shape learning}
% Previous constraints on segmentation have already been applied at the localization and region levels. According to the definition of the multi-level segmentation task proposed in this paper, we further impose semantic constraints on the segmentation shape at the edge level. As shown in Fig.~\ref{fig:correlation_loss}, we introduce a prototype correlation loss, which consists of two parts. One is the complementary consistency between the deep feature representations of high-confidence foreground and background features. the second is the consistency of the segmentation results and fused correlation segmentation.
To learn more precise segmentation shapes, building on the segmentation objective defined as \eqref{condition3}, we extend the shape constraint to edge-level by introducing a prototype correlation loss, as illustrated in Fig.~\ref{fig:correlation_loss}. This loss comprises two components: 1) complementary consistency between feature correlations refer to high-confidence foreground prototype and background prototype, and 2) consistency between network segmentation results and fused correlation segmentation results.

% By comparing the complementary consistency between the deep feature representations of high-confidence foreground and background features, we reduce the uncertainty of the segmentation result's shape boundaries. Additionally, by learning the consistency of the segmentation results after integrating the network's predictions with the foreground and background feature correlations, we propagate the fused segmentation boundaries to the predicted results, thereby achieving explicit boundary learning.
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.48\textwidth]{correlation_loss_new.png}
\caption{Diagram of Prototype Correlation Loss Competition.}
\label{fig:correlation_loss}
\end{figure}
Given the high-confidence foreground and background labels $G_f$ and $G_b$, we first extract the corresponding region features \( P_f \) and \( P_b \). Global pooling is then applied to reduce the dimensionality of these features to \( C \times 1 \times 1 \) where C means channels to obtain the prototype features. Using metric learning, we evaluate the correlation response of each position in the feature map $F$ with respect to the foreground prototypes and background prototypes, obtaining the foreground correlation response \( C_f \) and the background correlation response \( C_b \) as follows: 
\begin{equation}
\begin{aligned}
C_f = \max\left(0, \frac{F^T\cdot P_f}{\|P_f\|_2 + \epsilon}\right),\\
C_b = \max\left(0, \frac{F^T\cdot P_b}{\|P_b\|_2 + \epsilon}\right).
\label{cf_cb}
\end{aligned}
\end{equation}


Since foreground and background are mutually exclusive in segmentation tasks, the foreground correlation response \( \hat{C_f} \) derived from the background prototype \( C_b \) as follows:
% should be consistent with the foreground correlation response \( C_f \). 
\begin{align}
\hat{C_f} = 1 - C_b.
\label{back_corr}
\end{align}

The correlation map represents the similarity between image features and each prototype. The first component $L_{cor-fe}$ of the prototype correlation loss reflects the complementary consistency between the feature correlations referring to high-confidence foreground and background prototypes, which is defined as follows:
% \begin{eqnarray}
% L_{corr_1} &= -\frac{1}{2}\left[ C_f \log (\hat{C_f}) \right. \quad + \left. (1 - C_f) \log (1 - \hat{C_f}) \right]\\ 
% &-\frac{1}{2}\left[ \hat{C_f} \log (C_f) \right. \quad + \left. (1 - \hat{C_f}) \log (1 - C_f) \right] 
% % L_{corr_1} &=& \frac{2 \times |C_f \cap \hat{C_f}|}{|C_f| + |\hat{C_f}|}.
% \label{correlation1}
% \end{eqnarray}
% \begin{equation}
% \begin{aligned}
\begin{align}
L_{cor-fe} = -\frac{1}{2}\left[ C_f \log (\hat{C_f})\right. 
+\left. (1 - C_f) \log (1 - \hat{C_f}) \right] \nonumber\\ 
-\frac{1}{2}\left[ \hat{C_f} \log (C_f)\right. 
+\left. (1 - \hat{C_f}) \log (1 - C_f) \right].
\label{correlation1}
\end{align}

% \end{aligned}
% \end{equation}

The fused predictions \( C_i \) that should exhibit the same region distribution as that of the segmentation branch are obtained by balancing the foreground correlation maps \( C_f \) based on the foreground and those  \( \hat{C_f} \) on the background prototypes. Therefore, the second component \( L_{cor-seg} \) of the correlation loss measures the consistency between the fused correlation map and the segmentation prediction is defined as:
% By balancing the foreground correlation maps \( C_f \) based on the foreground and those  \( \hat{C_f} \) on the background prototypes, the fused predictions \( C_i \) are obtained that should exhibit the same region distribution as that of the segmentation branch. Therefore, the second component of the correlation loss \( L_{cor-seg} \) measures the consistency between the fused correlation map and the segmentation prediction:
\begin{align}
L_{cor-seg} = \frac{2 \times |m_i \cap \hat{C_i}|}{|m_i| + |\hat{C_i}|}.
\label{correlation2}
\end{align}

The total prototype correlation loss \( L_{corr} \) is then calculated as follows:
\begin{align}
L_{corr} = L_{cor-fe} + L_{cor-seg}.
\label{correlation_loss}
\end{align}

By considering the complementary consistency of foreground and background prototype correlation, the algorithm obtained segmentation edges with low uncertainty. By directly propagating the fused segmentation edges to the predicted results, we achieved explicit shape learning, effectively addressing the challenge of refining the edges of the nodule region. 

\subsubsection{Overall Loss Function}
By combining the losses of learning location information and shape information mentioned above, the overall loss function during the training process can be expressed as:
\begin{align}
L_{all} = L_{align} + \lambda L_{con} + \beta \cdot L_{corr},\label{loss_loc_shape}
\end{align}
where $L_{all}$ indicates the overall loss, $L_{align}$ represents alignment loss. is  $L_{con}$ represents Contrastive Loss and $L_{corr}$ is prototype correlation loss, $\lambda$ and $\beta$ are weighted parameter.

\section{Experiments}
\subsection{Experimental Materials}
\subsubsection{Dataset}
To evaluate the effectiveness of the proposed segmentation framework, we conduct ablation and comparative experiments on two publicly available thyroid ultrasound datasets: TN3K\cite{gong2021tn3k} and DDTI\cite{pedraza2015DDTI}. The TN3K dataset consists of 3,494 high-resolution thyroid nodule images, following a standardized clinical split protocol with 2,879 images designated for training and 614 for testing. For the DDTI dataset, which contains 637 thyroid ultrasound scans, the data is randomly split into training and testing sets at a 4:1 ratio. Additionally, 10-fold cross-validation was applied to both datasets to address the challenge of limited data while ensuring statistical reliability.

% The TN3K dataset includes 3,494 images, with 2,879 images for training and 614 images for testing. 
% The TN3K dataset comprises 3,494 high-resolution thyroid nodule images with pixel-level annotations, following a standardized clinical split protocol where 2,879 images are designated for training and 614 for testing. 
% For the DDTI dataset containing 637 thyroid ultrasound scans with expert-validated nodule boundaries, we adopt 10-fold cross-validation to mitigate data scarcity challenges while ensuring statistical reliability. 

Notably, in the TN3K and DDTI datasets, the ground truth annotations of thyroid nodules in ultrasound images for metric calculation were performed by experienced radiologists, while point annotations were carried out by less-experienced senior medical students to simulate a real-world weakly supervised learning scenario.
% to simulate a real-world weakly supervised learning scenario. This differential annotation approach provides our experiments with more representative evaluation benchmarks. 

% Experimental results demonstrate that our algorithm can achieve precise segmentation using coarse point labels, thus providing better support for clinical diagnosis.

\subsubsection{Evaluation Metrics}
We performed a quantitative comparison using four common segmentation evaluation metrics: 
\textbf{Mean Intersection over Union (mIoU)}~\cite{ling2023dsc_iou_pr}, \textbf{Dice Similarity Coefficient (DSC)}~\cite{ling2023dsc_iou_pr, wang2022dsc_pr, zhu2024dsc_hd}, 
\textbf{Hausdorff Distance (HD)}~\cite{zhu2024dsc_hd}, and \textbf{Prediction Precision (Pr)}~\cite{ling2023dsc_iou_pr, wang2022dsc_pr}, which evaluate the segmentation's overall accuracy, precision of segmented regions, boundary matching, and the reliability of predicted positives, respectively.

\subsubsection{Parameter Setting and Implementation}
For training the network, we set a learning rate of 0.0001, a batch size of 16, and trained for 100 epochs with images resized to $256 \times 256$. The network was optimized using the Adam optimizer. For comparative experiments, we adhered to the parameter configurations outlined in their respective papers. All experiments were carried out using PyTorch on an Nvidia 3090 GPU equipped with 24GB of memory.

% \begin{figure*}[!htbp]
% \centering
% \includegraphics[width=0.98\textwidth]{LaTeX/pic/compare_result.png}
% \caption{\textbf{Quantitative comparison results on TN3K and DDTI.} The top three rows show validation results for the model trained on TN3K, while the bottom three rows show results for DDTI. Red indicates correct thyroid predictions, green represents missing thyroid segmentation, and blue shows an overestimation of other organs as the thyroid.}
% \label{fig:comparison_result}
% \end{figure*}

\subsection{Comparisons with State-of-the-art}
\subsubsection{Comparative Results on TN3K Dataset}
\begin{table*}[!htbp]
\centering
\caption{Quantitative comparison results of different methods on the TN3K and DDTI dataset}
\resizebox{0.98\textwidth}{!}{%
\begin{tabular}{l|cccc|cccc}
\hline
\multirow{2}{*}{Method(backbone)} & \multicolumn{4}{c}{TN3K} & \multicolumn{4}{c}{DDTI} \\
\cline{2-9}
 & \makecell{mIoU(\%)$\uparrow$} & \makecell{HD(mm)$\downarrow$} & \makecell{DSC(\%)$\uparrow$} & \makecell{Pr(\%)$\uparrow$} & 
 \makecell{mIoU(\%)$\uparrow$} & \makecell{HD(mm)$\downarrow$} & \makecell{DSC(\%)$\uparrow$} & \makecell{Pr(\%)$\uparrow$} \\
\hline
\hline
U-Net\cite{ronneberger2015unet} & \textcolor[rgb]{0.80,0.40,0.50}{$64.76\pm25.01$} & \textcolor[rgb]{0.80,0.40,0.50}{$5.83\pm2.20$} & \textcolor[rgb]{0.80,0.40,0.50}{$75.13\pm23.32$} & \textcolor[rgb]{0.80,0.40,0.50}{$77.66\pm25.89$} & \textcolor[rgb]{0.80,0.40,0.50}{$54.24\pm22.48$} & \textcolor[rgb]{0.80,0.40,0.50}{$7.05\pm1.82$} & \textcolor[rgb]{0.80,0.40,0.50}{$66.80\pm18.35$} & \textcolor[rgb]{0.80,0.40,0.50}{$68.75\pm19.84$} \\
Dense-UNet\cite{cai2020denseunet} & \textcolor[rgb]{0.50, 0.40, 0.95}{$66.61\pm25.74$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$5.26\pm2.06$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$76.27+25.19$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$77.95+26.47$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$52.07\pm23.38$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$7.09\pm1.87$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$64.99\pm23.00$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$65.37\pm28.39$} \\
Cenet\cite{tao2022cenet} & \textcolor[rgb]{0.20, 0.80, 0.10}{$74.07\pm21.11$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$4.70\pm1.84$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$82.95\pm18.80$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$83.52\pm19.48$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$66.63\pm21.72$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$5.88\pm1.76$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$77.46\pm19.50$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$77.44\pm23.63$} \\
% Cenet\cite{tao2022cenet} & $\textbf{74.07}\pm\textbf{21.11}$ & $\textbf{4.70}\pm\textbf{1.84}$ & $\textbf{82.95}\pm\textbf{18.80}$ & $\textbf{83.52}\pm\textbf{19.48}$ & $\textbf{66.63}\pm\textbf{21.72}$ & $\textbf{5.88}\pm\textbf{1.76}$ & $\textbf{77.46}\pm\textbf{19.50}$ & $\textbf{77.44}\pm\textbf{23.63}$ \\
\hline
\hline
SCRF\cite{zhang2020scrf} & $56.68\pm21.85$ & $6.58\pm1.89$ & $69.19+23.00$ & $63.47+22.81$ & $50.29\pm20.38$ & $7.86\pm1.62$ & $64.22\pm20.19$ & $57.27\pm24.04$ \\
UNCRF\cite{mahani2022uncrf}& $56.27\pm23.89$ & $6.51\pm1.97$ & $68.21\pm25.25$ & $66.29\pm24.58$ & $49.55\pm20.67$ & $7.64\pm1.58$ & $63.42\pm20.84$ & $61.52\pm25.6$ \\
BoxInst\cite{tian2021BoxInst} & \underline{$65.42\pm22.54$} & $5.47\pm2.10$ & \underline{$76.22+21.47$} & $76.99+22.68$ & $44.62\pm17.03$ & $7.50\pm1.39$ & $59.67\pm17.44$ & $64.52\pm29.32$ \\
WSDAC\cite{li2023wsdac} & $57.22\pm22.45$ & $5.41\pm1.85$ & $70.58\pm17.33$ & $76.81\pm28.02$ & $52.05\pm17.46$ & $6.62\pm1.59$ & $66.41\pm18.05$ & \underline{$70.93\pm26.04$} \\
S2ME\cite{wang2023s2me} & $59.47\pm23.37$ & $5.34\pm2.10$ & $70.97+23.41$ & $75.42+25.73$ & $51.60\pm23.65$ & $7.01\pm1.52$ & $64.54\pm23.66$ & $68.97\pm28.17$ \\
    IDMPS\cite{zhao2024IDMPS} & $62.76\pm26.64$ & \underline{$5.21\pm1.99$} & $73.47\pm26.73$ & \underline{$80.40\pm25.45$} & \underline{$57.03\pm23.11$} & $\underline{6.55\pm1.84} $ & \underline{$69.43\pm22.02$} & $69.44\pm26.89$ \\
\hline
\hline
% MLMC(unet) & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{68.91}\pm\textbf{22.92}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{5.01}\pm\textbf{2.01}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{78.63}\pm\textbf{22.22}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{80.64}\pm\textbf{22.69}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{59.13}\pm\textbf{22.25}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{6.09}\pm\textbf{1.68}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{71.41}\pm\textbf{20.51}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{72.28}\pm\textbf{25.34}$} \\
HCL-HRL(U-Net) & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{69.30}\pm\textbf{22.38}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{5.01}\pm\textbf{2.01}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{79.10}\pm\textbf{21.37}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{80.64}\pm\textbf{22.69}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{59.82}\pm\textbf{21.98}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{6.41}\pm\textbf{1.68}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{72.14}\pm\textbf{20.76}$} & \textcolor[rgb]{0.80,0.40,0.50}{$\textbf{72.96}\pm\textbf{24.60}$} \\
HCL-HRL(Dense-UNet)  & \textcolor[rgb]{0.50, 0.40, 0.95}{$68.77\pm23.68$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$5.12\pm2.10$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$78.32\pm22.779$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$79.04\pm23.23$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$59.00\pm21.07$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$6.56\pm1.69$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$71.64\pm19.77$} & \textcolor[rgb]{0.50, 0.40, 0.95}{$72.50\pm25.21$} \\
HCL-HRL(CENet) & \textcolor[rgb]{0.20, 0.80, 0.10}{$73.32\pm20.12$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$4.76\pm1.89$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$82.56\pm17.97$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$85.31\pm18.80$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$69.18\pm20.34$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$5.81\pm1.83$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$79.66\pm17.89$} & \textcolor[rgb]{0.20, 0.80, 0.10}{$77.73\pm22.36$} \\
\hline
\end{tabular}}
\label{tab:comparison_result}
\end{table*}

% In order to prove the progressiveness and robustness of the performance of our algorithm, 
The segmentation performance of our proposed method was compared against
the state-of-art weakly supervised methods SCRF\cite{zhang2020scrf}, UNCRF\cite{mahani2022uncrf}, BoxInst\cite{tian2021BoxInst}, WSDAC\cite{li2023wsdac}, S2ME\cite{wang2023s2me}, IDMPS\cite{zhao2024IDMPS}, and fully supervised methods U-Net\cite{ronneberger2015unet}, Dense-UNet\cite{cai2020denseunet} and Cenet\cite{tao2022cenet}. The quantitative results of TN3K and DDTI are shown in Table.~\ref{tab:comparison_result}. The qualitative results of TN3K and DDTI are shown in Fig.~\ref{fig:comparison_tn3k} and Fig.\ref{fig:comparison_ddti}, respectively.

% Table~\ref{tab:comparison_result} presents the performance of our method compared to existing state-of-the-art approaches for weakly supervised semantic segmentation. Using the baseline U-Net backbone, our method achieves a 6.15\% increase in Mean Intersection over Union (mIoU) compared to the single-level learning method IDMPS and a 3.5\% increase over the method BoxInst. Furthermore, using the same feature extraction backbone, our network either improves upon or matches the segmentation mIoU of fully supervised methods, even when trained with weakly supervised labels. Our method shows a 4.15\% increase in mIoU compared to the fully supervised U-Net model, and only a 0.07\% reduction compared to CeNet. This trend is consistent with Dice Similarity Coefficient (DSC), which follows the same pattern as mIoU. In terms of Hausdorff Distance (HD), our approach also demonstrates improvements. Specifically, we observe a reduction of 0.20 compared to the single-level method WSDAC and 0.16 compared to the multi-level method WSDAC. The HD for our method is reduced by 0.8 compared to the fully supervised U-Net model. Additionally, precision shows a slight improvement over the single-level method IDMPS and the multi-level method WSDAC. Notably, precision improves by more than 2\% compared to the corresponding fully supervised networks.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.98\textwidth]{compare_tn3k.png}
\caption{\textbf{Quantitative comparison results on TN3K dataset.} Red indicates correct thyroid predictions, green represents missing thyroid segmentation, and blue shows an overestimation of other organs as the thyroid.}
\label{fig:comparison_tn3k}
\end{figure*}
% the fully supervised networks U-Net, Dense-UNet, and CENet yield results that exhibit minor over-segmentation and under-segmentation. 
As shown in Fig.~\ref{fig:comparison_tn3k}, SCRF and UNCRF tended to produce box-like segmentation with significant over-segmentation. WSDAC and S2ME frequently under-segmented thyroid nodules in images with multiple nodules or irregular nodule shapes. While BoxInst and IDMPS outperformed other existing methods in high-contrast scenarios, such as the example in the third row, they still struggled with under-segmentation of incomplete nodules located at image boundaries. In contrast, our proposed method achieved more consistent segmentation regions and delicate segmentation edges, while exhibiting even less over- and under-segmentation than fully supervised networks using the same backbone. Quantitative results in Table~\ref{tab:comparison_result} further support that our framework using the U-Net backbone outperformed state-of-the-art weakly-supervised methods. Specifically, it achieved an average mIOU of 69.30\%, a Hausdorff distance (HD) of 5.01 mm, a DSC of 79.10\%, and a precision (Pr) of 80.64\%. These results were even better than the fully supervised U-Net, which achieved 64.76\% mIOU, 5.83 mm HD, 75.13\% DSC, and 77.66\% Pr.
% As shown in Figure~\ref{fig:comparison_tn3k}, our proposed weakly supervised model demonstrates outstanding performance in generating accurate segmentation results. Existing single-stage learning methods (such as SCRF and UNCRF) tend to produce box-like segmentation results when handling multi-targets or complex objects with unclear features, significantly reducing segmentation accuracy. These methods also tend to over-segment small objects, leading to suboptimal results. Although S2ME and IDMPS generate pseudo-labels through point labels and balance predictions from dual backbones, S2ME often over-segmentation in ultrasound images, occasionally misclassifying laryngeal tissue as thyroid nodules. While IDMPS performs better than S2ME for thyroid segmentation, it still struggles with over-segmenting large targets and under-segmenting medium-sized targets, particularly in images with blurry edges. In contrast, our method effectively overcomes these limitations, achieving more accurate and consistent segmentation across different target sizes and complexities.

% Qualitative results in Fig.~\ref{fig:comparison_tn3k} highlight the strong potential of our weakly supervised model to produce more precise outputs. Single-level learning methods like SCRF and UNCRF algorithms tend to produce box-like segmentations when confronted with multiple or challenging targets that exhibit indistinct features, which significantly impairs segmentation accuracy. Additionally, these algorithms often generate larger masks than necessary for small objects, leading to suboptimal performance. S2ME and IDMPS, which leverage point labels to generate pseudo-labels, aim to balance predictions from two backbones. S2ME tends to over-segment regions in ultrasound images, occasionally misclassifying laryngeal tissue as thyroid nodules. Although IDMPS is better suited for thyroid segmentation than S2ME, it still struggles with the over-segmentation of large targets and under-segmentation of medium-sized targets, especially in images with blurry edges. 

% As illustrated in Fig.~\ref{fig:comparison_tn3k}, the superior performance of our weakly supervised model in producing precise segmentation outputs. Existing single-level learning methods, such as SCRF and UNCRF, often yield box-like segmentation results when handling multiple or challenging targets with indistinct features, significantly compromising segmentation accuracy. These methods also tend to over-segment small objects, leading to suboptimal results. While S2ME and IDMPS leverage point labels to generate pseudo-labels and balance predictions from dual backbones, S2ME frequently over-segmentation ultrasound images, sometimes misclassifying laryngeal tissue as thyroid nodules. Although IDMPS performs better than S2ME for thyroid segmentation, it still struggles with over-segmenting large targets and under-segmenting medium-sized ones, particularly in images with blurry edges. In contrast, our method effectively addresses these limitations, producing more accurate and consistent segmentations across varying target sizes and complexities.


% Using the U-Net backbone, our method achieves a 6.15\% improvement in Mean Intersection over Union (mIoU) compared to the single-level learning method IDMPS and a 3.5\% increase over BoxInst. Notably, our method either matches or surpasses the segmentation performance of fully supervised methods, even when trained with weakly supervised labels. Specifically, our approach achieves a 4.15\% higher mIoU than the fully supervised U-Net model and is only 0.07\% lower than CeNet. Similar trends are observed for the Dice Similarity Coefficient (DSC), which aligns closely with mIoU. In terms of Hausdorff Distance (HD), our method reduces the metric by 0.20 compared to WSDAC and by 0.16 compared to IDMPS. Additionally, our HD is 0.8 lower than that of the fully supervised U-Net model. Precision also shows slight improvements over IDMPS and WSDAC, with a 2\% increase compared to fully supervised networks. These quantitative results underscore the robustness and effectiveness of our method in achieving state-of-the-art performance.

\subsubsection{Comparative results on DDTI dataset}
% As shown in Table~\ref{tab:comparison_result}, our method also shows improvements across all metrics compared to other methods on the DDTI dataset. Specifically, mIoU shows a 2.1\% increase over the single-level method IDMPS and a 6.95\% increase over the multi-level method WSDAC. Our framework, when implemented with the same backbone as the fully supervised method, also results in significant improvements. The DSC and precision metrics exhibit the same trend as mIoU. Moreover, our method achieves the lowest HD compared to other methods. 
\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.98\textwidth]{compare_ddti.png}
\caption{\textbf{Quantitative comparison results on DDTI dataset.} Red indicates correct thyroid predictions, green represents missing thyroid segmentation, and blue shows an overestimation of other organs as the thyroid.}
% \vspace{-0.2in}
\label{fig:comparison_ddti}
\end{figure*}
% xianzaishi
As illustrated in Fig.~\ref{fig:comparison_ddti}, fully supervised networks exhibited substantial over-segmentation in images where the foreground and background tissues are similar. Weakly supervised algorithms, such as SCRF, UNCRF, and S2ME, struggled with severe over-segmentation of small targets, as well as in images with similar foreground and background tissues. Both BoxInst and WSDAC encountered significant challenges with over- and under-segmentation when processing such images. Notably, BoxInst delivered superior segmentation results with minimal over-segmentation in simpler background scenarios, while WSDAC tended to exhibit more under-segmentation in these cases. IDMPS outperformed other weakly supervised algorithms by reducing over-segmentation in small nodules, however, it still faced substantial issues with under-segmentation and excessive over-segmentation in more complex cases. In contrast, our algorithm not only reduced over-segmentation but also illustrated enhanced shape adaptation, surpassing fully supervised methods. Additionally, by incorporating a more efficient feature extraction backbone such as CENet, we achieved better segmentation precision, with accurate shape delineation and fine edge fitting. The quantitative comparison presented in Table~\ref{tab:comparison_result} further substantiates these qualitative observations.
% As shown in Fig.~\ref{fig:comparison_ddti}, fully supervised networks exhibit significant over-segmentation in images with similar foreground and background tissues. Weakly supervised algorithms such as SCRF, UNCRF, and S2ME struggle with the heavy over-segmentation of small targets and images with similar foreground and background tissues. BoxInst and WSDAC both encounter substantial issues with over- and under-segmentation when processing images with similar foreground and background tissues. Notably, BoxInst achieves superior segmentation results with minimal over-segmentation in simpler background scenarios, whereas WSDAC tends to exhibit more under-segmentation in these cases. IDMPS outperforms other weakly supervised algorithms by reducing over-segmentation on small nodules, however, it still faces challenges with significant under-segmentation and excessive over-segmentation in complex ones. In comparison, our algorithm reduces over-segmentation while demonstrating improved shape adaptation surpassing fully supervised methods. Furthermore, incorporating a more efficient feature extraction backbone such as CENet enhances segmentation precision with accurate shape segmentation and fine edge fitting. The quantitative comparison from Table~\ref{tab:comparison_result} further confirms these qualitative observations.
% As demonstrated in Fig.~\ref{fig:comparison_ddti}, fully supervised networks exhibit significant over-segmentation in complex background images as illustrated by the second and third examples. Weakly supervised algorithms SCRF, UNCRF, and S2ME have heavy over-segmentation of small targets and also struggle with significant over-segmentation in complex background images. BoxInst and WSDAC both face considerable issues with over-segmentation and under-segmentation when dealing with complex background images. Notably, BoxInst achieves superior segmentation results with minimal over-segmentation in simpler background scenarios, whereas WSDAC tends to display more under-segmentation in these cases. IDMPS outperforms other weakly supervised algorithms by reducing over-segmentation on small nodules in complex backgrounds images, but it still face under-segmentation in simple backgrounds and excessive over-segmentation in complex ones. In comparison, our algorithm, utilizing a generic backbone network like U-Net, effectively reduces over-segmentation in both complex background examples (second and third) while demonstrating improved shape adaptation in simpler backgrounds—thereby surpassing fully supervised methods. Furthermore, by incorporating a more efficient feature extraction backbone such as CENet, our approach achieves precise segmentation results with minimal over-segmentation and under-segmentation, even under challenging conditions in complex background scenarios. 
% BoxInst, effective for medium-sized targets, struggles with both under- and over-segmentation at large target boundaries, leading to incomplete segmentations. This is due to its reliance on pixel color, which impacts edge detection. Additionally, BoxInst performs well on the larger TN3K dataset but faces over-segmentation on the smaller DDTI dataset, resulting in lower metric scores. Similarly, the WSDAC network often produces diamond-shaped segmentations, with under-segmentation as its primary issue, capturing only partial target areas despite correct target location. In contrast, our model consistently delivers accurate and complete segmentations across all target sizes, overcoming the limitations of existing methods. These findings are supported by the quantitative comparison in Table~\ref{tab:comparison_result}.

% As shown in Fig.~\ref{fig:comparison_ddti}, BoxInst demonstrates strong performance on medium-sized targets but encounters difficulties with both under-segmentation and over-segmentation at the boundaries of large targets, resulting in incomplete segmentations. This network heavily relies on pixel color information, which hampers accurate edge detection. Furthermore, its performance is highly dependent on dataset size: while it performs well on the larger TN3K dataset, it exhibits over-segmentation on the smaller DDTI dataset, leading to lower metric scores. The WSDAC network frequently produces diamond-shaped segmentations that poorly represent the actual target shapes, particularly in large field-of-view segmentations. The primary issue with this algorithm is under-segmentation: although it can locate the target, the segmented edges are often too small to capture the full target area. The proposed MLMC model excels in segmenting targets of various sizes. 

% As illustrated in Fig.~\ref{fig:comparison_ddti}, BoxInst, while effective for medium-sized targets, struggles with both under-segmentation and over-segmentation at the boundaries of large targets, leading to incomplete segmentations. This limitation stems from its heavy reliance on pixel color information, which compromises edge detection accuracy. Additionally, BoxInst's performance is highly dataset-dependent: it performs well on the larger TN3K dataset but exhibits over-segmentation on the smaller DDTI dataset, resulting in lower metric scores. Similarly, the WSDAC network often produces diamond-shaped segmentations that fail to accurately represent target shapes, particularly in large field-of-view scenarios. Its primary issue is under-segmentation, as it frequently captures only partial target areas despite correctly locating the target. In contrast, our proposed model consistently delivers precise and complete segmentations across targets of all sizes, addressing the limitations of existing methods. The quantitative comparison from Table~\ref{tab:comparison_result} further confirms these qualitative observations.

\subsection{Ablation Analysis}
% We conduct an ablation study to analyze the individual components of our framework. Table~\ref{ablation_model} presents a comprehensive summary of the different design choices evaluated in the ablation experiments (Models A to E). The quantitative analysis in Table.~\ref{tab:ablation_results} evaluates the effectiveness of each constraint condition, inference experiments were conducted using each model on the two datasets, with the visualized results shown in Fig.~\ref{fig:ablation_result}.
We conducted an ablation study to analyze the effectiveness of individual components in our framework. Table~\ref{ablation_model} provides an overview of different design strategies (Models A to E). The quantitative assessment of each constraint condition is detailed in Table~\ref{tab:ablation_results}, with feature maps inferred and visualized across two datasets as shown in Fig.~\ref{fig:ablation_result}.

\begin{table}[!htbp]
\centering
\caption{Design Strategies of the Ablation Models}
\label{tab:different_design}
\begin{tabularx}{0.48\textwidth}{c|>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X}
\hline
Model & single-level learning & L\_align & L\_cnt & L\_corr \\ 
\hline
A & $\checkmark$ &              &              & \\
B &              & $\checkmark$ &              &\\
C &              & $\checkmark$ & $\checkmark$ &\\
D &              & $\checkmark$ &  & $\checkmark$ \\
E &              & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
\hline
\end{tabularx}
\label{ablation_model}
\end{table}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.45\textwidth]{ablation_cam.png}
\caption{\textbf{Quantitative results of ablation experiments.} Top 3 rows: validation on TN3K; bottom 3 rows: validation on DDTI. Red: correct thyroid predictions, green: under-fitting thyroid segmentation, blue: over-fitting of thyroid.}
% \vspace{-0.2in}
\label{fig:ablation_result}
\end{figure}

\begin{table*}[!htbp]
\centering
\caption{Quantitative Ablation Results of Different Design Strategies on the TN3K and DDTI Datasets}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc|cccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{4}{c}{TN3K} & \multicolumn{4}{c}{DDTI} \\
\cline{2-9}
 & \makecell{mIoU(\%) $\uparrow$} & \makecell{HD (mm) $\downarrow$} & \makecell{DSC (\%) $\uparrow$} & \makecell{Pr (\%) $\uparrow$} & 
 \makecell{mIoU (\%) $\uparrow$} & \makecell{HD (mm) $\downarrow$} & \makecell{DSC (\%) $\uparrow$} & \makecell{Pr (\%) $\uparrow$} \\
\hline
\hline
A & $58.97\pm22.61$ & $5.61\pm2.03$ & $70.13\pm23.50$ & $74.48\pm23.26$ & $50.43\pm20.41$ & $7.26\pm1.54$ & $63.65\pm20.33$ & $69.46\pm27.28$ \\
B & $60.78\pm23.47$ & $5.79\pm2.14$ & $72.16\pm22.57$ & $77.37\pm23.76$ & $51.97\pm22.12$ & $7.42\pm1.64$ & $65.58\pm21.64$ & $70.82\pm27.10$ \\
% B & $65.82\pm22.76$ & $5.29\pm2.11$ & $76.20\pm22.86$ & $77.74\pm23.68$ & $56.58\pm24.16$ & $6.93\pm1.97$ & $71.35\pm23.56$ & $74.73\pm26.79$ \\
C & $67.49\pm22.49$ & $5.23\pm2.04$ & $77.67\pm21.58$ & $78.49\pm22.38$ & $58.25\pm22.25$ & $6.66\pm1.90$ & $70.68\pm20.15$ & $72.35\pm24.84$ \\
D & $67.61\pm24.72$ & $5.08\pm2.03$ & $77.23\pm24.08$ & $78.63\pm24.34$ & $58.84\pm22.46$ & $6.59\pm1.82$ & $71.05\pm20.74$ & $71.60\pm26.08$ \\
\textbf{E} & $\textbf{69.30}\pm\textbf{22.38}$ & $\textbf{5.01}\pm\textbf{2.01}$ & $\textbf{79.10}\pm\textbf{21.37}$ & $\textbf{80.64}\pm\textbf{22.69}$ & $\textbf{59.82}\pm\textbf{21.98}$ & $\textbf{6.41}\pm\textbf{1.68}$ & $\textbf{72.14}\pm\textbf{20.76}$ & $\textbf{72.96}\pm\textbf{24.60}$ \\
\hline
\end{tabular}}
\label{tab:ablation_results}
\end{table*}

\subsubsection{Effectiveness of Alignment Loss for Spatial Learning}
% The quantitative segmentation results for the two thyroid nodule datasets, evaluated under different design strategies, are summarized in Table~\ref{tab:ablation_results}. A comparison between the original Model A and Model B reveals a nearly 6\% improvement in the mIoU and DSC for the TN3K dataset and the DDTI dataset. These improvements in mIoU, DSC, and precision suggest that decoupling weakly supervised tasks and incorporating positional constraints significantly enhances segmentation performance.

% Regarding the HD, an increase of approximately 0.5 for TN3K and 0.3 for DDTI was observed. This can be attributed to the fact that the alignment loss is primarily utilized to localize the segmentation region and therefore must be combined with the subsequent shape loss to provide complete supervision for the segmentation task.

% The quantitative segmentation results for the two thyroid nodule datasets, evaluated under different design strategies, are summarized in Table~\ref{tab:ablation_results}. Compared to Model A, Model B achieves nearly 6\% improvements in mIoU and DSC on both the TN3K and DDTI datasets, indicating that decoupling weakly supervised tasks and incorporating positional constraints significantly enhance segmentation accuracy and region consistency. Precision also shows notable improvements, further validating the effectiveness of the proposed approach.

% However, the Hausdorff Distance (HD) increases by approximately 0.5 for TN3K and 0.3 for DDTI. This is because the alignment loss primarily focuses on region localization and must be combined with the shape loss to provide comprehensive supervision for boundary refinement. These results highlight the complementary roles of localization and shape constraints in achieving robust segmentation performance.

% Figure~\ref{fig:ablation_result} shows results in reduced under-segmentation (e.g., rows 3 and 5) and over-segmentation (e.g., rows 4 and 6) compared to Model A for both datasets, which further illustrates that the inclusion of the alignment loss improves the network's ability to accurately capture the target regions. 

The effectiveness of the alignment loss for spatial learning is demonstrated through quantitative segmentation results on two datasets, summarized in Table~\ref{tab:ablation_results}. Compared to Model A, Model B showed nearly 1.5\% improvements in mIoU and DSC across both datasets, indicating that decoupling weakly supervised tasks and incorporating positional constraints improves segmentation accuracy and region consistency. While precision also increased significantly, the Hausdorff Distance (HD) slightly increased by 0.18 for TN3K and 0.16 for DDTI, because the alignment loss focused primarily on region localization and required complementary shape loss for shape refinement.

Fig.~\ref{fig:ablation_result} illustrated Model B reduced under-segmentation and over-segmentation compared to Model A, achieving more accurate location capturing and shape fitting. These observations further illustrate the network's improved ability to accurately capture target location without misleading the segmentation shapes with the inclusion of alignment loss.

% \subsubsection{Effectiveness of Region-Level Consistency Loss for Location}
% Model C, which incorporates both alignment loss and consistency loss, outperforms Model B, which only utilizes alignment loss, across all evaluation metrics. Specifically, the mIoU increased by nearly 5\%, and the HD decreased by 0.5 on both the TN3K and DDTI datasets. These improvements in mIoU, DSC, and Precision reflect a significant enhancement in segmentation accuracy. Furthermore, the reduction in HD, particularly for thyroid nodules with variable shapes, indicates that removing pseudo-labels for shape constraints allows the network to better capture and delineate precise boundaries. As shown in Figure~\ref{fig:ablation_result}, the addition of consistency loss reduces prediction holes (e.g., cow 1) and results in smoother, more coherent segmentation outputs (e.g., cows 2, 3, 4 and 5).
% % However, overfitting persists in shape refinement. To address this, we introduced additional losses to further improve shape accuracy.

\subsubsection{Effectiveness of Contrastive Loss for Semantic Feature Learning}
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.48\textwidth]{performance_comparison_metrics.png}
\caption{Performance comparison of Models C and D on the TN3K and DDTI datasets. The lines represent the performance of each model at different weight parameters ($\lambda$ for contrastive loss and $\beta$ for prototype correlation loss).}
% The metrics shown are mIoU (higher is better) and HD (lower is better) for both datasets. The solid and dashed lines correspond to mIoU, HD, DSC, and Precision values of Model C and Model D, respectively.}
% \vspace{-0.2in}
\label{fig:weighted_comparison}
\end{figure}

% This section demonstrates the effectiveness of contrastive loss by comparing Models C and E, which incorporate contrastive loss, against Models B and D, which do not. 
Model C combines alignment losses with contrastive loss, where the weight of the prototype correlation loss is controlled by parameter $\lambda$. Experimental results demonstrate that Model C achieved its best comprehensive performance when $\lambda$ was set to 0.8, as shown in Fig.~\ref{fig:weighted_comparison}.

Table~\ref{tab:ablation_results} shows significant improvements in Model C over Model B across various metrics. Specifically, Model C achieved improvements in mIoU and DSC of more than 5.5\% on TN3K and over 5\% on DDTI. Similarly, there is a notable increasement in Precision for both datasets. Additionally, the Hausdorff distance decreased by 0.56 mm on TN3K and 0.76 mm on DDTI. These improvements indicate that contrastive loss contributes to more accurate and refined segmentation. The segmentation feature heatmaps shown in Fig.~\ref{fig:ablation_result} visually support the quantitative findings. 

Furthermore, by integrating contrastive loss with other losses, Model E also showed more precise segmentation regions than Model D without contrastive loss, reducing over-segmentation and under-segmentation. The inclusion of contrastive loss leads to predicted regions that closely align with the ground truth, demonstrating its effectiveness in enhancing feature learning of segmentation shapes.
% the integration of contrastive loss with other losses (Model D: alignment and prototype correlation) as Model E also shows more precise segmentation regions than Model D, reducing over-segmentation and under-segmentation. The inclusion of contrastive loss leads to predicted regions that closely align with the ground truth, demonstrating its effectiveness in enhancing feature learning of segmentation shapes.

% These results emphasize the significant role of contrastive loss in improving model performance, particularly in better region localization and shape refinement.

% with the network trained using the combined loss functions achieving the highest segmentation accuracy. A comparison of the quantitative results for Models C and B, as well as Models E and D in Table~\ref{tab:ablation_results}, clearly demonstrates that models incorporating Contrastive Loss consistently achieve higher segmentation accuracy and better edge conformity. Specifically, the improvement in mIoU and DSC exceeds 1.5\%. 
% Table~\ref{tab:ablation_results} show that Models C and E, incorporating contrastive loss, outperform Models B and D without contasrtve  improvements exceeding 5.5\% in mIoU and DSC on TN3K and 5\% on DDTI. They also reduce Hausdorff distance (HD) by 0.56 mm and 0.76 mm on TN3K and DDTI, respectively. , respectively. The segmentation results in Fig.~\ref{fig:ablation_result} further demonstrate that contrastive loss improves segmentation accuracy by aligning predicted shapes more closely with thyroid nodules ground truth. Additionally, combining contrastive loss with positional and prototype correlation losses enhances edge accuracy, as shown by comparisons between Models E and D. These findings highlight the importance of contrastive loss in improving model performance through better region localization and shape refinement.
% The qualitative results presented in Fig.~\ref{fig:ablation_result} illustrate that the inclusion of Contrastive Loss results in segmentation shapes that more closely align with the ground truth. This also reduces under-fitting and over-fitting, as evidenced by the improved segmentation of rows 1 to 3. Moreover, a comparison between Models E and D reveals that positional losses and prototype correlation loss, when combined with Contrastive Loss, lead to more accurate edge delineation and further reduce under-fitting. These results highlight the importance of Contrastive Loss in improving model performance.

\subsubsection{Effectiveness of Prototype Correlation loss for Semantic Shape Learning}
% Model D incorporates both positional losses and prototype correlation loss, with the weight of the Contrastive Loss controlled by different values of $\beta$. Fig.~\ref{fig:weighted_comparison} presents the performance metrics of Model D across a range of $\beta$ values, demonstrating that the optimal balance is achieved at $\beta = 0.5$ or $\beta = 0.8$, which yields the most favorable outcomes.

% A comparative analysis between Models D and B reveals that incorporating prototype correlation loss as a shape constraint enhances the network's segmentation capabilities. Specifically, there is an improvement of nearly 2\% in both mIoU and DSC across the datasets, accompanied by a reduction in Hausdorff Distance (HD). Qualitative assessments, shown in Fig.~\ref{fig:ablation_result}, further support this conclusion: Model D, with the integration of prototype correlation loss, generates more distinct segmentation boundaries, with the feature CAM map indicating higher confidence in these boundaries and better delineation between the foreground and background, thereby outperforming Model B.

% Model E, compared to Model D, demonstrates sharper edges, which facilitates more accurate boundary detection in blurred ultrasound images, producing segmentation results that more closely approximate the true anatomical structures. When all losses are combined, the effect of the two shape constraints on performance is not strictly linear. Nonetheless, this combination yields clearer segmentation boundaries for multiple targets, improving accuracy by approximately 1.5\% and achieving the network’s optimal segmentation performance.

% Model D integrates both positional losses and prototype correlation loss, with the weight of the Contrastive Loss controlled by the parameter $\beta$. Fig.~\ref{fig:weighted_comparison} shows the performance metrics of Model D across various $\beta$ values, revealing that the best performance is achieved at $\beta = 0.5$ or $\beta = 0.8$.

% A comparison between Models D and B highlights the benefit of adding prototype correlation loss as a shape constraint, which improves the network's segmentation capabilities. Specifically, there is a nearly 2\% increase in both mIoU and DSC across the datasets, along with a reduction in Hausdorff Distance (HD). Qualitative results in Fig.~\ref{fig:ablation_result} further support this, showing that Model D, with prototype correlation loss, produces more defined segmentation boundaries. The feature CAM map also demonstrates higher confidence in these boundaries and better separation of the foreground from the background, resulting in superior performance compared to Model B.

% Model E, in contrast to Model D, produces sharper edges, which enhances boundary detection in blurred ultrasound images and yields segmentation results that closely align with true anatomical structures. Although the combined effect of all losses is not strictly linear, this integration of shape constraints leads to clearer segmentation boundaries for multiple targets, improving accuracy by approximately 1.5\% and achieving the network's best overall segmentation performance.
Model D combines alignment losses with prototype correlation loss, where the weight of the prototype correlation loss is controlled by parameter $\beta$. Experimental results illustrate that Model D achieved its best comprehensive performance when $\beta$ was set to 1.0 on the TN3K dataset and 0.5 on the DDTI dataset, as shown in Fig.~\ref{fig:weighted_comparison}. 

To validate the benefits of adding prototype correlation loss as a shape constraint, we compared Model D with Model B, which only using alignment loss. The experimental results show that Model D achieved nearly 7\% improvement in mIoU on the TN3K dataset and 6\% improvement on the DDTI dataset compared to Model B, and the Hausdorff Distance was significantly reduced, with a decrease of 0.71 on the TN3K dataset and 0.67 on the DDTI dataset. These results are supported by qualitative results shown in Figure~\ref{fig:ablation_result}, Model D generated more accurate segmentation of shapes, with lower uncertainty boundaries in feature heatmaps. 

When comparing Model E and Model C, the integration of prototype correlation loss provided superior segmentation regions and edges. This resulted in clear boundary delineation and segmentation outcomes closely aligning with ground truth annotations as shown in Fig.~\ref{fig:weighted_comparison}. Although the effects of combined losses were not linear, this approach leaded to 0.98\% to 1.69\% improvement in mIoU, positioning Model E as the best-performing model among all variants. These observations confirm that prototype correlation loss effectively constrains shape information, enhancing segmentation accuracy.

% Comparing Model D with Model B demonstrates the benefits of incorporating prototype correlation loss as a shape constraint, enhancing segmentation edge accuracy. Specifically, there is a nearly 7\% increase in mIoU on TN3K dataset and 6\% increase on DDTI dataset, along with a reduction in Hausdorff Distance (HD) of 0.71 on TN3K and 0.67 on DDTI. Qualitative results from Figure~\ref{fig:ablation_result} support these improvements, showing that Model D produces more defined segmentation regions with less uncertainty in heatmap. Additionally, the feature heatmaps indicate higher confidence in these boundaries and better foreground-background separation compared to Model B. Moreover, Model E with all losses enhances boundary detection by producing sharper edges than Model C, particularly beneficial for blurred ultrasound images, which segmentation results closely aligned with true anatomical structures. While the combined effect of all losses is not strictly linear, integrating shape constraints achieves clearer boundaries for multiple targets, improving accuracy by approximately 1.5\% and attaining the network's best overall segmentation performance.

% This leads to segmentation results closely aligned with true anatomical structures. 
\section{Discussion}
\subsection{Comparation with weakly-supervised Segmentation (WSS) methods}
Weakly Supervised Segmentation (WSS) methods have attracted increasing attention due to their ability to utilize sparse annotations for generating segmentation results, thereby reducing the need for fully annotated masks. However, these methods often encounter limitations due to label noise stemming from low-confidence pseudo-labels and insufficient discriminative features extracted for diverse and complex
nodule variations through rigid learning strategies.
SCRF and UNCRF generated box-like predictions because they directly utilized box labels to learn segmentation results, which introduced significant inaccuracies in shape representation and misguides the training process. 
% Similarly, S2ME and IMDPS demonstrate strong performance on the DDTI dataset, where shape variations are minimal, but their effectiveness diminishes on the TN3K dataset, which features more diverse and irregular nodule shapes. This underperformance is attributed to their reliance on fixed geometric pseudo-labels shape learning, which fail to capture the discriminative features necessary for accurately segmenting nodules with complex and variable shapes.
Similarly, S2ME and IDMPS performed well on the DDTI dataset, where shape variations are minimal. However, their performance decreased on the TN3K dataset, which suffers from diverse and irregular nodule shapes. This decline in effectiveness was due to their reliance on fixed geometric pseudo-labels for shape learning, which failed to capture the discriminative features needed for accurately segmenting nodules with complex and variable shapes.
BoxInst exhibited reasonable performance on the larger TN3K dataset but struggled with the smaller DDTI dataset due to its heavy dependence on color similarity for learning segmentation shapes. This approach was effective for high-contrast images but required a larger volume of training data. 
WSDAC consistently produced under-segmentation results because it heavily relies on initial contours and image gradients, which proved to be less effective for images with blurred boundaries.

In contrast, our proposed method effectively addresses these challenges through two key innovations: (1) the generation of high-confidence labels to mitigate label noise and improve training stability, and (2) the introduction of a high-rationality loss function designed to capture location-level, region-level, and edge-level features for segmentation location and shape learning. These advancements are integrated into our proposed framework, showing comparable or even surpassing results to those of fully supervised networks, demonstrating its robustness and effectiveness in addressing the unique challenges posed by thyroid ultrasound image segmentation.
% Weakly Supervised Segmentation (WSS) methods have gained increasing popularity due to their ability to use sparse annotations to obtain segmentation results and alleviate the burden of obtaining fully masks.
% However, the effectiveness of existing WSS methods is hindered by label noise introduced by low-confidence pseudo-labels and inadequate discriminative features extracted by rigid learning strategy. SCRF and UNCRF generated box-like predictions due to they use box labels to directly learn segmentation results, which introduce significant incurrate shape information to misleading training processes. S2ME and IMDPS performed well on the DDTI dataset with minimal shape changes but underperformed on TN3K with diverse and varying nodule shapes, because they utilize fixed geometric pseudo-labels to learn shapes can not capture discriminative features of nodules with irregular shapes. BoxInst demonstrated decent performance on the larger TN3K dataset but struggled with the smaller DDTI dataset, due to its heavily dependence on color similarity to learn sgemnetation shape, which only suitable for High contrast images and requires more training data. WSDAC always produced over-segmentation results, because it heavily relying on initial contours and image gradients, For thyroid ultrasound images with blurred boundaries, it is difficult to effectively extract features at the edges. Notably, our approach addresses the challenges related to label noise by generating high-confidence labels, and use high-rationality loss to effectively learn location-level, region-level and edge-level features indicated by a new weakly supervised segmentation learning objective we proposed. Our framework excelled on both datasets, achieving results comparable to full-supervised networks or even better, Which shows that our method established a robust solution tailored to the challenges of thyroid ultrasound image segmentation.

% As shown in Fig.~\ref{fig:comparison_tn3k} and Fig.~\ref{fig:comparison_ddti}, SCRF~\cite{zhang2020scrf} and UNCRF~\cite{mahani2022uncrf} tend to generate box-like predictions on both datasets because they directly exploit sparse annotations into fixed pseudo-labels and the inaccurate pseudo-labels misguide the model during training. The S2ME and IMDPS algorithms, which also utilize fixed pseudo-labels, perform well on the DDTI dataset, where thyroid nodules tend to exhibit more regular shapes. These algorithms achieve better results than many other methods on DDTI. However, their performance on the more complex TN3K dataset, which contains more variability in nodule shapes, is less satisfactory, trailing behind other algorithms. BoxInst~\cite{tian2021BoxInst}  performance on large dataset TN3K is well but for small dataset DDTI is not satisfying. because they refines the segmentation using color similarity to produce detailed boundaries in low-contrast ultrasound need more training dataset, and low-level color and texture information in the image is hard to fully indicate the thyroid nodule's boundary's segmentation. The segmentation results of WSDAC~\cite{li2023wsdac} on small dataset DDTI the results tend to be rhombus-like, they learn segmentation results highly rely on the initial contour and image gradient, training dataset size, generate an initial segmentation thyroid nodule boundary based on point labels. An interesting finding from our experiments is that the performance of our network shows better metrics on small dataset DDTI and achieving results that are very close to large dataset TN3K compare to full-supervised network, This demonstrates our algorithm's ability to extract the essence of features even in few images and has higher robustness.

% \subsection{Domain adaptation on different datasets}
% In our experiments, we evaluated the performance of WSS algorithms with training on the other different datasets, to test the WSS generalization of algorithms as shown in Table~\ref{tab:pretrain-DDTI}. 
% \begin{table}[!htbp]
%     \centering
%     \caption{Quantitative results of pre-training models and testing on different datasets.}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{l|cc|cc}
%             \hline
%             \multirow{2}{*}{Method} & \multicolumn{2}{c}{Train:TN3K Test:DDTI} & \multicolumn{2}{c}{Train:DDTI Test:TN3K} \\
%             \cline{2-5}
%             & DSC(\%)$\uparrow$ & HD(mm)$\downarrow$ & DS(\%)$\uparrow$ & HD(mm)$\downarrow$ \\
%             \hline
%             \hline
%             U-Net & $57.04\pm27.77$ & $7.74\pm2.11$ & $47.42\pm29.29$ & $7.92\pm2.00$\\
%             Dense-UNet & $62.98\pm28.67$ & $6.94\pm2.28$ & $48.74\pm28.13$ & $8.61\pm2.10$\\
%             Cenet & $64.27\pm31.14$ & $6.69\pm2.41$ & $57.57\pm29.87$ & $5.88\pm1.68$\\
%             \hline
%             SCRF & $48.28\pm31.23$ & $7.93\pm2.18$ & $43.01\pm26.77$ & $8.78\pm1.83$\\
%             UNCRF & $48.54\pm31.55$ & $7.97\pm2.25$ & $45.63\pm26.77$ & $8.55\pm1.87$\\
%             BoxInst & $54.32\pm29.31$ & $7.39\pm2.68$ & $41.11\pm23.48$ & $7.50\pm1.39$\\
%             WSDAC & $55.74\pm26.51$ & $7.02\pm2.21$ & $54.46\pm25.66$ & $7.37\pm2.15$\\
%             S2ME & $45.15\pm36.11$ & $7.63\pm3.03$ & $52.61\pm30.37$ & $7.44\pm2.25$\\
%             IDMPS & $40.54\pm33.38$ & $7.78\pm2.88$ & $47.90\pm29.94$ & $7.57\pm2.08$\\
%             Proposed(U-Net) & $57.35\pm24.31$ & $7.12\pm2.12$ & $55.48\pm27.33$ & $7.24\pm1.98$\\
%             \hline
%         \end{tabular}
%     }
%     \label{tab:pretrain-DDTI}
% \end{table}

% We observed that, across different datasets, when trained on the smaller DDTI dataset and tested on TN3K, fully supervised networks like U-Net performed even worse than weakly supervised algorithms using the same backbone. However, when trained on the larger TN3K dataset, fully supervised networks showed significant performance improvements. This indicates that fully supervised networks can learn richer and more complete representations through extensive training, enabling them to better associate training data with ground truth masks. In contrast, weakly supervised algorithms, relying on coarse labels, can quickly extract limited representations in small datasets but remain limited by their use of labels and learning strategies when scaled to larger datasets. Our algorithm achieves outstanding generalization capabilities by improving the labeling and feature learning strategies for weak supervision, operating solely based on images and point labels.

% with the nodule’s shape. S2ME ~\cite{wang2023s2me} and IDMPS~\cite{zhao2024IDMPS} introduce dual or multi-branch designs that help the model learn the consistency of pseudo-labels during training, thus improving the balance in pseudo-mask generation.

% The ability of large models to be trained on one dataset and then applied to another dataset with known labels is commonly referred to as transfer learning. However, pre-training large models typically requires very large datasets (on the order of billions of samples) and a vast number of parameters. 

% In our experiments, we evaluated the performance of our algorithm on smaller datasets, consisting of hundreds or thousands of samples, to test its generalization. 

% The first experiment involved training a network on the DDTI dataset and evaluating the pre-trained model on the TN3K dataset as well as a self-collected CMU-TSUD dataset, which includes 194 thyroid nodule ultrasound images. 

% The second experiment involved training on the TN3K dataset and evaluating the model on both the DDTI and CMU-TSUD datasets.

% \begin{table}[!htbp]
%     \centering
%     \caption{Quantitative results of pre-training models using DDTI dataset on TN3K and CMU-TSUD datasets.}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{l|cc|cc}
%             \hline
%             \multirow{2}{*}{Method} & \multicolumn{2}{c}{TN3K} & \multicolumn{2}{c}{CMU-TSUD} \\
%             \cline{2-5}
%             & mIoU(\%)$\uparrow$ & HD(mm)$\downarrow$ & mIoU(\%)$\uparrow$ & HD(mm)$\downarrow$ \\
%             \hline
%             \hline
%             U-Net & $\textbf{67.02}\pm\textbf{21.87}$ & $5.28\pm1.97$ & $62.15\pm18.17$ & $\textbf{4.53}\pm\textbf{1.12}$\\
%             Dense-UNet & $67.27\pm20.00$ & $\textbf{5.15}\pm\textbf{1.70}$ & $62.12\pm15.86$ & $4.57\pm0.98$\\
%             Cenet & $65.49\pm22.99$ & $5.30\pm1.97$ & $\textbf{63.71}\pm\textbf{17.91}$ & $4.58\pm1.26$\\
%             \hline
%             SCRF & $59.11\pm20.21$ & $6.66\pm1.92$ & $62.25\pm17.77$ & $5.78\pm1.35$\\
%             UNCRF & $65.32\pm18.75$ & $5.85\pm1.94$ & $64.59\pm14.88$ & $4.83\pm1.33$\\
%             BoxInst & $67.82\pm18.80$ & $5.02\pm1.97$ & $68.47\pm13.43$ & $4.23\pm1.02$\\
%             WSDAC & $59.40\pm16.02$ & $5.05\pm1.76$ & $64.74\pm12.74$ & $4.24\pm0.82$\\
%             S2ME & $69.95\pm21.47$ & $5.51\pm1.86$ & $64.97\pm17.56$ & $4.69\pm1.13$\\
%             IDMPS & $68.94\pm21.85$ & $5.05\pm1.92$ & $65.43\pm17.24$ & $4.38\pm1.12$\\
%             % Proposed(U-Net) & $71.28\pm20.04$ & $4.95\pm1.93$ & $67.36\pm15.68$ & $4.03\pm18.54$\\
%             Proposed(U-Net) & $\textbf{71.28}\pm\textbf{20.04}$ & $\textbf{4.95}\pm\textbf{1.93}$ & $\textbf{67.36}\pm\textbf{15.68}$ & $\textbf{4.03}\pm\textbf{1.54}$\\
%             Proposed(CENet) & $\textbf{77.43}\pm\textbf{19.66}$ & $\textbf{4.58}\pm\textbf{1.89}$ & $\textbf{73.58}\pm\textbf{14.33}$ & $\textbf{3.99}\pm\textbf{1.23}$\\
%             \hline
%         \end{tabular}
%     }
%     \label{tab:pretrain-DDTI}
% \end{table}

% An interesting finding from our experiments is that the network can achieve effective domain adaptation when trained on a larger dataset, approaching the performance of MedSAM. When trained on the larger TN3K dataset, which contains over 3,000 images, our model shows significant improvement, highlighting the strong potential of multi-level learning within our framework. 

% As demonstrated in Tables~\ref{tab:pretrain-TN3K} and~\ref{tab:pretrain-DDTI}, comparative quantitative experiments reveal that different algorithms adapt differently to various datasets. For example, the S2ME and IMDPS algorithms, which utilize pseudo-labels, perform well on the DDTI dataset, where thyroid nodules tend to exhibit more regular shapes. These algorithms achieve better results than many other methods on DDTI. However, their performance on the more complex CMU-TSUD dataset, which contains more variability in nodule shapes, is less satisfactory, trailing behind most other algorithms. In contrast, BoxInst, which employs a multi-level constraint approach for segmentation target learning instead of relying on pseudo-labels, performs well on both datasets. This further emphasizes the effectiveness of the proposed decoupled training method in enhancing model generalization across different domains. 

\subsection{Limitations and Future work}
% \begin{table}[!htbp]
%     \centering
%     \caption{Comparison of Scale and Performance of Various Models with batchsize=16 and inputsize=256.}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{l|ccc}
%             \hline
%             Method & Parameter & Training Efficiency(flops) & Inference Time (s)\\
%             \cline{2-4}
%             % \hline
%             U-Net & 31.04M & $4.8327 \times 10^{10} $ & $0.0029$\\
%             Dense-UNet & 12.21M & $4.4041 \times 10^9$ & $0.0097$\\
%             Cenet & 29.00M & $7.1835 \times 10^9$ & $0.0077$\\
%             \hline
%             % SCRF & 31.04M & $4.8343 \times 10^{10}$ & $0.0071$\\
%             % UNCRF & 31.04M & $5.4737 \times 10^{10}$ & $0.0076$\\
%             % BoxInst & 31.04M & $4.8327 \times 10^{10}$ & $0.0047$\\
%             WSDAC & 28.03M & $7.1835 \times 10^9$ & $0.0084$\\
%             S2ME & 8.74M & $2.5101 \times 10^8$ & $0.0121$ \\
%             IDMPS & 25.06M & $2.4957 \times 10^8$ & $0.0123$\\
%             HCL-HRL(U-Net) & 33.66M & $4.9168 \times 10^{10}$ & $0.0065$\\
%             HCL-HRL(Dense-UNet) & 24.09M & $5.7031 \times 10^9$ & $0.0127$\\
%             HCL-HRL(CENet) & 39.24M & $7.2328 \times 10^9$ & $0.0087$\\
%             \hline
%             % MedSAM & $84.27\pm8.64$ & $3.90\pm1.47$ & $75.45\pm10.89$ & $3.91\pm1.18$\\
%             % \hline
%         \end{tabular}
%     }
%     \label{tab:param}
% \end{table}

Our method showed convincing segmentation results on the thyroid nodules dataset but still faces challenges as follows:

% Firstly, while achieving moderate inference speed (0.0065 seconds per image), our algorithm relies on MedSAM for generating high-confidence labels. This reliance increases computational complexity and training time. Future work will focus on developing more efficient, parameter-light models specifically tailored for ultrasound or thyroid ultrasound images to improve generalization and streamline the incorporation of anatomical prior information.
Firstly, Although our algorithm achieved moderate inference speed (0.0065 seconds per image), it relied on MedSAM for generating high-confidence labels for training, which increased computational complexity and training time. Future work will focus on developing more efficient, lightweight models for incorporating anatomical prior information to generate high-confidence labels.

Secondly, The feature extraction backbone in our framework can be seamlessly integrated and used as needed. In this work, we employed general feature extraction backbones, improving these backbones for specialized tasks can further enhance segmentation accuracy. Future research can develop based on our framework can focus on addressing feature extraction challenges related to low contrast and speckle noise.

% Addressing feature extraction challenges related to low contrast and speckle noise in thyroid nodule ultrasound images remains a critical focus for future research.
% In this work, we employ general feature extraction backbones, enhancing backbones with improved feature extraction capabilities for special tasks will help boost segmentation accuracy and operational efficiency. Addressing challenges posed by low contrast and speckle noise in thyroid nodule ultrasound images remains a priority for future research to achieve more precise segmentation results in complex imaging backgrounds.
% Although our method achieves convincing segmentation results on the thyroid ultrasound dataset, several key challenges remain to be addressed in future research.

% Firstly, as shown in Table~\ref{tab:param}, while the inference speed of our algorithm reaches a moderate level of 0.0065 seconds per image, the generation of high-confidence labels during training depends on anatomical prior information provided by pre-trained large-scale models for medical images. This dependency substantially increases model complexity and parameter count, thereby prolonging the training time of the algorithm. To mitigate these issues in the future, we aim to develop more parameter-efficient models with enhanced generalization capabilities, specifically tailored for ultrasound or thyroid ultrasound images. These improvements will enable us to more effectively incorporate anatomical prior information and guide the generation of high-confidence labels.

% Secondly, our algorithm framework demonstrates high flexibility by employing interchangeable feature extraction backbone networks. During experimentation, we utilized classical architectures such as U-Net, Dense-UNet, and Cenet for feature extraction. By designing backbones with stronger and more streamlined feature extraction capabilities, we can enhance both the segmentation accuracy and operational efficiency of the algorithm. To address the challenges posed by low contrast and speckle noise interference in thyroid nodule ultrasound images, future work will focus on optimizing the feature extraction modules to better capture detailed texture features. This optimization will facilitate more precise segmentation in complex imaging backgrounds.

% In scenarios with limited data, that training on the DDTI dataset and evaluating on the TN3K and CMU-TSUD datasets, fully supervised algorithms tend to perform worse than weakly supervised ones. This suggests that, due to the limited size of the dataset, even pixel-wise learning with fully supervised methods struggles to capture generalized features effectively. In contrast, our approach, which focuses on learning meaningful features at multiple levels during network training, demonstrates superior generalization performance, as evidenced by achieving the highest performance across both the TN3K and CMU-TSUD datasets. However, it is worth noting that our method still experiences a drop of over 10\% mIoU compared to the MedSAM network.
% \begin{table}[!htbp]
%     \centering
%     \caption{Quantitative results of pre-training models using TN3K dataset on DDTI and CMU-TSUD datasets.}
%     \resizebox{\columnwidth}{!}{%
%         \begin{tabular}{l|cc|cc}
%             \hline
%             \multirow{2}{*}{Method(backbone)} & \multicolumn{2}{c}{DDTI} & \multicolumn{2}{c}{CMU-TSUD} \\
%             \cline{2-5}
%             & mIoU(\%)$\uparrow$ & HD(mm)$\downarrow$ & mIoU(\%)$\uparrow$ & HD(mm)$\downarrow$ \\
%             \hline
%             U-Net & $\textbf{85.27}\pm\textbf{7.51}$ & $4.58\pm1.41$ & $74.68\pm11.43$ & $4.01\pm1.13$\\
%             Dense-UNet & $83.28\pm8.01$ & $4.67\pm1.19$ & $74.51\pm10.89$ & $\textbf{3.86}\pm\textbf{1.09}$\\
%             Cenet & $84.31\pm6.46$ & $\textbf{4.35}\pm\textbf{13.89}$ & $\textbf{75.07}\pm\textbf{11.90}$ & $3.92\pm1.68$\\
%             \hline
%             SCRF & $71.16\pm6.99$ & $6.89\pm1.64$ & $64.42\pm10.13$ & $5.26\pm1.57$\\
%             UNCRF & $75.71\pm7.61$ & $6.36\pm1.47$ & $69.20\pm9.78$ & $5.02\pm1.52$\\
%             BoxInst & $78.25\pm8.81$ & $4.75\pm1.34$ & $71.65\pm10.84$ & $4.13\pm1.15$\\
%             WSDAC & $70.24\pm5.97$ & $4.60\pm1.23$ & $68.22\pm8.12$ & $4.38\pm1.01$\\
%             S2ME & $78.91\pm9.66$ & $4.94\pm1.26$ & $65.55\pm9.66$ & $4.29\pm1.26$\\
%             IDMPS & $75.61\pm5.97$ & $4.84\pm1.23$ & $67.14\pm11.08$ & $4.11\pm0.98$\\
%             % Proposed(U-Net) & $82.21\pm7.41$ & $4.52\pm1.04$ & $73.01\pm11.25$ & $4.16\pm1.13$\\
%             Proposed(U-Net) & $\textbf{82.21}\pm\textbf{7.41}$ & $\textbf{4.52}\pm\textbf{1.04}$ & $\textbf{73.01}\pm\textbf{11.25}$ & $\textbf{4.06}\pm\textbf{1.13}$ \\
%             Proposed(CENet) & $\textbf{84.21}\pm\textbf{7.22}$ & $\textbf{4.23}\pm\textbf{1.01}$ & $\textbf{75.01}\pm\textbf{10.56}$ & $\textbf{3.82}\pm\textbf{1.20}$ \\
%             \hline
%             MedSAM & $83.71\pm6.71$ & $4.24\pm1.44$ & $75.45\pm10.89$ & $3.91\pm1.18$\\
%             \hline
%         \end{tabular}
%     }
%     \label{tab:pretrain-TN3K}
% \end{table}

% The gap can be effectively reduced by improving the backbone for feature extraction, demonstrating the adaptability of the proposed algorithm as a general weakly supervised framework for the backbone network used for feature extraction. Future research on models with high generalization and low dataset requirements can further focus on designing more efficient feature extraction networks tailored to the weakly supervised framework presented in this paper.

% These finding encourages further research into the mining of semantic information from images, particularly in the context of weak supervision. 

% By leveraging incomplete annotations, our model not only performs competitively with fully supervised models but also provides a more scalable solution for real-world applications where labeled data is limited.

\section{Conclusion}
% In this paper, we propose a novel weakly supervised segmentation framework that utilizes clinical point annotations to segment thyroid nodules. Our method utilizes geometric transformations with topology
% priors and the Medical Segment Anything Model (Med-
% SAM) prediction with anatomical information to generate high-confidence multi-level labels. We also proposed multi-level learning strategies, which are realized by alignment loss for low-level spatial learning and contrastive and prototype correlation loss for high-level semantic learning.

% Experimental results demonstrate that the proposed framework outperforms state-of-the-art weakly supervised methods on publicly available datasets TN3K, DDTI, and a self-collected dataset CMU-TSUD, achieving superior segmentation accuracy, edge fitting, and generalization. Our framework is versatile and can be easily applied to various weakly supervised segmentation models to enhance their performance.

% In this paper, we present a novel weakly supervised segmentation framework for thyroid nodule segmentation using clinical point annotations. Our method leverages geometric transformations with topology priors and the Medical Segment Anything Model (Med-SAM) prediction with anatomical information to generate high-confidence multi-level labels. Additionally, we introduce a high-rationality feature learning strategy based on a new segmentation theory consist of location elements and shape elements. The multi-level learning strategies, incorporating alignment loss for low-level spatial learning and contrastive and prototype correlation loss for high-level semantic learning. Experimental results show that our framework outperforms state-of-the-art weakly supervised methods on publicly available datasets TN3K and DDTI. Furthermore, our framework is highly versatile and can be easily integrated into various feature extraction backbones.
In this paper, we present a novel weakly supervised segmentation framework for thyroid nodule segmentation based on clinical point annotations. We clarify the segmentation objective that integrates location and shape elements to indicate the learning process. Our method combines geometric transformations with topology priors and leverages the MedSAM prediction with anatomical information to generate high-confidence labels. Furthermore, we propose a multi-level learning strategy through high-rationality losses. The alignment loss is for precise location learning, while contrastive and prototype correlation losses are for robust shape understanding. Experimental results demonstrate superior performance compared to state-of-the-art weakly supervised methods on benchmark datasets, including TN3K and DDTI. The framework is highly versatile and can be seamlessly integrated into various feature extraction architectures, offering flexibility for diverse application scenarios.

% that incorporates alignment loss for precise location learning and contrastive and prototype correlation loss for robust shape understanding. Experimental results demonstrate superior performance compared to state-of-the-art weakly supervised methods on benchmark datasets, including TN3K and DDTI. The framework is highly versatile and can be seamlessly integrated into various feature extraction architectures, offering flexibility for diverse application scenarios.

\bibliographystyle{ieeetr}
\bibliography{mytmi}

\end{document}
