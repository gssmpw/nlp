
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% for figure and caption
\usepackage{graphicx}
\usepackage{caption}
% for list items
\usepackage{enumitem}
% math symbols
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{mathtools}

\usepackage[textsize=tiny]{todonotes}
%for the box
\usepackage{mdframed} % for creating framed boxes
\usepackage{lipsum}  % for generating filler text
\newmdenv[
  topline=true,
  bottomline=true,
  skipabove=\baselineskip,
  skipbelow=\baselineskip
]{protocolbox}

\usepackage{hyperref}
\usepackage[affil-it]{authblk}
% for algorithm
\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\usepackage{wrapfig}   % For text wrapping around figures

% for table
\usepackage{tabularray}

\definecolor{mypink}{HTML}{FB2E99}

\setlength{\marginparwidth}{0.6in}
\newcommand{\allnotes}[1]{}
\renewcommand{\allnotes}[1]{#1} % Comment to turn off notes
\newcommand{\qian}[1]{\allnotes{\todo[color=yellow!30]{QL: #1}}}
\newcommand{\Qian}[1]{\textcolor{blue}{[QL: #1]}}

\title{CipherPrune:  Efficient and Scalable Private Transformer Inference}
% \Qian{pruning to reduction(include approximation); efficient and scalable}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.



\newcommand{\update}[1]{\textcolor{orange}{#1}}
\newcommand{\delete}[1]{\textcolor{blue}{#1}}

\newcommand{\mysoftmax}{$\mathsf{SoftMax}\ $}
\newcommand{\gelu}{$\mathsf{GELU}\ $}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\renewcommand\Authand{}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\usepackage[affil-it]{authblk}  % Use the correct package

\author{
Yancheng Zhang\textsuperscript{1},
Jiaqi Xue\textsuperscript{1},
Mengxin Zheng\textsuperscript{1}\protect\\ 
Mimi Xie\textsuperscript{2}, 
Mingzhe Zhang\textsuperscript{3},
Lei Jiang\textsuperscript{4},
Qian Lou\textsuperscript{1*}\\
\textsuperscript{1}University of Central Florida \quad
\textsuperscript{2}University of Texas at San Antonio\\
\textsuperscript{3}Ant Research \quad
\textsuperscript{4}Indiana University Bloomington \\
% \{yczhang, jiaqi.xue, mengxin.zheng, qian.lou\}@ucf.edu \\
}


% \author[1]{Yancheng Zhang}
% \author[1]{Jiaqi Xue}
% \author[1]{Mengxin Zheng}
% \author[2]{\and Mimi Xie}
% \author[3]{Mingzhe Zhang}
% \author[4]{Lei Jiang}
% \author[1]{Qian Lou}


% \affil[1]{University of Central Florida}
% \affil[2]{Illinois Institute of Technology}
% \affil[3]{Samsung Research America}

% \affil[ ]{\texttt {\{qian.lou, yancheng.zhang, mengxin.zheng\}@ucf.edu;}}
% \affil[ ]{\texttt{yshang4@hawk.iit.edu;\ xunchen@outlook.com}}




\begin{document}

\maketitle

\def\thefootnote{$*$}\footnotetext{ Corresponding Author. Email: qian.lou@ucf.edu.}

\begin{abstract}
Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference.  Building on these observations, we propose \textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\times$ for 128-token inputs and $10.6\times$  for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at \href{https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference}{\textcolor{mypink}{https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference}}.

%\qian{replace xxx and add concrete number}


%Fortunately, most of the inputs have redundancy tokens. The current token pruning method even could prune token numbers largely and reduce the complexity from quadratic to linear by pruning tokens with larger pruning ratios for longer tokens. Although it is promising to incorporate it to improve the efficiency and scalability of private inference, it poses challenges for designing protocols for encrypted token pruning. The process must be efficient, with overhead less than the pruning benefits, and ensure privacy, including for the pruning mask. Additionally, pruning must maintain accuracy above a user-defined threshold. Also, 

%its several requirements bring challenging. First, this pruning requires input-specific pruning, i.e., various inputs have different pruning ratios, which requires to design a input-specific private inference protocol for token pruning ; second, it requires a progressive layer-wise pruning, since one-time pruning in one single layer will not achieve desired effect, thus the token pruning 


%the Transformer's operation complexity is quadratic on token numbers. Online Token Pruning (OTP) can reduce the complexity, even from quadratic to linear, by pruning tokens with larger pruning ratios for longer tokens. While OTP is effective for plaintext Transformers, it poses challenges for encrypted token pruning (ETP), which involves calculating the token importance score, pruning mask, and executing pruning on secure shares. The process must be efficient, with overhead less than the pruning benefits, and ensure privacy, including for the pruning mask. Additionally, pruning must maintain accuracy above a user-defined threshold.

%To address these challenges, we propose CipherPrune, which enables fast private Transformer inferences. Cipherprune constructs a secure token pruning protocol, $\Pi_{prune}$, that efficiently generates importance scores of private tokens, deriving pruning masks and executing pruning. The pruning masks in the protocol are restored and released for pruning execution, which can lead to privacy leakage. To ensure end-to-end confidentiality, we propose a protocol, $\Pi_{mask}$, to complement $\Pi_{prune}$ with guaranteed mask privacy protection. Also, pruning still retains some tokens requiring encrypted execution, with non-linear functions being latency bottlenecks. Thus, we maximize the ratio of cheaper linear approximation of these non-linear functions for these non-linear functions using Encrypted Non-Linear Approximation (ENA). ENA re-invokes the $\Pi_{prune}$ protocol on the retained tokens, replacing pruning with approximation, and since the mask remains private, $\Pi_{mask}$ is not needed again. We adopt a gradient-based search to optimize the pruning and approximation ratios to meet the desired accuracy threshold. 
%Our experiments show that CipherPrune reduces the execution overhead of private Transformer inferences by up to $21\times$ compared to previous methods without sacrificing accuracy. The code is publicly available at \href{https://anonymous.4open.science/r/CipherPrune-366A/README.md}{\textcolor{mypink}{https://anonymous.4open.science/r/CipherPrune-366A}}.
\end{abstract}

\section{Introduction}
\label{s:intro}
\input{contents/1_intro}

\section{Preliminaries}
\label{s:related}
\input{contents/2_background}

% \section{Preliminaries}
% \label{s:pre}
% \input{contents/3_preliminaries}

% \qian{Methods. typo?}
\section{CipherPrune Framework}
\label{s:method}
\input{contents/3_method}

\section{Experiments}
\label{s:expt}
\input{contents/4_expt}

% \section{Results}
% \label{s:res}
% \input{contents/5_results}

\section{Conclusion}
\label{s:conc}
\input{contents/6_conclusion}

\bibliography{homo}
\bibliographystyle{iclr2025_conference}

\appendix
\newpage
\section*{Appendix}
\input{contents/7_appendix}


\end{document}
