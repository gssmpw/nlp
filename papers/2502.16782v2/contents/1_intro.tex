Transformers~\citep{vaswani2017attention} have become the predominant approach for tackling a wide range of machine learning tasks, spanning Natural Language Processing (NLP)~\citep{xue2024trojllm} and Computer Vision (CV)~\citep{zheng2022trojvit} domains. Notably, Transformer-as-a-Service (TaaS)~\citep{radford2018openai} has emerged as an effective means for average users to harness the capabilities of sophisticated and accurate Transformers deployed on cloud servers.
Privacy has become a major concern, driving a growing demand for privacy-preserving TaaS solutions~\citep{zheng2023primer,hao2022iron-iron,zeng2023mpcvit,santriaji2024dataseal,lou2021safenet}. 

Homomorphic Encryption (HE)~\citep{gentry2009fully} is a promising secure computing technology that protects data privacy by enabling computations on encrypted data without decryption. However, applying HE continuously for deep computation tasks often results in prohibitively high latency~\citep{lou2019glyph,lou2019she,zheng2024ofhe,zhang2023hebridge, jiang2022matcha}. To address this, hybrid HE/Multi-party Computation (MPC)-based techniques~\citep{chen2022thex,zheng2023primer,hao2022iron-iron,zeng2023mpcvit,zhang2023sal, lu2023bumblebee, pang2023bolt, xu2024privcirnet} have been widely adopted for private Transformer inference, as illustrated in Figure~\ref{fig:motivation}
(a). This hybrid approach achieves state-of-the-art performance by using HE for linear operations and MPC for non-linear operations.

% \Qian{check if this number is still correct (I changed the IRON citation with BOLT) }
% The figure is too small, as shown in Figure~\ref{fig:example2}.
Unfortunately, prior private Transformer inferences~\citep{lu2023bumblebee, pang2023bolt,zheng2023primer}  still suffer from significant latency and poor scalability over long-token inputs.  As Figure~\ref{fig:motivation} (b) shows, the prior private inference process for a GPT2 Transformer~\citep{pang2023bolt} with 128 input tokens extends to $\sim10$ minutes. It necessitates the exchange of over 60 gigabytes of data between the server and the client. Furthermore, as the token length increases, the runtime overhead grows super-linearly, indicating poor scalability. This is primarily because the operational complexity of Transformers~\citep{vaswani2017attention, kim2022LTP} scales quadratically with the number of input tokens. Reducing the number of input tokens without compromising accuracy is essential. 
% ~\textcolor{blue}{[Lou: add citations for quadratic complexity]}
% \begin{figure}
% % \vspace{-0.4in}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/intro.pdf}
%     \captionsetup{skip=2pt}
%     % \vspace{-0.2in}
%     \caption{A 2PC-based private Transformer inference.}
%     \label{fig:example2}
% \vspace{-0.3in}
% \end{figure}

% \Qian{Add citations}
We observe that most inputs contain redundant words/tokens, with varying levels of redundancy across different inputs. As illustrated in Figure~\ref{fig:motivation} (c), in a sentiment analysis task, an input that retains only the tokens \textit{movie} and \textit{great} while removing almost all others still maintains inference confidence and accuracy. We refer to such tokens that can be removed without significantly impacting accuracy as \textit{redundancy}. Meanwhile, the different inputs have various levels of redundancy~\citep{wang2021spatten,kim2022LTP,yudha2024boostcom}. Figure~\ref{fig:motivation} (d) illustrates that some inputs exhibit greater redundancy, while others have less, with this variation being particularly evident across different tasks. Classification tasks typically have more redundancy compared to sequence-to-sequence tasks~\citep{fu2024lazyllm}. To effectively prune more tokens from longer inputs and potentially reduce the Transformer's quadratic complexity to linear, pruning should be done progressivelyâ€”that is, tokens should be pruned layer by layer over multiple stages, as illustrated in Figure~\ref{fig:motivation}
(e), rather than performing a one-time pruning at the first layer~\citep{wang2021spatten,kim2022LTP,xu2024freepruner}. Another key observation is that previous private Transformer inference methods, whether relying on precise non-linear activations~\citep{hao2022iron-iron,yudha2024boostcom} or using large-degree polynomial approximations~\citep{lu2023bumblebee, pang2023bolt} for these activations, continue to suffer from significant execution overhead for non-linear operations.
Therefore, replacing non-linear activations or high-degree polynomials with lower-degree polynomials can be beneficial. As shown in Figure~\ref{f:polynomial-reduction}
, a degree-$d$ polynomial activation for tokens (Figure~\ref{f:polynomial-reduction} (a)) can be reduced to a degree-$d_i$ polynomial (Figure~\ref{f:polynomial-reduction} (b)), where $d_i \leq d$. 







Adopting existing plaintext-level pruning techniques (\textit{PlainPrune})~\citep{wang2021spatten,kim2022LTP} to accelerate private Transformer inferences presents a formidable challenge. The primary reason is that the tokens are encrypted, and we need to calculate token importance scores layer by layer for specific encrypted inputs. This requires redesigning a new encrypted token pruning protocol. Meanwhile, the polynomial reduction in the encrypted domain poses a similar challenge, and we need to design an encrypted polynomial reduction protocol for efficient private activation. Also, at the network level, we need to learn the pruning and polynomial reduction thresholds while maximizing \textit{efficiency}, ensuring \textit{privacy}, and maintaining the desired level of \textit{accuracy}. 


%, generating pruning masks (removing tokens with importance scores below a learned threshold), and removing tokens. Implementing this in private Transformers is challenging due to the need for Encrypted Token Pruning (ETP) to ensure \textit{(1) efficiency}, where the execution overhead of ETP is less than the benefits gained from pruning tokens; \textit{(2) privacy}, ensuring the end-to-end ETP process does not introduce privacy leakage; and \textit{(3) accuracy}, maintaining the desired level of accuracy. 



\begin{figure}
%\vspace{-0.1in}
    \centering    \includegraphics[width=1\linewidth]{figures/cipherprune-moti.pdf}
    \captionsetup{skip=2pt}
    \vspace{-0.1in}
    \caption{(a) An illustration of HE/MPC-based private inference. (b) The high-latency and scalable challenge of private Transformer models over lengthy inputs. (c) An example of redundant input in sentiment analysis tasks. (d) Demonstration of varying levels of redundancy across different inputs. (e) An example showcasing progressive redundancy pruning.}
    \label{fig:motivation}
\vspace{-0.3in}
\end{figure} 
%although token pruning, as designed to quadratically reduce the computational overhead of plaintext Transformer inferences, progressively removes unimportant tokens from input sequences during inferences, while preserving the accuracy of Transformers. State-of-the-art token pruning employs a learnable threshold~\citep{kim2022LTP} for each Transformer layer, determined during the Transformer's finetuing process, with distinct thresholds for different layers to maximize the removal of unimportant tokens. While token pruning has demonstrated effectiveness in expediting plaintext Transformer inferences, its straightforward adaptation to 2PC-based encrypted Transformers is fraught with difficulties for several reasons.
%\begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]
%\item Firstly, in pursuit of maintaining inference accuracy, token pruning~\citep{kim2022LTP} in plaintext Transformers tends to retain a significant portion of tokens \delete{in the initial layers.} However, the presence of layer normalization, SoftMax and GELU operations in these initial layers substantially hampers the efficiency of private inferences in 2PC-based encrypted Transformers.

%\item Secondly, safeguarding the plaintext pruning mask, which specifies the tokens to be removed in the sequence, is paramount, as its exposure could potentially reveal sensitive information, such as the critical segments within the sequence. Consequently, the pruning mask must be encrypted. Nevertheless, existing cryptographic protocols do not offer a means to remove tokens using an encrypted pruning mask. 
%\end{itemize} 

%In this paper, we introduce \textit{CipherPrune}, an encrypted token pruning technique designed to accelerate private inferences of 2PC-based transformers. We first propose a learnable threshold-based token pruning method, where unimportant \delete{tokens in the initial layers are aggressively removed.} To perform token pruning on ciphertext privately, we then propose two secure protocols: (1) The first protocol enables two parties to compute the pruning mask without learning the actual values of tokens; (2) To prevent privacy leakage from the pruning mask, we then present a private mask protocol to conceal the sparsity in the pruning mask. \update{(3) dynamic token approximation} Our extensive experiments demonstrate that the CipherPrune significantly reduces both the latency and communication overhead by $\sim5\times$ compared to previous methods~\citep{rathee2021sirnn, hao2022iron-iron}, all while ensuring the marginal accuracy degradation of Transformers within $1\%$.





To address these challenges, we introduce CipherPrune, a scalable and efficient framework for private inference that incorporates a secure encrypted token pruning protocol, a polynomial reduction protocol, and tailored Transformer network optimizations. At the protocol level, CipherPrune adaptively prunes unimportant tokens from encrypted inputs in a progressive, layer-by-layer manner. It also applies encrypted polynomial reduction by assigning lower-degree polynomials to less important tokens post-pruning, thereby improving efficiency without
\begin{wrapfigure}{r}{0.4\textwidth}  % 'r' for right, and the width of the figure area
  \centering
    \includegraphics[width=0.4\textwidth]{figures/polynomial-reduction.pdf}
  \caption{Polynomial Reduction.}
  \vspace{-0.1in}
  \label{f:polynomial-reduction}
\end{wrapfigure}
requiring decryption. At the network level, we implement protocol-aware optimization using a gradient-based search, aiming to maximize pruning thresholds and polynomial reduction conditions while preserving the required accuracy. Our experiments show that CipherPrune reduces the execution overhead of private Transformer inference by about 6.1$\times$ for 128-token inputs and 10.6 $\times$ for 512-token inputs compared to prior method~\citep{pang2023bolt}, {with only a marginal drop in accuracy}.


%Our experiments show that CipherPrune reduces the execution overhead of private Transformer inferences by up to $21\times$ compared to previous methods~\citep{rathee2021sirnn, hao2022iron-iron} without sacrificing accuracy.



%we propose CipherPrune, which enables fast private Transformer inferences. CipherPrune constructs a secure token pruning protocol, $\Pi_{prune}$, that efficiently generates importance scores of private tokens, derives pruning masks, and executes pruning. If the pruning mask is securely shared, using this mask to prune tokens directly is not straightforward, as it may reveal privacy information. One solution is to assume the mask does not compromise privacy and share it with each party for token pruning. However, this assumption is not always reliable, as the pruning mask can potentially lead to privacy leaks. To ensure end-to-end confidentiality, we propose a complementary protocol, $\Pi_{mask}$, to guarantee mask privacy protection.

%Moreover, since pruning retains some tokens requiring encrypted execution, and non-linear functions can be latency bottlenecks, we maximize the use of cheaper linear approximations for these non-linear functions using Encrypted Non-Linear Approximation (ENA). ENA re-invokes the $\Pi_{prune}$ protocol on the retained tokens, replacing pruning with approximation, and since the mask remains private, $\Pi_{mask}$ is not needed again. We adopt a gradient-based search to optimize the pruning and approximation ratios to meet the desired accuracy threshold. Our experiments show that CipherPrune reduces the execution overhead of private Transformer inferences by up to $21\times$ compared to previous methods~\citep{rathee2021sirnn, hao2022iron-iron} without sacrificing accuracy.