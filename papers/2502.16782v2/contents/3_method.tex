\subsection{Motivation}
%\qian{add motivation, challenges, and overview here.}
Although plaintext pruning methods~\citep{goyal2020power,kim2020lat,wang2021spatten} enable efficient and scalable inference for standard plaintext-domain Transformers, integrating these techniques into private Transformers remains challenging. Additionally, encrypted polynomial reduction and its joint optimization with token pruning remain largely unexplored. 

\noindent\textbf{Challenge 1: Lacking protocols for input-specific, progressive encrypted token pruning and encrypted polynomial reduction.} The efficiency and scalability of plaintext pruning methods~\citep{goyal2020power,kim2020lat,wang2021spatten} rely on two important features: (1) Input-specific pruning: This involves assigning an adaptive pruning ratio based on the dynamic importance of each input, as different inputs exhibit varying levels of redundancy. A fixed pruning ratio applied universally can result in suboptimal pruning or excessive pruning, leading to catastrophic accuracy loss. (2) Progressive pruning:  This approach prunes tokens layer by layer, rather than performing a one-time early-layer pruning. Early pruning may fail to correctly identify redundant tokens, resulting in suboptimal or incorrect pruning. As mentioned earlier, prior W.E. in BOLT~\citep{pang2023bolt} is not input-specific or progressive. Furthermore, there is an absence of an encrypted protocol for polynomial production aimed at reducing the overhead of non-linear approximations.

\noindent\textbf{Challenge 2: Lacking network optimization to support joint token pruning and polynomial reduction.} In plaintext-domain token pruning~\citep{goyal2020power,kim2020lat,wang2021spatten}, there is no strong motivation to use polynomials to approximate non-linear activations, as these operations are straightforward and inexpensive to compute. However, in the ciphertext domain, large-degree polynomials are employed to approximate non-linear functions for both efficiency and accuracy. We observed that polynomials of varying degrees can be assigned to different tokens in the encrypted domain based on their importance scores, which can also be optimized alongside token pruning. For instance, joint optimization of polynomial reduction and token pruning is essential, and network optimization involves searching for the optimal pruning and reduction thresholds.

\begin{wrapfigure}{r}{0.4\textwidth}  % 'r' for right, and the width of the figure area
  \centering
  \vspace{-0.2in}
  \includegraphics[width=0.4\textwidth]{figures/proposed-overview.pdf}
  \caption{Overview of CipherPrune.}
  \vspace{-0.18in}
  \label{f:Proposed-overview}
\end{wrapfigure}


\noindent\textbf{CipherPrune Overview.} In this paper, we introduce CipherPrune for an efficient and scalable private Transformer inference. Figure~\ref{f:Proposed-overview} shows the overview of CipherPrune. We first propose encrypted token pruning for both linear and non-linear activations in Section~\ref{sec:prune}. Then, we develop an encrypted polynomial reduction for efficient non-linear operations in Section~\ref{sec:reduction}. We also introduce a network optimization for the joint optimization of token pruning and polynomial reduction in Section~\ref{sec:finetune}. 

Figure~\ref{fig:overview} shows the workflow of a private Transformer inference implementation with our CipherPrune. During the private inference, \ding{182} the client's input is encrypted and multiplied with the embedding matrix in the server by the $\Pi_{MatMul}$ protocol. Then the result will be added to the positional encoding. \ding{183} The server performs the private attention computations, including linear projection via $\Pi_{MatMul}$ and non-linear attention map via $\Pi_{SoftMax}$, respectively. After obtaining the tokens and attention maps, \ding{184} our proposed encrypted token pruning method is performed to calculate token importance scores and compare these scores with the pruning threshold $\theta$ and reduction threshold $\beta$ to decide which tokens will be pruned or reduced in a privacy-preserving manner. The pruned tokens are discarded such that the following layers will have fewer operations. Low-degree polynomials are used to compute the \gelu and \mysoftmax functions on reduced tokens. \ding{185} Layernorm and Feedforward operations will be executed via the prior protocols, $\Pi_{LayerNorm}$, $\Pi_{MatMul}$ and $\Pi_{GeLU}$. We detail our proposed encrypted token pruning method in the subsequent subsections.
%We detail the encrypted token pruning method in Section \ref{sec:prune}, which lies in conjunction with existing protocols to support efficient and private Transformer inferences. In Section \ref{sec:approx}, we introduce the dynamic approximation method for non-linear functions, such as the \gelu and \softmax functions. To effectively determine the optimal pruning threshold and approximation threshold, we introduce a crypto-aware finetuning strategy in Section \ref{sec:finetune}, which involves learning the pruning and approximation thresholds in a crypto-aware manner.

\begin{figure}[h]
%\vspace{-0.1in}
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \captionsetup{skip=2pt}
    \vspace{-0.1in}
    \caption{The workflow of a private Transformer inference with CipherPrune. }
    \label{fig:overview}
\vspace{-0.2in}
\end{figure}

%Token pruning is simple in plaintext due to basic comparison needs. However, 
\subsection{Encrypted Token Pruning}
\label{sec:prune}
\noindent\textbf{Pruning protocol $\Pi_{prune}$.} In private inference, confidentially pruning tokens presents a challenge, as the server and client must share attention maps and inputs without accessing their actual values. As depicted in Figure~\ref{fig:prune}, during inference, attention maps are protected using ASS, with each party only able to view their respective shares. %For linear operations, we continue to employ ASS. However, for non-linear operations, we utilize OT combined with ASS.
%In private inference, confidentially pruning tokens is challenging, as the server and client share attention maps and inputs without knowing their real values. As Figure~\ref{fig:prune} shows, during inference, the attention maps are protected by ASS and each party can only see the shares. For the linear operations, we still use ASS operations but for the non-linear operations, we adopt OT and combine it with ASS. Specifically, 
%We propose the secure token pruning protocol, $\Pi_{prune}$, as shown in Figure~\ref{fig:protocol-prune} to solve this problem.  Most importantly, pruning needs to avoid revealing extra information, such as pruning mask.  We have thus designed a secure protocol, $\Pi_{mask}$, shown in Figure~\ref{fig:protocol-mask} to tackle this challenge.  $\Pi_{prune}$ and $\Pi_{mask}$ enable the encrypted token pruning.
%\noindent\textbf{Secure Token Pruning Protocol: $\Pi_{prune}$.}

\begin{figure}[h]
    \vspace{-0.1in}
    \centering
\includegraphics[width=1\linewidth]{figures/prune.pdf}
    \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Illustration of mask generation and token pruning in $\Pi_{prune}$ with a non-sharing mask. }
    \label{fig:prune}
     \vspace{-0.1in}
\end{figure}

Specifically, the proposed secure token pruning protocol takes the secret-shared attention maps $\left \langle Att \right \rangle^{h}$ and tokens $\left \langle x \right \rangle$ as inputs, and outputs the pruned tokens $\left \langle y \right \rangle$ in a secret-sharing format. The secret shares are held by server $P_0$ and client $P_1$, respectively. First, $P_0$ and $P_1$ compute the importance score on their local secret shares, respectively. As depicted in Equation \ref{e:score}, the computation of the importance score involves only addition and constant multiplication, which can be performed efficiently via ASS. After $P_0$ and $P_1$ acquire their respective shares of the importance score $\left \langle S \right \rangle $, they initiate a comparison protocol $\Pi_{CMP}$. This protocol contrasts the importance score against the threshold $\theta$ learned offline, enabling them to determine the shares of the resultant mask $\left \langle M \right \rangle $, where $M$ is 1 if  $S > \theta$, otherwise 0. 

Possessing the shares $\left \langle M \right \rangle$ without access to their real values prevents the direct pruning of tokens $\left \langle x \right \rangle$. A feasible solution involves reconstructing the non-shared mask, allowing both parties to independently prune their shares of the input sequence of $n$ tokens $\left \langle x \right \rangle$. This process then enables them to obtain the shares of the pruned output sequence of $m$ tokens $\left \langle y \right \rangle$. Appendix \ref{app:a}, Figure~\ref{fig:protocol-prune} includes a more detailed and formal definition of $\Pi_{prune}$.

%In cases where the pruning location is not sensitive, $P_0$ and $P_1$ will obtain the actual binary mask $M \in \{0,1\}^{n}$, as opposed to just the shares of the mask $\left \langle M \right \rangle $. This binary mask enhances efficiency since it allows both parties to independently prune their shares of the input sequence of $n$ tokens $\left \langle x \right \rangle $ and subsequently acquire the shares of the pruned output sequence of $m$ tokens $\left \langle y \right \rangle$.

% \begin{figure*}[h]
% % \vspace{-0.4in}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/prune.png}
%     \captionsetup{skip=2pt}
%     % \vspace{-0.1in}
%     \caption{Illustration of token pruning with unprotected mask. }
%     \label{fig:prune}
% \vspace{-0.1in}
% \end{figure*}


The overhead for our secure token pruning protocol is minimal. The importance score can be computed directly on shares, taking only $0.1$ ms per attention module. This is efficient even for large models like BERT-Large, which has 24 heads per layer. Additionally, our protocol only requires $n$ invocations of the comparison protocol $\Pi_{CMP}$, each consistently completed within 5 ms, independent of the number of heads or embedding dimension. Thus, the total time complexity of our pruning protocol $\Pi_{prune}$ is linear, $O(n)$, based on the input token sequence length $n$.

%The overhead associated with our secure token pruning protocol is minimal. Since the importance score is directly computable on shares, it takes merely $0.1\ ms$ per attention module, applicable even to large models such as BERT-Large, which feature 24 heads in each layer. Furthermore, our pruning protocol requires only $n$ invocations of the comparison protocol $\Pi_{CMP}$, which consistently operates within a $5ms$ timeframe, independent of the number of heads or embedding dimension. Consequently, the total time complexity of the proposed $\Pi_{prune}$ is linear, $O(n)$, relative to the length of the input token sequence.

\begin{figure*}[h]
    \vspace{-0.1in}
    \centering
    \includegraphics[width=1\linewidth]{figures/mask.pdf}
    \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Example of token pruning with a protected mask.} %We remark that the actual value of each sharing will change after every OT-based protocol. We omit the change here for simplicity. }
    \label{fig:mask}
    % \vspace{-0.1in}
\end{figure*}

\noindent\textbf{Pruning Mask Protocol $\Pi_{mask}$.}
\label{sec:mask}
%\noindent\textbf{Secure Pruning Mask Protocol: $\Pi_{mask}$.}
%Although many cases that binary pruning mask is not needed to protect privacy, i.e., just exposing the locations of pruned tokens, ensuring the end-to-end privacy protection still require pruning mask $M$ remains undisclosed to each party, $P_0$ and $P_1$, since some cases pruned token locations posing risks under token importance attacks~\citep{wang2021importance1, shakeel2022importance2}. 
To further safeguard the privacy of the binary pruning mask \( M \), specifically to protect the locations of pruned tokens, we design an additional Pruning Mask Protocol, \( \Pi_{\text{mask}} \). The goal is to ensure that both parties, \( P_0 \) and \( P_1 \), can obtain the pruned token sequence without knowing which specific tokens were pruned. One key observation is that the number of tokens after pruning, $n'$, can be safely disclosed, as it is essential for subsequent processing and is not typically associated with significant security risks like adversarial attacks~\citep{cui2021sparsity}. Knowing $n'$ is crucial because token pruning involves relocating the less-important $m = n - n'$ tokens to the end of the token sequence while maintaining the order of the remaining $n'$ tokens. After this rearrangement, one can simply discard the $m$ ASS tokens at the end of the token list.

%Our goal is to process $n$ encrypted tokens and produce $n' \leq n$ tokens, without exposing the locations of the pruned tokens to either $P_0$ or $P_1$. \update{That is $P_0$ and $P_1$ should gain the pruned token sequence without knowing which tokens are pruned.} This task presents significant challenges. The key to addressing this challenge lies in the fact that both parties are aware of the original token count $n$ and the length of pruned token sequence $n'$. \update{The number of tokens $n$ and $n'$ is public to both $P_0$ and $P_1$ during private inference, which does not result in extra privacy breaches~\citep{cui2021sparsity}. Thus, token pruning is equivalent to moving $m$ tokens to the end of the token sequence without changing the order of the rest $n'$ tokens, where $m=n-n'$. We achieve this goal by OT-based oblivious swap using the secretly shared mask $\left \langle M \right \rangle$.}

Figure \ref{fig:mask} shows the secure mask protocol $\Pi_{mask}$ that is used to ensure the mask privacy in $\Pi_{prune}$. The protocol takes secret-shared token sequence $\left \langle x \right \rangle$ and mask $\left \langle M \right \rangle$ as inputs, and generates the pruned tokens $\left \langle y \right \rangle $. %The mask $\left \langle M \right \rangle$ indicates whether the input token should be pruned, i.e., if the mask value for the $i-th$ token $M[i]=0$, the token $x[i]$ should be pruned. The mask $M$ is secretly shared by $P_0$ and $P_1$ such that either party knows the real mask values. We show how $P_0$ and $P_1$ can perform token pruning on the secretly shared mask.
\ding{182} \textbf{Bind Mask and Tokens.}  To swap tokens, their corresponding links with the mask will be disrupted. To preserve these links, there are two methods: one is to swap the masks and tokens respectively and simultaneously; the other is to bind the mask and tokens together so that they can be swapped as a unit. We adopt the second method, as binding the mask and tokens together proves more efficient than managing separate swaps for the mask and tokens. The bounded tokens $\left \langle \Bar{x} \right \rangle$ can be obtained via left-shifting $\left \langle M \right \rangle$ by $f$ bits and adding to $\left \langle x \right \rangle$, where $f$ is the bit width of a token in $x$. Figure~\ref{fig:mask} illustrates using $f=4$ as an example; however, in practice, $f$ can be flexibly adjusted. \ding{183} \textbf{Derive Pruning Token Number $n'$.} We found that $n'$ can be obtained by securely counting the number of $1$s in $M$, which does not reveal the locations of $1$s in $M$. Specifically, to determine $n'$, both $P_0$ and $P_1$ first convert their boolean mask shares $\left \langle M \right \rangle$ into a fixed-point format using $\Pi_{B2A}$. Each party then locally computes the sum of the arithmetic mask using ASS, yielding $\left \langle n' \right \rangle$. Finally, $P_0$ and $P_1$ obtain $n'$ by summing their respective shares, $\left \langle n' \right \rangle_{0}$ and $\left \langle n' \right \rangle_{1}$.  
\begin{equation}
\footnotesize
\label{e:swap}
\begin{aligned}
Swap(\Bar{x}[i],\Bar{x}[i+1]) = 
\begin{cases}
    b \cdot \Bar{x}[i] + (1-b) \cdot \Bar{x}[i+1], \\
      b \cdot \Bar{x}[i+1] + (1-b) \cdot \Bar{x}[i].
\end{cases}
\end{aligned}
\end{equation}

\ding{184} \textbf{Secure Swap.}  This step aims to enable $P_0$ and $P_1$ to iteratively move $m$ tokens to the end of the token sequence via OT-based oblivious swap defined in Equation~\ref{e:swap}.In each iteration, $P_0$ and $P_1$ perform an oblivious swap through the token sequence. To privately swap two tokens $\Bar{x}[i]$ and $\Bar{x}[i+1]$, they first extract the MSB $b$ from the bounded token $\Bar{x}[i]$ and perform four OT-based multiplications. \ding{185} \textbf{Truncate Tokens and MSB.} $P_0$ and $P_1$ can truncate the swapped token sequence $\left \langle \Tilde{x} \right \rangle$ and remove the MSB respectively to obtain the pruned token sequence. Figure~\ref{fig:protocol-mask} in the Appendix \ref{app:a} details more about the mask protection protocol. 

\textbf{Analysis.} The complexity of the proposed $\Pi_{mask}$ mainly depends on the number of oblivious swaps. To prune $m$ tokens out of $n$ input tokens, $O(mn)$ swaps are needed. Since token pruning is performed progressively, only a small number of tokens are pruned at each layer, which makes $\Pi_{mask}$ efficient during runtime. Specifically, for a BERT-Base model with 128 input tokens, the pruning protocol only takes $\sim0.9$s on average in each layer. %An alternative approach is to invoke an oblivious sort algorithm~\citep{bogdanov2014swap2,pang2023bolt} on $\left \langle \Bar{x} \right \rangle$. However, this approach is less efficient because it blindly sort the whole token sequence without considering $m$. That is, even if only $1$ token needs to be pruned, $O(nlog^{2}n)\sim O(n^2)$ oblivious swaps are needed, where as the proposed $\Pi_{mask}$ only need $O(n)$ swaps. \update{More generally, for an $\ell$-layer Transformer with a total of $m$ tokens pruned, the overall time complexity using the sort strategy would be $O(\ell n^2)$ while using the swap strategy remains an overall complexity of $O(mn).$ Specifically, using the sort strategy to prune tokens in one BERT Base model layer can take up to $3.8\sim4.5$ s depending on the sorting algorithm used. In contrast, using the swap strategy only needs $0.5$ s. Moreover, alternative to our MSB strategy, one can also swap the encrypted mask along with the encrypted token sequence. However, we find that this doubles the number of swaps needed, and thus is less efficient the our MSB strategy.
%Detailed results can be found in Figure \ref{fig:msb} in Section \ref{s:res}.}

\begin{figure*}[h]
 \vspace{-0.1in}
    \centering
    \includegraphics[width=1\linewidth]{figures/approx.png}
    \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Comparison of token pruning-only method and pruning with polynomial reduction.}
    \label{fig:approx}
    \vspace{-0.2in}
\end{figure*}

\subsection{Encrypted Polynomial Reduction}
\label{sec:reduction}
%\qian{Modify the polynomial approximation into polynomial pruning (prune large-degree polynomial into small-degree polynomial.)}

 %Our experiments show that directly adopting fully polynomial approximation-based methods~\citep{lu2023bumblebee, pang2023bolt} will introduce more than 5\% accuracy loss on BERT classification tasks. 

After pruning, retained tokens still require expensive operations, particularly for costly non-linear functions. These non-linear functions are computed via expensive high-degree polynomials~\citep{lu2023bumblebee, pang2023bolt}. We notice that we can reduce the high-degree polynomials to their low-degree counterparts for the less important tokens. As demonstrated in Figure~\ref{fig:approx} (a), the cost of reduced polynomial can be $0.1\times$ that of the high-degree polynomial. This motivates us to accelerate the non-linear operations with low-degree polynomials while maintaining the desired accuracy. Similar to employing a threshold $\theta$ to prune tokens with importance scores below $\theta$, we use another reduction threshold $\beta$ ($\beta > \theta$) to identify tokens for reduction. As shown in Figure~\ref{fig:approx} (b)(c), combining token pruning with polynomial reduction further reduces execution time compared to the pruning-only method. Importantly, we can optimize both $\theta$
\begin{wrapfigure}{r}{0.4\textwidth}  % 'r' for right, and the width of the figure area
  \centering
  \vspace{-0.1in}
  \includegraphics[width=0.4\textwidth]{figures/approxs.pdf}
  \caption{Secure polynomial reduction.}
  \vspace{-0.1in}
  \label{f:approx-protocol}
\end{wrapfigure}
and $\beta$ together during offline fine-tuning to enhance efficiency. During the online inference phase, polynomial reduction occurs after token pruning (the pruning ratio {greater} than zero). This simplifies the reduction process: the tokens have already been pruned, and the locations of these tokens are rotated and concealed. Consequently, there's no need to safeguard the token mask for reduction. Instead, we can simply modify the pruning protocol $\Pi_{Prune}$ to establish the reduction protocol. As illustrated in Figure~\ref{f:approx-protocol}, the pruned tokens are determined by executing protocols $\Pi_{prune}$ and $\Pi_{mask}$ in tandem. A secure comparison with the reduction threshold $\beta$ then produces the reduction mask $\left \langle M_{\beta} \right \rangle$. The location of this mask corresponds to pruned tokens, not the original tokens, so revealing it does not compromise the location privacy of reduced tokens, provided that the privacy of pruned locations is maintained. Once the reduction mask $M_{\beta}$ is known to each party, it can be used to guide the decision on whether to apply the high-degree polynomials or low-degree ones for the non-linear functions, where 1 indicates using the high-degree ones and 0 signifies low-degree ones. The choice of these approximation polynomials can be flexible. We utilize prior non-linear function approximation methods as detailed in references such as~\citep{kim2021ibert, lu2023bumblebee, pang2023bolt}. The specific configurations used are outlined in the Appendix \ref{app:b}.

\vspace{0.1in}
\subsection{Network Optimization for Encrypted Pruning and Polynomial Reduction}
\label{sec:finetune}

To support online token pruning and non-linear approximation, an offline fine-tuning method is needed to optimize pruning and approximation thresholds, $\theta$ and $\beta$, to minimize inference overhead and achieve user-defined accuracy above $a$. This process is challenging because (1) previous studies have not incorporated the efficiency and accuracy of encrypted token pruning and non-linear approximation into the fine-tuning phase, and (2) the thresholds for different Transformer layers vary and are challenging to pinpoint. To address these challenges, we introduce a crypto-aware fine-tuning method, outlined in Algorithm~\ref{alg:threshold}. This method uses a gradient search approach and proposes to incorporate crypto-aware pruning and approximation into the training phase. Additionally, the loss functions are designed to optimize both efficiency and accuracy. 


%To enhance the efficiency of private inference, we propose to prune less important tokens to reduce the computation overhead. Moreover, as the non-linear activation functions such as \softmax and \gelu are expensive to be computed accurately within cryptographic protocols, we propose to approximate these activations for part of the unpruned tokens to further enhance the efficiency. However, above methods pose two main challenges: (1) The thresholds vary in different Transformer layers and are hard to determine~\citep{kim2022LTP}. (2) The gap bewtween non-linear activation and polynomial approximation potentially degrades Transformer's performance~\citep{gilad2016cryptonets, lou2020safenet}. To address these challenges, we propose a crypto-aware finetuning strategy to learn the thresholds that are adaptive to the input length and content and to adapt the Transformer to polynomial activation functions.
\vspace{-0.1in}
\setlength{\textfloatsep}{3pt}
\begin{algorithm}[t!]
\caption{Crypto-aware Thresholds Learning}
\label{alg:threshold}
\footnotesize
$\text{Input: }
\text{ pre-trained Transformer } \mathcal{M},
\text{ training data } \mathcal{D},
%\text{ accuracy threshold }& \alpha \quad\\
\text{ initial thresholds } \theta, \beta \text{}
$\\
1. Set model $\mathcal{M}$ with $L$ layers, weights $w$, input tokens $x$, accuracy requirement $a$, and hyperparameters $T, \lambda, \alpha$.\\
2. Search for optimal thresholds $\theta$, $\beta$ and weights $w$ on data $\mathcal{D}$.
\begin{algorithmic}
%\WHILE{$accuracy < \alpha$}
\STATE (a) For $l \in [L]$, set soft masks for token $x_i$, $M^{(l)}_{\theta}(x_i) = \sigma (\frac{S^{(l)}(x_i)-\theta^{(l)}}{T})$ and $M^{(l)}_{\beta}(x_i) =\sigma (\frac{S^{(l)}(x_i)-\beta^{(l)}}{T})$.
\STATE (b) For $l \in [L]$, integrate the crypto-friendly polynomial activation functions. In the $l$-th layer, compute $\mathsf{GELU}$ function as: $\mathsf{GELU}(x_i) = M_{\beta}^{(l)}(x_i)\cdot \mathsf{GELU}(x_i) + (1-M_{\beta}^{(l)}(x_i))\cdot \mathsf{ApproxGELU}(x_i)$.
Except for the first layer, compute $\mathsf{SoftMax}$ function as: $\mathsf{SoftMax}(x_i) = M_{\beta}^{(l-1)}(x_i)\cdot \mathsf{SoftMax}(x_i) + (1-M_{\beta}^{(l-1)}(x_i))\cdot \mathsf{ApproxSoftMax}(x_i)$.
For the input feature $x_{in}$, compute the output feature $x_{out}$ in the $l$-th layer as: $x_{out} = M_{\theta}^{(l)}(x_{in})\cdot x_{out}$
\STATE (c) Update $w$, $\theta$ and $\beta$ jointly to minimize the loss $\mathcal{L}$, where $\mathcal{L} = \mathcal{L}_{task} + \lambda (\mathcal{L}_{prune} + \alpha \mathcal{L}_{approx.})$.\\
\end{algorithmic}
3. Finetune $w$ on data $\mathcal{D}$ with learned thresholds $\theta$ and  $\beta$.
\begin{algorithmic}
\STATE (a) Fix the threshold $\theta$, $\beta$. Binarize the mask $M^{(l)}_{\theta}$ in every layer as:
\STATE \hspace{\algorithmicindent} $M^{(l)}_{\theta}(x_i) = \begin{cases}
    1  &\text{if}\ S^{(l)}(x_i) > \theta^{(l)}, \\
    0  &\text{otherwise}.
\end{cases} $, and
$M^{(l)}_{\beta}$ is binarized in the same way.
\STATE (b) Update $w$ to minimize the loss $\mathcal{L}_{task}$. Derive the optimal fine-tuned transformer $\mathcal{M}^*$.
\end{algorithmic}
4. Output  $\theta$, $\beta$and $w$ if accuracy $\geq a$; otherwise back to step 2. 
\end{algorithm}

%\noindent\textbf{Crypto-aware Threshold Learning.} 
After initialization, we make the masks differentiable during fine-tuning to allow for trainable thresholds, as shown in step 2.(a) of Algorithm~\ref{alg:threshold}. Here, \(T\) represents the temperature, and \(\sigma\) is the Sigmoid function. This soft mask, a differentiable approximation of the binary mask, enables gradient-based updates to \(\theta\) and \(\beta\). In step 2.(b) of the same algorithm, we introduce polynomial activation functions during the fine-tuning phase. \(\mathsf{ApproxSoftMax}\) and \(\mathsf{ApproxGELU}\) are low-degree polynomial approximations of the \(\mathsf{SoftMax}\) and \(\mathsf{GELU}\) functions. {The $\mathsf{ApproxSoftMax}$ replaces the exponential function $e^x$ in original $\mathsf{SoftMax}$ with a Taylor series $(1+ \frac{x}{2^n})^{2^n}$. And the $\mathsf{ApproxGELU}$ leverages simple polynomials such as $p^3(x) = -0.51-0.42x^2 + -0.12x^2 - 0.01x^3$ to approximate $\mathsf{GELU}$. We defer the detailed polynomial in Equations~\ref{eq:app softmax} and \ref{eq:app gelu} in the Appendix \ref{app:b}.} If a token \(x_i\)'s importance score exceeds the threshold \(\beta\), it activates mainly through the original \(\mathsf{SoftMax}\) or \(\mathsf{GELU}\) functions; otherwise, through their polynomial approximations.

% detailed in Equations~\ref{eq:app softmax} and \ref{eq:app gelu} in the Appendix, 

%This method partially replaces activation functions during the fine-tuning phase, adapting the Transformer model for polynomial approximations and enhancing stability for inference where functions are partially replaced.
%After initialization, to render the thresholds trainable, we make the masks differentiable during fine-tuning, as illustrated in step 2.(a) of Algorithm \ref{alg:threshold}. Here, \( T \) represents the temperature and \( \sigma \) is the Sigmoid function. This soft mask, a differentiable approximation of the binary mask, allows for gradient-based updates to \(\theta\) and \(\beta\). Next, we introduce polynomial activation functions in the finetuning phase as is shown in step 2.(b) of Algorithm \ref{alg:threshold}. Here $\mathsf{ApproxSoftMax}$ and $\mathsf{ApproxGELU}$ are low-degree polynomial approximations of $\mathsf{SoftMax}$ and $\mathsf{GELU}$ functions detailed in Equation (\ref{eq:app softmax}) and Equation (\ref{eq:app gelu}) in Appendix. If the importance score of token $x_i$ is above the threshold $\beta$, $x_i$'s activation will mainly be computed by the $\mathsf{SoftMax}$ or $\mathsf{GELU}$ function; otherwise by their polynomial approximations.  This approach replaces the activation function partially during finetuning phase and adapts the Transformer model to polynomial approximations. Such crypto-aware funetuing contributes to a more stable performance for inference, where activation functions are partially replaced. 
\vspace{-0.25in}
\begin{equation}
\footnotesize
% \begin{aligned}
\label{e:loss}
\mathcal{L}_{prune} = \frac{1}{L}\sum_{l=0}^{L-1} \left \| M^{(l)}_{\theta}(x) \right \|_{1}, \mathcal{L}_{approx.} = \frac{1}{L}\sum_{l=0}^{L-1} \left \| M^{(l)}_{\beta}(x) \right \|_{1}
% \end{aligned}
\vspace{-0.2in}
\end{equation}

The overall objective function is designed to minimize the loss function 
$\mathcal{L} = \mathcal{L}_{task} + \lambda(\mathcal{L}_{prune} + \alpha \mathcal{L}_{approx.}) $
where \(L\) denotes the number of layers. \(\mathcal{L}_{\text{task}}\) optimizes accuracy for downstream tasks, while \(\mathcal{L}_{\text{prune}}\) and \(\mathcal{L}_{\text{approx.}}\), defined by \(M_{\theta}\)'s and \(M_{\beta}\)'s \(l_1\)-norms respectively, target efficiency as detailed in Equation~\ref{e:loss}. The hyperparameters \(\lambda\) and \(\alpha\) dictate the extent of pruning and approximation, with higher values leading to increased pruning or approximation. This structure introduces additional gradients, pushing \(\theta\) and \(\beta\) towards minimizing \(\mathcal{L}_{\text{prune}}\) and \(\mathcal{L}_{\text{approx.}}\). Once the optimized \(\theta\) and \(\beta\) are determined, we fix them and proceed to fine-tune the model weights to meet the accuracy requirement \(a\) as specified in Step 3. For each Transformer layer, we binarize the masks \(M_{\theta}\) and \(M_{\beta}\) to select tokens for pruning or approximation. Subsequently, we update the model weights to minimize the downstream task loss \(\mathcal{L}_{task}\).

