%We introduce CipherPrune, an encrypted token pruning technique designed to boost the efficiency of private Transformer inference without sacrificing accuracy or privacy. CipherPrune includes protocols for token pruning, pruning mask privacy, and polynomial reduction alongside pruning. Our experiments show that CipherPrune is lightweight and significantly reduces both runtime and communication overhead in 2PC private Transformer inference while preserving model accuracy.
% \Qian{This paragraph also needs to be updated, especially for some terms. }
The proposed CipherPrune addresses the critical efficiency and scalability challenges of private Transformer inference by introducing a novel approach that combines encrypted token pruning and polynomial reduction protocols. By progressively pruning redundant tokens and reducing the polynomial degree for less important tokens, CipherPrune significantly reduces runtime overhead while maintaining accuracy. Our experiments confirm its effectiveness, achieving a substantial reduction in execution time compared to previous methods. 