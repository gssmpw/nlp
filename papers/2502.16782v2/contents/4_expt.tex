\subsection{Experimental Setup}
%In this section, we introduce the experimental methodology.


\noindent\textbf{Models and Datasets}. We evaluated CipherPrune on the GPT2-Base and three BERT variants~\citep{devlin2018bert}: BERT-Medium, BERT-Base, and BERT-Large. These models are commonly used in private Transformer frameworks. Similar to prior work~\citep{pang2023bolt}, we fine-tune the BERT models on four downstream NLP tasks in GLUE benchmarks~\citep{wang2018glue}: the Multi-Genre Natural Language Inference Corpus (MNLI), the Stanford Question Answering Dataset (QNLI), the Stanford Sentiment Treebank (SST-2), and the Microsoft Research Paraphrase Corpus (MRPC).

% achieving comparable performance to floating-point counterparts~\citep{rathee2020cryptflow2, hao2022iron-iron}. 

\noindent\textbf{System Setup and Implementation}. We encode floating-point parameters in Transformers into fixed-point numbers and set the scale according to prior works\citep{hao2022iron-iron, lu2023bumblebee, pang2023bolt}. CipherPrune uses the EzPC~\citep{EzPC} framework and the SEAL~\citep{SEAL} library. EzPC compiles TensorFlow-based deep neural networks into secure computation protocols running on cryptographic backends. We simulate LAN with 3Gbps bandwidth and 0.8ms ping, and WAN with 200Mbps bandwidth and 40ms ping, following~\citep{pang2023bolt}. All experiments are conducted on an AMD Ryzen Threadripper PRO 3955WX (2.2GHz, 125GB RAM) and fine-tuning of the BERT model with threshold learning is done on NVIDIA GeForce RTX 3090 GPUs with CUDA 11.0.3. %We encode floating-point parameters in Transformers into 37-bit fixed-point variables like prior work~\citep{hao2022iron-iron}. Such a fixed-point encoding can perform similarly to its floating point counterpart~\citep{rathee2020cryptflow2, hao2022iron-iron}. CipherPrune leverages EzPC~\citep{EzPC} framework and utilizes the SEAL~\citep{SEAL} library. The EzPC framework compiles deep neural networks from TensorFlow code to secure computation protocols, which run on the proposed cryptographic backends. Similar to~\citep{rathee2021sirnn, hao2022iron-iron, huang2022cheetah}, we simulate the LAN setting with $1Gbps$ bandwidth and a ping time of $0.5ms$ and the WAN setting with $400Mbps$ bandwidth and a ping time of $10ms$. All experiments are performed on AMD Ryzen Threadripper PRO 3955WX running at 2.2GHz with $125GB$ memory. The fine-tuning of BERT model and threshold learning is performed with NVIDIA GeForce RTX 3090 GPU with CUDA 11.0.3.



%For the SoftMax protocol, we adopt a similar strategy as~\citep{kim2021ibert, hao2022iron-iron}, where we evaluate $SoftMax(x-max_{i\in [d]}x_i)$. Different from~\citep{hao2022iron-iron}, we did not used the binary tree to find max value in the given vector. Instead, we traverse through the vector to find the max value. This is because each attention map is computed independently and the binary tree cannot be re-used, introducing additional overhead.

%\noindent\textbf{System Setup}. CipherPrune leverages EzPC~\citep{EzPC} framework and utilizes the SEAL~\citep{SEAL} library. The EzPC framework compiles deep neural networks from TensorFlow code to secure computation protocols, which runs on the proposed cryptographic backends. Similar to~\citep{rathee2021sirnn, hao2022iron-iron, huang2022cheetah}, we simulate the LAN setting with $1Gbps$ bandwidth and a ping time of $0.5ms$ and the WAN setting with $400Mbps$ bandwidth and a ping time of $10ms$. All experiments are performed on AMD Ryzen Threadripper PRO 3955WX running at 2.2GHz with $125GB$ memory. The fine-tuning of BERT model and threshold learning is performed with NVIDIA GeForce RTX 3090 GPU with CUDA 11.0.3.


%We conducted comprehensive experiments to evaluate the proposed CipherPrune's performance. Specifically, our experiments demonstrate notable runtime latency and communication savings over existing methods~\citep{hao2022iron-iron, pang2023bolt, lu2023bumblebee}, with only marginal degradation across various tasks.

\subsection{Results}
% Bumblebee use differen OT, directly comparison. (change all base to ezpc???)
\begin{table*}[h]
\captionsetup{skip=2pt}
\centering
\scriptsize
\caption{End-to-end comparison of CipherPrune with prior works on BERT models. Time is in seconds. Comm. stands for communication in GB and Acc. for accuracy in percentage.}
\begin{tblr}{
    colspec = {c |c c c | c c c | c c c},
    row{1} = {font=\bfseries},
    row{2-Z} = {rowsep=1pt},
    % row{4} = {bg=LightBlue},
    colsep = 2.5pt,
    }
\hline
\SetCell[r=2]{c}\textbf{Method}  &\SetCell[c=3]{c}\textbf{BERT Medium} &&&\SetCell[c=3]{c}\textbf{BERT Base} &&&\SetCell[c=3]{c}\textbf{BERT Large}
\\& \textbf{Time} & \textbf{Comm.} & \textbf{Acc.}
&\textbf{Time} & \textbf{Comm.} & \textbf{Acc.} &\textbf{Time} & \textbf{Comm.} & \textbf{Acc.}\\
\hline
IRON~\citep{hao2022iron-iron} &442.4 &124.5 &87.7$_{\pm 0.2}$ &1087.8 &281.0 &90.4$_{\pm 0.1}$ &2873.5 &744.8 &92.7$_{\pm 0.1}$\\
% BumbleBee~\citep{lu2023bumblebee} & & &87.5$_{\pm 0.5}$ & & &90.2$_{\pm 0.2}$ & & &92.7$_{\pm 0.1}$\\
BOLT w/o W.E.~\citep{pang2023bolt} &197.1 &27.9 &87.4$_{\pm 0.3}$ &484.5 &59.6 &90.3$_{\pm 0.1}$ &1279.8 &142.6 &92.6$_{\pm 0.2}$\\
BOLT~\citep{pang2023bolt} &99.5 &14.3 &87.2$_{\pm 0.3}$ &245.4 &25.7 &89.9$_{\pm 0.3}$ &624.3 &67.9 &92.4$_{\pm 0.2}$\\
\hline
CipherPrune &43.6 &6.7 &87.4$_{\pm 0.2}$ &79.1 &9.7 &90.1$_{\pm 0.2}$ &157.6 &18.4 &92.5$_{\pm 0.1}$\\
\hline
\end{tblr}
\label{tab:end}
\end{table*}
%\qian{The first column in the table added the citations; then we need to use the same numbers reported in the paper.  }

% This improvement is due to CipherPrune's token pruning strategy, excluding pruned tokens from calculations in subsequent layers, benefiting Transformers with more layers.

\noindent\textbf{End-to-end performance.}
In Table \ref{tab:end}, we evaluate CipherPrune on three BERT models, comparing it with previous private Trasnformer frameworks: IRON~\citep{hao2022iron-iron} and BOLT~\citep{pang2023bolt}. %IRON uses an OT-based look-up table (LUT) for non-linear functions, and BOLT uses high-degree polynomial approximations. 
CipherPrune achieves up to $\sim18.2\times$ speedup over IRON on the BERT-Large model and $\sim8.1\times$ speedup over vanilla BOLT without W.E.. When compared with BOLT with the word elimination technique, CipherPrune is still $\sim3.9\times$ faster without compromising accuracy. Communication costs are also reduced by $2.3\sim40.4\times$ compared to prior works. Compared with BOLT, CipherPrune can remove more redundant tokens during inference thorough the adaptive and progressive pruning strategy. Moreover, CipherPrune also leverages low-degree polynomials to further reduce the computation and communication overhead. CipherPrune can easily extend to other frameworks. Comparison with more related works like BumbleBee~\citep{lu2023bumblebee}, MPCFormer~\citep{li2022mpcformer} and PUMA~\citep{dong2023puma} can be found in Appendix~\ref{app:c}. 

%CipherPrune's improvement in efficiency scales with the model size. Specifically, CipherPrune shows $\sim4.5\times$ speedup over vanilla BOLT on BERT-Medium, and $\sim6.1\times$ and $\sim 8.1 \times$ speedup on BERT-Base and BERT-Large, respectively. 

% We detail the effect of the pruning and approximation strategies in CipherPrune in the next subsection. 

% The improvement of BOLT over IRON mainly results from reducing the number of tokens with word elimination and replacing heavy LUT with polynomial approximations.

% While BOLT and BumbleBee also reduce computation and communication overhead, they replace all $\mathsf{GELU}$ and $\mathsf{SoftMax}$ functions with polynomial approximations, leading to notable accuracy loss. CipherPrune approximates only part of these functions, maintaining stable performance with less than $1\%$ accuracy loss.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.66\textwidth}
        \centering
        \scriptsize
        % Using captionof to specify that this is a table caption
        \captionof{table}{Accuracy and time comparisons of different methods. CipherPrune$^\dag$  stands for CipherPrune with token pruning only. }
        \begin{tblr}{
            colspec = {c |c c c c | c },
            row{1} = {font=\bfseries},
            row{2-Z} = {rowsep=1pt},
        }
        \hline
        \SetCell[r=2]{c}\textbf{Method} & \SetCell[c=4]{c}\textbf{Accuracy Metric on Tasks (\%)} &&&& \SetCell[r=2]{c}\textbf{Time(Sec)}  \\
        & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST2} & \textbf{MPRC} \\
        \hline
        % IRON~\citep{hao2022iron-iron} &84.75 &90.87 &92.74 &84.55 &1897.8\\
        BOLT w/o W.E. &84.75 &90.32 &91.74 &90.53 &484.5\\
        BOLT &84.71 &89.94 &92.74 &89.95 &245.4\\
        CipherPrune$^\dag$ & 84.74 & 90.17 & 92.75 & 90.45 &115.3\\
        \hline
        CipherPrune &84.68 &90.11 &92.66 &90.18 &79.1\\
        \hline
        \end{tblr}
        \label{tab:prune_acc}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/tokens.pdf}
        \caption{Runtime on GPT2.}
        \label{fig:token_num}
    \end{minipage}
\end{figure}
%\qian{This table needs to change IRON with BOLT; Keep Prune-only, delete Prune and Fully Approx; Keep Cipherprune.} 

%\qian{This figure needs to add BOLT.  Since scalability is an advantage of the proposed work over BOLT.} 

% \begin{table*}[h]
% \captionsetup{skip=2pt}
% \centering
% \small
% \caption{Comparison of accuracy on different benchmarks.}
% \begin{tblr}{
%     colspec = {c |c c c c | c },
%     row{1} = {font=\bfseries},
%     row{2-Z} = {rowsep=1pt},
%     % colsep = 4pt,
%     }
% \hline
% \SetCell[r=2]{c}\textbf{Method} & \SetCell[c=4]{c}\textbf{Accuracy Metric on Tasks (\%)} &&&& \SetCell[r=2]{c}\textbf{Time(Sec)}  \\
% & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST2} & \textbf{MPRC} \\
% \hline
% Fixed-Point Baseline &84.75 &90.87 &92.74 &84.55 &1874.8\\
% Prune& 83.94 & 90.39 & 92.38 & 83.99 &468.6\\
% % Approximation &81.1 &87.6 &88.3 &80.8 &344.1\\
% Prune and Approx. &80.7 &87.2 &87.9 &80.1 &92.1\\
% \hline
% CipherPrune &83.9 &90.3 &92.3 &83.8 &119.2\\
% \hline
% \end{tblr}
% \label{tab:prune_acc}
% \end{table*}

 % \Qian{make term consistent (also check the whole paper).}
\noindent\textbf{Token pruning and polynomial reduction.}
%In Table \ref{tab:prune_acc}, we present the accuracy of BERT base model across four GLUE task. We construct a fixed-point baseline based on IRON and study the impact of the proposed methods. The baseline uses a fixed-point encoding to represent the floating-point numbers in plaintext models and leverages the lookup-table to compute non-linear function accurately. This makes the baseline's accuracy matches with the plaintext model. 
Table \ref{tab:prune_acc} demonstrates the the effects of the main design blocks in CipherPrune: adaptive token pruning and polynomial reduction. Our baseline is the vanilla BOLT framework without W.E.. BOLT's W.E. removes $50\%$ of the input tokens and effectively cuts the overhead of cryptographic protocols by half. With fine-tuning, the W.E. incurs only marginal accuracy loss. Yet, the adaptive and progressive token pruning in CipherPrune$^\dag$ can further improve the utility-accuracy trade-off. Instead of setting the pruning ratio as $50\%$ manually, CipherPrune$^\dag$ adaptively decides the pruning ratio based on both the input length and content. This contributes to up to $0.5\%$ better accuracy. On the other hand, the progressive pruning in CipherPrune$^\dag$ allows to remove more redundant information, contributing to $2.1\times$ runtime speed up over BOLT with W.E.. By incorporating polynomial reduction, CipherPrune can achieve up to $6.1\times$ speed up over BOLT. While the accuracy drops slightly from CipherPrune$^\dag$, it is still comparable or even higher than BOLT.
% of crypto-aware fine-tuning. 
% By introducing secure token pruning to our baseline IRON, we find that the \textit{prune-only} method results in marginal accuracy degradation within $1\%$, while latency is improved by $\sim4\times$. Fully approximating the retained tokens with low-degree polynomials can further improve runtime by $\sim 5\times$, but causes more than a $3\%$ accuracy drop across all tasks. CipherPrune's dynamic approximation and crypto-aware finetuning mitigate the accuracy drop while maintaining runtime efficiency. CipherPrune selectively approximates less important tokens and computes others accurately via the lookup table. As a result, CipherPrune achieves a stable performance with less than $1\%$ accuracy drop and a $\sim 16 \times$ runtime improvement compared to the baseline IRON.

%Moreover, by replacing partial non-linear functions with their polynomial approximations in the finetuning phase, CipherPrune enables the Transformer to adapt better to the polynomial activation functions during inference time. 
%CipherPrune maintains a stable performance, with only less than $1\%$ drop in model accuracy, while improving the runtime by $\sim 16 \times$ compared with the baseline. %CipherPrune can effectively work with both LUT-based protocols and approximation-based protols. This compatibility highlights CipherPrune as a versatile approach that can be incorporated into existing backend frameworks. This demonstrates the efficacy of the proposed CipherPrune strategy. 

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/breakdown.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime breakdown on BERT-Base model.}
%     \label{fig:breakdown}
% \end{figure*}

\noindent\textbf{Scalability with the input length.} 
In Figure \ref{fig:token_num}, we compare the runtime of CipherPrune and BOLT with varying input token numbers on GPT2. The baseline is BOLT without W.E.. The quadratic complexity of Transformer inference makes it challenging for BOLT to scale to long inputs. Although W.E. can reduce the overhead of private inference by half, BOLT with W.E. still scales quadratically with the number of input tokens. In contrast, CipherPrune demonstrates increasingly significant runtime savings as the input length grows. With 32 input tokens, CipherPrune achieves a $\sim 1.9\times$ speedup. When the input length reaches 512 tokens, CipherPrune is $\sim10.6\times$ faster than the baseline.

% Instead of pruning the input only once at the first layer, CipherPrune removes less important tokens gradually. This allows CipherPrune to effectively mitigate the quadratic complexity during private Transformer inference.

% This improvement is due to the quadratic complexity in BumbleBee, which becomes a performance bottleneck as input tokens increase. In contrast, CipherPrune effectively reduces the number of tokens during inference, boosting efficiency for longer token sequences.

\begin{figure}[h]
    \centering
    \vspace{-0.2in}
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/breakdown.pdf}
        \captionsetup{skip=2pt}
        \caption{Runtime breakdown on BERT-Base model.}
        \label{fig:breakdown}
    \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{figures/msb.pdf}
      \caption{Runtime comparison of different pruning protocols.}
      \label{fig:msb}
    \end{minipage}
  % \vspace{-0.2in}
\end{figure}
% \Qian{For Figure 11, indicating which bars are our works will be better. We could only leave the best if the other two are our works. It can also save description space.}
%\qian{Although this figure shows that prune overhead is marginal to the whole process, there is no place to show the concrete runtime comparison between BOLT's prune and the proposed method. This is essential with the complexity analysis (BOLT: $O(nlogn)$; ours: $O(n)$) }

% The SoftMax and GELU protocols, $\Pi_{SoftMax}$ and $\Pi_{GELU}$, significantly contribute to the total runtime. This is due to SoftMax's $O(n^2)$ complexity relative to the input length and GELU's involvement in multiple multiplication and truncation operations on high-dimensional matrices.


% While CipherPrune reduces the runtime of other protocols like $\Pi_{SoftMax}$ and $\Pi_{GELU}$ by a significant margin, the runtime of the pruning protocols is marginal. Such remarkable efficiency results from the lightweight design of the pruning protocols.
%\subsection{Overhead and Breakdown}
\noindent\textbf{Runtime breakdown.}
In Figure \ref{fig:breakdown}, we break down the runtime for each protocol in the BERT-Base model with 128 input tokens. In the LAN setting, the communication is efficient and the main bottleneck is the HE-based linear operation. In contrast, the massive communication of the non-linear operations becomes the bottleneck in the WAN setting. Since pruned tokens are excluded from the computation in all subsequent layers, CipherPrune can effectively reduce the overhead of both linear and non-linear operations. This contributes to CipherPrune's efficiency in both LAN setting and WAN setting. As shown in Figure \ref{fig:breakdown}, the proposed pruning protocols in CipherPrune are lightweight, accounting for only $1.6\%$ of the total runtime. This is because $\Pi_{prune}$ leverages ASS to offload substantial computation to the local side, such as accumulating the importance score. Additionally, $\Pi_{mask}$ utilizes the number of tokens in each layer to avoid sorting the whole token sequence.

% However, sorting the whole token sequence is not necessary. The runtime of pruning protocols are main decided by the number of oblivious swaps
\noindent\textbf{Analysis on different pruning protocols.}
As shown in Figure \ref{fig:msb}, we compare the efficiency of different pruning protocols. BOLT's W.E. uses Bitonic sort to sort the whole token sequence, which
\begin{wrapfigure}{r}{0.35\textwidth}  % 'r' for right, and the width of the figure area
    \vspace{-0.15in}
    \centering
    \includegraphics[width=1\linewidth]{figures/ratio.pdf}
    \captionsetup{skip=2pt}
    \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
    \label{fig:param}
    \vspace{-0.2in}
\end{wrapfigure}
needs $O(n\log ^2 n)$ oblivious swaps. 
In CipherPrune, the client and server only need $O(mn)$ oblivious swaps to relocate and prune the less important tokens. Since only a small number of tokens are removed in each layer, CipherPrune has a linear complexity to $n$ in general. By binding the mask with tokens on the MSB, CipherPrune can handle the token sequence and pruning mask in one go and achieves $2.2\sim20.3\times$ speed up.

%Since both the server and the client know the number of tokens before and after pruning, the number of tokens to be pruned, $m$, can be revealed to them in advance. 

% The swap-based pruning protocol in CipherPrune is $1.4\sim11.2\times$ more efficient than BOLT. Moreover, BOLT sorts not only the token sequence, but also the corresponding importance scores. In contrast,

% Moreover, BOLT sorts not only the token sequence, but also the corresponding importance scores. CipherPrune only needs to handle the token sequence by binding the mask with tokens on the MSB. As shown in Figure \ref{fig:msb}, the swap-based pruning protocol in CipherPrune is $1.4\sim11.2\times$ more efficient than BOLT. With the MSB strategy, the speed up can be $2.2\sim20.3\times$ in different layers.

% Thus, the runtime of $\Pi_{prune}$ is $1\sim2$ orders of magnitude smaller than other heavy non-linear protocols.

% \noindent\textbf{Comparison of different pruning strategy.} Show that adaptive pruning leads to better accuracy (compared with word elimination), especially in tasks like machine translation (generation task).  

\noindent\textbf{Study on the pruning parameters.} In Figure \ref{fig:param}, we show the accuracy-latency trade-off for the BERT-Base model under different parameter settings. Larger 
$\lambda$ and $\alpha$ result in more tokens being pruned or reduced. With $\lambda$ less than 0.05, an appropriate ratio of tokens is pruned, maintaining a stable accuracy of around 90\%. Smaller $\alpha$ leads to more tokens being computed with high-degree polynomials, which increases accuracy but also latency. Notably, accuracy with a large $\alpha$ is higher than with a large $\lambda$. This is because many tokens are reduced but not discarded, preserving necessary information for accurate inference.

% \noindent\textbf{Extension to other private inference frameworks.} 

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/msb.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime of $\Pi_{prune}$ and $\Pi_{mask}$ in different layers. We compare different secure pruning strategies based on the BERT Base model.}
%     \label{fig:msb}
% \end{figure*}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/tokens.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime on GPT2 base model with different input length.}
%     \label{fig:token_num}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/lambda.png}
%     \captionsetup{skip=2pt}
%     \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
%     \label{fig:param}
% \end{figure}


% \begin{figure}[h]
%     \centering
%     \begin{minipage}{0.3\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/tokens.pdf}
%         \captionsetup{skip=10pt}
%         \caption{Runtime on GPT2.}
%         \label{fig:token_num}
%     \end{minipage}\hfill
%     \begin{minipage}{0.65\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/ratio.pdf}
%         \captionsetup{skip=2pt}
%         \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
%         \label{fig:param}
%     \end{minipage}
% \end{figure}

 % base model with different input length.