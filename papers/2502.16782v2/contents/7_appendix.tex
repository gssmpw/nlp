\section{Secure Token Pruning Protocols}
\label{app:a}
We detail the encrypted token pruning protocols $\Pi_{prune}$ in Figure \ref{fig:protocol-prune} and $\Pi_{mask}$ in Figure \ref{fig:protocol-mask} in this section.

%Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
%All such materials \textbf{SHOULD be included in the main submission.}
\begin{figure}[h]
%vspace{-0.2in}
\begin{protocolbox}
\noindent
\textbf{Parties:} Server $P_0$, Client $P_1$.

\textbf{Input:} $P_0$ and $P_1$ holds $\{ \left \langle Att \right \rangle_{0}^{h}, \left \langle Att \right \rangle_{1}^{h}\}_{h=0}^{H-1} \in \mathbb{Z}_{2^{\ell}}^{n\times n}$ and $\left \langle x \right \rangle_{0}, \left \langle x \right \rangle_{1} \in \mathbb{Z}_{2^{\ell}}^{n\times D}$ respectively, where H is the number of heads, n is the number of input tokens and D is the embedding dimension of tokens. Additionally, $P_1$ holds a threshold $\theta \in \mathbb{Z}_{2^{\ell}}$.

\textbf{Output:} $P_0$ and $P_1$ get $\left \langle y \right \rangle_{0}, \left \langle y \right \rangle_{1} \in \mathbb{Z}_{2^{\ell}}^{n'\times D}$, respectively, where $y=\mathsf{Prune}(x)$ and $n'$ is the number of remaining tokens.

\noindent\rule{13.2cm}{0.1pt} % This creates the horizontal line
\textbf{Protocol:}
\begin{enumerate}[label=\arabic*:, leftmargin=*]
    \item For $h \in [H]$, $P_0$ and $P_1$ compute locally with input $\left \langle Att \right \rangle^{h}$, and learn the importance score in each head $\left \langle s \right \rangle^{h} \in \mathbb{Z}_{2^{\ell}}^{n} $, where $\left \langle s \right \rangle^{h}[j] = \frac{1}{n} \sum_{i=0}^{n-1} \left \langle Att \right \rangle^{h}[i,j]$.
    \item $P_0$ and $P_1$ compute locally with input $\{ \left \langle s \right \rangle^{i} \in \mathbb{Z}_{2^{\ell}}^{n}  \}_{i=0}^{H-1}$, and learn the final importance score $\left \langle S \right \rangle \in \mathbb{Z}_{2^{\ell}}^{n}$ for each token, where  $\left \langle S \right \rangle[i] = \frac{1}{H} \sum_{h=0}^{H-1} \left \langle s \right \rangle^{h}[i]$.
    \item  For $i \in [n]$, $P_0$ and $P_1$ invoke $\Pi_{CMP}$ with inputs  $\left \langle S \right \rangle$ and $ \theta $, and learn  $\left \langle M \right \rangle \in \mathbb{Z}_{2^{\ell}}^{n}$, such that$\left \langle M \right \rangle[i] = \Pi_{CMP}(\left \langle S \right \rangle[i] - \theta) $, where: \\
    $M[i] = \begin{cases}
        1  &\text{if}\ S[i] > \theta, \\
        0  &\text{otherwise}.
            \end{cases} $
    % \item If the pruning location is insensitive, $P_0$ and $P_1$ learn real mask $M$ instead of shares $\left \langle M \right \rangle$. $P_0$ and $P_1$ compute $\left \langle y \right \rangle$ with input $\left \langle x \right \rangle$ and $M$, where  $\left \langle x \right \rangle[i]$ is pruned if $M[i]$ is $0$.
    \item $P_0$ and $P_1$ invoke $\Pi_{mask}$ with inputs  $\left \langle x \right \rangle$ and pruning mask $\left \langle M \right \rangle$, and set outputs as $\left \langle y \right \rangle$.
\end{enumerate}
\end{protocolbox}
\setlength{\abovecaptionskip}{-1pt} % Reduces space above the caption
\caption{Secure Token Pruning Protocol $\Pi_{prune}$.}
\label{fig:protocol-prune}
\end{figure}




\begin{figure}[h]
\begin{protocolbox}
\noindent
\textbf{Parties:} Server $P_0$, Client $P_1$.

\textbf{Input:} $P_0$ and $P_1$ hold $\left \langle x \right \rangle_{0}, \left \langle x \right \rangle_{1} \in \mathbb{Z}_{2^{\ell}}^{n\times D}$ and  $\left \langle M \right \rangle_{0}, \left \langle M \right \rangle_{1} \in \mathbb{Z}_{2^{\ell}}^{n}$, respectively, where n is the number of input tokens and D is the embedding dimension of tokens.

\textbf{Output:} $P_0$ and $P_1$ get $\left \langle y \right \rangle_{0}, \left \langle y \right \rangle_{1} \in \mathbb{Z}_{2^{\ell}}^{n'\times D}$, respectively, where $y=\mathsf{Prune}(x)$ and $n'$ is the number of remaining tokens.

\noindent\rule{13.2cm}{0.1pt} % This creates the horizontal line
\textbf{Protocol:}
\begin{enumerate}[label=\arabic*:, leftmargin=*]
    \item For $i \in [n]$, $P_0$ and $P_1$ set $\left \langle M \right \rangle$ to the MSB of $\left \langle x \right \rangle$ and learn the masked tokens $\left \langle \Bar{x} \right \rangle \in Z_{2^{\ell}}^{n\times D}$, where
    $\left \langle \Bar{x}[i] \right \rangle = \left \langle x[i] \right \rangle + (\left \langle M[i] \right \rangle << f)$ and $f$ is the fixed-point precision.
    \item $P_0$ and $P_1$ compute the sum of $\{\Pi_{B2A}(\left \langle M \right \rangle[i]) \}_{i=0}^{n-1}$, and learn the number of remaining tokens $n'$ and the number of tokens to be pruned $m$, where $m = n-n'$.
    \item For $k\in[m]$, for $i\in[n-k-1]$, $P_0$ and $P_1$ invoke $\Pi_{msb}$ to learn the highest bit of $\left \langle \Bar{x}[i] \right \rangle$, where $b=\mathsf{MSB}(\Bar{x}[i])$. With the highest bit of $\Bar{x}[i]$, $P_0$ and $P_1$ perform a oblivious swap between $\Bar{x}[i]$ and $\Bar{x}[i+1]$:
    $\begin{cases}
        \Tilde{x}[i] = b\cdot \Bar{x}[i] + (1-b)\cdot \Bar{x}[i+1] \\
        \Tilde{x}[i+1] = b\cdot \Bar{x}[i+1] + (1-b)\cdot \Bar{x}[i]
    \end{cases} $ \\
    $P_0$ and $P_1$ learn the swapped token sequence $\left \langle \Tilde{x} \right \rangle$.
    \item $P_0$ and $P_1$ truncate $\left \langle \Tilde{x} \right \rangle$ locally by keeping the first $n'$ tokens, clear current MSB (all remaining token has $1$ on the MSB), and set outputs as $\left \langle y \right \rangle$.
\end{enumerate}
\end{protocolbox}
\setlength{\abovecaptionskip}{-1pt} % Reduces space above the caption
\caption{Secure Mask Protocol $\Pi_{mask}$.}
\label{fig:protocol-mask}
%\vspace{-0.2in}
\end{figure}

% \begin{wrapfigure}{r}{0.35\textwidth}  % 'r' for right, and the width of the figure area
%   \centering
%   \includegraphics[width=0.35\textwidth]{figures/msb.pdf}
%   \caption{Runtime of $\Pi_{prune}$ and $\Pi_{mask}$ in different layers. We compare different secure pruning strategies based on the BERT Base model.}
%   \label{fig:msb}
%   \vspace{-0.1in}
% \end{wrapfigure}

% \begin{figure}[h]  % 'r' for right, and the width of the figure area
%   \centering
%   \includegraphics[width=0.4\textwidth]{figures/msb.pdf}
%   \caption{Runtime of $\Pi_{prune}$ and $\Pi_{mask}$ in different layers. We compare different secure pruning strategies based on the BERT Base model.}
%   \label{fig:msb}
%   % \vspace{-0.1in}
% \end{figure}

\textbf{Complexity of $\Pi_{mask}$.} The complexity of the proposed $\Pi_{mask}$ mainly depends on the number of oblivious swaps. To prune $m$ tokens out of $n$ input tokens, $O(mn)$ swaps are needed. Since token pruning is performed progressively, only a small number of tokens are pruned at each layer, which makes $\Pi_{mask}$ efficient during runtime. Specifically, for a BERT base model with 128 input tokens, the pruning protocol only takes $\sim0.9$s on average in each layer. An alternative approach is to invoke an oblivious sort algorithm~\citep{bogdanov2014swap2,pang2023bolt} on $\left \langle \Bar{x} \right \rangle$. However, this approach is less efficient because it blindly sort the whole token sequence without considering $m$. That is, even if only $1$ token needs to be pruned, $O(nlog^{2}n)\sim O(n^2)$ oblivious swaps are needed, where as the proposed $\Pi_{mask}$ only need $O(n)$ swaps. More generally, for an $\ell$-layer Transformer with a total of $m$ tokens pruned, the overall time complexity using the sort strategy would be $O(\ell n^2)$ while using the swap strategy remains an overall complexity of $O(mn).$ Specifically, using the sort strategy to prune tokens in one BERT Base model layer can take up to $3.8\sim4.5$ s depending on the sorting algorithm used. In contrast, using the swap strategy only needs $0.5$ s. Moreover, alternative to our MSB strategy, one can also swap the encrypted mask along with the encrypted token sequence. However, we find that this doubles the number of swaps needed, and thus is less efficient the our MSB strategy, as is shown in Figure \ref{fig:msb}.

\section{Existing Protocols}
\label{app:protocol}
\noindent\textbf{Existing Protocols Used in Our Private Inference.}  In our private inference framework, we reuse several existing cryptographic protocols for basic computations. $\Pi_{MatMul}$ \citep{pang2023bolt} processes two ASS matrices and outputs their product in SS form. For non-linear computations, protocols $\Pi_{SoftMax}, \Pi_{GELU}$, and $\Pi_{LayerNorm}$\citep{lu2023bumblebee, pang2023bolt} take a secret shared tensor and return the result of non-linear functions in ASS. Basic protocols from~\citep{rathee2020cryptflow2, rathee2021sirnn} are also utilized. $\Pi_{CMP}$\citep{EzPC}, for example, inputs ASS values and outputs a secret shared comparison result, while $\Pi_{B2A}$\citep{EzPC} converts secret shared Boolean values into their corresponding arithmetic values.

\section{Polynomial Reduction for Non-linear Functions}
\label{app:b}
The $\mathsf{SoftMax}$ and $\mathsf{GELU}$ functions can be approximated with polynomials. High-degree polynomials~\citep{lu2023bumblebee, pang2023bolt} can achieve the same accuracy as the LUT-based methods~\cite{hao2022iron-iron}. While these polynomial approximations are more efficient than look-up tables, they can still incur considerable overheads. Reducing the high-degree polynomials to the low-degree ones for the less important tokens can imporve efficiency without compromising accuracy. The $\mathsf{SoftMax}$ function is applied to each row of an attention map. If a token is to be reduced, the corresponding row will be computed using the low-degree polynomial approximations. Otherwise, the corresponding row will be computed accurately via a high-degree one. That is if $M_{\beta}'[i] = 1$, $P_0$ and $P_1$ uses high-degree polynomials to compute the $\mathsf{SoftMax}$ function on token $x[i]$:
\begin{equation}
\mathsf{SoftMax}_{i}(x) = \frac{e^{x_i}}{\sum_{j\in [d]}e^{x_j}}
\end{equation}
where $x$ is a input vector of length $d$ and the exponential function is computed via a polynomial approximation. For the $\mathsf{SoftMax}$ protocol, we adopt a similar strategy as~\citep{kim2021ibert, hao2022iron-iron}, where we evaluate on the normalized inputs $\mathsf{SoftMax}(x-max_{i\in [d]}x_i)$. Different from~\citep{hao2022iron-iron}, we did not used the binary tree to find max value in the given vector. Instead, we traverse through the vector to find the max value. This is because each attention map is computed independently and the binary tree cannot be re-used. If $M_{\beta}[i] = 0$, $P_0$ and $P_1$ will approximate the $\mathsf{SoftMax}$ function with low-degree polynomial approximations. We detail how $\mathsf{SoftMax}$ can be approximated as follows:
\begin{equation}
\label{eq:app softmax}
\mathsf{ApproxSoftMax}_{i}(x) = \frac{\mathsf{ApproxExp}(x_i)}{\sum_{j\in [d]}\mathsf{ApproxExp}(x_j)}
\end{equation}
\begin{equation}
\mathsf{ApproxExp}(x)=\begin{cases}
    0  &\text{if}\ x \leq T \\
    (1+ \frac{x}{2^n})^{2^n} &\text{if}\ x \in [T,0]\\
\end{cases}
\end{equation}
where the $2^n$-degree Taylor series is used to approximate the exponential function and $T$ is the clipping boundary. The value $n$ and $T$ determines the accuracy of above approximation. With $n=6$ and $T=-13$, the approximation can achieve an average error within $2^{-10}$~\citep{lu2023bumblebee}. For low-degree polynomial approximation, $n=3$ is used in the Taylor series.

Similarly, $P_0$ or $P_1$ can decide whether or not to approximate the $\mathsf{GELU}$ function for each token. If $M_{\beta}[i] = 1$, $P_0$ and $P_1$ use high-degree polynomials~\citep{lu2023bumblebee} to compute the $\mathsf{GELU}$ function on token $x[i]$ with high-degree polynomial:
% \begin{equation}
% \mathsf{GELU}(x) = 0.5x(1+\mathsf{Tanh}(\sqrt{2/\pi}(x+0.044715x^3)))
% \end{equation}
% where the $\mathsf{Tanh}$ and square root function are computed via a OT-based lookup-table.

\begin{equation}
\label{eq:app gelu}
\mathsf{ApproxGELU}(x)=\begin{cases}
    0  &\text{if}\ x \leq -5 \\
    P^3(x), &\text{if}\ -5 < x \leq -1.97 \\
    P^6(x), &\text{if}\ -1.97 < x \leq 3  \\
    x, &\text{if}\ x >3 \\
\end{cases}
\end{equation}
where $P^3(x)$ and $P^6(x)$ are degree-3 and degree-6 polynomials respectively. The detailed coefficient for the polynomial is: 
\begin{equation*}
    P^3(x) = -0.50540312 -  0.42226581x - 0.11807613x^2 - 0.01103413x^3
\end{equation*}
, and
\begin{equation*}
    P^6(x) = 0.00852632 + 0.5x + 0.36032927x^2 - 0.03768820x^4 + 0.00180675x^6
\end{equation*}

For BOLT baseline, we use another high-degree polynomial to compute the $\mathsf{GELU}$ function.

\begin{equation}
\label{eq:app gelu}
\mathsf{ApproxGELU}(x)=\begin{cases}
    0  &\text{if}\ x < -2.7 \\
    P^4(x), &\text{if}\   |x| \leq 2.7 \\
    x, &\text{if}\ x >2.7 \\
\end{cases}
\end{equation}
We use the same coefficients for $P^4(x)$ as BOLT~\citep{pang2023bolt}.

\begin{figure}[h]
 % \vspace{-0.1in}
    \centering
    \includegraphics[width=1\linewidth]{figures/bumble.pdf}
    % \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Comparison with prior works on the BERT model. The input has 128 tokens.}
    \label{fig:bumble}
\end{figure}

If $M_{\beta}'[i] = 0$, $P_0$ and $P_1$ will use low-degree 
polynomial approximation to compute the $\mathsf{GELU}$ function instead. Encrypted polynomial reduction leverages low-degree polynomials to compute non-linear functions for less important tokens. For the $\mathsf{GELU}$ function, the following degree-$2$ polynomial~\cite{kim2021ibert} is used:
\begin{equation*}
\mathsf{ApproxGELU}(x)=\begin{cases}
    0  &\text{if}\ x <  -1.7626 \\
    0.5x+0.28367x^2, &\text{if}\ x \leq |1.7626| \\
    x, &\text{if}\ x > 1.7626\\
\end{cases}
\end{equation*}


\section{Comparison with More Related Works.}
\label{app:c}
\textbf{Other 2PC frameworks.} The primary focus of CipherPrune is to accelerate the private Transformer inference in the 2PC setting. As shown in Figure \ref{fig:bumble}, CipherPrune can be easily extended to other 2PC private inference frameworks like BumbleBee~\citep{lu2023bumblebee}. We compare CipherPrune with BumbleBee and IRON on BERT models. We test the performance in the same LAN setting as BumbleBee with 1 Gbps bandwidth and 0.5 ms of ping time. CipherPrune achieves more than $\sim 60 \times$ speed up over BOLT and $4.3\times$ speed up over BumbleBee.

\begin{figure}[t]
 % \vspace{-0.1in}
    \centering
    \includegraphics[width=1\linewidth]{figures/pumab.pdf}
    % \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Comparison with MPCFormer and PUMA on the BERT models. The input has 128 tokens.}
    \label{fig:pumab}
\end{figure}

\begin{figure}[h]
 % \vspace{-0.1in}
    \centering
    \includegraphics[width=1\linewidth]{figures/pumag.pdf}
    % \captionsetup{skip=2pt}
    % \vspace{-0.1in}
    \caption{Comparison with MPCFormer and PUMA on the GPT2 models. The input has 128 tokens. The polynomial reduction is not used.}
    \label{fig:pumag}
\end{figure}

\textbf{Extension to 3PC frameworks.} Additionally, we highlight that CipherPrune can be also extended to the 3PC frameworks like MPCFormer~\citep{li2022mpcformer} and PUMA~\citep{dong2023puma}. This is because CipherPrune is built upon basic primitives like comparison and Boolean-to-Arithmetic conversion. We compare CipherPrune with MPCFormer and PUMA on both the BERT and GPT2 models. CipherPrune has a $6.6\sim9.4\times$ speed up over MPCFormer and $2.8\sim4.6\times$ speed up over PUMA on the BERT-Large and GPT2-Large models.


\section{Communication Reduction in SoftMax and GELU.}
\label{app:e}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/layerwise.pdf}
    \caption{Toy example of two successive Transformer layers. In layer$_i$, the SoftMax and Prune protocol have $n$ input tokens. The number of input tokens is reduced to $n'$ for the Linear layers, LayerNorm and GELU in layer$_i$ and SoftMax in layer$_{i+1}$.}
    \label{fig:layer}
\end{figure}

\begin{table*}[h]
\captionsetup{skip=2pt}
\centering
\scriptsize
\caption{Communication cost (in MB) of the SoftMax and GELU protocol in each Transformer layer.}
\begin{tblr}{
    colspec = {c |c c c c c c c c c c c c},
    row{1} = {font=\bfseries},
    row{2-Z} = {rowsep=1pt},
    % row{4} = {bg=LightBlue},
    colsep = 2.5pt,
    }
\hline
\textbf{Layer Index} & \textbf{0}  & \textbf{1}  & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} & \textbf{11} \\
\hline
Softmax & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 & 642.19 \\
Pruned Softmax & 642.19 & 129.58 & 127.89 & 119.73 & 97.04 & 71.52 & 43.92 & 21.50 & 10.67 & 6.16 & 4.65 & 4.03 \\
\hline
GELU & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84 & 698.84\\
Pruned GELU  & 325.10 & 317.18 & 313.43 & 275.94 & 236.95 & 191.96 & 135.02 & 88.34 & 46.68 & 16.50 & 5.58 & 5.58\\
\hline
\end{tblr}
\label{tab:layer}
\end{table*}

{
In Figure \ref{fig:layer}, we illustrate why CipherPrune can reduce the communication overhead of both  SoftMax and GELU. Suppose there are $n$ tokens in $layer_i$. Then, the SoftMax protocol in the attention module has a complexity of $O(n^2)$. CipherPrune's token pruning protocol is invoked to select $n'$ tokens out of all $n$ tokens, where $m=n-n'$ is the number of tokens that are removed. The overhead of the GELU function in $layer_i$, i.e., the current layer, has only $O(n')$ complexity (which should be $O(n)$ without token pruning). The complexity of the SoftMax function in $layer_{i+1}$, i.e., the following layer, is reduced to $O(n'^2)$ (which should be $O(n^2)$ without token pruning). The SoftMax protocol has quadratic complexity with respect to the token number and the GELU protocol has linear complexity. Therefore, CipherPrune can reduce the overhead of both the GELU protocol and the SoftMax protocols by reducing the number of tokens. In Table \ref{tab:layer}, we provide detailed layer-wise communication cost of the GELU and the SoftMax protocol. Compared to the unpruned baseline, CipherPrune can effectively reduce the overhead of the GELU and the SoftMax protocols layer by layer.
}

\section{Analysis on Layer-wise redundancy.}
\label{app:f}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/layertime0.pdf}
    \caption{The number of pruned tokens and pruning protocol runtime in different layers in the BERT Base model. The results are averaged across 128 QNLI samples.}
    \label{fig:layertime}
\end{figure}

{
In Figure \ref{fig:layertime}, we present the number of pruned tokens and the runtime of the pruning protocol for each layer in the BERT Base model. The number of pruned tokens per layer was averaged across 128 QNLI samples, while the pruning protocol runtime was measured over 10 independent runs. The mean token count for the QNLI samples is 48.5. During inference with BERT Base, input sequences with fewer tokens are padded to 128 tokens using padding tokens. Consistent with prior token pruning methods in plaintext~\citep{goyal2020power}, a significant number of padding tokens are removed at layer 0.  At layer 0, the number of pruned tokens is primarily influenced by the number of padding tokens rather than token-level redundancy.
%In Figure \ref{fig:layertime}, we demonstrate the number of pruned tokens and the pruning protocol runtime in each layer in the BERT Base model. We averaged the number of pruned tokens in each layer across 128 QNLI samples and then tested the pruning protocol runtime in 10 independent runs. The mean token number of the QNLI samples is 48.5. During inference with BERT Base, input sequences with small token number are padded to 128 tokens with padding tokens. Similar to prior token pruning methods in the plaintext~\citep{goyal2020power}, a large number of padding tokens can be removed at layer 0. We remark that token-level redundancy builds progressively throughout inference~\citep{goyal2020power, kim2022LTP}. The number of pruned tokens in layer 0 mostly depends on the number of padding tokens instead of token-level redundancy.
}

{
%As shown in Figure \ref{fig:layertime}, more tokens are removed in the intermediate layers, e.g., layer $4$ to layer $7$. This suggests there is more redundant information in these intermediate layers. 
In CipherPrune, tokens are removed progressively, and once removed, they are excluded from computations in subsequent layers. Consequently, token pruning in earlier layers affects computations in later layers, whereas token pruning in later layers does not impact earlier layers. As a result, even if layers 4 and 7 remove the same number of tokens, layer 7 processes fewer tokens overall, as illustrated in Figure \ref{fig:layertime}. Specifically, 8 tokens are removed in both layer $4$ and layer $7$, but the runtime of the pruning protocol in layer $4$ is $\sim2.4\times$ longer than that in  layer $7$.
}

\section{Related Works}
\label{app:g}

{
In response to the success of Transformers and the need to safeguard data privacy, various private Transformer Inferences~\citep{chen2022thex,zheng2023primer,hao2022iron-iron,li2022mpcformer, lu2023bumblebee, luo2024secformer, pang2023bolt}  are proposed. To efficiently run private Transformer inferences, multiple cryptographic primitives are used in a popular hybrid HE/MPC method IRON~\citep{hao2022iron-iron}, i.e., in a Transformer, HE and SS are used for linear layers, and SS and OT are adopted for nonlinear layers. IRON and BumbleBee~\citep{lu2023bumblebee} focus on optimizing linear general matrix multiplications; SecFormer~\cite{luo2024secformer} improves the non-linear operations like the exponential function with polynomial approximation; BOLT~\citep{pang2023bolt} introduces the baby-step giant-step (BSGS) algorithm to reduce the number of HE rotations, proposes a word elimination (W.E.) technique, and uses polynomial approximation for non-linear operations, ultimately achieving state-of-the-art (SOTA) performance.
}

{Other than above hybrid HE/MPC methods, there are also works exploring privacy-preserving Transformer inference using only HE~\citep{zimerman2023converting, zhang2024nonin}. The first HE-based private Transformer inference work~\citep{zimerman2023converting} replaces \mysoftmax function with a scaled-ReLU function. Since the scaled-ReLU function can be approximated with low-degree polynomials more easily, it can be computed more efficiently using only HE operations. A range-loss term is needed during training to reduce the polynomial degree while maintaining high accuracy. A training-free HE-based private Transformer inference was proposed~\citep{zhang2024nonin}, where non-linear operations are approximated by high-degree polynomials. The HE-based methods need frequent bootstrapping, especially when using high-degree polynomials, thus often incurring higher overhead than the hybrid HE/MPC methods in practice.
}
