%\noindent\textbf{Attention-based Transformers}. Attention mechanisms underpin the superior performance of Transformers, which can effectively capture long-range dependencies in the input token sequence and model contextual information. The Transformer~\citep{devlin2018bert} consists of multiple encoder and decoder layers, both of which share the similar structure. A basic transformer encoder layer consists of two major computation blocks: a Multi-head Self-attention (MHA) module and a Feed Forward (FFN) module, each with residual connections around and followed by a Layer Normalization (LN) module. Given an input token sequence $x \in \mathbb{R}^{n \times D}$, where $n$ is the number of tokens and $D$ is the token embedding dimension, the attention is computed as: $Attention(Q,K,V) = SoftMax(QK^{T}/\sqrt{d_h})V$.
%The matrices $Q, K, V \in \mathbb{R}^{n \times d_h}$ are computed by multiplying input $x$ with three weight matrices $W_q, W_k, W_v \in \mathbb{R}^{D \times d_h}$, where $d_h$ is the head dimension.

%\begin{equation}
%Attention(Q,K,V) = SoftMax(QK^{T}/\sqrt{d_h})V
%\end{equation}

\noindent\textbf{Token Pruning in Transformers}. To reduce the computational overhead of plaintext Transformers, a range of strategies including efficient architecture design, knowledge distillation, quantization, and both model and token pruning have been developed. Among these, token pruning~\citep{goyal2020power,kim2020lat,wang2021spatten} stands out for its ability to dynamically reduce token complexity, enhancing efficiency for scalable input lengths. Online token pruning progressively eliminates nonessential input tokens during inference, with recent techniques like learning-based token pruning assigning tunable thresholds to each Transformer layer. These thresholds are fine-tuned during training to maximize the removal of tokens from the sequence. %while preserving the accuracy intact.


%model and token pruning emerge as a particularly potent approach for accelerating plaintext Transformer inferences. In pursuit of structured pruning, which can be practically used by cryptographic protocols, model pruning necessitates the removal of specific attention heads~\citep{fan2019weight1} or entire Transformer layers~\citep{michel2019weight2}, albeit at the cost of nontrivial accuracy degradation. In contrast, token pruning~\citep{goyal2020power,kim2020lat,wang2021spatten} is introduced to facilitate structured pruning by progressively eliminating nonessential tokens from the input sequence during inference. The latest token pruning technique~\citep{kim2022LTP} adopts a learning-based approach, where a tunable threshold is assigned to each Transformer layer. This threshold is fine-tuned during training to selectively remove as many tokens from the sequence as possible while preserving the accuracy intact.

% \begin{equation}
% \footnotesize
% \label{e:score}
% S[i] = \frac{1}{H} \frac{1}{n} \sum_{h=0}^{H-1} \sum_{j=0}^{n-1} Att^{h}[j,i]
% \end{equation}
\setcounter{equation}{0}
Given the attention map $Att = SoftMax(QK^{T} / \sqrt{d}) \in \mathbb{R}^{n \times n}$, $QK^T$ computes the dot product between the query and key vectors, resulting in an  $n \times n$ matrix. Dividing by $\sqrt{d}$ scales down the values to avoid excessively large dot products, especially when the dimensionality d  is large. The importance score $S \in \mathbb{R}^{n}$ of $n$ tokens $x_i \in \mathbb{R}^{D}$ in the input sequence can be computed as: $S[i] = \frac{1}{H} \frac{1}{n} \sum_{h=0}^{H-1} \sum_{j=0}^{n-1} Att^{h}[j,i]$ \refstepcounter{equation}\label{e:score}~(\theequation) 
where $H$ is the number of attention heads and $Att^{h}$ is the attention map in the $h$-th head. Importance score $S$ is essentially computed by accumulating attention scores vertically, which indicates the importance of a token across all heads in one Transformer layer.

\noindent\textbf{Cryptographic Primitives}. Our protocol uses multiple cryptographic primitives including Additive Secret Sharing (ASS), Homomorphic Encryption (HE), and Oblivious Transfer (OT). We reuse partial existing protocols detailed in Appendix \ref{app:protocol}.
\begin{itemize}[leftmargin=*, nosep, topsep=0pt, partopsep=0pt]

\item \textbf{ASS}. We employ a 2-out-of-2 ASS scheme~\citep{cramer2015ass} operating over the ring $\mathbb{Z}_{\ell}$, where $\ell$ is the bitwidth of the input $x$. ASS partitions $x$ into two distinct random shares $\left \langle x \right \rangle_{0}, \left \langle x \right \rangle_{1}$, where $x = \left \langle x \right \rangle_{0} + \left \langle x \right \rangle_{1}$ mod $\mathbb{Z}_{\ell}$. The parties, $P_0$ and $P_1$, respectively hold $\left \langle x \right \rangle_{0}, \left \langle x \right \rangle_{1}$. Importantly, it is guaranteed that neither $P_0$ nor $P_1$ can discern the actual value of $x$~\citep{cramer2015ass}. ASS lends itself to linear operations, i.e., addition and constant multiplication, without communications.
% homomorphic
\item \textbf{HE}. We leverage the BFV scheme~\citep{brakerski2012fv, fan2012fv,deng2024trinity}, a leveled HE scheme, to facilitate linear operations on ciphertexts. The HE scheme has 4 functions: \texttt{KeyGen}, \texttt{Enc}, \texttt{Dec}, and \texttt{Eval}. \texttt{KeyGen} generates a public key $pk$ and a secret key $sk$. \texttt{Enc} encrypts a message $m$ with the public key $pk$ to yield a ciphertext $c$. \texttt{Dec}, with the secret key $sk$ and ciphertext $c$ as inputs, decrypts the ciphertext to recover the message $m$. Finally, \texttt{Eval}, when given the public key $pk$, two ciphertexts $c_1$ and $c_2$ encrypting messages $m_1$ and $m_2$, along with a linear function $\mathcal{F}$, produces a new ciphertext $c'$ encrypting the result of $\mathcal{F}(m_1, m_2)$. 

% We denote homomorphic addition, subtraction, and multiplication between ciphertexts and plaintexts as $\boxplus$, $\boxminus$, and $\boxtimes$, respectively.

\item \textbf{OT}. We use OT for non-linear operations~\citep{rathee2020cryptflow2, rathee2021sirnn, xue2023cryptotrain} in a network model. Specifically, we employ 1-out-of-2 correlated OT~\citep{asharov2013ot2} (2-COT$_{\ell}$) and 1-out-of-$k$~\citep{kolesnikov2013otk} ($k$-OT$_{\ell}$). In 2-COT$_{\ell}$, the protocol takes as inputs the sender's correlation $x \in \mathbb{Z}_{\ell}$ and receiver's bit choice $i \in \{0,1\}$. It then produces a random element $r \in \mathbb{Z}_{\ell}$ for the sender and $r + i \cdot x$ for the receiver. In $k$-OT$_{\ell}$, the sender possesses $k$ messages $m_0,...m_{k-1}$ and the receiver holds an index $i \in [k]$. The protocol ensures the receiver learns $x_i$ as the output without learning any information about $x_j$, where $j\in [k]$ and $j\ne i$, while the sender learns nothing about the receiver's choice $i$. 
\end{itemize}


%Several prior cryptographic protocols for basic computation are resued in our private inference. $\Pi_{MatMul}$ ~\citep{huang2022cheetah, hao2022iron-iron} takes as input two secret shared matrices and output their multiplication result in secret sharing. Protocols for non-linear computation $\Pi_{SoftMax}, \Pi_{GELU}$ and $\Pi_{LayerNorm}$~\citep{rathee2021sirnn, hao2022iron-iron} takes as input a secret shared tensor and output the corresponding result of non-linear functions in secret sharing.  We also utilize basic protocols provided by~\citep{rathee2020cryptflow2, rathee2021sirnn}. $\Pi_{CMP}$~\citep{EzPC} takes as input secret shared values and returns the comparison result in secret sharing.  $\Pi_{B2A}$~\citep{EzPC} takes as input secret shared Boolean values and returns the corresponding arithmetic values in secret sharing.
% \Qian{This paragph may need update since you changed the baseline from IRON to BOLT. This part can be summarized and then put in the appendix for space saving.}
% $\Pi_{MUX}$ is a secure multiplexer that selects from two secret shared values.

\noindent\textbf{Prior Private Transformer Inference}. In response to the success of Transformers and the need to safeguard data privacy, various private Transformer Inferences~\citep{chen2022thex,zheng2023primer,hao2022iron-iron,li2022mpcformer, lu2023bumblebee, hou2023ciphergpt, luo2024secformer, pang2023bolt}  are proposed. To efficiently run private Transformer inferences, multiple cryptographic primitives are used in a popular hybrid HE/MPC method IRON~\citep{hao2022iron-iron}, i.e., in a Transformer, HE and SS are used for linear layers, and SS and OT are adopted for nonlinear layers. IRON and BumbleBee~\citep{lu2023bumblebee} focus on optimizing linear general matrix multiplications; {SecFormer~\citep{luo2024secformer} improves non-linear operations, such as the exponential function, through polynomial approximation.} BOLT~\citep{pang2023bolt} introduces the baby-step giant-step (BSGS) algorithm to reduce the number of HE rotations, proposes a word elimination (W.E.) technique, and uses polynomial approximation for non-linear operations, ultimately achieving state-of-the-art (SOTA) performance. Itâ€™s worth noting that the W.E. technique in BOLT is not input-specific, as it uniformly prunes half the tokens regardless of the input or task. This approach may fail to remove all redundancy when it exceeds half of the tokens and can harm accuracy when redundancy is less than half. Additionally, since W.E. performs one-time pruning at the first layer rather than progressive, layer-by-layer pruning, it is less effective at reducing tokens for longer inputs. Moreover, BOLT's W.E. protocol is computationally expensive due to its reliance on sorting, whereas our method achieves lower asymptotic complexity and faster concrete runtime. Specifically, the state-of-the-art BOLT still faces efficiency and scalability challenges with long-token inputs. For example, one private inference with a GPT2-Base model can take $\sim10$ minutes for 128-token inputs and $\sim1$ hour for 512-token inputs, requiring data exchanges of more than 60GB and 200GB, respectively. {Besides the hybrid HE/MPC methods, a line of works has explored performing private Transformer inference with only HE~\citep{zimerman2023converting, zhang2024nonin}}. {We leave a more detailed review of related works in Appendix \ref{app:g}.}
% \Qian{Please correct these numbers.} %Moreover, in the latest 2PC-based cryptographic protocol~\citep{hao2022iron-iron} for a private Transformer inference, non-linear operations including layer normalization, SoftMax and GELU have become the largest latency bottleneck, i.e., 65\% of the private Transformer inference latency is consumed by these operations. 

%\qian{Add the new SOTA, BOLT, and its follow-up works. }


% The latest cryptographic protocol~\citep{pang2023bolt} for private Transformer inferences suffers from significant computational and communication overhead, especially with the increased input token numbers. As Figure~\ref{fig:motivation}(a) shows, non-linear operations such as \mysoftmax, \gelu, and layer normalization are the largest latency bottlenecks in a private BERT Transformer. We observe that tokens contribute differently to the accuracy of the final inference. As Figure \ref{fig:motivation}(b) shows, tokens with higher attention scores have a larger impact on the inference accuracy. As a result, selectively pruning unimportant tokens in the sequence does not hurt the Transformer accuracy, as shown in Figure~\ref{fig:motivation}(c). These results motivate us to transplant token pruning to private Transformer inferences. 

% the motivation should mainly mention BOLT
% \noindent\textbf{Motivation}. \update{The latest private Transformer inference framework~\citep{pang2023bolt} still suffers from significant computational and communication overhead, especially with the increased input token numbers. As shown in Figure~\ref{fig:motivation} (a), the cryptographic protocols have a quadratic complexity to the number of input tokens. It is observed that different tokens contribute to the accuracy of the final inference differently. As a result, selectively pruning unimportant tokens in the sequence does not hurt the Transformer accuracy. As shown in Figure~\ref{fig:motivation} (b), the word elimination technique in~\citep{pang2023bolt} was proposed in this sense, which cuts the token sequence by half to reduce the computational and communication overhead. However, such a static strategy can lead to sub-optimal performance. On the one hand, the word elimination is not adaptive to input. Setting a static pruning ratio of $0.5$ for all inputs can over-prune some inputs, leading to worse accuracy, and under-prune others, resulting in inferior efficiency. On the other hand, the one time pruning in word elimination cannot reduce redundant information among tokens during inference, because such information is built progressively through Transformer layers~\citep{goyal2020power}. As shown in Figure~\ref{fig:motivation} (b), the adaptive and progressive token pruning can achieve better efficiency without compromising the accuracy. This motivates us to explore the possibilities of advanced token pruning in private Transformer inference. (The challenges are not well articulated, the motivation for approximation is not mentioned...)}

%However, it is challenging to na\"ively adopt prior token pruning techniques in private Transformer inferences. Firstly, to maintain the original accuracy, the latest token pruning technique learns a threshold for each Transformer layer to \delete{prune less tokens in the first several layers} of a plaintext Transformer, resulting in a moderate speedup on a private Transformer inference. Secondly, if the plaintext pruning mask defining which tokens should be removed is revealed to either party, sensitive information, e.g., which parts in the sentence are not important, can be inferred. However, no prior cryptographic protocol supports performing pruning operations with an encrypted pruning mask.
%\qian{Highlight BOLT instead of IRON; Illustrate the issues of BOLT, which should be solved by the proposed methods. This needs a deep understanding of the differences between BOLT and the proposed work. }




% \begin{figure*}
% %\vspace{-0.1in}
%     \centering    \includegraphics[width=1\linewidth]{figures/motivation.png}
%     \captionsetup{skip=2pt}
%     \vspace{-0.1in}
%     \caption{\update{(a) The runtime of cryptographic protocols grows quadratically with the token sequence length. (b) Comparison between token pruning and word elimination in sentiment analysis. Fig (a) should be revised, current no enough information }}
%     \label{fig:motivation}
% \vspace{-0.1in}
% \end{figure*} 

%\qian{Figure (a) is not correct for BOLT, where non-linear is not a bottleneck. To solve this problem, you don't need to show the breakdown of linear and non-linear; instead, only show the overall runtime.}

% Adding a clear and crisp description of the threat model in this paper and explaining how it differs from the threat models used in prior papers on the topic would be needed before the paper can meet the bar for publication at NeurIPS.

\noindent\textbf{Threat Model and Security Guarantee}. CipherPrune operates in a common private inference scenario where server $P_0$ owns a proprietary Transformer-based model $\mathcal{M}$ with private weights $w$, and client $P_1$ possesses private input data $x$. We assume the server and client are semi-honest, i.e., the server and client follow the designed protocols but are curious and attempt to learn extra information (e.g., $x$ or $w$). This setting is practical, as the server is incentivized to follow protocols and provide high-quality services for monetary gain, while the client is motivated to adhere to the protocol to receive those services. Consequently, this semi-honest setting is commonly adopted in existing works~\citep{rathee2020cryptflow2, huang2022cheetah, hao2022iron-iron, lu2023bumblebee, pang2023bolt}. In this semi-honest setting, our protocols prevent the server from learning the client's data and the inference result; meanwhile, these protocols also block the client from accessing the model's parameters. In our protocols, we assume that both the server and client are aware of the number of pruned tokens. We argue that this information does not compromise the client's data or inference results, nor does it enable the client to access the model's weight parameters. Attacks that deviate from the semi-honest setting are beyond the scope of this work.  %Here, model architecture is not protected 




%where the adversary can passively corrupt either $P_0$ or $P_1$, following protocol specifications but attempting to learn extra information (e.g., $x$ or $w$). Our protocol ensures that only $P_1$ learns the model's output $\mathcal{M}(x,w)$ without revealing $x$ to $P_0$ or $w$ to $P_1$. Similar to previous works, it also allows $P_1$ to learn some architectural details of the Transformer. \update{We note that the number of tokens in each layer is revealed to both $P_0$ and $P_1$. Yet, both $P_0$ and $P_1$ should not learn which tokens are pruned to mitigate the risk of token importance-based adversarial attacks~\citep{wang2021importance1, shakeel2022importance2}. Similar to prior works~\citep{hao2022iron-iron, lu2023bumblebee, pang2023bolt}, other attacks like the side channel attacks and inference attacks are beyond the scope of this work.}

%\qian{Here, explain what values are visible to servers and clients. If the proposed methods introduce the server's or clients visibility, it is necessary to justify it. } 

% like layer count and token number per layer. Additionally, during private token pruning, we protect the sensitive locations of tokens to mitigate the risk of token importance-based adversarial attacks~\citep{wang2021importance1, shakeel2022importance2}.