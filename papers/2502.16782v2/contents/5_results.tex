%We conducted comprehensive experiments to evaluate the proposed CipherPrune's performance. Specifically, our experiments demonstrate notable runtime latency and communication savings over existing methods~\citep{hao2022iron-iron, pang2023bolt, lu2023bumblebee}, with only marginal degradation across various tasks.

% Bumblebee use differen OT, directly comparison. (change all base to ezpc???)
\begin{table*}[h]
\captionsetup{skip=2pt}
\centering
\scriptsize
\caption{End-to-end comparison of CipherPrune with prior works on BERT models. Time is in seconds. Comm. stands for communication in GB and Acc. for accuracy in percentage. }
\begin{tblr}{
    colspec = {c |c c c | c c c | c c c},
    row{1} = {font=\bfseries},
    row{2-Z} = {rowsep=1pt},
    % row{4} = {bg=LightBlue},
    colsep = 2.5pt,
    }
\hline
\SetCell[r=2]{c}\textbf{Method}  &\SetCell[c=3]{c}\textbf{BERT Medium} &&&\SetCell[c=3]{c}\textbf{BERT Base} &&&\SetCell[c=3]{c}\textbf{BERT Large}
\\& \textbf{Time} & \textbf{Comm.} & \textbf{Acc.}
&\textbf{Time} & \textbf{Comm.} & \textbf{Acc.} &\textbf{Time} & \textbf{Comm.} & \textbf{Acc.}\\
\hline
IRON~\citep{hao2022iron-iron} &442.4 &124.5 &87.7$_{\pm 0.2}$ &1087.8 &281.0 &90.4$_{\pm 0.1}$ &2873.5 &744.8 &92.7$_{\pm 0.1}$\\
BumbleBee~\citep{lu2023bumblebee} & & &87.5$_{\pm 0.5}$ & & &90.2$_{\pm 0.2}$ & & &92.7$_{\pm 0.1}$\\
BOLT w/o W.E.~\citep{pang2023bolt} &197.1 &27.9 &87.4$_{\pm 0.3}$ &484.5 &59.6 &90.3$_{\pm 0.1}$ &1279.8 &142.6 &92.6$_{\pm 0.2}$\\
BOLT~\citep{pang2023bolt} &99.5 &14.3 &87.2$_{\pm 0.3}$ &245.4 &25.7 &89.9$_{\pm 0.3}$ &624.3 &67.9 &92.4$_{\pm 0.2}$\\
\hline
CipherPrune &43.6 &6.7 &87.4$_{\pm 0.2}$ &79.1 &9.7 &90.1$_{\pm 0.2}$ &157.6 &18.4 &92.5$_{\pm 0.1}$\\
\hline
\end{tblr}
\label{tab:end}
\end{table*}
\qian{The first column in the table added the citations; then we need to use the same numbers reported in the paper.  }

% This improvement is due to CipherPrune's token pruning strategy, excluding pruned tokens from calculations in subsequent layers, benefiting Transformers with more layers.

\noindent\textbf{End-to-end performance.}
In Table \ref{tab:end}, we evaluate CipherPrune on three BERT models, comparing it with previous 2PC-based private frameworks: IRON~\citep{hao2022iron-iron}, BumbleBee~\citep{lu2023bumblebee} and BOLT~\citep{pang2023bolt}. IRON uses an OT-based look-up table (LUT) for non-linear functions, and BOLT and BumbleBee use polynomial approximations. CipherPrune achieves up to $\sim18.2\times$ speedup over IRON on the BERT-Large model and $\sim8.1\times$ speedup over vanilla BOLT without its word elimination technique. When compared with BOLT with the word elimination technique, CipherPrune is still $\sim3.9\times$ faster without compromising accuracy. CipherPrune's improvement in efficiency scales with the model size.  Specifically, CipherPrune shows $\sim4.5\times$ speedup over vanilla BOLT on BERT-Medium, and $\sim6.1\times$ and $\sim 8.1 \times$ speedup on BERT-Base and BERT-Large, respectively. Communication costs are also reduced by $2.3\sim40.4\times$ compared to prior works. The improvement of BOLT over IRON mainly results from reducing the number of tokens with word elimination and replacing heavy LUT with polynomial approximations. Compared with BOLT, CipherPrune's adaptive and progressive pruning strategy allows removing more redundant tokens during inference, thus reducing the computation and communication overhead. Simultaneously, CipherPrune also 

% While BOLT and BumbleBee also reduce computation and communication overhead, they replace all $\mathsf{GELU}$ and $\mathsf{SoftMax}$ functions with polynomial approximations, leading to notable accuracy loss. CipherPrune approximates only part of these functions, maintaining stable performance with less than $1\%$ accuracy loss.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.66\textwidth}
        \centering
        \scriptsize
        % Using captionof to specify that this is a table caption
        \captionof{table}{Accuracy and time comparisons of different methods. CipherPrune\dag  stands for CipherPrune with only token pruning. }
        \begin{tblr}{
            colspec = {c |c c c c | c },
            row{1} = {font=\bfseries},
            row{2-Z} = {rowsep=1pt},
        }
        \hline
        \SetCell[r=2]{c}\textbf{Method} & \SetCell[c=4]{c}\textbf{Accuracy Metric on Tasks (\%)} &&&& \SetCell[r=2]{c}\textbf{Time(Sec)}  \\
        & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST2} & \textbf{MPRC} \\
        \hline
        % IRON~\citep{hao2022iron-iron} &84.75 &90.87 &92.74 &84.55 &1897.8\\
        BOLT w/o W.E. &84.75 &90.32 &91.74 &90.53 &484.5\\
        BOLT &84.71 &89.94 &92.74 &89.95 &245.4\\
        CipherPrune\dag & 84.74 & 90.17 & 92.75 & 90.45 &115.3\\
        \hline
        CipherPrune &84.68 &90.11 &92.66 &90.18 &79.1\\
        \hline
        \end{tblr}
        \label{tab:prune_acc}
    \end{minipage}\hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/tokens.pdf}
        \caption{Runtime on GPT2.}
        \label{fig:token_num}
    \end{minipage}
\end{figure}
\qian{This table needs to change IRON with BOLT; Keep Prune-only, delete Prune and Fully Approx; Keep Cipherprune.} 

\qian{This figure needs to add BOLT.  Since scalability is an advantage of the proposed work over BOLT.} 

% \begin{table*}[h]
% \captionsetup{skip=2pt}
% \centering
% \small
% \caption{Comparison of accuracy on different benchmarks.}
% \begin{tblr}{
%     colspec = {c |c c c c | c },
%     row{1} = {font=\bfseries},
%     row{2-Z} = {rowsep=1pt},
%     % colsep = 4pt,
%     }
% \hline
% \SetCell[r=2]{c}\textbf{Method} & \SetCell[c=4]{c}\textbf{Accuracy Metric on Tasks (\%)} &&&& \SetCell[r=2]{c}\textbf{Time(Sec)}  \\
% & \textbf{MNLI} & \textbf{QNLI} & \textbf{SST2} & \textbf{MPRC} \\
% \hline
% Fixed-Point Baseline &84.75 &90.87 &92.74 &84.55 &1874.8\\
% Prune& 83.94 & 90.39 & 92.38 & 83.99 &468.6\\
% % Approximation &81.1 &87.6 &88.3 &80.8 &344.1\\
% Prune and Approx. &80.7 &87.2 &87.9 &80.1 &92.1\\
% \hline
% CipherPrune &83.9 &90.3 &92.3 &83.8 &119.2\\
% \hline
% \end{tblr}
% \label{tab:prune_acc}
% \end{table*}


\noindent\textbf{Crypto-aware fine-tuning.}
%In Table \ref{tab:prune_acc}, we present the accuracy of BERT base model across four GLUE task. We construct a fixed-point baseline based on IRON and study the impact of the proposed methods. The baseline uses a fixed-point encoding to represent the floating-point numbers in plaintext models and leverages the lookup-table to compute non-linear function accurately. This makes the baseline's accuracy matches with the plaintext model. 
Table \ref{tab:prune_acc} studies the the effects of crypto-aware fine-tuning. 
By introducing secure token pruning to our baseline IRON, we find that the \textit{prune-only} method results in marginal accuracy degradation within $1\%$, while latency is improved by $\sim4\times$. Fully approximating the retained tokens with low-degree polynomials can further improve runtime by $\sim 5\times$, but causes more than a $3\%$ accuracy drop across all tasks. CipherPrune's dynamic approximation and crypto-aware finetuning mitigate the accuracy drop while maintaining runtime efficiency. CipherPrune selectively approximates less important tokens and computes others accurately via the lookup table. As a result, CipherPrune achieves a stable performance with less than $1\%$ accuracy drop and a $\sim 16 \times$ runtime improvement compared to the baseline IRON.

%Moreover, by replacing partial non-linear functions with their polynomial approximations in the finetuning phase, CipherPrune enables the Transformer to adapt better to the polynomial activation functions during inference time. 
%CipherPrune maintains a stable performance, with only less than $1\%$ drop in model accuracy, while improving the runtime by $\sim 16 \times$ compared with the baseline. %CipherPrune can effectively work with both LUT-based protocols and approximation-based protols. This compatibility highlights CipherPrune as a versatile approach that can be incorporated into existing backend frameworks. This demonstrates the efficacy of the proposed CipherPrune strategy. 

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/breakdown.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime breakdown on BERT-Base model.}
%     \label{fig:breakdown}
% \end{figure*}

\noindent\textbf{Scalability with the input length.} 
In Figure \ref{fig:token_num}, we illustrate the runtime of CipherPrune with varying input token numbers, comparing it to the prior method, BumbleBee, on GPT-2. CipherPrune shows more significant runtime savings as input length increases. With 32 input tokens, CipherPrune achieves a $\sim 1.9\times$ speedup. When the input length reaches 512 tokens, CipherPrune is $\sim6\times$ faster than the baseline. This improvement is due to the quadratic complexity in BumbleBee, which becomes a performance bottleneck as input tokens increase. In contrast, CipherPrune effectively reduces the number of tokens during inference, boosting efficiency for longer token sequences.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.75\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/breakdown.pdf}
        \captionsetup{skip=2pt}
        \caption{Runtime breakdown on BERT-Base model.}
        \label{fig:breakdown}
    \end{minipage}\hfill
    \begin{minipage}{0.25\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{figures/msb.pdf}
      \caption{Runtime comparison of different pruning protocols.}
      \label{fig:msb}
      % \vspace{-0.1in}
    \end{minipage}
\end{figure}
\qian{Although this figure shows that prune overhead is marginal to the whole process, there is no place to show the concrete runtime comparison between BOLT's prune and the proposed method. This is essential with the complexity analysis (BOLT: $O(nlogn)$; ours: $O(n)$) }

%\subsection{Overhead and Breakdown}
\noindent\textbf{Runtime breakdown.}
In Figure \ref{fig:breakdown}, we break down the runtime for each protocol in the BERT-Base model with 128 input tokens. The SoftMax and GELU protocols, $\Pi_{SoftMax}$ and $\Pi_{GELU}$, significantly contribute to the total runtime. This is due to SoftMax's $O(n^2)$ complexity relative to the input length and GELU's involvement in multiple multiplication and truncation operations on high-dimensional matrices. Figure \ref{fig:breakdown} also shows the overhead of the proposed pruning protocol $\Pi_{prune}$ and $\Pi_{mask}$ in CipherPrune compared to other protocols for linear and non-linear computations. The pruning protocol demonstrates remarkable efficiency due to $\Pi_{prune}$ leveraging ASS to offload substantial computation to the local side, such as accumulating the importance score.

\noindent\textbf{Analysis on different pruning protocols.}
As shown in Figure \ref{fig:msb}, ...
During the online phase, the client and server only need to perform $O(n)$ OT-based comparisons to determine the mask and $O(mn)$ oblivious swaps to sort and prune less important tokens, resulting in minimal overhead. The savings from encrypted token pruning outweigh the overhead of invoking $\Pi_{prune}$. Specifically, in the BERT-Base model, the average runtime for $\Pi_{prune}$ in each layer is only 0.9 seconds, while non-linear functions typically take $50\sim100$ seconds in IRON using
\begin{wrapfigure}{r}{0.35\textwidth}  % 'r' for right, and the width of the figure area
    \centering
    \includegraphics[width=1\linewidth]{figures/ratio.pdf}
    \captionsetup{skip=2pt}
    \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
    \label{fig:param}
    \vspace{-0.2in}
\end{wrapfigure}
a lookup table and $\sim10$ seconds in BumbleBee using polynomial approximation. Thus, the runtime of $\Pi_{prune}$ is $1\sim2$ orders of magnitude smaller than other heavy non-linear protocols.

\noindent\textbf{Comparison of different pruning strategy.} Show that adaptive pruning leads to better accuracy (compared with word elimination), especially in tasks like machine translation (generation task).  

\noindent\textbf{Study on the pruning parameters.} In Figure \ref{fig:param}, we show the accuracy-latency trade-off for the BERT-Base model under different parameter settings. Larger 
$\lambda$ and $\alpha$ result in more tokens being pruned or approximated. With $\lambda$ less than 0.05, an appropriate ratio of tokens is pruned, maintaining a stable accuracy of around 90.5\%. However, if $\lambda$ is too large, excessive pruning leads to a notable accuracy drop. In the bottom figure, fixing $\lambda$ at 0.05 and varying $\alpha$, we see that smaller $\alpha$ values approximate fewer tokens, computing activation functions using LUT, which increases accuracy but also latency. Notably, accuracy with a large $\alpha$ is higher than with a large $\lambda$, as many tokens are approximated rather than discarded, preserving necessary information for accurate inference.

\noindent\textbf{Extension to other private inference frameworks.} 

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/msb.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime of $\Pi_{prune}$ and $\Pi_{mask}$ in different layers. We compare different secure pruning strategies based on the BERT Base model.}
%     \label{fig:msb}
% \end{figure*}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/tokens.pdf}
%     \captionsetup{skip=2pt}
%     \caption{Runtime on GPT2 base model with different input length.}
%     \label{fig:token_num}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/lambda.png}
%     \captionsetup{skip=2pt}
%     \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
%     \label{fig:param}
% \end{figure}


% \begin{figure}[h]
%     \centering
%     \begin{minipage}{0.3\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/tokens.pdf}
%         \captionsetup{skip=10pt}
%         \caption{Runtime on GPT2.}
%         \label{fig:token_num}
%     \end{minipage}\hfill
%     \begin{minipage}{0.65\textwidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/ratio.pdf}
%         \captionsetup{skip=2pt}
%         \caption{Ablation study on hyperparameters $\lambda$ and $\alpha$.}
%         \label{fig:param}
%     \end{minipage}
% \end{figure}

 % base model with different input length.