\section{Related Work}
\textbf{Reward-free offline RL} proposes to learn from the offline data that does not contain rewards in a task-agnostic way.
The goal is to extract general behaviors from the offline data to solve a variety of downstream tasks. 
One approach to this is to use goal-conditioned RL, and sample goals using a technique proposed in Hindsight Experience Replay \citep{andrychowicz2017hindsight}. \citet{park2024hiql} show that this can be applied to learn a goal-conditioned
policy using IQL, as well as to learn a hierarchical value function. \citet{hatch2022example} proposes using a small
set of observations corresponding to the solved task to define the task and learn from reward-free data. \citet{kim2024unsupervised} study how to transition from offline to online RL, and uses HILP \citep{park2024foundation} for unsupervised pre-training, then fine-tunes it on online data. \citet{yu2022leverage, hu2023provable} propose to use labeled data to train a reward function, than label the reward-free trajectories.

\textbf{Zero-shot methods} go beyond just goal-reaching from offline data, and aim to solve any possible task specified during test time. 
HILP \citep{park2024foundation} propose learning a distance-preserving representation space such that the distance in that space is proportional to the number of steps between two states, similar to Laplacian representations \citep{wu2018laplacian, wang2021towards, wang2022reachability}. Forward-Backward representations \citep{touati2021learning, touati2022does} tackle this with an approach akin to successor-features \citep{barreto2017successor}.  \citet{frans2024unsupervised} propose to learn a transformer model to encode target task's state action sequences. \citet{Chen_Zhu_Agrawal_Zhang_Gupta_2023} propose to learn basis Q-functions that implicitly model dynamics and enable generalization to tasks that can be represented as a linear combination of the learned basis functions.

\textbf{Optimal Control}, similar to RL, tackles the problem of selecting actions 
of an agent in an environment in order to optimize a given objective (reward for RL, cost for control). Unlike RL, optimal control commonly assumes that the transition dynamics of the environment are known \citep{Bertsekas2019}. This paradigm has been used to great success long before the advent of deep learning, and has enabled applications ranging from controlling aircraft, rockets and missiles \citep{bryson1996optimal} to controlling humanoid robots \citep{kuindersma2016optimization, schultz2009modeling}. When the transition dynamics cannot be defined precisely, they can often be learned. For example, \citep{Watter_Springenberg_Boedecker_Riedmiller_2015} learns the environment dynamics, and uses iLQG \citep{todorov2005generalized} on the linear approximation of the dynamics.
When dynamics are unknown and need to be approximated, 
the line between RL and optimal control becomes blurry, as a lot of RL methods can be interpreted as approximating dynamic programming approaches in control in the context of unknown dynamics \citep{bertsekas2012dynamic, sutton2018reinforcement}.  
In this work, we use the term RL to refer to methods that either implicitly or explicitly use rewards information to train a policy function, and the term optimal control to refer to methods that use a dynamics model and, during inference, explicitly search for the best actions that optimize a given objective function. 

\begin{figure}[t]
    %
    \centering
    \includegraphics[width=0.44\linewidth]{Figures/main_maze_figure_short.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/layout_change_res.pdf}
    \includegraphics[width=0.6\linewidth]{Figures/layout_change_res_legend.pdf}
    %
    \caption{\textbf{Left:} We train offline goal-conditioned agents on trajectories collected in a subset of maze layouts (left), and evaluate on held out layouts, observing trajectories shown on the right. Only \JEPA{} solves the task (see \Cref{fig:sample_trajs} for more). \textbf{Right:} Success rates of tested methods on held-out layouts, as a function of the number of training layouts. Rightmost plot shows success rates of models trained on data from five layouts,
    evaluated on held-out layouts ranging from those similar to training layouts to out-of-distribution ones. We use map layout edit distance from the training layouts as a measure of distribution shift. \JEPA{} demonstrates the best generalization performance. Results are averaged over 3 seeds, shaded area denotes standard deviation. See \Cref{fig:main_idea} for more details on \JEPA{}.}
    \label{fig:maze_main_figure}
    %
\end{figure}

\pagebreak

\textbf{Investigating importance of offline data.} ExORL \citep{yarats2022don} show the importance of data for offline RL, and demonstrate that data collected using unsupervised RL enables off-policy RL algorithms to perform well in the offline setting; however, that study only compares using data collected by unsupervised RL and by task-specific agents, without giving a more fine-grained analysis on how different aspects of the data affect performance. \citet{buckman2020importance} investigates the data importance for offline RL with rewards.
Recently proposed OGBench \citep{park2024ogbench} introduces multiple versions of offline data for a variety of goal-conditioned tasks; in contrast to that work, we focus on top-down 
navigation environments and build \textit{23 different datasets} to perform a detailed study of methods' generalization, including to new tasks and environment layouts, as opposed to at most only three dataset versions for one task in OGBench and its focus on the single layout, goal-conditioned setting. \citet{cobbe2018quantifying} investigate generalization in RL using variations in the environment akin to what we did in \Cref{exp:layout_changes}, although that study is using the online setting with rewards. \citet{yang2023essential} also study generalization of offline GCRL, but focus on reaching out-of-distribution goals. \citet{ghugare2024closing} study stitching generalization.