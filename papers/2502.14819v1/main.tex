\pdfoutput=1
\documentclass{article}
%

%
%


%
\input{include/preamble/preamble.tex}
\input{include/preamble/preamble_math}
\input{include/preamble/definitions_basic}
\input{include/preamble/math_commands}

%
%
%
%
\input{include/preamble/minimalist_biblatex}

\addbibresource{main}

\usepackage{thm-restate}
\usepackage{pifont}
\usepackage[table]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{float}
%

\crefformat{equation}{(#2#1#3)}
%
%
%

\crefname{example}{Example}{Examples}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{cor}{Corollary}{Corollaries}
\crefname{theorem}{Theorem}{Theorems}
\crefname{assumption}{Assumption}{Assumptions}

%
%
%
\usepackage{enumitem} %
\usepackage[separate-uncertainty=true,multi-part-units=single]{siunitx} %

\newtcolorbox{conclusionbox}{
  colback=blue!5,        %
  colframe=blue!75!black, %
  coltitle=black,        %
  fonttitle=\bfseries,   %
  boxrule=1pt,           %
  arc=1mm,               %
  left=2mm,              %
  right=2mm,             %
  top=1mm,               %
  bottom=1mm,            %
}

\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

\definecolor{softgreen}{RGB}{85, 168, 104}
\definecolor{softyellow}{RGB}{255, 220, 100}
\definecolor{softred}{RGB}{233, 83, 83}

%
\newcommand{\YES}{{\color{softgreen}\ding{51}}}
\newcommand{\NO}{{\color{softred}\ding{55}}}

%
%

%
%
%
%

%
%
%
%

%
%
%
%

%
\newcommand{\GOOD}{{\color{softgreen}\ding{72}\ding{72}\ding{72}}}
\newcommand{\MEDIUM}{{\color{softyellow}\ding{72}\ding{72}\ding{73}}}
\newcommand{\POOR}{{\color{softred}\ding{72}\ding{73}\ding{73}}}

\newcommand{\maxf}[1]{{\cellcolor[gray]{0.8}} #1}
\global\long\def\embedding{\lambda}

%
\declaretheoremstyle[
%
spacebelow=\parsep,
    spaceabove=\parsep,
  mdframed={
    backgroundcolor=gray!10!white,     %
    hidealllines=true, 
    innertopmargin=8pt, 
    innerbottommargin=4pt, 
    skipabove=8pt,
    skipbelow=10pt,
    nobreak=true
}
]{grayboxed}
\declaretheorem[style=grayboxed,name=Assumption]{gassumption}
%
%
%
\crefname{gassumption}{Assumption}{Assumptions}



%
%
%
\usepackage{xcolor}
\input{include/preamble/commenting.tex}
%
%
%
%
%
%
%
%
%
%

%
\usepackage{authblk}
%

%
\usepackage{makecell}


\newcommand{\ip}[2] {\ensuremath{\langle #1 , #2 \rangle}}
\newcommand{\id}{\ensuremath{\mathrm{Id}}}
\newcommand{\norm}[1]{\ensuremath{\lVert #1 \rVert}}
%
\newcommand{\RR}{\mathbb{R}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calZ}{\mathcal{Z}}
%
\newcommand{\sig}{\sigma}
\newcommand{\Sig}{\Sigma}
\newcommand{\al}{\alpha}
\newcommand{\lda}{\lambda}
\newcommand{\Lda}{\Lambda}
%
\newcommand{\gam}{\gamma}
\newcommand{\Del}{\Delta}
\newcommand{\del}{\delta}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calY}{\mathcal{Y}}
%


%
\usepackage{multirow}
%
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows, automata}
\usetikzlibrary{patterns}
%
\newcommand{\uk}{\diamond}
\newcommand{\embx}{\bar{f}}
\newcommand{\emby}{\bar{g}}
\newcommand{\latx}{h_x}
\newcommand{\laty}{h_y}
\newcommand{\latentembx}{f}
\newcommand{\latentemby}{g}
\newcommand{\onehot}{\mathbf{1}}
\newcommand{\core}[1]{\textrm{core}(#1)}
\newcommand{\neigh}[1]{\textrm{ne}(#1)}

%
\def\showauthornotes{1}
\ifnum\showauthornotes=1
\newcommand{\Authornote}[2]{{\sf\small\color{blue}{[#1: #2]}}}
\else
\newcommand{\Authornote}[2]{}
\fi

\newcommand{\TR}{\Authornote{TR}}

\newcommand{\JEPA}{PLDM}
\newcommand{\paperlink}{\href{https://latent-planning.github.io/}{latent-planning.github.io}}

%
%

%

\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}

\title{Learning from Reward-Free Offline Data: \\ A Case for Planning with Latent Dynamics Models}

\author{
\begin{center}
Vlad Sobal*\textsuperscript{1}~~~~Wancong Zhang*\textsuperscript{1}~~~
Kynghyun Cho\textsuperscript{1,2}~~~
Randall Balestriero\textsuperscript{3}\\\vspace*{-3pt}
Tim G. J. Rudner\textsuperscript{1}~~~~~Yann LeCun\textsuperscript{1,4}
\linebreak\linebreak
$^{1}$New York University~~~~$^{2}$Genentech~~~~$^{3}$Brown University~~~~$^{4}$Meta -- FAIR
\end{center}
}

%
\renewcommand{\thefootnote}{}
\footnotetext{* Equal contribution. Author ordering determined by coin flip.}
%
\renewcommand{\thefootnote}{\arabic{footnote}}

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%

\date{}

\linepenalty=100

\begin{document}

\maketitle

\begin{abstract}
A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations.
In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties—such as data diversity, trajectory quality, and environment variability—affect the performance of these approaches.
Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data. 
\end{abstract}

\section{Introduction}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/main_idea.pdf}
    %
    \caption{\textbf{Overview of our analysis.} We test six methods for learning from offline reward-free
    trajectories on 23 different datasets across two top-down navigation environments. We evaluate for six generalization properties required to scale to large offline datasets of suboptimal trajectories. We find that planning with a latent dynamics model (PLDM) demonstrates the highest level of generalization. For a full comparison, see \Cref{tab:method_comparison_transposed}. \textbf{Right:} diagram of PLDM. Circles represent variables, rectangles -- loss components, half-ovals -- trained models. }
    \label{fig:main_idea}
    %
\end{figure*}


How can we bulid a system that can perform well on unseen task and environment combinations? One promising way to achieve this is to not rely on online interactions or expert demonstrations, and instead leverage large amounts of existing suboptimal trajectories without reward annotations \citep{kim2024unsupervised, park2024foundation, Dasari_Ebert_Tian_Nair_Bucher_Schmeckpeper_Singh_Levine_Finn_2020}. Broadly, two dominant fields offer promising solutions to leveraging such data: reinforcement learning and optimal control.

Although online reinforcement learning enables learning to solve increasingly complex tasks---ranging from Atari games \citep{mnih2013playing} to Go \citep{silver2016mastering} to controlling real robots \citep{openai2018learning}---it requires numerous environment interactions to do so.
For example, \citet{openai2018learning} used the equivalent of 100 years of real-time hand manipulation experience to train a robot to reliably handle a Rubik's cube. 
To address this sample complexity problem,
offline RL methods \citep{kostrikov2021offline, levine2020offline, ernst2005tree} have been developed to learn behaviors from state--action trajectories with corresponding reward annotations.
Unfortunately, conventional offline RL limits agents to one task, making it impossible to use a trained agent to solve another downstream task.
To address this shortcoming, recently proposed methods learn desired behaviors from offline reward-free trajectories \citep{park2024hiql, touati2021learning, kim2024unsupervised, park2024foundation}. 
This reward-free paradigm is particularly appealing as it allows agents to learn from suboptimal data and use the learned policy to solve a variety of downstream tasks.
For example, a system trained on a large dataset of low-quality robotic interactions with cloths can generalize to new tasks, such  as folding laundry \citep{black2024pi_0}.

On the other hand, the field of optimal control approaches the challenge from a different angle, and instead of optimizing a policy function through trial and error, 
aims to use a known dynamics model \citep{Bertsekas2019, ilqg, Tassa_Erez_Smart_2007} to plan out actions. Of course, the dynamics model is often hard to define exactly, prompting a range of methods that learn the dynamics model instead \citep{Watter_Springenberg_Boedecker_Riedmiller_2015, Finn_Levine_2017, Yen-Chen_Bauza_Isola_2019}. This approach, when applied to manipulation, has also been shown to achieve generalization to unseen objects \citep{Ebert_Finn_Dasari_Xie_Lee_Levine_2018}. Notably, dynamics model learning approaches can easily use offline trajectories without reward signal for training, making this a promising approach for training a general agent on offline data \citep{Dasari_Ebert_Tian_Nair_Bucher_Schmeckpeper_Singh_Levine_Finn_2020, rybkin2018learning}.


Despite significant advances in RL and optimal control, the impact of pre-training data quality on reward-free offline learning remains largely unexplored. Prior work primarily focuses on RL methods and trains on data collected either from expert policies or unsupervised RL \citep{fu2020d4rl, yarats2022don}, and does not test proposed methods on more than three different dataset types per task. In this work, we bridge this gap by systematically analyzing the strengths and weaknesses of various approaches for learning from reward-free trajectories. Through carefully designed experiments, we evaluate how different learning paradigms handle offline data across varying levels of quality and quantity.


Our contributions can be summarized as follows:
\vspace*{-2pt}
\begin{enumerate}[leftmargin=20pt]
%
    \item We propose two navigation environments with granular control over the data generation process, and generate a total of \textit{23 datasets} of varying quality;
    \item  We evaluate methods for learning from offline, reward-free trajectories, drawing from both reinforcement learning and optimal control paradigms. Our analysis systematically examines their performance across the proposed environments, assessing their ability to learn from random policy trajectories, stitch together short sequences, train effectively on limited data, and generalize to unseen environments and tasks;
    \item We demonstrate that learning a latent dynamics model and using it for planning is robust to suboptimal data quality and achieves the highest level of generalization to environment variations;
    \item We present a list of guidelines to help practitioners choose between methods depending on available data and generalization requirements.
\end{enumerate}

To facilitate further research into methods for learning from offline trajectories without rewards, we release code, data, environment visualizations, and more at \paperlink{}.
%

\pagebreak

\section{Related Work}


\textbf{Reward-free offline RL} proposes to learn from the offline data that does not contain rewards in a task-agnostic way.
The goal is to extract general behaviors from the offline data to solve a variety of downstream tasks. 
One approach to this is to use goal-conditioned RL, and sample goals using a technique proposed in Hindsight Experience Replay \citep{andrychowicz2017hindsight}. \citet{park2024hiql} show that this can be applied to learn a goal-conditioned
policy using IQL, as well as to learn a hierarchical value function. \citet{hatch2022example} proposes using a small
set of observations corresponding to the solved task to define the task and learn from reward-free data. \citet{kim2024unsupervised} study how to transition from offline to online RL, and uses HILP \citep{park2024foundation} for unsupervised pre-training, then fine-tunes it on online data. \citet{yu2022leverage, hu2023provable} propose to use labeled data to train a reward function, than label the reward-free trajectories.

\textbf{Zero-shot methods} go beyond just goal-reaching from offline data, and aim to solve any possible task specified during test time. 
HILP \citep{park2024foundation} propose learning a distance-preserving representation space such that the distance in that space is proportional to the number of steps between two states, similar to Laplacian representations \citep{wu2018laplacian, wang2021towards, wang2022reachability}. Forward-Backward representations \citep{touati2021learning, touati2022does} tackle this with an approach akin to successor-features \citep{barreto2017successor}.  \citet{frans2024unsupervised} propose to learn a transformer model to encode target task's state action sequences. \citet{Chen_Zhu_Agrawal_Zhang_Gupta_2023} propose to learn basis Q-functions that implicitly model dynamics and enable generalization to tasks that can be represented as a linear combination of the learned basis functions.

\textbf{Optimal Control}, similar to RL, tackles the problem of selecting actions 
of an agent in an environment in order to optimize a given objective (reward for RL, cost for control). Unlike RL, optimal control commonly assumes that the transition dynamics of the environment are known \citep{Bertsekas2019}. This paradigm has been used to great success long before the advent of deep learning, and has enabled applications ranging from controlling aircraft, rockets and missiles \citep{bryson1996optimal} to controlling humanoid robots \citep{kuindersma2016optimization, schultz2009modeling}. When the transition dynamics cannot be defined precisely, they can often be learned. For example, \citep{Watter_Springenberg_Boedecker_Riedmiller_2015} learns the environment dynamics, and uses iLQG \citep{todorov2005generalized} on the linear approximation of the dynamics.
When dynamics are unknown and need to be approximated, 
the line between RL and optimal control becomes blurry, as a lot of RL methods can be interpreted as approximating dynamic programming approaches in control in the context of unknown dynamics \citep{bertsekas2012dynamic, sutton2018reinforcement}.  
In this work, we use the term RL to refer to methods that either implicitly or explicitly use rewards information to train a policy function, and the term optimal control to refer to methods that use a dynamics model and, during inference, explicitly search for the best actions that optimize a given objective function. 

\begin{figure}[t]
    %
    \centering
    \includegraphics[width=0.44\linewidth]{Figures/main_maze_figure_short.pdf}
    \includegraphics[width=0.49\linewidth]{Figures/layout_change_res.pdf}
    \includegraphics[width=0.6\linewidth]{Figures/layout_change_res_legend.pdf}
    %
    \caption{\textbf{Left:} We train offline goal-conditioned agents on trajectories collected in a subset of maze layouts (left), and evaluate on held out layouts, observing trajectories shown on the right. Only \JEPA{} solves the task (see \Cref{fig:sample_trajs} for more). \textbf{Right:} Success rates of tested methods on held-out layouts, as a function of the number of training layouts. Rightmost plot shows success rates of models trained on data from five layouts,
    evaluated on held-out layouts ranging from those similar to training layouts to out-of-distribution ones. We use map layout edit distance from the training layouts as a measure of distribution shift. \JEPA{} demonstrates the best generalization performance. Results are averaged over 3 seeds, shaded area denotes standard deviation. See \Cref{fig:main_idea} for more details on \JEPA{}.}
    \label{fig:maze_main_figure}
    %
\end{figure}

\pagebreak

\textbf{Investigating importance of offline data.} ExORL \citep{yarats2022don} show the importance of data for offline RL, and demonstrate that data collected using unsupervised RL enables off-policy RL algorithms to perform well in the offline setting; however, that study only compares using data collected by unsupervised RL and by task-specific agents, without giving a more fine-grained analysis on how different aspects of the data affect performance. \citet{buckman2020importance} investigates the data importance for offline RL with rewards.
Recently proposed OGBench \citep{park2024ogbench} introduces multiple versions of offline data for a variety of goal-conditioned tasks; in contrast to that work, we focus on top-down 
navigation environments and build \textit{23 different datasets} to perform a detailed study of methods' generalization, including to new tasks and environment layouts, as opposed to at most only three dataset versions for one task in OGBench and its focus on the single layout, goal-conditioned setting. \citet{cobbe2018quantifying} investigate generalization in RL using variations in the environment akin to what we did in \Cref{exp:layout_changes}, although that study is using the online setting with rewards. \citet{yang2023essential} also study generalization of offline GCRL, but focus on reaching out-of-distribution goals. \citet{ghugare2024closing} study stitching generalization.

\section{The Landscape of Available Methods}

In this section, we formally introduce the setting of learning from state-action sequences without reward annotations and overview available approaches. We also introduce a
method we call Planning with a Latent Dynamics Model (PLDM).

\subsection{Problem Setting}
We consider a Markov decision process (MDP) $\gM = (\gS, \gA, \mu, p, r)$, where $\gS$ is the state space, $\gA$ is the action space, $\mu \in \gP(\gS)$ denotes the initial state distribution, $p \in \gS \times \gA \rightarrow \gS$ denotes the transition dynamics, and $r \in \gS \rightarrow \sR$ denotes the reward function. We work in the offline setting, where we have access to a dataset of state-action sequences $\gD$ which consists of transitions $(s_0, a_0, s_1, \ldots, a_{T-1}, s_{T})$. We emphasize again that the offline dataset in our setting does not contain any reward information. In our experiments, we also consider only deterministic transition dynamics. The goal is, given $\gD$, to find a policy $\pi \in \gS \times \gZ \rightarrow \gA$, to maximize cumulative reward $r_z$, where $\gZ$ is the space of possible task definitions. Our goal is to make the best use of the offline dataset $\gD$ to enable the agent to solve a variety of tasks in a given environment with potentially different layouts. During evaluation, unless otherwise specified,
the agent is tasked to reach a goal state $s_g$, so the reward is defined as $r_g(s) = \mathbb{I}[s = s_g]$, and $\gZ$ is equivalent to $\gS$.

\subsection{Common Solutions}
\label{sec:approaches}

\begin{table*}[t]
\setlength{\tabcolsep}{7.8pt}
\caption{
\textbf{Road-map of our generalization stress-testing experiments.} 
We test 4 offline goal-conditioned methods - HIQL, GCIQL, CRL, GCBC; a zero-shot RL method HILP, and a learned latent dynamics planning method \JEPA{}. \GOOD{} denotes good performance in the specified experiment, \MEDIUM{} denotes average performance, and \POOR{} denotes poor performance. We see that HILP and \JEPA{} are the 
best-performing methods, with \JEPA{} standing out as the only method that reaches competitive performance in all settings.}
\vspace*{5pt}
\centering
\renewcommand{\arraystretch}{0.8} %
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|C{1.0cm}|C{1.0cm}|C{1.0cm}|C{1.0cm}|C{1.0cm}|C{1.0cm}}
\toprule
\textbf{Property} \scriptsize (Experiment section)                                                                                                & \textbf{HILP} & \textbf{HIQL} & \textbf{GCIQL} & \textbf{CRL} & \textbf{GCBC} & \textbf{\JEPA{}} \\
\midrule
\textbf{Transfer to new environments} \scriptsize (\ref{exp:layout_changes})                          & \POOR{}       & \POOR{}       & \POOR{}        & \POOR{}      & \POOR{}       & \GOOD{}          \\
\textbf{Transfer to a new task} \scriptsize (\ref{exp:zero_shot})                                               & \MEDIUM{}     & \POOR{}       & \POOR{}        & \POOR{}      & \POOR{}       & \GOOD{}          \\
\textbf{Data efficiency} \scriptsize (\ref{exp:dataset_size})                                                & \POOR{}       & \MEDIUM{}     & \MEDIUM{}      & \MEDIUM{}    & \MEDIUM{}     & \GOOD{}          \\
\textbf{Best-case performance} {\scriptsize (\ref{exp:best_case})}                                          & \GOOD{}       & \GOOD{}       & \GOOD{}        & \GOOD{}      & \MEDIUM{}     & \MEDIUM{}        \\
\textbf{Can learn from random policy trajectories} \scriptsize (\ref{exp:dataset_quality})                       & \GOOD{}       & \POOR{}       & \POOR{}        & \POOR{}      & \POOR{}       & \MEDIUM{}        \\
\textbf{Can stitch suboptimal trajectories} \scriptsize (\ref{exp:suboptimal}) & \GOOD{}       & \POOR{}       & \POOR{}        & \POOR{}      & \POOR{}       & \MEDIUM{}        \\
\midrule
\textbf{Competitve performance in all settings} & \NO{} & \NO{} & \NO{} & \NO{} & \NO{} & \YES{} \\
\bottomrule
\end{tabular}
}
\label{tab:method_comparison_transposed}
%
\end{table*}

In this work, we focus on methods that can learn to solve tasks purely from offline trajectories without rewards annotations. 
We do not consider methods that augment reward-labeled dataset with reward free data, as we believe that 
the fully reward-free approach is more general.
In offline RL, methods for learning without rewards fall into two broad categories: offline goal-conditioned RL and zero-shot RL methods that model the underlying task as a latent variable. In this work, we consider both categories, and select methods that we believe reflect the state of the art.
We test all methods on goal-reaching, \mbox{and test the zero-shot methods transfer to new tasks. The methods we investigate are:}\vspace*{-2pt}
\begin{itemize}
[leftmargin=20pt]
\setlength\itemsep{-2pt}
\item \textbf{GCIQL} \citep{park2024hiql} -- goal-conditioned version of Implicit Q-Learning \citep{kostrikov2021offline}, a strong and widely-used method for offline RL;
\item \textbf{HIQL} \citep{park2024hiql} -- a hierarchical GCRL method which trains two policies: one to generate subgoals, and another one to reach the subgoals. Notably, both policies use the same value function;
\item \textbf{HILP} \citep{park2024foundation} -- a method that learns state representations from the offline data such that the distance in the
learned representation space is proportional to the number of steps between two states. A direction-conditioned policy is then learned to be
able to move along any specified direction in the latent space;
\item \textbf{CRL} \citep{eysenbach2022contrastive} -- uses contrastive learning to learn compatibility between states and possible reachable goals. The learned representation, which has been shown to be directly linked to goal-conditioned Q-function, is then used to train a goal-conditioned policy;
\item \textbf{GCBC} \citep{lynch2020learning, ghosh2019learning} -- Goal-Conditioned Behavior Cloning. This is the simplest baseline for goal-reaching. 
\end{itemize}

\subsection{Planning with a Latent Dynamics Model}
\label{sec:jepa}
Although the methods outlined in \Cref{sec:approaches} cover a wide
range of paradigms, they all fall into the model-free RL category, and none of them use the model-based approach, which achieves impressive performance in other settings \citep{deisenroth2011pilco, silver2017mastering, silver2016mastering, rafailov2021offline}.
An easy way to use state-action sequences is to learn a dynamics model, making it a natural choice for our setting.
For example, \citet{nair2020goal, pertsch2020long} propose model-based methods for goal-reaching, and use an image reconstruction objective.
In this work, we choose to focus on just the dynamics learning objective, with
added representation learning objectives to prevent collapse, therefore bypassing the need for image reconstruction.
We introduce a model-based method named \JEPA{} -- Planning with a Latent Dynamics Model. We learn latent dynamics using a reconstruction-free self-supervised learning (SSL) objective, and utilize the joint-embedding predictive architecture (JEPA) \citep{lecun2022path}. 
During evaluation, we use planning to optimize the goal-reaching objective.
We opt for an SSL approach that involves predicting the latents as opposed to reconstructing the input observations \citep{hafner2018learning, hafner2019dream, hafner2020mastering, hafner2023mastering, Watter_Springenberg_Boedecker_Riedmiller_2015, Finn_Tan_Duan_Darrell_Levine_Abbeel_2016, Zhang_Vikram_Smith_Abbeel_Johnson_Levine_2019, Banijamali_Shu_Ghavamzadeh_Bui_Ghodsi_2018}
as recent work showed that reconstruction leads to suboptimal features \citep{balestriero2024learning, littwin2024jepa}, and that reconstruction-free representation learning can work well for control and RL \citep{Shu_Nguyen_Chow_Pham_Than_Ghavamzadeh_Ermon_Bui_2020, hansen2022temporal}.

Given agent trajectory sequence $(s_0, a_0, s_1, ..., a_{T-1}, s_T)$,  we specify the \JEPA{} world model as:
\begin{align}
%
    \mathrm{Encoder:} &\quad \hat{z_0} = z_0 = h_{\theta}(s_0)
    \\
    \mathrm{Predictor:} & \quad \hat{z}_t = f_\theta(\hat{z}_{t-1}, a_{t-1})
\end{align}
where $\hat{z}_t$ is the predicted latent state and $z_t$ is the encoder output at step $t$. The training objective involves minimizing the distance between predicted and encoded latents summed over all timesteps. Given latents $Z \in \mathbb{R}^{H \times N \times D}$, where $H \leq T$ is the model prediction horizon, $N$ is the batch dimension, and $D$ the feature dimension, the similarity objective between predictions and encodings is:
\begin{align}
    \mathcal{L}_{\mathrm{sim}}= \sum_{t=0}^H \frac{1}{N}\sum_{b=0}^N\|\hat{Z}_{t,b} - Z_{t,b} \|^2_2
\end{align}
To prevent representation collapse, we use a VICReg-inspired \citep{bardes2021vicreg} objective, and inverse dynamics modeling \citep{lesort2018state}. We show a diagram of PLDM in \Cref{fig:main_idea}. See \Cref{app:collapse_prevention} for details.

\paragraph{Goal-conditioned planning with \JEPA{}.}
In this work, we mainly focus on the task of reaching specified goal states. While methods outlined in \cref{sec:approaches} rely on trained policies to reach the goal, \JEPA{} relies on planning.
At test time, given the current observation $s_0$, goal observation $s_g$, pretrained encoder $h_\theta$ \mbox{predictor $f_\theta$, and planning horizon $H$, our planning objective is:}
\begin{gather}
    \hat{z}_0 = z_0 = h_{\theta}(s_0), \ \hat{z}_t = f_{\theta}(\hat{z}_{t-1}, a_{t-1}) \\
    \mathrm{Cost}(\mathbf{a}, s_0, s_g) = \sum_{t=0}^{H-1} \|h_\theta(s_g) - f_{\theta}(\hat{z}_{t}, a_{t}) \| \label{eq:cost} \\
    \mathbf{a}^* = \arg\min_{\mathbf{a}} \mathrm{Cost}(\mathbf{a}, s_0, s_g)
\end{gather}
Following the Model Predictive Control framework \citep{morari1999model}, our model re-plans at every $k_{\mathrm{th}}$ interaction with the environment. Unless stated otherwise, we use $k=1$.
In all our experiments with \JEPA{}, we use MPPI \citep{williams2015model} for planning. We note that \JEPA{} is not using rewards, neither explicitly nor implicitly, and should be considered as falling under the optimal control category. We also note that in order to apply \JEPA{} to another task, we do not need to retrain the encoder $h_\theta$ and the forward model $f_\theta$, we only need to change the definition of the cost in equation \Cref{eq:cost}. We demonstrate this flexibility in \cref{exp:zero_shot}, where we invert the sign of the cost to make the agent avoid the specified state.

\section{Every Method Can Excel but Few Generalize}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth} %
        \centering
        \includegraphics[width=0.98\linewidth]{Figures/wall_env.pdf}
        \vspace*{-10pt}
        \caption{}
        \label{fig:env}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth} %
    \small
    %
        \centering
        %
        \begin{tabular}{C{0.8cm}C{2.5cm}C{2.5cm}}
            \toprule
            \textbf{Method} & \textbf{Good-quality data} & \textbf{No door-passing trajectories} \\
            \midrule
CRL   & \phantom{1}89.3 \, ± \phantom{1}1.2 & \phantom{1}14.7 \, ± \phantom{1}7.0 \\
GCBC  & \phantom{1}86.0 \, ± \phantom{1}4.5 & \phantom{11}8.4 \, ± \phantom{1}3.7 \\
GCIQL & \phantom{1}93.6 \, ± \phantom{1}0.9 & \phantom{1}22.0 \, ± \phantom{1}4.1 \\
HILP  & 100.0 \; ± \phantom{1}0.0 & 100.0 \; ± \phantom{1}0.0 \\
HIQL  & \phantom{1}96.4 \, ± \phantom{1}3.0 & \phantom{1}26.3 \, ± 13.8 \\
PLDM  & \phantom{1}79.6 \, ± \phantom{1}4.4 & \phantom{1}34.4 \, ± \phantom{1}5.9 \\
            \bottomrule
        \end{tabular}%
        %
        \captionsetup{type=table}
        \vspace{-0.12cm}
        \caption{}
        \label{tab:wc_rate}
    \end{minipage}

    \vspace{-0.18cm}
    \captionsetup{labelformat=empty}
    \caption{\textbf{\Cref{fig:env}}: The Two-Rooms environment. The environment consists of two rooms separated by a wall with one door. The locations of the wall and the door remain fixed. The agent starts at a random location and is tasked with reaching the goal at another randomly sampled location in the other room within 200 steps. The evaluation episode is considered successful if the distance between the goal and the agent is below 1.4 pixels, with the environment being $64 \times 64$ pixels. See \paperlink{} for more visualizations.
    \textbf{\Cref{tab:wc_rate}}: Performance of tested methods on good-quality data and on data with no trajectories passing through the door. Numbers are average success rates $(\pm \text{std})$ across 3 seeds. The dataset size is kept constant at 3M.  }
    \label{fig:env_and_table}
    \captionsetup{labelformat=default}
    %
\end{figure*}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


In this section, we conduct thorough experiments testing methods spanning RL and optimal control outlined in
\Cref{sec:approaches} and \Cref{sec:jepa}. We test all methods on navigation tasks where the agent is a point mass. We present the task in \cref{sec:env}. We 
generate datasets of varying size and quality and test how a specific data type affects a given method.
We design our experiments to test the following properties of methods (see \Cref{tab:method_comparison_transposed} for experiment overview):\vspace*{2pt}
\begin{itemize}[topsep=1pt,itemsep=2pt,partopsep=2pt, parsep=2pt]
%
%
    \item Best-case performance with good data (\Cref{exp:best_case});
    \item Sample efficiency (\Cref{exp:dataset_size});
    \item Ability to stitch together suboptimal trajectories (\Cref{exp:suboptimal});
    \item Ability to learn from random poilcy trajectories (\Cref{exp:dataset_quality});
    \item Zero-shot generalization to a different task (\Cref{exp:zero_shot});
    \item Generalization to new environments variations (\Cref{exp:layout_changes}).
\end{itemize}

We then draw conclusions from our experiments and outline possible next steps in \cref{sec:conclusion}.

\pagebreak

\subsection{Environment}
\label{sec:env}
All our experiments are done with top-down navigation tasks with a point-mass agent. First, we introduce a two-rooms navigation task. Each observation $x_t$ is a top-down view of the two-rooms environment, $x_t \in \sR^{2 \times 64 \times 64}$, shown in \Cref{fig:env}. The first channel in the image is the agent, the second channel is the walls. Actions $a \in \sR^2$ denote the displacement vector of the agent position from one time step to the next one. The norm of the actions is restricted to be less than $2.45$. The goal is to reach another randomly sampled state within 200 environment steps. See \Cref{app:mazes_env} for more details.
This environment makes control of the data generation process very easy, enabling us to conduct our experiments efficiently and thoroughly, while not being so trivial that any
method can solve it with even a little bit of data. Movement and navigation is a big part of virtually every real-world robotic environment, 
making this a useful testbed for development. 

%
%
%
%
%
%
%
%

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{Figures/all_exps.pdf}
    %
    \caption{
    \textbf{Testing the selected methods' performance under different dataset constraints.} Values and shaded regions are means and standard deviations over 3 seeds, respectively.
    \textbf{Left}: To test the importance of the dataset quality, we mix the random policy trajectories with good quality trajectories (see \Cref{fig:traj_examples}). As the amount of good quality data goes to 0, methods begin to fail, with \JEPA{} and HILP being the most robust ones.
    \textbf{Center}: We measure methods' performance when trained with different sequence lengths. 
    We find that many goal-conditioned methods fail when train trajectories are short, which causes far-away goals to become out-of-distribution for the resulting policy.
    \textbf{Right}: We measure methods' performance with datasets of varying sizes. We see that \JEPA{}
    is the most sample efficient, and manages to get almost 50\% success rate even with a 
    few thousand transitions.
    }
    \label{fig:exps}
    %
\end{figure*}

\textbf{Offline data.} To generate offline data, we place the agent in a random location within the environment, and execute a sequence of actions for $T$ steps, where $T$ denotes the episode length. The actions are generated by first picking a random direction, then using Von Mises distribution with concentration 5 to sample action directions. The step size is uniform from $0$ to $2.45$. When sampling low-quality data, we do not bias the action directions using Von Mises, and instead sample the direction completely uniformly. Unless otherwise specified, the episodes' length is $T=91$, and the total number of transitions in the data is 3 million.

\subsection{What Methods Excel In-Distribution with a Large High-Quality Dataset?}
\label{exp:best_case}
To get the topline performance of the methods under optimal dataset conditions, we test them in a setting with a large amount of data, good state coverage, and good quality trajectories long enough to traverse the two rooms. With 3 million transitions, corresponding to around 30 thousand trajectories, all methods reach their best-case performance in this environment. We report the results in \Cref{tab:wc_rate}. On the goal-reaching task in the two-rooms environment, all methods achieve impressive performance, with HIQL and HILP nearing perfect 100\% success rate. \JEPA{} fails to achieve perfect performance here. 
We hypothesize that because \JEPA{}'s training objective is not to learn a policy but to learn dynamics, \mbox{\JEPA{} does not fully benefit from the high-quality trajectories like other model-free methods.}

\begin{conclusionbox}
\textbf{Takeaway}: model-free approaches perform better than the model-based approach when the data is plentiful and high-quality.
\end{conclusionbox}

\subsection{What Method is the Most Sample-Efficient?}
\label{exp:dataset_size}

We investigate how different methods perform when the dataset size varies. 
While our ultimate goal is to have a method that can make use of a large amount of suboptimal offline data, 
this experiment serves to distinguish which methods can glean the most information from available data. We tried ranges of dataset sizes all the way down to a few thousand transitions. In \Cref{fig:exps} we see that the model-based method \JEPA{} outperforms model-free methods when the data is scarce. In particular, HILP is more data-hungry than other \mbox{model-free methods but achieves perfect performance with enough data.}

\begin{conclusionbox}
\textbf{Takeaway}: The model-based approach is more sample-efficient than the model-free ones.
\end{conclusionbox}

\subsection{What Methods Can Stitch Suboptimal Trajectories?}
\label{exp:suboptimal}
\textbf{Can we learn from short trajectories?} In this experiment, we vary the episode length $T$ when generating the data. This experiment aims to test the methods' ability to stitch together shorter trajectories in order to 
get to the goal. In real-life scenarios, collecting long episodes may be much more challenging than having 
a large set of shorter trajectories, especially when we scale to more open-ended environments. Therefore, the ability to learn and generalize effectively from shorter trajectories, even when the evaluation trajectory may be much longer, is essential. In our environment, successfully navigating from the bottom left corner to the bottom right corner requires around 90 steps. This means that successful trajectories for the hardest start and goal pairings are never observed in a dataset with episodes of length 16. In order to successfully solve this task, the learning method
has to be able to stitch together multiple offline trajectories. We generate several datasets, with episode length of 91, 64, 32, 16. We adjust the number of episodes to keep the total number of transitions close to 3 million. 
The results are shown in \Cref{fig:exps} (center). We see that when the episode length is short, goal-conditioned methods fail. We hypothesize that because goal-conditioned methods sample state and goal pairs from a trajectory to train their policies, far away goals become out of distribution for the resulting policy. Although randomly sampling goals from other trajectories during training does not improve generalization, we hypothesize that data augmentation akin to the one outlined in \citep{ghugare2024closing} can help. On the other hand, HILP performs
well because instead of reaching goals, it learns to follow directions in the latent space, which can be learned even from short trajectories. Similarly, a model based method such as PLDM can learn an accurate model from short trajectories and stitch together a plan during test time.

%
%

\textbf{Can we learn from data with imperfect coverage?} 
We artificially constrain trajectories to always stay within one room within the episode, and never pass through the door. Without the constraint, around 35\% trajectories pass through the door.
During evaluation, the agent still needs to go through the door to reach the goal state. This also 
reflects possible constraints in real-life scenarios, as the ability to stitch offline trajectories together is essential to efficiently learn from offline data.
The results are shown in \Cref{tab:wc_rate}.
We see that HILP achieves perfect performance, while \JEPA{} performance drops, but is better than that of other methods. Similarly to the experiment with short trajectories, the GCRL methods fail. We hypothesize that the structure of the latent space allows HILP to stitch trajectories easily, while \JEPA{} retains some performance due to the learned dynamics.
Model-free GCRL methods fail because the goal in a different room from the current state is always out-of-distribution for the policy trained on trajectories staying in one room.

\begin{conclusionbox}
\textbf{Takeaway}: When solving the task requires `stitching', HILP works great. The performance of the model-based approach drops, but is better than that of offline model-free GCRL.
\end{conclusionbox}

\pagebreak

\subsection{What Methods Can Learn From Trajectories of a Random Policy?}
\label{exp:dataset_quality}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

In this experiment, we evaluate how trajectory quality affects agent performance. In practice, collecting trajectories with a random policy is easy, while access to skilled demonstrations cannot always be assumed. Therefore, developing an algorithm that can learn from noisy or random policy trajectories is critical for leveraging all available data. We generate a dataset of noisy trajectories, where at each step the direction is sampled completely at random. This effectively makes the agent move randomly, and throughout the episode it mostly stays close to where it started. In this dataset, the average maximum distance between any two points in a trajectory is $\sim10$ (when the whole environment is 64 by 64), while when using Von Mises to sample actions, it is $\sim 28$.
\mbox{Example trajectories from both types of action sampling are shown in \Cref{fig:traj_examples}.}

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-10pt}
    %
    \begin{subfigure}[b]{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/good_traj.pdf}
        \caption{}
        \label{fig:good_traj}
    \end{subfigure}
    %
    %
    \begin{subfigure}[b]{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/random_traj.pdf}
        \caption{}
        \label{fig:random_traj}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{\textbf{Examples of trajectories used in the two-rooms environment.} 
    \textbf{(a)} shows a trajectory where each step's direction is sampled from Von Mises distribution. \textbf{(b)} shows an example trajectory where each step is fully random, making the agent stay roughly in one place.}
    \label{fig:traj_examples}
    \vspace*{-20pt}
\end{wrapfigure}

We see that HILP and the model-based \JEPA{} perform better with very noisy data, while the goal-conditioned RL methods struggle (\Cref{fig:exps}). Similarly to the experiment with shorter trajectories, we hypothesize that because trajectories on average do not go far, the sampled state and goal pairs during training are close to each other, making faraway goals out of distribution. 
On the other hand, \JEPA{} uses the data only to learn the dynamics model, and random poilcy trajectories are 
still suitable for the purpose. HILP uses the data to learn the latent space and how to traverse it in various dimensions, and can also use the random policy trajectories effectively.

\begin{conclusionbox}
\textbf{Takeaway}: When the dataset quality is low, HILP and the model-based method perform better than offline GCRL.
\end{conclusionbox}

\subsection{What Methods Can Generalize to a New Task?}
\label{exp:zero_shot}
In order to build a system that can 
learn from offline data effectively, we need a learning algorithm that can generalize to different tasks.
So far, we compared all the methods on goal-reaching tasks. In this experiment, we test whether
the selected methods are able to generalize to a different task in the same environment. 
We compare the performances of
\JEPA{} and HILP on the task of avoiding another agent that is `chasing' the controlled agent. We evaluate models trained on optimal data from the experiment in \Cref{exp:best_case} without any additional training. In this task, the chasing agent follows an expert policy that moves toward the agent along the shortest path. To vary the difficulty of the task, we vary the speed of the chasing agent. The goal of the controlled agent is to avoid the chasing agent. We note that the goal-conditioned methods can only reach specified goals, and by definition are unable to avoid a given state. 
Therefore, we only test \JEPA{} and HILP.
At each step, the agent is given the state of the chaser agent, and has to choose actions to avoid the chaser.
To achieve that, in \JEPA{} we simply invert the sign of the planning objective, making planning maximize the distance in representation space to the goal state. In HILP, we invert the skill direction. To compare the two methods, we evaluate the success rate of the controlled agent avoiding the chaser agent. The episode is considered successful if the agent manages to stay away from the chaser by at least 1.4 pixels for the whole episode lasting 100 steps. The results are shown in \Cref{fig:chase_results}. To further analyze the results, we also investigate average distance between the agents throughout the episode, and plot the average, see \Cref{fig:chase_analysis}.
We see that \JEPA{} performs better than HILP, and is able to evade the chaser more efficiently, keeping a larger distance between the agents at the end of the episode.

\begin{conclusionbox}
    \textbf{Takeaway}: planning with a latent dynamics model generalizes better to a new task compared to HILP.
\end{conclusionbox}

\begin{figure*}[t]
    \centering
    %
    \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \begin{minipage}[c]{0.95\linewidth} %
        \centering
        \includegraphics[width=\linewidth]{Figures/chase_env.pdf}
        \vspace{0.001cm}
    \end{minipage}
    \caption{}
    \label{fig:chaser}
    \end{subfigure}
    \hfill
    %
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chase_results.pdf}
        \caption{}
        \label{fig:chase_results}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/chase_dist.pdf}
        \caption{}
        \label{fig:chase_analysis}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{\textbf{Testing zero-shot generalization to the chasing task.} \textbf{(a)} Chase environment. The controlled agent (in green) is tasked with avoiding the chaser agent (red). The chaser agent follows the shortest path to the agent. The observations of the agent remain unchanged: we pass the chaser state as the goal state. The agent has to avoid the specified state instead of reaching it. \textbf{(b)} Performance of the tested methods on the chasing task across different chaser speeds, with faster chaser making the task harder.
    We baseline against policies that always take zero actions (labeled as `Zero') and policies that always take random actions (labeled as `Random').
    \textbf{(c)} Average distance between the controlled agent and the chaser agent throughout the episode when chaser speed is $1.0$.}
    \label{fig:chase_big_figure}
    %
\end{figure*}

%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{What Methods Can Generalize to Unseen Environments?}
\label{exp:layout_changes}


In this experiment we test the ability of the tested methods to generalize to new environments. Generalization to new environment variations is a requirement for any truly general RL agent, as it is impossible to collect data for every scenario.
To test this, we introduce another navigation environment featuring more complex dynamics and configurable layouts, see \Cref{fig:maze_main_figure} for an example. We utilize the Mujoco PointMaze environment \citep{conf/iros/TodorovET12} and generate various maze layouts by randomly permuting wall locations. The data is collected by initializing the agent at a random location and sampling actions randomly at every step. The observation space contains the top down view of the maze in RGB image format and the velocity of the agent, while the action is the 2D acceleration vector. The goal is to reach a randomly sampled goal state in the environment. For more details about the environment, see \Cref{app:mazes_env}.

\begin{wrapfigure}{r}{0.48\textwidth}
\vspace*{-10pt}
    \centering
    \includegraphics[width=0.88\linewidth]{Figures/maze_plan_single.pdf}  %
    %
    \caption{\textbf{Left}: Plans generated by PLDM at test time. \textbf{Right}: Actual agent trajectories for the tested methods. PLDM is the only method that reliably succeeds 
    on held-out mazes.}
    \label{fig:maze_plan}
    \vspace{-0.40cm}
\end{wrapfigure}

To study the generalization ability of our agents, we vary the number (5, 10, 20, 40) of pre-training maze layouts in the offline dataset and evaluate the trained agents on a held-out set of unseen layouts. Furthermore, for agents trained on 5 layouts, we analyze how their performance is affected by the degree to which the test layouts differ from the training layouts in distribution. We show the results in \Cref{fig:maze_main_figure}, and more details in \Cref{fig:sample_trajs}. \JEPA{} demonstrates the best performance, generalizing to unseen environments, even when trained on as few as five maps, while other methods fail.
In particular, as the test layouts move out of distribution from train layouts, all methods except \JEPA{} suffer in performance as a consequence. To make sure all methods are able to solve the task, we also evaluate the methods on a fixed layout, and see that all of them are able to reach 100\% success rate, see \Cref{tab:single_maze_results}. \Cref{fig:maze_plan} and \ref{fig:sample_trajs} show the plans inferred by PLDM at test time, as well as the different agents' trajectories.


%
\begin{conclusionbox}
\textbf{Takeaway}: The model-based approach enables better generalization to unseen environment variations than model-free methods.
\end{conclusionbox}

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\pagebreak

\section{Conclusion}
\label{sec:conclusion}

In this work, we conducted a comprehensive study of existing methods for learning from offline data without rewards spanning both RL and optimal control, aiming to identify the most promising approaches for leveraging large datasets of suboptimal trajectories. Our findings highlight HILP and \JEPA{} as the strongest candidates, with \JEPA{} demonstrating the best generalization to new environment layouts and tasks. We aggregate our experimental results in \Cref{tab:method_comparison_transposed}.
Overall, we draw three main conclusions:\\
\vspace{-0.4cm}
\begin{conclusionbox}
\begin{enumerate}[leftmargin=12pt]
%
\itemsep0.2em 
\item Learning a latent dynamics model and using it to plan exhibits robustness to data quality, superior data efficiency, and the best generalization to new layouts and tasks;
\item Learning a well-structured latent-space (e.g. using HILP) enables trajectory stitching and robustness to data quality, although it is more data-hungry than other methods;
\item Model-free GCRL methods are a great choice when the data is plentiful and good quality.
\end{enumerate}
\end{conclusionbox}

\paragraph{Moving forward.} \JEPA{} works well across different dataset settings, and is able to learn from poor data and generalize to novel environments and tasks. Therefore, we believe that learning latent dynamics models is a promising candidate for pre-training on large datasets of suboptimal trajectories.
Dynamics learning can also be extended to data without actions by modeling them as a latent variable \citep{seo2022reinforcement, ye2022become}. We believe that other non-generative objectives for latent representation learning \citep{oquab2023dinov2, bardes2024revisiting} can be used to further improve performance.
Another promising direction of research is planning. 
%
Dynamics learning and planning bring their own set of issues, including accumulating prediction errors \citep{Lambert_Pister_Calandra_2022} and increased computational complexity during  inference. In our case, we used MPPI \citep{williams2015model} for planning with a learned dynamics model, which takes
a considerable amount of time, making evaluation with \JEPA{} about 100 times slower compared to model-free methods (see \Cref{sec:plan_time} for details). Further research into making planning more efficient by e.g. backpropagating through the forward model is needed \citep{bharadhwaj2020model}. In domains where inference speed is important, planning can be also used as the target to train a policy \citep{liu2022distilling}.

%
%

\textbf{Limitations.} All our experiments were conducted in simple navigation environments, and it is unclear if these findings
will translate to more complex environments, e.g. physical robots. However, we argue that the conceptual understanding of the effects
of data quality on the investigated methods will hold, as \mbox{even in the relatively simple setting, we see many recent methods break down in surprising ways.}

\clearpage

\printbibliography

%
%
%
%
%
\newpage
\appendix
\onecolumn


\section{Environments and Datasets}

\subsection{Two-Rooms Environment}
We build our own top-down navigation environment. It is implemented in PyTorch \citep{paszke2019pytorch}, 
and supports GPU acceleration. The environment does not model momentum, i.e. the agent
does not have velocity, and is moved by the specified action vector at each step. When the action
takes the agent through a wall, the agent is moved to the intersection point between the action vector
and the wall. We generate the specified datasets and save them to disk for our experiments. The
datasets generation takes under 30 minutes.

\subsection{Diverse PointMaze}
\label{app:mazes_env}
Here, we build upon the Mujoco PointMaze environment \citep{conf/iros/TodorovET12}, which contains a point mass agent with a 4D state vector $(\mathrm{global} \ x, \mathrm{global} \ y, v_x, v_y)$, where $v$ is the agent velocity. To allow our models to perceive the different maze layouts, we use as model input a top down view of the maze rendered as $(64,64,3)$ RGB image tensor instead of relying on $(\mathrm{global} \ x, \mathrm{global} \ y)$ directly. 

Mujoco PointMaze allows for customization of the maze layout via a grid structure, where a grid cell can either be a wall or space. We opt for a $4 \times 4$ grid (excluding outer wall). Maze layouts are generated randomly. Only the following constraints are enforced: 1) all the space cells are interconnected, 2) percentage of space cells range from $50\%$ to $75\%$. 

We set action repeat to $4$ for our version of the environment.

\subsubsection{Dataset Generation}
We produce four training datasets with the following parameters:

\begin{table}[H]
    \centering
    \begin{tabular}{C{2cm}C{2cm}C{2cm}C{2cm}}
    \toprule
    \textbf{\# Transitions} & \textbf{\# layouts} & \textbf{\# episodes per layout} & \textbf{episode length} \\
    \midrule
    1000000 & 5  & 2000 & 100 \\
    1000000 & 10 & 1000 & 100 \\
    1000000 & 20  & 500 & 100 \\
    1000000 & 40  & 250 & 100 \\
    \bottomrule
    \end{tabular}

    \caption{Details for Diverse PointMaze datasets}
    \label{tab:maze_data_table}
\end{table}

Each episode is collected by setting the $(\mathrm{global} \ x, \mathrm{global} \ y)$ at a random location in the maze, and agent velocity $(v_x, v_y)$ by randomly sampling a 2D vector with $ \| v \| \leq 5$, given that $v_x$ and $v_y$ are clipped within range of [$-5$, $5$] in the environment.


\subsubsection{Evaluation}

All the test layouts during evaluation are disjoint from the training layouts. For each layout, trials are created by randomly sampling a start and goal position guaranteed to be at least $3$ cells away on the maze. The same set of layouts and trials are used to evaluate all agents for a given experimental setting.

We evaluate agents in two scenarios: 1) How agents perform on test layouts when trained on various number of train layouts; 2) Given a constant number of training layouts, how agents perform on test maps with varying degrees of distribution shift from the training layouts.

For scenario 1), we evaluate the agents on $40$ randomly generated test layouts, $1$ trial per layout.

For scenario 2), we randomly generate test layouts and partition them into groups of $5$, where all the layouts in each group have the same degree of distribution shift from train layout as defined by metric $D_{min}$ defined as the following:

Given train layouts $\{ L^1_{\text{train}}, L^2_{\text{train}}, ... L^N_{\text{train}} \}$, test layout $L_{\text{test}}$, and let $d(L_1, L_2)$ represents the edit distance between two layouts $L_1$ and $L_2$'s binary grid representation. We quantify the distribution shift of $L_{\text{test}}$ as $D_{\min} = \min_{i \in \{1, 2, \ldots, N\}} d(L_{\text{test}}, L_{\text{train}}^{(i)})$.

In this second scenario we evaluate $5$ trials per layout, thus a total of $5 \times 5 = 25$ per group.

\section{Visualization of Plans and Trajectories for Diverse PointMaze}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/sample_trajs.pdf}
    \caption{\textbf{Top}: The training layouts used in the 5 layout setting. \textbf{Middle}: Trajectories of different agents navigating an unseen maze layout towards goal at test time. As the layouts become increasingly out-of-distribution, only PLDN consistently succeeds. Layouts can be represented as a 4x4 array, with each value being either a wall or empty space. The distribution shift is quantified by the minimum edit distance between a given test layout and the closest training layout. The top row corresponds to an in-distribution layout with a minimum edit distance of 0, and with each subsequent row, the minimum edit distance increases by 1 incrementally.}
    \label{fig:sample_trajs}
\end{figure}

\section{Models}


For CRL, GCBC, GCIQL, and HIQL we use the implementations from the repository\footnote{\label{ogbench_fn}\href{url}{https://github.com/seohongpark/ogbench}} of OGBench \citep{park2024ogbench}. Likewise, for HILP we use the official implementation\footnote{\label{hilp_fn}\href{url}{https://github.com/seohongpark/HILP}} from its authors.

For the Diverse PointMaze environment, to keep things consistent with our implementation of PLDM (\ref{pldm_pointmaze}), instead of using frame stacking, we append the agent velocity directly to the encoder output.

\subsection{PLDM}
\lstset{
    basicstyle=\ttfamily\footnotesize, %
    breaklines=true, %
    frame=none, %
    columns=fullflexible, %
}

\subsubsection{Objective for collapse prevention}
\label{app:collapse_prevention}
To prevent collapse, we introduce a VICReg-based \citep{bardes2021vicreg} objective. We modify it to apply variance objective across the time dimension
to encourage features to capture information that changes, as opposed to
information that stays fixed \citep{sobal2022joint}.
The objective to prevent collapse is defined as follows:
\begin{gather*}
    \mathcal{L}_{\mathrm{var}} = \frac{1}{HD} \sum^H_{t=0} \sum^D_{j=0} \mathrm{max}(0, \gamma - \sqrt{\mathrm{Var}(Z_{t,:,j}) + \epsilon} ) \\
    \mathcal{L}_{\mathrm{time-var}} = \frac{1}{ND} \sum^N_{b=0} \sum^D_{j=0} \mathrm{max}(0, \gamma - \sqrt{\mathrm{Var}(Z_{:,b,j}) + \epsilon} ) \\
    C(Z_t) = \frac{1}{N-1}(Z_t-\bar{Z_t})^\top(Z_t-\bar{Z_t}),  \ \bar{Z} =  \frac{1}{N} \sum^N_{b=1} Z_{t,b} \\
    \mathcal{L}_{\mathrm{cov}} = \frac{1}{H} \sum^{H}_{t=0} \frac{1}{D} \sum_{i \neq j} [C(Z_t)]^2_{i,j} \\
    \mathcal{L}_{\mathrm{IDM}} = \sum^H_{t=0} \frac{1}{N} \sum^N_{b=0} \| a_{t,b} - \mathrm{MLP}(Z_{(t,b)}, Z_{(t+1,b)}) \|^2_2
\end{gather*}

We also apply a tunable objective to enforce the temporal smoothness of learned representations:
\begin{equation*}
\begin{aligned}
    \mathcal{L}_{\mathrm{time-sim}}= \sum_{t=0}^{H-1} \frac{1}{N}\sum_{b=0}^N\|Z_{t,b} - Z_{t+1,b} \|^2_2
\end{aligned}
\end{equation*}

The combined objective is a weighted sum of above:
\begin{gather*}
    \mathcal{L_{\mathrm{JEPA}}} = \mathcal{L}_{\mathrm{sim}} + \alpha \mathcal{L}_{\mathrm{var}} + 
    \beta \mathcal{L}_{\mathrm{cov}} + 
    \lambda \mathcal{L}_{\mathrm{time-var}} + 
    \delta \mathcal{L}_{\mathrm{time-sim}} + 
    \omega \mathcal{L}_{\mathrm{IDM}}
\end{gather*}


\subsubsection{Model Details for Two-Rooms}
We use the same Impala Small Encoder used by the other methods from OGBench \citep{park2024ogbench}. 
For predictor, we use the a 2-layer Gated recurrent unit \citep{cho2014properties} with 512 hidden dimensions; the predictor input at timestep $t$ is a 2D displacement vector representing agent action at timestep $t$; while the initial hidden state is $h_{\theta}(s_0)$, or the encoded state at timestep $0$. A single layer normalization layer is applied to the encoder and predictor outputs across all timesteps. Parameter counts are the following:


\begin{lstlisting}
total params: 2218672
encoder params: 1426096
predictor params: 793600
\end{lstlisting}


\subsubsection{Model Details for Diverse PointMaze Environment}
\label{pldm_pointmaze}
For the Diverse PointMaze environment, we use convolutional networks for both the encoder and predictor. To fully capture the agent's state at timestep $t$, we first encode the top down view of the maze to get a spatial representation of the environment $h_{\theta}: \mathbb{R}^{3\times64\times64} \to \mathbb{R}^{16\times26\times26}, z^{env} = h_{\theta}(s^{env})$. We incorporate the agent velocity by first transforming it into planes $\mathrm{Expander2D}: \mathbb{R}^2 \to \mathbb{R}^{2\times26\times26}, s^{vp} = \mathrm{Expander2D}(s^v)$, where each slice $s^{vp}[i]$ is filled with $s^v[i]$. Then, we concatenate the expanded velocity tensor with spatial representation along the channel dimension to get our overall representation: $\ z=\mathrm{concat}(s^{vp}, z^{env}, \mathrm{dim}=0) \in \mathbb{R}^{18\times26\times26}$.

For the predictor input, we concatenate the state $s_t \in \mathbb{R}^{18\times26\times26}$ with the expanded action $\mathrm{Expander2D}(a_t) \in \mathbb{R}^{2\times26\times26}$ along the channel dimension. The predictor output has the same dimension as the representation: $\hat{z} \in \mathbb{R}^{18\times26\times26}$. Both the encodings and predictions are flattened for computing the VicReg and IDM objectives.

We set the planning-frequency (\Cref{sec:jepa}) in MPPI to $k=4$ for this environment.

The full model architecture is summarized using PyTorch-like notations.


\begin{lstlisting}
total params: 53666
encoder params: 33296
predictor params: 20370

PLDM(
    (backbone): MeNet6(
        (layers): Sequential(
            (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))
            (1): GroupNorm(4, 16, eps=1e-05, affine=True)
            (2): ReLU()
            (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))
            (4): GroupNorm(8, 32, eps=1e-05, affine=True)
            (5): ReLU()
            (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
            (7): GroupNorm(8, 32, eps=1e-05, affine=True)
            (8): ReLU()
            (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (10): GroupNorm(8, 32, eps=1e-05, affine=True)
            (11): ReLU()
            (12): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
        )
        (propio_encoder): Expander2D()
    )
    (predictor): ConvPredictor(
        (layers): Sequential(
            (0): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): GroupNorm(4, 32, eps=1e-05, affine=True)
            (2): ReLU()
            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (4): GroupNorm(4, 32, eps=1e-05, affine=True)
            (5): ReLU()
            (6): Conv2d(32, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (action_encoder): Expander2D()
    )
)
\end{lstlisting}

\section{Analyzing planning time of PLDM}
\label{sec:plan_time}
In order to estimate how computationally expensive it is to run planning with a latent dynamics model,
we evaluate PLDM and GCIQL on 25 episodes in the two-rooms environment. Each episode is 200 steps. 
We record the average time per episode and the standard deviation. 
We do not run HILP, GCBC, or CRL because the resulting policy architecture is the same, making the evaluation time identical to that of GCIQL. HIQL takes more time due to the hierarchy of policies.
The results are shown below:

\begin{table}[H]
    \centering
    \caption{Time of evaluation on one episode in two-rooms environment. PLDM is about 100 times slower than model-free methods. Time is calculated by running on 25 episodes. }
    \begin{tabular}{cc}
         \toprule
         \textbf{Method} & \textbf{Time per episode (seconds)} \\
         \midrule
         PLDM   &  13.44 ± 0.11 \\
         GCIQL  &  0.12 ± 0.03 \\
         HIQL   &  0.16 ± 0.03 \\
         \bottomrule
    \end{tabular}
    \label{tab:plan_time}
\end{table}

\section{Results for Single Maze Setting}
\begin{table}[H]
    \centering
    \caption{Results averaged over 3 seeds ± std}
    \begin{tabular}{cc}
         \toprule
         \textbf{Method} & \textbf{Sucess rate)} \\
         \midrule
         PLDM   &  0.990 ± 0.001 \\
         CRL  &  0.980 ± 0.001 \\
         GCBC  &  0.970 ± 0.024 \\
         GCIQL  &  1.000 ± 0.000 \\
         HIQL   &  1.000 ± 0.000 \\
         HILP   &  1.000 ± 0.000 \\
         \bottomrule
    \end{tabular}
    \label{tab:single_maze_results}
\end{table}

%

\section{Hyperparameters}

\subsection{CRL, GCBC, GCIQL, HIQL, HILP}
Unless listed below, all hyperparameters remain consistent with default values from OGBench and HILP repositories. \footref{ogbench_fn},\footref{hilp_fn}

\subsubsection{Two-Rooms}

For all methods, we used the learning rate of 3e-4. The rest of the hyperparameters were kept default.

\vspace{-10pt}
\begin{table}[H]
    \centering
    \caption{HILP hyperparameters}
    \begin{tabular}{cc}
         \toprule
         \textbf{Hyperparam} & \textbf{Value} \\
         \midrule
         Expectile & 0.7 \\
         Skill expectile & 0.7 \\
         \bottomrule
    \end{tabular}
    \label{tab:HILP_hparams}
\end{table}

\vspace{-20pt}
\begin{table}[H]
    \centering
    \caption{HIQL hyperparameters}
    \begin{tabular}{cc}
         \toprule
         \textbf{Hyperparam} & \textbf{Value} \\
         \midrule
         High-level AWR alpha & 3.0 \\
         Low-level AWR alpha & 3.0 \\
         Expectile & 0.7  \\
         \bottomrule
    \end{tabular}
    \label{tab:HIQL_hparams}
\end{table}

\vspace{-20pt}
\begin{table}[H]
    \centering
    \caption{GCIQL hyperparameters}
    \begin{tabular}{cc}
         \toprule
         \textbf{Hyperparam} & \textbf{Value} \\
         \midrule
         Actor-loss & DDPG-BC \\
         BC coefficient & 0.3 \\
         Expectile & 0.7 \\
         \bottomrule
    \end{tabular}
    \label{tab:GCIQL_hparams}
\end{table}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\subsubsection{Diverse PointMaze}
\begin{table}[H]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
    \caption{Dataset specific hyperparameters of CRL, GCBC, GCIQL, HIQL, HILP for the Diverse PointMaze environment. For HILP, we set the same value for expectile and skill expectile.}
    \begin{tabular}{l|c|c|c c|c c|c c} 
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & 
        \multicolumn{1}{c|}{\textbf{CRL}} & 
        \multicolumn{1}{c|}{\textbf{GCBC}} & 
        \multicolumn{2}{c|}{\textbf{GCIQL}} & 
        \multicolumn{2}{c|}{\textbf{HIQL}} & 
        \multicolumn{2}{c}{\textbf{HILP}} \\
        \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
        & \textbf{LR} & \textbf{LR} & 
        \textbf{LR} & \textbf{Expectile} & \textbf{LR} & \textbf{Expectile} & \textbf{LR} & \textbf{Expectile} \\
        \midrule
        \# map layouts = 5 & 0.0003 & 0.0003 & 0.0002 & 0.8 & 0.0001 & 0.7 & 0.0001 & 0.9 \\
        \# map layouts = 10 & 0.0003 & 0.0001 & 0.0001 & 0.9 & 0.0001 & 0.7 & 0.0001 & 0.9 \\
        \# map layouts = 20 & 0.0003 & 0.0001 & 0.0001 & 0.6 & 0.0003 & 0.7 & 0.0001 & 0.9 \\
        \# map layouts = 40 & 0.0003 & 0.0001 & 0.0003 & 0.9 & 0.0001 & 0.9 & 0.0001 & 0.9 \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_policies_maze}
\end{table}


\subsection{PLDM}

\subsubsection{Two-Rooms}

The best case setting is sequence length = 91, dataset size = 3M, non-random $\%$ = 100, wall crossing $\% \approx 35$. For our experiments we vary each of the above parameters individually.


\vspace{0cm}
\begin{table}[H]
    \centering
    \caption{Dataset-agnostic hyperparameters}
    \begin{tabular}{cc}
         \toprule
         \textbf{Hyperparameter} & \textbf{Value} \\
         \midrule
         Batch Size   &  64 \\
         Predictor Horizon ($H$) & 16 \\
         Optimizer  &  Adam \\
         Scheduler   &  Cosine \\
         MPPI noise $\sigma$ & 5 \\
         MPPI $\#$ samples & 500 \\
         MPPI $\lambda$ & 0.005 \\
         \bottomrule 
    \end{tabular}
    \label{tab:hyperparameters_pldm_wall_2}
\end{table}


For the dataset specific hyperparameters, we tune the following parameters from \Cref{app:collapse_prevention}:

\begin{table}[H]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
    \caption{Dataset specific hyperparameters}
    \begin{tabular}{l|c|c|c|c|c|c} 
        \toprule

        \textbf{Dataset} & \textbf{LR} & \textbf{$\alpha$} & \textbf{$\beta$} & 
        \textbf{$\lambda$} & \textbf{$\delta$} & \textbf{$\omega$}  \\
        \midrule
        Sequence length = 91 & 0.0007 & 4.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        Sequence length = 65 & 0.0003 & 5.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        Sequence length = 33 & 0.0007 & 5.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        Sequence length = 17 & 0.0028 & 3.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        \midrule %
        Dataset size = 634 & 0.0030 & 2.2 & 13.0 & 0.19 & 0.50 & 2.0  \\
        Dataset size = 1269 & 0.0010 & 2.2 & 13.0 & 0.19 & 0.50 & 2.0  \\
        Dataset size = 5078 & 0.0010 & 2.2 & 13.0 & 0.19 & 0.50 & 2.0  \\
        Dataset size = 20312 & 0.0030 & 2.2 & 13.0 & 0.19 & 0.50 & 2.0  \\
        Dataset size = 81250 & 0.0010 & 2.2 & 13.0 & 0.19 & 0.50 & 2.0  \\
        Dataset size = 325k & 0.0010 & 4.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        Dataset size = 1500k & 0.0010 & 4.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        \midrule %
        Non-random \% = 0.001 & 0.0007 & 5.5 & 9.7 & 0.42 & 0.38 & 1.4  \\
        Non-random \% = 0.01 & 0.0007 & 3.9 & 6.5 & 0.27 & 0.19 & 0.6  \\
        Non-random \% = 0.02 & 0.0007 & 3.9 & 6.5 & 0.31 & 0.72 & 0.5  \\
        Non-random \% = 0.04 & 0.0007 & 3.9 & 6.9 & 0.25 & 0.75 & 1.0  \\
        Non-random \% = 0.08 & 0.0007 & 3.9 & 6.5 & 0.31 & 0.24 & 1.5  \\
        Non-random \% = 0.16 & 0.0007 & 3.0 & 6.5 & 0.40 & 0.27 & 1.4  \\
        \midrule %
        Wall crossing \% = 0 & 0.0007 & 4.0 & 6.9 & 0.25 & 0.75 & 1.0  \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_pldm_wall}
\end{table}


\subsubsection{Diverse PointMaze}


\begin{table}[h]
    \centering
    \caption{Dataset-agnostic hyperparameters}
    \begin{tabular}{cc}
         \toprule
         \textbf{Hyperparameter} & \textbf{Value} \\
         \midrule
         Epochs & 5 \\
         Batch Size   &  128 \\
         Predictor Horizon ($H$) & 16 \\
         Optimizer  &  Adam \\
         Scheduler   &  Cosine \\
         MPPI noise $\sigma$ & 5 \\
         MPPI $\#$ samples & 500 \\
         MPPI $\lambda$ & 0.0025 \\
         \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_pldm_wall_2}
\end{table}

\begin{table}[h]
    \centering
    \small
    \renewcommand{\arraystretch}{1.2}
    \caption{Dataset specific hyperparameters}
    \begin{tabular}{l|c|c|c|c|c|c} 
        \toprule

        \textbf{Dataset} & \textbf{LR} & \textbf{$\alpha$} & \textbf{$\beta$} & 
        \textbf{$\lambda$} & \textbf{$\delta$} & \textbf{$\omega$}  \\
        \midrule
        \# map layouts = 5 & 0.04 & 35.0 & 12.0 & 3.0 & 0.1 & 5.4  \\
        \# map layouts = 5 & 0.04 & 35.0 & 12.0 & 3.0 & 0.1 & 5.4  \\
        \# map layouts = 20 & 0.05 & 54.5 & 15.5 & 2.7 & 0.1 & 5.2  \\
        \# map layouts = 40 & 0.05 & 54.5 & 15.5 & 2.7 & 0.1 & 5.2  \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_pldm_maze}
\end{table}

\subsection{Further related work}

\textbf{Offline RL.} aims to learn behaviors purely from offline data without online interactions. There, a big challenge is preventing the policy from selecting trajectories that were not seen in the dataset. CQL \citep{kumar2020conservative} relies on model
conservatism to prevent the learned policy from being overly optimistic about trajectories not observed in the data. IQL \citep{kostrikov2021offline} introduces an objective that avoids evaluating the Q-function on state-action pairs not seen in the data to prevent value overestimation. MOPO \citep{yu2020mopo} is a model-based approach to learning
from offline data, and uses model disagreement to constrain the policy. See \citep{levine2020offline} for a more in-depth survey.

\textbf{Foundation models in RL.} Recently, following the success of NLP, the RL community put a lot of effort into training large sequence models, which sparked dataset collection efforts like Open-X-Embodiment \citep{open_x_embodiment_rt_x_2023} and DROID \citep{khazatsky2024droid}. Large datasets have enabled training models such as RT-2 \citep{brohan2023rt} and Octo \citep{octo_2023}. 
See \citep{yang2023foundation} for a more extensive survey on the topic.

\textbf{Training representations for RL.} Another way to use large amounts of data to improve RL agents is using self-supervised learning (SSL). CURL \citep{laskin2020curl} introduce an SSL objective in addition to the standard RL objectives. Later works also explore 
using a separate pre-training stage \citep{schwarzer2021pretraining, zhang2022light, nair2022r3m}.
\citet{zhou2024dino} show that pre-trained visual representations from DINO \citep{caron2021emerging, oquab2023dinov2} can be used to learn a word model for planning.

\end{document}


%
%
%
%
%
%
%
%
%
%
%
%
%