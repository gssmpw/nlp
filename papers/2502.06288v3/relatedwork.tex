\section{Related Works}
In the early stages, the main line of research on cross-view image matching focused on generating hand-crafted features\cite{manual1,manual2}. However, these methods struggled to bridge the significant differences in point-of-view between ground-level and aerial images. 
Building on the success of deep learning in computer vision tasks, Workman and Jacobs \cite{Jacob} were the first to apply CNN-extracted features to cross-view image matching and retrieval, demonstrating its superiority over traditional hand-crafted features. On the other hand, Verde et al.\cite{9170787} and subsequently, Bonaventura et al.\cite{10167940} created a non-end-to-end deep learning approach based on generating and matching feature graph representations both for ground and aerial-view images. Despite previous research, many works using advanced Deep Learning techniques have emerged, significantly improving performance in this area.

These techniques can be grouped into three main categories: (I) \textit{Siamese-like Networks}, (II) \textit{Generative techniques}, (III) \textit{Vision Transformers}. Using a CNN architecture, Siamese-like networks aim to extract and learn viewpoint-invariant discriminative features from satellite and ground-view images. Moreover, in this category, a subset of works focused on spatially-aware image features\cite{shi2019optimalfeaturetransportcrossview, shi2020ilookingatjoint,pro2024}. More advanced methods have employed generative models, i.e. Generative Adversarial Networks (GANs)\cite{goodfellow2014generativeadversarialnetworks}, to synthesize aerial images from ground-view data and reduce the domain gap between ground and aerial images, introducing also additional information, enhancing the retrieval tasks\cite{regmi2018,deng2019,toker2021}. With the advent of Transformers\cite{vaswani2023attentionneed} and subsequently of Vision Transformers\cite{dosovitskiy2021imageworth16x16words}, the latter has been applied for this task, dividing images into patches and using the attention mechanism to capture relationships. This approach helps the model understand spatial patterns and long-range dependencies, making it well-suited for the task \cite{zhu2022, yang2021, zhang2022}.

%
%The proposed method is built on top of the SAN architecture proposed by \cite{pro2024}. The latter is based on a three-branches architecture not including as input the semantic segmentation mask for the ground-view images. The combination of the extracted aerial feature maps is done by concatenating them on the channel axis.
%
Building upon the Siamese-like Networks family, the proposed approach, which extends the methodology in Pro et al.\cite{pro2024}, integrates additional semantic segmentation masks for ground and satellite images. It introduces an additional feature extractor branch and a new feature combination methodology. Feature maps are first extracted from the ground and satellite-view images and their semantic segmentation masks, then combined to obtain global satellite
features, merging the features from the aerial images and their segmentation masks, and global ground features, combining the features from the ground-view images and their segmentation masks. The two resulting feature maps are then correlated computing a similarity matching score.

Recently, Wang et al.\cite{10.1145/3664647.3681431} exploit the combination of CNNs and Transformers to improve the performances, but opposite to the presented work, without taking into account the generalization on different FoV values. Tackling the problem from a different point of view Fan et al.\cite{s24123719} use images captured from drones instead of ground-view images, like the ones used in the presented work, aiming at geolocate the area depicted in the image taken from a drone through a matching with a satellite image depicting the same location.