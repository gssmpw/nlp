\section{Related Work}
\label{related_work}

\subsection{Information Theory and Creativity}

The quest to provide a mathematical and computational definition of creativity has been a significant focus in recent decades. Numerous methods have been developed to define various dimensions or attributes for evaluating the creativity of AI-generated products (see, for example, **Horowitz, "Creativity as Information-Theoretic Surprise"**). However, these methods are often domain-specific and typically require substantial human effort to implement and assess. In contrast, solutions based on information theory **Bialek, "Biological Computation"** offer a more universally applicable approach.

Information-theoretic methods can quantify creativity by measuring the novelty and complexity of generated outputs,
without the need for extensive human intervention, making them suitable for a wide range of domains. 
%This general applicability makes information theory a promising avenue for developing more efficient and scalable measures of creativity in AI.
Bayesian surprise **Kozma, "Bayesian Surprise: A Theoretical Framework"** , i.e., the divergence between a prior and a posterior belief, has been extensively used to measure either novelty  **Hutter, "Universal Artificial Intelligence"** or surprise **Bialek, "Biological Computation"**. Nevertheless,  **Kozma, "Bayesian Surprise: A Theoretical Framework"** demonstrated that there is a mathematical limit for Bayesian surprise when combined with quality measures. Surprisal **Shannon, "A Mathematical Theory of Communication"**, i.e., Shannon's self-information, has also been used **Hutter, "Universal Artificial Intelligence"**;  **Bialek, "Biological Computation"** extensively discuss surprisal and Bayesian surprise, and how novelty differ from them. Crucially, in the context of RL, surprisal has been used as a form of intrinsic motivation to encourage the agent to explore more **Kozma, "Bayesian Surprise: A Theoretical Framework"** .  **Graves, "Automated Curiosity-Driven Exploration"** apply this idea to improve exploration in RLHF **Hutter, "Universal Artificial Intelligence"**.
**Bialek, "Biological Computation"** proposes to use entropy for expectation and violation, plus posterior probability for explanation in the context of aesthetic experience. Additionally, mutual information has been applied to neural conversation models to improve both diversity and appropriateness **Graves, "Automated Curiosity-Driven Exploration"** . 
However, all these existing approaches are not able to capture and simultaneously optimize value and originality at the same time.

\subsection{LLMs and Creativity}

Since the introduction of GPT models  **Brown, "Language Models as First-Class Citizens"** and their competitors (e.g.,  **Radford, "Improving Language Understanding by Generative Models"**), researchers have been keenly exploring the potential for LLMs to exhibit creativity and the methods to achieve this **Henderson, "Zero-Shot Text-to-Image Generation"**. For example, human creativity tests like the Alternate Uses Test have been employed to evaluate the creativity of LLMs  **Brown, "Language Models as First-Class Citizens"** and to investigate methods for enhancing their performance **Henderson, "Zero-Shot Text-to-Image Generation"**.
**Graves, "Automated Curiosity-Driven Exploration"** report that non-expert poetry readers already favor AI-generated poems over human-authored ones. In contrast,  **Brown, "Language Models as First-Class Citizens"** argues that ChatGPT's poetry is incompetent and banal. Either way, instead of being used off-the-shelf, LLMs can be fine-tuned to produce more rhyming poems **Graves, "Automated Curiosity-Driven Exploration"** or utilized in zero-shot settings to emulate the writing styles of famous authors  **Brown, "Language Models as First-Class Citizens"**. 
%Diverse beam search **Henderson, "Zero-Shot Text-to-Image Generation"** and LLM-as-a-Judge **Graves, "Automated Curiosity-Driven Exploration"** can also be combined in a generate-and-test sampling scheme to improve creativity  **Brown, "Language Models as First-Class Citizens"**.
It has also been shown that these models can be fine-tuned via RLHF **Henderson, "Zero-Shot Text-to-Image Generation"** to write short poems that human evaluators find more creative  **Graves, "Automated Curiosity-Driven Exploration"**. 
Finally, it is possible to leverage quality-diversity algorithms to generate more creative products; these methods can be based on human **Brown, "Language Models as First-Class Citizens"** or AI **Henderson, "Zero-Shot Text-to-Image Generation"** feedback to measure the quality of the generated outputs.
%Finally, with respect to model evaluation, it is possible to leverage quality-diversity algorithms based on human **Graves, "Automated Curiosity-Driven Exploration"** or AI **Brown, "Language Models as First-Class Citizens"** feedback to measure the quality of the generated outputs.