\section{Related Work}
\label{related_work}

\subsection{Information Theory and Creativity}

The quest to provide a mathematical and computational definition of creativity has been a significant focus in recent decades. Numerous methods have been developed to define various dimensions or attributes for evaluating the creativity of AI-generated products (see, for example, \citealp{franceschelli2024creativity}). However, these methods are often domain-specific and typically require substantial human effort to implement and assess. In contrast, solutions based on information theory \cite{shannon1948mathematical, cover1999elements} offer a more universally applicable approach. 

Information-theoretic methods can quantify creativity by measuring the novelty and complexity of generated outputs,
without the need for extensive human intervention, making them suitable for a wide range of domains. 
%This general applicability makes information theory a promising avenue for developing more efficient and scalable measures of creativity in AI.
Bayesian surprise \cite{baldi2010bits}, i.e., the divergence between a prior and a posterior belief, has been extensively used to measure either novelty \cite{franca2016regent,varshney2019big} or surprise \cite{mazzaglia2022curiosity,schmidhuber2010formal}. Nevertheless, \citet{varshney2019mathematical} demonstrated that there is a mathematical limit for Bayesian surprise when combined with quality measures. Surprisal \cite{tribus1961thermodynamics}, i.e., Shannon's self-information, has also been used \cite{bunescu2019learning,fernandezmonsalve2012lexical}; \citet{barto2013novelty} extensively discuss surprisal and Bayesian surprise, and how novelty differ from them. Crucially, in the context of RL, surprisal has been used as a form of intrinsic motivation to encourage the agent to explore more \cite{achiam2017surprise}. \citet{sun2025curiosity} apply this idea to improve exploration in RLHF \cite{christiano2017deep}.
\citet{burns2006atoms} proposes to use entropy for expectation and violation, plus posterior probability for explanation in the context of aesthetic experience. Additionally, mutual information has been applied to neural conversation models to improve both diversity and appropriateness \cite{li2016diversity}. 
However, all these existing approaches are not able to capture and simultaneously optimize value and originality at the same time.

\subsection{LLMs and Creativity}

Since the introduction of GPT models \cite{brown2020language,openai2023gpt4} and their competitors (e.g., \citealp{touvron2023llama}), researchers have been keenly exploring the potential for LLMs to exhibit creativity and the methods to achieve this \cite{franceschelli2023creativity}. For example, human creativity tests like the Alternate Uses Test have been employed to evaluate the creativity of LLMs \cite{stevenson2022putting} and to investigate methods for enhancing their performance \cite{goes2023pushing,summers2023brainstorm}.
\citet{porter2024aigenerated} report that non-expert poetry readers already favor AI-generated poems over human-authored ones. In contrast, \citet{davis2024chatpgt} argues that ChatGPT's poetry is incompetent and banal. Either way, instead of being used off-the-shelf, LLMs can be fine-tuned to produce more rhyming poems \cite{popescu2023gpoet} or utilized in zero-shot settings to emulate the writing styles of famous authors \cite{sawicki2023bits}. 
%Diverse beam search \cite{vijayakumar2018diverse} and LLM-as-a-Judge \cite{zheng2023judging} can also be combined in a generate-and-test sampling scheme to improve creativity \cite{franceschelli2024creative}.\mm{I wonder if we should remove this sentence - we have several references to our own work. Perhaps we can remove this sentence now and then re-add it if the paper is accepted.} 
It has also been shown that these models can be fine-tuned via RLHF \cite{christiano2017deep} to write short poems that human evaluators find more creative \cite{pardinas2023leveraging}. 
Finally, it is possible to leverage quality-diversity algorithms to generate more creative products; these methods can be based on human \cite{li2023quality} or AI \cite{bradley2024quality} feedback to measure the quality of the generated outputs.
%Finally, with respect to model evaluation, it is possible to leverage quality-diversity algorithms based on human \cite{li2023quality} or AI \cite{bradley2024quality} feedback to measure the quality of the generated outputs.