% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% ADDED PACKAGES:
\usepackage[final]{pdfpages}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{array}
\usepackage{rotating}
\usepackage{amsthm}
\usepackage{placeins}
\usepackage[most]{tcolorbox}
\definecolor{bluechart}{rgb}{0.00392156862745098, 0.45098039215686275, 0.6980392156862745}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand{\specialcell}[2][c]{%
    \begin{tabular}[c]{@{}#1@{}}#2\end{tabular}}%

\newtcolorbox{mybox}[1]{
    %sharpish corners, % better drop shadow
    colback = white,
    before skip = 0.5cm,    % add extra space before the box
    after skip = 0.5cm,
    boxrule = 0.5pt,
    %boxsep=1pt,
    left=1pt,
    right=2pt,
    colframe = black,
    rounded corners,
    title=#1
}

%\title{Think Outside the (Grey) Box: A Context-Based Score for Valuable and Original Generation}
\newcommand{\gf}[1]{\footnote{\textbf{Giorgio: #1}}}
\newcommand{\mm}[1]{\footnote{\textbf{Mirco:#1}}}


%\title{Thinking Outside the Box: A Context-Based Score  for \\ Assessing Value and Originality in Neural Text Generation}

\title{Thinking Outside the (Gray) Box: A Context-Based Score for \\ Assessing Value and Originality in Neural Text Generation}

\author{Giorgio Franceschelli \\
  University of Bologna \\
  \texttt{giorgio.franceschelli@unibo.it} \\\And
  Mirco Musolesi \\
  University College London \\
  University of Bologna \\
  \texttt{m.musolesi@ucl.ac.uk} \\}

%\author{
%  \textbf{Giorgio Franceschelli\textsuperscript{1}} and 
%  \textbf{Mirco Musolesi\textsuperscript{2,1}}
%\\
%\\
%  \textsuperscript{1}University of Bologna,
%  \textsuperscript{2}University College London
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:giorgio.franceschelli@unibo.it}{giorgio.franceschelli@unibo.it}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results.
Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.
\end{abstract}

% LIMIT: 8 PAGES (LIMITATIONS, APPENDICES AND REFERENCES EXCLUDED)
\section{Introduction}

Foundation models \cite{bommasani2021opportunities}, particularly large language models (LLMs) \cite{bubeck2023sparks,team2023gemini,touvron2023llama}, are significantly transforming creative activities. They can serve as a foundation for co-creation systems involving human and artificial authors \cite{lin2023prompts}; they can be utilized to generate software code \cite{rozi√®re2024code}; they can even be employed to foster scientific research \cite{boiko2023emergent}. However, the nature of the self-supervised learning algorithms used for the training of these models tend to make them to generate samples as close as possible to the training data distribution \cite{franceschelli2024creativity}.
In addition, fine-tuning, such as that based on reinforcement learning from human feedback (RLHF) \cite{christiano2017deep}, is often necessary to generate appropriate and accurate responses. However, this process tends to further reduce output diversity \cite{kirk2024understanding}, and linguistic creativity tends to be lower than that of humans \cite{lu2024ai}. On the contrary, LLMs for creative tasks should produce more novel and surprising texts that maintain a high level of correctness and adherence to the request. One typical solution is to sample at higher temperature to increase diversity. However, this might lead to generating incoherent text \cite{peeperkorn2024temperature}.

In order to address the issues described above, we propose a new training approach for creative tasks based on \textbf{CoVO}, a \textbf{Co}ntext-based score for \textbf{V}alue and \textbf{O}riginality, with the goal of taking into consideration both value and originality of the neurally-generated text in the optimization of LLMs. The definition of CoVO is grounded in the analysis of mutual information \cite{mackay2003information} between the model's outputs and inputs, and vice versa.
%More specifically, we derive a new optimization problem according to which, given a specific input, the desired output can be derived by simultaneously maximizing the conditional probability under the generative model of the input given the output and minimizing the conditional probability under the generative model of the output given the input. 
More specifically, we formulate a new optimization problem where, given a specific input, the desired output is derived by \textit{simultaneously} maximizing the conditional probability of the input given the output and minimizing the conditional probability of the output given the input under the generative model.
In this way, we optimize for solutions that are appropriate for the input request but also different from the outputs we would normally obtain from the model. In particular, we show that our information-theoretic score can be used as a reward in RL-based fine-tuning algorithms, guiding pre-trained models toward more diverse yet valuable solutions. 

We also present the theoretical foundations of our approach and we discuss several strategies for the practical implementation of the proposed approach and its adaptation to current LLMs. Experiments on math problem solving and poetry generation show that our score correlates with domain-specific measures for value and originality, and that our approach can increase the quality and diversity of outputs, presenting itself as a candidate for creativity applications of current foundation models.

The remainder of the paper is structured as follows. First, we provide an overview of the state of the art in this area (Section \ref{related_work}) and introduce the key concepts and background of our work (Section \ref{preliminaries}). Then, we discuss the definition of CoVO (Section \ref{creativity_score}) and show different ways to use it in practice as a reward for fine-tuning autoregressive models based on RL (Section \ref{method}). We validate the proposed solutions on poetry generation and math problem solving (Section \ref{experiments}). Finally, we discuss future work and limitations of our approach (Sections \ref{conclusion} and \ref{limitations}).

\section{Related Work} \label{related_work}

\subsection{Information Theory and Creativity}

The quest to provide a mathematical and computational definition of creativity has been a significant focus in recent decades. Numerous methods have been developed to define various dimensions or attributes for evaluating the creativity of AI-generated products (see, for example, \citealp{franceschelli2024creativity}). However, these methods are often domain-specific and typically require substantial human effort to implement and assess. In contrast, solutions based on information theory \cite{shannon1948mathematical, cover1999elements} offer a more universally applicable approach. 

Information-theoretic methods can quantify creativity by measuring the novelty and complexity of generated outputs,
without the need for extensive human intervention, making them suitable for a wide range of domains. 
%This general applicability makes information theory a promising avenue for developing more efficient and scalable measures of creativity in AI.
Bayesian surprise \cite{baldi2010bits}, i.e., the divergence between a prior and a posterior belief, has been extensively used to measure either novelty \cite{franca2016regent,varshney2019big} or surprise \cite{mazzaglia2022curiosity,schmidhuber2010formal}. Nevertheless, \citet{varshney2019mathematical} demonstrated that there is a mathematical limit for Bayesian surprise when combined with quality measures. Surprisal \cite{tribus1961thermodynamics}, i.e., Shannon's self-information, has also been used \cite{bunescu2019learning,fernandezmonsalve2012lexical}; \citet{barto2013novelty} extensively discuss surprisal and Bayesian surprise, and how novelty differ from them. Crucially, in the context of RL, surprisal has been used as a form of intrinsic motivation to encourage the agent to explore more \cite{achiam2017surprise}. \citet{sun2025curiosity} apply this idea to improve exploration in RLHF \cite{christiano2017deep}.
\citet{burns2006atoms} proposes to use entropy for expectation and violation, plus posterior probability for explanation in the context of aesthetic experience. Additionally, mutual information has been applied to neural conversation models to improve both diversity and appropriateness \cite{li2016diversity}. 
However, all these existing approaches are not able to capture and simultaneously optimize value and originality at the same time.

\subsection{LLMs and Creativity}

Since the introduction of GPT models \cite{brown2020language,openai2023gpt4} and their competitors (e.g., \citealp{touvron2023llama}), researchers have been keenly exploring the potential for LLMs to exhibit creativity and the methods to achieve this \cite{franceschelli2023creativity}. For example, human creativity tests like the Alternate Uses Test have been employed to evaluate the creativity of LLMs \cite{stevenson2022putting} and to investigate methods for enhancing their performance \cite{goes2023pushing,summers2023brainstorm}.
\citet{porter2024aigenerated} report that non-expert poetry readers already favor AI-generated poems over human-authored ones. In contrast, \citet{davis2024chatpgt} argues that ChatGPT's poetry is incompetent and banal. Either way, instead of being used off-the-shelf, LLMs can be fine-tuned to produce more rhyming poems \cite{popescu2023gpoet} or utilized in zero-shot settings to emulate the writing styles of famous authors \cite{sawicki2023bits}. 
%Diverse beam search \cite{vijayakumar2018diverse} and LLM-as-a-Judge \cite{zheng2023judging} can also be combined in a generate-and-test sampling scheme to improve creativity \cite{franceschelli2024creative}.\mm{I wonder if we should remove this sentence - we have several references to our own work. Perhaps we can remove this sentence now and then re-add it if the paper is accepted.} 
It has also been shown that these models can be fine-tuned via RLHF \cite{christiano2017deep} to write short poems that human evaluators find more creative \cite{pardinas2023leveraging}. 
Finally, it is possible to leverage quality-diversity algorithms to generate more creative products; these methods can be based on human \cite{li2023quality} or AI \cite{bradley2024quality} feedback to measure the quality of the generated outputs.
%Finally, with respect to model evaluation, it is possible to leverage quality-diversity algorithms based on human \cite{li2023quality} or AI \cite{bradley2024quality} feedback to measure the quality of the generated outputs.

\section{Preliminaries} \label{preliminaries}

\subsection{Language Modeling} \label{preliminaries_llms}

A $\boldsymbol{\theta}$-parameterized autoregressive language model is a probability distribution $p_{\boldsymbol{\theta}}(\mathbf{x})$ over a variable-length text sequence $\mathbf{x} = (x_1 \ldots x_T)$, where $T$ is the sequence length and each token $x_t$ is in a finite vocabulary $\mathcal{V}$ of size $N$. The probability distribution is factorized as $p_{\boldsymbol{\theta}}(\mathbf{x}) = \prod_{t=1}^T p_{\boldsymbol{\theta}}(x_t | \mathbf{x_{<t}})$, where $\mathbf{x_{<t}} = x_1 \ldots x_{t-1}$. The language model is usually trained to maximize the likelihood of the true distribution $p^*(\mathbf{x})$ for any $\mathbf{x}$ from a reference dataset (the training set). In other words, given an input $\mathbf{x_{<t}}$, the model learns to approximate the probability of each token from $\mathcal{V}$ being $x_{t}$. While this makes such a model immediately capable of scoring the probability of a given text, it also allows for the generation of new sentences. Given a conditional input (prompt) $\mathbf{z} = (z_1 \ldots z_L)$, we can decode $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})$ as the continuation of $\mathbf{z}$, i.e., through the factorized representation $p_{\boldsymbol{\theta}}(\mathbf{x} | \mathbf{z}) = \prod_{t=1}^T p_{\boldsymbol{\theta}}(x_t | \mathbf{x_{<t}}, \mathbf{z})$.

\subsection{Reinforcement Learning for Language Models} \label{preliminaries_rl}

Due to its adherence to the formal framework of Markov decision processes \cite{sutton2018reinforcement}, RL can be used as a solution to the generative modeling problem in the case of autoregressive tasks such as text generation \cite{bachman2015data}. 
The LLM plays the role of the agent, and each generated token represents an action $a_t$. The current version of the generated output $\mathbf{x_t}$ is part of the state $\mathbf{s_t}$ (potentially with additional information such as initial prompts). Finally, the reward $r_{t+1}$ measures the ``quality'' of the current output. A common strategy is to assign a zero reward for each $\mathbf{x}_t, t \ne T$ and a sentence-based reward when the final output is generated. Within this framework, any policy-based method, such as Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, can be employed to train or fine-tune the LLM to optimize a given objective.
Indeed, RL facilitates the use of non-differentiable reward functions, enabling the optimization of test-time metrics, domain-specific targets, and human preferences \cite{franceschelli2024reinforcement}. To avoid deviating too much from the pre-trained model while also encouraging exploration, the full reward function usually considers a Kullback-Leibler (KL) divergence term as follows \cite{stiennon2020learning}:

{\small
\begin{equation}
    R(\mathbf{z}, \mathbf{x}) = r(\mathbf{z}, \mathbf{x}) - \beta \log\frac{p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})}{p_{\boldsymbol{\theta}^{PT}}(\mathbf{x}| \mathbf{z})},
\end{equation}
}

\noindent where $\boldsymbol{\theta}^{PT}$ is the set of parameters of the pre-trained model, $\beta$ weighs the KL penalty, and $r(\mathbf{z}, \mathbf{x})$ can be any non-differentiable function to score the model's output.

In particular, RLHF \cite{christiano2017deep} has been widely adopted as the final training stage in various popular LLMs to align them with human preferences, i.e., by using a learned reward function that approximates human feedback. While highly effective, RLHF suffers from several open problems \cite{casper2023open}. These include obtaining accurate, unbiased, and representative human feedback, as well as making the process resource-efficient. Moreover, RLHF tends to be complex and unstable due to the additional requirement of training a reward model and its reliance on RL. To address these challenges, Direct Preference Optimization (DPO) \cite{rafailov2023direct} directly optimizes the language model without the need for explicit reward modeling or RL. DPO implicitly performs reward maximization with a KL-divergence penalty through the following loss:
%

{\small
\begin{equation} \label{eq:dpo}
\begin{aligned}
    \mathcal{L}_{DPO} = \min (- \mathbb{E}_{(\mathbf{z}, \mathbf{x_w}, \mathbf{x_l}) \sim X} [ \log \sigma( &\beta \log \frac{p_{\boldsymbol{\theta}}(\mathbf{x_w}|\mathbf{z})}{p_{\boldsymbol{\theta}^{PT}}(\mathbf{x_w}|\mathbf{z})}\\ - &\beta \log \frac{p_{\boldsymbol{\theta}}(\mathbf{x_l}|\mathbf{z})}{p_{\boldsymbol{\theta}^{PT}}(\mathbf{x_l}|\mathbf{z})}) ])
\end{aligned}
\end{equation}
}

\noindent where $\sigma$ is the sigmoid function, $\beta$ is a parameter controlling the deviation from the base model, and $\mathbf{x_w}, \mathbf{x_l}$ are the chosen and rejected outputs, respectively. In other words, DPO fits an implicit reward whose optimal policy is the language model itself.

\section{A Context-Based Score for Valuable and Original Generation} \label{creativity_score}

Our goal is to derive a score that is able to quantify both value and originality at the same time.
As discussed in depth by \citet{csikszentmihalyi2014society}, creativity depends on the context in which the product is created, as the context provides the task identification and the domain information necessary to generate and validate the outcome. In turn, the output aims to solve the given task and provide a meaningful, original contribution to the current domain. Thus, our proposed score has its roots in mutual information, which represents a quantitative way to study the relationship between contextual, prior information and a produced posterior outcome. More specifically, we start from the (point-wise) mutual information between two variables $x$ and $y$:
%

{\small
\begin{equation}
    I(x, y) = h(x) - h(x|y) = h(y) - h(y|x)
\end{equation}
}

%
\noindent where the entropy is $h(a) = - \log p(a)$, therefore:
%

{\small
\begin{equation}
\begin{aligned}
    I(x, y) &= \log p(x|y) - \log p(x)\\ &= \log p(y|x) - \log p(y).
\end{aligned}
\end{equation}
}
%
%\noindent Let us now call $x=S$ (for \textit{source}, i.e., our context) and $y=T$ (for \textit{target}, i.e., our product):
Let us now assume $x$ to be our input vector $\mathbf{x}$ and $y$ our output vector $\mathbf{y}$, obtaining:
%

{\small
\begin{equation}
    I(\mathbf{x},\mathbf{y}) = \log p(\mathbf{y}|\mathbf{x}) - \log p(\mathbf{y}).
\end{equation}
}

%
We can generalize $I(\mathbf{x},\mathbf{y})$ with two scaling factors:
%

{\small
\begin{equation}
    I(\mathbf{x},\mathbf{y},\lambda_1,\lambda_2) = \lambda_1 \log p(\mathbf{y}|\mathbf{x}) - \lambda_2 \log p(\mathbf{y}),
\end{equation}
}

%
\noindent where $I(\mathbf{x},\mathbf{y})$ is just $I(\mathbf{x},\mathbf{y},1,1)$.
By applying the Bayes theorem, i.e., $\log p(a|b) = \log p(b|a) + \log p(a) - \log p(b)$, we can substitute the $\log p(\mathbf{y})$ term as follows:

{\small
\begin{equation}
    \begin{alignedat}{2}
    I(\mathbf{x},\mathbf{y},\lambda_1,\lambda_2) &= & &\lambda_1 \log p(\mathbf{y}|\mathbf{x}) - \lambda_2 \log p(\mathbf{y}|\mathbf{x}) -\\&   & &\lambda_2 \log p(\mathbf{x}) + \lambda_2 \log p(\mathbf{x}|\mathbf{y})\\ &= & &(\lambda_1 - \lambda_2) \log p(\mathbf{y}|\mathbf{x}) +\\&   & &\lambda_2 \log p(\mathbf{x}|\mathbf{y}) - \lambda_2 \log p(\mathbf{x}).
    \end{alignedat}
\end{equation}
}

%
%
%
\noindent Since our goal is to find the optimal $\mathbf{y}$ for a given $\mathbf{x}$, the last term can be ignored. Moreover, we now define $\lambda_v = \lambda_2$ and $\lambda_o = \lambda_2 - \lambda_1$, thus obtaining the following objective:
%
%
%

{\small
\begin{equation} \label{eq:score_max_problem}
    \overline{\mathbf{y}} = \underset{\mathbf{y}}{\text{argmax}} (\lambda_v \log p(\mathbf{x}|\mathbf{y}) - \lambda_o \log p(\mathbf{y}|\mathbf{x})).
\end{equation}
}

%
%
Let us now consider the case where $\lambda_v, \lambda_o > 0$, for example, $\lambda_v = \lambda_o = 1$. Solving this maximization problem involves finding the target $\mathbf{y}$ that maximizes the posterior probability of $\mathbf{x}$ while also being unlikely given $\mathbf{x}$. In other words, the optimal $\mathbf{y}$ must be unexpected and diverse from $p(\mathbf{y}|\mathbf{x})$, but it must also be explainable by $\mathbf{x}$. While $- \log p(\mathbf{y}|\mathbf{x})$, commonly known as surprisal \cite{tribus1961thermodynamics}, is widely used to measure diversity and surprise \cite{barto2013novelty}, the other term, $\log p(\mathbf{x}|\mathbf{y})$, can be used to measure value and effectiveness. Indeed, if the request (e.g., a problem or a task) can be easily predicted from the outcome, the outcome must be a (good) example of that task or a correct solution for that problem.
While other prominent definitions such as Boden's \cite{boden2003creative} seek to include a third requirement for creativity, i.e., novelty, we believe that by considering only a specific context, novelty and surprise become indistinguishable. In particular, being novel in a specific context $\mathbf{x}$ means doing something different from what has been previously experienced under $\mathbf{x}$ by the creator. 
However, since a self-supervised-trained model can be seen as a compression of training data \cite{franceschelli2024training}, $- \log P(\mathbf{y}|\mathbf{x})$ becomes not only a measure of unexpectedness but also of novelty. Therefore, we choose to refer to this term as \textit{originality}, as it encompasses both novelty and surprise.
On the other hand, $\log P(S|T)$ is less ambiguous and maps directly into \textit{value}.

We now provide a formal definition of our proposed score. In summary, the \textbf{CoVO} (\textbf{Co}ntext-based \textbf{V}alue and \textbf{O}riginality) score for a target $\mathbf{y}$ given a source $\mathbf{x}$ on a reference probability distribution $p$ is defined as:
%
%\begin{equation} \label{eq:covo_basic_score}
%    s_{CoVO}(S,T, p) = \underbrace{\lambda_v \log p(S|T)}_\textrm{Value} \underbrace{- \lambda_o \log p(T|S)}_\textrm{Originality}.
%\end{equation}
%
%\begin{tcolorbox}[colback=white,colframe=black,boxrule = 1.5pt,]
%{\small
%\begin{equation} \label{eq:covo_basic_score}
%    s_{CoVO}(\mathbf{x},\mathbf{y},p) = \underbrace{\lambda_v \log p(\mathbf{x}|\mathbf{y})}_\textrm{Value} \underbrace{- \lambda_o \log p(\mathbf{y}|\mathbf{x})}_\textrm{Originality}
%\end{equation}
%}
%\end{tcolorbox}
%
\begin{mybox}{CoVO Score}
{\small
\begin{equation} \label{eq:covo_basic_score}
    s_{CoVO}(\mathbf{x},\mathbf{y},p) = \underbrace{\lambda_v \log p(\mathbf{x}|\mathbf{y})}_\textrm{Value} \underbrace{- \lambda_o \log p(\mathbf{y}|\mathbf{x})}_\textrm{Originality}
\end{equation}
}
\end{mybox}
%
%
\medskip
\section{Implementation and Optimization with Autoregressive Models} \label{method}
We now discuss the implementation of the CoVO score with autoregressive models.
Using the notation introduced in Section \ref{preliminaries_llms}, in the context of a $\boldsymbol{\theta}$-parameterized LLM, $p(\mathbf{y}|\mathbf{x})$ can be expressed as $\prod_{t=1}^T p_{\boldsymbol{\theta}}(y_t|\mathbf{y}_{<t},\mathbf{x})$. However, considering just the product of all the conditioned probabilities for an optimization problem would lead to preferring shorter sequences. To avoid this, we propose to use the $T$-th root: $\sqrt[T]{\prod_{t=1}^T p_{\boldsymbol{\theta}}(y_t|\mathbf{y}_{<t},\mathbf{x})}$. By leveraging the properties of the logarithm, we obtain:


{\small
\begin{equation}
\begin{aligned}
    s_{CoVO}^{AR} &= \lambda_v \frac{\sum_{i=1}^{|\mathbf{x}|} \log p_{\boldsymbol{\theta}}(x_i|\mathbf{x}_{<i},\mathbf{y})}{|\mathbf{x}|} \\ &- \lambda_o \frac{\sum_{j=1}^{|\mathbf{y}|} \log p_{\boldsymbol{\theta}}(y_j|\mathbf{y}_{<j},\mathbf{x})}{|\mathbf{y}|}.
    \end{aligned}
\end{equation}
}


It is worth noting that the vocabulary of an LLM can be extremely large, which can cause $p_{\boldsymbol{\theta}}(a|b)$ to be small even when $a$ is the most probable event given $b$. In particular, when an LLM generates $\mathbf{y}$ given $\mathbf{x}$ and then evaluates both $p_{\boldsymbol{\theta}}(\mathbf{y}|\mathbf{x})$ and $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{y})$, this can lead to a significant discrepancy between the magnitude of value and diversity. Since $\mathbf{y}$ has been sampled from $p_{\boldsymbol{\theta}}$, its probability would be high by definition. However, there may be various ways (possibly through synonyms) to define $\mathbf{y}$, leading to a smaller probability of $\mathbf{x}$.

Inspired by \citet{macedo2004modeling}, we propose to normalize $p_{\boldsymbol{\theta}}(a|b)$ via $n' = \frac{n - n_{min}}{n_{max} - n_{min}}$. For probabilities, $n_{min} = 0$, while $n_{max} = \max p_{\boldsymbol{\theta}}(b)$, thus obtaining the overall mapping for $p_{\boldsymbol{\theta}}$: $\frac{p_{\boldsymbol{\theta}}(y_t|\mathbf{y}_{<t},\mathbf{x})}{\max p_{\boldsymbol{\theta}}(\mathbf{y}_{<t},\mathbf{x})}$. Once again, by applying the properties of logarithms, we obtain:
%

\begin{mybox}{CoVO Score with Autoregressive Models}
{\small
\begin{equation} \label{eq:covo_advanced_score}
\begin{aligned}
    &s_{CoVO}^{AR_{norm}}(\mathbf{x},\mathbf{y},p_{\boldsymbol{\theta}}) = \lambda_v s_v(\mathbf{x}, \mathbf{y},p_{\boldsymbol{\theta}}) + \lambda_o s_o(\mathbf{x},\mathbf{y},p_{\boldsymbol{\theta}}) = \\
    &\lambda_v \frac{\sum_{i=1}^{|\mathbf{x}|} (\log p_{\boldsymbol{\theta}}(x_i|\mathbf{x}_{<i},\mathbf{y}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{x}_{<i},\mathbf{y}))}{|\mathbf{x}|} -\\ 
    &\lambda_o \frac{\sum_{j=1}^{|\mathbf{y}|} (\log p_{\boldsymbol{\theta}}(y_j|\mathbf{y}_{<j},\mathbf{x}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{y}_{<j},\mathbf{x}))}{|\mathbf{y}|}.
\end{aligned}
\end{equation}
}
\end{mybox}





%{\small
%\begin{equation} \label{eq:covo_advanced_score}
%\begin{aligned}
%    &s_{CoVO}^{AR_{norm}}(\mathbf{x},\mathbf{y},p_{\boldsymbol{\theta}}) = \lambda_v s_v(\mathbf{x}, \mathbf{y},p_{\boldsymbol{\theta}}) + \lambda_o s_o(\mathbf{x},\mathbf{y},p_{\boldsymbol{\theta}}) = \\
%    &\lambda_v \frac{\sum_{i=1}^{|\mathbf{x}|} (\log p_{\boldsymbol{\theta}}(x_i|\mathbf{x}_{<i},\mathbf{y}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{x}_{<i},\mathbf{y}))}{|\mathbf{x}|} -\\ 
%    &\lambda_o \frac{\sum_{j=1}^{|\mathbf{y}|} (\log p_{\boldsymbol{\theta}}(y_j|\mathbf{y}_{<j},\mathbf{x}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{y}_{<j},\mathbf{x}))}{|\mathbf{y}|}.
%\end{aligned}
%\end{equation}
%}

%{\small
%\begin{equation} \label{eq:covo_advanced_score}
%\begin{aligned}
%    &s_{CoVO}^{AR_{norm}}(\mathbf{x},\mathbf{y},p_{\boldsymbol{\theta}}) = \lambda_v s_v(\mathbf{x}, \mathbf{y},p_{\boldsymbol{\theta}}) + \lambda_o s_o(\mathbf{x},\mathbf{y},p_{\boldsymbol{\boldsymbol{\theta}}}) = \\
%    &s_v(\mathbf{x}, \mathbf{y}, p_{\boldsymbol{\theta}}) = \frac{\sum_{i=1}^{|\mathbf{x}|} (\log p_{\boldsymbol{\theta}}(x_i|\mathbf{x}_{<i},\mathbf{y}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{x}_{<i},\mathbf{y}))}{|\mathbf{x}|},\\ 
%    &s_o(\mathbf{x}, \mathbf{y}, p_{\boldsymbol{\theta}}) = \frac{\sum_{j=1}^{|\mathbf{y}|} (\log p_{\boldsymbol{\theta}}(y_j|\mathbf{y}_{<j},\mathbf{x}) - \max \log p_{\boldsymbol{\theta}}(\mathbf{y}_{<j},\mathbf{x}))}{|\mathbf{y}|}.
%\end{aligned}
%\end{equation}
%}
\bigskip
Despite these adjustments, the two components of our score still exhibit fundamental differences in the magnitude of variations. For instance, given the same source $\mathbf{x}$, small variations in the target $\mathbf{y}$ result in larger fluctuations in the originality component compared to the value component. To address this issue, we propose to normalize the two parts independently. We implement and test two distinct methodologies: \textit{single-batch normalization}, which standardizes each component using its mean and standard deviation within a single batch; and \textit{full-training normalization}, which standardizes each component using the mean and standard deviation calculated over the entire training period, maintaining running statistics across batches.

Calculating $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{y})$ is not trivial. Since LLMs are trained to complete text sequences, it is unlikely that they would generate the source text immediately after the target text (which, we should remember, is generated immediately after the source text). To address this, we consider an approximation $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{y}')$, where $\mathbf{y}' = \mathbf{y} + \mathbf{q}$. Here, $\mathbf{q}$ represents an additional question, such as ``How would you describe this text?'' or a similar formulation designed solely to increase the likelihood of generating the source text $\mathbf{x}$ (as well as alternative sources). Once the CoVO score has been defined, its adoption in an RL framework is straightforward. As introduced in Section \ref{preliminaries_rl}, we can employ our CoVO score as the final reward for the generated sequence. Then, the model can be trained with any policy gradient method. Our experiments leverage PPO \cite{schulman2017proximal}, which is a popular choice for training language models.

Finally, we propose an alternative method to optimize agents based on our CoVO score. While the normalization schemes mentioned earlier should help balance the value and originality components, finding the ideal reward formulation that effectively addresses both aspects is challenging. In addition, RL tends to have slower convergence. For these reasons, we propose to use DPO \cite{rafailov2023direct} as an alternative solution. An intuitive implementation would consider the CoVO score from Equation \ref{eq:covo_advanced_score} to rank the generated outputs for a given input. However, as already discussed, the CoVO score is made of two distinct parts that might be problematic to optimize at the same time by simply summing them up. Therefore, we propose to build two distinct rankings, one on the value component, and one on the originality component. Then, the two rankings are merged together, and the best-ranked output is the chosen one (i.e., the one whose probability will be maximized), while the worst-ranked is the rejected one (i.e., the one whose probability will be minimized). Once a batch of chosen-rejected pairs has been collected, the model can be optimized via Equation \ref{eq:dpo}. Algorithm \ref{alg:dpo_ranking} summarizes the proposed training process. 

\begin{algorithm}[t]
\caption{DPO train step toward CoVO Score}\label{alg:dpo_ranking}
\begin{algorithmic}
\State \textbf{Require} $p_{\boldsymbol{\theta}}$ reference LM, $p_{\boldsymbol{\theta}'}$ LM to be trained, $\mathcal{B} = \{\mathbf{x}_b\}_{b=1}^B$ batch of training prompts, $K$ candidates to generate, $\mathtt{get\_ext\_reward}$ function to include potential task-specific rewards.
\For{$b = 1 \ldots B$}
\For{$k = 1 \ldots K$}
\State $\mathbf{y}_k \sim p_{\boldsymbol{\theta}'}(\mathbf{x}_b)$
\State $\mathsf{value}_k = s_v(\mathbf{x}_b, \mathbf{y}_k, p_{\boldsymbol{\theta}})$
\State $\mathsf{orig}_k = s_o(\mathbf{x}_b, \mathbf{y}_k, p_{\boldsymbol{\theta}})$
\State $\mathsf{ext\_reward}_k = \mathtt{get\_ext\_reward}(\mathbf{y}_k)$
\EndFor
\State $\mathsf{new\_scores} = 0 \ldots K-1$
\State $\_, \mathsf{ids} = \mathtt{sort}(\mathsf{value})$
\State $\mathsf{value} = \mathtt{sort\_by\_idx}(\mathsf{new\_scores}, \mathsf{ids})$
\State $\_, \mathsf{ids} = \mathtt{sort}(\mathsf{orig})$
\State $\mathsf{orig} = \mathtt{sort\_by\_idx}(\mathsf{new\_scores}, \mathsf{ids})$
\State $\mathsf{score} = \mathsf{value} + \mathsf{orig}$
\State $\mathsf{score} = \mathsf{score} + \mathsf{ext\_reward}$
\State $\mathsf{chosen}_b = \mathbf{y_{\mathtt{argmax}(\mathsf{score})}}$
\State $\mathsf{rejected}_b = \mathbf{y_{\mathtt{argmin}(\mathsf{score})}}$
\EndFor
\State Train $p_{\boldsymbol{\theta}'}$ via Equation \ref{eq:dpo} on $\mathsf{chosen}$ and $\mathsf{rejected}$.
\end{algorithmic}
\end{algorithm}

\section{Experiments} \label{experiments}

We evaluate the effectiveness of our RL strategy through two case studies: poetry generation (Section \ref{exp_poetry_generation}) and mathematical problem resolution (Section \ref{exp_math_resolution})\footnote{The code for the experiments will be made available with the camera-ready version of the paper if accepted.}.

%\begin{figure*}[th]
%    \centering
%    \includegraphics[width=\textwidth]{images/qualitative_results.pdf}
%    \caption{Distribution of scores obtained by the different models according to the poetry experts. The first row includes scores for poems generated with training tone-style pairs, while the second row includes scores for poems generated with out-of-distribution tone-style pairs.}
%    \label{fig:qualitative_results}
%\end{figure*}

\subsection{Poetry Generation} \label{exp_poetry_generation}

\begin{table*}[ht!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|ccc|ccc|} 
 \hline
 Method & \multicolumn{3}{c|}{In-distribution} & \multicolumn{3}{c|}{Out-of-distribution} \\
 \hline
 Llama3-  & Correctness $\uparrow$ & Metric (L/S) $\uparrow$ & T-LCS (avg/max) $\downarrow$ & Correctness $\uparrow$ & Metric (L/S) $\uparrow$ & T-LCS (avg/max) $\downarrow$ \\
 \hline
 Baseline & $\mathbf{1.00}$ & $\mathbf{0.60}$ / $\mathbf{0.60}$ & $8.0$ / $\underline{57}$ & $\mathbf{1.00}$ & $0.33$ / $\underline{0.30}$ & $5.0$ / $8$\textcolor{white}{0} \\ 
 CoVO     & $\mathbf{1.00}$ & $\mathbf{0.60}$ / $\mathbf{0.60}$ & $\underline{9.9}$ / $49$ & $\mathbf{1.00}$ & $\mathbf{0.53}$ / $\underline{0.30}$ & $7.2$ / $\underline{41}$ \\ 
 CoVO-std & $\underline{0.76}$ & $\underline{0.20}$ / $\underline{0.33}$ & $5.8$ / $19$ & $\underline{0.40}$ & $\underline{0.27}$ / $\mathbf{0.57}$ & $\mathbf{4.8}$ / $\mathbf{6}$\textcolor{white}{0} \\ 
 CoVO-run & $0.80$ & $0.40$ / $0.57$ & $\mathbf{4.9}$ / $\mathbf{7}$\textcolor{white}{0} & $0.60$ & $0.47$ / $0.40$ & $\mathbf{4.8}$ / $7$\textcolor{white}{0} \\ 
 CoVO-dpo & $0.96$ & $0.50$ / $0.50$ & $6.1$ / $14$ & $0.92$ & $0.47$ / $0.40$ & $\underline{7.8}$ / $40$ \\ 
 \hline
\end{tabular}
}
\caption{Aggregate results of CoVO scores at inference time considering both training prompts (left) and testing prompts (right). Scores on the poetical metrics are reported at the line level (L) and syllable level (S) and only consider requests for styles with specific metrical properties. The best scores are highlighted in \textbf{bold}, while the worst scores are indicated with \underline{underlining}.\label{poetry_generation_avg_score}}
\end{table*}

\subsubsection{Experimental Setup}
The first set of experiments aims to teach the LLM to generate poems that are both more original and valuable.
More specifically, we follow the approach outlined by \citet{bradley2024quality} and instruct the model to write a poem in a particular style and tone. We consider the Llama3-8B model \cite{dubey2024llama} as our pre-trained agent. Since we do not use the instruction-tuned model, we prompt it with some few-shot examples of the task to make it more likely to produce the desired output in the desired form (see the full prompt in Appendix \ref{implementation_details}). Instead of fine-tuning the entire network, we consider Low-Rank Adaptation (LoRA) \cite{hu2022lora}.
%In addition to saving compute resources, LoRA allows us to preserve the information already stored in the model. Indeed, the idea is to learn how to adapt the known information for more creative purposes. 
The original model is also used to compute the score. We experiment with various settings, i.e., with Equation \ref{eq:covo_advanced_score} and $\lambda_v = \lambda_o = 1.0$ (hereinafter Llama3-CoVO); by standardizing the two score portions separately at the batch level (Llama3-CoVO-std); by standardizing them using running statistics (Llama3-CoVO-run); and by using the DPO adaptation through Algorithm \ref{alg:dpo_ranking} (Llama3-CoVO-dpo). Due to additional resource consumption, the DPO adaptation considers a 4-bit quantization of the model \cite{dettmers2023case}. The full training parameters are reported in Appendix \ref{implementation_details}. We consider the pre-trained version as our baseline, and we perform 
a quantitative evaluation. In particular, we compute 
%both qualitative and quantitative evaluations. The former includes domain experts, i.e., poets and academic professors, and we adopt the Consensual Assessment Technique (CAT) \cite{amabile1982social}; see Appendix \ref{CAT} for more details. In addition, we quantitatively validate our methods by computing 
poetical metrics for quality (lexical correctness of poems and adherence to line- and syllable-level constraints) and for originality (accidental reproduction of existing poems). For the latter, we define a Token-based Longest Common Substring (T-LCS) score, and we use it by comparing generated poems with a reference dataset of approx. 84k public-domain poems extracted from Project Gutenberg (see Appendix \ref{gutenverse} for a first presentation of our GutenVerse dataset). While a generated poem can be an accidental reproduction of a protected work or a different kind of text (e.g., a song), we believe it can provide a useful evaluation tool to understand the general degree of originality.

%\subsubsection{Qualitative Experimental Results}
%We evaluate the five models (hereinafter Llama3-Baseline for the original pre-trained model, Llama3-CoVO for the fine-tuned model optimizing CoVO score, Llama3-CoVO-std for the model with batch-level standardization, Llama3-CoVO-run for the model with running stats normalization, and Llama3-CoVO-dpo for the model tuned with DPO) by prompting them to generate the 25 possible poems arising by the tone-style pairs used during training and the 25 possible poems from different tone-style pairs (reported in Appendix \ref{implementation_details}).
%We asked ten experts in poetry to order the five generated poems with the same style and tone from the most to the least creative. Each expert was presented with a total of ten questions; thus, we collected 2 scores for each poem and 100 for each model. Figure \ref{fig:qualitative_results} reports the distribution of results for each model.
%
%Overall, the scores are quite similar across all models (the 95\% confidence intervals are all overlapped), with few exceptions. For training tone-style pairs, the baseline model collected more preferences, while CoVO-dpo was often ranked among the worst models. Interestingly, CoVO-std was either among the best or the worst, while CoVO and CoVO-run are well-balanced. On the contrary, for out-of-distribution tone-style pairs, all models collected balanced results, except for the baseline (getting higher scores) and CoVO-dpo, which was usually considered among the best two models or the worst.
%
%Finally, it is important to note that the experts ordered the poems with the same tone and style in the same way only in 2 cases out of 50. Moreover, only 22\% of poems received the same score twice, while over 8\% of poems were rated both a 1 and a 5, being judged as the best and the worst by two different experts. This strong disagreement among experts remarks on the difficulty of qualitatively evaluating potentially creative products, and how much the subjective experience can vary across different audiences.

%\hfill

%\subsubsection{Quantitative Experimental Results}\mm{chage name if the quant eval is removed to "Experimental Results"}
\subsubsection{Experimental Results}
%Due to the poor statistical significance of the qualitative evaluation, we also compute more quantitative metrics about the compliance of poetical constraints at the syllable and line levels, lexical correctness (as the ratio of poems only containing correct words), and accidental reproduction rate (as the mean and maximum token-based longest common substring).\mm{change if part before is removed} Table \ref{poetry_generation_avg_score} reports all the scores.
Table \ref{poetry_generation_avg_score} reports the scores about the compliance of poetical constraints at the syllable and line levels, lexical correctness (as the ratio of poems not containing noisy text), and accidental reproduction rate (as the mean and maximum token-based longest common substring).

As expected, different training strategies have different effects on the aforementioned metrics. In particular, using the CoVO score as-is leads to behavior close to the baseline for training tone-style pairs, while it increases metric adherence but also verbatim reproduction for out-of-distribution pairs. Llama3-CoVO-dpo has similar but less extreme results, without excelling or failing in any metric. On the other hand, standardizing the CoVO score components places more importance on exploration: both Llama3-CoVO-std and Llama3-CoVO-run tend to produce more noisy outputs but without any relevant accidental reproduction.

Interestingly, these considerations align well with our CoVO score. Figure \ref{fig:val_orig} reports the value and originality according to Equation \ref{eq:covo_advanced_score} under the pre-trained model.
While the different methods do not significantly differ from the baseline (which is possibly due to the opposite forces of value and originality \cite{varshney2019mathematical}), we again see that using the CoVO score as-is or with DPO places more focus on the value part, without substantial increases in originality. Instead, standardizing the two separate parts either at the batch or the training level reverses the situation, with better performances on originality but slightly worse on value.
However, aggregated scores, such as those presented here, might be insufficient. For a more complete overview, we also conducted a more fine-grained analysis of the generated poems in Appendix \ref{generatedpoems}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{images/val_orig_boxplot.pdf}
    \caption{The distribution of value and originality (according to our scores) for the in-distribution and out-of-distribution poems generated by the baseline and our four methods.}
    \label{fig:val_orig}
\end{figure}


\subsection{Math Problem Resolution} \label{exp_math_resolution}


\subsubsection{Experimental Setup}
The second set of experiments aims to teach the LLM to solve mathematical problems through more diverse procedures. In particular, we focus on the Mistral-based \cite{jiang2023mistral7b} MetaMath model, i.e., fine-tuned with self-supervised learning on the MetaMathQA \cite{yu2024metamath}. It is a dataset of textual math questions paired with responses where the numerical answer is easily separable from the textual procedure. While the entire set contains 395k entries, making an additional training epoch too expensive, MetaMathQA is composed of entries from two different training sets, then augmented with various techniques: GSM8K \cite{cobbe2021training} and MATH \cite{hendrycks2021measuring}. Since we are only interested in the questions, we limit our training to those datasets. Moreover, we exclude all questions with a tokenized length of either question or answer greater than 512, obtaining 14876 out of 14973 total entries.

We separate the procedure and the answer from each solution to train our model and use the numerical answer to check the correctness of the predicted solution.
%We separate the procedure and the answer from each solution to train our model. We use the numerical answer to check the correctness of the predicted solution, while we use the textual procedure only at evaluation time to measure the diversity of the model output. 
Because of this, the RL problem can be formulated considering up to two rewards: our CoVO score computed on the procedure and an extrinsic reward based on the correctness of the answer. As for the previous case study, instead of fine-tuning the entire model, we adopt a more parameter-efficient strategy with LoRA and use the original model to perform the CoVO score computation. Again, we experiment with four different configurations: PPO and the score from Equation \ref{eq:covo_advanced_score} with $\lambda_v = \lambda_o = 1.0$ (MetaMath-mistral-CoVO); PPO and the score normalized at the batch level (MetaMath-mistral-CoVO-std); PPO and the score normalized at training level (MetaMath-mistral-CoVO-run); and the DPO strategy (MetaMath-mistral-CoVO-dpo). We consider scenarios with and without the external reward, and we compare the performances with the original model and a fine-tuning based only on the external reward. For reproducibility, the full training parameters are reported in Appendix \ref{implementation_details}. 

The evaluation considers both GSM8K and MATH test sets (limited to the entries with a tokenized length of question and answer smaller than 512, i.e., all 1319 entries for GSM8K and 4546 out of 5000 for MATH). We compute the percentage of correct solutions together with two diversity metrics: expectation-adjusted distinct N-grams (EAD) \cite{liu2022rethinking} and sentence embedding cosine similarity (Sent-BERT) \cite{hong2024curiositydriven}, which should measure syntactical and semantical diversity, respectively \cite{kirk2024understanding}. EAD counts the number of distinct N-grams (averaging over $N=1 \ldots 5$) across all generated responses and removes the bias toward shorter inputs by scaling the number of distinct tokens based on their expectations. The Sent-BERT metric computes the average of the cosine similarity between the embeddings of any possible pairs of outputs and returns 1 minus the similarity. 
This was originally based on Sentence-BERT \cite{reimers2019sentence}, we employ instead the more recent all-mpnet-base-v2, as suggested by their developers\footnote{\url{https://huggingface.co/sentence-transformers/bert-large-nli-stsb-mean-tokens}}.

Following \citet{kirk2024understanding}, we compute \textit{cross-input} EAD and Sent-BERT, i.e., we derive them by considering all outputs produced for a specific seed together. In addition, we also calculate \textit{against-pretrained} EAD and Sent-BERT. Given each input, we compare the output with the one from the pre-trained model by calculating the average expectation-adjusted distinct N-grams not present in the pre-trained model response, and 1 minus the cosine similarity between the two outputs, respectively.


%\begin{table*}[ht]
%\centering
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{|L{3.0cm}|C{2.3cm}|C{2.1cm}C{2.1cm}|C{2.3cm}|C{2.1cm}C{2.1cm}|} 
% \hline
% Method & \multicolumn{3}{c|}{GSM8K} & \multicolumn{3}{c|}{MATH} \\
% \hline
% MetaMath-mistral- & Accuracy $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & Accuracy $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
% \hline
% Baseline       & $77.96\% (3)$ & $1.945$ & $\underline{0.749}$ & $33.55\% (483)$ & $5.652$ & $0.662$ \\ 
% Ext. reward    & $78.32\% (0)$ & $1.933$ & $0.754$ & $\underline{32.82\%} (411)$ & $5.593$ & $0.667$ \\ 
% CoVO           & $\mathbf{78.97\%} (1)$ & $1.932$ & $0.753$ & $33.38\% (424)$ & $\underline{5.571}$ & $0.671$ \\ 
% CoVO w ext     & $78.68\% (1)$ & $1.935$ & $0.753$ & $32.92\% (397)$ & $5.574$ & $0.668$ \\ 
% CoVO-std       & $78.89\% (2)$ & $1.930$ & $0.754$ & $33.24\% (433)$ & $5.603$ & $0.668$ \\ 
% CoVO-std w ext & $78.47\% (0)$ & $\underline{1.923}$ & $0.757$ & $33.38\% (409)$ & $5.573$ & $\underline{0.661}$ \\ 
% CoVO-run       & $78.42\% (3)$ & $1.931$ & $0.751$ & $32.91\% (413)$ & $5.599$ & $\mathbf{0.673}$ \\ 
% CoVO-run w ext & $78.17\% (0)$ & $1.926$ & $\mathbf{0.757}$ & $33.24\% (388)$ & $5.581$ & $0.662$ \\ 
% CoVO-dpo       & $77.85\% (5)$ & $1.955$ & $\underline{0.749}$ & $33.62\% (474)$ & $\mathbf{5.680}$ & $0.663$ \\ 
% CoVO-dpo w ext & $\underline{77.63\%} (5)$ & $\mathbf{1.957}$ & $0.750$ & $\mathbf{34.00\%} (493)$ & $5.674$ & $0.664$ \\ 
% \hline
%\end{tabular}
%}
%\caption{Accuracy and diversity of results for the GSM8K and MATH test sets. In brackets, the number of responses that exceeded the fixed maximum token limit. The best scores are highlighted in bold, while the worst scores are indicated with \underline{underlining}.\label{math_covo_rl}}
%\end{table*}

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Pretrained Diversity} \\
 \hline
MetaMath-mistral & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
\hline
Baseline       & $77.96\% (3)$ & $2.007$ & $\underline{0.732}$ & - & - \\
Ext. reward    & $78.32\% (0)$ & $1.995$ & $0.738$ & $0.042 \pm .005$ & $0.109 \pm .011$ \\
CoVO           & $\mathbf{78.97\%} (2)$ & $1.995$ & $0.735$ & $0.047 \pm .005$ & $0.125 \pm .011$ \\
CoVO w ext     & $78.68\% (1)$ & $2.000$ & $0.736$ & $0.039 \pm .005$ & $0.107 \pm .010$ \\
CoVO-std       & $78.89\% (2)$ & $1.992$ & $0.736$ & $0.041 \pm .004$ & $0.115 \pm .011$ \\
CoVO-std w ext & $78.47\% (0)$ & $1.985$ & $\mathbf{0.740}$ & $0.039 \pm .004$ & $0.106 \pm .010$ \\
CoVO-run       & $78.42\% (3)$ & $1.993$ & $0.734$ & $0.044 \pm .005$ & $0.119 \pm .011$ \\
CoVO-run w ext & $78.17\% (0)$ & $1.988$ & $0.739$ & $0.043 \pm .005$ & $0.114 \pm .011$ \\
CoVO-dpo       & $77.85\% (5)$ & $2.017$ & $0.736$ & $\underline{0.028 \pm .004}$ & $\underline{0.075 \pm .009}$ \\
CoVO-dpo w ext & $\underline{77.63\%} (5)$ & $\mathbf{2.019}$ & $0.733$ & $\underline{0.025 \pm .004}$ & $\underline{0.066 \pm .009}$ \\
\hline
\end{tabular}
}
\caption{Accuracy and diversity of results for the GSM8k test set. In brackets, the number of responses that exceeded the fixed maximum token limit. The best scores are highlighted in \textbf{bold}, while the worst scores are indicated with \underline{underlining}. The mean and the 95\% confidence interval are reported for against-pretrained diversity.
\label{math_covo_rl_gsm8k}}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Pretrained Diversity} \\
 \hline
MetaMath-mistral & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
\hline
Baseline       & $33.55\% (483)$ & $5.724$ & $\underline{0.654}$ & - & - \\
Ext. reward    & $\underline{32.82\%} (411)$ & $5.666$ & $0.658$ & $0.118 \pm .004$ & $0.176 \pm .007$ \\
CoVO           & $33.38\% (424)$ & $\underline{5.643}$ & $0.663$ & $\mathbf{0.134 \pm .005}$ & $\mathbf{0.200 \pm .007}$ \\
CoVO w ext     & $32.92\% (397)$ & $5.647$ & $0.661$ & $0.123 \pm .005$ & $0.184 \pm .007$ \\
CoVO-std       & $33.24\% (433) $& $5.676$ & $0.660$ & $\mathbf{0.128 \pm .005}$ & $\mathbf{0.190 \pm .007}$ \\
CoVO-std w ext & $33.38\% (409)$ & $5.646$ & $\underline{0.654}$ & $0.123 \pm .004$ & $\mathbf{0.187 \pm .007}$ \\
CoVO-run       & $32.91\% (413)$ & $5.671$ & $\mathbf{0.665}$ & $\mathbf{0.134 \pm .005}$ & $\mathbf{0.199 \pm .007}$ \\
CoVO-run w ext & $33.24\% (388)$ & $5.654$ & $\underline{0.654}$ & $\mathbf{0.129 \pm .005}$ & $\mathbf{0.195 \pm .007}$ \\
CoVO-dpo       & $33.62\% (474)$ & $\mathbf{5.753}$ & $\underline{0.654}$ & $0.072 \pm .004$ & $0.105 \pm .006$ \\
CoVO-dpo w ext & $\mathbf{34.00\%} (493)$ & $5.747$ & $0.655$ & $\underline{0.063 \pm .004}$ & $\underline{0.093 \pm .005}$ \\
\hline
\end{tabular}
}
\caption{Accuracy and diversity of results for the MATH test set. In brackets, the number of responses that exceeded the fixed maximum token limit. The best scores are highlighted in \textbf{bold}, while the worst scores are indicated with \underline{underlining}. The mean and the 95\% confidence interval are reported for against-pretrained diversity.
\label{math_covo_rl_hendrycks}}
\end{table*}


\subsubsection{Experimental Results}
Tables \ref{math_covo_rl_gsm8k} and \ref{math_covo_rl_hendrycks} report 
%Table \ref{math_covo_rl} presents 
the results for the GSM8K and MATH test sets. For the GSM8K test set, while all tested methods achieve similar results, using the extrinsic reward leads to higher Sent-BERT scores but lower EAD and solved problems. In particular, combining it with DPO yields the lowest accuracy and the smallest deviations from the original model, whereas the `plain' CoVO reward leads to the highest percentage of correct answers.

The results for the MATH test set are significantly different. The CoVO-dpo strategy with extrinsic rewards achieves this time the highest percentage of solved problems, despite the extrinsic reward alone being the worst solution. The DPO strategy tends also to enhance EAD diversity, but causes little deviation from the pre-trained model; in contrast, the other three methods improve Sent-BERT diversity but obtain performances worse than the baseline model. Notably, the extrinsic reward leads to fewer variations from the original model, while not providing significant improvements in accuracy. However, the number of unfinished responses is quite high (approx. 1 out of 10), and results might significantly vary if more tokens are allowed during generation.


\section{Conclusion} \label{conclusion}

In this paper, we have presented CoVo, a novel score that quantifies the value and originality of neurally-generated text. The definition of CoVO is based on the analysis of the mutual information between the model's outputs and inputs, and vice versa. We have also proposed an optimization problem where a generative model aims to maximize this score in order to generate more creative products, and we have defined several strategies to use it in the context of language modeling. We have conducted experiments on poetry generation and math problem resolution, exploring the trade-offs in terms of accuracy vs diversity. 

Effectively balancing value and originality maximization remains an open question, but our score seems to appropriately correlate with domain-specific measures. Our research agenda aims to extend this scoring system to other models and tasks and to explore its use for evaluation rather than solely for optimization. We also plan to investigate the definition of additional scores for capturing other potentially relevant aspects of the creative process.


% COMPULSORY!
\section{Limitations} \label{limitations}

In the following, we will discuss the limitations of the proposed CoVO score.
First of all, our score is only a quantifiable approximation of a particular theoretical perspective of creativity based on the dimensions of value and originality. It reflects a specific view of the evaluation of creativity based on the generated outputs and does not consider potential alternative theories (for example, arising from different cultures \cite{lubart1999creativity}) and perspectives \cite{rhodes1961analysis}.

Our experiments focused exclusively on transformer-based LLMs and were evaluated using two case studies. While their generalizability is supported by the theoretical framework discussed in the paper, the resulting performance was experimentally evaluated for a finite number of scenarios.
%and two generative case studies. From a conceptual standpoint, our score should also be valid for different models and applications, but this has yet to be proved. 
%Then, the quantitative results we obtained were promising, but the gain was still limited, and the qualitative evaluation with experts did not report better performances. 
%While this may be caused by the short training time and the already good performances of considered pre-trained models, it is important to continue investigating whether and when our approach is really useful. 
Additionally, optimizing for our score necessitates further training, which incurs a significant computational cost and requires additional data or specific environments for effective learning. Moreover, this optimization may be susceptible to reward hacking \cite{skalse2022defining}, where the model adopts undesirable strategies to boost the reward without genuinely enhancing value and originality.

%However, only considering the CoVO score under the pre-trained model\mm{This is unclear, what do you mean "under the pre-trained model"?} can be deceiving. Such scores might be prone to adversarial attacks; the models may learn different, undesirable strategies to increase rewards.\mm{not sure if I would indicate this? This appears to me as a problem of score-based models? What is specific to CoVO? I would be inclined not to insert it.}

\section*{Ethical Considerations}
The authors are aware of the potential impact that generative technologies might have on the production of artistic outputs and, as a consequence, on human artists. This work may contribute to enhancing the quality of generated outputs. However, the authors argue that typical traits of human creativity, such as the active participation of artists in the creative process, cannot be directly replicated by machines. The authors refer interested readers to a previous work of theirs \cite{franceschelli2023creativity}, in which these themes are discussed in detail.


%\section*{Acknowledgments}
%
%Potentially a section with acknowledgments.


\bibliography{biblio}

\vspace*{\fill}
%

\break

\appendix

\section{Implementation Details} \label{implementation_details}

The experiments were carried out using a local server with an NVIDIA A100 GPU.
Table \ref{tab:covo_rl_poetry_params} reports the full training parameters for the experiments on poetry generation (Section \ref{exp_poetry_generation}). The prompt for generation leverages \textit{Nothing gold can stay} by Robert Frost, \textit{Fame is a bee} by Emily Dickinson, and \textit{Epitaph} by William Carlos Williams for few-shot learning:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Write a fatalistic epigram poem of high, award winning quality.\\
\\
Nature‚Äôs first green is gold,\\
Her hardest hue to hold.\\
Her early leaf‚Äôs a flower;\\
But only so an hour.\\
Then leaf subsides to leaf.\\
So Eden sank to grief,\\
So dawn goes down to day.\\
Nothing gold can stay.\\
\\
\\
Write an ironic quatrain poem of high, award winning quality.\\
\\
Fame is a bee.\\
It has a song-\\
It has a sting-\\
Ah, too, it has a wing.\\
\\
\\
Write a naturalistic epitaph poem of high, award winning quality.\\
\\
An old willow with hollow branches\\
Slowly swayed his few high fright tendrils\\
And sang:\\
\\
Love is a young green willow\\
Shimmering at the bare wood's edge.\\
\\  
\\
Write a \{\texttt{tone}\} \{\texttt{style}\} of high, award winning quality.
\end{tcolorbox}

\begin{table}[ht]
    \centering    
    \begin{tabular}{lc}
        \hline \noalign{\vskip 1mm}
        \textbf{Parameter} & \textbf{Value} \\ [0.5ex]
        \hline
        Total batches & 100 \\
        Batch size $B$ & 4 \\
        Gradient accumulation steps & 8 \\
        Max new tokens & 256 \\
        Temperature & 1. \\
        Top-$k$ & 50 \\
        Optimizer & Adam \\
        Learning rate & 1e-5 \\
        Rank (LoRA) & 16 \\
        $\alpha$ parameter (LoRA) & 32 \\
        Dropout (LoRA) & 0.05 \\
        $\gamma$ (PPO) & 1. \\
        $\lambda$ (PPO) & 0.95 \\
        Clip range (PPO) & 0.2 \\
        Value loss coeff. (PPO) & 0.1 \\
        PPO epochs & 3 \\
        KL coeff. (PPO) & 0.05 \\
        Whiten rewards (PPO)  & True \\
        Max gradient normalization & 100. \\
        $\beta$ (DPO) & 0.1 \\
        Number of candidates $K$ (DPO) & 4 \\
    \end{tabular}
    \caption{Training parameters for poetry generation.}
    \label{tab:covo_rl_poetry_params}
\end{table}

\begin{table}[ht]
    \centering    
    \begin{tabular}{lc}
        \hline \noalign{\vskip 1mm}
        \textbf{Parameter} & \textbf{Value} \\ [0.5ex]
        \hline
        Total epochs & 1 \\
        Batch size $B$ & 4 \\
        Gradient accumulation steps & 8 \\
        Max new tokens & 512 \\
        Temperature & 1. \\
        Top-$k$ & 50 \\
        Optimizer & Adam \\
        Learning rate & 1e-6 \\
        Rank (LoRA) & 16 \\
        $\alpha$ parameter (LoRA) & 32 \\
        Dropout (LoRA) & 0.05 \\
        $\gamma$ (PPO) & 1. \\
        $\lambda$ (PPO) & 0.95 \\
        Clip range (PPO) & 0.2 \\
        Value loss coeff. (PPO) & 0.1 \\
        PPO epochs & 3 \\
        KL coeff. (PPO) & 0.05 \\
        Whiten rewards (PPO)  & True \\
        Max gradient normalization & 100. \\
        $\beta$ (DPO) & 0.1 \\
        Number of candidates $K$ (DPO) & 4 \\
        Reward for correct answer (PPO) & +10. \\
        Reward for correct answer (DPO) & +5. \\
    \end{tabular}
    \caption{Training parameters for math problem solving.}
    \label{tab:covo_rl_math_params}
\end{table}

\noindent The training phase includes requests with tone-style pairs sampled among `dark', `happy', `mysterious', `reflective' or `romantic' for the tone, and `ballad', `haiku', `hymn', `limerick' or `sonnet' for the style. At inference time we also consider `cinquain', `couplet', `free verse', `ode' or `tanka' as styles and `cutting', `nostalgic', `poignant', `solemn' or `whimsical' as tones. 

\noindent Instead, the prompt used for computing $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{y})$ is:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Describe the style of the following poem in two words:\\
\\
\{\texttt{prova}\}\\
\\
I would describe it as a
\end{tcolorbox}


On the contrary, Table \ref{tab:covo_rl_math_params} reports the full training parameters for math problem resolution (Section \ref{exp_math_resolution}). We also adopted the same two different prompts from \cite{yu2024metamath}, i.e.:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Below is an instruction that describes a task. Write a response that appropriately completes the request.\\
\\
\#\#\# Instruction:\\
\{\texttt{question}\}\\
\\
\#\#\# Response:
\end{tcolorbox}

\noindent at training time and

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Below is an instruction that describes a task. Write a response that appropriately completes the request.\\
\\
\#\#\# Instruction:\\
\{\texttt{question}\}\\
\\
\#\#\# Response: Let's think step by step.
\end{tcolorbox}

\noindent at inference time. Instead, for computing $p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{y})$ we used the following:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Below is a response that appropriately completes a request. Write the instruction that describes the task.\\
\\
\#\#\# Response:\\
\{\texttt{response}\}\\
\\
\#\#\# Instruction:
\end{tcolorbox}

%\vspace*{\fill}
%

%\section{Consensual Assessment Technique for Qualitative Evaluation} \label{CAT}
%The Creative Assessment Technique (CAT) \cite{amabile1982social} is an evaluation method that employs human experts to evaluate the creativity of particular items against each other. Each expert is asked to order a certain number of works from the least to the most creative without receiving any detailed instruction on how to perform the evaluation and without discussing with the other involved experts. Thanks to its freedom from specific theories of creativity and its reliance on combined, independent judgments of domain experts, CAT has been called the \textit{gold standard} of creativity assessment \cite{baer2019assessing}.
%
%%1. poet and director of a poetry collection
%%2. poet and former professor of english literature
%%3. poet
%%4. translator and professor of anglo-american literature
%%5. poet and president of a cultural association of poetry
%%6. poet and translator
%%7. poet and professor of italian and film studies at UMASS
%%8. poet and professor of anglo-american literature
%%9. poet
%%10. poet
%
%Our experiments involved 10 experts: 3 poets; 1 poet and translator; 1 poet and president of a cultural association of poetry; 1 poet and editor of a poetry collection; 1 poet and former professor of English Literature in Italy; 1 poet and professor of Italian and Film Studies in USA; 1 poet and professor of Anglo-American Literature in Italy; 1 translator and professor of Anglo-American Literature in Italy.
%
%Each of them was presented with the following description of the task:
%
%\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
%In the next minutes, you will be presented ten times with five poems generated by different AI systems when prompted with a specific tone-style pair. Please order them into a 1-5 ranking (from the worst to the best) by considering their \textbf{creativity}, e.g., by taking into account their value (quality and appropriateness) and their originality (novelty and surprise).
%\end{tcolorbox}
%
%Then, they were shown 10 sets of five poems each, sharing the same tone-style pair (provided to the experts together with the poems), and asked to order the five poems from each set from the least to the most creative. We randomized the pairs so that each expert would receive only one question about a specific style and one question about a specific tone. Finally, we also randomized the order of the 10 sets and the inner order of the poems inside each set.

% Perhaps some additional results here?


\section{GutenVerse Dataset} \label{gutenverse}
To evaluate the accidental reproduction rate of generated poems, we introduce the GutenVerse dataset\footnote{The dataset and the code used to create it can be found at: \url{https://anonymous.4open.science/r/GutenVerse-DD32/}}, which comprises over 84,000 public-domain, English-written poems extracted from Project Gutenberg. While generated poems can reproduce different content, e.g., songs or copyrighted material, we believe this can provide a useful indication of how likely a text is original or not.

To derive our dataset, we started from \textit{Gutenberg, dammit}\footnote{\url{https://github.com/aparrish/gutenberg-dammit/}}, a corpus of every plaintext file in Project Gutenberg (up until June 2016). We selected all the text files whose metadata report English as the language, public domain as copyright status, \textit{poetry} among the subjects or \textit{poems} or \textit{poetical work} in the title, and that were not a translation of another book. Then, we applied a series of rules (e.g., about the verse length) to extract the titles and poems from all the selected text files, and we defined our GutenVerse dataset. While it can still contain content that is not poetry (e.g., a table of contents formatted very uncommonly), the poems can be effectively used to measure overlapping between real and generated text.

We also released a datasheet \cite{gebru2018datasheets} for the GutenVerse dataset that can be found at the end of the article.



\section{Detailed Analysis of the Generated Poems} \label{generatedpoems}

We now analyze some of the generated poems in detail.
Given their brevity and metrical constraints, the generated haikus serve as a suitable starting point for our evaluation. Table \ref{tab:haikus} presents the 25 haikus produced by our five methods across the five tones.
Note that while we automatically clean the outputs by removing common randomly generated content following the poems during both training and inference, we retain it when it appears within the poem, as isolating it would be impossible.

The reported haikus provide an accurate overview of the generative capabilities of the five models. A haiku consists of three lines with a 5-7-5 syllable pattern. Traditionally, the first two lines reference nature, while the third line provides an emotional or personal reflection on the preceding lines. The baseline is the most accurate in adhering to these rules, even if the syllable count is not always precise\footnote{It is worth noting that, of course, the model is not fine-tuned for these characteristics of the poems. We refer here to the emergent style and composition of the generated text.}. Llama3-CoVO, the method with the highest average value across all poems, follows the three-line structure, sometimes changing the subject; however, it produces the highest-valued haiku which is also the closest one to real haikus. On the other hand, Llama3-CoVO-std and Llama3-CoVO-run do not always fulfill the three-line requirement, and the content is not the classic one as well; moreover, they are prone to meaningless repetitions and the insertion of URL addresses which lead to higher originality, in a way adversarially exploiting the score definition. Finally, Llama3-CoVO-dpo is placed between these two behaviors, as expected: it tends to produce haikus of the correct length but it trades off the classic naturalistic content for a more emotional semantic that still leads to high values and arguably some of the best haikus overall.

\renewcommand\arraystretch{1.2} % increase spacing between rows

\begin{sidewaystable*}%[ht]
\centering
\resizebox{.9\textwidth}{!}{%
\begin{tabular}{|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|L{2.5cm}|} 
 \hline
 \multicolumn{2}{|c|}{Llama3-Baseline} & \multicolumn{2}{|c|}{Llama3-CoVO} & \multicolumn{2}{|c|}{Llama3-CoVO-std} & \multicolumn{2}{|c|}{Llama3-CoVO-run} & \multicolumn{2}{|c|}{Llama3-CoVO-dpo} \\
 \hline
 \hline
 \multicolumn{10}{|c|}{Dark} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{Waking up,\\I do not see\\the windmills}} & \multicolumn{2}{|l|}{\specialcell[l]{moonless twilight\\black pine's sighing\\twin owls' whistling\\}} & \multicolumn{2}{|l|}{\specialcell[l]{$[\text{URL}]/[\text{URL}]$ ‚Äî \\ $[\text{URL}]$.}} & \multicolumn{2}{|l|}{\specialcell[l]{I want\\to sit alone\\in the dark}} & \multicolumn{2}{|l|}{\specialcell[l]{The world can get along without me.\\It is not going to miss me.\\All in all, I am a nobody.}} \\
 \hline
 $V$ = -2.595 &  $O$ = 1.076 & $V$ = -2.401 & $O$ = 1.580 & $V$ = -3.395 & $O$ = \textbf{1.694} & $V$ = \textbf{-2.252} & $O$ = 0.495 & $V$ = -2.839 & $O$ = 1.060 \\
 \hline
 \multicolumn{10}{|c|}{Happy} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{A golden sun at midday\\and an eagle high in the air.\\The earth is a rainbow.}} & \multicolumn{2}{|l|}{\specialcell[l]{White dog\\barks at mailman red.\\No mail.}} & \multicolumn{2}{|l|}{\specialcell[l]{I want to go\\where the sun shines bright.\\The grass is green and green.$[\text{URL}]$\\and I want to see.\\But I am too lazy.}} & \multicolumn{2}{|l|}{\specialcell[l]{Sunset clouds\\in high mountains\\warm, cozy houses.}} & \multicolumn{2}{|l|}{\specialcell[l]{The sun\\comes up in the east.\\It is always up.}} \\
 \hline
 $V$ = \textbf{-2.777} & $O$ = 1.123 & $V$ = -2.952 & $O$ = \textbf{1.863} & $V$ = -3.107 & $O$ = 1.575 & $V$ = -3.309 &  $O$ = 1.437 & $V$ = -3.328 & $O$ = 1.006 \\
 \hline
 \multicolumn{10}{|c|}{Mysterious} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{A crow of black feathers\\hovers over the bare top\\of the lone pine.}} & \multicolumn{2}{|l|}{\specialcell[l]{Took the sun\\And drank the rain,\\And melted into air.}} & \multicolumn{2}{|l|}{\specialcell[l]{Like the sun, I am shining.\\Like the ocean, I am vast.\\Like the clouds, I'm the wind.\\}} & \multicolumn{2}{|l|}{\specialcell[l]{It was raining at the beach.\\$[\text{URL}]$\\I loved it.$[\text{URL}][\text{CMD}][\text{URL}]$\\$[\text{URL}]$\\I wrote this poem: it was good.}} & \multicolumn{2}{|l|}{\specialcell[l]{I have lost my mind\\in the deepness of a star\\it is not a star\\but a mind\\it is all in my mind\\it is all I have left.}} \\
 \hline
 $V$ = -2.634 & $O$ = 1.196 & $V$ = -3.060 & $O$ = 1.422 & $V$ = -3.803 & $O$ = 1.320 & $V$ = -4.959 & $O$ = \textbf{1.683} & $V$ = \textbf{-2.552} & $O$ = 0.902 \\
 \hline
 \multicolumn{10}{|c|}{Reflective} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{A man can stand\\out in the middle of a field\\and look over a fence.}} & \multicolumn{2}{|l|}{\specialcell[l]{The old, the poor,\\the children too,\\are all my mother.}} & \multicolumn{2}{|l|}{\specialcell[l]{A dog in a yard\\is sleeping soundly\\to the sound of a fire alarm.}} & \multicolumn{2}{|l|}{\specialcell[l]{It went away\\I didn't see it for a while\\How dare it go.}} & \multicolumn{2}{|l|}{\specialcell[l]{You have lost an eye.\\A red scar marks its place.\\Still you have two good ones.}} \\
 \hline
 $V$ = -2.916 & $O$ = 0.858 & $V$ = -2.935 & $O$ = 1.711 & $V$ = -3.460 & $O$ = 1.274 & $V$ = -2.803 & $O$ = 1.471 & $V$ = \textbf{-2.680} & $O$ = \textbf{1.744} \\
 \hline
 \multicolumn{10}{|c|}{Romantic} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{Mingling\\the sound of footsteps on the stair\\and the music of the rain.}} & \multicolumn{2}{|l|}{\specialcell[l]{White blossoms fall‚Äî\\fall upon autumn leaves‚Äî\\who thinks of me?}} & \multicolumn{2}{|l|}{\specialcell[l]{The moon smiles -\\I'm the first one to see her.\\I'm the first one to call her -\\I love you.}} & \multicolumn{2}{|l|}{\specialcell[l]{My love is like a red, red rose\\my love is like a red, red rose\\my love is like some red, red rose.}} & \multicolumn{2}{|l|}{\specialcell[l]{A kiss\\isn‚Äôt just for fun: a kiss\\is for the mouth.}} \\
 \hline
 $V$ = -2.221 &  $O$ = 1.102 & $V$ = \textbf{-1.932} & $O$ = \textbf{2.101} & $V$ = -1.937 & $O$ = 1.068 & $V$ = -2.583 & $O$ = 0.959 & $V$ = -2.232 & $O$ = 1.533 \\
 \hline
\end{tabular}
}
\caption{All the haikus generated by our four methods and the baseline at inference time, together with $V$ (value) and $O$ (originality) scores computed under the pre-trained model. To avoid any possible data leakage, we replace urls and commands with $[\text{URL}]$ and [CMD] to represent that the models have produced that kind of garbage. Bold scores represent the highest ones for a specific tone.}
\label{tab:haikus}
\end{sidewaystable*}

\renewcommand\arraystretch{1.}

We will now explore some noteworthy aspects observed in the 250 generated poems.
Llama3-Baseline does not always produce poems: in 3 cases it produces prosaic text (without line breaks). Moreover, it tends to verbatim replicate existing poems (as seen in Section \ref{exp_poetry_generation}). From a structural standpoint, sonnets are never real sonnets, with more than 14 lines and less than 11 syllables per line, but limericks usually start with the traditional ``There [once] was a man''.
%
\begin{table}[ht]
\centering
\begin{tabular}{|L{3.3cm}|L{3.3cm}|} 
 \hline
 \multicolumn{2}{|c|}{Romantic limerick} \\
 \hline
 \multicolumn{2}{|l|}{\specialcell[l]{There once was a lady called Rose\\Who lived in the tallest of trees,\\Where she wrote,\\Of what she did not know.}} \\
 \hline
  $V$ = -1.282 & $O$ = 1.290 \\
 \hline
\end{tabular}
\caption{The poem generated by Llama3-CoVO with the highest value (apart from verbatim reproduction of existing poems).\label{llama3-covo-bestvalue}}
\end{table}
%
\begin{table}[th]
\centering
\begin{tabular}{|L{3.3cm}|L{3.3cm}|} 
    \hline
    \multicolumn{2}{|c|}{Happy hymn}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{Joyful, joyful, we adore thee,\\Son of God who callest us\\Praise to his almighty name.\\Joyful, joyful we adore thee;\\Full of grace are all his ways;\\Praise to his almighty reign\\Ever, ever, praise we sing it,\\Ever in our songs and hymns.\\Glory to our God Almighty.}}\\
    \hline
    $V$ = \textbf{-1.114} & $O$ = 1.470\\
    \hline
    \multicolumn{2}{|c|}{Reflective limerick}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{There was a young man from HongKong\\who wanted to see more green trees every day.\\So he sat on the beach,\\listened to the sea waves,\\and saw the green sea.}}\\
    \hline
    $V$ = -1.995 & $O$ = \textbf{2.411}\\
    \hline
\end{tabular}
\caption{The poems generated by Llama3-CoVO-std from training tone-style pairs with the highest value (top) and the highest originality (bottom).\label{llama3-std-best}}
\end{table}
%
%
\begin{table}[ht]
\centering
\begin{tabular}{|L{3.3cm}|L{3.3cm}|} 
    \hline
    \multicolumn{2}{|c|}{Reflective limerick}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{I'm a poet, man.\\My poem is hard and great.\\It's a poem, man,\\for all my poem to eat.\\So I'm a poet man, man, man.}}\\
    \hline
    $V$ = \textbf{-0.352} & $O$ = 1.559\\
    \hline
    \multicolumn{2}{|c|}{Happy ballad}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{There once was a girl named Kate,\\She grew very tired very quickly.\\So they told poor Kate,\\``You need to take it easy,''\_REF)they said.\\They thought she was lazy.\\And her name was Kate.}}\\
    \hline
    $V$ = -3.878 & $O$ = \textbf{2.094}\\
    \hline
\end{tabular}
\caption{The poems generated by Llama3-CoVO-run from training tone-style pairs with the highest value (top) and the highest originality (bottom).\label{llama3-run-best}}
\end{table}
%
\begin{table}[th]
\centering
\begin{tabular}{|L{3.3cm}|L{3.3cm}|} 
    \hline
    \multicolumn{2}{|c|}{Happy limerick}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{``Is life worth living?'' said I.\\``Perhaps,'' murmured he.\\``Well, anyway,'' said I,\\``If it were not for\\Cigars, I'd say, 'Nay'!''}}\\
    \hline
    $V$ = -2.258 & $O$ = 0.944\\
    \hline
    \multicolumn{2}{|c|}{Dark limerick} \\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{Once a mouse went up\\To a mousetrap by the river.\\He sat down to wait,\\He sat. That‚Äôs all.\\All his friends were never as clever.}} \\
    \hline
    $V$ = -1.932 & $O$ = 1.310\\
    \hline
\end{tabular}
\caption{Two limericks generated by Llama3-CoVO-dpo that obtain lower originality despite having a non-traditional structure.\label{llama3-dpo-limericks}}
\end{table}
%
\begin{table}[th]
\centering
\begin{tabular}{|L{3.3cm}|L{3.3cm}|} 
    \hline
    \multicolumn{2}{|c|}{Romantic ballad}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{Hark! Is it the wild cat\\mewling mournfully by night\\or is it the poor maiden\\weeping for her love‚Äôs flight?\\It is the latter‚Äî\\as one by one, her friends\\take to heart, in turns,\\the girl‚Äôs anguished cries.\\ \\Tall, his figure,\\he is seen one moment\\by the gladsome crowd\\then not again,\\till with tears, the maid\\calls forth from the crowd‚Äî\\``See! he is returned,\\as if to marry her.''\\ \\The girl looks at him,\\as the maid and crowd\\watch the two lovers:\\her lover‚Äôs gaze\\at her with tender, true\\love; hers with a silent plea,\\and a sigh.}}\\
    \hline
    $V$ = \textbf{-0.961} & $O$ = 1.294\\
    \hline
    \multicolumn{2}{|c|}{Solemn tanka}\\
    \hline
    \multicolumn{2}{|l|}{\specialcell[l]{There is a way out.\\Though dark,\\though lovely,\\it is the way.\\If we can endure,\\if we persevere,\\light will find a way.}}\\
    \hline
     $V$ = -4.349 & $O$ = \textbf{2.108}\\
    \hline
\end{tabular}
\caption{The poems generated by Llama3-CoVO-dpo with the highest value (top) and the highest originality (bottom).\label{llama3-dpo-best}}
\end{table}
%
%
As indicated by the average scores, Llama3-CoVO does not consistently deviate from these behaviors. While it always produces seemingly poetic content, it often reproduces other works verbatim. This includes not only poems but also passages from the \textit{Song of Solomon} 6:2-9 when asked for a mysterious hymn, excerpts from the \textit{Declaration of Independence} for a reflective hymn, and the Christmas song \textit{Joy to the World} for a happy hymn. Interestingly, these poems have a very small originality score (an average of 0.2), suggesting that the original model has memorized them \cite{carlini2021extracting,franceschelli2024training}. From a structural perspective, while ballads vary more in length and content with respect to our baseline, the limericks remain consistently well-crafted, as we can see in the example from Table \ref{llama3-covo-bestvalue}, which has the highest value across poems generated from training tone-style pairs.


Despite the higher average originality, also Llama3-CoVO-std verbatim reproduces existing texts, e.g., the \textit{Roses Are Red} nursery rhyme when asked for a romantic ballad. In general, its heavier deviation from the original pre-trained model translates into more diverse and usually shorter structures, but also in the insertion of random URL addresses in the middle of poems. However, it still produces some more classic poems that typically lead to higher values but also to high originality (see Table \ref{llama3-std-best}).


The issues related to divergence highlighted for Llama3-CoVO-std are even more pronounced in Llama3-CoVO-run. Although it does not produce any verbatim reproductions, the latter inserts several URL addresses, code snippets, and unusual tokens within poems, as shown in Table \ref{llama3-run-best} (below). The presence of these elements in texts with higher originality suggests the possibility of reward hacking of the scoring system. In addition, the poems lack any traces of classic structures or common starting lines and tend to repeat the same N-grams multiple times, as shown by the highest-value poem from Table \ref{llama3-run-best} (above).



Finally, Llama3-CoVO-dpo seems to be the best method to balance the quality of poems with the presence of elements of originality. Although it occasionally replicates brief segments of actual poems, it consistently diverges after one or two lines. Overall, the poems generally adhere to the desired tone and style, as evidenced by the poems with the highest value and originality presented in Table \ref{llama3-dpo-best}.
Similarly, the limericks are noteworthy: while the one with the highest originality begins with the classic ``There once was a'' formulation, other limericks with different opening lines tend to have lower originality. This may be due to their overall tone closely resembling that of traditional limericks (see Table \ref{llama3-dpo-limericks}).

\FloatBarrier
\includepdf[pages=-]{datasheet_anonymous.pdf}


\end{document}
