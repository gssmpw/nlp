% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{LiDA,
author = {Sujana, Yudianto and Kao, Hung-Yu},
year = {2023},
month = {01},
pages = {1-1},
title = {LiDA: Language-Independent Data Augmentation for Text Classification},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2023.3234019}
}



@misc{thangaraj2024crosslingualtransfermultilingualmodels,
      title={Cross-lingual transfer of multilingual models on low resource African Languages}, 
      author={Harish Thangaraj and Ananya Chenat and Jaskaran Singh Walia and Vukosi Marivate},
      year={2024},
      eprint={2409.10965},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.10965}, 
}

@misc{teehan2024collegeconceptembeddinggeneration,
      title={CoLLEGe: Concept Embedding Generation for Large Language Models}, 
      author={Ryan Teehan and Brenden Lake and Mengye Ren},
      year={2024},
      eprint={2403.15362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.15362}, 
}

@misc{sennrich2016improvingneuralmachinetranslation,
      title={Improving Neural Machine Translation Models with Monolingual Data}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1511.06709},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1511.06709}, 
}

@misc{lample2018unsupervisedmachinetranslationusing,
      title={Unsupervised Machine Translation Using Monolingual Corpora Only}, 
      author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
      year={2018},
      eprint={1711.00043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1711.00043}, 
}

@misc{wei2019edaeasydataaugmentation,
      title={EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks}, 
      author={Jason Wei and Kai Zou},
      year={2019},
      eprint={1901.11196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.11196}, 
}

@inproceedings{miller-1994-wordnet,
    title = "{W}ord{N}et: A Lexical Database for {E}nglish",
    author = "Miller, George A.",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",
    year = "1994",
    url = "https://aclanthology.org/H94-1111/"
}

@misc{kobayashi2018contextualaugmentationdataaugmentation,
      title={Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations}, 
      author={Sosuke Kobayashi},
      year={2018},
      eprint={1805.06201},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1805.06201}, 
}

@misc{chen2020mixtextlinguisticallyinformedinterpolationhidden,
      title={MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification}, 
      author={Jiaao Chen and Zichao Yang and Diyi Yang},
      year={2020},
      eprint={2004.12239},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.12239}, 
}

@misc{yu2017seqgansequencegenerativeadversarial,
      title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}, 
      author={Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},
      year={2017},
      eprint={1609.05473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.05473}, 
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
} 

@misc{jia2017adversarialexamplesevaluatingreading,
      title={Adversarial Examples for Evaluating Reading Comprehension Systems}, 
      author={Robin Jia and Percy Liang},
      year={2017},
      eprint={1707.07328},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1707.07328}, 
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
    eprint={1606.05250},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{li2023syntheticdatagenerationlarge,
      title={Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations}, 
      author={Zhuoyan Li and Hangxiao Zhu and Zhuoran Lu and Ming Yin},
      year={2023},
      eprint={2310.07849},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.07849}, 
}

@inproceedings{rahamim-etal-2023-text,
    title = "Text Augmentation Using Dataset Reconstruction for Low-Resource Classification",
    author = "Rahamim, Adir  and
      Uziel, Guy  and
      Goldbraich, Esther  and
      Anaby Tavor, Ateret",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.466/",
    doi = "10.18653/v1/2023.findings-acl.466",
    pages = "7389--7402",
    abstract = "In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation."
}

@inproceedings{karimi-etal-2021-aeda-easier,
    title = "{AEDA}: An Easier Data Augmentation Technique for Text Classification",
    author = "Karimi, Akbar  and
      Rossi, Leonardo  and
      Prati, Andrea",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.234/",
    doi = "10.18653/v1/2021.findings-emnlp.234",
    pages = "2748--2754",
    abstract = "This paper proposes AEDA (An Easier Data Augmentation) technique to help improve the performance on text classification tasks. AEDA includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than EDA method (Wei and Zou, 2019) with which we compare our results. In addition, it keeps the order of the words while changing their positions in the sentence leading to a better generalized performance. Furthermore, the deletion operation in EDA can cause loss of information which, in turn, misleads the network, whereas AEDA preserves all the input information. Following the baseline, we perform experiments on five different datasets for text classification. We show that using the AEDA-augmented data for training, the models show superior performance compared to using the EDA-augmented data in all five datasets. The source code will be made available for further study and reproduction of the results."
}

@article{Litake2024IndiTextBT,
  title={IndiText Boost: Text Augmentation for Low Resource India Languages},
  author={Onkar Litake and Niraj Yagnik and Shreyas Rajesh Labhsetwar},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.13085},
  url={https://api.semanticscholar.org/CorpusID:267200269}
}


@inproceedings{sahu-etal-2022-data,
    title = "Data Augmentation for Intent Classification with Off-the-shelf Large Language Models",
    author = "Sahu, Gaurav  and
      Rodriguez, Pau  and
      Laradji, Issam  and
      Atighehchian, Parmida  and
      Vazquez, David  and
      Bahdanau, Dzmitry",
    editor = "Liu, Bing  and
      Papangelis, Alexandros  and
      Ultes, Stefan  and
      Rastogi, Abhinav  and
      Chen, Yun-Nung  and
      Spithourakis, Georgios  and
      Nouri, Elnaz  and
      Shi, Weiyan",
    booktitle = "Proceedings of the 4th Workshop on NLP for Conversational AI",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlp4convai-1.5/",
    doi = "10.18653/v1/2022.nlp4convai-1.5",
    pages = "47--57",
    abstract = "Data augmentation is a widely employed technique to alleviate the problem of data scarcity. In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3. An advantage of this method is that no task-specific LM-fine-tuning for data generation is required; hence the method requires no hyper parameter tuning and is applicable even when the available training data is very scarce. We evaluate the proposed method in a few-shot setting on four diverse intent classification tasks. We find that GPT-generated data significantly boosts the performance of intent classifiers when intents in consideration are sufficiently distinct from each other. In tasks with semantically close intents, we observe that the generated data is less helpful. Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one. We present preliminary evidence that a prompting-based GPT classifier could be helpful in filtering the generated data to enhance its quality."
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  journal  = ""
}

@inproceedings{zhao-etal-2022-epida,
    title = "{EP}i{DA}: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification",
    author = "Zhao, Minyi  and
      Zhang, Lu  and
      Xu, Yi  and
      Ding, Jiandong  and
      Guan, Jihong  and
      Zhou, Shuigeng",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.349/",
    doi = "10.18653/v1/2022.naacl-main.349",
    pages = "4742--4752",
    abstract = "Recent works have empirically shown the effectiveness of data augmentation (DA) in NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully mine the potential of DA for NLP. In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effective text classification. EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EPiDA outperforms existing SOTA methods in most cases, though not using any agent networks or pre-trained generation networks, and it works well with various DA algorithms and classification models."
}

@misc{parvess2023bantuberta,
  author = {Parvess, Jesse},
  title = {BantuBERTa: Using Language Family Grouping in Multilingual Language Modeling for Bantu Languages},
  year = {2023},
  howpublished = {\url{http://hdl.handle.net/2263/92766}},
  note = {Mini Dissertation (MIT (Big Data Science)), University of Pretoria},
  institution = {University of Pretoria},
  keywords = {Multilingual language modeling, BantuBERTa, Bantu Languages},
  abstract = {This dissertation explores whether a multilingual Bantu pretraining corpus can be created from freely available data. The resulting multilingual language model, BantuBERTa, was predictive across multiple Bantu languages in higher-order NLP tasks and simpler NLP tasks, proving that the dataset can be used for multilingual pretraining and transfer to multiple Bantu languages. Despite underperforming compared to other models on NER tasks, it produced state-of-the-art results on African News Topic Classification. The research suggests that pretraining datasets of similar language families can benefit low-resourced languages in simpler NLP tasks.}
}

@inproceedings{ogueji-etal-2021-small,
    title = "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages",
    author = "Ogueji, Kelechi  and
      Zhu, Yuxin  and
      Lin, Jimmy",
    editor = "Ataman, Duygu  and
      Birch, Alexandra  and
      Conneau, Alexis  and
      Firat, Orhan  and
      Ruder, Sebastian  and
      Sahin, Gozde Gul",
    booktitle = "Proceedings of the 1st Workshop on Multilingual Representation Learning",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.mrl-1.11/",
    doi = "10.18653/v1/2021.mrl-1.11",
    pages = "116--126",
    abstract = "Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world`s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our {\textquotedblleft}small data{\textquotedblright} approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at \url{https://github.com/keleog/afriberta}."
}

@inproceedings{alabi-etal-2022-adapting,
    title = "Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning",
    author = "Alabi, Jesujoba O.  and
      Adelani, David Ifeoluwa  and
      Mosbach, Marius  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.382",
    pages = "4336--4349",
    abstract = "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) {---} fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50{\%}. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
}

@inproceedings{muhammadSemEval2023,
    title = {{SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)}},
    author = {Shamsuddeen Hassan Muhammad and Idris Abdulmumin and Seid Muhie Yimam and David Ifeoluwa Adelani and Ibrahim Sa'id Ahmad and Nedjma Ousidhoum and Abinew Ali Ayele and Saif M. Mohammad and Meriem Beloucif and Sebastian Ruder},
    booktitle = {Proceedings of the 17th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2023}})},
    publisher = {{Association for Computational Linguistics}},
    year = {2023}
}


@InProceedings{yimametalcoling2020,
    title = "Exploring {A}mharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models",
    author = "Yimam, Seid Muhie  and
      Alemayehu, Hizkiel Mitiku  and
      Ayele, Abinew  and
      Biemann, Chris",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    pages = "1048--1060"
}

@misc{amol2024statenlpkenyasurvey,
      title={State of NLP in Kenya: A Survey}, 
      author={Cynthia Jayne Amol and Everlyn Asiko Chimoto and Rose Delilah Gesicho and Antony M. Gitau and Naome A. Etori and Caringtone Kinyanjui and Steven Ndung'u and Lawrence Moruye and Samson Otieno Ooko and Kavengi Kitonga and Brian Muhia and Catherine Gitau and Antony Ndolo and Lilian D. A. Wanzare and Albert Njoroge Kahira and Ronald Tombe},
      year={2024},
      eprint={2410.09948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09948}, 
}

@article{10.1162/coli_a_00425,
    author = {Şahin, Gözde Gül},
    title = {To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP},
    journal = {Computational Linguistics},
    volume = {48},
    number = {1},
    pages = {5-42},
    year = {2022},
    month = {04},
    abstract = {Data-hungry deep neural networks have established themselves as the de facto standard for many NLP tasks, including the traditional sequence tagging ones. Despite their state-of-the-art performance on high-resource languages, they still fall behind their statistical counterparts in low-resource scenarios. One methodology to counterattack this problem is text augmentation, that is, generating new synthetic training data points from existing data. Although NLP has recently witnessed several new textual augmentation techniques, the field still lacks a systematic performance analysis on a diverse set of languages and sequence tagging tasks. To fill this gap, we investigate three categories of text augmentation methodologies that perform changes on the syntax (e.g., cropping sub-sentences), token (e.g., random word insertion), and character (e.g., character swapping) levels. We systematically compare the methods on part-of-speech tagging, dependency parsing, and semantic role labeling for a diverse set of language families using various models, including the architectures that rely on pretrained multilingual contextualized language models such as mBERT. Augmentation most significantly improves dependency parsing, followed by part-of-speech tagging and semantic role labeling. We find the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese. Our results suggest that the augmentation techniques can further improve over strong baselines based on mBERT, especially for dependency parsing. We identify the character-level methods as the most consistent performers, while synonym replacement and syntactic augmenters provide inconsistent improvements. Finally, we discuss that the results most heavily depend on the task, language pair (e.g., syntactic-level techniques mostly benefit higher-level tasks and morphologically richer languages), and model type (e.g., token-level augmentation provides significant improvements for BPE, while character-level ones give generally higher scores for char and mBERT based models).},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00425},
    url = {https://doi.org/10.1162/coli\_a\_00425},
    eprint = {https://direct.mit.edu/coli/article-pdf/48/1/5/2006622/coli\_a\_00425.pdf},
}

@inproceedings{percin-etal-2022-combining,
    title = "Combining {W}ord{N}et and Word Embeddings in Data Augmentation for Legal Texts",
    author = "Per{\c{c}}in, Sezen  and
      Galassi, Andrea  and
      Lagioia, Francesca  and
      Ruggeri, Federico  and
      Santin, Piera  and
      Sartor, Giovanni  and
      Torroni, Paolo",
    editor = "Aletras, Nikolaos  and
      Chalkidis, Ilias  and
      Barrett, Leslie  and
      Goanț{\u{a}}, C{\u{a}}t{\u{a}}lina  and
      Preoțiuc-Pietro, Daniel",
    booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nllp-1.4/",
    doi = "10.18653/v1/2022.nllp-1.4",
    pages = "47--52",
    abstract = "Creating balanced labeled textual corpora for complex tasks, like legal analysis, is a challenging and expensive process that often requires the collaboration of domain experts. To address this problem, we propose a data augmentation method based on the combination of GloVe word embeddings and the WordNet ontology. We present an example of application in the legal domain, specifically on decisions of the Court of Justice of the European Union.Our evaluation with human experts confirms that our method is more robust than the alternatives."
}

@inproceedings{Jahan2022,
  author    = {Md Saroar Jahan and Djamila Romaissa Beddiar and Mourad Oussalah and Muhidin Mohamed},
  title     = {Data Expansion Using WordNet-based Semantic Expansion and Word Disambiguation for Cyberbullying Detection},
  booktitle = {Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)},
  pages     = {1761--1770},
  year      = {2022},
  address   = {Marseille, France},
  month     = {June 20--25},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/2022.lrec-1.187.pdf},  
}

@phdthesis{Adelani2022,
  author       = {David Ifeoluwa Adelani},
  title        = {Natural Language Processing for African Languages},
  school       = {Universität des Saarlandes},
  year         = {2022},
  language     = {English},
  type         = {Dissertation},
  advisor      = {Dietrich Klakow},
  url          = {http://dx.doi.org/10.22028/D291-40305},
  doi          = {10.22028/D291-40305},
  urn          = {urn:nbn:de:bsz:291--ds-403051},
  hdl          = {20.500.11880/36297},
  keywords     = {NLP, African Languages, Word Embeddings, Low-Resource Languages},
  abstract     = {Recent advances in pre-training of word embeddings ... (shortened for brevity)}
}

@inproceedings{nie-etal-2023-cross,
    title = "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
    author = {Nie, Ercong  and
      Liang, Sheng  and
      Schmid, Helmut  and
      Sch{\"u}tze, Hinrich},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.528/",
    doi = "10.18653/v1/2023.findings-acl.528",
    pages = "8320--8340",
    abstract = "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1{\%}) and labeled settings (+16.3{\%}). PARC also outperforms finetuning by 3.7{\%}. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs."
}