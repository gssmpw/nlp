@inproceedings{InstantaneousSafeRL, author = {Li, Jingqi and Fridovich-Keil, David and Sojoudi, Somayeh and Tomlin, Claire J.}, title = {Augmented Lagrangian Method for Instantaneously Constrained Reinforcement Learning Problems}, year = {2021}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/CDC45484.2021.9683088}, doi = {10.1109/CDC45484.2021.9683088}, abstract = {In this paper, we study the Instantaneously Constrained Reinforcement Learning (ICRL) problem, in which we are tasked to find a reward-maximizing policy while satisfying certain constraints at each time step. We first extend a result on the strong duality of Constrained Markov Decision Process (CMDP) in the literature and propose a sufficient condition for strong duality of the ICRL problem. Inspired by the Augmented Lagrangian Method in constrained optimization, we propose a new surrogate objective function for ICRL, which could be efficiently optimized by common policy-gradient based RL algorithms. We show theoretically that a feasible and optimal policy could be obtained by optimizing this surrogate function, under certain conditions related to the feasible policy set. Our empirical results on a tabular Markov Decision Process and two nonlinear optimal control problems, a constrained pendulum and a constrained half-cheetah, justify our analysis, and suggest that our method could promote safety during learning and converge in a smaller number of iterations compared to the existing algorithms.}, booktitle = {2021 60th IEEE Conference on Decision and Control (CDC)}, pages = {2982–2989}, numpages = {8}, location = {Austin, TX, USA} }

@inproceedings{Knap-Brantley,
  author={Kianté Brantley and Miroslav Dudík and Thodoris Lykouris and Sobhan Miryoosefi and Max Simchowitz and Aleksandrs Slivkins and Wen Sun},
  title={Constrained episodic reinforcement learning in concave-convex and knapsack settings},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/bc6d753857fe3dd4275dff707dedf329-Abstract.html},
  booktitle={NeurIPS},
}

@inproceedings{Knap-PreBrantley,
	author = {Cheung, Wang Chi},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives},
	url = {https://proceedings.neurips.cc/paper/2019/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf}}

@article{NoViolationPolicyGradient, title={Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25826}, DOI={10.1609/aaai.v37i6.25826}, abstractNote={We consider the problem of constrained Markov decision process (CMDP) in continuous state actions spaces where the goal is to maximize the expected cumulative reward subject to some constraints. We propose a novel Conservative Natural Policy Gradient Primal Dual Algorithm (CNPGPD) to achieve zero constraint violation while achieving state of the art convergence results for the objective value function. For general policy parametrization, we prove convergence of value function to global optimal upto an approximation error due to restricted policy class. We improve the sample complexity of existing constrained NPGPD algorithm. To the best of our knowledge, this is the first work to establish zero constraint violation with Natural policy gradient style algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of proposed algorithm via experimental evaluations.}, number={6}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Bai, Qinbo and Singh Bedi, Amrit and Aggarwal, Vaneet}, year={2023}, month={6}, pages={6737-6744} }

@inproceedings{PreInstantaneous1, author = {Fisac, Jaime F. and Lugovoy, Neil F. and Rubies-Royo, Vicen\c{c} and Ghosh, Shromona and Tomlin, Claire J.}, title = {Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning}, year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/ICRA.2019.8794107}, doi = {10.1109/ICRA.2019.8794107}, abstract = {Safety analysis is a necessary component in the design and deployment of autonomous robotic systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to find approximate yet proficient solutions to optimal control problems in complex and high-dimensional systems, however their application has in practice been restricted to problems with an additive payoff over time, unsuitable for reasoning about safety. In recent work, we introduced a time-discounted modification of the problem of maximizing the minimum payoff over time, central to safety analysis, through a modified dynamic programming equation that induces a contraction mapping. Here, we show how a similar contraction mapping can render reinforcement learning techniques amenable to quantitative safety analysis as tools to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting control-theoretic safety analysis and the reinforcement learning domain. We validate the correctness of our formulation by comparing safety results computed through Q-learning to analytic and numerical solutions, and demonstrate its scalability by learning safe sets and control policies for simulated systems of up to 18 state dimensions using value learning and policy gradient techniques.}, booktitle = {2019 International Conference on Robotics and Automation (ICRA)}, pages = {8550–8556}, numpages = {7}, location = {Montreal, QC, Canada} }

@article{PreInstantaneous2,
title = {Safe Reinforcement Learning via Projection on a Safe Set: How to Achieve Optimality?},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {8076-8081},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2276},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320329360},
author = {Sebastien Gros and Mario Zanon and Alberto Bemporad},
keywords = {Safe Reinforcement Learning, safe projection, robust MPC},
abstract = {For all its successes, Reinforcement Learning (RL) still struggles to deliver formal guarantees on the closed-loop behavior of the learned policy. Among other things, guaranteeing the safety of RL with respect to safety-critical systems is a very active research topic. Some recent contributions propose to rely on projections of the inputs delivered by the learned policy into a safe set, ensuring that the system safety is never jeopardized. Unfortunately, it is unclear whether this operation can be performed without disrupting the learning process. This paper addresses this issue. The problem is analysed in the context of Q-learning and policy gradient techniques. We show that the projection approach is generally disruptive in the context of Q-learning though a simple alternative solves the issue, while simple corrections can be used in the context of policy gradient methods in order to ensure that the policy gradients are unbiased. The proposed results extend to safe projections based on robust MPC techniques.}
}

@inproceedings{Safe-RL-Imagining,
 author = {Thomas, Garrett and Luo, Yuping and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {13859--13869},
 publisher = {Curran Associates, Inc.},
 title = {Safe Reinforcement Learning by Imagining the Near Future},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/73b277c11266681122132d024f53a75b-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{SafeBarrier, title={End-to-End Safe Reinforcement Learning through Barrier Functions for Safety-Critical Continuous Control Tasks}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4213}, DOI={10.1609/aaai.v33i01.33013387}, abstractNote={&lt;p&gt;Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees &lt;em&gt;during&lt;/em&gt; the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) online learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both &lt;em&gt;guarantee&lt;/em&gt; safety and &lt;em&gt;guide&lt;/em&gt; the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.&lt;/p&gt; &lt;p&gt;Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous carfollowing with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms &lt;em&gt;and&lt;/em&gt; maintains safety during the entire learning process.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Cheng, Richard and Orosz, Gábor and Murray, Richard M. and Burdick, Joel W.}, year={2019}, month={Jul.}, pages={3387-3395} }

@article{SafeComprSurvey,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@article{SafeE4, author = {Bossens, David M. and Bishop, Nicholas}, title = {Explicit Explore, Exploit, or Escape (E4): Near-Optimal Safety-Constrained Reinforcement Learning in Polynomial Time}, year = {2022}, issue_date = {Mar 2023}, publisher = {Kluwer Academic Publishers}, address = {USA}, volume = {112}, number = {3}, issn = {0885-6125}, url = {https://doi.org/10.1007/s10994-022-06201-z}, doi = {10.1007/s10994-022-06201-z}, abstract = {In reinforcement learning (RL), an agent must explore an initially unknown environment in order to learn a desired behaviour. When RL agents are deployed in real world environments, safety is of primary concern. Constrained Markov decision processes (CMDPs) can provide long-term safety constraints; however, the agent may violate the constraints in an effort to explore its environment. This paper proposes a model-based RL algorithm called Explicit Explore, Exploit, or Escape (E4), which extends the Explicit Explore or Exploit (E3) algorithm to a robust CMDP setting. E4 explicitly separates exploitation, exploration, and escape CMDPs, allowing targeted policies for policy improvement across known states, discovery of unknown states, as well as safe return to known states. E4 robustly optimises these policies on the worst-case CMDP from a set of CMDP models consistent with the empirical observations of the deployment environment. Theoretical results show that E4 finds a near-optimal constraint-satisfying policy in polynomial time whilst satisfying safety constraints throughout the learning process. We then discuss E4 as a practical algorithmic framework, including robust-constrained offline optimisation algorithms, the design of uncertainty sets for the transition dynamics of unknown states, and how to further leverage empirical observations and prior knowledge to relax some of the worst-case assumptions underlying the theory.}, journal = {Mach. Learn.}, month = {6}, pages = {817–858}, numpages = {42}, keywords = {Safe artificial intelligence, Safe exploration, Model-based reinforcement learning, Robust Markov decision processes, Constrained Markov decision processes} }

@InProceedings{SafeHardBarrier,
  title = 	 {Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments},
  author =       {Wang, Yixuan and Zhan, Simon Sinong and Jiao, Ruochen and Wang, Zhilu and Jin, Wanxin and Yang, Zhuoran and Wang, Zhaoran and Huang, Chao and Zhu, Qi},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {36593--36604},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wang23as/wang23as.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wang23as.html},
  abstract = 	 {It is quite challenging to ensure the safety of reinforcement learning (RL) agents in an unknown and stochastic environment under hard constraints that require the system state not to reach certain specified unsafe regions. Many popular safe RL methods such as those based on the Constrained Markov Decision Process (CMDP) paradigm formulate safety violations in a cost function and try to constrain the expectation of cumulative cost under a threshold. However, it is often difficult to effectively capture and enforce hard reachability-based safety constraints indirectly with such constraints on safety violation cost. In this work, we leverage the notion of barrier function to explicitly encode the hard safety chance constraints, and given that the environment is unknown, relax them to our design of <em>generative-model-based soft barrier functions</em>. Based on such soft barriers, we propose a novel safe RL approach with bi-level optimization that can jointly learn the unknown environment and optimize the control policy, while effectively avoiding the unsafe region with safety probability optimization. Experiments on a set of examples demonstrate that our approach can effectively enforce hard safety chance constraints and significantly outperform CMDP-based baseline methods in system safe rates measured via simulations.}
}

@inproceedings{SafeLyapunov,
 author = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Lyapunov-based Approach to Safe Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/4fe5149039b52765bde64beb9f674940-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{SafeReview,
      title={A Review of Safe Reinforcement Learning: Methods, Theory and Applications}, 
      author={Shangding Gu and Long Yang and Yali Du and Guang Chen and Florian Walter and Jun Wang and Alois Knoll},
      year={2024},
      eprint={2205.10330},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.10330}, 
}

@article{SafeShielding, title={Safe Reinforcement Learning via Shielding}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11797}, DOI={10.1609/aaai.v32i1.11797}, abstractNote={ &lt;p&gt; Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Rüdiger and Könighofer, Bettina and Niekum, Scott and Topcu, Ufuk}, year={2018}, month={Apr.} }

@inproceedings{SafeStable,
 author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Safe Model-based Reinforcement Learning with Stability Guarantees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{SafeStatePAC,
  title = 	 { Provably Safe PAC-MDP Exploration Using Analogies },
  author =       {Roderick, Melrose and Nagarajan, Vaishnavh and Kolter, Zico},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1216--1224},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {4},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/roderick21a/roderick21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/roderick21a.html},
  abstract = 	 { A key challenge in applying reinforcement learning to safety-critical domains is understanding how to balance exploration (needed to attain good performance on the task) with safety (needed to avoid catastrophic failure). Although a growing line of work in reinforcement learning has investigated this area of "safe exploration," most existing techniques either 1) do not guarantee safety during the actual exploration process; and/or 2) limit the problem to a priori known and/or deterministic transition dynamics with strong smoothness assumptions. Addressing this gap, we propose Analogous Safe-state Exploration (ASE), an algorithm for provably safe exploration in MDPs with unknown, stochastic dynamics. Our method exploits analogies between state-action pairs to safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE also guides exploration towards the most task-relevant states, which empirically results in significant improvements in terms of sample efficiency, when compared to existing methods. }
}

@inproceedings{SafeStateSurvey,
author = {Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
title = {State-wise safe reinforcement learning: a survey},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/763},
doi = {10.24963/ijcai.2023/763},
abstract = {Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potential future directions.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {763},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@article{cMDP-Actor-Critic,
title = {An actor-critic algorithm for constrained Markov decision processes},
journal = {Systems \& Control Letters},
volume = {54},
number = {3},
pages = {207-213},
year = {2005},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2004.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167691104001276},
author = {V.S. Borkar},
keywords = {Actor-critic algorithms, Reinforcement learning, Constrained Markov decision processes, Stochastic approximation, Envelope theorem},
abstract = {An actor-critic type reinforcement learning algorithm is proposed and analyzed for constrained controlled Markov decision processes. The analysis uses multiscale stochastic approximation theory and the `envelope theorem' of mathematical economics.}
}

@InProceedings{cMDP-Model-Violation-Free,
  title = 	 { Triple-Q: A Model-Free Algorithm for Constrained Reinforcement Learning with Sublinear Regret and Zero Constraint Violation },
  author =       {Wei, Honghao and Liu, Xin and Ying, Lei},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3274--3307},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {3},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/wei22a/wei22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/wei22a.html},
  abstract = 	 { This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three “Q” values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }{\delta}H^4 S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\delta$ is Slater’s constant. Furthermore, {Triple-Q} guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of {Triple-Q} is similar to SARSA for unconstrained MDPs, and is computationally efficient. }
}

@inproceedings{cMDP-ZeroDualityGap,
 author = {Paternain, Santiago and Chamon, Luiz and Calvo-Fullana, Miguel and Ribeiro, Alejandro},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Constrained Reinforcement Learning Has Zero Duality Gap},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/c1aeb6517a1c7f33514f7ff69047e74e-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{cMDP-book,
  title={Constrained Markov Decision Processes},
  publisher={Chapman and Hall/CRC},
  author={Eitan Altman},
  year={1999},
  doi = {10.1201/9781315140223}
}

@article{cMDP-sample-complexity-safe, title={Learning with Safety Constraints: Sample Complexity of Reinforcement Learning for Constrained MDPs}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16937}, DOI={10.1609/aaai.v35i9.16937}, abstractNote={Many physical systems have underlying safety considerations that require that the policy employed ensures the satisfaction of a set of constraints. The analytical formulation usually takes the form of a Constrained Markov Decision Process (CMDP). We focus on the case where the CMDP is unknown, and RL algorithms obtain samples to discover the model and compute an optimal constrained policy. Our goal is to characterize the relationship between safety constraints and the number of samples needed to ensure a desired level of accuracy---both objective maximization and constraint satisfaction---in a PAC sense. We explore two classes of RL algorithms, namely, (i) a generative model based approach, wherein samples are taken initially to estimate a model, and (ii) an online approach, wherein the model is updated as samples are obtained. Our main finding is that compared to the best known bounds of the unconstrained regime, the sample complexity of constrained RL algorithms are increased by a factor that is logarithmic in the number of constraints, which suggests that the approach may be easily utilized in real systems.}, number={9}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={HasanzadeZonuzy, Aria and Bura, Archana and Kalathil, Dileep and Shakkottai, Srinivas}, year={2021}, month={5}, pages={7667-7674} }

