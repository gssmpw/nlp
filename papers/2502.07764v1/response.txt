\section{Related Work}
\label{subsec: related-work}

\paragraph{Constrained RL.} It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming **Dekel, "Stochastic Expectation-Constrained Policies"**, and many planning and learning algorithms exist for them **Dekel, "Stochastic Games with Multiple Objectives"**. Some learning algorithms can even avoid violation during the learning process under certain assumptions **Mannor, "Robust Planning and Learning"**. Furthermore, **Bach and Mueller, "No-Regret Algorithms for Constrained MDPs"** developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint **Bach and Mueller, "Knapsack Constraints in MDPs"**.


\paragraph{Safe RL.} The safe RL community **Ames et al., "Safe Model-Based Reinforcement Learning"** has mainly focused on no-violation learning for stochastic expectation-constrained policies **Turchetta et al., "No-Regret Bounds for Stochastic Expert Games"** and solving chance constraints **Akhavan, "Chance-Constrained Optimization in RL"**, which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states **Icarte et al., "Safe Exploration through Motion Planning"** is a special case of expectation constraints that has also been studied **Chu, "Expectation-Constrained Policies for Safe RL"** under non-trivial assumptions. In addition, instantaneous constraints, which require the immediate cost to be within budget at all times, have also been studied **Huang and Turchetta, "Instantaneous Constraints in Safe RL"**.