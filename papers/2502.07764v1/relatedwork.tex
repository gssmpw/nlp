\section{Related Work}
\label{subsec: related-work}

\paragraph{Constrained RL.} It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming ~\cite{cMDP-book}, and many planning and learning algorithms exist for them ~\cite{cMDP-ZeroDualityGap, cMDP-Pac, cMDP-Actor-Critic, cMDP-sample-complexity-safe}. Some learning algorithms can even avoid violation during the learning process under certain assumptions~\cite{cMDP-Model-Violation-Free, NoViolationPolicyGradient}. Furthermore, \citet{Knap-Brantley} developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint~\cite{Knap-Brantley, Knap-PreBrantley}.


\paragraph{Safe RL.} The safe RL community~\cite{SafeComprSurvey, SafeReview} has mainly focused on no-violation learning for stochastic expectation-constrained policies~\cite{SafeLyapunov, SafeE4, SafeShielding, SafeBarrier, SafeStable} and solving chance constraints~\cite{SafeHardBarrier, SafeStateSurvey}, which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states~\cite{SafeStateSurvey} is a special case of expectation constraints that has also been studied~\cite{SafeStatePAC, Safe-RL-Imagining} under non-trivial assumptions. In addition, instantaneous constraints, which require the immediate cost to be within budget at all times, have also been studied~\cite{InstantaneousSafeRL, PreInstantaneous1, PreInstantaneous2}.