\documentclass[pdftex, a4paper, 12pt]{article}
\usepackage[square, numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{algos}
\usepackage{meta}

\newcommand{\jeremy}[1]{\textcolor{red}{#1}}
\newcommand{\supp}{\text{Supp}}
\newcommand{\round}[2]{\ceil{#2}_{#1}}
\newcommand{\mB}{\mathcal{B}}
\newcommand{\hb}{\hat{b}}
\newcommand{\cmin}{c_{min}}
\newcommand{\cmax}{c_{max}}
\newcommand{\hB}{\hat{\mathcal{B}}}
\newcommand{\tV}{\Tilde{V}}
\newcommand{\tC}{\Tilde{C}}
\newcommand{\gPi}{\overline{\Pi}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\bmax}{b_{max}}
\newcommand{\bmin}{b_{min}}
\newcommand{\oR}{\overline{R}}
\newcommand{\hR}{\hat{R}}
\newcommand{\hf}{\hat{f}}
\newcommand{\smax}{s_{max}}
\newcommand{\smin}{s_{min}}
\newcommand{\tc}{\Tilde{c}}
\newcommand{\hF}{\hat{F}}
\newcommand{\hmF}{\hat{\mathcal{F}}}
\newcommand{\hmB}{\hat{\mathcal{B}}}

\DeclareMathOperator*{\f}{\mathit{f}}
\DeclareMathOperator*{\g}{\mathit{g}}


\title{Polynomial-Time Approximability of Constrained Reinforcement Learning}

\author{Jeremy McMahan \thanks{University of Wisconsin-Madison. Corresponding author: \href{mailto:jmcmahan@wisc.edu}{\texttt{jmcmahan@wisc.edu}}}}

\begin{document}

\maketitle

\begin{abstract}

    We study the computational complexity of approximating general constrained Markov decision processes. Our primary contribution is the design of a polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for finding optimal constrained policies across a broad class of recursively computable constraints, including almost-sure, chance, expectation, and their anytime variants. Matching lower bounds imply our approximation guarantees are optimal so long as $P \neq NP$. The generality of our approach results in answers to several long-standing open complexity questions in the constrained reinforcement learning literature. Specifically, we are the first to prove polynomial-time approximability for the following settings: policies under chance constraints, deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes.
    
\end{abstract}



\section{Introduction}\label{sec: intro}

Constrained Reinforcement Learning (CRL) is growing increasingly crucial for managing complex, real-world applications such as medicine~\cite{MedSurvery, MedRisk, MedScheduling}, disaster relief~\cite{DisasterDigitalTwin, DisasterUAV, DisasterFlooding}, and resource management~\cite{ResourceGeneral, ResourceNetworkSlicing, ResourceUAV, ResourceMatching}. Various constraints, including expectation~\cite{cMDP-book}, chance~\cite{CCMDP-GoalComplexity}, almost-sure~\cite{AlmostSure}, and anytime constraints~\cite{acRL}, were each proposed to address new challenges. Despite the richness of the literature, most works focus on stochastic, expectation-constrained policies, leaving many popular settings with longstanding open problems. Even chance constraints, arguably a close second in popularity, still lack any polynomial-time, even approximate, algorithms despite being introduced over a decade ago~\cite{CCMDP-GoalComplexity}. Other settings for which polynomial-time algorithms are open include deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes. Consequently, we study the computational complexity of general constrained problems to resolve many of these fundamental open questions.

%\textcite{QuantileConstrained} argued that using Cvar constraints is not the right approach to capture probabilities of failure and instead proposed solving CCMDPs. In a CCMDP~\cite{OG-CCMDPs}, the agent seeks to maximize its reward subject to its probability of violating its budget being bounded. Many works have devised planning~\cite{QuantileConstrained, CCMDPPlanPredictors, CCSSPaircraft, RAO-CCMDP, Vulcan-CCMDP} and learning~\cite{CCMDP-RL, CCMDP-RL-Batch} algorithms for CCMDPs. CCMPDs are also a popular model in for safe reinforcement learning as they can constrain the probability of entering dangerous states. In principle, an optimal policy for CCMDPs would capture our hard constraint setting, but past algorithms~\cite{Percentile-Risk-MDPs, QuantileConstrained} do not apply to the probability $0$ regime which captures them. Regardless, CCMDPs are known to be NP-hard~\cite{ProbGoalMDP}, and approximation algorithms are only known for small time horizons~\cite{CCPOMDP}. Even the special case of Chance-constrained stochastic shortest paths~\cite{CC-SSP} is known to be NP-hard, and the state-of-the-art consists of exact Integer LP formulations. 

%Other important settings for which polynomial-time algorithms are open include: 
%In this work, we study the approximation complexity of a broad class of constraints in order to address these open problems. 
%Moreover, existing methods are specialized to a single constraint type, with no known work allowing multiple constraint types. 

%\paragraph{Motivation.} With great advances in reinforcement learning, much of the literature has pushed constrained formulations to better capture practical problems. Although much is known about stochastic policies for expectation-constrained problems, deterministic policies and most other constraint types remain understudied. In fact, chance constraints, which are arguably a close second in popularity to expectation constraints, are still unknown to even have a polynomial-time approximation algorithm despite their introduction more than a decade ago. Even more surprisingly, all known constrained methods are specialized to one constraint type; no known work can reliably compute policies satisfying multiple different types of constraints. We aim to remedy these gaps in the literature by developing a single unifying approximation algorithm that can provably handle the major constraint formulations studied in the literature. 
%\jeremy{Change to many open problems that need to be addressed?}

Formally, we study the solution of \emph{Constrained Markov Decision Processes} (CMDPs). Here, we define a CMDP through three fundamental parts: (1) a MDP $M$ that accumulates both rewards and costs, (2) a general cost criterion $C$, and (3) a budget vector $B$. Additionally, we allow the agent to specify whether they require their policy to be deterministic or stochastic, formalized through a goal policy class $\gPi$. The agent's goal is to solve $\max_{\pi \in \gPi} V^{\pi}_M$ subject to $C_M^{\pi} \leq B$, where $V^{\pi}_M$ denotes the agent's value and $C^{\pi}_M$ denotes the agent's cost under $\pi$. Our main question is the following:

\begin{quote}
    Can general CMDPs be approximated in polynomial time? 
\end{quote}

\paragraph{Hardness.} Solving general CMDPs is notoriously challenging. When restricted to deterministic policies, solving a CMDP with just one constraint is NP-hard~\cite{cMDP-hardness-OG, CCMDP-GoalComplexity,acRL, dcRL}. This difficulty increases with the number of constraints: with at least two constraints, finding a feasible deterministic policy, let alone a near-optimal one becomes NP-hard~\cite{acRL}. Even if we relax the deterministic requirement, this hardness remains for all constraint types other than expectation. Computational hardness aside, standard RL techniques fail to apply due to the combinatorial nature induced by many constraint types. Adding in additional constraints with fundamentally different structures then further complicates the problem.   

\paragraph{Past Work.} A few works have managed to derive provable approximation algorithms for some cases of CRL. ~\citet{dcRL} presented a fully polynomial-time approximation scheme (FPTAS) for the computation of deterministic policies of a general class of constraints, which includes expectation, almost-sure, and anytime constraints. Although powerful, their framework only works for one constraint and fails to capture anytime-expectation constraints along with chance constraints. Similarly, ~\citet{cMDP-deterministic-constH} achieves an FPTAS for expectation and chance constraints, but only in the constant horizon setting.
In contrast, ~\citet{acRL} develops a polynomial-time $(0,\epsilon)$-additive bicritiera approximation algorithms for anytime and almost-sure constraints. However, their framework is specialized to those constraint types and thus fails for our purpose. In contrast, ~\citet{CCMDP-GoalComplexity} developed a pseudo-polynomial time algorithm for finding feasible chance-constrained policies, but their methods do not lead to polynomial-time solutions. 
%\jeremy{Cost augmentation fails as the cumulative cost is not a sufficient statistic for expectation and other constraints, unlike the case for anytime constraints.}

\paragraph{Our Contributions.} We design a polynomial-time $(0,\epsilon)$-additive bicriteria approximation algorithm for tabular, SR-criterion CMDPs. An SR criterion is required to satisfy a generalization of the policy evaluation equations and includes expectation, chance, and almost-sure constraints along with their anytime equivalents. Our framework implies the first positive polynomial-time approximability results for (1) policies under chance constraints, (2) deterministic policies under multiple expectation constraints, and (3) policies under non-homogeneous constraints -- each of which has been unresolved for over a decade. We then extend our algorithm into a polynomial-time $(\epsilon,\epsilon)$-additive bicriteria approximation algorithm for continuous-state CMPDs under a general class of constraints, which includes expectation, almost-sure, and anytime constraints.  

%Moreover, our bicriteria approximation guarantees are best possible under worst-case analysis: no other method can achieve better guarantees in polynomial time.
%Lastly, we demonstrate how our techniques can easily translate to function approximation settings and demonstrate their efficacy on the famous knapsack problem, which is a special case of constrained formulations. 
%\jeremy{Don't forget the multi-objective implications of our framework!!}

\paragraph{Our Techniques.} Our algorithm requires several key techniques. First, we transform a constraint concerning all realizable histories into a simpler per-time constraint. We accomplish this by augmenting the state space with an artificial budget and augmenting the action space to choose future budgets to satisfy the constraint. However, bellman updates then become as hard as the knapsack problem due to the large augmented action space. For tabular cMDPs, we show the bellman updates can be approximately computed using dynamic programming and rounding. By strategically rounding the artificial budget space, we achieve an $(0,\epsilon)$-bicriteria approximation for tabular CMDPs. By appropriately discretizing the continuous state space, our method becomes an $(\epsilon,\epsilon)$-bicriteria approximation algorithm for continuous state CMDPs. 


\subsection{Related Work}\label{subsec: related-work}

\paragraph{Constrained RL.} It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming ~\cite{cMDP-book}, and many planning and learning algorithms exist for them ~\cite{cMDP-ZeroDualityGap, cMDP-Pac, cMDP-Actor-Critic, cMDP-sample-complexity-safe}. Some learning algorithms can even avoid violation during the learning process under certain assumptions~\cite{cMDP-Model-Violation-Free, NoViolationPolicyGradient}. Furthermore, \citet{Knap-Brantley} developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint~\cite{Knap-Brantley, Knap-PreBrantley}.


\paragraph{Safe RL.} The safe RL community~\cite{SafeComprSurvey, SafeReview} has mainly focused on no-violation learning for stochastic expectation-constrained policies~\cite{SafeLyapunov, SafeE4, SafeShielding, SafeBarrier, SafeStable} and solving chance constraints~\cite{SafeHardBarrier, SafeStateSurvey}, which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states~\cite{SafeStateSurvey} is a special case of expectation constraints that has also been studied~\cite{SafeStatePAC, Safe-RL-Imagining} under non-trivial assumptions. In addition, instantaneous constraints, which require the immediate cost to be within budget at all times, have also been studied~\cite{InstantaneousSafeRL, PreInstantaneous1, PreInstantaneous2}.

\section{Constraints}\label{sec: constraints}

\paragraph{Cost-Accumulating MDPs.} In this work, we consider environments that accumulate both rewards and costs. Formally, we consider a (finite-horizon, tabular) \emph{cost-accumulating Markov Decision Process} (caMDP) tuple $M = (H, \mS, \mA, P, R, C, s_0)$, where (i) $H$ is the finite time \emph{horizon}, (ii) $\mS_h$ is the finite set of \emph{states}, (iii) $\mA_h(s)$ is the finite set of available \emph{actions}, (iv) $P_h(s, a) \in \Delta(S)$ is the \emph{transition} distribution, (v) $R_h(s, a) \in \Delta(\Real)$ is the \emph{reward} distribution, (vi) $C_h(s,a) \in \Delta(\Real^m)$ is the \emph{cost} distribution, and (vii) $s_0 \in \mS$ is the initial state. 

To simplify notation, we let $r_h(s,a) \defeq \E[R_h(s,a)]$ denote the expected reward, $S \defeq |\mS|$ denote the number of states, $A \defeq |\mA|$ denote the number of joint actions, $[H] \defeq \set{1, \ldots, H}$, $\mathcal{M}$ be the set of all caMDPs, and $|M|$ be the total description size of the caMDP. We also use the Iverson Bracket notation $[P] \defeq 1_{\{P = True\}}$ and the characteristic function $\chi_{P}$ which is $\infty$ when $P$ is False and $0$ otherwise. 


\paragraph{Agent Interactions.} The agent interacts with $M$ using a \emph{policy} $\pi = \{\pi_h\}_{h = 1}^H$. In the fullest generality, $\pi_{h}: \mH_h \to \Delta(\mA)$ is a mapping from the observed history at time $h$ (including costs) to a distribution of actions. Often, researchers study \emph{Markovian policies}, which take the form $\pi_h : \mS \to \Delta(\mA)$, and \emph{deterministic policies}, which take the form $\pi_h: \mH_h \to \mA$. We let $\Pi$ denote the set of all policies and $\Pi^D$ denote the set of all deterministic policies.

The agent starts in the initial state $s_0$ with observed history $\tau_1 = (s_0)$. For any $h \in [H]$, the agent chooses a joint action $a_h \sim \pi_h(\tau_h)$. Then, the agent receives immediate reward $r_h \sim R_h(s,a)$ and cost vector $c_h \sim C_h(s,a)$. Lastly, $M$ transitions to state $s_{h+1} \sim P_h(s_h,a_h)$, prompting the agent to update its observed history to $\tau_{h+1} = (\tau_h, a_h, c_h, s_{h+1})$. This process is repeated for $H$ steps; the interaction ends once $s_{H+1}$ is reached.


\paragraph{Constrained Processes.} Suppose the agent has a \emph{cost criterion} $C : \mM \times \Pi \to \Real^m$ and a corresponding \emph{budget} vector $B \in \Real^m$. We refer to the tuple $(M, C, B)$ as a \emph{Constrained Markov Decision Process} (CMDP). Given a CMDP and desired policy class $\gPi \in \{\Pi^D, \Pi\}$., the agent's goal is to solve the constrained optimization problem: 
\begin{equation}\tag{CON}\label{equ: constrained}
\begin{split}
    \max_{\pi \in \gPi} \enspace & V_M^{\pi} \\
    \text{s.t.} \enspace & C_{M}^{\pi} \leq B 
\end{split}
\end{equation}
In the above, $V^{\pi}_M \defeq \E^{\pi}_M\brak{\sum_{h = 1}^H r_h(s_h,a_h)}$ denotes the value of a policy $\pi$, $\E^{\pi}_M$ denotes the expectation defined by $\P_M^{\pi}$, and $\P^{\pi}_M$ denotes the probability law over histories induced from the interaction of $\pi$ with $M$. Lastly, we let $V^*$ denote the optimal solution value to \eqref{equ: constrained}.

\paragraph{SR Criteria.} We study cost criteria that generalize the standard policy evaluation equations to enable dynamic programming techniques. In particular, we require the cost of a policy to be recursively computable with respect to the time horizon. For our later approximations in \cref{sec: bicriteria}, we will also need key functions defining the recursion to be short maps, i.e., $1$-Lipschitz, with respect to the infinity norm.

%\begin{proposition}[Constraint Formulations]
%    The classical constraints and their anytime variants can be modeled by a packing constraint as follows:
%    \begin{enumerate}
%        \item Expectation Constraints -- $C_M^{\pi} \defeq \E_M\brak{\sum_{h = 1}^H c_h} \leq B$. Anytime Variant -- $C_{M,t}^{\pi} \defeq \E_M^{\pi}\brak{\sum_{h = 1}^t c_h} \leq B$ for all $t \in [H]$.
%        \item Chance Constraints -- $C_M^{\pi} \defeq \P^{\pi}_M\brak{\sum_{h = 1}^H c_h > B} \leq \delta$. Anytime Variant -- $C_M^{\pi} \defeq \P^{\pi}_M\brak{\exists t \in [H], \; \sum_{h = 1}^t c_h > B} \leq \delta$.
%        \item Almost-sure Constraints -- $C_M^{\pi} \defeq \max_{T \subseteq \mH_{H+1}} \E_M^{\pi}\brak{\sum_{h = 1}^H c_h \mid T} \leq B$.
%    \end{enumerate}
%    The anytime variants can all be captured by repeating each constraint $H$ times, one for each terminal time $t$.
%    \jeremy{OPTIONAL: write the general C forms for constraints and anytime constraints here, and then do the SR variant seperately? Could avoid complex lists leading to better readability actually.}
%\end{proposition}

\begin{definition}[SR]\label{def: sr}
    We call a cost criterion \emph{shortly recursive} (SR) if for any caMDP $M$ and any policy $\pi \in \Pi^D$, $\pi$'s cost decomposes recursively into $C^{\pi}_M = C_1^{\pi}(s_0)$, where $C_{H+1}^{\pi} = 0$ and for all $h \in [H]$ and $\tau_h \in \mH_h$ letting $s = s_h(\tau_h)$ and $a = \pi_h(\tau_h)$, 
    \begin{equation}\label{equ: sr}\tag{SR}
        C_h^{\pi}(\tau_h) = c_h(s,a) + \f_{s'}\g\paren{P_h\paren{s' \mid s,a}} C^{\pi}_{h+1}\paren{\tau_h, a, s'}.
    \end{equation}
    Here, $\f_{s'}$ is the finite extension of an associative, non-decreasing, binary function $\f$, and $\g$ is a $[0,1]$-valued function rooted at $0$. Moreover, we require that $\f$ is a short map when either of its inputs are fixed, satisfies $f(0,x) = f(x,0) = x$ for all $x$, and when combined with $\g$, i.e., $\f_{s'}\g\paren{P_h\paren{s' \mid s,a}}x_{s'}$, is a short map in $x$. 
\end{definition}

\begin{remark}[Stochastic Variants]\label{remark: stochastic}
    Our results generalize to both stochastic policies and stochastic costs as well. The algorithmic approach is identical, but the definitions and analysis are more complex. Consequently, we focus on the deterministic cases in the main text.
\end{remark}

%\begin{remark}[Non-Homogeneity]\label{remark: non-homogenous}
    %\jeremy{Optional explicitly mention how non-homogeneous constraints are modeled through vectors!}
%\end{remark}

\paragraph{Constraint Formulations.} The fundamental constraints considered in the CRL literature are Expectation, Chance, and Almost-sure constraints. Each of these induces a natural \emph{anytime} variant that stipulates the required constraint must hold for the truncated cost $\sum_{h = 1}^t c_h$ at all times $h \in [H]$. We give the formal definitions in \cref{fig: constraints}. Importantly, each constraint is equivalent to $C^{\pi}_M \leq B'$ for some appropriately chosen SR criteria. 
%\jeremy{This is because chance constraints are more computationally difficult than the other constraint formulations.}


\begin{figure}
    \centering
    \begin{tabular}{c | c | c | c}
    Con/Time & Expectation & Chance & Almost-Sure\\
    \hline
    Classical &  $\displaystyle\E^{\pi}_M\brak{\sum_{h = 1}^H c_h} \leq B$ & $\displaystyle\P^{\pi}_M\brak{\sum_{h = 1}^H c_h > B} \leq \delta$ & $\displaystyle\P^{\pi}_M\brak{\sum_{h = 1}^H c_h \leq B} = 1$\\
    \hline
    Anytime ($\forall t \in [H]$) & $\displaystyle\E^{\pi}_M\brak{\sum_{h = 1}^t c_h} \leq B$ & $\displaystyle\P^{\pi}_M\brak{\sum_{h = 1}^t c_h > B} \leq \delta$ & $\displaystyle\P^{\pi}_M\brak{\sum_{h = 1}^t c_h \leq B} = 1$
    \end{tabular}
    \caption{The Constraint Landscape}
    \label{fig: constraints}
\end{figure}

\begin{proposition}[SR Modeling]\label{prop: modeling}
    The classical constraints can be modeled by SR constraints of the form $C_M^{\pi} \leq B'$ as follows:
    \begin{enumerate}
        \item Expectation Constraints -- $\f(x,y) \defeq x+y$, $\g(x) \defeq x$, and $B' \defeq B$.
        \item Chance Constraints -- $(\f,\g)$ defined as above and $B' \defeq \delta$. Here, we assume M's states are augmented with cumulative costs and that $c_h((s,\oc), a) \defeq [c_h(s,a) + \oc > B]$ for the anytime variant and $c_h((s,\oc), a) \defeq [c_h(s,a) + \oc > B][h = H]$ otherwise.
        \item Almost-sure Constraints -- $\f(x,y) \defeq \max(x,y)$, $\g(x) \defeq [x > 0]$, and $B' \defeq B$. Anytime variant -- $\f(x,y) \defeq \max(0, \max(x,y))$ while $\g$ and $B'$ remain the same. 
    \end{enumerate} 
    General anytime variants, including anytime expectation constraints, can be modeled by $\set{C^{\pi}_{M,t} \leq B}_{t \in [H]}$ where $C^{\pi}_{M,t}$ is the original SR criterion but defined for the truncated-horizon process with horizon $t$. 
\end{proposition}

%\begin{remark}[Anytime Variants]

%    Most anytime variants can be directly modeled by a SR cost constraint as shown in \cref{prop: modeling}. However, for anytime expectation and other anytime constraints, we can achieve nearly the same guarantees with a slight modification of our framework. The main idea is to iteratively solve time-truncated instances of the problem and then take the intersection of the solutions. We defer the details to the appendix.
    
%    Although antyime variants can easily be formulated as $H$ constraints, solving a CMDP with that many constraints we show is computationally intractable. Instead, we directly handle such constraints efficiently in the appendix. \jeremy{Check if they are NP-hard if cannot be directly formulated like the expectation case!}
%    \jeremy{What about for non-stationary budgets? Do we know if that is possible or not? Seems like it should follow easily using the truncated approach.}
%\end{remark}

\paragraph{Computational Limitations.} It is known that computing feasible policies for CMDPs is NP-hard ~\cite{dcRL, acRL}. As such, we must relax feasibility for any hope of polynomial time algorithms. Consequently, we focus on \emph{bicriteria} approximation algorithms.


\begin{definition}[Bicriteria]\label{def: bicriteria}
    A policy $\pi$ is an \emph{$(\alpha, \beta)$-additive bicriteria approximation} to a CMDP $(M, C, B)$ if $V^{\pi}_M \geq V^* - \alpha$ and $C^{\pi}_M \leq B + \beta$. We refer to an algorithm as an \emph{$(\alpha, \beta)$-bicriteria} if for any CMDP it outputs an \emph{$(\alpha, \beta)$-additive bicriteria approximation} or correctly reports the instance is infeasible. 
\end{definition}

%\begin{remark}[Relative bicriteria]
%    \jeremy{Say relative approximations are not possible outside what can be captured by additive anyway? Unless costs non-negative! then relative approx works but again can be done through additive.}
%\end{remark}

The existence of a polynomial-time bicriteria for our general constrained problem implies brand-new approximability results for many classic problems in the CRL literature. For clarity, we will explicitly state the complexity-theoretic implications for each classical setting. 

\begin{theorem}[Implications]\label{thm: results}
    A polynomial-time $(\epsilon, \epsilon)$-bicriteria implies that in polynomial time it is possible to compute a policy $\pi \in \gPi$ satisfying $V^{\pi}_M \geq V^* - \epsilon$ and any constant combination of the following constraints:
    \begin{enumerate}
        \item $\E_M^{\pi}\brak{\sum_{h = 1}^H c_h} \leq B + \epsilon$
        \item $\P^{\pi}_M\brak{\sum_{h = 1}^H c_h \leq B + \epsilon} = 1$
        \item $\P^{\pi}_M\brak{\sum_{h = 1}^H c_h > B + \epsilon} \leq \delta + \epsilon$.
    \end{enumerate}
    In other words, polynomial-time approximability is possible for each of the settings described in \cref{sec: intro} when the number of constraints is constant. 
    %\jeremy{Moreover, these guarantees minus the delta increase for chance constraints are best possible?}
\end{theorem}

\begin{remark}[Extensions]\label{remark: environments}
    All of our results hold for Markov games and infinite discounted settings. %We defer their analysis to the appendix.
\end{remark}

\section{Reduction}\label{sec: reduction}

In this section, we present a general solution approach to SR-criterion CMDPs. Our revolves around converting the general cost constraint into a per-step action constraint. This leads to the design of an augmented MDP that can be solved with standard RL methods. 

\paragraph{Augmentation.} State augmentation is the known approach for solving anytime-constrained MDPs~\cite{acRL}. The augmentation permits the problem to be solved by the following dynamic program:
\begin{equation}
    V^*_h(s,c) = \max_{\substack{a \in \mA, \\ c + c_h(s,a)\leq B}} r_h(s,a) + \sum_{s'} P_h(s' \mid s,a)V^*_{h+1}(s, c+c_h(s,a)).
\end{equation}
When moving to other constraints, the cumulative cost may no longer suffice to determine constraint satisfaction. For example, the expected cost depends on the cumulative cost of all realizable branches, not just the current branch.

\paragraph{Expectation Constraints.} Instead, we can exploit the recursive nature of the expected cost to find a solution. Suppose at stage $(s,h)$ we impose an artificial budget $b$ on the expected cost of a policy $\pi$ from time $h$ onward: $\E^{\pi}\brak{\sum_{t = h}^H c_t} \leq b$. By the policy evaluation equations, we know this equates to satisfying:
\begin{equation}
    C_h^{\pi}(s) = c_h(s,a) + \sum_{s'} P_h(s' \mid s,a) C_{h+1}^{\pi}(s') \leq b.
\end{equation}
For this invariant to be satisfied, it suffices for the agent to choose future artificial budgets $b_{s'}$ for $s' \in \mS$ satisfying,
\begin{equation}\label{equ: budgets}
    c_h(s,a) + \sum_{s'} P_h(s' \mid s,a) b_{s'} \leq b.
\end{equation}
and ensure the future artificial budgets are obeyed inductively: $C_{h+1}^{\pi}(s', b_{s'}) \leq b_{s'}$.

\paragraph{General Approach.} We can apply the same reasoning for general recursively computable cost criteria. If $C$ is SR, then we know that $C_h^{\pi}(s)$ obeys \eqref{equ: sr}. Thus, to guarantee that $C_h^{\pi}(s) \leq b$ it suffices to choose $b_{s'}$'s satisfying,
\begin{equation}
    c_h(s,a) + \f_{s'}\g\paren{P_h(s' \mid s,a)} b_{s'} \leq b,
\end{equation}
and inductively guarantee that $C_{h+1}^{\pi}(s') \leq b_{s'}$.

We can view choosing future artificial budgets as part of the agent's augmented actions. Then, at any augmented state $(s,b)$, the agent's augmented action space includes all $(a,\bb) \in \mA \times \Real^{S}$ satisfying \eqref{equ: budgets}. When $M$ transitions to $s' \sim P_h(s, a)$, the agent's new augmented state should consist of the environment's new state in addition to its chosen demand for that state, $(s', b_{s'})$. Putting these pieces together yields the definition of the reduced, action-constrained, MDP, \cref{def: reduced-mdp}. 

\begin{definition}[Reduced MDP]\label{def: reduced-mdp}
    Given any SR-criterion CMDP $(M, C, B)$, we define
    the \emph{reduced MDP} $\oM \defeq (H, \oS, \oA, \oP, \oR, \os_0)$ where,
    \begin{enumerate}
        \item $\oS_h \defeq \mS_h \times \mB$ where $\mB \defeq \bigcup_{\pi \in \Pi^D} \bigcup_{h \in [H+1]} \bigcup_{\tau_h \in \mH_h} \set{C_h^{\pi}(\tau_h)}$
        \item $\oA_h(s, b) \defeq \set{(a,\bb) \in \mA_h(s) \times \Real^{S} \mid c_h(s,a) + \f_{s'} \g\paren{P_h(s' \mid s,a), b_{s'}} \leq b}$
        \item $\oP_h((s', b') \mid (s,b), (a, \bb)) \defeq P_h(s' \mid s,a)[b' = b_{s'}]$
        \item $\oR_h((s,b), (a,\bb)) \defeq R_h(s,a)$
        \item $\os_0 \defeq (s_0, B)$
    \end{enumerate}
    We also re-define the base case value to $\oV^*_{H+1}(s,b) \defeq -\chi_{\set{b \geq 0}}$.
\end{definition}

\paragraph{Reduction.} Importantly, $\oM$'s augmented action space ensures constraint satisfaction. Thus, we have effectively reduced a problem involving total history constraints to one with only standard per-time step constraints.
So long as our cost is SR, $\oM$ can be solved using fast RL methods instead of the brute force computation required for general CMDPs. These properties ensure our method, \cref{alg: reduction}, is correct.

\begin{algorithm}[t]
    \caption{Reduction}\label{alg: reduction}
    \begin{algorithmic}[1]
        \Require{$(M, C, B)$}
        \State $\oM \gets \text{\cref{def: reduced-mdp}}(M, C, B)$
        \State $\pi, \oV^* \gets \textsc{Solve}(\oM)$
        \If{$\oV^* = -\infty$}
            \State \Return ``Infeasible''
        \Else 
            \State \Return $\pi$ 
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{lemma}[Value]\label{lem: value}
    For any $h \in [H+1]$, $\tau_h \in \mH_h$, and $b \in \mB$, if $s = s_h(\tau_h)$, then,
    \begin{equation}\label{equ: val-lemma}
        \begin{split}
            \oV_h^*(s,b) \geq \sup_{\pi \in \Pi^D}&\; V_h^{\pi}(\tau_h) \\
            \text{s.t.}&\; C_h^{\pi}(\tau_h) \leq b.
        \end{split}
    \end{equation}
\end{lemma}

\begin{lemma}[Cost]\label{lem: cost}
    Suppose that $\pi \in \Pi^D$. For all $h \in [H+1]$ and $(s,b) \in \oS$, if $\oV^{\pi}_h(s,b) > -\infty$, then $\oC^{\pi}_{h}(s,b) \leq b$. 
\end{lemma}


\begin{theorem}[Reduction]\label{thm: reduction}
    If \emph{\textsc{Solve}} is any finite-time MDP solver, then \cref{alg: reduction} correctly solves \eqref{equ: constrained} in finite time for any SR-criterion CMDP.
\end{theorem}

\begin{algorithm}[t]
    \caption{Augmented Interaction}\label{alg: interaction}
    \begin{algorithmic}[1]
        \Require{$\pi$}
        \State $\os_1 = (s_0, B)$ 
        \For{$h \gets 1$ to $H$}
            \State $(a, \bb) \gets \pi_h(\os_h)$
            \State $s_{h+1} \sim P_h(s_h,a)$ 
            \State $\os_{h+1} = (s_{h+1}, b_{s_{h+1}})$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{remark}[Deployment]\label{rem: execution}
    Given a budget-augmented policy $\pi$ output from \cref{alg: reduction}, the agent can execute $\pi$ using \cref{alg: interaction}. 
\end{remark}


\section{Bellman Updates}\label{sec: bellman}
In this section, we discuss efficient methods for solving $\oM$. Our approach relies on using \eqref{equ: sr} to break down the bellman update so that it is solvable using dynamic programming. We then use dynamic rounding to achieve an efficient approximation algorithm.

\paragraph{Bellman Hardness.} 
Even a small set of artificial budgets, $\mB$, needed to be considered, solving $\oM$ would still be challenging due to its exponentially large, constrained action space. Just one Bellman update equates to solving the constrained optimization problem:
\begin{equation}\label{equ: update}\tag{BU}
    \begin{split}
        \oV^*_h(s,b) = \max_{a, \bb} \; &r_h(s,a) + \sum_{s'} P_h(s' \mid s,a) V_{h+1}^*\paren{s', b_{s'}}\\
        \text{s.t.} \; &c_h(s,a) + \f_{s'} \g\paren{P_h\paren{s' \mid s,a}}b_{s'} \leq b.
    \end{split}
\end{equation}
Above, we used the fact that $(s',b') \in \supp(\oP_h((s,b), (a,\bb)))$ iff $s' \in \supp(P_h(s,a))$ and $b' = b_{s'}$. 
In fact, even when each $b_{s'}$ only takes on two possible values, $\{0, w_{s'}\}$, this optimization problem generalizes the knapsack problem, implying that it is NP-hard to solve. 

\paragraph{Dynamic Programming.} To get around this computational bottleneck, we must fully exploit \cref{def: sr}. For any fixed $(h,(s,b),a)$, the key idea is to treat choosing $b'$s as its own sequential decision making problem. Suppose we have already chosen $b_{1}, \ldots, b_{t-1}$ leading to partial cost $F \defeq \f_{s' = 1}^{t-1} \g(P_h(s' \mid s,a)) b_{s'}$. Since $\f$ is associative, we can update our partial cost after choosing $b_{t}$ to $\f(F, \g(P_h(t \mid s,a)) b_{t})$. Once we have made a choice for each future state, we can verify if $(a,\bb) \in \oA_h(s,b)$ by checking the condition: $c_h(s,a) + F \leq b$. By incorporating the value objective, we design a dynamic program for computing \eqref{equ: update}.

\begin{definition}[DP]\label{def: exact-dp}
    For any $h \in [H]$, $(s,b) \in \oS$, $a \in \mA$ and $F \in \Real$, we define $\oV_{h,b}^{s,a}(S+1,F) = -\chi_{\set{c_h(s,a) + F \leq b}}$, and for any $t \in [S]$,
    \begin{equation}\label{equ: exact-dp}
    \oV_{h,b}^{s,a}(t,F) \defeq \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h,b}^{s,a}\paren{t+1, \f\paren{F, \g(P_h(t \mid s,a)) b_t}}.
\end{equation}
\end{definition}

\begin{lemma}[DP Correctness]\label{lem: exact-dp}
    For any $h \in [H]$ and $(s,b) \in \oS$, we have that $\oV^*_h(s,b) = \max_{a \in \mA} r_h(s,a) + \oV^{s,a}_{h,b}(1, 0)$.
\end{lemma}

\paragraph{Dynamic Rounding.} Although a step in the right direction, solving \cref{def: exact-dp} can still be slow due to the exponential number of considered partial costs. We resolve this issue by rounding each partial cost to an element of some small set $\hmF$. Since $\f$ need not be linear, using rounding in a preprocessing step does not suffice: we must re-round at each step to ensure inputs are a valid element of our input set.

For any $\ell > 0$, we view $\ell$ as a new unit length. Our rounding function maps any real number to its closest upper bound in the set of integer multiples of $\ell$. We use upper bounds to guarantee the rounded partial costs are always larger than the true partial costs. Smaller $\ell$ ensures less approximation error while larger $\ell$ ensures fewer considered partial costs. Thus, $\ell$ directly controls the accuracy-efficiency trade-off. 
%Note, rounding down to attain feasible solutions is not possible due to the NP-hardness of feasibility.

\begin{definition}[Rounding Functions]\label{def: rounding}
    For any $\ell > 0$ and $x \in \Real$, we define $\round{\ell}{x} \defeq \ceil{\frac{x}{\ell}}\ell$ to be the smallest integer multiple of $\ell$ that is larger than $x$. We also define $\kappa_{\ell}(x) \defeq x + \ell(S+1)$.
    Note, when considering vectors, all operations are performed component-wise.
\end{definition}

Since we round up the partial costs, the approximate partial cost of a feasible $\bb$ could exceed $b$. To ensure all feasible choices of $\bb$ are considered, we must also relax the budget comparison. Instead, we compare partial costs to a carefully chosen upper threshold $\kappa(b)$. Putting these pieces together yields our approximate bellman update method.

\begin{definition}[Approximate Update]\label{def: approx-update}
    Fix any $\ell > 0$ and function $\kappa : \Real^m \to \Real^m$. For any $h \in [H]$, $(s,b) \in \oS$, $a \in \mA$ and $\hF \in \Real^m$, we define $\hV_{h,b}^{s,a}(S+1,\hF) \defeq -\chi_{\set{c_h(s,a) + \hF \leq \kappa(b)}}$, and for any $t \in [S]$,
    \begin{equation}\label{equ: approx-dp}\tag{ADP}
    \hV_{h,b}^{s,a}(t,\hF) \defeq \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \hV_{h,b}^{s,a}\paren{t+1, \round{\ell}{\f\paren{\hF, \g(P_h(t \mid s,a))b_t}}}.
    \end{equation}
    We then define the \emph{approximate update} by,
    \begin{equation}\label{equ: approx-update}\tag{AU}
    \hV_{h}^*(s,b) \defeq \max_{a \in \mA} r_h(s,a) + \hV_{h,b}^{s,a}(1, 0).
    \end{equation}
\end{definition}

Overall, solving the ADP yields an approximate solution. 

\begin{lemma}[Approximation]\label{lem: approx-update}
    For any $h \in [H]$, $(s,b) \in \oS$, $a \in \mA$, $\hF \in \Real^m$, and $t \in [S+1]$, we have that,
    \begin{equation}\label{equ: approx-opt}
        \begin{split}
            \hV^{s,a}_{h,b}(t,\hF) = \max_{\bb \in \mB^{S-t+1}} & \; \sum_{s' = t}^S P_h(s' \mid s,a) \oV^*_{h+1}(s',b_{s'}) \\
            \text{s.t.} \quad  & \; c_h(s,a) + \hat{\f}_{h, \bb}^{s,a}(t, \hF) \leq \kappa(b),
        \end{split}
    \end{equation}
    where $\hat{\f}_{h,\bb}^{s,a}(t, \hF)$ is the dynamic rounding of $\f\paren{\hF, \f_{s' = t}^S \g(P_h(t \mid s,a), b_t)}$. 
    Moreover, if $\round{\ell}{}$ and $\kappa$ are replaced with the identity function, \eqref{equ: approx-update} is equivalent to \eqref{equ: update}.
    %\jeremy{The range should always be $H [\cmin, \cmax] + H \ell$ ish. Might want to change base cases of everything to simplify or add a 0 regularity condition.}
\end{lemma}

\begin{remark}[DP details]
    Technically, to turn this recursion into a true dynamic program, we must also precompute the inputs to any subproblem. Unlike in standard RL, this computation must be done with a forward recursion. If we let $\hmF_h^{s,a}(t)$ denote the set of possible input rounded partial costs for state $t$, then the set satisfies the inductive relationship $\hmF_h^{s,a}(1) \defeq \set{0}$ and for any $t \in [S]$, $\hmF_h^{s,a}(t+1) \defeq \bigcup_{b_t \in \mB} \bigcup_{F \in \hmF_{h}^{s,a}(t)} \set{\round{\ell}{\f(F, + \g(P_h(t \mid s,a))b_t}}$. This relationship translates directly into an iterative algorithm for computing all needed inputs. Using this gives a complete DP algorithm for solving \eqref{equ: approx-dp}\footnote{We use the notation $x, o \gets \min_x z(x)$ to say that $x$ is the minimizer and $o$ the value of the optimization.}.
\end{remark}



\begin{algorithm}[t]
    \caption{Approximate Backward Induction}\label{alg: approximate-bi}
    \begin{algorithmic}[1]
        \Require{$\oM$}
        \State $\hV^*_{H+1}(s,b) \gets \chi_{\set{b \geq 0}}$ for all $(s,b) \in \oS$
        \For{$h \gets H$ down to $1$}
            \For{$(s,b) \in \oS$}
                \State $\ha, \hV^*_{h}(s,b) \gets \eqref{equ: approx-update}$
                \State $\pi_h(s,b) \gets \ha$
            \EndFor
        \EndFor
        \State \Return $\pi, \hV^*$
    \end{algorithmic}
\end{algorithm}

\begin{theorem}[Approx Solve]\label{thm: approximate-bi}
    When $\round{\ell}{}$ and $\kappa$ are replaced with the identity function, \cref{alg: approximate-bi} correctly solves any $\oM$ produced from \cref{def: reduced-mdp}. Moreover, \cref{alg: approximate-bi} runs in time $O\paren{H^{m+1}S^{m+2}A|\mB|^2\norm{\cmax - \cmin}_{\infty}^m/\ell^m}$.
\end{theorem}

\section{Bicriteria}\label{sec: bicriteria}

\cref{alg: approximate-bi} allows us to approximately solve $\oM$ in finite cases much faster than traditional methods. However, when $|\mB|$ is large, the algorithm still runs in exponential time. Similarly to the partial cost rounding in \cref{def: approx-update}, we can reduce the size of $|\mB|$ by considering a smaller approximate set based on rounding. Since we still desire optimistic budgets, we use the same rounding function from \cref{def: rounding} but with a different choice of $\ell$.

%\jeremy{Really distinguish artificial budgets from true budgets everywhere!}

\paragraph{Budget Rounding.} Rounding naturally impacts the state space, but has other consequences as well. To avoid complex computation, we consider the approximate set $\hB \defeq \set{\round{\ell}{b} \mid b \in [\bmin, \bmax]}$ where $[\bmin, \bmax] \supseteq \mB$ is a superset of all required artificial budgets that we formalize later. As before, rounding the budgets may cause originally feasible choices to now violate the constraint. To ensure all feasible choices are considered and that we can use \cref{alg: approximate-bi} to get speed-ups, we define the approximate action space to include all vectors that lead to feasible subproblems of \eqref{equ: approx-dp}. From \cref{lem: approx-update}, we know this set is exactly the set of $(a, \hbb)$ satisfying $c_h(s,a) + \hf_{h,\hbb}^{s,a}(1,0) \leq \kappa(\hb)$. Putting these ideas together yields a new, approximate MDP.  



\begin{definition}[Approximate MDP]\label{def: approximate-mdp}
    
    Given any SR-criterion CMDP $(M, C, B)$, we define
    the \emph{approximate MDP} $\hM \defeq (H, \hS, \hA, \hP, \hR, \hs_0)$ where,
    \begin{enumerate}
        \item $\hS_h \defeq \mS_h \times \hB$ where $\hB \defeq \set{\round{\ell}{b} \mid b \in [\bmin, \bmax]}$.
        \item $\hA_h(s, \hb) \defeq \set{(a,\hbb) \in \mA_h(s) \times \hB^{S} \mid c_h(s,a) + \hat{\f}_{h, \hbb}^{s,a}(1, 0) \leq \kappa(\hb)}$
        \item $\hP_h((s', \hb') \mid (s,b), (a, \hbb)) \defeq P_h(s' \mid s,a)[\hb' = \hb_{s'}]$
        \item $\hR_h((s,\hb), a) \defeq R_h(s,a)$
        \item $\hs_0 \defeq (s_0, \round{\ell}{B})$
    \end{enumerate}
    We again re-define the base case value to $\hV^*_{H+1}(s,\hb) \defeq -\chi_{\set{\hb \geq 0}}$.
\end{definition}

\begin{algorithm}[t]
    \caption{Bicriteria}\label{alg: bicriteria}
    \begin{algorithmic}[1]
        \Require{$(M, C, B)$}
        \State \textbf{Hyperparameter:} $\ell$ 
        \State $\hM \gets \text{\cref{def: approximate-mdp}}(M, (f,\g), B, \ell)$
        \State $\pi, \hV^* \gets \text{\cref{alg: approximate-bi}}(\hM, (f,\g), \ell)$
        \If{$\hV^*_1(s_0,\round{\ell}{B}) = -\infty$}
            \State \Return ``Infeasible"
        \Else 
            \State \Return $\pi$
        \EndIf
    \end{algorithmic}
\end{algorithm}

Since we always round budgets up, the agent can make even better choices than originally. It is then easy to see that policies for $\hM$ always achieves optimal constrained value. We formalize this observation in \cref{lem: optimal-value}.

\begin{lemma}[Optimal Value]\label{lem: optimal-value}
    For any $h \in [H+1]$ and $(s,b) \in \oS$, $\hV_h^*(s, \round{\ell}{b}) \geq \oV_h^*(s,b)$.
\end{lemma}

%Thus, \cref{alg: approx} always outputs a policy with better than optimal value when the instance is feasible, $C^*_M \leq B$. If the instance is infeasible, all policies have cost larger than $B$ by definition and so \cref{alg: approx} correctly indicates the instance is infeasible. The remaining question is whether \cref{alg: approx} outputs policies having near-optimal value. 

\paragraph{Time-Space Errors.} To assess the violation gap of \cref{alg: bicriteria} policies, we must first explore the error accumulated by our rounding approach. Rounding each artificial budget naturally accumulates approximation error over time. Rounding the partial costs while running \cref{alg: approximate-bi} accumulates additional error over (state) space. Thus, solving $\hM$ using \cref{alg: bicriteria} accumulates error over both time and space, unlike standard approximate methods in RL. As a result, our rounding and threshold functions will generally depend on both $H$ and $S$. 

\paragraph{Arithmetic Rounding.} Our approach is to round each value down to its closest element in a $\ell$-cover. Using the same rounding as in \cref{def: rounding}, we guarantee that $b \leq \round{\ell}{b} \leq b + \ell$. Thus, $\round{\ell}{b}$ is an overestimate that is not too far from the true value. By setting $\ell$ to be inversely proportional to $SH$, we control the errors over time and space. The lower bound must also be a function of $S$ since it controls the error over space.


\begin{lemma}[Approximate Cost]\label{lem: approximate-cost}
    Suppose that $\pi \in \Pi^D$. For all $h \in [H+1]$ and $(s,\hb) \in \hS$, if $\hV^{\pi}_h(s,\hb) > -\infty$, then $\hC^{\pi}_{h}(s,\hb) \leq \hb + \ell(S+1)(H-h+1)$. 
\end{lemma}


\begin{theorem}[Bicriteria]\label{thm: bicriteria}
    For any SR-criterion CMDP with polynomially-bounded costs and $\epsilon > 0$, the choice of $\ell \defeq \frac{\epsilon}{1 + (S+1)H}$ ensures \cref{alg: bicriteria} is a $(0, \epsilon)$-bicriteria running in polynomial time $O\paren{H^{6m+1}S^{4m+2}A\norm{\cmax - \cmin}_{\infty}^{3m}/\epsilon^{3m}}$. 
\end{theorem}

\begin{corollary}[Relative]\label{cor: relative}
    For any $\epsilon > 0$, the choice of $\ell \defeq \frac{\epsilon}{B(H(S+1) + 1)}$ ensures \cref{alg: bicriteria} is a polynomial time $(0, 1+\epsilon)$-relative bicriteria for the class of polynomial-budget-bounded-cost CMDPs with SR-cost criteria. This includes all SR-criterion CMDPs with non-negative costs.
\end{corollary}

\begin{remark}[Chance Constraints]
    Technically, for chance constraints, we first create a cost augmented MDP that is initially passed into the input. This allows us to write chance constraints in the SR form. Consequently, the $S$ term in \cref{thm: bicriteria} is really a larger augmented $S$. To achieve $\epsilon$ cost violation, \cite{acRL} showed an augmented space of size $O(SH^2\norm{\cmax - \cmin}_{\infty}/\epsilon)$ is needed, which still results in a polynomial-time complexity.
\end{remark}

\begin{remark}[Approximation Optimality]\label{rem: optimality}
    ~\cite{acRL} showed that our assumptions on cost bounds are necessary to achieve polynomial-time approximations. Thus, our approximations guarantees are best possible. Moreover, we can show our dependency in the number of constraints is also unavoidable. This is formalized in \cref{prop: multi-hardness}.
\end{remark}

\begin{proposition}[Multi-Constraint Hardness]\label{prop: multi-hardness}
    If $m = \Omega(n^{1/d})$ for some constant $d$, then computing an $\epsilon$-feasible policy for a CMDP is NP-hard for any $\epsilon > 0$.
\end{proposition}


\subsection{Continuous MDPs}\label{subsec: continous}

We also show approximations are possible in infinite state settings under certain continuity assumptions.

\begin{assumption}[Continuity]\label{assum: continous}
    We assume the caMDP $M$ is Lipschitz continous. Formally, we require that (1) $S = [\smin, \smax]$, (2) the reward function is $\lambda_r$ Lipschitz, (3) the cost function is $\lambda_c$ Lipschitz, (4) the transitions are $\lambda_p$ Lipschitz -- each with respect to the state input, and (5) each of these quanitites is polynomial-sized in the input representation. For SR-criterion CMDPs, we also assume that $\f$ has a natural finite equivalent denoted, $\Tilde{\f}$, $g$ is a sublinear short map, and $\f_{s'} z \leq (\smax - \smin)$ for any constant $z$.
\end{assumption}

All we need to do is descretized the state space, and run our previous algorithm on the following discretized CMDP.
\begin{definition}[Discretized CMDP]\label{def: discretized-cmdp}
    Given any SR-criterion CMDP $(M, C, B)$, we define the \emph{discretized CMDP} $(\tM, \tC, B)$ where $\tM = (H, \tS, \mA, \tP, R, C, \ts_0) $ is the discretized caMDP defined by,
    \begin{enumerate}
        \item $\tS_h \defeq \set{\round{\ell}{s} \mid s \in \mS}$
        \item $\tP_h(\ts' \mid \ts, a) \defeq  \int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \ts,a)ds'$
        \item $\ts_0 \defeq (\round{\ell}{s_0}, B)$
    \end{enumerate}
    and $\tC$ is the cost criterion defined by replacing $\f_{s'}$ with its natural finite equivalent $\Tilde{f}$.
\end{definition}

We see that discretization results in a small impact to both the value and cost that depend on the continuity parameters.

\begin{lemma}[Discretization]\label{lem: discretized}
    For all $h \in [H+1]$, $\tau_h \in \mH_h$, and $\pi \in \Pi^D$, we let $\Tilde{\tau}_h$ denote $\tau_h$ with each state $s_t$ rounded to $\round{\ell}{s_t}$. Then, we have that $\tV^{\pi}_h(\Tilde{\tau}_h) \geq V^*_h(\tau_h) - \ell (\lambda_r + \lambda_p)H\rmax(\smax - \smin)(H-h+1)$ and $\tC^{\pi}_h(\Tilde{\tau}_h) \leq C^*_h(\tau_h) + \ell (\lambda_c + \lambda_p)H\cmax(\smax - \smin)(H-h+1)$. For almost-sure/anytime constraints the cost incurs an additional factor of $1/\Tilde{p}_{min}$, where $\Tilde{p}_{min}$ denotes the smallest non-zero transition probability for $\tM$.
\end{lemma}

Overall, using our previous bicriteria on $\tM$ yeilds our approximation results.

\begin{theorem}[Continuous Bicriteria]\label{thm: discretized}
    For any SR-criterion CMDP satisfying \cref{assum: continous} and any $\epsilon > 0$, the choice of discretization $\ell_d \defeq \frac{\epsilon/2}{(\lambda_r + \lambda_c + \lambda_p)H\max(\cmax, \rmax)(\smax - \smin)}$ and approximation $\ell_a \defeq \frac{\epsilon/2}{1 + (S+1)H}$ ensures \cref{alg: bicriteria}($\tM$) is a $(\epsilon, \epsilon)$-bicriteria running in time $O\paren{H^{6m+1}\Tilde{S}^{4m+2}A\norm{\cmax - \cmin}_{\infty}^{3m}/\epsilon^{3m}}$, where $\Tilde{S} = O((\lambda_r + \lambda_c + \lambda_p)H\max(\cmax, \rmax)\allowdisplaybreaks(\smax - \smin)^2/\epsilon)$. This time is polynomial so long as $|\smax - \smin| = O(|M|)$. Moreover, almost-sure/anytime constraints enjoy the same guarantee with an additional factor of $\Tilde{p}_{min}$ in $\Tilde{S}$.
\end{theorem}

\begin{corollary}[Simplified]\label{cor: discretized}
    For continous-state SR-criterion CMDPs satisfying \cref{assum: continous}, there exist polynomial-time $(\epsilon,\epsilon)$-bicriteria solutions for expectation constraints, almost-sure constraints, anytime-almost-sure constraints, and any combinations of these constraints.
\end{corollary}


\section{Function Approximation}\label{sec: function}

When the state space is infinite and cannot be reduced to a nice finite set, the principles of our framework still apply. In fact, our augmentation approach can be easily combined with function approximation. In particular, we can in general view the artificial budgets as a function $b(s' \mid s,a)$ that outputs the next artificial budget as a function of the immediate history. 

Even if there are infinite states, we can naturally parameterize the artificial budgets as $b_{\theta}$ and attempt to learn the best $\theta$. For any fixed budget function $b_{\theta}$, we then get an entirely new reduced MDP $\oM_{\theta}$. This new MDP can then also be solved using function approximation methods. Overall, we can repeatedly solve $\oM_{\theta}$, and use the feedback to updated $b_{\theta}$ in a closed loop system.

\paragraph{Knapsack Example.} To illustrate how our framework works with function approximation, we apply it to the classic Knapsack Problem, which is a special case of expectation and almost-sure constraints. In the knapsack problem, the user is given $n$ items having values $v_1 \ldots, v_n$ and weights $w_1, \ldots, w_n$. The user also have a budget $B$ and wishes to solve the following integer program: $\max_{x \in \{0,1\}^n} \sum_{i = 1}^n x_i v_i$ subject to $\sum_{i = 1}^n x_i w_i \leq B$. We can model this problem as an almost-sure constrained CMDP by defining $\mS = [n]$, $\mA = \{0, 1\}$, $r(i,a) = [a > 0]v_i$, $c(i,a) = [a > 0]c_i$, and $H = n$. Any state transitions from $i$ to $i+1$ deterministically, and the initial state is $1$. 

We created an OpenAI gym environment modeling our approach with function approximation on this special CMDP. We compared the output to our own bicriteria, \cref{alg: bicriteria}, as well as the textbook FPTAS algorithm for the knapsack problem. We compared each on three metrics: (1) value, (2) weight -- notably how much the constraint is violated if at all, and (3) the running time each method took. In general, the function approximation method was fastest, followed by our bicriteria, and then finally the FPTAS. All methods achieved high value. The FPTAS always respected the budget, our bicriteria almost always did, and the function approximation approach did the worst concerning violation, but still not too bad. For very large state spaces, the function approximation method would likely be the only tractable approach.



\section{Conclusion}\label{sec: conclusion}

In this work, we studied the question of whether polynomial-time approximation algorithms exist for many of the classic formulations studied in the CRL literature. We conclude that for the vast majority of constraints, including all the standard constraints, polynomial-time approximability is possible. We demonstrated this phenomenon by developing polynomial-time bicriteria approximations with the strongest possible guarantees for a general class of constraints that can be written in a form that satisfies general policy evaluation equations. Overall, our work resolves the polynomial-time approximability of many settings, some of which have been lacking in any polynomial-time algorithm for over a decade. In particular, we are the first to develop a polynomial-time algorithm with any kind of guarantee or chance constraints, and non-homogeneous constraints. 

\bibliography{refs}


\appendix

\section{Proofs for \texorpdfstring{\cref{sec: constraints}}{sec: constraints}}

\subsection{Proof of \texorpdfstring{\cref{prop: modeling}}{prop: modeling}}

\begin{proof}\

    \paragraph{Expectation Constraints.} We define $C_M^{\pi} \defeq \E_M\brak{\sum_{h = 1}^H c_H}$. Under this definition, the standard policy evaluation equations imply that,
    \begin{equation}
        C^{\pi}_h(\tau_h) = c_h(s,a) + \sum_{s'} P_h(s' \mid s,a) C_{h+1}^{\pi}(\tau_{h+1}).
    \end{equation}
    It is then clear that this can be written in $(f,g)$-form for $\f$ being summation and $g$ being the identity. It is easy to see these functions have the desired properties.

    \paragraph{Chance Constraints.} Let $M^0$ denote the initial caMDP.We define $C_{M^0}^{\pi} \defeq 
    \P^{\pi}_M\brak{\sum_{h = 1}^H c_h > B}$. We see that the probability can be recursively decomposed as follows for the anytime variant:
    \begin{equation}
        C^{\pi}_h(\tau_h, \bar{c}) = [c_h(s,a) + \bar{c} > B] + \sum_{s'} P_h(s' \mid s,a) C_{h+1}^{\pi}(\tau_{h+1}, c_h(s,a) + \bar{c}).
    \end{equation}
    For the general invariant, we only include the indicator term at step $H$. To write this into the desired form, we can define a cost-augmented MDP $M$ that keeps track of the cumulative cost at each step as in \cite{acMARL}. In particular, the anytime variant has the immediate cost defined to be $c_h((s,\bar{c}),a) \defeq [c_h(s,a) + \bar{c} > B]$. Then, it is clear that the expected cost for the new $M$ exactly corresponds to the probability cost. Thus, the claim holds.

    \paragraph{Almost-sure Constraints.} We define $C_M^{\pi} \defeq \max_{\substack{\tau_{H+1}\\ \P^{\pi}_M[\tau_{H+1}] > 0}}\brak{\sum_{h = 1}^H c_H}$ to be the worst case cost. Under this definition, it is known that the worst-case cost decomposes by,
    \begin{equation}
        C^{\pi}_h(\tau_h) = c_h(s,a) + \max_{s'} [P_h(s' \mid s,a) > 0] C_{h+1}^{\pi}(\tau_{h+1}).
    \end{equation}
    It is then clear that this can be written in $(f,g)$-form for $\f$ being maximum and $g$ being the indicator. Properties of maximum imply that $\max_{s'} (C(s') + \epsilon) \leq \max_{s'} C(s') + \epsilon$. Thus, the total combination is a short map and the rest of the needed properties can be seen to hold. The anytime variant follows similarly.
    
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{thm: results}}{thm: results}}

\begin{proof}
    The theorem follows immediately by translating the results on SR-criterion into their original forms in the proof above.
\end{proof}

\section{Proof for \texorpdfstring{\cref{sec: reduction}}{sec: reduction}}

\subsection{Helpful Technical Lemmas}

\begin{definition}[Budget Space]\label{def: value-space}
    For any $s \in \mS$, we define $\mB_{H+1}(s) \defeq \set{0}$, and for any $h \in [H]$,
    \begin{equation}\label{equ: covering-states}
    \displaystyle \mB_h(s) \defeq 
    \displaystyle\bigcup_{a} \bigcup_{\bb \in \bigtimes_{s'} \mB_{h+1}(s')} \set{c_h(s,a) + \f_{s'} g(P_h(s' \mid s,a), b_{s'})}.
\end{equation}
We define $\mB \defeq \bigcup_{h,s} \mB_h(s)$. 
\end{definition}

\begin{lemma}[Budget Space Intution]\label{lem: value-space}
    For all $s \in \mS$ and $h \in [H+1]$, 
    \begin{equation}
        \mB_h(s) = \set{b \in \Real^d \mid \exists \pi \in \Pi^D, \tau_h \in \mH_h, \; \paren{s = s_h(\tau_h) \wedge C_h^{\pi}(\tau_h) = b}},
    \end{equation}
    and $|\mB_h(s)| \leq A^{\sum_{t=h}^{H} S^{H-t}}$. Thus, $\mB$ can be computed in finite time using backward induction. 
\end{lemma}

\begin{proof}
    We proceed by induction on $h$. Let $s \in \mS$ be arbitrary. 
    
    \paragraph{Base Case.} For the base case, we consider $h = H+1$. In this case, we know that for any $\pi \in \Pi^D$ and any $\tau \in \mH_{H+1}$, $C_{H+1}^{\pi}(\tau_{H+1}) = 0 \in \{0\} = \mB_{H+1}(s)$ by definition. Furthermore, $|\mB_{H+1}(s)| = 1 = A^0 = A^{\sum_{t = H+1}^{H}S^t}$.
    
    \paragraph{Inductive Step.} For the inductive step, we consider $h \leq H$. In this case, we know that for any $\pi \in \Pi^D$ and any $\tau_h \in \mH_{h}$, 
    if $s = s_h(\tau_h)$ and $a = \pi_h(\tau_h)$, then the policy evaluation equations imply,
    \begin{equation*}
        C_h^{\pi}(\tau_h) = c_h(s,a) + \f_{s'} g(P_h(s' \mid s,a), C_{h+1}^{\pi}(\tau_h, a, s')).
    \end{equation*}
    We know by the induction hypothesis that $V_{h+1}^{\pi}(\tau_h, a, s') \in \mB_{h+1}(s')$. Thus, by \eqref{equ: covering-states}, $C_h^{\pi}(\tau_h) \in \mB_h(s)$.
    Lastly, we see by \eqref{equ: covering-states} and the induction hypothesis that,
    \begin{equation*}
        |\mB_h(s)| \leq A\prod_{s'} |\mB_{h+1}(s')| \leq A \prod_{s'} A^{\sum_{t = h+1}^{H} S^{H-t}} = A^{1 +S\sum_{t = h+1}^H S^{H-t}} = A^{\sum_{t = h}^H S^{H-t}}.
    \end{equation*}
    This completes the proof.
\end{proof}


\subsection{Proof of \texorpdfstring{\cref{lem: value}}{lem: value}}

\begin{proof}
    First, let $V^*_h(\tau_h, b)$ denote the supremum in \eqref{equ: val-lemma}. We proceed by induction on $h$.

    \paragraph{Base Case.} For the base case, we consider $h = H+1$. \cref{def: sr} implies that $C_{H+1}^{\pi}(\tau_{H+1}) = 0$ for any $\pi \in \Pi^D$. Thus, there exists a $\pi \in \Pi^D$ satisfying $C_{H+1}^{\pi}(\tau_{H+1}) \leq b$ if and only if $b \geq 0$. We also know by definition that any policy $\pi$ satisfies $V^{\pi}_{H+1}(\tau_{H+1}) = 0$ and if no feasible policy exists $V^*_{H+1}(\tau_{H+1},b) = -\infty$ by convention. Therefore, we see that $V^*_{H+1}(\tau_{H+1},b) = -\chi_{\set{b \geq 0}}$. Then, by definition of $\oV^*_{H+1}$, it follows that,
    \begin{equation*}
        \oV_{H+1}^*(s,b) = -\chi_{\set{b \geq 0}} = V^*_{H+1}(\tau_{H+1},b).
    \end{equation*}

    \paragraph{Inductive Step.} For the inductive step, we consider any $h \leq H$. If $V^*_h(\tau_h,b) = -\infty$, then trivially $\oV^*_h(s,b) \geq V^*_h(\tau_h, b)$. Instead, suppose that $V^*_h(\tau_h,b) > -\infty$. Then, there must exist a $\pi \in \Pi^D$ satisfying $C_h^{\pi}(\tau_h) \leq b$. Let $a^* = \pi_h(\tau_h)$. By \eqref{equ: sr}, we know that,
    \begin{equation*}
        C_h^{\pi}(\tau_h) = c_h(s,a^*) + \f_{s'} \g\paren{P_h(s' \mid s,a^*)} C_{h+1}^{\pi}(\tau_h, a^*, s').
    \end{equation*}
    For each $s' \in \mS$, define $b_{s'}^* \defeq C_{h+1}^{\pi}(\tau_h, a^*, s')$ and observe that $b_{s'}^* \in \mB$. Thus, we see that $(a^*,\bb^*) \in \mA \times \mB^{S}$ and $c_h(s,a) + \f_{s'} \g(P_h(s' \mid s,a)) b_{s'} \leq b$, so $(a^*,\bb^*) \in \oA_h(s,b)$ by definition.

    Since $\pi$ satisfies $C_{h+1}^{\pi}(\tau_h, a^*, s') \leq b_{s'}^*$, we see that $V^*_{h+1}(s', b_{s'}^*) \geq V^{\pi}_{h+1}(\tau_h, a^*, s')$. Thus, the induction hypothesis implies $\oV^*_{h+1}(s', b_{s'}^*) \geq V^*_{h+1}(s', b_{s'}^*) \geq V^{\pi}_{h+1}(\tau_h, a^*, s')$.
    The optimality equations for $\oM$ then give us,
    \begin{align*}
        \oV^*_h(s,b) &= \max_{\oa \in \oA_h(s,b)} \orew_h((s,b),\oa) + \sum_{\os'} \oP_h(\os' \mid (s,b), \oa) \oV^*_{h+1}(\os')\\
        &=\max_{(a,\bb) \in \oA_h(s,b)} r_h(s, a) + \sum_{s'} P_h(s' \mid s,a) \oV^*_{h+1}(s',b_{s'}) \\
        &\geq r_h(s, a^*) + \sum_{s'} P_h(s' \mid s,a^*) \oV^*_{h+1}(s',b_{s'}^*) \\
        &\geq r_h(s, a^*) + \sum_{s'} P_h(s' \mid s,a^*) V^{\pi}_{h+1}(\tau_h, a, s') \\
        &= V_h^{\pi}(\tau_h).
    \end{align*}
    The second line used the definition of each quantity in $\oM$.
    The first inequality used the fact that $(a^*, \bb^*) \in \oA_h(s,b)$. The second inequality used the induction hypothesis. The final equality used the deterministic policy evaluation equations.

    Since $\pi$ was an arbitrary feasible policy for the optimization defining $V^*_h(\tau_h,b)$, we see that $\oV_h^*(s,b) \geq V^*_h(\tau_h, b)$. This completes the proof.
    
\end{proof}


\subsection{Proof of \texorpdfstring{\cref{lem: cost}}{lem: cost}}


\begin{proof}
    We proceed by induction on $h$. 
    
    \paragraph{Base Case.} For the base case, we consider $h = H+1$. By definition and assumption, $\oV^{\pi}_{H+1}(s,b) = -\chi_{\set{b \geq 0}} > -\infty$. Thus, it must be the case that $b \geq 0$ and so by \cref{def: sr} $\oC_{H+1}^{\pi}(s,b) = 0 \leq b$.

    \paragraph{Inductive Step.} For the inductive step, we consider any $h \leq H$. We decompose $\pi_h(s,b) = (a, \bb)$ where we know $(a,\bb) \in \oA_h(s,b)$ since $\oV^{\pi}_h(s,b) > -\infty$ \footnote{By convention, we assume $\max \varnothing = -\infty$}. Moreover, it must be the case that for any $s' \in \mS$ with $P_h(s' \mid s,a) > 0$ that $\oV_{h+1}^{\pi}(s', b_{s'}) > -\infty$ otherwise the average reward would be $-\infty$ which would imply a contradiction:
    \begin{align*}
        \oV^{\pi}_h(s,b) &= r_h(s,a) + \sum_{s'} P_h(s' \mid s,a) \oV_{h+1}^{\pi}\paren{s',b_{s'}} \\
        &= r_h(s,a) + \ldots  + P_h(s' \mid s,a) (-\infty) + \ldots \\
        &= -\infty.
    \end{align*}
    Thus, the induction hypothesis implies that $\oC^{\pi}_{h+1}(s', b_{s'}) \leq b_{s'}$ for any such $s' \in \mS$.
    By \eqref{equ: sr}, we see that,
    \begin{align*}
        \oC^{\pi}_h(s,b) &= c_h(s, a) + \f_{s'} \g(P_h(s' \mid s,a)) \oC^{\pi}_{h+1}(s',b_{s'}) \\
        &\leq c_h(s, a) + \f_{s'} \g(P_h(s' \mid s,a))b_{s'} \\
        &\leq b.
    \end{align*}
    The second line used the fact that $f$ is non-decreasing and $g$ is a non-negative scaler. The third line used the fact that $(a,\bb) \in \oA_h(s,b)$. This completes the proof.
\end{proof}


\subsection{Proof of \texorpdfstring{\cref{thm: reduction}}{thm: reduction}}


\begin{proof}
    If $\oV^*_{1}(s_0, B) = -\infty$, then we know by \cref{lem: value} that,
    \begin{equation}
        \begin{split}
            -\infty = \oV^*_1(s_0,B) \geq \sup_{\pi \in \Pi^D}&\; V_1^{\pi}(s_0) \\
            \text{s.t.}&\; C_1^{\pi}(s_0) \leq B.
        \end{split}
    \end{equation}
    In other words, no feasible $\pi$ exists, so \cref{alg: reduction} reporting ``Infeasible'' is correct. On the other hand, suppose that $\oV^*_{1}(s_0, B) > -\infty$ and let $\pi^*$ be any solution to the optimality equations for $\oM$. By \cref{lem: cost}, we know that $C^{\pi}_1(s_0, B) \leq B$ implying that $\pi^*$ is a feasible solution. Moreover, \cref{lem: value} again tells us that,
        \begin{equation}
        \begin{split}
            \oV^{\pi^*}_1(s_0,B) = \oV^*_1(s_0,B) \geq \sup_{\pi \in \Pi^D}&\; V_1^{\pi}(s_0) \\
            \text{s.t.}&\; C_1^{\pi}(s_0) \leq B.
        \end{split}
    \end{equation}
    Thus, $\pi^*$ is an optimal solution to \eqref{equ: constrained} and \cref{alg: reduction} correctly returns it. Therefore, in all cases, \cref{alg: reduction} is correct.
\end{proof}

\section{Proofs for \texorpdfstring{\cref{sec: bellman}}{sec: bellman}}

Formally, $\hat{\f}_{h,\bb}^{s,a}$ can be defined recursively by $\hat{\f}_{h,\bb}^{s,a}(t,\hF) \defeq \hat{\f}_{h,\bb}^{s,a}\paren{t+1, \round{\ell}{\f(\hF, \g(P_h(t \mid s,a))b_t)}}$ with base case $\hat{\f}_{h,\bb}^{s,a}(S+1, \hF) \defeq \hF$.


\subsection{Proof of \texorpdfstring{\cref{lem: exact-dp}}{lem: exact-dp}}

\begin{proof}
    First, we show that,
    \begin{equation}\label{equ: exact-opt}
        \begin{split}
            \oV^{s,a}_{h,b}(t,\hF) = \max_{\bb \in \mB^{S-t+1}} & \; \sum_{s' = t}^S P_h(s' \mid s,a) \oV^*_{h+1}(s',b_{s'}) \\
            \text{s.t.} \quad  & \; c_h(s,a) + f_{h, \bb}^{s,a}(t, F) \leq b,
        \end{split}
    \end{equation}
    For notational simplicity, we define $\oV^{s,a}_{h,\bb}(t) \defeq \sum_{s' = t}^S P_h(s' \mid s,a) \oV^*_{h+1}(s', b_{s'})$.
    We proceed by induction on $t$. 

    \paragraph{Base Case.} For the base case, we consider $t = S+1$. By definition, we know that $\oV_{h,b}^{s,a}(t,F) = -\chi_{\{c_h(s,a) + F \leq b\}}$. We just need to show that the maximum in \eqref{equ: exact-opt} also matches this expression. First, observe objective is the empty summation which is $0$. Also, $f_{h, \bb}^{s,a}(S+1,F) = F$, so the constraint is satisfied iff $c_h(s,a) + F \leq b$. Thus, the maximum is $0$ when $c_h(s,a) + F \leq b$ and is $-\infty$ due to infeasibility otherwise. In other words, it equals $-\chi_{\{c_h(s,a) + F \leq b\}}$ as was to be shown.

    \paragraph{Inductive Step.}  For the inductive step, we consider any $t \leq S$. From \eqref{equ: exact-dp}, we see that,
    \begin{align*}
        \oV_{h,b}^{s,a}(t,F) &= \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h,b}^{s,a}\paren{t+1, \f\paren{F, \g(P_h(t \mid s,a)) b_t}}\\
        &= \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \max_{\substack{\bb \in \mB^{S-t}, \\ c_h(s,a) + f_{h,\bb}^{s,a}\paren{t+1, \f\paren{F, \g(P_h(t \mid s,a)) b_t}} \leq b}}\oV_{h, \bb}^{s,a}(t+1)\\
        &= \max_{b_t \in \mB}\max_{\substack{\bb \in \mB^{S-t}, \\ c_h(s,a) + f_{h,\bb}^{s,a}\paren{t+1, \f\paren{F, \g(P_h(t \mid s,a)) b_t}} \leq b}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &=\max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + f_{h,\bb}^{s,a}\paren{t+1, \f\paren{F, \g(P_h(t \mid s,a)) b_t}} \leq b}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &=\max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + f_{h,\bb}^{s,a}\paren{t, F} \leq b}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &= \max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + f_{h,\bb}^{s,a}\paren{t, F} \leq b}} \oV_{h, \bb}^{s,a}(t)
    \end{align*}
    The second line used the induction hypothesis. The third line used the fact that the first term is independent of future $b$ values. The fourth line used properties of maximum. The fourth line used the recursive definition of $f_{h,\bb}^{s,a}(t, F)$. The last line used the recursive definition of $\oV_{h,\bb}^{s,a}(t)$. 

    For the second claim, we observe that,
    \begin{align*}
        \oV^*_h(s,b) &= \max_{\substack{a, \bb, \\ c_h(s,a) + \f_{s'}g(P_h(s' \mid s,a))b_{s'} \leq b}} r_h(s,a) + \sum_{s'} P_h(s' \mid s,a)\oV^*_{h+1}(s', b_{s'}) \\
        &= \max_{\substack{a, \bb, \\ c_h(s,a) + f_{h,\bb}^{s,a}(1,0) \leq b}} r_h(s,a) + \sum_{s'} P_h(s' \mid s,a)\oV^*_{h+1}(s', b_{s'}) \\
        &= \max_{a}\max_{\substack{\bb, \\ c_h(s,a) + f_{h,\bb}^{s,a}(1,0) \leq b}} r_h(s,a) + \sum_{s'} P_h(s' \mid s,a)\oV^*_{h+1}(s', b_{s'}) \\ 
        &= \max_{a} r_h(s,a) + \max_{\substack{\bb, \\ c_h(s,a) + f_{h,\bb}^{s,a}(1,0) \leq b}} \sum_{s'} P_h(s' \mid s,a)\oV^*_{h+1}(s', b_{s'}) \\
        &= \max_{a} r_h(s,a) + \oV^{s,a}_{h,b}(1,0).
    \end{align*}
\end{proof}


\subsection{Proof of \texorpdfstring{\cref{lem: approx-update}}{lem: approx-update}}

\begin{proof}
    Recall, as in the proof of \cref{lem: exact-dp}, we define $\oV^{s,a}_{h,\bb}(t) \defeq \sum_{s' = t}^S P_h(s' \mid s,a) \oV^*_{h+1}(s', b_{s'})$ to simplify expressions.
    We proceed by induction on $t$. 

    \paragraph{Base Case.} For the base case, we consider $t = S+1$. By definition, we know that $\hV_{h,b}^{s,a}(t,\hF) = -\chi_{\{c_h(s,a) + \hF \leq \kappa(b)\}}$. We just need to show that the maximum in \eqref{equ: approx-opt} also matches this expression. First, observe objective is the empty summation which is $0$. Also, $\hf_{h, \bb}^{s,a}(S+1,F) = F$, so the constraint is satisfied iff $c_h(s,a) + \hF \leq \kappa(b)$. Thus, the maximum is $0$ when $c_h(s,a) + \hF \leq \kappa(b)$ and is $-\infty$ due to infeasibility otherwise. In other words, it equals $-\chi_{\{c_h(s,a) + \hF \leq \kappa(b)\}}$ as was to be shown.

    \paragraph{Inductive Step.}  For the inductive step, we consider any $t \leq S$. From \eqref{equ: approx-dp}, we see that,
    \begin{align*}
        \hV_{h,b}^{s,a}(t,\hF) &= \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \hV_{h,b}^{s,a}\paren{t+1, \round{\ell}{\f\paren{\hF, \g(P_h(t \mid s,a))b_t}}} \\
        &= \max_{b_t \in \mB} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \max_{\substack{\bb \in \mB^{S-t}, \\ c_h(s,a) + \hat{f}_{h,\bb}^{s,a}\paren{t+1, \round{\ell}{\f\paren{\hF, \g(P_h(t \mid s,a))b_t}}} \leq \kappa(b)}}\oV_{h, \bb}^{s,a}(t+1)\\
        &= \max_{b_t \in \mB}\max_{\substack{\bb \in \mB^{S-t}, \\ c_h(s,a) + \hat{f}_{h,\bb}^{s,a}\paren{t+1, \round{\ell}{\f\paren{\hF, \g(P_h(t \mid s,a))b_t}}} \leq \kappa(b)}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &=\max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + \hat{f}_{h,\bb}^{s,a}\paren{t+1, \round{\ell}{\f\paren{\hF, \g(P_h(t \mid s,a))b_t}}} \leq \kappa(b)}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &=\max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + \hat{f}_{h,\bb}^{s,a}\paren{t, \hF} \leq \kappa(b)}} P_h(t \mid s,a)\oV^*_{h+1}(t,b_t) + \oV_{h, \bb}^{s,a}(t+1) \\
        &= \max_{\substack{\bb \in \mB^{S-t+1}, \\ c_h(s,a) + \hat{f}_{h,\bb}^{s,a}\paren{t, \hF} \leq \kappa(b)}} \oV_{h, \bb}^{s,a}(t)
    \end{align*}
    The second line used the induction hypothesis. The third line used the fact that the first term is independent of future $b$ values. The fourth line used the properties of maximum. The fourth line used the recursive definition of $\hf_{h,\bb}^{s,a}(t, \hF)$. The last line used the recursive definition of $\oV_{h,\bb}^{s,a}(t)$. 

    For the second claim, we simply observe without rounding that \eqref{equ: approx-dp} is the same as \eqref{equ: exact-dp}. Thus, \cref{lem: exact-dp} yields the result.
\end{proof}



\subsection{Proof of \texorpdfstring{\cref{thm: approximate-bi}}{thm: approximate-bi}}

\begin{proof}
    The fact that \cref{alg: approximate-bi} correctly solves any $\oM$ follows from the fact that \eqref{equ: approx-update} is equivalent to \eqref{equ: update} via \cref{lem: approx-update}.

    For the time complexity claim, observe that the number of subproblems considered is $O(HS^2A|\mB||\hmF|)$ and the time needed per subproblem is $O(|\mB|)$ to explicitly optimize each artificial budget. Thus, the running time is $O(HS^2A|\mB|^2|\hmF|)$. We can further analyze $|\hmF|$ in terms of the original input variables. First, we claim that $\hmF \subseteq [\bmin, \bmax + \ell S]$. To see this, observe the rounded input at state $t+1$ is,
    \begin{equation*}
        \f(\hF, \round{\ell}{b_t}) \geq \f(F, b_t) = \f_{s' = 1}^t g(P_h(s' \mid s,a))b_{s'} \geq \f_{s' = 1}^t g(P_h(s' \mid s,a))\bmin \geq \bmin.
    \end{equation*}
    Here, we used the fact that $\f$ is non-decreasing and the weighted combination is a short map rooted at $0$. Similarly, we see,
    \begin{align*}
        \f(\hF, \round{\ell}{b_t}) &\leq \f(F, \round{\ell}{b_t}) + \ell (t-1) \\
        &\leq \f_{s' = 1}^t g(P_h(s' \mid s,a))(b_{s'}+\ell) + \ell (t-1)\\
        &\leq \f_{s' = 1}^t g(P_h(s' \mid s,a))\bmax + \ell t\\
        &\leq \bmax + \ell t.
    \end{align*}
    
    Under this assumption, it is clear that the number of integer multiples of $\ell$ residing in this superset is $O((\bmax + \ell S - \bmin)/\ell)$ per constraint. When considering all constraints at once, this becomes $O(\norm{\bmax + \ell S - \bmin}_{\infty}^m/\ell^m) = O(\norm{\bmax - \bmin}_{\infty}^m/\ell^m + S^m)$. Incorporating this bound into the runtime then gives $O(HS^{m + 2}A|\mB|^2\norm{\bmax - \bmin}_{\infty}^m/\ell^m)$. 

    Similar to the reasoning above, we can see the cost of any policy, and thus the artificial budget set, is contained with $[H\cmin, H\cmax]$.
    Using this fact, we get the final running time $O(H^{m + 1}S^{m+2}A|\mB|^2\norm{\cmax - \cmin}_{\infty}^m/\ell^m)$.

    
\end{proof}

\section{Proofs for \texorpdfstring{\cref{sec: bicriteria}}{sec: bicriteria}}

\subsection{Time-Space Error Lemmas}

\begin{lemma}[Time Error]\label{lem: time-error}
    For any $h \in [H]$, $a \in \mA$, if $\bb' \leq \bb + x$, then,
    \begin{equation}
        f_{h,\bb}^{s,a}(1, 0) \leq f_{h,\bb'}^{s,a}(1, 0) \leq f_{h,\bb}^{s,a}(1, 0) + x.
    \end{equation}
    Here, we translate a scaler $x > 0$ into the vector $(x,\ldots, x)$.
\end{lemma}

\begin{proof}
    By definition of $f_{h, \bb'}^{s,a}$,
    \begin{align*}
        f_{h, \bb'}^{s,a}(1,0) &= \f(0, \f_{s'} \g(P_h(s' \mid s,a))b'_{s'}) \\
        &= \f_{s'} \g(P_h(s' \mid s,a))b'_{s'} \\
        &\geq \f_{s'} \g(P_h(s' \mid s,a))b_{s'} \\
        &= \f(0, \f_{s'} \g(P_h(s' \mid s,a))b_{s'}) \\
        &= f_{h, \bb}^{s,a}(1,0).
    \end{align*}
    The second and fourth line used the fact that $\f$ is identity preserving. The inequality uses the fact that $\f$ is non-decreasing and $g$ is a non-negative scaler, so the total weighted combination is also non-decreasing. 

    Similarly, we see that,
    \begin{align*}
        f_{h, \bb'}^{s,a}(1,0) &= \f(0, \f_{s'} \g(P_h(s' \mid s,a))b'_{s'}) \\
        &= \f_{s'} \g(P_h(s' \mid s,a))b'_{s'} \\
        &\leq \f_{s'} \g(P_h(s' \mid s,a))(b_{s'} + x) \\
        &\leq \f_{s'} \g(P_h(s' \mid s,a))b_{s'} + x \\
        &= \f(0, \f_{s'} \g(P_h(s' \mid s,a))b_{s'}) + x \\
        &= f_{h, \bb}^{s,a}(1,0) + x.
    \end{align*}
    The second and fifth line used the fact that $\f$ is identity preserving. The first inequality again uses the fact that the weighted combination is non-decreasing. The second inequality follows since the weighted combination is a short map with respect to the infinity norm. 
    
    In particular, since $|\alpha(y) - \alpha(z)| \leq \norm{y - z}_{\infty}$ holds for any infinity-norm short map $\alpha$, we see that $|\alpha(y+z) - \alpha(y)| \leq \norm{z}_{\infty}$. Moreover, if $\alpha$ is non-decreasing and $z$ is a positive scaler treated as a vector, we further have $\alpha(y+z) - \alpha(y) = |\alpha(y+z) - \alpha(y)| \leq \norm{z}_{\infty} = z$. This final inequality immediately implies that $\alpha(y+z) \leq \alpha(y) + z$. When $\alpha$ is vector-valued, this inequality holds component-wise.
\end{proof}

Since $f$ is associative, we can define $f_{h,\bb}^{s,a}(t,F) = f(F,\f_{s' = t}^S \g(P_h(s' \mid s,a))b_{s'})$ either forward recursively or backward recursively.

%\begin{proof}
%    We proceed by induction on $t$.

%    \paragraph{Base Case.} For the base case, we consider $t = S+1$. By definition, we have that $f_{h,\hbb}^{s,a}(S+1, u) = u = f_{h,\bb}^{s,a}(S+1, u)$. Thus, the claim holds.

%    \paragraph{Inductive Step.} For the inductive step, we consider any $t \leq S$. The recursive definition of $f_{h,\hbb}^{s,a}$ implies,
%    \begin{align*}
%        f_{h,\hbb}^{s,a}(t, u) &= f_{h,\hbb}^{s,a}(t+1, \f(u, \g(P_h(t \mid s,a))\hb_t)) \\
%        &\geq f_{h,\bb}^{s,a}(t+1, \f(u, \g(P_h(t \mid s,a))\hb_t)) \\
%        &= \f(\f(u, \g(P_h(t \mid s,a))\hb_t), \f_{s' = t+1}^S\g(P_h(s' \mid s,a)b_{s'}))\\
%        &= \f(\f(u, \g(P_h(t \mid s,a))\round{\ell}{b_t}), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))\\
%        &\geq \f(\f(u, \g(P_h(t \mid s,a))b_t), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) \\
%        &= \f(u, \f(\g(P_h(t \mid s,a))b_t, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))) \\
%        &= f_{h, \bb}^{s,a}(t, u).
%    \end{align*}
%    The first inequality used the induction hypothesis to replace future $\hbb$'s with $\bb$'s, and the second inequality used that $\f$ is non-decreasing in either input and $\round{\ell}{b_t} \geq b_t$. The other lines use $\f$'s associativity.

%    Similarly, we see that,
%    \begin{align*}
%        f_{h,\hbb}^{s,a}(t, u) &= f_{h,\hbb}^{s,a}(t+1, \f(u, \g(P_h(t \mid s,a))\hb_t)) \\
%        &\leq f_{h,\bb}^{s,a}(t+1, \f(u, \g(P_h(t \mid s,a))\hb_t)) + (S - t)\ell\\
%        &= \f(\f(u, \g(P_h(t \mid s,a))\hb_t), \f_{s' = t+1}^S\g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
%        &= \f(\f(u, \g(P_h(t \mid s,a))\round{\ell}{b_t}), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
%        &\leq \f(\f(u, \g(P_h(t \mid s,a))(b_t + \ell)), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
%        &\leq \f(\f(u, \g(P_h(t \mid s,a))b_t) + \ell, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
%        &\leq \f(u, \f(\g(P_h(t \mid s,a))b_t, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))) + (S-t+1)\ell\\
%        &= f_{h, \bb}^{s,a}(t, u) + (S-t+1)\ell.
%    \end{align*}
%    The first inequality used the induction hypothesis to replace future $\hbb$'s with $\bb$'s. The second inequality used the the fact that $\f$ is non-decreasing and that $\round{\ell}{b_t} \leq b_t + \ell$. The third inequality used the fact that $\g \in [0,1]$ so $g\ell \leq \ell$, the fact that $\f$ is non-decreasing, and the fact that $\f$ is a short map in its second input. The final inequality used the fact that $\f$ is a short map in its first input. The other lines use $\f$'s associativity.

%    This completes the proof.
%\end{proof}

\begin{lemma}[Space Error]\label{lem: space-error}
    For any $h \in [H]$, $a \in \mA$, $\bb \in \Real^{m\times S}$, $u \in \Real^m$, and $t \in [S+1]$,
    \begin{equation}
        f_{h,\bb}^{s,a}(t, u) \leq \hf_{h,\bb}^{s,a}(t, u) \leq f_{h,\bb}^{s,a}(t, u) + (S-t+1)\ell.
    \end{equation}
\end{lemma}

\begin{proof}
    We proceed by induction on $t$.

    \paragraph{Base Case.} For the base case, we consider $t = S+1$. By definition, we have that $\hf_{h,\bb}^{s,a}(S+1, u) = u = f_{h,\bb}^{s,a}(S+1, u)$. Thus, the claim holds.

    \paragraph{Inductive Step.} For the inductive step, we consider any $t \leq S$. The recursive definition of $\hf_{h,\bb}^{s,a}$ implies,
    \begin{align*}
        \hf_{h,\bb}^{s,a}(t, u) &= \hf_{h,\bb}^{s,a}(t+1, \round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}) \\
        &\geq f_{h,\bb}^{s,a}(t+1, \round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}) \\
        &= \f(\round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}, \f_{s' = t+1}^S\g(P_h(s' \mid s,a)b_{s'}))\\
        &\geq \f(\f(u, \g(P_h(t \mid s,a))b_t), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))\\
        &= \f(u, \f(\g(P_h(t \mid s,a))b_t, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))) \\
        &= f_{h, \bb}^{s,a}(t, u).
    \end{align*}
    The first inequality used the induction hypothesis to replace $\hf$ with $f$, and the second inequality used that $\f$ is non-decreasing in either input and $\round{\ell}{b_t} \geq b_t$. The other lines use $\f$'s associativity.

    Similarly, we see that,
    \begin{align*}
        \hf_{h,\bb}^{s,a}(t, u) &= \hf_{h,\bb}^{s,a}(t+1, \round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}) \\
        &\leq f_{h,\bb}^{s,a}(t+1, \round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}) + (S - t)\ell\\
        &= \f(\round{\ell}{\f(u, \g(P_h(t \mid s,a))b_t)}, \f_{s' = t+1}^S\g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
        &\leq \f(\f(u, \g(P_h(t \mid s,a))b_t) + \ell, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) + (S - t)\ell\\
        &\leq \f(\f(u, \g(P_h(t \mid s,a))b_t), \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'})) + (S - t+1)\ell\\
        &= \f(u, \f(\g(P_h(t \mid s,a))b_t, \f_{s' = t+1}^S \g(P_h(s' \mid s,a)b_{s'}))) + (S-t+1)\ell\\
        &= f_{h, \bb}^{s,a}(t, u) + (S-t+1)\ell.
    \end{align*}
    The first inequality used the induction hypothesis to replace $\hf$ with $f$. The second inequality used that $\f$ is non-decreasing in either input and $\round{\ell}{x} \leq x + \ell$. The third inequality used that $\f$ is a short map in the first input. The other lines use $\f$'s associativity.

    This completes the proof.
\end{proof}


\subsection{Proof of \texorpdfstring{\cref{lem: optimal-value}}{lem: optimal-value}}

\begin{proof}
    We proceed by induction on $h$.

    \paragraph{Base Case.} For the base case, we consider $h = H+1$. Since $\round{\ell}{b} \geq b$, we immediately see,
    \begin{equation}
        \hV_{H+1}^*(s,\round{\ell}{b}) = -\chi_{\set{\round{\ell}{b} \geq 0}} \geq -\chi_{\set{b \geq 0}} = \oV^*_{H+1}(s,b).
    \end{equation}

    \paragraph{Inductive Step.} For the inductive step, we consider any $h \leq H$. If $\oV^*_h(s,b) = -\infty$, then trivially $\hV^*_h(s,\round{\ell}{b}) \geq \oV^*_h(s, b)$. Now, suppose that $\oV^*_h(s,b) > -\infty$. Let $\pi$ be a solution to the optimality equations for $\oM$. Consequently, we know that $\oV_h^{\pi}(s,b) = \oV^*_h(s,b) > -\infty$, which implies $(a^*,\bb^*) = \pi_h(s,b) \in \oA_h(s,b)$.
    By definition of $\oA_h(s,b)$,
    \begin{equation}\label{equ: action-helper}
        c_h(s,a^*) + f^{s,a^*}_{h, \bb^*}(1, 0) = c_h(s,a^*) + \f_{s'} g(P_h(s' \mid s,a^*)) b_{s'}^* \leq b \leq \round{\ell}{b}.
    \end{equation}
    For each $s' \in \mS$, define $\hb_{s'}^* \defeq \round{\ell}{b_{s'}^*}$. We show $(a^*, \hbb_{s'}^*) \in \hA_h(s, \round{\ell}{b})$ as follows:
    \begin{align*}
        c_h(s,a^*) + \hf_{h,\hbb^*}^{s,a}(1,0) &\leq c_h(s,a^*) + f_{h,\hbb^*}^{s,a}(1,0) + \ell S \\
        &\leq c_h(s,a^*) + f_{h,\bb^*}^{s,a}(1,0) + \ell (S+1) \\
        &\leq \round{\ell}{b} + \ell (S+1) \\
        &= \kappa(\round{\ell}{b}).
    \end{align*}
    The first inequality follows from \cref{lem: space-error}. The second inequality follows from \cref{lem: time-error} with $\hbb^* \leq \bb^* + \ell$. The third inequality follows from \eqref{equ: action-helper}. The equality follows by definition of $\kappa$. Thus, $(a^*, \hbb_{s'}^*) \in \hA_h(s, \round{\ell}{b})$.

    Since $b_{s'}^* \in \mB$ by definition, the induction hypothesis implies that $\hV^*_{h+1}(s', \hb_{s'}^*) \geq \oV^*_{h+1}(s', b_{s'}^*) = \oV^{\pi}_{h+1}(s', b_{s'}^*)$.
    The optimality equations for $\hM$ then imply that,
    \begin{align*}
        \hV^*_h(s,\round{\ell}{b}) &= \max_{(a,\hbb) \in \hA_h(s,b)} r_h(s, a) + \sum_{s'} P_h(s' \mid s,a) \hV_{h+1}^{*}\paren{s',\hb_{s'}} \\
        &\geq r_h(s, a^*) + \sum_{s'} P_h(s' \mid s,a) \hV_{h+1}^{*}\paren{s', \hb_{s'}^*}\\
        &\geq r_h(s, a^*) + \sum_{s'} P_h(s' \mid s,a) \oV_{h+1}^{\pi}\paren{s', b^*_{s'}} \\
        &= \oV_h^{\pi}(s,b) \\
        &= \oV_h^*(s,b).
    \end{align*}
    The first inequality used the fact that $(a^*, \hbb^*) \in \hA_h(s,b)$. The second inequality follows from the induction hypothesis. The last two equalities follow from the standard policy evaluation equations and the definition of $\pi$ respectively. 
    This completes the proof.
    
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{lem: approximate-cost}}{lem: approximate-cost}}

\begin{proof}
    We proceed by induction on $h$. 
    
    \paragraph{Base Case.} For the base case, we consider $h = H+1$. By definition and assumption, $\hV^{\pi}_{H+1}(s,\hb) = -\chi_{\set{\hb \geq 0}} > -\infty$. Thus, it must be the case that $\hb \geq 0$ and so by definition $\hC_{H+1}^{\pi}(s,\hb) = 0 \leq \hb$.

    \paragraph{Inductive Step.} For the inductive step, we consider any $h \leq H$. As in the proof of \cref{lem: cost}, we know that $(a, \hbb) = \pi_h(s,b) \in \hA_h(s,\hb)$ and for any $s' \in \mS$ with $P_h(s' \mid s,a) > 0$ that $\hV_{h+1}^{\pi}(s', b_{s'}) > -\infty$.
    Thus, the induction hypothesis implies that $\hC^{\pi}_{h+1}(s', \hb_{s'}) \leq \hb_{s'} + \ell(S+1)(H-h)$ for any such $s'$. For any other $s'$, we have $\g(P_h(s' \mid s,a)) = \g(0) = 0$ by assumption.
    
    Thus, the weighted combination of $\hC^{\pi}_{h+1}(s', \hb_{s'})$ is equal to the weighted combination of $\hbb'$ where $\hb'_{s'} \defeq \hC^{\pi}_{h+1}(s', \hb_{s'})$ if $P_h(s' \mid s,a) > 0$ and $\hb'_{s'} \defeq 0$ otherwise. Moreover, we have $\hbb' \leq \hbb + \ell(S+1)(H-h)$ since $\ell > 0$. Thus, by \eqref{equ: sr}, 
    \begin{align*}
        \hC^{\pi}_h(s,\hb) &= c_h(s, a) + \f_{s'} g(P_h(s' \mid s,a))\hC^{\pi}_{h+1}(s',\hb_{s'}) \\
        &= c_h(s,a) + f_{h, \hbb'}^{s,a}(1,0)\\
        &\leq c_h(s,a) + f_{h, \hbb}^{s,a}(1,0) + \ell(S+1)(H-h)\\
        &\leq \kappa(\hb) + \ell(S+1)(H-h)\\
        &= \hb + \ell(S+1)(H-h+1).
    \end{align*}
    The first inequality used \cref{lem: time-error}. The second inequality used the fact that $(a, \hbb) \in \mA_h(s,\hb)$. The last line used the definition of $\kappa$.
    This completes the proof.
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{thm: bicriteria}}{thm: bicriteria}}


%\begin{lemma}
%    If \eqref{equ: constrained} is feasible, then $\hV^*_{1}(s_0, \round{\ell}{B}) > -\infty$.
%\end{lemma}

%\begin{proof}
%    Suppose $\pi$ is feasible 
%\end{proof}

\begin{proof}
    If \eqref{equ: constrained} is feasible, then inductively we see that $\hV^*_{1}(s_0, \round{\ell}{B}) > -\infty$. The contrapositive then implies if $\hV^*_{1}(s_0, \round{\ell}{B}) = -\infty$, then \eqref{equ: constrained} is infeasible. Thus, when \cref{alg: bicriteria} outputs ``Infeasible'' it is correct.

    On the other hand, suppose $\hV^*_{1}(s_0, \round{\ell}{B}) > -\infty$ and that $\pi$ is an optimal solution to $\hM$. By \cref{lem: optimal-value} and \cref{lem: value}, we know that $\hV^{\pi}_1(s_0, \round{\ell}{B}) \geq \oV^{\pi}_1(s_0, B) \geq V^*$. Also, by \cref{lem: approximate-cost}, we know that $\hC^{\pi}_1(s_0, \round{\ell}{B}) \leq \round{\ell}{B} + \ell(S+1)H \leq B + \ell(1 + (S+1)H)$. Our choice of $\ell = \frac{\epsilon}{1 + (S+1)H}$ then implies that $\hC^{\pi} = \hC^{\pi}_1(s_0, \round{\ell}{B}) \leq B + \epsilon$. Thus, $\pi$ is an $(0, \epsilon)$-additive bicriteria approximation for \eqref{equ: constrained}.

    Both cases together imply that \cref{alg: bicriteria} is a valid $(0,\epsilon)$-bicriteria.
    
    \paragraph{Time Complexity.} We see immediately from \cref{thm: approximate-bi} that the running time of \cref{alg: bicriteria} is at most $O\paren{H^{2m+1}S^{2m+2}A|\hmB|^2\norm{\cmax - \cmin}_{\infty}^m/\epsilon^m}$. To complete the analysis, we need to bound $|\hmB|$. First, we note $|\hmB|$ is at most the number of integer multiples of $\ell$ in the range $[\bmin, \bmax] \subseteq [H\cmin, H\cmax]^m$. For any individual constraint, this number is at most $O(H(\cmax - H\cmin)/\ell) \leq O(H^2S(\cmax - \cmin)/\epsilon)$ using the definition of $\ell = \frac{\epsilon}{1 + (S+1)H}$. Thus, over all constraints, the total number of rounded artificial budgets is at most $O((H^2S\norm{\cmax - \cmin}/\epsilon)^m)$. Squaring this quantity and plugging it back into our original formula yields: $O\paren{H^{6m+1}S^{4m+2}A\norm{\cmax - \cmin}_{\infty}^{3m}/\epsilon^{3m}}$.
    
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{prop: multi-hardness}}{prop: multi-hardness}}

\begin{proof}
    We consider a reduction from the Hamiltonian Path problem. The transitions reflect the graph structure, and the actions determine the edge to follow next. To determine if a Hamiltonian path exists, we can simply make an indicator constraint for each node that signals that node has been reached. It is then clear that relaxing the budget constraint does not help since we can always shrink the budget for any given $\epsilon$-slackness. Thus, the claim holds.
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{lem: discretized}}{lem: discretized}}

\begin{proof}
    We proceed by induction on $h$.

    \paragraph{Base Case.} For the base case, we consider $h = H+1$. By definition, we have $\tV^{\pi}_{H+1}(\Tilde{\tau}_{H+1}) = 0 = V^{\pi}_{H+1}(\tau_{H+1})$ and $\tC^{\pi}_{H+1}(\Tilde{\tau}_{H+1}) = 0 = C^{\pi}_{H+1}(\tau_{H+1})$.

    \paragraph{Inductive Step.} For the inductive step, we consider any $h \leq H$. For simplicity, let $x \defeq \ell (\lambda_r + \lambda_p)H\rmax(\smax - \smin)$. The standard policy evaluation equations imply that,
    \begin{align*}
        \tV^{\pi}_h(\Tilde{\tau}_h) &= r_h(\round{\ell}{s}, a) + \sum_{\ts'} \tP_h(\ts' \mid \round{\ell}{s}, a) \tV^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= r_h(\round{\ell}{s}, a) + \sum_{\ts'} \int_{s' = \ts'}^{\ts' + \ell}P_h(s' \mid \round{\ell}{s}, a)ds' \tV^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= r_h(\round{\ell}{s}, a) + \int_{s'}P_h(s' \mid \round{\ell}{s}, a) \tV^{\pi}_{h+1}(\Tilde{\tau}_{h+1})ds' \\
        &\geq r_h(\round{\ell}{s}, a) + \int_{s'}P_h(s' \mid \round{\ell}{s}, a) (V^{\pi}_{h+1}(\tau_{h+1}) - x(H-h))ds' \\
        &= r_h(\round{\ell}{s}, a) + \int_{s'}P_h(s' \mid \round{\ell}{s}, a)V^{\pi}_{h+1}(\tau_{h+1})ds' - x(H-h) \\
        &\geq r_h(s,a) -\ell\lambda_r +  \int_{s'}(P_h(s' \mid s, a) - \ell\lambda_p)V^{\pi}_{h+1}(\tau_{h+1})ds' - x(H-h) \\
        &=V_h^{\pi}(\tau_h) -\ell\lambda_r -\ell\lambda_p\int_{s'}V^{\pi}_{h+1}(\tau_{h+1})ds' - x(H-h) \\
        &\geq V_h^{\pi}(\tau_h) - \ell \lambda_r - \ell \lambda_pH\rmax(\smax - \smin) - x(H-h) \\
        &\geq V_h^{\pi}(\tau_h) - \ell (\lambda_r + \lambda_p)H\rmax(\smax - \smin) - x(H-h) \\
        &= V_h^{\pi}(\tau_h) - x(H-h+1).
    \end{align*}
    
    If we let $y \defeq \ell (\lambda_c + \lambda_p)H\cmax(\smax - \smin)$, we also see that,
    \begin{align*}
        \tC^{\pi}_h(\Tilde{\tau}_h) &= c_h(\round{\ell}{s}, a) + \Tilde{\f}_{\ts'}\tP_h(\ts' \mid \round{\ell}{s}, a)\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &=  c_h(\round{\ell}{s}, a) + \Tilde{\f}_{\ts'}\int_{s' = \ts'}^{\ts' + \ell}P_h(s' \mid \round{\ell}{s}, a)ds'\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= c_h(\round{\ell}{s}, a) + \f_{s'} P_h(s' \mid \round{\ell}{s}, a)\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &\leq c_h(\round{\ell}{s}, a) + \f_{s'} P_h(s' \mid \round{\ell}{s}, a)(C^{\pi}_{h+1}(\tau_{h+1}) + y(H-h)) \\
        &\leq c_h(\round{\ell}{s}, a) + \f_{s'} P_h(s' \mid \round{\ell}{s}, a)C^{\pi}_{h+1}(\tau_{h+1}) + y(H-h) \\
        &\leq c_h(s,a)  + \ell \lambda_c  + \f_{s'} (P_h(s' \mid s,a) + \ell \lambda_p)C_{h+1}^{\pi}(\tau_{h+1}) + y(H-h) \\
        &= c_h(s,a) + \f_{s'} P_h(s' \mid s,a) C_{h+1}^{\pi}(\tau_{h+1}) + \ell \lambda_c + \ell \lambda_p \f_{s'} C_{h+1}^{\pi}(\tau_{h+1}) + y(H-h) \\
        &\leq C_h^{\pi}(\tau_h) + \ell \lambda_c + \ell \lambda_p \f_{s'} H\cmax + y(H-h) \\
        &\leq C_h^{\pi}(\tau_h) + \ell \lambda_c + \ell \lambda_p (\smax - \smin)H\cmax + y(H-h) \\
        &\leq  C_h^{\pi}(\tau_h) + \ell (\lambda_c + \lambda_p) (\smax - \smin)H\cmax + y(H-h) \\
        &=  C_h^{\pi}(\tau_h)  + y(H-h+1).
    \end{align*}
    We note the above also holds if $P$ is replaced with a $g(P)$ for a sublinear short map $g$.

    For almost-sure constraints, the proof is slightly different since we need to keep the inner integral by definition of the worst-case cost for continuous state spaces. Letting $y \defeq \ell (\lambda_c + \lambda_p)H\cmax(\smax - \smin)/\Tilde{p}_{min}$, the bound then becomes,
    \begin{align*}
        \tC^{\pi}_h(\Tilde{\tau}_h) &=  c_h(\round{\ell}{s}, a) + \max_{\ts'}[\tP_h(\ts' \mid \round{\ell}{s}, a) > 0]\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= c_h(\round{\ell}{s}, a) + \max_{\ts'}[\int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \round{\ell}{s}, a)ds' > 0]\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= c_h(\round{\ell}{s}, a) + \max_{\ts'}\frac{\int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \round{\ell}{s}, a)ds'}{p_{\ts'}}\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1}) \\
        &= c_h(\round{\ell}{s}, a) + \max_{\ts'}\int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \round{\ell}{s}, a)\tC^{\pi}_{h+1}(\Tilde{\tau}_{h+1})ds'/p_{\ts'} \\
        &\leq c_h(\round{\ell}{s}, a) + \max_{\ts'}\int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \round{\ell}{s}, a)(C^{\pi}_{h+1}(\tau_{h+1})+y(H-h))ds'/p_{\ts'}\\
        &\leq c_h(\round{\ell}{s}, a) + \max_{\ts'}\int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid \round{\ell}{s}, a)C^{\pi}_{h+1}(\tau_{h+1})ds'/p_{\ts'} + y(H-h)\\
        &\leq c_h(s,a) + \ell\lambda_c + \max_{\ts'} \int_{s' = \ts'}^{\ts' + \ell} P_h(s' \mid s, a) C_{h+1}^{\pi}(\tau_{h+1})ds'/p_{\ts'} \\
        &+\ell \lambda_p \max_{\ts'} \int_{s' = \ts'}^{\ts' + \ell} C_{h+1}^{\pi}(\tau_{h+1})ds'/p{\ts'} + y(H-h)\\
        &\leq c_h(s,a) + \max_{\mS' \subseteq \mS} \int_{\mS'} \frac{P_h(s' \mid s,a)}{p_{\mS'}} C_{h+1}^{\pi}(\tau_{h+1})ds' + \ell \lambda_c + \ell^2 \lambda_p H\cmax/\Tilde{p}_{min}\\
        & + y(H-h)\\
        &= C_h^{\pi}(\tau_h) + \ell \lambda_c + \ell^2 \lambda_p H\cmax/\Tilde{p}_{min}+ y(H-h)\\
        &\leq C_h^{\pi}(\tau_h) + \ell(\lambda_c + \lambda_p)(\smax - \smin)H\cmax/\Tilde{p}_{min} + y(H-h) \\
        &=  C_h^{\pi}(\tau_h) + y(H-h+1).
    \end{align*}

\end{proof}

\subsection{Proof of \texorpdfstring{\cref{thm: discretized}}{thm: discretized}}

\begin{proof}
    The theorem follows immediately from \cref{thm: bicriteria} and \cref{lem: discretized}.
\end{proof}

\section{Extensions}

\paragraph{Markov Games.} It is easy to see that our augmented approach works to compute constrained equilibria. For efficient algorithms, using $-\infty$ to indicate infeasibility becomes problematic. However, we can still use per stage LP solutions and add a constraint that the equilibria value must be larger than some very small constant to rule out invalid $-\infty$ solutions. Alternatively the AND/OR tree approach used in \cite{acMARL} can be applied here to directly compute all the near feasible states. 

\paragraph{Infinite Discounting.}
Since we focus on approximation algorithms, the infinite discounted case can be immediately handled by using the idea of effective horizon. We can treat the problem as a finite horizon problem where the finite horizon $H$ defined so that $\sum_{h = H}^{\infty} \gamma^{h-1} \cmax \leq \epsilon'$. By choosing $\epsilon'$ and $\epsilon$ small enough, we can get equivalent feasibility approximations. The discounting also ensures the effective horizon $H$ is polynomially sized implying efficient computation. 

\paragraph{Stochastic Policies.}
For stochastic policies, our approximate results follow from simply replacing each $\max_{a}$ and $\max_{b_t}$ with a general linear program over a finite distribution, which can be solved in polynomial time. 

\paragraph{Stochastic Costs.} For finitely-supported cost distributions, all results remain the same except for almost-sure/anytime constraints which now must be written in the form:
\begin{equation}
    C_h^{\pi}(\tau_h) = \max_{c \in \supp(C_h(s,a))} c + \max_{s'} [P_h(s' \mid s,a) > 0]C_h^{\pi}(\tau_h, a, c, s').
\end{equation}
Also, note histories must now be cost dependent. 

Now, we have that future budgets depend on both the next state and the realized cost, so our \eqref{equ: approx-dp} must now be dependent on both states and immediate costs for subproblems. The construction is similar to the approach in ~\cite{dcRL}.

\end{document}