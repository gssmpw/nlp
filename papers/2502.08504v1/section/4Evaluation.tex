\section{Evaluation}\label{sec: Evaluation}
In this section, we aim to empirically evaluate the effectiveness of \(\text{\tool}\) in generating {\mccs}s. Specifically, we will address the following research questions:

\newcommand{\rqone}{Can \(\text{\tool}\) effectively generate {\mccs}s across different modules of the ADS?}
\newcommand{\rqtwo}{How accurate is \(\text{\oracle}\) in identifying {\mccs}s?}
\newcommand{\rqthree}{What is the usefulness of \(\text{\feedback}\) and \(\text{\select}\) in \(\text{\tool}\)?}
\newcommand{\rqfour}{How does \tool perform from the perspective of testing efficiency?}
\noindent \textbf{RQ1: } \rqone{}

\noindent \textbf{RQ2: } \rqtwo{}

\noindent \textbf{RQ3: } \rqthree{}

\noindent \textbf{RQ4: } \rqfour{}

To address these research questions, we conducted experiments using the following settings:

\textbf{Environment.} We conducted our experiments using Pylot~\cite{gog2021pylot} and CARLA~\cite{dosovitskiy2017carla}. Pylot, a widely popular open-source, multi-module ADS platform, includes modules for Perception $\mathcal{M}^{\text{perc}}$, Prediction $\mathcal{M}^{\text{pred}}$, Planning $\mathcal{M}^{\text{plan}}$, and Control $\mathcal{M}^{\text{ctrl}}$. CARLA is a high-fidelity simulator that is compatible with Pylot.

\textbf{Driving Scenarios.} 
We evaluate \tool on four representative scenarios derived from the NHTSA pre-crash typology~\cite{najm2007pre}, which is also widely utilized in existing ADS testing techniques~\cite{li2020av, cheng2023behavexplor}. Specifically, these scenarios are:

\noindent \textit{S1:} The ego vehicle starts in a weave zone and merges onto the highway.

\noindent \textit{S2:} The ego vehicle leaves the highway via an exit ramp.

\noindent \textit{S3:} The ego vehicle turns left at an uncontrolled intersection.

\noindent \textit{S4:} The ego vehicle turns right at an uncontrolled intersection; a full stop is required before turning.


\textbf{Baselines.}
We selected three state-of-the-art system-level testing methods for the comparisons. We first select a \textit{Random} method that randomly generates scenarios. Additionally, we included two representative state-of-the-art ADS testing techniques for reference: \textit{AVFuzzer}~\cite{li2020av} and \textit{BehAVExplor}~\cite{cheng2023behavexplor}. \textit{AVFuzzer} aims to efficiently identify collisions, whereas \textit{BehAVExplor} focuses on discovering a more diverse set of safety-critical scenarios. 
Note that \textit{AVFuzzer} and \textit{BehAVExplor} were originally evaluated using the Apollo~\cite{baiduapollo} with LGSVL~\cite{rong2020lgsvl}. However, since LGSVL was sunsetted in 2022~\cite{LGSVLSunsetting}, we adapted these techniques to our simulation environment, Pylot with CARLA, for comparison. 

\textbf{Metrics.} To evaluate the effectiveness of \tool, we utilize the metric $\#\mathcal{M}^{k}$, which quantifies the number of generated \mccs\ for the user-specific module $\mathcal{M}^{k}$. 
Additionally, we aim to confirm that the detected {\mccs}s are truly induced solely by errors in the module \( \mathcal{M} \). To this end, we define a repair rate metric as \( \%\mathcal{M}^{k} = \frac{\#\mathcal{M}^{k}_{\text{rep}}}{\#\mathcal{M}^{k}} \times 100\% \) to verify the correctness of \oracle, where \( \#\mathcal{M}^{k}_{\text{rep}} \) denotes the number of \mccs\ successfully repaired.
Here, $k$ is the identifier for the module $\mathcal{M}^{k}$; for instance, $k = \text{perc}$ denotes the Perception module. 
Specifically, we rerun and repair the detected \mccs\ by replacing the outputs of $\mathcal{M}^{k}$ with the ground truth. Subsequently, we count the number of \mccs\ that have been repaired and no longer present any safety-critical issues.


\textbf{Implementation.} 
According to our preliminary study (see Section \ref{sec: perliminary_study}), we set the tolerance thresholds $\lambda_{\mathcal{M}^k}$ for Perception, Prediction, and Control modules as $\lambda_{\mathcal{M}^{\text{perc}}} = 0.5$, $\lambda_{\mathcal{M}^{\text{pred}}} = 0.1$, $\lambda_{\mathcal{M}^{\text{plan}}} = 0$, and $\lambda_{\mathcal{M}^{\text{ctrl}}} = 0.05$, respectively. The detection window size $\Delta t$ is set to 0.5 seconds because this is the smaller length of the prediction and planning module output in Pylot's default configuration.
In our experiments, we utilize the synchronized mode in CARLA to mitigate the influence of non-determinism inherent in the simulation-based execution of the ADS. We repeat each experiment three times with different random seeds to report the average of the results. For each run, we use a consistent budget of three hours, which we found sufficient for comparison. 
% The implementation details of \tool are available on our website~\cite{ourweb}.

\begin{table}[!t]
    \centering
    \caption{Comparison results with baselines.}
    \vspace{-10pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccccc|ccccc|ccccc|ccccc}
    \toprule
         \multirow{2.5}*{Method} & \multicolumn{5}{c|}{\#$\mathcal{M}^{\text{perc}}$} & \multicolumn{5}{c|}{\#$\mathcal{M}^{\text{pred}}$} & \multicolumn{5}{c|}{\#$\mathcal{M}^{\text{plan}}$} & \multicolumn{5}{c}{\#$\mathcal{M}^{\text{ctrl}}$}\\
        \cmidrule(lr){2-6}\cmidrule(lr){7-11}\cmidrule(lr){12-16}\cmidrule(lr){17-21}
          & \textit{S1} & \textit{S2} & \textit{S3} & \textit{S4} & \cellcolor{lightgray!20}\textit{Sum.} & \textit{S1} & \textit{S2} & \textit{S3} & \textit{S4} & \cellcolor{lightgray!20}\textit{Sum.} & \textit{S1} & \textit{S2} & \textit{S3} & \textit{S4} & \cellcolor{lightgray!20}\textit{Sum.} & \textit{S1} & \textit{S2} & \textit{S3} & \textit{S4} & \cellcolor{lightgray!20}\textit{Sum.} \\
         \midrule
         \textit{Random} & 2.3 & 1.7 & 2.0 & 1.7 & \cellcolor{lightgray!20}7.7 & 6.3 & 11.3 & 4.0 & 1.7 & \cellcolor{lightgray!20}23.3 & 11.3 & 11.7& 7.3 & 3.7& \cellcolor{lightgray!20}34.0 & 3.3 & 0 & 1.3 & 0 & \cellcolor{lightgray!20}4.6\\
         \textit{AVFuzzer} & 3.3 & 4.7 & 1.3 & 3.7 & \cellcolor{lightgray!20}13.0 & 11.7 & 6.7 & 3.7& 3.3 & \cellcolor{lightgray!20}25.3 & 7.7 & 10.7 & 5.7 & 5.3 & \cellcolor{lightgray!20}29.3 & 3.7 & 0.7 & \textbf{2.0} & 0.3 & \cellcolor{lightgray!20}6.7\\
         \textit{BehAVExplor} & 5.3 & 4.3& 4.3 & 5.7 & \cellcolor{lightgray!20}18.3 & 6.7 & 10.3 & 6.0 & 1.3 & \cellcolor{lightgray!20}19.3 & 11.0 & 13.3 & 7.3 & 2.7& \cellcolor{lightgray!20}34.3 & 4.7 & 1.3 & 1.0 & 0.0 & \cellcolor{lightgray!20}7.0\\
         \tool & \textbf{23.0} & \textbf{7.7} & \textbf{14.0} & \textbf{10.7} & \cellcolor{lightgray!20}\textbf{55.3} & \textbf{28.7} & \textbf{24.3} & \textbf{13.3} & \textbf{9.0} & \cellcolor{lightgray!20}\textbf{75.3} & \textbf{26.0} & \textbf{21.0} & \textbf{18.0} & \textbf{6.7} & \cellcolor{lightgray!20}\textbf{71.7} & \textbf{6.0} & \textbf{5.0} & \textbf{2.0} & \textbf{1.3}&\cellcolor{lightgray!20} \textbf{14.3}\\
         \bottomrule
    \end{tabular}
    }
    
    \label{tab:RQ1}
\end{table}



\subsection{RQ1: Effectiveness of \tool}\label{sec:exp_rq1}

\subsubsection{Comparative Results} 
Table~\ref{tab:RQ1} presents the comparison results on the number of generated \mccs\ $\#\mathcal{M}^{k}$, for each individual module in the ADS, covering Perception $\#\mathcal{M}^{\text{perc}}$, Prediction $\#\mathcal{M}^{\text{pred}}$, Planning $\#\mathcal{M}^{\text{plan}}$, and Control $\#\mathcal{M}^{\text{ctrl}}$. 
From the results, we can find that \tool achieves significantly better performance than other baselines in generating \mccs\ for each module. 
Specifically, in terms of the \textit{Sum.} numbers of $\#\mathcal{M}^{k}$, \tool substantially outperforms the best baseline: \textit{BehAVExplor} on $\#\mathcal{M}^{\text{perc}}$ (55.3 vs. 18.3), \textit{AVFuzzer} on $\#\mathcal{M}^{\text{pred}}$ (75.3 vs. 25.3), \textit{BehAVExplor} on $\#\mathcal{M}^{\text{plan}}$ (71.7 vs. 34.3), and \textit{BehAVExplor} on $\#\mathcal{M}^{\text{ctrl}}$ (6.0 vs. 4.7). 
It is worth noting that \textit{BehAVExplor} outperforms other baselines in most modules due to its diversity feedback mechanism. This mechanism encourages the ego vehicle to explore different behaviors, making it more likely to generate safety-critical scenarios caused by different modules. 
By comparing results across different modules, we find that \tool identifies \#\mccs\ in descending order: Prediction (75.3) $>$ Planning (71.7) $>$ Perception (55.3) $>$ Control (14.3). This hierarchy indicates that in the ADS, the robustness of the Prediction, Planning, and Perception modules is comparatively lower and requires further development. 
We note that both comparison methods and \tool only detects a very small number of $\#\mathcal{M}^{\text{ctrl}}$. 
In the experiment, the control module is based on PID~\cite{johnson2005pid}, and with appropriate parameter settings, PID controllers generally exhibit high robustness~\cite{aastrom1993automatic}. On the other hand, scenario-based testing cannot directly generate sudden load changes and other variations commonly encountered in PID testing~\cite{brannstrom2010model}. Nonetheless, \tool still achieves an improvement over baseline methods in detecting $\mathcal{M}^{\text{Ctrl}}CCS$.
Moreover, \tool consistently achieves better performance than all baselines across the four scenarios, \textit{S1} to \textit{S4}, demonstrating its generalization capability in various situations. 

\begin{figure}[!t]
    \centering
    \subfloat[\scriptsize $\mathcal{M}^{\text{perc}}$ICS]{\includegraphics[width=0.22\linewidth]{fig/perception_caused.png}}
    \quad
    \subfloat[\scriptsize $\mathcal{M}^{\text{pred}}$ICS]{\includegraphics[width=0.22\linewidth]{fig/prediction_cause.png}}
    \quad
    \subfloat[\scriptsize $\mathcal{M}^{\text{plan}}$ICS]{\includegraphics[width=0.22\linewidth]{fig/planning_caused.png}}
    \quad
    \subfloat[\scriptsize $\mathcal{M}^{\text{ctrl}}$ICS]{\includegraphics[width=0.22\linewidth]{fig/control_cause.png}}
    \caption{Cases of {\mccs}s detected by \tool for different specific modules.}
    \label{fig:case}
\end{figure}
\subsubsection{Case Study} Fig.~\ref{fig:case} presents four representative \mccs\ examples for modules in the ADS, respectively. 

(a) $\mathcal{M}^{\text{perc}}$ICS. Fig.~\ref{fig:case}(a) illustrates a collision caused solely by the Perception module. In this scenario, the ego car turns left and erroneously detects the NPC cars' locations as they shift to the left. As a result, the ADS assumes a safe distance between the detected NPC cars and, therefore, maintains its acceleration decision, ultimately leading to a collision.

(b) $\mathcal{M}^{\text{pred}}$ICS. Fig.~\ref{fig:case}(b) illustrates a collision scenario resulting from inaccurate predictions of surrounding NPC cars' movements by the Prediction module. In this instance, the ego car maintains its lane, while a nearby NPC car on the right initiates a lane change to cut in. However, the Prediction module erroneously predicts that the NPC car will stay in its current lane. Consequently, the ADS continues its original trajectory without accounting for the NPC car's lane change, which ultimately leads to a collision.

(c) $\mathcal{M}^{\text{plan}}$ICS. Fig.~\ref{fig:case}(c) illustrates a collision caused by the Planning module of the ego car making an unsafe decision. In this case, the ego car proceeds along its initially planned path (Actual Plan), even though a safer alternative trajectory (Good Plan) is available. The ego car fails to account for the NPC car's predicted trajectory, which leads to a close interaction. By not choosing the safer path, the Planning module's decision ultimately results in a collision with the NPC car.

(d) $\mathcal{M}^{\text{ctrl}}$ICS. Fig~\ref{fig:case}(d) is a collision caused by the Control module in the ADS. Specifically, the ego of the upstream Planning makes a safe planning path (Ego Plan), while the Control module can not adjust the ego-motion according to the Ego Plan in time. The latency response in the Control module finally results in a collision with the NPC car. 

\begin{ansbox}
\textbf{Answer to RQ1:} \tool can effectively generate {\mccs}s for user-specified module $\mathcal{M}^{k}$ in the ADS, covering Perception, Prediction, Planning and Control modules.  
\end{ansbox}

\begin{table}[!t]
    \centering
    \caption{Correctness of \oracle for the Perception and Prediction modules.}
    \vspace{-10pt}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|ccc|ccc|ccc|ccc}
        \toprule
         \multirow{2.5}*{Scenario} & \multicolumn{3}{c|}{Perception} & \multicolumn{3}{c|}{Prediction} & \multicolumn{3}{c|}{Planning} & \multicolumn{3}{c}{Control}\\
         \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
         & \#$\mathcal{M}^{\text{perc}}$ & \#$\mathcal{M}^{\text{perc}}_{\text{rep}}$ & \%$\mathcal{M}^{\text{perc}}$ & \#$\mathcal{M}^{\text{pred}}$ & \#$\mathcal{M}^{\text{pred}}_{\text{rep}}$ & \%$\mathcal{M}^{\text{pred}}$ &
         \#$\mathcal{M}^{\text{plan}}$ & \#$\mathcal{M}^{\text{plan}}_{\text{rep}}$ & \%$\mathcal{M}^{\text{plan}}$  &
         \#$\mathcal{M}^{\text{ctrl}}$ & \#$\mathcal{M}^{\text{ctrl}}_{\text{rep}}$ & \%$\mathcal{M}^{\text{ctrl}}$ \\
         \midrule
         \textit{S1} & 23.0 & 21.0 & 91.3\% & 28.7 & 24.3 &85.0\% & 26.0 & 26.0 & 100.0\% & 6.0& 6.0 & 100.0\%\\
         \textit{S2} & 7.7 & 7.3 & 96.1\% &  24.3 & 22.0 & 90.5\% & 21.0& 21.0 & 100.0\% & 5.0& 5.0 & 100.0\%\\
         \textit{S3} & 14.0 & 11.7 &83.6\% & 13.3 & 12.7 & 95.5\% & 18.0& 18.0 & 100.0\%& 2.0& 2.0 & 100.0\%\\
         \textit{S4} & 10.7 & 9.3 &87.7\% & 9.0 & 8.3 & 92.2\% & 6.7& 6.7 & 100.0\% &1.3 & 1.3 & 100.0\%\\
         \textit{Sum.} &55.3 & 49.3 & 89.2\% &75.3 & 67.3 & 89.3\% & 71.7& 71.7 & 100.0\% &14.3 & 14.3 & 100.0\%\\
         \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
    \label{tab:rq2}
\end{table}
\subsection{RQ2: Correctness of \oracle}
\label{sec:exp_rq2}

\subsubsection{Setup}
To evaluate the fidelity of \oracle, we rerun all detected {\mccs}s by substituting the outputs of module $\mathcal{M}^{k}$ with perfect outputs starting from timestamp $t_{\mathcal{M}^{k}}$. 
Here, \( t_{\mathcal{M}^{k}} \) denotes the timestamp of the first error in module \( \mathcal{M}^{k} \) detected within the detection window (defined in Section~\ref{sec:filter}).
For the Perception module, we replace its results with bounding boxes retrieved from CARLA, ensuring that the module's outputs align with the ground truth. For the Prediction module, we obtain ground truth data from the collected dataset. Specifically, when rerunning a scenario $s$ with its collected observations $\mathcal{O}(s)$, at time $t$, we retrieve the actual trajectory of the NPC object over the interval $[t, t + H_{\text{pred}}]$ from the simulator's observation $\mathcal{Y}(s) \in \mathcal{O}(s)$ and use this as the ground truth for the Prediction module.
Considering the vast space of possible planned trajectories and control commands, we use the safest solution, which involves immediate braking, as the ground truth. This approach ensures that both the planning module and the control module consistently make the safest possible decisions.

\subsubsection{Results} 
Table~\ref{tab:rq2} presents the results for the correctness of \oracle. By replacing the module outputs with a safe ground truth, we observe that most of the detected {\mccs}s can be repaired, with average repair rates of 89.2\%, 89.3\%, 100\% and 100\% for the Perception, Prediction, Planning, and Control modules, respectively. These results demonstrate that our proposed \oracle can accurately identify the module-level root causes.
Notably, the repair rates for the Perception and Prediction modules are not 100\% (89.2\% and 89.3\%, respectively). This is primarily because these two modules are upstream; even when corrected with safe ground truth, it does not necessarily ensure that the downstream modules will continue to make safe decisions.

\begin{ansbox}
    \textbf{Answer to RQ2:} \oracle can accurately identify the module whose errors are the root cause of safety-critical violations.
\end{ansbox}


\begin{table}[!t]
\caption{Results of ablation study on Prediction module.}
\vspace{-10pt}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|cccccc}
        \toprule
        Scenario & \tool & \textit{w/o Fine} & \textit{w/o Coarse} & \textit{w/o Select} & \textit{w/o F-$\mathcal{M}$}  & \textit{Random} \\
        \midrule
        \textit{S1} & 28.7 & 14.3 & 7.7 &  15.3&7.7& 6.3 \\
        \textit{S2} & 24.3 & 17.0 & 13.3 &  20.7 & 15.7&11.3 \\
        \textit{S3} & 13.3 & 7.7 & 5.0 & 9.3 &6.7 & 4.0 \\
        \textit{S4} & 9.0 & 2.7 & 1.3 & 7.7 & 3.0 & 1.7\\
        \midrule
        \textit{Sum.} & 75.3 & 39.7 & 27.3 & 53 & 33 & 23.3 \\
        \bottomrule
    \end{tabular}}
    \vspace{-10pt}
\label{table:RQ3}
\end{table}
\subsection{RQ3: Usefulness of \feedback and \select}
We assess the usefulness of key components in the fuzzing process in \tool, namely \feedback and \select. To achieve this, we conducted a thorough evaluation by configuring a series of variants of \tool and then proceeded to evaluate their effectiveness. Currently, we verify this on the Prediction module. 


\subsubsection{\feedback} To verify the effectiveness of feedback, we compare \tool with two variants: (1) \textit{w/o F-$\mathcal{M}$}, which removes the module-directed feedback from Eq.~\ref{eq:feedback} to assess the impact of module-directed score on detecting {\mccs}s; (2) \textit{Random}, which serves as a reference by reflecting the influence of the entire feedback mechanism on detecting {\mccs}s.
As shown in Table~\ref{table:RQ3}, we find that \textit{Random} generates the fewest {\mccs}s (23.3), underscoring the importance of the designed feedback. Comparing \textit{w/o F-$\mathcal{M}$} with \tool, we observe that \tool generates more {\mccs}s than \textit{w/o F-$\mathcal{M}$} (75.3 vs. 33), highlighting the value of module-directed feedback in detecting a greater number of {\mccs}s.

\subsubsection{\select} For \select, we design three variants: (1) \textit{w/o Fine}, which removes the fine-grained mutation from the adaptive mutation to evaluate the effectiveness of this mutation level; (2) \textit{w/o Coarse}, similar to \textit{w/o Fine}, but removes the coarse-grained mutation; and (3) \textit{w/o Select}, which changes the adaptive seed selection to random seed selection to assess the usefulness of the adaptive mechanism. 
From the results shown in Table~\ref{table:RQ3}, we find that removing either the fine-grained mutation or the coarse-grained mutation reduces the number of detected {\mccs}s, with \textit{w/o Fine} detecting only 41.7 {\mccs}s and \textit{w/o Coarse} detecting 27.3 {\mccs}s. This demonstrates the effectiveness of our proposed adaptive mutation.
By comparing \textit{w/o Select} with \tool, we observe that \textit{w/o Select} detects only 53 {\mccs}s, indicating that adaptive seed selection is an effective strategy for generating {\mccs}s.



\begin{ansbox}
\textbf{Answer to RQ3:} Both \feedback and \select are useful for \tool in detecting {\mccs}s effectively.
\end{ansbox}

\subsection{RQ4: Performance of \tool}
\begin{table}[h]
    \centering
    \caption{Results of time performance in seconds (s). * denotes a negligible minimal value.}
    \vspace{-5pt}
   \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{l|cccc|c}
    \toprule
    Method & Seed Selection & Mutation & Feedback & Simulation & Total\\
    \midrule
         Random& 0.01* & 1.12 & N/A & 213.32 & 214.44 \\
         AVFuzzer& 0.01* & 1.99& 0.01* & 219.58 & 221.48\\
         BehAVExplor & 0.01* & 1.64 & 1.09 & 212.03 & 214.76\\
         \midrule
         \tool & 0.01* & 3.23 & 0.11 &172.79 & 176.13\\
         \bottomrule
    \end{tabular}
   }
   \vspace{-10pt}
    \label{tab:performace}
\end{table}

We further assess the time performance of different components in \tool, including the overhead of Seed Selection, Mutation, Feedback, and Simulation. Specifically, we analyze the average time required to process a scenario. Table~\ref{tab:performace} shows the overall results for all tools. From the results, we find that the simulation process occupies the majority of the time. For instance, on average, \tool takes 176.13 seconds to process a scenario, with 172.79 seconds (98.1\%) spent running the scenario in the simulator. All tools spend negligible time on seed selection, as it is a simple sampling operation. For mutation, \tool requires more time than others due to the additional overhead of dynamic mutation. Our feedback calculation takes an average of 0.11 seconds, which is faster than \textit{BehAVExplor}. In summary, the computation time of \tool remains within an acceptable range.

\begin{ansbox}
\textbf{Answer to RQ4:} \tool demonstrates efficiency, with the majority of the time (98.1\%) spent in the simulation phase, while the main algorithmic components consume approximately 3.34 seconds (1.9\%) of the total processing time.
\end{ansbox}


