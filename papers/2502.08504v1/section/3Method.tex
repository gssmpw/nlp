\section{Problem Definition and Motivation}

\subsection{Problem Definition}\label{def:mccs}
This paper aims to detect Module-Induced Critical Scenarios ({\mccs}s) for a specified module, defined as follows:

\begin{definition} [$\mathcal{M}$-Induced Critical Scenario] \label{def-mccs} 
Given a ADS $\mathcal{A} = \{{M}^{1}, \ldots, {M}^{K}\}$ that considers multiple modules as well as a target module $\mathcal{A}\in \mathcal{A}$ to be tested, the $\mathcal{M}$-Induced Critical Scenario $s$ satisfies the conditions: 
\begin{itemize}
    \item[a.] $s \in \mathbb{S}^{Fail}$ % scenario is failed
    \item[b.] $\exists s_i\in s. \ error(s_i, \mathcal{M})=True$
    \item[c.] $\forall s_i \in s. \ \forall M\in \mathcal{A}. \ M\neq \mathcal{M} \wedge error(s_i, M)=False$
\end{itemize}
where $\mathbb{S}^{Fail}$ denotes the set of scenarios containing ADS failures, $s_{i}$ is a scene in $s$ ,and the $error$ function determines whether the module $\mathcal{M}$ exhibits errors in a specific scene. Intuitively, if we identify a critical scenario $s$ that results in a failure, and only $\mathcal{M}$ induces errors while all other modules operate correctly across all scenes in $s$, then we can conclude that the failure is primarily caused by $\mathcal{M}$.
\end{definition}

Note that while failures that do not meet the \mccs conditions may still be useful, they do not align with our objectives as we aim to evaluate the quality of individual modules within the ADS. Specifically, we need to accurately localize the root cause in terms of specific modules. If several modules exhibit errors in a critical scenario, it becomes challenging to conclusively determine which module is the root cause. Hence, it is not a good case for developers to analyze and repair. Furthermore, based on our definition, this situation highlights our two main challenges: 1) the $error$ function, which determines whether a module functions correctly, and 2) the effective method to identify \mccs $s$ that satisfies all necessary conditions.





\subsection{Preliminary Study}\label{sec: perliminary_study}

Based on the problem definition, we would like to understand the limitations of existing methods in detecting \mccs. Specifically, both end-to-end system-level testing and module-level testing may generate failures that reveal limitations of individual modules. Therefore, we first conduct an empirical study to evaluate: 1) whether failures generated by system-level testing adequately reflect the diversity of module weaknesses, and 2) whether errors identified through module-level testing can trigger system failures.

\subsubsection{The ability of existing scenario-based testing methods to generate \mccs}\label{sec:perliminary_exist_mccs}


\begin{table}[]
    \centering
    \caption{Module Failures and Collision Distributions of Exising Methods}
    \vspace{-10pt}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{c|ccccc}
    \toprule
         Method & $\mathcal{M}^{\text{Perc}}$ICS & $\mathcal{M}^{\text{Pred}}$ICS & $\mathcal{M}^{\text{Plan}}$ICS & $\mathcal{M}^{\text{Ctrl}}$ICS & Non-\mccs\\
         \midrule
         AVFuzzer & 9  & 17  & 23   & 2 & 47\\
         BehAVExplor & 6 & 57 & 9&  3 & 190 \\
         \bottomrule
    \end{tabular}}
    \vspace{-10pt}
    \label{tab: preliminary_module}
\end{table}

Existing scenario-based methods aim to identify test scenarios that cause ADS failures efficiently but lack root cause analysis for module errors. To explore the capabilities of existing methods in generating \mccs, we conducted experiments using two scenario-based testing scenario generation methods, AVFuzzer\cite{li2020av} and BehAVExplor\cite{cheng2023behavexplor}. In these experiments, we utilized Pylot\cite{gog2021pylot} as the tested ADS and CARLA as the simulator. Starting with four basic scenarios (detailed in Section~\ref{sec: Evaluation}), each method was run for 6 hours, and we recorded the module errors and \mccs collected during this period.

The results shown in table~\ref{tab: preliminary_module} of these two existing works show high similarity. They generate many collisions, however, most are non-\mccs, in which multiple modules typically experience errors before a collision occurs. On the other hand, though some \mccs have been generated, the distribution is highly uneven, with most \mccs introduced by the prediction module, while \mccs from other modules are rare. This imbalance makes it challenging for developers to improve the corresponding modules effectively.


\subsubsection{Limitation on module-level evaluation for ADS testing}
\begin{table}[!t]
    \centering
    \caption{The ratio of module errors that can cause system failures}
    \vspace{-10pt}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{c|ccc|ccc|ccc}
    \toprule
     \multirow{2.5}*{Module}   & \multicolumn{3}{c|}{Perception} & \multicolumn{3}{c|}{Prediction} & \multicolumn{3}{c}{Planning} \\
     \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
      & 10\% & 20\% & 50\% & 0.1m & 0.5m & 1m & 0.1m & 0.2m & 0.5m\\
     \midrule
     R1 & 0.02 & 0.03 & 0.29 & 0.02 & 0.20 & 0.57 & 0.03 & 0.19 & 0.28\\
     R2 & 0.01 & 0.01 & 0.09 & 0.01 & 0.15 & 0.40 & 0.03 & 0.14 & 0.30\\
     R3 & 0.07 & 0.09 & 0.18 & 0.01 & 0.15 & 0.39 & 0.04 & 0.11 & 0.34\\
     \midrule
     Average & 0.03 & 0.06 & 0.19 & 0.01 & 0.17 &0.45 & 0.03 & 0.15 & 0.31\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
    \label{tab:preliminary_fail}
\end{table}

To better investigate the relationship between module-level errors and system-level failures in ADS, we manually introduced random noise to the output of the perception, prediction and planning module since the results in table~\ref{tab: preliminary_module} tend to be error-prone. For the perception module, we applied one of three operations—\textit{Zoom In}, \textit{Zoom Out}, and \textit{Random Offset}—randomly to each bounding box. For the prediction and planning modules, we added random perturbations to trajectory nodes. For each module, we established three levels of random error limits: conventional, moderate, and extreme. The specific settings are as follows:
\begin{inparaitem}
    \item Perception: [10\%, 20\%, 50\%];
    \item Prediction: [0.1m, 0.5m, 1m];
    \item Planning: [0.1m, 0.2m, 0.5m].
\end{inparaitem}
We randomly selected 100 normal running scenarios from \ref{sec:perliminary_exist_mccs}, with each experiment only perturbing one module's output. To mitigate the effects of randomness, each experiment was repeated three times.


Table~\ref{tab:preliminary_fail} shows the results of operations after introducing manual injections.
As the results show, aside from experiments with extreme perturbations (the third column for each module), the module errors alone does not effectively lead to system-level failures. With conventional-level perturbations to module outputs, only a few running failures occurred; even with moderate-level perturbations, the failure rate reached only up to 17\%. This suggests a significant gap between module-level errors and system failures, indicating the need for a mapping method to rapidly identify the corresponding \mccs and bridge this gap effectively.

\begin{ansbox}
   \textbf{Motivation:} Existing system-level and module-level testing methods failed to generate {\mccs}s. This limitation motivates us to develop an effective approach to detecting system failures induced by specified modules. 
\end{ansbox}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/rc_overview.png}
    \caption{Overview of \tool}
    \label{fig:overview}
\end{figure*}


\section{Approach}\label{sec:method}

\subsection{Overview}
Fig.~\ref{fig:overview} provides a high-level overview of \tool for generating {\mccs}s given initial seeds and the user-specified module $\mathcal{M}$. 
\tool comprises three main components: \oracle, \feedback and \select. 
\oracle functions as an oracle to check whether a scenario satisfies \textit{Definition}~\ref{def-mccs} and qualifies as an \mccs. 
\feedback provides feedback that guides the search process, which jointly considers the system-level specifications (i.e., safety) and the module-specific aspects (i.e., the extent of errors in $\mathcal{M}$).
% ensuring the effectiveness of the search for {\mccs}s.
\select implements an adaptive strategy, including seed selection and mutation, to generate new scenarios based on the module-specific feedback score, thereby improving search performance. 
Specifically, following a classical search-based fuzzing approach, \tool maintains a seed corpus with valuable seeds that facilitate the identification of {\mccs}s. In each iteration, \select first chooses a seed with a higher feedback score and applies an adaptive mutation to the selected seed to generate a new test scenario. Note that a higher feedback score indicates a greater likelihood of evolving into an \mccs. 
Then, \oracle and \feedback provide the identification result and feedback score for the new seed by analyzing the scenario observation, respectively.


Algorithm~\ref{algo:workflow} presents the main algorithm of \tool. The algorithm takes as input an initial seed corpus \( \mathbf{Q} \), a module-based ADS \( \mathcal{A} = \{{M}^{1}, \dots, {M}^{K}\} \), which consists of \( K \) modules, and the user-specified module $\mathcal{M}$ under test.
The output is a set of module $\mathcal{M}$ caused critical scenarios $s$ (Line 13).
In detail, \tool begins by initializing an empty set for $\mathbf{F}_{\mathcal{M}}$ (Line 1). 
Then \tool starts the fuzzing process, which continues until the given budget expires (lines 2-12). 
In each iteration, \tool first uses \select to return a new scenario \( s' \) and the feedback score \(\phi_s\) of its source seed \( s \) (Line 3). 
This new scenario \( s' \) is executed in the simulator with the ADS under test \( \mathcal{A} \), collecting the scenario observation \( \mathcal{O}(s') = \{\mathcal{A}(s'), \mathcal{Y}(s')\} \) including the ADS observation \( \mathcal{O}_{\mathcal{A}}(s') \) and the Simulator observation \( \mathcal{Y}_{\mathcal{A}}(s') \) (Line 4).
Based on these observations, \oracle identifies if the scenario \( s' \) contains system failures and if module \( \mathcal{M}^k \) is the root cause, returning the identification result \( r_{\mathcal{M}} \), module errors $\delta^{\mathcal{A}}$ and safety-critical distance $\delta^{\mathcal{A}}$ (Line 5). 
If \( r_{\mathcal{M}} \) is identified as \textit{Fail}, \tool keeps the scenario \( s' \) in the critical scenario set \(\mathbf{F}_{\mathcal{M}}\) (Line 6-7). 
Otherwise, \feedback calculates a feedback score for the benign scenario \( s' \) based on the module errors \( \delta^{\mathcal{A}} \) and the safety-critical distance \( \delta^{\text{safe}} \) (Lines 8-9).
A higher feedback score \( \phi_{s'} \) indicates a higher potential of \( s' \) for generating {\mccs}s. If the feedback score of seed \( s' \) is higher than that of its parent seed \( s \), \tool retains \( s' \) in the corpus $\mathbf{Q}$ for further optimization (Line 10-11). 
Finally, the algorithm ends by returning $\mathbf{F}_{\mathcal{M}}$ (Line 13).

\begin{algorithm}[!t]
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Para}{Parameters}
\SetKwProg{Fn}{Function}{:}{}
\SetKwFunction{AE}{\textbf{AdaptiveScenariGeneration}}
\SetKwFunction{OI}{\textbf{ModuleSpecificOracle}}
\SetKwFunction{FD}{\textbf{ModuleSpecificFeedback}}
\SetKwComment{Comment}{\color{blue}// }{}
\Input{
Initial seed corpus $\mathbf{Q}$ \\
ADS under test $\mathcal{A} = \{{M}^{1}, ..., {M}^{K}\}$\\
Specified module $\mathcal{M} \in \mathcal{A}$
}
\Output{
$\mathcal{M}$ module-induced critical scenarios $\mathbf{F}_{\mathcal{M}}$
}
$\mathbf{F}_{\mathcal{M}} \gets \{\}$ \\
\Repeat{given time budget expires}{
$s', {\phi}_{s} \gets \AE(\mathbf{Q})$ \Comment{Generate new scenarios}
$\mathcal{O}(s') = \{\mathcal{A}(s'), \mathcal{Y}(s')\} \gets \textbf{Simulator}(s', \mathcal{A})$ \\
$r_{\mathcal{M}}, \delta^{\mathcal{A}}, \delta^{\text{safe}} \gets \OI(\mathcal{A}(s'), \mathcal{Y}(s'), \mathcal{M}^k)$ \Comment{Analyze module errors}
\eIf{$r_{\mathcal{M}}$ is \textit{Fail}}{
    \Comment{Update failure sets}
    $\mathbf{F}_{\mathcal{M}} \gets \mathbf{F}_{\mathcal{M}} \cup \{s'\}$ \Comment{Update discovered {\mccs}s}
}{
    \Comment{Update seed corpus}
    ${\phi}_{s'} \gets \FD(\delta^{\text{safe}}, \delta^{\mathcal{A}})$ \Comment{Calculate feedback score}
    \If{${\phi}_{s'} > {\phi}_{s}$}{
        $\mathbf{Q} \gets \mathbf{Q} \cup \{s'\}$ \Comment{Update corpus for {\mccs}s search}
    }
}
}
\Return $\mathbf{F}_{\mathcal{M}}$
\caption{Workflow of \tool}
\label{algo:workflow}
\end{algorithm}

\subsection{Module-Specific Oracle}
The purpose of \oracle is to serve as an oracle for automatically detecting {\mccs}s by determining whether a given scenario satisfies all conditions outlined in \textit{Definition~\ref{def:mccs}}. This involves two parts: (1) detecting system failures in the scenario (\textit{Definition~\ref{def-mccs}.a}) and (2) determining errors for each module in the ADS (\textit{Definition~\ref{def-mccs}.b} and \textit{Definition~\ref{def-mccs}.c}).

For part (1), we consider the occurrence of collisions as a safety-critical metric to identify system failures.
For part (2), the main challenge is obtaining ground truth to evaluate the performance of individual modules without human annotation. To address this, we design \textit{Individual Module Metrics} to independently measure errors for each module using collected scenario observations, covering the four main modules in the ADS: perception, prediction, planning, and control.

\subsubsection{Safety-critical Metric}\label{sec:safe-metric} We check if the scenario contains ADS failures by detecting collisions. Specifically, we first calculate the minimum distance between the ego vehicle and other objects:
\begin{equation}\label{eq:safe}
    \delta_{s}^{\text{safe}} = \min \left\{ \| R_{\text{bbox}}({p}^{0}_{t}) - R_{\text{bbox}}({p}^{n}_{t}) \|_2 \ \big| \ t \in [0, T], \ n \neq 0 \right\}
\end{equation}
where \( {p}^{0}_{t} \) represents the position of the ego vehicle at time \( t \), \( R_{\text{bbox}}(\cdot) \) calculates the bounding box for an object based on its position \( p \), and \( {p}^{n}_{t} \) represents the position of the \( n \)-th object at the same time. 
Therefore, safety-critical failures can be detected if \( \delta_{s}^{\text{safe}} = 0 \).


\subsubsection{Individual Module Oracles}\label{sec:module-metric}
Given a scenario \( s \) with its observation \(\mathcal{O}(s) = \{\mathcal{A}(s), \mathcal{Y}(s)\}\), we first design individual metrics for each module to measure module-level errors \(\delta^{{M}} = \{\delta^{{M}}_t \ \big| \ t \in [0, \dots, T] \}\) for each module \({M} \in \mathcal{A} \), where \(\delta^{{M}}_t\) denotes the module error at timestamp \( t \) and \( T \) is the termination timestamp of the scenario \( s \).
We detail the calculation of \(\delta^{{M}}_t\) covering the \textit{Perception}, \textit{Prediction}, \textit{Planning}, and \textit{Control} modules as follows:

\noindent \textit{(1) Perception Module.} Given a scenario $s$, the error in the \textit{Perception} module can be directly measured by comparing object bounding boxes between Simulator observation $\mathcal{Y}(s)$ and ADS observations $\mathcal{Y}(s)$.
We adopt a weighted Intersection over Union (IoU)~\cite{girshick2014rich} to measure the errors in the perception module $\mathcal{M}^{\text{perc}}$, which is a widely recognized metric in object detection.
Specifically, the errors of the perception module at timestamp $t$ is calculated as follows:
\begin{equation}
    {\delta^{\text{perc}}_{t}} = 1 - \frac{1}{N_{t}} \sum_{n=1}^{N_{t}} (\frac{D-d_t^{n}}{D} \cdot \frac{|B_t^{n} \cap b_t^n|}{|B_t^{n} \cup b_t^{n}|})
\end{equation}
where \( N_{t} \) represents the number of detected objects within the perception range \( D \) meters, \( B_{t}^{n} \) and \( b_{t}^{n} \) denote the detected bounding box and the ground truth bounding box of the \( n \)-th object, respectively, obtained from the ADS observation $\mathcal{A}_{t} \in \mathcal{A}(s)$ and the scenario observation $\mathcal{Y}_{t} \in \mathcal{Y}(s)$. 
The weight \( \frac{D-d_t^n}{D} \) assigns a higher weight to objects closer to the ego vehicle, where \( d_t^n \) denotes the distance between the \( n \)-th object and the ego vehicle at timestamp \( t \).
A higher value of \(\delta^{\text{perc}}\) represents a greater detection error in the perception module, indicating a potential safety-critical situation.


\noindent \textit{(2) Prediction Module.} Unlike the \textit{Perception} module, directly comparing the predicted trajectories in ADS observation \(\mathcal{A}(s)\) with the collected trajectories in Simulator observation \(\mathcal{Y}(s)\) cannot accurately reflect errors in the \textit{Perception} module. This is because the inputs to the \textit{Prediction} module are derived from the \textit{Perception} module, which may introduce perception errors that subsequently affect prediction outcomes. 
To address this, we adopt a perception-biased trajectory to measure the errors in the \textit{Prediction} module. 
Specifically, at timestamp \( t \), the perception-biased trajectory for the $n$-th object within perception range $D$ is determined by incorporating the biases present in the perception module, formulated as:
\begin{equation}
    \overline{\tau}_t^n = \left\{\ p_{t+k}^n + \Delta p_t^n \mid k \in [0, \ldots, H_{\text{pred}}] \right\}
\end{equation}
where \( p_{t+k}^n \) represents the ground-truth position in the Simulator observation \(\mathcal{Y}_t\), and \( \Delta p_{t+k}^n \) is the position shift observed in the \textit{Perception} module, and $H_\text{pred}$ is the prediction horizon. Note that the position shift \(\Delta p_t^n = (\Delta x_t^n, \Delta y_t^n)\) represents the positional difference between the detected output and the ground truth at timestamp $t$. Since the following detect output after timestamp $t$ is unavailable, and the offset does not fluctuate significantly within a shorter prediction window(about 0.5 to 1 seconds) in most cases, we apply $\Delta p_t^n$ to its predicted sequence.

Consequently, we can measure the error of the \textit{Prediction} module by comparing the predicted trajectories with the perception-biased trajectories. This comparison is quantified by:
\begin{equation}
    \delta_{t}^{\text{pred}} = \max \left\{  \frac{D-d_t^{n}}{D} \cdot \|\hat{p}^{n}_{t+k} - \overline{p}^{n}_{t+k} \|_{2} \ \big| \ \hat{p}^{n}_{t+k} \in \tau_{t}^{n}, \ \overline{p}^{n}_{t+k} \in \overline{\tau}_{t}^{n}, \ n \in [1, \dots, N_{t}] \right\}
\end{equation}
where \( N_{t} \) is the number of detected objects within the perception range \( D \), \( \tau_{t}^{n} \) is the predicted trajectory for the \( n \)-th object, and \( d_t^n \) denotes the distance between the \( n \)-th object and the ego vehicle at timestamp \( t \). The calculation of \(\delta_{t}^{\text{pred}}\) selects the maximum error in all predicted trajectories because this emphasizes the worst-case performance within a given scenario, which is crucial for assessing the robustness and safety of the \textit{Prediction} module. 

\noindent \textit{(3) Planning Module.} We measure errors in the \textit{Planning} module from a safety perspective by evaluating the distance to collisions.
Similar to the prediction module, biases present in upstream modules (i.e., \textit{Perception} and \textit{Prediction}) affect the evaluation of the planning module when directly using ground truth data collected from the Simulator observation \(\mathcal{Y}(s)\). 
To mitigate these biases, we measure errors in the \textit{Planning} module by assessing if the planned trajectories collide with objects detected by the upstream modules. This is calculated by:
\begin{equation}\label{eq:plan}
    \delta^{\text{plan}}_{t} = \sum_{n=1}^{N_{t}} \sum_{k=1}^{H_{\text{plan}}} \mathbb{I}(\| R_{\text{bbox}}(p^{\mathcal{A}}_{t+k}) - R_{\text{bbox}}(p_{t+k}^{n}) \|_{2}=0)
\end{equation}
where \( N_{t} \) represents the number of detected objects, \( H_{\text{plan}} \) is the planning horizon, \( R_{\text{bbox}}(\cdot) \) calculates the bounding box for an object based on its position \( p \), \( \mathbb{I}(\cdot) \) is an indicator function. The indicator function \( \mathbb{I}(condition) \) returns 1 if the \( condition \) is true and 0 otherwise. Additionally, \( p^{\mathcal{A}}_{t+k} \in \mathcal{P}_{t} \) denotes a trajectory point planned by the \textit{Planning} module, and \( p_{t+k}^{n} \in \tau_{t}^{n} \) is the predicted trajectory point for the \( n \)-th object at timestamp \( t \). Ideally, the planned trajectory should be collision-free, maintaining a safety distance from all predicted states of all objects (i.e., \(\delta^{\text{plan}}_{t} = 0\)). Therefore, a larger \(\delta^{\text{plan}}_{t}\) indicates that the planning module has safety-critical errors, such as collisions.


\noindent \textit{(4) Control Module.} 
The control command is directly applied to the vehicle to manage its movement by following a trajectory from the upstream \textit{Planning} module. Obtaining a ground truth for control commands is challenging. Thus, we evaluate the \textit{Control} module by comparing the actual movement of the vehicle with the planned movement from the \textit{Planning} module.
At timestamp \( t \), we calculate the error for the \textit{Control} module by:
\begin{equation}
    \delta^{\text{ctrl}}_{t} = \| p_{t+1}^{\mathcal{A}} - p_{t+1}^{0} \|_{2} + \| v_{t+1}^{\mathcal{A}} - v_{t+1}^{0} \|_{2}
\end{equation}
where \( p_{t+1}^{\mathcal{A}} \) and \( v_{t+1}^{\mathcal{A}} \) represent the planned position and velocity from the \textit{Planning} module at timestamp \( t \), and \( p_{t+1}^{0} \) and \( v_{t+1}^{0} \) are the actual position and velocity of the vehicle at timestamp \( t+1 \). 
This error \(\delta^{\text{ctrl}}_{t}\) quantifies how well the \textit{Control} module is executing the planned trajectory. A larger error value indicates a significant deviation from the planned path and speed, suggesting potential issues in the \textit{Control} module (i.e., inaccuracies in executing the planned trajectory).


\begin{algorithm}[!t]
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Para}{Parameters}
\SetKwProg{Fn}{Function}{:}{}
\SetKwComment{Comment}{\color{blue}// }{}
\Input{
Scenario observation $\mathcal{O}(s) = \{{\mathcal{A}}(s), \mathcal{Y}(s)\}$ \\
ADS $\mathcal{A} = \{{M}^{1}, \dots, {M}^{K} \}$ \\
User-specified module $\mathcal{M}$
}
\Output{
\mccs identification result $r_{\mathcal{M}}$ \\
Module errors $\delta^{\mathcal{A}} = \{\delta^{{M}^{{1}}}, \dots, \delta^{{M}^{{K}}}\}$ \\
Safety-critical distance $\delta^{\text{safe}}$
}

 $\delta^{\mathcal{A}} \gets \{\}$, $r_{\mathcal{M}} \gets Pass$ \\
% $\delta^{\mathcal{A}} \gets \{\}, r_b \gets 0, r_c \gets 0$ \\
$\delta^{\text{safe}} \gets \text{calculate safety-critical distance by Eq.~\ref{eq:safe}}(\mathcal{Y}(s))$ \Comment{Safety-critical metric} 
\If{$\delta^{\text{safe}} \neq 0$}{
    $r_{\mathcal{M}} \gets Fail$ \Comment{Fail to satisfy Definition~\ref{def:mccs}.a}
}
\For{${M} \ in \ \mathcal{A}$}{
\Comment{Module error calculated by Individual Module Metrics}
    $\delta^{{M}} \gets \text{calculate the module-level error according to Section~\ref{sec:module-metric}}$ \\
    $\hat{\delta}^{{M}} \gets \text{filter module errors by Eq.~\ref{eq:system_error}}$ \\
    % $f_{\mathcal{M}^{i}}(\delta^{\mathcal{M}^{i}}) \gets \text{calculate system-level affects by Eq.~\ref{eq:system_error}}$ \\
    $\delta^{\mathcal{A}} \gets \delta^{\mathcal{A}} \cup \{\delta^{{M}}\}$ \\
    % \eIf{$\mathcal{M}^{i} = \mathcal{M}^{k}$}{
    %     $r_b \gets f_{\mathcal{M}^{i}}(\delta^{\mathcal{M}^{i}})$
    % }{
    %     $r_c \gets r_c + f_{\mathcal{M}^{i}}(\delta^{\mathcal{M}^{i}})$
    % }
    \Comment{Definition~\ref{def:mccs}.b}
    \If{${M} = \mathcal{M} \ and \ \hat{\delta}^{\mathcal{M}} = 0 $}{
        $r_{\mathcal{M}} \gets Fail$ 
    }

    \Comment{Definition~\ref{def:mccs}.c}
    \If{${M} \neq \mathcal{M} \ and \ \hat{\delta}^{{M}} \neq 0 $}{
        $r_{\mathcal{M}} \gets Fail$ 
    }
    
}
\Return $r_{\mathcal{M}}, \delta^{\mathcal{A}}$, $\delta^{\text{safe}}$
\caption{Algorithm for \oracle}
\label{algo:oracle}
\end{algorithm}
\subsubsection{Module-Specific Filter}\label{sec:filter}
The Module-Specific Filter aims to filter out less relevant module errors, as driving scenes farther from the termination have less impact on the final results~\cite{stocco2020misbehaviour, stocco2022thirdeye}.
The filter considers only module errors within a detection window \([T - \Delta t, T]\), where \( T \) is the timestamp of the occurrence of system failures in the scenario, and \( \Delta t \) is the detection window size. Therefore, the filtered module errors are calculated by:
\begin{equation}\label{eq:system_error}
    \hat{\delta}^{{M}} = \sum_{t=T-\Delta t}^{T} \mathbb{I}(\delta_t^{{M}} > \lambda^{{M}})
\end{equation}
where \( \lambda^{{M}} \) is the tolerance threshold for module ${M}$, and \( \mathbb{I} \) is an indicator function. The indicator function \( \mathbb{I}(condition) \) returns 1 if the \( condition \) is true and 0 otherwise. 

\subsubsection{Workflow of \oracle}
Algorithm~\ref{algo:oracle} illustrates the workflow of \oracle.
Specifically, the algorithm takes as inputs the scenario observation \( \mathcal{O}(s) \), the ADS \( \mathcal{A} \), and the user-specified module \( \mathcal{M} \), and it outputs three key results: the identification result \( r_{\mathcal{M}} \), the module errors's set \( \delta^{\mathcal{A}} \), the safety-critical distance \( \delta^{\text{safe}} \). 
Initially, \oracle creates an empty error set \( \delta^{\mathcal{A}} \) to store module errors and an identification flag \( r_{\mathcal{M}} \) set to `pass' (Line 1). Then, the algorithm calculates the safety-critical distance using Eq.~\ref{eq:safe} and checks for system failures (Lines 2-4), aiming to confirm the satisfaction of \textit{Definition~\ref{def-mccs}.a}.
Subsequently, the algorithm calculates and filters module errors for each module in the ADS \( \mathcal{A} \), storing these errors in \( \delta^{\mathcal{A}} \) for further analysis and feedback (Lines 5-8).
Once the user-specified module does not trigger errors (Line 9-10) or other modules do trigger errors (Line 11-12), the identification flag is set to `Fail' as they violate the requirements of \textit{Definition~\ref{def-mccs}.b} and \textit{Definition~\ref{def-mccs}.c}. 
Finally, \oracle returns the identification flag \( r_{\mathcal{M}} \), the module error set \( \delta^{\mathcal{A}} \), and the safety-critical distance \( \delta^{\text{safe}} \) (Line 13).





\subsection{Module-Specific Feedback} % Feedback

To provide guidance for searching {\mccs}s, we design a \feedback providing a feedback score, including two parts: (1) \textit{safety-critical score} and (2) \textit{module-directed score}. The \textit{safety-critical score} aims to guide the search for safety-critical scenarios that include system-level violations (i.e., collisions). To focus more specifically on the user-specified module, we introduce the \textit{module-directed score}, which provides guidance to bias the generation of safety-critical scenarios towards this module.


\subsubsection{Safety-critical Score}
We directly leverage the safety-critical distance from Section~\ref{sec:safe-metric} as our safety-critical feedback score, denoted by \( \phi_{s}^{\text{safe}} = \delta^{\text{safe}}_{s} \).
The safety-critical score \(\phi_{s}^{\text{safe}}\) quantifies the minimum distance between the ego vehicle and other objects over the time horizon \([0, T]\), capturing how close the vehicle comes to a collision scenario. A lower value of \(\phi_{s}^{\text{safe}}\) indicates a more dangerous situation for the ego vehicle.


\subsubsection{Module-directed Score} 
Given a user-specified module \( \mathcal{M} \), we calculate the \textit{module-directed score} as follows:
\begin{equation}
    \phi_{s}^{\mathcal{M}} = \sum_{t=T-\Delta t}^{T}\left( \delta_t^{\mathcal{M}} - \frac{\sum_{M \neq \mathcal{M}} \delta_t^{{M}}}{K - 1} \right)
\end{equation}
where \( K \) is the total number of modules in the ADS \(\mathcal{A}\), \( T \) is the termination timestamp of scenario \( s \), \( \Delta t \) is the detection window, and \( \delta_t^{{M}} \) represents the module-level errors for each module obtained from \oracle, as detailed in Section~\ref{sec:module-metric}. 
The first term, \( \delta_t^{\mathcal{M}} \), quantifies the errors specific to the user-specified module \( \mathcal{M} \). 
The second term, \( \frac{\sum_{M \neq \mathcal{M}} \delta_t^{{M}}}{K - 1} \), represents the average of the cumulative errors across all other modules. 
The score \( \phi_{s}^{\mathcal{M}} \) promotes the search for scenarios in which only the user-specified module \( \mathcal{M} \) exhibits errors during the detection window, while other modules do not. Therefore, this score can provide guidance to enhance the impact of the user-specified module \( \mathcal{M} \) on detected violations.


The final feedback score combines the \textit{safety-critical score} and the \textit{module-directed score} by:
\begin{equation}\label{eq:feedback}
    \phi_{s} =  \phi_{s}^{\mathcal{M}^k} - \phi_{s}^{\text{safe}}
\end{equation}
A larger \(\phi_{s}\) indicates that the scenario \( s \) is closer to becoming a {\mccs}. Consequently, \tool aims to generate {\mccs}s by maximizing this feedback score.



\begin{algorithm}[!t]
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Para}{Parameters}
\SetKwProg{Fn}{Function}{:}{}
\SetKwFunction{AE}{\textbf{AdaptiveEvolver}}
\SetKwFunction{OI}{\textbf{ModuleCauseIdentifier}}
\SetKwComment{Comment}{\color{blue}// }{}
\Input{
Selected seed $s$ with feedback score ${\phi}_{s}$ \\
Maximum feedback score ${\phi}_{max}$ and Minimum feedback score ${\phi}_{min}$ in the seed corpus
}
\Output{
Mutated seed $s'$
}
$s' \gets s$ \Comment{Copy the selected scenario seed $s$}
$\lambda_{m} \gets \frac{{\phi}_{max} - {\phi}_{s}}{|{\phi}_{max} - {\phi}_{min}|}$ \Comment{Assign a dynamic threshold for determining mutation strategy}

\eIf{random() > $\lambda_{m}$}{
    \Comment{Fine-grained mutation: add small perturbation}
    $\mathbb{E}_{s'} \gets \mathbb{E}_{s'} + \text{GaussSample}(\lambda_m)$ \\
    \For{$P \in \mathbb{P}_{s'}$}{
    $W_{P}^{v} \gets W_{P}^{v} + \text{GaussSample}(\lambda_m)$ 
    }
}{
    \Comment{Coarse-grained mutation: regenerate new parameters}
    $\mathbb{E}_{s'} \gets \text{UniformSample}([\mathbb{E}_{min}, \mathbb{E}_{max}])$ \\
    \For{$P \in \mathbb{P}_{s'}$}{
    $W_{P}^{l} \gets \text{RouteGenerate}(\lambda_m)$ \\
    $W_{P}^{v} \gets \text{UniformSample}([V_{min}, V_{max}],\lambda_m)$
    }
}
\Return $s'$
\caption{Algorithm for \textit{Adaptive Mutation}}
\label{algo:mutation}
\end{algorithm}
\subsection{Adaptive Scenario Generation} % Change to generator
To improve the searching performance of \tool, we design an \select mechanism including \textit{Adaptive Seed Selection} and \textit{Adaptive Mutation}, which adaptively generate new seed scenarios based on the feedback score obtained from \feedback.


\subsubsection{Adaptive Seed Selection} The seed selection process aims to choose a seed scenario from the seed corpus for further mutation, thereby generating a new scenario.
We design this selection process to favor seeds with higher feedback scores, indicating they are more likely to evolve into a {\mccs}.
Therefore, we assign a selection probability to each seed in the corpus based on the feedback score calculated by \feedback. The selection probability for each seed \( s \) is defined as:
\begin{equation}
    p(s) = \frac{\phi_{s} - \phi_{\min} + \epsilon}{\sum_{s' \in \mathbf{Q}} (\phi_{s'} - \phi_{\min} + \epsilon)}
\end{equation}
where \( \phi_{\min} \) is the global minimum feedback score in the corpus, \( \phi_{s} \) is the feedback score for seed \( s \), \( \epsilon \) is a small positive constant to ensure that the seed with the global minimum feedback score has a non-zero selection probability, and \( \mathbf{Q} \) denotes the set of all seeds in the corpus. This probability formulation ensures that seeds with higher feedback scores have a higher chance of being selected for mutation, thereby promoting the generation of scenarios with a higher likelihood of evolving into a {\mccs}.


\subsubsection{Adaptive Mutation} Beyond seed selection, we also design an adaptive mutation strategy that applies different mutation methods based on the feedback score of each seed. 
Algorithm~\ref{algo:mutation} presents the details of \textit{Adaptive Mutation}. 
Specifically, given a scenario \( s \), the \textit{Adaptive Mutator} first copies the source seed \( s \) to \( s' \) (Line 1) and calculates a dynamic threshold \( \lambda_m = \frac{\phi_{\text{max}} - \phi_s}{|\phi_{\text{max}} - \phi_{\text{min}}|} \in [0, 1] \) by normalizing the feedback score \( \phi_s \), ensuring \( \lambda_m \) (Line 2).
Then, the mutation selects either \textit{fine-grained mutation} or \textit{coarse-grained mutation} based on the derived dynamic threshold $\lambda_m$ (Line 3-11). Seeds with higher feedback scores (resulting in smaller dynamic thresholds) are regarded as closer to {\mccs}s; therefore, we employ \textit{fine-grained mutation} to add Gaussian noise to the weather parameters (Line 4) and the speeds of each object (Lines 5-6). Otherwise, for seeds with lower feedback scores (resulting in larger dynamic thresholds), we utilize \textit{coarse-grained mutation} to introduce more significant variations. These include changing the trajectory waypoints of each object and altering environmental conditions through uniform sampling (Lines 7-11).
Finally, a mutated scenario \( s' \) is produced (Line 12).

