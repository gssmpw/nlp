\section{Background}\label{sec: Background}

\subsection{Autonomous Driving System}\label{sec: Background-ads}
Autonomous Driving Systems (ADSs) rely on advanced artificial intelligence algorithms as the brain of autonomous vehicles, governing their movements. ADSs can be broadly categorized into two types: End-to-End (E2E) ADSs and Module-based ADSs.
E2E ADSs, such as Openpilot \cite{openpilot}, approach the system as a single deep learning model that processes sensor data (e.g., images) as input and directly outputs control commands to operate the vehicle. The development of E2E ADSs requires vast amounts of data to achieve satisfactory performance levels, which limits their generalization capabilities and industrial-level reliability.
In contrast, module-based ADSs, such as Pylot \cite{gog2021pylot}, Apollo \cite{baiduapollo}, and Autoware \cite{Autoware}, demonstrate more reliable performance in industrial applications by incorporating multiple modules dedicated to specific tasks. Therefore, in this paper, we focus on testing module-based ADSs to highlight critical issues arising from a specified module. 
Formally, we denote the module-based ADS under test as $\mathcal{A} = \{{M}^{1}, \ldots, {M}^{K}\}$, where ${M}^{{i}}$ represents an individual module within the ADS, and $K$ is the total number of modules in the system.
Typically, with perfect sensor data (e.g., GPS signals) and map data, module-based ADSs consist of four primary functional modules: \textit{Perception}, \textit{Prediction}, \textit{Planning}, and \textit{Control}.

\textit{Perception.} 
The Perception module aims to detect objects (e.g., vehicles, pedestrians) surrounding the ego vehicle by processing sensor data (e.g., images, LiDAR points). Formally, at timestamp $t$, given sensor data $\mathcal{I}_{t}$, the perception module outputs a sequence of detected objects in the format of bounding boxes, represented as $\mathcal{B}_{t} = \{B_{t}^{1}, \ldots, B_{t}^{N_{t}}\}$, where $B_{t}^{i}$ represents the predicted bounding box of the $i$-th object and $N_{t}$ is the number of detected objects.

\textit{Prediction.} The Prediction module aims to anticipate the future movements of detected objects. Using the outputs from the \textit{Perception} module, it predicts the trajectories of these objects over a future time horizon $\Delta t$. Formally, at timestamp $t$, given the detected objects $\mathcal{B}_{t}$, the prediction module outputs a sequence of future trajectories for each object, represented as $\mathcal{T}_{t}  = \{\tau_{t}^{1}, \ldots, \tau_{t}^{|\mathcal{B}_{t}|}\}$, where $\tau_{t}^{i}$ is the predicted trajectory of the $i$-th object, and $|\mathcal{B}_{t}|$ is the number of detected objects. Each trajectory $\tau_{t}^{i} = \{p_{t+k}^{i} \mid k = 1, \ldots, H_{\text{pred}}\}$ includes a series of future waypoints over a specified prediction horizon $H_{\text{pred}}$, indicating the number of future timesteps for which predictions are made.


\textit{Planning.} The \textit{Planning} module is responsible for determining a safe and efficient path for the ego vehicle, considering the current road conditions and the predicted movements of surrounding objects. Based on the outputs from both the \textit{Perception} and \textit{Prediction} modules, it computes a trajectory for the ego vehicle that adheres to safety constraints and follows driving goals, such as reaching a destination or maintaining lane position. Formally, at timestamp $t$, given the detected objects $\mathcal{B}_{t}$ and predicted trajectories $\mathcal{T}_{t}$, the planning module generates a planned path $\mathcal{P}_{t}$ for the ego vehicle. Here, $\mathcal{P}_{t} = \{p_{t+k}^{\mathcal{A}} \mid k = 1, \ldots, H_{\text{plan}}\}$ is a sequence of waypoints that the ego vehicle should follow over a specified planning horizon $H_{\text{plan}}$.

\textit{Control.} The \textit{Control} module translates the planned path from the \textit{Planning} module into actionable control commands that adjust the ego vehicleâ€™s steering and acceleration to follow the planned trajectory. This module works at a high frequency to ensure real-time responsiveness and vehicle stability. At timestamp $t$, given the planned path $\mathcal{P}_{t}$, the control module generates a control command, represented as $\mathcal{C}_{t} = \{C_{t}^{\text{steer}}, C_{t}^{\text{throttle}}, C_{t}^{\text{brake}}\}$, where $C_{t}^{\text{steer}}$, $C_{t}^{\text{throttle}}$, and $C_{t}^{\text{brake}}$ represent the steering, throttle, and braking commands to be executed by the vehicle at timestamp $t$, respectively.


\subsection{Scenario}
\subsubsection{Scenario Definition} In ADS testing, scenarios represent driving environments structured as sequences of scenes, each capturing a snapshot of scenery and dynamic objects. Scenarios consist of configurable static attributes (e.g., map, weather) and dynamic attributes (e.g., behaviors of NPC vehicles), derived from Operational Design Domains (ODDs)~\cite{thorn2018framework}.
Given the vast attribute space, it is impractical to test all attribute combinations. Following previous studies~\cite{cheng2023behavexplor, li2020av}, we focus on subsets aligned with specific testing goals. In this study, our objective was to test the safety of a module-based ADS, including perception, prediction, planning, and control modules, by configuring the trajectories and weather parameters of NPC vehicles.
Formally, a \textit{scenario} can be defined as a tuple $s = \{\mathbb{A}, \mathbb{P}, \mathbb{E}\}$, where $\mathbb{A}$ represents the ADS motion task, including the start position and destination; $\mathbb{P}$ is a finite set of objects, encompassing static obstacles, dynamic NPC vehicles, and pedestrians; and $\mathbb{E}$ is the set of weather parameters, such as cloudiness. For each object $P \in \mathbb{P}$, we represent its behavior with a sequence of waypoints, denoted by $W^{P} = \{w_{1}, \ldots, w_{n}\}$. Each waypoint $w_{i}$ specifies the location $(x_{i}, y_{i})$ and velocity $v_{i}$. 

\subsubsection{Scenario Observation} A scenario observation is a sequence of scenes recorded during the execution of the scenario, where each scene represents the states of the ego vehicle and other participants at a specific timestamp. Formally, given a scenario $s$, its observation is denoted as $\mathcal{O}(s) = \{o_0, o_1, \ldots, o_{T}\}$, where $T$ is the length of the observation, and $o_t = \{\mathcal{A}_{t}, \mathcal{Y}_t\}$ is the scene at timestamp $t$, including the observation $\mathcal{A}_{t}$ extracted from the ADS and $\mathcal{Y}_t$ from the simulator. In detail, $\mathcal{Y}_t = \{ y_t^\mathcal{A}, y_t^{P_{1}}, \ldots, y_t^{P_{|\mathbb{P}|}}\}$ contains the states of all objects, including the ego vehicle ($y_t^\mathcal{A}$), where $y_t^P = \{p_{t}^{P}, b_{t}^{P}, \theta_{t}^{P}, v_{t}^{P}, a_{t}^{P}\}$ denotes the waypoint of participant $P \in \mathbb{P}$ at timestamp $t$. Each waypoint includes the center position $p_{t}^{P}$, bounding box $b_{t}^{P}$, heading $\theta_{t}^{P}$, velocity $v_{t}^{P}$, and acceleration $a_{t}^{P}$.
Note that the ADS observation $\mathcal{A}_{t}$ contains the outputs of all ADS modules at timestamp $t$. 


In the following content, to better distinguish between observations from the ADS and the simulator, we use $\mathcal{A}(s)$ and $\mathcal{Y}(s)$ to denote ADS observation and Simulator observation, respectively. 
