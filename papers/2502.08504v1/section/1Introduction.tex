\section{Introduction}\label{sec: Introduction}


Autonomous Driving Systems (ADSs) have rapidly advanced in recent years, aiming to enhance transportation and urban mobility. ADSs act as the brain of autonomous vehicles (AVs), processing extensive sensor data (e.g., images, LiDAR points) through various modules (e.g., perception, prediction, planning, and control) to operate vehicles. ADSs are considered highly safety-critical systems, as they have demonstrated vulnerability to critical issues arising from minor perturbations in the driving environment. For example, Tesla’s Autopilot system has been involved in multiple accidents where it failed to recognize stationary emergency vehicles, leading to crashes and prompting federal investigations~\cite{electrek2023tesla}.
Therefore, ADSs require extensive testing to ensure their safety and reliability during real-world deployment.  

ADS testing can be broadly divided into \textit{real-world testing} and \textit{simulation-based testing}. Real-world testing is necessary, but it is impractical and costly due to the vast number of miles required and the rarity of critical events. In contrast, simulation-based testing offers a controllable, efficient, and cost-effective environment for both developing and testing autonomous vehicles (AVs), allowing for the creation and testing of diverse scenarios that would be difficult to reproduce in real-world conditions. In recent years, significant efforts have been made to advance simulation-based testing for ADSs to identify potential system failures such as collision and timeout. 
Specifically, existing studies have primarily focused on reconstructing scenarios from real-world data~\cite{van2015automated,fang2020augmented,kruber2018unsupervised} or developing search algorithms~\cite{rana2021building,cheng2023behavexplor,zhang2022adversarial} to generate safety-critical scenarios. 


An ADS typically comprises multiple modules, including perception, prediction, planning, and control, each tasked with specific functions. Recent research~\cite{lou2022testing} underscores the importance of understanding the root causes of detected failures, which poses challenges in effectively detecting scenarios that reflect diverse root causes. While current methods can identify numerous safety-critical scenarios, they often treat the ADS as a black box. It remains unclear \textit{which specific module leads to a failure and how the diversity of generated failures accurately reflects the weaknesses of different modules}. Existing black-box testing methods may be biased toward detecting failures predominantly caused by weaker modules, such as those in planning or perception, thus not providing a understanding and comprehensive view of all potential weaknesses (see results in Section~\ref{sec: perliminary_study}). Detecting failures across various modules is crucial for developers to understand the root causes accurately and enhance the corresponding components effectively. 


One might question whether it is sufficient to test individual modules within an ADS, such as evaluating the object detection model in the perception module. Indeed, some studies~\cite{ma2024slowtrack, zhang2024data} have focused on model-level evaluations, testing specific modules like perception. However, model errors do not always lead to system failures, and the multiple modules within an ADS can often tolerate some model errors (refer to Section~\ref{sec: perliminary_study}). For instance, an error in the perception module to detect a distant vehicle may not necessarily lead to a system failure, such as a collision, particularly if the missed vehicle does not impact the trajectory of the ego vehicle. Therefore, a more effective approach to ADS testing should generate diverse \textit{system-level} failure, which can pinpoint the limitations of individual modules. It is the primary goal of this paper.




To fill this gap, we aim to investigate the generation of ADS system failures—critical scenarios predominantly triggered by specific modules, such as perception, prediction, planning and control. We define these scenarios as 
``\textbf{\underline{M}}odule-\textbf{\underline{I}}nduced \textbf{\underline{C}}ritical \textbf{\underline{S}}cenarios'' (\mccs), which are safety-critical scenarios revealing system-level failures caused by malfunctions within a designated module $\mathcal{M}$. For instance, consider a collision precipitated by a mis-detection in the perception module while other modules operate correctly. Nonetheless, generating \mccs for a particular module $\mathcal{M}$ presents two primary challenges: 
\ding{182} The first challenge is the lack of effective oracles for identifying \mccs. Intuitively, generating \mccs requires the target module to exhibit errors while other modules function correctly. This necessitates an oracle capable of accurately determining whether a module is malfunctioning or operating as intended. For example, the interactive nature of modules in ADSs complicates the attribution of failures, making it difficult to discern whether a failure originates from the planning module or upstream modules like perception and prediction.
\ding{183} Another challenge lies in effectively and efficiently generating {\mccs}s considering the complex interactions of multiple modules. For instance, most existing methods rely on safety-critical feedback, such as minimizing distance to collision, which may not effectively generate \mccs for the given target module, $M$. As noted earlier, the optimization process in testing tools may be biased towards identifying failures induced by the least robust module, rather than the target module $M$. Therefore, it is essential to employ an effective and efficient optimization strategy that specifically generates \mccs caused by errors in the target module, leading to system failures.



To address these challenges, we propose \tool, a Module-Directed Testing framework for Autonomous Driving Systems, designed to automatically generate {\mccs}s that reveal ADS failures for different modules. \tool comprises three main components: \oracle, \feedback and \select. 
To tackle the first challenge \ding{182}, we design various module-specific metrics that can be used as the oracles in the ADS, allowing each module's performance to be evaluated independently. During the testing phase, \oracle can filter failures that are not caused by the errors of the target module. 
To address the second challenge \ding{183}, we introduce \feedback and \select to improve the optimization of detecting {\mccs}s. 
Specifically, \feedback provides feedback that reflects the extent of errors in different modules, enabling \tool to search for {\mccs}s by maximizing errors in the targeted module while minimizing errors in other modules. 
Furthermore, \select enhances testing performance by incorporating adaptive seed selection and mutation strategies. It prioritizes seeds with higher feedback values and adaptively adjusts mutation strategies based on feedback, thereby increasing the performance of detecting {\mccs}s.



We have evaluated \tool on the ADS platform Pylot~\cite{gog2021pylot} using the high-fidelity simulator CARLA~\cite{dosovitskiy2017carla}. 
We compared \tool with three baseline methods: \textit{Random}, i.e., which generates scenarios randomly, along with two state-of-the-art ADS testing methods, i.e., AVFuzzer~\cite{li2020av} and BehAVExplor~\cite{cheng2023behavexplor}. 
The evaluation results demonstrate the effectiveness and efficiency of \tool in detecting {\mccs}s, uncovering ADS failures across four different modules. 
For example, \tool generates a total of 55.3, 75.3, 71.7, and 14.3 {\mccs}s for the perception, prediction, planning, and control modules across four testing scenarios, respectively, while the best-performing baseline detects only 18.3, 25.3, 34.3, and 7.0 {\mccs}s for these modules.
Further experimental results demonstrate the fidelity of the designed \oracle, the effectiveness of \feedback, and the usefulness of the \select component within \tool.

In summary, this paper makes the following contributions:
\begin{enumerate}[leftmargin=*]

\item To the best of our knowledge, this is the first study to investigate the generation of Module-Induced Critical Scenarios, which highlight specific weaknesses within individual modules of the ADS.

\item We conduct an empirical study on existing methods, revealing limitations in module-based testing for causing system failures and in system-level testing for covering weaknesses across diverse modules in the ADS.

\item We have developed a novel testing framework that effectively and efficiently generates {\mccs}s. This framework provides fine-grained evaluation results at the module level. To assess module correctness, we have designed specific metrics that serve as oracles for various modules.


\item We conduct extensive experiments to evaluate the effectiveness and usefulness of \tool on Pylot. On average, 13.8, 18.8, 17.9, and 2.6 {\mccs}s were discovered across four initial scenarios for the four main modules (perception, prediction, planning, and control), demonstrating \tool's value in detecting module-specific failures.
\end{enumerate}



