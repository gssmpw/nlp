% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{inconsolata}
\usepackage{times}
 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{array}
\usepackage{pdfpages}
\usepackage{colortbl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{longtable}
\usepackage{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{paralist}
% \usepackage{xcolor}
\usepackage{enumitem}
\usepackage{titlesec}

\usepackage{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\newcommand{\jie}[1]{{\textcolor{red}{[jie to do: #1]}}}
\newcommand{\pei}[1]{{\textcolor{orange}{ [Pei: #1]}}}

\usepackage{amssymb}
\newcommand{\nb}[1]{\textcolor{red}{$\blacktriangleright$}\footnote{\textcolor{blue}{#1}}}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jie He$^1$\thanks{Work was done at Microsoft.}\footnotemark[2], Jennifer Neville$^2$\thanks{Corresponding author: j.he@ed.ac.uk, jenneville@microsoft.com,  pei.zhou@microsoft.com.}, Mengting Wan$^2$, Longqi Yang$^2$, \\ \textbf{Hui Liu$^2$}, \textbf{Xiaofeng Xu$^2$}, \textbf{Xia Song$^2$}, \textbf{Jeff Z. Pan$^1$}, \textbf{Pei Zhou$^2$}\footnotemark[2]\\
 $^1$ School of Informatics, University of Edinburgh, UK \\
 $^2$ Microsoft Corporation \\
 }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
\textbf{L}arge \textbf{L}anguage \textbf{M}odels (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present \textbf{GenTool}, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: \textit{Zero-to-One Generalization}, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and \textit{Weak-to-Strong Generalization}, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.


% In this work, we introduce a novel training approach to simulate diverse generalization scenarios for LLMs. We focus on two critical situations that LLMs may encounter: \textit{Zero-to-One Generalization} and \textit{Weak-to-Strong  Generalization}. These scenarios reflect real-world deployment cases, where models might face entirely new tools or advanced versions of existing tools. To address these challenges, we generate synthetic training data to simulate these scenarios and fine-tune LLMs using the synthetic data. Additionally, we propose a two-stage fine-tuning method where the model first ranks potential tools and then selects the most appropriate tool. We conduct extensive experiments across various generalization settings, demonstrating that the proposed method significantly enhances the tool-learning capabilities of LLMs ranging from 1B to 8B parameters, even surpassing GPT-4o. Furthermore, we provide an in-depth analysis of the challenges faced by LLMs in tool generalization.

\end{abstract}
\section{Introduction}
Tool learning has emerged as a crucial capability for \textbf{L}arge \textbf{L}anguage \textbf{M}odels (LLMs), enabling them to expand their functionality through external tool integration \cite{huang-etal-2024-planning, tang2023toolalpaca, patil2023gorilla}. By interfacing with external tools, LLMs can dynamically access real-time information, validate responses against external knowledge bases, and improve outputs through iterative feedback \cite{deng2023mind2web, wang-etal-2024-llms-imaginarium}. This capability fundamentally enhances LLMs' ability to process and respond to real-world information beyond their pre-trained knowledge.

% Tool learning has recently gained widespread attention among open-source LLMs, such as LLaMA3 \cite{dubey2024llama3herdmodels} and Mistral \cite{jiang2023mistral7b}, as external tools to expand model capabilities beyond their training data\cite{huang-etal-2024-planning, tang2023toolalpaca, patil2023gorilla, schick2023toolformer}. This approach enables LLMs to interface with external tools, significantly enhancing their ability to process and respond to real-world information. For example, LLMs can access real-time weather data to provide accurate, current weather information in response to user queries\cite{deng2023mind2web}. Additionally,  these models can leverage tool calls to improve their outputs through iterative feedback loops\cite{wang-etal-2024-llms-imaginarium, xu-etal-2024-enhancing-tool, wu2024toolplannertoolaugmentedllm, qiao-etal-2024-making}, and validate their responses against external knowledge bases \cite{wu2024avataroptimizingllmagents}. Rather than relying solely on pre-trained knowledge, the integration of external tools enables LLMs to dynamically access, verify, and incorporate current information into their reasoning processes, fundamentally expanding their ability to perceive and interact with the external world.

% Tool learning has recently gained widespread attention among open-source LLMs, such as LLaMA3 \cite{dubey2024llama3herdmodels} and Mistral \cite{jiang2023mistral7b}, as external tools enable models to access information beyond their inherent knowledge \cite{huang-etal-2024-planning, tang2023toolalpaca, patil2023gorilla, schick2023toolformer}. For example, LLMs can query real-time weather data to address weather-related user requests \cite{deng2023mind2web}. They can also improve their outputs by obtaining feedback through tool calls \cite{wang-etal-2024-llms-imaginarium, xu-etal-2024-enhancing-tool, wu2024toolplannertoolaugmentedllm, qiao-etal-2024-making}, verifying consistency with external knowledge bases \cite{wu2024avataroptimizingllmagents}. Tool integration significantly expands LLMs' perception of the external world.
 

\begin{figure}[!tpb]
    \centering
    \includegraphics[width=0.9\linewidth]{images/tool_intro.pdf}
    \caption{ 
    An example illustrating tool generalization challenges in selecting the most suitable tool for a user query. While the model was trained on tools like Yelp and Web-Search, and encountered the same query:  ``Could you ... in New York City?'', it struggles during testing to select the more appropriate Yelp tool over Web-Search for the same query during test. 
    % \pei{be sure to make the caption self-contained. Also make fonts bigger}
    }
    \label{fig:intro}
    \vspace{-0.6cm}
\end{figure}

% The current paradigm for tool utilization by LLMs generally follows this workflow \cite{qu2024toollearninglargelanguage}: given a user query and documentation of available tools, the model identifies the appropriate tool, parses the query into the required parameter values, receives information from the tool, and synthesizes the final output. However, since LLMs are not inherently trained to interact with external tools during pretraining, current tool learning approaches can be categorized into two strategies. 
% \pei{maybe add a high-level descriptions: ``two strategies: 1) in-context learning; 2) fine-tuning''}

% First, leveraging detailed tool documentation or usage examples helps models comprehend tools and make accurate calls. \pei{Make it clear here that these descriptions are used in-context, i.e. not fine-tuning} However, limited context lengths constrain models' ability to process numerous tools simultaneously \cite{DBLP:journals/corr/abs-2303-17491, paranjape2023artautomaticmultistepreasoning, hsieh2023tooldocumentationenableszeroshot}.

% Second, collecting real user tool usage cases in specific domains and employing LLMs like ChatGPT to synthesize additional tool usage cases can fine-tune open-source models 
% \pei{can we simplify this by just saying that the second strategy is collecting tool-using data and fine-tune? Synthesize is one way of doing it and seems detailed in Intro} 
% \cite{mekala-etal-2024-toolverifier, Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}. For instance, \citet{qin2024toolllm} constructed a dataset of 16,000 tool usage cases to enhance models' tool utilization capabilities through supervised fine-tuning.

The current paradigm for tool utilization follows a standard workflow \cite{qu2024toollearninglargelanguage}: given a user query and available tool documentation, LLMs identify appropriate tool, extract required parameters, obtain information from the tool and synthesize final outputs. Two primary approaches have been developed to enable this capability: in-context learning, which leverages tool documentation and examples within the model's context \cite{hsieh2023tooldocumentationenableszeroshot}, and fine-tuning on collected tool usage data \cite{qin2024toolllm, mekala-etal-2024-toolverifier}. However, both approaches face fundamental limitations. In-context learning is constrained by context length restrictions, preventing comprehensive tool understanding \cite{DBLP:journals/corr/abs-2303-17491, paranjape2023artautomaticmultistepreasoning}. Fine-tuning methods risk over-fitting to specific tools and usage patterns, compromising generalization to unseen tools and queries. These limitations pose significant challenges for real-world applications, where new tool categories and usage patterns constantly arise as shown in Figure \ref{fig:intro}.

% The current paradigm for tool utilization by LLMs follows a standard workflow \cite{qu2024toollearninglargelanguage}: when presented with a user query and documentation of available tools, the model identifies the appropriate tool, converts the query into required parameter values, obtains information from the tool, and synthesizes the final output. 
% Since LLMs are not inherently trained to interact with external tools during pretraining, two primary strategies have emerged to enable tool learning: \textbf{In-context Learning}: this approach leverages detailed tool documentation and usage examples provided in the model's context  to help LLMs understand and make accurate tool calls \cite{hsieh2023tooldocumentationenableszeroshot}; \textbf{Fine-tuning}: This strategy involves collecting tool usage data and fine-tuning models on these examples \cite{mekala-etal-2024-toolverifier, Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}. For example, \citet{qin2024toolllm} developed a dataset of 16,000 tool usage cases for supervised fine-tuning to enhance models' tool utilization capabilities.

% \pei{This transition could be made more smooth, starting with ``Although these two paradigms bring improvements, challenges remained ...''}
% Although these two primary learning strategies bring improvements, each faces limitations. In-context learning approaches are constrained by context length restrictions, preventing models from accessing comprehensive tool documentation and diverse usage examples needed for robust generalization.\cite{DBLP:journals/corr/abs-2303-17491, paranjape2023artautomaticmultistepreasoning, hsieh2023tooldocumentationenableszeroshot}. Meanwhile, fine-tuning approaches might risk overfitting to specific tools and usage patterns in the training data, potentially compromising their ability to handle novel tools or unexpected query variations. These limitations indicates that tool generalization remains a critical challenge, particularly in real-world applications where new tool categories and unseen tool-query combinations frequently emerge.



% Additionally, previous work primarily focus on generalizing to unseen tools \cite{zhang2024dontfinetunedecodesyntax, gui2024lookleapdecisionawaregeneralizable}, but they do not consider the query. . Tool learning generalization is inherently a complex combinatorial problem.}

% \pei{are you saying that these two work don't do combinatorial generalization? This is important to distinct our work, consider spelling out the differences more clearly, potentially with an example}. 
% \pei{this can connect back to the distinctions (to be added) from the prev work also doing tool generalization. Something like: ``prev work just fine-tune on new examples, still facing overfitting, we tackle this by simulating unseen tools in training by augmenting ...''}
% \pei{we need an argument that why the previous 2 strategies don't solve generalization. Maybe overfitting or context-length limits?}
% \pei{are you saying that these two work don't do combinatorial generalization? This is important to distinct our work, consider spelling out the differences more clearly, potentially with an example}. 

% To tackle these challenges, we introduce \textbf{GenTool}, a novel framework designed to enhance LLM's tool generalization capabilities across two core dimensions: \textbf{zero-to-one generalization} which handles cases where a query initially has no available useful tool, requiring the model to adopt and employ one once it becomes available and \textbf{weak-to-strong generalization} which focuses on situations where models trained on partially helpful tool must adapt to fully helpful tool during testing, requiring them to appropriately prioritize stronger tools when multiple options are available. Through these two dimensions, we identify four distinct generalization scenarios for evaluations: \textit{seen\_query\_unseen\_tool}, \textit{seen\_query\_seen\_tool}, \textit{unseen\_query\_unseen\_tool}, and \textit{unseen\_query\_seen\_tool}.

To tackle these challenges, we introduce \textbf{GenTool}, a novel framework designed to enhance LLM's tool generalization capabilities across two core dimensions: \textbf{zero-to-one generalization}, where a query initially has no available useful tool, and \textbf{weak-to-strong generalization}, where models must adapt from a weak tool which is ineffective in fully addressing the query to a strong one which can help provide an answer that perfectly matches the query. Through these dimensions, we identify four evaluation scenarios: \textit{seen\_query\_unseen\_tool}, \textit{seen\_query\_seen\_tool}, \textit{unseen\_query\_unseen\_tool}, and \textit{unseen\_query\_seen\_tool}.


% The first is \textbf{zero-to-one generalization}, where the model must invoke a previously unseen tool during testing to resolve queries. The second is \textbf{weak-to-strong generalization} in which the model encounters a weaker tool during training but faces a stronger version during testing. The model must prioritize the stronger tool when both are available. Building on these two paradigms, we identify four distinct generalization scenarios: \textit{seen\_query\_unseen\_tool}, \textit{seen\_query\_seen\_tool}, \textit{unseen\_query\_unseen\_tool} and \textit{unseen\_query\_seen\_tool}.

 % While effective for simple tasks, this method faces limitations due to limited context lengths, which restrict models' ability to process numerous tools simultaneously

% Despite these advancements, real-world applications frequently introduce novel tool categories or unseen combinations of tools and user queries. Generalization thus becomes a critical challenge in tool learning.
% As illustrated in Figure~\ref{fig:intro}, models may encounter combinations of tools and queries during testing that were never seen together during training.  While previous work has primarily focused on generalizing to unseen tools \cite{zhang2024dontfinetunedecodesyntax, gui2024lookleapdecisionawaregeneralizable}, tool learning generalization is inherently a complex combinatorial problem 



% Building on these two dimensions, we identify four distinct generalization scenarios. The first is \textit{seen\_query\_unseen\_tool}, where queries are seen during training, but the corresponding tools are not. The second is \textit{seen\_query\_seen\_tool}, where both queries and tools are observed during training. The third, \textit{unseen\_query\_unseen\_tool}, features queries and tools that were entirely absent during training. The fourth, \textit{unseen\_query\_seen\_tool}, presents queries are unseen but rely on tools that were previously encountered during training.


To enable effective training across the two important dimensions, we developed a high-quality synthetic training dataset. This dataset comprises 834 new synthetic tools and their descriptions. For each tool, we generated 10 diverse queries, resulting in 8,515 distinct queries and 33,286 high-quality training samples. Each sample includes structured query-tool pairs and detailed parameter information. Additionally, we complement our data augmentation strategy with a novel two-stage fine-tuning strategy that
explicitly teaches models to rank tools by capability before selection, enabling systematic understanding of tool relationships rather than simple query-tool mappings. 


% teaches models to rank tools by capability before selection. This approach enables models to develop a systematic understanding of tool relationships, 

Our extensive evaluation demonstrates GenTool's effectiveness across various model architectures including LLaMA 3.2 1B, LLaMA 3.1 8B, Mistral 7B, and Phi 3B. GenTool achieves state-of-the-art performance across all metrics, significantly outperforming both tuning-free and tuning-based baselines, including GPT-4o, with a 14.28\% improvement in tool selection accuracy. Moreover, GenTool exhibits exceptional generalization capabilities across our four evaluation scenarios. Additionally, ablation studies validate the importance of our ranking mechanism, where its removal leads to performance degradation of up to 10.89\%.




% Our evaluation across various model architectures (LLaMA 3.2 1B, LLaMA 3.1 8B, Mistral 7B, and Phi 3B) demonstrates GenTool's effectiveness, outperforming both tuning-free and tuning-based baselines, including GPT-4o, with a 14.28\% improvement in tool selection accuracy. Ablation studies confirm the importance of our ranking mechanism, 




% While we construct training sets to expose models to generalization scenarios, we recognized that the fine-tuning process itself needed to better leverage this rich training data. Specifically, models must not only recognize tools but also understand their relative capabilities. To achieve this, we devised a two-stage fine-tuning process that maximizes learning from our generated data. Models first output a ranked list of tools, forcing explicit reasoning about tool comparisons, followed by calling the highest-ranked tool. This ranking-then-selection framework complements our our data construction strategy, which ensures models develop a structured understanding of tool relationships rather than only learning query-tool mappings.

% Using this dataset, we fine-tuned models such as LLaMA 3.2 1B, LLaMA 3.1 8B, Mistral 7B, and Phi 3B, significantly improving their generalization performance during testing.



% \pei{the motivation why we add this two-stage should be clearer, even better if it's connected with our motivation for data augmentation}, 
% Our extensive evaluation across four distinct generalization scenarios validates GenTool 's effectiveness. The methodology consistently improved tool generalization capabilities across all tested models. To further demonstrate GenTool 's versatility, we extended our evaluation to in-context learning (ICL) settings using GPT-series models, where the framework proved equally effective.


\textbf{Our main contributions are summarized as follows:}
\begin{compactenum}     
\item We propose GenTool, a novel framework that enhances LLMs' generalization capabilities on unseen tools through structured simulation of two critical dimensions. GenTool achieves state-of-the-art performance across all metrics, significantly outperforming various strong baselines.

\item We introduce a two-stage fine-tuning strategy combining tool selection with ranking supervision, enabling enabling robust tool generalization through better understanding of functional differences between similar tools.

% models to capture key functional differences between similar tools. This approach demonstrates significant effectiveness for robust tool generalization.
    
% \item We conduct comprehensive empirical analysis, revealing key insights into tool generalization, including the limitations of data scaling and the  importance of diverse training setting, as evidenced by a 67\% performance drop when training exclusively on test-set-similar cases.

\item We provide comprehensive empirical analysis revealing key insights into tool generalization, including the limitations of data scaling and the importance of diverse training settings, evidenced by a 67\% performance drop when training exclusively on test-set-similar cases.

\end{compactenum} 
% \textbf{Our main contributions} are as follows:
% \begin{enumerate}
%     \item We propose GenTool , a novel tool learning framework that simulates real-world generalization scenarios to enhance LLMs' tool generalization capabilities.
%     \item We introduce a two-stage fine-tuning strategy combining tool selection with ranking supervision, enabling models to capture key functional differences between similar tools. This approach achieves state-of-the-art performance on standard tool learning benchmarks.
%     \item We systematically analyze tool learning generalization challenges,demonstrating that conventional fine-tuning approaches focusing on seen tools or specific query patterns significantly limit generalization capability. Our findings provide crucial insights for developing robust tool learning systems.
%     \item We also offer an in-depth analysis of generalization performance, showing that increasing training examples related to test tools does not always improve accuracy. Moreover, if the model is trained only on tools related to the test set, its generalization ability will be significantly poor.
% \end{enumerate}


% \textbf{Our main contributions} are as follows:
% \begin{enumerate}
%     \item We propose GenTool , a novel tool learning framework that simulates real-world generalization scenarios to enhance LLMs' tool generalization capabilities.
%     \item We propose a two-stage fine-tuning strategy that leverages ranking-based outputs to  improve the model's ability to distinguish fine-grained differences within toolsets.
%     \item We systematically analyze tool learning generalization challenges,demonstrating that fine-tuning solely on seen tools or queries drastically reduces generalization performance. GenTool  achieves an absolute improvement of 49.63\% for LLaMA-3.1-8B-Instruct model, surpassing GPT-4.
%     \item We also offer an in-depth analysis of generalization performance, showing that increasing training examples related to test tools does not always improve accuracy. Moreover, if the model is trained only on tools related to the test set, its generalization ability will be significantly poor.
% \end{enumerate}


% \section{Introduction}

% Tool learning has recently gained widespread adoption among open-source LLMs (e.g., LLaMA3 \cite{dubey2024llama3herdmodels} , Mistral \cite{jiang2023mistral7b}) as external tools enable models to access information beyond their inherent knowledge \cite{huang-etal-2024-planning,tang2023toolalpaca,patil2023gorilla,schick2023toolformer}. For instance, models can query real-time weather data to address weather-related user requests \cite{deng2023mind2web}. Furthermore, LLMs can improve their outputs by obtaining feedback through tool calls \cite{wang-etal-2024-llms-imaginarium,xu-etal-2024-enhancing-tool,wu2024toolplannertoolaugmentedllm,qiao-etal-2024-making}, such as verifying consistency between their outputs and knowledge stored in external knowledge bases \cite{wu2024avataroptimizingllmagents}. In essence, tool integration significantly expands LLMs' perception of the external world.

% The current paradigm for tool utilization by LLMs typically follows this workflow \cite{qu2024toollearninglargelanguage}: given a user request and documentation of available tools, the model determines which tool to call, parses the user request into parameter values required by the selected tool, receives necessary information from the called tool, and synthesizes a final output based on the tool's response. However, since LLMs are not inherently trained to interact with external tools during pre-training, current tool learning approaches can be categorized into two main strategies:

% 1. Approaches that leverage detailed tool documentation or usage examples to enable models to comprehend tools thoroughly and make appropriate calls. However, models' limited context length constrains their ability to handle numerous tools simultaneously \cite{DBLP:journals/corr/abs-2303-17491, paranjape2023artautomaticmultistepreasoning,hsieh2023tooldocumentationenableszeroshot}.

% 2. Approaches that collect real user tool usage cases in specific domains and employ LLMs like ChatGPT to synthesize additional tool usage cases for fine-tuning open-source models \cite{mekala-etal-2024-toolverifier,Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}. For example, \citet{qin2024toolllm} constructed a dataset encompassing 16,000 tool category usage cases to enhance models' tool utilization capabilities through supervised fine-tuning.

% However, real-world applications continually introduce new tool categories not covered during model training or demonstration examples. Thus, generalization becomes a critical challenge in tool learning. While existing research has primarily focused on generalization to unseen tools \cite{zhang2024dontfinetunedecodesyntax,gui2024lookleapdecisionawaregeneralizable}, tool learning generalization is actually a complex combinatorial problem. As illustrated in Figure 1, models may have encountered both a tool and a user request during training, but during testing, they may face novel combinations of tools and requests previously unseen together.

% To comprehensively enhance LLMs' tool learning generalization capabilities and address gaps in tool learning generalization, we propose GenTool , a data augmentation and training methodology that simulates tool generalization scenarios. We decompose tool generalization into two perspectives:

% 1. Zero-to-One generalization: During model training, the model has never encountered a particular tool, but during testing, it must invoke this tool to resolve user queries.

% 2. Weaker-to-Stronger generalization: During training, the model encounters a weaker tool and corresponding queries, but during testing, it faces a stronger tool. When both weaker and stronger tools are present in the tool set, the model should prioritize calling the stronger tool.

% Based on these two generalization scenarios, we investigate four generalization problems:
% 1. Seen query & unseen tool: The model has encountered specific queries during training but not the corresponding tools
% 2. Seen query & seen tool
% 3. Unseen query & unseen tool
% 4. Unseen query & seen tool

% Since existing tool datasets typically provide only one suitable tool for a given query, we created a high-quality synthetic training dataset based on UltraTool \cite{huang-etal-2024-planning-creation} to simulate weaker-to-stronger generalization. This dataset contains 834 synthetic tools with corresponding descriptions. Following MetaTool \cite{huang2024metatool}, we generated 10 queries per tool to increase query diversity, resulting in 8515 new queries and 33286 total samples with Ultra Tool, each involving a query, gold tool, and corresponding parameter names and values. We fine-tuned LLaMA 3.2 1B, LLaMA 3.1 8B, Mistral 7B, and Phi 3B using this data, enabling better generalization during testing through simulated tool generalization scenarios during training.

% Moreover, recognizing that multiple tools might be useful for a single query within a toolset, directly selecting the best tool as output proves overly simplistic. Therefore, we incorporate tool priority ranking within toolsets, requiring models to first output a ranked list of tools before calling the highest-ranked tool. This two-stage approach (ranking followed by selection) enhances models' comprehension of toolset relationships. Experimental results across four generalization scenarios validate GenTool 's effectiveness, demonstrating significantly improved model generalization capabilities across all scenarios. We also investigated GenTool 's generalization effectiveness in in-context learning (ICL) scenarios with GPT-series models.

% Our main contributions include:
% 1. Introduction of GenTool , a tool learning framework that simulates real-world generalization scenarios to enhance LLMs' tool generalization capabilities
% 2. A two-stage fine-tuning approach based on ranking output that enhances LLMs' perception of subtle differences within tool sets
% 3. Systematic categorization and analysis of tool learning generalization problems, revealing that fine-tuning solely on seen tools or queries significantly impairs model generalization performance. GenTool  enhances performance across various generalization scenarios, particularly achieving XX\% absolute improvement for XX model, surpassing GPT-4
% 4. In-depth analysis of generalization performance, revealing that more training examples related to test tools don't always yield better results, instead showing a U-shaped accuracy curve as related training examples increase.
\section{Problem Definition: Generalization in Tool Learning}
\label{sec2}
Our framework aims to improve LLM generalization across diverse query-tool combinations. This section defines the tool generalization problem and presents a systematic framework for its evaluation.
\subsection{Preliminaries}
% To establish a foundation for analyzing tool learning, we begin by formalizing the core components of our framework. Each instance $d$ in our dataset is represented as a tuple $(T_s, q, y, t)$, where:
% (1) $T_s = [\text{t}_1, \ldots, \text{t}_k]$ represents the toolsets, consisting of $k$ distinct tools. 
% (2) $q$ denotes the query submitted to th the model.
% (3) $t$ indicates the ground truth tool necessary for query resolution, where where $t \in T_s$.
% (4) $y$ is the ground truth tool calling information which contains the following elements:
%    the tool name,
%    parameter names, and
%    their corresponding parameter values.

To analyze tool learning, we first define the core components of our framework. Each instance $d$ in our dataset is represented as a tuple $(T_s, q, y, t)$, where the toolset $T_s$ consists of $k$ distinct tools,   $q$   the query submitted to the model,  $t\in T_s$ the ground truth tool  required to resolve the query, and $y$ the tool-calling information  specifing the tool name, parameter names, and their corresponding parameter values.
% \nb{There seem to be some overlaps between t and y. Does y contain only information about t, or information about all tools in $T_s$?}

% Each $t_i \in T_s$ is accompanied by additional metadata, which includes:
% the tool name,
% descriptions of its parameters,
% names of the expected results, and
% descriptions of the expected results.
Each tool $t_i$ within $T_s$ is equipped by metadata that includes its name, descriptions of its parameters, names of expected results, and descriptions of those results. This formalization provides a foundation for assessing the model's ability to generalize across varying combinations of queries and tools, establishing the groundwork for our framework.

% Given these definitions, the fundamental learning objective can be formulated as follows: for any input pair $(T_s, q)$, the model must accurately identify the appropriate tool $t$ and extract the requisite parameter names and values to generate the correct tool calling information $y$



% Our GenTool  framework aims to equip LLMs with robust generalization capabilities across diverse combinations of queries and tools. Here, we define the tool learning generalization problem. Specifically, we begin with the fundamentals of tool learning.

% Each instance $d$ in our dataset consists of a tuple $(T, q, y, t)$. Here:
% \begin{itemize}
%     \item $T = [\text{tool}_1, \ldots, \text{tool}_k]$ is the toolsets, where $k$ is a predefined hyperparameter. Due to input length constraints, we set $k = 5$.
%     \item $q$ is the query.
%     \item $t$ is the ground truth tool selected to answer the query. It is always included in $T$.
%     \item $y$ contains the tool name, parameter names, and their corresponding parameter values.
% \end{itemize}

% For each tool $\text{tool}_i$ in the toolsets, we provide additional information, including the tool name, parameter names with descriptions, result names, and result descriptions. In our experiments, we exclude result information from $y$, as we focus on the tool generalization problem rather than actual API calls.



% In summary, the learning task can be formalized as follows: given a pair $(T_s, q)$, the model needs to infer the correct tool $t$ to call and parse the query to extract the parameter names and corresponding parameter values to generate $y$. 

 %inspired by UltraTool and ToolVerifier.

% To comprehensively evaluate the model's generalization capabilities, we categorize test cases into four scenarios based on query and tool exposure during training. Below, we detail each category and provide examples.
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/tool_data_generation.pdf}
    \caption{The construction process for synthetic data to simulate the generalization process involves three steps: First, existing datasets are utilized to create new tools. Next, diverse instructions guide the generation of matching queries for these new tools. Finally, corresponding invocation details are created for various tool-query combinations.}
    \label{fig:data_cons}
    \vspace{-0.3cm}
\end{figure*}
\subsection{Four Generalization Scenarios}
\label{sec2.2}
To systematically evaluate a model's generalization capabilities, we define four scenarios based on the familiarity of queries and tools during training.
% we establish a taxonomy of four distinct scenarios, each representing a unique combination of query and tool familiarity during training.

\paragraph{Seen Query and Unseen Tool}
% \subsubsection{Seen Query and Unseen Tool}
The training set contains a sample $d_{\text{train}}^1 = (T^1_s, q_1, t_1, y_1^1)$, while the test set includes a sample $d_{\text{test}}^1 =  (T^2_s, q_1, t_{2}, y_1^2)$. The query $q_1$ is identical across training and test sets, but the ground truth tool $t_{2}$ in the test case is not present in any training toolsets $T_s$, making $q_1$ a \textit{seen query} and $t_2$ an \textit{unseen tool}.

% Thus, $q_1$ is a \textit{seen query}, while $t_{2}$ is an \textit{unseen tool}.

\paragraph{Seen Query and Seen Tool}
% \subsubsection{Seen Query and Seen Tool}
Both the query and the ground truth tool are present during training. The training set contains    $d_{\text{train}}^1 = ( T_s^2, q_1, t_2, y_1^2 )$, and  $d_{\text{train}}^2 = ( T_s^1, q_2, t_1, y_2^1 )$ and the test set includes $d_{\text{test}}^1 = ( T_s^1, q_1, t_1, y_1^1 )$. $q_1$ and $t_1$ are \textit{seen}.

\paragraph{Unseen Query and Unseen Tool}
% \subsubsection{Unseen Query and Unseen Tool}
Neither the query nor the tool has been observed during training. The training set includes $(T_s^1, q_1, t_1, y_1^1)$, while the test set contains $(T_s^2, q_2, t_2, y_2^2)$, making $q_2$ and $t_2$ both \textit{unseen}.
% In this scenario, neither the query nor the ground truth tool has been encountered during training. The training set contains $d_{\text{train}}^1 = ( T_s^1, q_1, t_1, y^1_1 )$, while the test set includes $d_{\text{test}}^1 = ( T_s^2, q_2, t_{2}, y_2^2 )$. Here, $q_2$ is an \textit{unseen query}, and $t^{2}$ is an \textit{unseen tool}, as neither is present in any training toolset.



\paragraph{Unseen Query and Seen Tool}
% \subsubsection{Unseen Query and Seen Tool}
The test query is unseen, but the ground truth tool appears in training. The training set includes $(T_s^1, q_1, t_1, y_1^1)$, and the test set has $(T_s^1, q_2, t_1, y_2^1)$. Here, $q_2$ is \textit{unseen}, and $t_1$ is \textit{seen}.
% In this scenario, the query in the test set is unseen, but the ground truth tool has been observed during training. The training set contains $d_{\text{train}} = ( T_s^1, q_1, t_1, y_1^1 )$, and the test set includes $d_{\text{test}} = ( T_s^1, q_2, t_1, y_2^1 )$. Here, $q_2$ is an \textit{unseen query}, while $t^1$ is a \textit{seen tool}, since it appears in the training toolset. 


These scenarios provide a structured framework for assessing model generalization across queries and tools, offering valuable insights into robustness and adaptability of tool learning models in real-world applications.

% These four scenarios constitute a comprehensive evaluation framework that enables systematic assessment of model generalization across the query-tool space. This structured approach provides valuable insights into the robustness and adaptability of tool learning models in real-world applications, thereby informing future advances in tool learning systems. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/tool_training.pdf}
    \caption{Overview of the GenTool  Framework for Tool Learning and Generalization. Initially, the model handles a query by defaulting to \texttt{generate\_response} when no suitable tool is available. Next, when a relevant tool, \texttt{web\_search}, is added to the toolset, the model selects \texttt{web\_search}, demonstrating zero-to-one generalization training. Later, 
    upon adding \texttt{map\_search}, the model demonstrates weak-to-strong generalization by correctly ranking and selecting it over web\_search and other alternatives.}
    % (map\_search > web\_search > generate\_response > others)
    \label{fig:GenTool _framework}
    \vspace{-0.3cm}
\end{figure*}




% Our GenTool  (Weak Supervision Tool Generalization) framework aims to enhance LLMs' generalization capabilities across various query-tool combinations during the tool learning process. Here, we formally define our tool learning generalization problem.

% We begin with fundamental tool learning concepts. Each instance $d$ in our dataset comprises a tuple $(T, q, y, T_{gold})$, where $T = [tool_1, ..., tool_k]$ represents the toolsets ($k$ is a hyperparameter set to 5 due to model input length constraints), $q$ denotes the query, and $T_{gold}$ represents the ground truth tool selected to address the query, with $T_{gold} \in T$. The variable $y$ encompasses the tool name, parameter names, and parameter values.

% For each $tool_i$, we maintain the following information: tool name, parameter names with corresponding descriptions, and result names with their descriptions. Our experiments focus solely on tool generalization without actual API invocation; therefore, we exclude results from $y$.

% Formally, our learning process can be defined as: given $\langle Tool\ list, q\rangle$, the model must:
% \begin{enumerate}
%     \item Infer the appropriate tool $T_{gold}$
%     \item Parse the query to extract parameter names and corresponding values for $T_{gold}$
% \end{enumerate}

% To comprehensively evaluate generalization capabilities, we categorize test cases into four distinct scenarios:

% \subsection{Seen Query and Unseen Tool}
% Given a training instance $Tr_i = \langle Tool\_list_1, query_1, T_{gold}, y\rangle$ and a test instance $Te_i = \langle Tool\_list_2, query_1, T_{gold2}, y\rangle$, where $query_1$ is identical in both cases but $T_{gold2}$ differs from $T_{gold}$ and has never appeared in any training set toolsets.

% \subsection{Seen Query and Seen Tool}
% Consider a training instance $Tr_i = \langle Tool\_list_1, query_1, T_{gold}, y\rangle$ and a test instance $Te_i = \langle Tool\_list_2, query_1, T_{gold}, y\rangle$, where $query_1$ appears in both cases and $T_{gold}$ has been observed in training set toolsetss, though possibly in different combinations. This scenario evaluates the model's ability to maintain consistent tool selection across varied contexts.

% \subsection{Unseen Query and Unseen Tool}
% Given $Tr_i = \langle Tool\_list_1, query_1, T_{gold}, y\rangle$ and $Te_i = \langle Tool\_list_2, query_2, T_{gold2}, y\rangle$, where $query_2$ follows a pattern distinct from all training queries, and $T_{gold2}$ has never appeared in training set toolsetss. This represents the most challenging generalization scenario, testing the model's capacity to adapt to entirely novel situations.

% \subsection{Unseen Query and Seen Tool}
% Consider $Tr_i = \langle Tool\_list_1, query_1, T_{gold}, y\rangle$ and $Te_i = \langle Tool\_list_2, query_2, T_{gold}, y\rangle$, where $query_2$ exhibits a new pattern, but $T_{gold}$ has been encountered during training. This tests the model's ability to recognize appropriate tool applications in unfamiliar query contexts.
\section{GenTool : Synthetic Data Generation for Generalization Simulation } 
% \pei{I think this section is still missing a motivation pitch of why we are generating new data to boost generalization. Sentences before ``Our data'' seem to closer to related work discussion and should be shortened. But we need to convey the message that first, the gap in generalization is hard to address with current data, because model learns biases to using tools in training set, second: we attempt to address this problem by simulating generalization scenarios in training phase, with the hypothesis that learning from simulated scenario requiring generalization, it performs better during testing on generalization too. Maybe even need to have a toy example of what ``simulation'' means: something like an instance where model chooses tool A, but we augment with an instance where tool A and a better version of tool A-hat exists too, it should choose A-hat.}
% \pei{changed typo: syntactic-->synthetic}
\label{sec:data_generation}


% The gap in generalization is hard to address with current data \cite{qin2024toolllm, tang2023toolalpaca}, because model learns biases to using tools in training set. We attempt to address this problem by simulating generalization scenarios in training phase, with the hypothesis that \textbf{learning from simulated scenario requiring generalization, it performs better during testing on generalization too}. Like the instance in Figure \ref{fig:data_cons}, where model chooses the  simple calender $\text{C}^{'}$ to solve the complex query when only it exists, but we augment with an instance where the  simple calender tool $\text{C}^{'}$ and a more complex calender tool $\text{C}$ exists too, it should choose $\text{C}$.
% To address this limitation, we use GPT-4o to create alternative ground truth tools ($t$) formatted consistently with existing tools. Our data generation process consist of \textbf{three steps}: 
% 1) for each query-tool pair, we generate two weaker tool; 2) for each generated weak tool, we generate 10 queries that are better answered with the previous weaker tool (thus, making it a better tool for the generated query); 3) for each query-tool pair, we generate the ground truth calling information.  The detailed collection guideline can be found in Appendix \ref{app_a}.

% Current tool datasets, such as UltraTool \cite{huang-etal-2024-planning-creation}, T-Eval \cite{chen-etal-2024-eval}, ToolLLaMA \cite{qin2024toolllm}, and ToolAlpaca \cite{tang2023toolalpaca}, only provide a single annotated solution for a given query. These datasets cannot evaluate LLMs' generalization capabilities across different tool sets. Thus, they are insufficient for constructing various tool generalization scenarios. 
The gap in generalization remains challenging to address with current data \cite{qin2024toolllm, tang2023toolalpaca}, as models tend to learn biases from the tools available in the training set. To tackle this issue, we propose GenTool, a synthetic data generation framework that simulates generalization scenarios during the training phase.

Our approach is motivated by a key observation: when a model only has access to a basic calendar tool  $\text{C}'$, it defaults to using it. However, when presented with both $\text{C}'$ and a better tool, $\text{C}$, the model should recognize and select $\text{C}$ for complex queries. We employ GPT-4o to generate additional tools that maintain the same format as the original tools in our dataset. For each original tool t, we generate weaker variants that have limited functionality while preserving the API structure.

Figure \ref{fig:data_cons} illustrates our data generation pipeline, which consists of three primary components: tool generation, query generation, and calling information synthesis. Each component is designed to create diverse, high-quality training instances that specifically target generalization capabilities. Details for prompts and data quality checking, please refer to App. \ref{app_datasets}.

% Our data generation pipeline consists of three components: tool generation for creating diverse tools, query generation for testing tool selection, and calling information synthesis for constructing invocation patterns. Each component is designed to enhance generalization capabilities. 


% We employ GPT-4o to generate complementary tools that maintain consistent formatting with our existing dataset. For each original tool t, we generate weaker variants while preserving the API structure.



% To enable such behavior, we leverage GPT-4o to generate synthetic tools that adhere to the same format as the original tools in our dataset. For each original tool $t$, we create weaker variants with limited functionality while maintaining the same API structure, ensuring consistency and realism.



% The gap in generalization remains challenging to address with current data \cite{qin2024toolllm, tang2023toolalpaca}, as models tend to learn biases from the tools available in the training set. To tackle this issue, we propose GenTool, a synthetic data generation framework that simulates generalization scenarios during the training phase. This  approach is motivated by the observation illustrated in Figure \ref{fig:data_cons}, where a model might default to using a simpler calendar  $\text{C}^{'}$  when it is the only option available. However, when presented with both  $\text{C}^{'}$ and a more comprehensive calendar tool $\text{C}$, the model should ideally recognize and select the more appropriate tool $\text{C}$ for complex queries. We use GPT-4o to generate additional tools that maintain the same format as the original tools in our dataset. For each original tool t, we generate weaker variants that have limited functionality while preserving the API structure. 

% Our data generation pipeline consists of three primary components: tool generation, query generation, and calling information synthesis. Each component is designed to create diverse, high-quality training instances that specifically target generalization capabilities. The detailed collection guideline can be found in Appendix \ref{app_a}.



% with the hypothesis that \textbf{learning from simulated scenario requiring generalization, it performs better during testing on generalization too}.
% Like the instance in Figure \ref{fig:data_cons}, where model chooses the  simple calender $\text{C}^{'}$ to solve the complex query when only it exists, but we augment with an instance where the  simple calender tool $\text{C}^{'}$ and a more complex calender tool $\text{C}$ exists too, it should choose $\text{C}$.
% To address this limitation, we use GPT-4o to create alternative ground truth tools ($t$) formatted consistently with existing tools. 

% Our data generation process consist of \textbf{three steps}: 
% 1) for each query-tool pair, we generate two weaker tools; 2) for each generated weak tool, we generate 10 queries that are better answered with the previous weaker tool (thus, making it a better tool for the generated query); 3) for each query-tool pair, we generate the ground truth calling information.
% \pei{I suggest that this paragraph include a highlevel data generation flow description with clearly-labeld steps, something like an algorithm description: e.g. 1) for each query-tool pair, we generate a weaker tool; 2) for each generated weaker tool, we generate 10 queries that are better answered with the previous weaker tool (thus, making it a better tool for the generated query); 3) ...} 

\subsection{Tool Generation}
\label{section3.1}
% We use UltraTool as our initial toolset because it contains 5,000 high-quality user queries refined by human annotators, including 800 single-tool usage cases where each query can be resolved by invoking a single tool. We focus on these single-tool usage cases as seed data to avoid the complexity of multi-tool planning and interactions, which have been extensively studied in works like MetaTool \cite{huang2024metatool}. Given that this is the first comprehensive study on tool generalization, we start with single-tool usage cases.

% We begin our investigation with 800 single-tool usage cases to establish foundational principles of tool generalization before addressing multi-tool interactions, as explored in MetaTool \cite{huang2024metatool}.

% We utilize the UltraTool dataset, comprising 5,000 high-quality user queries refined by human annotators. For our initial investigation, we focus on 800 single-tool usage cases to establish foundational principles of tool generalization before exploring multi-tool interactions, as in MetaTool \cite{huang2024metatool}.
We build upon the UltraTool \cite{huang-etal-2024-planning-creation} dataset, comprising 5,000 high-quality user queries refined by human annotators. For our initial investigation, we focus on the 800 single-tool usage cases (\textbf{original data}). This scope allows us to establish fundamental principles of tool generalization before tackling multi-tool interactions explored in works like MetaTool \cite{huang2024metatool}. 
% addressing the additional complexity of multi-tool interactions, which have been extensively explored in works like MetaTool \cite{huang2024metatool}.

For each query and its gold tool description, We use GPT-4o to generate two ``weak tools'' which is ineffective in fully addressing the query and fails to provide an answer that perfectly matches the query. For example, for the query  ``What are the best restaurants in New York City, and what cuisines do they serve?'', a weak tool may return the top 10 restaurants but not provide cuisines information, whereas a strong tool could provide both the top 10 restaurants and their cuisines. In this case, the strong tool should be called. This process ensures consistent API formatting while varying tool capabilities.

% For each query and its associated gold tool description, we generate two distinct ``weak tool'' using GPT-4o. A weak tool, in our framework, is defined as a tool that:1)Maintains similar core functionality to the gold tool; 2)Contains fewer parameters or features; 3) Can only partially satisfy the requirements of the original query. For example, given a gold calendar tool that supports complex recurring event creation, a corresponding weak tool might only handle basic single-event scheduling. The generation process enforces consistent API formatting while systematically varying tool capabilities. 

% Weak tools are defined relative to a given query. For each query and its corresponding gold tool description, we prompt GPT-4o to generate a new tool $t^{'}$ that can partially solve the query, referred to as a ``weak tool''. The prompt includes constraints requiring the generated weak tool to have similar functionality to the gold tool but with fewer parameters, limiting its ability to fully resolve the query or produce entirely accurate results. For example, a weak tool may return incomplete answers or lack certain features compared to the gold tool. To ensure diversity, we generate two weak tools for each tool-query pair.


% The generated weak tools, along with their corresponding queries, form the foundation of our generalization scenarios. These tools are further organized into carefully constructed toolsets to simulate different generalization challenges, particularly the zero-to-one generalization scenario (detailed in Section xx).


\subsection{Query Generation}
% To fully utilize the generated weak tools, following the approach in MetaTool, we encourage GPT-4o to generate diverse queries for each weaker tool. Specifically, we input the original gold tool $t$ and its corresponding weak tool $t^{'}$into GPT-4 and ask it to generate a new query $q'$ that $q'$ can be better solved by the weak tool.
% but can only be partially solved by the original gold tool.
% For each tool and weaker tool pair, we generate 10 diverse queries to ensure sufficient variety in the dataset.
To capture diverse generalization scenarios, we generate tailored queries for each weak tool. For each tool pair (gold tool $t$ and weak tool $t^{'}$), we prompt GPT-4o to generates 10 queries meeting two criteria: 1) solvable by the weak tool $t^{'}$, and 2) only partially addressable by the gold tool $t$.
% \nb{Do the 10 queries have to meet both criteria or just one of them?} 
This approach forces the model to distinguish tools based on capabilities, and the selection of 10 queries per tool pair provides sufficient variation while maintaining dataset manageability.

% To create a comprehensive training dataset that captures various generalization scenarios, we generate diverse queries specifically tailored to each weak tool. For each tool pair (gold tool  $t$  and weak tool $t^{'}$), we prompt GPT-4o to generate 10 distinct queries that satisfy two key criteria: 1) The query can be effectively solved using the weak tool t'; 2) The query can only be partially addressed by the gold tool t. This deliberate query generation strategy creates scenarios where the model must learn to discriminate between tools based on their capabilities. The selection of 10 queries per tool pair provides sufficient variation while maintaining dataset manageability.

\subsection{Calling Information Generation}
Accurate tool invocation is crucial for training. We employ GPT-4o to generate calling information for each query-tool combination. For a given query $q$ and a tool $t$, GPT-4o generates the necessary parameter names and corresponding parameter values required to invoke $t$, ensuring the dataset reflects the realistic process of tool invocation.



% \section{synthetic Data Generation}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{data_generation.pdf}
%     \caption{Overview of our synthetic data generation pipeline}
%     \label{fig:data_generation}
% \end{figure}

% Current datasets \cite{huang-etal-2024-planning-creation,chen-etal-2024-eval,qin2024toolllm,tang2023toolalpaca}  provide only single annotated solutions for each query, limiting evaluation of LLMs' generalization capabilities across diverse tool combinations. To address this limitation, as illustrated in Figure~\ref{fig:data_generation}, we employ GPT-4o\footnote{https://platform.openai.com/docs/models#gpt-4o} to generate alternative $T_{gold}$ following existing tool formats.

% \subsection{Tool Generation}
% We select Ultra Tool as our initial toolset, leveraging its 5,000 human-optimized queries, specifically focusing on 800 single-tool-usage cases. This focus on single-tool scenarios, rather than complex multi-tool interactions involving intricate planning \cite{huang2024metatool,chen-etal-2024-eval}, provides a foundational starting point for our pioneering systematic study of tool generalization.

% Given that tool effectiveness is query-relative, we prompt GPT-4o with query-gold tool pairs, constraining it to generate new tools that can partially solve the query (detailed in Appendix A.1.1). These weaker tools typically feature fewer parameters while maintaining similar functionality, though they may not fully address the query or provide completely accurate results. For diversity, we generate two weaker tools per tool-query pair.

% \subsection{Query Generation}
% Following \citet{huang2024metatool}, we maximize utilization of generated weaker tools ($tool'$) by prompting GPT-4 to generate diverse query types. For each original tool and weaker tool pair, we generate 10 diverse queries ($query'$) that can be fully solved by $tool'$ while only partially addressed by the original tool.

% \subsection{Calling Information Generation}
% As shown in Figure 3b, we employ GPT-4 to generate tool calling information for each query-tool combination, specifying parameter values required for tool invocation. This process realistically reflects the operations LLMs must perform during tool utilization.

% \subsection{Data Quality Assessment}
% Despite leveraging GPT-4 for data generation, we conducted rigorous quality evaluation through expert annotation. We randomly sampled 200 instances containing original queries with their strong tools, generated weaker tools, and new queries generated for weaker tools ($query'$).

% Expert annotators (authors) evaluated each instance examining tool effectiveness across two scenarios. For original queries, strong tools should outperform weak tools in solution quality, while for generated queries ($query'$), weak tools should demonstrate superior performance. Additionally, we assessed tool uniqueness by examining whether generated tools maintain sufficient distinctiveness within the toolset.

% Our evaluation methodology, inspired by \cite{iskander-etal-2024-quality}, ensures the validity of query appropriateness, tool set construction, and generated outputs. Special attention was paid to identifying potential redundancies in generated tools, as GPT-4 may produce tools with overlapping functionalities.

% Results demonstrate exceptional data quality, with over 90\% validity across all fields. This significantly outperforms existing benchmarks such as ToolBench and ToolAlpaca, which exhibit parameter alignment errors in more than 33\% of instances, validating the effectiveness of our GPT-4-assisted data synthesis approach.
\section{GenTool: Training and Evaluation Framework}
% This section details how we utilize our constructed dataset for model training and evaluation across various generalization scenarios (Figure~\ref{fig:GenTool _framework}). First, we detail our training methodology for generalization paradigms: \textit{zero-to-one} and \textit{weak-to-strong} generalization. Next, we explain how we split the data to evaluate different generalization scenarios outlined in Section~\ref{sec2}. Finally, we introduce a two-stage output mechanism for enhanced tool selection.
This section presents our training and evaluation methodology as shown in Figure \ref{fig:GenTool _framework}. We first introduce two generalization dimensions: \textit{zero-to-one} and \textit{weak-to-strong} generalization. We then describe our test instances construction for evaluating the generalization scenarios outlined in Section\ref{sec2.2}. Finally, we propose a two-stage output mechanism to enhance tool selection accuracy.

% This section outlines our training and evaluation methodology (Figure \ref{fig:GenTool _framewosrk}), introducing the \textit{zero-to-one} and \textit{weak-to-strong} generalization dimensions, detailing our test instance construction for scenarios in Section \ref{sec2.2}, and proposing a two-stage output mechanism to improve tool selection accuracy.


%Generalization Scenario Generation
% \subsection{Training Strategy for Two Generalization Paradigms} \pei{this subsection seems should be included in Section 3 where it's all about data generation and construction, and section 4 is just about training. If you take this out from Section 3, then it won't be clear why we are generating bunch of new tools and queries withou knowing how we are using them. But I do see a disproportion issue, maybe a compromise is to refer to this subsection when you motivat and describe generation process in Section 3.}

\subsection{Training Strategy for Two Generalization Dimensions}
Our training addresses two real-world generalization dimensions: \textit{zero-to-one} and \textit{weak-to-strong}. We first describe our toolset generation process, which is fundamental to both frameworks.

% To simulate real-world scenarios, our training methodology addresses \textit{zero-to-one} and \textit{weak-to-strong} generalization problems. Toolsets play a crucial role in both scenarios, so we first describe how toolsets are generated.

\subsubsection{Toolset Generation}
For each query $q$, the model is provided with a toolset $T_s$ consisting of $k$ potential candidate tools, including the gold tool  $t$ required for the query. The construction process involves: 1) Computing embeddings for all tools in the dataset using the text-embedding-ada-003 model \cite{OpenAI2023EmbeddingModel}; 2) For each $(q, t)$ pair, retrieving the $k-1$ most similar tools based on cosine similarity for $T_\text{gold}$ to generate the $T_s$; 3) Ensuring diversity by excluding redundant tool-query pairs in which  tools already paired with the same query during retrieval. Considering the input length limitation of the model, we set \(k = 5\). To handle cases where no suitable tool exists in the toolset, we introduce \texttt{generate\_response} as an additional tool, which enables the model to generate direct responses when necessary.

% Details on this filtering process can be found in Appendix~\ref{appendix:toolset_generation}.
\subsubsection{Zero-to-One Generalization Training}
Given the dataset $\text{QT} = \{(q_1, t_1, q_1^{'}, t_1^{'}), \dots, (q_n, t_n, q_n^{'}, T_n^{'})\}_{i=1}^n$, we first sample a subset $\text{QT}_{\text{pure\_tr}} = \{(q_i, t_i, q_i^{'}, t_i^{'})\}_{i=1}^k$ to  construct training instances simulating \emph{zero-to-one} and \emph{weak-to-strong} transitions. The remaining portion of the dataset is defined as $\text{QT}_{\text{test}} = \{(q_j, T_j, q_j^{'}, T_j^{'})\}_{j=k+1}^{n}$ serves as our evaluation set for testing generalization under four distinct scenarios (See \ref{sec:scenario_generation}). 

For each $\{q, t, q^{'}, t^{'}\}_i \in \text{QT}_{\text{pure\_tr}}$, we constructed two training examples:

\begin{compactenum} 
    \item   $d_{\text{train}}^1 = ( T_s^{N}, q_1, \text{None}, y_{N} )$, where \( t = \text{None} \), it indicates that the toolsets do not contain a tool matching the given query. In this case, $y_{N} = \texttt{generate\_response()}$.
    \item $d_{\text{train}}^2 = ( T_s^1, q_1, t_1, y_1^1)$, where the model should select $t_1$.
\end{compactenum} 
By including both $d_{\text{train}}^1$ and $d_{\text{train}}^2$ in the training set, the model learns to distinguish between cases where no tools in the toolset can solve the query and cases where a valid tool exists, thereby simulating \textit{zero-to-one} generalization. 
Similarly, \( q' \) and \( t' \) follow similar construction patterns for \textit{zero-to-one} generalization training example.

\subsubsection{Weak-to-Strong Generalization Training}
As described in Section \ref{sec:data_generation}, for each $\{q, t, q^{'}, t^{'}\}_i \in \text{QT}_{\text{pure\_tr}}$, we have a \textit{weak tool} $t^{'}$ and a corresponding query $q'$. Using the synthesized data, we create:
\begin{compactenum} 
    \item $d_{\text{train}}^1 = ( T_s^{1'}, q_1, t_{1}^{'}, y_1^{1'}  )$, where the model needs to select \( t_{1}^{'} \), as only this weak tool is available.
    \item $d_{\text{train}}^2  = ( T_s^{1+}, q_1, t_1, y_1^1)$, where $T_s^{1+} = \{ t_1, t_{1}^{'}, \dots \}$, the model needs to select the strong tool $t_1$, with both weak and strong tools present.
\end{compactenum} 
By including $d_{\text{train}}^1$ and $d_{\text{train}}^2$ in the training set, the model learns to prioritize strong tools over weak ones, simulating \textit{weak-to-strong} generalization.


% In fact, \( q' \) and \( t' \) are also used to construct examples for \textit{zero-to-one} generalization. However, due to space limitations, this aspect has been omitted here, along with the subsequent roles of \( q' \) and \( T' \).

% \textcolor{red}{don't say space limitations but give another reason such as similarity, etc.) 



\subsection{Test Instances Construction}
% \pei{Section 4.1's title is also Generalization Scenario Generation which seems redundant with this, what are the differences between 4.1 and 4.2? Can we find a way to simply the section structure by merging? Btw you seem to be calling both Zero-to-One/Weak-to-Strong and the four categories as scenarios, this could cause confusion.}
\label{sec:scenario_generation}
Building upon the four scenarios defined in Section \ref{sec2.2}, we now describe how to construct instances from $\text{QT}_{\text{test}}$ for each scenario. Note that the \texttt{generate\_response} tool remains available across all scenarios.

% \paragraph{\textit{Seen\_Query\_Unseen\_Tool}}
% For a given query-tool cluster $\{ q_2, q_2^{'}, t_2, t_{2'} \} \in \text{QT}_{\text{test}}$, we first construct a training instance with no available tools:
% \[
% d_{\text{train}}^1: (T_s^{N}, q_2, \text{None}, y_N)
% \]
% where the model must use \texttt{generate\_response}. For testing, we introduce unseen tools and construct:
% \begin{align*}
% d_{test}^{1}: (T_s^{2}, q_2, t_2, y_2^2), \\
% d_{test}^{2}: (T_s^{2'}, q_2, t_{2}^{'}, y_2^{2'})
% \end{align*}

% Using another cluster $(q_3, q_3^{'}, t_3, t_{3}^{'})$, we create a training instance with only weak tools:
% \[
% d_{train}^{1}: (T_s^{3'}, q_3, t_{3}^{'}, y_3^{3'})
% \]
% and test with both weak and strong tools:
% \[
% d_{test}^{1}: (T_s^{3+}, q_3, t_3, y_3^3)
% \]


% Additionally, a subset of data sampled from $\text{QT}_{\text{test}}$ is incorporated into the training set to construct instances involving either \emph{seen queries} or \emph{seen tools}.

\paragraph{\textit{Seen\_Query\_Unseen\_Tool}}
\label{para:cluster_2}
For a given query-tool cluster $C_2 = \{ q_2, q_2^{'}, t_2, t_{2'} \} \in \text{QT}_{\text{test}}$, we first construct a training instance with no available tools:
\[
d_{\text{train}}^1: (T_s^{N}, q_2, \text{None}, y_N)
\]
where the model must use \texttt{generate\_response}. For testing, we introduce unseen tools and construct:
\begin{align*}
d_{test}^{1}: (T_s^{2}, q_2, t_2, y_2^2), \\
d_{test}^{2}: (T_s^{2'}, q_2, t_{2}^{'}, y_2^{2'})
\end{align*}

\label{para:cluster_3}
Additionally, we use another cluster $C_3 = (q_3, q_3^{'}, t_3, t_{3}^{'}) \in \text{QT}_{\text{test}}$ to create a training instance with only weak tools:
\[
d_{train}^{1}: (T_s^{3'}, q_3, t_{3}^{'}, y_3^{3'})
\]
and test with both weak and strong tools:
\[
d_{test}^{1}: (T_s^{3+}, q_3, t_3, y_3^3)
\]
In this case, the strong tool remains unseen in the training data, 

% \paragraph{\textit{Seen\_Query\_ Unseen\_Tool}}
% \label{seen_query_unseen_tool}
% % In the \textit{seen\_query\_ unseen\_tool} scenario, 
% For a given query-tool cluster $\{ q_2, q_2^{'}, t_2, t_{2'} \} \in  \text{QT}_{\text{test}}$, we first create a training example:
% \vspace{-0.5\baselineskip}
% \[
% \textstyle
% d_{\text{train}}^1:  (T_s^{N}, q_2, \text{None}, y_N)
% % d_{\text{train}}^2: \langle T_s^{1'}, q_1^{'},  \text{None}, y\rangle
% \]
% % \begin{align*}
% % d_{\text{train}}^1: \langle T_s^1, q_1, \text{None}, y\rangle
% % \end{align*}
% These examples require the model to rely solely on its parametric knowledge to generate responses using \texttt{generate\_response} tool since there are no useful tools in toolsets.
% For testing, we have new toolsets that include unseen tools. The constructed examples are:
% % \vspace{-0.5\baselineskip}
% \begin{align*}
% d_{test}^{1}: (T_s^{2}, q_2, t_2, y_2^2), \\
% % e_4 &= \langle \text{query}, \text{strong tool toolset} \rangle, \\
% % e_5 &= \langle \text{query}', \text{weak tool toolset} \rangle, \\
% d_{test}^{2}:  (T_s^{2'}, q_2, t_{2}^{'}, y_2^{2'})
% \end{align*}
% Additionally, we use another  cluster $(q_3, q_3^{'}, t_3, t_{3}^{'})$, to create a training example where the toolset contains only weak tools:
% \[
% d_{train}^{1}:  (T_s^{3'}, q_3, t_{3}^{'}, y_3^{3'})
% \]
% During testing, we expand the toolset to include both weak and strong tools:
% \[
% d_{test}^{1}:  (T_s^{3+}, q_3, t_3, y_3^3)
% \]
% In this case, the strong tool remains unseen in the training data, further testing the models ability to generalize to unknown tools.

\paragraph{Unseen\_Query\_Unseen\_Tool}
Using cluster $C_3$, we extend testing to unseen query and unseen tool:
\[
d_{test}^{1}: (T_s^{3}, q_3^{'}, t_3, y_{3'}^{3})
\]
where both $q_3^{'}$ and $t_3$ are absent from training data.


% \paragraph{Unseen\_Query\_Unseen\_Tool}
% We expand the test example from the tool cluster $ (q_3, q_3^{'}, t_3, t_{3}^{'})$ in section \ref{seen_query_unseen_tool}:
% \[
% d_{test}^{1}:  (T_s^{3}, q_3^{'}, t_3, y_{3'}^{3})
% \]
% where $T_s^{3}$ denotes the toolset containing weak tools $t_3$ for $q_3^{'}$.

% For testing, we replace the training queries with unseen queries and expand the toolsets to include strong tools:
% \[
% e_3 = \langle \text{query}', T_{\text{strong}} \rangle, \quad 
% e_4 = \langle \text{query}, T_{\text{strong}} \rangle,
% \]
% where $T_{\text{strong}}$ denotes the toolset containing strong tools.

% This scenario evaluates the model's ability to handle entirely new queries and tools that were absent during training.

\paragraph{\textit{Seen\_Query\_Seen\_Tool}}
\label{para:cluster_4}
Using cluster $C_4 = \{q_4, q_4^{'}, t_4, t_4^{'}\} \in \text{QT}_{\text{test}}$, we construct training instances:
\begin{align*}
d_{train}^{1}: (T_s^{4'}, q_4, t_{4}^{'}, y_4^{4'}), \\
d_{train}^{2}: (T_s^{4}, q_{4'}, t_4, y_{4'}^{4})
\end{align*}
and test with expanded toolsets:
\[
d_{test}^{1}: (T_s^{4+}, q_{4}, t_4, y_{4}^{4})
\]



% \paragraph{ \textit{Seen\_Query\_Seen\_Tool}}
% For training, we use a new tool cluster  $\langle q_4, q_4^{'}, t_4, t_4^{'} \rangle$ to construct examples such as:
% \begin{align*}
%     d_{train}^{1}:  (T_s^{4'}, q_4, t_{4}^{'}, y_4^{4'}), \\
% d_{train}^{2}:  (T_s^{4}, q_{4'}, t_4, y_{4'}^{4}) 
% \end{align*}
% During testing, we evaluate the model on expanded toolsets that include both weak and strong tools:
% \begin{align*}
% d_{test}^{2}:  (T_s^{4+}, q_{4}, t_4, y_{4}^{4})
% \end{align*}

% This scenario examines whether the model can prioritize tools it has observed during training when additional tools are introduced.

\paragraph{\textit{Unseen\_Query\_Seen\_Tool}}
Using cluster $C_3$, we construct test instances:
\begin{align*}
d_{test}^{1}: (T_s^{3'}, q_{3'}, t_{3}^{'}, y_{3'}^{3'}) \\
d_{test}^{2}: (T_s^{3'}, q_{3'}, \text{None}, y_N)
\end{align*}
where $q_{3'}$ is unseen while the tools have been exposed during training.

% \paragraph{\textit{Unseen\_Query\_Seen\_Tool} } We expand the test example from $ (q_3, q_3^{'}, t^3, t^{3'})$ tool cluster. 
% % To construct training examples, we pair queries with their corresponding tools as follows:
% % \[
% % e_1 = \langle \text{query}, T_{\text{weak}} \rangle, \quad 
% % e_2 = \langle \text{query}', T_{\text{weak}} \rangle,
% % \]
% % where $T_{\text{weak}}$ denotes a toolset containing weak tools observed during training.
% During testing, the model encounters unseen queries paired with both toolsets and the \texttt{generate\_response} tool. The test examples are constructed as:
% \begin{align*}
%     d_{test}^{1}:  (T_s^{3'}, q_{3'}, t_{3}^{'}, y_{3'}^{3'}) \\
%     d_{test}^{2}:  (T_s^{3'}, q_{3'}, \text{None}, y_N) 
% \end{align*}
% \[
% e_3 = \langle \text{query}', T_{\text{weak}} \rangle,
% \]
% \[
% e_4 = \langle \text{query}', \texttt{generate\_response} \rangle,
% \]
% \[
% e_5 = \langle \text{query}, T_{\text{weak}} \rangle,
% \]
% \[
% e_6 = \langle \text{query}, \texttt{generate\_response} \rangle.
% \]
% This scenario evaluates whether the model can generalize its knowledge of tools to previously unseen queries while maintaining accuracy in tool selection.

% from $\text{QT}_{\text{test}}$(Details for these scenarios, please see \ref{sec2.2}. The \texttt{generate\_response} tool remains available and is considered a seen tool across all scenarios.
% The \textit{zero-to-one} and \textit{weak-to-strong} generalization tasks serve to enhance the models ability to generalize during training. However, during testing, the model may face the four distinct generalization scenarios described in Section~\ref{sec2}. 
% In this section, we describe how to construct the four test generalization  cases from $\text{QT}_{\text{test}}$. It is important to note that \texttt{generate\_response} is universally available and always considered a \textit{seen tool}.

% \paragraph{\textit{Seen\_Query\_ Unseen\_Tool}}
% \label{seen_query_unseen_tool}
% % In the \textit{seen\_query\_ unseen\_tool} scenario, 
% For this type of cases, the query is observed during training, but the tool required to answer it during testing is unseen. For a given query-tool cluster $\{ q_2, q_2^{'}, t_2, t_{2'} \} \in  \text{QT}_{\text{test}}$, we first create a training example:
% \vspace{-0.5\baselineskip}
% \[
% \textstyle
% d_{\text{train}}^1:  (T_s^{N}, q_2, \text{None}, y_N)
% % d_{\text{train}}^2: \langle T_s^{1'}, q_1^{'},  \text{None}, y\rangle
% \]
% % \begin{align*}
% % d_{\text{train}}^1: \langle T_s^1, q_1, \text{None}, y\rangle
% % \end{align*}
% These examples require the model to rely solely on its parametric knowledge to generate responses using \texttt{generate\_response} tool since there are no useful tools in toolsets.

% For testing, we have new toolsets that include unseen tools. The constructed examples are:
% % \vspace{-0.5\baselineskip}
% \begin{align*}
% d_{test}^{1}: (T_s^{2}, q_2, t_2, y_2^2), \\
% % e_4 &= \langle \text{query}, \text{strong tool toolset} \rangle, \\
% % e_5 &= \langle \text{query}', \text{weak tool toolset} \rangle, \\
% d_{test}^{2}:  (T_s^{2'}, q_2, t_{2}^{'}, y_2^{2'})
% \end{align*}
% Additionally, we use another  cluster $(q_3, q_3^{'}, t_3, t_{3}^{'})$, to create a training example where the toolset contains only weak tools:
% \[
% d_{train}^{1}:  (T_s^{3'}, q_3, t_{3}^{'}, y_3^{3'})
% \]
% During testing, we expand the toolset to include both weak and strong tools:
% \[
% d_{test}^{1}:  (T_s^{3+}, q_3, t_3, y_3^3)
% \]
% In this case, the strong tool remains unseen in the training data, further testing the models ability to generalize to unknown tools.
% \paragraph{Unseen\_Query\_Unseen\_Tool}
% Neither the query nor the required tools are observed during training. We expand the test example from the tool cluster $ (q_3, q_3^{'}, t_3, t_{3}^{'})$ in section \ref{seen_query_unseen_tool}:
% \[
% d_{test}^{1}:  (T_s^{3}, q_3^{'}, t_3, y_{3'}^{3})
% \]
% where $T_s^{3}$ denotes the toolset containing weak tools $t_3$ for $q_3^{'}$.

% % For testing, we replace the training queries with unseen queries and expand the toolsets to include strong tools:
% % \[
% % e_3 = \langle \text{query}', T_{\text{strong}} \rangle, \quad 
% % e_4 = \langle \text{query}, T_{\text{strong}} \rangle,
% % \]
% % where $T_{\text{strong}}$ denotes the toolset containing strong tools.

% This scenario evaluates the model's ability to handle entirely novel queries and tools that were absent during training.

% \paragraph{ \textit{Seen\_Query\_Seen\_Tool}}
% Both the query and the required tool are present during training. For training, we use a new tool cluster  $\langle q_4, q_4^{'}, t_4, t_4^{'} \rangle$ to construct examples such as:
% \begin{align*}
%     d_{train}^{1}:  (T_s^{4'}, q_4, t_{4}^{'}, y_4^{4'}), \\
% d_{train}^{2}:  (T_s^{4}, q_{4'}, t_4, y_{4'}^{4}) 
% \end{align*}
% During testing, we evaluate the model on expanded toolsets that include both weak and strong tools:
% \begin{align*}
% d_{test}^{2}:  (T_s^{4+}, q_{4}, t_4, y_{4}^{4})
% \end{align*}

% This scenario examines whether the model can prioritize tools it has observed during training when additional tools are introduced.


% \paragraph{\textit{Unseen\_Query\_Seen\_Tool} } Finally, in this cases, the query is new, but the required tools are present in the training data. We expand the test example from $ (q_3, q_3^{'}, t^3, t^{3'})$ tool cluster. 
% % To construct training examples, we pair queries with their corresponding tools as follows:
% % \[
% % e_1 = \langle \text{query}, T_{\text{weak}} \rangle, \quad 
% % e_2 = \langle \text{query}', T_{\text{weak}} \rangle,
% % \]
% % where $T_{\text{weak}}$ denotes a toolset containing weak tools observed during training.
% During testing, the model encounters unseen queries paired with both toolsets and the \texttt{generate\_response} tool. The test examples are constructed as:
% \begin{align*}
%     d_{test}^{1}:  (T_s^{3'}, q_{3'}, t_{3}^{'}, y_{3'}^{3'}) \\
%     d_{test}^{2}:  (T_s^{3'}, q_{3'}, \text{None}, y_N) 
% \end{align*}
% % \[
% % e_3 = \langle \text{query}', T_{\text{weak}} \rangle,
% % \]
% % \[
% % e_4 = \langle \text{query}', \texttt{generate\_response} \rangle,
% % \]
% % \[
% % e_5 = \langle \text{query}, T_{\text{weak}} \rangle,
% % \]
% % \[
% % e_6 = \langle \text{query}, \texttt{generate\_response} \rangle.
% % \]

% This scenario evaluates whether the model can generalize its knowledge of tools to 

\subsection{Two-stage Output Training}
% \pei{This should be called two-stage output ot two subtasks for training.}
Efficient tool usage in real-world applications requires prioritizing tools in the toolset. Existing fine-tuning approaches often directly output the selected tool without considering their relative priority. For instance, even when the gold tool is included in the toolsets, \texttt{generate\_response} still take precedence over other irrelevant tools. To address this, we propose a two-stage process:
(1) Generating a ranked list of tools in the toolset.
(2) Providing detailed calling information for the highest-ranked tool.

Our ranking system considers three distinct cases:
\begin{compactenum}
    \item \textbf{No useful tools:} \texttt{generate\_response} ranks first, followed by other tools.
    \item \textbf{Single useful tool:} useful tool $>$ \texttt{generate\_response} $>$ others.
    \item \textbf{Multiple tools:} strong tool $>$ weak tool $>$ \texttt{generate\_response} $>$ other tools.
\end{compactenum}
The detailed prompt design for this task is available in Appendix~\ref{our_ex_app}.


% Our ranking system considers three distinct cases:
% 1. No useful tools: \texttt{generate\_response} ranks first, followed by other tools.
% 2. Single useful tool: useful tool $>$ \texttt{generate\_response} $>$ others.
% 3. Multiple tools: strong tool $>$ weak tool $>$ \texttt{generate\_response} $>$ other tools. The detailed prompt design for this task is available in Appendix~\ref{our_ex_app}.




% \section{Training Methodology}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{training_framework.pdf}
%     \caption{Overview of our training framework: (a) Construction of zero-to-one and weak-to-strong generalization scenarios; (b) Generation of test scenarios; (c) Tool ranking architecture}
%     \label{fig:training_framework}
% \end{figure}

% This section details our methodology for leveraging the constructed dataset to enhance model training and evaluate performance across diverse generalization scenarios. We first describe the formation of zero-to-one and weak-to-strong training sets, followed by the data partitioning strategy for different generalization scenarios as outlined in Section 3. Finally, we introduce a dual-task output framework: tool ranking list generation and detailed calling information for the highest-ranked tool.

% \subsection{Generalization Scenario Generation}
% Our training process addresses generalization challenges in tool invocation through two perspectives: zero-to-one generalization and weak-to-strong generalization. Given the crucial role of toolsets in both scenarios, we first elaborate on the toolset generation process.

% \subsubsection{Toolset Generation}
% In typical model prompts, we provide a query $q$ and a toolsets containing $n$ potential tool candidates. For a query $q$ with corresponding gold tool $t_{gold}$, we construct the toolsets by computing embeddings for all tool descriptions, denoted as $emb(t_i) \in \text{all\_tools}$. We utilize OpenAI's text-embedding-ada-003 model for embedding generation. For each $\langle q, t \rangle$ pair, we retrieve the top-$(n-1)$ tools based on cosine similarity with $t$. To address the one-to-many mapping between tools and queries in the original dataset, we establish query-specific retrieval sets, excluding duplicate tools (detailed in Appendix \ref{appendix:toolset_generation}).

% \subsubsection{Zero-to-One Generalization}
% For each $\langle \text{original\_tool}, \text{query}\rangle$ pair from Section 4, we generate a corresponding weak tool and $\text{query}'$. We construct two distinct training examples with different toolsets:

% \begin{equation}
% \begin{split}
% e_1 &= \langle \text{toolsets}_1, \text{query}\rangle \\
% \text{toolsets}_1 &= \{t_1, t_2, ..., t_n\} \text{ where } \text{original\_tool} \notin \text{toolsets}_1
% \end{split}
% \end{equation}

% \begin{equation}
% \begin{split}
% e_2 &= \langle \text{toolsets}_2, \text{query}\rangle \\
% \text{toolsets}_2 &= \{t_1, t_2, ..., t_{n-1}, \text{original\_tool}\}
% \end{split}
% \end{equation}

% The model's expected outputs for these examples are:
% \begin{equation}
% \text{output}(e_1) = \text{generate\_response} \text{ (no suitable tool available)}
% \end{equation}
% \begin{equation}
% \text{output}(e_2) = \text{original\_tool} \text{ (gold tool available)}
% \end{equation}

% This construction enables the model to learn zero-to-one generalization by exposing it to scenarios both with and without an appropriate tool for the same query.

% \subsubsection{Weak-to-Strong Generalization}
% For each $\langle \text{weak\_tool}, \text{query}\rangle$ pair, we construct two training examples with progressively enhanced toolsets:

% \begin{equation}
% \begin{split}
% e_1 &= \langle \text{toolsets}_1, \text{query}\rangle \\
% \text{toolsets}_1 &= \{t_1, t_2, ..., t_{n-1}, \text{weak\_tool}\}
% \end{split}
% \end{equation}

% \begin{equation}
% \begin{split}
% e_2 &= \langle \text{toolsets}_2, \text{query}\rangle \\
% \text{toolsets}_2 &= \{t_1, t_2, ..., t_{n-2}, \text{weak\_tool}, \text{strong\_tool}\}
% \end{split}
% \end{equation}

% The model's expected outputs demonstrate progression from weak to strong tool selection:
% \begin{equation}
% \text{output}(e_1) = \text{weak\_tool}
% \end{equation}
% \begin{equation}
% \text{output}(e_2) = \text{strong\_tool}
% \end{equation}

% This structured approach teaches the model to recognize and prefer stronger tools when available, facilitating weak-to-strong generalization learning.
% \section{Test Scenario Generation and Evaluation Framework}

% While zero-to-one and weak-to-strong generalization serve as training enhancement mechanisms, real-world applications require evaluation across diverse scenarios. Here, we describe our systematic approach to constructing test scenarios, implementing a ranking-based evaluation framework, and analyzing dataset statistics.

% \subsection{Construction of Test Scenarios}
% We treat $\text{generate\_response}$ as a consistently available tool across all scenarios, making it a permanently seen tool. For each scenario, we carefully construct training and test sets as follows:

% \subsubsection{Seen Query and Unseen Tool Scenario}
% For each tool cluster, we construct training examples with no specific tools:
% \begin{equation*}
% e_1 = \langle \text{query}, \emptyset \rangle, \quad e_2 = \langle \text{query}', \emptyset \rangle
% \end{equation*}

% The corresponding test set introduces previously unseen tools:
% \begin{equation*}
% \begin{split}
% \text{Test Set} = \{ &\langle \text{query}, \mathcal{T}_{\text{weak}} \rangle, \langle \text{query}, \mathcal{T}_{\text{strong}} \rangle, \\
% &\langle \text{query}', \mathcal{T}_{\text{weak}} \rangle, \langle \text{query}', \mathcal{T}_{\text{strong}} \rangle \}
% \end{split}
% \end{equation*}

% This construction ensures that both weak and strong tools in the test set are entirely novel to the model, allowing us to evaluate true tool generalization capabilities.

% \subsubsection{Seen Query and Seen Tool Scenario}
% We examine three distinct cases to evaluate the model's ability to handle familiar queries and tools in new combinations:

% Case 1: Training with weak tools and testing with generate_response:
% \begin{equation*}
% \begin{split}
% \text{Train: } &\langle \text{query}, \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &\langle \text{query}, \text{generate\_response} \rangle
% \end{split}
% \end{equation*}

% Case 2: Similar to Case 1 but using $\text{query}'$:
% \begin{equation*}
% \begin{split}
% \text{Train: } &\langle \text{query}', \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &\langle \text{query}', \text{generate\_response} \rangle
% \end{split}
% \end{equation*}

% Case 3: Training with weak tools and testing with combined toolsets:
% \begin{equation*}
% \begin{split}
% \text{Train: } &\{\langle \text{query}, \mathcal{T}_{\text{weak}} \rangle, \langle \text{query}', \mathcal{T}_{\text{weak}} \rangle\} \\
% \text{Test: } &\{\langle \text{query}, \mathcal{T}_{\text{weak}} \cup \mathcal{T}_{\text{strong}} \rangle\}
% \end{split}
% \end{equation*}

% \subsubsection{Unseen Query and Unseen Tool Scenario}
% For this challenging scenario, we construct two complementary configurations within each tool cluster:

% Configuration 1:
% \begin{equation*}
% \begin{split}
% \text{Train: } &e_1 = \langle \text{query}, \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &e_2 = \langle \text{query}', \mathcal{T}_{\text{strong}} \rangle
% \end{split}
% \end{equation*}

% Configuration 2:
% \begin{equation*}
% \begin{split}
% \text{Train: } &e_1 = \langle \text{query}', \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &e_2 = \langle \text{query}, \mathcal{T}_{\text{strong}} \rangle
% \end{split}
% \end{equation*}

% This bidirectional construction ensures comprehensive evaluation of the model's ability to handle both unseen queries and unseen tools simultaneously.

% \subsubsection{Unseen Query and Seen Tool Scenario}
% We examine two distinct configurations for evaluating the model's ability to apply known tools to novel queries:

% Configuration 1:
% \begin{equation*}
% \begin{split}
% \text{Train: } &e_1 = \langle \text{query}, \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &\{\langle \text{query}', \mathcal{T}_{\text{weak}} \rangle, \langle \text{query}', \text{generate\_response} \rangle\}
% \end{split}
% \end{equation*}

% Configuration 2:
% \begin{equation*}
% \begin{split}
% \text{Train: } &e_1 = \langle \text{query}', \mathcal{T}_{\text{weak}} \rangle \\
% \text{Test: } &\{\langle \text{query}, \mathcal{T}_{\text{weak}} \rangle, \langle \text{query}, \text{generate\_response} \rangle\}
% \end{split}
% \end{equation*}

% In both configurations, we evaluate the model's ability to leverage familiar tools ($\mathcal{T}_{\text{weak}}$ and $\text{generate\_response}$) while processing previously unseen queries. This setup tests the model's generalization capacity in applying known tool functionalities to novel query contexts.
% \subsection{Ranking-based Output Framework}
% \label{subsec:ranking}

% Existing tool learning approaches typically focus solely on tool selection, overlooking the inherent priority structure within toolsetss. This limitation is particularly evident when considering that even with a gold tool present, the model's native response capability ($\text{generate\_response()}$) should take precedence over irrelevant tools in the list.

% To address this limitation, we decompose the model's output into two distinct tasks:
% \begin{enumerate}
%     \item Generation of a prioritized tool ranking list
%     \item Production of detailed invocation specifications for the highest-ranked tool
% \end{enumerate}

% Given the diverse compositions of our toolsets, we establish three hierarchical ranking patterns:

% \begin{equation}
% \text{Ranking} = \begin{cases}
% \text{generate\_response} \succ \text{other\_tools} & \text{if } \mathcal{T}_{\text{useful}} = \emptyset \\[2ex]
% \text{useful\_tool} \succ \text{generate\_response} \succ \text{other\_tools} & \text{if } |\mathcal{T}_{\text{useful}}| = 1 \\[2ex]
% \text{strong\_tool} \succ \text{weak\_tool} \succ \text{generate\_response} \succ \text{other\_tools} & \text{if } \{\text{weak}, \text{strong}\} \subseteq \mathcal{T}
% \end{cases}
% \end{equation}

% where $\succ$ denotes priority ranking, $\mathcal{T}_{\text{useful}}$ represents the set of useful tools, and $\mathcal{T}$ denotes the complete toolset.

% Each case represents a distinct scenario:
% \begin{enumerate}
%     \item No useful tools available: Model's native response takes precedence
%     \item Single useful tool: The appropriate tool ranks highest, followed by native response
%     \item Both weak and strong tools present: Strict hierarchy from strong to weak tools, followed by native response
% \end{enumerate}

% The detailed prompting templates for implementing this ranking framework are provided in Appendix~\ref{appendix:ranking_templates}.
% \subsection{Dataset Statistics and Distribution}
% \label{subsec:dataset_stats}

% To rigorously evaluate GenTool 's effectiveness, we constructed a comprehensive synthetic dataset with the following composition:

% \begin{equation*}
% \mathcal{D} = \{\mathcal{D}_{\text{domain}}, \mathcal{D}_{\text{tool}}, \mathcal{D}_{\text{sample}}\}
% \end{equation*}

% where:
% \begin{itemize}
%     \item $|\mathcal{D}_{\text{domain}}| = X$ unique domains
%     \item $|\mathcal{D}_{\text{tool}}| = Y$ distinct tools
%     \item $|\mathcal{D}_{\text{sample}}| = Z$ total samples
% \end{itemize}

% We implemented a strategic split of tool clusters to ensure robust evaluation:

% \begin{equation}
% \text{Split}_{\text{cluster}} = \begin{cases}
% 67\%: & \text{Training Set} \\
% & \bullet \text{ Zero-to-One Generalization} \\
% & \bullet \text{ Weak-to-Strong Generalization} \\[2ex]
% 33\%: & \text{Evaluation Set} \\
% & \bullet \text{ Generalization Scenarios} \\
% & \bullet \text{ Corresponding Test Cases}
% \end{cases}
% \end{equation}

% A detailed breakdown of dataset statistics and distribution across different scenarios is presented in Table~\ref{tab:dataset_stats}.

% \begin{table}[h]
% \centering
% \caption{Dataset Statistics and Distribution}
% \label{tab:dataset_stats}
% \begin{tabular}{lcc}
% \toprule
% \textbf{Category} & \textbf{Training (67\%)} & \textbf{Testing (33\%)} \\
% \midrule
% Domains & $X_1$ & $X_2$ \\
% Tools & $Y_1$ & $Y_2$ \\
% Samples & $Z_1$ & $Z_2$ \\
% \bottomrule
% \end{tabular}
% \end{table}
\section{Experimental Setup and Dataset}
\subsection{Evaluation Metrics}
We evaluate model performance through a comprehensive assessment of tool invocation accuracy, comparing predicted outputs against ground truth across four critical dimensions: tool selection accuracy, parameter name identification, parameter value matching, and syntactic format correctness. For detailed explanations, please refer to App. \ref{app_eval}.
% Format correctness evaluates whether the model's output tool invocation is free from syntax or matching errors. Since LLMs may not always follow instructions perfectly, their output may contain syntax errors that prevent the tool invocation from being parsed and executed correctly. Therefore, we assess whether the output can be correctly parsed into the standard tool invocation JSON format. This metric is crucial as it determines whether the generated tool calls can be successfully parsed and executed.


\subsection{Baseline}
% We use the following models for our experiments: \textbf{Llama-3.1-8B-Instruct}, \textbf{Llama-3.2-1B-Instruct} \cite{dubey2024llama3herdmodels}, \textbf{Mistral-Instruct-7B-v0.2} \cite{jiang2023mistral7b}, and \textbf{Phi-3.5-3B} \cite{abdin2024phi3technicalreporthighly}. These models are used to thoroughly explore the performance of different model frameworks and model sizes on GenTool . We compare \textbf{tuning-free} methods, using closed-source LLMs like GPT-3.5, GPT-4o-mini, GPT-4o for zero-shot evaluation, as well as ToolLlama \cite{qin2024toolllm} and GPT4Tools \cite{yang2023gpttools}, which are open-source LLMs fine-tuned on a synthetic tool dataset. We also compare \textbf{tuning-based} methods, \textbf{Original} means that we fine-tune LLMs on seed data to demonstrate the effectiveness of our synthetic data generation method(See \ref{section3.1}); \textbf{Half-Sample} uses only one example from each original pair (e.g., selecting either the ``zero'' or ``one'' example from zero-to-one pairs) for fine-tuning. This baseline helps verify whether our paired construction, which simulates generalization through example transitions, contributes to better generalization capabilities compared to training with isolated examples. For more experimental details, please refer to App.\ref{our_ex_app}.

We evaluate our approach using four models: \textbf{Llama-3.1-8B-Instruct}, \textbf{Llama-3.2-1B-Instruct} \cite{dubey2024llama3herdmodels}, \textbf{Mistral-Instruct-7B-v0.3} \cite{jiang2023mistral7b}, and \textbf{Phi-3.5-3B} \cite{abdin2024phi3technicalreporthighly}. For simplicity, we omitted the name ``instruct'' in some experimental results. We compare two types of methods: \textbf{tuning-free} approaches using GPT-3.5, GPT-4o-mini, GPT-4o \cite{openai2024gpt4technicalreport}, ToolLlama \cite{qin2024toolllm}, and GPT4Tools \cite{yang2023gpttools}; and \textbf{tuning-based} methods including \textbf{Original:fine-tuning on seed data} (cf. Section \ref{section3.1}) and \textbf{Half-Sample: using single examples from paired data} (e.g., selecting either the ``zero'' or ``one'' example from zero-to-one pairs). For model implementation details and data statistics, please see App.\ref{our_ex_app}.

% including balanced examples from zero-to-one and weak-to-strong pairs, to validate our pair construction methodology

% \subsection{Implementation Details}

% \section{Data analysis}
% \section{Training methodology }
% \subsection{Generalization scenario generation}
% show how to construct zero-one and weaker-stronger
% \subsection{how to generate 4 scenarios from our construction}
% \subsection{ranking}


\section{Experimental Results}
% \begin{table*}[h!]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Method & Tool Selection & Parameter Name  & Parameter Value & Format accuracy \\ \hline
% gpt3.5\_zero\_shot & 48.61 & 70.34 & 59.25 & 99.3 \\ \hline
% gpt4o\_mini\_zero\_shot & 58.8 & 78.86 & 69.05 & 99.93 \\ \hline
% gpt4o\_zero\_shot & 75.87 & 72.0 & 64.09 & 99.81 \\ \hline
% gpt4o\_tool & 22.34 & 28.19 & 22.47 & 99.53 \\ \hline
% tool\_llama & 17.73 & 24.59 & 18.43 & 99.38 \\ \hline
% llama\_1b\_zero\_shot & 29.02 & 0.08 & 0.07 & 40.12 \\ \hline
% llama\_1b\_ft\_with\_original & 31.63 & 35.67 & 31.12 & 99.88 \\ \hline
% llama\_1b\_ft\_with\_strong & 61.52 & 58.48 & 52.19 & 99.92 \\ \hline
% llama\_1b\_ft\_ours\_rank & 86.31 & 76.92 & 68.81 & 99.88 \\ \hline


% phi\_3b\_zero\_shot & 24.15 & 30.06 & 19.14 & 68.19 \\ \hline
% phi\_3b\_ft\_with\_original & 55.49 & 58.58 & 52.63 & 99.9 \\ \hline
% phi\_3b\_ft\_with\_strong & 80.77 & 75.23 & 68.22 & 99.85 \\ \hline
% phi\_3b\_ft\_ours\_rank & 87.37 & 83.48 & 75.62 & 99.83 \\ \hline

% mistral\_7b\_zero\_shot & 45.33 & 66.27 & 56.07 & 97.89 \\ \hline
% mistral\_7b\_ft\_with\_original & 18.28 & 19.71 & 17.18 & 99.25 \\ \hline
% mistral\_7b\_ft\_with\_strong & 51.0 & 49.74 & 44.52 & 99.78 \\ \hline
% mistral\_7b\_ft\_ours\_rank & 80.28 & 80.3 & 72.43 & 99.68 \\ \hline

% llama\_8b\_zero\_shot & 40.52 & 60.71 & 50.27 & 99.96 \\ \hline
% llama\_8b\_ft\_with\_original & 48.09 & 42.67 & 38.03 & 99.82 \\ \hline
% llama\_8b\_ft\_with\_strong & 60.47 & 73.39 & 66.53 & 99.72 \\ \hline
% llama\_8b\_ft\_ours\_rank & 90.15 & 86.05 & 77.64 & 99.45 \\ \hline
% \end{tabular}
% \caption{Evaluation Results for Models}
% \label{tab:eval_results}
% \end{table*}
\begin{table}[!th]
\centering
\tiny
% \renewcommand{\arraystretch}{1.2}
% \setlength{\tabcolsep}{6pt}
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c|}{\textbf{Parameter}} & \multirow{2}{*}{\textbf{Format Accuracy}} \\ \cline{3-4}&& \textbf{Name}& \textbf{Value}       &\\ \hline

% \textbf{Method} & \textbf{Tool Selection} & \textbf{Parameter Name} & \textbf{Parameter Value} & \textbf{Format Accuracy} \\ \hline
\rowcolor[gray]{0.9} % 
\multicolumn{5}{c}{\textbf{Baselines}} \\ \hline
GPT-3.5       & 48.61 & 70.34 & 59.25 & 99.30 \\ \hline
GPT-4o Mini    & 58.80 & 78.86 & 69.05 & 99.93 \\ \hline
GPT-4o         & 75.87 & 72.00 & 64.09 & 99.81 \\ \hline
GPT4Tools              & 22.34 & 28.19 & 22.47 & 99.53 \\ \hline
ToolLLaMA                 & 17.73 & 24.59 & 18.43 & 99.38 \\ \hline
\rowcolor[gray]{0.9} 
\multicolumn{5}{c}{\textbf{Llama-3.2-1B-Instruct }} \\ \hline
Zero-shot        & 29.02 &  0.08 &  0.07 & 40.12 \\ \hline
Original & 31.63 & 35.67 & 31.12 & 99.88 \\ \hline
Half-Sample  & 61.52 & 58.48 & 52.19 & 99.92 \\ \hline
\textbf{GenTool }   & 86.31 & 76.92 & 68.81 & 99.88 \\ \hline
\rowcolor[gray]{0.9} % 
\multicolumn{5}{c}{\textbf{Phi-3.5-3B-Instruct }} \\ \hline
Zero-shot          & 24.15 & 30.06 & 19.14 & 68.19 \\ \hline
Original   & 55.49 & 58.58 & 52.63 & 99.90 \\ \hline
Half-Sample    & 80.77 & 75.23 & 68.22 & 99.85 \\ \hline
\textbf{GenTool }    & 87.37 & 83.48 & 75.62 & 99.83 \\ \hline
\rowcolor[gray]{0.9} % 
\multicolumn{5}{c}{\textbf{Mistral-Instruct-7B-v0.3 }} \\ \hline
Zero-shot      & 45.33 & 66.27 & 56.07 & 97.89 \\ \hline
Original & 18.28 & 19.71 & 17.18 & 99.25 \\ \hline
Half-Sample   & 51.00 & 49.74 & 44.52 & 99.78 \\ \hline
\textbf{GenTool }   & 80.28 & 80.30 & 72.43 & 99.68 \\ \hline
\rowcolor[gray]{0.9} % 
\multicolumn{5}{c}{\textbf{LLaMA-3.1-8B-Instruct }} \\ \hline
Zero-shot        & 40.52 & 60.71 & 50.27 & \textbf{99.96} \\ \hline
Original& 48.09 & 42.67 & 38.03 & 99.82 \\ \hline
Half-Sample   & 60.47 & 73.39 & 66.53 & 99.72 \\ \hline
\textbf{GenTool }    & \textbf{90.15} & \textbf{86.05} & \textbf{77.64} & 99.45 \\ \hline
\end{tabular}
\caption{Overall tool-calling results. All baseline models are evaluated in a zero-shot setting. For different LLMs, the \textbf{original} and \textbf{half-sample} settings represent results obtained through fine-tuning.}
\label{tab:main_res}
\vspace{-0.5cm}
\end{table}

% Comprehensive evaluation results (Table \ref{tab:main_res}) show GenTool's superior performance across all metrics. Fine-tuned models consistently outperform GPT-based baselines, with notable gains in \textit{Tool Name Selection}. LLaMA3.1-8B fine-tuned with GenTool surpasses GPT-4o by 14.28\%, highlighting the effectiveness of our structured training in capturing tool selection patterns.

\subsection{Overall Performance}
Comprehensive evaluation results (Table \ref{tab:main_res})
% \nb{Where is the half-sample setting in Table 1?}
show GenTool 's superior performance across all metrics. Models fine-tuned with GenTool consistently outperform GPT-based baselines, with particularly notable improvements in \textit{Tool Name Selection}. The LLaMA3.1-8B model fine-tuned with GenTool outperforms GPT-4o by 14.28\%, demonstrating GenTool 's strong potential in selecting the appropriate tool. This substantial gain suggests that our structured training approach better captures the underlying patterns of tool selection.

For \textit{tuning-free} methods, distinct performance patterns across model scales. Smaller models (LLama-3.1-1B, Phi-3.5-3B) struggle with instruction following, while models like GPT4Tools and ToolLLaMA achieve high format accuracy but lower performance than zero-shot baselines, probably due to domain shifts between their training datasets and UltraTool specific requirements, highlighting the challenges in cross-domain tool generalization.

For \textit{tuning-based} approaches, \textit{Half-Sample} outperforms \textit{Original}, highlighting the benefits of our training data. In addition, performance shows no consistent correlation with model size, indicating it might not be critical for tool generalization.


% For \textit{tuning-based} approaches, Half-Sample outperforms Original, showing the benefit of our training data. However, model size shows no consistent correlation with performance, suggesting it may not be critical for tool learning generalization.

% For \textit{tuning-free baselines}, we observe distinct performance patterns across model scales. Smaller models (LLama-3.1-1B, Phi-3.5-3B) exhibit difficulty in following instructions and generating correctly formatted outputs in zero-shot settings. Surprisingly, GPT4Tools and ToolLLaMA, despite having high format accuracy, perform worse than zero-shot models. This could be because of a domain distribution shifts between their training datasets and UltraTool specific requirements, highlighting the challenges in cross-domain tool generalization.

% For \textit{tuning-based baselines}, models fine-tuned on half of the data outperform those fine-tuned on the original data, indicating that more training data improves performance. However, within the same setting, larger model sizes do not consistently yield better performance, suggesting that model size might not play a critical role in generalization for tool learning.
\begin{figure*}[!tpb]
    \centering
    \includegraphics[width=1\linewidth]{images/scen_4.png}
    \caption{Detailed results for different test set scenarios. GenTool consistently outperforms all baselines. %We introduce MiCEval, a framework proposed to evaluate the quality of MCoT answer.
    }
    \label{scen4}
    
    \vspace{-0.4cm}
\end{figure*}

\subsection{Performance across Different Generalization Scenarios}
We examine GenTool 's performance across different generalization scenarios, excluding 
\textit{tuning-free baselines} as they treat all inputs as unseen. Figure \ref{scen4} shows that GenTool excels particularly with seen queries. For the \textit{Tool Selection} task, our GenTool demonstrates substantial improvements over the strongest baseline, achieving average gains of 18.4\% in the \textit{Seen\_Query\_Unseen\_Tool} and 34.1\% \textit{Seen\_Query\_Seen\_Tool} scenarios. 

The framework maintains strong performance even in challenging unseen query scenarios.
In the \textit{Unseen\_Query\_Unseen\_Tool} and \textit{Unseen\_Query\_Seen\_Tool} scenarios, GenTool  consistently outperforms all baselines. Our approach outperforms the best baseline (half-sample) by 2.8\% and 3.2\% respectively.  This performance suggests successful mitigation of overfitting to seen queries, a common challenge in tool learning systems.

% These results highlight while significantly improving seen query performance, GenTool  is able to enhance unseen query generalization by alleviating potential over-reliance on seen queries.

\begin{table}[!t]
\scriptsize
\centering
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l|c|c|c}
\hline
\multirow{2}{*}{\textbf{Model}}                 & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c}{\textbf{Parameter}} \\ \cline{3-4}
                                &                         & \textbf{Name}        & \textbf{Value}        \\ \hline
LLama-3.1-8B (Ours)                & \textbf{90.15}                   & \textbf{86.05}               & \textbf{77.64}                \\ 
-- w/o Rank                    & 83.68                   & 84.58               & 76.32                \\ \hline
Mistral-7B (Ours)              & \textbf{80.28}                   & \textbf{80.30}               & \textbf{72.43}                \\ 
-- w/o Rank                    & 69.39                   & 71.49               & 63.98                \\ \hline
\end{tabular}
\caption{Ablation Study Results: Impact of Removing the Tool Ranking Task During Fine-Tuning. We instruct the model to directly output the most suitable tool-calling information.}
\label{tab:ablation_study_inline_no_parse}
\vspace{-0.3cm}
\end{table}
\begin{figure}[!tpb]
    \centering
    \includegraphics[width=0.8\linewidth]{images/tool_embedding.pdf}
    \caption{
    The relationship between the number of training examples related to the test set's gold tool and the test set's accuracy. The \textcolor{blue}{
    blue} line represents seen tools, while the \textcolor{violet}{purple} line denotes unseen tools.
    }
   
    \label{embedding}
    \vspace{-0.6cm}
\end{figure}
\subsection{Ablation Study}
To validate the effectiveness of our ranking mechanism, we conduct systematic ablation experiments examining its contribution to overall performance. Table \ref{tab:ablation_study_inline_no_parse} reveals that removing the ranking component leads to consistent performance degradation across all metrics. The impact is particularly obvious in Tool Selection accuracy, where LLama-3.1-8B and Mistral-7B models show significant decreases of 6.47\% and 10.89\%, respectively.

% In Table \ref{tab:ablation_study_inline_no_parse}, when LLMs are fine-tuned using only standard tool call information as output, performance decreases across all metrics. Notably, \textit{Tool Selection} sees the largest drop, with LLama-3.1-8B and Mistral-7B models experiencing reductions of 6.47\% and 10.89\%, respectively. These findings validate our theoretical motivation for incorporating the ranking mechanism: by learning to prioritize tools effectively, the model demonstrates enhanced discrimination in tool selection.

\section{Empirical Analysis}
In this section, we conduct comprehensive experiments to understand the factors influencing tool generalization and the limitations of models. Our analysis focuses on two key aspects: examining how different training data compositions affect model performance and investigating the fundamental characteristics of model capabilities. 

\subsection{Impact of Related Examples Quantity}
To investigate how related training examples influence model performance, we analyze the correlation between the number of related examples and test performance. We hypothesize that performance improves with increased exposure to related tools in the training set. Using the text-embedding-3-large model \cite{OpenAI2023EmbeddingModel}, we calculate semantic similarities for embeddings between gold tools in the training and test sets. For a test tool $i$, if a training tool $j$ has a cosine similarity greater than 0.5, $j$ is considered related to $i$.

% To understand the underlying mechanism of GenTool 's improved tool generalization, we examine the relationship between training examples and model performance. Specifically, we hypothesize that the more examples in the training set related to a test tool, the better its performance in the test set. Using the text-embedding-3-large model \cite{OpenAI2023EmbeddingModel}, we calculate semantic similarities for embeddings between gold tools in the training and test sets. For a test tool $i$, if a training tool $j$ has a cosine similarity greater than 0.5, $j$ is considered related to $i$.

% The results are categorized into Seen Tools and Unseen Tools, as illustrated in Figure \ref{embedding}. For\textit{Unseen Tools}, we observe a non-monotonic trend: initial performance declines followed by subsequent improvement as related examples increase. This pattern might suggest an interaction between memorization and generalizationmodels initially overfit to seen patterns before developing more robust generalization capabilities through increased exposure to diverse examples. In contrast, \textit{Seen Tools} exhibit consistent improvement with additional related examples, indicating effective knowledge transfer within similar tool categories.

Figure \ref{embedding} reveals distinct trends for Seen and Unseen Tools. For Unseen Tools, we observe a non-monotonic trend: performance initially declines but improves as related examples increase, suggesting an interaction between memorization and generalization.
% \nb{Should we comment on when having further samples, the performance on Unseen Tool would drop again?}
In contrast, Seen Tools demonstrate consistent improvement with additional related examples, indicating effective knowledge transfer within similar tool categories.


% For all fine-tuning setups, zero-shot models perform well initially. However, for \textit{Unseen Tools}, accuracy decreases before increasing as the number of related examples grows. For \textit{Seen Tools}, accuracy consistently increases with the number of related examples. We hypothesize that for \textit{Unseen Tools}, an initial increase in related examples may result in over-memorization of seen tools, negatively impacting generalization. However, as training diversity grows, this effect is mitigated.

\subsection{Contribution of Different Pair Types}
\begin{table}[t!]
\tiny
\centering
\begin{tabular}{l|c|c|c}
\hline
\multirow{2}{*}{\textbf{Model}}                 & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c}{\textbf{Parameter}} \\ \cline{3-4}
                                &                         & \textbf{Name}        & \textbf{Value}        \\ \hline
GenTool  & \textbf{90.15} & \textbf{86.05} & \textbf{77.64} \\ \hline
Weak-to-Strong & 63.36 & 82.72 & 73.65  \\ \hline
Zero-to-One & 71.01 & 62.37 & 55.87  \\ \hline
\end{tabular}
\caption{Results of the LLaMA-3.1-8B-Instruct model trained in a single generalization simulation scenario. \textbf{Weak-to-strong} indicates the model is trained only on the weak-to-strong-related dataset, while \textbf{zero-to-one} indicates the model is trained only on the zero-to-one-related dataset.}
\label{tab:transfer}
\vspace{-0.3cm}
\end{table}

We evaluate the individual impact of \textit{Zero-to-One} and \textit{Weak-to-Strong} pairs on model generalization by fine-tuning models exclusively on either Zero-to-One or Weak-to-Strong pairs. Table \ref{tab:transfer} shows that \textit{weak-to-strong} pairs enhance parameter name accuracy but lead to lower tool selection accuracy, excelling when tools are available but struggling to identify when no tool is applicable. In contrast, fine-tuned models with \textit{zero-one-one} pairs exhibit higher tool selection accuracy, indicating better capability in identifying scenarios where tool usage is inappropriate. This suggests both pair types are essential for comprehensive tool generalization.

\subsection{Training with Test-Related Examples Only}
To examine whether models understand tools during learning, we train models exclusively on test-relevant tools and queries.
% \nb{What happens if we use both training and test data to train the models?} 
Table \ref{tab:train_test}
% \nb{Would be good to give the detailed results of each of the four categories} 
reveals significant performance deterioration across all models, especially fine-tuned models. For instance, Mistral-7B's accuracy in \textit{Tool Selection} drops from 45.33\% to 11.56\%. These results indicate that models may not fully understand tools or queries, and slight variations in tool usage scenarios lead to sharp accuracy declines. This aligns with observations in RoTtuning \cite{ye-etal-2024-rotbench}.


% Table \ref{tab:transfer}  shows that under the \textit{weak-to-strong} simulation setting, the fine-tuned models achieve higher parameter name accuracy but lower tool selection accuracy. Specifically, \textit{weak-to-strong} pairs help the model learn to select tools effectively when an available tool exists. However, the model is prone to select incorrectly when no tool is applicable. In contrast, fine-tuned models with \textit{zero-one-one} pairs exhibit higher tool selection accuracy, indicating better capability in identifying scenarios where tool usage is inappropriate. This suggests both pair types are essential for comprehensive tool generalization.

 % Conversely, \textit{zero-to-one} pairs improve tool selection accuracy, particularly in identifying inappropriate tool usage scenarios. 
% Table shows that \textit{Weak-to-Strong} pairs improve generalization for \textit{Unseen Tools}, with an average gain of XX points, while \textit{Zero-to-One} pairs are more effective for \textit{Seen Tools}. This indicates that both types of pairs are indispensable for improving tool generalization.



% We investigate the impact of training data volume by evaluating model performance across different data percentages (10\% to 100\%). Figure \ref{data_scaling} reveals several key insights: 1)significant performance gains occur when training data increases from 10\% to 20\% and from 40\% to 50\%; 2)Performance generally improves with more data, but the gains vary across generalization scenarios. The greatest benefit of more training data is observed in \textit{seen\_query\_seen\_tool}, while the growth in \textit{unseen\_query\_unseen\_tool} slows down as the training data increases. Notably, models achieve 78.61\% tool selection accuracy with just 30\% of training data, comparable to the Half-Sample baseline. This demonstrates that GenTool's effectiveness stems not merely from data quantity but from our structured approach to generalization pair construction.
\begin{table}[t!]
\scriptsize
\centering
\definecolor{specialblue}{RGB}{29, 66, 168}
\definecolor{specialgreen}{RGB}{27, 143, 34}

\begin{tabular}{l|c|c|c}
\hline
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c}{\textbf{Parameter}}  \\ \cline{3-4} &  & \textbf{Name}& \textbf{Value}   \\ \hline
Llama-3.2 & 16.32 {\color{specialblue}(-69.99)} & 18.55 {\color{specialblue}(-58.37)} & 16.43 {\color{specialblue}(-52.38)}  \\ \hline
Phi-3.5-3B & \textbf{27.99} {\color{specialblue}(-59.38)} & \textbf{35.91} {\color{specialblue}(-47.57)} & \textbf{32.07} {\color{specialblue}(-43.55)} \\ \hline
Mistral-7B & 11.56 {\color{specialblue}(-68.72)} & 16.36 {\color{specialblue}(-63.94)} & 14.32 {\color{specialblue}(-58.11)} \\ \hline
Llama-3.1-8B & 17.11 {\color{specialblue}(-73.04)} & 24.57 {\color{specialblue}(-61.48)} & 21.51 {\color{specialblue}(-56.13)} \\ \hline
\end{tabular}
\caption{Performance comparison of models trained exclusively on test-relevant examples. {\color{specialgreen}{green}} and {\color{specialblue}{blue}} values represent the differences compared to the GenTool method respectively.}
\label{tab:train_test}
% \vspace{-0.4cm}
\end{table}
\subsection{Top Ranking Performance}
\begin{table}[!t]
\footnotesize
\centering
\begin{tabular}{l|c|c}
\hline
\textbf{Model} & (1) & (2) \\ \hline
Llama-3.2-1B-Instruct & 99.81 & 78.45 \\ \hline

Phi-3.5-3B & \textbf{99.82} & 78.56 \\ \hline
Mistral-Instruct-7B-v0.3 & 99.53 & 71.48 \\ \hline

Llama-3.1-8B-Instruct & 99.41 & \textbf{80.09} \\ \hline
\end{tabular}
\caption{Tool ranking analysis: (1) consistency between top-ranked in the first task and re-predicted tools in the second task; (2) accuracy of useful\/irrelevant tool ranking relative to \texttt{generate\_response}.}
\label{tab:rank_analysis}
\vspace{-0.4cm}
\end{table}

Our ranking-based mechanism requires models to rank tools and generate detailed calls for the top-ranked option. As shown in Table \ref{tab:rank_analysis}, fine-tuned models achieve nearly 100\% accuracy in executing
% \nb{Not fully follow here. Is it about ranking or execution? Or both?}
the top-ranked tool, but struggle to distinguish useful tools from irrelevant ones, with consistency below 80\%. This highlights a challenge in filtering irrelevant tools despite strong \textit{Tool Selection} accuracy.


% Our ranking-based output mechanism requires models to rank available tools and generate detailed calling information for the top-ranked option. Table \ref{tab:rank_analysis} shows that all fine-tuned models achieve nearly 100\% accuracy in executing the top-ranked tool, demonstrating that the models follow instructions effectively. However, consistency in distinguishing useful tools from irrelevant ones is below 80\%, significantly lower than their \textit{Tool Selection} accuracy. This indicates that while models can identify the best tool, distinguishing between good and bad tools remains challenging.








% \subsubsection{In-Context Learning Capability}
% We evaluate GenTool 's effectiveness in in-context learning by providing two demonstration pairs(one \textit{Zero-to-One} and one \textit{Weak-to-Strong}) for each test example. Due to input length constraints, this analysis focuses on GPT-series models. The half-sample baseline is also included here. 

% Results in Table \ref{tab:in_context} demonstrates GenTool 's consistent improvements. GPT4 achieves the highest accuracy (84.27\%) in \textit{Tool Name Selection}. And GPT3.5 and GPT4o show significant gains over half-data baseline. Although GPT4o-mini underperforms in \textit{Tool Name Selection}, it surpasses the baseline in \textit{Param Name Matching} and \textit{Param Value Matching} by 3.15\% and 1.97\%, respectively. These results demonstrate GenTool 's effectiveness in in-context learning.


\label{analysis_train_only}
% \begin{table}[h!]
% \tiny
% \centering
% \definecolor{specialblue}{RGB}{29, 66, 168}
% \definecolor{specialgreen}{RGB}{27, 143, 34}

% \begin{tabular}{l|c|c|c|c}
% \hline
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c|}{\textbf{Parameter}} & \multirow{2}{*}{\textbf{Format Accuracy}} \\ \cline{3-4} &  & \textbf{Name}& \textbf{Value} &   \\ \hline
% Llama-3.2 & 16.32 {\color{specialblue}(-69.99)} & 18.55 {\color{specialblue}(-58.37)} & 16.43 {\color{specialblue}(-52.38)} & 99.97 {\color{specialgreen}(+0.09)} \\ \hline
% Phi-3.5-3B & 27.99 {\color{specialblue}(-59.38)} & 35.91 {\color{specialblue}(-47.57)} & 32.07 {\color{specialblue}(-43.55)} & 100.0 {\color{specialgreen}(+0.17)} \\ \hline
% Mistral-7B & 11.56 {\color{specialblue}(-68.72)} & 16.36 {\color{specialblue}(-63.94)} & 14.32 {\color{specialblue}(-58.11)} & 99.50 {\color{specialblue}(-0.18)} \\ \hline
% Llama-3.1-8B & 17.11 {\color{specialblue}(-73.04)} & 24.57 {\color{specialblue}(-61.48)} & 21.51 {\color{specialblue}(-56.13)} & 99.86 {\color{specialgreen}(+0.41)} \\ \hline
% \end{tabular}
% \caption{Results for LLMs fine-tuned solely on training examples that are directly relevant to the test examples. The {\color{specialgreen}{green}} and {\color{specialblue}{blue}} values represent the differences compared to the GenTool  method, respectively.}
% \label{tab:train_test}
% \end{table}


\section{Related Work}
\subsection{Tool Learning with LLMs}  
Recent studies have enhanced large language models (LLMs) with tools such as code-related APIs \cite{patil2023gorilla}, mathematical functions \cite{gou2024critic}, and feedback mechanisms to refine outputs and reduce hallucinations \cite{dhuliawala-etal-2024-chain}. For example, \citet{mekala-etal-2024-toolverifier} emphasizes tool-based verification for reliable answers, while \citet{wang-etal-2024-llms-imaginarium} uses simulated errors to guide tool learning. \citet{Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024} propose progressive learning to improve tool usage across tasks. However, these approaches lack a comprehensive evaluation of tool generalization. Our work addresses this by defining the tool generalization problem and incorporating diverse scenarios into training.  


% \subsection{Tool Learning with LLMs}
% Recent work has explored enhancing large language models (LLMs) with various tools to solve diverse tasks \cite{schick2023toolformer,ruan2023tptulargelanguagemodelbased,shen2023hugginggptsolvingaitasks,wang2024mllmtoolmultimodallargelanguage}. These tools include code-related APIs \cite{patil2023gorilla}, mathematical functions \cite{gou2024critic}, and more. LLMs not only leverage these tools to better answer questions but also use tools as feedback to refine their outputs \cite{shridhar-etal-2024-art}, thereby reducing hallucinations in model generation \cite{dhuliawala-etal-2024-chain}. 

% For instance, \citet{mekala-etal-2024-toolverifier} focuses on verifying different output options when using tools to select the most reliable answer. In contrast, our ranking method trains the model to prioritize the use of different tools during output generation. Similarly, \citet{wang-etal-2024-llms-imaginarium} introduces simulated tool usage errors to guide LLMs in learning from feedback and avoiding mistakes, while \citet{Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024} creates a progressive learning environment to improve tool usage across tasks of varying complexity. However, these studies lack a comprehensive evaluation of tool performance across generalization scenarios. Our work addresses this gap by defining the tool generalization problem and simulating generalization scenarios during training.
\subsection{Synthetic Data Generation}
LLMs' generative capabilities have facilitated synthetic data generation, reducing annotation costs while maintaining consistency \cite{zelikman2022star, honovich-etal-2023-unnatural}. For tool learning, \citet{huang-etal-2024-planning-creation} and \citet{huang2024metatool} generate complex queries to evaluate tool usage, while \citet{qin2024toolllm} and \citet{tang2023toolalpaca} use APIs and instruction-response pairs to improve fine-tuning. However, these methods often overlook scenario complexity, limiting generalization performance. Our approach extends these methods by synthesizing data tailored to enhance performance across diverse tool generalization scenarios.

% \subsection{Synthetic Data Generation}
% LLMs' strong generative capabilities have enabled synthetic data generation, reducing the need for costly manual annotation \cite{zelikman2022star, honovich-etal-2023-unnatural, huang-etal-2023-large} while maintaining high consistency \cite{pavlovic-poesio-2024-effectiveness}.

% In the context of tool learning, \citet{huang-etal-2024-planning-creation} leverages LLMs to generate complex queries and solutions for curated benchmarks to evaluate LLM tool usage capabilities, while \citet{huang2024metatool} creates diverse queries to test tool usage across different queries. 

% Similar to our work, \citet{qin2024toolllm} employs real-world APIs and uses ChatGPT to generate instructions for invoking given toolsets, enhancing tool utilization. Likewise, \citet{tang2023toolalpaca} elaborates API documentation to construct paired instructions and responses for fine-tuning. However, these methods often overlook scenario complexity, limiting generalization performance. Our work builds on these approaches by synthesizing data specifically to enhance performance across diverse tool generalization scenarios.

\section{Conclusion}
This work addresses the generalization challenges of LLMs in tool learning by proposing GenTool, a novel approach simulating the generalization process from two dimensions: \textit{Zero-to-One} and \textit{Weak-to-Strong Generalization}. GenTool further enhances LLMs tool selection by guiding them to output ranked tool lists.

Extensive experiments demonstrate that our method outperforms GPT-4o, achieving superior results across all generalization scenarios when compared to strong tuning-based and tuning-free baselines. Further analysis not only highlight that GenTool consistently delivers better performance but also reveal a key insight: existing LLMs rely heavily on memorizing tools rather than understanding their use. 
This indicates a significant gap between current LLMs and the goal of developing truly intelligent agents capable of effective tool utilization. 
We hope this research inspires further advancements in LLMs' tool-learning capabilities.

% This paper represents the \textbf{first} work to thoroughly investigate and improve generalization in tool learning. To clearly introduce the problem of tool learning generalization, we conduct experiments only in single-query, single-tool scenarios. In our future work, we intend to develop our methods to more complex scenarios such as multi-query and multi-tool.





% Comprehensive experiments demonstrate that our method achieves better performance than GPT-4o. Further analysis shows that compared to tuning-based baselines, GenTool  performs better across all generalization scenarios. Additionally, our findings reveal that existing LLMs often rely heavily on memorizing tools, indicating a significant gap between current LLMs and truly intelligent agents in tool usage. We hope this research will encourage further studies on improving LLMs' capabilities in tool utilization.

\section*{Limitation}
\paragraph{Model Scale}
% Due to limited computational resources, our experiments were restricted to backbone models with fewer than 8 billion parameters. The potential of GenTool  to generalize effectively to larger models remains an open question for future research. This is especially significant given that our data was synthesized using GPT-4o. Investigating whether GenTool can enhance the fine-tuning performance of GPT-4o itself could be a promising direction for future work.
Our experimental scope was constrained to backbone models under 8 billion parameters due to computational limitations. While GenTool has demonstrated promising results with these smaller models, its effectiveness with larger-scale architectures remains unexplored. This limitation is particularly noteworthy given that our training data was synthesized using GPT-4o. An important avenue for future research would be investigating whether GenTool's approach can enhance the fine-tuning capabilities of larger models, including GPT-4o itself.

\paragraph{Focusing on Single-query and Single-tool Scenarios}
Our work currently addresses single-query, single-tool scenarios, which represent 44.26\% of real-world applications according to the ToolBench dataset \cite{qin2024toolllm}. While this focus enabled us to conduct \textbf{the first} comprehensive investigation of tool learning generalization across two dimensions examining four distinct scenarios: \textit{seen\_query\_unseen\_tool}, \textit{seen\_query\_seen\_tool}, \textit{unseen\_query\_unseen\_tool}, and \textit{unseen\_query\_seen\_tool}, we acknowledge that real-world applications often require more complex multi-step planning and tool combinations. Our work establishes a foundation for understanding generalization in simpler contexts, paving the way for future research into more complex orchestration challenges involving multiple queries and tools.

% This study focuses on single-query, single-tool scenarios, whereas real-world applications might require multi-step planning and tool combinations. In the widely used ToolBench \cite{qin2024toolllm}, examples of single-query, single-tool scenarios account for 44.26\% of the dataset. Therefore, single-query, single-tool scenarios also form a significant component of real-world applications. Additionally, we are the first work to comprehensively 
% investigate and improve generalization in tool learning by simulating two dimensions and considering four distinct generalization scenarios for evaluations: \textit{seen\_query\_unseen\_tool}, \textit{seen\_query\_seen\_tool}, \textit{unseen\_query\_unseen\_tool}, and \textit{unseen\_query\_seen\_tool}. Starting from single-query, single-tool scenarios benefits our investigations, while multi-query and multi-tool scenarios will definitely introduce more complexity. We hope based on our work, more attention can be given to this issue and future research can explore these more complex orchestration challenges.  

\paragraph{Bias of Synthetic Data}
While using a single LLM for dataset construction is common in related research \cite{huang2024metatool,Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}, our choice of GPT-4o for data synthesis may introduce inherent biases. However, although utilizing multiple LLMs could potentially reduce these biases, it would likely introduce variance in generation quality, potentially compromising evaluation stability. Given that our task primarily involves generating straightforward tools and queries, and that GPT-4o's biases are not directly related to the tool learning generalization problem, we determined that using a single, high-quality model for data synthesis was the most appropriate approach for this work.

% Although constructing datasets with a single LLM is a prevalent approach in related research \cite{huang2024metatool,Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}, 
% GenTool utilizes GPT-4o for data synthesis because of its remarkable high-quality generation capabilities, but this might lead to the biases inherent in GPT-4o. However, although using multiple large language models (LLMs) might mitigate biases, it could also lead to fluctuations in generation quality, which may, in turn, affect the stability of evaluations. While the biases of GPT-4o are not closely linked to the generalization issue in tool learning, our task merely demands the generation of simple, less complex tools and their corresponding queries. Therefore, after consideration, we only use GPT-4o for synthesizing data in this work.  


% In Table \ref{data_compare}, we listed several related works, which either focus solely on the Unseen Query, Seen Tool scenario or the Unseen Query, Unseen Tool scenario. Ours is the \textbf{first} work to thoroughly investigate generalization in tool learning. Multi-query, multi-tool scenarios are much more complex; for instance, in multi-query settings, some queries may be seen while others are not. To clearly introduce the problem of tool learning generalization to the community and explain it comprehensively, we conducted experiments only in single-query, single-tool scenarios.

% Moreover, in the widely used ToolBench \cite{qin2024toolllm}, examples of single-query, single-tool scenarios account for 44.26\% of the dataset. Thus, single-query, single-tool scenarios also form a significant component of real-world applications. Future research can explore these more complex orchestration challenges. 


% As an initial exploration, we focus solely on scenarios where a single query can be resolved with a single tool. For more complex queries requiring planning and the invocation of multiple tools, the generalization problem becomes significantly more challenging. We leave such explorations for future work.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{acl_latex}
\clearpage
\appendix
\begin{table*}[!t]
\footnotesize
\centering
\begin{tabular}{lcccc}
% \begin{tabular}{p{3cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}

\toprule
\textbf{Method} & \textbf{Base Model} & \makecell{\textbf{Dataset} \\ \textbf{Construction}} & \makecell{\textbf{Generalization Scenairo} \\ \textbf{in Evaluation}} & \makecell{\textbf{Candidates} \\ \textbf{Construction}} \\

\midrule
% Chameleon \cite{lu2023chameleon}    & GPT-4        & -                          & 2  & Manually  \\
Toolformer \cite{schick2023toolformer} & GPT-J-6B    & In-Context Learning        & 1   & Manually  \\
GPT4Tools \cite{yang2023gpttools}   & Vicuna-13B  & Manually                  & 1   & Manually  \\
ToolAlpaca \cite{tang2023toolalpaca}   & Vicuna-13B  & Simulation                  & 1 & Manually  \\
ToolBench \cite{qin2024toolllm}     & LLaMA-30B   & Self-Instruct                & 1& Manually  \\
ToolVerify \cite{mekala-etal-2024-toolverifier}& LLaMA-70B & Self-Instruct& 2  & Random \\
Confucius \cite{Gao_Shi_Zhu_Fang_Xin_Ren_Chen_Ma_Ren_2024}                            & LLaMA-7B    & Iterative Self-Instruct                     & 2  & Retrieval \\
STE \cite{wang-etal-2024-llms-imaginarium} & GPT-3.5 &Simulation& 1 &Manually   \\
\textbf{Ours} & GPT-4o & Simulation & 4 & Retrieval \\
\bottomrule
\end{tabular}
\caption{\textbf{Dataset construction} refers to the method used to obtain the training dataset.
\textbf{Generalization scenario in evaluation} indicates the number of generalization types covered during testing. \textbf{Candidates construction} specifies whether the candidate set is meticulously curated manually or generated using the same retrieval method as employed in real-world application scenarios.
}
\label{data_compare}
\end{table*}

\begin{table}[t!]
\footnotesize
    \centering
\begin{tabular}{c|ccc}
\hline
\textbf{Dataset} & \makecell{\textbf{Tools} \\ \textbf{amount}} & \makecell{\textbf{Instance} \\ \textbf{amount}} & \makecell{\textbf{Avg. word} \\ \textbf{input/output}} \\
\hline         Training &868&26,042&597/27 \\
        \hline
        Test cases (1) &471&2,996&610/33 \\
        Test cases (2)  &278&3,072&573/22 \\ 
        Test cases (3)  &252&448&574/32 \\
        Test cases (4)  &125&728&554/25  \\
        \hline
    \end{tabular}
    \caption{The data statistics for training and testing sets constructed from synthetic data, where the test set is divided into the following four categories based on generalization scenarios: (1) Seen\_query\_unseen\_tool (2) Seen\_query\_seen\_tool (3) Unseen\_query\_unseen\_tool
    (4) Unseen\_query\_seen\_tool}
    \label{tab:data_statistics}
\end{table}

\begin{table}[tbp]
\footnotesize
\centering
\begin{tabular}{p{0.25\textwidth}c}
\hline
\textbf{Quality Review Question} & \textbf{Correctness Rate} \\
\hline
Is the instruction a valid and well-formed query? & 94\% \\
\hline
Can the reference tool provide at least a partial solution to the query? & 97\% \\
\hline
Does the tool set selection comply with output specifications? (e.g., for ``none'' outputs, confirming no tool in the set can partially address the query) & 97\% \\
\hline
Are all tool call parameters correctly extracted or reasonably inferred from the instruction without missing or imagined values? & 96\% \\
\hline
All fields are valid & 93\% \\
\hline
\end{tabular}
\caption{Quality Review Results for synthetic data}
\label{tab:quality_review}
\end{table}

\begin{table*}[htbp]
\small
\centering
\begin{tabular}{p{0.95\textwidth}}
\hline
% \textbf{Tool Generation Process} \\

\multicolumn{1}{c}{\textbf{Tool Ranking and Invocation Process}} \\
\hline
You are a professional tool selection assistant. You will be given a query and a corresponding toolset. You have two consecutive tasks. \\
\textbf{First Task} \\
Rank the available tools in the toolset based on relevance. Output format is a list where each item is the name of the tool (from 'name' field in toolset list). Irrelevant tools should be placed after generate\_response(), relevant tools before generate\_response(). The order of irrelevant tools doesn't matter. Example format: [tool\_name\_1, generate\_response(), tool\_name\_2, tool\_name\_3, tool\_name\_4, tool\_name\_5] \\
\textbf{Second Task} \\
Invoke the tool ranked first from the previous task and fill required parameter values. Parameter format: "parameter\_name"="parameter\_value", with multiple parameters separated by commas. Parameter names from 'properties' field under 'arguments', parameter values from user's query. If generate\_response() ranks first, output generate\_response(). Example format: ["tool\_name\_1"("parameter\_name1"="parameter\_value1", "parameter\_name2"="parameter\_value2")] \\
\textbf{Final Output Format (JSON):} \\
\{ \\
\quad "The output of the first task": [], \\
\quad "The output of the second task": [] \\
\} \\
\textbf{Input:} \\
Query: \{input\_query\} \\
Toolset: \{tools\} \\
\hline
\end{tabular}
\caption{Prompt for the GenTool  methodology to rank and invoke the most relevant and effective tools from a given toolset}
\label{GenTool _prompt}
\end{table*}

\begin{table*}[htbp]
\small
\centering
\begin{tabular}{p{0.95\textwidth}}
\hline
\multicolumn{1}{c}{\textbf{Tool Generation Process}} \\
\hline
You are a professional tool creation assistant. Given a \#user query\#, a set of \#existing tools\#, your task is as follows: \\
1. Create a \#new tool\# to solve the \#user query\#. \\
2. The \#new tool\# you create should be less effective than the \#existing tools\# in answering the \#existing problem\#. \\
3. Effectiveness can be judged from the following perspectives: \\
\quad 3.1 The match between the tool names and the problem. \\
\quad 3.2 The length of the tool descriptions and their match with the problem. \\
\quad 3.3 The names, descriptions, and number of tool parameters and their match with the problem. \\
\quad 3.4 The tool's returned results and descriptions and their match with the problem. \\
4. The format of the \#new tool\# should be the same as the tools in the \#existing tools\#. \\
Below are examples of inputs for \#user query\#, and \#existing tools\#, and outputs for the \#new tool\#: \\
Please generate \#new tool\# in JSON format based on the following \#user query\# and \#existing tools\#. Ensure the JSON string format is correct and do not output anything else. \\
\textbf{\#user query\#:} \\
\{user\_query\} \\
\textbf{\#existing tools\#:} \\
\{ex\_tools\} \\
\textbf{\#new tool\#} \\
\hline
\end{tabular}
\caption{Prompt for using GPT-4o to generate new weak tool}
\label{tab:tool-generation}
\end{table*}


\begin{table*}[ht]
\small
\centering
\begin{tabular}{p{0.9\textwidth}}
\hline
\multicolumn{1}{c}{\textbf{Query Generation Process}} \\
\hline
You are a professional question generation assistant. Based on the \#weak tool sets\# and \#strong tool sets\#, your task is to generate 10 questions that are suitable for answering using the \#strong tool sets\#. The requirements are as follows: \\
The questions should be as detailed as possible. The questions should ideally only require the \#strong tool sets\# for answering, without needing assistance from other tools. Ensure that the examples provided are distinct from each other. You can use various sentence structures, such as commands or requests, and adjust the level of detail as needed. The questions can also be answered by the \#weak tool sets\#, but the \#weak tool sets\# should not match the questions as well as the \#strong tool sets\#. \\
Based on the following \#weak tool sets\# and \#strong tool sets\#, generate 10 different questions in list format, following the example format. Do not output anything else. \\
\textbf{\#weak tool sets\#:} \texttt{\{weak\}} \\
\textbf{\#strong tool sets\#:} \texttt{\{strong\}} \\
\textbf{\#output\#:} \\
\hline
\end{tabular}
\caption{Prompt designed for generating diverse new queries.}
\label{tab:query_generation_prompt}
\end{table*}


\begin{table*}[ht]
\small
\centering
\begin{tabular}{p{0.9\textwidth}}
\hline
\textbf{Calling Annotation Prompt:} \\
You are a professional tool matching assistant. Based on the \#user query\# and \#toolsets\#, your task is to perform tool matching for the \#user query\# and output a \#detailed execution plan\# with the following requirements: \\
In the \#detailed execution plan\#, provide the tool names and their specific parameters. The form of parameter passing should be: "parameter name: parameter value". Ensure that the parameter names used come from the defined parameter name set in the corresponding tools, and ensure that the parameter values have sources, which can come from two parts: user instructions or some user-related information (such as personal information like name, ID number, account, password, etc.). \\
Below are some examples of input \#user query\# and \#toolsets\# and the output \#detailed execution plan\#: \\
\textbf{\#user query\#:} \\
'Please create a new file named "2023 October Work Log.txt" with the initial entry "Tasks Completed Today"' \\
\textbf{\#toolsets\#:} \\
\{demons\_example\_tool\_set\} \\
\textbf{\#output\#:} \\
file\_write(file\_path='Desktop/2023 October Work Log.txt', content='Tasks Completed Today') \\
\\
Please generate a \#detailed execution plan\# in JSON format based on the following \#user query\# and \#toolsets\#, following the format of the examples. Ensure the JSON string format is correct and do not output anything else. \\
\textbf{\#user query\#:} \\
\texttt{\{query\}} \\
\textbf{\#toolsets\#:} \\
\texttt{\{tools\}} \\
\textbf{\#output\#:} \\
\hline
\end{tabular}
\caption{Prompt for generating a detailed calling information based on user query and toolsets.}
\label{tab:calling_annotation_prompt}
\end{table*}


\section{Dataset generation and checking}
\label{app_datasets}
\subsection{Dataset Generation Prompts}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/tool_data_scenarios.pdf}
    \caption{ 
    % \pei{this figure is hard to parse. Can we simplify and add more legends in text to explain what each instance is for? For example, it's not clear to me why there are eight input-output boxes for the first cluster, is each row supposed to mean one scenario? And what does left column and right column of these boxes imply? Everything should be made explicit.}
    The detailed process of training and testing data division based on queries and the tool library: Different tool-query pairs play distinct roles. The frist cluster focuses on constructing the training set. The second cluster is dedicated to building the training and testing sets for \textit{seen\_query\_unseen\_tool}. The third cluster provides training and testing sets for \textit{seen\_query\_unseen\_tool}, \textit{unseen\_query\_unseen\_tool}, and \textit{unseen\_query\_seen\_tool}. Finally, the fourth cluster constructs both the training and testing sets for \textit{seen\_query\_seen\_tool}.
    % \pei{bottom half of figure text too small, fonts should be at least as large as the top half, also make caption more detailed and deliver key messages, same comment for Figure 4}
    }
    \label{fig:GenTool _eval_scenario}
\end{figure*}
\label{app_a}
Table~\ref{tab:tool-generation}, Table~\ref{tab:query_generation_prompt} and Table~\ref{tab:calling_annotation_prompt} present the prompts used to generate our dataset with GPT4o. Specifically:
\begin{itemize}
    \item Table~\ref{tab:tool-generation} contains prompts designed to generate \textit{weak tools} based on existing query-tool pairs.
    \item Table~\ref{tab:query_generation_prompt} includes prompts for diversifying query generation based on \textit{strong tools} and \textit{newly generated weak tools}.
    \item Table~\ref{tab:calling_annotation_prompt} provides prompts for generating output information based on query and tool combinations.
\end{itemize}

Figure ~\ref{fig:GenTool _eval_scenario} shows the example that we used in Section \ref{sec:scenario_generation} to explain  how we generate the four generalization scenarios.

\subsection{Human Verification}
\label{app_check}
To ensure data quality, we implement a systematic verification process. We randomly sample 200 instances, including queries, strong tools, the generated weak tools, and the queries derived from weak tools. An expert annotator, with extensive experience in API design and natural language processing, evaluates a random sample of 200 instances including queries, strong tools, the generated weak tools, and the queries derived from weak tools. Inspired by \citet{iskander-etal-2024-quality}, the verification process for each instance follow the criteria in Table \ref{tab:quality_review}.

 % we assess whether the queries, constructed toolsets, and generated outputs form valid tasks. We also verify if additional tools beyond the reference tool can address the query, as GPT-4 may generate tools with overlapping functionality. 
Results in Table \ref{tab:quality_review} show that over 90\% of all fields are valid, significantly higher than the 33\% parameter alignment error rate observed in datasets like ToolBench and ToolAlpaca \cite{iskander-etal-2024-quality}. These high verification scores demonstrate the effectiveness of our generation pipeline in producing high-quality, reliable training data for tool generalization tasks.

\section{Dataset Comparison}

% To further highlight the differences between our work and existing approaches, in Table~\ref{data_compare}, we compare  the recent works that also use LLMs for synthetic data generation followed by fine-tuning. It can be observed that our method utilizes the state-of-the-art GPT-4o model for data generation to ensure data quality. Additionally, we explore the most diverse generalization scenarios, including the complex compositional generalization relationships between queries and tools, which have not been addressed in previous works. Finally, to increase the difficulty of tool selection, we adopt a retrieval-based approach to construct candidate toolsets, which significantly challenges the model's ability to select the appropriate tool. Our GenTool  method achieves up to 90.15\% accuracy on the tool selection task, demonstrating the superiority of our approach.

To further emphasize the distinctions between our work and existing approaches, Table~\ref{data_compare} presents a comparison with recent studies that leverage LLMs for synthetic data generation followed by fine-tuning. Notably, our method employs the state-of-the-art GPT-4o model for data generation, ensuring high-quality data. Moreover, we tackle diverse generalization scenarios, including complex compositional relationships between queries and tools, which remain unexplored in prior research. To further elevate the challenge of tool selection, we adopt a retrieval-based strategy to construct candidate toolsets, significantly testing the models ability to identify the correct tool. Our GenTool  method achieves an accuracy of up to 90.15\% on the tool selection task, demonstrating the effectiveness and superiority of our approach.




\section{Experiment Setup}
\label{our_ex_app}



\subsection{Data Statistics}
To validate the effectiveness of GenTool , we synthesized a dataset covering 22 domains, 1,633 tools, and 33,286 samples. We used 67\% of the tool clusters to construct training sets for \textit{zero-to-one} and \textit{weak-to-strong} generalization and 33\% of the clusters to simulate test scenarios. Detailed statistics are shown in Table~\ref{tab:data_statistics}.

\subsection{Implementation Details}
For our fine-tuning experiments, all model fine-tuning is done using the AdamW optimizer \cite{loshchilov2018decoupled} with \( \beta = (0.9, 0.999) \), \( \epsilon = 10^{-8} \), and no weight decay. The learning rate is set to 3e-5, and we use a linear scheduler with a warm-up ratio of 0.1. The batch size is set to 4, with a training epoch of 4, and training examples are truncated to 3076 tokens. We optimize the model training using the DeepSpeed zero strategy \cite{10.1145/3394486.3406703}. The model training can be completed in 15 hours using four Nvidia A100 PCIe 80GB GPUs. Notably, we exclude the result information from calling $T_{gold}$, as the primary focus of this study is on the tool generalization problem rather than the execution of actual API calls.  

In our fine-tuning process, we involve two tasks: \textit{Tool Ranking} and \textit{Tool Invocation}. The instructions are set as follows:
\begin{itemize}
    \item The input to the model follows the format: \texttt{[Input Query: ... | Toolset: ...]}.
    \item The model's gold output format is:
    \begin{quote}
        \texttt{The output of the first task is: [Tool1, Tool2, Tool3]. The output of the second task is: ...}.
    \end{quote}
\end{itemize}

Table~\ref{GenTool _prompt} shows the detailed prompt when we finetune the LLMs under the GenTool  training framework.


% \section{Detailed Results in Different Scenarios}
% We provide a breakdown of results across various generalization scenarios, as shown in Table~XX. These results highlight GenTool 's performance improvements in \textit{seen-query} and \textit{unseen-query} cases across different tool combinations.

\begin{table*}[h!]
\centering
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c|}{\textbf{Parameter}} & \multirow{2}{*}{\textbf{Format Accuracy}} \\ \cline{3-4}&& \textbf{Name}& \textbf{Value}       &\\ \hline
\multicolumn{5}{c}{\textbf{Seen Query, Unseen Tool}} \\ \hline
GenTool  & \textbf{92.84} & \textbf{85.27} & \textbf{76.55} & 98.96 \\ \hline
Weak-to-Strong & 90.21 & 83.80 & 75.82 & 99.57 \\ \hline
Zero-to-One & 63.32 & 57.18 & 50.81 & \textbf{99.77} \\ \hline
\multicolumn{5}{c}{\textbf{Seen Query, Seen Tool}} \\ \hline
GenTool  & \textbf{88.57} & \textbf{90.30} & \textbf{80.89} & \textbf{99.71} \\ \hline
Weak-to-Strong & 31.62 & 42.79 & 41.72 & 98.73 \\ \hline
Zero-to-One & 74.47 & 65.91 & 56.96 & \textbf{99.71} \\ \hline
\multicolumn{5}{c}{\textbf{Unseen Query, Unseen Tool}} \\ \hline
GenTool  & 80.58 & 77.94 & 71.18 & \textbf{100.00} \\ \hline
Weak-to-Strong & \textbf{84.38} & \textbf{80.60} & \textbf{73.23} & 99.78 \\ \hline
Zero-to-One & 74.78 & 71.62 & 67.18 & \textbf{100.00} \\ \hline
\multicolumn{5}{c}{\textbf{Unseen Query, Seen Tool}} \\ \hline
GenTool  & \textbf{91.62} & \textbf{94.50} & \textbf{88.48} & \textbf{100.00} \\ \hline
Weak-to-Strong & 73.90 & 94.48 & 87.66 & 99.73 \\ \hline
Zero-to-One & 85.71 & 87.05 & 81.59 & \textbf{100.00} \\ \hline
\end{tabular}
\caption{Detailed results of the LLaMA-3.1-8B-Instruct model trained in a single generalization simulation scenario across different scenarios}
\label{tab:detail_transfer}
\end{table*}

\section{Detailed explanation of evaluation metrics}
\label{app_eval}
\subsection{Tool Selection}
We assess the model's ability to discriminate and select the best tool by comparing the predicted tool name against the ground truth. This metric directly evaluates the model's capability to understand query contexts and match them with appropriate tools.

% Tool selection accuracy is determined by comparing the model's predicted tool name against the ground truth, evaluating the model's discrimination capabilities in identifying the optimal tool for each query context.

% Tool selection is assessed by checking whether the model's output tool name matches the standard tool name. This evaluates the model's ability to choose the correct tool for resolving the query. 

\subsection{Parameter Name Identification}
After selecting the appropriate tool, the model identifies the parameters required for the tool invocation based on the query. The output consists of the parameter names, and we compare the model's output parameter names with the standard parameter names, focusing on both completeness and correctness.

\subsection{Parameter Value Matching}
After selecting the tool and the required parameters, the model parses the query to fill in the content needed for the parameters. The parameter value evaluation focuses on the model's ability to extract and generate appropriate values from queries. Following \citet{huang-etal-2024-planning-creation}, we employ a normalized Levenshtein distance-based scoring function. Given the model's key-value format response $(p_k, p_v)$ and the reference answer $(y_k, y_v)$, where keys $p_k$ and $y_k$ represent the steps and values $p_v$ and $y_v$ represent task-specific results, the score is defined as follows::

\[
S = 
\begin{cases} 
F(p_v, y_v) & \text{if } p_k = y_k \\
0 & \text{if } p_k \neq y_k 
\end{cases}
\]
where \( S \) is the calculated score, and \( F \) represents the calculation function using normalized Levenshtein distance.

\subsection{Format Correctness}
Format correctness evaluates whether the model's tool invocation output is free from syntax or matching errors and can be correctly parsed into the standard JSON format. This metric ensures the generated tool calls are executable.

\section{Additional analysis}
\subsection{Data Scaling Effects} %Analysis (Section 9.3) (split this to 4 scenarios and only contain the tool selection results)

\begin{table*}[!t]
\definecolor{specialblue}{RGB}{29, 66, 168}
\definecolor{specialgreen}{RGB}{27, 143, 34}
\small
\centering
\begin{tabular}{l|c|c|c|c}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c|}{\textbf{Parameter}} & \multirow{2}{*}{\textbf{Format Accuracy}} \\ \cline{3-4}&& \textbf{Name}& \textbf{Value}       &\\ \hline
gpt-3.5 & 31.75 {\color{specialblue}(-16.86)} & 38.20 {\color{specialblue}(-32.14)} & 31.99 {\color{specialblue}(-27.26)} & 99.2 {\color{specialblue}(-0.09)}\\ \hline
gpt-4o mini & 54.63 {\color{specialblue}(-4.17)} & \textbf{62.53} {\color{specialblue}(-16.33)} & \textbf{55.32} {\color{specialblue}(-13.73)} & \textbf{100.0} {\color{specialgreen}(+0.07)}\\ \hline
gpt-4o& \textbf{68.96} {\color{specialblue}(-6.91)} & 59.93 {\color{specialblue}(-10.07)} & {\color{specialblue}53.31 (-10.78)} & 99.96 {\color{specialgreen}(+0.16)} \\ \hline
\end{tabular}
\caption{Results for GPT models with one demonstration that are directly relevant to the test examples.  The {\color{specialgreen}{green}} and {\color{specialblue}{blue}} values represent the differences compared to the model's zero-shot performance, respectively.}
\label{tab:incontext_tool}
\end{table*}
\begin{figure}[!tpb]
    \centering
    \includegraphics[width=1\linewidth]{images/data_scaling.png}
    \caption{ Effects of different proportions of training data on various test scenarios.
   }
    \label{data_scaling}
\end{figure}

We analyze model performance across different data percentages (10\% to 100\%). Figure \ref{data_scaling} shows that performance improves significantly between 10-20\% and 40-50\% of data. Moreover, performance generally improves with more data, with varying gains across scenarios. The \textit{seen\_query\_seen\_tool} scenario benefits most from additional data, while the growth in \textit{unseen\_query\_unseen\_tool} slows down as the training data increases. Additionally, models achieve 78.61\% tool selection accuracy with just 30\% of data, comparable to the Half-Sample baseline, suggesting GenTool's effectiveness comes from structured pair construction rather than data quantity.


\subsection{Fine-grained Analysis for Transfer Learning}
We evaluate the LLaMA-3.1-8B-Instruct model fine-tuned solely on weak-to-Strong pairs and zero-to-One pairs, respectively, and analyze their performance on the test set across four scenarios. As shown in Table~\ref{tab:detail_transfer}, the Weak-to-Strong model performs better on \textit{seen\_query\_unseen\_tool} and \textit{unseen\_query\_unseen\_tool} scenarios. In contrast, the Zero-to-One model achieves better results on \textit{seen\_query\_seen\_tool} and \textit{unseen\_query\_seen\_tool} scenarios.

\begin{table}[h!]
\footnotesize
\centering
\begin{tabular}{l|ccc}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Tool Selection}} & \multicolumn{2}{c}{\textbf{Parameter}}  \\ \cline{3-4}&& \textbf{Name}& \textbf{Value}\\ \hline
gpt-3.5 (1) & 48.61 & 70.34 & 59.25  \\ 
gpt-3.5 (2) & 49.65 & 52.55 & 44.61  \\ k
gpt-3.5 (3) & \textbf{57.89} & \textbf{72.30} & \textbf{60.24} \\ \hline

gpt-4o mini (1) & 58.80 & 78.86 & \textbf{69.05} \\ 
gpt-4o mini (2) & \textbf{76.18} & 76.05 & 66.00   \\ 
gpt-4o mini (3) & 66.88 & \textbf{79.20} & 67.97  \\ \hline

gpt-4o (1) & 75.87 & 72.00 & 64.09 \\ 
gpt-4o (2) & 79.24 & 66.68 & 59.47 \\ 
gpt-4o (3) & \textbf{84.27} & \textbf{77.14} & \textbf{67.48}  \\ \hline
\end{tabular}
\caption{Results for the GenTool  method when applied to in-context learning. (1) represents results under the zero-shot setting, (2) represents results where half of the data from zero-to-one and weak-to-one pairs is randomly sampled for demonstrations, and (3) represents results where zero-to-one and weak-to-strong pairs are used for demonstrations.}
\label{tab:in_context}
\end{table}
\subsection{In-Context Learning Capability}  
We evaluate GenTool's in-context learning using two demonstration pairs per test example (\textit{Zero-to-One} and \textit{Weak-to-Strong}), focusing on GPT-series models due to input length constraints. Table \ref{tab:in_context} shows consistent improvements with GenTool. GPT-4 achieves the highest \textit{Tool Name Selection} accuracy (84.27\%), while GPT3.5 and GPT-4o outperform the half-data baseline. Although GPT4o-mini lags in \textit{Tool Name Selection}, it surpasses the baseline in \textit{Param Name Matching} and \textit{Param Value Matching} by 3.15\% and 1.97\%, respectively, showcasing GenTool's effectiveness.

\subsection{In-context Learning for Related Demonstrations}
We further investigate the performance of GPT-series models when using training data constructed from test example-related pairs as demonstrations for in-context learning. As shown in Table~\ref{tab:incontext_tool}, all models exhibit varying degrees of performance degradation across nearly all metrics. Notably, GPT-3.5 shows the most significant drop, consistent with findings in Section~\ref{analysis_train_only}. This highlights that current large models, whether fine-tuned or used in in-context learning, fail to genuinely acquire tool-use capabilities. Instead, they primarily rely on memorizing previously seen knowledge.



\section{Case Study}
Table~\ref{tab:case1}, \ref{tab:case2} showcase two examples to demonstrate the effectiveness of GenTool . 
\begin{itemize}
    \item In the first example, the GenTool -finetuned LLaMA-8B model successfully outputs the correct priority ranking for tools and provides accurate information for tool invocation. In contrast, the zero-shot result shows that the model fails to select the correct tool.
    \item In the second example, the GenTool -finetuned LLaMA-8B model selects the most useful tool from two viable options. However, the LLaMA-8B zero-shot result fails to achieve this and instead outputs invalid tool.
\end{itemize}
These examples illustrate the superiority of GenTool  in guiding the model to make better decisions in tool selection and usage.


\section{Error Analysis}
We analyze incorrect examples where LLaMA-8B fails to generate the correct tool call. The errors can be summarized into seven categories, with examples shown in from Table \ref{tab:complete-api-docs-with-returns} to Table \ref{tab:calendar-event-api-docs} :
\begin{enumerate}
    \item \textbf{Insufficient Query Understanding:} Table \ref{tab:complete-api-docs-with-returns} shows that the model fails to parse the query correctly, resulting in incorrect parameter inputs.
    \item \textbf{Parameter Misinterpretation:} Table \ref{tab:reminder-api-docs} shows that the model provides parameter values inconsistent with tool specifications.
    \item \textbf{Query Ambiguity:} Table \ref{tab:real-estate-api-docs} shows that query ambiguity leads to hallucinated information in the tool call.
    \item \textbf{Tool Repetition:} Table \ref{tab:bond-search-api-docs} shows that the model redundantly calls the same tool multiple times, despite needing only one call.
    \item \textbf{The Second Best Tool Selection:} Table \ref{tab:job-search-api-docs} shows that the model selects the second-best tool instead of the optimal one.
    \item \textbf{Default Response Generation:} The model defaults to Generate\_Response despite available specialized tools (Table \ref{tab:calendar-event-api-docs}).
\end{enumerate}
% \jie{to do: lack of detailed 4 scenarios results in appendix for some analysis (1) transfer learning (2) in-context learning (3) llms finetuned soly on test cluster examples}
\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.8\textwidth}}
\hline
\textbf{Toolsets} & 1. flight\_pickup\_service \\
& Description: Service to arrange pick-up for flights \\
& Required parameters: \{"flight\_number": "Flight number", "arrival\_time": "Arrival time", "pickup\_location": "Pick-up location", "destination": "Destination"\} \\
& Returns: \{"status": "Pick-up service status", "service\_details": "Service details"\} \\
\cline{2-2}
& 2. car\_transfer\_service \\
& Description: Tool to arrange car transfer services \\
& Required parameters: \{"pickup\_time": "Pickup time", "pickup\_location": "Pickup location", "destination": "Destination", "vehicle\_type": "Vehicle type", "passenger\_name": "Passenger name"\} \\
& Returns: \{"transfer\_status": "Transfer status", "service\_details": "Service details", "confirmation\_code": "Confirmation code"\} \\
\cline{2-2}
& 3. simple\_car\_rental \\
& Description: Basic Car Rental Service \\
& Required parameters: \{"pickupLocation": "Pickup location", "dropOffLocation": "Drop-off location", "pickupTime": "Pickup time", "dropOffTime": "Drop-off time", "carCode": "Car code"\} \\
& Returns: \{"status": "Rental status", "cost": "Rental cost"\} \\
\cline{2-2}
& 4. airport\_dropoff\_service \\
& Description: Service to arrange drop-off to the airport \\
& Required parameters: \{"dropoff\_time": "Drop-off time", "flight\_number": "Flight number", "pickup\_location": "Pickup location", "airport\_name": "Airport name"\} \\
& Returns: \{"status": "Drop-off status", "service\_details": "Service details"\} \\
\cline{2-2}
& 5. taxi\_booking \\
& Description: Tool for booking a taxi service \\
& Required parameters: \{"pickup\_time": "Pickup time", "pickup\_location": "Pickup location", "destination": "Destination", "passenger\_name": "Passenger name", "luggage\_count": "Luggage count"\} \\
& Returns: \{"booking\_status": "Booking status", "taxi\_service\_information": "Service info", "confirmation\_number": "Confirmation number"\} \\
\hline
\textbf{User query}: & I need to know the exact pick-up and drop-off times for a car rental reservation with the code ABC123, including any grace periods for late returns. \\
\hline
\textbf{Ground truth tool calling}: & simple\_car\_rental   \{"carCode": "ABC123"\} \\
\hline
\textbf{Predicted tool calling (GenTool )}: & \{"The output of the first task: "["simple\_car\_rental", "generate\_response", "car\_transfer\_service", "flight\_pickup\_service", "airport\_dropoff\_service", "taxi\_booking"], "The output of the second task: ": ["simple\_car\_rental(carCode='ABC123')"]\}
 \\
\hline
\textbf{Predicted tool calling (Zero-Shot)}: & car\_transfer\_service \{"pickup\_time": "2023-03-15T12:00:00", "pickup\_location": "Airport", "destination": "Car Rental Agency", "vehicle\_type": "Sedan", "passenger\_name": "John Doe"\} \\
\hline
\end{tabular}
\caption{Two examples predicted by the Llama-3.1-8B-Instruct model, one from finetuning using the GenTool method and the other from performing zero-shot directly. Our method successfully enabled Llama-3.1-8B-Instruct to select the correct tool and rank the tools appropriately.}
\label{tab:case1}
\end{table*}

\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.8\textwidth}}
\hline
\textbf{Toolsets}  & 1. appliance\_repair\_rescheduling \\
& Description: Request to adjust the appliance repair appointment time \\
& Required parameters: \{"originalRequestID": "Original repair request number", "newTime": "New appointment time"\} \\
& Returns: \{"rescheduling\_status": "Status", "newRequestID": "New request number"\} \\
\cline{2-2}
& 2. appliance\_repair\_status\_query \\
& Description: Appliance Repair Inquiry Tool \\
& Required parameters: \{"requestID": "Request number", "applianceType": "Appliance type", "applianceBrand": "Brand", "applianceModel": "Model", "contactInformation": "Contact info", "address": "Address"\} \\
& Returns: \{"repair\_status": "Inquiry result"\} \\
\cline{2-2}
& 3. appliance\_repair\_cancellation \\
& Description: Appliance Repair Cancellation Tool \\
& Required parameters: \{"requestID": "Request number", "cancellationReason": "Reason"\} \\
& Returns: \{"cancellation\_status": "Status"\} \\
\cline{2-2}
& 4. appliance\_repair\_request \\
& Description: Appliance Repair Request Tool \\
& Required parameters: \{"applianceType": "Type", "applianceBrand": "Brand", "applianceModel": "Model", "issueDescription": "Issue", "contactInformation": "Contact", "address": "Address", "time": "Service time"\} \\
& Returns: \{"request\_status": "Status", "requestID": "Request ID"\} \\
\cline{2-2}
& 5. repair\_service\_request \\
& Description: Request a repair service \\
& Required parameters: \{"device": "Device type", "make": "Make", "model\_number": "Model", "issue": "Issue", "phone": "Phone", "address": "Address", "appointment\_time": "Time"\} \\
& Returns: \{"request\_status": "Status", "service\_id": "Service ID"\} \\
\hline
\textbf{User query}: & Please help me schedule a repair service for my LG microwave, model LMV2031ST, which is not heating food. My contact information is 876-543-2109, and my address is 404 Cedar St. I am available on Friday at 1 PM. \\
\hline
\textbf{Ground truth tool call}: & repair\_service\_request  \{"device": "microwave", "make": "LG", "model\_number": "LMV2031ST", "issue": "not heating food", "phone": "876-543-2109", "address": "404 Cedar St", "appointment\_time": "Friday at 1 PM"\} \\
\hline
\textbf{Predicted tool calling (GenTool )}: & \{"The output of the first task: ": ["repair\_service\_request", "appliance\_repair\_request", "generate\_response", "appliance\_repair\_status\_query", "appliance\_repair\_cancellation", "appliance\_repair\_rescheduling"], "The output of the second task: ": ["repair\_service\_request(device='microwave', make='LG', model\_number='LMV2031ST', issue='not heating food', phone='876-543-2109', address='404 Cedar St', appointment\_time='Friday at 1 PM')"]\}
\\
\hline
\textbf{Predicted tool calling (Zero-shot)}: & appliance\_repair\_status\_query  \{"requestID": "876-543-2109", "applianceType": "LG microwave", "applianceBrand": "LG", "applianceModel": "LMV2031ST", "contactInformation": "876-543-2109", "address": "404 Cedar St."\} \\
\hline
\end{tabular}
\caption{Two examples were predicted by the Llama-3.1-8B-Instruct model: one using the GenTool method for fine-tuning and the other performing zero-shot inference directly. Our method successfully positioned the two useful tools before ``generate\_response'' and ultimately selected the correct tool.}
\label{tab:case2}
\end{table*}

\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. schedule\_repair\_service \\
& Description: Tool to schedule a repair service for home appliances \\
& Required parameters: \{"appliance": "Type and brand", "model": "Model number", "problem": "Issue description", "contact": "Contact number", "location": "Service location", "appointment\_time": "Service time"\} \\
& Returns: \{"status": "Status of the repair service request", "serviceID": "ID of the scheduled service"\} \\
\cline{2-2}
& 2. appliance\_repair\_cancellation \\
& Description: Appliance Repair Cancellation Tool \\
& Required parameters: \{"requestID": "Repair request number", "cancellationReason": "Reason for cancellation"\} \\
& Returns: \{"cancellation\_status": "Repair cancellation status"\} \\
\cline{2-2}
& 3. appliance\_repair\_status\_query \\
& Description: Appliance Repair Inquiry Tool \\
& Required parameters: \{"requestID": "Request number", "applianceType": "Type of appliance", "applianceBrand": "Brand name", "applianceModel": "Model number", "contactInformation": "Contact info", "address": "Service address"\} \\
& Returns: \{"repair\_status": "Repair inquiry result"\} \\
\cline{2-2}
& 4. appliance\_repair\_request \\
& Description: Appliance Repair Request Tool \\
& Required parameters: \{"applianceType": "Type of appliance", "applianceBrand": "Brand name", "applianceModel": "Model number", "issueDescription": "Problem details", "contactInformation": "Contact info", "address": "Service address", "time": "Service time"\} \\
& Returns: \{"request\_status": "Repair request status", "requestID": "Repair request ID"\} \\
\cline{2-2}
& 5. appliance\_repair\_rescheduling \\
& Description: Request to adjust the appliance repair appointment time \\
& Required parameters: \{"originalRequestID": "Original request number", "newTime": "New appointment time"\} \\
& Returns: \{"rescheduling\_status": "Rescheduling status", "newRequestID": "New repair request number"\} \\
\hline
% \multicolumn{2}{l}{--------------------------------------------------------------------------------------------------------} \\
\textbf{User query}: & I need to arrange a repair service for my LG washing machine, model WM3900HBA, which is making a loud noise during the spin cycle. Please schedule it for Saturday at 2 PM. My contact number is 987-654-3210, and my address is 456 Elm St. \\
\hline
% \multicolumn{2}{|l|}{--------------------------------------------------------------------------------------------------------} \\
\textbf{Ground truth tool calling}: & schedule\_repair\_service \{"appliance": "LG washing machine", "model": "WM3900HBA", "problem": "making a loud noise during the spin cycle", "contact": "987-654-3210", "location": "456 Elm St", "appointment\_time": "Saturday at 2 PM"\} \\
\hline 
\textbf{Predicted tool calling}: & schedule\_repair\_service  \{"appliance": "washing machine", "model": "WM3900HBA", "problem": "loud noise", "contact": "987-654-3210", "location": "456 Elm St", "time": "Saturday 2 PM"\} \\
\hline
\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: Here the model lacks a comprehensive understanding of the query. During parsing, it overlooks key textual information. ``LG'' is a brand and should be associated with ``washing machine.'' }
\label{tab:complete-api-docs-with-returns}
\end{table*}
\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. add\_reminder \\
& Description: Add reminder event \\
& Required parameters: \{"reminder\_id": "ID of the reminder", "event\_title": "Event title", "reminder\_time": "Time of the reminder", "reminder\_location": "Location of the reminder", "reminder\_frequency": "Frequency of the reminder (days)", "reminder\_name": "Name of the reminder"\} \\
& Returns: \{"status": "Whether the reminder was successfully added"\} \\
\cline{2-2}
& 2. create\_event\_reminder \\
& Description: Create a one-time event reminder \\
& Required parameters: \{"event\_name": "Name of the event", "event\_time": "Time of the event", "event\_location": "Location of the event"\} \\
& Returns: \{"confirmation": "Confirmation of the event reminder creation"\} \\
\cline{2-2}
& 3. set\_reminder \\
& Description: Set a reminder for an event \\
& Required parameters: \{"event\_id": "ID of the event", "reminder\_time": "Time before the event to set the reminder"\} \\
& Returns: \{"confirmation": "Confirmation of the reminder being set"\} \\
\cline{2-2}
& 4. add\_event\_to\_calendar \\
& Description: Add an event to the calendar \\
& Required parameters: \{"event\_name": "Name of the event", "date": "Date of the event", "start\_time": "Start time of the event", "end\_time": "End time of the event", "location": "Location of the event"\} \\
& Returns: \{"status": "Status of the event addition"\} \\
\cline{2-2}
& 5. add\_event \\
& Description: Add a single event to the schedule \\
& Required parameters: \{"event\_name": "Name of the event", "start\_time": "Start time of the event", "end\_time": "End time of the event", "location": "Location of the event", "reminder": "Reminder setting (e.g., '15 minutes before')"\} \\
& Returns: \{"status": "Status of the event addition"\} \\
\hline
\textbf{User query}: & Could you create a one-time event reminder for a dinner reservation at "The Gourmet Bistro" at 7:00 PM on October 25th, 2023? \\
\hline
\textbf{Ground truth tool calling}: & create\_event\_reminder  \{"event\_name": "Dinner Reservation at The Gourmet Bistro", "event\_time": "2023-10-25T19:00:00", "event\_location": "The Gourmet Bistro"\} \\
\hline
\textbf{Predicted tool calling}:  & create\_event\_reminder  \{"event\_name": "The Gourmet Bistro", "event\_time": "2023-10-25T19:00:00", "event\_location": "Dinner Reservation"\} \\
\hline

\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: Parameter type confusion - model places location ``gourmet bistro'' in event name field instead of location field.}
\label{tab:reminder-api-docs}
\end{table*}
\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. real\_estate\_search\_tool \\
& Description: Real estate search tool that allows for setting multiple search criteria for filtering \\
& parameters: \{"location": "Geographic location of the property", "priceRange": "Price range (format: 'min-max')", "areaRange": "Area range (format: 'min-max')", "propertyType": ["apartment", "villa", "townhouse", "condo"], "bedrooms": "Number of bedrooms", "bathrooms": "Number of bathrooms"\} \\
& Returns: \{"listings": [Array of properties with details including id, location, price, area, type, bedrooms, bathrooms, imageURL]\} \\
\cline{2-2}
& 2. market\_price\_checker \\
& Description: Market Price Checker Tool \\
& Required parameters: \{"marketType": ["Energy", "Agriculture", "Metals"], "commodity": "Commodity Name"\} \\
& Returns: \{"current\_price": "Current Market Price"\} \\
\cline{2-2}
& 3. price\_comparison\_tool \\
& Description: Product price query and comparison tool \\
& Required parameters: \{"productID1", "productID2", "date1", "date2", "price1", "price2", "storeName1", "storeName2"\} \\
& Returns: \{"price": "Product price", "lowestStore": "Store with lowest price"\} \\
\cline{2-2}
& 4. simple\_apartment\_comparator \\
& Description: Tool to compare a specific apartment with the market price of similar properties \\
& Required parameters: \{"apartmentID": "ID of apartment to compare", "marketPrice": "Market price of similar properties"\} \\
& Returns: \{"comparison": "Comparison result"\} \\
\cline{2-2}
& 5. compare\_prices \\
& Description: Tool to compare prices of two products \\
& Required parameters: \{"productId1": "First Product ID", "productId2": "Second Product ID"\} \\
& Returns: \{"priceDifference": "Price difference between products"\} \\
\hline
\textbf{User query}: & Please compare for me the finely decorated two-bedroom apartments around 100 square meters in the center of Beijing, priced between 3 million and 5 million RMB, especially the one priced at 3.2 million near the subway station, with the market price of other similar properties, so that I can make a purchase decision. \\
\hline
\textbf{Ground truth tool calling}: & real\_estate\_search\_tool  \{"location": "center of Beijing", "priceRange": "3.2 million", "areaRange": "around 100 square meters", "propertyType": "apartment", "bedrooms": 2, "bathrooms": 1"\} \{"location": "center of Beijing", "priceRange": "3-5 million", "areaRange": "around 100 square meters", "propertyType": "apartment", "bedrooms": 2, "bathrooms": 1"\} \\
\hline
\textbf{Predicted tool calling}: & simple\_apartment\_comparator  \{"apartmentID": "A123", "marketPrice": 3200000\} \\
\hline
\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: Model incorrectly invokes simple\_apartment\_comparator and hallucinates A123, failing to recognize the query's intent for comparative property search}
\label{tab:real-estate-api-docs}
\end{table*}
\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. savings\_bond\_lookup \\
& Description: Lookup tool for savings bonds \\
& Required parameters: \{"bond\_term": "Term of the bond in months", "issue\_date": "Date when the bond was issued", "interest\_method": "Method of paying interest", "coupon\_rate\_type": "Type of coupon rate", "term\_variability": "Indicates if the term is variable", "keyword": "Keyword for bond search"\} \\
& Returns: \{"bond\_info": "Information about the bond"\} \\
\cline{2-2}
& 2. job\_info\_search \\
& Description: Job Information Query Tool \\
& Required parameters: \{"jobTitle": "Job Title", "industry": "Industry Type", "location": "Work Location", "postDate": "Date of Job Posting"\} \\
& Returns: \{"job\_descriptions": "Job Descriptions Summary", "companyName": "Company Name", "jobDescription": "Job Requirements", "salaryRange": "Salary Range"\} \\
\cline{2-2}
& 3. train\_ticket\_query \\
& Description: Train Ticket Booking Inquiry Tool \\
& Required parameters: \{"departureDate": "Departure Time", "startStation": "Departure Station", "endStation": "Arrival Station", "trainID": "Train Number", "seatType": "Seat Type"\} \\
& Returns: \{"ticket\_status": "Booking List", "order\_details": "Order Details"\} \\
\cline{2-2}
& 4. company\_credit\_check \\
& Description: Enterprise Credit Inquiry Tool \\
& Required parameters: \{"companyName": "Company name", "country": "Company location", "checkDate": "Inquiry date", "checkType": ["financial", "legal", "business"]\} \\
& Returns: \{"credit\_score": "Enterprise credit score", "credit\_rating": "Credit rating", "credit\_status": "Credit status"\} \\
\cline{2-2}
& 5. basic\_job\_search\_tool \\
& Description: A simple tool for searching job positions based on basic criteria \\
& Required parameters: \{"jobTitle": "Title", "location": "Location", "salary": "Salary range"\} \\
& Returns: \{"jobListings": [Array of jobs with company, title, location, salary]\} \\
\hline
\textbf{User query}: & I need to find the savings government bonds issued from today with a 36-month term, periodic interest payments, fixed coupon rate, and a variable term type, and view the details of the first record, especially using 'government bonds' as the keyword to obtain relevant information. \\
\hline
\textbf{Ground truth tool calling}: & savings\_bond\_lookup  \{"bond\_term": 36, "issue\_date": "today's date", "interest\_method": "periodic", "coupon\_rate\_type": "fixed", "term\_variability": "variable", "keyword": "government bonds"\} \\
\hline
\textbf{Predicted tool calling}: & savings\_bond\_lookup  \{"bond\_term": 36, "issue\_date": "today's date", "interest\_method": "periodic interest payments", "coupon\_rate\_type": "fixed coupon rate", "term\_variability": "variable term type", "keyword": "government bonds"\} savings\_bond\_lookup  \{"bond\_term": 36, "issue\_date": "today's date", "interest\_method": "periodic interest payments", "coupon\_rate\_type": "fixed coupon rate", "term\_variability": "variable term type", "keyword": "government bonds"\}\\
\hline
\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: Redundant duplicate tool calls in response output.}
\label{tab:bond-search-api-docs}
\end{table*}

\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. part\_time\_job\_tool \\
& Description: Part-time job information collection and management tool, only applicable to college students \\
& Required parameters: \{"jobType": ["Education", "Sales", "Media", "Service Industry", "IT"], "hourlyWage": "number >= 1", "workingHours": "format 'HH:MM-HH:MM'", "jobLocation": "string", "experienceRequired": "boolean", "studentIdentityRequired": "boolean", "jobDescription": "string", "contact": "string", "postDate": "date"\} \\
& Returns: \{"success": "boolean", "jobId": "string"\} \\
\cline{2-2}
& 2. job\_info\_search \\
& Description: Job Information Query Tool \\
& Required parameters: \{"jobTitle": "Job Title", "industry": "Industry Type", "location": "Work Location", "postDate": "Date of Job Posting"\} \\
& Returns: \{"job\_descriptions": "Summary", "companyName": "Company", "jobDescription": "Requirements", "salaryRange": "Salary"\} \\
\cline{2-2}
& 3. basic\_job\_search\_tool \\
& Description: A simple tool for searching job positions based on basic criteria \\
& Required parameters: \{"jobTitle": "Title", "location": "Location", "salary": "Salary range"\} \\
& Returns: \{"jobListings": [Array of jobs with company, title, location, salary]\} \\
\cline{2-2}
& 4. job\_hiring\_tool \\
& Description: Job hiring tool for posting vacancies and filtering candidates \\
& Required parameters: \{"jobPosition": "string", "educationRequirement": ["Doctorate", "Master's", "Bachelor's", "Associate", "Other"], "workExperience": "string", "salaryRange": "string", "jobLocation": "string", "companySize": "string", "requiredSkills": "comma-separated skills"\} \\
& Returns: \{"candidates": [Array of candidates], "jobPosted": "boolean"\} \\
\cline{2-2}
& 5. savings\_bond\_lookup \\
& Description: Lookup tool for savings bonds \\
& Required parameters: \{"bond\_term": "Term of the bond in months", "issue\_date": "Date when the bond was issued", "interest\_method": "Method of paying interest", "coupon\_rate\_type": "Type of coupon rate", "term\_variability": "Indicates if the term is variable", "keyword": "Keyword for bond search"\} \\
& Returns: \{"bond\_info": "Information about the bond"\} \\
\cline{2-2}
\hline
\textbf{User query}: & Can you find job listings for a Content Writer in Sydney with a salary range of 50k-70k? \\
\hline
\textbf{Ground truth tool calling}: & basic\_job\_search\_tools \{"jobTitle": "Content Writer", "location": "Sydney", "salary": "50k-70k"\} \\
\hline
\textbf{Predicted tool calling}: & job\_info\_search  \{"jobTitle": "Content Writer", "location": "Sydney", "salaryRange": "50k-70k"\} \\
\hline
\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: For this simple query, the model should have called the ``basic\_job\_search\_tool''. However, it incorrectly chose the ``job\_info\_search'' tool, and the parameters it used for calling this tool were inconsistent with the tool's descriptions.}
\label{tab:job-search-api-docs}
\end{table*}

\begin{table*}[htbp]
\begin{tabular}{p{0.15\textwidth}|p{0.80\textwidth}}
\hline
\textbf{Toolsets}  & 1. add\_event\_to\_calendar \\
& Description: Add an event to the calendar \\
& Required parameters: \{"event\_name": "Event name", "date": "Event date", "start\_time": "Start time", "end\_time": "End time", "location": "Event location"\} \\
& Returns: \{"status": "Event addition status"\} \\
\cline{2-2}
& 2. create\_event \\
& Description: Create a new event in the calendar \\
& Required parameters: \{"event\_title": "Event title", "date": "Event date", "start\_time": "Start time", "end\_time": "End time", "venue": "Event venue"\} \\
& Returns: \{"status": "Event creation status"\} \\
\cline{2-2}
& 3. create\_event\_label \\
& Description: Create a label for an event \\
& Required parameters: \{"event\_id": "Event ID", "event\_name": "Event name", "label\_color": "Label color", "label\_description": "Label description"\} \\
& Returns: \{"label\_creation\_status": "Label creation status"\} \\
\cline{2-2}
& 4. add\_event \\
& Description: Add a new event to the calendar \\
& Required parameters: \{"event\_name": "Event name", "event\_date": "Event date", "event\_time": "Event time"\} \\
& Returns: \{"status": "Event addition status"\} \\
\cline{2-2}
& 5. delete\_event \\ & 
Description: Delete an event from the calendar \\ &
Required parameters: 
\{ "event\_id": "Event ID", "event\_name": "Event name" \} \\
& Returns: 
\{ "status": "Event deletion status" \} \\ 
\hline
\textbf{User query}: & Please set a one-time reminder for me to attend the 'Team Building Activity' held in the company meeting room at 2 PM three days from now, and set a recurring reminder to check the project progress every Monday morning at 8 AM in my office. \\
\hline
\textbf{Ground truth tool calling}: & create\_event  \{"event\_title": "Team Building Activity", "date": "2023-10-14", "start\_time": "14:00", "end\_time": "16:00", "venue": "company meeting room"\}  create\_event \{"event\_title": "Check Project Progress", "date": "2023-10-16", "start\_time": "08:00", "end\_time": "09:00", "venue": "office"\} \\
\hline
\textbf{Predicted tool calling}: & generate\_response() \\
\hline
\end{tabular}
\caption{LLaMA-3.1-8B-Instruct error case: Model defaults to generate\_response despite availability of suitable create\_event tool.}
\label{tab:calendar-event-api-docs}
\end{table*}

% \begin{table*}[htbp]
% \begin{tabular}{|p{0.15\textwidth}|p{0.85\textwidth}|}
% \hline
% toolsets & 1. room\_reservation \\
% & Description: Reserve rooms at a hotel \\
% & Required parameters: \{"hotel\_id": "Hotel Identifier", "check\_in": "Check-in Date", "check\_out": "Check-out Date", "room\_type": "Type of Room", "number\_of\_rooms": "Number of Rooms"\} \\
% & Returns: \{"status": "Reservation Status", "details": "Reservation Details"\} \\
% \cline{2-2}
% & 2. hotel\_booking\_modification \\
% & Description: Hotel order modification interface, used to modify existing hotel reservation orders \\
% & Required parameters: \{"confirmation\_number": "Reservation confirmation number", "check\_in": "New check-in date", "check\_out": "New check-out date", "room\_type": "New room type", "number\_of\_rooms": "New number of rooms"\} \\
% & Returns: \{"modification\_status": "Order modification status"\} \\
% \cline{2-2}
% & 3. simple\_hotel\_reservation \\
% & Description: Reserve a hotel room with basic details \\
% & Required parameters: \{"hotelID": "Hotel Identifier", "checkIn": "Check-in Date", "checkOut": "Check-out Date", "roomCount": "Number of Rooms"\} \\
% & Returns: \{"status": "Reservation Status", "details": "Reservation Details"\} \\
% \cline{2-2}
% & 4. simple\_hotel\_booking \\
% & Description: Book a hotel room with basic details \\
% & Required parameters: \{"hotel\_id": "Hotel Identifier ID", "checkInDate": "Check-in Date", "checkOutDate": "Check-out Date", "guestName": "Guest Name"\} \\
% & Returns: \{"reservation\_status": "Reservation Status", "confirmation\_number": "Booking Confirmation Number"\} \\
% \cline{2-2}
% & 5. basic\_hotel\_reservation \\
% & Description: Reserve a hotel room with essential information \\
% & Required parameters: \{"hotel\_id": "ID of the Hotel", "checkInDate": "Date of Check-in", "checkOutDate": "Date of Check-out", "guestName": "Guest's Name"\} \\
% & Returns: \{"reservation\_status": "Reservation Status", "confirmation\_number": "Confirmation Number"\} \\
% \hline
% \multicolumn{2}{|l|}{--------------------------------------------------------------------------------------------------------} \\
% User query: & Can you update the check-in and check-out dates for my hotel reservation with ID 12345? \\
% \hline
% \multicolumn{2}{|l|}{--------------------------------------------------------------------------------------------------------} \\
% Gold API call: & hotel\_booking\_modification \\
% Args: & \{"confirmation\_number": "12345", "check\_in": "2023-10-01", "check\_out": "2023-10-05"\} \\
% \hline
% \multicolumn{2}{|l|}{--------------------------------------------------------------------------------------------------------} \\
% Predicted API call: & hotel\_booking\_modification \\
% Args: & \{"confirmation\_number": "12345", "check\_in": "2023-10-15", "check\_out": "2023-10-20", "room\_type": "Deluxe", "number\_of\_rooms": 1\} \\
% \hline
% \end{tabular}
% \caption{Hotel Booking API Documentation and Usage Example}
% \label{tab:hotel-booking-api-docs}
% \end{table*}


\end{document}
