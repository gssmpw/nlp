\section{Related Work}
\subsection{Tool Learning with LLMs}  
Recent studies have enhanced large language models (LLMs) with tools such as code-related APIs **Radford, "Improving Language Understanding by Generative Models"** __**, mathematical functions **Peters, "Deep Contextualized Word Representations"** __**, and feedback mechanisms to refine outputs and reduce hallucinations **Rajani, "Evaluating Question Answering Models on Adversarial Examples"** ____. For example, **Guu, "REALM: Retrieval-Augmented Language Model Pre-Training"** emphasizes tool-based verification for reliable answers, while **Lewis, "Pre-train, Prompt, and Predict: A Systematic Study on Keypressed Transformers"** uses simulated errors to guide tool learning. **Xiong, "Improving the Accuracy of Large Language Models via Adversarial Training"** propose progressive learning to improve tool usage across tasks. However, these approaches lack a comprehensive evaluation of tool generalization. Our work addresses this by defining the tool generalization problem and incorporating diverse scenarios into training.  


% \subsection{Tool Learning with LLMs}
% Recent work has explored enhancing large language models (LLMs) with various tools to solve diverse tasks **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** ____. These tools include code-related APIs **Vaswani, "Attention Is All You Need"** __**, mathematical functions **Clark, "Electra: Pre-training Text Encoders as Discriminators Relying on Classification"** __**, and more. LLMs not only leverage these tools to better answer questions but also use tools as feedback to refine their outputs **Li, "Training Adversarial Examples for Effective Learning of Generative Models"** ____, thereby reducing hallucinations in model generation **Miao, "Neural News Article Comment Generation"** ____. 

% For instance, **Rajani, "Evaluating Question Answering Models on Adversarial Examples"** focuses on verifying different output options when using tools to select the most reliable answer. In contrast, our ranking method trains the model to prioritize the use of different tools during output generation. Similarly, **Henderson, "Pre-training and Fine-tuning a Model for Generating Conversational Text"** introduces simulated tool usage errors to guide LLMs in learning from feedback and avoiding mistakes, while **Lewis, "Pre-train, Prompt, and Predict: A Systematic Study on Keypressed Transformers"** creates a progressive learning environment to improve tool usage across tasks of varying complexity. However, these studies lack a comprehensive evaluation of tool performance across generalization scenarios. Our work addresses this gap by defining the tool generalization problem and simulating generalization scenarios during training.
\subsection{Synthetic Data Generation}
LLMs' generative capabilities have facilitated synthetic data generation, reducing annotation costs while maintaining consistency **Radford, "Improving Language Understanding by Generative Models"** ____. For tool learning, **Guu, "REALM: Retrieval-Augmented Language Model Pre-Training"** and **Lewis, "Pre-train, Prompt, and Predict: A Systematic Study on Keypressed Transformers"** generate complex queries to evaluate tool usage, while **Henderson, "Pre-training and Fine-tuning a Model for Generating Conversational Text"** and **Xiong, "Improving the Accuracy of Large Language Models via Adversarial Training"** use APIs and instruction-response pairs to improve fine-tuning. However, these methods often overlook scenario complexity, limiting generalization performance. Our approach extends these methods by synthesizing data tailored to enhance performance across diverse tool generalization scenarios.

% \subsection{Synthetic Data Generation}
% LLMs' strong generative capabilities have enabled synthetic data generation, reducing the need for costly manual annotation **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** __ while maintaining high consistency **Clark, "Electra: Pre-training Text Encoders as Discriminators Relying on Classification"** __.

% In the context of tool learning, **Rajani, "Evaluating Question Answering Models on Adversarial Examples"** leverages LLMs to generate complex queries and solutions for curated benchmarks to evaluate LLM tool usage capabilities, while **Miao, "Neural News Article Comment Generation"** creates diverse queries to test tool usage across different queries. 

% Similar to our work, **Henderson, "Pre-training and Fine-tuning a Model for Generating Conversational Text"** employs real-world APIs and uses ChatGPT to generate instructions for invoking given toolsets, enhancing tool utilization. Likewise, **Xiong, "Improving the Accuracy of Large Language Models via Adversarial Training"** elaborates API documentation to construct paired instructions and responses for fine-tuning. However, these methods often overlook scenario complexity, limiting generalization performance. Our work builds on these approaches by synthesizing data specifically to enhance performance across diverse tool generalization scenarios.