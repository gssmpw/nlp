\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl} % review, final
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{adjustbox}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{hyperref}

\usepackage{xspace}

\usepackage{enumitem} \setlist{nosep, left=\parindent} 
\usepackage[ruled, vlined, noend]{algorithm2e}


\newcommand{\nrk}[1]{\textcolor{blue}{NRK: #1}}
\newcommand{\pld}[1]{\textcolor{purple}{PLD: #1}}
\newcommand{\sparta}{\texttt{SpaRTA}\xspace}
\newcommand{\lora}{\texttt{LoRA}\xspace}

\newcommand{\gemma}{\texttt{gemma-2b}\xspace}
\newcommand{\gemmait}{\texttt{gemma-2b-it}\xspace}
\newcommand{\mistral}{\texttt{mistral-7b}\xspace}
\newcommand{\mistralit}{\texttt{mistral-7b-it}\xspace}

\newcommand{\eat}[1]{}

\DeclareMathOperator*{\softmax}{softmax}


\title{Sparsity May Be All You Need:\\Sparse Random Parameter Adaptation} 


\author{Jesus Rios, \, Pierre Dognin, \, Ronny Luss, \,  Karthikeyan Natesan Ramamurthy \\ 
IBM Research \\ 
 \{jriosal, pdognin, rluss, knatesa\}\!@us.ibm.com}  



\begin{document}

\maketitle

\begin{abstract}
Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including  LoRA, as well as full parameter fine-tuning.
\end{abstract}
 

 
\section{Introduction}

It has become common practice to train application-ready language models in two phases~\citep{radford2018gpt1, kenton2019bert}: first, the model is \emph{pre-trained} on a very large and general corpus of (unlabeled) text; then further trained (or \emph{fine-tuned}) on a smaller specific set of examples demonstrating the intended behavior for a particular application, such as instruction following~\citep{ouyang2022training}. 

Overall, supervised fine-tuning (SFT) requires less computational resources than pre-training (PT) due to the significantly smaller size of the training set as well as the typical use of \emph{early stopping} to deal with overfitting. This means orders-of-magnitude less gradient computations and parameter updates are needed during SFT compared to PT. 
However, a major drawback is that memory requirements remain the same, unless a parameter-efficient fine-tuning (PEFT) technique is used. The main memory bottleneck during training is the number of trainable parameters, since additional memory must be allocated for their gradients and other per-parameter statistics needed by the optimizer.
The idea behind PEFT~\citep{lialin2023scaling} is to significantly reduce the number of trainable parameters during fine-tuning while maintaining performance.

\begin{figure}[t!]
\centering
        \includegraphics[width=0.9\linewidth]{sparta.png}
    \caption{The proposed \sparta method which randomly chooses a subset of parameter indices vector $\phi$ from pre-trained model parameters $\theta_{\text{PT}}$ and updates them via adapter $\Delta_{\phi}$.}
    \label{fig:sparta}
\end{figure}


Low Rank Adaptation (\lora), first introduced by~\citet{hu2021lora}, currently remains the most popular PEFT technique. \lora freezes all the pre-trained model parameters $\theta_{\text{PT}}$ and introduces trainable low-rank matrices (e.g., $B$, $A$) for a pre-selected subset of matrix tensors to represent the changes ($\Delta = BA$) needed for adapting the model to a new task. The adapted model parameters are given by $\theta_{\text{PT}} + \Delta$. Memory and computational efficiency are achieved by optimizing only over the parameters of these newly added, but significantly smaller, matrices. 

The success of \lora begs one to ask what properties make this method perform well. Is the low-rank structure critical, i.e., does $\Delta$ need to be low-rank? Is it sufficient to constrain $\Delta$ to be low dimensional? A main goal of this paper is to investigate these questions. An abundance of research is going into new methods for structured $\Delta$ (see Section~\ref{s:related_work} below) and novel directions into unstructured methods for fine-tuning could open avenues in areas such as model merging \cite{model-soups, model-merging-fisher} or pluralistic alignment \cite{feng-etal-2024-modular}.

In this work, we propose a different approach where $\Delta$ is not factorized into low rank matrices but rather chosen to be a random subset of the model parameters. This \underline{Spa}rse \underline{R}andom parame\underline{T}er \underline{A}daptation ({\sparta}) method imposes a sparsity constraint on the adapter that can be easily controlled. By changing the desired sparsity, one can change the number of adaptation parameters. Regardless of how the selected parameters are sampled from the model parameters, subsequent updates only affect these parameters. This sparsity constraint and randomness of selected parameters is in contrast to techniques such as \lora that effectively affect all parameters in $\theta_{\text{PT}}$. See Figure \ref{fig:sparta} for an illustration of the method. Generally, one samples $m$ parameters from the pre-trained model $\theta_\text{PT}$, stores the indices of these parameters in $\phi$, and uses adapter $\Delta_\phi$ to fine-tune $\theta_\text{PT}$. 

To investigate the performance of \sparta, we build adapters with different sparsity levels and test on the GLUE classification tasks \citep{glue} as well as the IMDB dataset \citep{IMDBdataset}. \sparta is compared to a fully fine-tuned model (Full FT), and PEFT approaches including \lora.
\sparta is quite competitive compared to \lora given that it only modifies a small sparse number of model parameters. 


\section{Motivation}

\citet{yu2024SuperMario} look at the differences between a language model's parameters before and after fine-tuning, and demonstrate empirically that it is possible to randomly drop (i.e., set to zero) up to 99\% of these parameter changes without significantly affecting the model performance. The sparsity in the parameter changes represented by the tensor $\Delta$ is known as $\Delta$-sparsity.
  
This motivates our approach, \sparta, which produces a performant fine-tuned model by directly adapting only a small percentage of the pre-trained model parameters: \sparta randomly selects the (scalar) parameters to train and freezes the remaining parameters (i.e., setting the corresponding $\Delta$ values to zero). This parameter sparsity also helps in reducing overfitting, as pre-trained models typically have enough capacity to learn the often limited amount of (labeled) data used for fine-tuning. \eat{Indeed, the redundancy in the $\Delta$ parameters observed after full fine-tuning is plausibly the result of the methods applied to prevent overfitting (\nrk{What redundancy and we need some reference}).}
There is no guarantee of $\Delta$-sparsity in \lora, but this is a desired property since it reduces parameter interference~\citep{ties-merging} when merging fine-tuned, task-specific models into a single one that can perform all tasks simultaneously. 

One byproduct of $\Delta$-sparsity is sparse $\Delta$ updates which reduce gradient computations during training, ensure fast merge back of the sparse $\Delta$ into the model for inference, and ultimately same inference cost as the original model (like LoRA).
The properties of low memory, low computation needs in training, identical inference time as the original model are all quite desirable for an adaptation technique. \sparta has all these properties plus the unique added benefit of producing only sparse changes in a small number of the parameters of the original model that can be beneficial for merging multiple \sparta adapters.





\subsection{Is low-rank necessary for $\Delta$?}
\label{lora_justification_analysis}

In Appendix \ref{sec:need_for_lora}, we show that the weight matrix changes (i.e., $\Delta$) during full parameter fine-tuning are, in fact, not generally low-rank for capable models such as \gemma and \mistral. This indicates that LoRA works, not because of the low-rank constraint particularly, but rather due to the capacity reduction achieved since fine-tuning with LoRA is done with limited training data. Such insight also hints that any constraint which reduces the capacity of the original model could perform competitively, motivating our approach that selects a small number of parameters of the original model to be updated during training.






  
\section{Related work}
\label{s:related_work}
The last few years has seen many advances in fine-tuning Large Language Models (LLMs). Perhaps the most well-known method (also most used in practice) is \lora \cite{hu2021lora}, which has spurred many variants including DoRA -- which adapts only the directions of pre-trained weights \cite{dora2024}, VeRA -- which shares low-rank matrices across layers \cite{vera2024}, AdaLoRA -- which adaptively allocates parameter budgets among weight matrices according to their importance \cite{adalora2023}, and SoRA -- which dynamically adjusts the intrinsic rank during the adaptation \cite{sora2023}. This set of methods each have a different structure to the $\Delta$ being optimized with the commonality being the training of some function of low-rank matrices. For example, further expanding on VeRA \cite{vera2024}, the authors propose to learn $\Delta=DBEA$ where $A$ and $B$ are random low-rank matrices and $D$ and $E$ are diagonal matrices to be trained. 

Beyond adding structured parameters is the concept of fine-tuning a small subset of the total parameters of the model, i.e., sparse fine-tuning, where one must first decide which subset of parameters to fine-tune (similar to deciding which parameters in each layer to add adapters to in \lora). \citet{ansell2024scaling} start with an initial set of parameters and offer procedures to \emph{drop} (according to parameter magnitude) and \emph{grow} (according to gradient) the set, i.e., they learn the set of parameters to train. Alternatively, \citet{ma-etal-2024-sparsity} focus on the sparsity of neuron activations during training by pre-computing neuron importance scores and only including \emph{important} neurons in computations during training. \citet{ansell-etal-2022-composable} first fine-tune on all parameters, select the parameters that change the most, and then fine-tune again from scratch on the selected parameters. 
In contrast to these, our proposed \sparta method produces a performant fine-tuned model by directly adapting only a small percentage of the pre-trained model parameters chosen completely at random.

Yet another related direction is that of compression which results in sparse models; algorithms in this genre take a dense fine-tuned model with the goal of compressing it while maintaining performance. Compression (see \citet{compression_survey} for a survey) could be accomplished by quantization, low-rank approximation, pruning (i.e., removing neurons, attention heads, or even layers), or distillation. The focus of this paper, however, is on fine-tuning dense models rather than learning sparse models as in \citet{sparsity-may-cry}.





\section{\sparta: Adapting a Random Subset of Model Parameters}

Suppose the vectorized parameters of the pre-trained language model are $\theta_{\text{PT}} \in  \mathbb{R}^n$ where $n$ is the number of parameters, and full parameter fine-tuning (FT) of the model is performed with a labeled dataset characterizing a task. FT typically updates $\theta_{\text{PT}}$ using a stochastic first-order gradient-based optimization algorithm (e.g., Adam from \citet{diederik2015adam}) to maximize the conditional probability of the labels in the training dataset under the model. The FT model is then given by $\theta_{\text{FT}} = \theta_{\text{PT}} + \Delta_{\text{FT}}$ where $\Delta_{\text{FT}} \in  \mathbb{R}^n$. 

This has two drawbacks in terms of memory efficiency. First, the optimizer state can be large, e.g., the state of the Adam optimizer is $4$ times as large as the parameter space as it includes current parameter values, a moving average of gradients (per parameter), etc. 
Second, storing a new FT model takes as much memory as the PT model which could be an issue when many task-specific models are requested.


\sparta proposes to \textit{randomly} select a small subset of the model parameters to optimize while freezing the rest. The model parameters $\theta \in \mathbb{R}^{n}$ are partitioned into trainable $\theta_\text{T} \in \mathbb{R}^{m}$ and frozen $\theta_\text{F} \in \mathbb{R}^{n-m}$ ones, with $m\ll n $ being the number of parameters selected. 
This allows our approach to have a drastically lower memory footprint than FT (similar to \lora) by reducing the size of the optimizer state as well as faster training by reducing the number of gradients to compute. 

In our \sparta's implementation, we introduce: (i) non-trainable \textit{indices} $\phi \in \mathbb{R}^m$ containing the indices of the randomly selected elements in $\theta_\text{PT}$, and (ii) trainable parameters $\Delta_\phi \in \mathbb{R}^m$ representing the subset of $\Delta$ that \sparta learns at indices $\phi$. The pseudocode for \sparta is given in Algorithm~\ref{algo:sparta}. 

\begin{algorithm}[t!]
\SetAlgoLined
\KwIn{Pre-trained model with $\theta_\text{PT}\in\mathbb{R}^{n}$} 

\KwIn{Task labeled dataset $\mathcal{D}$}


\textbf{Sample $\phi$:} Indices of parameters to be optimized 

\textbf{Initialize:} $\Delta_\phi=0$

\While{validation loss has not converged} 
{

\textbf{1. Merge:} $\theta_\phi = \theta_\phi + \Delta_\phi$ 

\textbf{2. Compute loss:} Continue forward pass from $\theta_\phi$ using a batch of labeled data from $\mathcal{D}$

\textbf{3. Compute gradients} with respect to $\Delta_\phi$ using backpropagation 

\textbf{4. Unmerge:} $\theta_\phi = \theta_\phi - \Delta_\phi$, without recording this operation in the computational graph 

\textbf{5. Update $\Delta_\phi $} with the Adam Optimizer

}
\KwOut{$\phi, \,\Delta_\phi$}
\caption{{\bf \sparta}}
  \label{algo:sparta} 
\end{algorithm}

Whereas optimizing \lora requires computing gradients with respect to the $A$ and $B$'s used to compute  $\Delta$ (which constitute additional parameters, independent from $\theta_\text{PT}$), \sparta requires computing gradients with respect to $\Delta_\phi$, the changes over a subset of parameters chosen from $\theta_\text{PT}$ and indexed by $\phi$. This nuance is why \lora can trained with out-of-the-box optimizers, but \sparta requires a different implementation. For example, Step 4 in Algorithm~\ref{algo:sparta} must take into account the computational graph so as to not interfere with the forward pass to be computed at the following iteration. Specifically, this unmerge operation must not be recorded in the computational graph.

Generating the index set $\phi$ is done as follows. For each $i\in\{1,\ldots,n\}$, \sparta samples $x\sim \mbox{Bernoulli}(m/n)$ and includes $i\in\phi$ if $x=1$. Hence, \sparta uses $m$ trainable parameters in expectation. Step 5 also depends on the specific optimizer and settings being used; \sparta uses Adam in our implementation. Finally, inference is performed, similarly to \lora, by merging the fine-tuned $\Delta$ into $\theta_\text{PT}$ and then making the necessary forward passes. Thus, \sparta does not introduce any additional \emph{inference} latency when deploying the resulting fine-tuned model.





\section{Memory Usage} 
\label{mem_usage}

Recall that \sparta freezes $n-m$ ($m \ll n$) of the model parameters.
We define sparsity as $s = 1 - m/n \in (0, 1)$, the percentage of frozen model parameters (e.g., if $1\%$ of parameters are trainable, then the sparsity is $99\%$). Subsequently, density is defined as $k = m/n = 1 -s$, the percentage of trainable model parameters. 
In practice, for a chosen sparsity $s$, one can freeze a model parameter with probability $s$, expecting a total sparsity percentage of $s$ over all model parameters. Thus, in expectation, $k= m/n$ percent of the model parameters are chosen as trainable, for a total of $n \, k = m$ trainable parameters. 


For \sparta, only $\Delta_\phi$ (of size $m$) is trainable, which is significantly smaller than the total number of trainable parameters for FT since $m \ll n$. However, the indices of these randomly chosen model parameters must be recorded into $\phi$, adding to the memory requirements. Indices can be stored in $16$-bit integers for all the Transformer models \citep{attention-is-all-you-need} considered in this paper.


\sparta sparsifies neither the model \emph{head} (which is kept fully trainable) nor the \emph{embeddings} (which is kept frozen during training). The parameters in Transformer networks consist of bias vectors and two-dimensional weight matrices. Storing indices for all these trainable parameters would require at most $m\,(2  \times 16)$ bits of memory ($m$ parameters, two integers to index a 2-dimensional matrix, 16 bits per integer).  


The values in $\Delta_\phi$ are of the same type as the model parameters, e.g., using $16$-bit brain floating-point (\texttt{bfloat16}) tensors. Thus, \sparta requires up to $m \, (2 \times 16 + 16 )= 3 m \times 16 $ bits of extra memory to specify the index $\phi$ and delta $\Delta_\phi$ tensors. That is $ 3 \, k $ 
times more memory than the original model, which requires just $n \times 16$ bits for storing its parameters. 
For instance, using \sparta on a model with sparsity 80\%, 90\%, 95\% and 99\% would require up to 60\%, 30\%, 15\% and 3\% more memory, respectively.


% training advantage 

We next analyze\footnote{This analysis does not include memory requirements associated with model buffers (e.g., those used to track running statistics for layer normalization) because they are relatively small and the same for both the Full FT and \sparta.} 
\sparta's memory savings during training. Optimizing the full set of model parameters (FT) using Adam \citep{diederik2015adam} requires memory for the parameters, their gradients with their (adaptive) first and second moments estimates. This requires $4\,n \times 16$ bits of memory when using a \texttt{bfloat16} representation.



In contrast, \sparta only optimizes $\Delta_\phi$, requiring a total of $ m \, (4  \times 16 +  2 \times 16) + n \times 16$ bits of memory: 
(i) $m \, (4  \times 16)$ bits for the statistics needed by Adam to optimize $\Delta_\phi$ of size $m$, 
(ii) $m \, (2 \times 16)$ bits for $\phi$ with the indices identifying the model parameters associated with $\Delta_\phi$, 
and 
(iii) the PT model parameters ($n \times 16$ bits). 

Memory savings appear if and only if
\begin{align}
   m \, (4 \!\times\! 16 +  2 \!\times\!16) + n \times 16 &< 4n \!\times\! 16, \\ 
   k n \, (6 \!\times\! 16) & < 3n \!\times\! 16, \nonumber \\
   k & < \frac{1}{2}, \nonumber
\end{align}
that is, \sparta is a real PEFT method iff $k < 0.5$, i.e., a sparsity higher than 50\% is required ($s\!>\!0.5$), making less than 50\% of the model parameters trainable. For instance, using \sparta on a model with sparsity $s=$ 80\%, 90\%, 95\%, and 99\% requires
45\%, 60\%, 67.5\% and 73.5\%  less memory than full parameter FT, 
respectively, see Table~\ref{tab:memory_efficiency_2b}.




\begin{table}[!ht]
\footnotesize
    \centering
    \begin{adjustbox}{max width=\columnwidth}
    \begin{tabular}{cccccccc} 
    \toprule  
      Model & storage & Full FT&  \multicolumn{5}{c}{\sparta} \\ 
      %\cmidrule(lr){1-2} 
      %\cmidrule(lr){3-3} 
      \cmidrule(lr){4-8}
     $n$ & (on disk) & 0\% & 50\% & 80\%  & 90\% & 95\% & 99\% \\
	%\midrule
    \cmidrule(lr){1-2}
    \cmidrule(lr){3-3}
    \cmidrule(lr){4-8}
    2B & 4 & 16 & 16 & 8.8 & 6.4 & 5.2  & 4.2 \\
    7B & 14 & 56 & 56 & 30.8 & 22.4 & 18.2 & 14.8  \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Memory usage efficiency for \sparta during training for $n=\text{2B}$ and 7B parameter models. Memory in Gigabytes (GB) for storing the parameters on disk (storage) as well as for FT with Adam on (i) the full set of parameters (Full FT) equivalent to 0\% sparsity (ii) a sparse random subset (\sparta) for different sparsity percentages from 50\% to 99\%.} 
    \label{tab:memory_efficiency_2b}
\end{table}




\section{Experimental Setup}
We next detail the framework for conducting the experiments in Section \ref{s:experimental_results}. Motivation is first given for the tasks followed by a description of models to be used and how these models are adapted (with manipulations) for the desired tasks.

\subsection{Tasks}

The main experimental focus is on Natural Language Understanding (NLU) tasks, specifically \emph{sequence classification}, which involves classifying natural language sequences into a given number of classes.
NLU tasks are easier to evaluate than Natural Language Generation (NLG) tasks, as one can use simple metrics (Accuracy, F1, Recall, Precision, etc.) that can avoid the inherent ambiguity of evaluating more general generative tasks. While \sparta is also applicable to NLG tasks, they are not used in the following demonstrations due to the challenges associated with their evaluation, typically requiring human judgments as the gold standard measure of performance. Human evaluations can be expensive and time consuming, do not scale easily, and are not as \emph{objective} as NLU evaluations. 



\subsection{Language Models}
\label{LMs}

Starting with available open-weight language models, the goal is to adapt them to perform \emph{sequence classification}. Two types of trained models are used: \emph{base} and \emph{instruction}-tuned models, where the latter have additionally been trained to follow users' intent when prompted, such as answering questions or responding to instructions.
Specifically, we consider the following language models: \gemma and \gemmait from the Gemma~\citep{gemma} family, and \mistral and \mistralit from the Mistral\footnote{We  use v0.3 for both models.}~\citep{mistral7b} family, developed by Google and Mistral AI respectively. The \texttt{"it"} suffix refers to instruction-tuned models. They are of particular interest for our experiments as they will show results for models with two different numbers of parameters. All models are readily available text-to-text, decoder-only transformer models with open weights, downloadable from Hugging Face\footnote{\url{https://huggingface.co/}}(HF). 
Note that when using an \emph{instruction}-following (\texttt{it}) model, inputs are formatted according to the conventions established when training the model on instructions.



\subsection{Task Adaptation}
\label{task_adaption}

% input 
When using a \emph{base} model for sequence classification, the raw sequences to be classified are input directly into the model. However, when using an \emph{instruction} model, these sequences are first wrapped into a classification-specific instruction. For example, a possible instruction could be: ``Determine if the following sentence has a positive sentiment. Respond Yes or No.'', followed by the sequence itself to be classified.  

% model
A (generative) pre-trained Transformer model with a decoder-only architecture has a head that transforms the final hidden state of each token in the input sequence into vocabulary logits. 
Efficiently adapting this model for sequence classification requires the swap of this head for a sequence classification head, which uses only the final hidden state of the last token in the input sequence $h \in \mathbb{R}^d$ to do the classification. 
This reduces the parameter size of the head, which is just a linear layer with no bias applied to a token's final hidden state, from a weight matrix $W \in \mathbb{R}^{v \times d} $ to $W \in \mathbb{R}^{c \times d}$, where $d$ is the dimension of the model's hidden states (e.g., 2,048 and 4,096 for the \gemma and \mistral models respectively), $v$ is the number of tokens in the vocabulary (e.g., 256,000 for \gemma or 32,768 for \mistral), and $c$ is the number of classes in the individual task (e.g. $2$ to $3$ in the experiments that follow). With this small change, the model outputs classification probabilities for each whole input sequence through 
\begin{equation} \label{model-head}
p = \softmax(h\, W^T) \in \mathbb{R}^c.
\end{equation}
 
The classification head of our \emph{base} pre-trained models (i.e., \gemma and \mistral) is initialized with random weights, sampled from a normal distribution $\mathcal{N}(0,\sigma^2)$, with $\sigma$ chosen such that the model at initialization predicts either class with equal probability. This is achieved with $\sigma = 0.06/\sqrt{d}$. % d = fan_{in}
With this, the loss starts at approximately $-\log(1/2) \approx 0.693$ and the accuracy at $\approx 50\%$ for a balanced evaluation dataset. 

Regarding instruction-tuned models (i.e., \gemmait and \mistralit), the original vocabulary weights are rather reused when initializing their classification heads. To do so requires to first identify the tokens in the vocabulary that the model is expected to use for classification following the instruction. For example, these could be the tokens associated with a ``Yes'' or ``No'' response by the model. The embeddings in the original model (head) associated with those classification tokens are extracted and used to initialize the classification head. While many models tie their vocabulary heads to their tokens embedding matrices, these new classification heads are never tied to the model's input embedding matrix.   


\section{Experimental Results}   
\label{s:experimental_results}

Efficacy of \sparta is demonstrated empirically by comparing it to the following baselines:
\begin{itemize}
\item \textbf{Full parameter Fine-Tuning (Full FT)}: all model parameters are optimized. 
\item \textbf{\lora}: Only the additional matrices representing low-rank adapters  are optimized.  
\item \textbf{Head}: Only the classification head is optimized; all other parameters remain frozen.
\end{itemize}
This enables the exploration of the performance of \sparta on a range of sparsity levels, varying the number of trainable parameters. A complete set of results can be found in Appendix~\ref{additional_experiment_results} accompanied by a detailed description of the training setup in Appendices~\ref{training_details} and~\ref{training_parameters}. 

\subsection{Datasets}

Each sequence classification task considered in our experiments is given by a dataset of labeled examples. Table~\ref{tab:datasets} summarizes these datasets and their splits for training, development, and testing for both the GLUE classification tasks \citep{glue} and IMDB \citep{IMDBdataset}.
See Appendix~\ref{datasets_appendix} for more detailed descriptions.

\begin{table}[!h]
\centering
\begin{tabular}{lcrrr} 
 \toprule
 Dataset & Classes & Train & Dev & Test \\
 \midrule
  COLA  &  2 & 7,551 & 1,000  & 1,043  \\
  MNLI  &  3 & 100,000 & 10,000 & 19,647 \\
  MRPC  &  2 & 3,668 & 408 & 1725  \\
  QNLI  &  2 & 99,725 & 5,000  & 5,463 \\
  QQP   &  2 & 100,000 & 5,000 & 40,430  \\
  RTE   &  2 & 2,182 & 300 & 277 \\
  SST-2 &  2&   66,349 & 1,000 &  872 \\
  \midrule
  IMDB  &  2&   25,000 & 5,000 & 20,000 \\
\bottomrule 
\end{tabular}
\caption{Sequence classification datasets. Training sets limited to 100K samples. Training samples with > 256 tokens are removed (here using \gemma tokenizer, with \mistral tokenizer in Appendix~\ref{datasets_appendix}).}
\label{tab:datasets}
\vspace{-.25cm}
\end{table}

\subsection{IMDB}

\begin{table*}[htb]
\centering
\begin{adjustbox}{max width=0.85\textwidth}
\begin{tabular}{llcccccccccccc} 
 \toprule
  & & \multicolumn{2}{c}{\gemma} &  \multicolumn{2}{c}{\gemmait} &  
 \multicolumn{2}{c}{\mistral}  &   \multicolumn{2}{c}{\mistralit} \\ \cmidrule(lr){3-10}
  Adaptation & density (\%) & loss & acc. &  loss & acc. &  loss & acc. &  loss & acc. \\
 \midrule 
%PT   &              &--     &--       & 0.425 & 92.0\% & --    & --     & 0.435 & 86.1\% \\ \midrule 
Full FT & 100\%           & 0.092 &  96.9\% & 0.107 & 96.3\% & 0.080 & 97.4\% & 0.080 & 97.3\% \\ \midrule 
%\sparta (10\%)     & 0.094 &  96.8\% & 0.100 & 96.6\% & 0.077 & 97.4\% & 0.080 & 97.4\% \\ 
\sparta & 5\%      & 0.096 &  96.7\% & 0.105 & 96.3\% & 0.084 & 97.2\% & 0.081 & 97.5\% \\ 
%\sparta (1\%)      & 0.096 &  96.7\% & 0.104 & 96.4\% & 0.085 & 97.1\% & 0.083 & 97.1\% \\ 
\sparta & 0.5\%    & 0.096 &  96.7\% & 0.103 & 96.4\% & 0.087 & 96.9\% & 0.087 & 97.0\% \\ 
\sparta &  0.05\%  & 0.106 &  96.3\% & 0.114 & 96.2\% & 0.080 & 97.3\% & 0.076 & 97.4\% \\ \midrule 
\lora  & $\approx$0.05\%             & 0.101 &  96.5\% & 0.113 & 96.2\% & 0.086 & 97.1\% & 0.081 & 97.1\% \\ \midrule
Head  & $\approx$2e-4\%              & 0.165 &  93.6\% & 0.194 & 92.7\% & 0.139 & 94.9\% & 0.117 & 95.8\% \\  
\bottomrule 
\end{tabular}
\end{adjustbox}
\caption{Test loss and accuracy of models adapted to the IMDB dataset with different fine-tuning methods. For training details see Table~\ref{tab:imdb-training-params}.} 
\label{tab:imdb}
\end{table*}

Table~\ref{tab:imdb} shows the results for the IMDB classification task where each model is asked to rate a review as positive or negative. Each model is fine-tuned using adaptation techniques including: (i) Full FT where all model parameters are updated (100\% density); (ii) \sparta for different orders of magnitude of density levels 5\%, 0.5\%, $\approx$0.05\% -- the last one allowing for \sparta to have close to the same number of trainable parameters as for \lora; (iii) \lora with rank $r=8$ equivalent to $\approx 0.05\%$ of trainable parameters compared to the model full parameter size; (iv) Head adaptation where only the classification head is updated ($\approx$2e-4\% density). 

%%% GEMMA 2B IT
\begin{table*}[htb]
\centering
\centering
% \footnotesize
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccccccccc} 
\multicolumn{15}{c}{\textbf{\texttt{Model: \texttt{gemma-2-2b-it}}}} \\ 
\toprule
 & \multicolumn{2}{c}{\textbf{QNLI}} &  \multicolumn{2}{c}{\textbf{RTE}} &  
 \multicolumn{2}{c}{\textbf{SST2}}  &   \multicolumn{2}{c}{\textbf{QQP}} &
 \multicolumn{2}{c}{\textbf{MNLI}}  &   \multicolumn{2}{c}{\textbf{MRPC}} & \multicolumn{2}{c}{\textbf{COLA}} 
 \\ \cmidrule{2-15}
    & loss &acc.&  loss &acc.&  loss &acc.&  loss &acc.
    & loss &acc.&  loss &acc.&  loss & mcc\\ \midrule
\textbf{Full FT (100\%)}  & 0.46 & 81.2 & 0.33 & 87.4 & 0.18 & 93.3 & 0.36 & 85.5 & 0.26 & 88.9 & 0.12 & 96.1 & 0.36 & 63.6 \\ \midrule
\textbf{\sparta (5\%)}  & 0.45 & 79.8 & 0.33 & 87.6 & 0.18 & 93.0 & 0.40 & 81.3 & 0.24 & 89.9 & 0.21 & 95.6 & 0.41 & 57.2 \\ 
\textbf{\sparta (0.5\%)}  & 0.55 & 72.9 & 0.37 & 86.1 & 0.25 & 89.8 & 0.34 & 85.4 & 0.24 & 89.9 & 0.13 & 95.6 & 0.39 & 56.9 \\ 
\textbf{\sparta (0.037\%)}  & 0.45 & 79.8 & 0.38 & 85.5 & 0.18 & 93.0 & 0.40 & 82.3 & 0.27 & 88.6 & 0.12 & 95.8 & 0.45 & 56.5 \\ \midrule
\textbf{\lora (0.037\%)} & 0.46 & 78.3 & 0.33 & 87.5 & 0.24 & 90.4 & 0.36 & 84.8 & 0.26 & 89.0 & 0.13 & 95.5 & 0.39 & 59.9 \\ \midrule
\textbf{Head (1.63e-4\%)}  & 2.60 & 47.3 & 0.73 & 67.4 & 0.96 & 37.1 & 0.55 & 74.2 & 0.66 & 64.9 & 0.55 & 84.2 & 0.62 & 18.0 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Test loss and accuracy of model \texttt{google/gemma-2-2b-it} adapted to glue datasets with different fine-tuning methods. For training details see Table~\ref{tab:glue-training-params}.} \label{tab:gemma-it}
\end{table*}

In Table~\ref{tab:imdb}, the results for adaptation techniques (rows) are sorted in order of descending density as to show the impact of decreasing the number of trainable parameters on the overall performance. For \gemma and \gemmait, Full FT adaptation gives strong test loss and accuracy numbers, often the best available -- this is expected since all model parameters are fine-tuned. \sparta results at 5\% density are close to Full FT, even matching them for \gemmait. As the density decreases by orders of magnitude, the results slowly degrade. For $\approx$0.05\%, \texttt{gemma} models' performances are slightly behind or matching performance from \lora. Head adaptation provides the worse results as one would expect since it has so few parameters to work with. 

For \texttt{mistral} models, the trend is similar with Full FT adaptation showing strong loss and accuracy numbers. \sparta performs well, matching and even improving upon Full FT with a density of 0.05\%. \sparta even improves over \lora for both \mistral and \mistralit at this low density. Once again, Head adaptation gives the worst results. Overall these results are encouraging and show that \sparta is competitive and provides similar performance to \lora for low densities.


\subsection{GLUE}
 
%%% MISTRAL 7B IT
\begin{table*}[htb]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccccccccc} 
\multicolumn{15}{c}{\textbf{\texttt{Model: \texttt{Mistral-7B-Instruct-v0.3}}}} \\ 
\toprule
 & \multicolumn{2}{c}{\textbf{QNLI}} &  \multicolumn{2}{c}{\textbf{RTE}} &  
 \multicolumn{2}{c}{\textbf{SST2}}  &   \multicolumn{2}{c}{\textbf{QQP}} &
 \multicolumn{2}{c}{\textbf{MNLI}}  &   \multicolumn{2}{c}{\textbf{MRPC}} & \multicolumn{2}{c}{\textbf{COLA}} 
 \\ \cmidrule{2-15}
\multicolumn{1}{c}{}  & loss &acc.&  loss &acc.&  loss &acc.&  loss &acc.
    & loss &acc.&  loss &acc.&  loss &mcc\\ \midrule
\textbf{Full FT (100\%)}  & 0.32 & 91.7 & 0.26 & 91.3 & 0.14 & 95.7 & 0.33 & 88.6 & 0.23 & 90.3 & 0.11 & 96.7 & 0.32 & 70.5 \\ \midrule
\textbf{\sparta (5\%)}  & 0.30 & 89.9 & 0.23 & 91.4 & 0.12 & 95.8 & 0.28 & 88.9 & 0.23 & 90.5 & 0.10 & 96.7 & 0.30 & 72.1 \\ 
\textbf{\sparta (0.5\%)}  & 0.29 & 89.5 & 0.23 & 91.6 & 0.13 & 95.2 & 0.29 & 88.6 & 0.25 & 89.2 & 0.10 & 96.6 & 0.32 & 70.3 \\ 
\textbf{\sparta (0.048\%)}  & 0.27 & 89.9 & 0.24 & 91.1 & 0.12 & 95.9 & 0.31 & 87.6 & 0.25 & 89.3 & 0.10 & 96.6 & 0.32 & 69.9 \\ \midrule
\textbf{\lora (0.048\%)} & 0.32 & 88.8 & 0.26 & 90.4 & 0.12 & 95.6 & 0.32 & 87.2 & 0.27 & 88.5 & 0.11 & 96.9 & 0.30 & 71.3 \\ \midrule
\textbf{Head (1.15e-4\%)}  & 0.31 & 86.3 & 0.35 & 87.0 & 0.23 & 90.4 & 0.35 & 84.5 & 0.32 & 85.5 & 0.15 & 93.5 & 0.36 & 62.1 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Test loss and accuracy of the \texttt{mistralai/Mistral-7B-Instruct-v0.3} model adapted to glue datasets with different fine-tuning methods. For training details see Table~\ref{tab:glue-training-params}.} \label{tab:mistral-it}
\end{table*}

Table~\ref{tab:gemma-it} compares the results of \gemmait on 7 of the GLUE tasks for the same adaptation methods as in Table~\ref{tab:imdb}. An equivalent set of results for the \gemma \emph{base} model is given in Table~\ref{tab:gemma-pt}, see Appendix~\ref{additional_experiment_results}.
Here again, adaptation results (rows) are ordered in descending order of density with \sparta evaluated for different orders of magnitude of density levels 5\%, 0.5\%, 0.037\% -- the last one allowing \sparta to have approximately the same number of trainable parameters as \lora. \lora has rank $r=8$ equivalent to 0.037\% of trainable parameters; Head adaptation uses a very small 1.63e-4\% density. 
Overall, the same trend as for IMDB is observed. Full FT results are very often the best, sometimes bested by \sparta 5\% (RTE, MNLI). For a low density, where \lora and \sparta use the same number of trainable parameters, \sparta can best \lora (QNLI, SST2, RTE) but is overall competitive with \lora results. 

A similar set of results on the same GLUE tasks is provided for \mistralit in Table~\ref{tab:mistral-it}. Again, results for the \mistral PT \emph{base} model can be found in Table~\ref{tab:mistral-pt}, see Appendix~\ref{additional_experiment_results}. Full FT results can often be matched or bested by \sparta 5\% (RTE, SST2, QQP, MNLI, MRPC, COLA). \sparta often gives better results than \lora (QNLI, RTE, SST2, QQP, MNLI) for a low density (0.0048\%) where both techniques have comparable numbers of parameters.
Once again, \sparta is a competitive adaptation technique, often providing similar results to \lora with low density. 

\subsection{Remarks}

Results on GLUE and IMDB establish that \sparta is a viable adaptation technique that can be competitive with \lora, especially for larger LMs. These results demonstrate that a simple sparsifying scheme can offer a valid adaptation technique. This opens up the possibility for more investigation in sparse adaptation as the low rank approximation of a model's parameter changes is not the only mechanism to provide a performant adaptation method. This indicates that what matters is not necessarily the adapter structure used in PEFT techniques, but rather the number of independent parameters, that is relevant to the adaptation task at hand.



\section{Exploring Model Merging} % of sparsely modified models
\label{s:model_merging}
\citet{yu2024SuperMario} demonstrate that one can perform Full FT, compute $\Delta=\theta_\text{FT}-\theta_{\text{PT}}$, sparsify $\Delta$ to get $\Tilde{\Delta}$, and obtain good performance using $\Tilde{\theta}_\text{FT}=\theta_{\text{PT}}+\Tilde{\Delta}$ as model parameters. Furthermore, they show that merging models via their sparsified $\Tilde{\Delta}$-s from different tasks can also maintain the performance of the individual models. This motivates exploring model merging via \sparta, where no Full FT is required and sparse adapters are instead fine-tuned. 
\eat{
Adapting models to new tasks with our approach results in task-specific models for which only a small percentage of randomly chosen parameters are modified. 

If we were to merge these models (for example, by averaging their parameters), we would expect parameter conflicts to occur with very low probability; almost negligible probability if sparsification is high enough. This allow us to easily merge multiple models fine-tuned with our approach into a single model, as far as they are initialized from the same pre-trained model. 
}

In our setup, because the heads of our \sparta-tuned models are fully trained, merging them into a single head would create parameter interference. We thus merge everything except the heads and add each unmerged head on top of the merged model creating a multi-head model. \eat{If we wanted to merge the heads as well, we recommend sparsifying their trainable parameters. However, our goal here is to create a single model that simultaneously performs all tasks in parallel over the same input.  } We need the merged model to produce multiple outputs (i.e., one for each task) per input and this is exactly what a multi-head merged model does. In the forward pass, a sequence of input tokens goes through the merged model, producing a final state representation of the tokens. This representation is shared by all tasks and is passed to each head to produce one prediction per task as output. This way all tasks are processed concurrently on the same input. 
\eat{
We demonstrate the merging capabilities of models fine-tuned with our approach with a simple experiment. Suppose we want to classify English sentences against two criteria: sentiment (positive\,/\,negative) and grammatical acceptability; and, we have already fine-tuned two classifiers with our SRP-FT method: one for sentiment classification using the SST-2 (Stanford Sentiment Treebank, binarized) dataset; and another for grammatical acceptability using the CoLA (Corpus of Linguistic Acceptability) dataset. Both classification models are based on the same pre-trained model: Gemma 2B. 
}

We experiment merging two models: one individually fine-tuned with \sparta for SST2 (classifying text sentiment) and another for COLA (classifying grammatical correctness), using \gemma as PT model and 99\% sparsity in both cases. We merge these two models by adding the respective $\Delta_\phi$ to their PT model's original parameters $\theta_\text{PT}$, and obtaining a two-headed model that can simultaneously classify any input sentence on both criteria in a single forward pass. Table~\ref{tab:merging_example} compares performance of the merged model with that of the unmerged models. This result encourages future exploration of this practical usecase for \sparta.

\begin{table}[!h]
\footnotesize
\centering
\begin{tabular}{ccccc} 
 \toprule
% & \multicolumn{4}{c}{Datasets}  \\
 &\multicolumn{2}{c}{SST2} & \multicolumn{2}{c}{COLA} \\
 \cmidrule{2-5}
Model & loss & acc & loss & acc \\ 
 \midrule
SST2  & 0.113 & 96.3\% & - & - \\
COLA  & - & - &0.487 & 82.7\% \\
\midrule
Merged  &0.118 & 96.7\%  & 0.428 & 84.4\% \\
 \bottomrule 
\end{tabular}
\caption{Test loss an accuracy of SST2 and COLA models fine-tuned on \gemma using \sparta with 99\% sparsity. The merged model outperforms both individual models in this scenario.} \label{tab:merging_example}
\vspace{-.25cm}
\end{table}


\section{Conclusion}
\label{ref:concl}
As PT language models have grown in size, PEFT has become crucial for enabling fine-tuning large PT language models on limited hardware and financial resources. We have introduced \sparta, an approach that sharply decreases the set of trainable parameters, reducing the GPU memory used by the optimizer and speeding up training. We have demonstrated on a variety of task adaptation scenarios that our fine-tuning approach is parameter-efficient and competitive with \lora, the current PEFT standard. Experiments with 2B and 7B parameter pre-trained models demonstrate good performance, and as per \citet{yu2024SuperMario}, we expect larger models to allow for higher levels of sparsity in training; meaning that efficiency of \sparta  should get better with larger model sizes as also suggested in Table~\ref{tab:memory_efficiency_2b}. 

Regarding future directions, while \sparta has been applied to \emph{supervised learning}, it is also amenable to \emph{reinforcement learning} often used for model alignment \citep{NEURIPS2022_b1efde53}. Furthermore, \sparta allows one to select which modules to sparsify and at what layers. Like \lora, this decision affects both the number of trainable parameters and the performance of the fine-tuned model, and we plan to investigate its impact in \sparta. Lastly, we plan to explore model merging as discussed in Section \ref{s:model_merging}; \sparta opens the potential for model merging without Full FT but with possibly little interference. 



\section{Limitations} 
We have demonstrated various benefits of \sparta, including low memory and high performance. Regarding limitations, questions remain about how to best deal with overfitting, though we have some insights. We have observed in our experiments that as we increase the sparsity level, and reduce, in turn, the number of trainable parameters, there is less overfitting. Indeed, there is a point in which both the training and validation losses converge together without significantly diverging from each other, eliminating the need for explicit overfitting mitigation techniques. Moreover, further increasing of the sparsity level beyond this point results in underfitting. Thus, we can think of our approach as a technique to improve generalization by limiting a model's capacity to overfit to the training data. However, finding the breaking point at which this happens requires expensive experimentation. We leave as future work the investigation of how such an optimal sparsity level depends on the model, dataset sizes, and task complexity. Knowing this relation will allow us to determine in advance how much sparsity in the trainable parameters is needed for reducing the capacity of a large model to a point where it learns a new task on a relatively small amount of examples without overfitting.





\bibliography{main.bbl}


\appendix


\section{Ranks of differences in fine-tuned weight matrices}
\label{sec:need_for_lora}

\lora is based on the idea that the changes in the model parameters after adapting it to a new task can have a low-rank approximation. To see if this is the case, we can easily check this presumption in a few well-known fine-tuned models by comparing their weight matrices before and after fine-tuning. Thus, for every weight matrix $W_{\text{PT}}$ (i.e.\ a $2$-dimensional trainable tensor) in the \emph{pre-trained} model, we compute the \emph{delta} matrix $\Delta = W_{\text{FT}} - W_{\text{PT}}$, where $W_{\text{FT}}$ is the corresponding weight matrix after \emph{fine-tuning} the model over all the original model parameters using standard supervised (or reinforcement) learning. \lora assumes that these matrices don't change or their differences given by $\Delta$ are low-rank.

We compute all delta weight matrices for two well-known \emph{instruction-following} fine-tuned models: \gemmait and \mistralit, see Section~\ref{LMs}. As we can see in Tables~\ref{tab:delta_rank_gemma-2b} and~\ref{tab:delta_rank_mistral-7b}, the feedforward (MLP)  \emph{delta} $\Delta$-matrices associated with each layer of the transformer network are all full rank in both models. This is also the case for all self attention key (k) and value (v) projection matrices. The self attention query (q) and output (o) $\Delta$-matrices all show relatively small rank deficiencies: between $9$ and $2$ for the query projection matrices and between $46$ and $3$ for the output projection matrices, out of a potential maximum rank of $2,048$ for the \gemmait model; and between $1,788$ and $28$ for the query projection matrices and between $246$ and $10$ for the output projection matrices, out of a potential maximum rank of $4,096$ for \mistralit model. The token embedding $\Delta$-matrix for \mistralit is full rank. However, it has a rank of $97$ for \gemmait, being the only matrix for that model whose delta change shows a significant rank deficiency (of $1,951$). Of the two models, \mistralit is the only one that does not tie its tokens embedding weights to its head; so that \mistralit model's head (untied LM head), which is trained independently from the tokens embedding, has a rank deficiency of $1$ for its delta change. Basically, we observe that the fine-tuning changes in all weight matrices of these models are full rank or very close to it, with the only exception being the changes in the tokens embedding for \gemmait which are significantly low rank. 
  
We analyzed here the changes in fine-tuned weight matrices for two popular open-weights language models. As discussed in Section~\ref{lora_justification_analysis}, we compared instruction-following models with their original, pre-trained versions, by computing the differences (delta) in their weight matrices. Tables~\ref{tab:delta_rank_gemma-2b} and~\ref{tab:delta_rank_mistral-7b} provide a detailed breakdown of the ranks for all these delta matrices. The observed ranks suggest that constraining the (delta) changes in weight matrices to be low-rank is not essential for fine-tuning models efficiently.

\begin{table*}[p]
    \centering
    \begin{tabular}{lcccc} 
    	\toprule         
	Weight matrix & dims & Layer(s) & Rank & Rank deficiency \\   % rank deficiency: the difference between the lesser of the number of rows and columns, and the rank.
	\midrule
	tokens embed & [ 256000,   2048 ]   &  - & 97     & 1951 \\
	\midrule % W_q, W_k, W_v, W_o, W_{mlp}
	self attn q proj   & [ 2048,   2048 ] & 0  &2039   & 9 \\
		                                            && 15& 2041& 7\\
	                                                    && 14& 2042& 6\\	
	                                                    && 6& 2044& 4\\	                                                    
	                                                    && 1,3-5,7,8,10-13,16,17& 2045 & 3\\
	                                                    && 2,9& 2046 & 2\\
	\midrule
	self attn k proj   &[ 256, 2048 ]   & 0-17  &256     &0 (full rank)\\
	\midrule
	self attn v proj   &[ 256, 2048 ]   & 0-17  &256     &0 (full rank)\\
	\midrule
	self attn o proj   &[ 2048, 2048 ]  & 0  & 2002 & 46\\
	                                                    && 16 & 2040 &  8\\	
	                                                    && 11,12,14& 2042 &  6\\	      
	                                                    && 6,8,10,13,15& 2043 &  5\\
	                                                    && 3-5,7,9,17& 2044 &  4\\                                                                                                 
	                                                    && 1,2  & 2045 &  3\\
        \midrule 
	mlp gate proj     &[ 16384,   2048 ] & 0-17  &2048   & 0 (full rank)\\
	\midrule
	mlp up proj        &[ 16384,   2048 ] & 0-17  &2048   & 0 (full rank)\\
	\midrule
	mlp down proj   & [ 2048,  16384 ] & 0-17  &2048   & 0 (full rank)\\
    	\bottomrule
    \end{tabular}
    \caption{Rank of the change differences in \gemmait model weight matrices after full fine-tuning.}
    \label{tab:delta_rank_gemma-2b}
\end{table*}

\begin{table*}[p]
    \centering
    \begin{tabular}{lcccc} 
    	\toprule         
	Weight matrix & dims & Layer(s) & Rank & Rank deficiency \\   % rank deficiency: the difference between the lesser of the number of rows and columns, and the rank.
	\midrule
	tokens embed & [ 32768,   4096 ]   &  - & 4096     & 0 (full rank) \\
	\midrule % W_q, W_k, W_v, W_o, W_{mlp}
	self attn q proj   & [ 4096, 4096 ] &    0  &2308 & 1788 \\
		                                            && 1 & 3103 &  993 \\
	                                                    && 3 & 4023  &   73 \\  
	                                                    && 31& 4046 &   50 \\
	                                                    && 30& 4050 &   46 \\	                                                    	            
	                                                    && 2,4& 4051 &  45\\        
		                                            && 6 & 4052  &   44\\         
	                                                    && 29 &4056 &   40 \\	        
	                                                    && 28 & 4057 &  39 \\	                                                           
	                                                    &&11,27&4058& 38 \\        
	                                                    && 8,9 & 4059 & 37\\
	                                                    && 10,19,25,26 &4061& 35 \\	
	                                                    && 12,18,23&4062 & 34 \\	                                                    
	                                                    && 5,7,14,17& 4063 & 33\\
	                                                    && 13,20,22 & 4064 & 32\\
	                                                    && 15 & 4065 & 31 \\
	                                                    &&  21,24 & 4066 &30 \\
	                                                    && 16 & 4068 &28 \\                                                    	                                                    
	\midrule
	self attn k proj   &[ 1024, 4096 ]   & 0-31  &1024     &0 (full rank)\\
	\midrule
	self attn v proj   &[ 1024, 4096 ]   & 0-31  &1024     &0 (full rank)\\
	\midrule
	self attn o proj   &[ 4096, 4096 ]  & 0  & 3850 & 246\\
	                                                    &&31& 4059 & 37 \\	
	                                                    &&1,30& 4060 & 36 \\	      
	                                                    &&14& 4068 & 28 \\
	                                                    &&18& 4069 & 27 \\                                                                                                 
	                                                    &&23& 4070 & 26 \\	
	                                                    &&2,10,26& 4071 & 25 \\	
	                                                    &&8,27& 4072 & 24 \\	      
	                                                    &&3,9&4074& 22 \\
	                                                    &&4,11,29& 4075 & 21 \\                                                                                                 
	                                                    &&6,13,22,28&4076& 20 \\	
	                                                    &&7,12& 4077 & 19 \\	
	                                                    &&15,16,19& 4079 & 17 \\	      
	                                                    &&17& 4080 & 16 \\
	                                                    &&5& 4081 & 15 \\                                                                                                 
	                                                    &&25& 4083 & 13 \\
	                                                    &&21,24& 4085 & 11 \\	
	                                                    &&20& 4086 & 10 \\
        \midrule 
	mlp gate proj     &[ 14336,   4096 ] & 0-31  &4096   & 0 (full rank)\\
	\midrule
	mlp up proj        &[ 14336,   4096 ] & 0-31  &4096   & 0 (full rank)\\
	\midrule
	mlp down proj   & [ 4096,  14336 ] & 0-31  &4096   & 0 (full rank)\\	
	\midrule
	untied lm head& [ 4096, 32768 ]  & - & 4095  &  1 \\
    	\bottomrule
    \end{tabular}
    \caption{Rank of the change differences in \mistralit model weight matrices after full fine-tuning.}
    \label{tab:delta_rank_mistral-7b}
\end{table*}
   
\section{Additional Dataset details}   
\label{datasets_appendix}

\begin{table}[!h]
\centering
\begin{tabular}{lrrrr} 
 \toprule
 Dataset & Classes & Train & Dev & Test \\
 \midrule
  COLA  &  2 & 7,551 & 1,000  & 1,043  \\
  MNLI  &  3 & 100,000 & 10,000 & 19,647 \\
  MRPC  &  2 & 3,668 & 408 & 1,725  \\
  QNLI  &  2 & 99,701 & 5,000  & 5,463 \\
  QQP   &  2 & 100,000 & 5,000 & 40,430  \\
  RTE   &  2 & 2,165 & 300 & 277 \\
  SST-2 &  2&   66,349 & 1,000 &  872 \\
  \midrule
  IMDB  &  2&   25,000 & 5,000 & 20,000 \\
\bottomrule 
\end{tabular}
\caption{Sequence classification datasets. Training sets limited to 100K samples. Training samples with more than 256 tokens are removed (here using \mistral tokenizer).}
\label{tab:datasets_app}
\end{table}

\textbf{IMDB}: contains a sample of ``highly polar'' movie reviews obtained from the online Internet Movie Database (IMDb) website.  IMDb registered users provide a rating (from $1$ to $10$) with each review. The reviews are binary (positive/negative) labeled with their sentiment, defined from user ratings~\citep{IMDBdataset}. Reviews with a rating higher or equal than $7$ are given a positive label; and a negative label if the rating is lower or equal than $4$. No reviews with ratings beyond these ranges are present in the dataset, which was constructed to have an equal number of positive and negative reviews, so guessing randomly yields 50\% accuracy. 

\textbf{GLUE}: Refer to \citet{glue} which contains descriptions and details on all GLUE datasets.




Table~\ref{tab:datasets_app} shows the actual splits used in our experiments for the IMDB and GLUE datasets, where the training data was filtered using the \mistral tokenizer by a threshold in the maximum token length of an network input. The only differences are for the training splits of the MRPC, QNLI, and RTE datasets.


\section{Training details}
\label{training_details}
   
We adapted each (generative) pre-trained model in Section~\ref{LMs} to do sequence classification as per the examples in any of the datasets from Table~\ref{tab:datasets}, using different supervised fine-tuning methods, including \sparta. Base models were adapted by switching their vocabulary heads to randomly initialized sequence classification heads with $c$ output classification tokens. For instruction models, we re-used the vocabulary heads as described in Section~\ref{task_adaption}. 
   
The examples (e.g.,\ text extracts, sentences) to be classified were converted into sequences of tokens before passing them as inputs to a model. We wrapped each example into an instruction to take advantage of instruction-following models, adding to the token length of the model inputs. We tried to keep such instructions as short as possible while still achieving a good initial performance before fine-tuning. For instance, after tokenization, the maximum token length of a training input from the SST-2 dataset was 
\begin{itemize}
\item 67 (without) and 87 (with instruction) for the \gemmait;   
\item 72 (without) and 95 (with instruction) for \mistralit.   
\end{itemize}
We observe here how the Gemma's tokenizer compresses more the input than Mistral's because of its larger vocabulary size; 256,000 (Gemma) vs. 32,767 (Mistral). 
 

We tokenized all training examples before starting the fine-tuning and looked at the histogram of their token lengths. To avoid batches with too much padding and improve training efficiency, we dropped those training examples with a disproportionate large token length, i.e., corresponding to a tail of extreme values in the histogram. We only do this for the training data since evaluation (on the development or test data) requires much less compute and memory (no gradients need to be calculated and stored) and is performed less frequently. We will indicate in Tables~\ref{tab:datasets} and~\ref{tab:datasets_app} the final splits after filtering the training data this way, with the final number of training examples used for the fine-tuning. 

With \sparta, we froze the token embeddings layer, made fully trainable the classification head, and randomly chose a sparse proportion of (scalar) parameters to be trained in all the other layers of a model. We demonstrated our method with varying density levels. The total average number of trainable parameters in each case was approximately:
$99$M (5\% density), $10$M (0.5\%) and $800$k (0.037\%) for the (base and instruct) Gemma 2B models; and  
$349$M (5\% density), $35$M (0.5\%) and $3.5$M (0.048\%) for the (base and instruct) Mistral 7B models.

For \lora, we factorized the changes in the query and value self-attention projection weight matrices with rank $r = 8$ decomposition matrices, which were optimized while keeping all other model parameters frozen. The number of new trainable parameters introduced by the \lora approach in the Gemma 2B and Mistral 7B models were approximately $925$k and $3.5$M, respectively. Also, we set $\alpha = 16$ to scale the \lora adapters. 



We observed overfitting when training with Full parameter FT: the development loss started deteriorating after a few epochs (e.g., approximately $2$ for SST-2) while the training loss went quickly to zero. We used a combination of early stopping, dropout and weight decay to deal with overfitting. We noticed that \sparta is a natural regularizer: increasing sparsity resulted in less overfitting, to a point in which there was no more overfitting (e.g., this was achieved at a sparsity $s>99\%$ for all models under consideration). In general, overfitting was more noticeable with the Mistral 7B models; as expected since they are larger than the Gemma 2B models. We also noticed that larger models require higher sparsity levels to eliminate overfitting given the same training data. On the other hand, Head adapation always resulted in significant underfitting, making this fine-tuning method always underperform.

We also observed that \sparta achieves a speedup over Full FT for sparsity levels greater than 90\%. Thus, at some sparsity level between 80\% and 90\%, computing gradients on a smaller amount of trainable parameters starts to offset the extra compute of adding and subtracting the deltas before and after the forward pass respectively. These extra operations require slicing the original parameter tensors over the randomly selected indices. 


\section{Additional Experiment Results}
\label{additional_experiment_results}

Tables~\ref{tab:gemma-pt} and Table~\ref{tab:mistral-pt} present the complete set of results on GLUE for the PT \emph{base} models \gemma and \mistral, respectively.    



%%%% GEMMA-2-2B 
\begin{table*}[h]
\centering
% \footnotesize
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccccccccc} 
\multicolumn{15}{c}{\textbf{\texttt{Model: \texttt{google/gemma-2-2b}}}} \\ 
\toprule
 & \multicolumn{2}{c}{\textbf{QNLI}} &  \multicolumn{2}{c}{\textbf{RTE}} &  
 \multicolumn{2}{c}{\textbf{SST2}}  &   \multicolumn{2}{c}{\textbf{QQP}} &
 \multicolumn{2}{c}{\textbf{MNLI}}  &   \multicolumn{2}{c}{\textbf{MRPC}} & \multicolumn{2}{c}{\textbf{COLA}} 
 \\ \cmidrule{2-15}
\multicolumn{1}{c}{}  & loss &acc.&  loss &acc.&  loss &acc.&  loss &acc.
    & loss &acc.&  loss &acc.&  loss &mcc\\ \midrule
\textbf{Full FT}         &  0.20 &  92.6 &  0.43 &  80.1 &  0.11 &  96.7 &  0.28 &  88.3 &  0.36 &  86.4 &  0.33 &  86.1 &  0.41 &  60.3 \\\midrule
\textbf{\sparta (5\%)}   &  0.15 &  94.4 &  0.46 &  80.5 &  0.11 &  96.8 &  0.23 &  90.1 &  0.28 &  89.4 &  0.38 &  83.4 &  0.34 &  64.6 \\
\textbf{\sparta (0.5\%)} &  0.15 &  94.1 &  0.61 &  69.0 &  0.12 &  96.3 &  0.24 &  89.8 &  0.30 &  89.0 &  0.38 &  83.4 &  0.38 &  62.2 \\
\textbf{\sparta (0.037\%)} &  0.29 &  88.3 &  0.69 &  57.8 &  0.11 &  96.2 &  0.30 &  86.9 &  0.47 &  81.1 &  0.58 &  70.3 &  0.52 &  42.1 \\\midrule
\textbf{\lora (0.037\%)}  &  0.17 &  93.6 &  0.59 &  69.0 &  0.11 &  96.1 &  0.26 &  88.6 &  0.30 &  88.6 &  0.36 &  84.5 &  0.36 &  61.7 \\\midrule
\textbf{Head (1.63e-4\%)}  &  0.56 &  71.8 &  0.68 &  60.3 &  0.32 &  89.1 &  0.43 &  79.9 &  0.89 &  58.7 &  0.59 &  70.4 &  0.53 &  33.2 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Test loss and accuracy of model \texttt{google/gemma-2-2b} adapted to glue datasets with different fine-tuning methods. For training details see Table~\ref{tab:glue-training-params}.} \label{tab:gemma-pt}
\end{table*}

%%%% MISTRAL-7B 
\begin{table*}[h]
\centering
%\footnotesize
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccccccccc}
\multicolumn{15}{c}{\textbf{\texttt{Model: \texttt{mistralai/Mistral-7B-v0.3}}}} \\
\toprule
& \multicolumn{2}{c}{\textbf{QNLI}} &  \multicolumn{2}{c}{\textbf{RTE}} &  
 \multicolumn{2}{c}{\textbf{SST2}}  &   \multicolumn{2}{c}{\textbf{QQP}} &
 \multicolumn{2}{c}{\textbf{MNLI}}  &   \multicolumn{2}{c}{\textbf{MRPC}} & \multicolumn{2}{c}{\textbf{COLA}} 
 \\ \cmidrule{2-15}
\multicolumn{1}{c}{}  & loss &acc.&  loss &acc.&  loss &acc.&  loss &acc.
    & loss &acc.&  loss &acc.&  loss &mcc\\ \midrule
\textbf{Full FT}         &  0.12 &  95.4 &  0.28 &  89.9 &  0.11 &  96.9 &  0.23 &  90.1 &  0.25 &  91.1 &  0.34 &  85.9 &  0.32 &  66.8 \\\midrule
\textbf{\sparta (5\%)}   &  0.12 &  95.5 &  0.34 &  86.3 &  0.11 &  96.9 &  0.23 &  90.3 &  0.25 &  90.9 &  0.34 &  87.6 &  0.37 &  64.3 \\
\textbf{\sparta (0.5\%)} &  0.12 &  95.9 &  0.35 &  86.3 &  0.11 &  96.7 &  0.22 &  90.5 &  0.24 &  91.2 &  0.32 &  88.8 &  0.34 &  68.2 \\
\textbf{\sparta (0.048\%)} &  0.13 &  95.3 &  0.36 &  86.3 &  0.11 &  96.8 &  0.24 &  89.7 &  0.26 &  90.3 &  0.38 &  86.3 &  0.34 &  66.9 \\\midrule
\textbf{\lora (0.048\%)}  &  0.12 &  95.6 &  0.30 &  88.1 &  0.11 &  96.7 &  0.23 &  90.0 &  0.25 &  91.0 &  0.31 &  87.3 &  0.32 &  67.8 \\\midrule
\textbf{Head (1.15e-4\%)}       &  0.39 &  82.8 &  0.63 &  70.4 &  0.16 &  94.8 &  0.33 &  85.8 &  0.63 &  74.0 &  0.53 &  73.8 &  0.48 &  45.5 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Test loss and accuracy of model \texttt{mistralai/Mistral-7B-v0.3} adapted to glue datasets with different fine-tuning methods. For training details see Table~\ref{tab:glue-training-params}.} \label{tab:mistral-pt}
\end{table*}


\section{Training Hyper-Parameters}
\label{training_parameters}

The hyper-parameters used for investigating \sparta and other adaptation methods on IMDB and GLUE are summarized next. 

\subsection{IMDB}

The sets of best parameters used in training the various adaptation methods for each model are given in Table~\ref{tab:imdb-training-params}.
To improve training efficiency, we excluded training examples exceeding 384 tokens in length, resulting in the use of only 19,306 examples for \gemma, 18,744 examples for \gemmait, 18,373 examples for \mistral, and 17,672 examples for \mistralit for training. 
That is 77\%, 75\%, 73\%, and 71\% of the original training data of 25,000 examples, respectively.

\begin{table*}[p]
\centering
\begin{adjustbox}{max width= \textwidth}
\begin{tabular}{llcccc} 
 \toprule
 & Parameter& \gemma & \gemmait & \mistral & \mistralit  \\
 \midrule
 Full FT  & batch size   & 32 & 32 & 36 &  36 \\
        & num epochs     & 2  & 2 & $\ast$ &  $\ast$ \\
        & learning rate  & 1e-5 & 1e-5 & 3e-6 & 3e-6 \\
        & max grad norm  & 10 & 50 & 120 & 120 \\
        & dropout        & 0.1 & 0.1 & 0.15 & 0.15 \\
        & weight decay   & 0.1 & 0.1 & 0.01 & 0.01 \\
 \midrule
   \sparta  & batch size  &  40   &  40  & 16 & 16 \\   
            & num epochs.  & 2     & 2    & $\ast$ &  $\ast$ \\ 
% $d=$ 10\%   & learning rate & 1e-5   & 4e-6& 2e-6  & 2e-6 \\ 
$d=$ 5\%    & learning rate & 1.5e-5 & 8e-6&  2e-6 & 2e-6 \\
%$d=$ 1\%    & learning rate & 5e-5  & 5e-5&  3e-6 & 3e-6 \\
$d=$ 0.5\%  & learning rate & 1e-4  & 5e-5& 3e-6  & 3e-6 \\
$d=$ 0.05\% & learning rate & 6e-5  & 6e-5 & 6e-5  & 6e-5 \\ 
% 	            &max grad norm & 10  &  10 & 50 & 50 \\
%	            &dropout             & 0.0 & 0.0  & 0.0  &  0.0 \\
% 	            &weight decay    & 0.0 & 0.0 & 0.0  & 0.0   \\	    
 \midrule	              
  \lora  & batch size & 40       & 40    & 20 & 20 \\
        & num epochs    &  3   &   3    & 3 & 3 \\    
  		& learning rate & 2e-4 & 2e-4 & 5e-6 & 5e-6  \\
	      & max grad norm & 15   & 15  & - &  - \\
	      & dropout       & 0.1  & 0.1    & 0.1  &  0.1\\
		& $r$           & 8    &  8   &  8  &  8 \\
		& $\alpha$      &16 & 16   & 16 & 16 \\ 
  \midrule
  Head & batch size   &  40  & 40 & 16 & 16 \\ 
       & num epochs&  4    &  4 & 3 & 3 \\ 
       & learning rate & 2e-4& 2e-4  & 1e-4 &  1e-4 \\
\bottomrule 
\end{tabular}
\end{adjustbox}
\caption{(IMDB) Training parameters used with each fine-tuning method and model in Table~\ref{tab:imdb}.  $\ast$ in number of epochs indicates early stopping was used. For \sparta, parameters for density 5\%, 0.5\% 0,05\% are reported.} 
\label{tab:imdb-training-params}
\end{table*}


\subsection{GLUE}

For GLUE, given the number of tasks (7) and the number of methods tried (6), reporting each best set of training parameters would be just prohibitive. Instead, the trainings of these methods was done using a simple grid search over some of the hyper-parameters. The range on these hyper-parameters are reported in Table~\ref{tab:glue-training-params}. Similarly to IMDB, any sample longer than 256 tokens (which is dependnent on the tokenizer used) is discarded from the dataset to avoid having few minibatch with one very long sample compared to the rest, as seen in Table~\ref{tab:datasets}.
\begin{table*}[p]
\centering
\begin{tabular}{llc} 
 \toprule
 & Parameter & Grid Search Value Range \\
 \midrule
 Full FT & batch size & [32, 24, 8] depending on task and model to fit in GPU memory \\
    & learning rate & [1e-3, 5e-4, 2e-4, 1e-4, 5e-5, 1e-5, 5e-6]  \\
    % & max grad norm & [ None ] \\
    & dropout       &  [0.1] \\
    & weight decay  &  [0.1] \\ \midrule
 \sparta  & batch size & [same as for Full FT] \\ 
    & learning rate & [same as for Full FT] \\
    % & max grad norm & [ None ] \\
    % & dropout        & [0.0] \\
    & weight decay   & [0.1] \\	    
 \midrule	              
  \lora  & batch size & [same as for Full FT] \\ 
     & learning rate & [same as for Full FT] \\
     & $r$           &  [8] \\
     & $\alpha$      &  [16] \\
     & dropout       & [0.1] \\
     & weight decay  & [0.1] \\ 
 \midrule
 Head & batch size & [same as for Full FT] \\ 
   & learning rate & [same as for Full FT] \\
   % & max grad norm & \\
   & dropout       & [0.1] \\
   & weight decay  & [0.0]  \\
 \bottomrule 
\end{tabular}
\caption{(GLUE) Training parameters and their grid search value range to get results for each adaptation method for each model in Table~\ref{tab:gemma-it}, Table~\ref{tab:mistral-it}, Table~\ref{tab:gemma-pt}, and Table~\ref{tab:mistral-pt}. Best set of parameters were chosen from best validation loss results over 2 epochs of training.} \label{tab:glue-training-params}
\end{table*}
 


 
\end{document}   







