\section{Related work}
\label{s:related_work}
The last few years has seen many advances in fine-tuning Large Language Models (LLMs). Perhaps the most well-known method (also most used in practice) is \lora ____, which has spurred many variants including DoRA -- which adapts only the directions of pre-trained weights ____, VeRA -- which shares low-rank matrices across layers ____, AdaLoRA -- which adaptively allocates parameter budgets among weight matrices according to their importance ____, and SoRA -- which dynamically adjusts the intrinsic rank during the adaptation ____. This set of methods each have a different structure to the $\Delta$ being optimized with the commonality being the training of some function of low-rank matrices. For example, further expanding on VeRA ____, the authors propose to learn $\Delta=DBEA$ where $A$ and $B$ are random low-rank matrices and $D$ and $E$ are diagonal matrices to be trained. 

Beyond adding structured parameters is the concept of fine-tuning a small subset of the total parameters of the model, i.e., sparse fine-tuning, where one must first decide which subset of parameters to fine-tune (similar to deciding which parameters in each layer to add adapters to in \lora). ____ start with an initial set of parameters and offer procedures to \emph{drop} (according to parameter magnitude) and \emph{grow} (according to gradient) the set, i.e., they learn the set of parameters to train. Alternatively, ____ focus on the sparsity of neuron activations during training by pre-computing neuron importance scores and only including \emph{important} neurons in computations during training. ____ first fine-tune on all parameters, select the parameters that change the most, and then fine-tune again from scratch on the selected parameters. 
In contrast to these, our proposed \sparta method produces a performant fine-tuned model by directly adapting only a small percentage of the pre-trained model parameters chosen completely at random.

Yet another related direction is that of compression which results in sparse models; algorithms in this genre take a dense fine-tuned model with the goal of compressing it while maintaining performance. Compression (see ____ for a survey) could be accomplished by quantization, low-rank approximation, pruning (i.e., removing neurons, attention heads, or even layers), or distillation. The focus of this paper, however, is on fine-tuning dense models rather than learning sparse models as in ____.