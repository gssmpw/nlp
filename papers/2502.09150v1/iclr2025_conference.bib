@misc{haim2022reconstructingtrainingdatatrained,
      title={Reconstructing Training Data from Trained Neural Networks}, 
      author={Niv Haim and Gal Vardi and Gilad Yehudai and Ohad Shamir and Michal Irani},
      year={2022},
      eprint={2206.07758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.07758}, 
}

@misc{buzaglo2023reconstructingtrainingdatamulticlass,
      title={Reconstructing Training Data from Multiclass Neural Networks}, 
      author={Gon Buzaglo and Niv Haim and Gilad Yehudai and Gal Vardi and Michal Irani},
      year={2023},
      eprint={2305.03350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.03350}, 
}

@InProceedings{pmlr-v206-wang23g,
  title = 	 {Reconstructing Training Data from Model Gradient, Provably},
  author =       {Wang, Zihan and Lee, Jason and Lei, Qi},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6595--6612},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/wang23g/wang23g.pdf},
  url = 	 {https://proceedings.mlr.press/v206/wang23g.html},
  abstract = 	 {Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: Even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild assumptions: with shallow or deep neural networks and wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential  severe threats to privacy, especially in federated learning.}
}

@INPROCEEDINGS{9833677,
  author={Balle, Borja and Cherubin, Giovanni and Hayes, Jamie},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Reconstructing Training Data with Informed Adversaries}, 
  year={2022},
  volume={},
  number={},
  pages={1138-1156},
  keywords={Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning;Data models;machine learning;neural networks;reconstruction attacks;differential privacy},
  doi={10.1109/SP46214.2022.9833677}}

@article{KINDERMANN1990277,
author = {J Kindermann and A Linden},
title = {Inversion of neural networks by gradient descent},
journal = {Parallel Computing},
volume = {14},
number = {3},
pages = {277-286},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90081-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090081J},
keywords = {Inversion, multilayer perceptrons, backpropagation, generalization, robustness},
abstract = {Inversion answers the question of which input patterns to a trained multilayer neural network approximate a given output target. This method is a tool for visualization of the information processing capability of a network stored in its weights. This knowledge about the network enables one to make informed decisions on the improvement of the training task and the choice of training sets. An inversion algorithm for multilayer perceptrons is derived from the backpropagation scheme. We apply inversion to networks for digit recognition. We observe that the multilayer perceptrons perform well with respect to generalization, i.e. correct classification of untrained digits. They are however bad on rejection of counterexamples, i.e. random patterns. Inversion gives an explanation for this drawback. We suggest an improved training scheme, and we show that a tradeoff exists between generalization and rejection of counterexamples.}
}

@ARTICLE{784232,
  author={Jensen, C.A. and Reed, R.D. and Marks, R.J. and El-Sharkawi, M.A. and Jae-Byung Jung and Miyamoto, R.T. and Anderson, G.M. and Eggen, C.J.},
  journal={Proceedings of the IEEE}, 
  title={Inversion of feedforward neural networks: algorithms and applications}, 
  year={1999},
  volume={87},
  number={9},
  pages={1536-1549},
  keywords={Neural networks;Feedforward neural networks;Training data;Sonar;Power system security;Neurons;Performance analysis;Control systems;Power generation;Multi-layer neural network},
  doi={10.1109/5.784232}}

@inproceedings{suhail2024network,
title={Network Inversion of Binarised Neural Nets},
author={Pirzada Suhail},
booktitle={The Second Tiny Papers Track at ICLR 2024},
year={2024},
url={https://openreview.net/forum?id=zKcB0vb7qd}
}

@article{deng2012mnist, 
  title={The mnist database of handwritten digit images for machine learning research}, 
  author={Deng, Li}, 
  journal={IEEE Signal Processing Magazine}, 
  volume={29}, 
  number={6}, 
  pages={141--142}, 
  year={2012}, 
  publisher={IEEE} 
}

@misc{xu2015empiricalevaluationrectifiedactivations,
      title={Empirical Evaluation of Rectified Activations in Convolutional Network}, 
      author={Bing Xu and Naiyan Wang and Tianqi Chen and Mu Li},
      year={2015},
      eprint={1505.00853},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1505.00853}, 
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@InProceedings{pmlr-v37-ioffe15,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}
@misc{xiao2017fashionmnistnovelimagedataset,
      title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, 
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1708.07747}, 
}
@article{SAAD200778,
title = {Neural network explanation using inversion},
journal = {Neural Networks},
volume = {20},
number = {1},
pages = {78-93},
year = {2007},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2006.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608006001730},
author = {Emad W. Saad and Donald C. Wunsch},
keywords = {Rule extraction, Neural network explanation, Explanation capability of neural networks, Inversion, Hyperplanes, Evolutionary algorithm, Pedagogical},
abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., & Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373–389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e.calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity–complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems.}
}
@inproceedings{Wong2017NeuralNI,
  title={Neural network inversion beyond gradient descent},
  author={Eric Wong},
  year={2017},
  booktitle={WOML NIPS},
  url={https://api.semanticscholar.org/CorpusID:208231247}
}

@inproceedings{10.1145/3319535.3354261,
author = {Yang, Ziqi and Zhang, Jiyi and Chang, Ee-Chien and Liang, Zhenkai},
title = {Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354261},
doi = {10.1145/3319535.3354261},
abstract = {The wide application of deep learning technique has raised new security concerns about the training data and test data. In this work, we investigate the model inversion problem under adversarial settings, where the adversary aims at inferring information about the target model's training data and test data from the model's prediction values. We develop a solution to train a second neural network that acts as the inverse of the target model to perform the inversion. The inversion model can be trained with black-box accesses to the target model. We propose two main techniques towards training the inversion model in the adversarial settings. First, we leverage the adversary's background knowledge to compose an auxiliary set to train the inversion model, which does not require access to the original training data. Second, we design a truncation-based technique to align the inversion model to enable effective inversion of the target model from partial predictions that the adversary obtains on victim user's data. We systematically evaluate our approach in various machine learning tasks and model architectures on multiple image datasets. We also confirm our results on Amazon Rekognition, a commercial prediction API that offers "machine learning as a service". We show that even with partial knowledge about the black-box model's training data, and with only partial prediction values, our inversion approach is still able to perform accurate inversion of the target model, and outperform previous approaches.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {225–240},
numpages = {16},
keywords = {deep learning, model inversion, neural networks, privacy, security},
location = {London, United Kingdom},
series = {CCS '19}
}
@inproceedings{NEURIPS2020_373e4c5d,
 author = {Kumar, Aviral and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5126--5137},
 publisher = {Curran Associates, Inc.},
 title = {Model Inversion Networks for Model-Based Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf},
 volume = {33},
 year = {2020}
}
@misc{liu2022landscapelearningneuralnetwork,
      title={Landscape Learning for Neural Network Inversion}, 
      author={Ruoshi Liu and Chengzhi Mao and Purva Tendulkar and Hao Wang and Carl Vondrick},
      year={2022},
      eprint={2206.09027},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.09027}, 
}
@misc{ansari2022autoinverseuncertaintyawareinversion,
      title={Autoinverse: Uncertainty Aware Inversion of Neural Networks}, 
      author={Navid Ansari and Hans-Peter Seidel and Nima Vahidi Ferdowsi and Vahid Babaei},
      year={2022},
      eprint={2208.13780},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.13780}, 
}

@article{cf,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}
@article{Geirhos_2020,
   title={Shortcut learning in deep neural networks},
   volume={2},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-020-00257-z},
   DOI={10.1038/s42256-020-00257-z},
   number={11},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
   year={2020},
   month=nov, pages={665–673} }

@inproceedings{
hermann2024on,
title={On the Foundations of Shortcut Learning},
author={Katherine Hermann and Hossein Mobahi and Thomas FEL and Michael Curtis Mozer},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Tj3xLVuE9f}
}
@article{Brown_2023,
   title={Detecting shortcut learning for fair medical AI using shortcut testing},
   volume={14},
   ISSN={2041-1723},
   url={http://dx.doi.org/10.1038/s41467-023-39902-7},
   DOI={10.1038/s41467-023-39902-7},
   number={1},
   journal={Nature Communications},
   publisher={Springer Science and Business Media LLC},
   author={Brown, Alexander and Tomasev, Nenad and Freyberg, Jan and Liu, Yuan and Karthikesalingam, Alan and Schrouff, Jessica},
   year={2023},
   month=jul }

@InProceedings{10.1007/978-3-031-72111-3_59,
author="Lin, Manxi
and Weng, Nina
and Mikolaj, Kamil
and Bashir, Zahra
and Svendsen, Morten B. S.
and Tolsgaard, Martin G.
and Christensen, Anders N.
and Feragen, Aasa",
editor="Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.",
title="Shortcut Learning in Medical Image Segmentation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="623--633",
abstract="Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation. Our code is public at https://github.com/nina-weng/shortcut{\_}skinseg.",
isbn="978-3-031-72111-3"
}

@article{
bleeker2024demonstrating,
title={Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning},
author={Maurits Bleeker and Mariya Hendriksen and Andrew Yates and Maarten de Rijke},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=gfANevPraH},
note={}
}

@ARTICLE{10250856,
  author={Ma, Chong and Zhao, Lin and Chen, Yuzhong and Guo, Lei and Zhang, Tuo and Hu, Xintao and Shen, Dinggang and Jiang, Xi and Liu, Tianming},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Rectify ViT Shortcut Learning by Visual Saliency}, 
  year={2024},
  volume={35},
  number={12},
  pages={18013-18025},
  keywords={Data models;Medical diagnostic imaging;Task analysis;Predictive models;Visualization;Deep learning;Training;Interpretability;saliency;shortcut learning;vision transformer (ViT)},
  doi={10.1109/TNNLS.2023.3310531}}

@inproceedings{suhail2024networkcnn,
    title={Network Inversion of Convolutional Neural Nets},
    author={Pirzada Suhail and Amit Sethi},
    booktitle={Muslims in ML Workshop co-located with NeurIPS 2024},
    year={2024},
    url={https://openreview.net/forum?id=f9sUu7U1Cp}
}

@inproceedings{
suhail2024net,
title={Network Inversion for Training-Like Data Reconstruction},
author={Pirzada Suhail and Amit Sethi},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=rJy9ytRnep}
}

@misc{suhail2024networkinversionapplications,
      title={Network Inversion and Its Applications}, 
      author={Pirzada Suhail and Hao Tang and Amit Sethi},
      year={2024},
      eprint={2411.17777},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.17777}, 
}

@InProceedings{pmlr-v162-guo22c,
  title = 	 {Bounding Training Data Reconstruction in Private (Deep) Learning},
  author =       {Guo, Chuan and Karrer, Brian and Chaudhuri, Kamalika and van der Maaten, Laurens},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8056--8071},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/guo22c/guo22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/guo22c.html},
  abstract = 	 {Differential privacy is widely accepted as the de facto method for preventing data leakage in ML, and conventional wisdom suggests that it offers strong protection against privacy attacks. However, existing semantic guarantees for DP focus on membership inference, which may overestimate the adversary’s capabilities and is not applicable when membership status itself is non-sensitive. In this paper, we derive the first semantic guarantees for DP mechanisms against training data reconstruction attacks under a formal threat model. We show that two distinct privacy accounting methods—Renyi differential privacy and Fisher information leakage—both offer strong semantic protection against data reconstruction attacks.}
}


@misc{oz2024reconstructingtrainingdatareal,
      title={Reconstructing Training Data From Real World Models Trained with Transfer Learning}, 
      author={Yakir Oz and Gilad Yehudai and Gal Vardi and Itai Antebi and Michal Irani and Niv Haim},
      year={2024},
      eprint={2407.15845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15845}, 
}

@misc{wang2023reconstructingtrainingdatamodel,
      title={Reconstructing Training Data from Model Gradient, Provably}, 
      author={Zihan Wang and Jason D. Lee and Qi Lei},
      year={2023},
      eprint={2212.03714},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.03714}, 
}
