\section{Related Works}
Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. This issue has been identified as a fundamental limitation of deep learning models, leading to poor robustness and transferability \cite{Geirhos_2020}. Shortcut learning has been observed across various domains, including vision, natural language processing, and speech recognition, where models often exploit unintended correlations in the training data to minimize loss instead of learning meaningful patterns. The study in \cite{hermann2024on} examines shortcut learning from a theoretical perspective by investigating the factors that influence whether a model will rely on a shortcut. The authors propose that a feature's predictivity and availability determine whether it will be used as a shortcut, with more easily extractable (but less predictive) features being favored over harder-to-extract (but more predictive) features.

This phenomenon is particularly concerning in medical AI, where it has been linked to algorithmic unfairness. In \cite{Brown_2023}, the authors propose a method to detect shortcut learning in clinical machine learning models by applying multitask learning to identify improper correlations that may cause biased predictions. Beyond classification, the phenomenon has also been studied in medical image segmentation, where commonly used dataset preparation techniques, such as zero-padding and center-cropping, introduce unintended shortcuts that influence segmentation accuracy\cite{10.1007/978-3-031-72111-3_59}. 

The paper \cite{bleeker2024demonstrating} investigates shortcut learning in vision-language models and evaluates how contrastive learning-based models tend to latch onto unintended patterns in multi-caption training scenarios. By injecting synthetic shortcuts into image-text data, the authors show that contrastive losses often fail to encourage models to learn all task-relevant information, instead reinforcing the simplest available features. Shortcut learning has also been studied in the context of Vision Transformers. The work in \cite{10250856} explores how ViTs might be particularly prone to shortcut learning due to their reliance on self-attention mechanisms. The authors introduce a saliency-guided ViT model that leverages computational visual saliency maps to guide ViTs toward learning meaningful features rather than background artifacts.

This paper exploits network inversion-based reconstruction techniques to analyze different model architectures for their shortcut learning susceptibility. Inversion has been studied in \cite{KINDERMANN1990277,784232} using the back-propagation and evolutionary algorithms for feed-forward networks that identify multiple inversion points simultaneously, providing a more comprehensive view of the networkâ€™s input-output relationships. Later inversion of Convolutional Neural Nets was performed in \cite{suhail2024networkcnn} and \cite{suhail2024networkinversionapplications}, using a conditoned generator that learns the input space of the trained models. The work in \cite{SAAD200778} introduces an inversion-based method for rule extraction to calculate the input patterns that correspond to specific output targets. The study in \cite{Wong2017NeuralNI} addresses the problem of inverting deep networks to find inputs that minimize certain output criteria by reformulating network propagation as a constrained optimization problem. Recent work by \cite{liu2022landscapelearningneuralnetwork} proposes learning a loss landscape where gradient descent becomes efficient, significantly improving the speed and stability of the inversion process. Later, \cite{suhail2024network} proposes an alternate approach to inversion by encoding the network into a Conjunctive Normal Form (CNF) propositional formula and using SAT solvers and samplers to find satisfying assignments for the constrained CNF formula. The paper \cite{NEURIPS2020_373e4c5d} presents a method for tackling data-driven optimization problems, where the goal is to find inputs that maximize an unknown score function by proposing Model Inversion Networks (MINs). Also, \cite{ansari2022autoinverseuncertaintyawareinversion} introduces an automated method for inversion by seeking inverse solutions near reliable data points that are sampled from the forward process and used for training the surrogate model.

We are specifically interested in trying to reconstruct the training data as perceived by the models in presence of shortcuts. The work in \cite{haim2022reconstructingtrainingdatatrained} explores the extent to which neural networks memorize training data, revealing that a significant portion of the training data can be reconstructed from the parameters of a trained neural network classifier. Later, \cite{buzaglo2023reconstructingtrainingdatamulticlass} improves on these results by showing that training data reconstruction is not only possible in the multi-class setting, but that the quality of the reconstructed samples is even higher than in the binary case. In \cite{10.1145/3319535.3354261} an attacker aims to infer training data from a model's predictions by training a secondary neural network to perform the inversion, using the adversary's background knowledge to construct an auxiliary dataset. The study by \cite{wang2023reconstructingtrainingdatamodel, pmlr-v206-wang23g, 9833677} demonstrated that training samples could be fully reconstructed from a single gradient query, even without explicit training or memorization. Recent advancements like \cite{oz2024reconstructingtrainingdatareal}, adapt reconstruction schemes to operate in the embedding space of large pre-trained models like DINO-ViT and CLIP.

In this paper we use network inversion-based reconstruction method proposed in \cite{suhail2024net} to understand the internal representations of neural networks and the patterns they memorize during training and compare the shortcut learning susceptibility of different vision classifier architectures allowing us to investigate whether the models primarily store shortcut-based representations or capture meaningful semantic features.