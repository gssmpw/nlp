
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{graphicx}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\title{Shortcut Learning Susceptibility \\in \\Vision Classifiers}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Pirzada Suhail \& Amit Sethi \thanks{ Footnote ---\emph{here}} \\
Department of Electrical Engineering\\
IIT Bombay\\
Mumbai, IN \\
\texttt{\{psuhail,asethi\}@iitb.ac.in} \\
%\And
%Supratik Chakraborty \thanks{ Footnote ---\emph{here}} \\
%Department of Computer Science \\
%IIT Bombay\\
%Mumbai, IN \\
%\texttt{\{supratik\}@iitb.ac.in} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Shortcut learning, where machine learning models exploit spurious correlations in data instead of capturing meaningful features, poses a significant challenge to building robust and generalizable models. This phenomenon is prevalent across various machine learning applications, including vision, natural language processing, and speech recognition, where models may find unintended cues that minimize training loss but fail to capture the underlying structure of the data. Vision classifiers such as Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers (ViTs) leverage distinct architectural principles to process spatial and structural information, making them differently susceptible to shortcut learning. In this study, we systematically evaluate these architectures by introducing deliberate shortcuts into the dataset that are positionally correlated with class labels, creating a controlled setup to assess whether models rely on these artificial cues or learn actual distinguishing features. We perform both quantitative evaluation by training on the shortcut-modified dataset and testing them on two different test sets—one containing the same shortcuts and another without them—to determine the extent of reliance on shortcuts. Additionally, qualitative evaluation is performed by using network inversion-based reconstruction techniques to analyze what the models internalize in their weights, aiming to reconstruct the training data as perceived by the classifiers. We evaluate shortcut learning behavior across multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and CIFAR-10, to compare the susceptibility of different vision classifier architectures to shortcut reliance and assess their varying degrees of sensitivity to spurious correlations.

\end{abstract}

\section{Introduction}

Machine learning models are expected to learn meaningful patterns from data to make accurate predictions and generalize well across different domains. However, in many cases, models do not learn the intended task-relevant features but instead rely on shortcut learning, where they exploit spurious correlations in the training data that happen to be predictive of the labels. Rather than learning intended patterns, models often latch onto unintended statistical correlations present in the training data, leading to poor generalization across different domains. While shortcut learning may improve performance on in-distribution test data, it significantly degrades model robustness when evaluated on out-of-distribution samples in a real-world settings where these spurious correlations no longer hold. This issue is not limited to computer vision but extends to various domains, including natural language processing (NLP), where models rely on dataset biases rather than true linguistic understanding, and speech recognition, where models can leverage background noise instead of learning speech patterns.

In computer vision, different classification architectures process input data in fundamentally distinct ways, which may lead to varying tendencies toward shortcut learning. Convolutional Neural Networks (CNNs) operate through convolutional filters that extract spatial patterns, Multi-Layer Perceptrons (MLPs) learn feature representations in a fully connected manner without spatial biases, and Vision Transformers (ViTs) leverage self-attention mechanisms to model long-range dependencies. Since each of these architectures encodes spatial and structural information differently, their responses to shortcuts may vary. However, the specific nature of their susceptibility to shortcut learning remains unclear. In this paper, we aim to systematically evaluate how CNNs, MLPs, and ViTs learn from data when shortcut cues are deliberately introduced, providing insights into their respective tendencies toward shortcut reliance and generalization.

To systematically assess shortcut susceptibility, we introduce deliberate shortcuts into the training dataset. Specifically, we modify certain pixel regions in the images to correlate deterministically with class labels, embedding artificial cues that serve as an unintended yet easily learnable path to minimize training loss. These modifications serve as an alternative path for models to reduce training error, creating a controlled setup where we can analyze whether models rely on these shortcuts or learn the actual distinguishing features in the images.

To measure the extent to which models depend on shortcut cues, we train CNNs, MLPs, and ViTs on the modified datasets and evaluate them on two test sets: one containing the same shortcuts and another without any shortcuts. If a model primarily learns the artificial shortcut instead of the actual class-relevant features, its performance is expected to drop significantly when evaluated on the test set without shortcuts. By comparing the performance gap across architectures, we quantify each model’s reliance on shortcuts and determine whether certain architectures are inherently more vulnerable.

Beyond performance metrics, we also conduct qualitative evaluations using network inversion-based reconstruction techniques to analyze what the models internalize in their weights. By reconstructing the training data as perceived by the classifiers, we investigate whether the models primarily store shortcut-based representations or capture meaningful semantic features. We also explore how learning rate variations impact shortcut reliance across different architectures. Models trained with a low learning rate may learn more nuanced and meaningful features, whereas a high learning rate may encourage faster convergence at the cost of relying on simpler, easily learnable patterns, such as shortcuts. 

We conduct experiments across multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and CIFAR-10 to provide insights into how different vision architectures process data and how susceptible they are to shortcut learning.

\section{Related Works}

Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. This issue has been identified as a fundamental limitation of deep learning models, leading to poor robustness and transferability \cite{Geirhos_2020}. Shortcut learning has been observed across various domains, including vision, natural language processing, and speech recognition, where models often exploit unintended correlations in the training data to minimize loss instead of learning meaningful patterns. The study in \cite{hermann2024on} examines shortcut learning from a theoretical perspective by investigating the factors that influence whether a model will rely on a shortcut. The authors propose that a feature's predictivity and availability determine whether it will be used as a shortcut, with more easily extractable (but less predictive) features being favored over harder-to-extract (but more predictive) features.

This phenomenon is particularly concerning in medical AI, where it has been linked to algorithmic unfairness. In \cite{Brown_2023}, the authors propose a method to detect shortcut learning in clinical machine learning models by applying multitask learning to identify improper correlations that may cause biased predictions. Beyond classification, the phenomenon has also been studied in medical image segmentation, where commonly used dataset preparation techniques, such as zero-padding and center-cropping, introduce unintended shortcuts that influence segmentation accuracy\cite{10.1007/978-3-031-72111-3_59}. 

The paper \cite{bleeker2024demonstrating} investigates shortcut learning in vision-language models and evaluates how contrastive learning-based models tend to latch onto unintended patterns in multi-caption training scenarios. By injecting synthetic shortcuts into image-text data, the authors show that contrastive losses often fail to encourage models to learn all task-relevant information, instead reinforcing the simplest available features. Shortcut learning has also been studied in the context of Vision Transformers. The work in \cite{10250856} explores how ViTs might be particularly prone to shortcut learning due to their reliance on self-attention mechanisms. The authors introduce a saliency-guided ViT model that leverages computational visual saliency maps to guide ViTs toward learning meaningful features rather than background artifacts.

This paper exploits network inversion-based reconstruction techniques to analyze different model architectures for their shortcut learning susceptibility. Inversion has been studied in \cite{KINDERMANN1990277,784232} using the back-propagation and evolutionary algorithms for feed-forward networks that identify multiple inversion points simultaneously, providing a more comprehensive view of the network’s input-output relationships. Later inversion of Convolutional Neural Nets was performed in \cite{suhail2024networkcnn} and \cite{suhail2024networkinversionapplications}, using a conditoned generator that learns the input space of the trained models. The work in \cite{SAAD200778} introduces an inversion-based method for rule extraction to calculate the input patterns that correspond to specific output targets. The study in \cite{Wong2017NeuralNI} addresses the problem of inverting deep networks to find inputs that minimize certain output criteria by reformulating network propagation as a constrained optimization problem. Recent work by \cite{liu2022landscapelearningneuralnetwork} proposes learning a loss landscape where gradient descent becomes efficient, significantly improving the speed and stability of the inversion process. Later, \cite{suhail2024network} proposes an alternate approach to inversion by encoding the network into a Conjunctive Normal Form (CNF) propositional formula and using SAT solvers and samplers to find satisfying assignments for the constrained CNF formula. The paper \cite{NEURIPS2020_373e4c5d} presents a method for tackling data-driven optimization problems, where the goal is to find inputs that maximize an unknown score function by proposing Model Inversion Networks (MINs). Also, \cite{ansari2022autoinverseuncertaintyawareinversion} introduces an automated method for inversion by seeking inverse solutions near reliable data points that are sampled from the forward process and used for training the surrogate model.

We are specifically interested in trying to reconstruct the training data as perceived by the models in presence of shortcuts. The work in \cite{haim2022reconstructingtrainingdatatrained} explores the extent to which neural networks memorize training data, revealing that a significant portion of the training data can be reconstructed from the parameters of a trained neural network classifier. Later, \cite{buzaglo2023reconstructingtrainingdatamulticlass} improves on these results by showing that training data reconstruction is not only possible in the multi-class setting, but that the quality of the reconstructed samples is even higher than in the binary case. In \cite{10.1145/3319535.3354261} an attacker aims to infer training data from a model's predictions by training a secondary neural network to perform the inversion, using the adversary's background knowledge to construct an auxiliary dataset. The study by \cite{wang2023reconstructingtrainingdatamodel, pmlr-v206-wang23g, 9833677} demonstrated that training samples could be fully reconstructed from a single gradient query, even without explicit training or memorization. Recent advancements like \cite{oz2024reconstructingtrainingdatareal}, adapt reconstruction schemes to operate in the embedding space of large pre-trained models like DINO-ViT and CLIP.

In this paper we use network inversion-based reconstruction method proposed in \cite{suhail2024net} to understand the internal representations of neural networks and the patterns they memorize during training and compare the shortcut learning susceptibility of different vision classifier architectures allowing us to investigate whether the models primarily store shortcut-based representations or capture meaningful semantic features.

\section{Methodology}

Our approach to analyze shortcut learning susceptibility in different vision classifier architectures involves the introduction of deliberate shortcuts in the dataset that would other wise be present in the real world data. The structured shortcuts are introduced into the dataset by modifying specific pixel regions in images such that their locations deterministically correlate with the class labels. This forces the models to either learn the actual class-relevant features or exploit the artificial shortcut cues. 

The trained vision classifiers are then analyzed for their performance on test sets both with and without shortcuts, and perform network inversion to reconstruct the data perceived by the classifiers. In this study, we analyze the shortcut learning susceptibility of different vision classifier architectures using network inversion-based reconstruction techniques. By reconstructing the data that the models internalize in their weights, we aim to uncover whether classifiers rely on these shortcuts or extract meaningful features.

We analyse the shortcut learning susceptibility of classifiers trained on three distinct architectures including MLPs, CNNs, ViTs. MLPs are fully connected networks where each neuron in one layer is connected to every neuron in the next layer. MLPs process flattened input images, lacking any inherent spatial hierarchy. CNNs extract hierarchical spatial features from images through convolutional layers, enabling them to capture local patterns effectively. ViTs leverage self-attention mechanisms applied on the patch embeddings of an image to capture global dependencies across the entire image. These architectures inherently differ in how they process spatial and structural information, which affects their susceptibility to shortcut learning.

To reconstruct the training data perceived by the classifiers, we employ a vector-matrix conditioned generator. This generator is conditioned on vectors and matrices to ensure diverse and representative reconstructions. Unlike conventional label conditioning, this method encodes label information in a structured manner, allowing the generator to capture class-specific features, including potential shortcut correlations.

The generator is conditioned using N-dimensional vectors for an N-class classification task, drawn from a normal distribution and transformed via softmax to form a probability distribution. This implicit encoding allows the generator to learn complex relationships beyond direct label mappings.

Additionally, a Hot Conditioning Matrix of size \(N \times N\) is applied. Each row or column corresponding to the encoded label is set to 1, while the rest remain 0. This structured conditioning refines the diversity of generated images, ensuring that the inversion process accurately captures the internal representations of the classifier while distinguishing between shortcut features and meaningful patterns.

The reconstruction process aims to generate training-like samples based on the classifier's learned representations. The network inversion-based reconstruction composite loss function that integrates multiple constraints ensuring that the reconstructed data closely resembles the training distribution while minimizing shortcut reliance. The loss function is formulated as:

\[
\mathcal{L}_{\text{Recon}} =
\alpha \mathcal{L}_{\text{KL}} +
\alpha' \mathcal{L}_{\text{KL}}^{\text{pert}} +
\beta \mathcal{L}_{\text{CE}} +
\beta' \mathcal{L}_{\text{CE}}^{\text{pert}} +
\gamma \mathcal{L}_{\text{Cosine}} +
\delta \mathcal{L}_{\text{Ortho}} +
\eta_1 \mathcal{L}_{\text{Var}} +
\eta_2 \mathcal{L}_{\text{Pix}} +
\eta_3 \mathcal{L}_{\text{Grad}}
\]

where:
- \( \mathcal{L}_{\text{KL}} \) is the KL Divergence loss ensuring the generated data distribution aligns with the classifier’s learned distribution.
- \( \mathcal{L}_{\text{CE}} \) is the Cross-Entropy loss enforcing class consistency.
- \( \mathcal{L}_{\text{Cosine}} \) is the Cosine Similarity loss minimizing angular deviations between feature embeddings.
- \( \mathcal{L}_{\text{Ortho}} \) is the Feature Orthogonality loss reducing redundancy in the generated features.
- \( \mathcal{L}_{\text{KL}}^{\text{pert}} \) and \( \mathcal{L}_{\text{CE}}^{\text{pert}} \) apply perturbations to test robustness against shortcut reliance.
- \( \mathcal{L}_{\text{Var}} \), \( \mathcal{L}_{\text{Pix}} \), and \( \mathcal{L}_{\text{Grad}} \) impose pixel constraints, ensure smoothness, and penalize abrupt changes.

By integrating these loss components, the generator is trained to produce samples that closely resemble the training data, revealing the extent to which classifiers rely on shortcuts versus meaningful features.

\section{Experiments and Results}

In this section, we evaluate the shortcut learning susceptibility of three different vision classifier architectures—Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision Transformers (ViTs)—on four benchmark datasets: MNIST, Fashion-MNIST, SVHN, and CIFAR-10. The introduced artificial shortcuts into the dataset are positionally correlated with class labels. Specifically, a \textbf{4×4 white patch} is inserted at different spatial locations for different classes, embedding a spurious correlation that models may leverage instead of learning the meaningful features of the data.

The models are trained on the shortcut-modified dataset, and their generalization capabilities are evaluated by comparing their performance on two test sets: \textbf{(i) the test set with the same shortcut modifications}, and \textbf{(ii) a clean test set without shortcuts}. A model that generalizes well and does not rely on shortcuts should perform comparably on both test sets, whereas a model that latches onto shortcuts will show significant degradation in performance on the later.

To quantitatively evaluate shortcut susceptibility, we define two simple difference-based metrics. The \textbf{Accuracy Difference} is the absolute difference between the model's accuracy on the shortcut test set and the normal test set, indicating the extent to which the model relies on shortcut features for classification. Similarly, the \textbf{Loss Difference} is the absolute difference in cross entropy loss between the shortcut test set and the normal test set, providing a measure of the inconsistency in model confidence across the two test conditions. Models that heavily rely on shortcuts exhibit high Accuracy and Loss Difference values, whereas those that generalize well show minimal differences.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{acc_diff.pdf}
    \caption{Comparison of Accuracy Difference across architectures (MLP, CNN, ViT) on different datasets. Lower values indicate better generalization.}
    \label{fig:accuracy_diff}
\end{figure}

In Figure~\ref{fig:accuracy_diff}, and~\ref{fig:loss_diff}, a we observe a consistent trend across all datasets. While accuracy on the \textit{shortcut test set} is nearly perfect, accuracy on the \textit{normal test set} is significantly degraded. Among the architectures, \textbf{ViTs show the highest susceptibility to shortcut learning}, with the largest values for both Accuracy and Loss Difference . On the other hand, \textbf{CNNs demonstrate the best resistance to shortcuts}, exhibiting the smallest differences in accuracy and loss across the two test sets. \textbf{MLPs display intermediate levels of shortcut susceptibility}, performing better than ViTs but worse than CNNs.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{loss_diff.pdf}
    \caption{Comparison of Loss Difference across architectures (MLP, CNN, ViT) on different datasets. Lower values indicate lesser reliance on shortcuts.}
    \label{fig:loss_diff}
\end{figure}

Furthermore, we observe that models, when exposed to a dataset with shortcuts for prolonged training periods, tend to \textit{reinforce shortcut reliance}. This reinforcement effect leads to a continuous degradation of performance on the normal test set compared to the shortcut test set, emphasizing the models' increasing dependency on spurious correlations. This trend highlights the architectural biases of different vision classifiers and their varying tendencies to rely on shortcuts rather than learning meaningful, generalizable representations.

To further investigate shortcut susceptibility, we utilize \textbf{network inversion-based reconstruction} techniques to analyze how different classifiers internalize training data. Specifically, for a given dataset, we reconstruct training samples as perceived by each classifier—MLP, CNN, and ViT—based on their learned representations. The reconstructed samples provide insight into whether a model has stored meaningful semantic features or primarily memorized the shortcut-based patterns.

Figure~\ref{fig:reconstruction_samples} presents a qualitative comparison of reconstructed training images. The first column contains actual training samples with embedded shortcuts, while the subsequent columns show the reconstructed images for ViTs, MLPs, and CNNs respectively. We observe that \textbf{CNNs tend to reconstruct meaningful features with spatial coherence}, while \textbf{MLPs exhibit moderate shortcut reliance}, capturing a mix of class-relevant and shortcut features. \textbf{ViTs, on the other hand, reconstruct highly distorted samples dominated by shortcut-based artifacts}, suggesting a stronger reliance on positional correlations rather than intrinsic image structures.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{recon.pdf}
    \caption{Reconstructed training samples as perceived by the classifier.}
    \label{fig:reconstruction_samples}
\end{figure}

Beyond model-specific evaluations, we also analyze the impact of learning rate on shortcut susceptibility. Our experiments across all architectures reveal a consistent trend and a strong correlation between learning rate and shortcut dependence. When trained with a \textbf{large learning rate}, models converge faster but tend to latch onto easily learnable patterns, including shortcuts, leading to higher Accuracy and Loss Difference values. In contrast, models trained with a \textbf{small learning rate} exhibit a more gradual learning process, favoring the acquisition of meaningful, class-relevant features over shortcut-based cues. 

\section{Conclusion and Future Work}

In this paper, we evaluated the shortcut learning susceptibility of three different vision classifier architectures—MLPs, CNNs, and ViTs—across four benchmark datasets: \textbf{MNIST}, \textbf{Fashion-MNIST}, \textbf{SVHN}, and \textbf{CIFAR-10} by introducing artificial shortcuts into the training data and assessing how each of the models behave during training. Our evaluation was conducted using both direct performance comparisons between shortcut-augmented and normal test sets and network inversion-based reconstruction techniques. The results consistently demonstrated that ViTs exhibited the highest reliance on shortcut features, whereas CNNs showed the strongest resistance, with MLPs displaying intermediate behavior. Additionally, we observed that training models with higher learning rates reinforced shortcut learning, while smaller learning rates encouraged models to capture meaningful, class-relevant features. These findings provide key insights into how different vision classifiers internalize training data and emphasize the impact of model architecture and optimization settings on shortcut susceptibility.

In future we aim to extend our investigation to real-world datasets and scenarios, where shortcuts are naturally embedded and unobvious. Understanding how models behave in such settings will offer deeper insights into their generalization capabilities. Additionally, we plan to explore shortcut susceptibility in larger-scale models with higher capacity for memorization, such as deep CNNs and Transformer-based architectures trained on high-resolution datasets. Given that larger models have more room to exploit shortcuts, studying their behavior will be crucial in developing strategies for mitigating shortcut reliance in practical applications.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\end{document}
