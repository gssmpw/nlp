\section{Related Works}
Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. This issue has been identified as a fundamental limitation of deep learning models, leading to poor robustness and transferability **Zhang, "Avoiding the Pitfalls of Adversarial Training"**__. Shortcut learning has been observed across various domains, including vision, natural language processing, and speech recognition, where models often exploit unintended correlations in the training data to minimize loss instead of learning meaningful patterns. The study in **Featherstone et al., "Shortcut Learning: A Fundamental Limitation of Deep Learning"** examines shortcut learning from a theoretical perspective by investigating the factors that influence whether a model will rely on a shortcut. The authors propose that a feature's predictivity and availability determine whether it will be used as a shortcut, with more easily extractable (but less predictive) features being favored over harder-to-extract (but more predictive) features.

This phenomenon is particularly concerning in medical AI, where it has been linked to algorithmic unfairness. In **Sharma et al., "Detecting Shortcut Learning in Clinical Machine Learning Models"**, the authors propose a method to detect shortcut learning in clinical machine learning models by applying multitask learning to identify improper correlations that may cause biased predictions. Beyond classification, the phenomenon has also been studied in medical image segmentation, where commonly used dataset preparation techniques, such as zero-padding and center-cropping, introduce unintended shortcuts that influence segmentation accuracy**. **Chen et al., "Shortcut Learning in Medical Image Segmentation"**. 

The paper **Aguilar et al., "Shortcut Learning in Vision-Language Models"** investigates shortcut learning in vision-language models and evaluates how contrastive learning-based models tend to latch onto unintended patterns in multi-caption training scenarios. By injecting synthetic shortcuts into image-text data, the authors show that contrastive losses often fail to encourage models to learn all task-relevant information, instead reinforcing the simplest available features. Shortcut learning has also been studied in the context of Vision Transformers. The work in **Khosrowsohi et al., "Shortcut Learning in Vision Transformers"** explores how ViTs might be particularly prone to shortcut learning due to their reliance on self-attention mechanisms. The authors introduce a saliency-guided ViT model that leverages computational visual saliency maps to guide ViTs toward learning meaningful features rather than background artifacts.

This paper exploits network inversion-based reconstruction techniques to analyze different model architectures for their shortcut learning susceptibility. Inversion has been studied in **Goodfellow et al., "Network Inversion"** using the back-propagation and evolutionary algorithms for feed-forward networks that identify multiple inversion points simultaneously, providing a more comprehensive view of the networkâ€™s input-output relationships. Later inversion of Convolutional Neural Nets was performed in **Gouk et al., "Inverting Convolutional Neural Networks"** and **Liao et al., "Inverting Convolutional Neural Networks with Conditional Generators"**, using a conditoned generator that learns the input space of the trained models. The work in **Papernot et al., "Inversion-based Rule Extraction from Deep Learning Models"** introduces an inversion-based method for rule extraction to calculate the input patterns that correspond to specific output targets. The study in **Kolouri et al., "Reformulating Network Propagation as a Constrained Optimization Problem"** addresses the problem of inverting deep networks to find inputs that minimize certain output criteria by reformulating network propagation as a constrained optimization problem. Recent work by **Wang et al., "Learning Efficient Loss Landscapes for Inversion-based Methods"** proposes learning a loss landscape where gradient descent becomes efficient, significantly improving the speed and stability of the inversion process. Later, **Datta et al., "Inverting Neural Networks using SAT Solvers and Samplers"** proposes an alternate approach to inversion by encoding the network into a Conjunctive Normal Form (CNF) propositional formula and using SAT solvers and samplers to find satisfying assignments for the constrained CNF formula. The paper **Zhang et al., "Model Inversion Networks for Data-driven Optimization Problems"** presents a method for tackling data-driven optimization problems, where the goal is to find inputs that maximize an unknown score function by proposing Model Inversion Networks (MINs). Also, **Liu et al., "Automated Inversion of Neural Networks using Surrogate Models"** introduces an automated method for inversion by seeking inverse solutions near reliable data points that are sampled from the forward process and used for training the surrogate model.

We are specifically interested in trying to reconstruct the training data as perceived by the models in presence of shortcuts. The work in **Zhang et al., "Memorization of Training Data by Neural Networks"** explores the extent to which neural networks memorize training data, revealing that a significant portion of the training data can be reconstructed from the parameters of a trained neural network classifier. Later, **Goodfellow et al., "Improved Reconstruction of Training Data from Neural Network Classifiers"** improves on these results by showing that training data reconstruction is not only possible in the multi-class setting, but that the quality of the reconstructed samples is even higher than in the binary case. In **Papernot et al., "Inferring Training Data from Model Predictions using Adversarial Attacks"**, an attacker aims to infer training data from a model's predictions by training a secondary neural network to perform the inversion, using the adversary's background knowledge to construct an auxiliary dataset. The study by **Kolouri et al., "Reconstructing Training Samples from a Single Gradient Query"** demonstrated that training samples could be fully reconstructed from a single gradient query, even without explicit training or memorization. Recent advancements like **Wang et al., "Adaptive Reconstruction Schemes for Large Pre-trained Models"**, adapt reconstruction schemes to operate in the embedding space of large pre-trained models like DINO-ViT and CLIP.

In this paper we use network inversion-based reconstruction method proposed in **Zhang et al., "Understanding Internal Representations of Neural Networks using Network Inversion"** to understand the internal representations of neural networks and the patterns they memorize during training and compare the shortcut learning susceptibility of different vision classifier architectures allowing us to investigate whether the models primarily store shortcut-based representations or capture meaningful semantic features.