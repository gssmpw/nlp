@InProceedings{10.1007/978-3-031-72111-3_59,
author="Lin, Manxi
and Weng, Nina
and Mikolaj, Kamil
and Bashir, Zahra
and Svendsen, Morten B. S.
and Tolsgaard, Martin G.
and Christensen, Anders N.
and Feragen, Aasa",
editor="Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.",
title="Shortcut Learning in Medical Image Segmentation",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="623--633",
abstract="Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation. Our code is public at https://github.com/nina-weng/shortcut{\_}skinseg.",
isbn="978-3-031-72111-3"
}

@inproceedings{10.1145/3319535.3354261,
author = {Yang, Ziqi and Zhang, Jiyi and Chang, Ee-Chien and Liang, Zhenkai},
title = {Neural Network Inversion in Adversarial Setting via Background Knowledge Alignment},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3354261},
doi = {10.1145/3319535.3354261},
abstract = {The wide application of deep learning technique has raised new security concerns about the training data and test data. In this work, we investigate the model inversion problem under adversarial settings, where the adversary aims at inferring information about the target model's training data and test data from the model's prediction values. We develop a solution to train a second neural network that acts as the inverse of the target model to perform the inversion. The inversion model can be trained with black-box accesses to the target model. We propose two main techniques towards training the inversion model in the adversarial settings. First, we leverage the adversary's background knowledge to compose an auxiliary set to train the inversion model, which does not require access to the original training data. Second, we design a truncation-based technique to align the inversion model to enable effective inversion of the target model from partial predictions that the adversary obtains on victim user's data. We systematically evaluate our approach in various machine learning tasks and model architectures on multiple image datasets. We also confirm our results on Amazon Rekognition, a commercial prediction API that offers "machine learning as a service". We show that even with partial knowledge about the black-box model's training data, and with only partial prediction values, our inversion approach is still able to perform accurate inversion of the target model, and outperform previous approaches.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {225–240},
numpages = {16},
keywords = {deep learning, model inversion, neural networks, privacy, security},
location = {London, United Kingdom},
series = {CCS '19}
}

@ARTICLE{10250856,
  author={Ma, Chong and Zhao, Lin and Chen, Yuzhong and Guo, Lei and Zhang, Tuo and Hu, Xintao and Shen, Dinggang and Jiang, Xi and Liu, Tianming},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Rectify ViT Shortcut Learning by Visual Saliency}, 
  year={2024},
  volume={35},
  number={12},
  pages={18013-18025},
  keywords={Data models;Medical diagnostic imaging;Task analysis;Predictive models;Visualization;Deep learning;Training;Interpretability;saliency;shortcut learning;vision transformer (ViT)},
  doi={10.1109/TNNLS.2023.3310531}}

@ARTICLE{784232,
  author={Jensen, C.A. and Reed, R.D. and Marks, R.J. and El-Sharkawi, M.A. and Jae-Byung Jung and Miyamoto, R.T. and Anderson, G.M. and Eggen, C.J.},
  journal={Proceedings of the IEEE}, 
  title={Inversion of feedforward neural networks: algorithms and applications}, 
  year={1999},
  volume={87},
  number={9},
  pages={1536-1549},
  keywords={Neural networks;Feedforward neural networks;Training data;Sonar;Power system security;Neurons;Performance analysis;Control systems;Power generation;Multi-layer neural network},
  doi={10.1109/5.784232}}

@INPROCEEDINGS{9833677,
  author={Balle, Borja and Cherubin, Giovanni and Hayes, Jamie},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Reconstructing Training Data with Informed Adversaries}, 
  year={2022},
  volume={},
  number={},
  pages={1138-1156},
  keywords={Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning;Data models;machine learning;neural networks;reconstruction attacks;differential privacy},
  doi={10.1109/SP46214.2022.9833677}}

@article{Brown_2023,
   title={Detecting shortcut learning for fair medical AI using shortcut testing},
   volume={14},
   ISSN={2041-1723},
   url={http://dx.doi.org/10.1038/s41467-023-39902-7},
   DOI={10.1038/s41467-023-39902-7},
   number={1},
   journal={Nature Communications},
   publisher={Springer Science and Business Media LLC},
   author={Brown, Alexander and Tomasev, Nenad and Freyberg, Jan and Liu, Yuan and Karthikesalingam, Alan and Schrouff, Jessica},
   year={2023},
   month=jul }

@article{Geirhos_2020,
   title={Shortcut learning in deep neural networks},
   volume={2},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-020-00257-z},
   DOI={10.1038/s42256-020-00257-z},
   number={11},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
   year={2020},
   month=nov, pages={665–673} }

@article{KINDERMANN1990277,
author = {J Kindermann and A Linden},
title = {Inversion of neural networks by gradient descent},
journal = {Parallel Computing},
volume = {14},
number = {3},
pages = {277-286},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90081-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090081J},
keywords = {Inversion, multilayer perceptrons, backpropagation, generalization, robustness},
abstract = {Inversion answers the question of which input patterns to a trained multilayer neural network approximate a given output target. This method is a tool for visualization of the information processing capability of a network stored in its weights. This knowledge about the network enables one to make informed decisions on the improvement of the training task and the choice of training sets. An inversion algorithm for multilayer perceptrons is derived from the backpropagation scheme. We apply inversion to networks for digit recognition. We observe that the multilayer perceptrons perform well with respect to generalization, i.e. correct classification of untrained digits. They are however bad on rejection of counterexamples, i.e. random patterns. Inversion gives an explanation for this drawback. We suggest an improved training scheme, and we show that a tradeoff exists between generalization and rejection of counterexamples.}
}

@inproceedings{NEURIPS2020_373e4c5d,
 author = {Kumar, Aviral and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5126--5137},
 publisher = {Curran Associates, Inc.},
 title = {Model Inversion Networks for Model-Based Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{SAAD200778,
title = {Neural network explanation using inversion},
journal = {Neural Networks},
volume = {20},
number = {1},
pages = {78-93},
year = {2007},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2006.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608006001730},
author = {Emad W. Saad and Donald C. Wunsch},
keywords = {Rule extraction, Neural network explanation, Explanation capability of neural networks, Inversion, Hyperplanes, Evolutionary algorithm, Pedagogical},
abstract = {An important drawback of many artificial neural networks (ANN) is their lack of explanation capability [Andrews, R., Diederich, J., & Tickle, A. B. (1996). A survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8, 373–389]. This paper starts with a survey of algorithms which attempt to explain the ANN output. We then present HYPINV,11HYPINV stands for an algorithm which extracts HYPerplanes using INVersion. a new explanation algorithm which relies on network inversion; i.e.calculating the ANN input which produces a desired output. HYPINV is a pedagogical algorithm, that extracts rules, in the form of hyperplanes. It is able to generate rules with arbitrarily desired fidelity, maintaining a fidelity–complexity tradeoff. To our knowledge, HYPINV is the only pedagogical rule extraction method, which extracts hyperplane rules from continuous or binary attribute neural networks. Different network inversion techniques, involving gradient descent as well as an evolutionary algorithm, are presented. An information theoretic treatment of rule extraction is presented. HYPINV is applied to example synthetic problems, to a real aerospace problem, and compared with similar algorithms using benchmark problems.}
}

@inproceedings{Wong2017NeuralNI,
  title={Neural network inversion beyond gradient descent},
  author={Eric Wong},
  year={2017},
  booktitle={WOML NIPS},
  url={https://api.semanticscholar.org/CorpusID:208231247}
}

@misc{ansari2022autoinverseuncertaintyawareinversion,
      title={Autoinverse: Uncertainty Aware Inversion of Neural Networks}, 
      author={Navid Ansari and Hans-Peter Seidel and Nima Vahidi Ferdowsi and Vahid Babaei},
      year={2022},
      eprint={2208.13780},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.13780}, 
}

@misc{buzaglo2023reconstructingtrainingdatamulticlass,
      title={Reconstructing Training Data from Multiclass Neural Networks}, 
      author={Gon Buzaglo and Niv Haim and Gilad Yehudai and Gal Vardi and Michal Irani},
      year={2023},
      eprint={2305.03350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.03350}, 
}

@misc{haim2022reconstructingtrainingdatatrained,
      title={Reconstructing Training Data from Trained Neural Networks}, 
      author={Niv Haim and Gal Vardi and Gilad Yehudai and Ohad Shamir and Michal Irani},
      year={2022},
      eprint={2206.07758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.07758}, 
}

@misc{liu2022landscapelearningneuralnetwork,
      title={Landscape Learning for Neural Network Inversion}, 
      author={Ruoshi Liu and Chengzhi Mao and Purva Tendulkar and Hao Wang and Carl Vondrick},
      year={2022},
      eprint={2206.09027},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.09027}, 
}

@misc{oz2024reconstructingtrainingdatareal,
      title={Reconstructing Training Data From Real World Models Trained with Transfer Learning}, 
      author={Yakir Oz and Gilad Yehudai and Gal Vardi and Itai Antebi and Michal Irani and Niv Haim},
      year={2024},
      eprint={2407.15845},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15845}, 
}

@InProceedings{pmlr-v206-wang23g,
  title = 	 {Reconstructing Training Data from Model Gradient, Provably},
  author =       {Wang, Zihan and Lee, Jason and Lei, Qi},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6595--6612},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/wang23g/wang23g.pdf},
  url = 	 {https://proceedings.mlr.press/v206/wang23g.html},
  abstract = 	 {Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: Even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild assumptions: with shallow or deep neural networks and wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential  severe threats to privacy, especially in federated learning.}
}

@inproceedings{suhail2024network,
title={Network Inversion of Binarised Neural Nets},
author={Pirzada Suhail},
booktitle={The Second Tiny Papers Track at ICLR 2024},
year={2024},
url={https://openreview.net/forum?id=zKcB0vb7qd}
}

@inproceedings{suhail2024networkcnn,
    title={Network Inversion of Convolutional Neural Nets},
    author={Pirzada Suhail and Amit Sethi},
    booktitle={Muslims in ML Workshop co-located with NeurIPS 2024},
    year={2024},
    url={https://openreview.net/forum?id=f9sUu7U1Cp}
}

@misc{suhail2024networkinversionapplications,
      title={Network Inversion and Its Applications}, 
      author={Pirzada Suhail and Hao Tang and Amit Sethi},
      year={2024},
      eprint={2411.17777},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.17777}, 
}

@misc{wang2023reconstructingtrainingdatamodel,
      title={Reconstructing Training Data from Model Gradient, Provably}, 
      author={Zihan Wang and Jason D. Lee and Qi Lei},
      year={2023},
      eprint={2212.03714},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.03714}, 
}

