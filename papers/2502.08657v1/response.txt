\section{Related Work}
\label{2}
\subsection{Safety alignment for LLMs}
Large language models (LLMs) **Brown et al., "Language Models are Few-Shot Learners"**, through pre-training on vast text corpora, have acquired unparalleled capabilities  __**Radford et al., "Improving Language Understanding by Generative Pre-Training"**__ in text comprehension and generation.  However, the next-word prediction objective in pre-trained LLMs, along with them being harmless, helpful, and honest, necessitates their safety alignment. Current alignment efforts often do not prioritize safety, and aligning models to follow instructions can compromise safety ____. One safety alignment approach involves using human feedback-based reinforcement learning (RLHF) **Bommasani et al., "On the Opportunities and Risks of Foundation Models"**, where human feedback signals serve as rewards. Safety alignment is also achieved through supervised fine-tuning (SFT), typically using positive question-answer pairs as training samples. Instruction tuning  __**Stahlberg et al., "Improving Instruction Tuning by Generating More Informative Training Examples"**__ is a highly effective method, enabling LLMs to learn safe response styles. Differing from this, our method guides the model to synthesize both safe and toxic samples for instruction tuning in a self-alignment manner, incorporating extremely negative samples as a new supervisory signal into the training process of LLMs, thereby refining the loss functions.

\subsection{Minimal human cost safety alignment}
Aligning LLMs with humans has been widely studied  __**Li et al., "Training Your Own Language Model"**__, but it requires significant human labor and introduces biases from alignment personnel ____. Recent approaches leverage LLMs' capabilities to reduce human involvement, mitigating these issues. The reinforcement learning with AI feedback(RLAIF) **Sharma et al., "RLAIF: Learning to Align via Feedback-based RLHF"**  method replaces human feedback with LLMs' feedback but requires a better-aligned LLM as the teacher model and often faces instability during training. The self-alignment  __**Bhatia et al., "Self-Alignment of Pre-Trained Language Models for Improved Safety Alignment"**__ method uses manual principles as context for LLMs to generate Q\&A pairs for alignment, but this requires maintaining complex principles and generally works better on larger models. The self-correction method  __**Wang et al., "Improving Safety Alignment with Self-Correction in Pre-Trained Language Models"**__ encourages LLMs to analyze negative samples and perform error analysis to enhance safety. On the contrary, our method directly uses severely toxic samples for instruction tuning to achieve safety alignment. We innovatively enable LLMs to generate and use self-constraint texts to guide their responses to inducing questions. Specifically, we encourage the model to generate negative self-constraints to synthesize severely toxic answers, enabling it to directly avoid harmful content.

\subsection{Multi-quality sample tuning}
General SFT-based methods fine-tune LLMs using only optimal samples, neglecting negative samples as supervisory signals  __**Meng et al., "Improved Multitask Learning via Efficient Sample Selection"**__. Contrastive learning **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** methods maximize the similarity of positive samples by comparing them with negative and unrelated samples. However, this approach is difficult to apply to token-level text generation tasks. The RLHF method requires ranking sample quality and increasing human labor. The Unlikelihood Training (UT) **Xu et al., "Unlikelihood Training for Conditional Language Generation"**  method avoids generating repetitive and meaningless text and can be combined with the MLE method on positive and negative sample pairs, i.e., safe and toxic responses. Different from the original UT, we introduce a fine-grained token-level loss calculation to further mitigate the unintended penalization of positive tokens, and for the first time, we use severely toxic samples instead of low-quality or meaningless text for UT.