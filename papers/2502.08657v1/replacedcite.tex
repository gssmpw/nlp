\section{Related Work}
\label{2}
\subsection{Safety alignment for LLMs}
Large language models (LLMs)____, through pre-training on vast text corpora, have acquired unparalleled capabilities ____ in text comprehension and generation.  However, the next-word prediction objective in pre-trained LLMs, along with them being harmless, helpful, and honest, necessitates their safety alignment. Current alignment efforts often do not prioritize safety, and aligning models to follow instructions can compromise safety ____. One safety alignment approach involves using human feedback-based reinforcement learning (RLHF)____, where human feedback signals serve as rewards. Safety alignment is also achieved through supervised fine-tuning (SFT), typically using positive question-answer pairs as training samples. Instruction tuning ____ is a highly effective method, enabling LLMs to learn safe response styles. Differing from this, our method guides the model to synthesize both safe and toxic samples for instruction tuning in a self-alignment manner, incorporating extremely negative samples as a new supervisory signal into the training process of LLMs, thereby refining the loss functions.

\subsection{Minimal human cost safety alignment}
Aligning LLMs with humans has been widely studied ____, but it requires significant human labor and introduces biases from alignment personnel ____. Recent approaches leverage LLMs' capabilities to reduce human involvement, mitigating these issues. The reinforcement learning with AI feedback(RLAIF) ____ method replaces human feedback with LLMs' feedback but requires a better-aligned LLM as the teacher model and often faces instability during training. The self-alignment ____ method uses manual principles as context for LLMs to generate Q\&A pairs for alignment, but this requires maintaining complex principles and generally works better on larger models. The self-correction method ____ encourages LLMs to analyze negative samples and perform error analysis to enhance safety. On the contrary, our method directly uses severely toxic samples for instruction tuning to achieve safety alignment. We innovatively enable LLMs to generate and use self-constraint texts to guide their responses to inducing questions. Specifically, we encourage the model to generate negative self-constraints to synthesize severely toxic answers, enabling it to directly avoid harmful content.

\subsection{Multi-quality sample tuning}
General SFT-based methods fine-tune LLMs using only optimal samples, neglecting negative samples as supervisory signals ____. Contrastive learning ____ methods maximize the similarity of positive samples by comparing them with negative and unrelated samples. However, this approach is difficult to apply to token-level text generation tasks. The RLHF method requires ranking sample quality and increasing human labor. The Unlikelihood Training (UT) ____ method avoids generating repetitive and meaningless text and can be combined with the MLE method on positive and negative sample pairs, i.e., safe and toxic responses. Different from the original UT, we introduce a fine-grained token-level loss calculation to further mitigate the unintended penalization of positive tokens, and for the first time, we use severely toxic samples instead of low-quality or meaningless text for UT.