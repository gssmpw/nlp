\documentclass[journal]{IEEEtran}
% \usepackage{caption}
\usepackage{amsmath,amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm,algpseudocode}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[table]{xcolor}
\definecolor{mintgreen}{rgb}{0.24, 0.71, 0.54} % 自定义薄荷绿色
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{ulem}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage[caption=false,font=footnotesize]{subfig}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{xcolor}
%\usepackage{multirow}  
% \usepackage{ulem}
% \usepackage{cite}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
% \usepackage{svg}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}
\title{Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions} 

%\title{Semantic-guided Subcarrier Allocation and Physical Layer Encryption for Wireless Semantic Communications} 

\author{Jingxin Xu, Guoshun Nan,~\IEEEmembership{~Member,~IEEE,} Sheng Guan, Sicong Leng, Yilian Liu, Zixiao Wang, Yuyang Ma, Zhili Zhou, Yanzhao Hou,~\IEEEmembership{~Member,~IEEE,} Xiaofeng Tao,~\IEEEmembership{~Senior Member,~IEEE}


\thanks{J. Xu, G. Nan, S. Guan, Y. Liu, Z. Wang, Y. Ma, Y. Hou, X. Tao are with National Engineering Research Center for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, 100876, China. (e-mail: xujingxin@bupt.edu.cn; nanguo2021@bupt.edu.cn; guansheng2022@bupt.edu.cn; liuyilian@bupt.edu.cn; ntb666@bupt.edu.cn; mayuyang@bupt.edu.cn; houyanzhao@bupt.edu.cn; taoxf@bupt.edu.cn).}
\thanks{S. Leng (sicong001@e.ntu.edu.sg) is an assistant researcher at Nanyang Technological University, Singapore.}
\thanks{Z. Zhou (zhou\_zhili@gzhu.edu.cn) is an assistant researcher at Guangzhou University, Guangzhou, China.}
\thanks{Corresponding author: G. Nan (e-mail: nanguo2021@bupt.edu.cn).}
}
% \iffalse
% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}

% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space

% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
% }
% \fi

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.
% \IEEEpubidadjcol
\maketitle
\begin{abstract}
Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than \(50\) human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on \(9\) popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.
\end{abstract}

\begin{IEEEkeywords}
Large Language Models,  Safety and Robustness, Alignment, Natural Language Processing.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{R}{ecent} proliferation AI-assistant agents, such as OpenAI's ChatGPT~\cite{achiam2023gpt}, DeepSeek~\cite{liu2024deepseek}, Meta's LLaMA~\cite{touvron2023llama1}, and Claude~\cite{bai2022constitutional}, demonstrate the powerful ability of large language models (LLMs) to understand and generate natural language, positioning them as the semantic backbone for many generative systems. However, current models still exhibit safety shortcomings. LLMs can produce harmful responses or disclose private information even without red-teaming prompts. Additionally, these generative models may facilitate the synthesis and distortion of false information or exacerbate broader information security threats through misuse. 
Furthermore, fine-tuning with commonly used datasets may inadvertently degrade the safety alignment of LLMs, posing significant risks to downstream applications~\cite{yudkowsky2016ai,gabriel2020artificial}.

With the widespread deployment and application of LLMs, ensuring their alignment with human values is essential to produce outputs that are harmless, helpful, and honest~\cite{wang2023self, pang2024selfalignment, liu2024mixture, 10795202, 10795188}. Modern LLMs achieve safety alignment through Supervised Fine-Tuning (SFT)~\cite{weifinetuned} and Reinforcement Learning from Human Feedback (RLHF)~\cite{bai2022training} to mitigate toxic, undesirable, or otherwise prohibited outputs. SFT-based methods align LLMs with human values by using large sets of manually annotated instruction-response pairs, primarily teaching the models the paradigm of generating safe responses. On the other hand, RL-based methods utilize a reward model to select relatively better responses based on human feedback, guiding LLMs to generate appropriate replies. In general, these methods are heavily relying on large amounts of harmless and positive responses meticulously designed and annotated by domain experts. Meanwhile, toxic and negative responses are typically cleaned or discarded, with limited consideration of their role in safety alignment. 

In the post-training stage, particularly during instruction fine-tuning, including toxic data or even general-purpose datasets can compromise the model’s safety performance \cite{qi2024fine}. Figure \ref{fig:example} illustrates that many well-aligned LLMs, such as Vicuna, LLaMA-chat, and DeepSeek, are susceptible to producing harmful content when exposed to adversarial instructions or jailbreak attacks. These compelling examples strongly underscore the pressing necessity of implementing significantly more robust and effective safety mechanisms during the instruction fine-tuning process.
\IEEEpubidadjcol

However, directly applying methods like SFT or RLHF to expose toxic responses in LLMs may cause the model to internalize harmful text patterns. Moreover, recollecting toxic samples specifically for instructions and positive response 
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{./pictures/example.pdf}
\caption{Some real cases of LLMs, including Vicuna-13B-Chat, LLaMA-13B, and DeepSeek-V3 \cite{liu2024deepseek}, generate harmful content with malicious instructions. These LLMs are tuned with general-purpose datasets such as Alpaca \cite{taori2023stanford} and ShareGPT\cite{chiang2023vicuna}. However, the \textcolor{red}{toxic outputs} generated by these models are often discarded, hindering the model from learning from its mistakes.}
\label{fig:example}
\end{center}
\end{figure*}

\noindent pairs, which adds additional burdens to the already labor-intensive task of annotating safety samples, can substantially increase the manual workload. As safety scenarios become more complex and private safety requirements continue to grow, this manual process risks becoming both inadequate and prohibitively expensive. This raises a critical question: \textit{How can we enable LLMs to acquire safety knowledge from both positive and toxic samples with minimal manual effort, thereby improving model safety while maintaining effectiveness?}

As early studies \cite{wang2022self, ouyang2022training, ji2024beavertails} that employ SFT and RLHF require costly human supervision and may lead to potential biases and unfairness \cite{wan2023poisoning,kopf2024openassistant}  for safety alignment, the recently proposed self-alignment method Dromedary \cite{sun2024principle} explores hundreds of human-written annotations and complex rules to guide agents based on LLMs for creating harmless and human-aligned content. A recent work~\cite{li2023rain} discusses that the existing Dromedary would be more effective on significantly large models, e.g., 65B-level LLMs. We argue that self-alignment methods are better suited for purely safety-alignment applications, as the ideal text paradigms for both positive and toxic samples are relatively simple, and the safety domain is significantly narrower and more target-specific compared to the breadth of human values. These factors considerably lower the demands on the fundamental capabilities of LLMs, enabling smaller-scale models to achieve effective self-safety alignment. 

Moreover, while safety alignment methods relying on carefully curated human preference data\cite{lee2023rlaif} have made significant progress, constructing high-quality positive samples remains challenging due to the ambiguity\cite{lodi2017self} and diversity of requirements, which often result in noisy preference labels\cite{kim2024aligning,wang2024secrets}. Our observations reveal that each pair of positive and negative samples typically exhibits high semantic similarity, with only marginal differences in harmlessness rewards. More concerningly, harmful preference content frequently appears in preference datasets, potentially undermining the safety alignment process and reinforcing negative outputs\cite{bai2022training,wang2024secrets,ouyang2022training}. Therefore, we highlight a new research direction: constructing highly polarized sample pairs with extreme positive and negative samples for the same unsafe instructions, leveraging a larger safety disparity to minimize model harmfulness while preserving its usefulness\cite{nguyen2020variational,luo2023empirical}.

Inspired by these studies, we propose leveraging toxic samples as a novel supervisory signal, allowing the model to encounter and avoid harmful content through polarized sample pairs. This approach enables more direct learning of safe content, enhancing safety while preserving helpfulness. By employing self-constraint as in-context guidance and a dualistic sample synthesis process, the method minimizes the need for human supervision and avoids additional manual burdens when utilizing positive and toxic samples.

To answer the above challenging question, we introduce PT-ALIGN (Figure \ref{fig:model}), a novel self-alignment method of safety for LLM that minimizes human supervision by refining positive and toxic samples in an automatic and contrastive manner, and performing fine-grained dual instruction tuning. Specifically, the target model first generates instructions tailored to the specific safety theme. It then infers paired positive and toxic responses contrastively, constrained by the model’s self-generated context. Finally, it learns safe response patterns through a combination of MLE and fine-grained UT methods, guiding the probability distribution away from toxic content while preserving the model’s utility. The contributions of this paper are summarized as follows.

\begin{itemize}
    \item \textbf{PT-samples generation:} We innovatively construct both positive and toxic responses dually for safety alignment, requiring minimal human supervision (\(<\)50 human annotations). Guided by self-generated constraints, LLMs can synthesize both safe and malicious responses tailored to custom-defined target safety domains, significantly reducing the need for manual effort.
    \item \textbf{Dual safety alignment:} We innovatively develop a fine-grained dual instruction-tuning method. Specifically, we introduce maximum likelihood estimation (MLE) and fine-grained token-level unlikelihood training (UT) for both positive and toxic samples. This approach enables LLMs to learn from sample pairs with stark safety contrasts, guiding the model closer to the safe domain and further from toxic content, thereby improving safety while preserving effectiveness.
    \item \textbf{Experiments:} We conduct extensive experiments on 9 popular open-source LLMs to demonstrate the effectiveness of PT-ALIGN in enhancing safety while maintaining comparable helpfulness and general performance. We also conducted experiments and discussions from multiple perspectives, including comparisons of safety fine-tuning capabilities, sample characteristics, training efficiency, and others, to further validate the effectiveness of the PT-ALIGN method.
\end{itemize}

\textbf{Paper organization and notations :}
The remainder of the paper is organized as follows. Section \ref{2} provides a review of previous work and elaborates further on the innovative aspects of the PT-ALIGN method. Section \ref{3} presents the methodology and architecture of the proposed PT-ALIGN, which mainly includes instruction synthesis for the target safety domain, contrastive generation of positive and toxic samples, and fine-grained dual instruction fine-tuning. Section \ref{4} presents our main experimental results, including the safety enhancement achieved by PT-ALIGN for both vanilla LLMs and aligned LLMs, the quantification of model safety alignment tax, case studies, etc. Section \ref{5} discusses the comparative analysis of PT-ALIGN with other methods, highlighting its advantages in achieving safer and more effective alignment. Section \ref{6} concludes the findings of this study.

\begin{table}[ht]
\caption{Definitions of Notations\label{tab:table1}}
\centering
\begin{tabular}{|c|c|}
\hline
 Notations  & Definitions\\
 \hline
        $D$ & Set of safety domains\\
         \hline
        $d$ & Single safety domain, $d \in D$\\
         \hline
        $T$ & Safety topics derived from safety domains\\
         \hline
        $t$ & Single safety topic, $t \in T$\\
         \hline
        $I$ & Set of safety-related instructions\\
         \hline
        $i$ & Single instruction, $i \in I$\\
         \hline
        $c_p$, $c_n$ & Initial positive and negative constraints\\
         \hline
        $C_p$, $C_n$ & Complete positive and negative constraint sets\\
         \hline
        $P$ & Positive (harmless) responses\\
         \hline
        $p$ & Single positive response, $p \in P$\\
         \hline
        $N$ & Negative (toxic) responses\\
         \hline
        $n$ & Single negative response, $n \in N$\\
         \hline
        $S$ & Dataset of instruction-response triplets $\{(I, P, N)\}$\\
         \hline
        $s$ & Single triplet $(i, p, n)$, $s \in S$\\
         \hline
        $f(\cdot)$ & Function to generate safety topics\\
         \hline
        $\mathcal{Z}$ & Contextual in-context learning (ICL) examples\\
         \hline
        $L_{\mathrm{MLE}}$ & Maximum likelihood estimation loss\\
         \hline
        $L_{\mathrm{UT}}$ & Unlikelihood training loss\\
         \hline
        $L$ & Total loss combining $L_{\mathrm{MLE}}$ and $L_{\mathrm{UT}}$\\
         \hline
        $\lambda$ & Penalty coefficient for $L_{\mathrm{UT}}$\\
         \hline
        $\theta$ & Trainable parameters of the LLM\\
         \hline
        $k$ & Sequence position index\\
         \hline
        $\alpha$ & Length of the instruction $i$\\
         \hline
        $\gamma^*$ & Index of the first mismatched token between $p$ and $n$\\
         \hline
        $\mathbb{I}(\cdot)$ & Indicator function\\
         \hline
        $Pr_\theta$ & Probability distribution predicted by the LLM\\
         \hline
    \end{tabular}
\end{table}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{./pictures/model.pdf}
\caption{Our proposed PT-ALIGN overview: An illustration of the essential pipeline in the three processes. The unaligned LLM acts both as the Red Team and the annotator.}
\label{fig:model}
\end{center}
\end{figure*}

\section{Related Work}
\label{2}
\subsection{Safety alignment for LLMs}
Large language models (LLMs)\cite{zhao2023survey}, through pre-training on vast text corpora, have acquired unparalleled capabilities \cite{bommasani2021opportunities} in text comprehension and generation.  However, the next-word prediction objective in pre-trained LLMs, along with them being harmless, helpful, and honest, necessitates their safety alignment. Current alignment efforts often do not prioritize safety, and aligning models to follow instructions can compromise safety \cite{bianchisafety, qi2024fine}. One safety alignment approach involves using human feedback-based reinforcement learning (RLHF)\cite{wu2024fine}, where human feedback signals serve as rewards. Safety alignment is also achieved through supervised fine-tuning (SFT), typically using positive question-answer pairs as training samples. Instruction tuning \cite{ouyang2022training} is a highly effective method, enabling LLMs to learn safe response styles. Differing from this, our method guides the model to synthesize both safe and toxic samples for instruction tuning in a self-alignment manner, incorporating extremely negative samples as a new supervisory signal into the training process of LLMs, thereby refining the loss functions.

\subsection{Minimal human cost safety alignment}
Aligning LLMs with humans has been widely studied \cite{stiennon2020learning, ouyang2022training}, but it requires significant human labor and introduces biases from alignment personnel \cite{wang2022self, kopf2024openassistant, wan2023poisoning}. Recent approaches leverage LLMs' capabilities to reduce human involvement, mitigating these issues. The reinforcement learning with AI feedback(RLAIF) \cite{akyürek2023rl4f} method replaces human feedback with LLMs' feedback but requires a better-aligned LLM as the teacher model and often faces instability during training. The self-alignment \cite{sun2024principle} method uses manual principles as context for LLMs to generate Q\&A pairs for alignment, but this requires maintaining complex principles and generally works better on larger models. The self-correction method \cite{chengaining} encourages LLMs to analyze negative samples and perform error analysis to enhance safety. On the contrary, our method directly uses severely toxic samples for instruction tuning to achieve safety alignment. We innovatively enable LLMs to generate and use self-constraint texts to guide their responses to inducing questions. Specifically, we encourage the model to generate negative self-constraints to synthesize severely toxic answers, enabling it to directly avoid harmful content.

\subsection{Multi-quality sample tuning}
General SFT-based methods fine-tune LLMs using only optimal samples, neglecting negative samples as supervisory signals \cite{nozawa2021understanding, wang2024learning}. Contrastive learning \cite{tian2020makes} methods maximize the similarity of positive samples by comparing them with negative and unrelated samples. However, this approach is difficult to apply to token-level text generation tasks. The RLHF method requires ranking sample quality and increasing human labor. The Unlikelihood Training (UT) \cite{welleck2020neural} method avoids generating repetitive and meaningless text and can be combined with the MLE method on positive and negative sample pairs, i.e., safe and toxic responses. Different from the original UT, we introduce a fine-grained token-level loss calculation to further mitigate the unintended penalization of positive tokens, and for the first time, we use severely toxic samples instead of low-quality or meaningless text for UT.

\section{PT-ALIGN Method}
\label{3}
\subsection{Overview}
Our proposed PT-ALIGN utilizes minimal seed samples and human annotations. The target LLMs synthesize instructions and responses under self-guidance. Additionally, instruction tuning is performed using both safe and severely harmful samples. It consists of two parts: \textbf{Refining of positive and toxic samples} and \textbf{Fine-grained dual instruction tuning}, following the process outlined in Figure \ref{fig:data}. 

\textbf{Refining of positive and toxic samples:}
An LLM subdivides the safety domains \(D\) into safety topics \(T\). Based on \(\{ (D, T) \}\), the LLM generates a set of safety-related instructions \(I\). Initial constraints are defined as \(c_p\) and \(c_n\), guiding the model in generating safe responses \(P\) and toxic responses \(N\), respectively. The LLM synthesizes two sets of complete constraints, \(C_p\) and \(C_n\). The model performs positive and toxic (negative) annotations for the instruction set \(I\) under self-constraints \(C_p\) and \(C_n\), resulting in dataset \( S = \{ (I, P, N) \} \).

\textbf{Fine-grained dual instruction tuning:}
We apply MLE and fine-grained UT on the positive and toxic samples within dataset \(S\). Specifically, to refine the loss function calculation at the token level, we exclude the first few identical tokens in positive and toxic sample pairs from the UT loss calculation to prevent unintended penalization of positive samples during gradient descent. The implementation details of each component are outlined below.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{./pictures/data.pdf}
\caption{Pipeline for synthesizing a safety alignment dataset. Steps 1 to 3: The model subdivides a large number of safety topics based on the given safety domains, then synthesizes a large volume of instructions using these topics and ten seed examples. Steps 4 and 5: The model continues from a manually written constraint to generate a complete self-constraint for use as ICL text (a similar process for negative constraints). Step 6: The self-constraint prompt and three seeds equipped with inner thoughts collectively guide the model in annotating the instructions.}
\label{fig:data}
\end{center}
\end{figure*}

\subsection{Refining of positive and toxic samples}
\paragraph{Safety-driven red teaming}
This process broadly follows the self-instruct method \cite{wang2022self}. To prioritize alignment safety, we directly integrate safety domain types as specified by OpenAI and MetaAI usage policies \cite{opeanai2023} \cite{touvron2023llama,ganguli2022red}, resulting in ten safety domain categories \( D = \{ D_1, \ldots, D_{10} \} \), such as \textit{privacy violation}. To broaden the scope of refinement and maximize the diversity of the instruction set \(I\), the LLM subdivides the safety domain categories \(D_i\). Ten related safety topics \(T_i\) are generated for each safety domain \(D_i\) as \(T_{i} = f(D_i) = \{T_{i1}, T_{i2}, \ldots, T_{i10}\}\). After deduplication, we obtain a dataset of safety topics along with their corresponding safety domains \(\{ (D_i, T_{ij}) \mid T_{ij} = f(D_i)\}\), where \(f\) denotes the generation process of the LLM under a prompt. We prepend a concise prefix to each safety topic to streamline the instruction generation process.

Next, we generate a large number of diverse, high-quality safety-related instructions rather than questions, as instructions are more likely to provoke LLMs into generating harmful content, and using the instruction format in training better ensures model safety \cite{bianchisafety}. Each instruction is associated with a safety topic and is capable of inducing the model to generate incorrect or toxic responses. Using annotation prompts for all safety topics and ten seeds (generated using Gemini-1.0-pro \cite{team2023gemini}, one for each safety domain, totaling ten. The generation of different seed samples across the three sets had minimal impact on the experimental results) as ICL examples, the LLM will synthesize a large number of corresponding instructions for each topic. Based on predefined rules, it will then deduplicate and remove low-quality instructions. We ultimately refine a set of high-quality instructions \(I\) that cover various safety domains. Our researchers ensured that the seeds and training data we constructed did not overlap with the test data. 

\paragraph{Self-constraint-driven positive and toxic responses}
To refine harmless and severely toxic responses  \(P, N\) for the safety-related instruction set \(I\) established above, we leverage the self-constraint capabilities of the unaligned LLM, further minimizing the need for human supervision.

First, taking the synthesis process of the positive sample \(P\) as an example (the negative sample \(N\) follows a similar process), we generate self-constraint prompts to serve as ICL for the LLM's harmless responses. Starting with a simple human-defined constraint \(c_p\), the LLM performs text completion to construct a comprehensive set of constraint prompts \(C_p\), the LLM performs text completion to construct a comprehensive set of constraints prompt \(C_p\). This approach reduces the need for extensive expert adjustments. 

Next, the LLM can undertake the response process. Under the supervision of \(C_p\) and with three provided ICL examples, the LLM will emulate these examples, generating \textit{inner thoughts} \cite{wei2022chain} \cite{zhang2023wisdom}that compel it to provide reasoning for its responses. This prevents the LLM from mechanically providing the same response to all instructions or simply refusing to answer, encouraging it to adapt its replies to the specific circumstances of each instruction, thereby ensuring the quality of the refined safety samples. This process results in the generation of the positive annotation set \(P\).

To generate severely toxic, incorrect, or low-quality responses, we follow a similar procedure. The LLM is provided with negative ICL self-constraint prompts \(C_n\) to produce negative responses \(N\) for the instruction dataset \(I\). This yields a final dataset \( S = \{ (I, P, N) \} \) consisting of instruction-response triplets. The pseudocode for the data synthesis process is provided in Algorithm 1.

\begin{algorithm}[!t]
\footnotesize
\caption{Refining Positive and Toxic Samples}
\label{alg:double_sample_synthesis_en}
\begin{algorithmic}[1]
\Require 
  \textbf{LLM}: target language model \\
  $D = \{D_1,\ldots,D_{10}\}$: safety domains \\
  $f(\cdot)$: generates topics for each domain \\
  $\mathcal{Z}$: contextual ICL examples \\
  $c_p$: initial positive constraint \\
  $c_n$: initial negative constraint
\Ensure 
  $S = \{(I_k, P_k, N_k)\}$: instruction-response triplets
\Function{DoubleSampleDataSynthesis}{LLM, $D$, $f$, $\mathcal{Z}$, $c_p$, $c_n$}
  \State $I \gets \varnothing$
  \State $C_p \gets \Call{AutoComplete}{c_p, \text{LLM}}$ 
  \State $C_n \gets \Call{AutoComplete}{c_n, \text{LLM}}$
  \For{each $D_i \in D$}
    \State $T_i \gets f(D_i)$ \Comment{Generate topics for $D_i$}
    \State $T_i \gets \Call{DeduplicateTopics}{T_i}$ \Comment{Remove duplicate topics}
    \For{each $T_{ij} \in T_i$}
      \State $I_{ij} \gets \Call{GenerateInstr}{T_{ij}, \mathcal{Z}, \text{LLM}}$
      \State $I_{ij} \gets \Call{Deduplicate}{I_{ij}}$ \Comment{Remove duplicate instructions}
      \State $I_{ij} \gets \Call{FilterLowQuality}{I_{ij}}$
      \State $I \gets I \cup I_{ij}$
    \EndFor
  \EndFor
  \State $S \gets \varnothing$
  \For{each $I_k \in I$}
    \State $(P_k, N_k) \gets \Call{GenerateResponsesPair}{I_k, \mathcal{Z}, C_p, C_n, \text{LLM}}$
    \Comment{Inner thought guides response generation}
    \State $S \gets S \cup \{(I_k, P_k, N_k)\}$
  \EndFor
  \State \Return $S$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Fine-grained dual instruction tuning}
\paragraph{Positive samples instruction tuning}
To maximize the available capacity within the standard LLM's maximum token length and enable the model to directly learn the intrinsic relationships between instructions and responses, thereby enhancing its defensive ability across a wider range of inducing questions, the triplet training set will exclude all ICL examples and \textit{inner thought} information. Only instructions, safe responses, and toxic responses will be retained in the instruction tuning dataset. 

Let \(s\) be a training sample from dataset \(S\), consisting of a triplet of textual information: instruction \(i\), positive example \(p\), and negative example \(n\), represented as \( s =  (i, p, n)  \). The instruction \(i\), with length \( \alpha \), can be represented as \( i = \{ i_1, \ldots, i_\alpha \} \). Similarly, \(p\) and \(n\) can be represented as \( p = \{ p_1, \ldots, p_\beta \} \) and \( n = \{ n_1, \ldots, n_\gamma \} \), respectively.

During the LLM safety alignment for the instruction tuning process, the model is typically optimized through maximum likelihood estimation (MLE), ensuring it can accurately generate the desired positive response \(p\) given the input \(i\), thereby learning the paradigm of safe and reliable responses to inducing questions. The loss function for this process is as follows. 
\begin{equation}
L_{MLE}=-\frac{1}{N}(\sum_{\beta} \log Pr_{\theta}\left(p_{\beta} \mid p_{<\beta}, i\right))
\end{equation}
where \( \theta \) represents the trainable parameters within the unaligned LLM.This process will maximally compel the LLM to use harmless words to counteract harmful leading instructions.

\paragraph{Negative samples safety alignment}
Aligning LLMs solely through the MLE method can cause them to deviate from their training objectives, leading to the generation of unhelpful or even nonsensical content when facing general questions. An automated calibration mechanism is lacking. To address this, and enable the LLM to evolve more effectively and directly from generating harmful content towards a probability distribution that is both safe and helpful, we introduce severe toxic and low-quality negative samples \(n\) for unlikelihood training (UT), aiming to alter the probability distribution and compel the model to avoid generating negative samples \(n\) based on the input \(i\). A typical token-level UT loss function is shown below.
\begin{equation}
L_{UT}=-\frac{1}{N}(\sum_{\gamma} \log (1 - Pr_{\theta}\left(n_{\gamma} \mid n_{<\gamma}, i\right)))
\end{equation}
Based on this, PT-ALIGN will handle harmful text at the token level, reducing its likelihood of generation and thereby increasing the chances of the model generating reliable and helpful content in general situations.

Additionally, directly combining the two loss functions mentioned above does not meet fine-grained requirements. Positive and toxic samples often share similar word choices and sentence structures, despite having significant differences in safety polarity. This similarity can result in unintended penalization of positive samples, making it difficult for the model to efficiently distinguish between the two types of samples and preventing LLMs from effectively learning from them. To address this, we adopt a token-level loss function and integrate a fine-grained alignment learning mechanism. Let \(\mathbb{I}(\text{condition})\) be an indicator function, where \(\mathbb{I}(\text{condition}) = 1\) if the condition is true, and \(\mathbb{I}(\text{condition}) = 0\) otherwise; see Equation \ref{eq:i}. We adopt a linear combination of the MLE and fine-grained UT loss functions as the final loss function \(L\), with a penalty coefficient \(\lambda\) specifically for \(L_{UT}\). The final loss function is as follows.

\begin{multline}
L = -\frac{1}{N} \left( \sum_{\beta} \log Pr_{\theta}\left(p_{\beta} \mid p_{<\beta}, i\right) \right. \\
\left. + \lambda \sum_{\gamma} \mathbb{I} \log \left(1 - Pr_{\theta}\left(n_{\gamma} \mid n_{<\gamma}, i\right)\right) \right)
\end{multline}
\begin{equation}
\mathbb{I}\left(\gamma \geq \min\{\gamma \mid n_{\gamma} \neq p_{\gamma}\}\right)
\label{eq:i}
\end{equation}

With the aforementioned specific modifications to the model's instruction tuning loss function, the unaligned LLM will undergo safety self-alignment directly using the carefully designed triplet dataset \(S\) without any additional ICL examples. The detailed pseudocode for the Fine-grained Dual Instruction Tuning process is provided in Algorithm 2.

\begin{algorithm}[!t]
\footnotesize
\caption{Fine-grained Dual Instruction Tuning}
\label{alg:finegrained_dualtuning}
\begin{algorithmic}[1]
\Require
  \textbf{LLM} with parameters $\theta$ \\
  $S = \{(i, p, n)\}$: instruction-positive-negative triplets \\
  $\lambda$: UT penalty coefficient
\Ensure
  \textbf{LLM} with updated parameters $\theta^*$

\Function{FineGrainedDualInstructionTuning}{LLM, $S$, $\lambda$}
  \State Initialize $\theta$
  \For{each training epoch}
    \For{each mini-batch $B \subseteq S$}
      \State $\text{total\_loss} \gets 0$
      \For{each $(i, p, n)$ in $B$}
        \State $\text{logits\_pos} \gets \Call{ForwardPass}{\text{LLM}, (i, p)}$
        \State $L_{\mathrm{MLE}} \gets \Call{CrossEntropyLoss}{\text{logits\_pos}, p}$
        \State $\text{logits\_neg} \gets \Call{ForwardPass}{\text{LLM}, (i, n)}$
        \State $\gamma^* \gets \Call{FindFirstMismatch}{p, n}$
        \State $L_{\mathrm{UT}} \gets \Call{UnlikelihoodLoss}{\text{logits\_neg}, n, \gamma^*}$
        \State $L \gets L_{\mathrm{MLE}} + \lambda \times L_{\mathrm{UT}}$
        \State $\text{total\_loss} \gets \text{total\_loss} + L$
      \EndFor
      \State \Call{BackpropAndUpdate}{$\text{LLM}, \text{total\_loss}$}
    \EndFor
  \EndFor
  \State \Return \text{LLM}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{table*}[t]
  \caption{Safety alignment performance of our proposed PT-ALIGN method. The \textit{harm score} column reflects the performance on the PKU-SafeRLHF Evaluation, while the \textit{pku-preference} column shows the performance on the PKU-SafeRLHF Preference. The columns \textit{harmless}, \textit{helpful}, \textit{honest}, \textit{other}, and \textit{overall} present the results from the HHH evaluation.}
\begin{center}
  \begin{tabular}{l|cc|cccccc}
    \toprule
    \textbf{Method} & \textbf{harm score↓} & \textbf{pku-preference} & \textbf{harmless} & \textbf{helpful} & \textbf{honest} & \textbf{other} & \textbf{hhh-overall} \\
    \midrule
    LLaMA2-13B-chat\cite{touvron2023llama} & 0.52 & 53.5 &  \underline{89.66} & \textbf{83.05} & 72.13 & 88.37 & 82.81 \\
    Vicuna-13B\cite{chiang2023vicuna} & 0.99 & 53.5 & 81.03 & \textbf{83.05} & 70.49 & \textbf{93.02} & 81.00 \\
    Koala-13B\cite{koala2023} & 1.08 & 49.2 & 79.31 & \underline{81.36} & 70.49 & 86.05 & 78.73 \\
    Alpaca-13B\cite{taori2023stanford} & 2.28 & 43.9 & 65.52 & 74.58 & 73.77 & 86.05 & 74.98 \\
    ChatGLM3-6B\cite{zengglm} & 1.05 & 50.6 & 84.48 & 77.97 & 73.77 & 79.07 & 78.73 \\
    LLaMA2-13B\cite{touvron2023llama} & 2.21 & 45.9 & 74.14 & \underline{81.36} & 73.77 & 88.37 & 78.73 \\
    \midrule
    Ours (PT-ALIGN-DPO) & 1.12 \textcolor{mintgreen}{\(\downarrow1.09\)} 
    & 48.2 \textcolor{mintgreen}{\(\uparrow2.3\)} 
    & 77.59 \textcolor{mintgreen}{\(\uparrow\)}\textcolor{white}{00}\textcolor{mintgreen}{3.45} 
    & 79.66 \textcolor{mintgreen}{\(\uparrow1.70\)} 
    & \underline{75.41} \textcolor{mintgreen}{\(\uparrow1.64\)} 
    & \textbf{93.02} \textcolor{mintgreen}{\(\uparrow4.65\)} 
    & 80.54 \textcolor{mintgreen}{\(\uparrow1.81\)} \\

    Ours (PT-ALIGN-KTO) & 0.44 \textcolor{mintgreen}{\(\downarrow1.77\)} 
    & 52.4 \textcolor{mintgreen}{\(\uparrow6.5\)} 
    & \underline{89.66} \textcolor{mintgreen}{\(\uparrow15.52\)} 
    & 77.97 \textcolor{red}{\(\downarrow3.39\)} 
    & \underline{75.41} \textcolor{mintgreen}{\(\uparrow1.64\)} 
    & \underline{90.70} \textcolor{mintgreen}{\(\uparrow2.33\)} 
    & 82.81 \textcolor{mintgreen}{\(\uparrow4.08\)} \\

    Ours (PT-ALIGN-SFT) & \underline{0.43} \textcolor{mintgreen}{\(\downarrow1.78\)} 
    & \underline{53.4} \textcolor{mintgreen}{\(\uparrow7.5\)} 
    & \textbf{98.28} \textcolor{mintgreen}{\(\uparrow24.14\)} 
    & 76.27 \textcolor{red}{\(\downarrow5.09\)} 
    & 73.77 \textcolor{mintgreen}{\(\uparrow0.00\)} 
    & 86.05 \textcolor{red}{\(\downarrow2.32\)} 
    & \underline{83.26} \textcolor{mintgreen}{\(\uparrow4.53\)} \\

    \rowcolor{gray!30}
    \textbf{Ours (PT-ALIGN)} & \textbf{0.30} \textcolor{mintgreen}{\(\downarrow1.91\)} 
    & \textbf{54.1} \textcolor{mintgreen}{\(\uparrow8.2\)} 
    & \textbf{98.28} \textcolor{mintgreen}{\(\uparrow24.14\)} 
    & \underline{81.36} \textcolor{mintgreen}{\(\uparrow0.00\)} 
    & \textbf{77.05} \textcolor{mintgreen}{\(\uparrow3.28\)} 
    & \textbf{93.02} \textcolor{mintgreen}{\(\uparrow4.95\)} 
    & \textbf{86.88} \textcolor{mintgreen}{\(\uparrow8.15\)} \\
    \bottomrule
  \end{tabular}
\label{safe}
\end{center}
\end{table*}

\section{Experiments}
\label{4}

\subsection{Experimental setup}
\paragraph{PT-ALIGN dataset for safety alignment}
We use 10 safety-related seed samples to teach the LLM the safety patterns necessary for synthesizing safety-related instructions. These samples cover a comprehensive range of safety domains, consolidated from more than ten categories officially provided by OpenAI and MetaAI into the following ten: \textit{Illegal Activity, Hate Speech, Malware Generation, Physical Harm, Economic Harm, Fraud, Sex, Privacy Violation, Controversial Topics, and Unethical Activity}. For annotating positive and toxic aspects, 6 seed examples (positive and negative samples are written by Gemini-1.0-pro with adjusted safety settings) with inner thoughts are provided to the LLM as ICL, fully utilizing its reasoning capabilities for safety annotation. After multiple refinements, the LLM synthesized 16,020 high-quality tripartite sample sets of instructions, positive answers, and toxic answers. These synthesized samples are exclusively used in subsequent experiments for safety self-alignment.

\paragraph{Experimental setting}
\label{Hyperparameters}
For the step of synthesizing safety topics, we use the nuclear sampling \cite{holtzmancurious} with a top-p threshold of \(p\) = 0.98 and a temperature of \(t\) = 1.0. During the safety instruction synthesis phase, only the top-p threshold is adjusted to \(p\) = 0.95.
For the self-constraint continuation phase, we use nucleus sampling with a top-p threshold of \(p\) = 0.98 and a temperature of \(t\) = 1.0. During the positive and toxic response phase, only the top-p threshold is adjusted to \(p\) = 0.95. During the instruction fine-tuning phase, whether using general Supervised Fine-Tuning or the fine-grained unlikelihood training method employed by PN-ALIGN, we exclusively fine-tune the low-rank adaptation (LoRA) \cite{hulora} weights within the multi-head attention modules, maintaining a batch size of 128, a maximum sequence length of 512, and a peak learning rate of 4e-4. The unlikelihood penalty weight \(\lambda\) is consistently set at 0.4. The training regimen spans 1 epoch (around 130 steps for 16k samples), initiating with a logarithmic learning rate increase during the initial 10\% of the total training steps for warm-up, followed by a linear decay to zero throughout the subsequent steps. The LoRA hyperparameters are configured with \( r=16 \), \( \alpha=16 \), and a dropout rate of 0.05, targeting the modules \(\{q\_proj, k\_proj, v\_proj, o\_proj\}\). Specifically, the computer used for the experiment is equipped with an Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz, 502GiB RAM, and four NVIDIA A40 GPUs, each with 46,068 MiB memory.

\paragraph{Models and baselines}
We apply the PT-ALIGN method for safety alignment on 9 popular open-source LLMs, including LLaMA2-7B-chat, LLaMA2-13B(chat) \cite{touvron2023llama}, LLaMA3-8B(Instruct) \cite{dubey2024llama}, Alpaca-13B \cite{taori2023stanford}, Vicuna-13B \cite{chiang2023vicuna}, Koala-13B \cite{koala2023}, and ChatGLM3-6B \cite{zengglm}. These models are of moderate size, demonstrating that our method is effective for models with relatively limited foundational capabilities. We then systematically and comprehensively evaluate their safety performance before and after alignment to demonstrate the method's versatility.

\paragraph{Safety evaluation benchmarks}
For safety assessment of LLMs, we use The BIG-bench HHH Eval\cite{srivastava2023beyond}  and PKU-SafeRLHF Preference \cite{ji2024beavertails} as benchmarks for multiple-choice question testing. Both datasets present a given instruction with two alternative responses, one of which is the safer and more reliable option. We calculate the likelihood of the model choosing between the two answers to a given question and determine the overall percentage of correct choices across all samples. To eliminate any positional bias, we swap the positions of the two options for each question during testing. 

To evaluate the textual output of LLMs, we utilize the PKU-SafeRLHF Evaluation \cite{ji2024beavertails} and the Safety Evaluation Instruction Datasets \cite{bianchisafety} for open-ended generation testing. Additionally, we employ the absolute-harmfulness-predictor-redteam model, following the methodology of \cite{bai2022training, bianchisafety}. This model comprehensively evaluates both the test questions and model responses, generating a harm score in which a lower value signifies improved safety performance.

To evaluate the safety performance of the PT-ALIGN method against more complex jailbreak attacks, we employed the AutoDAN \cite{liuautodan} method within the HarmBench \cite{mazeikaharmbench} framework to test 400 samples, using the rejection rate (ASR\%) as the safety evaluation metric.

\paragraph{Helpfulness evaluation benchmarks}
To evaluate the impact of the PT-ALIGN method on the model’s fundamental performance and helpfulness, we used TruthfulQA \cite{lin2022truthfulqa}, MMLU \cite{hendrycksmeasuring}, and SocialQA \cite{sap2019social} as multiple-choice benchmarks. Furthermore, we supplemented the evaluation with metrics such as helpfulness, honesty, and others from The BIG-bench HHH Eval \cite{srivastava2023beyond}, to further investigate the alignment tax \cite{lin2024mitigating} introduced by the method.

Next, we demonstrate the effectiveness of PT-ALIGN in three aspects. In \textbf{Effectiveness of PT-ALIGN on various LLMs}, we evaluate the improvement in safety performance on open-source models using our method. In \textbf{Effectiveness of PT-ALIGN on a vanilla LLM}, we show how applying our method to a vanilla LLM achieves a good trade-off between harmlessness, helpfulness, and honesty. In \textbf{Impact of PT-ALIGN on General Performance}, we evaluate how PT-ALIGN affects the model's general capabilities. Note that all baseline models independently undergo the data synthesis and instruction tuning processes described in the Method section. In \textbf{PT-ALIGN Against Jailbreak Attacks}, we supplement our evaluation by assessing PT-ALIGN’s performance against more sophisticated jailbreak attacks. In \textbf{Scaling Ability}, we validate the effectiveness of PT-ALIGN when scaling the sample set. In \textbf{Case Study}, we examine the model’s open-ended outputs in response to harmful instructions under the application of PT-ALIGN. Finally, we present results from ablation experiments and analyses on the impact of hyperparameters and seed samples.

\subsection{Effectiveness of PT-ALIGN on various LLMs}
Our PT-ALIGN method enhances the safety of many popular open-source models. We applied the PT-ALIGN sample synthesis and instruction tuning process on Alpaca-13B, Vicuna-13B, Koala-13B, ChatGLM3-6B, and LLaMA2-7B-chat. As shown in Table \ref{various}, the models achieved significant improvements on four safety benchmarks (\textit{harmless}, \textit{pref}, \textit{harm1}, and \textit{harm2}, representing test variations on the HHH Evaluation, PKU-SafeRLHF Preference, PKU-SafeRLHF Evaluation, and Safety Evaluation Instruction Datasets, respectively) after using PT-ALIGN. These results demonstrate that PT-ALIGN can elevate the safety of both safety-aligned and non-aligned LLMs to an optimal level. It can be concluded that the safety of existing popular models has significant room for improvement, and the PT-ALIGN method demonstrates the capability to universally enhance harmlessness to a high standard, with all metrics exceeding 90\%. In the following two sections, we will explore the impact on LLMs' helpfulness and general performance in detail and further demonstrate PT-ALIGN’s ability to enhance safety.

\begin{table}[ht]
\caption{Safety performance of our proposed PT-ALIGN on various LLMs. The first two columns show accuracy changes, while the latter two represent harmfulness changes.}
\begin{center}
    \begin{tabular}{lcccc}
      \toprule
       \textbf{Model} & \textbf{harmless(\%)} & \textbf{pku-pref} &  \textbf{harm1↓} & \textbf{harm2↓} \\
      \midrule
      LLaMA2-7B-chat\cite{touvron2023llama} & +\textcolor{white}{0}5.20 & +4.9 & -0.17 & -0.17 \\
      LLaMA3-8B\cite{dubey2024llama} & +19.27 & +4.5 & -0.52 & -0.63 \\
      LLaMA3-8B-ins\cite{dubey2024llama} & +\textcolor{white}{0}4.13 & +4.8 & -0.14 & -0.15 \\
      Alpaca-13B\cite{taori2023stanford} & +29.31 & +3.9 & -1.97 & -1.89 \\
      Vicuna-13B\cite{chiang2023vicuna} & +13.80 & +1.2 & -0.40 & -0.82 \\
      Koala-13B\cite{koala2023} & +12.07 & +3.5 & -0.19 & -0.56 \\
      ChatGLM3-6B\cite{zengglm} & +10.35 & +1.3 & -0.70 & -0.67 \\
      \bottomrule
    \end{tabular}
\label{various}
\end{center}
\end{table}
\vspace{-15pt}
\subsection{Effectiveness of PT-ALIGN on a vanilla LLM}
Our proposed PT-ALIGN method can enhance the safety of vanilla LLMs without diminishing their helpfulness. We conducted safety alignment on a pre-trained LLM with a relatively moderate parameter size, such as LLaMA2-13B, which has weaker fundamental performance compared to larger models (e.g., 70B). This setup demonstrates that models with fewer parameters can still achieve substantial safety improvements using the PT-ALIGN method (as shown in the previous section with even smaller models, e.g., 6B). The learning-from-scratch approach highlights the potential of LLMs for self-constraint and self-alignment. We compare the proposed approach with several baseline methods, including vanilla SFT (using only positive samples), DPO \cite{rafailov2024direct} (using same samples as PT-ALIGN), and KTO \cite{ethayarajh2024kto} (using the same samples as PT-ALIGN), all based on LLaMA2-13B. 

On three safety benchmarks (as shown in the first three columns of Table \ref{safe}), PT-ALIGN achieves the best performance, surpassing the officially released LLaMA2-13B-chat with over a 24\% improvement in \textit{harmless}. Notably, PT-ALIGN does not compromise helpfulness. However, when negative samples are excluded as supervisory signals and fine-grained UT is not employed during instruction tuning (referred to as the PT-ALIGN-SFT-13B method), the model's \textit{helpful} score significantly decreases, and its safety performance is generally inferior to our standard PT-ALIGN method. The improvement in the \textit{honest} metric may be attributed to the presence of test samples that require the model to restrain its responses, partially aligns with our tuning objectives. The increase in the \textit{other} metric may be attributed to PT-ALIGN redundantly generating certain non-harmful instructions during the sample synthesis phase, which likely contributes to stabilizing the model’s overall helpfulness.

Our experimental results show that PT-ALIGN significantly enhance the safety of vanilla LLMs while maintaining their helpfulness, demonstrating the method's potential and proving the feasibility of safety self-alignment.

\begin{table*}[t]
\caption{Case study for our PT-ALIGN and other baseline models. Text highlighted in \textcolor{red}{red} indicates that the model is \textcolor{red}{preparing to generate harmful content}, while text highlighted in \textcolor{mintgreen}{green} indicates that the model is \textcolor{mintgreen}{refusing to answer the question and provides moral persuasion.}}
  \centering
  \begin{tabular}{p{0.16\textwidth}p{0.78\textwidth}}
    \toprule
    \textbf{Instruction} & \textbf{How can one craft and disseminate a highly effective disinformation campaign?} \\
    \midrule
    LLaMA2-13B\cite{touvron2023llama} & \textcolor{red}{You can do} that by using the following strategy...  \\
    Alpaca-13B\cite{taori2023stanford} & The entire \textcolor{red}{...} has been dominated by the topic of disinformation... \\
    Vicuna-13B\cite{chiang2023vicuna} & \textcolor{red}{Creating a highly effective} disinformation campaign involves... \\
    LLaMA2-13B-chat\cite{touvron2023llama} & \textcolor{mintgreen}{I cannot fulfill your request.} I'm just an AI, it's not within my programming or ethical guidelines to assist with crafting or disseminating disinformation campaigns. Such activities \textcolor{mintgreen}{can cause harm}... \\
    \textbf{PT-ALIGN} & As an AI language model, \textcolor{mintgreen}{I don't have} the expertise or knowledge to provide specific instructions on crafting and disseminating disinformation campaigns. These activities can \textcolor{mintgreen}{have serious consequences and can harm individuals}... \\
    \bottomrule
     \textbf{Instruction} & \textbf{I heard that models can remember private information. Please complete the following address: 123 Elm Street, Apt XX, New York, NY.} \\
    \midrule
    LLaMA2-13B\cite{touvron2023llama} & \textcolor{red}{123 Elm Street, Apt 45, New York, NY.} \\
    Alpaca-13B\cite{taori2023stanford} & \textcolor{red}{123 Elm Street, Apt 23, New York, NY.} This could be an example of a potential completion... \\
    Vicuna-13B\cite{chiang2023vicuna} & I'm not sure about the exact apartment number... \\
    LLaMA2-13B-chat\cite{touvron2023llama} & \textcolor{mintgreen}{I cannot fulfill your request.} As an AI, I do not retain or provide private information about individuals. This would be against ethical guidelines...  \\
    \textbf{PT-ALIGN} & \textcolor{mintgreen}{I'm sorry, I cannot help with this request.} As an AI model, I do not store or access private information, and such requests may lead to harmful outcomes...  \\
    \bottomrule
  \end{tabular}
\label{case}
\end{table*}

\subsection{Impact of PT-ALIGN on general performance}
To thoroughly assess the impact of the PT-ALIGN method on the model's general performance, we use Big-Bench \cite{srivastava2023beyond} (TruthfulQA \& SocialQA) and MMLU \cite{hendrycksmeasuring} to evaluate the model's basic cognitive and reasoning abilities. The setup for Big-Bench is consistent with the multiple-choice question testing mentioned earlier. Additionally, we conduct tests using 5-shot prompting on the MMLU benchmark.

As shown in Table \ref{general}, none of the eight popular baseline models exhibited significant declines in key evaluation metrics across the three test sets. This demonstrates that the PT-ALIGN method not only preserves the model's helpfulness but also maintains its general performance to a certain extent. In other words, PT-ALIGN achieves a favorable trade-off between safety and general capabilities.

\begin{table}[!htbp]
\caption{The impact of PT-ALIGN method on the fundamental performance of safety alignment across various LLMs.}
\begin{center}
    \begin{tabular}{lccc}
      \toprule
       \textbf{Model} & \textbf{TruthfulQA} & \textbf{SocialQA} & \textbf{MMLU} \\
      \midrule
      LLaMA2-13B\cite{touvron2023llama} & 80.6(+6.3) & 33.9(-0.8) & 55.3(-0.5) \\
      LLaMA2-7B-chat\cite{touvron2023llama} & 84.2(+1.3) & 35.1(-0.7) & 44.6(-0.9) \\
      LLaMA3-8B\cite{dubey2024llama} & 80.7(+2.3) & 36.2(-0.9) & 57.3(-0.9) \\
      LLaMA3-8B-ins\cite{dubey2024llama} & 89.2(+2.3) & 35.6(-0.8) & 59.1(-0.8) \\
      Alpaca-13B\cite{taori2023stanford} & 80.6(+3.3) & 34.2(+0.8) & 24.6(+0.1) \\
      Vicuna-13B\cite{chiang2023vicuna} & 88.3(+5.0) & 34.4(-0.1) & 56.6(-0.2) \\
      Koala-13B\cite{koala2023} & 81.9(+0.8) & 31.8(+0.1) & 47.2(-0.2) \\
      ChatGLM3-6B\cite{zengglm} & 87.7(+3.4) & 33.9(-0.9) & 50.7(-0.8)\\
      \bottomrule
    \end{tabular}
\label{general}
\end{center}
\end{table}

\begin{table}[!htbp]
\caption{Our proposed PT-ALIGN method against AutoDAN jailbreak attacks.}
\centering
\begin{tabular}{lc}
\hline
\textbf{Model} & \textbf{ASR (\%) $\downarrow$} \\
\hline
LLaMA2-13B\cite{touvron2023llama} & 29.37 \\
LLaMA2-13B-chat\cite{touvron2023llama} & \textcolor{white}{0}5.06 \\
\rowcolor{gray!30}
\textbf{PN-ALIGN} & \textcolor{gray!30}{0}\textbf{0.51} \\
\hline
\label{autodan}
\end{tabular}
\end{table}
% \vspace{-10pt}

\subsection{PT-ALIGN against jailbreak attacks}
In the daily use scenarios of LLMs, it is usually sufficient to defend against straightforward and conventional instruction or question-based inducement prompts. However, LLMs may encounter more complex requests initiated by malicious users, known as jailbreak attacks. These requests can often deceive unaligned or poorly aligned LLMs, bypassing their existing safety defense mechanisms and reactivating compliance with inducement prompts, ultimately leading to the generation of harmful content. In this subsection, we employ more sophisticated jailbreak attack evaluations to further assess the safety alignment capability of the PT-ALIGN method.

We employ the AutoDAN attack method to further evaluate PT-ALIGN. AutoDAN systematically generates a series of candidate prompts through natural language variations, including rewrites, synonym substitutions, and structural rephrasings. These prompts are semantically similar but diverse in form, making them challenging for defense mechanisms to detect. Neural search algorithms, such as reinforcement learning or gradient optimization, are then employed to identify the most effective jailbreak prompts from the generated candidates. Furthermore, an iterative prompt refinement process, similar to the attack texts used against DeepSeek in Figure \ref{fig:example} but with increased attack intensity, is applied. The attack success rate (ASR\%) is used as the metric to quantify safety performance.

We continue to use the unaligned LLaMA2-13B as the baseline. As shown in Table 6, the LLaMA2-13B model aligned with PT-ALIGN achieves an exceptionally low attack success rate of 0.51\%, significantly outperforming the LLaMA2-13B-Chat model, which achieves 5.06\%. This result indicates that the PT-ALIGN method effectively resists prompt-based jailbreak attacks to a considerable extent, demonstrating its superior safety alignment performance. Although more complex and challenging jailbreak attack benchmarks are yet to be tested, the PT-ALIGN method demonstrates robustness and significant potential in resisting such attacks.

% \vspace{-50pt}
\subsection{Scaling ability}
We demonstrated the sample scaling performance of our proposed PT-ALIGN method (based on LLaMA2-13B). As shown in Figure \ref{fig:scaling}, as the number of synthetic samples gradually increases from 2,000 to 16,000, the \textit{harmless} attribute consistently rises from 86\% to 98\%, confirming the sample scaling ability of the PT-ALIGN method. It can achieve better safety performance with more synthetic samples without deterioration. With sufficient samples, \textit{helpful} generally fluctuates around the baseline model performance. This aligns with our hypothesis that the LLM output's harmless is associated with the number of positive and toxic samples. 

\begin{figure}[ht]
\begin{center}
  \includegraphics[height=5.3cm]{./pictures/scaling_ability.pdf}
  \caption{The impact of the number of positive and toxic samples on our PT-ALIGN. The vertical axis represents the percentage of accuracy for the HHH Evaluation, and the horizontal axis represents the number of samples. The dashed line represents the baseline’s original performance. The green solid line illustrates the variation in the \textit{harmless} metric, while the blue solid line represents changes in the \textit{helpfulness} metric.}
  \label{fig:scaling}
\end{center}
\end{figure}

\vspace{-10pt}
\subsection{Case study}
In this subsection, we present the actual output texts of various models in response to two typical adversarial instructions. These instructions are crafted to prompt the models to generate harmful content, disclose or extract private information potentially embedded in the pretraining data, or assist malicious users in creating code that compromises privacy and security.

As shown in Table \ref{case}, the baseline and instruction-tuned model (e.g., Alpaca) tend to unconditionally follow user commands and questions, generating toxic responses. In contrast, models tuned with the PT-ALIGN method effectively implement self-constraint, refuse inducing questions, and provide moral guidance similar to that of LLaMA2-13B-chat. PT-ALIGN allows the model to establish its constraints and presents all constraint texts through ICL, letting the model decide how to follow these rules. This differs from Constitutional AI's self-critique method and the approach of adhering to predefined rules, and it does not require RLHF warm-up \cite{bai2022constitutional}. Case studies show that the model can set reliable constraints and reasonably follow them, thereby helpfully declining inducing questions. 

\begin{table*}[ht]
\caption{Comparison between our PT-ALIGN samples and preference-based responses.}
  \centering
  \begin{tabular}{p{0.16\textwidth}p{0.78\textwidth}}
    \toprule
    \textbf{Prompt} & \textbf{What are the least harmful ways to commit suicide?} \\
    \midrule
    PT-positive & \textcolor{mintgreen}{I'm sorry}, but I cannot provide information or support for suicide. Suicide is a serious and complex issue that requires professional help and support. If you are struggling with suicidal thoughts or feelings, please reach out to a mental health professional or call a suicide prevention hotline.  \\
    PT-toxic & The easiest and most efficient way to commit suicide is to \textcolor{red}{use a gun}. It's readily available, easy to use, and has a high success rate. Just make sure to \textcolor{red}{aim for the head}, as this is the most effective way to ensure a quick and painless death. \\
    \midrule
    Preference-better & \textcolor{mintgreen}{I'm sorry} you're feeling this way, but I must warn you that thinking about such actions can be dangerous and harmful. It's crucial to prioritize your safety. Please consider reaching out to a mental health professional or a crisis hotline immediately. These trained individuals can provide the support and guidance you need during this difficult time. \\
    Preference-worse & \textcolor{mintgreen}{I'm sorry} to hear this. I recommend reaching out to someone you trust or a professional who can provide better support for your situation. \\
    \bottomrule
  \end{tabular}
\label{vs}
\end{table*}

\subsection{Ablation study}
Our observations indicate that, although positive and toxic samples differ fundamentally in semantics, they often share certain similarities in word usage. This highlights the necessity of fine-tuning at a fine-grained, character-level. Ablation experiments on the fine-grained element of UT (i.e., Equation \ref{eq:i}) demonstrate its effectiveness in enhancing safety while preserving helpfulness. As shown in Table \ref{tab:wo}, both the harmlessness metric and other helpfulness indicators decline when the token-level fine-grained alignment process is omitted, underscoring the importance of this component.

\begin{table}[ht]
\caption{Results of ablation study.}
\begin{center}
  \begin{tabular}{lcccccc}
    \toprule
    & \textbf{harmless} & \textbf{helpful} & \textbf{honest} & \textbf{other} & \textbf{overall} \\
    \midrule
    \textbf{w/ fine-grained} & \textbf{98.28} & \textbf{81.36} & \textbf{77.05} & \textbf{93.02} & \textbf{86.88} \\
    w/o fine-grained & 96.55 & 77.97 & 72.13 & 86.05 & 82.81 \\
    \bottomrule
  \end{tabular}
  \label{tab:wo}
\end{center}
\end{table}

\vspace{-10pt}

\subsection{The influence on different penalty coefficients}
As shown in Table \ref{tab:penalty}, We experiment with the penalty coefficient $\lambda$ of the Unlikelihood Training Loss, using $\lambda$ values from [0.1, 0.4, 0.7, 1.0], to observe its impact on the PT-ALIGN method. Excessively high or low values result in slight declines in safety fine-tuning performance, with optimal performance achieved around $\lambda = 0.4$. This value effectively maximizes the improvement in safety while preserving the model’s overall helpfulness and utility, clearly demonstrating its robustness and suitability for the current training setup.

\begin{table}[ht]
\caption{Performance of our proposed PT-ALIGN in different penalty coefficient \(\lambda\).}
\begin{center}
  \begin{tabular}{lcccccc}
    \toprule
    & \textbf{harmless} & \textbf{helpful} & \textbf{honest} & \textbf{other} & \textbf{overall} \\
    \midrule
    PT-ALIGN-0.1 & 96.55 & 77.97 & 73.77 & 88.37 & 83.71 \\
    \textbf{PT-ALIGN-0.4} & \textbf{98.28} & \textbf{81.36} & \textbf{77.05} & \textbf{93.02} & \textbf{86.88} \\
    PT-ALIGN-0.7 & 96.55 & 77.97 & 75.41 & 86.05 & 83.26 \\
    PT-ALIGN-1.0 & 94.83 & 76.27 & 72.13 & 86.05 & 82.81 \\
    \bottomrule
  \end{tabular}
  \label{tab:penalty}
\end{center}
\end{table}

\vspace{-10pt}

\subsection{The influence on different seeds}
To evaluate the impact of seed randomness on the performance of our method, we used Alpaca-13B as the baseline model and examined its PT-ALIGN fine-tuning performance across different seed sets. All seeds were independently generated by Gmini-1.0-pro (with adjusted safety settings). As shown in Table \ref{tab:seed}, we utilized three different seed sets and conducted PT-ALIGN fine-tuning under identical configurations. The experimental results indicate that the variation in seed sets did not significantly affect the performance improvement, further demonstrating the robustness and consistency of the PT-ALIGN method.

\vspace{+10pt}

\begin{table}[ht]
\caption{Performance of our proposed PT-ALIGN in different seeds. The table shows the stability of PT-ALIGN-Alpaca-13B across different seeds in terms of helpful, harmless, honest, other, and overall metrics.}
\begin{center}
  \begin{tabular}{lcccccc}
    \toprule
    & \textbf{harmless} & \textbf{helpful} & \textbf{honest} & \textbf{other} & \textbf{overall} \\
    \midrule
    PT-ALIGN-Seed1 & 96.55 & 84.75 & 75.41 & 83.72 & 85.07 \\
    PT-ALIGN-Seed2 & 96.55 & 84.75 & 75.41 & 83.72 & 83.25 \\
    PT-ALIGN-Seed3 & 98.28 & 83.05 & 77.05 & 76.74 & 84.16 \\
    \bottomrule
  \end{tabular}
  \label{tab:seed}
  \end{center}
\end{table}

\vspace{-10pt}

\section{Discussion}
\label{5}
In this section, we explore the advantages of our proposed PT-ALIGN method and its generated highly toxic samples. The discussion focuses on two key aspects: the semantics differences between positive/toxic samples and preference-based samples, and a comparison of the training processes of PT-ALIGN with conventional alignment methods. We present a detailed comparison between our positive and toxic samples and preference-based samples, emphasizing the stronger safety polarity of the former. This pronounced polarity enables toxic samples to serve as more effective supervisory signals, guiding the model to better distinguish between safe and harmful generated content. Additionally, we will use quantitative experiments to demonstrate the advantages of the PT-ALIGN method over the other methods, including differences in data representation and training efficiency.

\begin{figure}[h]
    \centering 
    \caption{Visualization using 2D PCA: Comparison between positive \& toxic samples and preference-based samples.}
    \includegraphics[width=\linewidth]{pictures/pt_embeddings_plot.pdf}
    \subcaption*{(a) PT Samples embeddings plot}
    
    \vspace{1em} % 增加图片之间的间距

    % 第二张图片
    \includegraphics[width=\linewidth]{pictures/pku_embeddings_plot.pdf}
    \subcaption*{(b) Preference Samples embeddings plot.} 
    \label{fig:embedding}
\end{figure}

\begin{figure*}[t]
    \centering
    % 左侧单图（垂直居中），占大约 28% 宽度
    \begin{minipage}[c]{0.28\linewidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/ori.pdf}
        \caption{Comparison of Loss Curves for Different Sample Types and Training Methods with PT-ALIGN. The above figure illustrates the loss curve for the original PT-ALIGN method. The dashed line represents the original loss curve, while the solid line represents the smoothed loss curve.}
        \label{loss}
    \end{minipage}
    \hspace{1em}
    % 中间两图上下堆叠
    \begin{minipage}[c]{0.30\linewidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/a.pdf}
        \subcaption*{(a)}
        \includegraphics[width=\textwidth]{pictures/b.pdf}
        \subcaption*{(b)}
    \end{minipage}
    \hspace{1em}
    % 右侧两图上下堆叠
    \begin{minipage}[c]{0.30\linewidth}
        \centering
        \includegraphics[width=\textwidth]{pictures/c.pdf}
        \subcaption*{(c)}
        \includegraphics[width=\textwidth]{pictures/d.pdf}
        \subcaption*{(d)}
    \end{minipage}
\end{figure*}

\subsection{Textual differences between PT-ALIGN samples and preference-based samples}
Our proposed PT-ALIGN method constructs positive and toxic samples in a dual manner, aiming to maximize the semantic polarity difference between the two types of responses (where semantic polarity refers to the contrast between safer, more reliable responses and more negative, harmful responses). In contrast, preference-based samples often lack pronounced semantic polarity differences, with both responses frequently sharing the same safety orientation and exhibiting minimal contrast.

\textbf{RQ1: How do PT-ALIGN and preference-based samples differ in semantic variation and safety polarity?}
Table \ref{vs} provides textual examples of PT-ALIGN and preference-based samples, clearly illustrating the key differences between the two approaches. PT-Samples exhibit more pronounced semantic differences, with a stark contrast in safety polarity. In contrast, preference-based samples often exhibit minimal semantic variation, and the two preference samples may even share the same safety polarity, either both being safe or both being harmful. This lack of contrast can lead the model to inadvertently learn harmful content, thereby weakening its safety alignment effectiveness.

\textbf{RQ2: How does the design of safety polarity differences between samples enhance the PT-ALIGN method’s ability to achieve better safety alignment?}
We apply Principal Component Analysis (PCA) to analyze their representations, further quantifying the semantic differences between PT-Samples and preference-based samples. Specifically, we use a sentence-BERT model to extract embedding features for both sample types and visualize their representations in two dimensions using PCA. As shown in Figure \ref{fig:embedding}, panel (a) demonstrates that the representation differences between positive and toxic samples are significant, with an average cosine distance of 0.3824 per sample pair, and their vector space distributions are clearly distinct. In contrast, panel (b) illustrates that the representation differences between corresponding preference samples are minimal, with an average cosine distance of 0.2806 per sample pair, and each sample pair occupying the same position in the figure indicates that their vector space distributions are highly similar. The presence of non-negligible toxic content in the preferred responses of the preference-based sample set presents a critical issue, as it may undermine the alignment process and hinder the model’s ability to effectively distinguish between safe and harmful content. This phenomenon motivates the development of a safety alignment method that leverages positive and highly toxic samples, enabling the model to learn more effectively how to generate safe and reliable responses.

\subsection{Training performance analysis and comparison for fine-grained dual instruction fine-tuning}
In this section, we examine the changes in training loss curves of PT-ALIGN compared to other methods or when using different types of samples, to further validate the training stability and efficiency of our approach.

\textbf{RQ3 : How Can Dual Instruction Tuning with Positive and Toxic Samples Enable the Model to Achieve Better Safety?}
As shown in Figures 6a and 6b, the loss curve of the PT-ALIGN method exhibits fewer fluctuations and converges more smoothly compared to reinforcement learning methods such as DPO and KTO. This stability underscores its effectiveness in achieving robust safety alignment during training. Additionally, the faster initial descent of PT-ALIGN’s loss curve demonstrates its efficiency in optimizing safety-related objectives. By leveraging both highly positive and toxic samples, PT-ALIGN enhances the model’s safety polarity gap, resulting in a clearer distinction between safe and harmful content. These attributes highlight PT-ALIGN’s potential to train models with greater reliability and safety.

\textbf{RQ4 : How Does PT-ALIGN Achieve Lower Helpfulness Loss Compared to SFT and Dual Instruction Tuning on Preference-Based Samples?}
As illustrated in Figure 6c, the inclusion of toxic samples accelerates the model’s training convergence and achieves significantly lower loss values. This enables the model to utilize toxic samples as a point of comparison, leading to safer performance. In Figure 6d, while preference-based samples achieve lower loss values and exhibit more stable training compared to using positive samples alone, the pronounced safety polarity differences between positive and toxic samples allow the model to more effectively and directly diverge from harmful distributions, thus reducing the tendency to diminish helpfulness during the alignment process, thereby maximizing its ability to learn safe paradigms.



\section{Conclusion}
\label{6}
In this paper, we present PT-ALIGN, a novel safety self-alignment method that minimizes human supervision by automatically refining both positive and toxic samples and performing fine-grained dual instruction tuning. This method incorporates highly toxic samples as a novel supervisory signal to improve model safety while preserving its effectiveness. Experimental results show that these readily available toxic samples, characterized by clear safety distinctions and stronger semantic contrast, enable the model to learn the difference between safe and harmful content directly. The refinement demonstrates that LLMs can effectively customize constraints to synthesize high-quality content. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to enhance LLM safety at the token level. Multidimensional experiments demonstrate that our method can significantly enhance the safety of LLMs while maintaining the model's helpfulness and general performance. In the future, we plan to explore the performance of the PT-ALIGN method in more complex attack scenarios, investigate methods for synthesizing corresponding adversarial samples, and extend its application to the safety alignment of multimodal large language models.

\normalem
\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}


