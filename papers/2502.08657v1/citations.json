[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhao2023survey",
        "author": "Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others",
        "title": "A survey of large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "bommasani2021opportunities",
        "author": "Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others",
        "title": "On the opportunities and risks of foundation models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bianchisafety",
        "author": "Bianchi, Federico and Suzgun, Mirac and Attanasio, Giuseppe and Rottger, Paul and Jurafsky, Dan and Hashimoto, Tatsunori and Zou, James",
        "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions"
      },
      {
        "key": "qi2024fine",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2024fine",
        "author": "Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh",
        "title": "Fine-grained human feedback gives better rewards for language model training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      },
      {
        "key": "kopf2024openassistant",
        "author": "K{\\\"o}pf, Andreas and Kilcher, Yannic and von R{\\\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\\'a}rd and others",
        "title": "Openassistant conversations-democratizing large language model alignment"
      },
      {
        "key": "wan2023poisoning",
        "author": "Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan",
        "title": "Poisoning language models during instruction tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "aky\u00fcrek2023rl4f",
        "author": "Afra Feyza Aky\u00fcrek and Ekin Aky\u00fcrek and Aman Madaan and Ashwin Kalyan and Peter Clark and Derry Wijaya and Niket Tandon",
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sun2024principle",
        "author": "Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang",
        "title": "Principle-driven self-alignment of language models from scratch with minimal human supervision"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chengaining",
        "author": "Chen, Kai and Wang, Chunwei and Yang, Kuo and Han, Jianhua and Lanqing, HONG and Mi, Fei and Xu, Hang and Liu, Zhengying and Huang, Wenyong and Li, Zhenguo and others",
        "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "nozawa2021understanding",
        "author": "Nozawa, Kento and Sato, Issei",
        "title": "Understanding negative samples in instance discriminative self-supervised representation learning"
      },
      {
        "key": "wang2024learning",
        "author": "Wang, R and Li, H and Han, X and others",
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "tian2020makes",
        "author": "Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip",
        "title": "What makes for good views for contrastive learning?"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "welleck2020neural",
        "author": "Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason",
        "title": "NEURAL TEXT DEGENERATION WITH UNLIKELIHOOD TRAINING"
      }
    ]
  }
]