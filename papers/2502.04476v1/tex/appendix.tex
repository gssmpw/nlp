\section{Ethics statement} \label{appendix: ethics} \vspace{-0.1in}
This paper adheres to the ethical guidelines set forth by the ICLR conference and aims to contribute to the research community in a responsible and transparent manner. The study utilizes publicly available datasets, namely AudioCaps (\cite{audiocaps}) and Clotho (\cite{clotho}), which have been used in accordance with their respective licenses and ethical use guidelines. No personally identifiable information (PII) or sensitive data was used in this research, ensuring the privacy and confidentiality of all subjects involved.

The research focuses on developing methods for explaining audio differences using natural language, and all experiments were conducted with the intent to advance scientific knowledge in a positive and constructive way. We acknowledge the potential implications of our work in fields such as audio forensics and surveillance, and we emphasize that our models are intended solely for academic and benign applications. We do not condone or support the misuse of our work in ways that could infringe upon individual privacy or be used for unethical purposes. All human evaluators and annotators participated voluntarily with their informed consent. There were no conflicts of interest or biases influencing the results of this study. Finally, we are committed to open science and will release our datasets and model checkpoints, to the research community to promote transparency, reproducibility, and collaboration. 

\section{Reproducibility Statement} \label{appendix: reproduce} \vspace{-0.1in}
To ensure the reproducibility of our work, we have taken several measures, which are thoroughly documented throughout the main paper, appendix, and supplemental materials. For our proposed model, ADIFF, and the associated benchmarks, we have provided a downloadable source code anonymously in the supplementary materials to facilitate replication of our experiments. Detailed explanations of our experimental setup, hyperparameters, and model architecture can be found in Section \ref{sec: adiff} and Appendix \ref{appendix: exp setup}. We have outlined the data processing steps, including the creation of the AudioCaps Difference (ACD) and Clotho Difference (CLD) datasets, in Section \ref{sec: audio difference dataset} and the Appendix \ref{appendix: llm gen}, ensuring transparency in data preparation and usage. All datasets used in this work are publicly available, and we have provided links to these resources along with data processing scripts in the supplementary materials. Our ablation studies in Section \ref{sec: ablation overview} further validate the robustness of our model design, and comprehensive results are presented to demonstrate consistency and reliability of the findings.

\section{Related work} \vspace{-0.1in}
\noindent \textbf{Audio-text learning.} Audio-text learning focuses on developing multi-modal representations to address specific tasks. These tasks include text-to-audio retrieval (\cite{deshmukh23_interspeech}), audio captioning (\cite{clotho,mei2022automated}), and text-to-audio generation (\cite{liu2023audioldm}). Among these, audio captioning is particularly relevant for the task of audio difference explanation, as it involves generating natural language descriptions from audio recordings. The architecture typically comprises an encoder to learn audio representations and a decoder to generate text tokens (\cite{drossos2017automated}). Research in this area has explored enhancing diversity (\cite{morato2021diversity, mei2024towards, mei2022diverse}) in outputs, improving audio encoder and training methods (\cite{wu2023beats}), and leveraging LLMs as decoder (\cite{llmcap}).

\noindent \textbf{Audio-Language Models.} Instead of building task-specific models, recent literature has focused on building general-purpose audio models by using language. These models are referred to as Audio-Language Models. For example, CLAP (\cite{msclap1,msclap2,laionclap}) and AudioFlamingo (\cite{audioflamingo}) is trained on millions of audio-text pairs to learn multimodal audio-text representations, and are used for tasks like zero-shot and few-shot classification and retrieval. To address open-ended tasks such as audio question answering and audio captioning, a decoder or LLM is added to the architecture. For example, Pengi (\cite{mspengi}) employed prefix-tuning to learn a mapper and prompt a frozen language model. The later Audio-Language Models like LTU (\cite{ltu,ltuas}), SALMONN (\cite{salmonn}), GAMA (\cite{ghosh2024gama}) utilized LoRA (\cite{hulora}) to develop lightweight adapters on language models. While most Audio Language Models (ALMs) concentrate on non-speech audio, Qwen-audio and LTU-AS integrate both speech and audio understanding by utilizing the Whisper encoder (\cite{radford2023robust}).

\noindent \textbf{Difference Captioning.} In the vision domain, the task of explaining the difference between two images is studied under Image Difference Captioning (IDC) (\cite{jhamtani-berg-kirkpatrick-2018-learning,Park_2019_ICCV}). The architectures explored include encoder-decoder (\cite{jhamtani-berg-kirkpatrick-2018-learning}), attention-based (\cite{Park_2019_ICCV}), clip-based decoder (\cite{guo2022clip4idc}), and cross-modal transformer (\cite{yao}). In the audio literature, the first relevant work \cite{adl} explored using difference learning to improve audio captioning. The data consists of a mixture with audio 1 and audio 2, and then the model predicts the caption for audio 1, indirectly highlighting the difference. This approach differs from directly comparing two audios and highlighting differences; instead, it focuses on captioning one of the audios. The second work (\cite{Takeuchi2023}) focuses on audio difference captioning, aiming to find the semantic difference between similar but slightly different audio clips. The difference captioning takes the form of ``change," ``remove," or ``add", for example, ``change sound of footsteps." The task is concerned with adding or removing something from audio 1, instead of explaining how the audios differ concerning audio events, sound sources, acoustics, and emotional response. Therefore, in the audio literature, the task of audio difference explanation with different tiers of explanation is yet to be explored.

\section{Comparative reasoning for audio}  \vspace{-0.1in}
Comparative reasoning (\cite{yu2023pre}) for audio involves analyzing and contrasting different audio sources, events, and signals to draw meaningful conclusions. This process typically starts with identifying the audio sources, which could range from live recordings, synthesized sounds, or pre-recorded tracks. Each source has unique characteristics such as frequency range, amplitude, and timbre. Audio events, which are specific occurrences within the audio signal like a note being played or a word being spoken, are then identified and analyzed. Temporal characteristics, including the timing, duration, and sequence of these events, play a crucial role in comparative reasoning. By examining these elements, one can discern patterns, anomalies, and relationships within the audio data. The signal processing aspect of comparative reasoning involves breaking down the audio signal into its fundamental components. For example, analysis of the spectral content, Signal-to-noise ratio (SNR), harmonic distortion, and other metrics can be used to evaluate the quality and integrity of the audio signal. By comparing these metrics across different audio sources or events, one can make informed judgments about their similarities and differences. Moreover, Comparative reasoning in audio is closely related to deductive reasoning (\cite{johnson1999deductive}), where conclusions are drawn based on the analysis of specific data points. For instance, if two audio signals share similar temporal and spectral characteristics, one might deduce that they originate from the same source or have been processed similarly. Conversely, significant differences in these characteristics could indicate different sources or processing methods. By systematically comparing and contrasting audio data, one can make logical inferences and draw conclusions that are supported by empirical evidence. Therefore, enabling ALMs to comparative reasoning improves their overall logical reasoning ability. 

\begin{figure*}[ht]
   \centering
     \includegraphics[width=1\textwidth]{figs/datagen.png}
     \caption{\small Using LLM to generate audio difference explanation. The process involves three key steps: sampling from human-annotated data sources, generating explanations, and verifying the generated explanations. The verification step is conducted only for the test set.
     % \vspace{-0.2in}
     }
     \label{fig:datagen}
\end{figure*}

\section{Using LLMs to Generate Audio Difference Explanation} \label{appendix: llm gen}  \vspace{-0.1in}

Large Language Models (LLMs) have been effectively utilized to generate text descriptions for audio across various tasks, such as question-answering, compositional reasoning, and deductive reasoning. These models leverage vast amounts of data and sophisticated algorithms to understand and generate human-like text. We adopt a similar approach for generating explanations for audio difference tasks, which involve identifying and describing differences between two audio recordings. The process involves three main steps: data sources, explanation generation, and explanation verification.

\textbf{Data Sources:} For our study, we limit the audio recordings and human-annotated descriptions to those from two audio captioning datasets: AudioCaps and ClothoV21. AudioCaps is a large-scale dataset that provides audio recordings along with detailed human-annotated descriptions. ClothoV21 is another comprehensive dataset that includes a variety of audio recordings with corresponding textual descriptions. By using these datasets, we ensure that our audio samples and descriptions are diverse and representative of real-world scenarios.

\textbf{Explanation Generation:} To generate explanations, we prompt an LLM to describe the differences between two audio recordings using the provided human-annotated descriptions. To select captions for prompting LLM, we perform random sampling per audio to eliminate the edge case of multiple captions. This process involves three steps. First, we flatten the .csv files to include one audio file and a single caption. For Clotho, which has 5 captions, this results in 5 entries per audio. Second, we conduct sampling. For instance, if the dataset contains N audios, we loop through the N audios and, for any $i^{\text{th}}$ audio, randomly pick a second audio ($j^{\text{th}}$) not including the i to i+4th audio. This is achieved by constructing an index list that excludes the i, i+1, ..., i+4 indices and sampling from this list. Third, the selected $i^{\text{th}}$ and $j^{\text{th}}$ audios are then used to look up the captions for the $i^{\text{th}}$ and randomly picked $j^{\text{th}}$ audio. These captions then become the seed for the LLM to generate the difference explanation. Our LLM prompting setup is similar to the one used in previous studies \cite{audioentail}, with two key modifications. First, we instruct the LLM to incorporate knowledge about sound sources, materials, acoustics, and emotions when generating explanations. This ensures that the explanations are not only accurate but also rich in context and detail. Second, we define the tier of explanation by restricting the sources the LLM can use and the length of the explanation. This tiered approach allows us to generate explanations of varying complexity and detail. The base prompt is detailed in Table \ref{table: prompt}.

\textbf{Explanation Verification}: Finally, human annotators verify the difference explanations generated by the LLM. If an explanation is found to be inaccurate, the human annotators manually remove the hallucinated audio event and add necessary details to correct it. Due to the cost and effort involved in this process, we restrict verification to the test set of three tiers. This ensures that our testing dataset is both accurate and reliable. The resulting dataset comprises audio recordings paired with three tiers of generated explanations.


% Large Language Models (LLMs) have shown impressive reasoning abilities, especially when scaled up. In our research, we use Llama3 to create three levels of audio difference explanations. To create dataset for Audio Difference Explanation, we tested various prompting methods and identified three effective strategies for generating hypotheses based on audio-related descriptions: (1) Prompting the LLM to apply its understanding of audio, acoustics, and psychoacoustics in forming hypotheses. (2) Using examples in the prompts to enhance the quality of difference explanations. The base prompt is detailed in Table \ref{table: prompt}.

\begin{table*}[t]
\centering
\scriptsize
\label{tab:open_res}
\setlength\tabcolsep{1pt}
% \fontsize{7.7}{3}\selectfont
\begin{tabular}{p{13cm}} \\ \toprule  
\textbf{Prompt for LLM} \\ \midrule
You are a helpful assistant with expert knowledge about audio, acoustics, and psychoacoustics. You study audio, which is the study of sound and its properties. You study acoustics, which revolve around the generation, propagation, and reception of sound waves. You study Psychology which posits that a sound is a complex stimulus that encompasses a vast range of acoustic properties involving aspects of cognition, psychoacoustics, and psychomechanics. Your task is to perform audio captioning which consists of describing audio content using natural language. To describe the acoustic content, you utilize words related to their acoustic properties, such as their semantic relations, their spectro-temporal characteristics, frequency, loudness, duration, materials, interactions, and sound sources.\\ \bottomrule 
\end{tabular}
\caption{\small Base prompt for LLM before generating responses} \label{table: prompt}
\end{table*}

\section{Analysis of Generated Description} \label{appendix: reviewer analysis}
The three tiers of audio difference explanation are explained in Section \ref{sec: audio difference task tiers}, while the dataset creation process is explained in Section \ref{sec: audio difference dataset} and Section \ref{appendix: llm gen}. In creating the dataset, we rely on LLM to generate the base difference explanation before it is verified for human annotators for the test-set. Therefore, it's important to explore the types of audio differences described in different tiers to uncover insights and variations in the three tiers of difference explanations. For instance, by splitting text into words, converting them to lowercase, and focusing on words related to audio, we can identify words that appear in Tier 2 but not in Tier 1, showing unique audio characteristics used for the difference explanation in Tier 2. Similarly, we can use the same method to examine the distinctions between Tier 2 and Tier 3, where the differences often lie in more detailed and immersive sound characteristic descriptions. The results, presented in Figure \ref{fig:datagen3}, highlight two main points. First, the unique words between Tier 3 and Tier 2 are significantly more than those between Tier 2 and Tier 1. This is expected, as Tier 3 explanations add considerably more detail than Tier 2. Second, the left plot of Figure \ref{fig:datagen3} shows that the additional characteristics mentioned in Tier 2 over Tier 1 focus on audio events, sound sources, and the nature of the sounds, making it more analytical. In contrast, the right plot of Figure \ref{fig:datagen3} illustrates that the additional characteristics in Tier 3 over Tier 2 focus on the sonic qualities of each audio, including potential sources, listener’s experience, comparing the two audios in terms of the complexity and engagement, and therefore providing a more comprehensive difference description. Overall, we see the audio characteristics used in Tier 3 over Tier 2 (Figure \ref{fig:datagen3}, right subplot)  and Tier 2 over Tier 1 (Figure \ref{fig:datagen3}, left subplot) are different, and focus on the different aspect and details between two audios. We further analyze the density of information across three Tiers in Appendix Section \ref{appendix: information density}. 

\begin{figure*}
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figs/tier2.png}
     \label{fig:datagen1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figs/tier3.png}
     \label{fig:datagen2}
    \end{minipage}
    \vspace{-0.2in}
    \caption{\small Comparison of vocabulary differences between explanatory tiers: The left figure illustrates words that are present in the Tier 2 explanation but not in the Tier 1 explanation, while the right figure highlights words unique to the Tier 3 explanation that are absent in the Tier 2 explanation.\label{fig:datagen3}}
\end{figure*}

\section{Experimental setup} \label{appendix: exp setup} \vspace{-0.1in}
\textbf{Encoder and Decoder.}  We utilized the HTSAT (\cite{chen2022hts}) audio transformer, trained on AudioSet (\cite{audioset}), as our audio encoder. The audio was sampled at 32 kHz and converted into log Mel spectrograms with 64 Mel bins, a hop size of 320 ms, and a window size of 1024 ms. For HTSAT, all audio files were randomly truncated to 10 seconds in length. To maintain computational efficiency, the maximum length of user text input was set to 40. The audio encoder remained frozen throughout the experiments, enabling hallucination output using audio event detection probabilities. For the decoder or Language model, we use the GPT2 series of models, specifically the GPT2-base (124M) variant. The model remained unchanged throughout all the experiments except for Section (\ref{subsec: scaling of language model}).

\textbf{Projection.} The audio projection uses an 8-layer transformer with a prefix length of 40. When combining two audio inputs, the total prefix length becomes 80. Adding a separator token (1 token) and a text projection (40 tokens) results in a total prefix length of 121. This combined input is then processed by the cross-projection layer, a 4-layer transformer, which maintains the prefix length of 121 in its output.

\textbf{Training.}  We use Adam Optimiser (\cite{adam}) with warmup and step decay of the learning rate. For stage 2, multimodal grounding, all models are trained for 30 epochs on 8 A100 GPUs. Only for Section (\ref{subsec: scaling of language model}), we retrain the models for 50 epochs. In the final stage of fine-tuning, we limit the training to 10 epochs to prevent the training loss from plateauing and to avoid catastrophic forgetting.

\textbf{Inference.} We use top-k and top-p decoding because of its better performance in producing diverse explanations that accurately capture details. We explore different decoding strategies and compare the performance of top-k and top-p decoding against the greedy decoding in Section \ref{appendix: diversity}.

\section{User text prompt} \vspace{-0.1in}
Audio-based prefix-tuning models have been utilized for various tasks (\cite{noaudiocap,selm}), including as supervision to learn general-purpose audio representations (\cite{mspengi}). The dataset for training these models consists of (audio, user input, output) triplets. During training, the user input remains fixed for a specific task and only varies across tasks. Consequently, the model performs the necessary task only when the exact text prompt or user input is provided. This limitation restricts the model’s applicability. To address this, we explore supporting multiple input text prompts by randomly sampling prompts during training.

For the audio difference task, the ADIFF model must produce three tiers of explanation based on the user prompt. Additionally, it needs to generate audio captions for the first or second audio to enhance grounding (\ref{subsec: improving modality alignment with training}). We aim for the user to be able to request different levels of explanation using flexible prompting, and the model should still provide accurate outputs. To achieve this, we create a database of user prompts for each tier. During training, depending on the required output tier, we sample a specific prompt from the database. In our case, the database is a simple dictionary containing about 100 sample user inputs per output tier. Randomly sampled examples of the user inputs are shown in Table \ref{tab:user prompts}.

\begin{table}[!ht]
\centering
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Tier} & \textbf{Randomly sampled user prompts}\\ \midrule
\multirow{3}{*}{1 } & ``Summarize the differences between the two audios briefly.”, \\& “Highlight the main differences between the two audio tracks.”, \\& “Provide a concise comparison of the two audios.” \\ \midrule
\multirow{3}{*}{2} & ``Explain the contrast between the two audio pieces in one long sentence.”, \\& “In one elaborate sentence, summarize the differences between both audio files.”, \\& “Describe the variation between the two audio tracks in one extended sentence.” \\ \midrule
\multirow{3}{*}{3} & ``Explain the difference between both audios in detail.",\\& ``Could you elaborate on the distinctions between the two audio tracks?",\\& ``Please provide a detailed comparison of both audio files.” \\
\bottomrule
\end{tabular}}
\caption{\small Randomly sampled three user prompts for each tier of audio difference explanation. }
\label{tab:user prompts}
\end{table}

\section{Evaluation} \vspace{-0.1in}
\subsection{Objective evaluation} \label{appendix: evaluation}  
 \vspace{-0.1in}
For objective evaluation, we leverage audio captioning metrics: BLEU, METEOR, SPICE, CIDEr, SPIDEr. The BLEU (Bilingual Evaluation Understudy) metric measures the precision of n-grams between the generated text and reference text, making it simple and fast, though it doesn't account for recall and can be insensitive to word order and meaning. METEOR (Metric for Evaluation of Translation with Explicit ORdering) combines precision and recall, incorporating features like stemming and synonymy matching, which results in better correlation with human judgment at the sentence level, albeit with increased complexity and computational demands. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) focuses on the recall of n-grams and longest common subsequences, making it popular for text summarization, though its recall-centric nature might not always reflect overall quality. CIDEr (Consensus-based Image Description Evaluation) measures consensus between generated image descriptions and multiple reference descriptions, excelling in image captioning but requiring multiple reference captions for accuracy. SPICE (Semantic Propositional Image Caption Evaluation) evaluates the semantic content of image captions by comparing scene graphs, capturing semantic meaning more effectively than surface-level metrics, though it is computationally intensive. Lastly, SPIDEr (SPICE and CIDEr) combines SPICE and CIDEr to leverage both semantic and consensus-based evaluation, providing a balanced assessment but inheriting the computational complexity of both metrics. Therefore, we primarily use SPIDEr to compare models or calculate the average score across all the above metrics for model comparison.

\subsection{Subjective evaluation} \label{appendix: human evaluation}
\vspace{-0.1in}
While objective metrics are correlated with human evaluation, they have their limitations. Therefore, we also set up a human evaluation task to assess the difference explanations generated by the model. In this task, evaluators are asked to evaluate the differences between two audio samples. These differences could pertain to various aspects such as audio events (e.g., specific sounds or occurrences within the audio), acoustic scenes (e.g., the overall environment or context of the audio), signal characteristics (e.g., frequency, amplitude, timbre), and listener emotions (e.g., the emotional impact or response elicited by the audio). Evaluators then provide scores based on three dimensions: Correctness, Granularity, and Readability. These dimensions ensure that the evaluations are comprehensive, focusing on the accuracy, detail, clarity, and acoustic quality of the explanations provided. The evaluation dimensions and the corresponding score descriptions are: \\
\textbf{Correctness (1-5)} \vspace{-0.1in}
\begin{itemize}
    \item 1: The explanation is mostly incorrect, with significant inaccuracies in identifying audio events, acoustic scenes, or signal characteristics.
    \item 2: The explanation has several inaccuracies but captures some correct audio events and acoustic scenes.
    \item 3: The explanation is partially correct, with a mix of accurate and inaccurate details.
    \item 4: The explanation is mostly correct, with minor inaccuracies.
    \item 5: The explanation is entirely correct, accurately identifying all relevant audio events, acoustic scenes, and signal characteristics.
\end{itemize} \vspace{-0.1in}
\textbf{Granularity (1-5)}  \vspace{-0.1in}
\begin{itemize}
    \item 1: The explanation is very vague, lacking detail and specificity.
    \item 2: The explanation provides some detail but is generally too broad.
    \item 3: The explanation is moderately detailed, covering key aspects but missing finer points.
    \item 4: The explanation is detailed, covering most aspects with good specificity.
    \item 5: The explanation is highly detailed, thoroughly covering all relevant aspects with precise specificity.
\end{itemize} \vspace{-0.1in}
\textbf{Readability (1-5)}  \vspace{-0.1in}
\begin{itemize}
    \item 1: The explanation is very difficult to read, with poor structure and clarity.
    \item 2: The explanation is somewhat difficult to read, with several issues in structure and clarity.
    \item 3: The explanation is moderately readable, with some issues in structure and clarity.
    \item 4: The explanation is mostly readable, with minor issues in structure and clarity.
    \item 5: The explanation is very easy to read, with excellent structure and clarity.
\end{itemize}

We recruited five professionals to evaluate the generated audio difference explanations. Each annotator was asked to use headphones to listen to both audio samples. After listening and analyzing the two audios, annotators provided scores based on three criteria: correctness, granularity, and readability. Each criterion had a maximum score of 5. The exact task guidelines are shown in Table \ref{tab:instruction_ttm}, an individual instance of the task is shown in Table \ref{tab:questions}, and the results of the human evaluation are presented in Table \ref{table:human_evaluation_study}.

\begin{table}[ht!]
\centering
\small 
\begin{tabular}{p{0.9\linewidth}}
\toprule\toprule
\textbf{Task Instructions} \\
Your task is to rate the difference explanation between the two audios. First, listen to each audio clip and analyze the difference between the two audios. Pay attention to differences in terms of audio events (dog barking vs cat meowing), sound sources (musical instrument, human voice), acoustic scene (park vs living room), signal characteristics (frequency, pitch, loudness, etc) emotion response (listening to the audio invokes a sense of happiness, fear, in you) and any other difference using world-knowledge or using deductive reasoning. Once you have analyzed the clip, asses the below audio difference explanations provided. \\ \\

\textbf{Important Notice: } \\
Please note that certain sections of this audio test may include unexpectedly loud sounds. To ensure a comfortable experience, we suggest setting your volume to a suitable level before starting and adjusting it as necessary throughout the test. \\
\bottomrule \bottomrule
\end{tabular}
\caption{\small Task guidelines provided to annotators.\label{tab:instruction_ttm}
}
\end{table} 

\begin{table}[ht!]
\centering
\small 
\begin{tabular}{p{0.9\linewidth}}
\toprule\toprule
\textbf{Please read the task guidelines before completing the task. First, listen to the two audio recordings carefully and analyze the difference between them. Second, read the descriptions which explains the difference between the two audios. Finally, provide your rating of the difference explanation across three dimensions of correctness, granularity and readability} 
\\ \\

\textbf{How accurate is the audio difference explanation?}
\\
\textbf{1 (Poor)} The explanation is mostly incorrect, with significant inaccuracies in identifying audio events, acoustic scenes, or signal characteristics.\\
\textbf{2 (Fair)} The explanation has several inaccuracies but captures some correct audio events and acoustic scenes.\\
\textbf{3 (Good)} The explanation is partially correct, with a mix of accurate and inaccurate details.\\
\textbf{4 (Very Good)} The explanation is mostly correct, with minor inaccuracies.\\
\textbf{5 (Excellent)} The explanation is entirely correct, accurately identifying all relevant audio events, acoustic scenes, and signal characteristics.
\\
\\

\textbf{How detailed and granular is the audio difference explanation?}\\
\textbf{1 (Poor)} The explanation is very vague, lacking detail and specificity.\\
\textbf{2 (Fair)}  The explanation provides some detail but is generally too broad.\\
\textbf{3 (Good)}  The explanation is moderately detailed, covering key aspects but missing finer points.\\
\textbf{4 (Very Good)} The explanation is detailed, covering most aspects with good specificity.\\
\textbf{5 (Excellent)} The explanation is highly detailed, thoroughly covering all relevant aspects with precise specificity.
\\
\\
  
\textbf{How easy to read and understand is the audio difference explanation?}\\
\textbf{1 (Poor)} The explanation is very difficult to read, with poor structure and clarity.\\
\textbf{2 (Fair)} The explanation is somewhat difficult to read, with several issues in structure and clarity.\\
\textbf{3 (Good)} The explanation is moderately readable, with some issues in structure and clarity.\\
\textbf{4 (Very Good)} The explanation is mostly readable, with minor issues in structure and clarity.\\
\textbf{5 (Excellent)} The explanation is very easy to read, with excellent structure and clarity.
\\
\\

\bottomrule \bottomrule
\end{tabular}
\caption{\small Individual labeling instance for audio difference explanation task. \label{tab:questions}
}
\end{table}

\begin{figure*}[!ht]
   \centering
     \includegraphics[width=1.0\textwidth]{figs/ablation.png}
     \caption{\small The plots for different ablation studies are organized as follows: The (1,1) subplot represents the naive baseline with random audio encoder weights, while the (1,2) subplot shows the naive baseline with HTSAT pretrained audio encoder weights. The (1,3) subplot features ADIFF with cross-projection and GPT2-base. Moving to the second row, the (2,1) subplot illustrates ADIFF with the GPT2-med language model, the (2,2) subplot displays ADIFF with the GPT2-large language model, and the (2,3) subplot presents ADIFF with GPT2-XL. In the third row, the (3,1) subplot replicates the ADIFF result from subplot (1,3), the (3,2) subplot shows ADIFF with audio position training, and the (3,3) subplot combines ADIFF with audio position training and a finetuning stage.
     % \vspace{-0.2in}
     }
     \label{fig:subplots}
\end{figure*}

\section{Cross-projection} \vspace{-0.1in} \label{appendix: cross-projection}
We introduce a cross-projection layer with a latent separator token between two audio inputs to address two key challenges. First, it enhances the model's ability to differentiate between perceptually similar sounds and identical audio events. Second, it improves Tier 2 and Tier 3 explanations, which involve discerning subtle audio differences such as scenes, acoustic sources, and their composition, rather than merely identifying audio events. From an architectural standpoint, we concatenate the first audio latent embedding, the separator embedding, and the second audio latent embedding to form a prefix. The separator embedding is derived from the $<\text{|endoftext|}>$ token embedding of GPT-2. This concatenated prefix is then processed through the cross-projection layer, which consists of transformer layers with a learnable constant vector. The output of the cross-projection layer is in the same latent space as GPT-2, allowing us to analyze the cross-projection outputs.

Let the output of the cross-projection layer be [n, d], where (n) represents the number of tokens, equal to the prefix length (121), and (d) is the hidden dimension, matching GPT-2’s intermediate layer size of 768. The GPT-2 vocabulary embedding has dimensions [v, d], where (v) is the vocabulary size and (d) is the hidden dimension. To find the closest match between the cross-projection tokens (n) and the vocabulary embeddings (v), we normalize the vectors and compute the dot product between them. The highest dot product indicates the most likely vocabulary token. These vocabulary tokens are numerical and tokenized using the BPE tokenizer. We then detokenize them to obtain English subwords. As expected, the interpreted prefix initially appears as gibberish, similar to findings in prefix tuning for other tasks. However, these gibberish English words can be parsed to extract meaningful audio words or subwords. Below, we present these subwords for readability.

\begin{table}[!ht]
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{lll}
\toprule
\textbf{Audio 1 prefix} & \textbf{Audio 2 prefix} & \textbf{Text prefix}\\ \midrule
explosion, crack, hit & helicop, loud & mid-high, frequency, pitch, dynamic, range \\ \midrule
bee, robots voices, repetition, downstairs, speakers & motor, rustic, radio pursuit, vintage throttle & development, plus, nature, catch, dynamic range, loud, single\\ \midrule
baby, cry, speaker, female & speaker, cheering, male & clean, predict, loud, greater \\ \midrule
clap, man, speaker, after & drum, bang, tik & dur, loud, intelli, soft, surprise \\
\bottomrule
\end{tabular}}
\caption{\small Parsed words and subwords from the interpreted prefix. The prefix here is the output of the cross-projection layer and the corresponding model outputs are shown in Table \ref{tab:sents}}
\label{tab:cp2}
\end{table}

We observe that even after the cross-projection layer is supposed to mix information between audios, the prefix for audio 1 retains information about audio 1, while the prefix for audio 2 retains information about audio 2. Instead, by using the cross-projection layer, it reuses the text prefix to store attributes and difference information, which is later used to form the explanation. Therefore, the text prefix gets used for not only steering the language model but also for storing information about attributes to be used for comparison. For example, in the first row of Table \ref{tab:cp2}, the text prefix contains mid-range, frequency, and pitch attributes, which differ between the two audios and appear in the ADIFF output (Table \ref{tab:sents}, row 1). Similarly, in the second example, we see loudness and dynamic range attributes in the prefix, with corresponding difference explanations using those for comparison (Table \ref{tab:sents}, row 2). The main caveat of these results is that we are approximately estimating the English token, assuming the output of the cross-projection is in the same space as GPT-2 and using a dot product. However, this result still provides insight into the model's workings and how the cross-projection layer benefits and improves model performance for Tier 2 and Tier 3 explanations.

\section{Effect of Stage-3 finetuning} \label{appendix: stage 3 finetune} \vspace{-0.1in}
We also perform qualitative analysis and find that Model 2 consistently outperformed Model 1 across three tiers. For example, one randomly picked example of which is shown in Figure \ref{fig:random FT example}. For Tier 1, Model 2 accurately described the ground truth for both audio samples, identifying the mix of human voices, animal sounds, and environmental noise in Audio 1, and the presence of machine sounds and animal noises without human speech in Audio 2. In Tier 2, Model 2 captured the specific details of the dominant sound sources, correctly identifying the complexity and variety of sounds in Audio 1 and contrasting them with the mechanical sounds and horse's hooves in Audio 2. For Tier 3, Model 2 provided a more accurate and detailed comparison, describing the distinct sounds and their acoustic properties, as well as the overall ambiance and spatial characteristics of each audio, making it a more comprehensive and precise analysis compared to Model 1.

\begin{figure*}[ht]
   \centering
     \includegraphics[width=1\textwidth]{figs/ex2.png}
     \caption{\small We compare two models: first, GPT2, the model after stage-2 multimodal grounding, Second, GPT2 FT after stage-3 finetuning. The architecture for both models is the same and consist of audio encoder, audio projection, cross-projection, and GPT2-base as the decoder. The top pane provides a concise explanation, the middle pane offers a brief explanation, and the bottom pane presents a detailed explanation.
     % \vspace{-0.2in}
     }
     \label{fig:appendix 3ft}
\end{figure*}

\section{Ablation extension} \label{appendix: ablation extension} \vspace{-0.1in}
The ablations results are shown in Figure \ref{fig:subplots} for easy comparison. In the (1,1) subplot, the language-only model (with a randomly initialized and frozen audio encoder) demonstrates lower performance, particularly in Tier 1, which is the hardest due to its limited words and focus on audio-related content. In contrast, Tier 2 is the easiest to learn due to its simpler linguistic structure, and Tier 3 falls in between. The (2,1) subplot, which presents baseline model performance with a pretrained and frozen audio encoder, shows improved results across all tiers. However, qualitative analysis reveals two limitations with this baseline model: difficulty distinguishing between perceptually similar sounds and struggling to differentiate subtle audio differences in Tiers 2 and 3. To address these issues, a cross-projection layer with a latent separator token is introduced, as shown in the (3,1) subplot. This adjustment improves performance, particularly in distinguishing audios in ACD and CLD datasets across all tiers.

Further experiments to explore the impact of model scale are depicted in subplots (2,1), (2,2), and (2,3). Using GPT2-base, GPT2-medium, and GPT2-large (256M, 774M, and 1.5B parameters, respectively), the models were trained with the same compute budget. Results show that the base and medium models perform similarly, while the larger models require more epochs to achieve better performance. The (3,2) subplot presents results after incorporating audio captioning data to further improve audio grounding, where the model distinguishes between audios more accurately, especially in the ACD dataset, which includes more human speech comparisons. However, the CLD dataset shows mixed results due to its wider variety of audio comparisons. Despite this, the audio captioning model continues to be used for stage-3 finetuning. Finally, the (3,3) subplot presents the performance after fine-tuning the language model over a few epochs with a small learning rate. This stage consistently improves performance across tiers and datasets, with the finetuned model outperforming the base model (subplot (3,1)) in all cases.

\section{Qualitative Analysis} \vspace{-0.1in}
We perform analysis of explanations generated by ADIFF. The different scenarios and explanation are shown in Table \ref{tab:sents}. In the first scenario, Audio (1) features a sudden, sharp explosion followed by crackling sounds, while Audio (2) presents a continuous low-frequency hum of a helicopter. This comparison highlights differences in frequency content and dynamic range, with Audio (1) having a more gradual onset and decay, and Audio (2) exhibiting sudden changes in loudness and frequency. The second scenario contrasts a baby crying and a woman speaking in Audio (1) with a man speaking and a crowd cheering in Audio (2). Here, the focus is on the distinct acoustic signatures and dynamic ranges, with Audio (1) being more consistent and predictable, while Audio (2) has greater variability in loudness and pitch. Here, we compare different voice characteristics which involves understanding pitch, frequency, and intensity. In the third scenario, the model compares a brief, loud applause followed by a male voice in Audio (1) with sharp, percussive sounds in Audio (2). This scenario emphasizes differences in acoustic properties such as frequency, loudness, and duration. The fourth scenario involves a dog barking loudly in Audio (1) and several dogs barking with a man speaking in Audio (2), highlighting the chaotic and dissonant quality of the latter. In this scenario, we compare model performance on similar audio events to spot differences. Finally, the fifth scenario compares a group playing a piano in Audio (1) with a piano followed by a drum beat in Audio (2), focusing on the timbre and frequency content. These scenarios were chosen to evaluate the model's ability to describe various audio events and scenes. 

We also compare ADIFF generation against existing ALMs in the literature. As of yet, only Qwen-Audio Chat (\cite{qwenaudio}) supports two audios as input. The qualitative comparison is shown in Figure \ref{fig:qwencompare}. We consider three scenarios: Contrasting Acoustic Qualities, Overlapping Sound Sources, and Overlapping Sound Events. For Contrasting Acoustic Qualities, Qwen Audio identified the first clip as someone cutting wood with a chainsaw and the second as someone walking on snow, while ADIFF provided a detailed analysis, noting the harsh, high-frequency sound of the chainsaw and the mellow, low-frequency sound of walking on snow. In the Overlapping Sound Sources scenario, Qwen Audio described both clips as featuring dogs barking, with the first being playful and the second aggressive, whereas ADIFF detailed the first clip's consistent, sharp barks and the second clip's chaotic mix of dogs barking and a man speaking. Lastly, in the Overlapping Sound Events scenario, Qwen Audio simply stated that both clips were the same song performed by different orchestras, while ADIFF offered a detailed comparison, highlighting the rich, resonant timbre of the first clip's piano and the structured sound profile of the second clip's piano and drum beat, noting the first as harmonious and calming and the second as energetic and attention-grabbing. Overall, we find ADIFF explanations to be more accurate (audio event coverage) and with finer granularity compared to Qwen-Audio. 

\begin{figure*}[ht]
   \centering
     \includegraphics[width=0.8\textwidth]{figs/qwen.png}
     \caption{\small Comparing ADIFF generation against current ALMs across three scenarios: Contrasting Acoustic Qualities, Overlapping Sound Sources, and Overlapping Sound Events. Notably, Qwen-Audio Chat is the only ALM in the literature that supports two audio inputs.
     }
     \label{fig:qwencompare}
\end{figure*}

\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Scenario}   & \textbf{Difference explanation from ADIFF} \\ \midrule
\multirow{10}{*}{1} & As I listen to the two audios, I notice that Audio (1) features \\ & a sudden, sharp, and high-frequency explosion followed by a series of \\ & crackling sounds, likely generated by a small object being hit. The \\ & explosion’s frequency content is likely dominated by mid-to-high frequencies, with \\ & a prominent presence of low-frequency energy. In contrast, Audio (2) \\ & presents a continuous, low-frequency hum of a helicopter flying overhead, \\ & with a consistent pitch and amplitude. The helicopter’s sound is likely \\ & generated by a series of high-frequency transients, with a prominent presence \\ & of mid-to-high frequencies. The overall dynamic range of Audio (2) \\ & is much greater, with sudden changes in loudness and frequency content, \\ & whereas Audio (1) has a more gradual onset and decay. \\
\midrule
\multirow{10}{*}{2} & As I listen to the two audios, I notice that the first audio, \\ & featuring a baby crying and a woman speaking, has a distinct acoustic \\ & signature. The baby’s cries are characterized by high-pitched, irregular patterns \\ & of sounds, with varying frequencies and durations, creating a sense of \\ & urgency and distress. The woman’s voice, on the other hand, is softer \\ & and more gentle, with a consistent pitch and volume. In contrast, the \\ & second audio, featuring a man speaking and a crowd cheering, has a \\ & more structured sound profile. The man’s voice is clear and intelligible, \\ & with a consistent pitch and volume, while the crowd’s cheering adds \\ & a layer of complexity with varying frequencies, loudness, and durations. \\ & The overall dynamic range of the second audio is much greater, with \\ & sudden changes in loudness and pitch, whereas the first audio has \\ & a more consistent and predictable loudness. \\ 
\midrule
\multirow{5}{*}{3} & The two audios differ significantly in terms of their acoustic properties, \\ & with the first audio featuring a sudden, brief, and loud applause \\ & followed by a clear and intelligible male voice, whereas the second \\ & audio consists of a series of sharp, percussive, and percussive sounds \\ & from the tapping and banging, with distinct differences in frequency, \\ & loudness, and duration. \\ 
\midrule
\multirow{10}{*}{4} & As I listen to the two audios, I notice that the first audio, featuring \\ & a dog barking loudly, has a distinct acoustic signature characterized by a \\ & consistent, repetitive pattern of barks with varying frequencies and durations. \\ & The sound is loud and has a sharp attack, with a relatively short duration. In \\ & contrast, the second audio, featuring several dogs barking and a man \\ & speaking, has a more chaotic and dissonant quality. The dog's barks are \\ & loud and have a wide range of frequencies, from low rumbles to high-pitched \\ & yips, with varying durations and intensities. The man’s voice is softer and more \\ & muffled, with a more gradual attack and decay. The overall texture of the second \\ & audio is more chaotic and dissonant, with a greater emphasis on the dog’s \\ & vocalizations and the man’s speech. \\
\midrule
\multirow{10}{*}{5} & I notice that the first audio, featuring a group of people playing a piano, has a distinct acoustic \\ & signature characterized by a rich, resonant timbre with a wide range \\ & of frequencies, from low rumbles to high-pitched notes. The sound is \\ & rich in low-frequency energy, with a prominent presence of mid-range \\ & frequencies, giving it a bright and piercing quality. In contrast, the \\ & second audio, featuring a piano playing followed by a drum beat, \\ & has a more structured and structured sound profile. The piano’s timbre \\ & is characterized by a consistent pitch, with a prominent presence of \\ & high-frequency energy, creating a sense of rhythm and melody. The \\ & drum beat’s frequency content is more balanced, with a prominent \\ & presence of low-frequency energy, creating a sense of tension and \\ & release. Overall, the first audio is more harmonious and calming, \\ & while the second audio is more energetic and attention-grabbing. \\
\bottomrule
\end{tabular}}
\caption{\small Example outputs generated from ADIFF for different scenarios}
\label{tab:sents}
\end{table}

\begin{figure*}[ht]
   \centering
     \includegraphics[width=1\textwidth]{figs/exe2.png}
     \caption{\small A randomly selected example showcases the model’s detailed explanation of audio differences, accompanied by log mel spectrograms and audio presence plots. The top pane displays the model-generated explanation. For each audio clip, it includes the log mel spectrogram and the top three audio event presence probabilities over time.
     }
     \label{fig:random FT example}
\end{figure*}

\section{Generation diversity} \label{appendix: diversity} \vspace{-0.1in}
If we utilize greedy decoding for both inference and evaluation, it results in outputs with repetitive starting phrases like “As I listen to the two audios” and the same tokens being used to describe sound events and scenes, as shown in Table \ref{tab:sents}. This repetition is expected due to the nature of greedy decoding, which selects the most likely token at each step. To enhance the diversity of the generated content, we explore top-k and top-p decoding. We set top-p to a high value (0.8) to limit the sampling of low-probability tokens. Additionally, we choose a top-k value of 3, allowing other high-scoring tokens a chance to be selected, thereby introducing randomness and improving the diversity of the output. The results are shown in Table \ref{tab:gendiversity}. Using greedy decoding, the temporal order of events for audio 1 is incorrectly identified. However, with top-k and top-p decoding, the temporal order of events for audio 1 is correctly identified. Overall, top-k and top-p decoding increases the diversity of the output, reducing monotonicity and helps to improve performance. The results with top-k and top-p decoding are compared against greedy decoding in Table \ref{table: topp topk vs greedy}. 

\begin{table}
\centering
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Decoding}   & \textbf{Generated difference explanation} \\ \midrule
\multirow{10}{*}{Ground truth} &  As I listen to the two audios, I notice that the \\&
first audio is characterized by a sudden onset of sounds, starting \\&
with the clear and crisp voice of a young female, followed by \\&
a sharp, high-pitched sound of spraying, and then a piercing scream. \\&
In contrast, the second audio features a steady, gentle flow of water, \\&
accompanied by a soft and distant voice of a woman speaking. The most \\&
striking difference is the dynamic range and temporal structure of the \\&
two audios. The first audio has a sudden and dramatic change in sound \\&
levels, with a clear distinction between the soft voice and the loud \\&
spraying and screaming, whereas the second audio has a more consistent \\&
and gradual sound level, with the water flow and distant voice \\&
blending together seamlessly. \\
\midrule
\multirow{10}{*}{Greedy} & As I listen to the two audios, I notice that Audio 1, featuring \\&
a girl speaking and spraying, has a distinctively different acoustic \\&
profile compared to Audio 2, which consists of water running and \\&
a distant woman speaking. The first audio is characterized by a \\&
sudden, sharp attack of a spray sound, followed by a woman's \\&
voice speaking in a calm tone, and then a sudden, sharp attack \\&
of a spray sound. The frequency content of the two audios is \\&
also distinct, with Audio 1 featuring a wider range of frequencies, \\&
from low rumbles to high-pitched squeals, whereas Audio 2 has a \\&
more even distribution of frequencies across the audio spectrum. \\&
Overall, the two audios have distinct acoustic properties, with \\&
Audio 1 featuring a sudden, attention-grabbing event and Audio 2 \\&
featuring a calming, serene atmosphere. \\
\midrule
\multirow{10}{*}{Top-k and Top-p  (1)} & As I listen to the two audios, I notice that Audio 1, featuring a woman \\&
talking followed by spraying and spraying noise, has a distinct sonic \\&
landscape. The woman's voice is characterized by a high-frequency range, \\&
likely in the 100-500 Hz range, with a rapid tempo and a steady loudness \\&
level. The spraying noise adds a sudden, high-energy event, followed \\&
by a sharp, transient event, and then a brief pause before a woman \\&
speaks again. In contrast, Audio 2, featuring water running, has a more \\&
gradual and continuous sound, with a gentle, mid-frequency range \\&
(500-2000 Hz) that provides a sense of texture and continuity. The \\&
sound of the water is characterized by a consistent, gentle flow, with \\&
a consistent loudness level and a steady tempo. The difference lies in \\&
the type of sound sources, materials, and interactions involved, with \\&
Audio 1 featuring sudden, attention-grabbing events and Audio 2 presenting \\&
a more continuous, soothing background. \\
\midrule
\multirow{10}{*}{Top-k and Top-p (2)} & As I listen to the two audios, I notice a stark contrast between them. \\&
Audio (1) features a sudden, high-frequency burst of a female voice, \\&
followed by a sudden, sharp, and high-frequency spraying sound, and \\&
then another female voice. The audio is characterized by a sharp attack \\&
and decay, with a clear and intelligible voice that is likely in the \\&
mid-to-high frequency range. In contrast, Audio (2) features a gentle, \\&
low-frequency gurgling sound from a water source, accompanied by \\&
distant speech. The audio is dominated by mid-to-high frequency range, \\&
with a smooth and continuous quality that is characteristic of water \\&
sounds. The acoustic properties of these two audios are fundamentally \\&
different, with Audio (1) evoking a sense of excitement and excitement, \\&
while Audio (2) evokes a sense of serenity and relaxation. \\
\bottomrule
\end{tabular}}
\caption{\small Generated outputs from different decoding methods. (1) and (2) indicates run 1 and run 2 respectively.}
\label{tab:gendiversity}
\end{table}

\vspace{-0.1in}
\section{ADIFF without cross-projection layer} \vspace{-0.1in}
In Table \ref{subsec: effect of cross-projection}, we examine the impact of the cross-projection layer. The middle section of the table presents the performance of the baseline architecture, while the bottom section illustrates the performance with the inclusion of the separator token and cross-projection layer. On average, we notice improvements for ACD and CLD across all three tiers of explanations. However, in the final ADIFF model, the LLM is fine-tuned, potentially learning what the cross-projection layer achieves, thereby rendering it redundant. To address this, we conducted another ablation study comparing the ADIFF model with three-step training without the cross-projection layer against the ADIFF model with the cross-projection layer. The results of this experiment are shown in Table \ref{table: cross-projection adiff}. We found that the difference between ADIFF with and without the cross-projection layer is reduced. Nevertheless, ADIFF with the cross-projection layer still outperforms ADIFF without the cross-projection layer across all tiers for the ACD and CLD datasets. We hypothesize that when the LM is fine-tuned, the cross-projection layer acts as block attention over the audio and text prefixes, which has been shown to improve performance in vision LLMs \cite{beyer2024paligemma}. Thus, retaining the cross-projection layer still leads to better performance even when the LLM is fine-tuned.

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule 
ACD & 1 & 0.384 & 0.256 & 0.182 & 0.129 & 0.208 & 0.352 & 0.470 & 0.130 & 0.300 & 0.268 \\
ACD & 2 & 0.444 & 0.304 & 0.223 & 0.162 & 0.150 & 0.377 & 0.407 & 0.185 & 0.296 & 0.288 \\
ACD & 3 & 0.477 & 0.305 & 0.214 & 0.164 & 0.199 & 0.323 & 0.148 & 0.152 & 0.150 & 0.237 \\
CLD & 1 & 0.446 & 0.338 & 0.258 & 0.191 & 0.304 & 0.478 & 0.832 & 0.190 & 0.511 & 0.394 \\
CLD & 2 & 0.517 & 0.354 & 0.261 & 0.194 & 0.231 & 0.458 & 0.971 & 0.251 & 0.611 & 0.427 \\
CLD & 3 & 0.496 & 0.325 & 0.228 & 0.166 & 0.203 & 0.357 & 0.232 & 0.184 & 0.208 & 0.266 \\ \midrule
ACD & 1 & 0.405 & 0.276 & 0.192 & 0.135 & 0.221 & 0.381 & 0.469 & 0.137 & 0.303 & 0.280 \\
ACD & 2 & 0.440 & 0.301 & 0.232 & 0.180 & 0.197 & 0.379 & 0.518 & 0.173 & 0.345 & 0.307 \\
ACD & 3 & 0.486 & 0.317 & 0.225 & 0.171 & 0.208 & 0.331 & 0.204 & 0.162 & 0.183 & 0.254 \\
CLD & 1 & 0.433 & 0.338 & 0.265 & 0.203 &  0.302 & 0.488 & 1.093 & 0.211 & 0.652 & 0.443 \\
CLD & 2 & 0.519 & 0.362 & 0.274 & 0.213 & 0.235 & 0.464 & 1.124 & 0.260 & 0.692 & 0.460 \\
CLD & 3 & 0.522 & 0.355 & 0.255 & 0.191 & 0.220 & 0.360 & 0.634 & 0.200 & 0.417 & 0.350 \\ \bottomrule
\end{tabular} 
\caption{\label{table: cross-projection adiff} 
\small ADIFF model performance without cross-projection layer. The top half of the table shows the performance of the ADIFF model without the cross-projection layer. The bottom half of the table shows the performance of the ADIFF model with the cross-projection layer. Both models are trained on ACD and CLD train split. The Tier 1, 2, and 3 classifications correspond to the explanation tiers detailed in Section \ref{sec: audio difference task tiers}.}
\end{table*}

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule 
ACD & 1 & 0.351 & 0.247 & 0.174 & 0.118 & 0.210 & 0.368 & 0.329 & 0.111 & 0.220 & 0.236\\
ACD & 2 & 0.425 & 0.298 & 0.221 & 0.163 & 0.193 & 0.375 & 0.284 & 0.166 & 0.225 & 0.261 \\
ACD & 3 & 0.442 & 0.282 & 0.199 & 0.153 & 0.188 & 0.318 & 0.117 & 0.129 & 0.123 & 0.217\\
CLD & 1 & 0.368 & 0.266 & 0.192 & 0.128 & 0.237 & 0.431 & 0.270 & 0.156 & 0.212 & 0.251\\
CLD & 2 & 0.517 & 0.376 & 0.294 & 0.233 & 0.234 & 0.476 & 0.990 & 0.292 & 0.641 & 0.450 \\
CLD & 3 & 0.448 & 0.298 & 0.211 &0.157 &0.199 &0.327 &0.127 &0.205 &0.166 & 0.238\\ \midrule
ACD & 1 & 0.382 & 0.257 & 0.185 & 0.132 & 0.214 & 0.374 & 0.366 & 0.105 & 0.235 & 0.250 \\
ACD & 2 & 0.426 & 0.296 & 0.221 & 0.166 & 0.204 & 0.377 & 0.234 & 0.191 & 0.212 & 0.259\\
ACD & 3 & 0.480 & 0.306 & 0.216 & 0.165 & 0.202 & 0.329 & 0.196 & 0.151 & 0.173 & 0.246 \\
CLD & 1 & 0.387 & 0.288 & 0.217 & 0.140 & 0.285 & 0.456 & 0.287 & 0.174 & 0.230 & 0.274 \\
CLD & 2 & 0.514 & 0.376 & 0.292 & 0.233 & 0.236 & 0.473 & 1.272 & 0.239 & 0.756 & 0.488 \\
CLD & 3 & 0.485 & 0.308 & 0.214 & 0.155 & 0.200 & 0.310 & 0.186 & 0.178 & 0.182 & 0.247 \\ \midrule
ACD & 1 & 0.359 & 0.224 & 0.156 & 0.110 & 0.183 & 0.331 & 0.411 & 0.104 & 0.258 & 0.237\\
ACD & 2 & 0.442 & 0.302 & 0.219 & 0.163 & 0.199 & 0.376 & 0.311 & 0.170 & 0.241 & 0.269 \\
ACD & 3 & 0.462 & 0.290 & 0.200 & 0.151 & 0.194 & 0.309 & 0.030 & 0.134 & 0.082 & 0.206 \\
CLD & 1 & 0.350 & 0.258 & 0.195 & 0.126 & 0.231 & 0.401 & 0.290 & 0.117 & 0.204 & 0.241 \\
CLD & 2 & 0.558 & 0.421 & 0.336 & 0.273 & 0.254 & 0.517 & 1.620 & 0.301 & 0.958 & 0.581 \\
CLD & 3 & 0.468 & 0.282 & 0.185 & 0.130 & 0.207 & 0.328 & 0.162 & 0.182 & 0.172 & 0.235 \\ \midrule
ACD & 1 & 0.405 & 0.276 & 0.192 & 0.135 & 0.221 & 0.381 & 0.469 & 0.137 & 0.303 & 0.280 \\
ACD & 2 & 0.440 & 0.301 & 0.232 & 0.180 & 0.197 & 0.379 & 0.518 & 0.173 & 0.345 & 0.307 \\
ACD & 3 & 0.486 & 0.317 & 0.225 & 0.171 & 0.208 & 0.331 & 0.204 & 0.162 & 0.183 & 0.254 \\
CLD & 1 & 0.433 & 0.338 & 0.265 & 0.203 &  0.302 & 0.488 & 1.093 & 0.211 & 0.652 & 0.443 \\
CLD & 2 & 0.519 & 0.362 & 0.274 & 0.213 & 0.235 & 0.464 & 1.124 & 0.260 & 0.692 & 0.460 \\
CLD & 3 & 0.522 & 0.355 & 0.255 & 0.191 & 0.220 & 0.360 & 0.634 & 0.200 & 0.417 & 0.350 \\ \bottomrule
\end{tabular} 
\caption{\label{table: appendix audio difference results} 
\small Comparing baseline models against ADIFF model performance. The top half of the table shows the performance of the naive model. The second part shows the performance of QwenAC finetuned with LoRA. The third part shows the performance of QwenAC finetuned fully. While the bottom half of the table shows the performance of the ADIFF model. Both models are trained on ACD and CLD train split. The Tier 1, 2, and 3 classifications correspond to the explanation tiers detailed in Section \ref{sec: audio difference task tiers}. 
% \vspace{-0.1in}
}
\end{table*}

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule 
ACD & 1 & 0.303 & 0.199 & 0.130 & 0.082 & 0.184 & 0.319 & 0.175 & 0.082 & 0.128 & 0.178\\
ACD & 2 & 0.396 & 0.263 & 0.188 & 0.135 & 0.176 & 0.349 & 0.156 & 0.152 & 0.154 & 0.219\\
ACD & 3 & 0.439 & 0.271 & 0.192 & 0.149 & 0.175 & 0.319 & 0.0736 & 0.140 & 0.107 & 0.207 \\
CLD & 1 & 0.350 & 0.257 & 0.195 & 0.138 & 0.223 & 0.409 & 0.116 & 0.138 & 0.127 & 0.217\\
CLD & 2 & 0.507 & 0.343 & 0.258 & 0.199 & 0.236 & 0.459 & 0.922 & 0.256 & 0.589 & 0.419 \\
CLD & 3 & 0.411 & 0.249 & 0.170 & 0.123 & 0.166 & 0.300 & 0.111 & 0.143 & 0.127 & 0.200 \\ \midrule
ACD & 1 & 0.351 & 0.247 & 0.174 & 0.118 & 0.210 & 0.368 & 0.329 & 0.111 & 0.220 & 0.236\\
ACD & 2 & 0.425 & 0.298 & 0.221 & 0.163 & 0.193 & 0.375 & 0.284 & 0.166 & 0.225 & 0.261 \\
ACD & 3 & 0.442 & 0.282 & 0.199 & 0.153 & 0.188 & 0.318 & 0.117 & 0.129 & 0.123 & 0.217\\
CLD & 1 & 0.368 & 0.266 & 0.192 & 0.128 & 0.237 & 0.431 & 0.270 & 0.156 & 0.212 & 0.251\\
CLD & 2 & 0.517 & 0.376 & 0.294 & 0.233 & 0.234 & 0.476 & 0.990 & 0.292 & 0.641 & 0.450 \\
CLD & 3 & 0.448 & 0.298 & 0.211 &0.157 &0.199 &0.327 &0.127 &0.205 &0.166 & 0.238\\ \midrule
ACD & 1 & 0.376 & 0.258 & 0.183 & 0.131 & 0.214 & 0.364 & 0.445 & 0.129 & 0.287 & 0.265\\
ACD & 2 & 0.427 & 0.287 & 0.209 & 0.155 & 0.197 & 0.373 & 0.305 & 0.181 & 0.243 & 0.264\\
ACD & 3 & 0.471 & 0.293 & 0.200 & 0.149 & 0.203 & 0.316 & 0.161 & 0.147 & 0.154 & 0.233 \\
CLD & 1 & 0.395 & 0.296 & 0.224 & 0.156 & 0.257 & 0.431 & 0.214 & 0.176 & 0.195 & 0.260 \\
CLD & 2 & 0.585 & 0.427 & 0.340 & 0.280 & 0.267 & 0.525 & 1.485 & 0.322 & 0.904 & 0.571 \\
CLD & 3 & 0.444 & 0.266 & 0.177 & 0.127 & 0.187 & 0.305 & 0.221 & 0.171 & 0.196 & 0.233 \\ \bottomrule
\end{tabular} 
\caption{\small \label{table: appendix language-only and baseline} Architecture results. The top half of the table uses baseline architecture with random audio encoder weights. The middle half of the table uses pretrained audio encoder weights. The bottom half of the table is ADIFF which modifies baseline architecture with separator token and cross-projection} 
% \vspace{-0.2in}
\end{table*}

\begin{table*}[!ht]
\center
\scriptsize
\begin{tabular}{ccccccccccccc}
\toprule
LM & \makecell{Task} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule
Base & ACD-1 & 0.376 & 0.258 & 0.183 & 0.131 & 0.214 & 0.364 & 0.445 & 0.129 & 0.287 & 0.265\\
Base & ACD-2 & 0.427 & 0.287 & 0.209 & 0.155 & 0.197 & 0.373 & 0.305 & 0.181 & 0.243 & 0.264\\
Base & ACD-3 & 0.471 & 0.293 & 0.200 & 0.149 & 0.203 & 0.316 & 0.161 & 0.147 & 0.154 & 0.233 \\
Base & CLD-1 & 0.395 & 0.296 & 0.224 & 0.156 & 0.257 & 0.431 & 0.214 & 0.176 & 0.195 & 0.260 \\
Base & CLD-2 & 0.585 & 0.427 & 0.340 & 0.280 & 0.267 & 0.525 & 1.485 & 0.322 & 0.904 & 0.571 \\
Base & CLD-3 & 0.444 & 0.266 & 0.177 & 0.127 & 0.187 & 0.305 & 0.221 & 0.171 & 0.196 & 0.233 \\ \midrule
Med. & ACD-1 & 0.401 & 0.264 & 0.178 & 0.119 & 0.211 & 0.373 & 0.520 & 0.131 & 0.325 & 0.280 \\
Med. & ACD-2 & 0.427 & 0.287 & 0.210 & 0.154 & 0.198 & 0.370 & 0.324 & 0.171 & 0.248 & 0.266 \\
Med. & ACD-3 & 0.468 & 0.300 & 0.210 & 0.160 & 0.201 & 0.337 & 0.304 & 0.160 & 0.232 & 0.264 \\
Med. & CLD-1 & 0.409 & 0.287 & 0.209 & 0.149 & 0.270 & 0.459 & 0.486 & 0.147 & 0.317 & 0.304 \\
Med. & CLD-2 & 0.560 & 0.410 & 0.324 & 0.264 & 0.287 & 0.510 & 0.941 & 0.340 & 0.641 & 0.475 \\
Med.& CLD-3 & 0.474 & 0.296 & 0.200 & 0.143 & 0.203 & 0.326 & 0.262 & 0.146 & 0.204 & 0.250\\ \midrule
Large & ACD-1 & 0.362 & 0.250 & 0.175 & 0.124 & 0.221 & 0.371 & 0.356 & 0.124 & 0.240 & 0.247 \\
Large & ACD-2 & 0.428 & 0.284 & 0.208 & 0.156 & 0.197 & 0.353 & 0.153 & 0.141 & 0.147 & 0.230 \\
Large & ACD-3 & 0.479 & 0.311 & 0.216 & 0.162 & 0.207 & 0.328 & 0.182 & 0.149 & 0.165 & 0.244 \\
Large & CLD-1 & 0.365 & 0.269 & 0.198 & 0.133 & 0.238 & 0.419 & 0.405 & 0.121 & 0.263 & 0.268 \\
Large & CLD-2 & 0.502 & 0.329 & 0.239 & 0.180 & 0.223 & 0.457 & 0.930 & 0.219 & 0.574 & 0.406 \\
Large & CLD-3 & 0.483 & 0.318 & 0.230 & 0.177 & 0.191 & 0.331 & 0.263 & 0.192 & 0.227 & 0.268 \\ \midrule
XL & ACD-1 & 0.373 & 0.259 & 0.181 & 0.129 & 0.216 & 0.376 & 0.391 & 0.153 & 0.272 & 0.261 \\
XL & ACD-2 & 0.431 & 0.297 & 0.219 & 0.164 & 0.202 & 0.373 & 0.256 & 0.173 & 0.215 & 0.259 \\
XL & ACD-3 & 0.498 & 0.310 & 0.209 & 0.154 & 0.208 & 0.338 & 0.072 & 0.141 & 0.107 & 0.226 \\
XL & CLD-1 & 0.373 & 0.271 & 0.195 & 0.112 & 0.256 & 0.420 & 0.402 & 0.149 & 0.275 & 0.273 \\
XL & CLD-2 & 0.546 & 0.386 & 0.296 & 0.234 & 0.255 & 0.486 & 0.788 & 0.266 & 0.527 & 0.420 \\
XL & CLD-3 & 0.490 & 0.300 & 0.205 & 0.146 & 0.208 & 0.344 & 0.243 & 0.180 & 0.212 & 0.259 \\ \bottomrule
\end{tabular} 
\caption{\small \label{table: appendix scale language model} Scaling language model results. The language model in ADIFF architecture (transformer decoder) ranges in size from 128 million (Base) to 1.5 billion parameters (XL). 
% \vspace{-0.2in}
}  
\end{table*}

\begin{table*}[!ht]
\center
\scriptsize
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule
ACD & 1 & 0.384 & 0.256 & 0.182 & 0.129 & 0.208 & 0.352 & 0.469 & 0.130 & 0.300 & 0.268 \\
ACD & 2 & 0.444 & 0.304 & 0.223 & 0.162 & 0.195 & 0.377 & 0.407 & 0.185 & 0.296 & 0.288 \\
ACD & 3 & 0.477 & 0.305 & 0.214 & 0.164 & 0.199 & 0.323 & 0.148 & 0.152 & 0.150 & 0.237 \\
CLD & 1 & 0.446 & 0.338 & 0.258 & 0.191 & 0.304 & 0.478 & 0.832 & 0.190 & 0.511 & 0.394 \\
CLD & 2 & 0.520 & 0.380 & 0.299 & 0.236 & 0.242 & 0.489 & 1.108 & 0.270 & 0.689 & 0.470 \\
CLD & 3 & 0.439 & 0.277 & 0.190 & 0.136 & 0.188 & 0.340 & 0.069 & 0.179 & 0.124 & 0.216 \\ \midrule
ACD & 1 &  0.405 & 0.275 & 0.192 & 0.135 & 0.221 & 0.381 & 0.469 & 0.137 & 0.303 & 0.280 \\
ACD & 2 & 0.440 & 0.301 & 0.232 & 0.180 & 0.197 & 0.379 & 0.518 & 0.173 & 0.345 & 0.307 \\
ACD & 3 & 0.485 & 0.317 & 0.225 & 0.171 & 0.208 & 0.331 & 0.204 & 0.162 & 0.183 & 0.254 \\
CLD & 1 & 0.433 & 0.338 & 0.265 & 0.203 & 0.302 & 0.488 & 1.093 & 0.211 & 0.652 & 0.443 \\
CLD & 2 & 0.519 & 0.362 & 0.274 & 0.213 & 0.235 & 0.464 & 1.124 & 0.260 & 0.692 & 0.460 \\
CLD & 3 & 0.522 & 0.355 & 0.255 & 0.191 & 0.220 & 0.360 & 0.634 & 0.200 & 0.417 & 0.350\\ \bottomrule
\end{tabular} 
\caption{\label{table: appendix audio grounding} \small Audio grounding and finetuning results. The top half of the table shows ADIFF performance with position captioning. The bottom half of Table finetunes the language model of ADIFF. 
% \vspace{-0.1in}
}
\end{table*}

\section{Using WavCaps for training}
AudioCaps \cite{audiocaps} and Clotho \cite{clotho} are used for audio captioning evaluation and used in every Audio-Language Model paper \cite{mspengi, ghosh2024gama, ltu, ltuas, qwenaudio, salmonn} in literature. Therefore, we use AudioCaps \cite{audiocaps} and Clotho \cite{clotho} for creating the ACD and CLD dataset. The human annotations in AudioCaps and Clotho may have problems, therefore we also perform human verification on the test difference explanations generated to remove any discrepancies. To improve performance on diverse audio recordings and scaling training, audio captioning and Audio-Language literature has used WavCaps \cite{wavcaps} dataset. The WavCaps dataset is a large-scale, weakly-labelled audio captioning dataset designed for audio-language multimodal research. It comprises approximately 400k audio clips with paired captions. The audio clips are sourced from various web sources, including FreeSound, BBC Sound Effects, SoundBible, and the AudioSet Strongly-labelled Subset. The dataset uses ChatGPT to filter and transform raw descriptions into high-quality captions. As there is no human verification in the process, we only use this data for training and not for evaluation. To generate the audio difference explanation for WavCaps, we follow the same process listed in Section \ref{sec: audio difference dataset} and Section \ref{appendix: llm gen}. We call the resulting dataset WavCaps Difference (WCD) and its statistics are shown in Table \ref{table: wavcaps difference data stats}. For example, in Tier 1, the WCD Train split has 366k examples with explanations having a median of 27, a maximum of 58, and a vocabulary size of 38425, while Tier 3 has 366k examples and the explanations have a median of 155, a maximum of 234, and a vocabulary size of 51028. 

\begin{table}[!ht]
\scriptsize
\center
\begin{tabular}{c|c|c|ccc|ccc|ccc} \toprule
 & & & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\
Data & Split & \makecell{Examples\\per tier} & Med. & Max & Vocab. & Med. & Max & Vocab. & Med. & Max & Vocab. \\
\midrule
CLD & Train & 19195 & 27 & 49 & 6528 & 51 & 92 & 6462 & 155 & 221 & 10818\\
CLD & Val & 5225 & 27 & 46 & 3743 & 51 & 89 & 4024 & 154 & 223 & 7026\\
CLD & Test & 5225 & 27 & 28 & 5225 & 52 & 86 & 4059 & 156 & 219 & 7152\\ \midrule
ACD & Train & 48660 & 29 & 47 & 3287 & 52 & 104 & 8483 & 155 & 235 & 12891\\
ACD & Val & 2456 & 28 & 46 & 2350  & 52 & 87 & 2563 & 154 & 227 & 4566\\ 
ACD & Test & 4680 & 29 & 47 & 3287 & 53 & 95 & 3329 & 154 & 220 & 5489 \\ \midrule
WCD & Train & 366083 & 28 & 58 & 38425 & 53 & 101 & 38277 & 155 & 234 & 51028 \\
\bottomrule
\end{tabular}
\caption{\small Comparing WavCaps Difference (WCD) dataset statistics with AudioCaps Difference (ACD) and Clotho Difference (CLD) dataset} \label{table: wavcaps difference data stats} \vspace{-0.1in}
\end{table}

We trained the ADIFF model using a three-stage training process on the WavCaps, ACD, and CLD datasets, and evaluated it on the ACD and CLD test sets. A summary of the results is shown in Table \ref{table: wavcaps}, with detailed metrics in Table \ref{table: appendix wavcaps}. Although incorporating WCD into the training data improves performance on some metrics, it generally results in a performance decline on the ACD and CLD test sets. This performance drop after adding WavCaps has been observed in existing literature \cite{wavcaps, mspengi, noaudiocap}, primarily because adding WavCaps changes the training distribution, shifting it further away from the testing distribution of ACD and CLD. This shift highlights a limitation of objective metrics that focus on linguistics. However, integrating WCD into ADIFF significantly enhances the model's vocabulary at inference, increases the coverage of sounds and audio sources, and provides greater granularity compared to ADIFF without WCD.

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\ \midrule
\makecell{Task} & \makecell{Models} & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr \\ \midrule 
\multirow{2}{*}{ACD} & ADIFF & \textbf{0.135} & \textbf{0.221} & \textbf{0.303} & 0.180 & 0.197 & \textbf{0.345} & \textbf{0.171} & \textbf{0.208} & \textbf{0.183} \\ 
& ADIFF + WCD & 0.105 & 0.189 & 0.198 & \textbf{0.197} & \textbf{0.205} & 0.259 & 0.152 & 0.197 & 0.152 \\ \midrule 
\multirow{2}{*}{CLD} & ADIFF & \textbf{0.203} & \textbf{0.302} & \textbf{0.652} & 0.213 & 0.235 & 0.692 & \textbf{0.191} & \textbf{0.220} & \textbf{0.417}\\ 
& ADIFF + WCD & 0.126 & 0.246 & 0.278 & \textbf{0.322} & \textbf{0.290} & \textbf{1.022} & 0.180 & 0.211 & 0.367 \\ \bottomrule
\end{tabular}
\caption{\label{table: appendix wavcaps} 
\small ADIFF performance with WavCaps difference explanation (WCD) added to training data. The top half of the table shows the performance of models on the task of ACD, while The bottom half of the table shows the performance of models on the task of CLD. The results with all metrics and the average score are available in Appendix Table \ref{table: wavcaps}. \vspace{-0.1in}}
\end{table*}

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule
ACD & 1 & 0.405 & 0.276 & 0.192 & 0.135 & 0.221 & 0.381 & 0.469 & 0.137 & 0.303 & 0.280 \\
ACD & 2 & 0.440 & 0.301 & 0.232 & 0.180 & 0.197 & 0.379 & 0.518 & 0.173 & 0.345 & 0.307 \\
ACD & 3 & 0.486 & 0.317 & 0.225 & 0.171 & 0.208 & 0.331 & 0.204 & 0.162 & 0.183 & 0.254 \\
CLD & 1 & 0.433 & 0.338 & 0.265 & 0.203 &  0.302 & 0.488 & 1.093 & 0.211 & 0.652 & 0.443 \\
CLD & 2 & 0.519 & 0.362 & 0.274 & 0.213 & 0.235 & 0.464 & 1.124 & 0.260 & 0.692 & 0.460 \\
CLD & 3 & 0.522 & 0.355 & 0.255 & 0.191 & 0.220 & 0.360 & 0.634 & 0.200 & 0.417 & 0.350 \\ \midrule
ACD & 1 & 0.346 & 0.226 & 0.155&0.105&0.189&0.323&0.292&0.104&0.198&0.215 \\
ACD & 2 & 0.472&0.342&0.257&0.197&0.205&0.399&0.338&0.179&0.259&0.294 \\
ACD & 3 & 0.463&0.284&0.198&0.152&0.197&0.304&0.161&0.142&0.152&0.228 \\
CLD & 1 & 0.368&0.269&0.196&0.126&0.246&0.401&0.461&0.096&0.278&0.271 \\
CLD & 2 & 0.599&0.467&0.387&0.322&0.290&0.546&1.73&0.318&1.022&0.631 \\
CLD & 3 & 0.505&0.328&0.235&0.180&0.211&0.348&0.564&0.170&0.367&0.323 \\ \bottomrule
\end{tabular} 
\caption{\label{table: wavcaps} 
\small ADIFF performance with WavCaps difference explanation (WCD) added to training data. The top half of the table shows the ADIFF model performance. The bottom part shows the performance of ADIFF with WCD added to the training data. The Tier 1, 2, and 3 classifications correspond to the explanation tiers detailed in Section \ref{sec: audio difference task tiers}. 
}
\end{table*}

\begin{table*}[t]
\centering
\scriptsize
\setlength\tabcolsep{1pt}
% \fontsize{7.7}{3}\selectfont
\begin{tabular}{p{13cm}} \\ \toprule  
\textbf{LLM prompt} \\ \midrule
You are a helpful assistant with expert knowledge about audio, acoustics, and psychoacoustics. You study audio, which is the study of sound and its properties. You study acoustics, which revolve around the generation, propagation, and reception of sound waves. You study Psychology which posits that a sound is a complex stimulus that encompasses a vast range of acoustic properties involving aspects of cognition, psychoacoustics, and psychomechanics. Your task is to rate the difference explanation between the two audios and provide a score for how detailed and granular is the audio difference explanation. Pay attention to differences in terms of audio events (dog barking vs cat meowing), sound sources (musical instrument, human voice), acoustic scene (park vs living room), signal characteristics (frequency, pitch, loudness, etc) emotion response (listening to the audio invokes a sense of happiness, fear, in you) and any other difference using world-knowledge or using deductive reasoning. The score can take the value of 1,2,3,4, and 5. The score of 1 (poor) indicates the explanation is very vague, lacking detail and specificity. The score of 2 (fair) indicates the explanation provides some detail but is generally too broad. The score of 3 (good) indicates the explanation is moderately detailed, covering key aspects but missing finer points. The score of 4 (Very Good) indicates the explanation is detailed, covering most aspects with good specificity. 5 (Excellent) the explanation is highly detailed, thoroughly covering all relevant aspects with precise specificity. Return your rating as a JSON with the key ``score".\\ \bottomrule
\end{tabular}
\caption{\small Prompting LLM to score information density in the generated audio difference explanation} \label{table: density prompt}
\end{table*}

\section{Information density in difference explanation} \label{appendix: information density}
We create the three Tiers in Section \ref{sec: audio difference task tiers} with each tier containing more detailed and granular information than the previous Tier. We explain the data statistics in \ref{sec: audio difference dataset} and further analysis on difference explanations in \ref{appendix: reviewer analysis}. In this section, we demonstrate the quality of the three tiers, showing that Tier-3 offers greater granularity in descriptions compared to Tier-1. We propose three ways to verify and prove this. 

\subsection{Qualitative Analysis and Information density score} \label{appendix: information density score}
Qualitatively, one can review the data files and compare them. Each entry contains audio filepath1, audio filepath2, and an answer. This format is consistent across the data, making the difference explanations directly comparable across the ACD test set Tier-1, Tier-2, and Tier-3. Manual inspection will reveal that Tier-3 contains more information or is denser than Tier-2 compared to Tier-1. 

Quantitatively, we can construct metrics that measure the density of information per difference explanation and compare these metrics across tiers. One proxy metric is the unique vocabulary used across the three tiers, shown in data statistics Table \ref{table: difference data stats}. We can see that the unique vocabulary in Tier-3 is higher than in Tier-2 and Tier-1, indicating more vocabulary and concepts introduced in Tier-3. However, more vocabulary does not necessarily imply more information, especially concerning audio characteristics. Therefore, we propose an information density score, asking human annotators to rate each explanation between 1 and 5. Due to budget constraints, we use an LLM to simulate human evaluation and provide scores between 1 and 5. The LLM prompt used is shown in Table \ref{table: density prompt}. We use this prompt to get scores for each difference explanation across the ACD and CLD datasets. Then we average the score across each Tier of difference explanation. The results are shown in Table \ref{table: information density results}.  

\begin{table}[!ht]
\scriptsize
\center
\begin{tabular}{c|ccc} \toprule
& Tier-1 & Tier-2 & Tier-3 \\
\midrule
\makecell{Avg. \\ score} & 2.91 & 3.64 & 4.17 \\
\bottomrule
\end{tabular}
\caption{\small Information density score across the three Tiers. The Avg. density score is obtained by averaging the density score per difference explanation averaged across the ACD and CLD explanations for each Tier. } \label{table: information density results} \vspace{-0.1in}
\end{table}

The results demonstrate that Tier-3 information density scores are the highest, while Tier-1 scores are the lowest. Although this metric serves as a proxy for human evaluation, the difference of at least 0.5 between the scores, averaged over 85k examples per tier (ACD + CLD), indicates that Tier-3 explanations offer greater granularity and information in descriptions compared to Tier-1 explanations.

\subsection{Entropy of each tier}
Each Tier of the ACD and CLD dataset contains different levels of information. To measure this information one can use entropy. Entropy is a measure of uncertainty or randomness in a dataset, and computing it for a corpus at the character and word levels will provide us insights into the structure and predictability of the language. Character-level entropy quantifies the variability in how individual characters are used and combined, reflecting language complexity, encoding efficiency, and text predictability. Word-level entropy, on the other hand, captures the diversity and distribution of words, shedding light on lexical richness and the underlying linguistic patterns. To compute entropy, we use the Shannon's formula:
\begin{equation}
    H = -\sum p(x) \log_2 p(x) \label{entropy formula}
\end{equation}
where \( p(x) \) is the probability of a character or word occurring in the corpus. One note is we assume a simple unigram model, where each token's probability is independent of others. For character-level entropy, we count the frequency of each character. Normalize these frequencies to convert to valid probabilities and compute entropy by equation \ref{entropy formula}. For word-level entropy, we tokenize the corpus into words, compute their frequencies, and normalize as before to obtain probabilities. Finally, we sum the entropy contributions for each unique word and compute entropy by equation \ref{entropy formula}. 

\begin{table}[!ht]
\scriptsize
\center
\begin{tabular}{l|ccc} \toprule
Entropy & Tier-1 & Tier-2 & Tier-3 \\
\midrule
\makecell{Character} & 4.28 & 4.35 & 4.54 \\
\makecell{Word} & 7.89 & 8.01 & 8.50 \\
\bottomrule
\end{tabular}
\caption{\small Character and word entropy across the three Tiers. For the entropy calculation, we construct the corpus for each Tier. This involves combining ACD and CLD data for each Tier across the three splits of dev, val, and test. } \label{table: entropy results} \vspace{-0.1in}
\end{table}

The results are shown in Table \ref{table: entropy results}. A higher entropy at the character or word level suggests greater diversity or unpredictability in the corpus. A higher character entropy would mean the text has a wide variety of characters with a relatively even distribution (e.g., many unique letters, numbers, punctuation marks, etc). That is the text is less redundant, meaning characters appear less predictably. Similarly, a higher word entropy would mean the corpus has a rich vocabulary with words appearing in a more evenly distributed manner. That is the text is less predictable at the word level, indicating diverse topics or language use. Across datasets, the character-level entropy is relatively stable, implying the character set and its usage (letters, punctuation, spaces, etc.) don't vary significantly between corpora. The word-level entropy is higher for Tier-3, indicating more complex and varied vocabulary. Lower entropy in Tier-1 suggests more uniform and constrained language, which is expected for shorter texts or structured responses. We also computed the numbers per data source and found the CLD datasets tend to have slightly higher word-level entropy compared to the ACD datasets (e.g., detail answers have $\sim$8.52 vs. $\sim$8.43) which reflect differences in diversity of text content as ACD is sourced from AudioSet which mainly contains speech. From a model development perspective, higher word-level entropy datasets (e.g., Tier-3) will be more challenging for models, requiring greater capacity to handle vocabulary diversity. Though entropy allows us to measure uncertainty in a dataset, it does not measure semantic information, which would be measured in the density score proposed in Section \ref{appendix: information density score}. 

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{lccccccccccc}
\toprule
\makecell{Task} & \makecell{Tier} & $\text{BLEU}_1$ & $\text{BLEU}_2$ & $\text{BLEU}_3$ & $\text{BLEU}_4$ & METEOR & $\text{ROUGE}_L$ & CIDEr & SPICE & SPIDEr  & AVG \\ \midrule
ACD & 1 & 0.405 & 0.276 & 0.192 & 0.135 & 0.221 & 0.381 & 0.469 & 0.137 & 0.303 & 0.280 \\
ACD & 2 & 0.440 & 0.301 & 0.232 & 0.180 & 0.197 & 0.379 & 0.518 & 0.173 & 0.345 & 0.307 \\
ACD & 3 & 0.486 & 0.317 & 0.225 & 0.171 & 0.208 & 0.331 & 0.204 & 0.162 & 0.183 & 0.254 \\
CLD & 1 & 0.433 & 0.338 & 0.265 & 0.203 &  0.302 & 0.488 & 1.093 & 0.211 & 0.652 & 0.443 \\
CLD & 2 & 0.519 & 0.362 & 0.274 & 0.213 & 0.235 & 0.464 & 1.124 & 0.260 & 0.692 & 0.460 \\
CLD & 3 & 0.522 & 0.355 & 0.255 & 0.191 & 0.220 & 0.360 & 0.634 & 0.200 & 0.417 & 0.350 \\ \midrule
ACD & 1 & 0.411 & 0.287 & 0.207 & 0.148 & 0.217 & 0.388 & 0.220 & 0.127 & 0.174 & 0.242 \\
ACD & 2 & 0.497 & 0.365 & 0.289 & 0.234 & 0.234 & 0.437 & 0.200 & 0.222 & 0.211 & 0.299 \\
ACD & 3 & 0.500 & 0.321 & 0.226 & 0.169 & 0.211 & 0.340 & 0.117 & 0.171 & 0.144 & 0.244 \\
CLD & 1 & 0.412 & 0.295 & 0.215 & 0.152 & 0.219 & 0.402 & 0.163 & 0.119 & 0.141 & 0.235 \\
CLD & 2 & 0.491 & 0.368 & 0.296 & 0.245 & 0.230 & 0.445 & 0.156 & 0.231 & 0.194 & 0.295 \\
CLD & 3 & 0.495 & 0.320 & 0.227 & 0.171 & 0.203 & 0.337 & 0.088 & 0.167 & 0.128 & 0.237 \\ \bottomrule
\end{tabular} 
\caption{\label{table: topp topk vs greedy} 
\small ADIFF performance with top-k and top decoding vs greedy decoding. The top half of the table shows the ADIFF model performance with top-k and top-p decoding. The bottom part shows the performance of ADIFF model with greedy decoding. We observe overall improvement in performance with top-p and top-k decoding. The Tier 1, 2, and 3 classifications correspond to the explanation tiers detailed in Section \ref{sec: audio difference task tiers}. 
}
\end{table*}




