 \vspace{-0.1in}
\section{Ablations} \vspace{-0.1in} \label{sec: ablation overview}
In this section, we examine various components of model architecture and the impact of training methods on audio difference task performance. It covers- baseline architecture and the contribution of linguistics (Section \ref{subsec: baseline architecture}), effect of cross-projection layer (Section \ref{subsec: effect of cross-projection}), scaling language model (Section \ref{subsec: scaling of language model}), position captioning (Section \ref{subsec: improving modality alignment with training}), and impact of stage three fine-tuning (Section \ref{subsec: finetuning}).

\vspace{-0.1in}
\subsection{Baseline architecture} \label{subsec: baseline architecture} \vspace{-0.1in}
To build the baseline architecture for the audio difference task, we draw on previous works on prefix tuning (\cite{mspengi}). This architecture includes an audio encoder to extract features, an audio mapper to translate these features into the latent space of a language model, and a frozen language model (GPT2-base) to generate text. For the audio difference task, we use the audio encoder to extract features from both audio inputs, followed by mappers to convert these features into the language modelâ€™s latent space. The frozen language model prompting and the cross-entropy loss setup remain unchanged. We refer to this as the baseline architecture for this task.

In question-answering and captioning tasks, models have been observed to learn linguistic information and answer questions without relying on perception modalities like vision or audio. This phenomenon is noted in both vision and audio literature (\cite{clothoaqa}). Similarly, for the audio difference task, models can game metrics by learning the language patterns instead of analyzing the audio, thereby inflating the metrics. To address this, we establish a baseline where the audio encoder is randomly initialized and kept frozen. This approach allows us to determine the maximum performance achievable without analyzing audio, focusing solely on learning the language patterns for each tier of explanation. We refer to this as the language-only performance for the audio difference task.

We summarize the language-only and baseline performance in Table \ref{table: language-only and baseline}. The full table is available in Appendix Table \ref{table: appendix language-only and baseline}. Experiment A presents language-only performance, where the audio encoder is randomly initialized and frozen. In contrast, Experiment B shows baseline performance with the audio encoder initialized from HTSAT pretrained weights and frozen.  In Experiment A, we observe that Tier 2 is the easiest to learn from a linguistic standpoint, followed by Tier 3, and then Tier 1. This is because Tier 1 has the fewest words, with most containing audio-related information. Conversely, about 15\% of the words in Tier 2 pertain to the linguistic structure of contrasting audios. This suggests that Tier 2's higher scores, even in subsequent experiments, are due to its linguistic simplicity for the model to match. In Experiment B, where the audio encoder is pretrained, the model performs better across all tiers, as expected. This indicates that the model architecture effectively leverages audio information to improve explanations.

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\ \midrule
\makecell{Task} & \makecell{Exp.} & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr \\ \midrule 
\multirow{3}{*}{ACD} & \makecell{A} & 0.082 & 0.184 & 0.128 & 0.135 & 0.176 & 0.154 & 0.149 & 0.175 & 0.107 \\ 
& \makecell{B} & 0.118 & 0.210 & 0.220 & 0.163 & 0.193 & 0.225 & \textbf{0.199} & 0.188 & 0.123 \\
& \makecell{C} & \textbf{0.131} & \textbf{0.214} & \textbf{0.287} & \textbf{0.155} & \textbf{0.197} & \textbf{0.243} & 0.149 & \textbf{0.203} & \textbf{0.154} \\ \midrule 
\multirow{3}{*}{CLD} & \makecell{A} & 0.138 & 0.223 & 0.127 & 0.199 & 0.236 & 0.589 & 0.123 & 0.166 & 0.127 \\ 
& \makecell{B} & 0.128 & 0.237 & \textbf{0.212} & 0.233 & 0.234 & 0.641 & \textbf{0.157} & \textbf{0.199} & 0.166 \\ 
& \makecell{C} & \textbf{0.156} & \textbf{0.257} & 0.195 & \textbf{0.280} & \textbf{0.267} & \textbf{0.904} & 0.127 & 0.187 & \textbf{0.196} \\ \bottomrule
\end{tabular}
\caption{\small \label{table: language-only and baseline} Architecture results. Experiment A is baseline architecture with random audio encoder weights. Experiment B is pretrained audio encoder weights. Experiment C is ADIFF which modifies baseline architecture with separator token and cross-projection. The results with all metrics and the average score are available in Appendix Table \ref{table: appendix language-only and baseline}} \vspace{-0.2in}
\end{table*}

\vspace{-0.1in}
\subsection{Effect of cross-projection} \label{subsec: effect of cross-projection} \vspace{-0.1in}
We observe two limitations with the baseline model performance by qualitative analysis. First, when the audio input includes perceptually similar sounds and the same audio events, the model tends to confuse the distinct information between audios. Second, the model struggles with Tier 2 and Tier 3 explanations, where it needs to discern subtle audio differences such as scenes, acoustics, sources, and their composition rather than just audio events. To address the second limitation and partially the first, we introduce a cross-projection layer with a latent separator token between the two audio inputs. We concatenate the first audio latent embedding, the separator embedding, and the second audio latent embedding to form a prefix. The separator embedding is derived from the $<|\text{endoftext}|>$ token embedding of GPT-2 base. This concatenated prefix is then passed through the cross-projection layer, which consists of transformer layers with a learnable constant vector.

The results with the cross-projection layer are summarized in Table \ref{table: language-only and baseline}. The full table is available in Appendix Table \ref{table: appendix language-only and baseline}. Experiment B shows the performance of baseline architecture. Experiment C shows performance of baseline architecture with separator token and cross-projection layer. On average, we see improvements for ACD and CLD across the three tiers of explanations. Specifically, in Appendix Table \ref{table: appendix language-only and baseline}, the average metric score of Experiment C is consistently higher than Experiment B across all tiers of ACD and CLD. 

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\ \midrule
\makecell{Task} & \makecell{Exp.} & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr \\ \midrule 
\multirow{4}{*}{ACD} & \makecell{Base} & \textbf{0.131} & 0.214 & 0.287 & 0.155 & 0.197 & 0.243 & 0.149 & 0.203 & 0.154 \\
& \makecell{Med} & 0.119 & 0.211 & \textbf{0.325} & 0.154 & 0.198 & \textbf{0.248} & 0.160 & 0.201 & \textbf{0.232} \\
& \makecell{Large} & 0.124 & \textbf{0.221} & 0.240 & 0.156 & 0.197 & 0.147 & \textbf{0.162} & 0.207 & 0.165 \\
& \makecell{XL} & 0.129 & 0.216 & 0.272 & \textbf{0.164} & \textbf{0.202} & 0.215 & 0.154 & \textbf{0.208} & 0.107 \\
\midrule 
\multirow{4}{*}{CLD} & \makecell{Base} & \textbf{0.156} & 0.257 & 0.195 & \textbf{0.280} & 0.267 & \textbf{0.904} & 0.127 & 0.187 & 0.196 \\ 
& \makecell{Med.} & 0.149 & \textbf{0.270} & \textbf{0.317} & 0.264 & \textbf{0.287} & 0.641 & 0.143 & 0.203 & 0.204 \\ 
& \makecell{Large} & 0.133 & 0.238 & 0.263 & 0.180 & 0.223 & 0.574 & 0.177 & 0.191 & \textbf{0.227} \\ 
& \makecell{XL} & 0.112 & 0.256 & 0.275 & 0.234 & 0.255 & 0.527 & \textbf{0.146} & \textbf{0.208} & 0.212 \\
\bottomrule
\end{tabular}
\caption{\small \label{table: scale language model} Scaling language model results. The language model in ADIFF architecture (transformer decoder) ranges in size from 128 million (Base) to 1.5 billion parameters (XL). The results with all metrics and the average score are available in Appendix Table \ref{table: appendix scale language model} 
% \vspace{-0.2in}
}  \vspace{-0.2in}
\end{table*}

\vspace{-0.1in}
\subsection{Scaling language model parameters} \label{subsec: scaling of language model} \vspace{-0.1in}
The performance of vision-language models (\cite{alabdulmohsin2024getting}) tends to improve as their scale increases. Empirical evidence suggests that this improvement often follows a predictable power law (\cite{kaplan2020scaling}). Similarly, recent studies have found a correlation between the scale of language models and their performance on audio reasoning tasks (\cite{audioentail}). However, these observations are typically made at the compute-optimal frontier (\cite{hoffmann2022training}) and are not often examined under limited compute conditions. In practical scenarios, where compute resources and time are constrained, larger models may not be the best option. This has been illustrated in the case of language models, where the newer Chinchilla model (\cite{hoffmann2022training}) outperformed its predecessor Gopher (\cite{rae2021scaling}), despite being four times smaller. Therefore, we study scaling language for audio-language models for prefix tuning. 

To investigate the impact of scale, we modify the language model (transformer decoder) in the architecture depicted in Figure \ref{fig:pengi_arch}. We experiment with GPT2-base (128M), GPT2-medium (256M), GPT2-large (774M), and GPT2-XL (1.5B). Each model is trained with the same compute budget, approximately equivalent to 30 epochs. The results, presented in Table \ref{table: scale language model}, indicate that with the same data (audio and text tokens) and limited compute, the base and medium models perform similarly on average (Table \ref{table: appendix scale language model}), while the large and XL models perform worse. Moreover, the higher per-epoch computational cost of larger models widens the gap between small and large LM in ADIFF. It is important to note that we do not scale the number of training tokens as we scale the language models, which affects performance. 

\begin{wrapfigure}[11]{r}{0.40\textwidth}
\begin{center}
     \vspace{-0.2in}
     \includegraphics[width=0.40\textwidth,trim={0.0cm 0.3cm 0.22cm 0.25cm},clip]{figs/plotfig.png}
     \caption{\small Change in average score across tiers with increase in LM parameters.
     }
     \label{fig:scale}
\end{center}
\end{wrapfigure}

To determine if increased computational power helps larger LMs, we train the models longer for an additional 20 epochs. The outcomes, shown in Figure \ref{fig:scale}, indicate that for the 128M and 256M models, performance peaks around 30 epochs. In contrast, for the 774M and 1.5B models, performance continues to improve with more epochs. This suggests that aligning and guiding larger models with prefix-tuning requires a greater number of epochs. Given the computation budget and performance, we choose to use GPT2-base for ADIFF and subsequent experiments.

 \vspace{-0.1in}
\subsection{Audio grounding with position captioning} \label{subsec: improving modality alignment with training} \vspace{-0.1in}
To improve audio grounding, we instruct the model to caption either audio 1 or audio 2. This is accomplished by training the model on both audio captioning data and audio difference datasets. The audio captioning data includes \{audio1, audio2, prompt\}, where the prompt is ``caption the first audio" or ``caption the second audio". By incorporating single or position-specific captioning data in the training process, we ensure the model does not get confused between audio 1 and audio 2, and accurately distinguishes between similar or acoustically alike audio events.

Table \ref{table: audio grounding} presents the results after incorporating audio captioning data. Experiment D uses baseline architecture with cross-projection, while the bottom section shows the same architecture enhanced with captioning data. For reference, Experiment C in Table \ref{table: language-only and baseline} is the performance of baseline architecture with cross-projection. On average, the ACD dataset shows improvements across all Tiers, whereas the CLD dataset yields mixed results. This discrepancy can be attributed to AudioCaps being sourced from AudioSet, which mainly contains speech recordings. As a result, the ACD dataset includes more instances of human speech being compared under various acoustic conditions. In contrast, the CLD dataset features a wider variety of audio comparisons. Despite the mixed performance, we continue to use the model trained with audio captioning, as it demonstrates greater improvements during stage-3 finetuning. \vspace{-0.1in}

\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\ \midrule
\makecell{Task} & \makecell{Exp.} & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr \\ \midrule 
\multirow{2}{*}{ACD} & \makecell{D} & 0.129 & 0.208 & 0.300 & 0.162 & 0.195 & 0.296 & 0.164 & 0.199 & 0.150 \\
& \makecell{E} & \textbf{0.135} & \textbf{0.221} & \textbf{0.303} & \textbf{0.180} & \textbf{0.197} & \textbf{0.345} & \textbf{0.171} & \textbf{0.208} & \textbf{0.183} \\ \midrule 
\multirow{2}{*}{ACD} & \makecell{D} & 0.191 & \textbf{0.304} & 0.511 & \textbf{0.236} & \textbf{0.242} & 0.689 & 0.136 & 0.188 & 0.124 \\ 
& \makecell{E} &  \textbf{0.203} & 0.302 & \textbf{0.652} & 0.213 & 0.235 & \textbf{0.692} & \textbf{0.191} & \textbf{0.220} & \textbf{0.417} \\ \bottomrule
\end{tabular}
\caption{\label{table: audio grounding} \small Audio grounding and finetuning results. Experiment D is the ADIFF model with position captioning. Experiment E finetunes the language model of ADIFF along with position captioning. The results with all metrics and the average score are available in Appendix Table \ref{table: appendix audio grounding}\vspace{-0.1in}}
\end{table*}

\vspace{-0.1in}
\subsection{Stage-3 finetuning} \label{subsec: finetuning} \vspace{-0.1in}
In the final stage, following multimodal grounding, we fine-tune the language model. This training is conducted over a few epochs with a small learning rate of approximately 1e-6, using a cosine learning rate scheduler. The results of this fine-tuning are presented in Table \ref{table: audio grounding}. Experiment D is position captioning training, while Experiment C shows the performance post-fine-tuning. The final fine-tuning consistently improves performance across various tiers and datasets. We also find that the finetuned model outperforms the base model across three tiers, especially Tier 3. More details on qualitative analysis are available in Appendix \ref{appendix: stage 3 finetune}.

\vspace{-0.1in}
\section{Hallucination} \vspace{-0.1in}
\begin{wrapfigure}[18]{r}{0.5\textwidth}
\begin{center}
     \vspace{-0.15in}
    \includegraphics[width=0.5\textwidth]{figs/specex2.png}
     \caption{\small Audio event presence probabilities from ADIFF to detect hallucinations. 
     % \vspace{-0.08in}
     }
     \label{fig:audio events and hallucination}
\end{center}
\end{wrapfigure}
Language models often generate misleading or incorrect outputs, known as hallucinations. For Audio-Language Models (ALMs), this means producing responses not based on actual audio input, such as inventing audio events or misinterpreting common sounds (\cite{kuan24_interspeech, nishimura2024audio}). This lack of grounding can impair deductive reasoning and comparative skills. To address this, we use audio grounding with position captioning to differentiate similar-sounding events from different sources. However, some hallucination issues persist. We aim to provide tools to detect these hallucinations by keeping the HTSAT audio encoder frozen. This encoder, trained on AudioSet, predicts 527 audio events and their presence probabilities over time, allowing users to verify the accuracy of generated descriptions. An example of this is shown in Figure \ref{fig:audio events and hallucination}. The top pane displays the audio difference explanation generated by the model. For each audio, the log mel spectrogram and the top three audio event presence probabilities over time are plotted. From the figure, we can see that the model missed the whip sound in audio 2 in the difference explanation and provides a way of analyzing explanations.