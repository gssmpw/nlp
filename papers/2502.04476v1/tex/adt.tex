\vspace{-0.1in}
\section{Audio Difference Explanation} \label{sec: audio difference task tiers} \vspace{-0.1in}
The task of explaining audio differences consists of finding the difference between two audio files and explaining the difference in natural language. Human explanations draw on various sources of information, including acoustic details and human perception, as well as linguistic nuances. From an acoustic standpoint, explanations can range from broad audio event differences to finer-grained signal variations. They can also be based on objective facts and world knowledge or on human perceptual differences. Linguistically, these explanations can be concise, like a single sentence, or more detailed, like a paragraph. This allows us to segregate audio difference explanations that vary in detail, focus, and style:

% \begin{enumerate} 
\noindent\textbf{Concise}: This description is concise and straightforward, briefly mentioning the key characteristics of each audio without much elaboration. It highlights the main differences in ambiance and focus between the two audios. We refer to this as the tier-1 explanation. \\
\textbf{Brief}: This answer provides more detail and context. It not only describes the sounds but also includes audio events, sound sources, the nature of the sounds, making it slightly more analytical. It also describes the sequence of sounds (temporal order) and potential scene differences across audios. We refer to this as the tier-2 explanation. \\
\textbf{Detail}: This is the most detailed and descriptive answer. It delves into the sonic qualities of each audio, including potential sources and the listenerâ€™s experience. It compares the two audios in terms of complexity and engagement, providing a richer and more immersive description. It analyzes the audio events, acoustic scenes, sound sources, signal characteristics, tonal differences, and overall feel of each audio. We refer to this as the tier-3 explanation. 

The first-tier explanation is concise and to the point, the second-tier explanation provides more context, and the third explanation offers a thorough, immersive analysis. A secondary benefit of dividing the explanation into tiers allows us to do fine-grained analysis of model performance. 

\begin{wrapfigure}[19]{r}{0.5\textwidth}
\begin{center}
     \vspace{-0.2in}
     \includegraphics[width=0.5\textwidth]{figs/examples.png}
     \caption{\small A random sample from the ACD dataset is displayed across three levels of explanation. The top pane provides a concise explanation, the middle pane offers a brief explanation, and the bottom pane presents a detailed explanation.
     \vspace{-0.2in}
     }
     \label{fig:examples}
\end{center}
\end{wrapfigure}

\subsection{Audio Difference Dataset} \vspace{-0.1in} \label{sec: audio difference dataset}
In this section, we outline the development steps for the AudioCaps Difference (ACD) and Clotho Difference (CLD) datasets.

\noindent \textbf{Audio recordings.} We source the audio recordings from the AudioCaps and ClothoV21 datasets. The AudioCaps dataset comprises 46,000 audio samples, each lasting 10 seconds, sourced from AudioSet, and includes human-annotated captions that describe the audio content. Annotators had access to both audio and visual cues during the annotation process. On the other hand, the Clotho dataset, though smaller, features audio samples ranging from 15 to 30 seconds, sourced from the Freesound platform, and includes five human-annotated captions per sample. These annotators only had access to audio information and followed a detailed protocol to minimize errors and ensure diversity in the captions. By utilizing both the AudioCaps and Clotho, we ensure variability in audio content, duration, and annotation style.

\noindent \textbf{Difference explanations.} 
Large Language Models (LLMs) have been effectively utilized to generate text descriptions for audio across various tasks, such as question-answering (\cite{ltu}), compositional reasoning (\cite{compa,ghosh2024gama}), and deductive reasoning (\cite{audioentail}). We adopt a similar approach for generating explanations for the audio difference task. The process involves three main steps: data sources, explanation generation, and explanation verification.

For data sources, we limit the audio recordings and human-annotated descriptions to those from AudioCaps and ClothoV21. To generate explanations, we prompt an LLM to describe the differences between two audio recordings using the provided human-annotated descriptions. Our prompting setup is similar to (\cite{audioentail}), with two key modifications. First, we instruct the LLM to incorporate knowledge about sound sources, materials, acoustics, and emotions when generating explanations. Second, we define the tier of explanation by restricting the sources the LLM can use and the length of the explanation. Therefore, we sample two annotated human descriptions and prompt LLM to generate different tiers of explanations. Finally, human annotators verify the difference explanations. If an explanation is found to be inaccurate, the human annotators manually remove the hallucinated audio event and add necessary details. Due to the cost of this process, we restrict verification to the test set of three tiers. Details on prompting setup and data creation are available in the Appendix \ref{appendix: llm gen}. The resulting dataset comprises of audio recordings paired with the three tiers of generated explanations.

\begin{table}[!ht]
\scriptsize
\center
\begin{tabular}{c|c|c|ccc|ccc|ccc} \toprule
 & & & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\
Data & Split & \makecell{Examples\\per tier} & Med. & Max & Vocab. & Med. & Max & Vocab. & Med. & Max & Vocab. \\
\midrule
CLD & Train & 19195 & 27 & 49 & 6528 & 51 & 92 & 6462 & 155 & 221 & 10818\\
CLD & Val & 5225 & 27 & 46 & 3743 & 51 & 89 & 4024 & 154 & 223 & 7026\\
CLD & Test & 5225 & 27 & 28 & 5225 & 52 & 86 & 4059 & 156 & 219 & 7152\\ \midrule
% ACE & train & - & - & - \\
% ACE & val & - & - & - \\
ACD & Train & 48660 & 29 & 47 & 3287 & 52 & 104 & 8483 & 155 & 235 & 12891\\
ACD & Val & 2456 & 28 & 46 & 2350  & 52 & 87 & 2563 & 154 & 227 & 4566\\ 
ACD & Test & 4680 & 29 & 47 & 3287 & 53 & 95 & 3329 & 154 & 220 & 5489 \\ \bottomrule
\end{tabular}
\caption{\small Dataset statistics of AudioCaps Difference (ACD) and Clotho Difference (CLD) dataset} \label{table: difference data stats} \vspace{-0.1in}
\end{table}

\noindent \textbf{Dataset statistics.} The Audio difference dataset consists of $\{a_i, a_j, e_{ij}\}$ where $a_i$ is first audio, $a_j$ is the second audio, and $e_{ij}$ is explanation belonging to one of the three tiers. The statistics of AudioCaps Difference (ACD) and Clotho Difference (CLD) datasets across three tiers: Train, Validation, and Test splits are presented in Table \ref{table: difference data stats}. For example, in Tier 1, the ACD Train split has 48k examples with explanations having a median of 27, a maximum of 49, and a vocabulary size of 6528, while the CLD Train split has 19k examples and the explanations have a median of 51, a maximum of 92, and a vocabulary size of 6462. A randomly sampled example is shown in Figure \ref{fig:examples}.

 \vspace{-0.1in}
\subsection{Evaluation} \label{subsec: metrics} \vspace{-0.1in}
For objective evaluation, we use audio captioning metrics- BLEU (\cite{bleu}), METEOR (\cite{meteor}), SPICE (\cite{spice}), CIDEr (\cite{cider}), and SPIDEr (\cite{spider}). BLEU measures n-gram precision, while METEOR combines precision and recall with features like stemming and synonymy matching. ROUGE focuses on recall of n-grams and sequences, CIDEr measures consensus in audio descriptions, and SPICE evaluates semantic content. SPIDEr combines SPICE and CIDEr for a balanced assessment, making it our primary metric for model comparison. More details on metric choice are available in Appendix \ref{appendix: evaluation}.