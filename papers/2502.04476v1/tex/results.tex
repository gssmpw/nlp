
\begin{table*}[!ht]
\scriptsize
\center
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule
& & \multicolumn{3}{c|}{Tier 1} & \multicolumn{3}{c|}{Tier 2} & \multicolumn{3}{c}{Tier 3} \\ \midrule
\makecell{Task} & \makecell{Models} & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr & $\text{BLEU}_4$ & METEOR & SPIDEr \\ \midrule 
\multirow{4}{*}{ACD} & Baseline & 0.118 & 0.210 & 0.220 & 0.163 & 0.193 & 0.225 & 0.153 & 0.188 & 0.123 \\ 
& QwenAC (L) & 0.132 & 0.214 & 0.235 & 0.166 & \textbf{0.204} & 0.212 & 0.165 & 0.202 & 0.173 \\ 
& QwenAC (F) & 0.110 & 0.183 & 0.258 & 0.163 & 0.199 & 0.241 & 0.151 & 0.194 & 0.082 \\
\rowcolor[HTML]{EFEFEF} & ADIFF & \textbf{0.135} & \textbf{0.221} & \textbf{0.303} & \textbf{0.180} & 0.197 & \textbf{0.345} & \textbf{0.171} & \textbf{0.208} & \textbf{0.183} \\ \midrule 
\multirow{4}{*}{CLD} & Baseline & 0.128 & 0.237 & 0.212 & 0.233 & 0.234 & 0.641 & 0.157 & 0.199 & 0.166\\ 
& QwenAC (L) & 0.140 & 0.285 & 0.230 & 0.232 & 0.236 & 0.756 & 0.155 & 0.200 & 0.182 \\ 
& QwenAC (F) & 0.126 & 0.232 & 0.204 & \textbf{0.273} & \textbf{0.254} & \textbf{0.958} & 0.130 & 0.207 & 0.172 \\ 
\rowcolor[HTML]{EFEFEF} & ADIFF & \textbf{0.203} & \textbf{0.302} & \textbf{0.652} & 0.213 & 0.235 & 0.692 & \textbf{0.191} & \textbf{0.220} & \textbf{0.417}\\ \bottomrule
\end{tabular}
\caption{\label{table: audio difference results} 
\small Benchmarking different models on Audio Difference task. The top half of the table shows the performance of different models on the task of ACD, while The bottom half of the table shows the performance of different models on the task of CLD. All the models are trained on ACD and CLD train split. The Tier 1, 2, and 3 classifications correspond to the explanation tiers detailed in Section \ref{sec: audio difference task tiers}. The results with all metrics and the average score are available in Appendix Table \ref{table: appendix audio difference results}. \vspace{-0.1in}}
\end{table*}

\vspace{-0.1in}
\section{Results} \label{sec:results}

\vspace{-0.1in}
\subsection{Experimental Setup} 
\vspace{-0.1in}
The base model includes an audio encoder, audio projection, and a language model (GPT2-base). It is trained end-to-end with the language model frozen, using captioning loss. This is referred to as the baseline or naive model. For ADIFF, we add a separator and a cross-projection layer to the architecture. ADIFF’s training process has three stages to preserve audio information and prevent destructive gradient updates during unaligned full finetuning: unimodal pretraining, multimodal alignment training, and finetuning, as detailed in Section \ref{subsec: training stages}. For SoTA ALMs, we use Qwen-AC (\cite{qwenaudio}) as it is the only ALM in literature that supports two audio inputs. We consider three versions of Qwen-AC: Zero-Shot (Z), LoRA (L), and full fine-tuning (F). All models except for the zero-shot version use the train set of ACD and CLD for training and are evaluated on the test set of ACD and CLD across three tiers.

\subsection{Results} 
\vspace{-0.1in}
\textbf{Objective evaluation.} The performance of the naive model, QwenAC versions and ADIFF model is compared in Table \ref{table: audio difference results}, using objective metrics described in Section \ref{subsec: metrics}. The ADIFF model outperforms the naive baseline across all three tiers. Moreover, it also outperforms QwenAC (L) and QwenAC (F) across all Tiers of ACD and CLD except on Tier 2 CLD. For Tier 2, we observe similar trends as in ablation conducted for baseline architecture and language-only performance \ref{subsec: baseline architecture}. For Tier 2, we observe similar trends to those seen in the ablation studies conducted for the baseline architecture and language-only performance. Linguistically, Tier 2 is the easiest to learn from, followed by Tier 3, and then Tier 1. This is because Tier 1 has the fewest words, with most of them containing audio-related information. In contrast, about 15\% of the words in Tier 2 pertain to the linguistic structure of contrasting audios. This indicates that Tier 2's higher scores, even in subsequent experiments, are due to its linguistic simplicity for the model to match. Consequently, QwenAC, which uses a much stronger 7B parameter LLM, performs better than ADIFF, which has a 128M parameter LLM.

\textbf{Subjective evaluation.} The objective metrics are limited because they rely on linguistic biases, which can unfairly penalize diverse outputs (\cite{morato2021diversity,mei2024towards}). Therefore, we also perform subjective evaluation, where human annotators rate the audio difference explanations across different dimensions of correctness (1-5), granularity (1-5), and readability (1-5). The dimension definitions and human evaluation setup and is explained in Appendix \ref{appendix: human evaluation}. The subjective evaluation also ensures a fair comparison with QwenAC (Z), which is not trained on the ACD and CLD datasets and thus do not align with the data distribution, leading to poorer objective metric scores. We evaluate the models across three scenarios. The first scenario, Studio, involves recordings from the same sources, such as different breeds and numbers of dog barks, all recorded in a professional studio. The second scenario involves random samples from FSD50K (\cite{fsd50k}). The third scenario uses random samples from GTZAN Genres (\cite{gtzan}), featuring music from various genres and settings. The subjective evaluation results are shown in Table \ref{table:human_evaluation_study}. Across all scenarios, ADIFF outperforms the naive baseline and Qwen-AC (Z) across all metrics and domains.  Specifically, ADIFF sees the largest improvement in the granularity metric, highlighting the model's ability to produce in-depth descriptions. ADIFF also beats the finetuned variants of Qwen-AC on average across all metrics. However, in two cases—readability on Studio and granularity on FSD50K—a fine-tuned version of Qwen-AC outperforms ADIFF. This is likely because Qwen-AC's LLM, Qwen-7B, is significantly larger than the 128M LLM of ADIFF. Qualitative analysis shows it produces more coherent output, slightly improving readability. Additionally, we highlight the difficulty of the task, as even ADIFF achieves average scores of 3.5 across the dimensions out of 5.

\begin{table}
\scriptsize
\centering
\begin{tabular}{l|l|lll|lll|lll|lll} 
\toprule
 & & \multicolumn{3}{c|}{Studio} & \multicolumn{3}{c|}{FSD50K} & \multicolumn{3}{c}{GTZAN} & \multicolumn{3}{|c}{Average}\\
\midrule
\quad Model & LLM & COR & GRA & RDB & COR & GRA & RDB  & COR & GRA & RDB & COR & GRA & RDB \\
\midrule
QwenAC (Z) & 7B & 2.73 & 2.64 & 3.09 & 2.76 & 2.20 & 3.28 & 2.95 & 2.83 & 3.33 & 2.81 & 2.56 & 3.23 \\
Baseline & 128M & 2.99 & 3.26 & 3.21 & 3.43 & 3.36 & 3.47 & 3.37 & 3.28 & 3.49 & 3.26 & 3.35 & 3.39 \\
QwenAC (L) & 7B & 3.05& 3.41& 3.25& 3.35& \textbf{3.73} & 3.45& 3.31& 3.29& 3.63& 3.24& 3.48& 3.44 \\
QwenAC (F) & 7B & 3.09 & 3.50 &\textbf{3.37}& 3.41& 3.54& 3.49& 3.35& 3.32& 3.61& 3.28& 3.45& 3.49 \\ \midrule
\rowcolor[HTML]{EFEFEF} ADIFF & 128M & \textbf{3.12} & \textbf{3.73} & 3.34 & \textbf{3.82} & 3.51 & \textbf{3.61} & \textbf{3.46} & \textbf{3.34} & \textbf{3.75} & \textbf{3.47} & \textbf{3.53} & \textbf{3.57} \\
\bottomrule
\end{tabular}
\caption{\small Human evaluation for generated explanations based on Correctness (COR), Granularity (GRN), and Readability (RDB). For QwenAC, Z indicates zero-shot, L indicates LoRA finetune, and F indicates full-finetuning. \vspace{-0.2in}}
\label{table:human_evaluation_study}
\vspace{-0.1in}
\end{table}