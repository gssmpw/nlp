 \vspace{-0.2in}
\section{Model} \vspace{-0.1in} \label{sec: adiff}
In this section, we describe our proposed model ADIFF, which employs prefix tuning to prompt a frozen Language Model. The model accepts three inputs: audio 1, audio 2, and a user prompt, and generates free-form text as output. The model architecture is covered in Section \ref{subsec: architecture}, the training process in Section \ref{subsec: training}, the three stages of training in Section \ref{subsec: training stages}, and lastly inference in Section \ref{subsec: inference}. 

\begin{figure}[!ht]
   \centering
     \includegraphics[width=0.8\textwidth,trim={0.0cm 0.28cm 0.22cm 0.25cm},clip]{figs/crossprefixarch2.png}
     \caption{\small ADIFF takes two audio recordings and text prompt as input and generates free-form text as output. The two audios and prompt are independently encoded by the audio encoder and text embedder respectively, followed by projection layers to project the embeddings to the latent space of the transformer decoder. The two audio latent are separated by a separator token in latent space. The prefix formed by audio latent 1, SEP, audio latent 2 and text prompt prefix is fed to the cross-projection layer. The output of the cross-projection layer is used to prompt the transformer decoder to generate natural language explanations.
     \vspace{-0.1in}
     }
     \label{fig:pengi_arch}
\end{figure}

\vspace{-0.1in}
\subsection{Architecture} \label{subsec: architecture} \vspace{-0.1in}
ADIFF consists of four main components: an audio encoder, an audio projection layer, a cross-projection layer, and a decoder-only language model.

\noindent \textbf{Audio Encoder.} The audio encoder is used for extracting general-purpose audio representations for both the audios. We use HTSAT (\cite{chen2022hts}) which is pretrained on AudioSet (\cite{audioset}) and achieves SoTA on identifying various sound events and acoustic scenes.

\noindent \textbf{Projection.} The audio projection is used to project the audio embeddings from audio encoder to latent space of Language model. The audio projection converts a single embedding into a sequence of latent tokens $[s, d]$. The projection first expands the hidden dimension to a larger hidden dimension $k$, which is then split to form $[s, d]$ where $k = s * d$. This is followed by concatenating with a learnable constant, resulting in $[s + c, d]$. This output is passed to the transformer, followed by clipping of the learnable constant output $c$. The resulting output of audio projection is of shape $[s, d]$. This projection architecture is shown to perform well for prefix-tuning architectures (\cite{mspengi, noaudiocap, mokady2021clipcap}). The text projection translates text embeddings with learnable transformer layers. The architecture is similar to audio projection (Fig \ref{fig:pengi_arch}) without the first linear layer. 

\noindent \textbf{Cross Projection.} Once the audio embeddings are in the same latent space of Language model, the cross-projection layer is used to improve the model's ability to highlight differences. The cross-projection layer adds a separator token in latent space between the two audio embeddings followed by transformer layer to learn differences.  

\noindent \textbf{Language Model.} The language model is a decoder-only language model which is used to auto-regressively generate text conditioned on the output of cross-projection and user prompt. We use GPT2 as the language model following prefix tuning literature (\cite{mspengi, noaudiocap, mokady2021clipcap}) and due to compute limitations. 

\vspace{-0.1in}
\subsection{Training} \label{subsec: training} \vspace{-0.1in}
The training process uses the next-token prediction objective to learn unfrozen (learnable) parameters. A sample input to the model is \{$x_1^i$,$x_2^i$,$t^i$,$c^i$\} where $x_1^i$ is the first audio, $x_2^i$ is the second audio, $t^i$ is the text prompt, and $c^i$ is the difference explanation respectively. The user provides Audio 1 ($x_1^i$) and Audio 2 ($x_2^i$) along with a prompt ($t^i$) specifying the desired level of explanation. Each audio file is independently encoded by the audio encoder ($a_\phi$), creating separate audio embeddings. Simultaneously, the user’s text prompt and separator token ($s$) is tokenized using a BPE tokenizer and embedded with the language model’s vocabulary embeddings ($g_\psi$). These embeddings including the audios, separator embedding, and text prompt embeddings are then projected into the language model’s latent space using a cross-projection layer ($m_\zeta$) layer to form a prefix. 
\vspace{-0.05in}
\begin{equation}
    c^i = c^i_1,...,c^i_{k} = \text{concat}\{m_\zeta(a_\phi(x_1^i)), g_\psi(s),m_\zeta(a_\phi(x_2^i)),g_\psi(t^i)\} \label{equation: prefix}
    \vspace{-0.05in}
\end{equation}
\vspace{-0.05in}
\begin{equation}
    p^i_1,...,p^i_{k} = h_\beta(c^i) \label{equation: cross prefix} 
\end{equation}
The combined prefix $\{p^i_j\}_{j=1}^k$ is of length (tokens) $k$ in the language models space. This prefix is used to prompt the language model ($f_\theta$) to generate a text explanation highlighting the differences between the two audios. The model is trained as a typical captioning system, learning to predict a caption (text tokens) $o^i$ based on the prefix $p^i$ in an autoregressive manner. The loss function is Cross-Entropy per token:
\begin{equation}
% \vspace{-0.05in}
\mathcal{L} = - \sum_{i=1}^N \sum_{j=1}^{l} \log p_{\gamma} (o^i_j| p^i_1,...,p^i_{k}, o^i_1,...,o^i_{j-1}) 
% \vspace{-0.05in}
\end{equation}
where $\gamma$ denotes model's trainable parameters and consists of $\phi,\zeta,\psi,\beta$. Out of all the parameters- $\zeta,\beta$ are always trained while the rest are determined per stage (Section \ref{subsec: training stages}).

\subsection{Training stages} \label{subsec: training stages} \vspace{-0.1in}
The model’s training process involves several stages to preserve audio information and prevent the destructive gradient updates  during unaligned full finetuning. These stages are: unimodal pretraining, multimodal alignment training, and finetuning.

\noindent \textbf{Unimodal Pretraining.} Unimodal pretraining entails training the audio encoder and language model on tasks specific to their respective modalities. For the audio encoder, this means training on AudioSet to predict audio events. For the language model, it involves pretraining on a large corpus of text. For the audio difference task, we skip independent modality pretraining and instead use pretrained unimodal models like HTSAT and GPT-2 which have performed the unimodal training.\\
\textbf{Multimodal grouding.} After unimodal pretraining, the language model must be grounded in the audio to generate audio-conditioned text. During this stage, both the audio encoder and language model are frozen, and the model learns the audio projection and cross projection using cross-entropy loss for each predicted next text token. Freezing the audio encoder and language model ensures that the initial gradient updates do not cause the loss of modality-specific information. This approach is similar to the prefix-tuning literature, where the language model remains frozen (\cite{mspengi,mokady2021clipcap}).\\
\textbf{Finetuning.} In this final stage, we finetune the audio-grounded language model by unfreezing all the modules. This allows us to retain audio-specific information and better steer the language model to produce the necessary descriptive answers using acoustic terminology. This stage uses a low learning rate with warmup and minimum training steps to minimize catastrophic forgetting.

 \vspace{-0.1in}
\subsection{Inference} \label{subsec: inference} \vspace{-0.1in}
During inference, the prefix is created from the two test audios and the user-provided text prompt. The decoder (language model), generates the next token in sequence based on this prefix. At each step, the model assigns probabilities to all vocabulary tokens, which are then used to select the next token according to the chosen decoding method. In our experiments, we employed top-k and top-p decoding for better performance across all experiments. The details on experimental setup, training, inference, and implementation can be found in Appendix \ref{appendix: exp setup}.