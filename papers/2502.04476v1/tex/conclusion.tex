\vspace{-0.1in}
\section{Conclusion} \vspace{-0.1in}
In this paper, we introduce the task of audio difference explanation, create relevant datasets, and propose an strong baseline. We present two new datasets from AudioCaps and Clotho. Using Large Language Models (LLMs), we generated three levels of explanations for audio differences: concise descriptions, brief sentences about events and scenes, and comprehensive explanations including semantics and emotions. We propose ADIFF, an audio prefix tuning-based language model with a cross-projection module and a three-step training process to address the limitation of the naive baseline. The objective and subjective evaluations show significant performance improvements over the naive baseline and SoTA ALM. We also conduct ablation studies to understand the effects of various components and find various insights relevant to training audio-language models. Our benchmarks and ablation studies show the effectiveness of our method, utilizing comparative reasoning to generate more human-like explanations for audio differences.


