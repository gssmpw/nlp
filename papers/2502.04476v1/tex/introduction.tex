\section{Introduction} \vspace{-0.1in}

\begin{wrapfigure}[17]{r}{0.40\textwidth}
\small
\begin{center}
     \vspace{-0.2in}
     \includegraphics[width=0.40\textwidth,trim={0.0cm 0.3cm 0.4cm 0.4cm},clip]{figs/motiv5.png}
     \caption{\small Humans use auditory information to compare scenes and make deductions.
     % \vspace{-0.08in}
     }
     \label{fig:motiv}
\end{center}
\end{wrapfigure}

In the Boston Marathon bombing of 2014, forensic investigators were faced with a challenge -- were the various audio recordings purportedly captured of the event (and put up on social media) by various people recordings of the same event, or were they mistaken or fraudulent uploads actually from different events? This anecdote highlights the complexity of audio analysis. One can have different sounding recordings of the same event, similar sounding recordings of different events, and other combinations. To know the relation, one must be able to describe the recordings in relative or comparative terms. This natural human ability is used in various contexts- in audio forensics to verify recording authenticity (\cite{af1,af2}), crucial for legal investigations. In production and broadcasting, it aids in Audio Quality Assessment (\cite{campbell2009audio, deshmukh2024pam}) to detect subtle variations. In audio generation (\cite{liu2023audioldm, kreukaudiogen}), it helps create realistic synthetic audio. Currently, identifying audio differences requires a human listener with expertise in phonetics (\cite{johnson2004acoustic}), the acoustic-phonetic approach (\cite{stevens2000acoustic}), and spectrogram analysis to examine recordings and identify differences across various parameters.

Recent advancements in audio-text learning has enabled new applications such as text-to-audio retrieval (\cite{koepke2022audio, deshmukh23_interspeech}), audio captioning (\cite{mei2022automated, noaudiocap}), and audio question answering (\cite{clothoaqa, ltu}). To excel in these applications, models must possess both fine-grained and coarse-grained understanding of audio, enabling them to describe audio content in human-like natural language. By pretraining models on multiple audio-text tasks, individual audio-text tasks see a benefit in performance improvement (\cite{mspengi}). Therefore, the field has shifted towards training audio models capable of performing the aforementioned audio-text tasks using a single model. Examples of such Audio-Language Models (ALM) include SALMONN (\cite{salmonn}), GAMA (\cite{ghosh2024gama}), and LTU (\cite{ltu, ltuas}). However, the current literature has not addressed the task of audio difference explanation. This is due to the challenges in generating coherent paragraph-level descriptions, the lack of cognitive processing and understanding similar to human auditory perception, and the absence of annotated datasets specifically designed for this task. 

Explaining the differences between two audio samples also acts as an effective benchmark for assessing a model’s comparative reasoning and its ability to integrate audio information with world knowledge. When a model is tasked with distinguishing between two audios, it must employ comparative reasoning to identify both subtle and significant details. This requires the model to understand the fundamental properties of audio signals, such as frequency, amplitude, and temporal patterns, as well as determining pitch, timbre, and loudness. Beyond these signal characteristics, the model must also use world-knowledge to grasp contextual elements, like the genre of music or the type of environment where the audio was recorded. Finally, the model must use comparative and deductive reasoning to interpret and compare these features, identifying nuanced differences and similarities, and draw conclusion. This makes audio difference explanation an effective benchmark for evaluating audio-text models and exploring methods for integrating audio information, world knowledge, and comparative reasoning. 

In this paper, our main contributions are: \vspace{-0.1in}
\begin{itemize}
    \item Introduce the audio difference explanation task which aims to provide natural language explanation for the differences in two audio files. To build and benchmark models for this task, we create two datasets ACD and CLD, where the difference explanation is generated by LLM using human annotated captions and later verified by human annotators (test set). To mimic human explanations, each dataset contains three tiers of explanation ranging from one-sentence (audio events) to detailed explanations (scene, semantic, listener emotions).
    \item We propose a naive baseline for this task using prefix-tuning. In this approach, audio embeddings from the two audio files are used to prompt a frozen language model. To address the limitations of this naive baseline, we introduce the ADIFF model, which incorporates separator token, cross-projection layer and undergoes a three-stage training process. These enhancements improve the model’s ability to identify detailed differences and distinguish perceptually similar sounds, resulting in a performance improvement over the naive baseline and SoTA ALM (Section \ref{sec:results}). The checkpoint will be publicly released\footnote{
Dataset and pretrained model are available at \url{https://github.com/soham97/ADIFF}}. 
    \item Under the proposed framework, we conduct several ablation studies to understand the impact of cross-projection, language model scaling, audio-grounding, and fine-tuning. Our findings reveal that (1) cross-projection aids in utilizing text prefixes to store difference attributes, (2) under limited compute and data, smaller language models are easier to ground in audio with proper training and (3) position-based audio captioning with multiple audio inputs enhances the model’s performance on similar-sounding acoustic sources. Lastly, we address language model hallucinations by enabling users to detect inaccuracies through comparisons between generated descriptions and predicted audio event probabilities.
\end{itemize}