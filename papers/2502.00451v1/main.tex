\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage[T1]{fontenc}
\title{Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities}

\author[1]{Aishik Mandal}
\author[2,3,*]{Tanmoy Chakraborty}
\author[1,*]{Iryna Gurevych}
\affil[1]{Ubiquitous Knowledge Processing Lab (UKP Lab)\\
Department of Computer Science and Hessian Center for AI (hessian.AI)\\
Technische Universität Darmstadt}
\affil[2]{Department of Electrical Engineering, Indian Institute of Technology Delhi, India}
\affil[3]{Yardi School of Artificial Intelligence, Indian Institute of Technology Delhi, India}

 \affil[*]{Corresponding author: Tanmoy Chakraborty (tanchak@iitd.c.in) and Iryna Gurevych (iryna.gurevych@tu-darmstadt.de)}

% \affil[+]{these authors contributed equally to this work}

%\keywords{Keyword1, Keyword2, Keyword3}

\begin{abstract}
Mental illness is a widespread and debilitating condition with substantial societal and personal costs. Traditional diagnostic and treatment approaches, such as self-reported questionnaires and psychotherapy sessions, often impose significant burdens on both patients and clinicians, limiting accessibility and efficiency. Recent advances in Artificial Intelligence (AI), particularly in Natural Language Processing and multimodal techniques, hold great potential for recognizing and addressing conditions such as depression, anxiety, bipolar disorder, schizophrenia, and post-traumatic stress disorder. However, privacy concerns, including the risk of sensitive data leakage from datasets and trained models, remain a critical barrier to deploying these AI systems in real-world clinical settings. These challenges are amplified in multimodal methods, where personal identifiers such as voice and facial data can be misused. This paper presents a critical and comprehensive study of the privacy challenges associated with developing and deploying AI models for mental health. We further prescribe potential solutions, including data anonymization, synthetic data generation, and privacy-preserving model training, to strengthen privacy safeguards in practical applications. Additionally, we discuss evaluation frameworks to assess the privacy-utility trade-offs in these approaches. By addressing these challenges, our work aims to advance the development of reliable, privacy-aware AI tools to support clinical decision-making and improve mental health outcomes.
\end{abstract}
\begin{document}

\flushbottom
\maketitle
% * <john.hammersley@gmail.com> 2015-02-09T12:07:31.197Z:
%
%  Click the title above to edit the author information and abstract
%
\thispagestyle{empty}

% \noindent Please note: Abbreviations should be introduced at the first mention in the main text – no abbreviations lists. Suggested structure of main text (not enforced) is provided below.

\section*{Introduction}

Mental disorders are highly prevalent and represent a major cause of disability worldwide. The societal, economic, and personal impacts of mental health issues make swift diagnosis and treatment essential. Current diagnostic methods involve self-reported questionnaires and clinical interviews, while treatment typically consists of multiple therapy sessions with trained therapists. This approach requires therapists to dedicate substantial time to each patient, limiting their ability to treat a larger number of individuals. Combined with a shortage of trained therapists, this often leaves many patients undiagnosed. Additionally, completing self-reported questionnaires after every therapy session places a significant burden on patients.

These issues have driven the development of systems aimed at automating diagnosis and assisting therapists in treating mental disorders. Therapists often rely on various multimodal cues to diagnose mental illnesses. For example, depression exhibits distinct verbal and non-verbal characteristics, such as facial expressions \cite{slonim2023facing,5349358,SCHERER2014648}, prosodic features \cite{cummins2015review,5349358,SCHERER2014648}, and semantic patterns \cite{chim-etal-2024-overview}. Similarly, patients with anxiety often struggle to maintain eye contact \cite{langer2017social,app122312298}, particularly during conflict-laden conversations. Speech features are instrumental in detecting Post-Traumatic Stress Disorder (PTSD) \cite{kathaneffect,HU2024859}, while both speech and facial features are valuable for identifying Bipolar Disorder \cite{bipolar_speech,gilanie2024robust}. Consequently, multimodal AI models capable of analyzing text, audio, and video data are being developed to automate diagnosis and support therapists in managing mental health conditions.

% With Natural Language Processing methods performing well in tasks like dialogue act classification and emotion recognition, its potential for diagnosing mental health illnesses was recognised. Initial works on applying NLP methods to the mental health domain included recognising mental illnesses like major depression disorder \cite{guntuku2017detecting}, anxiety \cite{shen-rudzicz-2017-detecting}, bipolar disorder \cite{KADKHODA2022101042,lee-etal-2024-detecting-bipolar}, schizophrenia \cite{schizo_social,mitchell2015quantifying} and PTSD \cite{Coppersmith_Harman_Dredze_2014} from social media data. However, social media data is not clinically grounded, and thus, NLP models trained on such data are not suitable for real-world applications\cite{info:doi/10.2196/jmir.7215}.

Training such multimodal models requires multimodal data, typically obtained from recorded therapy sessions. However, collecting such data and training models face significant challenges due to privacy concerns. Data collection must comply with regulations like the General Data Protection Regulation (GDPR) \cite{gdpr} and the Health Insurance Portability and Accountability Act (HIPAA) \cite{act1996health}, which prohibit releasing information that could disclose a person’s gender, age, or identity. Therapy session recordings inherently contain sensitive personal information, including patients’ voices and facial features, which could be misused for impersonation \cite{av_deepfake}. To ensure privacy, datasets are kept confidential, but many patients remain reluctant to record their sessions due to insufficient privacy guarantees for both the data and the models trained on it. As a result, available multimodal datasets are often small, which leads to biased models when used for training. Such small datasets also restrict the evaluation of the models’ generalizability and reliability. Furthermore, the trained model weights cannot be released, as they may inadvertently reveal private training data \cite{membership-inference-attack, embedding-data-leakage,nn-memorise, fine-tune-data-leak}. Such privacy breaches could expose patients’ identities or enable impersonation, potentially worsening their mental health. These challenges significantly hinder the development and deployment of mental health AI models in real-world applications.

In recent years, there has been a growing interest in AI privacy. We examine privacy solutions that could be applied to develop privacy-aware AI models in the mental health domain. These solutions can be broadly categorized into two areas: (i) ensuring data privacy and (ii) ensuring model privacy. Data privacy involves modifying data to remove private information while retaining relevant mental health information\cite{lstm_pii,lstm_bert_pii,gpt-4-deid,speech-pii-flechl,VP2022,tomashenko2024voiceprivacy,human_attribute_privacy}. An alternative approach is the creation of synthetic data for training mental health AI models \cite{chen-etal-2023-soulchat,PTSD_synth,patient-psi,lee-etal-2024-cactus,chu2024syntheticpatientssimulatingdifficult}. Model privacy, on the other hand, focuses on privacy-preserving training methods, which enhance the robustness of models against malicious attacks \cite{dp-sgd,context-aware-dp-lm,plant-etal-2021-cape,yu2022differentially,kerrigan-etal-2020-differentially,aud-dp,bu2022differentially}. 
However, implementing privacy protection methods often results in reduced model utility. Therefore, novel evaluation methods are essential to assess the privacy guarantees of these methods \cite{re-id,pii_poor_gen,VP2022,tomashenko2024voiceprivacy,unlinkability-2,human_attribute_privacy,demogarphic_face_privacy,synthetic-privacy-all,MURTAZA2023100546,privacy-gain,context-aware-dp-lm} and their impact on the model’s performance \cite{re-id,VP2022,tomashenko2024voiceprivacy,multi-speaker-anon,human_attribute_privacy,qiu-2024,zhang2024cpsycounreportbasedmultiturndialogue,Synth_Soc_1,Mehta_2024_CVPR,Mughal_2024_CVPR,context-aware-dp-lm}. Both automatic \cite{qiu-2024,zhang2024cpsycounreportbasedmultiturndialogue} and human evaluation \cite{chen-etal-2023-soulchat,Mughal_2024_CVPR,Mehta_2024_CVPR,VP2022} approaches are employed to analyze the privacy-utility trade-off for each method.

In summary, multimodal AI models hold significant potential to assist therapists and make mental illness diagnoses more accessible. However, privacy issues limit the availability of suitable datasets and, consequently, the development of robust models for real-world deployment. We discuss these privacy issues and explore potential solutions that can ensure privacy in mental health datasets and models. Additionally, we explore evaluation methods to analyze the privacy-utility trade-offs of these solutions. Figure \ref{fig:schematic} presents a schematic diagram summarizing the discussion. Finally, we recommend a privacy-aware pipeline for data collection and model training and outline future research directions to support the development of such a pipeline.


\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{Schematic.pdf}
\caption{Potential solutions to address current privacy challenges and threats across modalities in mental health dataset creation, as well as in the development and evaluation of mental health AI models, to determine the privacy-utility trade-offs of the solutions.}
\label{fig:schematic}
\end{figure}




%\todo{Improve Fig 1 caption. Connect it to MH. Currently, this is generic. (Done)}


\section*{Current Privacy Issues}

Current privacy issues in mental health datasets and models include the risk of private information leaking from both data and models to malicious actors. Privacy leakage from data prevents the public release of datasets, while leakage from models restricts the sharing of trained model weights.

\subsection*{Private information leakage from datasets}

Privacy leakage from data includes personally identifiable information (PII) present in text transcripts and audio recordings of therapy sessions. Additionally, these recordings reveal the voices of patients and therapists, as well as the faces of patients. Malicious actors can also exploit the extracted audio and video features used in mental health diagnosis models to infer sensitive attributes, such as the patient’s age and gender.

\paragraph{PII leakage.}
In the EU, personal information is protected under GDPR, which defines any information relating to an identified or identifiable natural person. An identifiable natural person is someone who can be identified, directly or indirectly, through an identifier such as a name, identification number, location data, online identifier, or factors specific to their physical, physiological, genetic, mental, economic, cultural, or social identity. Similarly, in the US, HIPAA safeguards individually identifiable health information, which includes details such as an individual’s name, address, birth date, Social Security number, and records of their past, present, or future physical or mental health conditions. Many of these types of personal information are frequently discussed in therapy sessions, such as where a person lives, their age, or any mental or physical health concerns they may have. While such information can be identified and removed in structured data formats like tables, therapy sessions often involve detailed personal narratives, which can inadvertently reveal sensitive information. As a result, textual transcripts and speech recordings of therapy sessions often contain PII that could be used to identify a patient. Even with anonymization of PII, they can still show identification vulnerabilities through the use of other public datasets \cite{cross-dataset-identity}.

\paragraph{Voice from audio.}
Speech data are classified as personal data under GDPR because they can reveal sensitive information about the speaker, including their identity, age, gender, health status, personality, racial or ethnic origin and geographical background \cite{NAUTSCH2019441}. Mental Health diagnosis often involves using audio features like Mel-frequency cepstral coefficients (MFCCs), Mel-spectrogram, and pitch extracted using tools like OpenSmile \cite{opensmile}. However, these features can inadvertently leak personal information, such as the patient’s age and gender \cite{speech_gender_age}. Furthermore, MFCCs can be utilized for speech reconstruction \cite{mfcc_speech_reconstruct}, posing a risk of impersonation for both therapists and patients. Similarly, speech embeddings like Wav2Vec can enable voice conversion \cite{wav2vec-vc}, further compromising the privacy of patients and therapists by exposing their unique vocal characteristics.


\paragraph{Face from video.}
Video recordings of therapy sessions often capture patients’ faces, as facial expressions and gaze during conversations are critical factors in mental health diagnosis. However, a patient’s face can directly reveal their identity, raising significant privacy concerns. Mental health models typically utilize facial features extracted from deep encoder models such as ResNet \cite{He_2016_CVPR}, or facial landmarks obtained through tools like OpenFace \cite{openface} for behavior, expression, and gaze analysis. These features, however, are susceptible to privacy breaches. Image reconstruction is feasible using features extracted by deep models such as AlexNet \cite{Dosovitskiy_2016_CVPR}, and malicious actors can reconstruct faces from deep templates like FaceNet \cite{Schroff_2015_CVPR} through template reconstruction attacks \cite{template_reconstruct}. Even facial landmarks can be exploited for facial reconstruction \cite{landmark_reconstruct}, potentially enabling identification and impersonation of patients.


\subsection*{Private information leakage from models}

Trained models are often susceptible to leaking training data when subjected to attacks, such as membership inference attacks \cite{membership-inference-attack}, from malicious actors. Song et al. \cite{embedding-data-leakage} demonstrated that embedding models are particularly vulnerable to leaking membership information for infrequent training data inputs, which is especially concerning for small mental health datasets with a higher prevalence of rare data points. Similarly, Carlini et al. \cite{nn-memorise} highlighted the issue of neural networks memorizing unique training data, which can then be extracted from the trained models. Moreover, in text, private data can be leaked through context \cite{context-aware-dp-lm}. This is especially true for the mental health domain, where discussing life events can indirectly leak private data. Models are also prone to exposing user information contained in the data used for fine-tuning \cite{fine-tune-data-leak}. This poses a significant privacy challenge to releasing models trained or fine-tuned on mental health datasets, as they may inadvertently memorize and disclose sensitive patient information. 

%\todo{This para needs more context of MH. seems quite generic. Aishik comment: In my PoV, the data is special but the models are the same showing basic data leakage vulnerabilities. So, I don't think there is much context to add except that MH datasets are small with unique data points and thus more vulnerable.}

% For audio data, trained ASR encoders are also prone to memorising training data \cite{Shejwalkar2024QuantifyingUM}. Other than ASRs, pre-trained speech models are also vulnerable to attacks like noise masking attacks which can recover private information from the pretraining data \cite{speech-pretrain-leak}. Vision model like CLIP also leaks which individuals were present in the training data \cite{clip-leak}.




\section*{Threats}


The leakage of a mental health patient’s private information, such as their voice or face, can lead to identification, social stigma, and exploitation. This includes risks of defamation, blackmail through deepfakes, impersonation, and misuse of biometrics, which could worsen the patient’s mental health condition.

\paragraph{Identification.}

% \if 0
% Private data leakage from mental health datasets can lead to the identification of patients within the dataset and, thus, releasing their mental health records in public. This could lead to serious consequences like workplace discrimination, social isolation and judgment. Malicious agents can also use this information to blackmail patients. Such threats could aggravate the mental health of patients. During therapy sessions, people often reveal their age, address and gender. The age and gender of a patient can also be extracted from audio and video recordings of their therapy sessions. Most Americans can be identified only using this information \cite{pii_identification}. Also, LLMs and LMs trained on therapy session data might leak this information \cite{membership-inference-attack, embedding-data-leakage,nn-memorise,training-leak-model,fine-tune-data-leak,llm-leak,NEURIPS2023_llm_leak} leading to the identification of the patient. Moreover, voice leaks from an audio recording, speech features, or speech encoder models can also be used to identify a patient through automatic speaker verification (ASV) systems \cite{identification_asv,identification_asv_2,identification_asv_3}. Similarly, video recordings of therapy sessions and video models trained on them could reveal a patient's face, resulting in identification through face recognition or verification systems \cite{Taigman_2014_CVPR,identification_face_reco,Huber_2024_WACV}.
% \fi


Private data leakage from mental health datasets can lead to patient identification and public exposure of their mental health records, resulting in workplace discrimination, social isolation, and blackmail, further aggravating their mental condition. Sensitive information, such as age, address, and gender, revealed during therapy or extracted from audio and video recordings, can uniquely identify most Americans \cite{pii_identification}. Large Language Models (LLMs) trained on therapy data are prone to privacy breaches, leaking such information \cite{membership-inference-attack, embedding-data-leakage, nn-memorise, fine-tune-data-leak}. Voice data can be exploited for identification via speaker verification systems \cite{identification_asv, identification_asv_3}, while video data may reveal faces, enabling identification through face recognition \cite{identification_face_reco, Huber_2024_WACV}.

%\todo{I shortened this para. The section heading needs to be specific}


\paragraph{Impersonation.}

% \if 0
% Voice and video leakage from mental health datasets containing therapy recordings introduces another threat where malicious agents can impersonate the patients. The voice and face of a person can be used to create deepfakes and impersonate them. Deepfakes can be either based on audio, video or audio-visual. Audio deepfakes use a person's voice to create false speech or impersonate a person. Audio deepfakes includes voice conversion, text-to-speech and replay attacks \cite{speech_deepfake_1,speech_deepfake_4,KIETZMANN2020135}. There are also impersonation attacks where a human impersonator tries to replicate a target’s specific traits and speech styles \cite{impersonation_voice,impersonation_voice_2}. Video and image deepfakes use a person's face and body to create false videos of the target. Video deepfakes are created through reenactment, video synthesis and face swaps \cite{TOLOSANA2020131,KIETZMANN2020135,vid_deepfake,speech_deepfake_4}. Audio-visual deepfakes manipulate both audio and video to a target's voice and face \cite{av_deepfake}. Deepfakes can be used for fraud, blackmail, intimidation, sabotage, harassment, defamation, revenge porn, identity theft, and bullying \cite{deepface_cons_1,MUSTAK2023113368}. Such events are mentally draining and stressful and can significantly worsen the mental condition of the patients.
% \fi

The leakage of voice and video data from mental health datasets enables malicious agents to impersonate patients through deepfakes, which can be audio, video, or audio-visual. Audio deepfakes use a person’s voice for false speech or impersonation via voice conversion, text-to-speech, and replay attacks \cite{speech_deepfake_1,speech_deepfake_4,KIETZMANN2020135,vid_deepfake}. Impersonation attacks by humans mimicking speech traits also pose a risk \cite{impersonation_voice}.  
Video deepfakes manipulate faces and bodies using reenactment, video synthesis, and face swaps \cite{TOLOSANA2020131,KIETZMANN2020135,vid_deepfake,speech_deepfake_4}, while audio-visual deepfakes combine voice and appearance \cite{av_deepfake,KIETZMANN2020135}. Deepfakes can be exploited for fraud, blackmail, harassment, identity theft, and other malicious activities \cite{MUSTAK2023113368}, causing severe psychological distress and worsening patients' mental health.

%\todo{I shortened this para. The section heading needs to be specific}

% Audio deepfakes use a person's voice to create false speech or impersonate a person. Audio deepfakes includes voice conversion, text-to-speech and replay attacks \cite{speech_deepfake_1,speech_deepfake_2,speech_deepfake_3,speech_deepfake_4,KIETZMANN2020135}. Voice conversion converts the voice in a recorded speech to a target speech. Text-to-speech generates synthetic speech based on a text input and the target person's voice. Replay attacks replay the recording of the target speaker's voice. They can directly replay the recorded voice or cut and paste recordings to use the target voice to say something specific. There are also impersonation attacks where a human impersonator tries to replicate specific traits and speech styles of a target \cite{impersonation_voice,impersonation_voice_2}.

% Video and image deepfakes use a person's face and body to create false videos of the target. Video deepfakes are created through reenactment, video synthesis and face swaps \cite{TOLOSANA2020131,KIETZMANN2020135,vid_deepfake,speech_deepfake_3,speech_deepfake_4}. In reenactment, the facial expressions are manipulated by emulating the movements of the target. Video synthesis generates the video with the target's face, and face swap swaps the target person's face with the face of a different person in a video. Deepfakes can also manipulate audio and video to a target's voice and face for creating multimodal deepfakes \cite{av_deepfake}.


\section*{Addressing the Privacy Issues}
Privacy concerns in mental health datasets can be addressed through data anonymization or by generating synthetic data derived from real datasets. Data anonymization involves removing PII from therapy transcripts and audio recordings, as well as applying voice and face anonymization techniques while preserving features crucial for mental health diagnosis. An alternative approach is the creation of synthetic data that mimics the real dataset without exposing specific patient attributes. Homomorphic encryption can also be used for data protection \cite{tee}; however, it demands significant computational resources, making it impractical in many cases \cite{tee-bad}. Privacy issues arising from models trained on mental health datasets leaking patient information can be mitigated using privacy-aware training methods.

%\todo{pls summarize this section here as well. (done)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \if 0

% \subsection*{Data Anonymization}
% Data anonymisation is the removal of personal information from datasets. These include PII in transcripts and audio recordings. Voice in audio recordings and face from video recordings also reveal the personal information of patients. These data need to be removed or anonymised so that they will not lead to identification or impersonation threats.\todo{briefly summarize the subsection here. MH is missing}

% \subsubsection*{PII Detection and Removal from Text}

% PII in therapy transcripts poses a threat to the identification of the patient. PII detection is performed through Named Entity Recognition (NER) task \cite{lstm_pii,lstm_bert_pii}. These detected PII should be replaced with synthetically generated values which make sense both grammatically and semantically. However, therapy sessions often contain conversations which can indirectly reveal private data and thus, PII removal through NER is not enough. With the advent of LLMs, GPT-4 is used to directly de-identify medical texts \cite{gpt-4-deid}. However, in real-world situations, this method leaks private information through API and thus cannot be used for mental health datasets. Moreover, these methods show poor generalisation capability on cross-institute datasets \cite{pii_poor_gen,lstm_bert_pii}. To tackle this, Kim et al. \cite{kim-etal-2024-generalizing} suggest augmenting de-identification datasets with synthetic data generated by GPT-4. Mental health transcripts also contain spoken language, which is different from written text. Thus, Dhingra et al. \cite{speech_pii_augment} present an augmentation strategy to improve PII removal from text transcribed from spoken language.
% An alternative approach to remove private information from text is through text rewriting \cite{nap2}. However, this approach is only tested for written text and not for transcribed conversational data present in therapy sessions. Moreover, speaking style and language use are often important cues for mental illness prediction. Rewriting obscures these signals, thus limiting the application of this approach in therapy session transcripts.

% \subsubsection*{Text rewriting}

% Text rewriting is one of the methods to remove private information from text. Xu et al. \cite{xu-etal-2019-privacy} propose privacy-aware back translation method for protecting sensitive information in rewritten text. Differential privacy \cite{DP} is also used to provide theoretical guarantees of privacy in text rewriting \cite{dp_rewrite,dp_rewrite_2,dp_rewrite_3,dp_rewrite_4}. However, good guarantees require large noise additions which reduce utility. Huang et al. \cite{nap2} creates a benchmark for naturalness and privacy-preserving text rewriting. They present a method which deletes sensitive information or obscures it through abstraction. This is similar to how humans approach the task. While text rewriting is a viable option for other sensitive areas, in case of mental health prediction, rewriting obscures the speaking style and language use of a person which can reduce the utility of the data as speaking style and language use are often used for mental illness prediction. 

% \subsubsection*{PII detection and removal from Audio}

% PII are also present in recorded audio. Removing PII from audio involves identifying the audio segment with PII and replacing it. Cohn et al. \cite{cohn-etal-2019-audio} presents a pipeline where the audio is transcribed using Automatic Speech Recognition (ASR), and then a text NER model detects the PII. The starting and ending time of the PII is determined, and that segment of the audio is silenced. SpeeDF \cite{Veerappan2024} modifies the last part of the pipeline. They propose two methods to redact the PII. One way is to replace the PII audio segment with white noise or beep. However, this makes the audio unnatural. Thus, in the other method, they replace the PII with fictional data of the same category and use a text-to-speech model or voice conversion to convert the transcribed text to audio. Flechl et al. \cite{speech-pii-flechl} also use a random word sequence of the same category as a replacement for the PII. However, instead of generating the full text through text-to-speech models, they only generate the audio of the replacement with text-to-speech and replace that with the PII audio segment. Another method suggested by them includes splicing together matching audio fragments from the corpus to replace the PII audio segment. 

% Even after removing PII from text transcripts, PII are still present in recorded audio. Removing PII from audio involves identifying the audio segment with PII and redacting it. Cohn et al. \cite{cohn-etal-2019-audio} presents a pipeline where the audio is transcribed using ASR then a text NER model detects the PII. The starting and ending time of the PII is determined and that segment of the audio is silenced. SpeeDF \cite{Veerappan2024} modifies the last part of the pipeline. They propose two methods to redact the PII. One way is to replace the PII audio segment with white noise or beep. In the other method they replace the PII with fictional data of the same category and use text-to-speech model or voice conversion to convert the transcribed text to audio. Flechl et al. \cite{speech-pii-flechl} also use a random word sequence of the same category as replacement for the PII. However, instead of generating the full text through text-to-speech models they only generate the audio of the replacement with text-to-speech and replace that in place of the PII audio segment. Another method suggested by them includes splicing together matching audio fragments from the corpus to replace the PII audio segment. Due to the nature of therapy sessions, similar to the text transcripts, the recorded audio can indirectly reveal private data from conversations. Similarly spoken language is present in recorded audio files. Thus, re-identification model \cite{re-id} and spoken language augmentation \cite{speech_pii_augment} can be used similar to text PII detection for detecting audio PII segments. Then these segments can be replaced by using methods from Flechl et al. \cite{speech-pii-flechl}.

% \subsubsection*{Voice anonymisation}

% Voice privacy challenges \cite{VP2022,tomashenko2024voiceprivacy} are created aimed at protecting speaker information in data shared for Automatic Speech Recognition (ASR) and Speech Emotion Recognition. A basic baseline method proposed to protect speaker identity is by replacing the x-vector of the speaker with a public x-vector. However, randomly choosing an x-vector from the public x-vector pool leads to less diversity in speech and also struggles with language change. Thus, to choose suitable public x-vectors for maintaining diversity, orthogonal householder neural networks \cite{OHNN} are used. In spite of changing x-vectors to public x-vectors, speaker information is still present in pitch and audio BN features. One way to reduce the speaker information in BN features is by quantising the BN features \cite{champion:hal-03753746}. An alternative method is to add noise to pitch and BN features to ensure differential privacy (DP)\cite{shamsabadi:hal-03588932}. Many mental health prediction methods also use OpenSmile features or features extracted from pre-trained audio models like HuBERT. The speaker information leakage from these features also needs to be studied, and suitable privacy-preserving feature extraction methods should be developed. Anonymising voice in recorded therapy sessions is also generally a multi-speaker anonymisation task since both the therapist's and patient's identities need to be protected. Miao et al. \cite{multi-speaker-anon} presents a benchmark for such a multi-speaker anonymisation task.

% \subsubsection*{Face Anonymisation}
% Face anonymisation techniques are often used for privacy protection in face recognition systems. Various methods like Face-Off \cite{face-off}, LowKey \cite{lowkey}, Foggysight \cite{foggysight} and FAWKES \cite{fawkes} are used for face obfuscation. However, face recognition systems have the ability to adapt to perturbed faces \cite{perturb_faces}. Moreover, these obfuscation methods are unfair and demonstrate unfairness according to demographic attributes of faces \cite{demogarphic_face_privacy}. Also, it has been shown that age, gender and emotion recognition are not affected much through de-identification of eyes, lower face, head and face obfuscation if situational context is present \cite{human_attribute_privacy}. While attributes like emotion detection are desirable in mental health diagnosis, age and gender leakage are not wanted.
% \fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TC wrote this part. Please check

\subsection*{Data anonymization}

%\todo{TC wrote this subsection. pls check this (Done: modified some parts which felt to have missing or slightly different information)}

Data anonymization involves removing PII in transcripts and audio recordings, voice anonymization in audio recordings and face anonymization in video recordings of therapy sessions to prevent identification and impersonation threats. Below, we outline approaches for anonymizing textual, audio, and visual data to ensure privacy while retaining essential information for mental health diagnosis.

\paragraph{Text anonymization by detecting and removing PII.}

PII in therapy transcripts, such as names, addresses, and dates, poses identification risks. Named Entity Recognition (NER) models can detect PII and replace them with synthetically generated values that align grammatically and semantically \cite{lstm_pii,lstm_bert_pii}. However, therapy conversations often indirectly reveal private information, making simple NER-based methods insufficient. LLMs, such as GPT-4, have shown promise in de-identifying text \cite{gpt-4-deid}, though real-world application faces challenges like data leakage through APIs and previous Language Model (LM) based models show poor generalization across datasets \cite{pii_poor_gen,lstm_bert_pii,kim-etal-2024-generalizing}. Augmenting de-identification datasets with synthetic data \cite{kim-etal-2024-generalizing,lstm_bert_pii} and specialized strategies for transcribed spoken language \cite{speech_pii_augment} improve performance. Text rewriting \cite{nap2} offers an alternative but remains untested for conversational data and risks obscuring linguistic cues critical for mental health diagnosis.

\paragraph{Audio anonymization by addressing PII in speech data.}

Audio anonymization involves detecting and replacing PII in recorded sessions. Pipelines often use Automatic Speech Recognition (ASR) to transcribe audio, followed by NER-based PII detection and redaction. Approaches include replacing PII segments with silence \cite{cohn-etal-2019-audio}, white noise or beeps \cite{Veerappan2024}. But it makes speech unnatural. Therefore, a better approach is to use fictional content from the same category to replace PII and convert it to speech using text-to-speech or voice conversion \cite{Veerappan2024}. However, this approach modifies the entire audio. Flechl et al. \cite{speech-pii-flechl} proposed splicing matching audio fragments to generate the PII replacement and only modify the PII segment in the audio.

\paragraph{Voice anonymization for speaker privacy.}

Voice anonymization aims to protect speaker identity in data used for automatic speech and emotion recognition \cite{VP2022,tomashenko2024voiceprivacy}. Automatic speaker verification  (ASV) systems use speaker representations like x-vectors for verification. Thus voice anonymization techniques include replacing speaker x-vectors with public x-vectors, although this reduces diversity as well as struggles with language change \cite{OHNN}. Orthogonal householder neural networks \cite{OHNN} tackle this by choosing suitable public x-vectors for maintaining diversity. However, speaker information is still present in pitch and audio bottleneck features \cite{bn} (a low-dimensional phonetic representation extracted from an intermediate layer of an ASR model). To address this, bottleneck features can be quantized \cite{champion:hal-03753746} or perturbed with noise for differential privacy (DP) \cite{shamsabadi:hal-03588932}. Features from pre-trained models like HuBERT \cite{hubert} and OpenSmile \cite{opensmile}  also require research for privacy-preserving extraction. Miao et al. \cite{multi-speaker-anon} benchmarked the Multi-Speaker Anonymization (MSA), crucial for therapy recordings.

\paragraph{Face anonymization for visual privacy.}

Face anonymization prevents identification through video recordings. Tools like Face-Off \cite{face-off}, LowKey \cite{lowkey}, Foggysight \cite{foggysight}, and FAWKES \cite{fawkes} obfuscate faces in images but may fail against adaptive face recognition systems \cite{perturb_faces}. AI stylization \cite{vid_anon} provides another alternative for face obfuscation in images while maintaining emotions of the person, crucial for mental health applications. Face anonymization in videos can be performed through applying image face obfuscation methods in every frame. However, these will be costly; therefore, specialized video face anonymization methods like FIVA \cite{Rosberg_2023_ICCV} are more suited for mental health datasets. Other methods of image obfuscation include extracting identity representation from the image, adding noise for DP guarantees and reconstructing the image \cite{img-dp}. Although DP-based methods for video anonymization focus on object indistinguishability to protect human identity \cite{vid-dp}, its direct applicability for preventing facial recognition in therapy videos is unclear.
These methods can also introduce demographic biases \cite{demogarphic_face_privacy}. While useful attributes like emotion detection remain unaffected by obfuscation, detection of sensitive traits such as age and gender are also unaffected \cite{human_attribute_privacy,gender_leak_face_obfus}, necessitating targeted anonymization methods for mental health applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \if 0
% \subsection*{Synthetic Data Generation}

% Synthetic data is generated using original data and AI generative models, which are close to the real data but do not belong to a real person, thus ensuring privacy. AI Models could be trained with these generated synthetic to learn to diagnose mental illnesses instead of training them on real data. Moreover, synthetic data can help in solving the scarcity and diversity problem of mental health datasets.

% \subsubsection*{Text}

% Textual synthetic data generation includes generating synthetic social media posts or therapy transcripts containing multi-turn dialogues. Ghanadian et al. \cite{Synth_Soc_1} generated synthetic social media data by extracting social factors related to suicidal ideation from psychology literature and prompting ChatGPT, FlanT5 and Llama-2 with the factors. Mori et al. \cite{mori-etal-2024-towards} generated synthetic reddit like posts by prompting GPT-3 with demographic data. This created a highly diverse dataset. However, the synthetic data generated through these methods only contain single-turn posts. Mental health therapy data consists of conversations between therapist and patient and thus multi-turn in nature. Datasets like SoulChat \cite{chen-etal-2023-soulchat} and SMILE \cite{qiu2024smilesingleturnmultiturninclusive} were generated by prompting ChatGPT to convert single turn psychological Question Answer into a multi-turn conversation. They showed the effectiveness of training model with these datasets as well. CPsyCoun \cite{zhang2024cpsycounreportbasedmultiturndialogue} was created by using LLMs to counseling notes from counseling memos and then feed both the memos and notes to LLMs to generate multi-turn conversations. Wu et al. \cite{PTSD_synth} used zero-shot and few-shot prompting of ChatGPT to generate standardized transcripts of PTSD interviews. These synthetic interviews augmented with E-DAIC dataset showed improvement in PTSD diagnosis. SAPE \cite{lozoya-etal-2024-generating} explored genetic algorithm to create a suitable prompt for generating therapy transcripts. Chen et al. \cite{chen2023llmempoweredchatbotspsychiatristpatient} tested the feasibility of using LLM powered chatbots to simulate patients and psychiatrist. However, they did not contain any psychological knowledge so, Patient-$\Psi$ \cite{patient-psi} prompted GPT-4 with Cognitive models based on Cognitive Behavioral Therapy (CBT) principles to get simulations closer to actual patients. Qiu et al. \cite{qiu-2024} generated a synthetic dataset by simulating GPT-4 as both patients and psychiatrist and making them converse with this role-playing setup. CACTUS \cite{lee-etal-2024-cactus} improves upon this by training the psychiatrist model for CBT planning and inputting personality, medical history, and problems for patient model.

% \subsubsection*{Multimodal}

% As multimodal data and multimodal models have been shown to perform better in mental health diagnosis, it is important to create multimodal synthetic data to improve AI-driven mental health applications. Mehta et al. \cite{Mehta_2024_CVPR} develop a unified speech-gesture synthesis framework with text input. This can be used in conjunction with text synthetic data generation methods to generate synthetic multimodal data. Style-Talker \cite{li2024styletalker} takes audio of the conversation partner, the transcribed chat history and speech styles to generate speaking style and text for the response for a TTS system. This framework can be used similarly to Qiu et al. \cite{qiu-2024} to simulate patient-psychiatrist conversation with text and audio modality. On the other hand, ConvoFusion \cite{Mughal_2024_CVPR} introduces a method to generate gestures from text and audio. This can be used to add the video modality to generate simulated conversation with text, audio and video modality. However, these methods generate single-turn dialogue at a time and sequentially generate the different modalities. The sequential generation of modalities adds noise in each modality generation step, thus having higher noise in the multimodal output. Furthermore, this limits the capability of multimodal generation as everything follows from the generated text. Thus, generating a long multi-turn multimodal synthetic conversation with such methods is computationally expensive. Ng et al. \cite{Ng_2024_CVPR} present a method to generate photoreal avatars with gestures from audio input for dyadic conversations. While this is a solution to single-turn generation, it still produces video from text and audio and does not produce a multimodal output directly. Chu et al. \cite{chu2024syntheticpatientssimulatingdifficult} generates synthetic patients using multimodal generative AI to train trainee doctors. They provide instructions and patient data to GPT-4, which generates a response. This is fed to a TTS model that generates the response audio with a suitable voice. This is fed to a video generation model like Runway Gen2 or Pika or stable diffusion to create a lip-synced animated video with a custom voice. This method also suffers from single-turn generation and generates modalities sequentially. Moreover, none of these methods have been tried to generate multimodal data specific to mental health diagnosis.
% \fi
%%%%%%%%%%%%%
%% TC wrote this part. Please check

\subsection*{Synthetic data generation}
%\todo{TC wrote this subsection. pls check this (Done)}
Synthetic data, generated using AI models, mirrors real data but does not belong to actual individuals, ensuring privacy. It offers a solution to data scarcity and diversity challenges in mental health datasets, enabling effective AI training while protecting sensitive information.

\paragraph{Synthetic text generation.} Textual synthetic data generation includes generating therapy transcripts with multi-turn dialogues. This is addressed by datasets like SoulChat \cite{chen-etal-2023-soulchat} and SMILE \cite{qiu2024smilesingleturnmultiturninclusive}, generated by converting single-turn psychological Q\&A into multi-turn conversations via ChatGPT. CPsyCoun \cite{zhang2024cpsycounreportbasedmultiturndialogue} used LLMs to generate multi-turn dialogues from counseling reports. Wu et al. \cite{PTSD_synth} employed ChatGPT for zero-shot and few-shot generation of PTSD interview transcripts, improving PTSD diagnosis when combined with real datasets.
SAPE \cite{lozoya-etal-2024-generating} used genetic algorithms for creating better prompts to enhance synthetic therapy data generation. Role-playing setups, like those in Patient-$\Psi$ \cite{patient-psi} and CACTUS \cite{lee-etal-2024-cactus}, simulate patient-psychiatrist interactions by incorporating cognitive models and contextual details, improving realism and utility. Other synthetic text generation methods give theoretical guarantees using DP with language models \cite{yue-etal-2023-synthetic,nahid2024safesynthdp}.

\paragraph{Synthetic multimodal data generation.}

Given the superior performance of multimodal models in mental health diagnosis, synthetic multimodal data generation is critical. Mehta et al. \cite{Mehta_2024_CVPR} proposed a unified framework for speech-gesture synthesis using text input, complementing textual generation methods. Style-Talker \cite{li2024styletalker} integrates speech styles and chat history to generate conversational responses, supporting simulations of patient-psychiatrist dialogues in text and audio. ConvoFusion \cite{Mughal_2024_CVPR} adds gesture generation from text and audio, enabling text, audio, and video simulation. However, the sequential generation of modalities introduces cumulative noise and computational inefficiencies. Ng et al. \cite{Ng_2024_CVPR} developed a method for generating photo-realistic avatars with gestures for dyadic conversations, addressing single-turn limitations but still relying on sequential modality generation. Chu et al. \cite{chu2024syntheticpatientssimulatingdifficult} created synthetic patients for medical training, producing video outputs by combining GPT-4, text-to-speech, and video generation models. While promising, these methods remain computationally intensive and lack specific applications for mental health diagnosis.

%%%%%%%%%%%%%%%%%%

% \if 0
% \subsection*{Privacy-aware training}

% \subsubsection*{Differential Privacy}

% Differential Privacy \cite{DP} gives a theoretical guarantee of privacy protection. Thus, it is reliable and used to train privacy-aware deep learning models using DP-SGD \cite{dp-sgd}. However, using DP-SGD for language modelling tasks shows poor performance and convergence \cite{dp-sgd-bad-performance}. Moreover, in text, private data can be leaked through context \cite{context-data-leak}. This is especially true for the mental health domain, where discussing life events can indirectly leak private data. In the case of the mental health domain, pre-trained language models will be fine-tuned on private mental health data. Thus, methods for differentially private fine-tuning language models are very important in this context \cite{kerrigan-etal-2020-differentially, yu2022differentially}. Differential privacy methods can also be applied to conformer-based encoder models for audio modality \cite{aud-dp} and to VGG and ResNet for image and video modality \cite{bu2022differentially}.

% \subsubsection*{Autoencoder}

% Autoencoders are often used in speech models to get an intermediate representation that contains linguistic and paralinguistic features while removing the speaker's identity information. The auto-encoders were trained to maximise utility through good performance on downstream tasks and maximise privacy by reducing the performance of speaker identification or gender of the speaker. Ravuri et al. \cite{ravuri} used such an autoencoder model to maintain depression severity prediction and reduce speaker classification performance. Pranjal et al. \cite{pranjal} also used an autoencoder to transform a set of physiological, acoustic and daily life measurements for anxiety detection on the TILES 2018 dataset while reducing identification.
% \fi

%%%%%%%%%%%%%
%% TC wrote this part. Please check
\subsection*{Privacy-aware training}
%\todo{TC wrote this section. Please check (Done)}

Privacy-aware training methods are essential for developing AI models in mental health, ensuring that private and sensitive data is protected in trained models while maintaining model utility.

\paragraph{Differential Privacy.} Differential Privacy \cite{DP} provides theoretical privacy guarantees and is widely used to train privacy-aware models through Differentially-Private Stochastic Gradient Descent (DP-SGD)  \cite{dp-sgd}. However, DP-SGD often suffers from poor performance in language modeling tasks \cite{kerrigan-etal-2020-differentially}. In mental health, contextual information can inadvertently reveal private data \cite{context-aware-dp-lm}. Context-aware DP methods \cite{context-aware-dp-lm,plant-etal-2021-cape} mitigate such issues by accounting for contextual leakage during training.
Fine-tuning LLMs on private mental health data requires differentially private fine-tuning techniques \cite{kerrigan-etal-2020-differentially,yu2022differentially}. Beyond text, DP methods can be applied to conformer-based encoders for audio \cite{aud-dp} and to models like ResNet for image and video data \cite{bu2022differentially}.

\paragraph{Federated learning.} Federated Learning (FL) is another popular privacy-preserving training method where training is distributed and locally trained model gradients are communicated to a central server \cite{fl}. This is useful for combining mental health data from different medical institutes. However, FL, on its own, provides limited privacy and is vulnerable to attacks \cite{fl-attack1,fl-attack2,fl-attack3} and leaking data through the local model weights and gradients \cite{NAGY2023110475}. Therefore, it is often combined with Local Differential Privacy (LDP) to improve privacy guarantees \cite{NAGY2023110475,dp-fl1}.

\paragraph{Confidential computing with Trusted Execution Environment (TEE).} Confidential computing aims to safeguard the data during processing. It loads the data and the model in a TEE where they are protected from unauthorized access and modification \cite{tee}. However, it requires more computational resources and special hardware, limiting its wide-spread usage \cite{tee-bad}.

\paragraph{Autoencoders for privacy preservation.}
Autoencoders are commonly used in speech models to extract latent representations containing linguistic and paralinguistic information while obfuscating speaker identity. These models are trained to maximize downstream task performance, such as mental health prediction while minimizing speaker classification accuracy.
Ravuri et al. \cite{ravuri} demonstrated the use of autoencoders to retain depression severity prediction performance while reducing speaker classification accuracy. Similarly, Pranjal et al. \cite{pranjal} used autoencoders to transform physiological, acoustic, and daily life measurements for anxiety detection on the TILES 2018 dataset while reducing identification risks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Evaluating Privacy-aware Alternatives}
%\todo{I changed the entire section and shortened it. }

% \if0
% Improving data and model privacy results in a drop in downstream performance. Maintaining patients' privacy in mental health diagnosis is of utmost importance. However, we must also ensure that the AI models are useful in mental health diagnosis. So, we discuss evaluation methods to study the privacy-utility trade-offs of the proposed privacy solutions.
% \fi

Ensuring privacy in AI models for mental health diagnosis is essential to protect patient confidentiality. However, this often comes at the cost of reduced performance in downstream diagnostic tasks. This section discusses methodologies for evaluating privacy-utility trade-offs across three key areas: data anonymization, synthetic data generation, and privacy-aware training.


\subsection*{Data anonymization}

% \if 0
% Since data anonymisation removes private information, privacy evaluation will determine how much private data can be extracted from the anonymised data. The naturalness of the anonymised data and the performance of models trained on it will determine the utility of the data anonymisation method.
% \fi

Data anonymization techniques focus on removing or masking private information across text, audio, and video modalities. Effective anonymization should minimize privacy risks while preserving the diagnostic utility of the data. Evaluation can be categorized into privacy and utility metrics.


% \if 0 
% \subsubsection*{Privacy Evaluation}
% Data anonymisation in therapy transcripts is performed through PII detection and removal. Standard measures of privacy through PII removal include precision, recall, accuracy, f1-score and fallout \cite{neamatullah2008automated,SANCHEZ2014189,lstm_pii,pii_poor_gen,kim-etal-2024-generalizing,gpt-4-deid,lstm_bert_pii,neamatullah2008automated}. They generally measure the classification ability of the method to detect PII words. However, in therapy sessions, PIIs can be revealed indirectly. Thus, a better approach is to train an adversarial re-identification model which tries to detect PII present in the text after performing data anonymisation \cite{re-id}. This can be further improved by using LLMs to develop stronger adversarial re-identification models for improved privacy evaluation.


% Text rewriting is also used to maintain data anonymity in text modality. Here, the sensitive data is obfuscated by rewriting the original text into modified text. One method to evaluate privacy is through training a model to estimate the probability of sensitive information in a text. Then this probability is used to Entropy, P-Acc, and M-Acc \cite{xu-etal-2019-privacy}. A higher entropy shows a more chaotic and random probability by the sensitive attribute detection model and thus better. P-Acc calculates the portion of correct prediction of the sensitive attribute. Thus it is best if it is 50\% in case of binary classification of sensitive text. For, M-Acc, probabilities of sensitive attributes are calculated for original and rewritten text. If the probability decreases on rewriting it is taken as a modified sentence. M-Acc calculates the rate of modified sentences. Finally, human evaluations are also performed to determine the privacy leakage in rewritten texts \cite{nap2}.

% PII detection and removal are also crucial in audio recordings of the sessions. Similar to the transcripts, standard privacy evaluation metrics measure the classification performance of PII detection through precision, recall and f1 scores \cite{cohn-etal-2019-audio,Veerappan2024,speech-pii-flechl}. However, similar to transcripts, an adversarial re-identification model \cite{re-id} should give better privacy measures. Data anonymisation of audio recordings also includes anonymisation of the patient's and therapist's voices. The privacy of voice anonymisation techniques is measured through the Equal Error Rate (EER) of ASV systems \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932,multi-speaker-anon}. A higher EER signifies that the ASV system performs poorly, thus showing better privacy guarantees for the voice anonymisation method. However, researchers generally mention the voice anonymisation method utilised when releasing data. Therefore, robustness against attack models \cite{VP2022, tomashenko2024voiceprivacy, OHNN} with information about the anonymisation strategies is another way to measure the privacy capabilities of the voice anonymisation method. Unlinkability \cite{shamsabadi:hal-03588932,unlinkability-1,unlinkability-2} is another standard measure of privacy in voice anonymisation. Unlinkability measures the overlap between distributions of ASV scores for the same speaker and different speakers. Human evaluation is also used to determine speech verifiability \cite{VP2022}.

% Next, we move on to the audio modality. PII detection and removal are also important data anonymisation processes in speech. Similar to text, privacy is evaluated through PII detection measures like precision, recall and f1 scores \cite{cohn-etal-2019-audio,Veerappan2024,speech-pii-flechl}. Other metrics like the percentage of perfect PII detection and partial PII detection are also used \cite{Veerappan2024}. A higher percentage of perfect and partial PII detection shows better privacy. However, for therapy sessions, conversations can indirectly give away personal information. Thus, an adversarial re-identification model \cite{re-id} should be used for privacy evaluation similar to text modality.

% Voice anonymisation is another data anonymisation method in the audio modality. Automatic speech verification is used to test the privacy of anonymised voice. If the ASV system can verify an anonymised voice with the original voice it shows poor privacy. Various attack models are used to improve speech verification of anonymised voice. Attack models can be uninformed or have information regarding the anonymisation strategies \cite{VP2022, tomashenko2024voiceprivacy, OHNN}. Privacy is primarily measured through Equal Error Rate (EER) of the ASV system \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932,multi-speaker-anon}. EER represents the threshold at which the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). A higher EER shows higher false acceptance and rejection rate and thus a worse ASV system showing greater privacy of the anonymisation process. Unlinkability \cite{shamsabadi:hal-03588932,unlinkability-1,unlinkability-2} is another measure of privacy in voice anonymisation which does not depend on a threshold. Unlinkability measures the amount of overlap between distributions of ASV scores for the same speaker and different speakers. Human evaluation is also used to determine speech verifiability \cite{VP2022}. In mental health domain, the recorded audios often contain voices of both therapist and patient. Thus, it is important to ensure that both the therapists and patients voices are unlinkable to the person.

% The privacy evaluation of face anonymisation methods measures the performance of face recognition systems on the face-anonymised video of therapy sessions \cite{face-off,lowkey,foggysight,fawkes}. The face recognition systems also adapt and develop defences against anonymisation methods. Thus, it becomes essential to test the face anonymisation against such defences. Video recordings of faces can also reveal the age and gender of the patients, which can be used to identify them. Thus, it is also essential to evaluate the leakage of such attributes from the anonymised video  \cite{human_attribute_privacy}. Face anonymisation techniques also demonstrate unfairness and have low privacy for specific demographics \cite{demogarphic_face_privacy}. Thus, another privacy measure should be demographic-specific for a holistic privacy evaluation. Moreover, unlike images, video recordings of therapy sessions are long and contain face images at various positions and angles. Thus, it is necessary to evaluate facial recognition systems on all frames.

% Finally, the text, audio and video modalities together might leak more private information and lead to the identification of a person. Thus, developing a multimodal re-identification model and measuring its performance against the anonymised data for privacy evaluation is essential.

% \fi

\paragraph{Privacy evaluation.} Privacy of PII detection and removal methods used for therapy transcripts is evaluated using standard metrics such as precision, recall, accuracy and F1-score \cite{SANCHEZ2014189,lstm_pii,pii_poor_gen,kim-etal-2024-generalizing,gpt-4-deid,lstm_bert_pii}, to measure the ability to classify PII words. However, indirect PII leakage in therapy sessions necessitates testing against adversarial re-identification models \cite{re-id}, which can be enhanced using LLMs for improved privacy evaluation. It is also important to test vulnerabilities that arise from any related public dataset \cite{cross-dataset-identity}. The privacy of PII removal methods for audio recordings is similarly measured using PII detection metrics \cite{cohn-etal-2019-audio,Veerappan2024,speech-pii-flechl}. Voice anonymization techniques are evaluated using the Equal Error Rate (EER) \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932}or False Accept Rate (FAR) \cite{multi-speaker-anon} of ASV systems, where higher EER and lower FAR indicate better privacy. Robustness against attack models \cite{VP2022,tomashenko2024voiceprivacy,OHNN} and unlinkability \cite{shamsabadi:hal-03588932,unlinkability-2} further assess privacy capabilities.
For face anonymization in video recordings, privacy is tested by evaluating face recognition systems against anonymized faces \cite{face-off,lowkey,foggysight,fawkes,Rosberg_2023_ICCV}, along with leakage of attributes like age and gender \cite{human_attribute_privacy}. Robustness against facial reconstruction attack should also be tested \cite{Rosberg_2023_ICCV}. Demographic fairness is crucial, as anonymization methods may disproportionately affect certain groups \cite{demogarphic_face_privacy}.
Finally, multimodal data can exacerbate privacy risks. Thus, multimodal re-identification models are essential for holistic privacy evaluation across text, audio, and video.


% \if 0

% \subsubsection*{Utility Evaluation}

% Utility evaluation of PII removal in text transcript measures the performance of models trained on PII-removed text transcripts on downstream tasks. Sanchez et al. \cite{SANCHEZ2014189} measure the utility of the PII removal method by calculating the information provided by all terms in a text and calculating the proportion of information preserved in the PII removed text. Morris et al. \cite{re-id} measured utility through an average percentage of masked words and information loss percentage. For therapy transcripts, the information preservation measures should only determine the preservation of information related to the mental health of the patient.
% Utility evaluation for text rewriting uses automatic metrics like BLEU, GLEU, METEOR and WMD between the original text and rewritten text \cite{xu-etal-2019-privacy}. Human evaluation of the utility of rewritten text includes rating the fluency, relevance and naturalness of the rewritten text \cite{xu-etal-2019-privacy,nap2}. The language use of a person is often an important factor to understand their mental health issues. Thus, evaluating models trained with rewritten text on downstream mental health tasks can also be used as a measure of utility.

% Utility of PII removed audio is measured through automatic metrics like percentage of substitutions, hallucinations and omissions \cite{Veerappan2024}. In therapy session recordings, the percentages should be calculated using only mental health related portions of the conversation. Moreover, the speaking style and language used by the patient should be kept constant as they provide crucial mental health cues. Thus, utility evaluation should include human evaluation of naturalness, style consistency and relevance. Finally,  speech models trained with PII-removed audio should be evaluated on downstream mental health tasks as a utility measure.

% The utility of voice anonymisation methods is measured through speech intelligibility, preservation of emotional features, intonation and diversity in voices. Intelligibility is automatically measured through Word Error Rate (WER) in ASR of the anonymised speech \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932,multi-speaker-anon}. Emotion preservation is measured by the performance of speech emotion recognition system on the anonymised speech \cite{tomashenko2024voiceprivacy}. Pitch correlation between the original and anonymised speech is used to measure intonation preservation \cite{VP2022}. Diversity preservation of the anonymisation method is measured through Gain of Voice Distinctiveness \cite{VP2022,OHNN}. Human evaluations are also used to assess speech naturalness and speech intelligibility \cite{VP2022}. Speech naturalness can also be automatically measured through Predicted Mean Opinion Score (PMOS) \cite{multi-speaker-anon}. Along with these, the preservation of mental health features is vital for mental health diagnosis. While the mental health cues are often related to pitch, intonation, and emotions, another utility evaluation should measure the performance of models trained on the anonymised voice for mental health downstream tasks. Moreover, therapy data contains the voices of both the therapist and the patient, thus requiring multi-speaker anonymisation. In multi-speaker anonymisation settings, diarisation is essential, and it is measured through the Diarisation Error Rate (DER) \cite{multi-speaker-anon}.

% The utility evaluation of face anonymisation is performed by using the anonymised video to perform downstream tasks to determine the mental illness of a person or the emotions of a person \cite{human_attribute_privacy}.
% \fi


\paragraph{Utility evaluation.} Utility evaluation of PII removal in text measures model performance on downstream tasks using PII-removed transcripts. Sanchez et al. \cite{SANCHEZ2014189} assessed utility by calculating the proportion of information preserved, while Morris et al. \cite{re-id} used metrics like masked word percentage and information loss. For therapy transcripts, the utility should focus on preserving mental health-relevant information.  
For PII-removed audio, utility is evaluated using metrics like substitution, hallucination, and omission percentages \cite{Veerappan2024}. Such calculations should be limited to mental health-relevant segments. Additional human evaluations measuring naturalness, style consistency, and relevance should also be performed. Finally, models trained on PII-removed audio should be tested on mental health-related tasks to gauge utility.
Voice anonymization utility is assessed through intelligibility (via Word Error Rate \cite{VP2022,tomashenko2024voiceprivacy,OHNN,multi-speaker-anon}), emotion preservation (using emotion recognition performance \cite{tomashenko2024voiceprivacy}), intonation preservation (via pitch correlation \cite{VP2022}), and diversity (Gain of Voice Distinctiveness \cite{VP2022,OHNN}). Human evaluations of naturalness and intelligibility \cite{VP2022}, as well as automatic measures like Predicted Mean Opinion Score (PMOS) \cite{multi-speaker-anon}, further refine utility assessment. Utility should also evaluate models trained on anonymized voices. Since therapy data is multi-speaker, multi-speaker anonymization requires utility evaluation through Diarization Error Rate (DER) \cite{multi-speaker-anon}. For face anonymization, utility is tested by evaluating anonymized videos on downstream tasks, such as emotion detection or mental health diagnosis \cite{human_attribute_privacy,vid_anon}.  

% \if 0
% \subsection*{Synthetic Data Generation}

% While synthetic data does not belong to real people, it is generated using the data from real people. Thus, synthetic data can reveal data related to real people if the generation model overfits. This is especially likely in mental health data as the datasets are small. Thus, the synthetic data generation models are likely to overfit. Synthetic data generation can also produce generic data, which will be useless when training AI models. Thus, evaluating the privacy-utility trade-offs of synthetic data generation methods is essential.

% \subsubsection*{Privacy Evaluation}

% Privacy evaluation of synthetic data tests the robustness of the synthetic data against membership inference attacks and attribute inference attacks \cite{synthetic_data_privacy_1,goncalves2020generation,synthetic-privacy-all, kaabachi2023can}. A membership inference attack on the synthetic dataset tries to ascertain if a particular person is included in the real dataset. Outliers in real data are even more vulnerable to leakage as the synthetic dataset can memorise them to maintain a data distribution similar to the real dataset. Outliers are more likely in a mental health dataset where we have a small number of participants from diverse groups. Thus, vulnerability to outlier membership inference should be prioritised in mental health synthetic datasets. Outlier membership inference performance can be measured using metrics like privacy gain \cite{privacy-gain} and outlier similarity metric \cite{MURTAZA2023100546}. In attribute inference attacks, the attacker has some prior knowledge about attributes like the address and workplace of a patient and tries to determine other attributes like mental health conditions from the synthetic data. In mental health datasets, it is also crucial to ensure that the synthetic sessions do not contain duplicates of actual sessions. Thus, reproduction rate \cite{MURTAZA2023100546} in the synthetic data must be measured and strictly ensured to be zero.

% Privacy evaluation of synthetic data tests the robustness of the synthetic data against membership inference attacks and attribute inference attacks \cite{synthetic_data_privacy_1,goncalves2020generation,synthetic-privacy-all, kaabachi2023can}. A membership inference attack tries to ascertain from the synthetic dataset if a certain person is included in the real dataset. A modified form of membership inference attack can also be used where the attacker has some background information on a patient and uses it to find the nearest neighbor in the synthetic data and estimate unknown attributes about the patient \cite{synthetic_data_privacy_2}. Outliers in real data are even more vulnerable to leakage as they can be memorised by the synthetic dataset to maintain a data distribution similar to the real dataset. Outliers are more likely in a mental health dataset where we have a small number of participants from diverse groups. Privacy Gain \cite{privacy-gain} measures the performance of membership inference attacks on such outliers. Outlier similarity metric \cite{MURTAZA2023100546} is another measure for evaluating the risk of outlier leakages. In attribute inference attacks, the attacker has some prior knowledge about certain attributes of a patient and tries to determine other attributes from the synthetic data. Other privacy metrics for synthetic data include distance-based metrics \cite{synthetic-privacy-all,MURTAZA2023100546}, which determines the closeness of synthetic data points to real data points. Some distance-based privacy metrics include Distance to Closest Record (DCR) \cite{synthetic_data_privacy_1}, $\epsilon$-identifiability \cite{eps-identifiability}, memorization coefficient \cite{mem-coeff}, Adversarial Accuracy Privacy Loss \cite{YALE2020244} and minimum distance between embeddings of synthetic data to real data \cite{kang2024syntheticdatagenerationllm}. Another measure of privacy leaks is reproduction rate \cite{MURTAZA2023100546}, which measures the proportion of duplicated records in the synthetic data. This is very important for mental health datasets as any duplicated records will reveal the identity of a patient which can aggravate their mental health. The distributional similarity between synthetic data, training data, and hold-out data can also be used as a privacy measure \cite{MURTAZA2023100546}. If the synthetic data is closer to hold-out data than training data, it shows less chance of privacy leakage.

% \subsubsection*{Data Quality and Utility Evaluation}

% The quality of synthetic data is evaluated based on data faithfulness and diversity. Utility evaluation includes the performance of models trained with synthetic data on downstream tasks. Data diversity is measured on lexical features, semantic features and dialogue topics \cite{qiu2024smilesingleturnmultiturninclusive,qiu-2024}. Data faithfulness measures the closeness of the synthetic data to real data. For this, Qiu et al. \cite{qiu-2024} utilised measures like vocabulary overlap rate and semantic consistency to measure the similarity between actual and simulated patients. However, it is challenging to measure faithfulness only through statistical measures. Thus, human evaluation is necessary. Human evaluation is done by psychology experts or through user ratings. 

% Expert evaluation of transcripts includes rating the transcripts on aspects like content naturalness, empathy level, helpfulness and safety \cite{chen-etal-2023-soulchat}. Cactus \cite{lee-etal-2024-cactus} relies on human experts to evaluate the counselling agents' general counselling skills and CBT-specific skills. On the other hand, patient chatbots are rated by experts on their resemblance and rationality when compared to actual patients \cite{chen2023llmempoweredchatbotspsychiatristpatient}. Other methods include ranking the generated synthetic data in comparison to other synthetic data \cite{lozoya-etal-2024-generating}, four-level rating systems \cite{wang-etal-2023-self-instruct}, and Elo scores \cite{elo}. User ratings for psychiatrist simulation or chatbot are based on fluency, empathy, expertise and engagement \cite{chen2023llmempoweredchatbotspsychiatristpatient}. User ratings for multimodal synthetic data judge the naturalness of generated speech, generated gestures and their coherence and plausibility for the context \cite{Mehta_2024_CVPR,Mughal_2024_CVPR}. A ranking system with different generated data can also be used \cite{Ng_2024_CVPR}.

% However, these methods require a lot of time commitment from human experts. Thus, various works utilise LLMs to automatically rate generated data on perspectives like comprehensiveness, professionalism, authenticity, and safety \cite{qiu-2024} or based on psychological measures like working alliance inventory \cite{zhang2024cpsycounreportbasedmultiturndialogue}. LLMs as judges with elo scores are also used \cite{qiu-2024}. Cactus \cite{lee-etal-2024-cactus} also showed that GPT-4o shows potential to assess the quality of generated counselling sessions.

% The utility of generated synthetic data can be evaluated by the performance of models trained with synthetic data on downstream tasks \cite{Synth_Soc_1,PTSD_synth}. Data augmentation with generated data is also often used to evaluate the effectiveness of using the generated synthetic data. 
% \fi

%%%%%%%%%%%%%%%%%%%
\subsection*{Synthetic data generation}
%\todo{I changed this section. Please check if imp info are still intact}

Synthetic data is generated by models trained on real-world data and may inadvertently reveal sensitive information if the models overfit, particularly when the real-world dataset is small (which is the case for most mental health datasets). Overfitting increases the risk of privacy violations, while overly generic synthetic data can reduce utility. Thus, evaluating privacy-utility trade-offs in synthetic data generation is crucial.

\paragraph{Privacy evaluation.}
Privacy evaluation tests synthetic data robustness against membership and attribute inference attacks \cite{goncalves2020generation,privacy-gain,MURTAZA2023100546,synthetic-privacy-all}. Membership inference attacks identify if an individual is part of the real dataset, with outliers being especially vulnerable. This is critical for mental health datasets, where diverse, small participant groups make outlier protection a priority. Metrics like privacy gain \cite{privacy-gain} and outlier similarity \cite{MURTAZA2023100546} are used for outlier privacy evaluation. Ensuring zero duplication of real sessions in synthetic data, measured through reproduction rate \cite{MURTAZA2023100546}, is essential. Further memorization, overfitting and identification metrics include memorization coefficients \cite{mem-coeff} and $\epsilon$-identifiability \cite{eps-identifiability}.
Attribute inference attacks exploit known attributes to deduce sensitive details like mental health conditions. Additional metrics include distance-based metrics \cite{MURTAZA2023100546,synthetic-privacy-all}.

\paragraph{Data quality and utility evaluation.}
Synthetic data quality is assessed through faithfulness (similarity to real-world data) and diversity (lexical, semantic, and topic variation) \cite{qiu2024smilesingleturnmultiturninclusive,qiu-2024}. Faithfulness is measured via vocabulary overlap and semantic consistency\cite{qiu-2024}. However, these need to be supplemented by expert evaluations. Experts rate transcripts on naturalness, empathy, helpfulness, and safety \cite{chen-etal-2023-soulchat}. Other metrics include four-level rating systems \cite{wang-etal-2023-self-instruct}. For multimodal data, ratings assess naturalness of speech, gestures, and their coherence and contextual plausibility \cite{Mehta_2024_CVPR,Mughal_2024_CVPR}.
To reduce reliance on human evaluations, LLMs can automatically rate synthetic data on attributes like professionalism, comprehensiveness, authenticity and safety \cite{zhang2024cpsycounreportbasedmultiturndialogue}, or use psychological measures like the Working Alliance Inventory \cite{qiu-2024}.
The utility is also evaluated through model performance on downstream tasks \cite{Synth_Soc_1,PTSD_synth} using synthetic data or synthetic data augmentation with real data. Ensuring synthetic data utility in mental health applications involves preserving relevant features while maintaining diversity and faithfulness.
%%%%%%%%%%%%%%
% \if 0
% \subsection*{Privacy-aware training}

% The mental health AI models must be trained with privacy-aware training methods. However, such training methods add noise to the model. Thus, a privacy-utility evaluation is required to determine the best model training method that ensures enough privacy and utility.

% \subsubsection*{Privacy Evaluation}

% Privacy evaluation of privacy-aware training methods tests the robustness of trained models against malicious attacks. In language models, this is determined by measuring exposure through canary insertion and inference accuracy in membership inference attacks \cite{context-aware-dp-lm}. An additional measurement is the performance of model embedding used for mental health prediction in predicting private attributes like location, age, gender, or identity \cite{plant-etal-2021-cape,ravuri,pranjal}. Most privacy-aware training methods use DP-SGD \cite{dp-sgd}. In this training method, the value of $\epsilon$ gives a theoretical guarantee of privacy \cite{kerrigan-etal-2020-differentially,yu2022differentially,aud-dp,bu2022differentially}. All these privacy evaluations should be performed to ascertain the privacy of the training methods.

% \subsubsection*{Utility Evaluation}

% Utility evaluation compares the performance of models trained with privacy-aware training methods on downstream mental health diagnosis tasks \cite{context-aware-dp-lm,plant-etal-2021-cape,kerrigan-etal-2020-differentially,yu2022differentially,bu2022differentially,ravuri,pranjal}. However, differential privacy based training also amplifies the unfairness of models \cite{dp-sgd-bad-performance}. Thus, an additional utility evaluation should evaluate the performance of the privacy-aware models on mental health datasets of diverse cultures.
% \fi

\subsection*{Privacy-aware training}

Mental health AI models must employ privacy-aware training methods. However, such methods introduce noise, necessitating a privacy-utility evaluation to identify optimal approaches that balance privacy and utility.

\paragraph{Privacy evaluation.}
Privacy evaluation tests the robustness of trained models against malicious attacks. For language models, this includes measuring exposure through canary insertion and membership inference attack accuracy \cite{context-aware-dp-lm}. Another aspect is assessing whether model embeddings used for mental health predictions inadvertently reveal private attributes like location, age, gender, or identity \cite{plant-etal-2021-cape,ravuri,pranjal}.
Most privacy-aware training methods utilize DP-SGD \cite{dp-sgd}, where the privacy guarantee is theoretically quantified by $\epsilon$-value \cite{kerrigan-etal-2020-differentially,yu2022differentially,aud-dp,bu2022differentially} (which determines the distance within which errors are considered to be zero in SGD). If FL is involved in the training process, its robustness against FL specific attacks \cite{fl-attack1,fl-attack2,fl-attack3} should be determined along with leakage through local model weights and communicated gradients \cite{NAGY2023110475}. Evaluating privacy through these methods ensures the robustness of training techniques. 

\paragraph{Utility evaluation.}
Utility evaluation examines model performance on downstream mental health diagnosis tasks \cite{context-aware-dp-lm,plant-etal-2021-cape,kerrigan-etal-2020-differentially,yu2022differentially,bu2022differentially,ravuri,pranjal}. However, differential privacy training often exacerbates model unfairness \cite{dp-sgd-bad-performance}. Thus, utility evaluations must also consider the performance of privacy-aware models on culturally and demographically diverse mental health datasets.

%%%%%%%%%%%%%%%%%%%%

% \if 0
% \section*{Recommendation}

% Based on the discussion, we present a workflow to build privacy-aware mental health AI models and datasets. The workflow consists of data collection, data anonymisation and synthetic data generation, privacy-utility evaluation of the data, privacy-aware model training, and privacy-utility evaluation of the training method. Figure \ref{fig:workflow} shows the recommended pipeline.

% \textbf{Data collection.} Video recordings of therapy sessions should be collected for training AI models for mental health diagnosis and assisting therapists. However, such recordings violate privacy, putting the patients at risk of identification and impersonation attacks and thus require explicit consent from patients and privacy guarantees from the collecting institute. The consent form should state that the data will be anonymised, stored securely, and used for research purposes only. An ethics committee must also approve the data collection and storage procedure.

% \textbf{Data anonymisation.} Next, this collected data should be passed through a data anonymisation pipeline to ensure that the patients cannot be identified and none of their private data that can be used for impersonation is present. This can be done either through anonymising the real data or generating synthetic data based on the real data. To anonymise the real data, we should train LLMs \cite{gpt-4-deid} to remove and replace PII from therapy session transcripts. However, they show poor generalisation due to the small size of the datasets. Thus, LLMs should be trained with augmented datasets \cite{kim-etal-2024-generalizing,speech_pii_augment}. To remove and replace the same PIIs from audio recordings, we can combine matching audio segments to create the replacements \cite{speech-pii-flechl}. The voice of the therapist and the patient can be anonymised through multi-speaker anonymisation \cite{multi-speaker-anon}. Finally, face anonymisation techniques \cite{face-off,lowkey,foggysight,fawkes} must be used to anonymise the video recording. 
% Alternatively, synthetic data can be generated to ensure that it does not relate to a real person. This can be performed through multimodal LLMs \cite{next-gpt} roleplaying as therapists and patients \cite{qiu-2024}. However, they might take a long time to produce data. Thus, an alternative could be using multimodal LLMs to generate therapy sessions with zero-shot or few-shot prompting \cite{PTSD_synth}.

% \textbf{Privacy-utility evaluation of anonymised data.} Next, the data anonymisation methods need to be evaluated for the privacy-utility trade-off to ensure the privacy leakage and utility of the data to perform mental health diagnosis tasks. PII leakage in the transcript and audio should be measured by the performance of a strong adversarial re-identification model \cite{re-id}. The utility evaluation of transcripts should measure the information preservation of mental health issues. The privacy measures of voice anonymisation should include EER of ASV \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932,multi-speaker-anon}, robustness against attack models \cite{VP2022, tomashenko2024voiceprivacy, OHNN} and unlinkability \cite{shamsabadi:hal-03588932,unlinkability-1,unlinkability-2}. Since multi-speaker anonymisation is used in audio recordings, utility measures should contain WER \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932,multi-speaker-anon}, measures for preservation of emotional features \cite{tomashenko2024voiceprivacy}, intonation \cite{VP2022}, diversity \cite{VP2022,OHNN} and DER \cite{multi-speaker-anon}. The Naturalness of the anonymised should also be determined \cite{VP2022,multi-speaker-anon}. For face anonymisation, the performance face verification models provide a good measure of privacy. The model performance on downstream mental health tasks should also be reported for holistic utility evaluation of the anonymised data.
% Privacy in synthetic data should be measured through the robustness of the synthetic data against membership inference attacks and attribute inference attacks \cite{synthetic_data_privacy_1,goncalves2020generation,synthetic-privacy-all, kaabachi2023can}, outlier leakage \cite{privacy-gain,MURTAZA2023100546} and reproduction rate \cite{MURTAZA2023100546}. The utility evaluation can be performed through LLMs to automatically rate generated data on perspectives like comprehensiveness, professionalism, authenticity, and safety \cite{qiu-2024} or based on psychological measures like working alliance inventory \cite{zhang2024cpsycounreportbasedmultiturndialogue}. Finally, the model performance on downstream mental health tasks should also be reported for holistic utility evaluation of the anonymised data.

% \textbf{Privacy-aware training.} In the next step, the anonymised data will be used to train multimodal mental health methods for diagnosis. Multimodal mental health methods generally use pre-trained models and fine-tune them on mental health datasets. To ensure privacy-aware training, differential privacy-based fine-tuning methods should be used \cite{kerrigan-etal-2020-differentially, yu2022differentially}. An alternative method uses pre-trained models to generate encodings and train single modality and fusion layers on top of them to perform downstream tasks. DP-SGD \cite{dp-sgd} should be used to train these layers to ensure privacy.

% \textbf{Privacy-utility measure for privacy-aware training.} Finally, the training methods should be evaluated to determine the privacy-utility trade-off. Privacy measurements include the performance of membership inference attack against the model \cite{context-aware-dp-lm}. Another measure of privacy is the value $\epsilon$ used in differential privacy \cite{kerrigan-etal-2020-differentially,yu2022differentially,aud-dp,bu2022differentially}. It gives a theoretical privacy guarantee for the trained model. For utility, the model is evaluated on downstream mental health diagnosis tasks  \cite{context-aware-dp-lm,plant-etal-2021-cape,kerrigan-etal-2020-differentially,yu2022differentially,bu2022differentially,ravuri,pranjal}.
% \fi

\begin{figure}[!ht]
\centering
\includegraphics[width=\linewidth]{workflow_privacy_2.pdf}
\caption{Our proposed pipeline for data collection and model training to develop privacy-aware mental health AI models.}
\label{fig:workflow}
\end{figure}

\section*{Recommendations}
%\todo{I modified this section. This section is still weak. We need to say something new here (Done)}
Based on the advances and pitfalls of existing studies, we recommend a comprehensive workflow for developing privacy-aware mental health AI models and datasets. The workflow involves data collection, data anonymization as well as synthetic data generation, privacy-utility evaluation of the data, privacy-aware model training, and evaluation of the privacy-utility trade-off in the training process. Figure \ref{fig:workflow} shows the recommended pipeline.

\paragraph{Data collection.}
The first step in building mental health AI systems is collecting video recordings of therapy sessions. However, due to the sensitive nature of this data, there is a high risk of privacy breaches, including identification and impersonation attacks. To mitigate these risks, explicit, informed consent from patients is mandatory. The consent form should specify that the data will be anonymized, stored securely, and used exclusively for research purposes. Additionally, an ethics committee must review and approve the data collection and storage procedures to ensure compliance with privacy regulations and ethical standards. The audio recording should be performed using two channel recorder, such that therapist and patient voice can be untangled easily. The video recording should be focused on the patient showing their full face and posture so that facial expressions, gaze and body language can be studied. The recorded sessions should be transcribed by involved researchers or through local ASR systems to assure privacy.
%\todo{whats new here}
\paragraph{Data anonymization and synthetic data generation.}
Once collected, the data must be anonymized or replaced with synthetic data to protect patient privacy. This decision is based on the dataset size and diversity. Training transformer models requires a large amount of diverse data for generalization. Therefore, if a dataset contains a small number of participants or less diversity among participants, synthetic data should be generated to improve the data utility. The generated data can also be augmented with real datasets for training models. In case the dataset already contains a large number of diverse data points, only data anonymization suffices. Anonymization of real data involves processing text, audio, and video to remove or replace PII. For text transcripts, LLMs can be employed to detect and redact PII \cite{gpt-4-deid}. However, their performance on small datasets may be limited, necessitating training on augmented datasets for improved generalization \cite{kim-etal-2024-generalizing,speech_pii_augment,lstm_bert_pii}. For audio recordings, PII can be replaced by synthesizing matching audio segments \cite{speech-pii-flechl}, and multi-speaker anonymization techniques can be used to disguise voices while preserving conversational dynamics \cite{multi-speaker-anon}. Video recordings should undergo face anonymization using advanced methods such as FIVA \cite{Rosberg_2023_ICCV}. Alternatively, synthetic data can be generated to ensure it does not relate to real individuals. This can be achieved using multimodal LLMs \cite{next-gpt} capable of role-playing as therapists and patients \cite{qiu-2024}. Alternatively, these models can be used to generate realistic therapy sessions through zero-shot or few-shot prompting techniques \cite{PTSD_synth}, ensuring the generated data bears no resemblance to actual individuals.

\paragraph{Privacy-utility evaluation of anonymized and synthetic data.}
To ensure the efficacy and safety of the data, it is necessary to evaluate the privacy-utility trade-off after anonymization or synthetic data generation. Privacy evaluation of anonymized data includes testing re-identification risks using adversarial models \cite{re-id}, other related datasets \cite{cross-dataset-identity} and measuring the effectiveness of techniques like multi-speaker anonymization through metrics such as EER \cite{VP2022,tomashenko2024voiceprivacy,OHNN,champion:hal-03753746,shamsabadi:hal-03588932} and FAR \cite{multi-speaker-anon} in speaker verification. Voice anonymization should also be evaluated for unlinkability \cite{shamsabadi:hal-03588932,unlinkability-2} and robustness against attacks \cite{VP2022,tomashenko2024voiceprivacy,OHNN}. For video recordings, the privacy risks can be assessed using face verification systems and face reconstruction attacks\cite{Rosberg_2023_ICCV} to determine the degree of obfuscation.
Synthetic data must be rigorously tested against membership inference attacks and attribute inference attacks to ensure it does not inadvertently reveal details of the real dataset \cite{goncalves2020generation,privacy-gain,MURTAZA2023100546,synthetic-privacy-all}. Outlier leakage is another critical concern, especially in mental health datasets, where outliers are more prevalent due to the diversity and small size of participant groups. Metrics such as privacy gain \cite{privacy-gain}, outlier similarity \cite{MURTAZA2023100546}, and reproduction rate \cite{MURTAZA2023100546} are effective for evaluating these risks. These empirical privacy evaluations should be performed on different languages, cultures and demographics to obtain a more holistic idea about the privacy guarantees. Moreover, multimodal privacy measures need to be developed to understand and test cross-modal vulnerabilities. The empirical privacy measures should also be accompanied by theoretical guarantees similar to $\epsilon$-value in DP.
Utility evaluation should assess the usefulness of the data for mental health diagnosis tasks, focusing on information preservation \cite{SANCHEZ2014189,re-id}, intonation preservation \cite{VP2022}, conversational diversity \cite{VP2022,OHNN}, naturalness \cite{VP2022,multi-speaker-anon}, and emotional feature retention \cite{tomashenko2024voiceprivacy,vid_anon}. LLMs can be leveraged to automatically evaluate the utility of synthetically generated data on dimensions such as comprehensiveness, professionalism, authenticity, and safety \cite{zhang2024cpsycounreportbasedmultiturndialogue} or psychological measures like the working alliance inventory \cite{qiu-2024}.
\paragraph{Privacy-aware model training.} Privacy-preserving methods, particularly differential privacy, are crucial during this stage. %DP-SGD is a widely adopted approach that adds noise to gradients during training, ensuring that individual data points are not memorized by the model \cite{dp-sgd}. The $\epsilon$ parameter in DP provides a theoretical guarantee of privacy, and its value must be carefully tuned to balance privacy with model utility \cite{kerrigan-etal-2020-differentially, yu2022differentially,aud-dp,bu2022differentially}. An alternative approach involves using pre-trained models to extract embeddings from anonymized or synthetic data. These embeddings can then be used to train lightweight layers for specific mental health diagnosis tasks, reducing the risk of privacy leakage while maintaining high performance.
Mental health models either consist of pre-trained models fine-tuned on mental health data or use pre-trained models to extract embeddings to train lightweight modality fusion layers for specific mental health diagnosis tasks. In the first approach, pre-trained models should use differential privacy-based fine-tuning methods \cite{kerrigan-etal-2020-differentially, yu2022differentially} to ensure privacy. In the second approach, fusion layers should be trained with DP-SGD \cite{dp-sgd} to ensure no privacy leakages from the trained layers. For even greater privacy Local Differential Privacy for Federated Learning (LDP-FL) \cite{NAGY2023110475,dp-fl1} can be used. This can be especially useful when datasets from different institutes are involved and they are required to be stored in the collected institutes for privacy.
\paragraph{Privacy-utility evaluation of privacy-aware training.}
Finally, the trained models must be evaluated for their privacy-utility trade-off. Privacy measurements include testing the models against membership inference attacks \cite{context-aware-dp-lm} and analyzing the theoretical guarantees provided by the $\epsilon$ value in differential privacy \cite{kerrigan-etal-2020-differentially,yu2022differentially,aud-dp,bu2022differentially}. To assess utility, the models should be evaluated on downstream mental health diagnosis tasks \cite{context-aware-dp-lm,plant-etal-2021-cape,kerrigan-etal-2020-differentially,yu2022differentially,bu2022differentially,ravuri,pranjal}. Additionally, testing on diverse datasets can help identify any biases or disparities amplified during the training process \cite{dp-sgd-bad-performance}.


% \if 0

% \section*{Prospects}

% The discussion shows various prospects for future work to develop privacy-aware mental health AI models. Some of these works include multimodal data anonymization, multimodal synthetic data generation and the development of privacy-utility measures for multimodal data and models.

% \textbf{Multimodal data anonymization.} While data anonymization has been extensively studied in separate modalities like text, audio and video, there has been no work in multimodal data anonymization. Cross-modal features might reveal information about other modalities. Like lip reading through video might give away a PII, which was scrubbed in the text and audio. Thus, such cross-modal vulnerabilities must be studied by developing adversarial multimodal re-identification models and solutions for such vulnerabilities.

% \textbf{Multimodal synthetic data generation for mental health datasets.} Current multimodal synthetic methods do not contain any psychiatric knowledge. They generate synthetic data based on only patient information. Adding cognitive models like CBT similar to Patient-$\psi$ \cite{patient-psi} can improve the multimodal synthetic data generation. Moreover, current multimodal synthetic data is initially generated as text and then passed through text-to-speech and video synthesis models to create multimodal data. However, this can be improved with multimodal synthetic data generators that can generate synthetic therapy videos using multimodal LLMs \cite{next-gpt}.

% \textbf{Privacy-utility evaluations.} While privacy and utility evaluations have been studied in separate modalities, research on privacy-utility trade-offs of multimodal models has been lacking. Cross-modal features might introduce new vulnerabilities which need to be evaluated. Also, no prior work has compared the privacy-utility trade-offs of data anonymization and synthetic data. Moreover, some data anonymization methods struggle with specific demographics \cite{demogarphic_face_privacy}, and privacy-aware training tends to amplify the unfairness of models \cite{dp-sgd-bad-performance}. Thus, the privacy-utility evaluation should be performed with mental health datasets containing different demographics, cultures and languages.
% \fi

\section*{Prospects}
%\todo{I rephrased this section. This is still weak. Need more points (Done)}
%The discussion highlights several promising avenues for advancing privacy-aware mental health AI models. These include developing methods for multimodal data anonymization, enhancing multimodal synthetic data generation for mental health datasets, and establishing comprehensive privacy-utility evaluation frameworks for multimodal data and models.

\paragraph{Multi-Speaker Anonymization (MSA).} While Miao et al. \cite{multi-speaker-anon} provided a benchmark for MSA, they assume weak attack models where the attacker does not have knowledge about the used anonymization scheme. However, in real-life situations, the attacker might possess knowledge of the anonymization strategy, and thus, privacy evaluation of MSA should be performed with stronger adversaries and subsequently develop better anonymization strategies. Moreover, overlapping segments in multi-speaker conversations like therapy sessions present another vulnerability that can be utilized by attackers. While Miao et al. \cite{multi-speaker-anon} tested the ability of attackers to infer speakers from overlapping segments, in real-life situations, attackers might possess the ability to separate the speakers in overlapping segments and identify them. This shows the need to create stronger MSA schemes.
\paragraph{Anonymization in video.} Current video anonymization methods show vulnerability to leaking the gender and age of people even after face obfuscation \cite{human_attribute_privacy}. Mental health datasets might contain very few participants from a certain gender or age group, thus leaking such private information could lead to identification. Recording the body language of patients could help in mental health diagnosis; however, it can also reveal the gender of the patient if only face anonymization is performed \cite{gender_leak_face_obfus}. Moreover, current methods are also prone to demographic unfairness \cite{demogarphic_face_privacy}. Thus it is essential to develop fair and improved video anonymization techniques that can prevent leakage of private information like age and gender.
\paragraph{Theoretical guarantees in data anonymization.} While we discuss various data anonymization processes for text, audio and video modalities, most of them do not provide any theoretical guarantees like DP provides in privacy-aware model training. In text modality, word-level or sentence-level perturbations through DP provide theoretical guarantees\cite{nap2}. However, they significantly reduce the utility of the text \cite{nap2}, necessitating better anonymization techniques with privacy guarantees for text. For voice anonymization, Shamsabadi et al. \cite{shamsabadi:hal-03588932} provided privacy guarantees in single-speaker anonymization settings. However, therapy sessions require multi-speaker anonymization, leaving room to create a multi-speaker anonymization algorithm with privacy guarantees. DP-based face anonymization techniques already exist for images \cite{img-dp}. For videos DP-based methods focus on making two objects within the video indistinguishable \cite{vid-dp}. However, its direct applicability for preventing facial recognition in therapy videos is unclear. Thus the method needs to go through privacy and utility evaluations and we need to develop more specialized DP-based video face anonymization methods. In synthetic data generation DP-based methods for theoretical guarantees have been explored for text \cite{yue-etal-2023-synthetic,nahid2024safesynthdp}, tabular data \cite{SUN2023104404,qian2024synthetic}, multimodal tabular and 3D image data \cite{ziegler2022multimodal} generation and with FL \cite{dp-fl-gan}. However, no DP-based methods have been developed for multimodal therapy session generation.
\paragraph{Multimodal data anonymization.}
Although significant progress has been made in anonymizing individual data modalities, such as text, audio, and video in isolation, there remains a lack of research on anonymization techniques for multimodal data. Multimodal datasets inherently carry cross-modal features, where information from one modality may inadvertently expose sensitive details from another. For instance, lip-reading from video can reveal PII that is removed from the text and audio modalities. Addressing these cross-modal vulnerabilities requires the development of adversarial multimodal re-identification models that can identify such risks and inspire solutions. Effective approaches will need to account for the interplay between modalities and ensure comprehensive anonymization across all channels of information.
\paragraph{Multimodal synthetic data generation.}
Current methods for generating multimodal synthetic data often lack integration with psychiatric knowledge, which limits their utility in mental health applications. Existing techniques typically generate synthetic data based solely on patient characteristics without incorporating cognitive models or therapeutic frameworks. Enhancing these methods with CBT-based models, akin to Patient-$\psi$ \cite{patient-psi}, could significantly improve the quality and relevance of the generated data.
Additionally, most approaches rely on a sequential process, where synthetic text is generated first and then converted into audio and video using text-to-speech and video synthesis models. This pipeline can introduce inconsistencies and reduce authenticity. The development of multimodal synthetic data generators capable of producing therapy videos directly through advanced multimodal LLMs \cite{next-gpt} represents a critical next step. Such systems could generate more cohesive and realistic synthetic data that better supports mental health research and applications.
\paragraph{Privacy-utility evaluations for multimodal data and models.}
While privacy-utility evaluations have been explored for individual data modalities, there is a significant gap in understanding the trade-offs for multimodal data and models. The integration of cross-modal features may introduce unique vulnerabilities that require specialized evaluation frameworks. Additionally, comparative studies on the privacy-utility trade-offs between anonymized and synthetic data have yet to be conducted. Another challenge lies in addressing demographic-specific limitations. Some data anonymization methods perform poorly for certain demographic groups, such as those with distinct facial features \cite{demogarphic_face_privacy}. Similarly, differential privacy-based training methods often exacerbate fairness issues, amplifying biases against underrepresented populations \cite{dp-sgd-bad-performance}. To tackle these challenges, privacy-utility evaluations must be conducted on diverse mental health datasets representing different demographics, cultures, and languages. This will ensure that privacy-preserving methods are inclusive, equitable, and effective across varied contexts.
\paragraph{Local Differential Privacy for Federated Learning (LDP-FL).} While LDP-FL has been explored in recent times \cite{NAGY2023110475,dp-fl1}, there has not been enough privacy-utility evaluation in mental health tasks. Basu et al. \cite{dp-fl-benchmark} demonstrated that LDP-FL experiences greater utility degradation when applied to realistic data resembling medical datasets, small datasets, or large models. Therefore, a LDP-FL setup with an improved privacy-utility trade-off is necessary. Additionally, no existing work compares the privacy performance of DP methods with LDP-FL methods, a comparison essential for determining the most suitable approach.

%%%%%%%%%%%%%%%%
\section*{Conclusion}
%\todo{I modified. Pls check this section (Done: added a line about voice and face anonymisation)}
% \if 0
% This work discusses the difficulty of training and deploying AI models for real-world mental health diagnosis due to the privacy issues surrounding mental health data and private data leakages from trained models. We explored various solutions to overcome privacy issues, such as data anonymization, synthetic data generation, and privacy-aware model training. We also discussed the methods to evaluate the privacy and utility of these methods. Finally, we proposed a recommended pipeline for developing privacy-aware mental health models and present research prospects for its development. We believe this will progress research towards privacy-aware mental health AI models that could be deployed in clinics to help improve the quality and convenience of receiving therapy.
% \fi


This paper highlights significant challenges of training and deploying AI models for real-world mental health diagnosis due to the sensitive nature of mental health data and the risks of private data leakage from trained models. To address these challenges, we examined key solutions, including data anonymization pipelines to remove PII, voice and face anonymization in therapy recordings, methods for generating synthetic data that replicate real-world scenarios without exposing real individuals, and differential privacy-based approaches for privacy-aware model training. Additionally, we detailed evaluation frameworks to assess the privacy and utility trade-offs of these methods, ensuring they maintain clinical relevance while safeguarding patient confidentiality.
%
We proposed a comprehensive pipeline for developing privacy-aware mental health AI models, encompassing data collection, anonymization, synthetic data generation, privacy-utility evaluations, and privacy-aware training. This workflow aims to balance privacy protection with the utility required for effective mental health diagnosis and therapy assistance. 
%
Finally, we identified research prospects, such as advancing multimodal data anonymization techniques to address cross-modal vulnerabilities, improving synthetic data generation by integrating psychiatric knowledge and multimodal capabilities, and establishing robust evaluation frameworks for diverse demographics and cultures. These advancements will lay the groundwork for deploying privacy-preserving mental health AI systems in clinical settings, enabling better access to therapy while upholding the highest standards of data privacy and security.



\section*{Author Contributions} I.G., A.M. and T.C. contributed to conceptualizing the manuscript. A.M. led the effort
of writing the initial draft of the manuscript. I.G., A.M. and T.C. finalized the manuscript. Tc. and I.G. supervised the project.

\section*{Funding Information}
This research work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. This work has also been funded by the LOEWE Distinguished Chair ``Ubiquitous Knowledge Processing'', LOEWE initiative, Hesse, Germany (Grant Number: LOEWE/4a//519/05/00.002(0002)/81). T.C. acknowledges the support of Tower Research Capital Markets toward using machine learning for social good and Rajiv Khemani Young Faculty Chair Professorship in Artificial Intelligence.
 



\section*{Competing Interests}
The authors declare no competing interests.

\section*{Additional Information}

\noindent{\bf Materials \& Correspondence} should be emailed to Tanmoy Chakraborty (\url{tanchak@iitd.ac.in}) and Iryna Gurevych (\url{iryna.gurevych@tu-darmstadt.de}).


\bibliography{sample}

% \noindent LaTeX formats citations and references automatically using the bibliography records in your .bib file, which you can edit via the project menu. Use the cite command for an inline citation, e.g.  \cite{Hao:gidmaps:2014}.

% For data citations of datasets uploaded to e.g. \emph{figshare}, please use the \verb|howpublished| option in the bib entry to specify the platform and the link, as in the \verb|Hao:gidmaps:2014| example in the sample bibliography file.

% \section*{Acknowledgements (not compulsory)}

% Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.

% \section*{Author contributions statement}

% Must include all authors, identified by initials, for example:
% A.A. conceived the experiment(s),  A.A. and B.A. conducted the experiment(s), C.A. and D.A. analysed the results.  All authors reviewed the manuscript. 

% \section*{Additional information}

% To include, in this order: \textbf{Accession codes} (where applicable); \textbf{Competing interests} (mandatory statement). 

% The corresponding author is responsible for submitting a \href{http://www.nature.com/srep/policies/index.html#competing}{competing interests statement} on behalf of all authors of the paper. This statement must be included in the submitted article file.

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Condition & n & p \\
% \hline
% A & 5 & 0.1 \\
% \hline
% B & 10 & 0.01 \\
% \hline
% \end{tabular}
% \caption{\label{tab:example}Legend (350 words max). Example legend text.}
% \end{table}

% Figures and tables can be referenced in LaTeX using the ref command, e.g. Figure \ref{fig:stream} and Table \ref{tab:example}.

\end{document}