% ###############################################
% Start of file - body.tex
% ###############################################

% ===============================================
% Section
% ===============================================
\section{Introduction}
\label{sec:introduction}
One of the important activities involved in a successful strategy towards predictive maintenance for industrial Cyber-Physical Systems (CPS) is anomaly detection and identification. Examples of such systems are semiconductor photolithography machines, production printing machines, die bonder machines, and so forth. What these systems all have in common is the presence of highly complex, multi-node compute and control elements, limited domain of operational tasks (highly purpose-built), and continuous high yield targets for machine production output.

In the context of industrial CPS, data-centric solutions consuming time-series data from machine sensors, have proven to be highly capable~\cite{Odyurt:2022:IRIC}. For such solutions, there are numerous data processing and Machine Learning algorithms suitable for time-series data analysis, to choose from. Generally speaking, with industrial CPS, we also have the abundance of available data, which can be collected from a multitude of available sensors, especially in modern CPS, while the machine operates. Needless to say, these machines are intended to operate non-stop, at full capacity, requiring any data collection and monitoring to be well-planned.

Contrary to one's initial assumption, the abundance of data becomes a challenge. Besides the complexities and resource cost imposed with excessive data collection, high amounts of data does not necessarily lead to better prediction. As such, \emph{it is highly advantageous to be able to select the right data processing steps, choose the best ML algorithm, and focus on the most effective portion of the data}.

It is even more advantageous to know which of the above ingredients (data processing, ML algorithm and data subset) match and work best, allowing for the selection of the most effective combination, should one ingredient be restricted. For instance, if we are limited to a specific part of data, the best complementary ML algorithm shall be considered. \emph{Most importantly, we want to know all such compatibilities upfront}.

\paragraph*{Contribution}
We introduce the first iteration of our \emph{InfoPos framework}, intended to support designers and engineers in the selection of most effective elements when building ML-assisted solutions for industrial Cyber-Physical Systems (CPS). Examples of such element variations are the type of ML algorithm, data processing/transformation steps applied, or the  level of these steps, and the considered portion of data. We demonstrate the use of InfoPos framework within the context of an anomaly identification use-case. Our results are based on real data and our data processing code, as well as the generated data sets, are made publicly available. In short, we provide:
%
\begin{itemize}
	\item The InfoPos framework as a pre-design support tool for ML-assisted solution design fine-tuning.
	\item Preliminary results from a real-world platform, as our demonstrator use-case, covering numerous combinations of available knowledge, available data and traditional ML algorithms.
	\item Publicly available processed data sets~\cite{Odyurt:2025:DATASET} and the data workflow code~\cite{Odyurt:2025:CODE}, covering the data processing and ML model training.
\end{itemize}

% ===============================================
% Section
% ===============================================
\section{Background and definitions}
\label{sec:background}
To explain our perspective and what we consider roles of knowledge and data are in shaping data-centric and ML-assisted solutions, it is important to clarify the terminology first. Throughout this paper, what we consider as \emph{data} is primarily metric traces collected from a multitude of available sensors, a.k.a., Extra-Functional Behaviour metrics. Industrial CPS machines, especially modern ones, are equipped with sensors, mainly intended for product quality control. We consider both individual hardware sensors, e.g., a torque measuring sensor, a voltage collector, or a temperature sensor, and software sensors. The latter refers to system resource monitoring virtual metric collectors to record variables such as computational time, memory usage and so forth. This type of sensing will be the case for the compute and control elements.

What we consider as \emph{knowledge} can be sourced from different artefacts, e.g., blueprints, system/machine logs (not to be confused with traces), design documentation. System knowledge reveals its operational sequence, characteristics, applied configuration, input material parameters, and physical environment specifics. For example, size and type of input, production rate (which could be translated to frequency or required yield), machine cycle steps and their order, are all parts of this knowledge.

\subsection{Knowledge and data}
We consider the two major dimensions influencing the design and the effectiveness of ML-assisted solutions, or rather most data processing solutions, to be the \emph{knowledge position} and the \emph{data position}. In this context, the knowledge position refers to the level of understanding present of the system's internals, its interactions with the physical domain, and how it related to any accompanying data. Similarly, the data position refers to how extensive, complete, and granular the collected or available data is. The data position provides the level of qualities such as descriptiveness, comprehensiveness and accuracy\footnote{By accuracy we refer to the absence/presence of noise.} of collected data.

Both dimensions are to be considered as a spectrum, spanning from a poor state to a rich one. To provide examples of opposing states for knowledge, as depicted in \Cref{fig:knowledge_spectrum}, abstract and black-box versus descriptive and white-box representations come to mind. For data, as shown in \Cref{fig:data_spectrum}, we can think of coarse or incomplete versus granular or comprehensive data.
%
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\linewidth}
    	\centering
	    \includegraphics[width=0.7\linewidth]{figures/knowledge_spectrum.pdf}
	    \caption{Knowledge spectrum with representative extremities.}
	    \label{fig:knowledge_spectrum}
    \end{subfigure}
    \qquad
    \begin{subfigure}{\linewidth}
    	\centering
    	\includegraphics[width=0.7\linewidth]{figures/data_spectrum.pdf}
		\caption{Data spectrum with representative extremities.}
		\label{fig:data_spectrum}
    \end{subfigure}
	\caption{Knowledge and data positions as the two main dimensions affecting data-centric solutions.}
	\label{fig:spectrums}
\end{figure}

\subsection{Information positions}
With both dimensions taken into account, any solution design task could land on either of the cells from the $3 \times 3$ quadrant given in \Cref{fig:infopos_quadrant}.
%
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/infopos_quadrant.pdf}
	\caption{Information position quadrant resulting from the composition of knowledge and data dimensions.}
	\label{fig:infopos_quadrant}
\end{figure}

Depending on practical circumstances involved with the use-case at hand, one can expand or shrink the quadrant by adding or removing steps to/from each dimension. To simplify our demonstration and to deliver the message, only considering the very extreme cases, is a suitable approach.

% ===============================================
% Section
% ===============================================
\section{Methodology}
\label{sec:methodology}
We consider the demonstrator platform from~\cite{Odyurt:2021:PPFT} and the associated data collected from it as our source. The main advantage of this platform is the collection of real and balanced data, i.e., not synthetic. Though the scale of the platform is small, it reflects the real-world task of continuous live image processing. Image analysis using a pre-trained ML model is performed as a computational workload (not to be mistaken with ML models used in our anomaly identification flow) to detect the presence of cars in various parking areas.

The data collection experimental set-up is covered in \Cref{fig:demonstrator_setup}, with the presence of a dedicated power data logger with an isolated power supply for accuracy.
%
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/demonstrator_setup.pdf}
	\caption{Data collection from the demonstrator set-up, including a dedicated electrical data logger and with the application of different workloads, as well as different anomalous conditions for individual experiments.}
	\label{fig:demonstrator_setup}
\end{figure}

\subsection{Data processing workflow}
The preprocessing applied to the collected electrical metrics\footnote{Voltage is collected, but not considered.}, i.e., \emph{current}, \emph{power} and \emph{energy}, is depicted in the diagram given in \Cref{fig:data_processing}. Note that a similar preceding workflow generated the Mean Passport information, which will act as the reference point for comparing unknown execution data. Mean Passports are signatures belonging to executions with no anomalies, i.e., normal behaviour (denoted as Normal).
%
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/data_processing.pdf}
	\caption{Our detailed data processing workflow, covering different steps, as well as the in-house simple orchestrator to run the workflow in parallel and at scale.}
	\label{fig:data_processing}
\end{figure*}

Note that the extensive nature of preprocessing is to generate features required for traditional ML algorithms, which has proven to be rather effective.

\subsection{Data set}
The final output from the preprocessing workflow is a labelled data set used for supervised ML model training and testing. Included feature columns are:
%
\begin{itemize}
	\item The time span covered by the data segment, i.e., the cut trace (\texttt{execution\_time}).
    \item Different parameters from linear or quadratic regression functions, representing the data segment (\texttt{coefficient\_2}, \texttt{coefficient\_1}, \texttt{intercept}).
    \item Different goodness-of-fit comparison calculations, quantifying the diversion of the unknown execution data from the reference execution data (\texttt{R2}, \texttt{R2\_absolute\_diff}, \texttt{RMSE}, \texttt{RMSE\_absolute\_diff}).
\end{itemize}

Considering the 8 data collection cases described in~\cite{Odyurt:2021:PPFT}, as well as the three experiment conditions applied, i.e., Normal, NoFan, and UnderVolt, we end up with 24 data collection scenarios. For each scenario, we consider three quartile-based phase cuts (reductions or segmentations if you may), alongside the full phase data (see \Cref{fig:uninformed_segmentation}). As such, there will be 4 phase data cuts per scenario, i.e., \emph{ini}, \emph{mid}, \emph{end}, and \emph{full}, resulting in 96 individual cases to be processed by our workflow. 
%The results of our data processing boils down to data sets organised with data per quartile-based segmentation, i.e., individual data sets for \emph{ini}, \emph{mid}, \emph{end}, and \emph{full} cuts.
Needless to say, it is trivial to combine such data, as the format and headers are the same in all. We apply these data sets separately during ML model training and provide relevant results in separate tables in \Cref{sec:results}.

\subsection{Data segmentation}
One of the steps most dependent on the available knowledge is segmentation (cutting) of data. There can be two segmentation types, informed, which cuts the data into known phases, or uninformed, which lack of the internal operation of the system forces the segmentation to be more simplistic. Both types are depicted in \Cref{fig:data_segmentation}.
%
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\linewidth}
    	\centering
	    \includegraphics[width=\linewidth]{figures/informed_phase_cuts.pdf}
	    \caption{Informed segmentation}
	    \label{fig:informed_segmentation}
    \end{subfigure}
    \qquad
    \begin{subfigure}{\linewidth}
    	\centering
    	\includegraphics[width=\linewidth]{figures/uninformed_segmentation_cuts.pdf}
		\caption{Uninformed segmentation}
		\label{fig:uninformed_segmentation}
    \end{subfigure}
	\caption{Different types of segmentation depending on the availability of the operational knowledge.}
	\label{fig:data_segmentation}
\end{figure}

\paragraph*{Phase-based (informed) segmentation}
Phase-based segmentation is the informed type of segmentation. In our use-case, images are processed as the computational workload. As any, this processing activity is not a single step one. The processing of a single data instance (an image) is covered by the \texttt{cycle-op} phase type, hence, one cycle of operation for this platform. Each cycle is composed of two inner and sequential phase types, \texttt{image-op} and \texttt{neural-op} to load the image and to apply ML inference, respectively. The knowledge of this design and the knowledge of start and end events per phase type allows us to cut the metric data into chunks associated with each phase type. In \Cref{fig:informed_segmentation}, we can consider C1 as a \texttt{cycle-op} phase, composed of A1 and B1 corresponding to \texttt{image-op} and \texttt{neural-op} phases.

\paragraph*{Quartile-based (uninformed) segmentation}
In the absence of such knowledge, segmentation of data based on phase execution time quartiles can be considered. This is a rather simple, but effective, segmentation strategy. Basically any phase type's execution duration can be divided in 4 quartiles. Data contained in the first and the last are considered as \emph{ini} and \emph{end} segment, while the data from the two middle quartiles is the \emph{mid} segment, as shown in \Cref{fig:uninformed_segmentation}. It is important to note that, as a general rule, quartile-based segmentation is applied to phases, which can happen in both informed or uninformed situations. To be true to the uninformed case here, quartile-based segmentation only makes sense for the \texttt{cycle-op} phase type. In an uninformed knowledge position, we will not be aware of sub-phases structure beyond the \texttt{cycle-op} phase. \emph{The motivation behind quartile-based segmentation lies in the presence of cold-start and comparable effects at the start and at the end of most computational tasks}.

\subsection{ML algorithms for anomaly identification}
We have considered an exhaustive collection of traditional ML model types in our experiments. These model types are, Boosted Decision Tree (BDT)~\cite{Friedman:2001:BDT}, Decision Tree (DT)~\cite{Breiman:1984:DT}, Extra Trees (ET)~\cite{Geurts:2006:ET}, Gaussian Naive Bayes (NB), Kernel Support-Vector Machine (SVM), Linear Support Vector Classification (SVC) and Random Forest (RF)~\cite{Breiman:2001:RF}. These model types are utilised as multi-class classifiers and identify the type of system behaviour. We cover the normal behaviour, as well as two anomalous behaviours (NoFan and UnderVolt) in our experiments. Note that our training is supervised and the list of classes can be easily expanded if representative data exists. We consider both prediction accuracy and F1 score for model performance evaluation. As it can be observed in \Cref{sec:results}, traditional ML models are still very capable for this job and very much worth exploring and improving upon.

For our training, we apply 3-fold cross-validation and calculate the average accuracy and average F1 score from all folds. In each experiment, models are trained with specific portions of data, resulting from aforementioned segmentation strategies. Note that while we search for the best model performance, the primary goal is to discover the interplay between different scenario variables making up the information position for that particular scenario.

% ===============================================
% Section
% ===============================================
\section{Results}
\label{sec:results}
Considering the high number of cases, variety of metrics and the number of considered ML model types, we end up with a vast amount of results, of which we only provide the most interesting bit. We have seen in previous research~\cite{Odyurt:2021:PPFT} and repeated the same observation that the most effective metric to consider in these experiments is \emph{electrical current}, leading to highest ML model performances. This is valid throughout. Thus, in the following tables, we only cover results based on the electrical current metric.

Considering that our data set is well-balanced, prediction and F1 score calculations match rather well and either one can be considered as a single model performance metric. We do provide both metrics, but rely on model accuracy to draw our conclusions, which is corroborated by the F1 score as well.

Another point to make is that it is quite clear from our results that tree-based algorithms excel at this type of classification. Tree-based traditional ML algorithms refer to algorithms using decision trees or ensembles of decision tree. As such, we only focus on and provide the results from BDT, DT, ET and RF classifiers.

Detailed results provided in \Cref{tab:model_performance} cover model performance metrics for the aforementioned classifier model types, covering numerous data segments. In particular, results dedicated to each data cut with uninformed segmentation, i.e., \emph{full}, \emph{ini}, \emph{mid} and \emph{end}, are provided separately in \Cref{tab:model_performance_full,tab:model_performance_ini,tab:model_performance_mid,tab:model_performance_end}, respectively. Here, the \emph{full} type is actually the representation of complete data. As it can be seen, all available phase types, as well as their combinations as input for the ML model training is covered. For instance, phase type \enquote{all} refers to the use of data from all three individual phase types, i.e., \texttt{cycle-op}, \texttt{image-op}, and \texttt{neural-op}. Note that the three phase types are the result of informed segmentation, utilising the knowledge from system's internal operation.
%
\begin{table*}[htbp]
    \centering
    \caption{Model performance results for different training data}
    \label{tab:model_performance}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Model performance results for full-cut segmentation, i.e., no segmentation, applied to each phase type}
        \label{tab:model_performance_full}
	    \begin{tabular}{@{}lrrrrrrrr@{}}
	        \toprule
	        \multicolumn{1}{c}{\textbf{Phase type}} & 
	        \multicolumn{1}{c}{\textbf{BDT accuracy}} & 
	        \multicolumn{1}{c}{\textbf{BDT F1}} & 
	        \multicolumn{1}{c}{\textbf{DT accuracy}} &
	        \multicolumn{1}{c}{\textbf{DT F1}} &
	        \multicolumn{1}{c}{\textbf{ET accuracy}} &
	        \multicolumn{1}{c}{\textbf{ET F1}} &
	        \multicolumn{1}{c}{\textbf{RF accuracy}} &
	        \multicolumn{1}{c}{\textbf{RF F1}} \\
	        \midrule
	        \multicolumn{9}{c}{Signature regression type: linear} \\
	        \midrule
	        all						& 95.71\%  & 0.96  & 95.83\%  & 0.96  & 95.99\%  & 0.96  & 96.27\%  & 0.96 \\
	        cycle-op 				& 98.88\%  & 0.99  & 98.40\%  & 0.98  & 98.78\%  & 0.99  & 98.91\%  & 0.99 \\
	        image-op 				& 91.44\%  & 0.91  & 89.96\%  & 0.90  & 91.64\%  & 0.92  & 91.90\%  & 0.92 \\
	        neural-op 				& 99.19\%  & 0.99  & 99.14\%  & 0.99  & 98.93\%  & 0.99  & 99.11\%  & 0.99 \\
	        image-op + neural-op 	& 94.75\%  & 0.95  & 94.22\%  & 0.94  & 95.12\%  & 0.95  & 95.30\%  & 0.95 \\
	        \midrule
	        \multicolumn{9}{c}{Signature regression type: polynomial quadratic} \\
	        \midrule
	        all 					& 96.16\%  & 0.96  & 95.94\%  & 0.96  & 96.44\%  & 0.96  & 96.60\%  & 0.97 \\
	        cycle-op 				& 99.03\%  & 0.99  & 98.78\%  & 0.99  & 99.06\%  & 0.99  & 98.93\%  & 0.99 \\
	        image-op 				& 92.15\%  & 0.92  & 89.89\%  & 0.90  & 92.81\%  & 0.93  & 92.81\%  & 0.93 \\
	        neural-op 				& 99.21\%  & 0.99  & 98.76\%  & 0.99  & 99.11\%  & 0.99  & 99.06\%  & 0.99 \\
	        image-op + neural-op 	& 95.16\%  & 0.95  & 94.49\%  & 0.94  & 95.80\%  & 0.96  & 95.80\%  & 0.96 \\
	        \bottomrule
		\end{tabular}
	\end{subtable}
    %
    \vspace{1em}
	%
	\begin{subtable}{\textwidth}
        \centering
        \caption{Model performance results for ini-cut segmentation, applied to each phase type}
        \label{tab:model_performance_ini}
        \begin{tabular}{@{}lrrrrrrrr@{}}
            \toprule
            \multicolumn{1}{c}{\textbf{Phase type}} & 
            \multicolumn{1}{c}{\textbf{BDT accuracy}} & 
            \multicolumn{1}{c}{\textbf{BDT F1}} & 
            \multicolumn{1}{c}{\textbf{DT accuracy}} &
            \multicolumn{1}{c}{\textbf{DT F1}} &
            \multicolumn{1}{c}{\textbf{ET accuracy}} &
            \multicolumn{1}{c}{\textbf{ET F1}} &
            \multicolumn{1}{c}{\textbf{RF accuracy}} &
            \multicolumn{1}{c}{\textbf{RF F1}} \\
            \midrule
            \multicolumn{9}{c}{Signature regression type: linear} \\
	        \midrule
            all               		& 93.67\%  & 0.94  & 93.12\%  & 0.93  & 93.85\%  & 0.94  & 94.10\%  & 0.94 \\
            cycle-op          		& 97.79\%  & 0.98  & 97.89\%  & 0.98  & 97.61\%  & 0.98  & 97.59\%  & 0.98 \\
            image-op          		& 86.48\%  & 0.86  & 83.00\%  & 0.83  & 86.36\%  & 0.86  & 86.76\%  & 0.87 \\
            neural-op         		& 98.91\%  & 0.99  & 98.76\%  & 0.99  & 98.65\%  & 0.99  & 98.81\%  & 0.99 \\
            image-op + neural-op 	& 92.44\%  & 0.92  & 91.03\%  & 0.91  & 92.35\%  & 0.92  & 92.67\%  & 0.93 \\
            \midrule
	        \multicolumn{9}{c}{Signature regression type: polynomial quadratic} \\
	        \midrule
	        all               		& 94.44\%  & 0.94  & 93.55\%  & 0.94  & 94.92\%  & 0.95  & 94.95\%  & 0.95 \\
            cycle-op          		& 98.32\%  & 0.98  & 97.54\%  & 0.98  & 98.12\%  & 0.98  & 98.32\%  & 0.98 \\
            image-op          		& 88.54\%  & 0.88  & 85.21\%  & 0.85  & 88.52\%  & 0.88  & 88.95\%  & 0.89 \\
            neural-op         		& 99.14\%  & 0.99  & 98.45\%  & 0.98  & 99.06\%  & 0.99  & 98.98\%  & 0.99 \\
            image-op + neural-op 	& 93.18\%  & 0.93  & 92.26\%  & 0.92  & 93.84\%  & 0.94  & 93.95\%  & 0.94 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    %
    \vspace{1em}
	%
    \begin{subtable}{\textwidth}
        \centering
        \caption{Model performance results for mid-cut segmentation, applied to each phase type}
        \label{tab:model_performance_mid}
        \begin{tabular}{@{}lrrrrrrrr@{}}
            \toprule
            \multicolumn{1}{c}{\textbf{Phase type}} & 
            \multicolumn{1}{c}{\textbf{BDT accuracy}} & 
            \multicolumn{1}{c}{\textbf{BDT F1}} & 
            \multicolumn{1}{c}{\textbf{DT accuracy}} &
            \multicolumn{1}{c}{\textbf{DT F1}} &
            \multicolumn{1}{c}{\textbf{ET accuracy}} &
            \multicolumn{1}{c}{\textbf{ET F1}} &
            \multicolumn{1}{c}{\textbf{RF accuracy}} &
            \multicolumn{1}{c}{\textbf{RF F1}} \\
            \midrule
            \multicolumn{9}{c}{Signature regression type: linear} \\
	        \midrule
            all               		& 94.88\%  & 0.95  & 94.51\%  & 0.95  & 95.16\%  & 0.95  & 95.13\%  & 0.95 \\
            cycle-op          		& 98.53\%  & 0.99  & 98.45\%  & 0.98  & 98.37\%  & 0.98  & 98.48\%  & 0.98 \\
            image-op          		& 88.41\%  & 0.88  & 85.44\%  & 0.85  & 88.34\%  & 0.88  & 88.62\%  & 0.89 \\
            neural-op         		& 99.14\%  & 0.99  & 99.16\%  & 0.99  & 98.78\%  & 0.99  & 98.98\%  & 0.99 \\
            image-op + neural-op 	& 93.31\%  & 0.93  & 91.92\%  & 0.92  & 93.50\%  & 0.93  & 93.75\%  & 0.94 \\
            \midrule
	        \multicolumn{9}{c}{Signature regression type: polynomial quadratic} \\
	        \midrule
	        all               		& 95.14\%  & 0.95  & 94.60\%  & 0.95  & 96.06\%  & 0.96  & 95.98\%  & 0.96 \\
            cycle-op          		& 99.11\%  & 0.99  & 98.65\%  & 0.99  & 98.93\%  & 0.99  & 99.01\%  & 0.99 \\
            image-op          		& 89.48\%  & 0.89  & 87.30\%  & 0.87  & 90.17\%  & 0.90  & 90.04\%  & 0.90 \\
            neural-op         		& 99.54\%  & 1.00  & 99.16\%  & 0.99  & 99.19\%  & 0.99  & 99.42\%  & 0.99 \\
            image-op + neural-op 	& 94.03\%  & 0.94  & 92.71\%  & 0.93  & 94.74\%  & 0.95  & 94.66\%  & 0.95 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    %
    \vspace{1em}
	%
    \begin{subtable}{\textwidth}
        \centering
        \caption{Model performance results for end-cut segmentation, applied to each phase type}
        \label{tab:model_performance_end}
        \begin{tabular}{@{}lrrrrrrrr@{}}
            \toprule
            \multicolumn{1}{c}{\textbf{Phase type}} & 
            \multicolumn{1}{c}{\textbf{BDT accuracy}} & 
            \multicolumn{1}{c}{\textbf{BDT F1}} & 
            \multicolumn{1}{c}{\textbf{DT accuracy}} &
            \multicolumn{1}{c}{\textbf{DT F1}} &
            \multicolumn{1}{c}{\textbf{ET accuracy}} &
            \multicolumn{1}{c}{\textbf{ET F1}} &
            \multicolumn{1}{c}{\textbf{RF accuracy}} &
            \multicolumn{1}{c}{\textbf{RF F1}} \\
            \midrule
            \multicolumn{9}{c}{Signature regression type: linear} \\
	        \midrule
            all               		& 95.10\%  & 0.95  & 95.03\%  & 0.95  & 95.57\%  & 0.96  & 95.75\%  & 0.96 \\
            cycle-op          		& 98.45\%  & 0.98  & 98.20\%  & 0.98  & 98.35\%  & 0.98  & 98.40\%  & 0.98 \\
            image-op          		& 89.86\%  & 0.90  & 88.08\%  & 0.88  & 89.91\%  & 0.90  & 90.37\%  & 0.90 \\
            neural-op         		& 98.76\%  & 0.99  & 98.53\%  & 0.99  & 98.37\%  & 0.98  & 98.60\%  & 0.99 \\
            image-op + neural-op 	& 93.75\%  & 0.94  & 93.13\%  & 0.93  & 94.11\%  & 0.94  & 94.27\%  & 0.94 \\
            \midrule
	        \multicolumn{9}{c}{Signature regression type: polynomial quadratic} \\
	        \midrule
	        all               		& 94.48\%  & 0.94  & 94.94\%  & 0.95  & 96.11\%  & 0.96  & 96.12\%  & 0.96 \\
            cycle-op          		& 98.48\%  & 0.98  & 97.99\%  & 0.98  & 98.40\%  & 0.98  & 98.32\%  & 0.98 \\
            image-op          		& 89.13\%  & 0.89  & 88.77\%  & 0.89  & 91.08\%  & 0.91  & 90.93\%  & 0.91 \\
            neural-op         		& 98.81\%  & 0.99  & 98.60\%  & 0.99  & 98.50\%  & 0.98  & 98.63\%  & 0.99 \\
            image-op + neural-op 	& 93.28\%  & 0.93  & 93.24\%  & 0.93  & 95.07\%  & 0.95  & 94.86\%  & 0.95 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
\end{table*}

The following immediate implications can be observed from the results.

\subsection{Metrics to consider}
Data from different metrics result in different prediction performances, which is the motivation behind our focus on the data from the \emph{electrical current} metric. Selection of a metric beforehand cannot be directly deduced, but the effectiveness holds throughout. Therefore, it is a matter of trial.

\subsection{Signature levels}
Passports and signatures representing execution behaviour within arbitrary segments of data are based on regression function. Higher orders of regression functions (quadratic, cubic, etc.) result in more accurate representation of data points and better prediction performance, but impose extra computational cost during data preprocessing. There are a couple of negligible exceptions in our results, such as the DT accuracy for \texttt{neural-op} under \emph{full} (\Cref{tab:model_performance_full}) and \emph{ini} (\Cref{tab:model_performance_ini}) cuts.

\subsection{Data segmentation}
The choice of data segmentation is the most influential aspect. The consistent observation across the board in \Cref{tab:model_performance_full} points to the superior prediction performance from the \texttt{neural-op} phase type. However, presence of \texttt{neural-op} assumes an informed segmentation.

To compare the results for uninformed segmentation, we shall consider \texttt{cycle-op} results in every table. When it comes to linear signature regression functions, full-cut segments give the best results with the exception of DT, for which a mid-cut segment is better. For quadratic signature regression functions, both BDT and RF show better performance with mid-cut segments. For all model types, a quadratic signature function, when considering a mid-cut, performs better than a linear signature function combined with a full-cut.

Considering the computational effort effect, i.e., energy and time, dealing with a mid-cut segment is much more advantageous than using a full-cut, even if a single step is upgraded to polynomial quadratic regression function generation. Considering the scale of preprocessing, the net result is better prediction performance at lower energy and faster preprocessing times. While we do not have dedicated collections, we can confirm the time difference for preprocessing is rather noticeable. We can conclude that the lack of informed segmentation can be effectively compensated by an increase in the preprocessing levels, combined with a lighter preprocessing flow.

The most interesting result however, is when uninformed segmentation is applied on top of the informed one, i.e., quartile-based segmentation for each phase type. While results are close for the linear categories with only DT neural mid-cut demonstrating an advantage over neural full-cut, for the polynomial quadratic categories all models work much better under neural mid-cut. This clearly indicates that more data does not necessarily mean better predictions, which is also confirmed by lower performance when combining phase types. One has to find the most effective portion of data, in this case the \emph{mid} segment of the \texttt{neural-op} phase type.

\subsection{ML algorithm of choice}
We have already narrowed down the ML algorithm choices to tree-based algorithms and these are very performant. Amongst these algorithms, BDT and RF have a consistent edge over DT and ET, with BDT posting the accuracy of 99.54\% with a quadratic regression function as the signature level and under the \emph{mid} segment of the \texttt{neural-op} phase type (\Cref{tab:model_performance_mid}).

\subsection{Covered information positions}
As we do not cover data quality aspects in this paper, we shall consider the bottom row for the data dimension, which is the case with our data set.

Considering the provided results and the information position quadrant, we can fill some of the cells, i.e., \Cref{fig:quadrant_coverage}. The knowledge dimension is clearly divided between informed and uninformed segmentations, matching white box and black box positions, respectively. When it comes to the data dimension the richness and poorness are to be considered in terms of the effectiveness quality.
%
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/quadrant_coverage.pdf}
	\caption{Considering the comprehensiveness of data and the various considered knowledge positions in our cases, we are covering the bottom row of the information position quadrant.}
	\label{fig:quadrant_coverage}
\end{figure}

For a designer, the availability, or lack there of, knowledge of system internals would mean that only the left column from \Cref{fig:infopos_quadrant} is to be considered. Accordingly, it is known that an uninformed segmentation considering the mid-cut in combination with polynomial quadratic and BDT, works best. Note that this combination works better than a full-cut. This lands us on the bottom left cell.

The opposite situation, in which the segmentation can be done in an informed fashion, the designer will still apply the mid-cut on top of the \texttt{neural-op} phase type selection. This lands us on the bottom right cell.

% ===============================================
% Section
% ===============================================
\section{Related work}
\label{sec:related_work}
While there are numerous literature considering effects of ML data quality~\cite{Mohammed:2024:EDQM, Foroni:2021:EEED, Frenay:2014:CPLN, Li:2021:CSEI, Neutatz:2022:DCAW, Shah:2024:HDCD}, which can be defined with a number of dimensions itself~\cite{Mohammed:2024:EDQM}, the presence and effects of knowledge has not been considered. The closest concept to the consideration of knowledge as a separate dimension is \enquote{task-dependent quality}~\cite{Foroni:2021:EEED}, which still considers data quality in the context of the task it is being used for, i.e., a variable quality limit.

We on the other hand take into account the knowledge involved in the design of the solution and its availability, which leads to a more comprehensive view of the overall information position (knowledge combined with data). Accordingly, one major difference with the above cited literature is the need for detailed understanding of the solution. This generally is not a factor in the literatures, as studies consider standard tasks, e.g., regression, classification, and so forth. By bringing in the knowledge aspect, we aim to make the understanding of quality applicable to complex and custom solution design processes.

% ===============================================
% Section
% ===============================================
\section{Conclusion and future work}
\label{sec:conclusion}
It is evident from our results that the combination of applied preprocessing, selected data portions, and ML model of choice, has a direct impact on solution performance. Possessing such awareness, upfront, will lead to a much more streamlined design process.

When it comes to the question of reusability, our conclusion holds for the type of anomaly identification solution evaluated in this paper, i.e., ML models trained with constructs (signatures in our case) based on data segmentation. Depending on the information position, choices such as the application of a mid-cut and the BDT model hold by default. Case-specific variables, such as the discovery of the most effective informed segmentation (\texttt{neural-op} for our use-case), will need the execution of a minimal viable example. Effects of regression function level is also known upfront, as discussed in \Cref{sec:results} and should be evaluated and chosen by the designer. The industry utilising this type of CPS, e.g., semiconductor photolithography, production printing, even MRI machines in the health industry, is by no means small. Anomaly identification solutions are equally valuable across the board.

Immediate next steps for us are to complete the quadrant with representative scenarios of varying data quality, as well as execution of diverse types of ML-assisted solutions. The latter will include Deep Neural Networks and possibly Transformer-based alternative designs.

% ###############################################
% End of file
% ###############################################
