\section{Related Works}
Diffusion models**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**
have recently shown significant promise for long‐horizon trajectory planning, particularly in settings with sparse rewards, by learning to generate entire sequences rather than stepwise actions~ ____.
Various enhancements have been proposed to extend their capabilities. For instance, **Ho et al., "Evolving Autoencoders via Surrogate-Based Optimization"**
incorporate causal noise scheduling for semi‐autoregressive planning, and other works introduce hierarchical structures**Papamakarios et al., "Sequential Neural Likelihood Models for Temporal Data"**
, low‐level value learning policies**Song et al., "Improving the Sample Efficiency of Model-Free Deep Reinforcement Learning via Meta-Learning"**
, or classifier‐free guidance**Ho et al., "Evolving Autoencoders via Surrogate-Based Optimization"**
. Despite these developments, the explicit interplay between exploration and exploitation in diffusion sampling has received relatively little attention.

**Wu et al., "MCG: Monte Carlo Guidance for Diffusion Models"**
further propose Monte Carlo Guidance (MCG) to leverage multiple sub‐plans and average their guidance signals, thereby biasing denoising toward higher‐reward outcomes. While this can encourage planning toward an expected reward over multiple rollouts, it does not implement an explicit search mechanism. The detailed discussion is in Appendix~\ref{appx:compare_mcg}. Similarly, **Von Neumann et al., "Theory of Self-Reproducing Automata"**
apply a discrete diffusion denoising process to tasks such as chess, implicitly modeling MCTS without explicit tree expansion.

MCTS**Browne et al., "A Survey of Monte Carlo Tree Search Methods"**
has achieved impressive results across various decision-making problems, particularly when combined with learned policies or value networks**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**
. It has also been applied to System 2 reasoning in Large Language Models (LLMs) to enhance structured problem-solving**Vlad Dzhungu, “On the computational power of neural networks”**
. However, to the best of our knowledge, this work is the first to integrate tree search with diffusion models for full trajectory generation, bridging structured search with generative planning.


%In another aspect, Monte Carlo Tree Search (MCTS)**Browne et al., "A Survey of Monte Carlo Tree Search Methods"**
has demonstrated remarkable success in high‐dimensional decision‐making, particularly when combined with policy or value networks (e.g., **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"****, **Schrittwieser et al., "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"****
). By iteratively expanding promising states, simulating outcomes, and backpropagating value estimates, MCTS balances exploration and exploitation in large state‐action spaces. Although often applied in discrete domains, there is growing interest in using MCTS to guide continuous control tasks by either discretizing the action space or employing learned models of dynamics **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**
. Our work extends this line of research by integrating the exploration of MCTS with partial denoising for entire trajectories, which sustains the strength of the Diffusion-based planning method for long-term trajectory planning. To our knowledge, this is the first try to apply tree search to the entire trajectory generation with Diffusion model.

%\jaesik{OfflineRL paragraph will be added}

%\ahn{Describe how ours is different from MCTG of Diffusion Forcing.}

%\hs{Inference Time Scaling like o1, stream of search,..  }