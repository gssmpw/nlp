\section{Related Works}
Diffusion models____ have recently shown significant promise for long‐horizon trajectory planning, particularly in settings with sparse rewards, by learning to generate entire sequences rather than stepwise actions~ ____. Various enhancements have been proposed to extend their capabilities. For instance, ____(____) incorporate causal noise scheduling for semi‐autoregressive planning, and other works introduce hierarchical structures____, low‐level value learning policies____, or classifier‐free guidance____. Despite these developments, the explicit interplay between exploration and exploitation in diffusion sampling has received relatively little attention.

____ further propose Monte Carlo Guidance (MCG) to leverage multiple sub‐plans and average their guidance signals, thereby biasing denoising toward higher‐reward outcomes. While this can encourage planning toward an expected reward over multiple rollouts, it does not implement an explicit search mechanism. The detailed discussion is in Appendix~\ref{appx:compare_mcg}. Similarly, ____ apply a discrete diffusion denoising process to tasks such as chess, implicitly modeling MCTS without explicit tree expansion. 

MCTS____ has achieved impressive results across various decision-making problems, particularly when combined with learned policies or value networks____. It has also been applied to System 2 reasoning in Large Language Models (LLMs) to enhance structured problem-solving____. However, to the best of our knowledge, this work is the first to integrate tree search with diffusion models for full trajectory generation, bridging structured search with generative planning.


%In another aspect, Monte Carlo Tree Search (MCTS)____ has demonstrated remarkable success in high‐dimensional decision‐making, particularly when combined with policy or value networks (e.g., AlphaGo____, MuZero____). Recently, MCTS has been rehighlighted to apply it to Large Language Model for addressing complex reasoning tasks with Test-Time Computing scaling____. In unsupervised RL framework____, ____ proposes the ensemble of MCTS and unsupervisedly trained Transformer, but it is to accelerate the tree search through the inferenced action from Transformer compared to naive MCTS. Therefore, it also cannot be free from the limitations of MCTS for long-horizon planning, while MCTD can achieve this through holistic planning with diffusion models. In other aspect, there is growing interest in using MCTS to guide continuous control tasks by either discretizing the action space or employing learned models of dynamics ____. Our work extends this line of research by applying diverse guidance levels as "meta-actions" while sustaining the strength of the Diffusion-based planning method for long-term trajectory planning. To our knowledge, this is the first try to apply tree search to the entire trajectory generation with Diffusion model.





%has demonstrated remarkable success in high‐dimensional decision‐making, particularly when combined with policy or value networks (e.g., AlphaGo____, MuZero____). By iteratively expanding promising states, simulating outcomes, and backpropagating value estimates, MCTS balances exploration and exploitation in large state‐action spaces. Although often applied in discrete domains, there is growing interest in using MCTS to guide continuous control tasks by either discretizing the action space or employing learned models of dynamics ____. Our work extends this line of research by integrating the exploration of MCTS with partial denoising for entire trajectories, which sustains the strength of the Diffusion-based planning method for long-term trajectory planning. To our knowledge, this is the first try to apply tree search to the entire trajectory generation with Diffusion model.

%\jaesik{OfflineRL paragraph will be added}

%\ahn{Describe how ours is different from MCTG of Diffusion Forcing.}

%\hs{Inference Time Scaling like o1, stream of search,..  }