%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
%\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{kotex}
\usepackage{lipsum}
% \usepackage[font=small,labelfont=bf]{caption}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
% \usepackage[algo2e,ruled,linesnumbered]{algorithm2e}

%\usepackage{algorithmic}
\usepackage{algpseudocode}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage{makecell}

\usepackage{defs_ahn}


% % if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % THEOREMS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Monte Carlo Tree Diffusion}

\begin{document}

% \nolinenumbers

\twocolumn[
% \icmltitle{System 2 Diffuser: Complex Behavior Planning via Diffusion Tree Search}
% \icmltitle{System 2 Diffuser: Planning Sequential Decisions via Monte Carlo Tree Diffusion}
\icmltitle{Monte Carlo Tree Diffusion for System 2 Planning}
% \icmltitle{Monte Carlo Tree Diffusion}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jaesik Yoon}{kaist,sap}
\icmlauthor{Hyeonseo Cho}{kaist}
\icmlauthor{Doojin Baek}{kaist}
\icmlauthor{Yoshua Bengio}{mila}
\icmlauthor{Sungjin Ahn}{kaist,nyu}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST}
\icmlaffiliation{mila}{Mila}
\icmlaffiliation{sap}{SAP}
\icmlaffiliation{nyu}{New York University}
% \icmlaffiliation{prefrontal}{KAIST-Mila Prefrontal AI}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Jaesik Yoon}{jaesik.yoon@kaist.ac.kr}
\icmlcorrespondingauthor{Sungjin Ahn}{sungjin.ahn@kaist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\newcommand\doojin[1]{\textcolor{orange}{{DOOJIN: #1}}}
\newcommand\jaesik[1]{\textcolor{red}{{JAESIK: #1}}}
\newcommand\hs[1]{\textcolor{blue}{{Hyeonseo: #1}}}
\newcommand\ahn[1]{\textcolor{purple}{{Ahn: #1}}}



\begin{abstract}
Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)—whose performance naturally improves with additional test‐time computation (TTC)—standard diffusion‐based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree‐structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long‐horizon tasks show that MCTD outperforms diffusion baselines, yielding higher‐quality solutions as TTC increases. 
%Code and implementation details will be made available following the reviewing process at:\url{https://anonymous.4open.science/r/MCTD-D6DF}.


\end{abstract}

\section{Introduction}
\label{sec:intro}

Diffusion models have recently emerged as a powerful approach to planning, enabling the generation of complex trajectories by modeling trajectory distributions using large-scale offline data~\citep{diffuser, decisiondiffuer, d-mpc, diffusion_forcing, hd, plandq}. Unlike traditional autoregressive planning methods, diffusion-based planners, such as Diffuser~\citep{diffuser}, generate entire trajectories holistically through a series of denoising steps, eliminating the need for a forward dynamics model. This approach effectively addresses key limitations of forward models, such as poor long-term dependency modeling and error accumulation~\citep{director,drstrategy}, making it particularly well-suited for planning tasks with long horizons or sparse rewards.


Despite their strengths, it remains uncertain how diffusion-based planners can effectively enhance planning accuracy by utilizing additional test-time compute (TTC)—a property referred to as TTC scalability. One potential approach is to increase the number of denoising steps or, alternatively, draw additional samples~\citep{d-mpc}. However, it is known that performance gains from increasing denoising steps plateau quickly~\citep{karras2022elucidating,ddim,song2020score}, and independent random searches with multiple samples are highly inefficient as they fail to leverage information from other samples.  Moreover, how to effectively manage the exploration-exploitation tradeoff within this framework also remains unclear.


In contrast, Monte Carlo Tree Search (MCTS)~\citep{mcts}, a widely adopted planning method, demonstrates robust TTC scalability. By leveraging iterative simulations, MCTS refines decisions and adapts based on exploratory feedback, making it highly effective in improving planning accuracy as more computation is allocated. This capability has established MCTS as a cornerstone in many System 2 reasoning tasks, such as mathematical problem-solving~\citep{guan2025rstar, zhang2024accessing} and program synthesis~\citep{brandfonbrener2024vermcts}. However, unlike diffusion-based planners, traditional MCTS relies on a forward model for tree rollouts, inheriting its limitations including losing global consistency. In addition to being restricted to discrete action spaces, the resulting search tree can grow excessively large in both depth and width. This leads to significant computational demands, particularly in scenarios involving long horizons and large action spaces. 

% This raises a crucial question: \textit{how can we combine the strengths of Diffuser and MCTS to overcome their respective limitations and enhance the TTC scalability of diffusion-based planning?} To address this challenge, we propose Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the holistic trajectory generation of diffusion planners with the iterative search capabilities of MCTS. 

This raises a crucial question: \textit{how can we combine the strengths of Diffuser and MCTS to overcome their limitations and enhance the TTC scalability of diffusion-based planning?} To address this, we propose Monte Carlo Tree Diffusion (MCTD), a framework that integrates diffusion-based trajectory generation with the iterative search capabilities of MCTS for more efficient and scalable planning.

MCTD builds on three key innovations. First, it restructures denoising into a tree-based rollout process, enabling semi-autoregressive causal planning while maintaining trajectory coherence. Second, it introduces guidance levels as meta-actions to dynamically balance exploration and exploitation, ensuring adaptive and scalable trajectory refinement within the diffusion framework. Third, it employs fast jumpy denoising as a simulation mechanism, efficiently estimating trajectory quality without costly forward model rollouts. These innovations enable the four steps of MCTS (Selection, Expansion, Simulation, and Backpropagation) within diffusion planning, effectively bridging structured search with generative modeling. Experimental results show that MCTD outperforms existing approaches in long-horizon tasks, achieving superior scalability and solution quality.


The main contributions of this paper are as follows: First, to the best of our knowledge, this is the first work to propose an MCTS-integrated diffusion planning framework that explicitly incorporates the four steps of MCTS, providing an effective TTC scaling method for diffusion models. Second, we introduce three key innovations: Denoising as Tree-Rollout, Guidance Levels as Meta-Actions, and Jumpy Denoising as Fast Simulation. Lastly, we present experimental results demonstrating the effectiveness of MCTD.


\section{Preliminaries}
% \subsection{Diffusion Models}
\subsection{Diffuser: Diffusion Models for Planning}

Diffuser~\citep{diffuser} addresses long‐horizon decision-making by treating entire trajectories as a matrix
\begin{equation*}
  \mathbf{x} \;=\;
  \begin{bmatrix}
    s_{0} & s_{1} & \dots & s_{T}\\
    a_{0} & a_{1} & \dots & a_{T}
  \end{bmatrix},
\end{equation*}
where $s_{t}$ and $a_{t}$ denote the state and action at time t, respectively. A $\emph{diffusion process}$ is then trained to iteratively remove noise from samples of $\mathbf{x}$, ultimately producing coherent trajectories. In practice, this corresponds to reversing a forward noise‐injection procedure by learning a denoiser $p_{\theta}(\mathbf{x})$ over the trajectory space. Since $p_{\theta}$ by itself does not encode reward or other task objectives, Diffuser optionally incorporates a heuristic or learned guidance function $J_{\phi}(\mathbf{x})$. This function predicts the return or value of a partially denoised trajectory, thereby biasing the sampling distribution:
\begin{equation}
    \tilde{p}_{\theta}(\mathbf{x})
    \;\;\propto\;\;
    p_{\theta}(\mathbf{x})
    \exp \bigl(J_{\phi}(\mathbf{x})\bigr).
    \label{eq:guided_diffusion_sampling}
\end{equation}
Accordingly, at each denoising step, gradient information from $J_{\phi}$ nudges the model toward trajectories that appear both feasible (as learned from the offline data) and promising with respect to returns, in a manner akin to classifier guidance in image diffusion~\cite{classifier_guided}.

\textbf{Diffusion Forcing}~\citep{diffusion_forcing} extends the above framework by allowing tokenization of $\mathbf{x}$. This tokenization allows each token to be denoised at a different noise level, enabling partial denoising of segments where uncertainty is high (e.g., future plans), without requiring a complete transition from full noise to no noise across the entire trajectory. Such token-level control is particularly beneficial in domains that demand causal consistency, such as long-horizon planning problems.

%\hs{Is it okay not to cover MCTG?}
% \textbf{Diffusion Forcing}~\citep{diffusion_forcing} extends this framework by \emph{tokenizing} the trajectory into smaller segments,  

% \cite{1_janner2022diffuser}

%\subsection{Diffusion Forcing}
% \cite{1_diffusionforcing2024df}

\subsection{Monte Carlo Tree Search}
% \cite{2_coulom2006mcts,2_silver2016alphago,2_silver2017alphazero,2_schrittwieser2020muzero}

Monte Carlo Tree Search (MCTS) is a planning algorithm that combines tree search with stochastic simulations to effectively balance exploration and exploitation~\citep{mcts}. Typically, MCTS proceeds in four stages: $\emph{selection}$, $\emph{expansion}$, $\emph{simulation}$, and $\emph{backpropagation}$. During $\emph{selection}$, the algorithm starts at a root node corresponding to the current state (or partial plan) and descends the tree according to a policy such as Upper Confidence Bounds for Trees (UCT)~\citep{uct}, aiming to balancely choose the most promising child at each step or explore unexperienced states. Once a leaf or expandable node is reached, $\emph{expansion}$ adds new child nodes representing previously unexplored actions or plans.

Following expansion, $\emph{simulation}$ estimates the value of a newly added node by sampling a sequence of actions until reaching a terminal condition or a predefined rollout depth. The resulting outcome (e.g., cumulative reward in a Markov Decision Process) then updates the node’s estimated value through $\emph{backpropagation}$, propagating this information to all ancestor nodes in the tree. Over multiple iterations, MCTS prunes unpromising branches and refines its value estimates for promising subtrees, guiding the search toward more effective solutions.

%MCTS has demonstrated considerable success in a variety of domains, most notably in large‐scale board games (e.g., Go \cite{mcts_alphago}, Chess, and Shogi) and planning problems where the state or action spaces are too large for exhaustive search. More recent research focuses on integrating MCTS with generative modeling to address the complex reasoning problems such as mathematical problems~\citep{luo2024improve, zhang2024accessing}.

%In the context of system design, MCTS provides a structured means of selectively refining partial solutions while maintaining global exploration, which complements the diffusion‐based planning of System 2 Diffuser by focusing computation on partial trajectories that appear most promising. 

% \hs{This MCTS part could be simplified further}




\begin{figure*}[th!]
    \centering
    \subfloat[\centering MCTS Perspective]
    {{\includegraphics[width=11cm]{figs/MCTD_Main_Figure.png} }}%
    \hspace{2mm}
    \subfloat[\centering Diffusion Perspective]{{\includegraphics[width=5.5cm]{figs/mctd_df2.png} }}%
    \caption{
Two perspectives on Monte Carlo Tree Diffusion (MCTD).
\textbf{(a) MCTS Perspective:} The four steps of an MCTD round—\emph{Selection}, \emph{Expansion}, \emph{Simulation}, and \emph{Backpropagation}—are illustrated on a partial denoising tree. Each node corresponds to a partially denoised sub‐trajectory, and edges are labeled with binary guidance levels (0 = no guidance, 1 = guided). After a new node is expanded, “jumpy” denoising is performed to quickly estimate its value, which is then backpropagated along the path in the tree.
\textbf{(b) Diffusion Perspective:} The same process is viewed as partial denoising across both denoising depth (vertical axis) and planning horizon (horizontal axis).  Each colored block represents a partially denoised plan at a specific noise level, with darker shades indicating higher noise. Different expansions (0 or 1) create branches in the forward planning direction, representing alternative trajectory refinements. Notably, the entire row is denoised simultaneously, but with varying denoising levels. The MCTD framework unifies these two perspectives.} %
    \label{fig:four_steps}%
    \vspace{-3mm}
\end{figure*}



\section{Monte Carlo Tree Diffusion}
In this section, we first outline the key concepts that enable MCTD planning: (1) Denoising as Tree-Rollout, (2) {Guidance Levels as Meta-Actions}, and (3) {Jumpy Denoising as Simulation}. These innovations form the foundation of MCTD, bridging the gap between traditional tree search methods and diffusion-based planning. We then describe the MCTD process in terms of the four steps: {Selection}, {Expansion}, {Simulation}, and {Backpropagation}.





\subsection{Denoising as Tree-Rollout}
Traditional MCTS operates on individual states, leading to deep search trees and significant scalability challenges. Because each node in the tree represents a single state, the depth of the tree increases linearly with the planning horizon, resulting in an exponential growth of the search space. Furthermore, it lacks the holistic perspective of the entire trajectory that diffusion-based planners inherently provide. On the other hand, Diffuser does not offer the tree-like structure necessary for intermediate decision-point searches that effectively balance exploitation and exploration.

To address this issue, we first introduce the \textit{Denoising as Tree-Rollout} process by leveraging the semi-autoregressive denoising process~\citep{diffusion_forcing}. Specifically, we partition the full trajectory $\bx=(x_1,x_2,\dots,x_N)$ into $S$ subplans, $\bx=(\bx_1,\bx_2,\dots,\bx_S)$ such that $\cap_s \bx_s = \emptyset$. Unlike the standard diffuser, where all subplans share the same global denoising schedule, our approach allows assigning independent denoising schedules to each subplan. By applying faster denoising to earlier subplans and slower denoising to later subplans, the process effectively becomes causal and semi-autoregressive. This allows the denoising process to determine the future conditioned on the already determined past. Consequently, while \textit{preserving the globally consistent and holistic generation advantages of Diffuser}, the denoising process approximates:
\eq{
p(\bx) \approx \prod_{s=1}^S p(\bx_s|\bx_{1:s-1}) .\label{eq:semiauto}
}
Notably, although this formulation appears autoregressive, it is still executed as a single denoising process by controlling noise levels across subplans~\citep{diffusion_forcing}.


In MCTD, each subplan $\bx_s$, which represents a \textit{temporally extended state}, is treated as a node in the search tree, rather than using individual states  $x_n$ as nodes. This allows the tree to operate at a higher level of abstraction, improving efficiency and scalability. Denoising the entire plan $\bx$ can be viewed as rolling out a sequence of these nodes through a single denoising process of Diffuser. Because $S \ll N$, the tree depth becomes much smaller than that in MCTS. For instance, in one of our planning experiments, we use $S=5$ and $N=500$.

\subsection{Guidance Levels as Meta-Actions}

% In MCTS, the tree becomes increasingly informative as more computation is allocated, achieving TTC scalability. 

In MCTS, constructing and searching the tree becomes computationally expensive in large action spaces and is fundamentally impractical for continuous action spaces. To resolve this issue, in MCTD we introduce a novel approach by redefining the exploration-exploitation trade-off in terms of \textit{meta-actions}. While meta-actions can be any discrete decision that can control the exploration-exploitation, in this work we propose to implement it as \textit{guidance levels} applied during the denoising process. 

For simplicity, consider two guidance levels: \textsc{guide} and \textsc{no\_guide}. In MCTD, we observe that sampling from the prior distribution $p(\bx)$, i.e., using the standard diffusion sampler represents exploratory behavior as it does not attempt to achieve any goal but only imitates the prior behavior contained in the offline data. Thus, we correspond it to the \textsc{no\_guide} meta-action. Conversely, sampling from a goal-directed distribution $p_g(\bx)$, such as through classifier-guided diffusion~\citep{classifier_guided}, represents exploitative behavior, and thus meta-action, \textsc{guide}, is assigned. This steers the sampling process toward achieving a specific goal defined by a reward function $r_g(\bx)$.

Next, we integrate the concept of meta-actions with the tree-rollout denoising process described in Eqn.~\eqref{eq:semiauto}. To achieve this, we introduce a guidance schedule $\bg = (g_1, \dots, g_S)$, which assigns a guidance meta-action $g_s \in \{\textsc{guide}, \textsc{no\_guide}\}$ to each corresponding subplan $\bx_s$. If no guidance is chosen, the subplan is sampled from the explorative tree-rollout prior $p(\bx_s|\bx_{1:s-1})$ while a subplan with guidance is sampled from $p_g(\bx_s|\bx_{1:s-1})$ if the \textsc{guide} meta-action is chosen. This allows the guidance levels to be independently assigned to each subplan unlike the standard Diffuser assigning the guidance throughout both the whole trajectory and the whole denoising process. 

By dynamically adjusting the guidance schedule $\bg$, this enables the exploration-exploitation balancing at the level of subplans \textit{within a single denoising process}. The extended tree-rollout denoising process effectively approximates: 
\eq{
p(\bx|\bg) \approx \prod_{s=1}^S p(\bx_s|\bx_{1:s-1}, g_s) . \label{eq:meataction}
}
As a result, this approach enables efficient and scalable planning, even in complex or continuous action spaces.




\subsection{Jumpy Denoising as Fast Simulation}  
\label{sec:sim}

In MCTS, evaluating a node far from a leaf node, where evaluation of the plan is feasible, is a critical requirement. This is typically addressed in one of two ways: using a fast forward dynamics model to roll out trajectories to the leaf node, which is computationally expensive, or approximating the node's value via bootstrapping, offering faster but less accurate results. However, how to effectively incorporate these simulation strategies into the diffuser framework remains an open question.


In MCTD, we implement this simulation functionality using the \textit{fast jumpy denoising} process based on DDIM~\citep{ddim}. Specifically, when the tree-rollout denoising process has progressed up to the \( s \)-th subplan, the remaining steps are denoised quickly by skipping every $C$ step:  
\eq{
\tbx_{s+1:S} \sim p(\bx_{s+1:S}|\bx_{1:s}, \bg).  
}
This produces a full trajectory \( \tbx = (\bx_{1:s}, \tbx_{s+1:S})\), which is then evaluated using the reward function \( r(\tbx) \). While this fast denoising process may introduce larger approximation errors, it is highly computationally efficient, making it well-suited for the simulation step in MCTD. 
  




\begin{algorithm}[t!]
\footnotesize
% \scriptsize 
\caption{Monte Carlo Tree Diffusion}
\label{alg:MCTD}
\begin{algorithmic}[1]
\Procedure{MCTD}{$root, iterations$}
    \For{$i = 1$ to $iterations$}
        % Selection Step
        \State $node \gets root$
        \While{\Call{IsFullyExpanded}{$node$} \textbf{and not} \\ 
        \qquad\qquad\quad\;  \Call{IsLeaf}{$node$}}
            \State $node \gets$ \Call{BestUCTChild}{$node$}
        \EndWhile
        
        % Expansion Step
        \If{\Call{IsExpandable}{$node$}}
            \State $g_s \gets$ \Call{SelectMetaAction}{$node$} 
            \State $newSubplan \gets$ \Call{DenoiseSubplan}{$node, g_s$}
            \State $child \gets$ \Call{CreateNode}{$newSubplan$}
            \State \Call{AddChild}{$node, child$}
            \State $node \gets child$
        \EndIf
        
        % Simulation Step (Fast Jumpy Denoising)
        \State $partial \gets$ \Call{GetPartialTrajectory}{$node$}
        \State $remaining \gets$ \Call{FastDenoising}{$partial$}
        \State $fullPlan \gets (partial, remaining)$
        \State $reward \gets$ \Call{EvaluatePlan}{$fullPlan$}
        
        % Backpropagation Step
        \While{$node \neq null$}
            \State $node.visitCount \gets node.visitCount + 1$
            \State $node.value \gets node.value + reward$
            \State \Call{UpdateMetaActionSchedule(}{}
            \State \qquad $node, reward$\,)
            \State $node \gets node.parent$
        \EndWhile
    \EndFor
    
    \State \Return \Call{BestChild}{$root$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{The Four Steps of an MCTD Round}

Building on the description above, we detail how the four traditional steps of MCTS—Selection, Expansion, Simulation, and Backpropagation—are adapted and implemented in MCTD. This process is illustrated in Figure~\ref{fig:four_steps}.

\textbf{Selection.}
In MCTD, the selection process involves traversing the tree from the root node to a leaf or partially expanded node. At each step, a child node is chosen based on a selection criterion such as Upper Confidence Bound (UCB). Importantly, this step does not require computationally expensive denoising; it simply traverses the existing tree structure. Unlike traditional MCTS, MCTD nodes correspond to temporally extended states, enabling higher-level reasoning and reducing tree depth, which improves scalability. The guidance schedule \(\bg\) is dynamically adjusted during this step by UCB to balance exploration (\textsc{no\_guide}) and exploitation (\textsc{guide}).



\textbf{Expansion.}
Once a leaf or partially expanded node is selected, the expansion step generates new child nodes by extending the current partially denoised trajectory.  Each child node corresponds to a new subplan generated using the diffusion model. Depending on the meta-action $g_s$, the subplan is either sampled from the exploratory prior distribution \(p(\bx_s|\bx_{1:s-1})\) or the goal-seeking distribution \(p_g(\bx_s|\bx_{1:s-1})\). The meta-action controls which behavior is used for expansion, and the newly generated node is added to the tree as an extension of the current trajectory. Importantly, the guidance levels are not restricted to binary choices. For instance, it is possible to generalize the meta-actions to multiple guidance levels, such as \(\{\textsc{zero},\textsc{low},\textsc{medium},\textsc{high}\}\), offering finer control over the balance between exploration and exploitation during the expansion process. 
% This flexibility allows MCTD to adapt dynamically to the complexity of the planning task and optimize the tree-building process more effectively.


% Note that the guidance-level is not restricted to binary. For example, we can generalize the meta-actions to a number of guidance levels such as $\{\textsc{no},\textsc{low}, \textsc{medium}, \textsc{high}\}$.

\textbf{Simulation.}
Simulation in MCTD is implemented via the fast jumpy denoising. When a node is expanded, the remainder of the trajectory is quickly completed by using the fast denoising as described in Section~\ref{sec:sim}. The resulting plan $\tbx$ is then given to the plan evaluator $r(\tbx)$. This approach maintains computational efficiency while providing a sufficient estimate of the plan’s quality.

\textbf{Backpropagation.}
After the simulation step, the reward obtained from evaluating the complete plan is backpropagated through the tree to update the value estimates of all parent nodes along the path to the root. In MCTD, this backpropagation process also updates the meta-action-based guidance schedules, enabling the tree to dynamically adjust the exploration-exploitation balance for future iterations. This ensures that promising trajectories, as indicated by their rewards, are prioritized, while sufficient exploration is maintained to prevent premature convergence.



\section{Related Works}

Diffusion models~\citep{diffusion} have recently shown significant promise for long‐horizon trajectory planning, particularly in settings with sparse rewards, by learning to generate entire sequences rather than stepwise actions~ \citep{diffuser, diffusion_forcing, decisiondiffuer, d-mpc, hd, plandq, hmd}. Various enhancements have been proposed to extend their capabilities. For instance, \citeauthor{diffusion_forcing}(\citeyear{diffusion_forcing}) incorporate causal noise scheduling for semi‐autoregressive planning, and other works introduce hierarchical structures~\citep{hd, plandq, hmd}, low‐level value learning policies~\citep{plandq}, or classifier‐free guidance~\citep{decisiondiffuer, d-mpc}. Despite these developments, the explicit interplay between exploration and exploitation in diffusion sampling has received relatively little attention.

\citet{diffusion_forcing} further propose Monte Carlo Guidance (MCG) to leverage multiple sub‐plans and average their guidance signals, thereby biasing denoising toward higher‐reward outcomes. While this can encourage planning toward an expected reward over multiple rollouts, it does not implement an explicit search mechanism. The detailed discussion is in Appendix~\ref{appx:compare_mcg}. Similarly, \citet{anonymous2024implicit} apply a discrete diffusion denoising process to tasks such as chess, implicitly modeling MCTS without explicit tree expansion. 

MCTS~\citep{mcts} has achieved impressive results across various decision-making problems, particularly when combined with learned policies or value networks~\citep{mcts_alphago, muzero}. It has also been applied to System 2 reasoning in Large Language Models (LLMs) to enhance structured problem-solving~\citep{xiang2025towards, ToT, zhang2024rest}. However, to the best of our knowledge, this work is the first to integrate tree search with diffusion models for full trajectory generation, bridging structured search with generative planning.


%In another aspect, Monte Carlo Tree Search (MCTS)~\citep{mcts} has demonstrated remarkable success in high‐dimensional decision‐making, particularly when combined with policy or value networks (e.g., AlphaGo\citep{mcts_alphago}, MuZero\citep{muzero}). Recently, MCTS has been rehighlighted to apply it to Large Language Model for addressing complex reasoning tasks with Test-Time Computing scaling~\citep{guan2025rstar, zhang2024accessing,brandfonbrener2024vermcts}. In unsupervised RL framework~\citep{diffuser, chen2021decision, janner2021offline}, \cite{yaari2022mctransformer} proposes the ensemble of MCTS and unsupervisedly trained Transformer, but it is to accelerate the tree search through the inferenced action from Transformer compared to naive MCTS. Therefore, it also cannot be free from the limitations of MCTS for long-horizon planning, while MCTD can achieve this through holistic planning with diffusion models. In other aspect, there is growing interest in using MCTS to guide continuous control tasks by either discretizing the action space or employing learned models of dynamics \citep{piche2018probabilistic}. Our work extends this line of research by applying diverse guidance levels as "meta-actions" while sustaining the strength of the Diffusion-based planning method for long-term trajectory planning. To our knowledge, this is the first try to apply tree search to the entire trajectory generation with Diffusion model.





%has demonstrated remarkable success in high‐dimensional decision‐making, particularly when combined with policy or value networks (e.g., AlphaGo\citep{mcts_alphago}, MuZero\citep{muzero}). By iteratively expanding promising states, simulating outcomes, and backpropagating value estimates, MCTS balances exploration and exploitation in large state‐action spaces. Although often applied in discrete domains, there is growing interest in using MCTS to guide continuous control tasks by either discretizing the action space or employing learned models of dynamics \citep{piche2018probabilistic}. Our work extends this line of research by integrating the exploration of MCTS with partial denoising for entire trajectories, which sustains the strength of the Diffusion-based planning method for long-term trajectory planning. To our knowledge, this is the first try to apply tree search to the entire trajectory generation with Diffusion model.

%\jaesik{OfflineRL paragraph will be added}

%\ahn{Describe how ours is different from MCTG of Diffusion Forcing.}

%\hs{Inference Time Scaling like o1, stream of search,..  }





\section{Experiments}

%In this section, we present the empirical evaluations of MCTD under an offline goal-conditioned RL tasks. From offline RL tasks \citep{4_fu2020d4rl}, This setup increases the difficulty of the tasks with unseen goal at training phase, as the planner must synthesize new trajectories that extend beyond the behaviors seen in the dataset. 

%\subsection{Benchmark Setup}

We evaluate the proposed approach, MCTD, on a suite of tasks from the Offline Goal‐conditioned RL Benchmark (OGBench)~\citep{4_park2024ogbench}, which spans diverse domains such as maze navigation with multiple robot morphologies (e.g., point‐mass or ant) and robot‐arm manipulation. Our chosen tasks—point and antmaze navigation, multi‐cube manipulation, and a newly introduced {visual pointmaze}—jointly assess a planner’s ability to handle \emph{long‐horizon planning}, \emph{sequential manipulation}, and \emph{partial visual observability}. In the visual pointmaze, an agent perceives RGB image observations of the 3D environment, thereby testing each method’s resilience to partial observability and the ability to handle image-based planning.

\begin{table*}[t]
\centering
\caption{\textbf{Long-Horizon Maze Results.} Success rates (\%) on pointmaze and antmaze environments with medium, large, and giant mazes for the \emph{navigate} datasets. Each cell shows the mean $\pm$ standard deviation.}
\vspace{1mm}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{ll|cccc|c}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{\textbf{Environment}} 
 & \multirow{2}{*}{\textbf{Dataset}} 
 & \multicolumn{3}{c}{\textbf{Diffuser}}  % group 3 columns 
 & \multirow{2}{*}{\textbf{Diffusion Forcing}} 
 & \multirow{2}{*}{\textbf{MCTD}} \\
  &  & \textbf{Base} & \textbf{Replanning} & \textbf{Random Search}&  &  \\

%\textbf{Environment} & \textbf{Dataset} & \textbf{Diffuser} & \textbf{Diffuser-Replanning} & \textbf{Diffuser-Random Search} & \textbf{Diffusion Forcing-MCTG} &  \textbf{Diffusion Forcing}  & \textbf{MCTD} \\
\hline
\multirow{3}{*}{pointmaze} & medium-navigate-v0 &  $ 58 \pm 6$ & $  60\pm0 $ & $  60 \pm 9$ & $65  \pm 16$ & $\textbf{100} \pm \textbf{0}$ \\
  & large-navigate-v0 &   $ 44 \pm 8$ & $ 40\pm0 $ & $ 34 \pm 13$  & $74  \pm 9$ & $\textbf{98}  \pm \textbf{6}$ \\ 
  & giant-navigate-v0 & $0 \pm 0$ & $ 0\pm0 $ & $4 \pm 8$  & $50 \pm 10$ & $\textbf{100} \pm \textbf{0}$ \\
 \hline
\multirow{3}{*}{antmaze} & medium-navigate-v0 & $36 \pm 15$ & $ 40\pm18 $ & $48 \pm 10$  & $90 \pm 10$ & $\textbf{100} \pm \textbf{0}$ \\
 & large-navigate-v0 & $14 \pm 16$ & $ 26\pm13 $ & $20 \pm 0$  & $57 \pm 6$ & $\textbf{98} \pm \textbf{6}$ \\
 & giant-navigate-v0 & $0 \pm 0$ & $ 0\pm0 $ & $ 4\pm8 $  & $24 \pm 12$ & $\textbf{94} \pm \textbf{9}$ \\

\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:maze_results}
\vspace{-3mm}
\end{table*}

\subsection{Baselines}

We compare MCTD against several baselines, each employing a distinct strategy for test‐time computation. First, we include the standard single‐shot \textbf{Diffuser}~\citep{diffuser}, which plans only at the start of an episode and executes that plan without further adjustments. Next, we consider two methods that allocate additional test‐time compute differently. \textbf{Diffuser‐Replanning} periodically replans at fixed intervals, allowing partial corrections during the episode. This enables us to gauge the benefits of iterative replanning without a full tree search. \textbf{Diffuser‐Random Search}~\citep{d-mpc} generates multiple random trajectories in parallel and selects the highest‐scoring one according to the same reward used by MCTD. While this increases test‐time sampling and applies multiple guidance levels for different samples, it lacks the systematic expansion and pruning of a tree‐search algorithm. We also compare our method to \textbf{Diffusion Forcing}~\citep{diffusion_forcing}, which introduces a causal denoising schedule that enables semi‐autoregressive trajectory generation. This comparison helps isolate the benefits of semi‐autoregressive denoising from our full tree‐structured approach. Further implementation details about all baselines can be found in Appendix~\ref{appx:baselines}.

% \subsection{Additional Performer}

% A long‐standing challenge in diffusion‐based planning is the difficulty of preserving global trajectory coherence while managing local, high‐dimensional state‐action control~\citep{diffusion_forcing, plandq}. For example, PlanDQ~\citep{plandq} couples a high‐level diffusion planner with a learned low‐level policy. Similarly, our approach focuses the diffusion planner on lower‐dimensional, representative state information (e.g., object or agent positions), while delegating detailed action inference to an external “performer.” This performer, whether a simple heuristic controller or a learned policy, is integrated using Algorithm~\ref{alg:additional-performer}. Such a hierarchical design allows MCTD and baseline planners to concentrate on broader strategic decisions while leaving the finer‐grained execution details to the performer.



\subsection{Maze Navigation with Point‐Mass and Ant Robots}

We begin with the pointmaze and antmaze tasks from OGBench, featuring mazes of varying sizes (medium, large, and giant). The agent is rewarded for reaching a designated goal region, and the relevant dataset (\emph{navigate}) comprises long‐horizon trajectories that challenge each method’s capacity for exploration. As in prior work \citep{diffuser}, we employ a heuristic controller for the pointmaze environment, whereas the antmaze environment uses a value‐learning policy \citep{plandq} for low‐level control. Appendix~\ref{appx:maze} provides more details on this task evaluation.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/pointmaze_samples5.png}
    \vspace{-2mm}
    \caption{Comparison of generated plans and actual rollouts for three planners—Diffuser, Diffusion Forcing, and MCTD. While Diffuser and Diffusion Forcing fail to produce successful trajectory plans, MCTD succeeds by refining its plan adaptively.}%
    \label{fig:pointmaze_samples}%
    \vspace{-3mm}
\end{figure}

\paragraph{Results.}
Table~\ref{tab:maze_results} presents success rates across medium, large, and giant mazes for both point‐mass and ant robots. \textbf{We can see that MCTD consistently surpasses other methods by a large margin.} Notably, Diffuser‐Random Search, despite using roughly the same number of denoising steps as MCTD, demonstrates no improvement over one‐shot Diffuser, underscoring the importance of tree-based systematic branching over random sampling. Diffuser‐Replanning similarly fails to outperform the base Diffuser, whereas Diffusion Forcing exhibits higher success rates in larger mazes, indicating benefits from a semi‐autoregressive schedule for long horizons. However, only MCTD achieves near‐perfect performance on the most challenging (large, giant) mazes, thanks to its ability to refine partial trajectories and avoid dead‐ends or inconsistent plan segments. This process is visualized in Figure \ref{fig:tree_sample}. The performance on the giant map is impressive, because in the map, there are lots of detour pathes to reach the goal as shown in Figure~\ref{fig:pointmaze_samples}. Comparing with Diffuser‐Random Search further highlights MCTD’s efficiency, since it offers substantially better outcomes under the same computational budget.

In the antmaze domain, \textbf{MCTD continues to perform nearly perfectly, while baseline performance generally degrades more significantly than in pointmaze}, except for the medium-sized settings under Diffusion Forcing. A principal reason for the baseline's degradation is that the value learning policy~\citep{dql} struggles in high‐dimensional control tasks. Even the performance degradations exist due to inaccurate execution, MCTD consistently outperforms baselines and the gap is larger as the maze size is larger.

%Even if the planner produces a sound high‐level path, suboptimal low‐level execution often prevents reaching the goal within the environment’s step limit. For antmaze giant, due to its larger map size, we used 1500 planning steps, while all other tasks were limited to 1000 steps.

% In experiments with the antmaze giant task, extending the maximum episode length from 1000 to 1500 steps raises MCTD’s success from 50\% to 94\%, indicating that the main bottleneck stems from the value learning agent’s ability to follow the planner’s trajectory rather than the planner itself.

%In summary, these maze navigation experiments highlight the benefits of MCTD’s tree‐structured partial denoising for long‐range planning. By revisiting suboptimal branches and refining partial plans, MCTD achieves high success rates in large or giant mazes where single‐pass diffusion often struggles. While shorter tasks narrow the performance gap, MCTD’s consistent improvements underscore its adaptability to a range of horizon lengths.


\begin{figure}[t!]  
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/tree_sample4.png}
    \vspace{-3mm}
    \caption{Visualization of the MCTD tree‐search process with a binary guidance set $\{\textsc{NO\_GUIDE}, \textsc{GUIDE}\}$ on a pointmaze‐medium task. Each node corresponds to a partially denoised trajectory, where the left image shows the noisy partial plan and the right image shows the plan after fast denoising. The search expands child nodes by selecting either  \textsc{NO\_GUIDE} or \textsc{GUIDE}, evaluates each newly generated plan, and ultimately converges on the highlighted leaf as the solution. }%
    \label{fig:tree_sample}%
    \vspace{-3mm}
\end{figure}

\begin{table*}[ht]
\centering
\caption{\textbf{Robot Arm Cube Manipulation Results.} Success rates (\%) for single, double, triple, and quadruple cube tasks in OGBench. Parenthetical values in the MCTD-Replanning column denote performance when using a DQL performer trained only on the single-cube dataset.}
\vspace{1mm}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{p{3.5cm}|ccc|cc} 
\Xhline{2\arrayrulewidth}
\textbf{Dataset} & \textbf{Diffuser} & \textbf{Diffuser-Replanning} & \textbf{Diffusion Forcing}  & \textbf{MCTD}  & \textbf{MCTD-Replanning} \\ \hline
single-play-v0 &  $78\pm23$ & $92\pm13$ & $\textbf{100}\pm\textbf{0}$  & $98\pm 6$  & $\textbf{100}\pm\textbf{0}$ \\
double-play-v0 &  $12\pm10$ & $12\pm13$ & $18\pm11$  & $22\pm11$  & $50\pm16$ ($\textbf{78}\pm\textbf{11}$) \\
triple-play-v0 &  $ 8\pm10$ & $ 4\pm 8$ & $16\pm8$   & $ 0\pm 0$  & $6\pm9$ ($\textbf{40}\pm\textbf{21}$) \\
quadruple-play-v0 & $0  \pm0 $ & $0  \pm 0$ & $0\pm0$    & $ 0\pm 0$  & $0\pm0$ ($\textbf{24}\pm\textbf{8}$) \\

\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:robot_result}
\vspace{-3mm}
\end{table*}

\begin{table*}[ht]
\centering
\caption{\textbf{Visual Pointmaze Results.} Success rates (\%) on partially observable, image-based mazes of medium and large sizes.}
\vspace{1mm}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{p{3cm}|ccc|cc} 
\Xhline{2\arrayrulewidth}
\textbf{Dataset} & \textbf{Diffuser} & \textbf{Diffuser-Replanning} & \textbf{Diffusion Forcing}  & \textbf{MCTD}  & \textbf{MCTD-Replanning} \\ \hline
medium-navigate-v0 &  $8\pm13$ & $8\pm10$ & $66\pm32$  & $82\pm18$  & $\textbf{90}\pm\textbf{9}$ \\
large-navigate-v0 &  $0\pm0$ & $0\pm0$ & $8\pm12$  & $0\pm0$  & $\textbf{20} \pm \textbf{21}$ \\

\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:visual_maze_result}
\vspace{-3mm}
\end{table*}


\subsection{Robot Arm Cube Manipulation}

For multi‐cube manipulation tasks from OGBench, a robot arm must move one to four cubes to specific table locations as shown in Figure~\ref{fig:stack_sample}. Increasing the number of cubes grows both the planning horizon and the combinatorial complexity. While the planner (MCTD or a baseline) issues high‐level positions for the cubes, a value-learning policy~\citep{dql} executes local actions. We augment MCTD with \emph{object‐wise guidance} by selecting which cube is manipulated at each step, avoiding simultaneous movements of multiple cubes. We note that the object-wise guidance is particularly suited to MCTD because the tree-expansion mechanism naturally supports discrete "meta-actions" at each node, such as selecting which object to move next. Single-pass diffusion methods, by contrast, attempt to produce a single, holistic trajectory in one go, leaving no opportunity to decide object-by-object. This guidance and additional task‐specific reward shaping are described in Appendix~\ref{appx:robot}.


\begin{figure}[t]
    \centering
    \subfloat[\centering  Stacking Task]{{\includegraphics[width=0.25\columnwidth]{figs/cube_2cube_stacking_viz.png}\label{fig:stack_sample}}}%
    \hspace{15mm}
    \subfloat[\centering  Holistic Plan]{{\includegraphics[width=0.3\columnwidth]{figs/cube_df_plan.png}\label{fig:cube_holistic_plan}}}%
    % \hspace{6mm}
    \\
    \subfloat[\centering  Object-wise Plan]{{\includegraphics[width=0.3\columnwidth]{figs/cube_mctd_replanning_plan1.png}
    \hspace{12mm}
    \includegraphics[width=0.3\columnwidth]{figs/cube_mctd_replanning_plan2.png} \label{fig:cube_object_wise_plan}}}%
    %\hspace{5mm}
    %\subfloat[\centering \scriptsize Object-wise Plan-2]%{{\includegraphics[width=0.3\columnwidth]{figs/cube_mctd_replanning_plan2.png}}}\label{fig:cube_object_wise_plan}}
    \vspace{-2mm}
    \caption{(a) Illustration of the robot arm cube manipulation task. (b) A holistic plan for moving two cubes, where entanglement between their partial trajectories leads to an inaccurate movement (blue cube). (c) An object‐wise plan produced by MCTD‐Replanning, which avoids entanglement and achieves more reliable control over each cube’s movement.}%
    \vspace{-3mm}
\end{figure}

\paragraph{Results.}
Table~\ref{tab:robot_result} shows success rates for one to four cubes. All methods perform comparably on single‐cube tasks, but performance drops significantly for multi‐cube scenarios, especially for standard diffusion baselines that struggle to decide cube ordering and handle subtasks such as stacking. However, \textbf{MCTD-Replanning shows significant performance gaps from other models for multi-cube scenarios.} MCTD exhibits moderate gains (e.g., 22\% on two cubes), though it suffers from holistic plan entanglements when multiple objects are involved as shown in Figure~\ref{fig:cube_holistic_plan}. To address this, \emph{MCTD‐Replanning} periodically replans, effectively separating each cube’s movements as shown in Figure~\ref{fig:cube_object_wise_plan}. This boosts success on the double‐cube problem from 22\% to 50\%, and also benefits tasks with more cubes. Another challenge arises from the value-learning policy, which generalizes poorly as the number of cubes increases. Even if MCTD generates a suitable global plan, flawed local control undermines the final outcome. Interestingly, training the policy solely on single‐cube data still allows MCTD‐Replanning to solve a portion (24\%) of quadruple‐cube scenarios, illustrating the potential of object‐wise guidance and iterative replanning. 

%\paragraph{Summary.}
%Overall, these results underscore the advantages of \emph{object‐wise guidance} and \emph{periodic replanning} in complex multi‐object manipulation tasks. By isolating cube‐specific movements and leveraging a more robust DQL performer, \emph{MCTD‐Replanning} proves capable of navigating the combinatorial explosion inherent in multi‐cube scenarios, thereby offering a more effective solution than single‐shot diffusion methods. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/vog_env.pdf}
    % \caption{Task and trajectories examples in the visual pointmaze. The agent receives pixel-based initial and goal images under partial observability. Diffusion Forcing (top row) struggles to reach the goal failing to explore by focusing on exploitation, while MCTD (bottom row) successfully navigates to the goal by balancing exploration and exploitation.}
    \caption{Task and interaction examples in the visual pointmaze. The agent operates under partial observability, receiving pixel-based initial and goal images. In the top row, Diffusion Forcing struggles to reach the goal due to a focus on exploitation, failing to sufficiently explore the environment. In contrast, MCTD (bottom row) effectively balances exploration and exploitation, successfully navigating to the goal.}
    \label{fig:visual_pointmaze_viz}%
    \vspace{-7mm}
\end{figure}

\subsection{Visual Pointmaze}
% To investigate  \emph{partial observability}, we introduce a visual pointmaze environment. Although OGBench includes a visual antmaze, we opt for a pointmaze variant to reduce the complexity of high‐dimensional locomotion. Here, the agent receives top‐down initial and goal images as shown in Figure~\ref{fig:visual_pointmaze_viz}, forcing the planner to navigate through partial visual inputs rather than ground‐truth states.
% To evaluate our method in complex observation spaces, especially in the \emph{partial observability}, we introduce a visual pointmaze environment. Although OGBench includes a visual antmaze, we opt for a pointmaze variant to reduce the complexity of high‐dimensional locomotion, and focus on observation complexity. Here, the agent receives top‐down initial and goal images as shown in Figure~\ref{fig:visual_pointmaze_viz}, forcing the planner to navigate through partial visual inputs rather than ground‐truth states.
To evaluate our method in image-based planning, particularly under partial observability, we introduce a visual pointmaze environment where the agent receives top-down pixel images of the initial and goal states, as shown in Figure~\ref{fig:visual_pointmaze_viz}. Unlike the visual antmaze in OGBench, which involves complex locomotion, this environment allows us to focus on how well the planner operates with raw visual inputs without relying on ground-truth states.

We pre‐encode observations with a Variational Autoencoder (VAE)~\citep{vae} to generate latent states suitable for diffusion‐based planning. Because Diffuser’s default heuristic controller is inapplicable to purely visual inputs, we design an \emph{inverse dynamics} model that maps consecutive latent observations to actions, $\hat{a}_t = \mathrm{InvDyna}\bigl((x_{t-1},\, x_{t}),\, x_{t+1}\bigr)$. For guidance, we employ a position estimator trained on ground‐truth coordinates; however, this estimator does not update the underlying VAE, ensuring that the planner remains in a partially observable regime. The details for this task setting are discussed in Appendix~\ref{appx:visual_pointmaze}.


\paragraph{Results.}
Table~\ref{tab:visual_maze_result} compares success rates on medium and large versions of the visual pointmaze. \textbf{MCTD and MCTD-Replanning show better performance than baselines on image-based, partial observable tasks.} MCTD outperforms Diffuser baselines and Diffusion Forcing in the medium maze, presumably due to its tree‐structured exploration, which better balances exploration and exploitation under perceptional partial observability. In the large maze, single‐pass planners fail almost entirely, highlighting the difficulty of long‐horizon visual planning. MCTD‐Replanning improves performance by reevaluating partial plans mid‐trajectory, though success remains modest overall. This result underscores the inherent challenge of scaling diffusion‐based methods to extended horizons with partial observational visual data and suggests that future work may need more sophisticated visual encoders or hierarchical guidance strategies.

%\paragraph{Summary.}
%By extending diffusion‐based planning to visual observations, we demonstrate that \textbf{MCTD} retains its search advantages even without direct access to ground‐truth states. In medium‐scale mazes, MCTD’s partial denoising and branching lead to higher success rates than competing baselines. Larger maps remain problematic for all planners, highlighting the difficulty of sustaining coherent plans when the agent only perceives local image patches. These results point to the importance of integrating improved visual encoders, more sophisticated guidance signals, or hierarchical approaches—directions we leave for future work.

% \subsection{Visual Planning}
% To evaluate our method in complex observation spaces, we conducted experiments in an environment with high-dimensional pixel observations.
% To focus on observation complexity rather than action, we converted the \emph{visual-antmaze} from OGBench \citep{4_park2024ogbench} into a \emph{visual-pointmaze}.

% To manage the computational demands of the diffusion models effectively, we compressed each observation into a latent space, using a pretrained Variational Autoencoder (VAE) \citep{vae} following \citep{0_latent_diffusion_model2022ldm}. We then trained models to plan within the latent space \citep{hafner2019dream, hafner2019planet}.  \hs{Action generation은 ~\citep{decisiondiffuer} Inverse Dynamics를 offlinedm로 학습시켰고, guidance으로는 estimated l2 distance를 이용하였다. appendix}


% Table~\ref{tab:visual_pointmaze_results} shows the empirical results in this \emph{visual-pointmaze}.
% \hs{The planning from the visual partial observations planning, as map이 커지고 복잡해질 수록 planning 자체가 뭉게진다. However, the difficulty on planning with the inaccurate position estimation is increased as the planning horizon is increased, this effectiveness is less shown in large map. We expect this issue can be addressed with external memory to help to estimate spatial identification, but this is out of this study, we leave it to future work.특히 좀 복잡한 환경에서는 Replanning이 꼭 필요했는데, 안하면 좀 뭉게지더라. - 표 다시 맞춰놓겠습니다.}

% \begin{table}[h]
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lcccccc}
% \Xhline{2\arrayrulewidth}
% \textbf{Dataset} & \textbf{Method}          & \textbf{NoReplan}      & \textbf{Replan}        \\ \hline
% medium           & Diffuser                 & $0 \pm 0$            & $76 \pm 12$            \\
%                  & DF                       & $0 \pm 14$            & $53 \pm 16$            \\
%                  & S2Diffuser               & $80 \pm 12$            & $90 \pm 14$            \\ \hline
% large            & Diffuser                 & $30 \pm 14$            & $34 \pm 16$            \\
%                  & DF                       & $18 \pm 0$             & $20 \pm 0$             \\
%                  & S2Diffuser               & $18 \pm 0$             & $20 \pm 0$             \\ \hline
% % giant            & Diffuser                 & $0 \pm 0$              & $0 \pm 0$              \\
% %                  & DF                       & $0 \pm 0$              & $0 \pm 0$              \\
% %                  & S2Diffuser               & $0 \pm 0$              & $0 \pm 0$              \\ 
% \Xhline{2\arrayrulewidth}
% \end{tabular}
% }
% \caption{Visual Point Maze Results. The dataset is visual-pointmaze-\{medium, large, giant\}-navigate-v0. Results are separated by replanning and no replanning for each method (Diffuser, DF, S2Diffuser).}
% \label{tab:visual_pointmaze_results}
% \end{table}

% \begin{table}[h]
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lccccccccc}
% \Xhline{2\arrayrulewidth}
% \textbf{Dataset} & \textbf{Diffuser-Replanning} & \textbf{DF-Replanning}  & \textbf{S2Diffuser-Replanning} \\ \hline
% medium &  $ 76 \pm 12$ & $ 53 \pm 16$ & $  90 \pm 14 $  \\
% large &   $ 34 \pm 16$ & $ 20 \pm 0$ & $  20 \pm 0 $  \\
% giant &   $ 0 \pm 0$ & $ 0\pm 0$ & $  0\pm 0 $  \\

% \Xhline{2\arrayrulewidth}
% \end{tabular}
% }
% \caption{Visual Point Maze Results, dataset is visual-pointmaze-\{medium,large,giant\}-navigate-v0. }
% \label{tab:visual_maze_results}
% \end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/ttc_plots.png}
    \vspace{-5mm}
    \caption{Success rates (\%) and wall‐clock runtime (Sec.) as the maximum number of denoising steps (test‐time budget) increases for large and giant pointmaze tasks}%
    \label{fig:ttc_plots}%
    \vspace{-5mm}
\end{figure}

\subsection{Test-Time Compute Scalability \& Time Complexity}

We examine how each planner leverages additional \emph{test‐time computation} by varying the maximum number of denoising steps and measuring both \emph{success rate} and \emph{runtime} on the large and giant pointmaze tasks (Figure~\ref{fig:ttc_plots}). \textbf{As the TTC budget grows, MCTD consistently achieves high success rates—ultimately nearing perfection in giant mazes.} In contrast, simply increasing denoising steps (Diffuser) or drawing more random samples (Diffuser‐Random Search) yields modest returns, suggesting limited test‐time scalability. 

MCTD incurs additional overhead from maintaining and expanding a tree—leading to a larger runtime as the budget grows. For instance, Diffuser‐Random Search shows moderate runtime growth because multiple samples can be denoised in parallel batches, which is not directly feasible in MCTD’s tree‐structured approach. Nonetheless, MCTD can terminate early whenever a successful plan is found, thereby limiting unnecessary expansions. As a result, on the large maze, MCTD’s runtime quickly saturates once the task can be solved with a smaller search depth, rather than fully consuming the available budget. A similar pattern emerges in the giant maze: as success rates increase under higher budgets, the runtime growth for MCTD remains sublinear, indicating that many expansions terminate early once suitable trajectories are located.
%\textbf{Overall, MCTD's runtime overhead is managed by avoiding unnecessary expansions.}
%and by allocating computational resources selectively to the most promising trajectories, ensuring that search efforts translate into meaningful performance gains.}

%To investigate how each planner leverages additional \emph{test‐time computation} (TTC), we vary the maximum number of denoising steps and measure both \emph{success rate} and \emph{runtime} (in seconds) on the \emph{large} and \emph{giant} pointmaze tasks, as shown in Figure~\ref{fig:ttc_plots}. Along the horizontal axis, each method is afforded more denoising steps, samples, or searches as the computational budget grows.

%\paragraph{Success Rate.}
%\textbf{MCTD} consistently benefits from larger TTC budgets, achieving near‐perfect performance in giant mazes under high compute settings. In contrast, standard Diffuser and Diffuser‐Random Search offer only modest gains from additional denoising steps and samplings, implying that simply lengthening a one‐shot diffusion schedule or drawing more random samples yields limited returns in test‐time scalability.

%\paragraph{Runtime.}
%The lower plots of Figure~\ref{fig:ttc_plots} depict how wall‐clock time increases with higher TTC allocations. While \textbf{MCTD} incurs additional overhead from maintaining and expanding a tree—leading to a larger runtime as the budget grows. For instance, Diffuser‐Random Search shows moderate runtime growth because multiple samples can be denoised in parallel batches, which is not directly feasible in MCTD’s tree‐structured approach. Nonetheless, MCTD can terminate early whenever a successful plan is found, thereby limiting unnecessary expansions. As a result, on the large maze, MCTD’s runtime quickly saturates once the task can be solved with a smaller search depth, rather than fully consuming the available budget. A similar pattern emerges in the giant maze: as success rates increase under higher budgets, the runtime growth for MCTD remains sublinear, indicating that many expansions terminate early once suitable trajectories are located.

\section{Limitations \& Discussion}
\label{sec:limit}

While MCTD enhances test-time compute scalability by integrating search-based planning with diffusion models, it remains computationally expensive due to its System 2-style deliberate reasoning. Determining when to engage in structured planning versus relying on fast System 1 (model-free) planning is an open challenge, as expensive tree search may not always be necessary. Adaptive compute allocation based on task complexity or uncertainty could improve efficiency. 

Another limitation is inefficiency in large-scale search spaces, where evaluating multiple trajectory hypotheses remains computationally demanding despite using low-dimensional meta-actions. A promising direction is amortizing search learning, where the system meta-learns from test-time search to improve exploration efficiency over time. Instead of treating initial exploration as random, MCTD could incorporate test-time learning mechanisms to refine its search dynamically. Additionally, self-supervised reward shaping could improve trajectory evaluation in sparse-reward settings. Optimizing parallelized denoising, differentiable tree search, and model-based rollouts could further enhance efficiency.


\section{Conclusion}
\label{sec:conc}

We introduced Monte Carlo Tree Diffusion (MCTD), a framework designed to combine the best of both worlds: the structured search of Monte Carlo Tree Search and the generative flexibility of diffusion planning to enhance the test-time compute scalability of System 2 planning. MCTD leverages meta-actions for adaptive exploration-exploitation, tree-structured denoising for efficient diffusion-based expansion, and fast jumpy denoising for rapid simulation. Experimental results demonstrate that MCTD outperforms existing approaches in various planning tasks, achieving superior scalability and solution quality. Future work will explore adaptive compute allocation, learning-based meta-action selection, and reward shaping to further enhance performance, paving the way for more scalable and flexible System 2 planning.



% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}
% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\newpage

\section*{Acknowledgements}
This research was supported by GRDC (Global Research Development Center) Cooperative Hub Program (RS-2024-00436165) and Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT.

\section*{Impact Statement}

This work introduces Monte Carlo Tree Diffusion (MCTD), a framework that integrates diffusion models with Monte Carlo Tree Search (MCTS) for scalable, long-horizon planning. As a general-purpose planning framework, MCTD itself does not pose direct risks; however, care must be taken when applying it to safety-critical domains, where decision-making impacts human well-being or critical infrastructure. Depending on its implementation, MCTD could influence high-stakes decisions in areas such as autonomous systems, healthcare, or finance, necessitating robust oversight and alignment with ethical guidelines. Additionally, its computational demands raise considerations around energy efficiency and sustainability, highlighting the need for responsible AI deployment. 


% Looking ahead, MCTD provides a foundation for more powerful and adaptive AI planning systems, reinforcing the importance of safe and transparent integration in real-world applications.

%Authors are \textbf{required} to include a statement of the potential 
%broader impact of their work, including its ethical aspects and future 
%societal consequences. This statement should be in an unnumbered 
%section at the end of the paper (co-located with Acknowledgements -- 
%the two may appear in either order, but both must be before References), 
%and does not count toward the paper page limit. In many cases, where 
%the ethical impacts and expected societal implications are those that 
%are well established when advancing the field of Machine Learning, 
%substantial discussion is not required, and a simple statement such 
%as the following will suffice:

%``This paper presents work whose goal is to advance the field of 
%Machine Learning. There are many potential societal consequences 
%of our work, none which we feel must be specifically highlighted here.''

%The above statement can be used verbatim in such cases, but we 
%encourage authors to think about whether there is content which does 
%warrant further discussion, as this statement will be apparent if the 
%paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

%\bibliography{example_paper, hs_ref}
\bibliography{example_paper,refs_ahn}

\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Experiment Details}
\subsection{Baselines} \label{appx:baselines}
We compare MCTD against several baselines, each highlighting different ways to leverage test-time computing on diffusion models or broader planning strategies:
\begin{itemize}
\item \textbf{Diffuser} \cite{diffuser}: We use the standard single‐shot Diffuser as our primary diffusion baseline. It generates a full trajectory by denoising from maximum to zero noise in one pass, guided by a return‐predicting function $J_{\phi}$. While it can produce coherent plans, it lacks any mechanism to adapt or refine these plans at test time.
\item \textbf{Diffuser-Replanning}: To partially address the lack of iterative search, we evaluated a replanning variant of Diffuser. After an initial denoised plan is sampled, the policy is periodically re‐invoked at predefined intervals (e.g., every 10 time steps), using the agent’s new state as the “start” for a fresh diffusion process. This strategy leverages additional test‐time compute by attempting incremental corrections, but does not share information across different replans.
\item \textbf{Diffuser-Random Search}: We consider a random‐search variant of Diffuser that generates multiple trajectories from the randomly noised plans and picks the best candidate according to a heuristic or learned value function. This approach can be seen as a “Sample-Score-Rank” method proposed in \citep{d-mpc}, increasing test‐time compute by drawing more samples, but lacking the systematic exploration or adaptation found in tree‐search paradigms.
\item \textbf{Diffusion Forcing}: Diffusion Forcing~\citep{diffusion_forcing} extends Diffuser by introducing a tokenized, causal noise schedule. Subplans of the trajectory can be denoised at different rates, allowing partial re‐sampling of specific time segments without re‐optimizing the entire plan. In our experiments, we evaluate Diffusion Forcing with the Transformer backbone provided in their official repository, which performed better in our preliminary tests. This method helps isolate the benefit of tokenized partial denoising from our tree‐structured approach. We note that Diffusion Forcing baselines are basically designed to do replanning, so we follow the setting in this evaluation.
\end{itemize}

\subsection{Heuristic Controller, Value-Learning Policy and Inverse Dynamics Model}

A long‐standing challenge in diffusion‐based planning is the difficulty of preserving global trajectory coherence while managing local, high‐dimensional state‐action control~\citep{diffusion_forcing, plandq}. For example, PlanDQ~\citep{plandq} couples a high‐level diffusion planner with a learned low‐level policy. Similarly, our approach focuses the diffusion planner on lower‐dimensional, representative state information (e.g., object or agent positions), while delegating detailed action inference to the hueristic controller or value-learning policy or inverse dynamics model. The details of this integration is discussed in Algorithm~\ref{alg:additional-performer}. Such a hierarchical design allows MCTD and baseline planners to concentrate on broader strategic decisions while leaving the finer‐grained execution details to the heuristic controller or value-learning policy or inverse dynamics model.

\subsection{Model Hyperparameters}
% \subsection{Implementation Details}
For reproducibility, we provide details on the hyperparameters used in our experiments. These settings were chosen based on prior work and empirical tuning to ensure stable training and evaluation across all tasks. The almost same hyperparameters were applied consistently across tasks, except in cases where task-specific configurations are required. They will be discussed in each task section.
%where applicable, except in cases where model-specific configurations were required The full list of hyperparameters for Diffuser and Diffusion Forcing is provided in the following sections.

\subsubsection{Diffuser}
We build our tested Diffuser on the official implementation, which is available \url{https://github.com/jannerm/diffuser}. While keeping the core architecture and training procedure intact, we made the following modifications to align with our experimental setup:

\begin{itemize}
    \item \textbf{Guidance Function:} For pointmaze tasks, Diffuser originally applied the goal-inpainting guidance, but in our evaluation, we employed a distance-based reward function to ensure fair comparison across baselines.
    \item \textbf{Replanning Variant:} To evaluate the impact of test-time iterative refinement, we implemented a replanning strategy that periodically re-samples trajectories based on the agent’s updated state.
    \item \textbf{Random Search Variant:} To evaluate the impact of test-time compute scaling with more samples, we implemented Diffuser version "Sample-Score-Rank"~\citep{d-mpc}. Additionally, each sample can be guided through different guidances, we applied this by uniformly distributing the guidance set $[0.01, 0.05, 0.1, 0.2, 0.3]$ to the samples.
    % \hs{DOOJIN please check this part}
    % \item \textbf{Evaluation Protocol:} We adjusted the evaluation pipeline to match the constraints of our benchmark tasks, such as step limits and additional constraints in multi-object manipulation tasks.

\end{itemize}

\begin{table}[t]
\begin{center}
\caption{Diffuser Hyperparameters}
\vspace{1mm}
\begin{tabular}{lr}
\toprule
Hyperparameter & Value \\
\midrule
Learning Rate &  $2e-4$ \\
EMA Decay &  $0.995$ \\
Precision in Training/Inference &  $32$ \\
Batch Size &  $32$ \\
Max Training Steps &   $20000$\\
Planning Horizon & Different for each task, please check in the task sections \\
Open Loop Horizon & $50$ for Diffuser-Replanning, otherwise planning horizon \\
Guidance Scale & $0.1$ \\
Beta Schedule & Cosine \\
Diffusion Model Objective & $x_0$-prediction \\
U-Net Depth & $4$ \\
Kernel Size & 5 \\ 
The Number of Channels &32, 128, 256 \\ 
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\caption{Diffusion Forcing Hyperparameters}
\vspace{1mm}
\begin{tabular}{lr}
\toprule
Hyperparameter & Value \\
\midrule
Learning Rate &  $5e-4$ \\
Weight Decay &  $1e-4$ \\
Warmup Steps &  $10000$ \\
Precision in Training &  $16$-mixed \\
Precision in Inference &  $32$ \\
Batch Size &  $1024$ \\
Max Training Steps &  $200005$ \\
The Number of Frame Stack & $10$ \\
Planning Horizon & Different for each task, please check in the task sections \\
Open Loop Horizon & $50$ \\
Causal Mask & Not Used \\
Guidance Scale & $3$ for medium mazes and $2$ for other mazes \\
Scheduling Matrix & pyramid \\
Stabilization Level & $10$ \\
Beta Schedule & Linear \\
Diffusion Model Objective & $x_0$-prediction \\
DDIM Sampling eta & $0.0$ \\
Network Size & $128$ \\
The Number of Layers & $12$ \\
The Number of Attention Heads & $4$ \\
The Feedforward Network Dimension & $512$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Diffusion Forcing}
We build our tested Diffusion Forcing upon the officially released code from \url{https://github.com/buoyancy99/diffusion-forcing}. The following modifications were made:

\begin{itemize}
    \item \textbf{Replanning Frequency:} Since Diffusion Forcing inherently supports test-time replanning, we evaluated it under comparable conditions by applying periodic plan refinement, similar to Diffuser-Replanning.
    \item \textbf{Transformer Backbone:} To maintain consistency across models, we utilized the Transformer-based backbone provided in the Diffusion Forcing repository, rather than alternative architectures.
\end{itemize}


\subsubsection{Monte Carlo Tree Diffusion (MCTD)}

MCTD follows Diffusion Forcing hyperparameters, but some of them are modified for MCTD and tree search related hyperparameters are added.


\begin{table}[h]
\label{tab:mctd_hyperparam}
\begin{center}
\caption{MCTD Hyperparameters}
\vspace{1mm}
\begin{tabular}{lr}
\toprule
Hyperparameter & Value \\
\midrule
Learning Rate &  $5e-4$ \\
Weight Decay &  $1e-4$ \\
Warmup Steps &  $10000$ \\
Precision in Training &  $16$-mixed \\
Precision in Inference &  $32$ \\
Batch Size &  $1024$ \\
Max Training Steps &  $200005$ \\
The Number of Frame Stack & $10$ \\
Planning Horizon & Different for each task, please check in the task sections \\
\textbf{Open Loop Horizon} & Same to planning horizon for MCTD, $50$ for MCTD-Replanning \\
Causal Mask & Not Used \\
Scheduling Matrix & pyramid \\
\textbf{The Maximum Number of Search} & $500$ \\
\textbf{Guidance Set} & Different for each task, please check in task sections \\
\textbf{The number of Partial Denoising} & $20$ \\
\textbf{The Jumpy Denoising Interval} & $10$ \\
Stabilization Level & $10$ \\
Beta Schedule & Linear \\
Diffusion Model Objective & $x_0$-prediction \\
DDIM Sampling eta & $0.0$ \\
Network Size & $128$ \\
The Number of Layers & $12$ \\
The Number of Attention Heads & $4$ \\
The Feedforward Network Dimension & $512$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\label{tab:dql_hyperparams}
\begin{center}
\caption{Value-Learning Policy Hyperparameters}
\vspace{1mm}
\begin{tabular}{lr}
\toprule
Hyperparameter & Value \\
\midrule
Learning Rate &  $3e-4$ \\
Learning Eta &  $1.0$ \\
Max Q Backup &  False \\
Reward Tune &  cql\_antmaze \\
The Number of Training Epochs &  $2000$ \\
Gradient Clipping & $7.0$ \\
Top-k &  $1$ \\
Target Steps &  $10$ \\
Randomness on Data Sampling (p) & $0.2$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Value-Learning Policy}

For some tasks, to handle complex action space, we applied a value-learning policy by modifying the PlanDQ~\citep{plandq} source code, \url{https://github.com/changchencc/plandq}.



%\begin{table}[h]
%\caption{Hyperparameters used in Diffuser, we mostly followed the implementation from \citep{janner2021offline}}
%\vspace{6mm}
%\label{table:baseline_hyperparameter}
%\begin{center}
%\begin{tabular}{lr}
%\toprule
%Hyperparameter & Value \\
%\midrule
%Optimizer &  \\
%Learning Rate &  \\
%Optimizer Hyperparameters ($\beta_1$, $\beta_2$) &  \\
%Batch Size &  \\
%Gradient Steps & \\
%Horizons & 500, 500, 1000\\
%Replanning Frequency & 50 \\
%Gradient Clip & \\
%\bottomrule
%\end{tabular}
%\end{center}
%\end{table}



%\begin{table}[h]
%\caption{Hyperparameters used in Diffusion Forcing, we mostly followed the implementation from \citep{diffusion_forcing}}
%\vspace{6mm}
%\label{table:diffusion_forcing_hyperparameters}
%\begin{center}
%\begin{tabular}{lr}
%\toprule
%Hyperparameter & Value \\
%\midrule
%Optimizer &  \\
%Learning Rate &  \\
%Optimizer Hyperparameters ($\beta_1$, $\beta_2$) &  \\
%Batch Size &  4096\\
%Gradient Steps & 20k\\
%Horizons & 500, 500, 1000\\
%Replanning Frequency & 50 \\
%Gradient Clip & \\
%\bottomrule
%\end{tabular}
%\end{center}
%\end{table}

\subsection{The Evaluation Details}

We followed the OGBench~\citep{4_park2024ogbench} task configurations, 5 tasks per each environment. For each task, we evaluated 10 random seeds per each model, and reported the averaged success rates and their standard deviations.

\subsection{Maze Navigation with Point‐Mass and Ant Robots} \label{appx:maze}

\subsubsection{Modifications from OGBench}

 Although the original OGBench datasets include random noise in start and goal positions, we remove this noise to isolate model performance from environmental randomness. This modification facilitates clearer comparisons across methods and ensure that differences in performance can be attributed to planning capabilities rather than uncontrolled stochasticity. Additionally, we incorporate velocity into the state representation, which the original benchmark does not provide, in order to apply the heuristic controller for pointmaze tasks implemented in~\citep{diffuser}. For antmaze giant task, we found there are some cases where the plans are well generated, but due to the inaccurate execution, it cannot be succeed. To prevent this, we extend the episode length from $1000$ to $1500$.

\subsubsection{Guidance Function} \label{appx:maze_guidance}

To plan long-horizon while sustaining the plan effectiveness to reach the goal as fast as possible, we applied the guidance function to reduce the distances between every state and goal, $\sum_{i} ||x_i-g||_2$~\citep{diffusion_forcing}. We note that, to do fair comparison we applied the same guidance style over every baseline. For example, Diffuser originally used the goal-inpainting guidance for pointmaze, but in this evaluation, we used the distance guidance.

\subsubsection{Guidance Set}

We applied different guidance sets to MCTD for the different tasks. For pointmaze medium and large, $[0, 0.1, 0.5, 1, 2]$ is applied, and for pointmaze giant, $[0.5, 1, 2, 3, 4]$ is applied. For antmaze tasks, $[0, 1, 2, 3, 4, 5]$ is set.

\subsubsection{Planning Horizon}

For medium and large mazes, we applied $500$ planning horizons and $1000$ for giant to train the models with enough data. Medium and large \emph{navigate} dataset trajectory horizons are $1000$ and $2000$ for giant. To make more data through sliding window technique, we applied smaller planning horizon than the dataset horizon.

\subsubsection{Heuristic Controller and Value-Learning Policy}

For pointmaze, we take the heuristic controller designed in~\citep{diffuser}. For antmaze, we use the value-learning policy~\citep{dql} as done in \citep{plandq}. However, we note that different from \citep{plandq}, we only give the $10$ steps further planned position information as the subgoal for that.

\subsubsection{MCTD Reward Function}

For the reward function, we designed heuristic rules. One of those is to check whether there exists too large position difference between near states, because it is impossible physically. Another rule is to give a reward when reaching the goal. Earlier reaching can get the larger reward, $r=(H-t)/H$, where $H$ and $t$ are the horizon length and current step. 

\begin{table}[ht]
\centering
\caption{\textbf{Short-Horizon (Stitch) Maze Results.} Success rates (\%) on reduced-horizon \emph{stitch} variants of the pointmaze environment.}
\vspace{1mm}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{p{4.2cm}ccccccccc}
\Xhline{2\arrayrulewidth}
\textbf{Dataset} & \textbf{Diffuser-Replanning} & \textbf{Diffusion Forcing}  & \textbf{MCTD-Replanning} \\ \hline
pointmaze-medium-stitch-v0 &  $ 76 \pm 12$ & $ 53 \pm 16$ & $  90 \pm 14 $  \\
pointmaze-large-stitch-v0 &   $ 34 \pm 16$ & $ 20 \pm 0$ & $  20 \pm 0 $  \\

\Xhline{2\arrayrulewidth}
\end{tabular}
}
\label{tab:maze_stitch_results}
\end{table}

\subsubsection{Short‐Horizon (Stitch) Results}

To examine performance when only short-horizon trajectories are given in the training phase, we evaluate on the stitch datasets, summarized in Table~\ref{tab:maze_stitch_results}. The trajectory horizon in stitch datasets is much shorter than the required steps to reach the goal (e.g., the horizon in the dataset is 100 while over 200 steps are required in medium map tasks), we applied replanning on Diffuser, Diffusion Forcing, and MCTD. Different from the long-horizon dataset given cases, the Diffuser shows better performances than Diffusion Forcing, it can be seen that the implicit stitching ability of the Diffuser is better than Diffusion Forcing. On the other hand, MCTD shows better performance than baselines in the medium-sized map, due to multiple samplings with exploration, but the search length could not be long due to the limited length of trajectories in the training dataset, so it does not show good performance in large map.

\begin{table}[h]
\centering\caption{\textbf{Comparison Results with Goal‐Inpainting Diffuser.} Success rates (\%) on pointmaze and antmaze environments with medium, large, and giant mazes for the \emph{navigate} datasets.
%Parenthetical values in the AntMaze Giant row denote performance when interacting with more steps (1500 steps) than predefined steps in OGBench (1000 steps).
}
\vspace{1mm}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{ll|cccc|c}
\Xhline{2\arrayrulewidth}
\multirow{2}{*}{\textbf{Environment}} 
 & \multirow{2}{*}{\textbf{Dataset}} 
 & \multicolumn{2}{c}{\textbf{Diffuser}}  
 & \multicolumn{2}{c|}{\textbf{Goal-Inpainting Diffuer}} 
 & \multirow{2}{*}{\textbf{MCTD}} \\
  &  & \textbf{Base} & \textbf{Replanning} &\textbf{Base} & \textbf{Replanning}   &  \\
\hline
\multirow{3}{*}{pointmaze} & medium-navigate-v0 &  $ 58 \pm 6$ & $  60\pm0 $ & $  84 \pm 8$ & $80  \pm 9$ & $\textbf{100} \pm \textbf{0}$ \\
  & large-navigate-v0 &   $ 44 \pm 8$ & $ 40\pm0 $ & $ 94 \pm 9$  & $84  \pm 17$ & $\textbf{98}  \pm \textbf{6}$ \\ 
  & giant-navigate-v0 & $0 \pm 0$ & $ 0\pm0 $ & $30 \pm 16$  & $34 \pm 21$ & $\textbf{100} \pm \textbf{0}$ \\
 \hline
\multirow{3}{*}{antmaze} & medium-navigate-v0 & $36 \pm 15$ & $ 40\pm18 $ & $100 \pm 0$  & $94 \pm 9$ & $\textbf{100} \pm \textbf{0}$ \\
 & large-navigate-v0 & $14 \pm 16$ & $ 26\pm13 $ & $86 \pm 9$  & $66 \pm 13$ & $\textbf{98} \pm \textbf{6}$ \\
 & giant-navigate-v0 & $0 \pm 0$ & $ 0\pm0 $ & $12 \pm 10$  & $20 \pm 0$ &$\textbf{94} \pm \textbf{9}$ \\

\Xhline{2\arrayrulewidth}
\end{tabular}
}
% 
\label{tab:goal_inpainting_results}
\vspace{-3mm}
\end{table}

\subsubsection{Comparison with Goal‐Inpainting Diffuser}
\label{appx:goal_inpainting_comparison}

Diffuser~\citep{diffuser} typically employs a learned guidance function to bias generated trajectories toward high‐return outcomes. For pointmaze tasks, however, an alternative approach—\emph{goal‐inpainting guidance}—has also been introduced~\citep{diffuser}, which further improves single‐pass planning by “imagining” an intermediate trajectory whose final state is the designated goal. In the context of Diffuser‐Replanning, we adapt goal‐inpainting by providing the experienced trajectory as contextual information rather than a direct target for denoising.

As shown in Table~\ref{tab:goal_inpainting_results}, goal‐inpainting outperforms standard Diffuser with heuristic guidance across medium, large, and giant pointmazes. By treating the goal as the terminal state to inpaint, the Diffuser model effectively samples trajectories from both ends, leveraging local convolutional mechanisms to reconcile the start and goal conditions. Although this approach can yield marked performance gains, \textbf{MCTD} remains superior in long‐horizon scenarios, outperforming both the base and replanning variants of goal‐inpainting Diffuser. The key distinction lies in MCTD’s tree‐structured partial denoising and adaptive branching: while goal inpainting enhances single‐pass generation, it lacks the iterative exploration‐exploitation loop of explicit tree search. Moreover, the advantage of two‐ended trajectory imagination diminishes when the maze is extremely large (e.g., giant mazes). Consequently, MCTD uncovers robust solutions even where goal inpainting begins to show diminishing returns, underscoring the benefits of tree‐based refinement in complex, long‐range planning tasks.

\subsection{Robot Arm Cube Manipulation} \label{appx:robot}

\subsubsection{Object‐Wise Guidance}

Because the robot arm can only move a single cube at any given time, we adopt an \emph{object‐wise guidance} mechanism in MCTD. Rather than applying a single holistic diffusion schedule to all cubes simultaneously, MCTD treats the selection of which cube to move next as a meta‐action. This design partially alleviates issues that arise when a single plan attempts simultaneous motion of multiple cubes, though multi‐object manipulation still demands careful sequencing to avoid suboptimal interleavings.

\subsubsection{Guidance Function}

The guidance function itself is same to that for maze tasks, Appendix~\ref{appx:maze_guidance}, but the object-specific guidance is applied to MCTD and MCTD-Replanning. The guidance tries to reduce the distances between \emph{specific object states and the object's goal only}.

\subsubsection{Guidance Set}

For cube tasks, the guidance set $[1, 2, 4]$ for each object is applied. For example, for two cube tasks, the size of the guidance set is $6$ with three guidance set for each object.

\subsubsection{Reward Function for Robot Arm Cube Manipulation}

In addition to the reward function for the maze environment, we added some rules not to generate unrealistic plans. One of them is not to move multiple objects simultaneously. The small noise on planning is okay, but if multiple objects move adequately, the reward function decides the plan is unrealistic and gives a $0$ reward. Other rules are, that the final state of the plan cannot be in the air, it should be on top of another cube or on the floor. The cube cannot posit where another cube is located, and when there is another cube on top of the target cube, the plan to move that is determined as unrealistic.

\subsubsection{Planning Horizon}

For single cube tasks, the planning horizon is $200$ same to the episode length, and $500$ for others to train the models with enough data.

\subsubsection{Value-Learning Policy}
Similar to antmaze tasks, we applied value-learning policy~\citep{dql} by achieving the subgoals suggested by planners for $10$ steps further planned states.

\subsection{Visual Pointmaze} \label{appx:visual_pointmaze}
\subsubsection{Setup}
Because diffusion‐based planners typically operate on state‐action representations, we first learn an \emph{image encoder} via a Variational Autoencoder (VAE) \citep{vae}. The resulting latent states serve as inputs to the diffusion models. Notably, these latent encodings do not include explicit positional information, so the planner must infer underlying geometry and navigate the maze indirectly. We design an \emph{inverse dynamics model} (a small network of linear layers) to convert consecutive latent observations into actions: $\hat{a}_t = \mathrm{InvDyna}\bigl((x_{t-1},\, x_{t}),\, x_{t+1}\bigr)$. This replaces the heuristic controller of the original Diffuser, which cannot be directly applied here due to partial observability.

\subsubsection{Guidance}
To guide the diffusion process, we introduce a \emph{position estimator} trained on ground‐truth coordinates. Although this estimator provides an approximate distance to the goal, the image encoder is not updated from this learning signal and it does \emph{not} supply the planner with the full underlying state. Therefore, the image encoder still sees only the pixel‐level data without direct positional labels, thereby preserving partial observability in the core planning procedure. This design choice highlights an open‐ended question of how best to incorporate task‐relevant signals in visual domains. The same guidance function with maze task, Appendix~\ref{appx:maze_guidance} is applied.

%For this work, we rely on a simple position‐based heuristic that does not update or fine‐tune the VAE encoder. The same guidance function with maze task, Appendix~\ref{appx:maze_guidance} is applied.

\subsubsection{Guidance Set}

For this task, we applied $[0, 0.1, 0.5, 1, 2]$ guidance set for MCTD and MCTD-Replanning.

\subsubsection{Dataset Generation and Planning Horizon}
We follow the OGBench~\citep{4_park2024ogbench} dataset generation script for PointMaze, with trajectory horizons of 1000 for Medium and Large mazes. To enhance training, we use reduced planning horizons (500 for Medium/Large) and apply a sliding-window technique, as in our earlier pointmaze setup.

\subsubsection{Variational Autoencoder}
We pretrained a VAE~\citep{vae} to encode $64 \times 64 \times 3$ RGB observations into an $8$-dimensional latent space, compressing high-dimensional inputs into a structured representation for downstream planning. It represents the latent variable $z$ as a Gaussian distribution with mean $\mu$ and standard deviation $\sigma$, using the reparameterization trick:
\begin{equation}
z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I).
\end{equation}

We utilized this sampled latent embedding $z$ as the input representation for downstreaming tasks.

\subsubsection{Inverse Dynamics Model and Position Estimator}  

We trained Inverse Dynamics and Position Estimator models using an offline dataset encoded by the pretrained VAE. Both models are implemented as Multi-Layer Perceptrons (MLPs).

\textbf{Inverse Dynamics.}  
This model predicts the action $\hat{a}_t$ required to transition from the current state $x_t$ to the next state $x_{t+1}$. To incorporate velocity-related information in a \textit{partially observable} environment, the model takes both the previous state $x_{t-1}$ and the current state $x_t$ as inputs. The action is predicted as follows:  

\begin{equation}
    \hat{a}_t = \mathrm{InvDyna}\bigl((x_{t-1},\, x_{t}),\, x_{t+1}\bigr)
\end{equation}

For the initial state, where $x_{t-1}$ is unavailable, we use $x_t$ twice as a substitute.  

\textbf{Position Estimator.}  
The Position Estimator predicts the agent’s current position in the latent space encoded by the VAE, providing a weak positional signal to aid planning while preserving partial observability.

% By incorporating both current state and action information, the Position Estimator improves state prediction in \textit{partially observable} environments, enhancing the model’s ability to capture dynamic transitions effectively.  


\subsubsection{Replanning is Necessary to Solve Complex Environments}

In this subsection, we investigate the failure modes of diffusion-based planners, particularly Diffusion Forcing without replanning, in solving the visual pointmaze task. Figure~\ref{fig:planning_fails} demonstrates how the absence of iterative corrections leads to trajectory collapse, revealing critical challenges in pixel-level planning. These findings indirectly highlight the necessity of replanning for robust trajectory generation in complex and partially observable environments.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/planning_fails.pdf}
    \caption{Trajectory collapse in Diffusion Forcing without replanning in the visual pointmaze. The left panel shows the estimated position of the planned trajectory (yellow points) halting at a certain area. The right panels (top left to bottom right) visualize the planning progression over time, suddenly teleporting to an unknown area This underscores the importance of replanning in overcoming single-pass planning limitations.}
    \label{fig:planning_fails}
\end{figure}

\section{MCTD Architecture Implementation}
% \subsection{MCTD Architecture Implementation}

The Monte Carlo Tree Diffusion (MCTD) framework relies on three key elements within its tree‐search loop: sub‐plan noise scheduling, causal semi‐autoregressive denoising, and partial, jumpy denoising. First, rather than assigning a single noise schedule to the entire trajectory, each sub‐trajectory segment receives its own noise‐level schedule. This design allows different segments to transition from high to low noise at distinct rates, thereby enabling focused refinement of earlier parts of the plan while preserving flexibility in later segments.

Next, we adopt a \emph{causal noise schedule}~\citep{diffusion_forcing}, which renders the denoising process semi‐autoregressive: earlier plan segments undergo more thorough denoising, while subsequent ones shift more gradually from higher to lower noise. This preserves the globally consistent trajectory generation characteristic of diffusion models but also ensures that critical decisions near the beginning of the plan receive higher‐fidelity refinements. Finally, both planning and simulation employ partial and jumpy denoising via DDIM sampling~\citep{ddim}, allowing the system to skip directly from noise level $t$ to $t-\Delta$. This skip‐level update accelerates the evaluation of candidate trajectories during tree expansion and supports partial denoising steps essential for exploration.

We implement these strategies with a Transformer‐based model that processes an \emph{expandable} token sequence, thus avoiding any architectural constraint on the planning horizon. Each sub‐plan is annotated with a distinct noise‐level index, and the Transformer predicts the fully denoised data $x_{0}$ from the corresponding noisy tokens. Concretely, we employ an $x_{0}$‐parameterization by training the network to minimize the squared error between the predicted $\hat{x}_{0}$ and the ground‐truth $x_{0}$:
\begin{equation}
\mathcal{L}_{\text{train}} = \mathbb{E}_{x_{0},t,\epsilon}
[\lVert \hat{x}_{0} - x_{0} \rVert^{2}].
\end{equation}
Because each sub‐plan has its own noise schedule, the Transformer naturally accommodates varying horizon lengths and partial denoising intervals without requiring further modifications to the model architecture.

By integrating causal scheduling with skip‐level DDIM updates, MCTD achieves a semi‐autoregressive, partial denoising process within a tree‐search loop. Early plan segments are refined immediately, while future segments remain at higher noise until exploration or heuristic signals indicate their importance. This arrangement allows for fast approximate rollouts by skipping multiple noise levels when simulating a plan. Consequently, MCTD integrates the holistic generative capacity of diffusion models with the adaptive, branch‐and‐bound nature of Monte Carlo Tree Search, thereby enabling iterative trajectory refinement in tasks with long horizons and sparse rewards.

\section{Detailed Comparison between Monte Carlo Guidance (MCG) and Monte Carlo Tree Diffusion (MCTD)}\label{appx:compare_mcg}

While \textbf{MCG (Monte Carlo Guidance)} utilizes Monte Carlo sampling to adjust guidance levels during denoising, it lacks the structured search capabilities of \textbf{MCTD (Monte Carlo Tree Diffusion)} and does not explicitly refine trajectories over time. MCG focuses on adjusting guidance strengths rather than performing an explicit \textbf{trajectory search}, making it less effective for long-horizon planning where iterative improvement is crucial. Additionally, \textbf{MCG does not implement the four standard MCTS steps (Selection, Expansion, Simulation, and Backpropagation)}, limiting its ability to efficiently allocate test-time compute (TTC) for progressive refinement. Without a structured search process, \textbf{MCG cannot revisit and optimize subplans}, making it less scalable for complex decision-making tasks. Furthermore, MCG lacks \textbf{meta-actions for adaptive exploration-exploitation}, passively adjusting guidance instead of dynamically balancing search strategies. Finally, it does not incorporate \textbf{fast jumpy denoising} for simulation, making trajectory evaluation less efficient compared to MCTD, which efficiently estimates plan quality without costly forward model rollouts.

\section{Algorithms}

\begin{algorithm}
\caption{Monte Carlo Tree Diffusion (MCTD)}
\label{alg:MCTD_2}
\begin{algorithmic}[1]
\Procedure{MCTD}{$root, iterations$}
    \For{$i = 1$ to $iterations$}
        \State $node \gets$ \Call{SelectPromisingNode}{$root$}
        \If{\Call{IsExpandable}{$node$}}
            \State $node \gets$ \Call{ExpandNode}{$node$}
        \EndIf
        \State $reward \gets$ \Call{Simulate}{$node$}
        \State \Call{Backpropagate}{$node, reward$}
    \EndFor
    \State \Return \Call{BestChild}{$root$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Selection in MCTD}
\label{alg:Selection}
\begin{algorithmic}[1]
\Procedure{SelectPromisingNode}{$node$}
    \While{\Call{IsFullyExpanded}{$node$} \textbf{and not} \Call{IsLeaf}{$node$}}
        \State $node \gets$ \Call{BestUCTChild}{$node$} \Comment{Use UCB for exploration-exploitation}
    \EndWhile
    \State \Return $node$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Expansion in MCTD}
\label{alg:Expansion}
\begin{algorithmic}[1]
\Procedure{ExpandNode}{$node$}
    \State $g_s \gets$ \Call{SelectMetaAction}{$node$} \Comment{Determine guidance level}
    \State $child \gets$ \Call{DenoiseSubplan}{$node, g_s$} \Comment{Generate new subplan using diffusion}
    \State \Call{AddChild}{$node, child$}
    \State \Return $child$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Simulation in MCTD (Jumpy Denoising)}
\label{alg:Simulation}
\begin{algorithmic}[1]
\Procedure{Simulate}{$node$}
    % \State $partialPlan \gets$ \Call{GetPartialTrajectory}{$node$}
    \State $fullPlan \gets$ \Call{FastJumpyDenoising}{$node$} 
    % \Comment{X-shot decoding}
    \State \Return \Call{EvaluatePlan}{$fullPlan$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Backpropagation in MCTD}
\label{alg:Backpropagation}
\begin{algorithmic}[1]
\Procedure{Backpropagate}{$node, reward$}
    \While{$node \neq null$}
        \State $node.visitCount \gets node.visitCount + 1$
        \State $node.value \gets node.value + reward$
        \State \Call{UpdateMetaActionSchedule}{$node, reward$} \Comment{Adjust guidance levels}
        \State $node \gets node.parent$
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Meta-Action and Guidance Selection}
\label{alg:MetaAction}
\begin{algorithmic}[1]
\Procedure{SelectMetaAction}{$node$}
    \State \Return \Call{UCBSelection}{$\{\textsc{no}, \textsc{low}, \textsc{medium}, \textsc{high}\}$}
\EndProcedure

\Procedure{DenoiseSubplan}{$node, g_s$}
    \If{$g_s = \textsc{no}$}
        \State \Return \Call{SamplePrior}{$p(\bx_s|\bx_{1:s-1})$} \Comment{Exploration}
    \Else
        \State \Return \Call{SampleGuided}{$p_g(\bx_s|\bx_{1:s-1})$} \Comment{Exploitation}
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Jumpy Denoising for Fast Simulation}
\label{alg:JumpyDenoising}
\begin{algorithmic}[1]
\Procedure{FastJumpyDenoising}{$partialPlan$}
    \State \Return \Call{Sample}{$p(\bx_{s+1:S}|\bx_{1:s})$} \Comment{X-shot fast decoding}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
%\scriptsize
\caption{Partial and Jumpy Denoising with DDIM}
\label{alg:partial-denoising}
\begin{algorithmic}[1]
\Require 
  Initial plan $\mathbf{x}_{t_0}$ at noise level $t_0$,
  noise schedule $\mathcal{N}' = [t_0, t_1, \ldots, t_K]$,
  diffusion model $f_\theta(\cdot)$,
  guidance scale $g$
\Ensure 
  Partially denoised plan $\mathbf{x}_{\,t_K}$
\Statex

\For{$k = 0,\; 1,\; \dots,\; K-1$}
    \Statex \quad \textit{// Predict noise or score at level $t_k$}
    \State $\boldsymbol{\epsilon} \gets f_\theta\bigl(\mathbf{x}_{t_k},\; t_k\bigr)$ 
    \Comment{DDIM $\epsilon$-prediction or score function}
    \Statex \quad \textit{// Apply guide using gradients of return}
    \State $\boldsymbol{\epsilon}^{(\text{guided})} \sim \mathcal{N}\left(\boldsymbol{\epsilon} + \alpha \Sigma \nabla \mathcal{J}, \Sigma^t\right)$
    \Statex \quad \textit{// DDIM update from $t_k$ to $t_{k+1}$}
    \State $\displaystyle \mathbf{x}_{t_{k+1}} \;\gets\;
       \sqrt{\tfrac{\alpha_{t_{k+1}}}{\alpha_{t_k}}}
         \Bigl(\mathbf{x}_{t_k} - \sqrt{1 - \alpha_{t_k}}\;\boldsymbol{\epsilon}^{(\text{guided})}\Bigr)
         \;+\;\sqrt{\,1 - \alpha_{t_{k+1}}}\;\boldsymbol{\epsilon}^{(\text{guided})}$
\EndFor

\State \Return $\mathbf{x}_{t_K}$ 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
%\scriptsize
\caption{Additional Controller/Policy/Inverse Dynamics Model Integration}
\label{alg:additional-performer}
\begin{algorithmic}[1]
\Require 
  Long-term planner, $P_\theta$, environment $E$ and controller $\pi_\gamma$ (heuristic controller or value learning policy or inverse dynamics model), planning horizon $H$
\Statex

\State $e \sim E$ with initial observation and goal $s, g$
\While{\textbf{not} \textit{done}}
    \Statex \quad \textit{// Get a plan from planner $P_\theta$}
    \State $p_{1:H} = P_\theta(s, g)$
    \State $i = k$ \Comment{k steps after plan state is the subgoal for Controller}
    \State $g'=p_k$
    \While {$i \leq H$}
        \Statex  \quad\quad \textit{// Compute low-level action with Controller}
        \State $a \leftarrow \pi_\gamma (s, g')$ 
        \Statex \quad\quad \textit{// Interact with environment}
        \State (o, r, done) $\leftarrow$ e.step(a)
        \State s $\leftarrow$ o
        \If{$s \approx g'$}
            \State $i = i + k$
            \State $g'=p_k$
        \EndIf
        \If{done}
            \State break
        \EndIf
    \EndWhile
\EndWhile

\end{algorithmic}
\end{algorithm}



% \newpage
% \section{Model Architectures}


% \subsection{}

% \subsection{Pretrained Encoders for Visual Planning}

% \label{appendix:vae_architecture}
% To efficiently handle high-dimensional space generation, we pretrained variational autoencoders (VAEs)~\citep{vae} as per previous works~\citep{latent_diffusion_model2022ldm, esser2021taming_transformer}. 




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


