\section{Related Work}
Our work is closely aligned with the domain of 3D generative models, specifically focusing on the intricate task of 3D shape detailization. Additionally, we delve into the realm of autoregressive models, as our method harnesses their capabilities to achieve high-fidelity mesh detailization.

\noindent\textbf{3D generative models.}
Capitalizing on the capabilities of variational autoencoders (VAEs)~\cite{vae,vqvae}, generative adversarial networks (GANs)~\cite{gan}, autoregressive models~\cite{pixelcnn,pixelcnnplus}, and diffusion probabilistic models~\cite{diffusion,DBLP:conf/icml/Sohl-DicksteinW15,DBLP:conf/iclr/0011SKKEP21}, a plethora of 3D generative models have been proposed for the generation of 3D shapes. These sophisticated deep generative models utilize voxels~\cite{DBLP:conf/nips/0001ZXFT16,DBLP:conf/eccv/ChoyXGCS16,xcube,DBLP:conf/3dim/HaneTM17}, point clouds~\cite{DBLP:conf/icml/AchlioptasDMG18,DBLP:conf/nips/zengVWGLFK22,craftsman,DBLP:conf/cvpr/FanSG17,DBLP:journals/tog/YinCHCZ19}, neural radiance fields~\cite{DreamFusion,magic3d,nerf,dreamgaussian,LGM,LRM}, or neural implicit representations~\cite{DBLP:conf/cvpr/GroueixFKRA18,pixel2mesh,DBLP:conf/iclr/ZhangCLGZ0F21,diffsdf,DBLP:conf/siggrapha/HuiLHF22,get3d,fantasia3d,neusdfusion,DBLP:conf/cvpr/ChenZ19,occnetwork,deepsdf,3dshape2vecset} to model 3D shapes. Among these methodologies, several have been developed to facilitate controllable 3D shape generation for modeling applications. For instance, Point-E~\cite{pointe}, Shap-E~\cite{shape}, and One-2-3-45~\cite{one2345} are capable of generating a 3D model from text or single-image inputs. Conversely, DECOR-GAN~\cite{decorgan}, ShaDDR~\cite{shaddr}, and DECOLLAGE~\cite{DECOLLAGE} have been designed to synthesize intricate 3D shapes from coarse voxel input. Despite the fact that DECOLLAGE~\cite{DECOLLAGE} facilitates interactive style control during generation, it is constrained by its ability to generate only a limited range of detailization styles and its inability to handle out-of-distribution inputs. In stark contrast, our proposed method harnesses the potential of large data and autoregressive models, thereby enabling the generation of a diverse array of detailization styles with superior details.

\noindent\textbf{3D shape detailization.}
Beyond the generation of 3D shapes from scratch, recent advancements have proposed methodologies for coarse-to-fine shape detailization, synthesizing geometric details in the process. Neural subdivision techniques~\cite{DBLP:journals/tog/LiuKCAJ20, 3DStyleNet} are designed to learn local geometric features from a reference 3D mesh, subsequently transferring these features to a novel shape. Mesh differentiable rendering methods~\cite{DBLP:journals/tog/LiuTJ18, Text2Mesh}, on the other hand, generate geometric details conditional on a reference image or text. However, these methods are limited by their inability to modify the coarse mesh topology, thereby constraining the range of synthesized detailization. To mitigate this limitation, mesh quilting~\cite{DBLP:journals/tog/ZhouHWTDGS06} adopts a strategy of copying and deforming local patches from a given geometric texture patch to detailize the mesh surface. DECOR-GAN~\cite{decorgan} utilizes the concept of image-to-image translation to generate detailed shapes from coarse voxels, given a conditioned geometric style. ShaDDR~\cite{shaddr} further enhances the quality of geometry by incorporating a 2-level hierarchical GAN. DECOLLAGE~\cite{DECOLLAGE} extends this by employing local style control for improved detailization quality. While these aforementioned methods support the generation of arbitrary mesh topology, they are confined to learned topology and thus, cannot generalize to diverse shapes. In contrast, MARS capitalizes on autoregressive models for next-LOD mesh token prediction, thereby enabling the detailization of a diverse array of shapes.

\noindent\textbf{Autoregressive models.}
Within the realm of image generation, early autoregressive models~\cite{pixelcnn,pixelcnnplus} were proposed to generate images as sequences of pixels. VQVAE~\cite{vqvae} and VQGAN~\cite{vqgan} introduced a strategy to quantize images into discrete tokens and employed a transformer to learn the autoregressive priors. Subsequent methods~\cite{maskgit,DBLP:journals/corr/abs-2406-07550} further enhanced the efficiency of tokenization. RQVAE~\cite{rqvae} incorporated multi-scale quantization to improve the quality of reconstruction, while VAR~\cite{var} proposed next-scale prediction for superior reconstruction and accelerated sampling speed. Concurrently, certain methods~\cite{DBLP:conf/icml/RameshPGGVRCS21,DBLP:journals/corr/abs-2406-06525} have endeavored to scale up autoregressive models in the task of text-conditioned image generation. However, the autoregressive approach has not been extensively explored in the field of 3D content generation. Our proposed method bridges this gap between autoregressive models and 3D generative models, achieving state-of-the-art performance in the benchmark for 3D shape detailization.