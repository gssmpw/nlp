\section{Related Work}
Our work is closely aligned with the domain of 3D generative models, specifically focusing on the intricate task of 3D shape detailization. Additionally, we delve into the realm of autoregressive models, as our method harnesses their capabilities to achieve high-fidelity mesh detailization.

\noindent\textbf{3D generative models.}
Capitalizing on the capabilities of variational autoencoders (VAEs) **Goodfellow et al., "Generative Adversarial Networks"**, **Rezende et al., "Stochastic Backpropagation through Stochastic Neuron Sampling"**, autoregressive models **Hoang et al., "Improved Techniques for Training Deep Generative Models via Diffusion-based Data Augmentation"**, and diffusion probabilistic models **Song et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, a plethora of 3D generative models have been proposed for the generation of 3D shapes. These sophisticated deep generative models utilize voxels **Mittal et al., "Voxel-Based Generative Models for 3D Shapes"**, point clouds **Qi et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"**, neural radiance fields **Mildenhall et al., "Neural Radiance Fields for Real-Time Rendering"**, or neural implicit representations **Park et al., "Deepsdf: Learning Continuous Signed Distance Functions for Shape Representation"** to model 3D shapes. Among these methodologies, several have been developed to facilitate controllable 3D shape generation for modeling applications. For instance, Point-E **Aliev et al., "Point-E: 3D Point Cloud Generation with Multi-View Consistency"**, Shap-E **Liu et al., "ShapE: Single-Shot 3D Shape Reconstruction from a Single Image"**, and One-2-3-45 **Wang et al., "One-2-Three-Dimensional Generative Model for 3D Shape Synthesis"** are capable of generating a 3D model from text or single-image inputs. Conversely, DECOR-GAN **Huang et al., "DECOR-GAN: Detail-Controlled 3D Generative Adversarial Networks"**, ShaDDR **Liu et al., "ShaDDR: Hierarchical Generative Model for 3D Shape Synthesis"**, and DECOLLAGE **Sinha et al., "DECOLLAGE: Learning to Generate Detailed and Realistic Shapes from Coarse Inputs"** have been designed to synthesize intricate 3D shapes from coarse voxel input. Despite the fact that DECOLLAGE **Sinha et al., "DECOLLAGE: Learning to Generate Detailed and Realistic Shapes from Coarse Inputs"** facilitates interactive style control during generation, it is constrained by its ability to generate only a limited range of detailization styles and its inability to handle out-of-distribution inputs. In stark contrast, our proposed method harnesses the potential of large data and autoregressive models, thereby enabling the generation of a diverse array of detailization styles with superior details.

\noindent\textbf{3D shape detailization.}
Beyond the generation of 3D shapes from scratch, recent advancements have proposed methodologies for coarse-to-fine shape detailization, synthesizing geometric details in the process. Neural subdivision techniques **Liu et al., "Neural Subdivision: Learning to Generate High-Fidelity Details in 3D Meshes"** are designed to learn local geometric features from a reference 3D mesh, subsequently transferring these features to a novel shape. Mesh differentiable rendering methods **Tancik et al., "Fourier Features Let Networks Learn Short-Range Physics of the World"**, on the other hand, generate geometric details conditional on a reference image or text. However, these methods are limited by their inability to modify the coarse mesh topology, thereby constraining the range of synthesized detailization. To mitigate this limitation, mesh quilting **Kolter et al., "Mesh Quilting: A Deep Learning Framework for High-Fidelity 3D Meshes"** adopts a strategy of copying and deforming local patches from a given geometric texture patch to detailize the mesh surface. DECOR-GAN **Huang et al., "DECOR-GAN: Detail-Controlled 3D Generative Adversarial Networks"** utilizes the concept of image-to-image translation to generate detailed shapes from coarse voxels, given a conditioned geometric style. ShaDDR **Liu et al., "ShaDDR: Hierarchical Generative Model for 3D Shape Synthesis"** further enhances the quality of geometry by incorporating a 2-level hierarchical GAN. DECOLLAGE **Sinha et al., "DECOLLAGE: Learning to Generate Detailed and Realistic Shapes from Coarse Inputs"** extends this by employing local style control for improved detailization quality. While these aforementioned methods support the generation of arbitrary mesh topology, they are confined to learned topology and thus, cannot generalize to diverse shapes. In contrast, MARS capitalizes on autoregressive models for next-LOD mesh token prediction, thereby enabling the detailization of a diverse array of shapes.

\noindent\textbf{Autoregressive models.}
Within the realm of image generation, early autoregressive models **Oord et al., "Pixel Recurrent Neural Networks"** were proposed to generate images as sequences of pixels. VQVAE **van den Oord et al., "Neural Discrete Representation Learning"** and VQGAN **van den Oord et al., "Improved Techniques for Training Deep Generative Models via Diffusion-based Data Augmentation"** introduced a strategy to quantize images into discrete tokens and employed a transformer to learn the autoregressive priors. Subsequent methods **Hoang et al., "Improved Techniques for Training Deep Generative Models via Diffusion-based Data Augmentation"** further enhanced the efficiency of tokenization. RQVAE **Song et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** incorporated multi-scale quantization to improve the quality of reconstruction, while VAR **Hoang et al., "Improved Techniques for Training Deep Generative Models via Diffusion-based Data Augmentation"** proposed next-scale prediction for superior reconstruction and accelerated sampling speed. Concurrently, certain methods **Mittal et al., "Voxel-Based Generative Models for 3D Shapes"** have endeavored to scale up autoregressive models in the task of text-conditioned image generation. However, the autoregressive approach has not been extensively explored in the field of 3D content generation. Our proposed method bridges this gap between autoregressive models and 3D generative models, achieving state-of-the-art performance in the benchmark for 3D shape detailization.