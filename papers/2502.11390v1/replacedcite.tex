\section{Related Work}
Our work is closely aligned with the domain of 3D generative models, specifically focusing on the intricate task of 3D shape detailization. Additionally, we delve into the realm of autoregressive models, as our method harnesses their capabilities to achieve high-fidelity mesh detailization.

\noindent\textbf{3D generative models.}
Capitalizing on the capabilities of variational autoencoders (VAEs)____, generative adversarial networks (GANs)____, autoregressive models____, and diffusion probabilistic models____, a plethora of 3D generative models have been proposed for the generation of 3D shapes. These sophisticated deep generative models utilize voxels____, point clouds____, neural radiance fields____, or neural implicit representations____ to model 3D shapes. Among these methodologies, several have been developed to facilitate controllable 3D shape generation for modeling applications. For instance, Point-E____, Shap-E____, and One-2-3-45____ are capable of generating a 3D model from text or single-image inputs. Conversely, DECOR-GAN____, ShaDDR____, and DECOLLAGE____ have been designed to synthesize intricate 3D shapes from coarse voxel input. Despite the fact that DECOLLAGE____ facilitates interactive style control during generation, it is constrained by its ability to generate only a limited range of detailization styles and its inability to handle out-of-distribution inputs. In stark contrast, our proposed method harnesses the potential of large data and autoregressive models, thereby enabling the generation of a diverse array of detailization styles with superior details.

\noindent\textbf{3D shape detailization.}
Beyond the generation of 3D shapes from scratch, recent advancements have proposed methodologies for coarse-to-fine shape detailization, synthesizing geometric details in the process. Neural subdivision techniques____ are designed to learn local geometric features from a reference 3D mesh, subsequently transferring these features to a novel shape. Mesh differentiable rendering methods____, on the other hand, generate geometric details conditional on a reference image or text. However, these methods are limited by their inability to modify the coarse mesh topology, thereby constraining the range of synthesized detailization. To mitigate this limitation, mesh quilting____ adopts a strategy of copying and deforming local patches from a given geometric texture patch to detailize the mesh surface. DECOR-GAN____ utilizes the concept of image-to-image translation to generate detailed shapes from coarse voxels, given a conditioned geometric style. ShaDDR____ further enhances the quality of geometry by incorporating a 2-level hierarchical GAN. DECOLLAGE____ extends this by employing local style control for improved detailization quality. While these aforementioned methods support the generation of arbitrary mesh topology, they are confined to learned topology and thus, cannot generalize to diverse shapes. In contrast, MARS capitalizes on autoregressive models for next-LOD mesh token prediction, thereby enabling the detailization of a diverse array of shapes.

\noindent\textbf{Autoregressive models.}
Within the realm of image generation, early autoregressive models____ were proposed to generate images as sequences of pixels. VQVAE____ and VQGAN____ introduced a strategy to quantize images into discrete tokens and employed a transformer to learn the autoregressive priors. Subsequent methods____ further enhanced the efficiency of tokenization. RQVAE____ incorporated multi-scale quantization to improve the quality of reconstruction, while VAR____ proposed next-scale prediction for superior reconstruction and accelerated sampling speed. Concurrently, certain methods____ have endeavored to scale up autoregressive models in the task of text-conditioned image generation. However, the autoregressive approach has not been extensively explored in the field of 3D content generation. Our proposed method bridges this gap between autoregressive models and 3D generative models, achieving state-of-the-art performance in the benchmark for 3D shape detailization.