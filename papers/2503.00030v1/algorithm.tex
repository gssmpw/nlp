% \section{Implementation of Regularized Self-Play}
% \label{sec:framework}

% \textcolor{blue}{This section is problematic atm due to the incorrect theory.}

% \begin{table*}[t!]
%     \centering
%     \begin{tabular}{c|c|c}
%     \toprule
%     External Regularizer \colorbox{cyan!20}{$R(\pi_{\theta})$} & Action Preference Model & Preference Model Regularizer \\
%     \midrule
% Entropy: $ \mathcal{H}(\pi_{\theta}) $                                                               & $\mathbb{P}(y \succ y') -\tau \log {\pi_{\theta}(y)}\ + \tau \log {\pi'(y')}$                     & Entropy               \\
% Backward KL: $ D_{\text{KL}}(\pi_{\theta}||\mu) $                                       & $\mathbb{P}(y \succ y') -\tau \log \frac{\pi_{\theta}(y)}{\mu(y)} + \tau \log \frac{\pi'(y')}{\mu(y')}$  & Backward KL \\
% Forward KL: $ D_{\text{KL}}(\mu||\pi_{\theta})$ & $\mathbb{P}(y \succ y') +\tau \sqrt{\frac{\mu(y)}{\pi_{\theta}(y)}} -\tau \sqrt{\frac{\mu(y')}{\pi'(y')}}$ & Bhattacharyya Distance \\
% Chi-Square: $D_{\chi^2}(\pi_\theta||\mu)$ & $\mathbb{P}(y \succ y') +\tau \sqrt{\frac{\pi_{\theta}(y)}{\mu(y)}} -\tau \sqrt{\frac{\pi'(y')}{\mu(y')}}$ & $f$-Divergence ($f(r)=r^{3/2} $.) \\
% \bottomrule
%     \end{tabular}
%     \caption{Regularized Self-Play applying $G$ and $B$ of SPPO converges to the Nash Equilibrium of the different preference models with different policy update regularizer $R(\pi_{\theta})$.}
%     \label{tab:my_label}
% \end{table*}


% In the following section, we leverage some common divergence as external regularizers, including KL and Chi-Square divergences. External regularization might not lead to the same regularization in the preference model but also to reasonable distance or divergence. This also provides the flexibility for users to adjust reaching different Nash Equilibrium.
% \begin{align}
% \mathbb{E}_{{y\sim \pi_{\theta},  y' \sim \pi_t}}[g(y, \mu, y')]
% \end{align}



% \subsection{Instances with Different External Regularizers}

% We demonstrate our choices of external regularization, including backward KL, forward KL, and Chi-Square divergence. We also derive the corresponding preference model of different external regularizations.

% Since entropy regularization is a special case of backward KL regularization, we investigate the selection of the backward KL divergence as an external regularization directly.
% \begin{corollary}[\textbf{Backward KL}]
% Since $\nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu)=  \mathbb{E}_{\pi_{\theta}}[ \nabla_{\theta}\log {\pi_{\theta}(y)}/{\mu(y)}]^2/2$, when $R=D_{\text{KL}}(\pi_{\theta}||\mu)$, RSP converges to the Nash Equilibrium of preference model:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{\text{KL}}(\pi||\mu) +\tau D_{\text{KL}}(\pi'||\mu).
% \end{align}
% \label{theo:reverse_kl}
% \end{corollary}
% According to Proposition \ref{theorem:rsp}, when $R$ is the backward KL divergence, the function $g$ is $\log {\pi_{\theta}(y)}/{\mu(y)}$. Therefore, RSP with backward KL converges to the same Nash Equilibrium as OMD and Nash-MD, where the preference model regularizer is backward-KL divergence.

% The final term is independent to $\pi_{\theta}$, so it's optional to have it in the preference. As discussed in Nash-MD \citep{munos2023nash}, the regularizer of the preference model is $\mathbb{E}_{y \sim \pi_{\theta}}[\log {\pi_{\theta}(y)}/{\mu(y)}]=D_{\text{KL}}(\pi_{\theta} ||\mu)$. Backward KL external regularization is special since the external regularization leads to the convergence to the Nash Equilibrium with the same regularization.

% Similarly, we rewrite forward KL divergence with its score function estimator as follows, and then derive the regularized Nash Equilibrium based on Proposition \ref{theorem:rsp}.
% \begin{corollary}[\textbf{Forward KL}]
% Since $\nabla_{\theta} D_{\text{KL}}(\mu||\pi)=  \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} {\mu(y)}/{\pi_{\theta}(y)}]$, RSP with $R=D_{\text{KL}}(\mu||\pi)$ converges to the Nash Equilibrium of regularized preference as
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{\text{B}}(\pi||\mu) +\tau D_{\text{B}}(\pi'||\mu),
% \label{eq:forward_kl_rsp}
% \end{align}
% where $D_{\text{B}}(\pi_{\theta}||\mu)=\sum_{y \in \mathcal{Y}} \pi_{\theta}(y) \sqrt{\frac{\mu(u)}{\pi_{\theta}(y)}}$ is the Bhattacharyya Distance \citep{bhattacharyya1946measure}.
% \label{theo:forward_kl}
% \end{corollary}
% % It is interesting to observe the external divergence regularizer can lead to distance regularization in the Nash Equilibrium.



% \begin{corollary}[\textbf{Chi-Square}]
% Since $\nabla_\theta D_{\chi^2}(\pi_\theta||\mu)=\mathbb{E}_{\pi_\theta}\left[{\nabla_\theta \pi_\theta(y)}/{\mu(y)}\right]$,
% so RSP with $R=D_{\chi^2}(\mu||\pi)$ converges to the Nash Equilibrium of regularized preference as
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{f}(\pi||\mu) +\tau D_{f}(\pi'||\mu),
% \end{align}
% where $D_{f}$ is $f$-divergence with $f(r)=r^{3/2}$.
% \label{theo:chisquare}
% \end{corollary}

% Although both forward KL and Chi-Square divergence can be applied in theory, empirically a clipping of the ratio is needed when computing forward KL and Chi-Square divergence due to possible small denominator.

% In general, we derive the score function estimators for the backward KL, forward KL and Chi-Square divergence, and transform them into a quadratic form to satisfy the condition in Proposition \ref{theorem:rsp}. Then we can derive the regularized Nash Equilibrium of Self-Play with different Regularizations converge to.


% \subsection{Advantages over Internal Regularization}

% We formally show that the loss of RSP has less variance than the internal regularization in SP.  
% \begin{proposition}
% $\mathrm{Var}(\mathcal{L}_{\text{SP}}) \geq \mathrm{Var}(\mathcal{L}_{\text{RSP}})$.
% \end{proposition}
% This variance reduction influences the speed and stability of learning, especially when the batch size is small and thus the estimate of loss is biased. In practice, while fine-tuning the language tasks, setting a small batch size is very common due to the limited GPU memory.
