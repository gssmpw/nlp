\section{Preliminaries} 


We denote a prompt as $x$, a response as $y$, and a LLM policy as $\pi(y|x)$, where $\pi(\cdot| x) \in  \Delta_{\mathcal{Y}}$, $\mathcal{X}$ is the set of all prompts and $\mathcal{Y}=\{y^0,y^1,\cdots \}$ is the set of all responses. We denote the probability simplex over the responses given a specific prompt as $\Delta_{\mathcal{Y}}$. We parametrize the LLM policy $\pi$ as $\pi_\theta$. The reference policy is an LLM denoted as $\mu \in  \Delta^{\mathcal{X}}_{\mathcal{Y}}$. For notational brevity, we remove the dependence of policy $\pi$ and loss functions on the prompt $x$ throughout the paper.


%\subsection{Preference Optimization as Solving Two-Player Constant-Sum Games}
\subsection{Game-Theoretic Preference Optimization}

We study the preference optimization problem in an online setting by formulating it as a two-player max-min game, as studied in previous self-play works \citep{wu2024self}. The players are two LLMs whose strategies are LLM policies, denoted as max-player $\pi$ and min-player $\pi'$. The utility of the max-player is the preference:
\begin{align}
u(\pi; \pi') = \mathbb{P}(\pi \succ \pi') \overset{\text{def}}{=} \mathbb{E}_{y\sim \pi, y' \sim \pi'}[\mathbb{P}(y \succ y')],
\label{eq:utility}
\end{align}
where $u: \Delta^{\mathcal{X}}_{\mathcal{Y}} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \rightarrow \mathbb{R}$ is \textit{linear} in $\pi$ and $\pi'$; $\mathbb{P}: \mathcal{X} \times \mathcal{Y} \times \mathcal{Y} \rightarrow [0, 1]$ is a general preference model that quantifies the preference of $y$ over $y'$ given a prompt. We extend the notation $\mathbb{P}(y \succ \pi')=\mathbb{E}_{y' \sim \pi'}[\mathbb{P}(y \succ y')]$ for any response $y$. The objective is finding a \textit{NE} policy $\pi^*$ of the preference model:
\begin{align}
(\pi^*,\pi^*) = \arg \max_{\pi} \min_{\pi'} \mathbb{P}(\pi \succ \pi').
\label{eq:pm}
\end{align} 
Therefore, an NE strategy $\pi^*$ is an LLM that can generate \textit{the most preferred responses in expectation}, thus achieving human alignment based on the preference model.


Existing game-theoretic self-play methods solve this NE following Algorithm \ref{alg:selfplay} \citep{wu2024self,swamy2024minimaximalist,zhang2024iterativenashpolicyoptimization,wang2024magnetic}. Specifically, the policy is first initialized as $\pi_0=\mu$. Then in each iteration $t$, the opponent is set to be the last-iterate policy $\pi_t$ (the reason why it is called self-play), and the responses are sampled from $\pi_t$ (Line 4). The pairwise preferences of the sampled responses are collected using the preference model $\mathbb{P}$ (Line 5). The policy parameters are updated by minimizing a specified loss function $\mathcal{L}(\theta; \mathbb{P})$ based on preferences over responses (Line 6). The loss function $\mathcal{L}(\theta; \mathbb{P})$ is dependent on the inherent online learning method. 
% The main difference between these methods is the choice of loss function $\mathcal{L}(\theta; \mathbb{P})$ applied to the policy update. 



% To unify the notations in different methods, we use game-theoretic terminologies.  In preference optimization, utility $U$ can be either unregularized or regularized preference. The objective is to find the Nash Equilibrium policies such that $\max_\pi \min_{\pi'} u(\pi, \pi')$.

% \subsubsection{Multiplicative Weights Update}
\subsection{Preference Optimization via Multiplicative Weights Update}
An effective self-play method to solve the preference optimization game in \Cref{eq:pm} is Self-Play Policy Optimization (SPPO) \citep{wu2024self}. SPPO derives its loss function from the iterative no-regret learning algorithm, Multiplicative Weights Update (MWU) \citep{freund1997decision}. Specifically in a game setting, denote learning rate as $\eta$, and normalization constant $Z(\pi_t)$. In any iteration $t$, the policy update $\forall y \in \mathcal{Y}$ is
\begin{align}
\pi_{t+1}(y) = \pi_t(y) \cdot \frac{\exp \Big( \eta \mathbb{E}_{y' \sim \pi_t}[u({y}; y')]\Big)}{Z(\pi_t)},
\label{eq:mwu}
\end{align}
where $u(y;y')$ is the utility function defined in Equation \eqref{eq:utility}, with $y$ treated as a pure strategy.

The practical loss function of SPPO for policy update in each iteration $t$ is the square error between LHS and RHS in \Cref{eq:mwu} at a logarithmic scale, defined as \looseness=-1
\begin{align}
\mathcal{L}_{\text{SPPO}}(\theta) 
=  \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} -  \Big(  \eta \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big) \Big]^2.
\label{eq:sppo_mse}
\end{align}
\vspace{-.5cm}


SPPO converges to the NE of the preference optimization game in \Cref{eq:pm}. However, after running multiple iterations, the deviation of the policy $\pi_\theta$ from $\mu$ can be large. Such deviation is particularly problematic when the preference model is only accurate at evaluating responses sampled from the reference policy \citep{munos2023nash}. Furthermore, in aligning LLMs in practice, the preference model is typically a surrogate $\hat{\mathbb{P}}$, such as PairRM \citep{llm-blender-2023}, which may be misspecified at some out-of-distribution responses and inaccurate due to estimation error or limited model expressiveness (e.g., PairRM is only a 0.4B model), causing over-optimization problem. Regularizing the policy optimization to a reference SFT model, which is typically trained on high-quality data \citep{ouyang2022training}, can mitigate the problem. We provide a synthetic example in Appendix \ref{append:reg_game} to demonstrate the problem. \looseness=-1

% since the reference policy $\mu$ in preference optimization is a powerful SFT model trained on high-quality data \citep{rafailov2024direct,llama3modelcard,jiang2023mistral,ouyang2022training}, it can provide a guidance for policy optimization. Although the self-play algorithms designed to find Nash Equilibrium in \Cref{eq:pm} are effective in practical preference optimization, , if only initializing the policy with $\mu$ as in Algorithm \ref{alg:selfplay}. Such deviations are particularly problematic when the preference model is only accurate at evaluating responses sampled from the reference policy \citep{munos2023nash}. Furthermore, in aligning LLMs in practice,  the preference model is typically a surrogate $\hat{\mathbb{P}}$, such as PairRM \citep{llm-blender-2023}, which may be misspecified at some out-of-distribution responses and inaccurate due to estimation error or limited model expressiveness (e.g., PairRM is only a 0.4B model). \looseness=-1

\subsection{Regularized Preference Optimization Game with Reference Policy}
\label{sec:reg_game}

To address the regularization in self-play, we adopt the objective in Nash Learning from Human Feedback \citep{munos2023nash}, and extend the KL divergence regularization to a general regularization function, to penalize the deviation from reference policy. We define a \textit{convex} regularization function $R: \Delta^{\mathcal{X}}_{\mathcal{Y}} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \rightarrow (-\infty, \infty)$, where $R(\pi, \mu)$ measures the distance between $\pi$ and the reference model $\mu$, such as KL divergence $D_{\text{KL}}(\pi\|\mu)$. 
% A regularized preference model is $\mathbb{P}(\pi \succ \pi') - \tau R(\pi, \mu) + \tau R(\pi', \mu)$,
Denote regularization temperature as $\tau$, the objective becomes to optimize a \textit{regularized preference model} by solving the NE $(\pi^*,\pi^*)$ of the \textit{regularized} game, where the utility of max player is still $u(\pi;\pi')=\mathbb{P}(\pi \succ \pi')$: \looseness=-1
\begin{align}
\arg \max_{\pi} \min_{\pi'} \mathbb{P}(\pi \succ \pi') - \tau R(\pi, \mu) + \tau R(\pi', \mu).
\label{eq:rpm}
\end{align}
We provide proof of the existence and uniqueness of this NE in Appendix \ref{append:rpm_exists}. Various methods leverage Mirror Descent (MD) to find a regularized NE in \Cref{eq:rpm} \citep{munos2023nash,calandriello2024human,zhang2024iterativenashpolicyoptimization,wang2024magnetic}, based on its last-iterate convergence.
% Mirror Descent (MD) is also in a self-play manner to find a regularized NE in \Cref{eq:rpm}. 

\setcounter{footnote}{0}  % Restart footnote numbering

However, these MD-based methods are only compatible with the reverse KL divergence regularizer. Nash-MD\footnote{Throughout the paper, regularization specifically refers to the one between $\pi$ and $\mu$, rather than between $\pi$ and $\pi_t$.} addresses the reverse KL regularization of $\pi$ and $\mu$ using a geometric mixture policy $\pi_t^\mu$ defined as $\pi_t^\mu(y) = 
{\pi_t(y)^{1 - \eta \tau} \mu(y)^{\eta \tau}}
/{\sum_{y'} \pi_t(y')^{1 - \eta \tau} \mu(y')^{\eta \tau}}$ \citep{munos2023nash}, which updates policy as:
\begin{align}
% \pi_{t+1}=\arg\min_{\pi}  -\eta  \mathbb{E}_{\pi}[\nabla_\pi u(\pi_t; \textcolor{teal} {\pi^\mu_t})] + D_{\text{KL}}(\pi, \pi_t^\mu).
\pi_{t+1}=\arg\min_{\pi}  -\eta  \langle \pi, \partial_\pi u(\pi_t; {\pi^\mu_t}) \rangle + D_{\text{KL}}(\pi, \pi_t^\mu).
\label{eq:nash-md}
\end{align}
% \todoq{can you define $\pi_t^\mu$}
% The practical loss of Nash-MD is thus defined as the RHS of the $\arg\min$ in \Cref{eq:nash-md}, where $\pi=\pi_\theta$. 
% Other MD methods like Magnetic Policy Optimization \citep{wang2024magnetic} aim at the unregularized Nash Equilibrium. 
% Iterative Nash Policy Optimization \citep{zhang2024iterativenashpolicyoptimization} 


% Nash-MD converges to the Nash Equilibrium of the regularized preference objective in \Cref{eq:rpm}. For a parametrized $\pi_\theta$, the gradient of Nash-MD loss function is a direct result of taking derivative w.r.t. $\theta$ in \Cref{eq:nash-md} with $u$ to be the preference model:  
% \begin{align}
% &\nabla_\theta \mathcal{L}_{\text{Nash-MD}}(\theta) \nonumber \\
% &= \mathbb{E}_{y\sim \pi_{\theta}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ \pi_t^\mu) - \frac{1}{2}  
% - \tau \log \frac{\pi_\theta(y)}{\mu(y)} \Big) \Big].
% \label{eq:nashmd_pg}
% \end{align}

% Magnetic Policy Optimization (MPO) \citep{wang2024magnetic} updates the policy using
% \begin{align}
% \pi^{t+1} =   \arg\max_{\pi}  -\eta \mathbb{E}_{\pi} [\partial_\pi u(\pi_t; \textcolor{teal}{\mu})] + D_{\text{KL}}(\pi \| \pi_t)
%  \nonumber \\ + \tau D_{\text{KL}}(\pi \| \mu).
% \label{eq:mpo_md}
% \end{align}
% The KL divergences in the above \Cref{eq:mpo_md} can be merged to single KL divergence with geometric mixture of $\mu$ and $\pi_t$ as in the final term in \Cref{eq:nash-md}. Thus, the update of MPO is the same as in Nash-MD up to multiplying a constant, only except that the selection utility feedback is changed from $u(\pi_t;\textcolor{teal}{\pi_t^\mu})$ to $u(\pi_t;\textcolor{teal}{\mu})$. 
% % Therefore, although Nash-MD and MPO both are MD-based methods, their update directions, which are highly dependent on the utility feedback $u$, are different.
% Notably, MPO is required to update $\mu$ periodically with $\pi_t$ to find the unregularized Nash Equilibrium (\Cref{eq:pm}). MPO becomes equivalent to Online Mirror Descent (OMD) \citep{munos2023nash} if not updating $\mu$. And in this case, MPO and OMD both converge to regularized Nash Equilibrium (\Cref{eq:rpm}), according to the theory of \citep{wang2024magnetic}. 

% \subsection{Further Analysis on Regularized Preference Model}

% MWU-based methods such as SPPO \citep{wu2024self} lack regularization in the preference model. However, Regularized preference model is necessary in self-play. We demonstrate this with synthetic examples in Figure \ref{fig:rsppo_toy}.


While the LLMs optimized via self-play exhibit significant improvement \citep{wu2024self,wang2024magnetic,zhang2024iterativenashpolicyoptimization}, they all have limited regularization of $\pi$ and $\mu$. They either completely lack explicit regularization or only employ reverse KL divergence, imposing only a narrow form of regularization. The potential benefits of alternative regularization, such as adopting other $f$-divergences than reverse KL, remain unexplored. \looseness=-1

% Regularization is used to prevent over-optimization since in practice, a surrogate $\hat{\mathbb{P}}$ is used due to the preference estimation error. 

% For simplicity, we replace $y$ with $\pi$ in any functions to denote the expectation over $y\sim \pi$ in the rest of the paper, e.g., $\mathbb{P}(y \succ \pi') = \mathbb{E}_{y' \sim \pi'}[\mathbb{P}(y \succ y')]$. Here $g$ is a regularization function such that $\mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)]$ forms a proper divergence or distance for regularization. For instance, when $g(y) = -(\log \pi(y)/\mu(y))$, the preference model regularized by reverse KL divergence, which is the same as the objective of Nash-MD \citep{munos2023nash}. 



% Specifically, the opponent responses are generated by the policy in the latest iteration to achieve a self-play improvement.

% \begin{algorithm}[t]
% \caption{Iterative Self-Play Alignment}
% \label{alg:selfplay}
% \begin{algorithmic}[1]
%   \STATE \textbf{Input:} LLM policy $\pi_\theta = \pi_0$, initialized with reference policy $\mu$, regularization function $g$.
%   \FOR{$t \in [T]$}
%     \STATE Sample prompts and responses: $x \sim \mathcal{X}$, $y_{1:K} \sim \pi_t$.
%     \STATE Get pair-wise preferences $\hat{\mathbb{P}}(y_i \succ y_j),\ \forall i,j \in [K]$.
%     \STATE Update parametrized policy $\pi_\theta$ based on $\hat{\mathbb{P}}$ and $g$.
%     \STATE $\pi_{t+1} = \pi_\theta$.
%   \ENDFOR
% \end{algorithmic}
% \end{algorithm}







% For example, other than reverse KL, recently Chi-Square divergence has been used for offline policy optimization due to its connection to pessimism \citep{huang2024correcting}, and the theoretical guarantee of alleviating over-optimization. In this paper, we also attempt to use self-play with Chi-Square divergence regularizer in the preference model.




