\section{Experiments}

\begin{table}[t!]
    \centering
    % \begin{tabular}{l|c|c}
    %     \toprule
    %     \textbf{Model} & \textbf{AlpacaEval-2 LCWR} & \textbf{Arena-Hard-Auto-v0.1} \\
    %     \midrule
    %     Mistral-7B-Instruct & 17.1 & 12.6 \\
    %     Llama-3.1-8B-Instruct & 20.9& 21.3 \\
    %     Snorkel (Mistral-7B-PairRM-DPO) & 26.4 & 20.7 \\
    %     Mistral-7B-SPPO Iter3 & 28.5 & 19.2 \\
    %     Mistral-7B-SimPO & 32.1 & 21.0 \\
    %     \midrule
    %      \rowcolor[gray]{.9} Mistral-7B-RSPO (IS-For.+Rev.) Iter3 & \textbf{35.4} & \textbf{22.9} \\
    
    %      \bottomrule
    % \end{tabular}
    \begin{tabular}{c|c|c|c} 
\toprule
\begin{tabular}[c]{@{}c@{}}\textbf{Methods}\\\small{(Base Model: Mistral-7B-Instruct)}\end{tabular}                                                           & \begin{tabular}[c]{@{}c@{}}\textbf{AlpacaEval-2 }\\\textbf{LCWR ($\%$)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Arena-Hard}\\\textbf{Auto-v0.1}\end{tabular} & \textbf{MT-Bench}  \\ 
\midrule
Mistral-7B-Instruct \citep{jiang2023mistral}                                                     & 17.1                                                                          & 12.6                                                                            & 7.51               \\
Snorkel (Iterative DPO) \citep{viethoangtranduong}                                         & 26.4                                                                          & 20.7                                                                            & 7.58               \\
SPPO Iter3 \citep{wu2024self}                                                    & 28.5                                                                          & 19.2                                                                            & 7.59               \\
SimPO \citep{meng2024simpo}                                                         & 32.1                                                                          & 21.0                                                                            & 7.60               \\ 
\midrule
\rowcolor[gray]{.9} RSPO (IS-For.+Rev.) Iter3 & \textbf{35.4}                             & \textbf{22.9}                               &  \textbf{7.75}      \\
\bottomrule
\end{tabular}
\vspace{.3cm}
    \caption{Performance of existing methods, and our strongest model: RSPO with Importance-Sampling-based Forward KL ($\lambda_1=0.1$) + Reverse KL ($\lambda_2=0.5$) divergence as regularization, on AlpacaEval-2 and Arena-Hard-Auto-v0.1.}
    \label{tab:performance}
    \vspace{.3cm}
\end{table}

% \vspace{-.3cm}
In this section, we answer the following important questions of regularization in the self-play alignment of Large Language Models (LLMs) by testing on various popular benchmarks:
\begin{itemize}
    \item \textbf{Q1:} Does regularization improve the performance of self-play alignment of LLMs (Sec. \ref{sec:main_exp})?  
    \item \textbf{Q2:} Which regularization method is the most effective in self-play alignment (Sec. \ref{sec:main_reg})? 
    \item \textbf{Q3:} What additional advantages can be derived from utilizing regularization in self-play (Sec. \ref{sec:diversity})?
\end{itemize}


\textbf{Experiment Setup.} We investigate our methods mainly on benchmarks AlpacaEval \citep{dubois2024length}, Arena-Hard \citep{li2024crowdsourced}, and MT-Bench \citep{zheng2023judging}. We follow the experiment setup of SPPO and Snorkel-Mistral-PairRM-DPO
% \footnote{\href{snorkelai/Snorkel-Mistral-PairRM-DPO}{snorkelai/Snorkel-Mistral-PairRM-DPO}} 
(Snorkel) \citep{viethoangtranduong} to examine our regularization methods, where Snorkel is based on iterative DPO and has achieved strong performance on AlpacaEval. Our \textit{reference policy} model is Mistral-7B-Instruct-v0.2. 
% \footnote{\href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}} \citep{jiang2023mistral}. 
Since iterative self-play methods require no response data for training, we only use the \textit{prompts of the Ultrafeedback dataset} \citep{cui2023ultrafeedback}, whose size is $\sim 60$K. Following SPPO and Snorkel, we split the prompts into three subsets and use only one subset per iteration to prevent over-fitting. To understand the later-iterate performance of self-play, in section \ref{sec:main_exp}, we also train on the single fold of the prompts iteratively. We use a 0.4B response-pair-wise \textit{preference model} PairRM \citep{llm-blender-2023}, evaluated as comparable to $10\times$ larger reward/preference models \citep{cui2023ultrafeedback}. We investigate the effect of regularization mainly via AlpacaEval-2.0, where the main metric is length-controlled win rate (LCWR). \looseness=-1


\textbf{Implementations and Baselines.}  The implementation of self-play methods follows Algorithm \ref{alg:selfplay}. In each iteration, given response-pair-wise preference from PairRM and $K=5$ number of response samples from the current policy, we estimate the policies' preference $\mathbb{P}(\pi \succ \pi_t)$ and regularization via Monte-Carlo estimation to compute the loss function. We replicate the SPPO with the default hyper-parameters and extend it to $9$ iterations. We implement RSPO as described in \Cref{coro:rspo_converge}. The implementation of regularizations in RSPO is demonstrated in Appendix \ref{sec:implement_reg} using the $K$ samples. We report some of the baseline results from the previous papers, including SPPO, Snorkel (Mistral-PairRM-DPO) \citep{viethoangtranduong}, Mistral-7B (Instruct-v0.2) \citep{jiang2023mistral}, iterative DPO by \citet{wu2024self}, and SimPO \cite{meng2024simpo}. Since the SPPO paper only provides results across $3$ iterations \citep{wu2024self}, we replicate SPPO as an important baseline to study the performance across more than $3$ iterations. 



\subsection{Effectiveness of Regularization}
\label{sec:main_exp}

\begin{figure} 

    \centering 
    \vspace{-.5cm}
    \includegraphics[width=.49\linewidth]{figures/sppo_overopt.pdf}
    \includegraphics[width=.49\linewidth]{figures/RSPO_all.pdf}
    
    \caption{\textbf{Left:} LC win rate across iterations for unregularized self-play method SPPO, SPPO trained on a subset of the data (SPPO (subset)), and reverse-KL-regularized RSPO (RSPO (Rev. KL)). The base model is Mistrial-7B. SPPO starts to degrade after $3$ iterations. \textbf{Right:} LC win rate of SPPO and RSPO with different regularization methods. From left to right regularization methods: Reverse KL ($\lambda=0.5$), Forward KL ($\lambda=1.0$), Chi-Squared ($\lambda=0.1$), Importance-Sampling Forward KL ($\lambda=0.1$), Forward and Reverse KL linear combination ($\lambda_1=0.1$, $\lambda_2=0.5$). RSPO outperforms unregularized self-play (SPPO) significantly.}
    \label{fig:sppo_overopt}
    % \vspace{-.3cm}
\end{figure}

In this section, we assess the effectiveness of regularization primarily by comparing the performance of unregularized and regularized self-play methods. We first examine the over-optimization issue inherent in practical self-play preference optimization by extending the execution of SPPO to Iteration 5. As depicted in Figure \ref{fig:sppo_overopt}, a decline in performance appears during the later iterations of SPPO. We hypothesize that this behavior arises from the practical challenges associated with a misspecified preference model, as the signals driving policy updates in SPPO rely only on the preference model.


In Table \ref{table:iter_exp}, we further contrast the unregularized self-play method, SPPO, and other iterative methods with the best RSPO, namely RSPO (For.+Rev.). The regularization is a linear combination of Forward KL and Reverse KL divergence with coefficients $0.1$ and $0.5$, respectively. The comparative results reveal that regularization enhances the SPPO win rate from $31.02\%$ to $38.31\%$, and the LC win rate increases from $28.53\%$ to $35.44\%$ in iteration $3$. Notably, in the first iteration, reg. SPPO exhibits a slightly lower LC win rate, potentially attributable to the influence of strong regularization. However, subsequent iterations show a marked improvement, with the LC win rate of reg. SPPO increases by up to $7.53\%$ within a single iteration. In summary, the findings in Table \ref{table:iter_exp} underscore the effectiveness of regularization in self-play optimization.

Additionally, to rule out the possibility of insufficient iterations affecting performance, we report the best result among nine iterations of our replicated SPPO in Table \ref{table:iter_exp}, denoted as "SPPO $\leq9$". SPPO $\leq9$ consistently underperforms the RSPO result at iteration $3$. These observations emphasize that even extended training under the unregularized framework fails to match the performance gains achieved through regularization, thereby affirming the critical role of regularization in self-play methodologies for preference optimization.



\begin{table}[t!]
\centering
\resizebox{.48\textwidth}{!}{\begin{tabular}{cccc} 
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{AlpacaEval 2.0}          \\
                       & LC Win Rate    & Win Rate       & Avg. Len  \\ 
\midrule
Mistral-7B             & 17.11          & 14.72          & 1676      \\
Snorkel                & 26.39          & 30.22          & 2736      \\ 
SimPO                &  32.1           & 34.8          & 2193      \\ 
\midrule
DPO Iter1              & 23.81          & 20.44          & 1723      \\
DPO Iter2              & 24.23          & 24.46          & 2028      \\
DPO Iter3              & 22.30          & 23.39          & 2189      \\ 
% \midrule
% IPO Iter1              & 23.78          & 20.77          & 1693      \\
% IPO Iter2              & 21.08          & 23.38          & 2660      \\
% IPO Iter3              & 20.06          & 22.47          & 2760      \\ 
\midrule
SPPO Iter1             & 24.79          & 23.51          & 1855      \\
SPPO Iter2             & 26.89          & 27.62          & 2019      \\
SPPO Iter3             & 28.53          & 31.02          & 2163      \\ 
\rowcolor[gray]{.9} SPPO $\leq$ 9  & 29.17 &	29.75 & 2051 \\
\midrule \midrule
\rowcolor[gray]{.9} RSPO Iter1        & 23.16          & 21.06          & 1763      \\
\rowcolor[gray]{.9} RSPO Iter2        & 27.91          & 27.38          & 1992      \\
\rowcolor[gray]{.9} RSPO Iter3        & \textbf{35.44} & \textbf{38.31} & 2286      \\
\bottomrule
\end{tabular}}
\hspace{.38cm}
\resizebox{.48\textwidth}{!}{\begin{tabular}{cccc} 
\toprule
\multirow{2}{*}{Regularization}                                                   & \multirow{2}{*}{Iteration} & \multicolumn{2}{c}{AlpacaEval 2.0}        \\
&                            & LCWR $\uparrow$ & Self-BLEU $\downarrow$  \\ 
\midrule
\multirow{3}{*}{$\times$}                                                               & 1                          & 24.79           & 0.751                   \\
& 2                          & 26.89           & 0.754                   \\
& 3                          & 28.53           & 0.758                   \\ 
\midrule \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IS-Forward KL\\+ Reverse KL\end{tabular}} & 1                          & 23.16           & 0.747                   \\
& 2                          & 27.91           & 0.743                   \\
& 3                          & \cellcolor{gray!30} \textbf{35.44}  & \cellcolor{gray!30} 0.714                   \\ 
\midrule
\multirow{3}{*}{Reverse KL~}                                                      & 1                          & 25.52           & 0.747                   \\
& 2                          & 32.26           & 0.730                   \\
& 3                          & \cellcolor{gray!30} 34.21           & \cellcolor{gray!30} \textbf{0.691}          \\ 
\midrule
\multirow{3}{*}{IS-Forward KL}                                                       & 1                          & 24.88           & 0.756                   \\
& 2                          & 27.9            & 0.759                   \\
& 3                          & 30.09           & 0.760                   \\
\midrule
\multirow{3}{*}{$\chi^2$}                                                       & 1                          & 26.7           & 0.745                   \\
& 2                          & 28.78            & 0.740                   \\
& 3                          & 29.97           & 0.739                   \\
\bottomrule
\end{tabular}}
\vspace{.3cm}
\caption{\textbf{Left}: Comparisons of iterative methods with reference models Mistral-7B (Instruct-v0.2). SPPO $\leq$ 9 represents the best results among the $9$ iterations of SPPO. Here, the Regularized SPPO (RSPO) is regularized by the \textit{linear combination of Forward KL and Reverse KL divergence}, i.e. RSPO (For. + Rev.), where the regularization temperatures are $0.1$ and $0.5$, respectively. \textbf{Right:} Response diversity of SPPO with different regularization methods using Self-BLEU score. The regularization temperatures are the same as in Figure \ref{fig:sppo_overopt} (Right). A lower Self-BLEU score means a higher diversity of the sampled responses. 
Regularization methods involving Reverse KL resulted in higher diversity of the responses.}
\label{table:iter_exp}
% \vspace{-.2cm}
\end{table}

\begin{figure}[t!]
    \centering   \includegraphics[width=.49\linewidth]{figures/RSPO_para_length.pdf}
    \includegraphics[width=.49\linewidth]{figures/RSPO_para_wr.pdf}
    \caption{Ablation study of regularization temperature $\lambda$ conducted on AlpacaEval 2.0. We evaluate how the average response length and raw WR are affected by the regularization temperature.}
    \label{fig:rspo_length_para}
    % \vspace{-.2cm}
\end{figure}


% \begin{figure}[t!]
%     \centering
%     % \includegraphics[width=.33\linewidth]{figures/RSPO_all.pdf}
%     % \includegraphics[width=.48\linewidth]{figures/RSPO_all_wr.pdf}
%     \includegraphics[width=.5\linewidth]{figures/RSPO_all.pdf}
%     % \includegraphics[width=.48\linewidth]{figures/RSPO_all_length.pdf}
%     \caption{LC win rate of SPPO and RSPO with different regularization methods. From left to right regularization methods: Reverse KL ($\lambda=0.5$), Forward KL ($\lambda=1.0$), Chi-Squared ($\lambda=0.1$), Importance-Sampling Forward KL ($\lambda=0.1$), Forward and Reverse KL linear combination ($\lambda_1=0.1$, $\lambda_2=0.5$).}
%     \vspace{-.3cm}
%     \label{fig:RSPO_all_reg}
%     % \vspace{-2cm}
% \end{figure}


\subsection{Impact of Different Regularizations}
\label{sec:main_reg}
We then study the effect of applying different regularization $R$ in RSPO. To obtain a well-regularized self-play, the tuning of regularization temperature $\lambda$ is necessary. An ablation study of the regularization temperature of different methods is shown in Figure \ref{fig:rspo_length_para}. According to the figure, the response length increases along with the temperature in reverse KL divergence and Chi-square divergence regularized RSPO. Meanwhile, the length is decreased with stronger regularization via Forward KL divergence, which was implemented using importance sampling. 
This result underscores the distinct effects of different regularization strategies. In particular, the raw win rate analysis highlights reverse KL divergence as a crucial factor in enhancing self-play performance. Given that forward KL divergence tends to reduce response length while reverse KL divergence yields significant improvements, we adopt a linear combination of both. This approach is designed to balance their complementary effects, ultimately optimizing for a higher LCWR (RSPO (IS-For. + Rev.) in Figure \ref{fig:sppo_overopt} RHS).\looseness=-1



In Figure \ref{fig:sppo_overopt} (Right), we show the results of win rate and LCWR in AlpacaEval 2.0 of different regularizations. Only vanilla Forward KL decreases the win rate of SPPO. The regularizations that consists of Reverse KL including RSPO (Rev. KL) and RSPO (For.+Rev.) have shown significant improvement in win rates compared to vanilla SPPO. In particular, the results of RSPO (For.+Rev.) demonstrate the largest improvement between iterations, achieving the best LCWR. \looseness=-1
% \vspace{-.4cm}



We study the effect of applying different regularization $R$ in RSPO. In Figure \ref{fig:sppo_overopt} (Right), we show the results of win rate and average response length on AlpacaEval 2.0. Among different regularizations, only Forward KL decreases the win rate of SPPO. The regularizations that consist of Reverse KL including RSPO (Rev. KL) and RSPO (For.+Rev.) have shown significant improvement in win rates compared to vanilla SPPO. In particular, the results of RSPO (For.+Rev.) demonstrate the largest improvement between iterations. We test the best RSPO model on different benchmarks in Table \ref{tab:performance}, and observe that the proposed method RSPO with proper external regularization achieves superior performance than unregularized self-play and iterative methods like SPPO and iterative DPO, as well as the strong offline alignment method SimPO. \footnote{We report our replicated testing of SPPO Iter3 (\hyperlink{https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3}{https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3}) on Arena-Hard. So, it is different from the result presented in the original paper of SPPO \citep{wu2024self}.} \looseness=-1 
% As for average length, in general, the models with higher win rate have larger response length except RSPO (For. KL) which increases the average length but decreases the win rate.

% We have also done ablation study on regularization temperature.



\subsection{Response Diversity}
\label{sec:diversity}

% \begin{table}[t!] \label{tab: Self-BLEU}
% \centering
% \begin{tabular}{cccc} 
% \toprule
% \multirow{2}{*}{Regularization}                                                   & \multirow{2}{*}{Iteration} & \multicolumn{2}{c}{AlpacaEval 2.0}        \\
% &                            & LCWR $\uparrow$ & Self-BLEU $\downarrow$  \\ 
% \midrule
% \multirow{3}{*}{$\times$}                                                               & 1                          & 24.79           & 0.751                   \\
% & 2                          & 26.89           & 0.754                   \\
% & 3                          & 28.53           & 0.758                   \\ 
% \midrule \midrule
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IS-Forward KL\\+ Reverse KL\end{tabular}} & 1                          & 23.16           & 0.747                   \\
% & 2                          & 27.91           & 0.743                   \\
% & 3                          & \textbf{35.44}  & 0.714                   \\ 
% \midrule
% \multirow{3}{*}{Reverse KL~}                                                      & 1                          & 25.52           & 0.747                   \\
% & 2                          & 32.26           & 0.730                   \\
% & 3                          & 34.21           & \textbf{0.691}          \\ 
% \midrule
% \multirow{3}{*}{IS-Forward KL}                                                       & 1                          & 24.88           & 0.756                   \\
% & 2                          & 27.9            & 0.759                   \\
% & 3                          & 30.09           & 0.760                   \\
% \midrule
% \multirow{3}{*}{$\chi^2$}                                                       & 1                          & 26.7           & 0.745                   \\
% & 2                          & 28.78            & 0.740                   \\
% & 3                          & 29.97           & 0.739                   \\
% \bottomrule
% \end{tabular}
% \caption{Response diversity of SPPO with different regularization methods using Self-BLEU score. All models here used {Mistral-7B-Instruct-v0.2} as the reference model and {Snorkel-Mistral-PairRM-DPO} as the preference model. The first column shows the regularization method used during SPPO training. Lower Self-BLEU score means higher diversity of the sampled responses. While regularization methods involving Reverse KL resulted in higher diversity of the responses, SPPO model regularized with Forward KL also achieves higher LCWR, despite the response diversity not being significantly different from that of non-regularized SPPO models.}
% \vspace{-.2cm}
% \end{table}

% \begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-1cm}
% \begin{minipage}{0.95\linewidth}
% \begin{table}[H] 
% \centering
% \resizebox{\textwidth}{!}{\begin{tabular}{cccc} 
% \toprule
% \multirow{2}{*}{Regularization}                                                   & \multirow{2}{*}{Iteration} & \multicolumn{2}{c}{AlpacaEval 2.0}        \\
% &                            & LCWR $\uparrow$ & Self-BLEU $\downarrow$  \\ 
% \midrule
% \multirow{3}{*}{$\times$}                                                               & 1                          & 24.79           & 0.751                   \\
% & 2                          & 26.89           & 0.754                   \\
% & 3                          & 28.53           & 0.758                   \\ 
% \midrule \midrule
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}IS-Forward KL\\+ Reverse KL\end{tabular}} & 1                          & 23.16           & 0.747                   \\
% & 2                          & 27.91           & 0.743                   \\
% & 3                          & \textbf{35.44}  & 0.714                   \\ 
% \midrule
% \multirow{3}{*}{Reverse KL~}                                                      & 1                          & 25.52           & 0.747                   \\
% & 2                          & 32.26           & 0.730                   \\
% & 3                          & 34.21           & \textbf{0.691}          \\ 
% \midrule
% \multirow{3}{*}{IS-Forward KL}                                                       & 1                          & 24.88           & 0.756                   \\
% & 2                          & 27.9            & 0.759                   \\
% & 3                          & 30.09           & 0.760                   \\
% \midrule
% \multirow{3}{*}{$\chi^2$}                                                       & 1                          & 26.7           & 0.745                   \\
% & 2                          & 28.78            & 0.740                   \\
% & 3                          & 29.97           & 0.739                   \\
% \bottomrule
% \end{tabular}}
% \caption{Response diversity of SPPO with different regularization methods using Self-BLEU score. The regularization temperatures are the same as in Figure \ref{fig:sppo_overopt} (Right). Lower Self-BLEU score means higher diversity of the sampled responses. 
% Regularization methods involving Reverse KL resulted in higher diversity of the responses.
% % , SPPO model regularized with Forward KL also achieves higher LCWR, despite the response diversity not being significantly different from that of non-regularized SPPO models.
% }
% \label{tab: Self-BLEU}
% \vspace{-.4cm}
% \end{table}
% \end{minipage}
% \end{wrapfigure}

We demonstrate an additional advantage introduced by regularization, which is the diversity of the response.
We first provide a motivating example with synthetic data in Appendix \ref{append:2d_diversity}, which shows that the unregularized self-play may converge to a collapsed response when multiple equally good responses exist. On the contrary, RSPO with maximum entropy regularization can recover all high-reward responses.

For LLMs, we investigate the diversity of generations by estimating the variability of the responses. We use the Self-BLEU \citep{zhu2018texygen} score, where a lower score implies higher response diversity. We take the first $200$ tokens of each of the $16$ generated responses using the prompts of AlpacaEval.  \looseness=-1

The trend of Self-BLEU scores presented in \Cref{table:iter_exp} (Right) show that applying RSPO with Reverse KL increases response diversity the most, as well as the LCWRs of AlpacaEval 2.0. 
% \textcolor{blue}{Xiaohang: Not sure about this claim: We attribute this phenomenon to the fact that Reverse KL contains negative entropy term of $\pi_\theta$, so that minimising the loss leads to increasing the entropy of the policy.}  
The application of Forward KL results in slightly less generation diversity than vanilla SPPO, but it still achieves better win rates. The win rates are the highest when Forward KL and Reverse KL are linearly combined for regularization, while the Self-BLEU scores imply that the response diversity is lower than when only Reverse KL is applied. These results highlight that applying regularization in self-play methods can improve test performance and the diversity of the generations simultaneously.


\section{Conclusion}
In this paper, we study the regularization in self-play by proposing a framework, namely Regularized Self-Play Policy Optimization (RSPO). Based on RSPO, we can apply different regularization functions for policy updates by adding the regularization term to the loss functions, which is still guaranteed to converge to the NE of the regularized preference optimization game. In the empirical assessments, we achieve significant improvement over the base model and unregularized self-play method, SPPO. We also empirically demonstrate that regularization promotes response diversity. These findings underscore the critical role of regularization as a fundamental component in optimizing self-play alignment.

% \section*{Impact Statement}
% This paper presents a novel framework for fine-tuning Large Language Models via self-play, with regularization to a Supervised Fine-Tuning (SFT) reference model. Ethical challenges may arise if the SFT reference model tends to generate harmful content or if the preference model for policy updates assigns higher ratings to harmful texts. However, based on previous research, we believe our work does not have any direct negative societal consequences.