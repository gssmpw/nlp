\section{Regularized Self-Play Policy Optimization}
\label{sec:sp}
We propose a simple, general, and theoretically sound framework of self-play alignment, namely \textbf{Regularized Self-Play Policy Optimization (RSPO)}. Intuitively, RSPO can be understood as an extension of SPPO with an external regularization term, allowing different regularization strategies to be employed. The loss function of RSPO $\mathcal{L}_{\text{RSPO}}$ is defined as the sum of a mean-squared self-play loss, denoted as $\mathcal{L}_{\text{SP}}$, and a weighted regularization term:
\begin{align}
\mathcal{L}_{\text{RSPO}}(\theta; G, B, R)\overset{\text{def}}{=} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} -  \eta \Big(   G({y}, \pi_t, \mu) - B(\pi_t, \mu) \Big) \Big]^2  \colorbox{cyan!20}{$+ \lambda 
 R(\pi_{\theta}, \mu)$},
\label{eq:RSPO}
\end{align}
where $G({y}, \pi_t, \mu)$, $B(\pi_t, \mu)$, and $R(\pi_{\theta}, \mu)$ are configurable components.
First, $G:\mathcal{Y} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \rightarrow (-\infty, \infty)$ defines the \textit{update direction} of $\pi_{\theta}$, which can be set as the gradient of a utility function to guide the policy update towards increasing the utility. Second, the \textit{baseline} function $B :\Delta^{\mathcal{X}}_{\mathcal{Y}} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \rightarrow (-\infty, \infty)$ is for variance-reduction for $G$, similar to the baseline in REINFORCE \citep{williams1992simple}. 
Lastly, $R: \Delta^{\mathcal{X}}_{\mathcal{Y}} \times \Delta^{\mathcal{X}}_{\mathcal{Y}} \rightarrow \mathbb{R}$ is the regularization function. The coefficient $\lambda$ is the regularization temperature. 
The first Mean Square Error term in \Cref{eq:RSPO} can be interpreted as a self-play loss of conducting exponentiated gradient descent \citep{beck2003mirror}.
% \todoq{why changing $\tau$ to be $\lambda$ in Eq.(13  ` )?}



% where $\mathcal{L}_{\text{SP}} (\theta; G, B)$ is defined as
% \begin{align}
% . 
% \label{eq:sp}
% \end{align}

RSPO offers a simple way to introduce regularization into self-play alignment with \textit{only an additional term in the loss}. Using RSPO, self-play can be regularized by simply adding a term $R$ to the self-play loss functions. For example, to regularize SPPO (\Cref{eq:sppo_mse}), we can leverage RSPO loss function (\Cref{eq:RSPO}) by setting appropriate $G$ and $B$, and directly adding an additional term of $R$ to the loss function of SPPO. Therefore, RSPO offers the simplicity and flexibility to incorporate various regularization methods into self-play-based preference optimization methods. 

Furthermore, RSPO is a generalized and versatile framework that makes the import of regularization into existing self-play methods more efficient. We show in Section \ref{sec:general} that RSPO can generalize existing self-play methods with $\mathcal{L}_{\text{SP}}$ and without external regularization $R$. Thus, regularizing existing methods requires \textit{no change} to their loss functions or hyperparameters, but simply adding an external regularization to their loss function and tuning the temperature $\lambda$. 

We provide the theoretical guarantee for RSPO in Section \ref{sec:theory} by showing that RSPO is the RL implementation of a specific type of Mirror Descent. Given a specific regularizer $R$, RSPO has a last-iterate convergence to the NE of the corresponding \textit{regularized} game. We also introduce the implementation details of RSPO specifically for preference optimization (\Cref{eq:rpm}) in Section \ref{sec:implementation}.

\subsection{Generalizing Existing Self-Play Methods}
\label{sec:general}

% MD-based methods are powerful in online preference optimization, and demonstrate monotone improvement with iterations.

% Both MWU-based and MD-based of preference optimization methods perform well in practice \citep{wu2024self,zhang2024iterativenashpolicyoptimization}. But the way of conducting policy update in these methods differ. In existing works, understanding the connection between both optimization methods has been lacking, which prevent us from understanding  similarities that make these methods successful or further improvement.  \looseness=-1

We show that existing methods have loss functions equivalent to RSPO \textit{without external regularization}: $\mathcal{L}_{\text{RSPO}}(\theta; G, B, R=0)$. 
% Detailed derivations of some of the following equivalence between existing methods and $\mathcal{L}_{\text{SP}}$ are provided in Appendix \ref{append:nashmd_proof}.
First of all, unregularized self-paly method SPPO \citep{wu2024self}, has loss function in \Cref{eq:sppo_mse} satisfying:
% , where the update direction is $G(y, \pi_t, \mu)=\mathbb{P}(y \succ \pi_t)$, and the baseline is $B(\pi_t, \mu)=\log Z(\pi_t)$. In the practical version of SPPO, for simplification, the baseline is set to $\tfrac{1}{2}$ rather than estimating $\log Z(\pi_t)$. Formally,
\begin{align}
\mathcal{L}_{\text{SPPO}}(\theta) 
= \mathcal{L}_{\text{RSPO}}\bigg(\theta; G=\mathbb{P}(y \succ \pi_t), B=\frac{1}{2},
R=0\bigg).
\label{eq:sppo_sp}
\end{align}
Other unregularized self-play methods following the preference-based exponential update in \Cref{eq:mwu} can also be generalized by $\mathcal{L}_{\text{SP}}$, and thus can be regularized by simply adding regularization term to the loss functions. Based on the same exponential update rule as in SPPO, SPO \citep{swamy2024minimaximalist} is equivalent to updating policy with the loss in \Cref{eq:sppo_sp}. Magnetic Policy Optimization \citep{wang2024magnetic}, despite incorporating regularization in the policy update, periodically update $\mu=\pi_t$. Consequently, it inherently follows update in \Cref{eq:mwu} while incorporating multiple policy updates within each iteration, following the approach of \citep{tomar2020mirror}.

In addition, even existing regularized methods can be generalized by $\mathcal{L}_{\text{RSPO}}$ without external regularization. Mirror Descent methods including Online Mirror Descent and Nash-MD (implemented with RL) have direct connection to RSPO (the detailed derivations are provided in Appendix \ref{append:nashmd_proof}): 
% By setting external regularization in RSPO $R(\pi_\theta,\mu)=D_{\text{KL}}(\pi_\theta\|\mu)$: 
% According to the \Cref{eq:nash-md}, Nash-MD is generalized by RSPO with the same external regularization $R=D_{\text{KL}}$:
\begin{align}
&\nabla_\theta \mathcal{L}_{\text{Nash-MD}}(\theta) 
% = &\nabla_\theta \mathcal{L}_{\text{RSPO}}\big(\theta;G=\mathbb{P}(y\succ  \pi_t^\mu), B=\frac{1}{2}, R=D_{\text{KL}}(\pi_\theta\|\mu) \big) \\
= \nabla_\theta \mathcal{L}_{\text{RSPO}}\bigg(\theta;G=\mathbb{P}(y\succ  \pi_t^\mu) - \tau \log \frac{\pi_t(y)}{\mu(y)}, B=\frac{1}{2},
R=0\bigg).
% &\nabla_\theta \mathcal{L}_{\text{MPO}}(\theta) = \nabla_\theta \mathcal{L}_{\text{RSPO}}\big(\theta; G=\mathbb{P}(y\succ  \mu), \nonumber \\
% & \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \quad B=\mathbb{P}(\pi_\theta \succ  \mu) \big).
\end{align}
% Here in MPO, advantage function $G-B$ is used for update, where $G$ can be understood as $Q$ function in RL and $B$ is its optimal baseline---value function. 
% \vspace{-.5cm}
% We provide the .  
This equivalence fundamentally arises from the well-established connection between the reinforcement learning (RL) policy gradient and the gradient of a quadratic loss function, as explored in various works \citep{haarnoja2018soft,tomar2020mirror,malkin2022gflownets}. We summarize the generalization of RSPO in Table \ref{table:main}. Therefore, our generalized loss framework RSPO enables to even add extra regularization to existing regularized self-play methods. 


% implies that both MD-based and MWU-based methods rely on a similar quadratic form of loss for policy update, differentiated with update direction due to the regularization. 
% This is because MWU update in \Cref{eq:mwu} can also be written as an MD update.\looseness=-1

% According to the \Cref{eq:nashmd_mse}, $\mathcal{L}_{\text{SP}}$ generalizes Nash-MD with $G=\mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi^{\mu}_t(y)}{\mu(y)}$, and baseline $1/2$, since the gradient w.r.t. $\theta$ is also the same as the gradient of Nash-MD loss up to multiplying a constant. 
% Online Mirror Descent \citep{munos2023nash} has the similar update rule and can be treated as instances with the update direction slightly changed ($G=\mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)}$). Detailed derivations of all MD methods are in Appendix \ref{append:nashmd_proof}.




% We call the first Mean Square Error term in \Cref{eq:RSPO} a self-play loss function, denoted by $\mathcal{L}_{\text{SP}}$, which can be interpreted as a loss function facilitating exponentiated gradient descent \citep{beck2003mirror}. 
% Meanwhile, $R$ is also a strong regularization directly acted on the logits of the policy since the first term is quadratic in $\log \pi_\theta$. 


% In RSPO framework, the external regularization term $R$  . The term $\mathcal{L}_{\text{SP}}$ incentivizes the policy update to assign higher probabilities to actions associated with larger values of $G-B$. Due to the inherent connection between RSPO and Mirror Descent, the two components $G$ and $B$ provide the flexibility to instantiate across a diverse set of self-play methods, which will be discussed with details in section \ref{sec:general}. 

% In summary, we provide the following remark:
% \begin{remark}
% The last-iterate convergence guarantee of RSPO ensures that our framework facilitates simple integration of regularization, remains adaptable to diverse regularization techniques, and is theoretically well-founded.
% \end{remark}

% In practice, we set baseline $B=\tfrac{1}{2}$ following Nash-MD and SPPO. In theory, $B$ helps minimize the variance of $G$ the most when $B= \mathbb{E}_{y\sim \pi_t}[G(y, \pi_t, \mu)]$. But in preference optimization, due to the typically small minibatch size, estimation error of the mean of $G$ could be large, leading to additional estimation error of the loss. Thus, we also set the baseline value for variance reduction to be a constant $\tfrac{1}{2}$, the mean value of $G$ when the algorithm is converged. 

% In Section \ref{sec:theory}, we extend an existing no-regret learning method as the theoretical foundation of RSPO. In Section \ref{sec:RSPO}, we introduce our novel RSPO framework. Finally, in Section \ref{sec:general}, we demonstrate the novel connections of RSPO to existing self-play methods.\looseness=-1
%In Section \ref{sec:theory}, we first introduce the theoretical foundation of RSPO. In Section \ref{sec:RSPO}, we introduce the novel RSPO framework. In Section \ref{sec:general}, we demonstrate the connection and comparisons of RSPO to existing self-play methods.\looseness=-1


% \begin{table*}[t!]
%     \centering
%     \begin{tabular}{c|c} 
% \toprule
% Self-Play Method & Loss Function  \\ 
% \midrule
% SPO \citep{swamy2024minimaximalist} & $\mathbb{E}_{y \sim \pi_{t}} \Big[ \log \pi_{\theta} - Q^{\pi_t}(y) + \log Z(\pi_t)  \Big]^2$  \\
% SPPO \citep{wu2024self} & $\mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} -  \Big( \eta \mathbb{P}({y} \succ \pi_t) - \frac{1}{2} \Big) \Big]^2$ \\
% Nash-MD \citep{munos2023nash} & $\mathbb{E}_{y\sim \pi_{t}} \Big[ \log \frac{\pi_\theta(y)}{\pi_t(y)} - \frac{1}{\tau} \big( \mathbb{P}(y \succ \pi^{\mu}_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2}\big) \Big]^2$  \\
% MPO \citep{wang2024magnetic} & $\mathbb{E}_{y\sim \pi_{t}} \Big[ \log \frac{\pi_\theta(y)}{\pi_t(y)} - \frac{1}{\tau} \big( \mathbb{P}(y \succ \mu) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2}\big) \Big]^2$  \\
% \bottomrule
% \end{tabular}
% \caption{Existing game-theoretic self-play methods for preference optimization. $\mathcal{L}_{\text{SP}}$ generalize the non-contrastive ones. $\mathbb{P}^{\mu}(y \succ \pi_t) = \mathbb{P}(y \succ \pi^{\mu}_t)$, $\pi^{\mu}_t$ is the geometric mixture of $\pi_t$ and $\mu$. We abbreviate the estimated baseline that reduce the variance of $G$ the most as est.. $\mathbb{P}_{\tau}(y \succ y') = \mathbb{P}(y \succ y') -\tau \log \frac{\pi_{\theta}(y)}{\mu(y)} + \tau \log \frac{\pi'(y')}{\mu(y')}$ is the regularized preference model.}
% \label{table:main}
% \end{table*}



\subsection{Theoretical Results}
\label{sec:theory}

In this section, we examine the theoretical properties of RSPO, with a particular emphasis on its convergence guarantee. We adopt Mirror Descent (MD) as the foundational framework, given its well-established last-iterate convergence to the NE in game-solving.

We build upon Magnetic Mirror Descent (MMD) \citep{sokota2022unified}, a specialized variant of MD that guarantees convergence to a reverse-KL-regularized NE. To generalize beyond reverse-KL regularization, we introduce Generalized Magnetic Mirror Descent (GMMD), which can accommodate a broader class of regularization techniques. By demonstrating that optimizing the RSPO loss is equivalent to performing reinforcement learning (RL) within the GMMD framework, we establish a formal connection between RSPO and GMMD. This connection ensures the last-iterate convergence of RSPO to the NE of the corresponding \textit{regularized} game.

\textbf{Tabular GMMD.} Denote the utility function of the game as $U$, define $G$ as the element of the gradient vector of $U$: $G(y;\pi') \overset{\text{def}}{=} \partial_{\pi(y)} U(\pi;\pi')$. In other words, $\partial_\pi U(\pi;\pi') = (G(y^0;\pi'),\cdots, G(y^{|\mathcal{Y}|};\pi') )^\top \in \mathbb{R}^{|\mathcal{Y}|}$. Then in iteration $t$, GMMD updates policy as \looseness=-1
\begin{align}
\pi_{t+1} 
= \arg \min_{\pi} -\eta \mathbb{E}_{\pi}[G(y;\pi_t)] + B_{\psi}(\pi; \pi_t) + \tau R(\pi, \mu),
\label{eq:gmmd}
\end{align}
where $\tau$ is regularization temperature, $R$ is a general regularization function, serving as a ``magnet'' to attract $\pi$ to $\mu$ during policy updating. $B_{\psi}$ is the Bregman Divergence generated by a convex potential function $\psi$ \citep{bregman1967relaxation}.

Notably, the vanilla Magnetic Mirror Descent limits $R$ to be the same regularization method of $\pi$ and $\pi_t$, i.e., $R=B_{\psi}$ \citep[Section~3.2]{sokota2022unified}; whereas in this paper we aim at a general regularizer of $\pi$ and $\mu$, which could be different from $B_{\psi}$, and study the effects of different regularizations methods.

% \todoq{in the following proposition, why the strong convexity is with respect to $\psi$?}
\begin{proposition}[\textbf{Last-iterate Convergence}] If $R(\cdot, \mu)$ is $1$-strongly convex relative to $\psi$, $\eta \leq \tau$, and $U$ is linear, then policy updated by GMMD in \Cref{eq:gmmd} has last-iterate convergence to the following regularized NE:
\begin{align}
\max_{\pi} \min_{\pi'} U(\pi; \pi') - \tau R(\pi, \mu) + \tau R(\pi', \mu).
\end{align}
\label{prop:RSPO}
% \vspace{-.3cm}
\end{proposition}
Proposition \ref{prop:RSPO} is a direct application of Theorem 3.4 by \citet{sokota2022unified}, which guarantees the last-iterate convergence of GMMD to the NE of a regularized game. We provide the proof in Appendix \ref{append:rspo_proof}.  
% , which can be easily instantiated as the regularized preference optimization game in \Cref{eq:rpm} with $U(\pi; \pi')=\mathbb{P}(\pi \succ \pi')$. 

% \Cref{eq:gmmd} is an update rule that can generalize various game-theoretic methods. MWU can be generalized by $\tau=0$. 

\textbf{Deep RL Implementation of GMMD.} To adapt GMMD to preference optimization problems, RL techniques are commonly employed as practical implementations, as for many MD update \citep{tomar2020mirror,munos2023nash,wang2024magnetic}. Define the loss function of conducting GMMD in preference optimization as
\begin{align}
\mathcal{L}_{\text{GMMD}}(\theta) \overset{\text{def}}{=} -\eta \mathbb{E}_{\pi_\theta}\big[G(y;\pi_t)\big] + D_{\text{KL}}(\pi_\theta|| \pi_t)+ \tau R(\pi_\theta, \mu).
\label{eq:gmmd_loss_def}
\end{align}
Here, we set the Bregman divergence to Reverse KL in preference optimization as in previous works \citep{munos2023nash,zhang2024iterativenashpolicyoptimization}. The gradient estimation of $\mathcal{L}_{\text{GMMD}}(\theta)$ for policy updates is required since the expectation in the first term is dependent on $\pi_\theta$. Following Policy Gradient theorem \citep{sutton1999policy}, then we have
\begin{align}
\nabla_\theta \mathcal{L}_{\text{GMMD}}(\theta)= \mathbb{E}_{y \sim \pi_\theta}\bigg[\nabla_\theta \log \pi_\theta(y)\bigg( -\eta G(y;\pi_t) + \log \frac{\pi_\theta(y)}{ \pi_t(y)} + B \bigg)\bigg]  + \tau \nabla_\theta  R(\pi_\theta, \mu),
\label{eq:pg}
\end{align}
where $B$ is a baseline function to reduce the variance as in REINFORCE \citep{williams1992simple}. We set $B$ independent to $\theta$ so that adding $B$ does not affect the value of \Cref{eq:gmmd_loss_def}, due to $\mathbb{E}_{y \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(y) \cdot \eta B] = \eta B\nabla_\theta \mathbb{E}_{y \sim \pi_\theta}[1]=0$. 

% We further adopt three changes for practical preference optimization. We first leverage a baseline 
% Then we further rewrite this gradient as the derivative of a regularized square loss, \Cref{eq:pg} becomes \looseness=-1
% \begin{align}
% \mathbb{E}_{y \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(y)\big( -\eta G(y;\pi_t) + \log \frac{\pi_\theta(y)}{ \pi_t(y)} \textcolor{red}{+ B} \big)]  \nonumber \\ 
%  + \tau \nabla_\theta  R(\pi_\theta, \mu).
% \label{eq:reinforce}
% \end{align}
% Furthermore, we can rewrite this gradient as the derivative of a regularized square loss:
% \begin{align}
% \frac{1}{2} \mathbb{E}_{y \sim \pi_\theta}[ \nabla_\theta \big(  -\eta G(y;\pi_t) + \log \frac{\pi_\theta(y)}{ \pi_t(y)} {+ B}  \big)^2]    \nonumber \\ 
%  + \tau \nabla_\theta R(\pi_\theta, \mu).
%  \label{eq:square_loss}
% \end{align}
We follow SPPO to replace the samples $y \sim \pi_\theta$ with $y \sim \pi_t$ directly since they are equivalent while computing the loss before updating, and rewrite the loss equivalent to GMMD: 
% The expectation in \Cref{eq:pg} requires samples from $\pi_\theta$. Since sampling responses from LLMs is more costly than standard RL, using samples from $\pi_\theta$ only once per iteration may lead to insufficient policy optimization, necessitating additional update iterations.
% $$ becomes:
\begin{align}
&\nabla_\theta\mathcal{L}_{\text{GMMD}}(\theta) = \nabla_\theta \Bigg(\frac{1}{2}   {\mathbb{E}_{{y \sim 
\pi_t}}\Big[  -\eta G(y;\pi_t) + \log \frac{\pi_\theta(y)}{ \pi_t(y)} {+ \eta B}\Big]^2}  + \tau R(\pi_\theta, \mu) \Bigg).
 \label{eq:final_square_loss}
\end{align}
Therefore, according to \Cref{eq:final_square_loss}, RSPO is equivalent to the RL implementation of GMMD since the RHS of the \Cref{eq:final_square_loss} is RSPO loss up to multiplying a constant.

\subsection{Implementation for Preference Optimization}
\label{sec:implementation}
In this section, we introduce the implementation of RSPO. We set the update direction $G$ to be the gradient of the preference against $\pi_t$, $\forall y \in \mathcal{Y}$:
\begin{align}
G({y}, \pi_t, \mu)=\partial_{\pi(y)} \mathbb{P}(\pi \succ \pi_t) = \mathbb{P}(y \succ \pi_t).
\end{align}
As for baseline function $B$, we set $B=\tfrac{1}{2}$ following Nash-MD and SPPO. In theory, $B$ helps minimize the variance of $G$ the most when $B= \mathbb{E}_{y\sim \pi_t}[G(y, \pi_t, \mu)]$. But in preference optimization, due to the typically small minibatch size, the estimation error of the mean of $G$ could be large, leading to additional estimation error of the loss. Thus, we also set the baseline value for variance reduction to be a constant $\tfrac{1}{2}$, the mean value of $G$ when the algorithm is converged. 

Specifically, we execute Algorithm \ref{alg:selfplay} by applying the following RSPO loss with any regularization $R$ of interest:
\begin{align}
&\mathcal{L}_{\text{RSPO}}\big(\theta; G=\mathbb{P}(y \succ \pi_t), B=\tfrac{1}{2}, R \big) = \mathcal{L}_{\text{SPPO}} + \lambda 
 R(\pi_{\theta}, \mu).
\label{eq:RSPO_gmmd}
\end{align}
Therefore, we can directly use the off-the-shelf self-play alignment method SPPO to implement RSPO. The only modification required is to add the regularization to the SPPO loss. Various divergences for regularization are implemented, as discussed in Appendix \ref{sec:implement_reg}.

For completeness, we provide the convergence guarantee for this instance of RSPO to the Nash equilibrium of the regularized preference optimization game as follows (Proof in Appendix \ref{append:rspo_converge}).
\begin{corollary}
Self-play following Algorithm \ref{alg:selfplay} with the RSPO loss function in \Cref{eq:RSPO_gmmd} and regularizer $R$ satisfying the assumption in Proposition \ref{prop:RSPO}, has last-iterate convergence to the NE of the regularized preference optimization game, as described in \Cref{eq:rpm}.
\label{coro:rspo_converge}
% \vspace{-.3cm}
\end{corollary}
RSPO guarantees NE convergence while allowing flexible regularization strategies, making it a robust extension of self-play optimization. In summary, the proposed RSPO framework provides a generalized approach that simplifies the incorporation of regularization into existing self-play methods while maintaining theoretical soundness.



% The expectation term in \Cref{eq:final_square_loss} can be understood as a self-play loss for conducting exponentiated gradient descent \citep{beck2003mirror}. The subsequent regularization term $R$ can be treated as a strong regularization directly acted on the logits of the policy since the first term is quadratic in $\log \pi_\theta$. 

% However, directly applying the policy gradient method as formulated in \Cref{eq:pg} poses significant practical challenges. First, the estimation may suffer from high variance, a problem that becomes particularly severe when the regularization $R(y, \mu)$ has a wide distribution over the space $\mathcal{Y}$. Second, the regularization is indirectly applied to the policy update. Specifically, $R$ functions as part of the $Q$-value, serves as a term for reward transformation. Thus, this regularization may be ineffective if other terms dominate, necessitating additional effort to fine-tune the regularization temperature $\tau$, and further computational and optimization burdens.

% We consider Mirror Descent Policy Optimization (MDPO) \citep{}:
% \begin{align}
% \mathcal{L}_{\text{MDPO}}(\theta) =  D_{\text{KL}} \big( \pi_\theta || {\pi_{t} \exp (\eta A^{\pi_t}) / Z(\pi_t)} \big),
% \label{eq:mdpo}
% \end{align}
% in iteration $t$, where $A^{\pi_t}(\cdot)=U(\cdot , \pi_t)$ is advantage function, $\pi_{t+1}$ is updated based on the loss:\Cref{eq:mdpo} is adapted from Equation (16) in \citep{tomar2020mirror}, the equivalent policy-space loss, where $Z(\pi_t)$ is the normalization constant for ${\pi_{t} \exp (\eta U(\cdot , \pi_t))}$. 
% Leveraging MDPO to approximate Magnetic Mirror Descent ($R=D_{\text{KL}}$ in \Cref{eq:gmmd}), as in Magnetic Policy Optimization \citep{wang2024magnetic}. 
% \begin{align}
% & \arg \min_{\pi_\theta} -\eta \mathbb{P}(\pi_\theta \succ \pi_{t}) + D_{\text{KL}}(\pi|| \pi_t) + \tau D_{\text{KL}}(\pi|| \mu) \nonumber \\
% = & \arg \min_{\pi_\theta} -\eta \mathbb{P}(\pi_\theta \succ \pi_{t}) + D_{\text{KL}}(\pi|| \pi_t) + \tau D_{\text{KL}}(\pi|| \mu) 
% \end{align}
% There is an equivalent policy-space loss due to the closed-form solution of the equation above, as demonstrated in Equation (16) by \citet{tomar2020mirror}:
% \begin{align}
% \mathcal{L}_{\text{MMDPO}}(\theta) &=  D_{\text{KL}} \big( \pi_\theta || {\pi_{t}^\mu \exp (\eta A^{\pi_t^\mu})} \big) \nonumber \\
% & = D_{\text{KL}} \big( \pi_\theta ||{\pi_{t} \exp (\eta A^{\pi_t})} \big) + \lambda D_{\text{KL}}(\pi_\theta||\mu).
% \label{eq:mmdpo}
% \end{align}
% where $\lambda=\frac{1}{\tau+1}$, $A^{\pi_t^\mu}=\mathbb{P}(\cdot \succ \pi_t) - \log Z(\pi_t, \mu)$, $Z(\pi_t, \mu)$ is the normalization constant for ${\pi_{t} \exp (\eta A^{\pi_t^\mu})}$. In \Cref{eq:mmdpo}, the regularization between $\pi$ and $\mu$ is separated from $\pi_t$-related terms.

% To address the aforementioned problem, Given the critical importance of integrating the reference policy $\mu$, we propose a regularization directly applied to the policy space, as the term $D_{\text{KL}}(\pi_\theta||\mu)$ in \Cref{eq:mmdpo}. This regularization framework should also retain sufficient flexibility to accommodate alternative regularization methods. The essential criterion for such flexibility is the ability to decouple the term associated with $\mu$ from $\pi_t$, a property we refer to as \textit{external regularization}. Thus, we extend MMDPO in \Cref{eq:mmdpo} and propose Regularized Mirror Descent Policy Optimization (RMDPO) with an external regularization.




% \begin{align}
% \mathcal{L}_{\text{RMDPO}}(\theta) = D_{\text{KL}} \big( \pi_\theta ||{\pi_{t} \exp (\eta A^{\pi_t})} \big)\ \colorbox{cyan!20}{$+ \lambda 
%  R(\pi_{\theta}, \mu)$},  
% \end{align}
% where the first term, mainly for updating the policy towards advantage values, is equivalent to a quadratic loss function due to the same gradient up to multiplying a constant:
% \begin{align}
% & \nabla_\theta D_{\text{KL}} \big( \pi_\theta ||{\pi_{t} \exp (\eta A^{\pi_t})} \big) \nonumber \\ 
% =&  \nabla_\theta \mathbb{E}_{\pi_\theta} [\log \frac{\pi_\theta}{\pi_t}- \eta A^{\pi_t}(y)] \nonumber \\
% =&  \sum_y [\log \frac{\pi_\theta(y)}{\pi_t(y)}- \eta A^{\pi_t}(y)] \nabla_\theta \pi_\theta(y) + \nabla_\theta \pi_\theta(y)  \nonumber \\
% =&   \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta \big( \log \frac{\pi_\theta}{\pi_t}- \eta A^{\pi_t}(y) \big) ] \nonumber \\
% =&   \mathbb{E}_{\pi_\theta} [ \nabla_\theta \big( \log \frac{\pi_\theta}{\pi_t}- \eta A^{\pi_t}(y) \big) ^ 2 ] / 2.
% \end{align}



% In this section, we introduce the theory of Regularized Self-Play including the convergence guarantee and the generalizing ability to existing self-play methods.

% \begin{proposition}[\textbf{Equivalence to Internal Regularization}]
% The policy updated with external regularization (\Cref{eq:RSPO}) converges to the same solution to the IR-SP, if for some function $g$:
% \begin{align}
% \nabla_{\theta}R(\pi_{\theta}) := \mathbb{E}_{\substack{y\sim \pi_{\theta}, \\ y' \sim \pi'}}[ \nabla_{\theta}(g(y; \pi_{\theta}, \mu)^2)],
% \label{eq:reg}
% \end{align}
% and $\mathbb{E}_{{y\sim \pi^*, \\ y' \sim \pi^*}}[g(y; \pi^*, \mu) - g(y'; \pi^*, \mu)]=0$.
% \label{theorem:RSPO}
% \end{proposition}
% \begin{proof}[Proof sketch]
% Since both $\mathcal{L}_{\text{SP}}$ and $R$ can be written in quadratic form, the key idea is to apply Cauchyâ€“Schwarz inequality to show that external regularized self-play loss is the upper bound of internal regularization loss. Then we demonstrate that the optimization problems of both losses have the same optimal solution, thus they are equivalent.
% \end{proof}
% The equivalence of internal and external regularization in Proposition \ref{theorem:RSPO} results in the average-iterate convergence of RSPO to the regularized Nash Equilibrium. And the corresponding regularization term in the preference model $g$ has a direct connection to the external regularizer $R(\pi_{\theta})$. The RHS in \Cref{eq:reg} can also be treated as the equivalent form of score function estimator of gradient \citep{fu2006gradient}, which is widely used in various applications.


% In the subsequent section, we introduce our implementation of the regularization.

% Applying GMMD is sufficient to solve the regularized preference optimization. However, the proposed framework, RSPO, aims not only to solve the regularized game but also to elucidate the connections between existing methods, providing a deeper understanding of their underlying principles and investigating the shared characteristics that contribute to their success in performance. In next section, we reveal these similarities and show that existing self-play methods share a similar type of loss function.


% The loss with external regularization typically has less variance. This is because when two random variables (in this case $\mathcal{L}_{\text{SP}}$ and $R$) are not negative-correlated, the square of their sum has larger or equal variance than the sum of squares. 
% Third, 
% RSPO is more flexible in practice to apply different regularization function $R$. 
% Changing the policy update rule (i.e. $G$ and $B$) in $\mathcal{L}_{\text{SP}}$ to find different regularized NEs requires re-tuning the hyperparameters, leading to additional cost when altering the regularization methods. 




% We investigate how the generic self-play loss function $\mathcal{L}_{\text{SP}}$ ($\mathcal{L}_{\text{RSPO}}$ with $\lambda=0$) generalizes existing game-theoretic self-play methods. Variations in the choice of update direction ($G$) and baseline ($B$) result in convergence to the Nash Equilibrium policy corresponding to different preference models, reflecting distinct underlying games. 
% Table \ref{table:main} summarizes this relationship, showing how off-the-shelf methods iteratively update policies via \Cref{eq:sp}, with the specific update direction and baseline detailed in the second and third columns. The final column of Table \ref{table:main} confirms the convergence to the Nash Equilibrium of the respective preference model.




% \subsection{Implementation of Regularization}
% \label{sec:implement_reg}
% In practice, accurately estimating the gradient of the regularizer is essential, as many commonly used divergence measures are defined as expectations over $\pi_\theta$. The estimation of divergences has been extensively studied and widely applied in various domains \citep{rubenstein2019practical}. While for completeness, in this section, we introduce the regularization methods investigated in this study, including Reverse KL, Forward KL, and Chi-Square Divergence.

% We begin by deriving the estimation of the Reverse KL divergence based on the following proposition.
% \begin{proposition}
% Reverse KL divergence satisfies:
% $\nabla_\theta D_{\textit{KL}}(\pi_\theta||\mu)=\mathbb{E}_{y \sim \pi_\theta}[{\nabla_\theta (\log \pi_\theta(y) - \log \mu(y))^2}].$
% \label{theo:reverse_kl}
% \end{proposition}
% % \vspace{-.5cm}
% Due to the equivalent gradient in Proposition \ref{theo:reverse_kl}, we can estimate the divergence with $\mathbb{E}_{y \sim \pi_\theta}[{(\log \pi_\theta(y) - \log \mu(y))^2}]$.

% We employ two distinct approaches to estimate the forward KL divergence. The first method utilizes importance sampling, referred to as IS-For. KL, and is derived based on the following proposition.
% \begin{proposition} The gradient of forward KL divergence satisfies that $\nabla_{\theta} D_{\text{KL}}(\mu||\pi_\theta)=  \mathbb{E}_{y \sim \pi_{\theta}}[\nabla_{\theta} {\mu(y)}/{\pi_{\theta}(y)}].$
% \label{theo:forward_kl}
% \end{proposition}
% Therefore, we can estimate the forward KL divergence by leveraging the expectation $\mathbb{E}_{y \sim \pi_{\theta}}[{\mu(y)}/{\pi_{\theta}(y)}]$ to estimate the forward KL. Notably, to mitigate the risk of gradient explosion, we apply gradient clipping with a maximum value of $10$.

% The second method for forward KL is a direct estimation of $D_{\text{KL}}(\mu||\pi_\theta)$. To achieve this, we resample responses from the reference policy $\mu$ using the same prompts from the training dataset, constructing a reference dataset. The KL divergence is then estimated directly based on its definition by uniformly drawing samples from this reference dataset. A key advantage of this approach is that it eliminates the need for importance sampling, as each policy update iteration only requires samples from $\pi_t$.
% % \vspace{-.1cm}

% Similarly, we estimate the Chi-Square divergence using $\mathbb{E}_{y \sim \pi_\theta}\left[{\pi_\theta(y)}/{\mu(y)}\right]$, based on the following proposition. Due to the presence of the ratio term, Chi-Square divergence estimation also necessitates gradient clipping to prevent instability, for which we set a clip value of $10$.
% \begin{proposition} Chi-Square divergence has gradient $\nabla_\theta D_{\chi^2}(\pi_\theta||\mu)=\mathbb{E}_{y \sim \pi_\theta}\left[{\nabla_\theta \pi_\theta(y)}/{\mu(y)}\right].$
% \label{theo:chisquare}
% \end{proposition}
% % \vspace{-.5cm}
% We also explore the linear combination of different regularization functions to leverage their complementary effects, as in offline RLHF \citep{huang2024correcting}. The previously established propositions for estimating divergences can still be used in the combined regularization method.
% % As demonstrated in our experiments (Section \ref{sec:main_reg}), the combination of reverse and forward KL divergences yields the best performance.

% Apart from the flexibility and simplicity of applying different regularization methods, RSPO can generalize existing self-play methods including the unregularized ones, which enables regularizing off-the-shelf self-play methods in practice with \textit{no change} on their original loss functions or hyperparameters, directly adding external regularization term to their loss functions. Generalized Magnetic Mirror Descent (GMMD) is sufficient to solve the regularized game, while we are further interested in the connections between existing self-play algorithms and GMMD, and understand the inherent mechanisms causing their practical success in preference optimization.


% \vspace{-.5cm}

% The generalization in \Cref{eq:sppo_sp} has a natural theoretical explanation from the MWU update. Specifically, $\pi_{t+1}$ updated by MWU is the closed-form solution to the optimization problem in Mirror Descent:
% \begin{align}
% \arg \max_\pi \eta \mathbb{E}_{\pi_\theta}[\partial_\pi u(\pi_t; \pi_t)] - D_{\text{KL}}(\pi || \pi_t),
% \label{eq:mwu_md}
% \end{align}
% where $u(\pi; \pi_t)=\mathbb{P}(\pi \succ \pi_t)$. Thus, from the perspective of Mirror Descent, SPPO is a special case of RSPO without external regularization and with update direction $G$ to be the gradient of utility function $\partial_\pi u(\pi_t; \pi_t)= \mathbb{P}(\cdot \succ \pi_t)$.

% SPO \citep{swamy2024minimaximalist} adopts Soft Policy Iteration, which inherently has a connection to MWU. As in equation (4) in \citet{haarnoja2018soft}, Soft Policy Iteration is
% \begin{align}
% \pi_{t+1} = \arg\min_{\pi_\theta } D_{\text{KL}} \left( \pi_\theta(\cdot ) \, \Big\| \, \frac{\exp\left(Q^{\pi_{t}}( \cdot)\right)}{Z(\pi_{t})} \right),
% \label{eq:spo_spi}
% \end{align}
% where $Q^{\pi_t}(y)=\mathbb{P}(y \succ \pi_t)$, $Z(\pi_t)$ is a normalization constant. Directly taking derivative of the loss on the RHS w.r.t. $\theta$, we obtain:
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{SPO}}(\theta) =  \frac{1}{2}  \nabla_{\theta} \mathcal{L}_{\text{SP}} (\theta; Q^{\pi_t}(y) - \log \pi_t, \log Z(\pi_t)).
% \end{align} 






% Specifically, SPO applies soft policy iteration \citep{ziebart2010modeling} by modelling the optimization over preference at token-level. And thus the policy gradient in SPO is as following

% \begin{proposition}
% In $\mathcal{L}_{\text{SP}}$, $B=\log Z$ minimizes the variance of $G=\mathbb{E}_{y' \sim \pi_t}[U({y}, y')]$.
% \end{proposition}


% Additionally, RSPO loss is a regularized quadratic function at the logarithmic scale (i.e. logits) of the policies. Thus, minimizing $\mathcal{L}_{\text{RSPO}}$ in this case is more stable compared to the policy gradient methods in Mirror Descent. 


% \subsection{Internal Regularization}

% As in Mirror-Descent-based self-play, it is possible to address regularization by revising the update direction. We call this regularization method internal regularization (IR). Existing Mirror Descent methods are gradient-based. For methods which are not gradient-based such as SPPO which is based on vanilla MWU, IR is also applicable: 
% \begin{align}
% \mathcal{L}_{\text{IR-SP}} = \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \Big( \eta \mathbf{RP}(y \succ \pi_t) - \log Z \Big) \Big]^2,
% \label{eq:iRSPO}
% \end{align}
% where $\mathbf{RP}(y \succ \pi_t)=\mathbb{P}(y \succ \pi_t) + \tau g(y; \pi_{\theta}, \mu)$ is the regularized preference, $g$ is the regularization function which is dependent on $\pi_\theta$. This is the main difference from previous gradient-based methods. In Nash-MD and OMD, the preference is shifted by a term independent to $\pi_\theta$.

% Thus, this loss is also an instance of $\mathcal{L}_{\text{SP}}$ with the update direction $G=\mathbf{RP}(y \succ \pi_t)$. $Z$ is the constant for normalizing the equivalent exponential-update form of \Cref{eq:iRSPO}. Intuitively, this method aims to find the policy that has a higher win rate while considering the regularization.

% \begin{theorem}[\textbf{Convergence Guarantee}]
% The average policy of Self-Play with IR (\Cref{eq:iRSPO}) converges to the Nash Equilibrium of regularized preference model
% \begin{align}
% \mathbb{P}(\pi \succ \pi') - \mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] + \mathbb{E}_{y' \sim \pi'}[g(y'; \pi', \mu)].
% \end{align}
% \label{prop:irsp_average}
% \end{theorem}