
\section{Introduction}

\begin{wrapfigure}{r}{0.45\textwidth} 
    \centering
    \vspace{-.7cm}
    \includegraphics[width=\linewidth]{figures/bar_fig1.pdf}
    \caption{Our \textbf{Regularized} Self-Play Policy Optimization (\textbf{RSPO}) with Mistral-\textbf{7B}-Instruct outperforms Llama-3-\textbf{70B}, SimPO \citep{meng2024simpo}, and SPPO \citep{wu2024self} in AlpacaEval-2 with length-controlled (LC) win rate.}
    \label{fig:enter-label} 
    \vspace{-.8cm}

\end{wrapfigure}

% Improving pre-trained Large Language Models (LLM) policy to align with the human preference is critical 
Large Language Models (LLMs) recently have obtained remarkable capabilities to accomplish a range of tasks \citep{jiang2023mistral,dubey2024llama,deepseekai2025deepseekr1incentivizingreasoningcapability}, generating more desirable and helpful content following the user’s intention. One of the most important methods to align LLMs with human intentions is Reinforcement Learning from Human Feedback (RLHF), maximizing a preference-based reward penalized by a reverse KL regularization term of LLM policy and a supervised fine-tuning (SFT) reference model \citep{christiano2017deep,ouyang2022training,rafailov2024direct,azar2024general,xiong2024iterative}. 
% Since the reference model usually provides safer guidance for policy optimization \citep{munos2023nash}, 
This regularization is crucial in RLHF to prevent over-optimization, which has been extensively studied and even extended beyond KL divergence  \citep{go2023aligning,huang2024correcting}.
% optimizing the LLMs via reinforcement learning (RL) based on an offline (static) preference dataset or a pre-trained reward model  \citep{christiano2017deep,ouyang2022training,rafailov2024direct,azar2024general}.
% , even starting to acquire promising results in mathematical Olympiad, complicated algorithmic problems, and even software development 
% The objective of RLHF is to 
 % TODO: add references for harms of overoptimisation
 



Self-play is a general line of works conducting iterative self-competition of models, which has been demonstrated as an effective approach for improving AI systems \citep{goodfellow2020generative,wang2022diffusion}, particularly in strategic decision-making problems  \citep{silver2016mastering,heinrich2016deep,pinto2017robust,brown2018superhuman}. In the human alignment of LLMs, self-play recently started to be used and has shown superior empirical performance than other iterative RLHF methods on benchmarks like AlpacaEval and Arena-Hard Evaluation \citep{dubois2024length,jiang2024textual,wu2024self,rosset2024direct}. By formulating the preference optimization problem as a two-player game, self-play alignment methods seek to identify a \emph{Nash Equilibrium} (NE) of the game in which utility is determined by a general preference model \citep{munos2023nash,calandriello2024human,azar2024general}. This NE is regarded as the most aligned LLM policy, achieved without Bradley-Terry (BT) assumption \citep{david1963method}. 
% independent on the Bradley-Terry (BT) assumption \citep{david1963method}  \looseness=-1 
% An NE LLM policy is regarded as the most aligned policy, as it is expected to generate responses that are, on average, more preferred than those produced by an RLHF solution. This advantage arises because the general preference modeling does not depend on the Bradley-Terry (BT) assumption \citep{david1963method}. 
% Due to such stronger expressiveness of a general preference model, LLM can be aligned with more complicated human preference 


% Recent methods abandon the Bradley-Terry reward model to avoid the . They instead formulate the preference-based RLHF as a two-player game theory problem , where the preference probability is the utility and the optimal solution as Nash Equilibrium policy \citep{azar2024general,munos2023nash,calandriello2024human,wu2024self}. These methods iteratively update the policy directly according to the preference signals, and treat last-iterate policy as the opponent to seek for self-improvement. We refer to these methods as self-play.




Despite the significant empirical improvements achieved through self-play, the impact of regularization to the reference policy—commonly used in RLHF to mitigate over-optimization—has received insufficient investigation in self-play alignment. Most existing self-play methods lack explicit regularization \citep{swamy2024minimaximalist,rosset2024direct,wu2024self,wang2024magnetic,gao2024rebel}. In practice, unregularized self-play is also susceptible to over-optimization, particularly when the preference model is misspecified. While some approaches incorporate regularization, they are typically constrained to a reverse KL divergence penalty that restricts deviations from the reference policy \citep{munos2023nash,zhang2024iterativenashpolicyoptimization}. \looseness=-1


In this paper, we introduce a \textit{generalized framework} for incorporating diverse regularization methods into self-play alignment, termed \textbf{Regularized Self-Play Policy Optimization (RSPO)}. RSPO offers a simple way to apply various regularization strategies in self-play by directly adding the regularization term to the loss function, while maintaining last-iterate convergence to the NE of the corresponding regularized preference optimization game. Empirical analysis reveals distinct effects of different regularization methods: forward KL regularization reduces the response length in RSPO, whereas reverse KL regularization significantly enhances the raw win rate. Consequently, we adopt a linear combination of forward and reverse KL divergences, yielding a substantial improvement over the unregularized self-play alignment method, SPPO \citep{wu2024self}, on various benchmarks. Particularly on AlpacaEval-2, RSPO outperforms SPPO \textbf{with a $6.9$ percentage points increase in length-controlled win rate (LCWR) and an $18$ percentage point LCWR improvement over the base model}, Mistral-7B-Instruct. Furthermore, we offer an analysis of response diversity that regularization also promotes greater diversity. In summary, regularization plays a crucial role in self-play alignment, significantly improving both the quality and diversity of responses in previously unregularized self-play methods.




\section{Related Work}
% \paragraph{Preference Optimization.} Reinforcement Learning from Human Feedback (RLHF) solves preference optimization with reward models based on Bradley-Terry (BT) assumptions, which are used to obtain the rewards from preference data \citep{bradley1952rank,christiano2017deep,ouyang2022training,rafailov2024direct}. 

\citet{azar2024general} introduced the first approach for optimizing general preference models. Nash-MD \citep{munos2023nash} pioneered the application of self-play to general preference optimization by framing it as a two-player game. Subsequent methods have either focused on learning the NE of the original unregularized game (e.g. \citep{swamy2024minimaximalist, wu2024self, rosset2024direct, wang2024magnetic}) or the NE of a reverse-KL-regularized preference optimization game (e.g. \citep{munos2023nash, calandriello2024human, zhang2024iterativenashpolicyoptimization}). In contrast, our work explores a broader class of divergence-based regularization techniques for self-play alignment.


We emphasize the distinction between our self-play approach and self-play methods based on pairwise comparisons, which construct loss functions by leveraging the difference in policy logits between preferred and rejected responses—such as Direct Policy Optimization (DPO) \citep{rafailov2024direct} and Identity Policy Optimization (IPO) \citep{calandriello2024human}. Direct Nash Optimization (DNO) \citep{rosset2024direct} and Iterative Nash Policy Optimization (INPO) \citep{zhang2024iterativenashpolicyoptimization} follow the Mirror Descent (MD) update \citep{beck2003mirror} while computing loss using pairwise comparisons. However, optimizing such pairwise-comparison-based losses can lead to only an increase in the relative likelihood gap without necessarily enhancing the absolute probability of the preferred response \citep{pal2024smaug}. In contrast, our method directly approximates the MD update by converting MD as a reinforcement learning problem, thereby circumventing the limitations of pairwise comparison-based approaches. \looseness=-1

% Notably, self-play alignment methods requiring a pre-trained or external preference model for policy optimization is called (preference) model-based self-play \citep{munos2023nash}. Model-free self-play on the contrary directly optimize the policy based on the preference data. INPO and DNO are model-free methods \citep{zhang2024iterativenashpolicyoptimization,rosset2024direct}, where only bandit preference is provided in the dataset. To avoid estimating a preference model, these methods leverage similar idea of Direct Preference Optimization, subtracting the policy logits of preferred response by that of dis-preferred response to approximately conduct Mirror Descent or MWU. However, in this work, we investigate a general self-play policy optimization, similar to the original RLHF \citep{ouyang2022training} where a reward model is pre-trained for RL, we also assume to have a pre-trained or external preference model. And we empirically show that, a small external preference model is sufficient for improving the base model significantly with self-play.

Online iterative RLHF, which incorporates a reliable reward or preference model—including self-play—functions as a self-improving framework by iteratively generating new data using models and optimizing policies based on this data \citep{schulman2017proximal, ouyang2022training, bai2022training, touvron2023llama, dong2024rlhf}. Moreover, extending powerful offline methods such as Direct Preference Optimization (DPO) to iterative frameworks has led to significant performance gains \citep{xu2023some, liu2023statistical, viethoangtranduong, dong2024rlhf, calandriello2024human, pang2024iterative, xiong2024iterative, guo2024direct, tajwar2024preference, cen2024value, xie2024exploratory}. In contrast, our work investigates general preference optimization through self-play from a game-theoretic perspective, shifting the objective from conventional RL optimization to the computation of NE.\looseness=-1

% \paragraph{Online Learning and Game Theory.}
% Regularization in online learning has been extensively studied, with most works utilizing regularization to promote sparsity in the solution. However, in alignment and RLHF, the approach to regularization and its effects differ. The Multiplicative Weights Update (MWU) method employs a first-order technique, specifically exponentiated gradient descent \citep{beck2003mirror}. Most studies addressing regularized games \citep{zeng2022regularized,liu2022power} rely on gradient-based approaches, such as Mirror Descent.

