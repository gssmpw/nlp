\clearpage
\appendix
\section{Proofs}
In this section, we provide detailed derivations and proofs of propositions.
\subsection{Proof of Equivalence between MD and RSPO}
\label{append:nashmd_proof}

\begin{proposition}
Nash-MD and Online Mirror Descent \citep[Section~6]{munos2023nash} can be seen as particular instances of our general Regularized Self-Play Policy Optimization (RSPO) (\Cref{eq:RSPO}).
\end{proposition}

\begin{proof}
% \subsection{Gradient of Preference in RLHF}
% \begin{align}
% \nabla_\pi \mathbb{P}(\pi \succ \pi_t)
% \end{align}

In this section, we first provide derivations of Nash-MD and Online Mirror Descent to $\mathcal{L}_{\text{RSPO}}$ without external regularization.

Nash-MD practical loss satisfies that
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{Nash-MD}}(\theta)
% &=\mathbb{E}_{\substack{y\sim \pi_{\theta}, \nonumber \\ 
% y'\sim \pi^{\mu}_t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ y') - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\mu(y)} \Big) \Big] \nonumber \\
% &=\mathbb{E}_{\substack{y\sim \pi_{\theta}, \nonumber \\ y'\sim \pi^{\mu}_t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ y') - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big]  \nonumber \\
% &=\mathbb{E}_{y\sim \pi_{\theta}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ \pi^{\mu}_t)  - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big] \nonumber \\
% &=\mathbb{E}_{y\sim \pi_{t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ \pi^{\mu}_t)  - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big] \nonumber \\
% & = \nabla_\theta \mathbb{E}_{y\sim \pi_{t}}\Big[ \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \Big( \mathbb{P}(y \succ \pi^{\mu}_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2} \Big) \Big]^2 / 2\nonumber \\
% & = \tau^2 \nabla_\theta \mathbb{E}_{y\sim \pi_{t}}\Big[ \log \frac{\pi_\theta(y)}{\pi_t(y)} - \frac{1}{\tau} \Big( \mathbb{P}(y \succ \pi^{\mu}_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2} \Big) \Big]^2 / 2.
% \end{align}
\begin{align}
\nabla_{\theta} \mathcal{L}_{\text{Nash-MD}}(\theta) 
&=\mathbb{E}_{\substack{y\sim \pi_{\theta},  \\ 
y'\sim \pi^{\mu}_t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ y') - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\mu(y)} \Big) \Big] \label{eq:nash1} \\
&=\mathbb{E}_{\substack{y\sim \pi_{\theta},  \\ y'\sim \pi^{\mu}_t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ y') - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big]  \label{eq:nash2} \\
&=\mathbb{E}_{y\sim \pi_{\theta}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ \pi^{\mu}_t)  - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big] \label{eq:nash3} \\
&=\mathbb{E}_{y\sim \pi_{t}}\Big[ \nabla_\theta \log \pi_\theta(y) \Big( \mathbb{P}(y \succ \pi^{\mu}_t)  - \frac{1}{2} - \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \Big] \label{eq:nash4} \\
& = \nabla_\theta \mathbb{E}_{y\sim \pi_{t}}\Big[ \tau \log \frac{\pi_\theta(y)}{\pi_t(y)} - \Big( \mathbb{P}(y \succ \pi^{\mu}_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2} \Big) \Big]^2 / 2 \label{eq:nash5} \\
& = \tau^2 \nabla_\theta \mathbb{E}_{y\sim \pi_{t}}\Big[ \log \frac{\pi_\theta(y)}{\pi_t(y)} - \frac{1}{\tau} \Big( \mathbb{P}(y \succ \pi^{\mu}_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \frac{1}{2} \Big) \Big]^2 / 2. \label{eq:nash6}
\end{align}
\Cref{eq:nash1} is the definition of practical Nash-MD loss \citep[Section~7]{munos2023nash}. \Cref{eq:nash2} holds by adding an subtracting the same element $\log \pi_t(y)$. \Cref{eq:nash3} holds due to $\mathbb{E}_{y'\sim \pi_t^\mu}[\mathbb{P}(y \succ y')]= \mathbb{P}(y \succ \pi_t^\mu)$. \Cref{eq:nash4} holds since in each iteration before updating, while computing the loss, $y\sim \pi_\theta$ is equivalent to $y \sim \pi_t$. The learning rate $\eta$ is originally omitted in the paper \citep{munos2023nash}. Here Nash-MD is generalized by $\mathcal{L}_{\text{RSPO}}$ with $\eta=\tfrac{1}{\tau}$ and $R=0$. 

OMD is to execute
$\arg\max_{\pi}  \eta \mathbb{E}_{y \sim \pi} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \right] - \text{KL}(\pi, \pi_t)$.
Therefore, the loss function of the OMD update satisfies
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{OMD}}(\theta) &= -\nabla_\theta  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \right] + D_{\text{KL}}(\pi_\theta, \pi_t) \nonumber \\
% &= -\nabla_\theta  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta}{\pi_t} \right] \nonumber \\
% &=  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ -\nabla_\theta \log \pi_\theta \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta}{\pi_t} \Big) \right] \nonumber \\
% &=  \frac{\eta}{2} \cdot \mathbb{E}_{y \sim \pi_\theta} \left[ \nabla_\theta \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta(y)}{\pi_t(y)} \Big)^2 \right] \nonumber \\
% &=  \frac{\eta}{2} \cdot \mathbb{E}_{y \sim \pi_t} \left[ \nabla_\theta  \log \frac{\pi_\theta(y)}{\pi_t(y)} - \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \right]^2.
% \end{align}
\begin{align}
\nabla_{\theta} \mathcal{L}_{\text{OMD}}(\theta) &= -\nabla_\theta  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \right] + D_{\text{KL}}(\pi_\theta, \pi_t) \label{eq:omd_gradient_1} \\
&= -\nabla_\theta  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta}{\pi_t} \right] \label{eq:omd_gradient_2} \\
&=  \eta \mathbb{E}_{y \sim \pi_\theta} \left[ -\nabla_\theta \log \pi_\theta \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta}{\pi_t} \Big) \right] \label{eq:omd_gradient_3} \\
&=  \frac{\eta}{2} \cdot \mathbb{E}_{y \sim \pi_\theta} \left[ \nabla_\theta \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} - \log \frac{\pi_\theta(y)}{\pi_t(y)} \Big)^2 \right] \label{eq:omd_gradient_4} \\
&=  \frac{\eta}{2} \cdot \mathbb{E}_{y \sim \pi_t} \left[ \nabla_\theta  \log \frac{\pi_\theta(y)}{\pi_t(y)} - \Big( \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \Big) \right]^2. \label{eq:omd_gradient_5}
\end{align}
\Cref{eq:omd_gradient_1} holds because the OMD update is equivalent to descending negative gradient of the feedback $\eta \mathbb{E}_{y \sim \pi} \left[ \mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)} \right] - \text{KL}(\pi, \pi_t)$. \Cref{eq:omd_gradient_2} holds due to the definition of $D_{\text{KL}}$. \Cref{eq:omd_gradient_3} holds by conducting differentiation on multiplication. The remaining equations hold due to simple algebra. Therefore, OMD can also be generalized by RSPO with $G=\mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)}$ and without external regularization.
\end{proof}


% The final equation exists since PG only execute the the gradient descent of loss once.
% In the final equation, and in the setting of RLHF and NLHF, sampling responses from a probability distribution is costly. So, we aim to take full advantage of samples from $y \sim \pi_t$.

% \paragraph{Self-Play Optimization \citep{swamy2024minimaximalist}}
% SPO \citep{swamy2024minimaximalist} adopts Soft Policy Iteration, which inherently has a connection to MWU. As in equation (4) in \citet{haarnoja2018soft}, Soft Policy Iteration is
% \begin{align}
% \pi_{t+1} = \arg\min_{\pi \in \Pi} D_{\text{KL}} \left( \pi(\cdot ) \, \Big\| \, \frac{\exp\left(Q^{\pi_{t}}( \cdot)\right)}{Z(\pi_{t})} \right).
% \label{eq:spo_spi}
% \end{align}
% So the gradient of loss function for updating parametrized policy $\pi_{\theta}$ of each iteration is
% \begin{align}
% &\nabla_{\theta} \mathcal{L}_{\text{SPO}}(\theta) = \nabla_{\theta} \mathbb{E}_{y \sim \pi_{\theta}} \Big[ \log \pi_{\theta} - Q^{\pi_t}(y) + \log Z(\pi_t) \Big] \nonumber \\
% &= \mathbb{E}_{y \sim \pi_{\theta}} \Big[ \nabla_{\theta} \Big ( \log \pi_{\theta} - Q^{\pi_t}(y) + \log Z(\pi_t) \Big)^2 /2 \Big] \nonumber \\
% &=  \frac{1}{2} \cdot \nabla_{\theta} \mathcal{L}_{\text{SP}} (\theta; Q^{\pi_t}(y) - \log \pi_t, \log Z(\pi_t))
% \end{align}
% where $Q$ function is computed by treating the preference over the adversary $\pi_t$ as rewards. The second equation exists since the gradient of the loss can be rewritten with its score function estimator \citep{fu2006gradient,schulman2015gradient}. Based on the above equations, since the sampling process $y\sim \pi_t$ is equivalent to $y \sim \pi_{\theta}$ in SPO, the loss has the same gradient as $\mathcal{L}_{\text{SP}}$ up to multiplying a constant. So SPO is also generalized from $\mathcal{L}_{\text{SP}}$ with $G$ being the soft value function, $V=Q-\log \pi_t$, when the adversary is fixed as $\pi_t$ and $B=\log Z$, where $Z$ is to normalize $\exp Q(\cdot)$.

% \begin{lemma}
% Given a discrete random variable $X$. $\mathbb{E}[X^2] =0 \Rightarrow X\equiv0$.
% \label{lemma:xequivzero}
% \end{lemma}
% \begin{proof}
% \begin{align}
%     \mathbb{E}[X^2] = \sum_{x} \mathbb{P}(X=x) \cdot X^2 \geq 0.
% \label{eq:ss_lemma}
% \end{align}
% The equation condition is satisfied when $X\equiv0$ since Equation \ref{eq:ss_lemma} is sum of squares.
% \end{proof}




% \subsection{Proof of Theorem \ref{prop:irsp_average}}
% The average policy of Self-Play with IR (Equation \ref{eq:irsp}) converges to the Nash Equilibrium of regularized preference model in Equation \ref{eq:pm}.
% \begin{proof}
% The update of Self-Play with IR is to minimize the loss:
% \begin{align}
% \mathcal{L}_{\text{IR-SP}} = \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \eta \Big(  \mathbf{RP}(y \succ \pi_t) - \log Z \Big) \Big]^2,
% \end{align}
% This is equivalent to follow the update rule:
% \begin{align}
% \pi_{t+1} = \frac{\pi_t \cdot \exp \Big\{ \eta  \mathbf{RP}(y \succ \pi_t)\Big\}}{Z}.
% \label{eq:ir_mwu}
% \end{align}
% Given that $\mathbf{RP}(y \succ \pi_t)=\mathbb{P}(y \succ \pi') - g(y; \pi, \mu)$, since Equation \ref{eq:ir_mwu} is an exponential update of MWU, it has an average-iterate convergence to the Nash Equilibrium of the regularized preference model:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') - \mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] + \mathbb{E}_{y' \sim \pi'}[g(y'; \pi', \mu)].
% \label{eq:rsp_to_md}
% \end{align}
% \end{proof}

\subsection{Proof of the Existence of Nash Equilibrium}
\label{append:rpm_exists}
\begin{proposition}
Nash Equilibrium in the regularized game in \Cref{eq:rpm} exists, and is unique.
\end{proposition}

\begin{proof}We prove the existence of in this section, largely following the idea of proving the existence of KL regularized Nash Equilibrium by \citet{munos2023nash}.

Since the utility $u(\pi, \pi')$ is linear in $\pi$ and $\pi'$, and the regularization function is assumed to be convex (Assumption \ref{assumption:reg}), the regularized preference is concave in $\pi$ and convex in $\pi'$. Therefore, the existence and the uniqueness of a regularized Nash Equilibrium in \Cref{eq:rpm} can be directly derived from the minimax theorem \citep{sion1958general}.
\end{proof}



\subsection{Proof of Proposition \ref{prop:RSPO}}
\label{append:rspo_proof}

    % ]]]]]]gin{align}
% &\min_{\pi_\theta} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \eta \Big(  \mathbb{P}({y} \succ \pi_t) + \tau g(y; \pi_\theta, \mu) - \log Z(\pi_t) \Big) \Big]^2 \label{eq:qudratic_to_minp} \\
% \Leftrightarrow  &\min_{\pi_\theta} -\eta \mathbb{P}(\pi_\theta\succ \pi_{t}) + \tau' D_{f}(\mu || \pi_\theta) + D_{\text{KL}}(\pi_\theta|| \pi_t)
% \end{align}
% \end{lemma}

% \begin{proof}

% \end{proof}

% The policy updated with external regularization (Equation \ref{eq:rsp}) converges to the same solution as the IR-SP.
\begin{assumption}[\textbf{Relative Convexity w.r.t. $\psi$}]
We assume the regularization function $R$ of policy $\pi$ is a $1$-strongly convex relative to some function $\psi$. In other words, $\forall \pi, \pi' \in \Delta^{\mathcal{X}}_{\mathcal{Y}}$,
\begin{align}
\langle \partial_\pi R(\pi)- \partial_\pi R(\pi') , \pi - \pi' \rangle \geq \langle {\partial_\pi \psi(\pi) - \partial_\pi \psi(\pi')}, \pi - \pi' \rangle.
\end{align}
\label{assumption:reg}
\end{assumption}
%\todoq{this assumption is very strange, you're comparing two vectors? because the denominator is a vector?}

\begin{customprop}{\ref{prop:RSPO}}
If $R(\cdot, \mu)$ is $1$-strongly convex relative to $\psi$ (Assumption \ref{assumption:reg}), policy updated by GMMD in \Cref{eq:gmmd} has last-iterate convergence to the following Nash Equilibrium of a regularized game:
\begin{align}
\max_{\pi} \min_{\pi'} U(\pi; \pi') - \tau R(\pi, \mu) + \tau R(\pi', \mu).
\end{align}
\end{customprop}

\begin{proof}
% \begin{align}
% &\min_{\theta} \mathcal{L}_{\text{IR-SP}}(\theta) \label{eq:irsp_qudratic}\\
% \Leftrightarrow & \min_{\pi_\theta} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \eta \Big(  \mathbb{P}({y} \succ \pi_t) + \tau g(y; \pi_\theta, \mu) - \log Z(\pi_t) \Big) \Big]^2 \label{eq:qudratic_to_minp} \\
% \Leftrightarrow & \min_{\pi_\theta} -\eta \mathbb{P}(\pi_\theta\succ \pi_{t}) + \tau' D_{f}(\mu || \pi_\theta) + D_{\text{KL}}(\pi_\theta|| \pi_t) \\
% \Leftrightarrow & \min_{\pi_\theta} -\eta \mathbb{P}(\pi_\theta\succ \pi_{t}) + D_{\text{KL}}(\pi_\theta|| \pi_t),\ s.t. \ D_{f}(\mu || \pi_\theta) \leq \delta \\
% \Leftrightarrow & \min_{\pi_\theta} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \eta \Big(  \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big) \Big]^2,\ s.t. \  D_{f}(\mu || \pi_\theta) \leq \delta \\
% \Leftrightarrow & \min_{\pi_\theta} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} - \eta \Big(  \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big) \Big]^2 + \lambda  D_{f}(\mu || \pi_\theta)
% \\
% \Leftrightarrow & \min_{\theta} \mathcal{L}_{\text{RSP}}(\theta)
% \end{align}
% Eq. \ref{eq:irsp_qudratic} holds due to the definition of IR-SP. Eq. \ref{eq:qudratic_to_minp} holds due to 
% We first rewrite the problem of minimizing the RSPO loss function. Given a small positive value $\delta$, for some positive value $\lambda$ and $\tau$, the RSPO loss function for iterative policy update can be written as:
% \begin{align}
% & \arg \min_{\theta}  \mathcal{L}_{\text{RSPO}}(\theta) \nonumber \\
% = & \arg \min_{\theta} \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_\theta(y)}{\pi_t(y)} - \Big(  \eta \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big) \Big]^2 + \lambda  R(\pi_\theta) \\ 
% = & \arg \min_{\theta} -\eta \mathbb{P}(\pi_\theta\succ \pi_{t}) + \tau R(\pi_\theta) + D_{\text{KL}}(\pi_\theta|| \pi_t).
% \label{eq:rsp_to_md}
% \end{align}
% The $1$st equation holds due to definition of $\mathcal{L}_{\text{RSPO}}$. The $2$nd equation holds due to the equivalence between constratint optimization problem and its Lagrange multiplier form. The $3$rd equation holds due to the the closed form solution to a regularized optimization problem, specifically, based on Lemma \ref{lemma:xequivzero}, $\mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{t+1}(y)}{\pi_t(y)} - \Big(  
% \eta \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big) \Big]^2 =0 \Rightarrow \forall y \in \mathcal{Y},\ \log \frac{\pi_{t+1}(y)}{\pi_t(y)} - \Big(  
% \eta \mathbb{P}({y} \succ \pi_t) - \log Z(\pi_t) \Big)=0 \Rightarrow \forall y \in \mathcal{Y},\ \pi_{t+1} \propto \pi_t \exp (\eta \mathbb{P}(\pi \succ \pi_t)) \Rightarrow \pi_{t+1}= \arg \min_{\pi} -\eta \mathbb{P}(\pi\succ \pi_{t}) + D_{\text{KL}}(\pi|| \pi_t)$. The $4$-th equation holds also due to the equivalence between constratint optimization problem and its Lagrange multiplier form.


According to \Cref{eq:gmmd}, GMMD is equivalent to the Algorithm 3.1 in \citet{sokota2022unified}:
\begin{align}
z_{t+1} = \arg\min_{z \in \mathcal{Z}} \eta \left( \langle F(z_t), z \rangle + \alpha g(z) \right) + B_{\psi}(z; z_t),
\label{eq:mmd}
\end{align}
where in our setting, $z=\pi$ is the LLM policy, $F(z_t)=-\partial_\pi U(\pi; \pi_t)$ is the vector of negative partial derivatives of preference w.r.t. each component of $\pi$, $\alpha =\tau $, $g(z)$ is the regularizer $R(\pi)$, and we set $\psi(z) = z\log z$ to convert the Bregman divergence $B_{\psi}$ to KL divergence. Here $U(\pi; \pi_t)$ is treated as a function of vector form of $\pi$, i.e., $[\pi^0\ \pi^1\ \cdots\ \pi^{|\mathcal{Y}|}]$, thus the gradient is a vector gradient where $\partial_\pi U(\pi; \pi_t) = [\partial U /  \partial \pi^0\ \ \partial U /  \partial \pi^1 \ \ \cdots \ \ \  \partial U / \partial \pi^{|\mathcal{Y}|}]$. 

We then show that in our setting the following assumptions are satisfied. $F$ satisfies that for $\mu > 0$ and any $z, z'$, $\langle F(z) - F(z'), z- z' \rangle =0$ since $U$ is linear in $\pi$, and $F(z) - F(z') = -\partial_\pi U(\pi; \pi_t) + \partial_\pi U(\pi';\pi_t)=0$. Therefore, $F$ is Monotone and $L$-smooth. According to Assumption \ref{assumption:reg}, $g$ is $1$-strongly convex relative to $\psi$, i.e., $g(z) \geq g(z') + \frac{g'(z)}{\psi'(z)}(\psi(z) - \psi(z'))$.

Given the assumptions above, according to the Theorem 3.4. in \citet{sokota2022unified}, the update rule defined in \Cref{eq:mmd} has a last-iterate convergence guarantee to a policy $\pi^*$, which is the solution to the variational inequality problem $\text{VI}(\Delta^{\mathcal{X}}_{\mathcal{Y}}, F+ \alpha \partial g)$, i.e., $\pi^*$ satisfies
\begin{align}
% \langle -\mathbb{P}(y \succ \pi^*) + \tau \nabla R(\pi^*), \pi - \pi^* \rangle & \geq 0 \quad \forall \pi \in \Delta^{\mathcal{X}}_{\mathcal{Y}}. \\
\langle \partial_\pi \Big( -U(\pi; \pi^*) + \tau R(\pi, \mu) \Big) \mid_{\pi=\pi^*}, \pi - \pi^* \rangle &\geq 0, \quad \forall \pi \in \Delta^{\mathcal{X}}_{\mathcal{Y}} \nonumber \\
\Leftrightarrow  \langle \partial_\pi \Big( -U(\pi; \pi^*) + \tau R(\pi, \mu) - \tau R(\pi^*, \mu) \Big) \mid_{\pi=\pi^*}, \pi - \pi^* \rangle &\geq 0, \quad \forall \pi \in \Delta^{\mathcal{X}}_{\mathcal{Y}}.
\label{eq:vi_final}
\end{align}
\Cref{eq:vi_final} indicates that moving from $\pi^*$ towards any direction $\pi - \pi^*$ can not increase the value of the objective preference model $U(\pi; \pi^*) - \tau R(\pi, \mu) + \tau R(\pi^*, \mu)$ at the point of $\pi=\pi^*$, given the opponent is $\pi^*$. Therefore, by symmetry, $\pi^*$ is the Nash Equilibrium of the regularized preference model:
\begin{align}
\max_\pi \min_{\pi'} U(\pi;\pi') - \tau R(\pi, \mu) + \tau R(\pi', \mu).
\end{align}


% According to Proposition 2.4. in \citep{sokota2022unified}, only when $R = B_{\psi}= D_{\text{KL}}$, solving $\text{VI}(\Delta^{\mathcal{X}}_{\mathcal{Y}}, F+ \alpha \nabla g)$ is equivalent to solving the two-player general-sum game as follows via Magnetic Mirror Descent:
% \begin{align}
% \max_{\pi} \min_{\pi'} \eta \mathbb{P}(\pi \succ \pi' ) - \tau R(\pi) + R(\pi').
% \end{align}

% When the payoff or utility function (loss when opponent is fixed) is $U(\pi, \pi_t) = u(\pi; \pi_t) - \tau  g(\pi, \mu) + \tau  g(\pi_t, \mu)$, Online Mirror Descent is:
% \begin{align}
% &\max_{\pi}  \mathbb{E}_{\pi}[ \eta \partial_\pi U(\pi, \pi_t)\mid_{\pi=\pi_t}] - D_{\text{KL}}(\pi || \pi_t) \\
% \Longleftrightarrow
% &\max_{\pi}  \mathbb{E}_{\pi}[ \eta \partial_\pi \big( u(\pi; \pi_t) - \tau  g(\pi, \mu) + \tau  g(\pi_t, \mu) \big) \mid_{\pi=\pi_t}] - D_{\text{KL}}(\pi || \pi_t) \\
% \Longleftrightarrow & \max_{\pi} \mathbb{E}_{\pi}[ \eta \mathbb{P}(y \succ \pi_t) - \tau \partial_\pi \big( g(\pi, \mu) \big) \mid_{\pi=\pi_t} ] - D_{\text{KL}}(\pi || \pi_t) \\
% \Longleftrightarrow & \max_{\pi}  \eta u(\pi; \pi_t) - \tau \mathbb{E}_{\pi}[ \partial_\pi g(\pi_t, \mu)]   - D_{\text{KL}}(\pi || \pi_t).
% \end{align}

% Thus RSPO is equivalent to OMD if $f$ satisfies that
% \begin{align}
% R(\pi) = \mathbb{E}_{\pi}[ \partial_\pi g(\pi_t, \mu)] + C.
% \end{align}
\end{proof}

% \subsection{Proof of Proposition \ref{theorem:RSPO} (Gradient-based Proof)}
% The converged policy updated with external regularization loss in Equation \ref{eq:RSPO} is also the solution to the IR-SP.

% Define a sum of square loss (SS): 
% \begin{align}
% \mathcal{L}_{\text{SS}} = \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z(\pi_t) \Big)^2 + g(y; \pi_\theta, \mu)^2 \Bigg].
% \end{align}
% Rewrite the expectation as dot product, and treat policies as vectors.
% \begin{align}
% \mathcal{L}_{\text{SS}}(\pi_\theta) = \langle{\pi_t}, \Big(\log \frac{\pi_{\theta}(\cdot)}{\pi_t(\cdot)}- \eta \mathbb{P}({\cdot} \succ \pi_t) + \log Z(\pi_t) \Big)^2 \rangle + \langle \pi_t, g(\cdot; \pi_\theta, \mu)^2 \rangle .
% \end{align}
% Denote $\pi_{t+1} = \arg \min_{\pi_\theta} \mathcal{L}_{\text{SS}}(\pi_\theta)$, $\pi_{t+1}$ satisfies that $\nabla_{\pi_\theta} \mathcal{L}_{\text{SS}}(\pi^*) + \lambda (\sum_y \pi_\theta(y) - 1) = 0$. In other words, $\forall y \in \mathcal{Y}$,
% \begin{align}
%     2 {\pi_t}(y) \Big(\log \frac{\pi_{t+1}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z(\pi_t) \Big)\cdot \frac{1}{\pi_{t+1}(y)} + 2 \pi_t(y) g(y; \pi_{t+1}, \mu) \nabla_{\pi_\theta} g(y; \pi_{t+1}, \mu)+ \lambda &= 0 \\
%     \Longleftrightarrow 
%     \log \frac{\pi_{t+1}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z(\pi_t) + \pi_{t+1}(y) g(y; \pi_{t+1}, \mu) \nabla_{\pi_\theta} g(y; \pi_{t+1}, \mu)+ \lambda \frac{\pi_{t+1}(y)}{2 \pi_t(y)} &= 0 \\
%     \Longleftrightarrow 
%     \log \frac{\pi_{t+1}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z(\pi_t) + \big( \pi(y) \partial_\pi f(\pi) + \lambda \frac{\pi(y)}{2 \pi_t(y)} \big) \mid_{\pi=\pi_{t+1}} &= 0
% \end{align}
% OMD:
% \begin{align}
% &\arg\max_{\pi}  \eta \mathbb{E}_{y \sim \pi} \left[ \mathbb{P}(y \succ \pi_t) - \tau \partial_\pi g(y; \pi_t, \mu) \right] - \text{KL}(\pi, \pi_t). \\
% \Longleftrightarrow &  \log \frac{\pi_{t+1}(y)}{\pi_t(y)} -  \eta \big( \mathbb{P}(y \succ \pi_t) - \tau \partial_\pi g(y; \pi_t, \mu)\big) - \log Z(\pi_t) = 0
% \end{align}

% \begin{align}
% \partial_\pi \mathcal{L}_{\text{SS}}(\pi_{t}) = 2\mathbb{E}_{\pi_t} \left[ \log \frac{\pi_t(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z(\pi_t) + \lambda \pi_t(y) \partial_\pi  g(y; \pi_t, \mu) \partial_\pi g(y; \pi_t, \mu)  \right] \neq 0
% \end{align}

\subsection{Proof of Proposition \ref{coro:rspo_converge}}
\label{append:rspo_converge}
\begin{proof}
We prove that RSPO in \Cref{eq:RSPO_gmmd} is equivalent to GMMD up to multiplying a constant to the gradient, leading to a regularized Nash Equilibrium.
\begin{align}
&\nabla_\theta\mathcal{L}_{\text{RSPO}}(\theta; G=\mathbb{P}(y \succ \pi_t), B=\frac{1}{2})  \\
&=  \nabla_\theta \Big( \mathbb{E}_{y \sim \pi_t} \Big[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)} -  \eta \Big(   \mathbb{P}(y \succ \pi_t) - \frac{1}{2} \Big) \Big]^2 + \lambda 
 R(\pi_{\theta}, \mu) \Big) \label{eq:rspo_proof_2} \\
&=  \nabla_\theta \Big(  \langle \pi_t, , \big(  -\eta \partial_\pi \mathbb{P}(\pi \succ \pi_t) + \log \frac{\pi_\theta}{ \pi_t} {+ B}  \big)^2 \rangle + \lambda 
 R(\pi_{\theta}, \mu) \Big)  \label{eq:rspo_proof_3} \\
% &=  2 \nabla_\theta  \langle \pi_t, , \big(  -\eta \nabla_\pi u(\pi_t; \pi_t) + \log \frac{\pi_\theta}{ \pi_t} {+ B}  \big)^2 \rangle \cdot \frac{1}{2}   + \tau \nabla_\theta R(\pi_\theta, \mu) \label{eq:rspo_proof_4} \\
&=  2 \Big( \nabla_\theta  \mathbb{E}_{{y \sim 
\pi_t}}[\big(  -\eta G(y, \pi_t) + \log \frac{\pi_\theta(y)}{ \pi_t(y)} {+ B}  \big)^2] \cdot \frac{1}{2}   
 + \tau \nabla_\theta R(\pi_\theta, \mu) \label{eq:rspo_proof_5}\Big) \\
&=  2\nabla_\theta \mathcal{L}_{\text{GMMD}}(\theta).\label{eq:rspo_proof_6}
\end{align}
\Cref{eq:rspo_proof_2} holds due to definition. \Cref{eq:rspo_proof_3} holds by treating policy as a vector and rewrite the expectation in vector product form, and $\nabla_\pi \mathbb{P}(\pi \succ \pi_t)\mid_{\pi=\pi_t} \mid_{\pi=\pi_t} = [\mathbb{P}(y^0 \succ \pi_t)\quad \mathbb{P}(y^1 \succ \pi_t) \quad \cdots \quad \mathbb{P}(y^{|\mathcal{Y}|} \succ \pi_t)]^T$, where $y^0, y^1, \cdots, y^{\mathcal{Y}}$ represent all possible values of $y$.
    % \item \Cref{eq:rspo_proof_4} holds due to that when $u=\mathbb{P}$, $\nabla_\pi u(\pi_t; \pi_t) = \nabla_\pi \mathbb{E}_{y \sim \pi} [\mathbb{P}(y \succ \pi_t)]\mid_{\pi=\pi_t}$, and $\tau=\frac{\lambda}{2}$.
\Cref{eq:rspo_proof_5} holds by rewriting the form of dot product as expectation. \Cref{eq:rspo_proof_6} holds due to the equivalent loss form of GMMD in \Cref{eq:final_square_loss}.

Thus, according to Proposition \ref{prop:RSPO}, updating following Algorithm \ref{alg:selfplay} with the above loss function has last-iterate convergence to the Nash Equilibrium of the regularized preference optimization game in \Cref{eq:rpm} by setting $u(\pi;\pi')=\mathbb{P}(\pi \succ \pi')$.
   
\end{proof}

% \subsection{Proof of Proposition \ref{theorem:rsp}}

% The policy updated with external regularization (Equation \ref{eq:rsp}) converges to the same solution to the IR-SP, if for some function $g: \mathcal{Y} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \rightarrow \mathbb{R}$:
% \begin{align}
% \nabla_{\theta}R(\pi_{\theta}) := \mathbb{E}_{\substack{y\sim \pi_{\theta}}}[ \nabla_{\theta}(g(y; \pi_{\theta}, \mu)^2)],
% \label{eq:reg}
% \end{align}
% and $\mathbb{E}_{{y\sim \pi^*}}[g(y; \pi^*, \mu)]=0$, where $\pi^*$ is the Nash Equilibrium policy.

% \begin{proof}
% Define a sum of squares (SS) loss:
% \begin{align}
% \mathcal{L}_{\text{SS}} = \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + g(y; \pi_\theta, \mu)^2 \Bigg].
% \end{align}

% We first demonstrate that optimizing RSPO loss is equivalent to optimize $\mathcal{L}_{\text{SS}}$:
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{RSP}} &= \nabla_{\theta}\mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 \Bigg] + \nabla_{\theta} \lambda R(\pi_{\theta})  \\
% &=\nabla_{\theta} \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg] = \nabla_{\theta} \mathcal{L}_{\text{SS}} 
% % \\
% % &=  \mathbb{E}_{\pi_t} \Bigg[ 2 \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z \Big) \nabla_{\theta} \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z \Big) \nonumber \\ 
% % &\quad\quad\quad\ \ \  +  2 g(y; \pi_\theta, \mu) \nabla_{\theta} g(y; \pi_\theta, \mu) \Bigg]
% \label{eq:rspisss}
% \end{align}
% The second equation holds due to Equation \ref{eq:reg} and $y\sim \pi_t \Leftrightarrow y \sim \pi_\theta$. According to Eq. \ref{eq:rspisss}, $\arg \min_\theta \mathcal{L}_{\text{RSP}} = \arg \min_\theta \mathcal{L}_{\text{SS}}$.
% \begin{align}
% \mathcal{L}_{\text{SS}} &= \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg]  \\
% &\geq  \mathbb{E}_{\pi_t} \Bigg[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)}-  \Big(\eta \mathbb{P}({y} \succ \pi_t) +  {\sqrt{\lambda}} g(y; \pi_\theta, \mu) - \log Z\Big) \Bigg]^2 / 2 \label{eq:cs} \\ &= \frac{1}{2} \mathcal{L}_{\text{IR-SP}}.
% \end{align}


% \textcolor{red}{The followings are incorrect:} The inequality holds due to Cauchy–Schwarz inequality. And the inequality becomes equation (i.e. $\mathcal{L}_{\text{SS}}(\pi)=\mathcal{L}_{\text{IR-SP}}(\pi)/2$) when executing Algorithm \ref{alg:selfplay} with loss function $\mathcal{L}_{\text{IR-SP}}$ is converged in a large iteration $T$ and $\pi_{T+1} =\pi^*\overset{\text{def}}{=}\arg \min \mathcal{L}_{\text{IR-SP}}$. Based on Lemma \ref{lemma:xequivzero},
% \begin{align}
% \mathcal{L}_{\text{IR-SP}}(\pi^*)=0 &\Rightarrow \forall y \in \mathcal{Y},\  \log \frac{\pi^*(y)}{\pi_T(y)}-  \Big(\eta \mathbb{P}({y} \succ {\pi}_T) - \log Z\Big)  =  - \sqrt{\lambda} g(y; \pi^*, \mu) \\
% &\Rightarrow \mathcal{L}_{\text{SS}}(\pi^*) = \mathbb{E}_{\pi_T} \Bigg[ \Big(\log \frac{\pi^*(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z \Big)^2 + \lambda g(y; \pi^*, \mu)^2 \Bigg]  \\
% & \quad \quad \quad \quad \quad =  \mathbb{E}_{\pi_T} \Bigg[ \log \frac{\pi^*(y)}{\pi_T(y)}-  \Big(\eta \mathbb{P}({y} \succ \pi_T) - \sqrt{\lambda} g(y; \pi^*, \mu) - \log Z\Big) \Bigg]^2 / 2 
% \\ & \quad \quad \quad \quad \quad = \frac{1}{2} \mathcal{L}_{\text{IR-SP}}(\pi^*)
% \\ & \quad \quad \quad \quad \quad = 0.
% \label{eq:equal_condition}
% \end{align}
% Therefore, $\pi^* = \arg \min_{\pi} \mathcal{L}_{\text{IR-SP}}(\pi) = \arg \min_{\pi} \mathcal{L}_{\text{SS}}(\pi) = \arg \min_{\pi} \mathcal{L}_{\text{RSP}}(\pi)$. Optimizing $\mathcal{L}_{\text{RSP}}(\pi^*)$ can lead to the same solution to the Internal Regularization objective.
% \end{proof}

% \subsection{Proof of Proposition \ref{theorem:rsp} (Bounding Approximation Error via adaptive regularization coefficient)}

% \begin{lemma}
% Denote response $y$, a sequence of LLM policies $\pi_t$, where $t=1, \cdots, T$, reference policy $\mu$. If the regularization function $g(y; \pi, \mu)$ is concave in $\pi$, then we have
% \begin{align}
% \frac{1}{T}\sum_{t=1}^T g(y; \pi_t, \mu) \leq  g(y; \frac{1}{T}\sum_{t=1}^T \pi_t, \mu). 
% \end{align}
% \label{lemma:jensen}                              
% \end{lemma}

% \begin{lemma}
% Define
% \begin{align}
% {\delta(y)} 
% & \overset{\text{def}}{=} \log \frac{\pi_{t+1}(y)}{\pi_t(y)}- \eta \Big(\mathbb{P}({y} \succ \pi_t) + \underbrace {\frac{\sqrt{\lambda}}{\eta }}_{\tau} g(y; \pi_{t+1}, \mu) - \log Z \Big).
% \end{align}
% Then $\forall \pi$, $\mathbb{E}_{\pi}[|\delta(y)|] \leq $
% \end{lemma}
% \begin{proof}
% $\forall y \in \mathcal{Y}$
% \begin{align}
% \delta^2(y) & = \Big[ \log \frac{\pi_{t+1}(y)}{\pi_t(y)}-  \Big( \eta \mathbb{P}({y} \succ \pi_t) + \sqrt{\lambda} g(y; \pi_{t+1}, \mu) - \log Z \Big) \Big]^2 \\ 
% & \leq  \Big(\log \frac{\pi_{t+1}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) +  \log Z \Big)^2 + \lambda g(y; \pi_{t+1}, \mu)^2  \\
% & = \mathcal{L}_{\text{SS}}(\pi^*)
% \end{align}
% $\forall \pi$
% \begin{align}
% \mathbb{E}_{\pi}[|\delta(y)|] &= \langle \pi, | \delta | \rangle \\
% &\leq \| \pi \|_1 \| \delta \|_{\infty} \label{eq:lemma_cs}
% \end{align}

% \end{proof}

% The policy updated with external regularization (Equation \ref{eq:rsp}) converges to a regularized Nash Equilibrium, if for some function $g: \mathcal{Y} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \rightarrow \mathbb{R}$:
% \begin{align}
% \nabla_{\theta}R(\pi_{\theta}) := \mathbb{E}_{\substack{y\sim \pi_{\theta}}}[ \nabla_{\theta}(g(y; \pi_{\theta}, \mu)^2)],
% \label{eq:reg}
% \end{align}

% \begin{proof}
    
% Define a sum of squares (SS) loss:
% \begin{align}
% \mathcal{L}_{\text{SS}}(\pi_\theta;t) \overset{\text{def}}{=} \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg].
% \end{align}
% A direct result of the condition in \Cref{eq:reg} is that optimizing $\mathcal{L}_{\text{RSP}}$ is equivalent to optimize $\mathcal{L}_{\text{SS}}$, since:
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{RSP}}(\pi_\theta;t) &= \nabla_{\theta}\mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 \Bigg] + \nabla_{\theta} \lambda R(\pi_{\theta})  \\
% &=\nabla_{\theta} \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg] \\
% & = \nabla_{\theta} \mathcal{L}_{\text{SS}}. 
% % \\
% % &=  \mathbb{E}_{\pi_t} \Bigg[ 2 \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big) \nabla_{\theta} \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big) \nonumber \\ 
% % &\quad\quad\quad\ \ \  +  2 g(y; \pi_\theta, \mu) \nabla_{\theta} g(y; \pi_\theta, \mu) \Bigg]
% \label{eq:rspisss}
% \end{align}
% The second equation holds due to \Cref{eq:reg} and $y\sim \pi_t \Leftrightarrow y \sim \pi_\theta$. According to \Cref{eq:rspisss}, $\forall t \in [T],\ \arg \min_\theta \mathcal{L}_{\text{RSP}}(\pi_\theta;t) = \arg \min_\theta \mathcal{L}_{\text{SS}}(\pi_\theta;t)$. Denote $\pi_{t+1} = \arg \min_{\pi} \mathcal{L}_{\text{SS}}(\pi;t)$, $\tau = \sqrt{\lambda} / \eta$, $\forall y \in \mathcal{Y}$, define
% \begin{align}
% {\delta(y)} 
% & \overset{\text{def}}{=} \log \frac{\pi_{t+1}(y)}{\pi_t(y)}-  \Big(\eta 
% \mathbb{P}({y} \succ \pi_t) + \underbrace {\frac{\sqrt{\lambda}}\eta }_{\tau} g(y; \pi_{t+1}, \mu) - \log Z \Big).
% \end{align}
% Then denote the regularized Nash Equilibrium by $\pi^*$, we have:
% \begin{align}
% & D_{\text{KL}}(\pi^* || \pi_{t+1}) - D_{\text{KL}}(\pi^* || \pi_{t}) \\
% & = \mathbb{E}_{\pi^*}\big[ \log \frac{\pi_t(y)}{\pi_{t+1}(y)}\big] \\
% & = \mathbb{E}_{\pi^*}\big[ -\eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu) \Big) + \log Z - \delta(y) \big] \\
% & = \mathbb{E}_{\pi^*}\big[ -\eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu)\Big) + \log \sum_{\tilde{y}} \pi_t(\tilde{y}) \exp \big\{ \eta \mathbb{P}({\tilde{y}} \succ \pi_t) + \eta \tau g(\tilde{y}; \pi_{t+1}, \mu)  \big\}  - \delta(y) \big] \\
% & \leq \mathbb{E}_{\pi^*}\big[ -\eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu)\Big) + \log \sum_{\tilde{y}} \pi_t(\tilde{y}) \big(1 - (1 - e^\eta) \mathbb{P}({\tilde{y}} \succ \pi_t) - (1 - e^\eta) \tau g(\tilde{y}; \pi_{t+1}, \mu)  \big) - \delta(y) \big] \label{eq:one_step_ineq} \\
% & \leq \mathbb{E}_{\pi^*}\big[ - \eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu) \Big) \big] +  \log \big(1 - (1 - e^\eta) \mathbb{P}(\pi_t \succ \pi_t) - (1 - e^\eta) \tau g(\pi_t; \pi_{t+1}, \mu)  \big)  - \mathbb{E}_{\pi^*}[\delta(y)] 
%  \\
% & \leq \mathbb{E}_{\pi^*}\big[ - \eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu) \Big) \big] -    (1 - e^\eta) \mathbb{P}(\pi_t \succ \pi_t) -  (1 - e^\eta) \tau g(\pi_t; \pi_{t+1}, \mu)  - \mathbb{E}_{\pi^*}[\delta(y)].
% \label{eq:one_step_ineq_2}
% \end{align}
% \Cref{eq:one_step_ineq} holds due to $\beta^x \leq 1-(1-\beta)x$, where $\beta = \exp \eta,\ x= \mathbb{P}(\tilde{y} \succ \pi_t) + \tau g(\tilde{y}; \pi_{t+1}, \mu)$, if $x \in [0, 1)$, which can be easily satisfied by choosing proper $\lambda$. \Cref{eq:one_step_ineq_2} holds due to when $x<1$, $\log(1-x) \leq -x$, where $x=(1 - e^\eta)\mathbb{P}({y} \succ \pi_t) + (1 - e^\eta)\tau g(\tilde{y}; \pi_{t+1}, \mu)$.
% Summing the above inequality in \Cref{eq:one_step_ineq_2} over $t=0,\cdots, T$:
% \begin{align}
% &D_{\text{KL}}(\pi^* || \pi_{T+1}) - D_{\text{KL}}(\pi^* || \pi_{0}) \\
% & = \sum_{t=0}^T \mathbb{E}_{\pi^*}\big[ -\eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu) \Big) \big] -    (1 - e^\eta) \mathbb{P}(\pi_t \succ \pi_t) -  (1 - e^\eta) \tau g(\pi_t; \pi_{t+1}, \mu)  - \mathbb{E}_{\pi^*}[\delta(y)]. \\
% & = \sum_{t=0}^T \mathbb{E}_{\pi^*}\big[ -\eta \Big(\mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu) \Big) \big] -   (1 - e^\eta) \sum_{t=0}^T \big( \mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big)  - \sum_{t=0}^T \mathbb{E}_{\pi^*}[\delta(y)] .
% \label{eq:mwu_sum}
% \end{align}

% Rearrange \Cref{eq:mwu_sum}, since $D_{\text{KL}}(\pi^* || \pi_{T+1}) \geq 0$, we have
% \begin{align}
% &\sum_{t=0}^T \frac{(1 - e^\eta)}{-\eta} \big(\mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big) \\
% & \geq  \sum_{t=0}^T \mathbb{E}_{\pi^*} \big[  \mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu)  \big]  - \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta} + \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta}. 
% \end{align}
% Therefore,
% \begin{align}
% &\mathbb{P}(\pi^* \succ \bar{\pi}_T) + {\tau} g(\pi^*; \pi^*, \mu) - {\tau} g(\bar{\pi}_T; \bar{\pi}_T, \mu) \\
% & =  \frac{1}{T} \sum_{t=0}^T  \mathbb{E}_{\pi^*} \big[  \mathbb{P}({y} \succ \pi_t) + {\tau} g(y; \pi_{t+1}, \mu)  \big] + \mathbb{E}_{\pi^*} \big[ -{\tau} g(y; \pi_{t+1}, \mu) + {\tau} g(y; \pi^*, \mu) - {\tau} g(\bar{\pi}_T; \bar{\pi}_T, \mu) \big] \\
% & \leq \sum_{t=0}^T \frac{(1 - e^\eta)}{-\eta T} \big(\mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big)  + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T}  \nonumber \\ & \quad \quad \quad +\mathbb{E}_{\pi^*} \big[ -{\tau} g(y; \pi_{t+1}, \mu) + {\tau} g(y; \pi^*, \mu) - {\tau} g(\bar{\pi}_T; \bar{\pi}_T, \mu) \big] \\ 
% & \leq \sum_{t=0}^T \frac{(1 - e^\eta)}{-\eta T}   \big(\mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big) + \sum_{t=0}^T \frac{\tau}{T} [ - g(\bar{\pi}_T; \bar{\pi}_T, \mu) + g(\pi^*; \pi^*, \mu) - g(\pi^*; \pi_{t+1}, \mu) ] \nonumber  \\ 
% & \quad \quad + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T} \\
% & =  \sum_{t=0}^T  \frac{1 + \eta/2 + \mathcal{O}(\eta^2)}{ T} \big(\mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big) + 
%  \frac{\tau}{T} \sum_{t=0}^T - g(\bar{\pi}_T; \bar{\pi}_T, \mu) + g(\pi^*; \pi^*, \mu) - g(\pi^*; \pi_{t+1}, \mu)  \\ 
% & \quad \quad + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T} \\
% & \leq  \sum_{t=0}^T  \frac{1 + \eta/2 + \mathcal{O}(\eta^2)}{ T} \big(\mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu)  - \tau g(\bar{\pi}_T; \bar{\pi}_T, \mu) + \tau g(\pi^*; \pi^*, \mu) - \tau g(\pi^*; \pi_{t+1}, \mu) \big)  \\ 
% & \quad \quad + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T} \\
% & =  \sum_{t=0}^T  \frac{1 + \eta/2 + \mathcal{O}(\eta^2)}{ T} \big(\frac{1}{2} + \tau g(\pi_t; \pi_{t+1}, \mu)  - \tau g(\bar{\pi}_T; \bar{\pi}_T, \mu) + \tau g(\pi^*; \pi^*, \mu) - \tau g(\pi^*; \pi_{t+1}, \mu) \big)  \\ 
% & \quad \quad + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T}
% % & \leq \frac{1 + \eta/2 + \mathcal{O}(\eta^2)}{ T} \sum_{t=0}^T  \big( \mathbb{P}(\pi_t \succ \pi_t) + \tau g(\pi_t; \pi_{t+1}, \mu) \big) + \tau  g(\pi^*; \pi^*, \mu)  + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T} \\
% % & \leq \big( {1 + \eta/2 + \mathcal{O}(\eta^2)}\big) \cdot  \big(\frac{1}{2} + \tau g(\bar{\pi}_T; \bar{\pi}_T, \mu) \big) + \tau  g(\pi^*; \pi^*, \mu)  + \frac{D_{\text{KL}}(\pi^* || \pi_{0})}{\eta T} - \sum_{t=0}^T \frac{\mathbb{E}_{\pi^*}[\delta(y)]}{\eta T} \label{eq:jensen}
% \end{align}
% % \Cref{eq:jensen} holds due to Lemma \ref{lemma:jensen} and the fact that $\mathbb{P}(\pi_t \succ \pi_t)=1/2$.

% % The policy still follows an exponential update:
% % \begin{align}
% % \pi_{t+1}(y) = \pi_t(y) \cdot \exp \Bigg\{ \eta \Big(\mathbb{P}({y} \succ \pi_t) - {\tau}_t g(y; \pi_{t+1}, \mu) - \log Z\Big) - \delta_t \Bigg\}
% % \end{align}
% \end{proof}

% \subsection{Proof of Proposition \ref{theorem:rsp} (Bounding Approximation Error)}
% The policy updated with external regularization (Equation \ref{eq:rsp}) converges to the same solution to the IR-SP, if for some function $g: \mathcal{Y} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \times \Delta_{\mathcal{Y}}^{\mathcal{X}} \rightarrow \mathbb{R}$:
% \begin{align}
% \nabla_{\theta}R(\pi_{\theta}) := \mathbb{E}_{\substack{y\sim \pi_{\theta}}}[ \nabla_{\theta}(g(y; \pi_{\theta}, \mu)^2)],
% \end{align}

% \begin{proof}
    
% Define a sum of squares (SS) loss:
% \begin{align}
% \mathcal{L}_{\text{SS}}(\pi_\theta;t) = \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + g(y; \pi_\theta, \mu)^2 \Bigg].
% \end{align}

% A direct result of the condition in \Cref{eq:reg} is that optimizing $\mathcal{L}_{\text{RSP}}$ is equivalent to optimize $\mathcal{L}_{\text{SS}}$, since:
% \begin{align}
% \nabla_{\theta} \mathcal{L}_{\text{RSP}}(\pi_\theta;t) &= \nabla_{\theta}\mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 \Bigg] + \nabla_{\theta} \lambda R(\pi_{\theta})  \\
% &=\nabla_{\theta} \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg] = \nabla_{\theta} \mathcal{L}_{\text{SS}}. 
% % \\
% % &=  \mathbb{E}_{\pi_t} \Bigg[ 2 \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big) \nabla_{\theta} \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big) \nonumber \\ 
% % &\quad\quad\quad\ \ \  +  2 g(y; \pi_\theta, \mu) \nabla_{\theta} g(y; \pi_\theta, \mu) \Bigg]
% \label{eq:rspisss}
% \end{align}
% The second equation holds due to \Cref{eq:reg} and $y\sim \pi_t \Leftrightarrow y \sim \pi_\theta$. According to \Cref{eq:rspisss}, $\arg \min_\theta \mathcal{L}_{\text{RSP}}(\pi_\theta;t) = \arg \min_\theta \mathcal{L}_{\text{SS}}(\pi_\theta;t)$. When executing Algorithm \ref{alg:selfplay} using $\mathcal{L}_{\text{SS}}$ is converged in iteration $T+1$, and $\pi_{T+1} = \pi_{T}$. Assume $\min \mathcal{L}_{\text{SS}}(\pi_\theta;T) = \delta$, we have
% \begin{align}
% \delta &= \min \mathcal{L}_{\text{SS}}(\pi_\theta;T) \\
% &= \mathbb{E}_{\pi_T} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_T(y)}- \eta \mathbb{P}({y} \succ \pi_T) + \log Z \Big)^2 + \lambda g(y; \pi_\theta, \mu)^2 \Bigg]  \\
% &\geq  \mathbb{E}_{\pi_T} \Bigg[ \log \frac{\pi_{T+1}(y)}{\pi_T(y)}- \eta \Big(\mathbb{P}({y} \succ \pi_T) - \underbrace {\frac{\lambda}{\eta }}_{\tau} g(y; \pi_{T+1}, \mu) - \log Z\Big) \Bigg]^2 / 2 \label{eq:cs} \\
% &=  \mathbb{E}_{\pi_T} \Bigg[ \log \frac{\pi_{T+1}(y)}{\pi_T(y)}- \eta \Big(\mathbb{P}({y} \succ \pi_T) - \underbrace {\frac{\lambda}{\eta }}_{\tau} g(y; \pi_{T}, \mu) - \log Z\Big) \Bigg]^2 / 2.
% % \\ &= \frac{1}{2} \mathcal{L}_{\text{IR-SP}}.
% \end{align}

% \begin{align}
% & \log \frac{\pi_{t+1}(y)}{\pi_{t}(y)}- \eta \Big(\mathbb{P}({y} \succ \pi_{t}) - \underbrace {\frac{\lambda}{\eta }}_{\tau} g(y; \pi_{t}, \mu) - \log Z\Big) = \delta_{t} \\
% & \frac{\pi_{t+1}(y)}{\pi_{t}(y)} = \exp \Bigg\{\eta \Big(\mathbb{P}({y} \succ \pi_{t}) - {\tau} g(y; \pi_{t}, \mu) - \log Z\Big)  +\delta_{t} \Bigg\} \\
% & {\pi_{t+1}(y)} \propto {\pi_{t}(y)} \cdot \exp \Bigg\{\eta \Big(\mathbb{P}({y} \succ \pi_{t}) - {\tau} g(y; \pi_{t}, \mu) - \log Z\Big)  +\delta_{t} \Bigg\}
% \end{align}

% Next, we show that executing Algorithm \ref{alg:selfplay} using loss $\mathcal{L}_{\text{RSP}}$ for policy update can approximate updating with loss  $\mathcal{L}_{\text{IR-SP}}$. According to \Cref{eq:cs}, 
% \end{proof}


\subsection{Proof of Proposition \ref{theo:reverse_kl}}
\label{sec:reverse_kl_square}

% Since $\nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu)=  \mathbb{E}_{\pi_{\theta}}[ \nabla_{\theta}\log {\pi_{\theta}(y)}/{\mu(y)}]^2/2$, when $R=D_{\text{KL}}(\pi_{\theta}||\mu)$, RSPO converges to the Nash Equilibrium of preference model:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{\text{KL}}(\pi||\mu) +\tau D_{\text{KL}}(\pi'||\mu).
% \end{align}
\begin{proof}
$\pi$ is parametrized by $\theta$, $\nabla_{\theta} D_{\text{KL}}(\pi||\mu)=  \mathbb{E}_{\pi_{\theta}}[ \nabla_{\theta}\log \pi_{\theta}(y) - \log \mu(y)]^2/ 2$. This is because
\begin{align}
\nabla_{\theta} D_{\text{KL}}(\pi||\mu)
&= \nabla_{\theta} \sum_y \pi_{\theta}(y) \cdot (\log \pi_{\theta}(y) - \log \mu(y)) =\sum_y \nabla_{\theta} \pi_{\theta}(y) \cdot (\log \pi_{{\theta}}(y) - \log \mu(y)) + \sum_y\nabla_{\theta} \pi_{\theta}(y) \nonumber \\
&=\sum_y  \pi_{{\theta}}(y) \frac{\nabla_{\theta} \pi_{\theta}(y)}{ \pi_{{\theta}}(y)} \cdot (\log \pi_{{\theta}}(y) - \log \mu(y)) + \nabla_{\theta} \sum_y\pi_{\theta}(y) \nonumber \\
&=\mathbb{E}_{\pi_{{\theta}}} [(\log \pi_{{\theta}}(y) - \log \mu(y)) \cdot \nabla_{\theta} (\log \pi_{\theta}(y) - \log \mu(y))]  \nonumber \\
&= \mathbb{E}_{\pi_{{\theta}}} [\nabla_{\theta}(\log \pi_{\theta}(y) - \log \mu(y))^2] / 2. \label{eq: reverse_kl_result}
\end{align}
The first equation holds the following directly from the definition of the KL divergence. The second equation holds due to applying the product rule of differentiation. The third equation holds due to simple algebra, and the second term will then vanish because of the sum of the probabilities. The fourth equation holds because of simple algebra.
% Therefore
% According to Proposition \ref{theorem:rsp}, when $R$ is the backward KL divergence, RSPO converges to the same regularized Nash Equilibrium as IR-SP:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') - \mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] + \mathbb{E}_{y' \sim \pi'}[g(y'; \pi', \mu)],
% \end{align}
% where regularization function $g$ is $\log \pi(y) - \log \mu(y)$, and thus $\mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)]$ is backward KL divergence as well.
\end{proof}

% \textcolor{red}{
% Seongho: It seems Eq. 64 is using 
% }

% \begin{align}
%     \mathbb{E}_{\pi_t}[ \nabla_{\theta}\log \pi_{\theta}(y) - \log \mu(y)]^2/ 2 = \nabla_{\theta}\mathbb{E}_{\pi_t}[ \log \pi_{\theta}(y) - \log \mu(y)]^2/ 2.
% \end{align}

% \textcolor{red}{However,}
% \begin{align}
%     \nabla_{\theta} D_{\text{KL}}(\pi||\mu)&=  \mathbb{E}_{\pi_{\theta}}[ \nabla_{\theta}\log \pi_{\theta}(y) - \log \mu(y)]^2/ 2 &\neq \nabla_{\theta}\mathbb{E}_{\pi_{\theta}}[ \log \pi_{\theta}(y) - \log \mu(y)]^2/ 2, \\
%     D_{\text{KL}}(\pi||\mu) &= \int \tfrac{1}{2}\mathbb{E}_{\pi_\theta}[ \nabla_{\theta}\log \pi_{\theta}(y) - \log \mu(y)]^2d\theta &\neq \int \tfrac{1}{2}\mathbb{E}_{\pi_t}[ \nabla_{\theta}\log \pi_{\theta}(y) - \log \mu(y)]^2d\theta.
% \end{align}
% \textcolor{red}{I think the derivation from Eq. 91 to Eq. 95 is correct. However, I am not sure if Eq. 63-66 is compatible with Eq. 95.}


\subsection{Proof of Proposition \ref{theo:forward_kl}}
% Since $\nabla_{\theta} D_{\text{KL}}(\mu||\pi)=  \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta} {\mu(y)}/{\pi_{\theta}(y)}]$, RSPO with $R=D_{\text{KL}}(\mu||\pi)$ converges to the Nash Equilibrium of regularized preference as
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{\text{B}}(\pi||\mu) +\tau D_{\text{B}}(\pi'||\mu),
% \label{eq:forward_kl_rsp}
% \end{align}
% where $D_{\text{B}}(\pi_{\theta}||\mu)=\sum_{y \in \mathcal{Y}} \pi_{\theta}(y) \sqrt{\frac{\mu(u)}{\pi_{\theta}(y)}}$ is the Bhattacharyya Distance \citep{bhattacharyya1946measure}.
\begin{proof}
$\pi$ is parametrized by $\theta$, then $\nabla_{\theta} D_{\text{KL}}(\mu||\pi)=  \mathbb{E}_{\mu}[\nabla_{\theta} \frac{\mu(y)}{\pi_{\theta}(y)}]$ because
\begin{align}
\nabla_{\theta} D_{\text{KL}}(\mu||\pi)
&= \nabla_{\theta} \sum_y \mu(y) \cdot (\log \mu(y) - \log \pi_{\theta}(y)) = -\sum_y \mu(y) \nabla_{\theta} \log \pi_{\theta}(y) = -\sum_y \pi_{\theta}(y) \frac{\mu(y)}{\pi_{\theta}(y)} \nabla_{\theta} \log \pi_{\theta}(y) \nonumber \\
&= -\mathbb{E}_{\pi_{\theta}}\bigg[\frac{\mu(y) \nabla_{\theta} \log \pi_{\theta}(y)}{\pi_{\theta}(y)}\bigg] =  -\mathbb{E}_{\pi_{\theta}}\bigg[\frac{\mu(y) \nabla_{\theta} \pi_{\theta}(y)}{\pi_{\theta}(y)^2}\bigg] =   \mathbb{E}_{\pi_{\theta}}\bigg[\nabla_{\theta} \frac{\mu(y)}{\pi_{\theta}(y)}\bigg].
\end{align}
The first three equations hold due to the definition of forward KL divergence and simple algebra. The fourth equation comes from rewriting the forward KL following the first three equations. The fifth equation holds by taking the derivative of $\log \pi_\theta$. The sixth equation holds since $\frac{ \nabla_{\theta} \pi_{\theta}(y)}{\pi_{\theta}(y)^2}= \nabla_{\theta} \frac{-1}{\pi_{\theta}(y)}$.
% According to Proposition \ref{theorem:rsp}, when $R$ is the forward KL divergence, RSPO converges to the same regularized Nash Equilibrium as IR-SP:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') - \mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] + \mathbb{E}_{y' \sim \pi'}[g(y'; \pi', \mu)],
% \end{align}
% where $g = \sqrt{\frac{\mu(u)}{\pi(y)}}$, and $\mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] = \sum_{y \in \mathcal{Y}} \pi(y) \sqrt{\frac{\mu(u)}{\pi(y)}} =  D_{\text{B}}(\pi||\mu)$ is the Bhattacharyya Distance.
\end{proof}

\subsection{Proof of Proposition \ref{theo:chisquare}}
% Since $\nabla_\theta D_{\chi^2}(\pi_\theta||\mu)=\mathbb{E}_{\pi_\theta}\left[{\nabla_\theta \pi_\theta(y)}/{\mu(y)}\right]$,
% so RSPO with $R=D_{\chi^2}(\mu||\pi)$ converges to the Nash Equilibrium of regularized preference as
% \begin{align}
% \mathbb{P}(\pi \succ \pi') -\tau D_{f}(\pi||\mu) +\tau D_{f}(\pi'||\mu),
% \end{align}
% where $D_{f}$ is $f$-divergence with $f(r)=r^{3/2}$.
\begin{proof}
$\pi$ is parametrized by $\theta$, $\nabla_\theta D_{\chi^2}(\pi_\theta(y)||\mu(y))=\mathbb{E}_{\pi_\theta}\left[\frac{\nabla_\theta \pi_\theta(y)}{\mu(y)}\right]$ since
\begin{align}
D_{\chi^2}(\pi_\theta(y)||\mu(y))&=\frac{1}{2}\sum_y \left(\frac{\pi_\theta(y)}{\mu(y)}-1\right)^2 \mu(y) =\frac{1}{2}\sum_y \frac{\pi_\theta(y)^2 - 2\pi_\theta(y)\mu(y) + \mu(y)^2}{\mu(y)} \nonumber \\
&=\frac{1}{2}\sum_y \frac{\pi_\theta(y)^2}{\mu(y)}+ C(\mu) =\frac{1}{2}\mathbb{E}_{\pi_\theta(y)}\left[\frac{\pi_\theta(y)}{\mu(y)}\right]+C,
\end{align}
where $C(\mu)$ is independent to $\theta$. The first two equations hold according to the definition of Chi-squared divergence. The third equation holds by separating the terms only related to $\mu$ and the term related to $\pi_\theta$. The fourth equation holds by rewriting the summation as the expectation.
% Thus, according to Proposition \ref{theorem:rsp}, when $R$ is the Chi-Square divergence, RSPO converges to the same regularized Nash Equilibrium as IR-SP:
% \begin{align}
% \mathbb{P}(\pi \succ \pi') - \mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] + \mathbb{E}_{y' \sim \pi'}[g(y'; \pi', \mu)],
% \end{align}
% where $g = \sqrt{\frac{\pi(y)}{\mu(u)}}$, and $\mathbb{E}_{y \sim \pi}[g(y; \pi, \mu)] = \sum_{y \in \mathcal{Y}} \pi(y) \sqrt{\frac{\pi(y)}{\mu(u)}} =  D_{f}(\pi||\mu)$ is the $f$-Divergence with $f(r) = r^{3/2}$.
\end{proof}

\section{Additional Details}
In this section, we provide additional details of this paper, including the algorithm descriptions of self-play alignment methods, a summarizing table for generalizing existing methods, and our implementation of regularizations.
\subsection{Self-Play Alignment Algorithm} 
\begin{algorithm}[H]
\caption{Self-Play Alignment}
\label{alg:selfplay}
\begin{algorithmic}
  \STATE \textbf{Input:} LLM $\pi_\theta$, preference model ${\mathbb{P}}$, number of iterations $T$, reference policy $\mu$, loss function for policy update $\mathcal{L}(\theta; \mathbb{P})$, sample size $K$.
  \STATE \textbf{Initialize:}  $\pi_0 =\mu$.   
  \FOR{$t \in [T]$}
    \STATE Sample prompts and responses: $x \sim \mathcal{X}$, $y_{1:K} \sim \pi_t$
    \STATE Get pair-wise preferences ${\mathbb{P}}(y_i \succ y_j),\ \forall i,j \in [K]$
    \STATE Update policy parameters $\theta= \arg \min_\theta \mathcal{L}(\theta; \mathbb{P})$
    \STATE $\pi_{t+1} = \pi_\theta$
  \ENDFOR
  \STATE \textbf{Output:} Last-iterate policy $\pi_T$.
\end{algorithmic}
\end{algorithm}

\subsection{Generalizing Existing Methods}
\begin{table*}[h]
    \centering
    \begin{tabular}{c|c|c|c} 
\toprule
Loss & Update Direction ($G$) & Baseline ($B$) & Preference Model  \\ 
\midrule
% $\mathcal{L}_\text{SPO}$ \citep{swamy2024minimaximalist} & $\mathbb{P}(y \succ \pi_t)$ &  Est. &  $\mathbb{P}(y \succ y')$ \\
$\mathcal{L}_\text{SPPO}$ \citep{wu2024self} & $\mathbb{P}(y \succ \pi_t)$ &  $0.5$ &  $\mathbb{P}(y \succ y')$ \\
$\mathcal{L}_\text{OMD}$ \citep{munos2023nash} & $\mathbb{P}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)}$ & Est. & $\mathbb{P}_{\tau}(y \succ y')$  \\
$\mathcal{L}_\text{Nash-MD}$ \citep{munos2023nash} &$\mathbb{P}^{\mu}(y \succ \pi_t) - \tau \log \frac{\pi_t(y)}{\mu(y)}$ & $0.5$ & $\mathbb{P}_{\tau}(y \succ y')$  \\
 % $\mathcal{L}_\text{RPM}$ & $\mathbb{P}_{\tau}(y \succ \pi_t)$ &  Est.  & $\mathbb{P}_{\tau}(y \succ y')$ \\
% \midrule
% $\mathcal{L}_\text{MaxEnt} := \mathcal{L}_\text{MSE} + \tau' \mathcal{H}(\pi_{\theta}) $ & $\mathbb{P}$ &  $0.5$ &  Entropy  \\
% $\mathcal{L}_\text{reverse} := \mathcal{L}_\text{MSE} + \tau' D_{\text{KL}}(\pi_{\theta}||\mu) $  & $\mathbb{P}$ &  $0.5$ & Reverse KL  \\
% $\mathcal{L}_\text{forward} := \mathcal{L}_\text{MSE} + \tau' D_{\text{KL}}(\mu||\pi_{\theta})$  & $\mathbb{P}$ &  $0.5$ & Forward KL  \\
\bottomrule
\end{tabular}
\caption{Self-play losses $\mathcal{L}_{\text{RSPO}}$ generalizes different self-play policy optimization methods. $\mathbb{P}^{\mu}(y \succ \pi_t) = \mathbb{P}(y \succ \pi^{\mu}_t)$, $\pi^{\mu}_t$ is the geometric mixture of $\pi_t$ and $\mu$. We abbreviate the estimated baseline that reduce the variance of $G$ the most as est.. $\mathbb{P}_{\tau}(y \succ y') = \mathbb{P}(y \succ y') -\tau \log \frac{\pi_{\theta}(y)}{\mu(y)} + \tau \log \frac{\pi'(y')}{\mu(y')}$ is the regularized preference model.}
\label{table:main}
\end{table*}

\subsection{Implementation of Regularization}
\label{sec:implement_reg}
In practice, accurately estimating the gradient of the regularizer is essential, as many commonly used divergence measures are defined as expectations over $\pi_\theta$. The estimation of divergences has been extensively studied and widely applied in various domains \citep{rubenstein2019practical}. For completeness, in this section, we introduce the regularization methods investigated in this study, including Reverse KL, Forward KL, and Chi-Square Divergence.

We begin by deriving the estimation of the Reverse KL divergence based on the following proposition.
\begin{proposition}
Reverse KL divergence satisfies:
\begin{align}
\nabla_\theta D_{\textit{KL}}(\pi_\theta||\mu)=\mathbb{E}_{y \sim \pi_\theta}[{\nabla_\theta (\log \pi_\theta(y) - \log \mu(y))^2}].
\end{align}
\label{theo:reverse_kl}
\end{proposition}
% \vspace{-.5cm}
Due to the equivalent gradient in Proposition \ref{theo:reverse_kl}, we can estimate the divergence with $\mathbb{E}_{y \sim \pi_\theta}[{(\log \pi_\theta(y) - \log \mu(y))^2}]$.

We employ two distinct approaches to estimate the forward KL divergence. The first method utilizes importance sampling, referred to as IS-For. KL, and is derived based on the following proposition.
\begin{proposition} The gradient of forward KL divergence satisfies that 
\begin{align}
\nabla_{\theta} D_{\text{KL}}(\mu||\pi_\theta)=  \mathbb{E}_{y \sim \pi_{\theta}}[\nabla_{\theta} {\mu(y)}/{\pi_{\theta}(y)}].
\end{align}
\label{theo:forward_kl}
\end{proposition}
Therefore, we can estimate the forward KL divergence by leveraging the expectation $\mathbb{E}_{y \sim \pi_{\theta}}[{\mu(y)}/{\pi_{\theta}(y)}]$ to estimate the forward KL. Notably, to mitigate the risk of gradient explosion, we apply gradient clipping with a maximum value of $10$.

The second method for forward KL is a direct estimation of $D_{\text{KL}}(\mu||\pi_\theta)$. To achieve this, we resample responses from the reference policy $\mu$ using the same prompts from the training dataset, constructing a reference dataset. The KL divergence is then estimated directly based on its definition by uniformly drawing samples from this reference dataset. A key advantage of this approach is that it eliminates the need for importance sampling, as each policy update iteration only requires samples from $\pi_t$.
% \vspace{-.1cm}

Similarly, we estimate the Chi-Square divergence using $\mathbb{E}_{y \sim \pi_\theta}\left[{\pi_\theta(y)}/{\mu(y)}\right]$, based on the following proposition. Due to the presence of the ratio term, Chi-Square divergence estimation also necessitates gradient clipping to prevent instability, for which we set a clip value of $10$.
\begin{proposition} Chi-Square divergence has gradient \begin{align}
\nabla_\theta D_{\chi^2}(\pi_\theta||\mu)=\mathbb{E}_{y \sim \pi_\theta}\left[{\nabla_\theta \pi_\theta(y)}/{\mu(y)}\right].
\end{align}
\label{theo:chisquare}
\end{proposition}
% \vspace{-.5cm}
We also explore the linear combination of different regularization functions to leverage their complementary effects, as in offline RLHF \citep{huang2024correcting}. The previously established propositions for estimating divergences can still be used in the combined regularization method.
% As demonstrated in our experiments (Section \ref{sec:main_reg}), the combination of reverse and forward KL divergences yields the best performance.

Apart from the flexibility and simplicity of applying different regularization methods, RSPO can generalize existing self-play methods, including the unregularized ones, which enables regularizing off-the-shelf self-play methods in practice with \textit{no change} on their original loss functions or hyperparameters, directly adding an external regularization term to their loss functions.


\section{Additional Experiments}
In this section, we provide additional experiments, including two synthetic motivating examples and additional results on language tasks.
\subsection{Regularization in Game Solving}
\label{append:reg_game}

\begin{figure}[H]
    \centering
    \vspace{-.6cm}
    \includegraphics[width=.4\linewidth]{figures/simple_toy_2.pdf}
    \caption{Motivating Example: 20 iterations of MWU and regularized MWU with the same learning rate to solve saddle point problem $ \max_y \min_{y'} f(y, y', \alpha)$, where $f(y, y'; \alpha)=\frac{\alpha}{2}{y'}^2 + (y'-1)(y-1) - \frac{\alpha}{2}y^2$, first introduced in \citep{sokota2022unified}. We assume that we only have access to a misspecified (surrogate) preference $f(y, y'; \alpha=1)$, while the ground truth human preference is $f(y, y'; \alpha=2)$. }
    \label{fig:simpletoy}
    \vspace{-.5cm}
\end{figure}

The regularization in the preference model is not used in all game-theoretic self-play methods. Here we investigate the necessity of regularization and offer a motivating example in Figure \ref{fig:simpletoy}, a saddle point solving problem $\min_x \max_y \frac{\alpha}{2} x^2 + (x-1)(y-1) - \frac{\alpha}{2}  y^2$. There exists a reference point as the initial values of $x$ and $y$. We assume that both reference point and the Nash Equilibrium (NE) of the surrogate preference model (Surrogate NE) are close to the original NE but on different sides of the original NE. 



Typically, the surrogate preference/reward models are not positively related to the reference policy. Thus, it is a reasonable abstracted example of NLHF by treating the reference point as reference policy and surrogate NE as the optimal policy obtained by optimizing the surrogate preference/reward. The results of the $20$ iterations self-play MWU with an early stopping show that regularization can be used to prevent reward over-optimization (reaching surrogate NE). A well-tuned regularization leads to faster convergence to the unknown original NE. Thus, regularization can be effective in preventing over-optimization in self-play.


\subsection{Diversity on 2D Example}
\label{append:2d_diversity}
We offer an analysis of our method compared to unregularized self-play (SPPO) on a 2D example in Figure \ref{fig:RSPO_toy}. The area with a darker color is assigned a higher reward value. We use the preference defined by the $L^2$ norm between two actions. We also set the reference policy to be uniform. According to the figure, the unregularized method tends to converge to a single point on the manifold of the large reward. While regularized methods have diverse sampled actions.    
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/sppo_toy.pdf}
    \includegraphics[width=\linewidth]{figures/rsppo_toy.pdf}
    \caption{Samples in a 2D example of different iterations of SPPO (top) and RSPO (bottom) with external forward KL regularization to a uniform random reference policy. SPPO added simple external regularization that can generate multi-modal policies.}
    \label{fig:RSPO_toy}
    \vspace{-.4cm}
\end{figure*} 

\subsection{More Results on AlpacaEval-2.0}
\begin{figure}[H]
% \vspace{-1cm}
    \centering
    % \includegraphics[width=.33\linewidth]{figures/RSPO_all.pdf}
    \includegraphics[width=.48\linewidth]{figures/RSPO_all_wr.pdf}
    % \includegraphics[width=\linewidth]{figures/RSPO_all.pdf}
    \includegraphics[width=.48\linewidth]{figures/RSPO_all_length.pdf}
    \caption{Win rates and the average length of SPPO and RSPO with different regularization methods. From left to right, regularization methods: Reverse KL, Forward KL, Chi-Squared, Importance-Sampling Forward KL, Importance-Sampling Forward, and Reverse KL linear combination.}
    \label{fig:RSPO_all_reg_append}
    % \vspace{-2cm}
\end{figure}

% \begin{theorem}
% Assume $\forall y \sim \pi_t, \mu(y)>0$. Then  $\arg \min_{\theta} \mathcal{L}_{\text{RSP}}(R= D_{\text{KL}}(\pi||\mu)) = \arg \min_{\theta} \mathcal{L}_{\text{IR-SP}}$.
% \end{theorem} 
% \begin{proof}
% \begin{align*}
% \mathcal{L}_{\text{RSP}}(R= D_{\text{KL}}(\pi||\mu)) &= \mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 \Bigg] + \tau' D_{\text{KL}}(\pi_{\theta}||\mu)  \\
% &=\mathbb{E}_{\pi_t} \Bigg[ \Big(\log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \mathbb{P}({y} \succ \pi_t) + \log Z \Big)^2 + \frac{\tau'}{2} [\log \pi_{\theta}(y) - \log \mu(y)]^2 \Bigg] \\
% &\geq \mathbb{E}_{\pi_t} \Bigg[ \log \frac{\pi_{\theta}(y)}{\pi_t(y)}- \eta \Big(\mathbb{P}({y} \succ \pi_t) - \underbrace {\frac{\sqrt{2\tau'}}{2\eta }}_{\tau} \log \frac{\pi_{\theta}(y)}{\mu(y)} - \log Z\Big) \Bigg]^2 / 2 = \mathcal{L}_{\text{RPM}}/ 2.
% \end{align*}
% Since $\forall \pi,\ \mathcal{L}_{\text{RPM }}(\pi)/2 \leq \mathcal{L}_{\text{reverse}}(\pi)$, when $\pi^*=\arg \min \mathcal{L}_{\text{RPM}}$ at iteration $t$, $\forall y:$
% \begin{align}
% \log \frac{\pi^*(y)}{\pi_t(y)}- \Big(\mathbb{P}({y} \succ {\pi}_t) - \log Z\Big) = {\tau} \log \frac{\pi^*(y)}{\mu(y)}.
% \label{eq:equal_condition}
% \end{align} 
% According to the condition of equality in Cauchy inequality, Eq. (\ref{eq:equal_condition}) implies that
% \begin{align}
% \mathcal{L}_{\text{RPM}}(\pi^*)/2 = \mathcal{L}_{\text{reverse}}(\pi^*). 
% \end{align}
% Therefore, $\forall \pi, \ \mathcal{L}_{\text{reverse}}(\pi^*) = \mathcal{L}_{\text{RPM}}(\pi^*)/2  \leq \mathcal{L}_{\text{reverse}}(\pi)$, i.e. $\pi^* = \arg \min \mathcal{L}_{\text{reverse}}(\pi)$. Therefore, optimizing $\mathcal{L}_{\text{reverse}}(\pi^*)$ can also lead to the same solution to the RPM objective.
% \end{proof}

% \subsection{Checking: reverse KL derivation (\Cref{sec:reverse_kl_square})}

% Here, we check if $\nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu)=  \mathbb{E}_{\pi_{\theta}}[ \nabla_{\theta}\log {\pi_{\theta}(y)}/{\mu(y)}]^2/2$.

% \begin{lemma}
% \begin{align}
% \nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu) = \mathbb{E}_{\pi_\theta}[\log \tfrac{\pi_\theta(y)}{\mu(y)} \cdot \nabla_\theta \log \pi_\theta(y)].
% \end{align}
% \end{lemma}

% \textcolor{blue}{
% Xiaohang:
% \begin{align}
% \mathbb{E}_{\pi_\theta}[\log \tfrac{\pi_\theta(y)}{\mu(y)} \cdot \nabla_\theta \log \pi_\theta(y)] = \mathbb{E}_{\pi_\theta}[(\log \pi_\theta(y) - \log \mu (y)) \cdot \nabla_\theta (\log \pi_\theta(y) - \log \mu (y))].
% \label{eq:kl_temp}
% \end{align}
% Let $A(\theta) = (\log \pi_\theta(y) - \log \mu (y))$, then $A(\theta)\cdot \nabla_{\theta} A(\theta) = A(\theta)^2/2$, therefore Eq. \ref{eq:kl_temp} is equal to:
% \begin{align}
% \mathbb{E}_{\pi_\theta}[ \nabla_\theta (\log \pi_\theta(y) - \log \mu (y))^2]/2
% \end{align}
% }

% \textcolor{red}{ Seongho:
%     \begin{align}
%         \nabla_\theta A(\theta) &= \nabla_\theta(\log \pi_\theta(y) - \log\mu(y)) = \tfrac{\nabla_\theta \pi_\theta(y)}{\pi_\theta(y)} \\
%         \nabla_\theta (A(\theta))^2 &= 2
%     \end{align}

% }

% \begin{proof}
% \begin{align}
%     \nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu)& = \nabla_{\theta} \sum_y \pi_{\theta}(y) \cdot (\log \pi_{\theta}(y) - \log \mu(y)) \nonumber \\
%     &= \textcolor{blue}{\sum_y \nabla_{\theta}\left(\pi_{\theta}(y)\log \pi_{\theta}(y)\right)}  - \textcolor{purple}{\sum_y \nabla_\theta \left(\pi_\theta(y) \log \mu(y)\right)}\\
%     \textcolor{blue}{\sum_y \nabla_{\theta}\left(\pi_{\theta}(y)\log \pi_{\theta}(y)\right)} &= \sum_y \left(
%         (\nabla_{\theta}\pi_{\theta}(y))\log \pi_{\theta}(y) + \pi_\theta(y) \tfrac{\nabla_\theta \pi_\theta(y)}{\pi_\theta(y)}
%     \right) \nonumber \\
%     &= \sum_y 
%         (\nabla_{\theta}\pi_{\theta}(y))(\log \pi_{\theta}(y) + 1)
%      \nonumber \\
%      &= \sum_y 
%         (\nabla_{\theta}\pi_{\theta}(y))\log \pi_{\theta}(y) \quad (\sum_y\pi_\theta(y) = 1, \nabla_\theta\sum_y\pi_\theta(y) = 0)
%       \\
%     \textcolor{purple}{- \sum_y \nabla_\theta \left(\pi_\theta(y) \log \mu(y)\right)} &= -\sum_y (\nabla_\theta \pi_\theta(y)) \log \mu(y) \\
%     \nabla_\theta \pi_\theta(y) &= \pi_\theta(y) \nabla_\theta \log \pi_\theta(y) \\
%     \nabla_{\theta} D_{\text{KL}}(\pi_{\theta}||\mu)&= \sum_y (\pi_\theta(y) \nabla_\theta \log \pi_\theta(y)) \cdot (\log \pi_\theta(y) - \log \mu(y)) \nonumber \\
%     &= \mathbb{E}_{\pi_\theta}[\log \tfrac{\pi_\theta(y)}{\mu(y)}  \cdot \nabla_\theta \log \pi_\theta(y)]
% \end{align}

% Meanwhile, if we derive the differentiation in \Cref{eq: reverse_kl_result}, we obtain 
% \begin{align}
%     \mathbb{E}_{\pi_\theta}[\nabla_\theta (\log \pi_\theta(y) - \log \mu(y))^2]/2 &= \mathbb{E}_{\pi_\theta}[(\log \pi_\theta(y) - \log \mu(y)) \cdot \nabla_\theta (\log \pi_\theta(y) - \log \mu(y)) \cdot \textcolor{red}{\nabla_\theta \pi_\theta(y)}] \nonumber \\
%     &\neq \mathbb{E}_{\pi_\theta}[(\log \pi_\theta(y) - \log \mu(y)) \cdot \nabla_\theta (\log \pi_\theta(y) - \log \mu(y))],
% \end{align}
% despite $\nabla_\theta \log \mu(y) = 0.$

% \textcolor{blue}{
% Xiaohnag:
% \begin{align}
% \mathbb{E}_{\pi_\theta}[\nabla_\theta (\log \pi_\theta(y) - \log \mu(y))^2]/2 &= \mathbb{E}_{\pi_\theta}[(\log \pi_\theta(y) - \log \mu(y)) \cdot \nabla_\theta (\log \pi_\theta(y) - \log \mu(y))]
% \end{align}
% }
% \end{proof}