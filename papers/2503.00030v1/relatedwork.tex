\section{Related Work}
% \paragraph{Preference Optimization.} Reinforcement Learning from Human Feedback (RLHF) solves preference optimization with reward models based on Bradley-Terry (BT) assumptions, which are used to obtain the rewards from preference data \citep{bradley1952rank,christiano2017deep,ouyang2022training,rafailov2024direct}. 

\citet{azar2024general} introduced the first approach for optimizing general preference models. Nash-MD \citep{munos2023nash} pioneered the application of self-play to general preference optimization by framing it as a two-player game. Subsequent methods have either focused on learning the NE of the original unregularized game (e.g. \citep{swamy2024minimaximalist, wu2024self, rosset2024direct, wang2024magnetic}) or the NE of a reverse-KL-regularized preference optimization game (e.g. \citep{munos2023nash, calandriello2024human, zhang2024iterativenashpolicyoptimization}). In contrast, our work explores a broader class of divergence-based regularization techniques for self-play alignment.


We emphasize the distinction between our self-play approach and self-play methods based on pairwise comparisons, which construct loss functions by leveraging the difference in policy logits between preferred and rejected responses—such as Direct Policy Optimization (DPO) \citep{rafailov2024direct} and Identity Policy Optimization (IPO) \citep{calandriello2024human}. Direct Nash Optimization (DNO) \citep{rosset2024direct} and Iterative Nash Policy Optimization (INPO) \citep{zhang2024iterativenashpolicyoptimization} follow the Mirror Descent (MD) update \citep{beck2003mirror} while computing loss using pairwise comparisons. However, optimizing such pairwise-comparison-based losses can lead to only an increase in the relative likelihood gap without necessarily enhancing the absolute probability of the preferred response \citep{pal2024smaug}. In contrast, our method directly approximates the MD update by converting MD as a reinforcement learning problem, thereby circumventing the limitations of pairwise comparison-based approaches. \looseness=-1

% Notably, self-play alignment methods requiring a pre-trained or external preference model for policy optimization is called (preference) model-based self-play \citep{munos2023nash}. Model-free self-play on the contrary directly optimize the policy based on the preference data. INPO and DNO are model-free methods \citep{zhang2024iterativenashpolicyoptimization,rosset2024direct}, where only bandit preference is provided in the dataset. To avoid estimating a preference model, these methods leverage similar idea of Direct Preference Optimization, subtracting the policy logits of preferred response by that of dis-preferred response to approximately conduct Mirror Descent or MWU. However, in this work, we investigate a general self-play policy optimization, similar to the original RLHF \citep{ouyang2022training} where a reward model is pre-trained for RL, we also assume to have a pre-trained or external preference model. And we empirically show that, a small external preference model is sufficient for improving the base model significantly with self-play.

Online iterative RLHF, which incorporates a reliable reward or preference model—including self-play—functions as a self-improving framework by iteratively generating new data using models and optimizing policies based on this data \citep{schulman2017proximal, ouyang2022training, bai2022training, touvron2023llama, dong2024rlhf}. Moreover, extending powerful offline methods such as Direct Preference Optimization (DPO) to iterative frameworks has led to significant performance gains \citep{xu2023some, liu2023statistical, viethoangtranduong, dong2024rlhf, calandriello2024human, pang2024iterative, xiong2024iterative, guo2024direct, tajwar2024preference, cen2024value, xie2024exploratory}. In contrast, our work investigates general preference optimization through self-play from a game-theoretic perspective, shifting the objective from conventional RL optimization to the computation of NE.\looseness=-1

% \paragraph{Online Learning and Game Theory.}
% Regularization in online learning has been extensively studied, with most works utilizing regularization to promote sparsity in the solution. However, in alignment and RLHF, the approach to regularization and its effects differ. The Multiplicative Weights Update (MWU) method employs a first-order technique, specifically exponentiated gradient descent \citep{beck2003mirror}. Most studies addressing regularized games \citep{zeng2022regularized,liu2022power} rely on gradient-based approaches, such as Mirror Descent.