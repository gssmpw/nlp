\documentclass[11pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{fullpage,amsmath,amsfonts,amssymb,amsthm}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{mdframed}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwReturn}{Return}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\usepackage{graphbox}
\usepackage{authblk}
\usepackage{hyperref}


\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}

\def\ddef#1{\expandafter\def\csname r#1\endcsname{{\ensuremath{\mathbb{#1}}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

\def\ddef#1{\expandafter\def\csname c#1\endcsname{{\ensuremath{\mathcal{#1}}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop


\newcommand{\E}{{\mathbb{E}}}
\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\title{Reverse Markov Learning: \\Multi-Step Generative Models for Complex Distributions}
%Reverse Markov Learning of Complex Distributions
\author[$\star$]{Xinwei Shen}
\author[$\star$]{Nicolai Meinshausen}
\author[$\dag$]{Tong Zhang}
\affil[$\star$]{Seminar for Statistics, ETH Z\"urich}
\affil[$\dag$]{Department of Computer Science, University of Illinois Urbana-Champaign}
\date{February 18, 2025}

\newcommand\xinwei[1]{{\color{cyan}Xinwei: #1}}
\newtheorem{theorem}{Theorem}

\begin{document}

\maketitle

\begin{abstract}
Learning complex distributions is a fundamental challenge in contemporary applications. Generative models, such as diffusion models, have demonstrated remarkable success in overcoming many limitations of traditional statistical methods. \citet{shen2024engression} introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression struggles with highly complex distributions, such as those encountered in image data. In this work, we extend engression to improve its capability in learning complex distributions. We propose a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. Our approach supports general forward processes, allows for dimension reduction, and naturally discretizes the generative process. As a special case, when using a diffusion-based forward process, our framework offers a method to discretize the training and inference of diffusion models efficiently. Empirical evaluations on simulated and climate data validate our theoretical insights, demonstrating the effectiveness of our approach in capturing complex distributions. 
\end{abstract}

\section{Introduction}

Modern applications of statistics and machine learning often involve learning a distribution for more comprehensive uncertainty quantification or data generation. Consider a random vector $X\in\mathbb{R}^d$. Given an i.i.d.\ sample of $X$, classical parametric distribution families or nonparametric density estimation approaches, such as the kernel density estimation, have been well studied for simple or low-dimensional distributions of $X$. Generative models such as diffusion models~\citep{sohl2015deep,song2019generative,ho2020denoising}, on the other hand, have achieved remarkable success in generating samples from complex and high-dimensional data distributions such as images. When given some covariates $Y\in\mathbb{R}^p$, the target becomes the conditional distribution of $X$ given $Y$, denoted by $p^*_{X|Y}$, which is unknown. To this end, distributional regression approaches have been developed through estimating the cumulative distribution function~\citep{foresi1995conditional,hothorn2014conditional}, density function~\citep{dunson2007bayesian}, quantile function~\citep{meinshausen2006quantile}, etc. Alternatively, conditional generative models are potentially more powerful for high-dimensional responses.

\citet{shen2024engression} recently proposed \emph{engression}, a generative model-based method for learning distributions in the regression context. A generative model $g(Y,\varepsilon)$ takes as arguments the covariates $Y$ (if any) and standard Gaussian noise variables $\varepsilon$ and maps to the data space $\mathbb{R}^d$. Engression minimizes the so-called energy loss as follows
\begin{equation}\label{eq:engression}
    g^*\in\argmin_{g\in\mathcal{G}} \E\left[\|X-g(Y,\varepsilon)\| - \frac12\|g(Y,\varepsilon) - g(Y,\varepsilon')\|\right],
\end{equation}
where $\varepsilon$ and $\varepsilon'$ are two i.i.d.\ draws from the standard Gaussian, $\mathcal{G}$ is a class of functions, and $\|.\|$ is the $L^2$ norm. It holds that $g^*(y,\varepsilon)\sim p^*_{X|Y=y}$ for a fixed $y$, when such a $g^*$ exists. See Section~\ref{sec:eng} for a more detailed review.
Engression is very simple, tractable, and has been shown to be capable of learning many real-world distributions. However, when the true distribution becomes too complex, engression may yield subpar performance. 

This paper aims to address the challenge of learning complex distributions. Inspired by the success of diffusion models, we consider a forward process from the target distribution to a known distribution such as the standard Gaussian and then learn a {reverse Markov process} via multiple engression models to match all the reverse conditional distributions with the forward process. In this way, the final state of this reverse process matches the target distribution. Intuitively, the task of learning a complex distribution is split into learning multiple simpler (conditional) distributions, which potentially leads to a statistical gain. Our method, termed Reverse Markov Learning, is depicted in Section~\ref{sec:method}. %[summarize the statistical analysis results]

Moreover, due to the flexibility of engression in learning conditional distributions, we can in principle define the forward process in an arbitrary way. This makes our framework more general than diffusion models or flow matching. In fact, we can show that the continuous limit of a special case of our approach becomes flow matching. When the forward process is chosen as a diffusion process or a linear interpolation between data and noise, our framework provides a computationally more efficient method that discretizes training and sampling of diffusion models or flow matching while not suffering from any discretization error.

Diffusion models have two main computational burdens which can be alleviated by our approach. First, the processes in diffusion models maintain the data dimension, which is usually very high in most modern applications. Thus, training and sampling are both conducted in a very high-dimensional space, which can be computationally expensive. One existing approach is to reduce the dimension in a separate step through an autoencoder and then apply diffusion models to the latent space~\citep{rombach2022high}, which is usually lossy data compression and hence not guaranteed to preserve the exact data distribution. In comparison, our forward process can have varying, especially reducing dimensions, e.g., when the intrinsic data dimension is lower than $d$, which can significantly save the computational cost in both training and generation, while preserving the data distribution. In addition, generation in diffusion model often takes many steps, which is time consuming (although various techniques such as consistency models by \citet{song2023consistency} can be employed to alleviate this issue after training). Our framework, in contrast, is naturally discretized with a finite-step forward process so that the final generation algorithm is conducted directly in a finite number of steps. Both properties make our method computationally more appealing than diffusion models. 

Theoretical results are established in Section~\ref{sec:theory}. An application to regional precipitation prediction is presented in Section~\ref{sec:precip}.

\medskip
\begin{mdframed}
\emph{
During the early phase of this work, one of our authors visited Google DeepMind in October 2024 and privately spoke with Arnaud Doucet and Arthur Gretton about this idea that we have been working on; both of them found it interesting and original. Later, Arnaud Doucet and Valentin De Bortoli invited us to present our work at DeepMind in early November, where we further explained the idea in the context of diffusion models, particularly in response to a question from Kevin Murphy.
After that, we had no further communication with the DeepMind group until recently, when we came across an online preprint (arXiv:2502.02483) by these four researchers, presenting very similar ideas as their own contribution without any attribution to the source. After we reached out, they responded that they are willing to acknowledge our prior communication in their paper. While their tentative statement recognizes part of our earlier discussion, we think it might not fully acknowledge the extent of information they learned from the communication. 
We leave it to the readers to consider the appropriateness of these actions in the context of academic ethical standards.
}
\end{mdframed}


\section{Preliminaries and Motivations}\label{sec:eng}
\subsection{Scoring Rule-Based Generative Models}
To quantify how well a distributional model (e.g., a generative model) fits the observed data, \citet{shen2024engression} consider one of the commonly used proper scoring rules, the energy score~\citep{gneiting2007strictly}. For a candidate distribution $P$ and an observation $x$, the energy score is defined as
\begin{equation*}
	S(P,x) = \frac12\E\|X-X'\|^\beta - \E\|X-x\|^\beta,
\end{equation*}
where $X$ and $X'$ are two i.i.d.\ draws from $P$, and $\beta\in(0,2)$ is a hyperparameter. The energy score is a strictly proper scoring rule, that is, given $P^*$ and for any $P$, we have
\begin{equation*}
	\E_{P^*}[S(P^*,X)] \ge \E_{P^*}[S(P,X)],
\end{equation*}
where the equality holds if and only if $P=P^*$. There are many choices of strictly proper scoring rules, such as the log score and kernel score; in fact, the energy score is a special case of the kernel score. We stick to the energy score with $\beta=1$ due to its simplicity and attractive computational and theoretical properties as investigated previously by \citet{shen2024engression}.

The scoring rules are also associated with certain distance functions. In particular, the respective distance of the energy score for two distributions $P$ and $\tilde{P}$ is the energy distance~\citep{szekely2003statistics} defined as
\begin{equation}\label{eq:energy_distance}
    \E\|X-\tilde{X}\|-\frac12\E\|X-X'\|-\frac12\E\|\tilde{X}-\tilde{X}'\|,
\end{equation}
where $X$ and $X'$ are two i.i.d.\ samples from $P$ and $\tilde{X}$ and $\tilde{X}'$ are two i.i.d.\ samples from $\tilde{P}$. The energy distance can also be regarded as a variant of the maximum mean discrepancy (MMD) distance~\citep{gretton2012kernel}.

Recall that in distributional regression or conditional generation, the target is the conditional distribution of $X$ given covariates $Y=y$ for any fixed $y$. For a generative model $g(y,\varepsilon)$, denote by $p_g(x|y)$ its induced distribution. Applying the expected energy score to the conditional distribution, we have
\begin{equation*}
	\E_{p^*(x|y)}[S(p^*(x|y), X)] \ge \E_{p^*(x|y)}[S(p_g(x|y), X)],
\end{equation*}
where the equality holds if and only if $p_g(x|y)\equiv p^*(x|y)$.
Taking the negation and expectation of the right-hand side of the above inequality further with respect to $Y$ leads to the objective function of the engression method in \eqref{eq:engression}. It is also implied from this inequality that when optimized, we have $g^*(y,\varepsilon)\sim p^*_{X|Y=y}$ for any $y$ in the training support. See \citet[Proposition 1]{shen2024engression} for a formal statement.

As a generative model method, engression learns a single map from the noise to data, same as the goal of variational autoencoders~\citep{kingma2013auto} and adversarial generative networks~\citep{goodfellow2014}. However, engression does not rely on variational approximation or adversarial training that involves a second network (an encoder or a discriminator). Instead, it approaches the same target via a single minimization problem, which yields advantages in faster convergence and less hyperparameter tuning~\citep{shen2024engression}.


\subsection{From One-Step to Continuous-Time Generative Models}
%[comparisons with other generative models]
One-step generative models lead to efficient computation during test time and allows for the flexibility of dimensionality reduction and thus more structured latent space. Nevertheless, many real-world data distributions are complex especially in contemporary applications, for which a single map from noise to data could be either out of model capacity or very hard to optimize for. 

As an illustrative example, we consider mixtures of Gaussians with poor mixing, as shown on the left in Figure~\ref{fig:mog}. By directly applying (unconditional) engression in one step, as shown in the second left plot in Figure~\ref{fig:mog}, in one step, the model can capture the modes well but the learned generator map tends to be smoother, producing many unrealistic samples in the low-density areas. 

\begin{figure}
\centering
\begin{tabular}{@{}cccc@{}}
	\includegraphics[width=0.22\textwidth]{fig/mog_true.png} &
	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T1.png} &
	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T5.png} &
	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T10.png} \\
	True data & Engression samples & \small{RML samples ($T=5$)} & \small{RML samples ($T=10$)}
\end{tabular}
\caption{An illustrative example of a mixture of three Gaussians, i.e., $\frac13\mathcal{N}(\mu_1,\sigma I_2)+\frac13\mathcal{N}(\mu_2,\sigma I_2)+\frac13\mathcal{N}(\mu_3,\sigma I_2)$, where $\mu_1=(0,0)$, $\mu_2=(5,5)$, $\mu_3=(6,-1)$, and $\sigma=0.1$. The plots show samples from the true distribution and estimated distributions by engression and the proposed Reverse Markov Learning (RML) method. }\label{fig:mog}
\end{figure}


%[challenge of engression for more complex distributions]
Diffusion models and flow matching have successfully overcome this drawback of one-step generative models. These algorithms first define a forward process by progressively adding Gaussian noise to corrupt the data and then approximately sample from the time-reversed process by estimating the score function or a target vector field that generates the desired probability path. However, sampling from diffusion models often requires a large number of steps to control the discretization error and approximate the reverse process and thus the data distribution well. There is hence a pressing need to alleviate this computational burden. One technique that has been explored is distillation using consistency models to alleviate this issue after training.

We are therefore motivated to consider multi-step (discrete-time) generative models that inherit the powerful capacity of diffusion models, while ensuring correctness given any number of time steps, thus avoiding discretization errors. Moreover, we design the process so that the target distributions at each step are much easier to learn than the final target and we leverage scoring rule-based generative models (in particular, engression) at each step due to its simplicity and flexibility. 


\section{Reverse Markov Learning}\label{sec:method}
We introduce the method for conditional distribution learning with covariates $Y$, while unconditional generation is a special case with $Y$ being an empty set. 

\subsection{Forward Stochastic Bridging Process}

In the forward path, we consider a general $y$-conditioned stochastic process, referred to as a
Distributional Bridging Process. 

\begin{definition}
Given an unknown target distribution $p^*_{X|Y=y}$, which we would like to learn,
and a known distribution  $q^*_{X|Y=y}$ which is easy to sample from,
we call a stochastic process $\cP: \{X_t: t=0,1,\dots,T\}|Y=y$, a 
$y$-conditioned distributional bridging process from $p^*$ to $q^*$, if it satisfies the following two conditions: 
\begin{itemize}
    \item The marginal $X_0|Y=y$ is the unknown target distribution $p^*_{X|Y=y}$.
    \item   The marginal $X_T|Y=y$ is the known distribution
    $q^*_{X|Y=y}$.
\end{itemize}
\label{def:sbp}
\end{definition}
We do not impose additional assumptions on this process.
In applications, $q^*$ is usually taken as the standard Gaussian distribution $\mathcal{N}(0,I_l)$ that is independent of $Y$, where $l$ is the dimensionality of $X_T$. Here we make the more general assumption to allow it to be an arbitrary known distribution as long as it is easy to sample from.
%\begin{equation*}
%	X_0|Y=y\sim p^*_{X|Y=y}\quad\text{and}\quad X_T|Y=y \sim  q^*_{X|Y=y}
%\end{equation*}

It is worth noting that the intermediate states $X_t$ ($t \notin \{0,T\}$) can in principle be defined in arbitrary ways. For example, for each $t$, $X_t$ may have arbitrary 
dependency on $\{X_s: s \neq t\}$.
As we will notice later, the only condition for this forward process to be useful in facilitating learning the target distribution is that the process should be autocorrelated in some sense, so that the conditional distributions of $X_{t-1}$ given $X_t$ and $y$ is relatively easy to learn. 

Below we list a few examples of a forward process including those that have appeared in the literature. 

\medskip
\noindent\textbf{Example I: diffusion process.} As in diffusion models, we can start with the data and consider a Markov process that gradually adds noise. For example, for $t=1,\dots,T$,
\begin{equation*}
    X_t | X_{t-1}\sim \mathcal{N}(\sqrt{1-\sigma_t}X_{t-1}, \sigma_t I),
\end{equation*}
where $\sigma_t\in(0,1]$ controls the variance schedule and is an increasing function of $t$ such that $\sigma_T=1$. 

\medskip
\noindent\textbf{Example II: linear interpolation.} Conditional on $(X_0,X_T)$, we can define the forward process as a deterministic process
\begin{equation*}
    X_t = (1-t/T)X_0 + (t/T)X_T,
\end{equation*}
where $t=1,\dots,T$. This is commonly used as the forward process in flow matching. 

\medskip
\noindent\textbf{Example III: spatial pooling.} For spatial data like images or geographic maps, we can define the forward process based on scientifically meaningful operators such as average pooling
\begin{equation*}
    X_t = m(X_{t-1}),
\end{equation*}
where $m(\cdot)$ is the average pooling operator of a certain factor. For example, for a $r\times r$ spatial field $X$ and average pooling with a kernel size 2, $m$ is a deterministic map from $\mathbb{R}^{r\times r}$ to $\mathbb{R}^{(r/2)\times (r/2)}$. Later we adopt this forward process in an application to climate prediction. 


\subsection{Reverse Markov Sampling}


Consider a stochastic distributional bridging process from $p^*$ to $q^*$, as in Definition~\ref{def:sbp}.
Given a time step $t \in \{1,\ldots,T\}$, 
we further denote by $p^*_{t}(x_{t-1},x_t|y)$ the joint distribution of $(X_{t-1},X_t)$ conditioned on $Y=y$,
and we denote by $p^*_{t}(x_{t-1}|x_t,y)$ the conditional distribution of $X_{t-1}|X_t,Y$. 
Using this notation, we can define a reverse $y$-conditioned Markov process 
$\{\tilde{X}_t, t=T,T-1,\dots,0\}$ as in Algorithm~\ref{alg:rms}.
The output of the algorithm is $\tilde{X}_0$.

\medskip

\begin{algorithm}[H]
\caption{Reverse Markov Sampling}\label{alg:rms}
\KwIn{Condition \(y\), briding process $\cP$}
\KwOut{\(\tilde{X}_0\)}
Sample \(\tilde{X}_T \sim q^*_{X|Y=y}\)\;
\For{\(t = T, T-1, \ldots, 1\)}{
  Sample \(\tilde{X}_{t-1} \sim p^*_{t}(x_{t-1} \mid \tilde{X}_t, y)\)\;
}
\KwReturn{$\tilde{X}_0$}
\end{algorithm}
\medskip


 We have the following general result for this reverse Markov sampling process, which indicates that we can sample from the target distribution $p^*_{X|Y=y}$ using Algorithm~\ref{thm:rmp}, as long as we can sample from each Markov conditional distribution
 $p^*_{t}(x_{t-1} \mid \tilde{X}_t, y)$. 

 \begin{theorem}
 Consider $\tilde{X}_t$ generated according to Algorithm~\ref{alg:rms}.
 Let $\tilde{p}_t(\tilde{x}_{t-1},\tilde{x}_t|y)$
 be the joint density of $(\tilde{X}_{t-1},\tilde{X}_t)|Y=y$. 
 Then for all $t =1,\ldots,T$, we have
 \[
 \tilde{p}_t(x_{t-1},x_t|y)=p^*_t(x_{t-1},x_t|y) .
 \]
 This implies that the sampled distribution
 $\tilde{X}_0|Y=y$ is the same as that of the target distribution
 $p^*_{X|Y}(\cdot|Y=y)$.
 \label{thm:rmp}
 \end{theorem}
\begin{proof} 
Let $\tilde{p}_t(\tilde{x}_t|y)$ be the marginal distribution of 
$\tilde{X}_t$, conditioned on $Y=y$. 
Let $p^*_t(x_t|Y=y)$ be the marginal distribution of 
$X_t$, conditioned on $Y=y$. By the assumption of the forward process, we have
\[
\tilde{p}_T(x_T|y)=p^*_T(x_T|y)=q^*(x_T|y) .
\]
Assume that the statement of
\[
\tilde{p}_t(x_t|y)=p^*_t(x_t|y)
\]
 is true at any $1 \leq t \leq T$. Then
by Algorithm~\ref{alg:rms}, we have
\[
\tilde{p}_t(x_{t-1},x_t|y)
= p_t^*(x_{t-1}|x_t,y) \tilde{p}_t(x_t|y)
= p_t^*(x_{t-1}|x_t,y) p^*_t(x_t|y)
= p^*_t(x_{t-1},x_t|y) .
\]
This means that the theorem holds at $t$. 
By taking marginal at $x_{t-1}$, this implies also that
\[
\tilde{p}_{t-1}(x_{t-1}|y)=p^*_{t-1}(x_{t-1}|y) .
\]
Now we obtain the desired result by induction on $t$.
\end{proof}

% This result implies that we can sample from the target distribution using Algorithm~\ref{thm:rmp}, as long as we can sample from each Markov conditional distribution
% $p^*_{t}(x_{t-1} \mid \tilde{X}_t, y)$. 
 
\subsection{Training Algorithm}

In order to implement Algorithm~\ref{alg:rms} in practice, we need to sample from the distribution 
\[
p^*_{t}(x_{t-1} \mid \tilde{X}_t, y). 
\]
We refer to this sampling problem as conditional reverse Markov sampling. Since this conditional distribution is generally unknown, practical implementation requires learning  it from the data.



In this paper, we propose to learn this reverse Markov process via multi-step engression. Specifically, for each $t=1,\dots,T$, we aim to find a function $g_t$ so that the distribution of $g_t(x_t,y,\varepsilon_t)$ given $X_t=x_t$ and $Y=y$ matches $p^*_{t}(x_{t-1}|x_t,y)$. Here $\varepsilon_t$ is assumed to be drawn from a Gaussian distribution. Denote $g(t,x_t,y,\varepsilon_t)=g_t(x_t,y,\varepsilon_t)$.

To this end, we define
\begin{equation*}
    g^*\in\argmin_{g}\E\left[\|X_{t-1}-g_t(X_t,Y,\varepsilon_t)\| - \frac12\|g_t(X_t,Y,\varepsilon_t) - g_t(X_t,Y,\varepsilon_t')\|\right],
\end{equation*}
where $\varepsilon_t$ and $\varepsilon_t'$ are two i.i.d.\ draws from the standard Gaussian, and the expectation is taken over all random variables including $t\sim\mathrm{Unif}\{1,\dots,T\}$. 

When all $X_t$'s share the same dimension, we can parametrize all $g_t$'s as a shared function of $t,x_t,y$ and $\varepsilon_t$, similar to the time embedding in a score network in diffusion models. When we allow varying dimensions, we need separate models for $g_t$'s with different dimensions. %We summarize the training procedure given a finite sample of $p^*_{X,Y}$ in Algorithm~\ref{alg:rml}.

We note that in standard training data, we observe a set of training data
\[
\mathcal{S}=\{(x^1,y^1),\ldots,(x^n,y^n)\} \sim p^*_{X,Y} .
\]
For each sample $(x^i,y^i) \in \cS$, we can then augment it to 
\[
(x_0^i,x_1^i,\ldots,x_T^i,y^i) 
\]
according to the underlying stochastic bridging process $\cP$. 
In Algorithm~\ref{alg:rml}, we assume that given $\cP$, and $t \in \{1,\ldots,T\}$, we can take sample
$(x_{t-1}^i,x^i_{t},y^i)$ from $\cP$ and the training data $\cS$. Here we know that
\[
x^i_{t-1},x^i_{t}|y^i \sim p^*_t(x^i_{t-1},x^i_{t}|y^i) .
\]
With this in mind, we can state the algorithm of Reverse Markov Learning (RML) as follows. 

{\centering
\begin{minipage}{\linewidth}
\vskip 0.1in

\SetKwFor{Iterate}{Iterate}{do}{end iterate}

\begin{algorithm}[H]
\DontPrintSemicolon
\KwInput{Training sample $\mathcal{S}$, bridging process $\cP$, batch size $m$}
%\KwOut{Generators $\{g_t: t=1,\ldots,T\}$}

\Iterate{ \text{until converge}}{
Sample $t\sim\mathrm{Unif}\{1,\dots,T\}$\\
\For{$i=1\ldots,m$}{
%Sample $i \sim \mathrm{Unif}\{1,\ldots,n\}$\\
Take $y^i$ from $\cS$\\
Take a sample $(x^{i}_{t-1},x^{i}_t)\sim p^*_{t}(x^i_{t-1},x^i_{t}|y^i)$\\
Sample $\varepsilon_{i},\varepsilon'_{i}\sim\mathcal{N}(0,I)$
}
Update parameters of $g_t$ by descending the gradients of
\begin{equation*}
	\frac{1}{m}\sum_{i=1}^m\left[\left\|x^{i}_{t-1} - g_t(x^{i}_t,y^i,\varepsilon_{i})\right\| - \frac12\left\|g_t(x^{i}_{t},y^i,\varepsilon_{i}) - g_t(x^{i}_{t},y^i,\varepsilon'_{i})\right\|\right]
\end{equation*}
}
\KwReturn{Generators $\hat{g}_t:=g_t,t=1,\dots,T$}
\caption{Reverse Markov Learning (RML)}
\label{alg:rml}
\end{algorithm}
\end{minipage}
\vskip 0.1in
\par
}

 
\subsection{Reverse Markov Sampling with Learned Generator}

Using the generators $\{\hat{g}_t: t=1,\ldots,T\}$ learned from Algorithm~\ref{alg:rml}, we can instantiate Algorithm~\ref{alg:rms}
in Algorithm~\ref{alg:rmg}. 

\medskip

\begin{algorithm}[H]
\caption{Reverse Markov Generation}\label{alg:rmg}
\KwIn{Condition \(y\), $q^*$, generators $\{\hat{g}_t: t=1,\ldots,T\}$}
\KwOut{\(\tilde{X}_0\)}
Sample \(\tilde{X}_T \sim q^*_{X|Y=y}\)\;
\For{\(t = T, T-1, \ldots, 1\)}{
$\varepsilon_t \sim N(0,I)$\\
  $\tilde{X}_{t-1} =\hat{g}_t(\tilde{X}_t,y,\varepsilon_t)$
}
\KwReturn{$\tilde{X}_0$}
\end{algorithm}
\medskip

As an immediate demonstration, for the illustrative example in Figure~\ref{fig:mog}, we show the generated samples from our method in the right two plots. With 5 steps, RML already exhibits a significant advantage over engression (RML with one time step). With 10 steps, RML can produce samples from the true mixture of Gaussians very well. In Figure~\ref{fig:mog_inter_t10}, we further show the samples from Reverse Markov Generation at intermediate steps, from which we can see how the process evolves so that each reverse conditional distribution $p^*_{t}(x_{t-1}|x_t,y)$ is easier to learn than the original target $p^*_{X|Y=y}$.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{fig/mog_gen_diffusion_T10_inter.png}
	\caption{Samples at intermediate steps by Reverse Markov Learning with $T=10$ for the illustrative example in Figure~\ref{fig:mog}.}\label{fig:mog_inter_t10}
\end{figure}



\section{Additional Extension and Relationship to Flow-matching}\label{sec:theory}

We can also include additional variables, which we may refer to as $Z$, into the condition of the general forward stochastic process as
\[
\{X_0, X_1,\ldots,X_T\}| Y=y, Z=z .
\]
We assume that the latent variable $Z$ is some auxiliary information that is observed during training, and thus can be used in the forward process. However it is not observed during inference, and thus can not be used in the reverse generation process. It is easy to see that our algorithms can be still applied in this setting, by ignoring $Z$ in the reverse Markov process.
Moreover, the statement of Theorem~\ref{thm:rmp} still holds in this case.

In the following, we show that for a specially defined forward stochastic process, its continuous limit leads to the flow-matching method. Therefore our generation process may be regarded as a generalized discrete version of the flow-matching method. 



The following result shows that a version of the continuous limit of the method becomes flow matching.
Without loss of generality, we assume all target dependent information to be encoded into $Z$. For example, we may take $Z=(X_0,\varepsilon)$, conditioned on $y$, where $\varepsilon \sim q^*_{X|Y}$.

Now given latent variable $Z$, and conditioning variable $y$, and continuous time step $s \in [0,1]$, we may define a vector function
 \[
 h(Z,y,s) = (1-s)X_0 + s \varepsilon .
 \]
For this case, we know that $h(Z,y,0)$ has the same distribution as $p^*_{X|Y}$, and $h(Z,y,1)$ has the same distribution as 
$q^*_{X|Y}$. More generally, we introduce the following definition.

\subsection{Continuous Differentiable Bridge Function}

We introduce the following definition, for which we can use to define a continuous flow-matching method.

\begin{definition}
Consider $y \in \cY$. 
Let $Z|Y=y$ be a $y$ conditioned random variable defined on $\cZ$  according to $\cP_{Z|Y}$,
and let $X$ be a $y$-conditioned random variable on $\rR^d$.
Let $p^*_{X|Y}$ and $q^*_{X|Y}$ be two distributions on $\rR^d$.
Consider a function 
\[
h(Z,y,s) : \cZ \times \cY \times [0,1] \to \rR^d 
\]
that is continuously differentiable in $s \in [0,1]$.
We call $(h,\cP_{Z|Y})$
a differentiable bridge function from $p^*$ to $q^*$ if for all $y \in \cY$:
\[
h(Z,y,0) : Z\sim \cP_{Z|Y}(\cdot|Y=y)\text{ has the same distribution as }  p^*_{X|Y}(X|Y=y) ,
\]
and
\[
h(Z,y,1): Z|Y=y \sim \cP_{Z|Y}(\cdot|Y=y) \text{ has the same distribution as }  q^*_{X|Y}(X|Y=y) .
\]
\label{def:diff_bridge}
\end{definition}

The following theorem characterizes the continuous flow matching process of
\cite{lipman2022flow} using the terminology of this paper.

\begin{theorem}\label{thm:continuous}
Let $p^*_{X|Y}$ and $q^*_{X|Y}$ be two distributions on $\rR^d$.
Let $(h,\cP_{Z|Y})$ be a differentiable bridge from $p^*$ to $q^*$. 
Define
\[
g(\tilde{x},y,s)= \E_{Z \sim \cP_{Z|Y}(\cdot|Y=y)}\left[ \frac{\partial}{\partial s} h(Z,y,s)\Big| h(Z,y,s)=\tilde{x}\right] .
\]
Let $\tilde{x}(x_1,y,s)$ be the solution of the differentiable equation (conditioned on $y$):
\begin{equation}
    \frac{\partial}{\partial s} \tilde{x}(x_1, y,s) =  g(\tilde{x}(x_1, y,s),y, s) , 
    \quad 
    \tilde{x}(x_1,y,1)=x_1.  \label{eq:flow-ode}
  \end{equation}
  Then the random variable
  \[
     h(Z,y,s): Z \sim \cP_{Z|Y}(\cdot|Y=y)
  \]
   has the same distribution as that of
  \[
    \tilde{x}(\tilde{X}_1,y,s), \qquad \tilde{X}_1 \sim q^*_{X|Y}(\cdot|Y=y) .
  \]
\end{theorem}

The theorem can be proved by comparing the Fokker-Planck equations of the two processes. We will skip the proof. In this paper, we will treat it as the continuous limiting situation of Reverse Markov Sampling, as stated in Theorem~\ref{thm:continuous-discretized} below. 
Therefore the proof of Theorem~\ref{thm:continuous-discretized} directly implies Theorem~\ref{thm:continuous} under suitable regularity conditions. 

  We note that the flow matching method relies on the following optimization formula to learn $ g(\tilde{x},y, s)$ from data
  \begin{equation}
  \hat{g}=\arg\min_g  \E_{Y} 
  \E_{Z \sim \cP_{Z|Y}(\cdot|Y)}
  \E_{s \sim U[0,1]} 
  \; \left[ \left(g(h(Z,Y,s),y,s)- \frac{\partial}{\partial s} h(Z,Y,s)\right)^2\right]  . 
  \label{eq:fm-learn}
  \end{equation}

    For example, if we observe examples $(X,Y)$ in training, and let $Z=(X,\varepsilon)$ with $\varepsilon \sim N(0,I)$
    $h(Z,y,s)=(1-s) X + s \varepsilon$, then the learning algorithm tries to minimize
    \[
    \hat{g}=\arg\min_g  \E_{X,Y} \E_{\varepsilon \sim N(0,I)}
  \E_{s \sim U[0,1]} 
  \; \left[ \left(g((1-s)X+s\varepsilon,y,s)-  (\varepsilon-X)\right)^2\right]  . 
    \]

Using the $\hat{g}$ learned from \eqref{eq:fm-learn}, we can use the following generation process which discretizes the flow-ODE \eqref{eq:flow-ode}.

\medskip

\begin{algorithm}[H]
\caption{Reverse Flow-ODE Generation}\label{alg:fmg}
\KwIn{Condition \(y\), $q^*$, generators $\{\hat{g}(\tilde{x},y,s)\}$}
\KwOut{\(\tilde{X}_0\)}
Sample \(\tilde{X}_T \sim q^*_{X|Y}(\cdot|Y=y)\)\;
\For{\(t = T, T-1, \ldots, 1\)}{
  $\tilde{X}_{t-1} =\tilde{X}_t- (1/T) \hat{g}(\tilde{X}_t,y,t/T)$
}
\KwReturn{$\tilde{X}_0$}
\end{algorithm}




\subsection{Discretization of Continuous Flow Matching}

It is easy to see that we can define a stochastic bridging process from $p^*$ to $q^*$ using a differentiable bridge function as follows.
\begin{definition}
Let $p^*_{X|Y}$ and $q^*_{X|Y}$ be two distributions on $\rR^d$.
Let $(h,\cP_{Z|Y})$ be a differentiable bridge function from $p^*$ to $q^*$. 

Given positive integer $T>0$ and define a $y$-conditioned stochastic process as
\[
 \left\{X_t = h(Z,y,t/T) : t=0,1,\ldots,T\right\},  \;  Z \sim \cP_{Z|Y}(\cdot|Y=y).
\]
Then it is a stochastic distributional bridging process from $p^*$ to $q^*$ implied by $(h,\cP_{Z|Y})$. 
\label{def:implied_sbp}
\end{definition}


Before stating the convergence result, we introduce the Wasserstein distance below. Note that since we consider finite-dimensional space, we do not need to pay attention to the specific metric used.
\begin{definition}
Consider a metric $\|\cdot\|$ on $\rR^d$. 
Consider two probability distributions,  $\mu$  and  $\nu$ on $\rR^d$ , and denote by
$\Gamma(\mu,\nu)$  the set of all couplings of $\mu$ and $\nu$; that is, all probability measures on $\rR^d \times \rR^d$ whose marginals are $\mu$ and $\nu$.
We define a Wasserstein distance (with respect to the metric $\|\cdot\|$)  as follows
\[
W(\mu,\nu)=\left(\inf_{\gamma\in\Gamma(\mu,\nu)}\int_{\rR^d \times \rR^d} \|x-y\| \,d\gamma(x,y)\right) .
\]
\end{definition}


We have the following theorem, which shows that continuous flow-matching is a limiting situation of our method.

\begin{theorem}\label{thm:continuous-discretized}
Let $p^*_{X|Y}$ and $q^*_{X|Y}$ be two distributions on $\rR^d$.
Let $(h,\cP_{Z|Y})$ be a differentiable bridge from $p^*$ to $q^*$. 
Define its implied stochastic distributional bridging process 
\[
 \{X_t = h(Z,y,t/T) : t=0,1,\ldots,T\},  \;  Z \sim \cP_{Z|Y}(\cdot|Y=y).
\]
  Assume that $\frac{\partial}{\partial s} h(Z,y,s)\Big| h(Z,y,s)=\tilde{x}$ has uniformly bounded variance, and $g$ defined in Theorem~\ref{thm:continuous} is uniformly continuous,  then as $T \to \infty$,
  the distribution of $\tilde{X}_t$ converges to that of $X_t$ in Wasserstein distance, where
    $\tilde{X}_t$ is generated according to  Algorithm~\ref{alg:fmg} with $\hat{g}=g$. 
\end{theorem}
\begin{proof}
We know that a sample of $p^*_t(X_{t-1},X_t|y)$ is given by
\[
  h(Z,y,(t-1)/T) , h(Z,y,t/T) : \qquad Z \sim \cP_{Z|Y}(\cdot|Y=y) .
\]
We know that
\[
h(Z,y,(t-1)/T) = \tilde{x}_t - (1/T) \frac{\partial}{\partial s} h(Z,y,t/T) + o(1/T) , \qquad \tilde{x}_t=h(Z,y,t/T) .
\]
Therefore the mean of the reverse Markov distribution $p^*_t(X_{t-1}|X_t=\tilde{x}_t,y)$ in
Algorithm~\ref{alg:rms} is given by
\[
\tilde{x}_t - (1/T) g(\tilde{x}_t,y,t/T) + o(1/T) .
\]
The variance of $p^*_t(X_{t-1}|X_t=\tilde{x}_t,y)$ is $O(1/T^2)$ by using the uniform bounded variance assumption. 
The aggregated variance of $p^*_t(X_{0}|X_t=\tilde{x}_t,y)$ is no more than $O(1/T)$, 
which means that the effect of variance vanishes (in terms of Wasserstein distance)  as $T \to \infty$.
It follows that as $T \to \infty$, the reverse Markov distribution $p^*_t(X_{t-1}|X_t=\tilde{x}_t,y)$
becomes deterministic, which can be characterized by a shift in the mean:
\[
\tilde{x}_t - (1/T) g(\tilde{x}_t,y,t/T) + o(1/T) .
\]
Since the error term $o(1/T)$ does not affect the convergence in Wasserstein distance as $T \to \infty$, we obtain the desired result from Theorem~\ref{thm:rmp}. 
\end{proof}

Our result establish a relationship of the method proposed in this paper to the continuous flow matching method, as well as its flow-ODE based generation procedure in Algorithm~\ref{alg:fmg}. 
An advantage of Algorithm~\ref{alg:rmg} is that the method is correct with any fixed finite $T$. 
Increasing $T$ makes the reverse Markov process more deterministic as illustrated in the proof of Theorem~\ref{thm:continuous-discretized},
and thus makes the learning process easier. However, the correctness of Algorithm~\ref{alg:rmg} and Algorithm~\ref{alg:rml} does not require $T \to \infty$. In contrast, the correctness of 
Algorithm~\ref{alg:fmg} and the continuous flow matching method requires $T \to \infty$. 




%statistical efficiency

%\section{Experiments}
% Datasets:
% \begin{itemize}
%     \item Mixture of Gaussians (with poor mixing)
%     \item Image benchmarks
%     \item Scientific data (e.g., climate data)
% \end{itemize}
%\subsection{Synthetic Examples}
%We first show some simple illustrative results on synthetic mixture of Gaussians with poor mixing. 
%\begin{figure}
%\centering
%\begin{tabular}{@{}cc@{}c@{}c@{}}
%	\includegraphics[width=0.22\textwidth]{fig/mog_true.png} &
%	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T1.png} &
%	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T5.png} &
%	\includegraphics[width=0.22\textwidth]{fig/mog_gen_diffusion_T10.png} \\
%	True data & $T=1$ & $T=5$ & $T=10$
%\end{tabular}
%\caption{Mixture of Gaussians.}
%\end{figure}

\section{Application to Regional Precipitation Prediction}\label{sec:precip}
We consider the problem of climate data prediction for monthly precipitations in central Europe with a spatial resolution of $128\times128$. We are interested in the full distribution of precipitation rather than just the mean or median, because heavy or low rainfalls can cause floods or droughts and widespread damage. Note that to predict regional climate, in practice, one typically has extra information, such as coarse-scale climate, that provides more signal. Herein, as a proof of concept, we consider the more challenging unconditional setting without any covariates, so that the target distribution has a larger variance and is harder to learn.

We apply Reverse Markov Learning to estimate the precipitation distribution through sampling. We define the forward path via average pooling as described in Example III. As illustrated in Figure~\ref{fig:precip_true}, the original spatial map of resolution $128\times128$ is downsampled to $64\times64$ by 2d average pooling with a kernel size of 2, thus reducing the total dimensionality by a factor of $4$, and similarly all the way down to a resolution of $2\times2$. The final step is a standard Gaussian map. We adopt three different kernel sizes, 2, 4, and 8, which leads to 7, 4, and 3 time steps, respectively. 
In this way, we reduce the dimension by fixed, meaningful maps without learning an additional encoder. In the reverse Markov process, each step involves learning the high-resolution precipitation field given the low-resolution one, which on its own is a scientifically relevant task, which is similar to (but simpler than) statistical downscaling~\citep{wilby1998statistical,wilby2013statistical}.

\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}}
        $t=0$ & $t=1$ & $t=2$ & $t=3$ & $t=4$ & $t=5$ & $t=6$ & $t=7$\\
        \includegraphics[width=0.12\textwidth]{fig/precip/true_0_4}  & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_1_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_2_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_3_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_4_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_5_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/true_6_4} & 
        \includegraphics[width=0.12\textwidth]{fig/precip/noise}\vspace{-5pt} \\
        \small{$128\times128$} & \small{$64\times64$} & \small{$32\times32$} & \small{$16\times16$} & \small{$8\times8$} & \small{$4\times4$} & \small{$2\times2$} & \small{Gaussian}\\
    \end{tabular}
    \caption{True precipitation data and the average pooling forward process at a factor of $2^2$.}
    \label{fig:precip_true}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}c@{}c@{}}
        \includegraphics[height=0.46\textwidth]{fig/precip/precip_true.png}\hspace{4pt} &
        \includegraphics[height=0.46\textwidth]{fig/precip/avgpool2x/img_epoch100_t0_.png}\hspace{4pt} & 
        \includegraphics[height=0.46\textwidth]{fig/precip/avgpool8x/img_epoch100_t0_.png} \\
        Truth & Factor of $2^2$ ($T=7$) & Factor of $8^2$ ($T=3$)
    \end{tabular}
    \caption{Generated samples from Reverse Markov Learning with $T=7$ or $T=3$ steps, in comparison to real data. }
    \label{fig:precip_gen}
\end{figure}

Figure~\ref{fig:precip_gen} shows the generated samples from RML with $T=7$ steps (yield from average pooling with a kernel size of 2) and $T=3$ steps (average pooling with a kernel size of 8) in comparison to real data. We can see that samples generated from RML with too few steps ($T=3$) look patchy and unrealistic, significantly worse than samples obtained from RML with more steps ($T=7$), which look very similar to real data. Figure~\ref{fig:precip_all_t} shows the generated samples at each step and illustrates how the distributions of the reverse Markov process behave in this case. 

Furthermore, we consider several quantitative metrics. To assess the joint distribution of the entire precipitation field, we use the energy distance, defined as \eqref{eq:energy_distance}, between the true and estimated distributions. We also compute the average and maximum of marginal energy distances and Wasserstein distances across each location. In addition, we consider the rank histogram which is a common tool for evaluating probabilistic forecasts by repeatedly tallying the rank of the true observation relative to values from generated samples sorted from lowest to highest~\citep{hamill2001interpretation}. When the probabilistic forecasts are well calibrated, the rank histogram should be as flat as a uniform distribution. Figure~\ref{fig:rankhist} presents the rank histograms, where the histogram of RML samples with $T=7$ looks more uniform (flat) than that of RML samples with $T=3$. For a quantitative metric, we compute the TV distance between the rank histogram and a uniform distribution, which we call the rank histogram TV distance.  

Figure~\ref{fig:precip_metric} summarizes all the metrics as a function of the number of steps. We can see that RML with a larger number of steps consistently performs the best in all metrics. Despite being the largest one, it still only takes 7 steps in total, which is much smaller than what is typically adopted in diffusion models. Besides, we start from a very low dimensional space, which also saves computational costs. 
% For a comprehensive evaluation of marginal distributions, we consider rank histogram of 

\begin{figure}
\centering
\begin{tabular}{ccc}
	\footnotesize{Joint energy distance} & \footnotesize{Average marginal energy distances} & \footnotesize{Max marginal energy distance}\vspace{-2pt}\\
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/joint_energy_distance.pdf} &
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/average_marginal_energy_dist.pdf} &
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/max_marginal_energy_dist.pdf}  \\
	\footnotesize{Rank histogram TV distance} & \footnotesize{Average marginal Wasserstein distances} & \footnotesize{Max marginal Wasserstein distance}\vspace{-2pt}\\
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/rank_hist_distance.pdf} & 
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/average_marginal_w_dist.pdf} &
	\includegraphics[width=0.3\textwidth]{fig/precip_metrics/max_marginal_w_dist.pdf}    
\end{tabular}
\caption{Quantitative metrics for learning the regional precipitation distribution as a function of the number of steps in the RML method.}\label{fig:precip_metric}
\end{figure}

\begin{figure}
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.35\textwidth]{fig/precip_metrics/rankhist_t3.pdf} \hspace{4pt}&\hspace{4pt}
%		\includegraphics[width=0.3\textwidth]{fig/precip_metrics/rankhist_t4.pdf} &
		\includegraphics[width=0.35\textwidth]{fig/precip_metrics/rankhist_t7.pdf}\\
		$T=3$ \hspace{4pt}&\hspace{4pt} $T=7$
	\end{tabular}
	\caption{Rank histograms of RML samples.}\label{fig:rankhist}
\end{figure}

\begin{figure}
	\centering
	\begin{tabular}{@{}c@{}ccc@{}}
		&\small{Factor of $2^2$ ($T=7$)} & \small{Factor of $4^2$ ($T=4$)} & \small{Factor of $8^2$ ($T=3$)}\\
		\rotatebox[origin=c]{90}{\small{$t=0$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t0.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool4x/img_epoch100_t0.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool8x/img_epoch100_t0.png} \\
		\rotatebox[origin=c]{90}{\small{$t=1$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t1.png}\hspace{4pt} & &\\
		\rotatebox[origin=c]{90}{\small{$t=2$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t2.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool4x/img_epoch100_t1.png}\hspace{4pt} &
		 \\
		\rotatebox[origin=c]{90}{\small{$t=3$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t3.png}\hspace{4pt} & \hspace{4pt}&
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool8x/img_epoch100_t1.png}\\
		\rotatebox[origin=c]{90}{\small{$t=4$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t4.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool4x/img_epoch100_t2.png}\hspace{4pt} &		\\
		\rotatebox[origin=c]{90}{\small{$t=5$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t5.png}\hspace{4pt} & & \\
		\rotatebox[origin=c]{90}{\small{$t=6$}}\hspace{4pt}\smallskip & \includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool2x/img_epoch100_t6.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool4x/img_epoch100_t3.png}\hspace{4pt} &
		\includegraphics[width=.17\textwidth,align=c]{fig/precip/avgpool8x/img_epoch100_t2.png} 	\vspace{-0.05in}	
	\end{tabular}
	\caption{Generated samples at different time steps (resolutions) for monthly precipitation data.}\label{fig:precip_all_t}
\end{figure}


\section{Conclusion}

In this work, we introduced Reverse Markov Learning (RML), a novel approach to learning complex distributions through a multi-step generative process. Our method extends the concept of engression by constructing a reverse Markov process that incrementally reconstructs the target distribution from a known prior distribution. This allows for improved modeling of complex distributions while maintaining computational efficiency.

We established the theoretical correctness of our approach by proving that the proposed reverse process reconstructs the target distribution under mild assumptions. We also demonstrated that RML generalizes existing methods such as diffusion models and flow matching, while offering a more flexible and computationally efficient framework for high-dimensional generative modeling.

Our empirical studies validate the effectiveness of RML in both synthetic and real-world scenarios. In a simple illustrative example with Gaussian mixtures, RML successfully captured the true distribution with a small number of steps, avoiding the oversmoothing issue observed in one-step engression. In a more challenging application to regional precipitation prediction, RML effectively generated realistic precipitation fields, as demonstrated in both visualizations and quantitative metrics. These results highlight the advantages of our framework in learning high-dimensional distributions with structured dependencies.

Overall, RML provides a promising approach to generative modeling with several advantages: (i) it is more capable of learning complex distributions than one-step generative models; (ii) it allows for general forward processes, including those with dimension reduction, thus offering a flexible framework that can be tailored to various applications; (iii) it is computationally more efficient than diffusion models, as it avoids unnecessary high-dimensional computations and naturally discretizes the generative process in finite steps. %; and (iii) it offers a flexible framework that can be tailored to various applications, including scientific and real-world data modeling.
 
 
\bibliography{ref.bib}
\bibliographystyle{apalike}

%\newpage
%\appendix
%\input{appendix.tex}
 

\end{document}

%  LocalWords:  maketitle z_k,y ldots,x_N,y cdot ldots x_k,y x_k,y
%  LocalWords:  x_k,y qquad ldots,1 _k,y z,y,k y,t z,y,t z,y,t y,s
