\section{Related Work}
Label Distribution Learning (LDL) ____ is a learning paradigm designed to predict label distributions that capture the degree of association between each instance and its labels. Unlike traditional binary or multi-label learning, LDL provides a more detailed and comprehensive framework by leveraging \textit{label description degrees}, which quantify the relevance of each label to an instance ____. Since its inception, various models have been developed to enhance LDL’s modeling capabilities, such as adapting support vector machines, decision trees, and embedding-based methods, to represent label distributions effectively.

Existing LDL methods assume accurate label distributions, but in real-world scenarios, annotators’ biases, limited expertise, or label familiarity often lead to deviations between annotated and true distributions ____. This motivates the study of \textit{label distribution learning with bias (LDL with biased distributions)}, which focuses on recovering true label distributions while mitigating annotator-induced biases. Our work addresses this challenge by leveraging the hierarchical relationship between biased label distributions and their corresponding multi-label representations.

\textbf{Notations}: Denote \(\mathbf{X} \in \mathbb{R}^{d \times n}\) as the feature matrix, where \(d\) is the feature dimensionality and \(n\) is the number of instances. The label space is \(\mathcal{Y} = \{y_1,\ldots, y_m\}\), where \(m\) is the number of labels. The accurate training set for LDL is \(\mathcal{T} = \{(\mathbf{x}_i, \mathbf{d}_i)\}_{i=1}^n\), with \(\mathbf{d}_i = [d_{\mathbf{x}_i}^{y_1}, \cdots, d_{\mathbf{x}_i}^{y_m}]^\top \in \mathbb{R}^m\), satisfying \(d_{\mathbf{x}_i}^y \geq 0\) for all \(y \in \mathcal{Y}\) and \(\sum_{y} d_{\mathbf{x}_i}^y = 1\). The label distribution matrix is \(\mathbf{D} = [\mathbf{d}_1,\ldots, \mathbf{d}_n] \in \mathbb{R}^{m \times n}\). We assume the observed label distribution \(\hat{\mathbf{D}} \in \mathbb{R}^{m \times n}\) is biased, while the true label distribution is unknown. The goal is to learn a decision function \(\mathfrak{G}: \mathbb{R}^{d \times n} \to \mathbb{R}^{m\times n}\) using the training set \(\{\mathbf{X}, \hat{\mathbf{D}}\}\), such that \(\mathfrak{G}(\mathbf{X}_{i:}) \approx {\mathbf{D}}_{i:}\).