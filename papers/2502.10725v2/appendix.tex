\clearpage
\appendix
\section{PropNet Construction}
\subsection{Cases}
\label{app:propnet_cases}

Figure~\ref{fig:cs_3v_repr} shows the PropNet representation of a sentence with three propositions. The instance nodes pointing to \texttt{\_unknown} are all omitted, as well as their evolutionary nodes, in order to provide a clean view. From this representation, we can form a picture of what happens in our mind.

A more complex sentence with five propositions is presented in Figure~\ref{fig:cs_5v_repr}. Its splitting and merging processes are explained in Figure~\ref{fig:split_and_merge_cs_plus3v}. Notably, the subject of the last proposition is incorrect. The correct result should be ``man takes it out on the water''. A more robust strategy of subject identification in the splitting phase will be added in future work.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/cs_3v_repr.pdf}
  \caption{PropNet for ``A man is sitting in a chair wearing a cloak and holding a stick.'' It merges the representations of three propositions: ``a man is sitting in a chair'', ``man be wearing a cloak'', and ``man be holding a stick'', each enclosed by separate grey frames.}
  \label{fig:cs_3v_repr}
\end{figure*}


\begin{figure*}[h!]
  \includegraphics[width=1\textwidth]{figure/cs_5v_repr.pdf}
  \caption{PropNet for ``A black and white dog jumping in the air to catch a Frisbee when a man is getting his boat clean to take it out on the water.'' It merges the representations of five propositions: ``a black and white dog jumping in the air to identifier\_advcl'', ``dog catch a Frisbee'', ``a man is getting identifier\_ccomp to identifier\_advcl'', ``his boat clean'', and ``boat take it out on the water'', each enclosed within five grey frames.}
  \label{fig:cs_5v_repr}
\end{figure*}


\subsection{Relative Clause}
\label{app:propnet_relative_clause}

As a relative clause is more complex than other types of clauses, additional rules are proposed to address its splitting and merging. Currently, only relative pronouns \texttt{that}, \texttt{which}, \texttt{who} and relative adverbs \texttt{when}, \texttt{where} are considered for a \texttt{Prop2} sentence with a relative clause. Table~\ref{tab:splitting_rules_cs2v_relcl} shows the splitting and merging rules with examples. The merging results of these examples are shown in Figure~\ref{fig:relative_pronoun}, Figure~\ref{fig:relative_adv} and Figure~\ref{fig:no_relative}, respectively. It should be noted that the action stamp nodes of main proposition and subordinate proposition are linked by the evolutionary node \texttt{\#time\_same}. This merging rule is not included in the table. 



\begin{table*}[h]
\centering
\caption{Splitting and Merging Rules for a \texttt{Prop2} Sentence with a Relative Clause.}
\label{tab:splitting_rules_cs2v_relcl}
\small 
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{@{}p{1.2cm}p{4.5cm}p{2.7cm}p{4.8cm}@{}}
\toprule
\textbf{Type} & \textbf{Splitting Rule} & \textbf{Merging Rule} & \textbf{Example} \\ \midrule

Relative Pronouns & If \texttt{t.lemma\_ in (\textbf{that}, \textbf{which}, \textbf{who})} and \texttt{t.dep\_ in (\textbf{nsubj}, \textbf{dobj})}, replace \texttt{t} by the antecedent in subordinate proposition. & Merge the two instance nodes of antecedent. & The \textbf{book} that I borrowed from the library is interesting. -> (the \textbf{book} is interesting, I borrowed \textbf{book} from the library) [Merge instance nodes of \textbf{book}.]\\

Relative Adverbs & If \texttt{t.lemma\_ in (\textbf{when}, \textbf{where})} and \texttt{t.dep\_ in (\textbf{advmod})}, keep \texttt{t} in subordinate proposition. & Link instance node of antecedent to action stamp node of subordinate proposition. & I like the \textbf{place} where I live in the street. -> (I like the \textbf{place}, where I live in the street) [Link instance node of \textbf{place} to action stamp node invoked by \textbf{live}]\\
No Relatives & No processing in subordinate proposition. & Link instance node of antecedent to action stamp node of subordinate proposition. & The \textbf{book} I borrowed from the library is interesting. -> (the \textbf{book} is interesting, I borrowed from the library) [Link instance node of \textbf{book} to action stamp node invoked by \textbf{borrowed}]\\ \bottomrule
\end{tabular}
\end{table*}


\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{relative_noun2.pdf}
  \caption{A relative pronoun example ``The book that I borrowed from the library is interesting.'' It is split into two propositions: ``the book is interesting'', ``I borrowed book from the library''. The instance nodes of \textbf{book} are merged into \texttt{ins\_entity\_17}.}
  \label{fig:relative_pronoun}
\end{figure*}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{relative_adv.pdf}
  \caption{A relative adverb example ``I like the place where I live in the street.'' It is split into two propositions: ``I like the place'', ``where I live in the street''. The instance node of \textbf{place}, \texttt{ins\_entity\_19}, is linked to the action stamp node \texttt{stamp\_action\_41}, invoked by \textbf{live}.}
  \label{fig:relative_adv}
\end{figure*}


\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{no_relative.pdf}
  \caption{A no-relative example ``The book I borrowed from the library is interesting.'' It is split into two propositions: ``the book is interesting'', ``I borrowed from the library''. The instance node of \textbf{book}, \texttt{ins\_entity\_17}, is linked to the action stamp node \texttt{stamp\_action\_41}, invoked by \textbf{borrow}.}
  \label{fig:no_relative}
\end{figure*}


\subsection{Parsing}
\label{app:propnet_parse}


This section provides a detailed explanation of the primary parsing rules. For  
\texttt{Action}, \texttt{Subject}, \texttt{Object}, \texttt{Attribute} and \texttt{Part\_of}, the parsing primarily relies on \texttt{token.dep\_} and \texttt{token.pos\_}. For \texttt{Where}, \texttt{Auxiliary\_Object}, \texttt{Goal}, \texttt{Reason} and \texttt{Source}, the parsing system depends on detecting and classifying token's prepositions. The classification is based on the definitions of prepositions from the Merriam-Webster Dictionary. We acknowledge that current classification remains course and occasionally inaccurate. For example, in ``shoot at an elephant'', ``elephant'' is classified as \texttt{Where}. However, this \texttt{Where} should be interpreted as the action goal in conjunction with the meaning of "shoot". This issue can be addressed by adding more evolutionary nodes. Additionally, the temporal attributes of an action, such as its timing, are temporarily assigned to \texttt{Where}. More sophisticated space-time modules are planned for development in future research. 

The complete list of preposition categories is as follows:

\begin{itemize}
    \item \texttt{Where}: ``on'', ``in'', ``inside'', ``through'', ``over'', ``around'', ``down'', ``at'', ``near'', ``along'', ``outside'', ``past'', ``across'', ``during'', ``after'', ``before'', ``while'', ``whilst'', ``off'', ``amid'', ``behind''. 
    \item \texttt{Auxiliary\_Object}: ``with'', ``by'', ``about'', ``like'', ``as''.
    \item \texttt{Goal}: ``into'', ``to'', ``onto'', ``towards'', ``against''
    \item \texttt{Reason}: ``for'', ``due''.
    \item \texttt{Source}: ``from''.
    
\end{itemize}


\section{Comparing Two Sentences}
\subsection{Comparison Dimensions}
\label{sec:compare_dim}

Table~\ref{tab:dimensions_diff_vec} explains the comparison dimensions to compute a difference vector given two sentence representations. Be aware that since instance nodes themselves have no meanings, what are really compared are the corresponding developmental nodes.

\begin{table*}[h]
\centering
\begin{tabular}{@{}l@{}p{10cm}@{}}
\toprule
\textbf{Dimension} & \textbf{Explanation} \\
\midrule
\texttt{\#action} & Find and compare all the instance nodes that represent actions. \\
\midrule
\texttt{\#action|\#subject} & Find and compare all the instance nodes that represent the subject of an action. \\
\texttt{\#action|\#subject|\#attr} & Find and compare all the instance nodes that belong to attributes of an entity or concept that serves as the subject of the action. \\
\texttt{\#action|\#subject|\#part\_of} & Find and compare all the instance nodes that represent parts of an entity or concept that serves as the subject of the action. \\
\texttt{\#action|\#subject|\#where} & Find and compare all the instance nodes that represent the location of the subject of the action. \\
\midrule

\texttt{\#action|\#object} & Find and compare all the instance nodes that represent the object of an action. \\
\texttt{\#action|\#object|\#attr} & Find and compare all the instance nodes that belong to attributes of an entity or concept that serves as the object of the action. \\
\texttt{\#action|\#object|\#part\_of} & Find and compare all the instance nodes that represent parts of an entity or concept that serves as the object of the action. \\
\texttt{\#action|\#object|\#where} & Find and compare all the instance nodes that represent the location of the object of the action. \\
\midrule

\texttt{\#action|\#aux\_obj} & Find and compare all the instance nodes that represent auxiliary objects of an action. \\
\texttt{\#action|\#aux\_obj|\#attr} & Find and compare all the instance nodes that belong to attributes of an entity or concept that serves as the auxiliary object of the action. \\
\texttt{\#action|\#aux\_obj|\#part\_of} & Find and compare all the instance nodes that represent parts of an entity or concept that serves as the auxiliary object of the action. \\
\midrule

\texttt{\#action|\#where} & Find and compare all the instance nodes that represent the location of the action. \\
\texttt{\#action|\#where|\#attr} & Find and compare all the instance nodes that belong to attributes of the location of the action. \\
\texttt{\#action|\#where|\#part\_of} & Find and compare all the instance nodes that represent parts of the location of the action. \\
\midrule

\texttt{\#action|\#goal} & Find and compare all the instance nodes that represent the goal of the action. \\
\texttt{\#action|\#goal|\#attr} & Find and compare all the instance nodes that belong to attributes of the goal of the action. \\
\texttt{\#action|\#goal|\#part\_of} & Find and compare all the instance nodes that represent parts of the goal of the action. \\
\midrule

\texttt{\#action|\#source} & Find and compare all the instance nodes that represent the source of the action. \\
\texttt{\#action|\#source|\#attr} & Find and compare all the instance nodes that belong to attributes of the source of the action. \\
\texttt{\#action|\#source|\#part\_of} & Find and compare all the instance nodes that represent parts of the source of the action. \\
\midrule

\texttt{\#action|\#reason} & Find and compare all the instance nodes that represent the reason for the action. \\
\texttt{\#action|\#reason|\#attr} & Find and compare all the instance nodes that belong to attributes of the reason for the action. \\
\texttt{\#action|\#reason|\#part\_of} & Find and compare all the instance nodes that represent parts of the reason for the action. \\
\bottomrule
\end{tabular}
\caption{Comparison Dimensions for Computing A Difference Vector. }
\label{tab:dimensions_diff_vec}
\end{table*}

\subsection{Similarity between Two Words}
\label{app:llm_lexical_similarity}

Given two words, we use Doubao \footnote{\url{https://team.doubao.com/en/direction/llm}} to compute a similarity score ranging from 0 to 1. Since this task is straightforward, other LLM models like ChatGPT or Llama should work as well. 

For a verb pair, its prompt is "From physical action perspective, return similarity score (0\textasciitilde1) between two verbs: \{word\_text1\} and \{word\_text2\}. Only return the score".

For other pairs, its prompt is "From semantic perspective, return similarity score (0\textasciitilde1) between two words: \{word\_text1\} and \{word\_text2\}. Only return the score".

\begin{table*}[h!]
\centering
\renewcommand{\arraystretch}{2}
\small 
\begin{tabular}{|p{0.04\textwidth}|p{0.13\textwidth}|p{0.35\textwidth}|p{0.3\textwidth}|}
\hline
\textbf{Code} & \textbf{Meaning} &\textbf{Calculation Rules} & \textbf{Examples} \\
\hline
0 & Completely or Almost the Same & 
\makecell[l]{
(1) Two words have the same text; \\
(2) LLM similarity score >= 0.7; \\
(3) binary group similarity score >= 0.7
} 
& 
\makecell[l]{
(1) "guitar" vs "guitar"; \\
(2) "cat" vs "kitten" \\
(3) ["red", "cat"] vs ["pink", "cat"]
} \\
\hline
1  & Similar & 
\makecell[l]{
(1) 0.2 <= LLM similarity score < 0.7; \\
(2) One side is "unknown"; \\
(3) 0.2 <= binary group similarity score < 0.7\\
}
& 
\makecell[l]{
(1) "guitar" vs "piano"; \\
(2) "guitar" vs "unknown"; \\
"unknown" vs ["guitar", "piano"] \\
(3) ["short", "man"] vs ["tall", "man"]
} \\
\hline

2 & Different & \makecell[l]{(1) LLM similarity score < 0.2 \\ (2) binary group similarity score < 0.2} & \makecell[l]{(1) "tall" vs "short" \\ (2) ["short", "man"] vs ["big", "cat"]} \\
\hline
3 & Long and Equal Length & \makecell[l]{Two group words with equal length \\ which is greater than 2.} & \makecell[l]{["violin", "guitar", "piano"] \\ vs ["car", "bus", "truck"]} \\
\hline
4 & Different Length & \makecell[l]{(1) Single word and a group of words; \\(2) Two group words with different length.} & \makecell[l]{(1) "guitar" vs ["violin", "piano"] \\ (2) ["guitar", "piano"] \\ vs ["car", "bus", "truck"]} \\
\hline

\end{tabular}
\caption{Dimensional Similarity Code. }
\label{tab:similar_code}
\end{table*}

\subsection{Dimensional Similarity Code}
\label{app:similar_code}

Given a comparison dimension, it's common that both sentences have only one developmental node. For example, considering this pair "I like orange" and "she hates apple", \texttt{\#action} is "like" and "hates", \texttt{\#action|\#subject} is "I" and "she", and \texttt{\#action|\#object} is "orange" and "apple". But exceptions do exist that the developmental nodes might be more than one. For instance, in the representation of "I like orange and apple", \texttt{\#action|\#object} has two developmental nodes \texttt{\_orange} and \texttt{\_apple}. 

We define Dimensional Similarity Code (DSC) as an indicator for assessing similarity between two sentence representations under a specific comparison dimension. Three scenarios are considered: (1) comparing two words, (2) comparing a single word with a group of words, and (3) comparing two groups of words. The DSC codes and their corresponding computing rules are explained in Table~\ref{tab:similar_code}. 

Given two words, a code of 0 means two words are completely or almost the same. A code of 1 means they are similar, sharing some important details. Note that when we compare "unknown" with one single word or a group of words, they are regarded as similar. For example, in the comparison of "I cut an onion with a knife" and "I cut an onion", the dimension \texttt{\#action|\#aux\_obj} has "knife" for the first sentence and "unknown" for the second sentence, and this comparison receives a code of 1. A code of 2 means the two words are completely different. We manually check word pair cases along with their LLM scores to determine proper thresholds, best matching the DSC code standard. If LLM similarity score is no less than 0.7, a code of 0 is assigned. If LLM similarity score is less than 0.2, a code of 2 is assigned. Otherwise, a code of 1 is assigned.

% LLM similarity scores along with thresholds 0.2 and 0.7 are used to assign these DSC codes. The thresholds are determined by manually checking word pair cases, making them best match DSC code standard.

When comparing one single word with a group of words, or two groups of words of different lengths, we label this scenario as "different length" with a code of 4. An exception occurs when the single word is "unknown", in which case it receives a code of 1. Further refinement of similarity discrimination, incorporating semantic analysis, is left for future work.

Given two groups of words with equal length greater than 2, we label this situation as "long and equal length" and assign it a code of 3. More refined similarity discrimination, combined with semantics, is deferred to future work.

Finally, given two groups of words with an equal length 2, we consider a binary group similarity score which is calculated by the following steps:

\begin{enumerate}[label=(\arabic*)]
    \item Initialize \texttt{score} as 0.
    \item Directly calculate the intersection between two groups of words and obtain the length of the intersection.
    \item Add the length of the intersection to the \texttt{score} variable.
    \item Remove the intersecting parts from each word group.
    \item If each group contains only one remaining word, calculate the similarity score between these two words and add it to \texttt{score}. Otherwise do nothing.
    \item Divide \texttt{score} by 2 to obtain the binary group similarity score.
\end{enumerate}

This process is exemplified step by step by comparing ["apple", "banana"] and ["apple", "orange"] as follows:

\begin{enumerate}[label=(\arabic*)]
    \item Initialize \texttt{score} as 0.
    \item Intersection is ["apple"] and its length is 1.
    \item  \texttt{score} = 1 + \texttt{score}. Now \texttt{score} is 1.
    \item Remove "apple" from both groups. Now they become ["banana"] and ["orange"].
    \item Since each group has only one word left, LLM is used to calculate the similarity score which outputs a 0.2. Now \texttt{score} is 1.2.
    \item Binary group similarity score = score / 2 = 0.6
\end{enumerate}

As the final score lies between 0.2 and 0.7, a DSC code 1 (similar) is assigned for this binary-word groups comparison.


\section{Experiments}
\subsection{CART Training Details for STS Tasks}
\label{app:cart_train_details}
% The STS-B dataset consists of three files: \texttt{train.jsonl} (5749 samples), \texttt{validation.jsonl} (1500 samples) and \texttt{test.jsonl} (1379 samples). 

For the STS-B dataset, we tune only one parameter for CART training, the minimum number of samples at a leaf node (short as \texttt{min\_samples\_leaf}), on its \texttt{train.jsonl} (5749 samples) and \texttt{validation.jsonl} (1500 samples). The default values are adopted for other CART parameters, since finetuning models for higher Spearman's score is not a key point in our work. What we place high weight on, is finding the weaknesses of PropNet and its opitimization directions in future research. 

The SICK-R dataset contains a field named \texttt{SemEval\_set}, indicating the train/dev/test split. We use the training split (4439 samples) and dev split (495 samples) for tuning \texttt{min\_samples\_leaf}. 

CART Training is supported by the Scikit-Learn APIs, whose version is 1.6.0.


\subsection{Genres}
\label{app:genres}
STS-B dataset has a field \texttt{genre} with three values, which are explained as follows:

\begin{itemize}
    \item \texttt{main-captions}: image/video description, focusing on the main action or event in a video or an image. For example, "A woman is spreading mustard on a bread roll." The words have strong relations with physical concepts, as defined in \citep{10692615}.
    
    \item \texttt{main-news}: news, for instance, "Stones add second Hyde Park concert". The words have strong relations with social concepts.
    
    \item \texttt{main-forums}: forum question or answer, for instance, "This is a part answer to your question". The words appearing in such a sentence are mainly \texttt{mental}, as introduced in \citep{10692615}. 
\end{itemize}

Based on the action types introduced in \citep{Yang2024AutomaticEO}, a proposition of \texttt{main-captions}, \texttt{main-news} and \texttt{main-forums} correspond to \texttt{physical}, \texttt{mental} and \texttt{social} types, respectively.

% AbbVie cools on $ 55bn Shire deal after US tax changes

\subsection{Cognitive Differences in STS Tasks}
\label{app:experiment_cog_diff}
\subsubsection{Experimental Samples}
\label{app:experiment_cog_diff_samples}
% We select the \texttt{P1-} pairs from the STS-B dataset, which consist of sentences differing in only one dimension. Eight dimensions related to the verb of a proposition are considered: \texttt{\#action}, \texttt{\#subject}, \texttt{\#object}, \texttt{\#where}, \texttt{\#aux\_obj}, \texttt{\#goal}, \texttt{\#source}, and \texttt{\#reason}. For example, "A man is speaking" and "A man is cooking" is a valid pair for the \texttt{\#action} dimension, since they differ only in \texttt{\#action}. 

Due to insufficient pairs for \texttt{\#aux\_obj}, \texttt{\#goal}, \texttt{\#source}, and \texttt{\#reason}, these four dimensions are aggregated into one, labeled as \texttt{\#other}. Valid pairs from STS 2012-2016 are added for the \texttt{\#other} dimension. The experiment’s results are not affected by this supplement, since the scoring criteria remain consistent across STS 2012-2016 and STS-B datasets. 

All pair examples selected for \texttt{main-captions} and \texttt{main-news} in this experiment are listed in Table~\ref{tab:exp3_caption_examples} and Table~\ref{tab:exp3_news_examples} respectively. Pair distributions,  ground score means and standard deviations for \texttt{main-captions} and \texttt{main-news} are listed in Table~\ref{tab:exp3-stat-main-captions} and Table~\ref{tab:exp3-stat-main-news} respectively. 

\begin{table}[H]
\centering
\begin{tabular}{@{}p{1.5cm}p{1cm}p{1.5cm}p{0.8cm}p{0.8cm}@{}}
\toprule
\textbf{Dimension} & \textbf{Count} & \textbf{Proportion (\%)} & \textbf{Mean} & \textbf{STD} \\
\midrule
% \texttt{\#action} & 27 & 31.6 & 1.569 & 0.985 \\
% \texttt{\#subject} & 23 & 27.0 & 2.133 & 1.318 \\
% \texttt{\#object} & 33 & 42.6 & 2.202 & 1.052 \\
% \texttt{\#where} & 22 & 28.2 & 2.677 & 0.841 \\
% \texttt{\#other} & 4 & 5.2 & 2.525 & 0.804 \\

\texttt{\#action} & 24 & 27.91 & 1.40 & 0.83 \\
\texttt{\#subject} & 12 & 13.95 & 1.55 & 0.83 \\
\texttt{\#object} & 25 & 29.07 & 1.70 & 0.56 \\
\texttt{\#where} & 21 & 24.42 & 2.71 & 0.85 \\
\texttt{\#other} & 4 & 4.65 & 2.53 & 0.80 \\

\bottomrule
\end{tabular}
\caption{Pair statistics for \texttt{main-captions}. The total number of pairs is 86.}
\label{tab:exp3-stat-main-captions}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{@{}p{1.5cm}p{1cm}p{1.5cm}p{0.8cm}p{0.8cm}@{}}
\toprule
\textbf{Dimension} & \textbf{Count} & \textbf{Proportion (\%)} & \textbf{Mean} & \textbf{STD} \\
\midrule

\texttt{\#action}           & 6              & 12.77                    & 1.83          & 0.48         \\ \hline
\texttt{\#subject} & 10             & 21.28                    & 1.99          & 0.77         \\ \hline
\texttt{\#object}  & 12             & 25.53                    & 2.45          & 0.80         \\ \hline
\texttt{\#where}   & 9              & 19.15                    & 2.60          & 0.95         \\ \hline
\texttt{\#other}             & 10             & 21.28                    & 2.32          & 1.36         \\ 
\bottomrule
\end{tabular}
\caption{Pair statistics for \texttt{main-news}. The total number of pairs is 47.}
\label{tab:exp3-stat-main-news}
\end{table}


\subsubsection{Statistical Testing}
\label{app:experiment_cog_diff_testing}
Mann-Whitney U Test is performed to detect the statistical difference in ground score means between any two dimensions, as the ground scores are not normally distributed for all dimensions using Shapiro-Wilk Test. The significance level is set to 0.05. Results for \texttt{main-captions} are reported in Table~\ref{tab:u_test_caption}, indicating \texttt{\#action}, \texttt{\#subject} and \texttt{\#object} are significantly different from \texttt{\#where} and \texttt{\#other}. Although \texttt{\#subject} and \texttt{\#other} do not reach the significance level of 0.05, they are close to the significance level of 0.1. The situation for \texttt{\#object} and \texttt{\#other} is also similar. Results for \texttt{main-news} are reported in Table~\ref{tab:u_test_news}, revealing that there is no statistical significance.

The Levene's Test is conducted to assess the statistical difference in the ground score standard deviations between all pairs of dimensions. The significance level is set to 0.05. The results for \texttt{main-captions} and \texttt{main-news} are reported in Table~\ref{tab:levene_test_caption} and Table~\ref{tab:levene_test_news}, respectively. We do not find any statistically significant differences. 

\begin{table*}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dimension1} & \textbf{Dimension2} & \textbf{U-statistic} & \textbf{P-value} & \textbf{Statistical Significance (0.05)} \\
\hline
\#action & \#subject & 121.5 & 0.4595 & No \\
\hline
\#action & \#object & 217.5 & 0.1000 & No \\
\hline
\textbf{\#action} & \textbf{\#where} & \textbf{73.0} & \textbf{0.0000} & \textbf{Yes} \\
\hline
\textbf{\#action} & \textbf{\#other} & \textbf{16.5} & \textbf{0.0413} & \textbf{Yes} \\
\hline
\#subject & \#object & 126.5 & 0.4545 & No \\
\hline
\textbf{\#subject} & \textbf{\#where} & \textbf{47.0} & \textbf{0.0032} & \textbf{Yes} \\
\hline
\#subject & \#other & 10.5 & 0.1141 & No \\
\hline
\textbf{\#object} & \textbf{\#where} & \textbf{84.5} & \textbf{0.0001} & \textbf{Yes} \\
\hline
\#object & \#other & 21.5 & 0.0752 & No \\
\hline
\#where & \#other & 48.0 & 0.6815 & No \\
\hline
\end{tabular}
\caption{Results of the Mann-Whitney U Test for comparing \texttt{main-captions} dimensions.}
\label{tab:u_test_caption}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dimension1} & \textbf{Dimension2} & \textbf{U-statistic} & \textbf{P-value} & \textbf{Statistical Significance (0.05)} \\
\hline
\#action & \#subject & 23.5 & 0.5114 & No \\ \hline
\#action & \#object & 20.5 & 0.1570 & No \\ \hline
\#action & \#where & 14.0 & 0.1368 & No \\ \hline
\#action & \#other & 19.0 & 0.2526 & No \\ \hline
\#subject & \#object & 43.0 & 0.2747 & No \\ \hline
\#subject & \#where & 31.0 & 0.2682 & No \\ \hline
\#subject & \#other & 36.0 & 0.3049 & No \\ \hline
\#object & \#where & 49.5 & 0.7755 & No \\ \hline
\#object & \#other & 58.5 & 0.9472 & No \\ \hline
\#where & \#other & 47.5 & 0.8698 & No \\ \hline
\end{tabular}
\caption{Results of the Mann-Whitney U Test for comparing \texttt{main-news} dimensions.}
\label{tab:u_test_news}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dimension1} & \textbf{Dimension2} & \textbf{Statistic} & \textbf{P-value} & \textbf{Statistical Significance (0.05)} \\
\hline

\#action & \#subject & 0.0 & 0.9566 & No \\ \hline
\#action & \#object & 1.5 & 0.2269 & No \\ \hline
\#action & \#where & 0.0 & 0.9599 & No \\ \hline
\#action & \#other & 0.0 & 0.9543 & No \\ \hline
\#subject & \#object & 1.5 & 0.2234 & No \\ \hline
\#subject & \#where & 0.0 & 0.9935 & No \\ \hline
\#subject & \#other & 0.0 & 0.9291 & No \\ \hline
\#object & \#where & 1.4 & 0.2443 & No \\ \hline
\#object & \#other & 0.6 & 0.4614 & No \\ \hline
\#where & \#other & 0.0 & 0.9378 & No \\ \hline
\end{tabular}
\caption{Results of the Levene's Test for comparing \texttt{main-captions} dimensions.}
\label{tab:levene_test_caption}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dimension1} & \textbf{Dimension2} & \textbf{Statistic} & \textbf{P-value} & \textbf{Statistical Significance (0.05)} \\
\hline

\#action & \#subject & 2.0 & 0.1793 & No \\ \hline
\#action & \#object & 3.1 & 0.0986 & No \\ \hline
\#action & \#where & 3.4 & 0.0868 & No \\ \hline
\#action & \#other & 3.2 & 0.0967 & No \\ \hline
\#subject & \#object & 0.1 & 0.7885 & No \\ \hline
\#subject & \#where & 0.7 & 0.4097 & No \\ \hline
\#subject & \#other & 1.6 & 0.2186 & No \\ \hline
\#object & \#where & 0.5 & 0.5006 & No \\ \hline
\#object & \#other & 1.5 & 0.2343 & No \\ \hline
\#where & \#other & 0.4 & 0.5562 & No \\ \hline

\end{tabular}
\caption{Results of the Levene's Test for comparing \texttt{main-news} dimensions.}
\label{tab:levene_test_news}
\end{table*}

\subsubsection{Ground Score Rules}
\label{app:experiment_cog_diff_rules}

The ground score rules for STS-B and STS 2012-2016 are provided below, following the original papers \citep{Cer2017SemEval2017T1, Agirre2013SEM2S}.

\begin{itemize}
    \item \texttt{Score 5}: The two sentences are completely equivalent, as they mean the same thing.
    
    \item \texttt{Score 4}: The two sentences are mostly equivalent, but some unimportant details differ.
    
    \item \texttt{Score 3}: The two sentences are roughly equivalent, but some important information differs/missing.
    
    \item \texttt{Score 2}: The two sentences are not equivalent, but share some details.

    \item \texttt{Score 1}: The two sentences are not equivalent, but are on the same topic.
    
    \item \texttt{Score 0}: The two sentences are completely dissimilar.
    
\end{itemize}





\onecolumn

\onecolumn
\begin{center}
\begin{longtable}{|c|p{4cm}|p{4cm}|p{1.3cm}|p{1cm}|}
\caption{Sentence pair examples in \texttt{main-captions} with only one different dimension.}\label{tab:exp3_caption_examples} \\
\hline

\multicolumn{1}{|c|}{\textbf{Dimension}} & \multicolumn{1}{c|}{\textbf{Sentence1}} & \multicolumn{1}{c|}{\textbf{Sentence2}} & \multicolumn{1}{c|}{\textbf{Ground Score}} & \multicolumn{1}{c|}{\textbf{Data Source}} \\
\hline
\endfirsthead

\multicolumn{5}{c}%
{\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
\hline
\multicolumn{1}{|c|}{\textbf{Dimension}} & \multicolumn{1}{c|}{\textbf{Sentence1}} & \multicolumn{1}{c|}{\textbf{Sentence2}} & \multicolumn{1}{c|}{\textbf{Ground Score}} & \multicolumn{1}{c|}{\textbf{Data Source}} \\
\hline
\endhead

\hline \multicolumn{5}{|r|}{\textit{Continued on next page}} \\ \hline
\endfoot

\hline
\endlastfoot

\texttt{\#action}  & A man is cycling & A man is talking & 0.6 & STS-B \\ \hline
\texttt{\#action}  & A man is speaking & A man is cooking & 0.8 & STS-B\\ \hline
\texttt{\#action}  & A man is praying & A man is dancing & 0.75 & STS-B\\ \hline
\texttt{\#action}  & A man is dancing & A man is thinking & 1.2 & STS-B\\ \hline
\texttt{\#action}  & people walk home & People waiting & 1.6 & STS-B\\ \hline
\texttt{\#action}  & A man is speaking & A man is spitting & 0.636 & STS-B\\ \hline
\texttt{\#action}  & A man is dancing & A man is singing & 1.25 & STS-B\\ \hline
\texttt{\#action}  & A man is running & A man is singing & 1.25 & STS-B\\ \hline
\texttt{\#action}  & A man is speaking & A man is dancing & 1.2 & STS-B\\ \hline
\texttt{\#action}  & A man is dancing & A man is speaking & 1.2 & STS-B\\ \hline
\texttt{\#action}  & Two dogs fighting in the snow & Two dogs standing in the snow & 2.4 & STS-B\\ \hline
\texttt{\#action}  & two dogs running in the snow & A dog laying in the snow & 2.2 & STS-B\\ \hline
\texttt{\#action}  & The people are leaving the airplane & The people are entering the plane & 2.2 & STS-B\\ \hline
\texttt{\#action}  & Two men are sitting in the room & Two men are standing in a room & 3.0 & STS-B\\ \hline
\texttt{\#action}  & A woman is writing & A woman is swimming & 0.5 & STS-B\\ \hline
\texttt{\#action}  & A man is levitating & A man is talking & 0.8 & STS-B\\ \hline
\texttt{\#action}  & A woman is slicing up some meat & A woman is breading some meat & 2.25 & STS-B\\ \hline
\texttt{\#action}  & A man is praying & A man is running & 0.727 & STS-B\\ \hline
\texttt{\#action}  & A man is singing & A man is dancing & 0.5 & STS-B\\ \hline
\texttt{\#action}  & A man is running & A man is mooing & 1.0 & STS-B\\ \hline
\texttt{\#action}  & A man is dangling a mouse near a snake & A man is feeding a mice to a snake & 3.2 & STS-B\\ \hline
\texttt{\#action}  & A animal is eating & The animal is hopping & 0.4 & STS-B\\ \hline
\texttt{\#action}  & A group of people eat at a table outside & Group of elderly people sitting around a table & 2.8 & STS-B\\ \hline
\texttt{\#action}  & A man and a woman laughing & A man and a woman kiss & 1.2 & STS-B\\ \hline


\texttt{\#subject}  & The ballerina is dancing & A man is dancing & 1.75 & STS-B \\ \hline
\texttt{\#subject} & Men are playing soccer & Two teams play soccer & 3.0 & STS-B \\ \hline
\texttt{\#subject} & A slow lori walks around & A animal is walking around & 2.8 & STS-B \\ \hline
\texttt{\#subject} & The lamb is looking at the camera & A cat looking at the camera & 0.8 & STS-B \\ \hline
\texttt{\#subject} & A skateboarder jumps off the stairs & A dog jumps off the stairs & 0.8 & STS-B \\ \hline
\texttt{\#subject} & Raccoons are eating & A man is eating & 1.5 & STS-B \\ \hline
\texttt{\#subject} & A Woman is eating & A animal is eating & 1.6 & STS-B \\ \hline
\texttt{\#subject} & A boy and a girl is dancing in the rain & A man and woman is dancing in the rain & 2.6 & STS-B \\ \hline
\texttt{\#subject} & A black and white cow standing in a grassy field & A blue jay standing in a grassy field & 0.6 & STS-B \\ \hline
\texttt{\#subject} & Old green bottle sitting on a table & Three men in suits sitting at a table & 0.4 & STS-B \\ \hline
\texttt{\#subject} & Long - haired black dog standing in grassy field & A blue jay standing in a grassy field & 1.4 & STS-B \\ \hline
\texttt{\#subject} & Three dogs racing on a dirt track & Cars racing on a dirt track & 1.4 & STS-B \\ \hline


\texttt{\#object} & A woman is cutting onions & A woman is cutting tofu & 1.8 & STS-B\\ \hline
\texttt{\#object} & A man is slicing a tomato & A man is slicing a bun & 2.0 & STS-B\\ \hline
\texttt{\#object} & A woman is cutting tofu & A woman is cutting an onion & 2.4 & STS-B\\ \hline
% similar
% \texttt{\#object} & A man is eating a food & A man is eating a piece of bread & 3.4 & STS-B\\ \hline
\texttt{\#object} & A woman is cutting some fish & A woman is cutting tofu & 2.2 & STS-B\\ \hline
\texttt{\#object} & A woman is slicing ginger & A woman is cutting potatoes & 1.6 & STS-B\\ \hline
% no much difference
% \texttt{\#object} & A man climbing a rock - face & A man is climbing a rock wall & 4.6 & STS-B\\ \hline
\texttt{\#object} & A group of kids are having a jumping contest & A group of kids are having a sleepover & 1.2 & STS-B\\ \hline
\texttt{\#object} & The dog is chasing the geese & One dog is chasing the other & 1.6 & STS-B\\ \hline
% similar
% \texttt{\#object} & A woman is cooking eggs & A woman is cooking something & 3.0 & STS-B\\ \hline
\texttt{\#object} & A man is playing soccer & A man is playing flute & 1.0 & STS-B\\ \hline
\texttt{\#object} & A girl is riding a horse & A girl is riding a bicycle & 1.917 & STS-B\\ \hline
\texttt{\#object} & A man is playing a basketball & A man is playing a piano & 1.4 & STS-B\\ \hline
\texttt{\#object} & A man is playing an electronic keyboard & A man is playing a flute & 1.2 & STS-B\\ \hline
\texttt{\#object} & A guy is playing hackysack & A man is playing a key - board & 1.0 & STS-B\\ \hline
\texttt{\#object} & A woman is slicing a potato & A woman is slicing carrot & 2.5 & STS-B\\ \hline
\texttt{\#object} & A man is cutting up a potato & A man is cutting up carrots & 2.375 & STS-B\\ \hline
% no much difference
% \texttt{\#object} & The two men are wearing jeans & The two men are wearing pants & 4.4 & STS-B\\ \hline
\texttt{\#object} & The gate is blue & The gate is yellow & 1.6 \\ \hline
\texttt{\#object} & A man is slicing a bun & A man is slicing an onion & 2.4 & STS-B\\ \hline
\texttt{\#object} & A man is cutting up a potato & A man is cutting up carrots & 2.375 & STS-B\\ \hline
\texttt{\#object} & A woman is peeling a potato & A woman is peeling an apple & 2.0 & STS-B\\ \hline
\texttt{\#object} & The men are playing cricket & The men are playing basketball & 2.2 & STS-B\\ \hline
\texttt{\#object} & A woman is doing weight exercises & A woman is doing her hair & 0.5 & STS-B\\ \hline

\texttt{\#object} & The man is slicing a fish open & A man is slicing a potato & 2.2 & STS-B\\ \hline
\texttt{\#object} & A woman is cutting an apple & A woman is cutting potato & 1.8 & STS-B\\ \hline
\texttt{\#object} & A woman is cutting some flowers & A woman is cutting broccoli & 1.0 & STS-B\\ \hline
\texttt{\#object} & A woman is cutting potatoes & A woman is slicing carrots & 1.2 & STS-B\\ \hline
\texttt{\#object} & A woman is chopping garlic & A woman slices a fish & 1.0 & STS-B\\ \hline 



\texttt{\#where} & A yellow bird is eating fruit on a wire grate & A yellow bird eating fruit on a bird feeder & 2.8 & STS-B \\
\hline
\texttt{\#where} & Two dogs play in the grass & Two dogs playing in the snow & 2.8 & STS-B \\
\hline
\texttt{\#where} & A brown and white dog is running through the snow & a brown and white dog is running on the grass & 2.4 & STS-B \\
\hline
\texttt{\#where} & Three children playing in snow & Three children playing in hay & 2.4 & STS-B \\
\hline
\texttt{\#where} & Two dogs playing in snow & Two dogs playing in grass & 2.4 & STS-B \\
\hline
\texttt{\#where} & A jockey riding a horse in a pen & A jockey rides a horse at a gallop & 3.6 & STS-B \\
\hline
\texttt{\#where} & A red and white plane flying on a sunny day & Red and white plane flying through the air & 4.2 & STS-B \\
\hline
\texttt{\#where} & Two women sitting in lawn chairs & Two women are sitting in a cafe & 2.6 & STS-B \\
\hline
\texttt{\#where} & A white cat laying on an office chair & A white cat laying on a sheet & 2.4 & STS-B \\
\hline
\texttt{\#where} & Two girls walking in the ocean & two girls walking in the street & 1.8 & STS-B \\
\hline
\texttt{\#where} & Three dogs are playing in the white snow & Two dogs are playing in the grass & 1.6 & STS-B \\
\hline
\texttt{\#where} & A man and woman are driving down the street in a jeep & A man and woman are driving down the road in an open air vehicle & 4.0 & STS-B \\
\hline
\texttt{\#where} & Two men are fighting in a cow pasture & Two men are fighting in a cattle pen & 4.0 & STS-B \\
\hline
\texttt{\#where} & People sitting on the porch & People sitting on a couch & 1.4 & STS-B \\
\hline
\texttt{\#where} & A white jeep parked in front of a store & A white Jeep parked on a street & 3.5 & STS-B \\
\hline
\texttt{\#where} & The two dogs are running through the grass & Three dogs run in the snow & 2.2 & STS-B \\
\hline
\texttt{\#where} & A dog runs through the grass & A dog runs through the snow & 2.2 & STS-B \\
\hline
\texttt{\#where} & A black dog running in the snow & A black dog running on a beach & 1.8 & STS-B \\
\hline
\texttt{\#where} & a man walks two dogs on leashes down the street & A man walks two dogs in the city & 4.2 & STS-B \\
\hline
\texttt{\#where} & A dog is running in the snow & Two dogs running in the dirt & 2.2 & STS-B \\
\hline
\texttt{\#where} & A brown dog is running through green grass & A brown dog is running though a river & 2.4 & STS-B \\
\hline


\texttt{\#other} & The dog is running with food in his mouth & The dog is running with a yellow ball in his mouth & 2.2 & STS-B\\ \hline
\texttt{\#other}  & A baby tiger is playing with a ball & A baby is playing with a doll & 1.6 & STS-16\\ \hline
\texttt{\#other}  & A man is running with a bus &  A man runs with a truck & 2.5  & STS-16\\ \hline
\texttt{\#other}  & A cat is rubbing against baby's face & a cat is rubbing against a baby & 3.8 & STS-12\\ \hline

\end{longtable}
\end{center}

% ---------------------

\begin{center}
\begin{longtable}{|c|p{4cm}|p{4cm}|p{1.3cm}|p{1cm}|}
\caption{Sentence pair examples in \texttt{main-news} with only one different dimension.}\label{tab:exp3_news_examples} \\
\hline

\multicolumn{1}{|c|}{\textbf{Dimension}} & \multicolumn{1}{c|}{\textbf{Sentence1}} & \multicolumn{1}{c|}{\textbf{Sentence2}} & \multicolumn{1}{c|}{\textbf{Ground Score}} & \multicolumn{1}{c|}{\textbf{Data Source}} \\
\hline
\endfirsthead

\multicolumn{5}{c}%
{\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
\hline
\multicolumn{1}{|c|}{\textbf{Dimension}} & \multicolumn{1}{c|}{\textbf{Sentence1}} & \multicolumn{1}{c|}{\textbf{Sentence2}} & \multicolumn{1}{c|}{\textbf{Ground Score}} & \multicolumn{1}{c|}{\textbf{Data Source}} \\
\hline
\endhead

\hline \multicolumn{5}{|r|}{\textit{Continued on next page}} \\ \hline
\endfoot

\hline
\endlastfoot

\texttt{\#action} & Saudi Arabia gets a seat at the UN Security Council & Saudi Arabia rejects seat on UN Security Council & 2.8 & STS-B\\ \hline
\texttt{\#action} & Romney wins Florida Republican primary & Romney eyes US Republican primary endgame & 1.8 & STS-B\\ \hline
\texttt{\#action} & Hong Kong stocks close down 0.28 \% & Hong Kong stocks open 0.62 pct higher & 1.6 & STS-B\\ \hline
% This case don't have different #action, so it's aborted.
% \texttt{\#action} & Hrithik Roshan, wife Sussanne part ways & Hrithik Roshan, Sussanne to divorce & 4.4 & STS-B\\ \hline
\texttt{\#action} & French train derails south of Paris & French train passengers tell of crash ordeal & 1.2 & STS-B\\ \hline
\texttt{\#action} & Indian stocks open lower & Indian stocks close lower & 1.8 & STS-B\\ \hline

\texttt{\#action} & FAA continues ban on US flights to Tel Aviv & FAA lifts ban on U.S. flights to Tel Aviv & 1.8 & STS-B\\ \hline

\texttt{\#subject} & It has a margin of error of plus or minus 4 percentage points & That poll had 712 likely voters and sampling error of plus or minus 3.7 percentage points & 2.5  & STS-B\\ \hline
\texttt{\#subject} & 20 killed in bomb attack at Pakistani base & 8 soldiers killed in bomb attack in NW Pakistan & 2.0  & STS-B\\ \hline
\texttt{\#subject} & Pak religious body endorses underage marriage & CII endorses underage marriage & 3.0  & STS-B\\ \hline
\texttt{\#subject} & At least 15 killed in Nigeria church attack & At least 12 killed in Nigeria church bombing & 2.6  & STS-B\\ \hline
\texttt{\#subject} & Brooks pleads not guilty to hacking charges & Dave Lee Travis pleads not guilty to all charges & 1.4  & STS-B\\ \hline
\texttt{\#subject} & tirana is the capital of & abuja is the capital of & 1.0  & STS-B\\ \hline
\texttt{\#subject} & Santorum's 3 - year - old daughter hospitalized & 'Kony 2012' director hospitalized & 0.6  & STS-B\\ \hline
\texttt{\#subject} & 7 killed in attacks in Iraq & 27 killed in attacks across Iraq & 2.0  & STS-B\\ \hline
\texttt{\#subject} & Six Australians killed in Laos plane crash & Dozens killed in Laos plane crash & 3.0  & STS-B\\ \hline
% This case also has different #where, so it's aborted.
% \texttt{\#subject} & Workers killed in India building collapse & 76 killed in Bangladesh building collapse & 3.0  & STS-B\\ \hline
\texttt{\#subject} & Philip leaves hospital after 11 days & Mandela leaves hospital after 10 days & 1.8  & STS-B\\ \hline

\texttt{\#object} & US drone strike kills 11 in Pakistan & US drone kills 16 in Pakistan & 3.2 & STS-B\\\hline
\texttt{\#object} & Suicide attack kills eight in Baghdad & Suicide attacks kill 24 people in Baghdad & 2.4 & STS-B\\\hline
\texttt{\#object} & US drone strike 'kills 4 militants in Pakistan' & US drone strike kills 10 in Pakistan & 3.8 & STS-B\\\hline
\texttt{\#object} & US drone strike kills four in North Waziristan & U.S. drone strike kills 10 in northwest Pakistan: officials & 1.8 & STS-B\\\hline
\texttt{\#object} & Indonesian president to visit UK & Indonesian president to visit Australia & 1.4 & STS-B\\\hline
\texttt{\#object} & 6.0-magnitude quake hits northern Italy: USGS & 5.9-magnitude quake hits Sunda Strait, Indonesia: USGS & 1.6 & STS-B\\\hline
\texttt{\#object} & 6.3-magnitude earthquake hits Taiwan & 7.7-magnitude earthquake hits SW Pakistan & 1.2 & STS-B\\\hline
\texttt{\#object} & Roadside bombs kill 5 in Afghanistan & Roadside bomb kills 3 policemen in Afghanistan & 3.4 & STS-B\\\hline
\texttt{\#object} & US drone strike kills three in northwest Pakistan & US drone strike kills seven in North Waziristan & 2.4 & STS-B\\\hline
\texttt{\#object} & Iraq violence kills 11 & Iraq violence kills seven & 2.2 \\ \hline
\texttt{\#object} & Strong earthquake in western China kills 47 people & Strong earthquake in western China kills at least 75 & 3.0 & STS-B\\\hline
\texttt{\#object} & Bomb attacks kill 20 in Baghdad's Christian areas & Bombs Kill 35 in Baghdad Christian Area & 3.0 & STS-B\\\hline

\texttt{\#where} & Pakistan blocks Twitter over anti - Islamic material & Pakistan blocks Twitter over ' blasphemy ' & 3.8 & STS-B\\ \hline
\texttt{\#where} & Shenzhen stock indices close higher Monday & Shenzhen stock indices close lower -- Oct. 31 & 1.8 & STS-B\\ \hline
\texttt{\#where} & Two NATO soldiers killed in Afghanistan & NATO soldier killed in Afghan attack & 4.0 & STS-B\\ \hline
\texttt{\#where} & NATO Soldier Killed In Afghan Attack & NATO soldier killed in S. Afghanistan & 3.8 & STS-B\\ \hline
\texttt{\#where} & Two NATO soldiers killed in Afghanistan & NATO Soldier Killed in Afghan Blast & 3.6 & STS-B\\ \hline
\texttt{\#where} & ' Scores of bodies ' found in Syria & Eight more bodies found on ship & 1.4 & STS-B\\ \hline
\texttt{\#where} & Musharraf arrested in Lal Masjid case & Musharraf arrested in Pakistan & 3.0 & STS-B\\ \hline
\texttt{\#where} & Hushen 300 Index closes higher -- Oct. 14 & Hushen 300 Index closes lower -- March 12 & 2.0 & STS-B\\ \hline
\texttt{\#where} & Seven peacekeepers killed in Sudan's Darfur & Peacekeeper killed in Abyei clash & 1.6 & STS-B\\ \hline
\texttt{\#where} & Powerful 7.6 quake strikes off Solomons & Powerful 6.9 quake strikes off California coast & 2.2 & STS-B\\ \hline

\texttt{\#other} & 5 Things to Know About the Sochi Olympics & 7 Things to Know About Ethanol & 0.0 & STS-B\\ \hline
\texttt{\#other}  & van der merwe sentenced geiges to a total of 13 years imprisonment & van der merwe suspended geiges' sentence to 5 years imprisonment & 2.6 & STS-B\\ \hline
\texttt{\#other}  & Death toll in Nigeria police attack rises to 30 & Death toll in Kenya bus attack rises to six & 1.4 \\ \hline
\texttt{\#other}  & Residents return to Texas blast site & Residents return to Fallujah & 0.0 & STS-B\\ \hline
\texttt{\#other}  & China yuan strengthens to 6.2689 against USD & China yuan strengthens to new high against USD & 3.0 & STS-B\\ \hline
\texttt{\#other}  & Death toll from Philippine earthquake rises to 185 & Death toll from Philippines quake rises to 144 & 3.6 & STS-B\\ \hline
\texttt{\#other}  & Stars pay tribute to Cory Monteith & Stars pay tribute to James Garner & 2.2 & STS-B\\ \hline
\texttt{\#other}  & Death toll from Egypt protests rises to 49 & Death toll from Egypt violence rises to 638 & 3.2 & STS-B\\ \hline
\texttt{\#other}  & Newark mayor saves neighbor from fire & Newark mayor rescues neighbor from burning house & 4.2 & STS-B\\ \hline
\texttt{\#other}  & 10 Things to Know for Wednesday & 10 Things to Know for Today & 3.0 & STS-B\\ \hline

\end{longtable}
\end{center}

% --------------------------------

\twocolumn
