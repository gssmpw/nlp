\section{Introduction}


Sentence representation research has been carried out for over half a century, evolving from regular expressions \cite{1985Generalized}, statistical encoding methods \cite{harris54, Katz:87, 1968Automatic, jones72astatistical} to neural network embeddings \cite{mikolov2013efficient, bojanowski2016enriching, joulin2016bagtricksefficienttext, joulin2016fasttextzip, devlin2018bert, Reimers2019SentenceBERTSE, Gao2021SimCSESC, Li2024AoEAE}. In recent years, embedding models have dominated the research community, and also have achieved significant success in industry applications. However, their black-box nature and reliance on large-scale data-driven training raise concerns about bias \citep{conf/nips/BolukbasiCZSK16, Caliskan_2017, Brunet2018Understanding, Nadeem2021StereoSet}, trust \citep{Ribeiro2016why, conf/acl/RibeiroWGS20}, safety \citep{conf/naacl/BelinkovG19, conf/emnlp/PerezHSCRAGMI22} and other issues \citep{Bender2021On}. Researchers have attempted to improve the interpretability of embedding models \citep{conf/emnlp/JainBMMW18, conf/ijcnlp/LiaoLHL20} and remove their biases \citep{conf/naacl/GonenG19, conf/ltedi/GiraZL22}, but these problems remain fundamentally unsolved.  


In the cognitive science community, the question of how our brains represent a sentence has drawn a great deal of research interest. When humans read and comprehend a sentence, they decompose it into propositions. A proposition is a simple sentence with no more than one verb. The meanings of these propositions are stored in long-term memory as a hierarchical network, which can be recalled for use in downstream tasks \citep{Kintsch1978TowardAM, Collins1969RetrievalTF}. The brain's verbal short-term memory possesses a specialized subsystem. This subsystem is responsible for splitting propositions and handling the syntactic features of each proposition as the eyes sweep across the text\citep{1999Verbal}. Moreover, the cognitive load in sentence comprehension is primarily attributed to syntactic complexity, such as the number of verbs, rather than superficial length, such as the addition of adjectives and other modifiers \citep{1998Linguistic}. This implies proposition is a processing unit for sentence comprehension. Inspired by these findings, in this work, we propose a purely white-box and human-like sentence representation network named PropNet. 

\begin{table*}[h!]
\centering
%\small
\begin{tabular}{p{0.3\textwidth}|p{0.126\textwidth}|p{0.15\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}}
\hline
\textbf{Method} & \textbf{No Feature Engineering} & \textbf{Semantic Preservation} & \textbf{White\-Box} & \textbf{Human\-Like} \\ \hline
Regular Expressions & $\times$ & $\times$ & $\checkmark$ & $\times$ \\ \hline
Statistical Encoding & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ \\ \hline
Neural Network Embedding & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ \\ \hline
PropNet & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ \hline
\end{tabular}
\caption{Comparison of Sentence Representation Methods}
\label{tab:method_comparison}
\end{table*}

PropNet constructs a hierarchical network based on the
propositions comprising a sentence. The generation of PropNet involves four phases: splitting, parsing, representing and merging. The first two phases employ a mechanism analogous to short-term memory, whereas the last two phases function akin to long-term memory. Figure~\ref{fig:ss1} provides an example of PropNet.



PropNet is purely white-box since it is completely interpretable and transparent. It is also human-like because it has a hierarchy network structure to save the meaning in a sentence. With respect to semantics, dislike embedding models which focus more on the relationships between words and their contextual information, PropNet focuses on the true happenings in the world as expressed by a sentence. Therefore, given a proposition, eight dimensions related to a verb are considered: \texttt{Action}, \texttt{Subject}, \texttt{Object}, \texttt{Where}, \texttt{Auxiliary\_Object}, \texttt{Goal}, \texttt{Reason}, \texttt{Source}. Moreover, PropNet keeps the relationships among propositions in a sentence to convey a complete and coherent meaning. The principles behind PropNet's design are very natural, without specially crafted artificial features. The comparison of PropNet and other representation methods is summarized in Table~\ref{tab:method_comparison}. 

 

In semantic textual similarity (STS) tasks, given a pair of sentences, humans are required to assign a similarity score according to specific instructions. Cognitive processes play a key role in bridging sentences/instructions and annotation results. PropNet enables us to study this cognitive process. Experiments show that differences in actions, subjects or objects are a sufficient condition for humans to confirm an inequivalent proposition pair.

We compare PropNet with representative embedding methods on STS tasks. Although
experiments imply that PropNet has a large gap compared to state-of-the-art (SOTA) embedding models, case studies reveal significant potential for enhancement.



\section{Relative Work}

Text representation approaches could date back to rule-based regular expressions for search engines and question-answering systems \cite{1985Generalized}. Feature engineering is implemented heavily to develop expressions for pattern matching. This approach is inherently explainable, as the expressions consist of readable strings. However, it is pretty rigid and shallow, without a deep understanding of semantics. 

Statistical encoding methods compute word frequencies and represent text as discrete or continuous vectors. Typical methods include One Hot Embedding (OHE), Bag of Words (BOW) \cite{harris54}, N-grams \cite{Katz:87},  Normalized Term Frequency (NTF) \cite{1968Automatic} and TF-IDF \cite{jones72astatistical}. These methods are straightforward to implement since no feature engineering is required. They are also explainable, as each number in the representation vectors has a clear meaning. However, since word frequency cannot handle semantic issues, like synonyms and antonyms, or word relationships, these methods only provide shallow semantic analysis.

Neural network embedding approaches represent words as vectors in a continuous vector space. Representative methods include Word2Vec \cite{mikolov2013efficient}, FastText \cite{bojanowski2016enriching, joulin2016bagtricksefficienttext, joulin2016fasttextzip}, Glove \cite{2014Glove}, BERT \cite{devlin2018bert}, SBERT \citep{Reimers2019SentenceBERTSE}, SimCSE \citep{Gao2021SimCSESC} and AoE \citep{Li2024AoEAE}. They train neural networks on an extremely large corpus, using different Language Modeling (LM) tasks as the objective. Since embeddings are automatically computed by backpropagation, feature engineering is completely eliminated. In addition, the embeddings provide deep semantic understanding of the text, as they capture the semantic aspects of language due to LM tasks as the training goal. 

Embedding methods are considered as black-box algorithms because the embedding vectors are generally unexplainable. This characteristic surreptitiously encodes social biases contained in the training corpus \citep{conf/nips/BolukbasiCZSK16, Caliskan_2017, Brunet2018Understanding, Nadeem2021StereoSet}, and also raises concerns about trust \citep{Ribeiro2016why, conf/acl/RibeiroWGS20}, safety \citep{conf/naacl/BelinkovG19, conf/emnlp/PerezHSCRAGMI22} and other issues \citep{Bender2021On}. Numerous efforts have been made to improve interpretability \citep{conf/emnlp/JainBMMW18, conf/ijcnlp/LiaoLHL20} and remove biases \citep{conf/naacl/GonenG19, conf/ltedi/GiraZL22}, but these problems have not been fundamentally solved. 

All the aforementioned methods are not human-like, as their representation formats are either strings or vectors. Table~\ref{tab:method_comparison} summarizes the characteristics of these methods. 



    





