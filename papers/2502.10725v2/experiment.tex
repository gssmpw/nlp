\section{Experiments}

% Task (Problem Formulation)
% Datasets
% Baselines
% Evaluation Metrics
% Implementation Details
% Results

\subsection{Comparison with Embedding Models}

\begin{table*}[h!]
\small
\centering
\begin{tabular}{lcccccccc}
\toprule
Model & STS12 & STS13 & STS14 & STS15 & STS16 & STS-B & SICK-R & Avg. \\

\midrule
% GloVe (avg.) (2014) $\dagger$ & $55.14$ & $70.66$ & $59.73$  & $68.25$  & $63.66$  & $58.02$   & $53.76$ & $61.32$ \\ 
InferSent-GloVe (2017)$\dagger$  & $52.86$ & $66.75$ & $62.15$ & $72.77$ & $66.87$ & $68.03$ & $65.65$ & $65.01$ \\
USE (2018)$\dagger$  & $64.49$ & $67.80$ & $64.61$ & $76.83$ & $73.18$ & $74.92$ & $76.69$ & $71.22$ \\
SBERT (2019) $\dagger$ & $70.97$ & $76.53$ & $73.19$ & $79.09$ & $74.30$ & $77.03$ & $72.91$ & $74.89$ \\
SimCSE-BERT (2021) & $68.40$ & $82.41$ & $74.38$ & $80.91$ & $78.56$ & $76.85$ & $72.23$ & $76.25$ \\
% LLaMA2-7B $\ddagger$ & $50.66$ & $73.32$ & $62.76$ & $67.00$ & $70.98$ & $63.28$ & $67.40$ & $65.06$ \\
AoE-BERT (2024) & $75.26$ & $85.61$ & $80.64$ & $86.36$ & $82.51$ & $85.64$ & $80.99$ & $82.43$ \\

\midrule

PropNet & $\mathbf{47.01}$ & $\mathbf{36.33}$ & $\mathbf{40.74}$ & $\mathbf{34.54}$ & $\mathbf{12.06}$ & $\mathbf{47.41}$ & $\mathbf{55.35}$ & $\mathbf{39.06}$ \\ 

\bottomrule
\end{tabular}
\caption{Performance comparison with embedding models on STS tasks. We use Spearman's rank correlation coefficient ($\rho$) multiplied by 100 as evaluation metric. Results of the models with $\dagger$ are from \citep{Reimers2019SentenceBERTSE}, others are from their original papers. There is a significant gap between PropNet and the latest embedding models.
}
\label{table-main-sts-results}
\end{table*}

To assess the performance gap between PropNet and mainstream approaches, PropNet is compared with widely used embedding methods on semantic textual similarity (STS) tasks. 

\textbf{Baselines.} Given that our method is supervised, widely used supervised embedding methods are selected as baselines: InferSent \citep{2017Supervised}, USE \citep{Cer2018UniversalSE}, SBERT \citep{Reimers2019SentenceBERTSE}, SimCSE \citep{Gao2021SimCSESC}, and AoE \citep{Li2024AoEAE}.

\textbf{Datasets.} We conduct experiments on three benchmark datasets: STS 2012-2016 \citep{Agirre2012SemEval2012T6, Agirre2013SEM2S, Agirre2014SemEval2014T1, Agirre2015SemEval2015T2, Agirre2016SemEval2016T1}, STS-B \citep{Cer2017SemEval2017T1}, and Sick-R \citep{Marelli2014ASC}.

\textbf{Evaluation Metrics.} We employ Spearman's rank correlation coefficient ($\rho$) multiplied by 100 to measure the alignment between predicted and ground-truth scores. 

\textbf{Implementation Details. } The experiment employs a comparison module which consists of two components, (1) the calculation of the difference vector and (2) prediction using Classification and Regression Trees (CART) \citep{Breiman1984ClassificationAR}. Figure~\ref{fig:compare_module} presents the details of this module. After computing the difference vectors using the approach introduced in Section~\ref{sec: diff_vec} for all pairs within the benchmark datasets, two CART models are trained on these vectors: one for short pairs \texttt{P1-}/\texttt{P2} and another for long pairs \texttt{P3+}, because the sizes of the difference vectors are different. These models are denoted as \texttt{CART(P1-/P2)} and \texttt{CART(P3+)} respectively. We call the API provided by scikit-learn \footnote{API: \url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}} for training. Since STS 2012-2016 and STS-B share the same rules for ground scores, training is implemented only on the train set of STS-B to learn these rules. Similarly, for SICK-R, two CART models are trained on its training set. The random seed is fixed as 0, and the minimum number of samples at a leaf node is set to 10. Further training details are appended in Appendix~\ref{app:cart_train_details}.

Note that the predictions for short pairs and long pairs in the test set are concatenated before computing the Spearman's correlation.


\textbf{Results. } In Table~\ref{table-main-sts-results}, a significant gap is observed between PropNet and the latest embedding model, AoE, which scores above 80. This indicates the efficacy of big-data-trained embeddings in comprehending text semantics. This table also demonstrates that embedding models have undergone notable development over the last ten years, with scores ranging from 65.01 to 82.43. Since PropNet marks the inception of a completely new research direction, offering purely white-box and human-like sentence representation, we are confident that PropNet will improve in the future, considering that there is substantial room for enhancement in its current version, which is presented in the next section.

If we only focus on the scores of PropNet across all benchmarks, we observe a significant deviation, ranging from 12.06 on STS 2016 to 55.35 on SICK-R. The genre of STS 2016 is mainly forum question/answer and news, while SICK-R is image/video description. This genre-score discrepancy raises the question of whether PropNet has different representation abilities across different genres or if the comparison module fails for a specific genre. We investigate this factor in detail in the next section.



\subsection{Investigation of Genres}

In this section, we examine the performance of PropNet across different genres. The number of verbs is also considered as an important influencing factor. We anticipate that predicting accurately using our comparison module becomes more challenging for complex sentences.

\textbf{Dataset.} We utilize the STS-B test set \footnote{Approximately 8.7\% of the STS-B records lack similarity scores due to spaCy dependency parsing errors. These records are excluded from the analysis.} along with its predicted scores for analysis. The test set is split into three parts according to its field \texttt{genre}, which has three values: \texttt{main-captions}, \texttt{main-news} and \texttt{main-forums}. They represent image/video description, news and forum question/answer, respectively. Appendix~\ref{app:genres} provides more detailed explanations of these genres.


Another splitting dimension is verb number. The test set is divided into three parts according to pair types \texttt{P1-}, \texttt{P2}, and \texttt{P3+}. Generally, from \texttt{P1-} to \texttt{P3+}, each sentence in a pair contains an increasing number of verbs. Through intersection operation, pairs restricted to a specific genre and verb number can be selected.


\textbf{Evaluation Metrics.} Spearman's rank correlation coefficient ($\rho$) multiplied by 100, is used to measure the alignment between predicted and ground truth scores for a specific subset of the STS-B test set. Moreover, the number of pairs in each subset, and their proportion relative to the entire test dataset are calculated to illustrate data distribution.

\begin{table*}[h]
\centering
\begin{tabular}{@{\hspace{5pt}}l@{\hspace{5pt}}c@{\hspace{5pt}}c@{}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{}c@{\hspace{5pt}}c@{\hspace{5pt}}c}

	\multicolumn{1}{c}{\multirow{2}{*}{}} & \multicolumn{2}{c}{Total} & & \multicolumn{2}{c}{P1-} & & \multicolumn{2}{c}{P2} & &\multicolumn{2}{c}{P3+} \\
    
	\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
    
	& Count (\%) &  \  $\rho\times100$ & & Count (\%) & \ $\rho \times 100$& & Count (\%) &  \ $\rho \times 100$ & &  Count (\%) &  \ $\rho \times 100$\\
	\midrule
    
    \texttt{main-captions} & 616 (50\%)& 65.12 && 484 (39\%)& 70.41 & & 120 (10\%) & 43.15& &12 (1\%)&28.09 \\
    \texttt{main-news} & 411 (33\%)& 36.80 && 207 (17\%)&40.37 & & 124 (10\%) & 18.15& &80 (6\%) &20.22 \\
    \texttt{main-forums} & 212 (17\%)& 19.98 && 91 (7\%)& 43.53 & & 69 (6\%) & 26.70& &52 (4\%) &-0.04 \\
    Total & 1239 (100\%)&47.41 && 782 (63\%)& 59.76 & & 313 (25\%) & 26.11& &144 (12\%)&23.13 \\

	\bottomrule
\end{tabular}
\caption{Pair distributions and Spearmanâ€™s correlation scores corresponding to each genre and verb-number splitting. Count (\%) represents the number of pairs and their percentage of the total number. The results indicate that: (1) PropNet performs significantly better on \texttt{main-captions} than the other two genres; (2) As the number of verbs increases, Spearman scores decline significantly across all genres; (3) \texttt{main-news} and \texttt{main-forums} contain relatively more \texttt{P2} and \texttt{P3+} pairs than \texttt{main-caption}, implying these two genres have more complex sentence structures.}
\label{eval_error_anal}
\end{table*}

\textbf{Results.} As shown in Table~\ref{eval_error_anal}, PropNet scores 65.12 on \texttt{main-captions}, significantly higher than the 36.80 on \texttt{main-news} and 19.98 on \texttt{main-forums}. Even for the simplest pairs, \texttt{P1-}, PropNet attains only 40.37 on \texttt{main-news} and 43.53 on \texttt{main-forums}. This indicates that PropNet performs poorly on \texttt{main-news} and \texttt{main-forums}. The worst-performing pairs in \texttt{P1-}, whose prediction scores have the largest discrepancy with their ground scores, are analyzed to identify the underlying reasons. 

% PropNet fails for \texttt{main-news} and \texttt{main-forums}

For \texttt{main-news}, the lack of social common knowledge contributes to the failure of the comparison module for \texttt{P1-}. For instance, it cannot accurately measure the similarity between the pair "bitcoin haul" and "bitcoins" without knowledge of the terms "bitcoin" and "haul". Another reason is that the evolutionary graph of PropNet is too coarse to handle detailed comparisons. For example, in the sentence "At Northwest Medical Center of Washington County in Springdale , one child is in serious condition", the phrase "in serious condition" is incorrectly regarded as a description of \texttt{\#where}. However, it should be linked to child's \texttt{\#state}, which is not included in current PropNet's evolutionary graph. 

For \texttt{main-forums}, PropNet currently lacks nodes to measure the likelihood of an action, such as modal verbs (e.g., "can", "should") and negation indicators (e.g., "not", "hardly"). For example, it fails in comparing ``You should do it'' and ``You can do it'', or ``It's not a good idea'' and ``It's a good question''. In addition, \texttt{main-forums} also suffers from the coarseness of the evolutionary graph. For instance, expressions like "It is up to you" cannot be represented properly. 

Regarding verb number, Table~\ref{eval_error_anal} indicates that as the number of verbs increases, Spearman scores across all genres decrease substantially. The worst-performing pairs in \texttt{P2} for \texttt{main-news} and \texttt{main-forums} are analyzed to identify the reason. 

For \texttt{main-news}, the use of clear and concise language in its writing style sometimes causes difficulties for dependency parsing, leading to failed representations. For instance, consider the sentence "Coroner: Whitney Houston died in bathtub." PropNet cannot properly represent "Coroner: " as "Coroner claims". 

For \texttt{main-forums}, sentences describing mental activities are often redundant, which complicates the alignment of propositions for computing the difference vector in the comparison module. For instance, in the pair "There are two things to consider" and "A couple things to consider", the phrase "there are" hinders proper alignment. A potential improvement could be simplifying "There are two things to consider" to "two things to consider" during comparison. 

All the above case studies imply that there is a substantial improvement room in PropNet. Another point worthy of note is that, for \texttt{main-news} and \texttt{main-forums}, the internal proportions of \texttt{P2} and \texttt{P3+} are 49.6\% (204/411) and 57.1\% (121/212) respectively, which are significantly higher than 21.4\% (132/616) of \texttt{main-captions}. This suggests that sentences in \texttt{main-news} and \texttt{main-forums} contain more complex structures.



\subsection{Human Cognitive Differences in STS Tasks}
\label{cog_diff_in_sts}

\begin{figure*}[h]
  \centering
  \includegraphics[width=2\columnwidth]{experiment_part3_v2.pdf}
  \caption{(a) Mean Ground Scores. For \texttt{main-captions}, disparities in \texttt{\#action}, \texttt{\#subject} or \texttt{\#object} tend to make the two sentences appear more dissimilar than \texttt{\#where} and \texttt{\#other}. Additionally, a difference in \texttt{\#action}, \texttt{\#subject} or \texttt{\#object} is a sufficient condition for the non-equivalence of two propositions in human perception of sentence similarity. (b) Standard Deviation.}
  \label{fig:cap_vs_news}
\end{figure*}


% This section studies the cognitive process underlying how humans assign a score according to task instructions when given a sentence pair. Intuitively, within human cognition, each verb-related dimension adopted in Section~\ref{method:represent} ought to contribute differently to the disparity between two sentences. In addition, for the same dimension, human perceptions of the disparity should vary to a certain extent. We aim to quantify these cognitive differences in this experiment. 

% \textbf{Dataset.} The \texttt{P1-} pairs of the STS-B dataset, whose sentences differ at only one dimension, are selected. Compared to the black-box nature of embedding models, PropNet significantly facilitates this selection process by simply checking if the corresponding element of difference vector is 2. Only pairs from \texttt{main-captions} and \texttt{main-news} are used, since \texttt{main-forums} contains too few samples. In total, 133 valid pairs are collected for analysis. All pairs and their distribution are detailed in Appendix~\ref{app:experiment_cog_diff_samples}.

This section studies the cognitive process underlying how humans assign a score according to task instructions when given a sentence pair. Intuitively, different dimensions ought to contribute differently to the disparity between two sentences. In addition, for the same dimension, human perceptions of the disparity should vary to a certain extent. We aim to quantify these cognitive differences in this experiment. 

Eight dimensions from Section~\ref{pair_type_p1} are considered: \texttt{\#action}, \texttt{\#subject}, \texttt{\#object}, \texttt{\#where}, \texttt{\#aux\_obj}, \texttt{\#goal}, \texttt{\#source}, and \texttt{\#reason}. We aggregate \texttt{\#aux\_obj}, \texttt{\#goal}, \texttt{\#source}, and \texttt{\#reason} into one dimension, labeled as \texttt{\#other}. 

\textbf{Dataset.} The \texttt{P1-} pairs of the STS-B dataset, whose sentences differ at only one dimension, are selected. Compared to the black-box nature of embedding models, PropNet significantly facilitates this selection process by simply checking if the corresponding element of difference vector is 2. For example, "A man is speaking" and "A man is cooking" is a valid pair for the \texttt{\#action} dimension, since the difference vector has a value 2 at \texttt{\#action} and 0 at all other positions. Only pairs from \texttt{main-captions} and \texttt{main-news} are used, since \texttt{main-forums} contains too few samples. In total, 133 valid pairs are collected for analysis. All pairs and their distribution are detailed in Appendix~\ref{app:experiment_cog_diff_samples}.


\textbf{Evaluation Metrics.} The mean values of the ground-truth scores of the selected pairs within each dimension are calculated to reflect the human cognitive weight assigned to sentence similarity across the five dimensions. Additionally, the standard deviations of each dimension are provided to measure the cognitive deviations within a single dimension. 



\textbf{Results.} In Figure~\ref{fig:cap_vs_news} (a), for \texttt{main-captions}, the mean values of \texttt{\#action}, \texttt{\#subject} and \texttt{\#object} are around 1.5, which are lower than those of \texttt{\#where} and \texttt{\#other} (around 2.5), with statistical significance at the 0.05 level. This indicates that disparities in \texttt{\#action}, \texttt{\#subject} or \texttt{\#object} make the two sentences seem more different than the disparities in other two dimensions. For \texttt{main-news}, all dimensions have similar weights on difference perception, with no statistical significance. 

Another discovery is that pairs with disparity within the dimension \texttt{\#action}, \texttt{\#subject}, or \texttt{\#object} in both genres have ground scores below 2.5, which is the cut-off score for ``not equivalent'' and ``roughly equivalent''. This implies that a difference in any of these three dimensions is a sufficient condition for the non-equivalence of two propositions in human perception of caption or news sentence similarity. The ground score rules are provided in Appendix~\ref{app:experiment_cog_diff_rules}. Research regarding more complex sentence pairs, such as \texttt{P2}, is deferred to future work.

Figure~\ref{fig:cap_vs_news} (b) illustrates the differences in standard deviation for manual scoring. No statistically significant human cognitive deviation is observed within any single dimension at the 0.05 level. Overall, \texttt{main-captions} exhibits a standard deviation of approximately 0.8. For \texttt{main-news}, the notable difference between \texttt{\#action} and \texttt{\#other} presented in the figure is only statistically significant at the 0.1 level. Future work aims to collect more samples to reevaluate cognitive deviations within individual dimensions. All hypothesis testing results are supplied in Appendix~\ref{app:experiment_cog_diff_testing}.
