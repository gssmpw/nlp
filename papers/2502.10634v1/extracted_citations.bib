@inproceedings{Qin_2024,
   title={Dodo: Dynamic Contextual Compression for Decoder-only LMs},
   url={http://dx.doi.org/10.18653/v1/2024.acl-long.536},
   DOI={10.18653/v1/2024.acl-long.536},
   booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Qin, Guanghui and Rosset, Corby and Chau, Ethan and Rao, Nikhil and Van Durme, Benjamin},
   year={2024},
   pages={9961–9975} 
}

@misc{cao2024retainingkeyinformationhigh,
      title={Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs}, 
      author={Zhiwei Cao and Qian Cao and Yu Lu and Ningxin Peng and Luyang Huang and Shanbo Cheng and Jinsong Su},
      year={2024},
      eprint={2406.02376},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02376}, 
}

@misc{chuang2024learningcompresspromptnatural,
      title={Learning to Compress Prompt in Natural Language Formats}, 
      author={Yu-Neng Chuang and Tianwei Xing and Chia-Yuan Chang and Zirui Liu and Xun Chen and Xia Hu},
      year={2024},
      eprint={2402.18700},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.18700}, 
}

@misc{dai2023gptlearnincontextlanguage,
      title={Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers}, 
      author={Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei},
      year={2023},
      eprint={2212.10559},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10559}, 
}

@misc{fei2023extendingcontextwindowlarge,
      title={Extending Context Window of Large Language Models via Semantic Compression}, 
      author={Weizhi Fei and Xueyan Niu and Pingyi Zhou and Lu Hou and Bo Bai and Lei Deng and Wei Han},
      year={2023},
      eprint={2312.09571},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.09571}, 
}

@misc{jiang2023llmlinguacompressingpromptsaccelerated,
      title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models}, 
      author={Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2023},
      eprint={2310.05736},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05736}, 
}

@misc{jiang2024longllmlinguaacceleratingenhancingllms,
      title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression}, 
      author={Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2024},
      eprint={2310.06839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06839}, 
}

@misc{li2023compressingcontextenhanceinference,
      title={Compressing Context to Enhance Inference Efficiency of Large Language Models}, 
      author={Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin},
      year={2023},
      eprint={2310.06201},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06201}, 
}

@misc{liu2023lostmiddlelanguagemodels,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03172}, 
}

@misc{min2022rethinkingroledemonstrationsmakes,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.12837}, 
}

@misc{pan2024llmlingua2datadistillationefficient,
      title={LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression}, 
      author={Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Rühle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang},
      year={2024},
      eprint={2403.12968},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.12968}, 
}

@misc{vonoswald2023transformerslearnincontextgradient,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2023},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07677}, 
}

@misc{wang2023labelwordsanchorsinformation,
      title={Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning}, 
      author={Lean Wang and Lei Li and Damai Dai and Deli Chen and Hao Zhou and Fandong Meng and Jie Zhou and Xu Sun},
      year={2023},
      eprint={2305.14160},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14160}, 
}

@misc{xu2023recompimprovingretrievalaugmentedlms,
      title={RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation}, 
      author={Fangyuan Xu and Weijia Shi and Eunsol Choi},
      year={2023},
      eprint={2310.04408},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04408}, 
}

@misc{yoon2024compactcompressingretrieveddocuments,
      title={CompAct: Compressing Retrieved Documents Actively for Question Answering}, 
      author={Chanwoong Yoon and Taewhoo Lee and Hyeon Hwang and Minbyul Jeong and Jaewoo Kang},
      year={2024},
      eprint={2407.09014},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09014}, 
}

