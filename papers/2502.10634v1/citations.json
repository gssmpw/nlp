[
  {
    "index": 0,
    "papers": [
      {
        "key": "min2022rethinkingroledemonstrationsmakes",
        "author": "Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2023lostmiddlelanguagemodels",
        "author": "Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang",
        "title": "Lost in the Middle: How Language Models Use Long Contexts"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dai2023gptlearnincontextlanguage",
        "author": "Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei",
        "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"
      },
      {
        "key": "vonoswald2023transformerslearnincontextgradient",
        "author": "Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and Jo\u00e3o Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov",
        "title": "Transformers learn in-context by gradient descent"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2023labelwordsanchorsinformation",
        "author": "Lean Wang and Lei Li and Damai Dai and Deli Chen and Hao Zhou and Fandong Meng and Jie Zhou and Xu Sun",
        "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023compressingcontextenhanceinference",
        "author": "Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "jiang2023llmlinguacompressingpromptsaccelerated",
        "author": "Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jiang2024longllmlinguaacceleratingenhancingllms",
        "author": "Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
        "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "pan2024llmlingua2datadistillationefficient",
        "author": "Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor R\u00fchle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang",
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu2023recompimprovingretrievalaugmentedlms",
        "author": "Fangyuan Xu and Weijia Shi and Eunsol Choi",
        "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xu2023recompimprovingretrievalaugmentedlms",
        "author": "Fangyuan Xu and Weijia Shi and Eunsol Choi",
        "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chuang2024learningcompresspromptnatural",
        "author": "Yu-Neng Chuang and Tianwei Xing and Chia-Yuan Chang and Zirui Liu and Xun Chen and Xia Hu",
        "title": "Learning to Compress Prompt in Natural Language Formats"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yoon2024compactcompressingretrieveddocuments",
        "author": "Chanwoong Yoon and Taewhoo Lee and Hyeon Hwang and Minbyul Jeong and Jaewoo Kang",
        "title": "CompAct: Compressing Retrieved Documents Actively for Question Answering"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "fei2023extendingcontextwindowlarge",
        "author": "Weizhi Fei and Xueyan Niu and Pingyi Zhou and Lu Hou and Bo Bai and Lei Deng and Wei Han",
        "title": "Extending Context Window of Large Language Models via Semantic Compression"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "cao2024retainingkeyinformationhigh",
        "author": "Zhiwei Cao and Qian Cao and Yu Lu and Ningxin Peng and Luyang Huang and Shanbo Cheng and Jinsong Su",
        "title": "Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Qin_2024",
        "author": "Qin, Guanghui and Rosset, Corby and Chau, Ethan and Rao, Nikhil and Van Durme, Benjamin",
        "title": "Dodo: Dynamic Contextual Compression for Decoder-only LMs"
      }
    ]
  }
]