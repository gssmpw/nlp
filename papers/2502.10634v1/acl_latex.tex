% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs} 
\usepackage{tabularx}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{svg}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Lost in the Passage: \\Passage-level In-context Learning Does Not Necessarily Need a "Passage"}
\author{Hao Sun \quad Chenming Tang \quad Gengyang Li \quad Yunfang Wu\thanks{\ \ \ Corresponding author.} \\
National Key Laboratory for Multimedia Information Processing, Peking University \\
MOE Key Laboratory of Computational Linguistics, Peking University\\
School of Computer Science, Peking University \\
\texttt{\{2301213218, wuyf\}@pku.edu.cn} \quad 
\texttt{\{tangchenming, ligengyang\}@stu.pku.edu.cn}
\\
}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
By simply incorporating demonstrations into the context, in-context learning (ICL) enables large language models (LLMs) to yield awesome performance on many tasks. In this paper, we focus on passage-level long-context ICL for generation tasks and find that LLMs cannot learn the intrinsic relationships between the demonstration passage and 
the generation output.
We conduct  
experiments 
with different LLMs on two typical generation tasks including single-document QA and distractor generation, 
demonstrating that even a completely meaningless demonstration passage with 1/4 length 
achieves much better performance than the original full passage. 
Analysis via attention score reveals that LLMs pay little attention to 
passages compared to other components in prompt and little attention flows from the passage to other parts of the demonstration, which further confirms our finding. Additionally, experiments on context compression indicate that compression approaches proven effective on other long-context tasks are not 
suitable for passage-level ICL, since simply using shorter meaningless demonstration passages has achieved competitive performance. 
%We will release our code after the publication of this paper.

% This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
% The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
% These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}

With recent advancements in demonstration selection and prompt optimization, In-context Learning (ICL) has become an effective approach to enhancing large language models (LLMs). Instead of updating millions of model parameters, simply incorporating demonstrations into the context enables the model to learn more effectively, achieving better performance than in the zero-shot setting across various tasks. %such as text classification~\cite{min2022rethinkingroledemonstrationsmakes}. 
However, few studies on ICL focus on generation tasks, %while 
and existing research aimed at explaining the underlying mechanism of ICL primarily concentrates on tasks such as sentiment analysis or text classification~\cite{wang2023labelwordsanchorsinformation,min2022rethinkingroledemonstrationsmakes}. 

%Due to the constraints of computational resources and model's context window, 
Different from classification tasks, 
generation tasks, for instance question answering (QA) tasks, inherently require long contexts for both query and demonstrations, making it challenging to fit the ICL prompts into model's context window.
%contributing to the scarcity of research in ICL for generation tasks. 
In recent years, with advancements in computing hardware, training data and model architecture, 
%no longer have short context windows of only 512 to 2048 tokens like traditional Transformer based models~\cite{vaswani2023attentionneed}. Instead, 
the context window of current LLMs has been expanded to 8K, 32K and even 100K, 
%providing a model basis for ICL in generation tasks and 
allowing researchers to study ICL from the perspective of generation tasks.

However, in this paper, we observe a significant phenomenon in passage-level ICL for generation tasks: LLMs cannot learn the intrinsic relationships between the demonstration passage and the corresponding generation target and thus passage-level ICL does not necessarily need a regular well-formed \emph{"Passage"}. 
Specifically, 
%First, 
we use Mistral-7B~\cite{jiang2023mistral7b} and Llama2-13B~\cite{touvron2023llama2openfoundation,longlora} models to conduct experiments on two generation tasks: single-document question answering and sentence-level distractor generation. For each task, we conduct experiments with randomly generated passages and randomly sampled passages for demonstration. Experimental results show that LLMs are insensitive to demonstration passages in ICL. Even completely meaningless passage and generation %targets 
contents in demonstrations do not significantly impact performance. In some cases, they even outperform settings with real full passages.

Based on the finding of 
%previous 
prior experiments, we 
%further aim 
% try to 
validate the hypothesis 
%finding 
via attention analysis.
%analyzing attention scores 
%The attention score experiments consist of two parts: (1) 
First, we compute the average attention scores of the first generated token received from different components of the prompt. Second, we measure the attention scores transferred between the passage and other components within each demonstration. Through these experiments, we observe that the average attention scores LLMs receive from the passage are significantly lower than those from other components, and the attention score exchanging within the demonstration is also minimal. These results confirm that LLMs cannot 
%learn 
capture the 
%intrinsic 
%relationships 
cause-effect relationship between the demonstration
passage and %its corresponding 
the generation output. 
%target. %for QA tasks. 
%in the context of generation tasks.

Further, based on the prior experiments, % and interesting findings,  
%Lastly,
we explore context compression for 
%in-context learning of 
ICL. 
%With the development of long-context tasks and models, 
Compressing long contexts into compact texts while minimizing performance degradation has become a crucial approach to handling long-context tasks for effective ICL. 
%This not only reduces computational resource consumption but also %enhances model throughput and reduces 
%speedup inference.
%time. 
Most compression methods deal with long texts  
%focus on other long-context tasks 
rather than on ICL, where the long context contains information relevant to the query, and the main point is to retain key information while filtering out irrelevant content. However, in ICL, the demonstrations themselves do not explicitly contain
%relevant key 
information related to the query. %, and this constitutes the primary distinction between the two scenarios.
%In our work, we seek 
To investigate the effectiveness of compression methods for ICL, we perform compression experiments on prior passage-level tasks, and the results indicate that, under similar compression rates, the existing  compression methods do not 
%significantly 
outperform randomly generated or sampled 
%settings. 
passages, both for QA and DG tasks.

To sum up, %our contributions are as follows:
we conduct random perturbation experiments on two ICL tasks, and compute the average attention score and relative attention score during inference. Our results confirms that passage-level ICL does not necessarily need a regular \emph{"Passage"}. Further experiments of context compression show that conventional compression approaches do not provide superior performance to passage-level ICL since simply using random shorter passages has performed competitively. We hope %anticipate 
this work can inspire further research on the explanation for inner mechanism of ICL and demonstration compression in the passage-level ICL. 

\section{Single-document Question Answering}
\input{fig/prompt}
%The goal of this section is to verify whether large language models can learn the logical relationships between demonstration passages and their corresponding generation target during in-context learning of passage-level generation tasks.
%The goal of this section is to verify whether LLMs can effectively learn the intrinsic relationships between demonstration passages and their corresponding generation targets during ICL for passage-level generation tasks. To delve deeper into this issue, we 
To examine whether LLMs really comprehend the intrinsic relationships between the passage content and its generation targets during ICL for passage-level generation tasks,
%rather than simply relying on superficial pattern matching or simple imitation to generate results. 
%This study will contribute to a better understanding of the performance and underlying learning mechanisms of large language models in complex text generation tasks. 
%For this purpose, 
we conduct experiments on TriviaQA~\cite{joshi-etal-2017-triviaqa} from Longbench~\cite{bai2024longbenchbilingualmultitaskbenchmark}, which is a single-document question answering dataset designed for English reading comprehension. %Specifically, 
We introduce various random perturbations to the demonstrations in the context of ICL and measure the effect on model performance.
%to investigate whether the model effectively learns the intrinsic relationships between the passages and their corresponding QA pairs in the context of ICL.
\subsection{Experimental Setup}
\paragraph{Task Description}
In the QA task for reading comprehension of a single document, each test instance consists of a passage and a question, where the relevant information for the question can be retrieved from the passage. LLMs are required to generate the corresponding answer based on the passage and the question. Evaluation metrics include F1 score, which is used in Longbench, and exact match (EM).
%, considering the average length of the TriviaQA dataset,

\paragraph{LLMs} We use Mistral-7B-Instruct-v0.2~\cite{jiang2023mistral7b} as our primary LLM. It is an instruction-tuned variant of Mistral-7B, which supports a maximum context length of 32K tokens, making it particularly well suited for long-context tasks. Furthermore, we conduct experiments on the LongLoRA fine-tuned variant of the Llama2-13B~\cite{touvron2023llama2openfoundation} model: Llama2-13b-longlora-32k-ft~\cite{longlora}. The LongLoRA fine-tuning extends the context length of Llama2-13B to 32K tokens. All experiments were conducted on a single NVIDIA A100 GPU.

\paragraph{Prompt} Our prompt design adheres to the basic format of TriviaQA and \citet{qu-etal-2024-unsupervised}. Figure \ref{fig/pmt} shows the structure of our prompt and detailed prompt example can be seen in Appendix \ref{app:prompt}. While preserving the original prompt structure of TriviaQA, we incorporate task-specific instructions both before the demonstrations and the query. 

\paragraph{Passage Perturbation} In our systematic perturbation analysis, we mainly employ two methods to perturb the passages in the demonstrations: sampling and generation. For sampling-based perturbation, we randomly sample and reorder tokens from the original passage, ensuring that all the tokens are derived from the original text. 
%are preserved. 
For generation-based perturbation, we randomly generate a list of numbers as new \texttt{input\_ids} sequences, ensuring that the perturbed results are completely independent. Our goal is to validate the importance of the original passage tokens in the context of ICL through the comparison of these two methods.
% within the token input ids range of the original prompt

Our experiments contain two types of settings including 4-shot and full-shot. We apply perturbation ratios of 1/8, 1/4, 1/2, 3/4 for all 4-shot settings to the two demonstration passage perturbation methods, evaluating their effects on different LLMs. For full-shot settings, we apply perturbation ratios of 1/8, 1/4, and 3/10~\footnote{We use 3/10 because we cannot apply 1/2 due to limited memory.} .
Due to computational resource constraints, full-shot experiments with complete passages are infeasible. For comparison, we include 
%(primarily due to computational resource constraints, full-shot experiments with complete passages were infeasible) 
full-shot setting without any passage (that is, experiments where the passages in the demonstrations for ICL are entirely removed). For TriviaQA dataset, the full-shot setting means that we use all demonstrations in context for each input from the test data. Each input in the test data contains a different number of demonstrations, ranging from a minimum of 2 to a maximum of 25, with an average of 13.75 and a median of 14.

\subsection{Results and Discussion}
\label{qares}
\input{table/tqa_main_4-shot}
\input{table/tqa_main}
Experimental results of 4-shot and full-shot settings are presented in Table \ref{tab:tqa_main_4-shot} and \ref{tab:tqa_main} respectively.
The results for both models indicate that LLMs are unable to learn the intrinsic relationships between the passages and their corresponding generation targets in the demonstrations of ICL, and they look like \emph{"lost in the passage"}. 

All ICL results show significant improvement compared to the zero-shot setting, indicating that ICL is effective. However, both models demonstrate strong insensitivity to passage perturbations in passage-level ICL, with all results of passage perturbation exceeding the full passage settings.
On Mistral-7B, the F1 and EM scores show an average improvement of 3.37 points and 4.75 points compared to the full passage setting respectively. On Llama2-13B-longlora-32k-ft, the F1 and EM scores achieve an average gain of 2.85 points and 2.81 points respectively.
%Even the full-shot setting with no passage at all achieves better performance than the 6-shot with full prompt setting. which is not as good as other random perturbed settings. 
The 4-shot setting with 1/8 generated passage even reduce the average prompt length from 2780.90 to 909.64 (shortening the length by 67\%) while maintaining the performance, indicating that a significant portion of the context in passage-level ICL consumes computational resources while not contributing to the model's performance.
In contrast, as presented at the bottom of Table \ref{tab:tqa_main_4-shot}, when we try to perturb the question and answer in demonstrations, 
%and this 
it leads to a greater performance degradation than perturbing the passage, indicating that unlike passages, questions and answers in demonstrations are rather important for passage-level ICL.

Figure \ref{fig/passage} presents two examples of random passage we use in perturbation experiments. The passage on the left presents the random generated passage, which is completely unreadable and meaningless. The random generated words are more diverse, and it even contains words in other languages. On the contrary, the sampled passage on the right is more reasonable than the left passage, whose words are sampled from the original passage.

The comparison between generate and sample perturbation methods reveals that the settings with randomly generated, completely meaningless passages achieves comparable performance, sometimes even better, to that of sample settings. This indicates that the token sequences sampled from the original passages do not provide beneficial improvements to the model. Furthermore, except for the 4-shot experiment with Mistral-7B, in experiments involving passage content, the performance surpasses that of no passage settings, even when the generated tokens are entirely meaningless. This demonstrates that, in most cases, the presence of content in the passage position is more critical than better content in the passage position. 
%LLMs not only fails to learn the intrinsic relationships between demonstration passage and generation target, but also in fact \emph{"lost in the passage"}, leading to an overall degradation in performance.
\input{fig/passage}

We also conduct a detailed ablation study on demonstration selection and Q \& A perturbation. The results can be seen in Appendix \ref{app:qa_abla}.
%The full-shot results of Mistral-7B exceed 4-shot results and we attribute this to the discovery that including more examples provides additional QA pairs, and enables LLMs to better mimic ideal outputs and narrow down the sample space for generation~\cite{min2022rethinkingroledemonstrationsmakes}. 

%In addition, as presented at the bottom of Table \ref{tab:tqa_main_4-shot}, we try to perturb the question \& answer in demonstrations, and this leads to a greater performance degradation than perturbing the passage. 

%In Table \ref{tab:tqa_main}, the comparison between 6-shot full prompt setting and other full-shot settings is in fact unfair. To get a fair comparison, we conduct 4-shot expriments on Llama2-13B-longlora-32k-ft. Table \ref{tab:tqa_abla_shot} presents the results of experiments conducted under the 4-shot setting on the Llama2-32K model, which exhibit similar trends. Notably, perturbing the answer has an even greater impact on the results, further demonstrating that the model fails to learn the intrinsic relationships between the passage and generation target.

%\section{Can Robustness Be Transferred to Other Task?}

\section{Sentence-level Distractor Generation}
Given that LLMs cannot learn the intrinsic relationships between the demonstration passage and its corresponding demonstration target in single-document QA tasks such as TriviaQA, we further study whether similar trends can be observed in other passage-level ICL scenarios. To this end, we conduct experiments on RACE~\cite{lai-etal-2017-race}, a commonly used dataset sourced from the educational domain and annotated by professional teachers on the distractor generation task, which is more complex than single-document QA. 
%The perturbation methods remain consistent with those applied to the TriviaQA dataset.
%Given that various types of perturbation to the passage have minimal impact on model performance in the question-answering task, is it possible for this phenomenon to occur in other types of tasks?We perturb the dialogue data in the SAMSum dataset in the same manner to study the robustness of different models in this task.

\subsection{Experimental Setup}
\paragraph{Task Description} In the distractor generation task, each instance consists of a document, 
%(corresponding to the passage in single-document QA), 
a question, a correct answer to the question 
%(which can be derived from the document), 
and several distractors designed to mislead the solver. 
%The distractors are extracted from the document and appear to be the plausible answers but are, in fact, counterfactual. 
In our experiments, we require LLMs to generate three distinct distractors.  %Considering the fact that LLMs sometimes generate more than three distractors, we only evaluate the last three distractors in practice. 
Our evaluation metrics include average BLEU 
%(average BLEU) 
and Pairwise BLEU. The former assesses the quality of the generated content, while the latter evaluates the diversity of the generated distractors, with lower values indicating better diversity.
%SAMSum is a dataset designed for the dialogue summarization task, where each input consists of a dialogue text along with a reference summary. Furthermore, the dataset is specifically designed for few-shot learning. Each input instance comprises multiple demonstrations, each demonstration containing a dialogue and its corresponding summary. In our experiments, the model generates a summary for the given dialogue text, and we calculate the similarity between the generated summary and the reference summary.
%In this few-shot dialogue summarization task, the input is divided into two parts: (i) demonstrations, which include the dialogue text and the corresponding summary; (ii) the query part, which contains the dialogue text. Therefore, the goal of this input text is to enable the model to learn the mapping between dialogue and summary presented in the demonstrations, so that the model can generate the correct summary for the dialogue in the query.We conduct experiments with 1-shot, 5-shot, 10-shot and 20-shot settings, applying the same type of perturbation in each group. The group without perturbation served as the baseline for comparison with the other groups, and we observed the changes in ROUGE-L scores before and after perturbation.
Considering that RACE lacks pre-existing input context, we randomly select three sets of examples from the training set and use the same ICL demonstration examples for each test instance. We report the average metrics over three sets of experiments. 

\paragraph{LLMs} Since the overall prompt length is relatively short, we additionally include Llama2-13B-Chat alongside the two previously used models, aiming to explore whether models with extended context windows utilize contextual information more effectively. 
%than the original models.

\paragraph{Prompt} Our prompt format aligns with \citet{qu-etal-2024-unsupervised}, whose structure can be seen in Table \ref{fig/pmt}, and we conduct experiments under 1, 2, 4, and 8-shot settings. 

\paragraph{Passage Perturbation} The method for perturbing documents remains consistent with %our previous 
the experiments on TriviaQA. For each few-shot experiment, we configure perturbation ratios of 1/2 and 1/4, with each ratio incorporating both generation-based and sampling-based perturbation methods. 
%Our experimental setup involves three models, each employing a different token manipulation strategy: discarding a portion of tokens, randomly generating tokens of a specified length, and partially replacing some tokens. We compare the performance variations of the models under these different configurations.

%Since this task differs from the QA tasks, the dialogues in the SAMSum dataset are distinct from the passages used in the single-document QA tasks. Although the logical and semantic integrity is preserved even when passages are perturbed in Single-document QA task, dialogues are not only similar to passages, but also questions. Therefore, this task can help us determine whether the robustness of models to perturbations to passage can be transferred to other sections of In-context Learning tasks.

\subsection{Results and Discussion}
\input{table/dg_main}
Detailed experimental results are presented in Table \ref{tab:dg_main}.
%\subsection{Analysis}
From the results, we can summarize the following findings.

LLMs exhibit a similar trend in the distractor generation task, demonstrating %complete 
insensitivity to the passages in ICL. Across different models and numbers of shots, settings with perturbed passages achieve comparable and sometimes even better performance than those with full passages, while requiring a much smaller context window.

Completely removing the passages from the context has less impact on Avg BLEU scores (sometimes achieving the best performance). However, in most settings, this removal leads to a significant increase in Pairwise BLEU. This suggests that retaining some content in the passage position, even if it is entirely meaningless generated text, can enhance the diversity of the content produced through generation by LLMs. 
%Our experimental results demonstrate that this provides some valuable insight for enriching the content generation capabilities of models.
\input{fig/qa_atten}

LLM with context windows extended (Llama2-13B-longlora-32k-ft) enhances the general performance, while reducing output diversity and stability. The longlora model has a much higher Avg BLEU score, %in all settings, 
but it suffers from low diversity, with the highest Pairwise BLEU reaching 91.68, which means that the three distractors are almost the same. Moreover, the stability of longlora model is worse than other models, as shown by the drastic changes in both Avg BLEU and Pairwise BLEU. 
%This provides an insight that extending LLMs' context window makes the model better at handling long context and have better performance, but will decrease the model's other abilities such as diversity and stability.

It is noteworthy that the relative orders of Q \& A \& P in the prompt for two tasks are different. As shown in Figure \ref{fig/pmt}, the passage is at the beginning in TriviaQA's prompt, while in the other, it is at the end. However, models show insensitivity to passages in both tasks, indicating that the finding of previous experiments is universal, and the insensitivity of the model to passage is not due to the relative of Q \& A \& P, but rather to the model itself.

Detailed results of ablation study on distractor generation can be seen in Appendix \ref{app:abla}.
%This may be due to the tendency of long-context window models to process the overall information of long texts while overlooking local details and tend to generate more tokens, while the base LLama2-13B-Chat can follow instructions. This also explains the larger performance fluctuations observed in these models.

\section{Why Are LLMs Insensitive to Passage in ICL?}

In this section, from the aspect of attention during inference, we provide a deeper confirmation to our hypothesis extracted from the former experiments $\mathcal{H}$: \textit{In passage-level ICL, LLMs are in fact unable to learn the intrinsic relationships between demonstration passage and its generation target}.
%In section \ref{atten}, we analyze the average attention scores of the first token received from different components of the prompt, 
%across 5 hidden layers during inference. 
%and this analysis aims to confirm the model's insensitivity to the demonstration passage. Meanwhile in section \ref{re_atten}, we focus on the relative attention scores between the passage and other components of the demonstration, and this directly shows that only little attention is passed between the passage and other parts of the demonstration.

%\subsection{Attention Score Received by the First Generated Token}
\subsection{Attention Analysis on the First Generated Token}
\label{atten}

We compute the average attention scores received by the first generated token from different components of the prompt across five hidden layers during inference. This analysis, to some extent, reflects the influence of the prompt's components on the generation. 
%The layers we compute include: the first hidden layer, the 1/4 layer to the first layer in all layers(to edit), 1/2 layer, and the last layer(result of 3/4 layer can be seen in Appendix, which shows similar trend with the last layer*** ), and we believe that this can cover the overall situation during the whole inference process. 
Considering that returning the attention matrix will consume more computing resources than usual, we experiment with two settings for each task. On Trivia QA, we use a 2-shot + full passage prompt and a random half-shot + generate 1/4 passage. On RACE, we use 2-shot + full passage prompt and 2-shot + random generate 3/4 passage prompt. The results on TriviaQA can be seen in Figure \ref{fig/qa_atten}, 
%and \ref{fig/dg_atten},
and the results on RACE is in Appendix \ref{app:abla} for saving space.

%According to Figure \ref{fig/qa_atten}, we observe that 
In the first hidden layer, the attention scores received from different parts of the demonstrations remain approximately equal, indicating that the model does not exhibit a significant preference for any part during the early stage of inference. However, in other layers, the attention scores for demonstration passages decrease significantly, even falling behind those of other components in the demonstrations. This is consistent across both the full passage and randomly generated passage settings, suggesting that LLMs in fact pay little attention to the demonstration passage. The same trends can be seen in the RACE results. Additionally, an interesting finding is that, apart from the query components, task instruction contributes the most attention to the model. This observation partially explains why modifying instructions can lead to substantial performance changes in certain scenarios.

%\subsection{Relative Attention Score between Passage and Other Components of Demonstration}
\subsection{Attention Analysis between Passage and Other Components of Demonstration}
\label{re_atten}
\input{fig/qa_re}

%As mentioned earlier, 
In this section we directly compute the average attention scores between different parts of the demonstration,
%The overall experimental setup remains consistent with section \ref{atten}, but instead of using the attention scores received by the first generated token, we compute the scores that the demonstration components, 
such as the question, receive from or contribute to the passage, determined by their relative positions in the prompt. Since we only focus on the relative attention scores, we compute the scores on all hidden layers. The results for TriviaQA are presented in Figure \ref{fig/qa_re}, while the results for RACE can be found in Appendix \ref{app:atten}.

As shown in Figure \ref{fig/qa_re}, the attention passed from the demonstration passage to its corresponding answer is lower than that of the question, indicating models' relative insensitivity between the passage and the target. Apart from that, the scores drop after the first layer, and remain at a low level 
%with the most average attention score 
below 6.
%in each setting. 
This aligns with the observation from the previous section, which indicates that the model exhibits no preference for any part of demonstrations during the early stages of inference and pays almost no attention to the passage after the first layer. Results on RACE can also reveal this trend.


\section{Passage Compression in ICL}
In this section, we explore whether compression algorithms can preserve the most important parts of passages in ICL and achieve better experimental results than random generation and sampling. 
\subsection{Experimental Setup}
We perform two types of compression methods: retrieval-based~\cite{jiang2024longllmlinguaacceleratingenhancingllms} and perplexity-based.

In retrieval-based compression, we use the question from each ICL demonstration as the retrieval key. After segmenting the passage into sentences, we retrieve the top 5 sentences that are most similar to the question. Additionally, we include a comparison with retrieval re-ranking, where the retrieved sentences are reordered based on the retrieval model's score rather than retaining their original order in the passage. 

For perplexity-based compression, we employ LLMlingua~\cite{jiang2023llmlinguacompressingpromptsaccelerated} and LongLLMlingua~\cite{jiang2024longllmlinguaacceleratingenhancingllms}, two methods proven to exhibit good compression performance on multiple long-document tasks and have been shown to effectively preserve key information. However, in passage-level ICL, the demonstration passages do not directly relate to the query. Our focus is on whether shorter, potentially better passages can help LLMs learn the intrinsic relationships between the passage and its corresponding generation target.

\subsection{Results and Analysis}
Table \ref{tab:comp_qa} shows the results of our compression experiments using Mistral-7B-Instruct-v0.2 on TriviaQA. % for single-document QA task. 
The results indicate that the performance of all compression methods is inferior to that of random generation and sampling. 
%The performance of retrieval-based compression methods is comparable to that of the 6-shot full-passage setting.
The Lingua series methods outperform retrieval-based compression methods, but their performance is still 7 points lower than that of most random perturbation experiments. Furthermore, whether re-ranking or randomly selecting demonstrations has a minimal impact on performance, indicating that in ICL of single-document QA tasks, the presence of content in the passage position is more critical than having better content in the passage position.
\input{table/compress_qa}

Table \ref{tab:comp_dg} presents the results of compression experiments using Llama2-13B-Chat on RACE. %dataset for distractor generation. 
Compared to the previous experiments on TriviaQA, the performance of all compression methods is similar, with small performance fluctuations. Notably, although the performance of compression methods in the 4-shot and 8-shot settings is slightly higher than that of random perturbation experiments (improving by approximately 0.5 points), we consider this marginal performance gain insufficient to conclude that compression algorithms allow LLMs to learn the intrinsic relationships between passages and generation targets.%, based on prior ablation studies.
\input{table/compress_dg}

\section{Related Work}
\subsection{How do LLMs utilize the context?}
Numerous previous studies have explored, from various perspectives, how LLMs utilize context and derive certain insights from ICL. From the perspective of context perturbation, \citet{min2022rethinkingroledemonstrationsmakes} proposes that ground truth demonstrations are not essential. Instead, the label space, the distribution of the input text, and the input format play a more important role in ICL. Furthermore, \citet{liu2023lostmiddlelanguagemodels} finds that the position of key information within the context significantly impacts performance, with key information appearing in the middle position leading to worse performance. Another perspective explains the underlying mechanism of ICL, such as implicit Gradient Descent during ICL~\cite{dai2023gptlearnincontextlanguage,vonoswald2023transformerslearnincontextgradient} and considering label words as anchors in ICL~\cite{wang2023labelwordsanchorsinformation}.
\subsection{Compression Methods for LLMs}
%There is a substantial amount of prior work on context compression for long-context tasks.
In general, prior work on compression methods can be divided into three categories: extractive method, abstractive method, and soft prompt %compression
method. 

The extractive method mainly selects some tokens 
%and text 
from the original context, ensuring that the compressed results are completely derived from the original context. 
Representative works include selective context~\cite{li2023compressingcontextenhanceinference}, LLMLingua~\cite{jiang2023llmlinguacompressingpromptsaccelerated}, LongLLMLingua~\cite{jiang2024longllmlinguaacceleratingenhancingllms}, LLMLingua2~\cite{pan2024llmlingua2datadistillationefficient} and the ReCOMP extractive compressor~\cite{xu2023recompimprovingretrievalaugmentedlms}. 

The abstractive method aims to generate contextual summaries through language models, ensuring the coherence and fluency of the compression results. 
%Abstractive methods 
including ReCOMP abstractive compressor~\cite{xu2023recompimprovingretrievalaugmentedlms}, Nano-Capsulator~\cite{chuang2024learningcompresspromptnatural}, ComPact~\cite{yoon2024compactcompressingretrieveddocuments}, and semantic compression~\cite{fei2023extendingcontextwindowlarge}.

The soft prompt 
%compression
method compresses the natural language context into soft prompt, %or word embedding, 
aiming to aggregate the key information. Representative works include query-guided compressor~\cite{cao2024retainingkeyinformationhigh} and Dodo~\cite{Qin_2024}.


\section{Conclusion}
In this paper, we find that LLMs are unable to learn the intrinsic relationships between the passage and its corresponding generation targets in the passage-level ICL. Through experiments and ablation studies on single-document QA and distractor generation, we demonstrate that randomly perturbing the passage in the demonstrations has minimal impact on performance. 
%whereas perturbing other components more closely related to the generation target leads to the opposite effect.
Building on above experiments, we analyze the attention scores of components of the prompt during inference, as well as the relative attention scores between the passage and other components in demonstrations. The results consistently indicate that LLMs are insensitive to passage during inference. Finally, we introduce compression methods and experimentally show that these methods, while performing well in other long-context tasks, they do not provide significant advantages in passage-level ICL. All these results shows that Passage-level ICL does not necessarily need a regular \emph{"Passage"} during inference. We hope our finding will inspire future work on explaining the inner mechanisms of ICL.

\section*{Limitations}
First, due to resource limitations, we only study open-source LLMs no larger than 13B and the passage-level ICL performance on larger models, especially powerful models that are extremely good at processing very long context or perturbed content, remains under-explored. Second, we focus on traditional ICL paradigm and use a common prompt template only. The performance is not validated under other paradigms such as chain-of-thought \cite{wei2022chain} and different prompt templates. Furthermore, although we have shown that random perturbation can achieve competitive results with shorter context length compared to representative context compression approaches, how to effectively compress the context for passage-level ICL while keeping stable performance is still unclear and requires future exploration. A promising future direction is combining perturbation and compression since they are orthotropic.

\bibliography{custom}

\appendix
\section{Prompt Example}
\label{app:prompt}
We design two different prompt formats for the TriviaQA and RACE datasets, as shown in Table \ref{tab:prompt}. The prompts for both tasks consist of the following components: instructions, demonstrations, task description, and the query-related information. However, there are some differences in the prompts for the two tasks. For TriviaQA, since the questions and answers are typically limited to a single line, the different sections of the prompt are separated by only the newline character '\textbackslash n'. In contrast, the RACE dataset features multiple distractors for the same question and several newline characters within the single passage, which makes it difficult to distinguish different parts with only a single '\textbackslash n'. As a result, we decide to choose the '<>' as a more precise and efficient symbol to locate the corresponding content. In addition, the instructions and task descriptions are designed differently for the two different tasks. This tailored design enables both tasks to achieve strong performance.

When we look closely at the prompts for the two tasks, we can see that the instruction in TriviaQA primarily guides the model to focus on answering QA-type tasks. In contrast, the instruction for the RACE dataset requires the model to generate distractors that align with the relationship between the question and answer. At the same time, both tasks require the model to produce answers in a specified format.

\input{table/prompt}
\section{Ablation Study on single-document QA Task}
\label{app:qa_abla}
\input{table/tqa_abla_random_qa}
We conduct ablation studies on Mistral-7B. We introduce random demonstration selection, where we randomly select half of the context demonstrations, and random generation of question and answer in demonstrations. Experimental results are presented in Table \ref{tab:tqa_abla_random_qa}. The results show that randomly selecting half of the ICL examples causes a slight decline in performance, which perhaps results from the reduction of QA pairs. However, perturbing the question-answer pairs exhibits a more substantial impact on model performance. This effect becomes particularly pronounced when both components are altered simultaneously, resulting in significantly decreased F1 and EM scores. And this further confirms the discovery that instead of learning the intrinsic relationships from demonstrations, LLMs tend to mimic the generation target and then generate output based on query~\cite{min2022rethinkingroledemonstrationsmakes}

\section{Ablation Study on DG Task}
\label{app:abla}
We also conduct ablation study on perturbations of the question, answer, and distractor within the context of ICL demonstrations. In previous experiments, each demonstration contains only one question and answer. In the ablation experiments, we incorporate multiple questions, answers, and distractors from the given dataset into the demonstration in a list format, while keeping the query and other components unchanged. Compared to the perturbation of q\& a \& d in section \ref{qares}, a more regular perturbation will present a credible result . By introducing perturbations to the format of  questions, answers, and distractors in demonstrations, we can more clearly observe that perturbing parts more closely related to the generation target has a greater impact on the model than perturbing passages. The experimental results are presented in Table \ref{tab:dg_abla}.

It is observed that this modification leads to a significant performance degradation. Avg BLEU of almost each setting drops below 3.00, while the Pairwise BLEU remains the same trend. Through case studies, we find that the model's outputs mimic the list format in the demonstrations. The mere introduction of a list format for questions, answers, and distractors results in such a substantial change, whereas completely random generation of passages even improves overall performance in some settings. This reveals the model's insensitivity to the content of the passages.

\begin{table}[ht]
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{4pt}
\centering
\small
\begin{tabular}{l|l|ccc}
\bottomrule
  \multicolumn{2}{l}{} &\multicolumn{3}{c}{\textbf{list q\&a\&d}} \\
\hline
\textbf{shot num} & \textbf{Settings} & \textbf{AB} & \textbf{PB}($\downarrow$) & \textbf{Avg length}\\
\hline
1-shot & prior best & \textbf{4.12} & 26.25 & 546.03\\
 & full & 2.27 & \textbf{24.17} & 1018.69\\
 & no passage  & 2.27 & 28.08 & 587.36\\
 & generate 1/2  & \textbf{2.41} & 26.43 & 723.66\\
 & generate 1/4  & 2.31 & 27.39 & 654.98\\
 & sample 1/2   & 2.26 & 26.20 & 806.16\\
 & sample 1/4 & 2.26 & 26.98 & 697.20\\
\hline
2-shot & prior best & \textbf{5.17} & 24.98 & 706.92\\
 & full  & 2.44 & \textbf{20.50} & 1489.03\\
 & no passage  & 2.69 & 26.28 & 696.03\\
 & generate 1/2 & \textbf{2.76} & 24.55 & 948.59\\
 & generate 1/4  & 2.75 & 24.83 & 820.36\\
 & sample 1/2  & 2.61 & 23.90 & 1091.31\\
 & sample 1/4  & \textbf{2.76} & 25.29 & 895.92\\
\hline
4-shot & prior best & \textbf{5.47} & 27.36 & 882.83\\
 & full  & 2.72 & \textbf{23.60} & 2288.03\\
 & no passage  & 2.76 & 26.77 & 902.36\\
 & generate 1/2  & \textbf{2.81} & 28.40 & 1340.86\\
 & generate 1/4  & 2.72 & 28.26 & 1118.43\\
 & sample 1/2  & 2.66 & 27.55 & 1580.63\\
 & sample 1/4  & \textbf{2.81} & 28.03 & 1238.44\\
\hline
8-shot & prior best & \textbf{5.59} & 25.63 & 1261.58\\
 & full & - & - & - \\
 & no passage  & 2.62 & 27.64 & 1317.36\\
 & generate 1/2  & 2.14 & 35.20 & 2260.99\\
 & generate 1/4 & 2.97 & 26.84 & 1782.64\\
 & sample 1/2  & 2.76 & \textbf{24.72} & 2767.14\\
 & sample 1/4  & \textbf{3.16} & 27.12 & 2027.31\\
\hline
\toprule
\end{tabular}
\caption{Ablation study results of Llama2-13B-Chat on RACE dataset. The prior best refers to the best result from all random perturbation settings under the same shot.
%B4 refers to BLEU-4, R-L refer to Rouge-L, BS refers to BertScore, PB refers to Pairwise BLEU, FS refers to Faithful Score. 
}
\label{tab:dg_abla}
\end{table}

%\label{sec:appendix}
\section{Attention Results on Distractor Generation}
\label{app:atten}
To investigate the underlying reasons for this phenomenon, we visualized the attention scores of the LLM and performed a comparative analysis. The results are shown in Figure \ref{fig/dg_atten} and Figure \ref{fig/dg_re}.

Figure \ref{fig/dg_atten} illustrates the impact of two different settings on attention scores: the position of different model layer and different components of prompts. As mentioned in the previous section, the attention score distribution of an input sequence undergoes relatively significant changes as it passes through deeper layers of the model. Initially, the distribution is relatively uniform, but in the middle layers, attention shifts primarily to three parts: the output section within the demonstration, the instruction, and the query. In the attention distribution of the last layer, a trend similar to that of the middle layers can be observed. However, the model shows increased attention to the demonstration compared to the middle layer, probably due to its increased information on overall information in the final layer. Meanwhile, the concentrated attention on the instruction and query sections remains consistent with previous findings. Additionally, the attention distributions in different layers are highly similar between the full Original Passage and the 3/4 Generated Passage.

Figure \ref{fig/dg_re} reveals a similar trend to the previous finding. The experimental setup is similar to that of TriviaQA, However, since the question and the corresponding answer appear before the passage in the demonstration, while the distractors are positioned after the passage. Since the decoder-only architecture only access tokens preceding the current token, the relative attention scores are categorized into three types: Question2Passage, Answer2Passage, and Passage2Distractors. The trend of relative attention scores across layers under both settings is similar to that observed in the QA task. The P2D score is significantly lower than the Q2P and A2P scores, indicating that the connection between the passage and the corresponding target is much weaker than other parts' connection with the passage. When the number of the layers is less than six, the overall attention scores are low, corresponding to a flat attention distribution at the beginning. In deeper layers, the relative attention score and the attention distribution become more directional and focused. Although the trends of the three relative attention scores are generally similar under two settings, the overall relative attention scores for the random generated passage in deeper hidden layers are significantly lower than those for the full passage. This may be because the randomly generated passage has a weaker semantic connection to the corresponding question, answer, and distractors.
\input{fig/dg_re}
\input{fig/dg_atten}
\section{License}
\input{table/license}


\end{document}
