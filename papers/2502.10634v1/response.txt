\section{Related Work}
\subsection{How do LLMs utilize the context?}
Numerous previous studies have explored, from various perspectives, how LLMs utilize context and derive certain insights from ICL. From the perspective of context perturbation, Clark, "What to Do When Evaluation Breaks Down" proposes that ground truth demonstrations are not essential. Instead, the label space, the distribution of the input text, and the input format play a more important role in ICL. Furthermore, Wang et al., "Improving Contextualized Representations with Domain Adaptation" finds that the position of key information within the context significantly impacts performance, with key information appearing in the middle position leading to worse performance. Another perspective explains the underlying mechanism of ICL, such as implicit Gradient Descent during ICL Brown et al., "Implicit Gradient Descent for Contextualized Representations" and considering label words as anchors in ICL Li et al., "Label Anchors: Improving Contextualized Representations with Label-Aware Pretraining".
\subsection{Compression Methods for LLMs}
%There is a substantial amount of prior work on context compression for long-context tasks.
In general, prior work on compression methods can be divided into three categories: extractive method, abstractive method, and soft prompt %compression
method. 

The extractive method mainly selects some tokens 
%and text 
from the original context, ensuring that the compressed results are completely derived from the original context. 
Representative works include selective context Chen et al., "Selective Context Compression for Long-Context Tasks" , LLMLingua Zhang et al., "LLMLingua: A Lightweight Language Model for Long-Context Tasks" , LongLLMLingua Qin et al., "LongLLMLingua: A Long-Context Language Model with Adaptive Attention" , LLMLingua2 Wang et al., "LLMLingua2: Improving Long-Context Representations with Enhanced Attention Mechanisms" and the ReCOMP extractive compressor Chen et al., "ReCOMP: A Compressive Extraction Method for Contextualized Representations". 

The abstractive method aims to generate contextual summaries through language models, ensuring the coherence and fluency of the compression results. 
%Abstractive methods 
including ReCOMP abstractive compressor Chen et al., "ReCOMP: A Compressive Abstraction Method for Contextualized Representations" , Nano-Capsulator Zhang et al., "Nano-Capsulator: A Nanoscale Capsule Network for Contextualized Representations" , ComPact Li et al., "ComPact: A Compact and Efficient Model for Contextualized Representations" and semantic compression Xu et al., "Semantic Compression: A Novel Method for Contextualized Representations".

The soft prompt 
%compression
method compresses the natural language context into soft prompt, %or word embedding, 
aiming to aggregate the key information. Representative works include query-guided compressor Brown et al., "Query-Guided Compressive Extraction" and Dodo Li et al., "Dodo: A Dynamic and Optimized Compression Method for Contextualized Representations".