\section{Related Work}
\subsection{How do LLMs utilize the context?}
Numerous previous studies have explored, from various perspectives, how LLMs utilize context and derive certain insights from ICL. From the perspective of context perturbation, \citet{min2022rethinkingroledemonstrationsmakes} proposes that ground truth demonstrations are not essential. Instead, the label space, the distribution of the input text, and the input format play a more important role in ICL. Furthermore, \citet{liu2023lostmiddlelanguagemodels} finds that the position of key information within the context significantly impacts performance, with key information appearing in the middle position leading to worse performance. Another perspective explains the underlying mechanism of ICL, such as implicit Gradient Descent during ICL~\cite{dai2023gptlearnincontextlanguage,vonoswald2023transformerslearnincontextgradient} and considering label words as anchors in ICL~\cite{wang2023labelwordsanchorsinformation}.
\subsection{Compression Methods for LLMs}
%There is a substantial amount of prior work on context compression for long-context tasks.
In general, prior work on compression methods can be divided into three categories: extractive method, abstractive method, and soft prompt %compression
method. 

The extractive method mainly selects some tokens 
%and text 
from the original context, ensuring that the compressed results are completely derived from the original context. 
Representative works include selective context~\cite{li2023compressingcontextenhanceinference}, LLMLingua~\cite{jiang2023llmlinguacompressingpromptsaccelerated}, LongLLMLingua~\cite{jiang2024longllmlinguaacceleratingenhancingllms}, LLMLingua2~\cite{pan2024llmlingua2datadistillationefficient} and the ReCOMP extractive compressor~\cite{xu2023recompimprovingretrievalaugmentedlms}. 

The abstractive method aims to generate contextual summaries through language models, ensuring the coherence and fluency of the compression results. 
%Abstractive methods 
including ReCOMP abstractive compressor~\cite{xu2023recompimprovingretrievalaugmentedlms}, Nano-Capsulator~\cite{chuang2024learningcompresspromptnatural}, ComPact~\cite{yoon2024compactcompressingretrieveddocuments}, and semantic compression~\cite{fei2023extendingcontextwindowlarge}.

The soft prompt 
%compression
method compresses the natural language context into soft prompt, %or word embedding, 
aiming to aggregate the key information. Representative works include query-guided compressor~\cite{cao2024retainingkeyinformationhigh} and Dodo~\cite{Qin_2024}.