\documentclass[letter, 11pt]{article}

\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{natbib}
\setlength{\bibsep}{0pt plus 0.1ex}
\usepackage{amssymb, blkarray, amsmath,xcolor,xspace,colortbl,rotating} % 
\usepackage[raggedrightboxes]{ragged2e} 
\usepackage{textcomp}
\usepackage{appendix}  %% 100
\usepackage{bm}  %% 100
\usepackage{boxedminipage}  %% 100
\usepackage{color}  %% 100
\usepackage{endnotes}  %% 100
\usepackage{ragged2e}  %% 100
\usepackage[onehalfspacing]{setspace}  %% 100
\usepackage{tabulary}  %% 100  
\usepackage{varioref}  %% 100
\usepackage{wrapfig}  %% 100  
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[margin=1in]{geometry}
\DeclareGraphicsExtensions {.pdf,.svg,.eps,.ps,.png,.jpg,.jpeg}
\newtheorem {theorem}{Theorem}
\newtheorem {acknowledgement}{Acknowledgement}
%\newtheorem {algorithm}[theorem]{Algorithm}
\newtheorem {assumption}{Assumption}
\newtheorem {axiom}[theorem]{Axiom}
\newtheorem {case}[theorem]{Case}
\newtheorem {claim}{Claim}
\newtheorem {conclusion}[theorem]{Conclusion}
\newtheorem {condition}[theorem]{Condition}
\newtheorem {conjecture}[theorem]{Conjecture}
\newtheorem {corollary}{Corollary}
\newtheorem {criterion}[theorem]{Criterion}
\newtheorem {definition}{Definition}
\newtheorem {example}{Example}
\newtheorem {exercise}[theorem]{Exercise}
\newtheorem {lemma}{Lemma}
\newtheorem {notation}[theorem]{Notation}
\newtheorem {problem}[theorem]{Problem}
\newtheorem {proposition}{Proposition}
\newtheorem {remark}{Remark}
\newtheorem {solution}[theorem]{Solution}
\newtheorem {summary}[theorem]{Summary}
\newenvironment {proof}[1][Proof]{\noindent \textbf {#1.} }{\ \rule {0.5em}{0.5em}}



\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left| #1 \right|}
%\newcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\mathrm{argmax}}

\newcommand{\bl}[1]{{\color{green} [BL: #1]}}
\newcommand{\newbl}[1]{{\color{purple}  #1}}
\newcommand{\delbl}[1]{{\color{purple} \sout{#1}}}
\begin{document}

\title{Computing and Learning Mean Field Equilibria with Scalar Interactions: Algorithms and Applications}

\author{Bar Light\protect\thanks{Business School and Institute of Operations Research and Analytics, National University of Singapore, Singapore. e-mail: \textsf{barlight@nus.edu.sg} }} 
\maketitle

\thispagestyle{empty}

 \noindent \noindent \textsc{Abstract}:
\begin{quote}
Mean field equilibrium (MFE) has emerged as a computationally tractable solution concept for large dynamic games. However, computing MFE remains challenging due to nonlinearities and the absence of contraction properties, limiting its reliability for counterfactual analysis and comparative statics. This paper focuses on MFE in dynamic models where agents interact through a scalar function of the population distribution, referred to as the \textit{scalar interaction function}. Such models naturally arise in a wide range of applications in operations and economics, including quality ladder models, inventory competition, online marketplaces, and heterogeneous-agent macroeconomic models. 
The main contribution of this paper is to introduce iterative algorithms that leverage the scalar interaction structure and are guaranteed to converge to the MFE under mild  assumptions. Unlike existing approaches, our algorithms do not rely on monotonicity or contraction properties, significantly broadening their applicability. Furthermore, we provide a model-free algorithm that learns the MFE by employing simulation and reinforcement learning techniques such as Q-learning and policy gradient methods without requiring prior knowledge of payoff or transition functions. We establish finite-time performance bounds for this algorithm under technical Lipschitz continuity assumptions. 
We apply our algorithms to classic models of dynamic competition, such as capacity competition, and to competitive models motivated by online marketplaces, including ridesharing, dynamic reputation, and inventory competition, as well as to social learning models. Using our algorithms, we derive reliable comparative statics results that illustrate how key market parameters influence equilibrium outcomes in these stylized  models, providing insights that could inform the design of competitive systems in these contexts.







		 
\end{quote}


%\noindent {\small Keywords: }Dynamic programming;.

%\smallskip \noindent \emph{JELcis plassification}: 

\newpage 


\section{Introduction} 
The rise of digital marketplaces, e-commerce platforms, and blockchain-based systems has led to increasingly complex interactions among large numbers of strategic agents that compete and learn over time. These environments often require dynamic decision-making in response to competition and aggregate market conditions, making strategic behavior difficult to model and analyze. Mean field equilibrium (MFE) has gained prominence as a scalable and computationally tractable alternative to Markov perfect equilibrium (MPE) in such large dynamic games with strategic agents.  While MPE has traditionally served as the benchmark for such settings, its applicability is often limited in practice due to significant computational challenges. In MPE, agents are required to condition their strategies on the exact state of every competitor, leading to an exponential growth in the state space as the number of agents increases. This ``curse of dimensionality" makes the analysis and computation of MPE infeasible in many applications. Moreover, in large populations, the assumption that agents can precisely track and respond to every competitor’s state becomes increasingly unrealistic. 

MFE offers an approximation to MPE that addresses these limitations. In an MFE, each agent optimizes their expected discounted payoff under the assumption that the distribution of competitors’ states is fixed.   The equilibrium distribution of states is then characterized as the invariant distribution of the  stochastic process resulting from agents' policies. This approximation significantly simplifies the computational burden of equilibrium analysis. Moreover, the literature has established that MFE provides a close approximation to MPE in large games,  
 and consequently, MFE has become an appealing framework for analyzing dynamic competitive environments in large-scale systems.

Despite its advantages, computing mean field equilibrium (MFE) poses significant challenges. The nonlinearities inherent in state dynamics and agents' policies, combined with the lack of global contraction properties in many typical applications, often lead to the failure of traditional fixed-point methods to converge. This non-convergence reinforces the need for computational techniques that reliably compute MFEs.  Such techniques are crucial for applications where MFEs are used for counterfactual analysis of policy interventions and systemic changes.  Without reliable computation, MFE-based policy evaluations can produce misleading results.  Providing algorithms that ensure convergence to an MFE is therefore essential for the robustness and credibility of comparative statics and policy evaluation in these applications. 

In this paper, we develop an adaptive algorithmic framework for computing and learning MFE in dynamic models with scalar interaction. By ``scalar interaction,” we mean that the payoffs and state transitions depend on the distribution of the population’s states only through a single-dimensional function. This structure encompasses many important applications in economics and operations, including capacity competition models, heterogeneous agent macroeconomic models, and various platform settings where market conditions such as aggregate demand or supply are captured by a scalar.  For example, in capacity competition models, firms’  payoffs depend on the aggregate production; in heterogeneous agent settings, macro-level variables such as average wealth constitute natural scalar aggregates; and in dynamic reputation models in marketplace design, the distribution of ratings can often be compressed into a single-dimensional function that impacts sellers' payoffs. 


The main contribution of this paper is the development of adaptive algorithms that converge iteratively to the MFE  under mild ergodicity and continuity assumptions. Unlike many existing methods to find algorithms that converge to an MFE, our method does not rely on monotonicity or contraction conditions, nor does it impose the uniqueness of MFE, which is often assumed as a consequence of contraction properties. This allows our approach to be applied in settings where standard fixed-point iteration fails to find an equilibrium, ensuring that MFE can still be computed and learned in a broader range of models. 
The algorithm begins with an initial guess for the scalar interaction function, and iteratively adjusts it by solving for the agents' optimal policy and the corresponding population distribution. Specifically, for a given guess of the scalar interaction function, the algorithm computes the agents' value function and policy through standard dynamic programming techniques. It then determines the unique invariant distribution of states induced by this  policy. Using the invariant distribution, the scalar interaction function is iteratively updated to get closer to the MFE.  In Section \ref{Section:Value}, we introduce the Adaptive Value Function Iteration algorithm to compute the MFE and show that it guarantees convergence to some MFE.

We provide a data-driven version of the algorithm that uses simulation for settings where the underlying payoff and transition functions are unknown. By integrating reinforcement learning techniques such as Q-learning and policy gradient methods, alongside Monte Carlo sampling for finding approximate invariant distributions, the algorithm learns the MFE from simulated trajectories. Importantly, in Section \ref{Sec:Q-learning} we show that, even without explicit knowledge of the payoffs and transitions, the algorithm is guaranteed to converge asymptotically to an MFE.
This setting is particularly useful in modern operational environments, such as online marketplaces, where explicit model primitives may be difficult to estimate, yet data is typically abundant. We then provide in Section \ref{Section:Finite-time} a finite-time analysis that establishes explicit error bounds, linking the approximation errors from reinforcement learning techniques (e.g., Q-learning) and Monte Carlo sampling to the accuracy of the computed MFE. Specifically, when primitives are smooth enough, we provide bounds on the deviation between the algorithm's output and an MFE. These results provide guarantees on the reliability of the algorithm under smoothness conditions, which are sometimes assumed in operations and economics models. 

An advantage of our approach is that, in each iteration, the algorithm solves a standard dynamic programming problem while treating the scalar interaction as fixed. This allows us to leverage well-established tools and methodologies from the reinforcement learning literature. In Section \ref{Sec:LargeState} in the Appendix, we outline how the algorithm can be extended to accommodate large state spaces by incorporating reinforcement learning techniques tailored for these settings such as Q-learning with function approximation like neural networks. The Q-learning step can also be replaced with alternative reinforcement learning approaches, such as policy gradient methods (see Section \ref{Sec:Policy} in the Appendix) or actor-critic algorithms. More generally, any reinforcement learning method capable of reliably finding the optimal policy can be integrated into our framework while preserving the convergence guarantees.  This flexibility makes our algorithm design a natural choice for computing MFEs in high-dimensional settings with scalar interactions. 


 We apply our proposed algorithms first to classic models from the dynamic competition literature, including capacity competition and quality ladder models (see Section \ref{Subsec:Quality ladder}). Using our algorithms, we show how to compute and learn MFE and derive numerical comparative statics, demonstrating, for instance, how changes in demand influence equilibrium sellers' capacity distributions. We then present three applications including inventory competition (Section \ref{Section:Inventory}), dynamic reputation models (Section \ref{Sec:reputation}), and ridesharing platforms (Section \ref{Section:Ridesharing}) that are motivated by online marketplaces, which typically involve a large number of participants. As such, the mean field limit provides a tractable and reasonable framework for analyzing market design decisions in these applications.
 We apply our proposed algorithms to compute MFEs in these models and derive numerical comparative statics that can guide key market design decisions in these settings. For instance in the dynamic inventory model, motivated by platforms like Amazon, which often provide warehousing and fulfillment services for retailers and charge holding costs for inventory storage, we examine numerically how holding costs influence equilibrium distribution of inventories. In dynamic reputation models, we examine how the cost of improving service quality (e.g., Airbnb hosts investing in better cleaning standards or rideshare drivers enhancing reliability) affects the equilibrium distribution of reputation. For example, Airbnb can indirectly lower the cost of maintaining high ratings by offering discounts on professional cleaning services, effectively reducing the cost of quality improvement for hosts. 
In a ridesharing model, we analyze how payoffs for long trips influence equilibrium driver availability, motivated by changes in surge pricing mechanisms implemented by ridesharing platforms to mitigate cherry-picking behavior. Our last application considers a social learning setting where agents form beliefs about an unknown state and update them using a DeGroot-style process. Agents take actions to refine their knowledge of the true state, and we analyze how differences in their precision levels, that can be shaped by factors such as disclosure quality or platform moderation, shape the distribution of final beliefs.
While our numerical results focus on stylized settings, our approach is scalable to more realistic large state-space settings, as discussed earlier. Finally, we note that scalar interaction functions also play a central role in dynamic heterogeneous-agent macroeconomic models, such as those studied in \cite{acemoglu2012} where MFE is used as a solution concept.





The computation and learning of MFE in discrete-time games have been extensively studied in recent years (see \cite{lauriere2022learning} for a survey). \cite{guo2019learning}, \cite{xie2021learning}, \cite{guo2023general}, and \cite{anahtarci2023learning} propose various reinforcement learning techniques  to establish convergence to the MFE under contraction properties. \cite{perrin2020fictitious} introduce a fictitious play to compute the MFE under Lasry-Lions uniqueness conditions \citep{lasry2007mean} (see also \cite{cui2024learning}). \cite{subramanian2019reinforcement} introduce a policy gradient approach to find a local variant of MFE. \cite{weintraub2010computational} design algorithms for MFE computation in industry dynamics models with entry and exit decisions, while \cite{adlakha2013mean} analyze supermodular games, where MFE can be computed via standard fixed-point iteration. \cite{saldi2023linear} develop algorithms specifically for linear MFE games. 

Most existing methods for computing MFE rely on fixed-point iterations, which require contraction properties, monotonicity conditions, or specialized game structures to guarantee convergence. However, 
 these conditions often impose strong structural restrictions, limiting their  applicability to many models of interest. Our approach focuses on MFE with scalar interactions, developing algorithms for computing and learning MFE in general settings with scalar interactions and without requiring contraction, uniqueness or monotonicity assumptions that are not generally satisfied in the applications we study. 

Beyond algorithmic developments, mean field games have been widely applied in recent operations, management science, and economics literature, e.g.,  auction theory (\cite{iyer2014mean}, \cite{balseiro2015repeated}, dynamic oligopoly models (\cite{weintraub2008markov}
and \cite{adlakha2015equilibria}), heterogeneous agent macro models \citep{acemoglu2012}, matching
markets (\cite{kanoria2021facilitating} and \cite{arnosti2021managing}), spatial competition \citep{yang2018mean}, experimentation in equilibrium \citep{wager2021experimenting}, contract theory \citep{carmona2021finite},  and blockchain \citep{li2024mean}. 



\section{The Model} \label{Section: model}

This section introduces the mean field model with scalar interaction that we study in this paper. Numerous models in the operations and economics literature fit within our framework. Specific examples are provided in Section \ref{Section:applications}. The model and the definition of an MFE are similar to \cite{adlakha2013mean} and \cite{light2022mean} but with  scalar interactions. 


\textit{Time.} We study a discrete time setting. We index time periods by $t =1 ,2 ,\ldots $.


\textit{Agents.} There is a continuum of ex-ante identical agents of measure $1$. We use $i$ to denote a particular agent. For simplicity, our formulation assumes a continuum of agents, but our results immediately generalize for models where the primitives depend on the number of agents 
$m$, as in the study of oblivious equilibria \citep{weintraub2008markov}.


\textit{States.} The individual state of agent $i$ at time $t$ is denoted by $x_{i ,t} \in X$ where $X$ is a finite set. We let $\mathcal{P} (X)$ be the set of all probability measures on $X$ and we denote by
$s_{t} \in \mathcal{P} (X)$ the probability measure that describes the distribution of agents' states at time $t$. We refer to $s_{t}$ as the \emph{population state} of time $t$.


\textit{Actions.} The action taken by agent $i$ at time $t$ is denoted by $a_{i ,t} \in A$ where $A \subseteq \mathbb{R}^{q}$. The set of feasible actions for an agent in state $x$ is given by a  set $\Gamma  (x) \subseteq A$. We assume that the correspondence $\Gamma  :X \rightarrow 2^{A}$ is compact-valued and continuous.\footnote{
By continuous we mean both upper hemicontinuous and lower hemicontinuous.} 

\textit{Scalar interaction.} The simplifying assumption in this paper, compared to the standard literature on stationary mean field games, is that each agent's payoff and transition functions depend on other agents only through a real-valued function of the population state, rather than on the entire population state. Thus, the interaction between agents depends on a scalar that is a function of the population state, e.g., the mean or variance. More formally, if the population state is \( s \), the scalar interaction  is given by \( M(s) \), where \( M : \mathcal{P}(X) \rightarrow [a,b] \) is a continuous function that is called the scalar interaction function. Note that because $M$ is continuous, it is bounded and we assume that 
 the bounds \( a, b \) are known. In many examples, \( M \) is increasing with respect to stochastic dominance, in which case the lower bound \( a \) can be determined by the minimal element in \( X \) and the upper bound \( b \) can be determined by the maximal element in \( X \).

As we mentioned in the introduction, scalar interaction functions appear naturally in many models of competition. For instance, in quantity-based competition among many sellers, the demand function typically depends only on the total quantity produced. In such cases, the scalar interaction function is given by \( M(s) = \sum_{x \in X} q(x) s(x) \), where \( q(x) \) represents the quantity produced by a seller in state \( x \) that  corresponds to features such as the seller's  capacity, quality or costs. 


\textit{States' dynamics.} The individual state of each agent evolves according to a Markov process. 
If agent's $i$'s state at time $t -1$ is $x_{i ,t -1}$, the agent takes an action $a_{i ,t -1}$ at time $t -1$, the population state at time $t$ is $s_{t-1}$, and $\zeta _{i ,t}$ is agent $i$'s realized idiosyncratic random shock at time $t$, then agent $i$'s next period's state is given by
\begin{equation*}x_{i ,t} =w (x_{i ,t -1} ,a_{i ,t -1} ,M(s_{t-1}) ,\zeta _{i ,t}).
\end{equation*}
We assume that $\zeta _{i,t} $ are independent and identically distributed random variables across time and agents that take values on a compact separable metric space $E$ and  have a law $q$. 
We call $w :X \times A \times [a,b] \times E \rightarrow X$ the transition function.


\textit{Payoff.} In a given time period, if the state of agent $i$ is $x_{i}$, the population state is $s$, and the action taken
by agent $i$ is $a_{i}$, then the single period payoff for agent $i$ is $\pi (x_{i}  ,a_{i} ,M(s))$. The agents discount their future payoff
by a discount factor $0 <\beta  <1$. Thus, agent $i$'s infinite horizon payoff is given by $\sum _{t =1}^{\infty }\beta ^{t-1} \pi  ( x_{i ,t} ,a_{i ,t} ,M(s_{t}))$. 




For ease of notation, we will frequently omit the subscripts $i$ and $t$, and instead denote a generic transition function by \( w(x, a, M(s), \zeta) \) and a generic payoff function by \( \pi(x, a, M(s)) \). Throughout the paper, we assume that the payoff function \( \pi \) is bounded and continuous, and that the transition function \( w \) is continuous.


\begin{remark}
For simplicity, we assume that agents are ex-ante homogeneous. However, the model can be readily extended to an ex-ante heterogeneous setting, where each agent has a fixed type throughout the time horizon, drawn randomly at the first period. In this case, both the payoff and transition functions can depend on the agent's type. All results presented in this paper hold in this more general setting and the algorithms can be easily adjusted. In addition, the model can be extended to applications where the population state is represented as a joint distribution over states and actions. We omit the details for brevity (see \cite{light2022mean} for a detailed discussion and for incorporating these extensions).
\end{remark}




\subsection{Mean Field Equilibrium}


Informally, an MFE is a policy for the agents and a population state such that: (1) Each agent optimizes her expected discounted payoff assuming that this population
state is fixed; and (2) Given the agents' policy, the fixed population state is an invariant distribution of the states' dynamics.
The interpretation is that a single agent conjectures the population state to be $s$. Therefore, in determining her future expected payoff stream, an agent considers a payoff function and a transition function evaluated at the fixed population state $s$. In MFE, the conjectured $s$ is the correct one given the strategies being played.

Let $X^{t}:=\underbrace{X \times \ldots  \times X}_{t~ \mathrm{t} \mathrm{i} \mathrm{m} \mathrm{e} \mathrm{s}}$. For a fixed population state, a nonrandomized pure policy $\sigma $ is a sequence of  functions $(\sigma _{1} ,\sigma _{2} ,\ldots  ,)$ such that $\sigma _{t} :X^{t} \rightarrow A$ and $\sigma _{t} (x_{1} ,\ldots  ,x_{t}) \in \Gamma  (x_{t})$ for all $t \in \mathbb{N}$. That is, a policy $\sigma $ assigns a feasible action to every finite string of states.

For each initial state $x \in X$ and population state $s \in \mathcal{P} (X)$, a policy $\sigma $ induces a probability measure\protect\footnote{The probability measure on $X^{\mathbb{N}}$ is uniquely defined (see, for example, \cite{bertsekas1978stochastic}).}  over the space $X^{\mathbb{N}}$. We denote the expectation with respect to that probability measure by $\mathbb{E}_{\sigma }$, and the associated states-actions stochastic process by $\{x (t) ,a (t)\}_{t =1}^{\infty }$. 

When an agent uses a policy $\sigma $, the population state is fixed at $s \in \mathcal{P} (X)$, and the initial state is $x \in X$, then the agent's expected present discounted value is
\begin{equation} \label{Equation:V_signma} V_{\sigma } (x ,M(s)) =\mathbb{E}_{\sigma } \left (\sum _{t =1}^{\infty }\beta ^{t -1} \pi  (x (t) ,a (t) ,M(s)) \right ).
\end{equation}
Denote
\begin{equation*}V(x,M(s)) = \underset{\sigma } {\sup }\;V_{\sigma} (x,M(s)).
\end{equation*}
That is, $V(x,M(s))$ is the maximal expected payoff that the agent can achieve when the initial state is $x$ and the population state is fixed at $s \in \mathcal{P} (X)$. We call $V$ the \emph{value function} and a policy $\sigma$ attaining it \emph{optimal}. 

Standard dynamic programming arguments (e.g., see \cite{bertsekas1978stochastic} and \cite{light2024principle}) show that the value function satisfies the Bellman equation:
\begin{equation*}V (x ,M(s)) =\underset{a \in \Gamma  (x)}{\max } ~\pi  (x ,a ,M(s)) +\beta  \int _{E}  V (w (x ,a ,M(s) ,\zeta) ,M(s)) q(d\zeta)
\end{equation*}
and there exists an optimal stationary Markov policy. Let $G (x ,M(s))$ be the optimal stationary policy correspondence, i.e.,
\begin{equation} \label{Equation:optimalpolicy} G (x,M(s)) =\underset{a \in \Gamma  (x)}{\ensuremath{\operatorname*{argmax}}}\; \pi  (x ,a ,M(s)) + \beta  \int _{E}  V (w (x ,a ,M(s) ,\zeta) ,M(s)) q(d\zeta). 
\end{equation}
For a policy $g \in G$ and a fixed scalar interaction $M(s)$, the probability that the agent's next period's state will equal $y \in X$, given that her current state is $x \in X$, and she takes the action $a =g (x ,M(s))$, is:
\begin{equation*}W_{g} (x ,M(s) ,y) = q  ( \zeta \in E: w (x ,g (x ,M(s)) ,M(s) ,\zeta ) =y).
\end{equation*}


We now define an MFE. 


\begin{definition}
\label{Def MFE}A stationary policy $g$ and a population state $s \in \mathcal{P} (X)$ constitute an MFE if the following two conditions hold: 

1. Optimality: $g$ is optimal given $s$, i.e., $g (x ,M(s)) \in G (x ,M(s))$. 

2. Consistency: $s$ is an invariant distribution of $W_{g}$. That is,
\begin{equation*}s (y) = \sum _{x \in X} W_{g} (x ,M(s) ,y) s ( x).
\end{equation*}
for all $y \in X$. 
\end{definition}




\section{Algorithms for Computing MFE} \label{Sec:AlgorithmsMFE}

Finding a mean field equilibrium (MFE) directly from Definition \ref{Def MFE} is computationally challenging due to the need to simultaneously satisfy both optimality and consistency conditions. Specifically, the optimal policy $g$
 must be derived assuming that the population state  $s$
is fixed, yet  $s$ itself is defined as an invariant distribution that depends on the dynamics generated by the policy $g$. 
 This circular dependency and the nonlinearity of the consistency condition to find $s$ mean that solving for MFE becomes equivalent to finding a fixed-point of a nonlinear operator which is generally intractable even for relatively small state size problems. 



To overcome these computational difficulties, iterative methods are typically employed \citep{lauriere2022learning}. Instead of attempting to find a fixed-point, these algorithms update the policy and population state iteratively, each step aims to bring the solution closer to satisfying both the optimality and consistency conditions of the MFE. The most common method is the fixed-point iteration method. In this method, we start with some initial population state $s_{0}$, and then we generate a sequence of population states $\{s_{t} \} $ by
\begin{equation} \label{Eq:fixed}
s_{t+1}(y) =  \sum _{x \in X} W_{g_{t}} (x, M(s_{t}) , y) s_{t} (x)
\end{equation} 
where $g_{t}$ is the optimal policy given that the population state is $s_{t}$. This is a standard way to compute an MFE iteratively where at each iteration, the optimal policy is computed given the population state $s_{t}$ and the population state is updated according to the induced state dynamics. This method can also be used to learn MFE by determining the policy through a reinforcement learning approach and employing a sampling technique to approximate the next period's population state $s_{t+1}$
  (e.g., see \cite{guo2023general} and \cite{anahtarci2023learning}). If the sequence \( s_{t} \) converges, it will converge to an MFE. However, such a convergence requires strong contraction conditions that are not satisfied in many applications of interest. Example \ref{Example:non-convergence} shows that this fixed-point iteration method may fail to converge to the MFE, even in very simple cases.

\begin{example} \label{Example:non-convergence} 
    Assume that there are only two states $X=\{1,2\}$ and one action $A = \{a \}$. Payoffs are arbitrary. Assume that  $w(2,a,M(s),\zeta)= w(1,a,M(s),\zeta)= 1 +  1_{\{M(s) \leq \zeta\} }$ where $M(s) = s (\{2 \})$ is the fraction of agents in state $2$ and $\zeta$ is the uniform random variable on $[0,1]$.  Thus, the probability of transitioning to state $1$ is equal to the fraction of agents in state $2$. Applying the iterative fixed-point method (Equation (\ref{Eq:fixed})) we see that if $s_{0}(\{1\}) = \alpha$ then $s_{t}(\{1\}) = 1 - \alpha$ for odd $t$ and $s_{t}(\{1\}) = \alpha$ for an even $t$. Thus, the fixed-point iteration method does not converge for any initial state except the initial state $s_{0}(\{1\}) = 1/2$ which corresponds to an MFE. 
\end{example}


Example \ref{Example:non-convergence}  illustrates how, unless the initial population state is an MFE, the fixed-point iteration method fails to converge to the MFE for any other initial population states even in a two states case. This  is perhaps unsurprising, as convergence of fixed-point iteration typically requires strong monotonicity and contraction properties for nonlinear operators like those encountered in mean field games, assumptions often made in the learning MFE literature, as discussed in the introduction.\footnote{Note that by standard arguments, slowing the fixed-point iteration by considering \[s_{t+1}(y) = (1-\alpha) s_{t} (y) + \sum _{x \in X} W_{g_{t}}(x,M(s_{t}),y)s_{t}(x)\] for some \(\alpha \in (0,1)\) does not resolve the issue, as we can construct simple two-state examples that still exhibit non-convergence under this modification. For example, we could extend Example \ref{Example:non-convergence} and consider \( P_{[0,1]} ((1+\lambda)0.5 - \lambda s_t(\{2\})) \) for some \(\lambda\), where \(P_{[0,1]}\) is the projection onto the set \([0,1]\), as the probability of transitioning to state 2 from either state 1 or 2. This would cause non-convergence due to non-contractiveness for various values of \(\lambda\), which depend on the choice of \(\alpha\).}
 This observation motivates us to explore alternative algorithms, introduced in Sections \ref{Section:Value} and \ref{Sec:Q-learning}, that provide  convergence guarantees for MFE with a scalar interaction term and can be applied in a wide range of applications of interest as we illustrate in Section \ref{Section:applications}.

Before introducing Algorithms for computing MFE we introduce the following notation and assumption. 

We denote by $L_{m,g}$ the linear Markov chain that describes the state dynamics when the agents are using the policy $g$ and the scalar interaction  is fixed at $m$. That is, 
$$L_{m,g} (x,m,y) = \Pr  (w (x ,g (x ,m) ,m ,\zeta ) = y).$$
\begin{assumption} \label{Assumption:Unique}
    For each $m \in [a,b]$ and a corresponding optimal policy $g(x,m) \in G(x,m)$, the linear Markov chain $L_{m,g}$ on the finite set $X$ is ergodic, that is, it is irreducible and aperiodic. In particular, it has a unique fixed-point $s^{m,g}$ that satisfies 
    $$ s^{m,g} (y)= \sum _{x \in X} L_{m,g} (x,m,y) s^{m,g}(x), \text{ } \forall y \in X$$
    and for every probability measure $\theta \in \mathcal{P}(X)$, $L_{m,g} ^{n}\theta$ converges to $s^{m,g}$ as $n \rightarrow \infty$. 
\end{assumption}

Assumption \ref{Assumption:Unique} guarantees that the state dynamics generated by policy functions is ergodic and is quite standard in the theoretical reinforcement learning literature \citep{bertsekas2019reinforcement}. 


\subsection{Known Payoffs and Transitions: Adaptive Value Function Iteration} \label{Section:Value}

In this section we introduce the Adaptive Value Function Iteration algorithm for MFE with a scalar interaction function. Under the assumption that the payoff and transition functions are known, this algorithm provides a tractable approach to solve for MFE in settings which are commonly encountered in operations and economics applications where model parameters are needed to be estimated from data and the state space is relatively not big. 



The algorithms updates the scalar interaction, the policy function, and the population state until convergence. 
The key steps in each iteration involves two standard procedures. First, each iteration fixes the scalar interaction at a chosen value and preforms a typical value function iteration algorithm, yielding the optimal value function and the optimal policy for that specific scalar interaction. The convergence properties of standard value function iteration algorithms guarantee that the algorithm finds a policy that is optimal with respect to that fixed  scalar interaction. 
Second, with the derived optimal policy, finding the unique invariant distribution of the Markov chain that is induced by the policy and the stats' dynamics (see Assumption \ref{Assumption:Unique}) reduces to solving a linear system of equations, as it corresponds to the stationary distribution of a standard linear Markov chain. Both steps are computationally feasible for relatively small to medium size state spaces. 
After each iteration, the algorithm evaluates whether the difference between the current fixed scalar interaction and the one implied by the invariant distribution found in that iteration, satisfies a convergence criterion. If not, the algorithm uses a bisection method to update the scalar interaction.  
By iteratively updating the scalar interaction at each step, this algorithm converges to the MFE while bypassing the usual complexity of solving for MFE, which  involves finding fixed-points of complex nonlinear operators.

In Section \ref{Sec:Q-learning}  we introduce a version of this algorithm that does not require knowledge of the payoffs and transition functions and learns the MFE.

For a fixed scalar interaction $m \in [a,b]$, and an initial function $\bar{V}_{0}$, the value function iteration algorithm generates a sequence $\{\bar{V}_{h}\}$ by applying the Bellman equation:
        \begin{equation} \label{Eq:VFI}
            \bar{V}_{h+1}(x,m) = \max_{a \in \Gamma(x)} \left( \pi(x, a, m)  +\beta  \int _{E}  \bar{V}_{h} (w (x ,a ,m,\zeta) ,m) q(d\zeta) \right)
               \end{equation}
that converges to the value function.  


Recall that $s^{m,g}$ is the unique invariant distribution of the Markov chain $L_{m,g}$ (see Assumption \ref{Assumption:Unique}) and the scalar interaction function $M$ satisfies $a \leq M(s) \leq b$ for any population state $s$. 



\begin{algorithm}[H]
\caption{Adaptive Value Function Iteration for MFE with Scalar Interaction}
\label{alg:value_iteration_mfe}

\textbf{Input:}  Initiate  \( m_1 = a \) and \( m_2 = b \). \\
\vspace{1.5mm}
        Repeat until \( f(m_t) = 0 \):
    \begin{enumerate} [leftmargin=1em, itemsep=0.1ex]
        \item Set \( m_t = \frac{a + b}{2} \).       
        \item  Apply value function iteration under  $m_{t}$ until convergence to \( V_{t} \) (see Equation (\ref{Eq:VFI})) and 
           derive the optimal policy (see Equation (\ref{Equation:optimalpolicy})). 
           \item Compute the unique invariant distribution \( s^{m_{t},g_{t}} \) given $g_{t}$ and $m_{t}$: 
           \[
           s^{m_{t},g_{t}}(y) = \sum_{x \in X} L_{m_{t},g_{t}}(x, m_{t}, y)  s^{m_{t},g_{t}}(x), \quad \forall y \in X.
           \]  
        \item Evaluate \( f(m_t) = m_t - M(s^{m_{t},g_{t} }) \). Update bounds: $$ b = m_t \text{ if  } f(m_t) > 0 \text{; }  a = m_t  \text{ if } f(m_t) < 0 $$
    \end{enumerate}


\end{algorithm}

Importantly, as a byproduct of Theorem \ref{thm:Q_convergence} in Section \ref{Sec:Q-learning}, the Adaptive Value Function Iteration Algorithm guarantees to converge to an MFE under mild assumptions. 
We note that Algorithm~\ref{alg:value_iteration_mfe} guarantees convergence to MFE without relying on contraction or monotonicity properties, which are typically required for similar convergence results. In addition, uniqueness of an MFE is not guaranteed under the assumptions of Theorem \ref{Theorem:Value}. Instead, the convergence is achieved by leveraging the scalar interaction structure.  


Theorem \ref{thm:Q_convergence} and its proof immediately leads to the proof of Theorem \ref{Theorem:Value} regarding the convergence of Algorithm \ref{alg:value_iteration_mfe} to an MFE. 

\begin{theorem} \label{Theorem:Value}
    Suppose that Assumption \ref{Assumption:Unique} holds and the optimal policy correspondence $G$ is single-valued.\footnote{See Section \ref{Sec:Q-learning} and in particular in Footnote \ref{footnote_single} for a discussion on the single-valuedness of the policy.} 

    Let \( \{m_t\}_{t \in \mathbb{N}} \) be the sequence generated by Algorithm \ref{alg:value_iteration_mfe}. 
Then $m_{t}$ converges to $m^{*}$ and the corresponding policy function $g^{*}$ and population state $s^{m^*}$ from Steps 2 and 3 of Algorithm \ref{alg:value_iteration_mfe} constitute an MFE. 
\end{theorem}




\subsection{Unknown Payoffs and Transitions: Adaptive Q-learning} \label{Sec:Q-learning}


In this section we introduce an algorithm that does not require prior information on the model. The Adaptive Q-Learning algorithm for MFE with a scalar interaction presented in Algorithm \ref{alg:bisection_q_learning}  differs from the Adaptive Value Function Iteration described in Algorithm \ref{alg:value_iteration_mfe} in two aspects. In the Q-learning approach, rather than performing value function iteration, we use Q-learning, a model-free reinforcement learning method, to learn the optimal policy directly through interaction with the environment. This enables the algorithm to have no prior information on the payoff and transition functions. Furthermore, instead of directly solving for the invariant distribution induced by the policy through a linear system of equations that uses information on the transition function, the algorithm uses standard Monte Carlo sampling, to compute the approximate invariant distribution. This sampling leverages interaction with a simulator also. 
We note that the use of a simulator is standard in reinforcement learning, where the agent  engages repeatedly with the environment. Overall, Algorithm \ref{alg:bisection_q_learning} can be seen as a data-driven or a simulated version of Algorithm \ref{alg:value_iteration_mfe}.

Importantly,  we   show in Theorem \ref{thm:Q_convergence} that, asymptotically, the algorithm produces a sequence of scalar interactions that converges to a scalar interaction function that induces an MFE. 


 We now present Algorithm \ref{alg:bisection_q_learning} and state our asymptotic result for this algorithm.\footnote{\label{footnote_single}In Step 2 of Algorithm \ref{alg:bisection_q_learning}, in practical applications, \(\hat{Q}_{H,m_{t}}\) can be slightly perturbed (e.g., by adding a small random noise) to ensure that it does not have equal values with probability 1, making \(\hat{g}_t\) single-valued with probability $1$. For practical implementation, another option is to consider a soft-max relaxation to the argmax, e.g., \cite{guo2023general}. } 
 The proof is deferred to the Appendix. We denote by $Q^{*}_{m}$  the optimal $Q$-function when the value of the scalar interaction function is $m$ (see Equation (\ref{Eq:Q}) in the Appendix for the precise definition of the asynchronous $Q$-learning algorithm).






\begin{algorithm} [H]
\caption{Adaptive Q-Learning for MFE with Scalar Interaction}
\label{alg:bisection_q_learning}


\textbf{Input:} Samples $K,H$,  tolerance level \(\delta > 0\).  

\vspace{2mm}
Repeat until \(|\hat{f}(m_t)| \leq \delta\):
\begin{enumerate}[leftmargin=1em, itemsep=0.1ex]
    \item \textbf{Set:} \(m_t = \frac{a + b}{2}\).
    \item \textbf{Q-learning:} Apply Q-learning under \(m_t\) with \(H\) iterations to compute \(\hat{Q}_{H,m_{t}}\) (see Equation (\ref{Eq:Q}) in the Appendix). Derive the policy 
    $
   \hat{g}_t(x, m_t) = \argmax _{a \in A } \hat{Q}_{H,m_{t}}(x, a, m_t).
    $
    \item \textbf{Sampling:} Simulate \(K\) transitions using \(\hat{g}_t\):
    $
    x_{k+1} \sim L_{m_t, \hat{g}_t}(x_k, m_t, \cdot)$ for $k = 0, 1, \dots, K$. 
    
    Compute the approximate invariant distribution:
    \[
    \hat{s}^{m_t, \hat{g}_t}(y) = \frac{1}{K} \sum_{k=0}^{K} \mathbf{1}_{\{x_k = y\}}, \quad \forall y \in X.
    \]
    \item \textbf{Evaluate and update:} Evaluate \(\hat{f}(m_t) = m_t - M(\hat{s}^{m_t, \hat{g}_t})\). Update: $$ b = m_t \text{ if  } \hat{f} (m_t) > \delta \text{; }  a = m_t  \text{ if } \hat{f} (m_t) < -\delta. $$ 
\end{enumerate}
\end{algorithm}



\begin{theorem}
\label{thm:Q_convergence} (Asymptotic convergence).
Assume that the standard step-size conditions $ \sum _{h=0}^{\infty} \gamma_{h} (x,a) = \infty \text { and } \sum _{h=0}^{\infty} \gamma_{h} ^{2} (x,a) < \infty$ hold (see Section \ref{Section:AppendixProofs} for more details),  Assumption \ref{Assumption:Unique} holds, and the optimal policy correspondence $G$ is single-valued. 

Let \( \{m_t\}_{t \in \mathbb{N}} \) be the sequence generated by Algorithm \ref{alg:bisection_q_learning},  where the number of Q-learning updates \( H \to \infty \) and the number of samples \( K \to \infty \) at each iteration $t$ and the tolerance level $\delta = 0$. 
Then $m_{t}$ converges to $m^{*}$ and the corresponding policy function $g^{*}$ and population state $s^{m^*}$ from Steps 2 and 3 of Algorithm \ref{alg:bisection_q_learning} constitute an MFE. 
\end{theorem}

We now provide a few remarks regarding Theorem \ref{thm:Q_convergence}.


\begin{remark} \label{Remark:PolicyGrad}
\textbf{(Beyond Asynchronous $Q$-Learning):} Step 2 in Algorithm \ref{alg:bisection_q_learning} applies an asynchronous $Q$-learning algorithm to determine the optimal policy under a fixed scalar interaction function. However, since the scalar interaction function remains fixed in Step 2, the asynchronous $Q$-learning algorithm can be substituted with any other reinforcement learning method. If the method   guarantees to converge to the optimal policy then Theorem \ref{thm:Q_convergence} will still hold, ensuring convergence to the MFE. For example, other variants of $Q$-learning such as double $Q$-learning \citep{hasselt2010double} and tabular policy gradient methods with direct and softmax parameterization \citep{agarwal2021theory,bhandari2024global} have been shown to guarantee convergence. In Section \ref{Sec:Policy} in the Appendix, we propose an adaptive policy gradient algorithm for MFE with scalar interactions.
 \end{remark}
 
\begin{remark} \label{Remark:State}
\textbf{(Large State Spaces):} Algorithm \ref{alg:bisection_q_learning}  can be naturally extended to large state spaces by substituting the $Q$-learning algorithm employed in Step 2 of Algorithm with reinforcement learning methods tailored for such settings, such as $Q$-learning with function approximation using neural networks approximation or linear function approximation. As long as the reinforcement learning algorithm successfully identifies the optimal policy, the asymptotic convergence to MFE established in Theorem~\ref{thm:Q_convergence} remains valid. A brief discussion on adapting Algorithm \ref{alg:bisection_q_learning} to large state spaces is provided in Section~\ref{Sec:LargeState} in the Appendix.
 \end{remark}
 
\begin{remark} \textbf{(Multi-Valued Interaction Functions):} 
The approach of Algorithm \ref{alg:bisection_q_learning} can be extended to cases where the interaction function \( M \) maps the population state to a subset of \(\mathbb{R}^n\) for some $n \geq 2$. In this multi-dimensional setting, the algorithm updates the scalar interaction using numerical techniques, such as Broyden’s method. Unlike the scalar interaction case, this generalization does not guarantee global asymptotic convergence but it is a natural candidate to compute MFE if $n$ is small. A discussion of the multi-valued interaction case and algorithmic implementation are provided in Section \ref{sec:Multi} in the Appendix.
\end{remark}


\subsection{Finite-Time Analysis of Adaptive $Q$-learning} \label{Section:Finite-time}
Although Theorem \ref{thm:Q_convergence} provides theoretical asymptotic guarantees, in practice, for a finite sample size \(H\) and \(K\) in Algorithm \ref{alg:bisection_q_learning}, the function \(\hat{f}(m)\) may not converge to \(0\) due to inaccuracies in the estimated policy function and the approximate invariant distribution.
Furthermore, even if \(f(m) = m - M(s^{m,g}) \) is continuous, as shown in the proof of Theorem \ref{thm:Q_convergence}, \(\hat{f}(m)\) might not inherit this continuity. This lack of continuity can cause instability in the bisection approach. Nevertheless, it is often possible to find a small \(\delta > 0\) such that \(|\hat{f}(m_t)| \leq \delta\), either by using a bisection style method as in Algorithm \ref{alg:bisection_q_learning}, or through adaptive search techniques designed to minimize \(|\hat{f}(m)|\).
An important question is how close the solution obtained by these approximations to an MFE. 

In Theorem~\ref{Thm:finite_bounds}, we analyze the case where Algorithm~\ref{alg:bisection_q_learning} is executed for \(T\) iterations and in Theorem ~\ref{Thm:finite_bounds1} we analyze  the case where a small value of \(|\hat{f}(m)|\) is obtained using any optimization method. In both instances, our goal is to establish whether the resulting output qualifies as an approximate MFE and to quantify the approximation error. This analysis is important for evaluating the practical applicability of the proposed algorithms for finding MFE.


These approximation results rely on Lipschitz continuity assumptions we introduce in Assumption \ref{Assumption:Lip}. By combining the  well-developed finite-sample analyses for \(Q\)-learning algorithms and finite-sample analyses for Monte Carlo sampling  with the these Lipschitz properties, we derive finite-time error guarantees for Algorithm~\ref{alg:bisection_policy}. As usual, we denote by $\norm{}_p$ the $p$-norm. 


\begin{assumption} \label{Assumption:Lip}
Let $L_{Ms},L_{sm},L_{sg},L_{gm},L_{gQ}$ be positive Lipschitz constants. 
  
    (i) Scalar Interaction Lipschitz: 
    $$\abs{M(s) - M(s')} \leq L_{Ms} \norm{s - s'}_1$$ for all $s, s' \in \mathcal{P}(X)$. 

(ii) Invariant Distribution Lipschitz:  $$\norm{s^{m,g} - s^{m',g'}}_1 \leq L_{sm} \abs{m - m'} + L_{sg} \norm{g(\cdot, m) - g'(\cdot, m)}_\infty$$ for all $m, m' \in [a,b]$ and policies $g,g'$.

(iii) Policy function Lipschitz: 
$$\norm{g(\cdot, m) - g(\cdot, m')}_\infty \leq L_{gm} \abs{m - m'} \text { and } \norm{\hat {g} _{t} (\cdot, m) - g(\cdot, m)}_\infty \leq L_{gQ} \norm{\hat{Q}_{H,m}  - Q^{*} _{m} }_{\infty}$$   for all $m, m' \in [a,b]$. 

(iv) The function $f(m) = m - M(s^{m,g})$ is not too flat near its roots, in the sense that for every root $m^{*}$ of $f$, if  $$|f(m) - f(m^{*})| = |f(m)| \leq 2 \delta \text{ then } |m^{*}  - m| \leq b(\delta) $$
for some decreasing function $b$ such that $b(\delta) \rightarrow 0$ whenever $\delta \rightarrow 0$. 
\end{assumption}

Assumption~\ref{Assumption:Lip} imposes several Lipschitz continuity conditions that are used for derivation of finite-time error bounds for Algorithm \ref{alg:bisection_q_learning}. First, the scalar interaction function $M(s)$ is required to be Lipschitz continuous in the population state $s$, as assumed in part (i). This is typically satisfied in many practical settings, including cases where $M(s)$ represents the expected value operator. Part (ii) relates to the sensitivity of the invariant distribution $s^{m,g}$ to changes in the scalar $m$ and the policy $g$. This is a property that is heavily studied in perturbation theory for Markov chains, which establishes when the invariant distribution of a Markov kernel depends continuously on its parameters (typically, the Markov chain's transition function needs to satisfy Lipschitz continuity in those parameters \citep{shardlow2000perturbation}). Part (iii) ensures that the optimal policy is Lipschitz continuous in the scalar $m$, which can be derived using dynamic programming techniques (for some recent results see \cite{anahtarci2023learning}). Finally, part (iv) is a  technical condition that ensures that the function $f(m)$, is not excessively flat near its roots. 

We now present Theorem~\ref{Thm:finite_bounds} that provides finite-time error bounds for Algorithm \ref{alg:bisection_q_learning} 
 under Assumption~\ref{Assumption:Lip} and the conditions of Theorem~\ref{thm:Q_convergence}. These bounds characterize the accuracy of the learned policy $\hat{g}_t$ and the population state $\hat{s}^{m_t, \hat{g}_t}$ relative to some MFE policy $g(\cdot, m^*)$ and invariant distribution $s^{m^*, g}$. 
 In the case the algorithm terminates before iteration $T$, the errors depend on both the sampling errors (\(\delta_H, \delta_K\)) from the $Q$-learning algorithm and the Monte Carlo Sampling, the  Lipschitz constants, the number of iterations until terminating, and the function $b$ that controls the flatness of $f$. In this case, the error vanishes as the sampling errors tend to $0$ as expected. When the algorithm does not terminate until iteration $T$, the errors depend on both the sampling errors (\(\delta_H, \delta_K\)), the  Lipschitz constants, and the number of iterations $T$. 
We note that the finite sample bounds for the Monte Carlo sampling and the Q-learning approximation errors needed for Theorem \ref{Thm:finite_bounds} are standard in the literature on reinforcement learning and Markov chain analysis.\footnote{For Q-learning, these finite-time bounds can be found, for example, in \cite{even2003learning}, \cite{qu2020finite}, \cite{li2020sample},  and \cite{li2024q}. For Monte Carlo sampling, the accuracy of invariant distribution estimates is well-understood in Markov chain theory \cite{seneta2006non}.}






\begin{theorem} \label{Thm:finite_bounds} (Finite-time bounds). 
    Assume that the assumptions of Theorem \ref{thm:Q_convergence} and Assumption \ref{Assumption:Lip} hold.  Let \( \{m_t\} \) be the sequence  generated by Algorithm \ref{alg:bisection_q_learning} with tolerance $\delta$ and samples $K,H$ such that: for all $t$ we have finite sample bounds for the Q-learning and Monte Carlo sampling, i.e., 
    $$
    \|\hat{Q}_{H,m_{t}} - Q_{m_t}^*\|_\infty \leq \delta_{H}, \text{ and }   \|\hat{s}^{m_t, \hat{g}_t} - s^{m_{t}, \hat{g}_t}\|_1 \leq \delta_{K},
$$
with probability $1-\epsilon_{H}$ and $1-\epsilon_{K}$ respectively, and the tolerance level satisfies  $$  L_{Ms} \delta_K + L_{Ms} L_{sg} L_{gQ} \delta_H \leq \delta .$$



Suppose we execute Algorithm \ref{alg:bisection_q_learning} for at most $T$ iterations.  Then: 
    

    (i) If the algorithm stopped at iteration $n \leq T$, 
    then with probability at least $1- n(\epsilon_{H} + \epsilon_{K}) $,   the policy $\hat{g}_n(\cdot, m_n)$ and population state $\hat{s}^{m_{n},\hat{g}_{n} } $ generated from Algorithm \ref{alg:bisection_q_learning} satisfy 
$$ \|\hat{g}_n(\cdot, m_n) - g(\cdot, m^{*} ) \|_\infty \leq L_{gQ}\delta _{H} + L_{gm}C' \text { and } \norm{\hat{s}^{m_{n},\hat{g}_{n} }  - s^{m^{*},g  }}_1  \leq  \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}C'$$
with $C' = \min \{b(\delta), (b-a)2^{-n} \} $ where $g(\cdot , m^{*})$ and $s^{m^{*} , g}$ correspond to an MFE. 
    
(ii) If the algorithm didn't stop, then with probability at least  $1- T(\epsilon_{H} + \epsilon_{K}) $,   the policy $\hat{g}_T(\cdot, m_T)$ and population state $\hat{s}^{m_{T},\hat{g}_{T} } $ generated from Algorithm \ref{alg:bisection_q_learning} satisfy 
$$ \|\hat{g}_n(\cdot, m_n) - g(\cdot, m^{*} ) \|_\infty \leq L_{gQ}\delta _{H} + L_{gm}C'' \text { and } \norm{\hat{s}^{m_{n},\hat{g}_{n} }  - s^{m^{*},g  }}_1  \leq  \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}C''$$
with $C'' = (b-a)2^{-T} $ 
where $g(\cdot , m^{*})$ and $s^{m^{*} , g}$ correspond to an MFE. 

\end{theorem}


While Theorem~\ref{Thm:finite_bounds} provides error bounds with high probability  that depend on the number of iterations \(n\), it is worth noting that this dependence is typically not significant in our simulations. The number of iterations required for the algorithm to stop, \(n\), is generally much smaller than the number of Monte Carlo samples (\(K\)) or Q-learning updates (\(H\)) needed to achieve a sufficiently small \(\delta\), which in turn leads to small \(\epsilon_{H}\) and \(\epsilon_{K}\). This ensures that the overall error probability \(1 - n(\epsilon_{H} + \epsilon_{K})\) remains high. In Theorem \ref{Thm:finite_bounds1} we remove this dependence and provide finite-time errors that depend on the fact that \(|\hat{f}(m_{n})| \leq \delta\) is satisfied for some \(m_{n}\). More importantly, this result opens up the possibility of using alternative optimization methods, such as Nelder-Mead or other search algorithms, to minimize \(|\hat{f}(m)|\) instead of the bisection method that can be too aggressive for some applications. In addition, with an appropriate adjustment for Assumption \ref{Assumption:Lip}, this theorem continues to hold for the multi-valued case (see Section \ref{sec:Multi}) where we use a quasi-Newton method to compute the MFE. 
The key observation is that the proof of Theorem~\ref{Thm:finite_bounds} part (i) can be applied by replacing \(C'\) with the larger value \(b(\delta) \), which depends solely on the iteration where \(|\hat{f}(m_n)| \leq \delta\), to establish a finite-time bound.


In practice, it is possible to adaptively adjust the parameters \(K\) and \(H\) during the optimization process. For instance, \(K\) and \(H\) can initially be kept small while exploring the parameter space to identify a region where \(|f(m)|\) is relatively small. Once such a region is found, the number of samples \(K\) and Q-learning updates \(H\) can be increased to reduce \(\delta_{K}\) and \(\delta_{H}\), thereby tightening the error bounds at the expense of additional computational effort. 



\begin{theorem}
\label{Thm:finite_bounds1}
    Assume that the assumptions of Theorem \ref{thm:Q_convergence} and Assumption \ref{Assumption:Lip} hold. 
    Suppose that we evaluate $\hat{f}(m_{t}), \hat{g_{t} }, \hat{s_{t}} $ as in Algorithm \ref{alg:bisection_q_learning} with $\delta,K,H$  that satisfy the condition of Theorem \ref{Thm:finite_bounds}.

   Let $\{ m _{t} \}$ be a sequence that is generated by some algorithm (not necessarily Algorithm \ref{alg:bisection_q_learning}). If $|\hat{f}(m_{n}) | \leq \delta$ at some iteration $n$, then with probability at least with probability $1- (\epsilon_{H} + \epsilon_{K}) $,   the policy $\hat{g}_n(\cdot, m_n)$ and population state $\hat{s}^{m_{n},\hat{g}_{n} } $ satisfy 
$$ \|\hat{g}_n(\cdot, m_n) - g(\cdot, m^{*} ) \|_\infty \leq L_{gQ}\delta _{H} + L_{gm}b(\delta) \text { and } \norm{\hat{s}^{m_{n},\hat{g}_{n} }  - s^{m^{*},g  }}_1  \leq  \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}b(\delta)$$
 where $g(\cdot , m^{*})$ and $s^{m^{*} , g}$ correspond to an MFE. 
    
   

\end{theorem}


\section{Applications} \label{Section:applications}


In this section, we explore various dynamic models of competition that naturally have a scalar interaction and capture a broad range of phenomena in economics and operations. Using our algorithms to find MFEs, we provide numerical results for comparative statics. This section shows the prevalence of scalar interaction functions in dynamic competition environments and demonstrates how our algorithms enable reliable comparative statics analysis and to gain insights into how key market parameters influence on equilibrium outcomes.

Furthermore, scalar interaction functions are widespread in heterogeneous dynamic macroeconomic models, which are beyond the scope of this section. For example, \cite{acemoglu2012} provides numerous examples of such models incorporating scalar interaction functions.

\subsection{Capacity Competition and Quality Ladder Models}\label{Subsec:Quality ladder}

In this section, we explore dynamic models of capacity competition and quality ladders, both widely studied in the operations and economics literature. In these frameworks, a firm's state captures a crucial factor impacting its profitability, such as its production capacity or product quality. Profits in each period are determined through a static competition game that models the interplay of firms' differing state variables. Over time, firms strategically make decisions to advance their states, seeking to increase their future profitability. We describe the models we study below. 


\textit{States.} The state
of firm $i$ at time $t$ is denoted by $x_{i ,t} \in X$ where $X=\{0,\ldots,\overline{x} \}$ is a finite set. 

\textit{Actions.} Firm \( i \) invests $ a_{i,t} \in A$ at period \( t \) to improve its state where $A = \{0, \ldots, \overline{a} \}$ is a finite set.  


\textit{States' dynamics.}  We use standard transitions that have been widely used in these models (e.g., see \cite{weintraub2010computational}).

Each firm has the opportunity to invest in the quality of its product. The success of these investments is stochastic, with a successful investment resulting in an increase in the firm's state. The probability that an investment is successful is given by $\frac{a}{1+a}$, where $a$ is the action of the firm, corresponding to its level of investment. Upon a successful investment, the firm's state increases by one level.
Independently of the investment outcomes, a firm's state may depreciate over time. This depreciation is modeled as a stochastic process where the firm's state decreases by one level with probability $\delta \in (0,1)$ in each period.

Formally, the transition probabilities of the system are given by a Markov kernel $W$ where \(W(x, a, y)\) represents the probability of a firm transitioning from state \(x\) to state \(y\), given the firm's action $a$ and is given by: 
\begin{equation} \label{Eq:Oligopoly}
    W(x, a, y) = 
    \begin{cases}
   \frac{(1-\delta)a}{1 + a}, & \text{if } y = x + 1, \text{ (successful investment, no depreciation)} \\
    \frac{1-\delta+\delta a }{1+a}, & \text{if } y = x , \text{ (no change)} \\
    \frac{ \delta }{1 + a}, & \text{if } y = x-1, \text{ (unsuccessful investment, depreciation)} \\
    0, & \text{otherwise},
    \end{cases}
\end{equation}
for $ x=1,\ldots,\overline{x}-1$.  In addition, we define
$$W(0,a,1) =\frac{ (1-\delta) a}{1+a} \text { , } W(0,a,0) = 1- W(0,a,1) $$ 
and 
$$W(\overline{x},a,\overline{x}-1)= \frac{\delta}{1+a} \text{ , }  W(
\overline{x},a,\overline{x})  = 1-W(\overline{x},a,\overline{x}-1).$$
Thus, $0$ serves as the lower bound and $\overline{x}$ as the upper bound of the state space.





\textit{Payoff.} The cost of investing an amount $a$ is given by $c(a)$ for some function $c:A \rightarrow \mathbb{R}$.  In addition, there is a single-period profit function $u (x , M(s))$ derived from a static game. When a firm invests $a \in A$, the firm's state is $x \in X$, and the population state is $s \in \mathcal{P} (X)$, then the firm's single-period payoff function is given by $$\pi  (x ,a ,M(s)) =u (x ,M(s))-c(a).$$  

 
We now present two classic examples of profit functions, $u(x, M(s))$, commonly used in the literature. These functions exhibit a natural scalar interaction through an explicit function, $M$.


 The first is a simple model of capacity competition based on the models of \cite{besanko2004capacity}, \cite{besanko2010lumpy}, \cite{ifrach2016framework}.
Consider an industry producing homogeneous goods, where the state variable of each firm determines its production capacity. If a firm's state is $x$, its production capacity is represented by $\bar{q}(x)$. Each period, firms undertake costly actions to enhance their capacity for the subsequent period. Additionally, firms compete in a capacity-constrained quantity-setting game during each period. The market is characterized by an inverse demand function $P(Q)$, where $Q$ denotes the total output produced by the industry. For simplicity, we assume all firms have zero marginal costs.

Given the total output produced by its competitors, $Q_{-i}$, firm $i$ solves the following profit maximization problem:
\[
\underset{0 \leq q_{i} \leq \bar{q}(x_{i})}{\max} \; P(q_{i} + Q_{-i}) q_{i}.
\]

In general, the equilibrium of a static quantity-setting game can be derived, where firms’ actions determine their single-period profits. However, we focus on a limiting case in which a large number of firms each possess negligible market power. In this regime, firms treat $Q$ as fixed and produce at full capacity; the limiting payoff function is given by:

\begin{equation*} \pi(x,a,M(s)) =P \left (\sum_{y \in X} \bar{q} (y) s ( y)\right ) \bar{q} (x) - c(a).
\end{equation*} 

For the capacity competition model, the scalar interaction function is given explicitly by $M(s) = \sum _{y \in X} \bar{q}(y) s(y)$ and represents the average capacity in the economy. 



As a second example, we consider a classic quality ladder model, where the state of each firm reflects the quality of its product (see, for instance, \cite{pakes1994computing} and \cite{ericson1995markov}). Assume price competition takes place under a logit demand setting. The market consists of $N$ consumers, and the utility that consumer $j$ derives from consuming the product of firm $i$ at time $t$ is given by:
\begin{equation*}u_{i j t} =\theta _{1} \ln  (x_{it} +1) +\theta _{2} \ln  (Y-p_{it}) +v_{ijt},
\end{equation*}
where $\theta_1 \in (0,1)$, $\theta_2 > 0$, $p_{it}$ denotes the price of the good produced by firm $i$, $Y$ represents the consumer's income, $x_{it}$ indicates the quality of the good produced by firm $i$, and $\{v_{ijt}\}_{i,j,t}$ are i.i.d. Gumbel random variables capturing unobserved characteristics for each consumer-good pair.


The market has $m$ firms, each facing a constant and identical marginal production cost. The pricing game has a unique Nash equilibrium in pure strategies (see \cite{caplin1991aggregation}). These equilibrium prices determine the single-period profits. The limiting profit function of interest emerges in an asymptotic setting, where the number of consumers $N$ and the number of firms $m$ increase indefinitely while maintaining a fixed ratio. This limiting profit function 
 is given by:
\begin{equation*}u (x ,M(s)) =\frac{\tilde{c} (x +1)^{\theta _{1}}}{M(s)}.
\end{equation*}
The constant $\tilde{c}$ depends on the consumer's income, the marginal production cost, the limiting equilibrium price, and $\theta_2$, and the scalar interaction function is expressed explicitly as $M(s) = \sum_{y \in X} (y + 1)^{\theta_1} s(y)$ (see \cite{besanko1990logit}).

As an illustration of how our algorithms can be applied to analyze the impact of key economic variables on the MFE, we consider a linear demand structure in the capacity competition model, where the demand intercept plays a crucial role in determining capacity choices. Using our algorithms, we compute the MFE under varying demand intercepts. Our results show that higher demand intercepts shift the equilibrium distribution of capacities higher, and increase the average capacity. 
We provide a detailed analysis, including plots of the MFE distribution for various demand intercepts, in the Appendix. 



While the capacity competition model we consider is relatively simple, as we have shown in Example \ref{Example:non-convergence}, other algorithms such as standard fixed-point iteration may fail to converge to the mean field equilibrium even in simpler cases, and thus causing counterfactual analysis to be unreliable.  




\subsection{Dynamic Inventory Competition Models} \label{Section:Inventory}

In this section, we consider dynamic inventory competition models with many retailers. In these models, retailers' states correspond to their current inventory levels. Per-period profits are typically based on a newsvendor game where the demand for a retailer's product is uncertain and depends on the decisions of other retailers. Each retailer action corresponds to an inventory order-up-to level. Similar to the literature in competitive inventory models, such as \cite{lippman1997competitive,mahajan2001inventory,liu2007dynamic,olsen2014markov}, we assume that an increase in the inventory of retailer $j \neq i$ decreases the demand for retailer $i$'s products.
More specifically, we analyze the stockout-based substitution dynamics \citep{olsen2014markov}. Under this demand model, a portion of retailer $i$'s excess demand is reallocated to competing firms. Customers who encounter a stockout at their first-choice retailer may attempt to purchase the product from another retailer to fulfill their demand. Static versions of stockout-based substitution games have been analyzed in detail by \cite{lippman1997competitive}. We introduce and focus on a mean field version of the dynamic version of these models.
 



\textit{States.} The state of retailer $i$ is denoted by $x_{i ,t} \in X$ where $X=\{0,\ldots,\overline{x} \}$ is a finite set.

\textit{Actions.} At each period $t$, retailer $i$ choose a feasible level of inventory $a_{i,t} \in  \Gamma (x) = \{x,\ldots, \overline{x} \}$ where $A = \{0,\ldots, \overline{x} \}$. 

\textit{States' dynamics.} 
 We assume that $\zeta_{i,t}$ is independent across retailers and time and has law $v$ and we denote by $\mathbb{E}_{\zeta}$ the expected value operator with respect to $\zeta$. Each retailer faces a random discrete demand  $D_{i,t}(\zeta_{i,t}, M(s_{t}))$ that depends on the random variable $\zeta$ and the scalar interaction function $M$ and is independent of time and of other retailers conditional on the value of the scalar interaction. We provide explicitly the scalar interaction for the stockout-based substitution demand model we study below. The retailer's next period inventory is the level of inventory the retailer ordered minus the random demand, i.e., 
\begin{equation*}
  x_{i,t+1} = (a_{i,t}- D_{i,t}(\zeta_{i,t}, M(s_{t})))_{+}
\end{equation*}
where we use the standard notation $x^{+} = \max(x,0)$. We will write $D(\zeta, M(s))$ instead of $D_{i,t}(\zeta_{i,t}, M(s_{t}))$ for notational simplicity. 



\textit{Payoff.} The retailer incurs three type of costs: production costs, shortage costs and holding costs. We assume that the holding and shortage costs are linear. The cost of obtaining $a -x$ units is given by $c (a -x)$ where $c$ is some cost function. The holding cost per unit is $h \geq 0$ and the shortage cost per unit is $b \geq 0$. The price per unit sold is given by $r$. The retailer's single period payoff function is given by the expected revenue minus the expected costs: 
\begin{equation*}\pi  (x ,a ,M(s)) =r\mathbb{E}_{\zeta }\min (a ,D(\zeta  ,M(s))) -c(a -x) -h\mathbb{E}_{\zeta }(a -D(\zeta  ,M(s)))_{ +} -b\mathbb{E}_{\zeta }(D(\zeta  ,M(s)) -a)_{ +} .
\end{equation*}

\textit{Demand model.}
 The demand model is based on a stockout-based substitution model, where the total demand for a retailer is influenced by both its baseline demand and unmet demand from other retailers.  Formally, the demand  is given by:
 \begin{equation*} 
 D(\zeta, M(s)) = \lfloor \zeta + \gamma M(s) \rceil, 
 \end{equation*} 
 where $\lfloor \rceil$
denotes rounding to the nearest integer, \(\zeta\) represents the inherent baseline demand for the retailer's product,  \(0 < \gamma \leq 1\) denotes the fraction of unmet demand from competitors that spills over to the retailer, and the scalar interaction function is given by $
M(s) = \sum _{a} \int  _{\zeta}   (\zeta - a)_{+} v(d \zeta) \bar{s} (a)$
and quantifies the aggregate impact of stockouts across all retailers and $\bar{s}$ represents the retailers' action distribution.\footnote{In the dynamic stockout-based substitution model (e.g., \cite{olsen2014markov}) with \(N\) retailers, the total demand for retailer \(i\) arising from stockouts of other retailers is computed (assuming equal redistribution of unmet demand among all retailers) by summing over the actions of all retailers except retailer \(i\), i.e.,  
$
\sum _{i \neq j} (\zeta_{j} - a_{j})_{+}/ (N-1),
$
where \(\zeta_{j}\) represents the realized demand for retailer \(j\) and $a_{j}$ is retailer $j$'s action. The scalar interaction function is the mean field limit of this finite retailers case. We apply rounding to the resulting demand to keep the state space finite.}  Note that in this application, we assume that the population state depends on retailers' actions, but this can be easily incorporated in our setting and does not change the adaptive algorithms used to compute the MFE or their convergence properties.\footnote{In the implementation of the algorithms, rather than using the distribution \(\bar{s}(a )\) over actions, it is sufficient to represent the population state as a distribution \(s(x)\) over the states and compute \(M(s)\) using the policy function \(g(x, m)\). Specifically, the aggregate interaction is computed as \(M(s) = \sum_{x} \int_{\zeta} (\zeta - g(x, m))_{+} v(d \zeta) s(x)\), where \(g(x, m)\) is the policy function derived for the current scalar interaction \(m\). There are more direct ways to deal with action-state joint distributions in terms of modeling (e.g., see \cite{light2022mean}) but they are not necessary for our algorithmic treatment. }  Other demand models can be easily incorporated to our setting.  



We use the adaptive value function iteration (Algorithm \ref{alg:value_iteration_mfe}) and the adaptive Q-learning (Algorithm \ref{alg:bisection_q_learning}) to derive numerical comparative statics results to analyze how changes in the holding cost per unit influence the equilibrium distribution of retailers' inventories. E-commerce marketplaces like Amazon, which often provide warehousing and fulfillment services for retailers, typically charge holding costs for inventory storage.  Our numerical results show that higher holding costs lead to a shift in the equilibrium inventory distribution towards lower levels, as retailers reduce inventory to reduce storage expenses. However, this also increases the frequency of stockouts, which in turn raises equilibrium demand for each retailer due to substitution effects.
 Hence, an e-commerce platform, such as Amazon, can strategically use lower holding costs for retailers to encourage higher inventory levels on its platform. Higher inventories can have several positive effects for an e-commerce platform, e.g.,  faster delivery times, reducing the risk of stockouts, and increasing the overall volume of transactions that can generate revenues for the platform from other sources except holding costs. We now discuss and present the simulation details. 


We simulate a dynamic inventory competition model where the state space \( X = \{0, 1, \dots, 9\} \) represents inventory levels, and the action space \( A = \{0, 1, \dots, 9\} \) specifies replenishment decisions, so the feasible actions are constrained to \( \{x, x+1, \dots, 9\} \) at state \( x \). Retailers discount future payoffs with a factor \( \beta = 0.95 \). Costs include a quadratic production cost \( (a - x)^2 \), a per-unit shortage cost \( b = 2 \), and a per-unit price \( r = 30 \). Demand follows \( D(\zeta, M(s)) \) as described above, where \(\zeta \in \{0, 0.5, \dots, 9\}\) is a random variable drawn with probabilities \( p(\zeta) = \frac{1}{z + 5} / \sum_{z=0}^{18} \frac{1}{z + 5} \), normalized to sum to 1 and we set \( \gamma = 1 \).

We implement Algorithm \ref{alg:bisection_q_learning} for the dynamic inventory competition model using a standard episodic $Q$-learning algorithm with $\epsilon$-greedy  and experience replay. In the $Q$-learning algorithm, we set the learning rate to $\gamma_h = 0.003$ for the updated state-action pair (see Equation (\ref{Eq:Q}) in the Appendix for the exact update). The exploration rate in the $\epsilon$-greedy policy is initialized at $\epsilon_0 = 0.9$ and decays exponentially to a minimum of $0.05$ over training episodes. Each learning episode consists of $100$ time steps, after which the state is randomly reset.
To stabilize Q-learning updates, we employ an experience replay buffer of size 500, storing past transitions and sampling mini-batches of 16. Training is conducted over $1,000$ episodes, with Monte Carlo sampling performed over $200,000$ steps, resulting in $H = 100,000$ and $K = 200,000$ in the notation of Algorithm \ref{alg:bisection_q_learning}. The tolerance level is set to $\delta = 10^{-3}$. 





We find that the equilibrium outcomes obtained using Algorithm \ref{alg:bisection_q_learning} closely match those from Algorithm \ref{alg:value_iteration_mfe} and we report it in the Appendix. Thus, Algorithm \ref{alg:bisection_q_learning}, correctly learns the MFE despite the lack of knowledge on the payoffs and transition functions.
The comparative statics results are presented in the figures below. 


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_HoldingCost2.png}
        \caption{Holding cost = 2}
        \label{fig:Q_holding_cost_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_HoldingCost5.png}
        \caption{Holding cost = 5}
        \label{fig:Q_holding_cost_5}
    \end{subfigure}
    
    \vspace{0.5cm} % Add spacing between rows
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_HoldingCost8.png}
        \caption{Holding cost = 8}
        \label{fig:Q_holding_cost_8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_HoldingCost12.png}
        \caption{Holding cost = 12}
        \label{fig:Q_holding_cost_12}
    \end{subfigure}
    
    \caption{Equilibrium inventory distributions under different holding costs under Algorithm \ref{alg:bisection_q_learning}. As holding costs increase, retailers' equilibrium inventories are lower.}
    \label{fig:Q_holding_cost_comparative_statics}
\end{figure}









\subsection{Dynamic Reputation Model}\label{Sec:reputation}
Motivated by the rapid growth of online marketplaces, dynamic reputation models and the design of reputation systems have become a prominent focus in the operations and management science literature.\footnote{For example,  \cite{aperjis2010optimal},  \cite{papanastasiou2017crowdsourcing}, \cite{besbes2018information}, \cite{shin2023dynamic}, and \cite{maglaras2023product} analyze reputation systems in different settings. } 

We analyze a dynamic reputation model in which sellers progressively enhance their reputation levels over time. In many review systems, a seller's reputation is commonly represented by the simple average of their ratings. Additionally, platforms often provide additional information, such as the total number of reviews received, which is typically displayed to users. Motivated by this, we assume that the state of each seller is defined by two components: the average rating she has received over her history and the total number of reviews accumulated. For simplicity of exposition and simulations, we assume that each seller receives a review from buyers each period. We note that we can easily incorporate random arrivals of reviews to the model. 

We assume that the ranking of a seller is restricted to a finite set of possible values, denoted by \( X_{1} = \{0, 1, \ldots, \overline{x}_{1}\} \). Specifically, the seller's ranking is the value in \( X_{1} \) that is closest to her actual average rating. If the true average lies exactly halfway between two adjacent values in \( X_{1} \), we assume a tie-breaking rule (e.g., rounding up or down) to determine the ranking. We define a function \( f : \mathbb{R} \to X_{1} \) that maps any real number \( x \) to the closest value in \( X_{1} \), i.e., $
f(x) = \underset{y \in X_{1}}{\arg\min} \, |x - y| $ and  assume that $f$ is single-valued given the tie-breaking rule.

Sellers invest in improving their products or services to enhance their reviews over time. For instance, Airbnb hosts might invest in professional cleaning services or better furnishings for their apartments, while drivers in ridesharing platforms like Uber and Grab can invest in keeping their cars clean, providing amenities such as bottled water or phone chargers, to secure higher ratings from riders.
A seller's payoff is influenced by her ranking, the number of reviews she has received, and the rankings and reviews of competing sellers. 




The dynamic reputation model we consider in this section incorporates the arrival and departure of sellers over time. This reflects a realistic aspect of online marketplaces and ensures that the number of reviews for a seller does not grow indefinitely. To maintain a stationary setting, we assume that the rates of seller arrivals and departures are balanced, so the market size is on average constant over time.
After each review, a seller departs the market permanently with probability \( 1 - \beta \), where \( 0 < \beta < 1 \). For every departing seller \( i \), a new seller immediately enters the market, taking the same label \( i \) and starting with a ranking of \( 0 \) and no reviews. 
Under this assumption, it is straightforward to show that the seller's decision problem reduces to the stationary, infinite-horizon, expected discounted reward maximization problem introduced in Section~\ref{Section: model}. The discount factor in this setting corresponds to the probability of remaining in the market. A similar regenerative framework for arrivals and departures is discussed in other MFE models (e.g., \cite{iyer2014mean}).

We now formally describe the dynamic reputation model.  

\textit{States.} The state of seller \( i \) at time \( t \) is denoted by \( x_{i,t} = (x_{i,t,1}, x_{i,t,2}) \in X_{1} \times X_{2} = X \). Here, \( x_{i,t,1} \) represents the ranking of a seller at time \( t \). The second component, \( x_{i,t,2} \), represents the total number of reviews seller \( i \) has received up to time \( t \).




\textit{Actions.} At each time $t$, seller $i$ chooses an action $a_{i ,t} \in A = \{0 ,\ldots, \bar{a} \}$ from the finite set $A$ in order to improve her ranking.


\textit{States' dynamics.} If seller $i$'s state at time $t-1$ is $x_{i ,t -1}$, the seller takes an action $a_{i,t -1}$ at time $t -1$, and $\zeta _{i ,t}$ is seller $i$'s realized idiosyncratic random shock at time $t$, then seller $i$'s state in the next period is given by:
\begin{equation*}x_{i ,t} =\left (f \left (\frac{x_{i ,t-1 ,2}}{1 +x_{i ,t-1 ,2}}x_{i ,t-1 ,1} +\frac{k(a)\zeta _{i ,t}}{1 +x_{i ,t-1 ,2}} \right ) ,\min \left (x_{i ,t-1 ,2} +1 ,M_{2}\right )\right ),
\end{equation*}
 where $k :A \rightarrow \mathbb{R}_{+}$ determines the impact of the seller's investment on the next period's review. The next period's numerical review, $k(a)\zeta$, is assumed to be non-negative, and $M_{2}>0$ is the upper bound on the sellers' number of reviews to keep the state space finite. The first term in the dynamics corresponds to a rounded approximation of the simple average of numerical reviews received thus far, reflecting the seller's ranking. The second term captures the total number of reviews accumulated. The random shocks account for uncertainty inherent in the review process, such as variations in buyer experiences. 



\textit{Payoff.} The cost of a unit of investment is denoted by \( d > 0 \). When a seller's ranking is \( x_{1} \), her total number of reviews is \( x_{2} \), she chooses an action \( a \in A \), and the population state is \( s \in \mathcal{P}(X) \), the seller's single-period payoff is given by:
\[
\pi(x, a, M(s)) = \frac{\nu(x_{1}, x_{2})}{M(s)} - d a,
\]
where \( M(s) = \int \nu(x_{1}, x_{2}) \, s(d(x_{1}, x_{2})) \) is the scalar interaction function and $\nu$ is a positive function. 
The structure of this payoff function resembles the logit model discussed in Section~\ref{Subsec:Quality ladder}. Similar functional forms can be derived from various  static price competition models.


The unit cost of investment serves as a key lever that platforms can adjust to shape market dynamics and seller behavior. By lowering this cost, platforms can encourage sellers to invest more in improving their products or services, and thus, improving welfare for buyers. For example, an hospitality marketplace could support hosts through partnerships with professional cleaning services. 
In the Appendix, we use our algorithms to derive numerical comparative statics results. Our results show that higher investment costs lead to lower equilibrium rankings across the market. 

\subsection{Dynamic Ridesharing Model} \label{Section:Ridesharing}

In this section we analyze a dynamic ridesharing model in which drivers interact with a platform that assigns them ride requests over time. 




This model studies the strategic decision-making of drivers in a ridesharing context, where requests have varying values, and the probability of receiving requests depends on the aggregate behavior of the driver population.\footnote{There is a growing recent literature on drivers' strategic behavior in ridesharing platforms, e.g., \cite{bimpikis2019spatial}, \cite{guda2019your}, \cite{besbes2021surge}, \cite{garg2022driver}, \cite{ma2022spatio}, \cite{castillo2024matching}.} In the model, each driver's state reflects their availability (available or unavailable), the type of ride request they have received (if any), and the remaining trip duration. Available drivers can either accept or reject a request; accepting makes them unavailable for a duration determined by the ride type, while rejecting allows them to remain available. Unavailable drivers become available again once the ride duration is completed. The type of request a driver receives depends on probabilities determined by the fraction of available drivers in the population. Drivers earn a payoff based on the type of request they accept, while rejecting requests or being unavailable yields no payoff. We now present the model formally. 

\textit{States.} The state of driver \( i \) at time \( t \) is denoted by \( x_{i,t} = (x_{i,t,1}, x_{i,t,2}) \in X_{1} \times X_{2} = X \), where \( X_{1} = \{0, 1, \ldots, K'\} \) and \( X_{2} = \{0, 1, \ldots, K\} \). The first component, \( x_{i,t,1} \), represents the driver's availability and remaining ride duration. Specifically, \( x_{i,t,1} = 0 \) indicates that the driver is available, while \( x_{i,t,1} = d \) for \( d > 0 \) indicates that the driver is unavailable and will remain so for \( d \) more periods before becoming available again. The second component, \( x_{i,t,2} \), represents the type of ride request the driver has received, if any. Specifically, \( x_{i,t,2} = j \) corresponds to a request of type \( j \) for \( j \in \{1, \ldots, K\} \), while \( x_{i,t,2} = 0 \) indicates that the driver has no request. Hence, the state \( x_{i,t} \) captures both the driver’s availability status and the nature of the requests they receive.


\textit{Actions.} An available driver (\( x_{i,t,1} = 0 \)) with a non-zero ride request (\( x_{i,t,2} \neq 0 \)) chooses an action \( a_{i,t} \in A = \{0, 1\} \), where \( a_{i,t} = 1 \) represents accepting the ride request and \( a_{i,t} = 0 \) represents rejecting it. If \( x_{i,t,1} = 0 \)  and \( x_{i,t,2} = 0 \) (the driver has no request), the driver's action set is restricted to the element \(  \{0\} \), indicating that no action can be taken. Similarly, if \( x_{i,t,1} = d \) for some $d>0$, (the driver is unavailable), the driver’s action set is also restricted to \(\{0\} \), so drivers cannot take any action while unavailable. 


\textit{States' dynamics.} If the driver’s state at time \( t \) is \( x_{i,t} = (x_{i,t,1}, x_{i,t,2}) \), their next state depends on their action and random shocks. If the driver is available (\( x_{i,t,1} = 0 \)) and accepts a request of type \( j \in \{1,\ldots,K \} \) so (\( x_{i,t,2} = j \)), the driver becomes unavailable for a duration of $d_{j} \in \{1,\ldots,K' \}$ periods.  If the driver rejects the request or does not receive any request (\( x_{i,t,2} = 0 \)), they remain available in the next period (\( x_{i,t+1,1} = 0 \)). An unavailable driver with a remaining duration \( d > 0 \) transitions to the next state with the duration decremented by one period. That is, if \( x_{i,t,1} = d \) for \( d > 0 \), then in the next period, \( x_{i,t+1,1} = d - 1 \). When \( x_{i,t+1,1} = 0 \), the driver becomes available again.\footnote{For simplicity, we make several simplifying assumptions in these dynamics that can be extended at the cost of increasing the state space. First, the model can be easily extended to account for exit and entry decisions where  drivers can choose to leave the platform  or enter the platform. Second, the ride duration and trip requests could depend on specific characteristics of the job or the driver, such as the driver's current zone (spatial considerations), driver's ranking, or other attributes. These extensions can be incorporated by expanding the state space to account for these additional factors.
}
  Drivers receive a new request type in the next period according to the following probabilities:
\[
\Pr(x_{i,t+1,2} = j) =
\begin{cases}
f(M(s_{t})), & \text{if } j = 0 \text{ (no request)}, \\
p_j, & \text{if } j \in \{1, \dots, K\},
\end{cases}
\]
where \( f(M(s)) \) is a function of the fraction of available drivers in the population, $$ M(s) = \sum_{y \in X} 1_{\{y_{1} = 0\} }s(y_{1}, y_{2}) = \sum _{y_{2} \in X_{2} } s(0,y_{2}), $$
and $p_{j}$ represents the probabilities of receiving a request of type $j=1,\ldots,K$. The function \( f \) maps from \( [0,1] \) to \( [0,1] \) and is intuitively assumed to be increasing, reflecting that as the fraction of available drivers increases, the probability of receiving no request also increases.


\textit{Payoff.} If the driver is available (\( x_{i,t,1} = 0 \)) and accepts a request (\( a_{i,t} = 1 \)), the payoff is given by \( u_{j} \). Otherwise, the payoff is zero.

We note that drivers may act strategically by rejecting a request to receive a more favorable request in the future, whether in terms of payoff or trip duration. Additionally, the scalar interaction function 
$M$, which represents the fraction of available drivers, influences the system by affecting the probability that drivers will not receive requests.


Motivated by the shift in ridesharing platforms from multiplicative pricing schemes which amplify earnings for long trips during surge pricing to additive pricing schemes which add a surge bonus that is independent of the trip length \citep{garg2022driver}, we use our algorithms to compute and learn MFE and analyze two distinct payoff structures in the simulations in the Appendix. One payoff structure assigns higher rewards for longer trips, which incentivizes drivers to strategically ``cherry-pick'' rides by rejecting lower-paying requests in anticipation of more rewarding ones. We numerically demonstrate that higher payoffs for long trips result in greater equilibrium availability of drivers as they strategically reject lower-paying requests in equilibrium even that their equilibrium probability to be matched decreases as more drivers become available in equilibrium. 




\subsection{Social Learning Model} \label{Section:SocialLearning}

In this section, we study a mean field model of social learning, where agents interact and update their beliefs about an unknown state \( \theta \) over time.\footnote{Similar social learning settings, in which many agents face uncertainty about an unknown state, observe private signals, and interact, have been extensively studied. A static solution concept that focuses on asymptotic steady states to simplify the challenging task of computing equilibrium has been proposed recently (e.g., \cite{mossel2020social} and \cite{dasaratha2023learning}). } Each agent has access to private signals and aggregate social information, and they strategically allocate effort to improve the precision of their private signals. This model captures the interaction between private learning and social learning dynamics. In particular, each agent's belief evolves according to a DeGroot-style  linear updating rule that incorporates their prior belief, the average belief of the population, and a noisy private signal. The precision of the private signal is endogenous, as agents can exert effort to reduce its variance, though at a cost. An important feature of the model is the weight function $k(a)$, which determines how much weight is placed on the private signal relative to social learning and increases with effort. Hence, effort not only enhances signal precision but also   allows an agent to rely more on their own signal rather than the population. The agent's payoff depends on their belief about 
$\theta$, the actual state $\theta$, and the cost of effort. 

\textit{States.} The state of agent \( i \) at time \( t \) is denoted by \( x_{i,t} \in X \subset [0,1] \), representing the agent's belief about the unknown state \( \theta \). The state space \( X \) is discretized into a finite set for computational purposes. 

\textit{Actions.} Each agent chooses an action \( a_{i,t} \in A = \{0, 1, \ldots, \overline{a}\} \), where \( a \) represents the effort invested by the agent to improve the precision of their private signal. 

\textit{States' Dynamics.} The next state \( x_{i,t+1} \) is determined by a DeGroot-style linear updating rule that incorporates the agent's current belief, aggregate social information, and private signals. Specifically, we define the dynamics as:
\[
    x_{i,t+1} = f\left( c (1 - k(a_{t})) x_{i,t} + (1 - c)(1 - k(a_{t})) M(s_{t}) + k(a_{t}) \zeta_{i,t} \right),
\]
where \( c \in (0,1) \) is a weight on the agent's current belief \( x_{i,t} \),  \( k(a_{t}) \) is a weight function from $A$ to $[0,1]$ that is increasing in the agent's action \( a_{t} \), \( M(s_{t}) \) is the scalar interaction function that describes the average belief: \( M(s_{t}) = \sum_{x \in X} x  s_{t}(x) \), \( \zeta_{i,t} \) is agent's $i$ private signal, which is assumed for simplicity to be a normal random variable and drawn from \( \mathcal{N}(\theta, \sigma^2(a_{t})) \), where \( \sigma^2(a_{t}) \) is the variance of the signal, decreasing in the agent's effort \( a_{t} \).
The function \( f : \mathbb{R} \to X \) maps the real-valued belief update to the closest value in \( X \), ensuring the updated state remains in the discretized state space.\footnote{See Section \ref{Sec:reputation} for a formal description of $f$. As in Section \ref{Sec:reputation}, if the true average lies exactly halfway between two adjacent values in \( X \), we assume a tie-breaking rule to determine the ranking so $f$ is single-valued.} 

\textit{Payoff.} Each agent has a utility function $u(x,\theta)$ that depends only on her own belief and the unknown state $\theta$. The agent's payoff function is given by: 
\[
    \pi(x,a, M(s))  = u(x, \theta) - da
\]
where $d$ is the marginal cost of increasing effort. 

Since agents do not know the actual state 
$\theta$, we employ a model-free algorithm, Algorithm \ref{alg:bisection_q_learning}, to  learn the MFE. Using this algorithm, we analyze in the Appendix how changes in the precision of private signals affect the equilibrium distribution of beliefs. Specifically, we compare the equilibrium belief distributions under different levels of precision.

This precision parameter can have various interpretations across economic settings. For instance, in online social media platforms, it may represent a feature controlled by the platform that influences content quality. Higher content quality corresponds to greater precision, allowing users to rely more on the information provided to form accurate beliefs.\footnote{There is a growing literature on social learning with platform interventions affecting the quality of consumed content \citep{candogan2022social,acemoglu2024model}.} Another example is financial markets, where the precision parameter may represent the quality of information disclosure by firms. Higher disclosure quality enables investors to form more precise beliefs about the firm's value. 

Our numerical experiments reveal that the equilibrium scalar interaction function (i.e., the average belief) remains  close to the true state 
$\theta$, even when private signal precision is low. However, the variance of the equilibrium belief distribution is significantly higher under lower precision, leading to a wider dispersion of opinions in the population. This occurs, in part, because agents exert less effort to refine their beliefs when signal precision is low, as the marginal returns to effort in uncovering the true state are diminished.

In the simulations, the true state is $\theta=0.4$ and beliefs range from $0$ to $1$ in increments of $0.05$, creating a discretized state space of size $20$. Effort levels range from $0$ to $5$ in unit increments so $\overline{a}=5$. The weight on an agent's current belief is set to $c = 0.4$, and the weight function governing reliance on private signals follows $k(a) = \frac{0.5 + a}{1.5 + a}$ which is increasing in the effort. Private signals are drawn from a normal distribution $\mathcal{N}(\theta, \sigma^2(a))$, where the variance is given by $\sigma^2(a) = \frac{1}{3 + \gamma a}$, ensuring that higher effort improves signal precision. The agent's payoff function is quadratic in belief accuracy and is given by $u(x, \theta) = -20 (\theta - x)^2$ and the unit cost of effort is given by $d=0.1$.  
The Q-learning agent follows exactly the same setup as in Section \ref{Section:Inventory} and the Monte Carlo estimation of the stationary distribution is performed with $K = 200,000$ samples. The tolerance level is set $\delta = 10^{-3}$. We analyze two different precision levels $\gamma=5$ and $\gamma=15$ in the following Figure.  


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_Social_Learning5.png}
        \caption{Precision $\gamma=5$}
        \label{fig:precision5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Q_Social_Learning15.png}
        \caption{Precision $\gamma=15$}
        \label{fig:precision15}
    \end{subfigure}

   \caption{Equilibrium belief distributions under different levels of private learning effort. Higher precision leads to a sharper concentration of beliefs around \( \theta = 0.4 \), whereas lower precision results in more dispersed belief distributions. The average equilibrium variance is approximately \( 0.0393 \) for \( \gamma = 5 \) and \( 0.021  \) for \( \gamma = 15 \). Despite differences in precision, agents consistently learn on average the correct state in equilibrium. The average equilibrium belief  is approximately \( 0.3984 \) for \( \gamma = 5 \) and \( 0.4028 \) for \( \gamma = 15 \). This occurs despite agents lacking knowledge of payoffs and transition dynamics.}
\label{fig:sociallearning}
\end{figure}


\section{Conclusions}

This paper develops computational and learning algorithms for computing mean field equilibria (MFE) in large dynamic models with scalar interactions. We introduce an adaptive value function algorithm and show that it converges to an MFE under mild assumptions, without requiring contraction or monotonicity properties. Additionally, we provide a simulation based version that learns the MFE in model-free environments where payoffs and transition dynamics are unknown using reinforcement learning and Monte Carlo sampling. We also provide finite-time bounds for our algorithms under Lipschitz continuity assumptions.

We apply our framework to a range of models from management science and economics, including capacity competition, quality ladders, inventory competition, dynamic reputation, ridesharing, and social learning. The numerical results illustrate how key market parameters influence equilibrium outcomes, providing insights into competitive dynamics and platform design. Our numerical results illustrate that the model-free algorithm we introduce successfully learns the MFE in relatively small state instances. 



There are several promising directions for future research. First, extending the applicability of the algorithms to large state spaces using function approximation and understanding their performance in computing the MFE remains open. Second, while our algorithms guarantee convergence to an MFE, the specific equilibrium to which they converge in models with multiple MFE remains unclear. This has important implications for market design and counterfactual analysis. 







\bibliographystyle{ecta}
\bibliography{learning}

\section{Appendix} 

\subsection{Proof of Theorems \ref{thm:Q_convergence}, \ref{Thm:finite_bounds} and \ref{Thm:finite_bounds1}}
\label{Section:AppendixProofs}

We now present the standard tabular $Q$-learning algorithm used in Algorithm \ref{alg:bisection_q_learning} when the scalar interaction is fixed at $m_{t}$. Let $h$ be an index where we update our $Q$-functions. Then the $Q$-learning algorithm updates the $Q$-functions according to 
\begin{equation} \label{Eq:Q}
     Q_{h+1} (x,a,m_{t}) = (1- \gamma_{h}(x,a) ) Q_{h}(x,a,m_{t}) + \gamma_{h}(x,a) \left (\pi_{h}  + \beta \max_{\Tilde{a} \in \Gamma(x)} Q_{h}(x' , \Tilde{a},m_{t})\right )
     \end{equation}
where $x'$ is sampled randomly  according to the transition function when the scalar interaction is fixed at $m_{t}$ and $\gamma_{h}(x,a) =0$ for $(x,a)$ that are not updated. In asynchronous $Q$-learning, we assume that we update only $(x_{h},a_{h})$ (the state-action pair that was observed in iteration $h$) and do not update the $Q$ values for the other state-action pairs. As we discuss in Remark \ref{Remark:PolicyGrad},  our asymptotic convergence results hold for other versions of $Q$-learning as long as the algorithm convergences to the optimal $Q$-function which we denote by $Q^{*}_{m}$ when the value of the scalar interaction function is $m$.

In Equation (\ref{Eq:Q}), the scalar interaction is held fixed, so the setup is the same as the standard $Q$-learning algorithm. It is well known that, under standard conditions on the learning parameter $\gamma_{h}$, if the $Q$-learning algorithm visits every state-action pair infinitely often, then $Q_{h}$ converges to the optimal $Q^{*}$ function, and the value function is given by \( V(x) = \max_{a \in A} Q^{*}(x, a) \).
 We now state this result (see  \cite{bertsekas1996neuro} or Theorem 4.5 in \cite{light2024course} for a proof) as it will be used in the proof of Theorem \ref{thm:Q_convergence}. 

\begin{proposition} \label{prop:Q}
    Suppose that 

    $$ \sum _{h=0}^{\infty} \gamma_{h} (x,a) = \infty \text { and } \sum _{h=0}^{\infty} \gamma_{h} ^{2} (x,a) < \infty$$
    then $Q_{h}$ derived from the Q-learning described in Equation (\ref{Eq:Q}) converges with probability $1$ to $Q^{*}$.  
\end{proposition}

\begin{proof}[Proof of Theorem \ref{thm:Q_convergence}]
 Let $t$ be some iteration of the algorithm. First note that when  $H$ tends to $\infty$, then from Proposition \ref{prop:Q}, the $Q$-learning algorithm converges to the optimal $Q$ function. This implies that the policy $\hat{g}_{t}(x,m_{t})$ that is derived in Step 2 in Algorithm \ref{alg:bisection_q_learning} is the optimal policy under the scalar interaction $m_{t}$. Hence, from Assumption \ref{Assumption:Unique}, the Markov chain  $L_{m_{t}, \hat{g}_{t} } $ is ergodic. Thus, from the ergodic theorem for Markov chains, when the number of samples $K$ tends to $\infty$, the probability measure $\hat{s}^{m_{t},\hat{g}_{t}}$ is the unique invariant distribution of $L_{m_{t} , \hat{g}_{t}} $.
Hence, the function $f (m) = m - M(s^{m,g})$ defined in Algorithm \ref{alg:value_iteration_mfe} is equal to $\hat{f}$ when $H$ and $K$ tends to $\infty$. 

We claim that $f$ is continuous. To see this, first note that the value function is continuous because the payoff function $\pi$, the correspondence that describes the feasible actions $\Gamma$  and the transition function $w$ are continuous (see Section 3 in \cite{light2024principle}).  We can now apply the maximum theorem (see Theorem 17.31 in \cite{aliprantis2006infinite}) to conclude that the optimal policy correspoendence $G$ is upper semicontinuous.  Because $G$ is single-valued, then the optimal policy function $g$ is continuous. Hence, the transition function of the Markov chain $L_{m,g}$ is continuous which implies that the stationary distribution $s^{m,g}$ is continuous in $m$ (see Theorem 2 in \cite{light2024nonlinear} for a more general result).  Because the scalar interaction function $M$ is continuous we conclude that $f(m) = m - M(s^{m,g})$ is continuous. 


The algorithm generates a sequence \( \{m_t\}_{t \in \mathbb{N}} \) using a standard bi-section method to find the root of the continuous function $f$ that satisfies $f(b) \geq 0$ and $f(a) \leq 0$, and hence, guarantees to converge to the root of the function $f$, say $m^{*}$. As explained above, the induced policy $g^{*}(x,m^{*})$ is the optimal policy under $m^{*}$ and $s^{m^{*}}$ is unique invariant distribution of  $L_{m^{*}, g^{*}} $.  
Because $$f(m^{*}) = m^{*} - M(s^{m^{*}}) = 0,$$ we have 
   \begin{align*}
        s^{m^{*}} (y) & = \sum _{x \in X} L_{m^{*},g^{*}} (x,m^{*},y)  s^{m^{*}}(x) \\
        & =  \sum _{x \in X} L_{M(s^{m^{*}}), g^{*}} (x,M(s^{m^{*}}),y)  s^{m^{*}}(x) \\ & = \sum _{x \in X} W_{g^{*} } (x,M(s^{m^{*}}),y)  s^{m^{*}}(x) 
        \end{align*}
   for all  $y \in X$. 
   That is, $s^{m^{*}}$ and $g^{*}$ constitute a mean field equilibrium as required. 
\end{proof}


\begin{proof} [Proof of Theorem \ref{Thm:finite_bounds}]
Firs note that $f(m) = m - M(s^{m,g})$ is Lipschitz continuous. Indeed,  from part (i) and part (ii) in Assumption \ref{Assumption:Lip}, we have
\begin{align*}
\abs{f(m) - f(m')} &\leq \abs{m' - m} + \abs{M(s^{m,g}) - M(s^{m',g})} \nonumber \\
&\leq \abs{m' - m} + L_{Ms} \norm{s^{m,g} - s^{m',g}}_1 \nonumber \\
&\leq (1 + L_{Ms} L_{sm}) \abs{m' - m}.
\end{align*}
In particular, $f$ is continuous. 


We will now bound the difference between $f(m_{t})$ and $\hat{f}(m_{t})$ for some iteration $t$.

From the Monte Carlo sampling of the invariant distribution and the assumptions in Theorem \ref{Thm:finite_bounds}, we have 
\[
    \|\hat{s}^{m_t, \hat{g}_t} - s^{m_t, \hat{g}_t}\|_1 \leq \delta_K,
\]
with probability $1-\epsilon_{K}$.  
From the Lipschitz continuity of $M(\cdot)$ in the population state (Assumption \ref{Assumption:Lip}(i)) we have
\begin{equation} \label{Ineq:M1}
    |M(\hat{s}^{m_t, \hat{g}_t}) - M(s^{m_t, \hat{g}_t})| \leq L_{Ms} \|\hat{s}^{m_t, \hat{g}_t} - s^{m_t, \hat{g}_t}\|_1 \leq L_{Ms} \delta_K.
\end{equation}



From the assumption on the $Q$-learning error in Theorem \ref{Thm:finite_bounds}, with probability $1-\epsilon_{H}$, we have 
\[
    \|\hat{Q}_{H,m_{t}} - Q_{m_t}^*\|_\infty \leq \delta_H,
\]
where $Q_{m_t}^*$ is the optimal Q-function given $m_t$. 

From Assumption \ref{Assumption:Lip}(iii) and Assumption \ref{Assumption:Lip}(ii), we have 
\begin{align*}
    \|s^{m_t, \hat{g}_t} - s^{m_t, g }\|_1 & \leq L_{sg} \|\hat{g}_t(\cdot, m_t) - g(\cdot, m_t ) \|_\infty  \\
    & \leq  L_{sg} L_{gQ} \|\hat{Q}_{H,m_{t}} - Q_{m_t}^*\|_\infty \\
    & \leq L_{sg} L_{gQ} \delta_H
\end{align*}



Applying the Lipschitz continuity of $M$ again yields 
\begin{equation} \label{Ineq:M2}
    |M(s^{m_t, \hat{g}_t}) - M(s^{m_{t} ,g_{t}})| \leq L_{Ms} \|s^{m_t, \hat{g}_t} - s^{m_{t},g_{t}}\|_1 \leq L_{Ms} L_{sg} L_{gQ} \delta_H.
\end{equation}


Combining Inequality (\ref{Ineq:M1}) and Inequality (\ref{Ineq:M2}) yields 

\begin{align*}
    |f(m_t) - \hat{f}(m_t)| & = |M(\hat{s}^{m_t, \hat{g}_t}) - M(s^{m_{t},g_{t}})| \\ 
    & \leq |M(\hat{s}^{m_t, \hat{g}_t}) - M(s^{m_t, \hat{g}_t})| + |M(s^{m_t, \hat{g}_t}) - M(s^{m_{t},g_{t}})| \\
    & \leq  L_{Ms} \delta_K + L_{Ms} L_{sg} L_{gQ} \delta_H  
\end{align*}
with probability at least $1 - \epsilon_{H} - \epsilon_{K}$. 
Thus, from the tolerance level choice in Theorem \ref{Thm:finite_bounds} we have
\begin{equation} \label{Inequality:hat_error}
    |f(m_t) - \hat{f}(m_t)| \leq \delta 
\end{equation}
with probability at least $1 - \epsilon_{H} - \epsilon_{K}$. 



Suppose first that the algorithm stopped at iteration $n \leq T$ and let $[a_{n},b_{n}]$ be the interval when the algorithm stopped so $m_{n} = (a_{n}+b_{n})/2$. From the union bound, we conclude that with probability at least  $1 - n(\epsilon_{H} + \epsilon_{K})$ we have $ |f(m_t) - \hat{f}(m_t)| \leq \delta $ for $t=1,\ldots,n$.


 
Hence, from the construction of the algorithm and Inequality (\ref{Inequality:hat_error}),  we have $f(b_{t}) \geq \hat{f}(b_{t}) - \delta  > 0$ and $f(a_{t}) < \hat{f}(a_{t}) + \delta <  0$ for $t=1,\ldots,n$ with probability at least $1- n(\epsilon_{H} + \epsilon_{K})$. 


Thus, in this case, there is an $m^{*} \in [a_{n},b_{n}]$  such that $f(m^{*}) = 0$. In addition, in this case, at iteration $n$, when the algorithm stopped, we have $|\hat{f}(m_n)| \leq \delta$, and hence,  
 $$|f(m_{n})| \leq |\hat{f}(m_n)| + |f(m_n) - \hat{f}(m_n)| \leq 2\delta. $$

From the non-flatness assumption, this implies that
$$|m_{n} - m^{*}| \leq b (\delta).$$
 On the other hand, by construction of the algorithm we also have $|m_{n}-m^{*}| \leq (b-a)2^{-n}$. 
 
 Hence, 
 $$|m_{n}-m^{*}| \leq C' = \min \{ b(\delta) , (b-a)2^{-n}\}.$$  


Thus, using Assumption \ref{Assumption:Lip}(iii), we have
\begin{align*}
\|\hat{g}_{n}(\cdot, m_n) - g(\cdot, m^{*} ) \|_\infty & \leq   \|\hat{g}_n(\cdot, m_n) - g(\cdot, m_{n} ) \|_\infty +  \|g(\cdot, m_n) - g(\cdot, m^{*} ) \|_\infty  \\
& \leq L_{gQ} \|\hat{Q}_{H,m_{n}} - Q_{m_n}^*\|_\infty  +  L_{gm} |m_{n} - m^{*} | \\
& \leq  L_{gQ}\delta _{H} + L_{gm}C' 
\end{align*} 
and we also have 
\begin{align*}   
\norm{\hat{s}^{m_{n},\hat{g}_{n} }  - s^{m^{*},g   }}_1 & \leq \norm{\hat{s}^{m_{n},\hat{g}_{n} }  - s^{m_n, \hat{g}_{n}}}_1 + \norm{s^{m_{n},\hat{g}_{n} }  - s^{m_{n},g}}_1 + \norm{s^{m_{n},g} - s^{m^{*},g}}_1 \\
& \leq \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}|m_{n}-m^{*}| \\
& \leq  \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}C'
\end{align*}
 with probability at least $1- n(\epsilon_{H} + \epsilon_{K})$.
  
Now assume that the algorithm didn't stop. Then, from the same argument as above, with probability at least $1-T(\epsilon_{H}+\epsilon_{K})$ the sign of $f$ is the same as the sign of $\hat{f}$. 

Thus, in this case,  by construction of the algorithm and continuity of $f$, we have $|m_{T} - m^{*} | \leq (b-a)2^{-T}$ for some $m^{*}$ such that $f(m^{*}) = 0$. 

Hence, we can use the same inequalities as above to conclude that 
\begin{align*}
\|\hat{g}_{T}(\cdot, m_{T}) - g(\cdot, m^{*} ) \|_\infty  \leq L_{gQ}\delta _{H} + L_{gm}(b-a)2^{-T}
\end{align*} 
and 
\begin{align*}   
\norm{\hat{s}^{m_{T},\hat{g}_{T} }  - s^{m^{*},g }}_1  \leq  \delta_{K} + L_{sg}L_{gQ}\delta_{H} + L_{sm}(b-a)2^{-T}
\end{align*}
which proves the theorem. 
\end{proof}

\begin{proof} [Proof of Theorem \ref{Thm:finite_bounds1}]
From the proof of Theorem \ref{Thm:finite_bounds} (see Inequality (\ref{Inequality:hat_error}) we have 
\begin{equation*} 
    |f(m_n) - \hat{f}(m_n)|  \leq L_{Ms} \delta_K + L_{Ms} L_{sg} L_{gQ} \delta_H \leq \delta .
\end{equation*}
with probability at least $1 - \epsilon_{H} - \epsilon_{K}$. 



From assumption we have $|\hat{f}(m_n)| \leq \delta$, and hence,  
 $$|f(m_{n})| \leq |\hat{f}(m_n)| + |f(m_n) - \hat{f}(m_n)| \leq 2\delta $$
with probability at least $1 - \epsilon_{H} - \epsilon_{K}$. 
From the non-flatness assumption, in this case, we have 
$$|m_{n} - m^{*}| \leq b (\delta).$$
Now we can proceed exactly as in proof of Theorem \ref{Thm:finite_bounds} to conclude the result. 
\end{proof}

\subsection{Additional Simulations}

In this section, we provide simulation details for the models described in Section \ref{Section:applications}. The full code will be publicly available in a GitHub repository. 

\subsubsection{Capacity Competition Model}

In the simulations, capacity levels range from $0$ to $9$ in $1$-unit increments, and investment actions range from $0$ to $3$ in $0.5$-unit increments. The market operates under a linear inverse demand function, $P(Q) = \alpha - Q$, where $\alpha$ is the demand intercept and the slope is $-1$. Investment costs are given by a linear function, $c(a) = 3a$ and the production capacity is the identity function $\overline{q}(x)=x$. In the simulations, Algorithm \ref{alg:value_iteration_mfe} converges fast, typically within $15$ iterations, to the mean field equilibrium. Results are reported for two distinct demand intercepts, showing how the equilibrium outcomes changes with demand. In more detail, we provide plots of the exact mean field equilibrium distribution, that represents the equilibrium production capacity distribution for two different demand intercepts. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Cap9M5.7315.png}
        \caption{Demand intercept $\alpha=9$. The equilibrium average capacity is $5.7315$. }
        \label{fig:D9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Cap10M6.2339.png}
        \caption{Demand intercept $\alpha=10$. The equilibrium average capacity is $6.2339$.}
          \end{subfigure}
           \vspace{0.5cm} 
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Cap11M6.401.png}
        \caption{Demand intercept $\alpha=11$. The equilibrium average capacity is $6.401$.}
        \label{fig:D11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Cap12M6.6741.png}
        \caption{Demand intercept $\alpha=12$. The equilibrium average capacity is $6.6741$.}
        \label{fig:D12}
    \end{subfigure}
    \caption{In the simulations, the depreciation rate is set to $0.68$, and the discount factor is $0.95$. The figures illustrate the equilibrium capacity distributions for four different demand intercepts: $\alpha = 9,10,11,12$. The equilibrium is computed using Algorithm \ref{alg:value_iteration_mfe}. As expected, production capacities are higher when demand is greater.}
    \label{fig:combined_figures}
\end{figure}

We note that results generated from Algorithm \ref{alg:bisection_q_learning} are similar to the results of Algorithm \ref{alg:value_iteration_mfe}. We omit the details here for brevity.  

\subsubsection{Dynamic Inventory Competition}

We use exactly the same parameters of the model as described in Section \ref{Section:Inventory} but instead of Algorithm \ref{alg:bisection_q_learning} we use the Adaptive Value Function Iteration algorithm (Algorithm \ref{alg:value_iteration_mfe}) to compute the MFE. We use a maximum of 1000 iterations for value function convergence and tolerance \( 10^{-4} \). The computed MFE is very similar to the learned MFE described in Section \ref{Section:Inventory}.  


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{holding_cost_2.png}
        \caption{Holding cost = 2}
        \label{fig:holding_cost_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{holding_cost_5.png}
        \caption{Holding cost = 5}
        \label{fig:holding_cost_5}
    \end{subfigure}
    
    \vspace{0.5cm} % Add spacing between rows
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{holding_cost_8.png}
        \caption{Holding cost = 8}
        \label{fig:holding_cost_8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{holding_cost_12.png}
        \caption{Holding cost = 12}
        \label{fig:holding_cost_12}
    \end{subfigure}
    
    \caption{Equilibrium inventory distributions under different holding costs under Algorithm \ref{alg:value_iteration_mfe} As holding costs increase, retailers' equilibrium inventories are lower.}
    \label{fig:holding_cost_comparative_statics}
\end{figure}


\subsubsection{Dynamic Reputation Model}

In this section present numerical results for the  dynamic reputation model with scalar interactions we described in Section \ref{Sec:reputation}. The state space is defined as \( X = X_{1} \times X_{2} \), where \( X_1 = \{ 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5 \} \) is the set of possible scores of a seller and $X_{2} = \{0, 1, \ldots, 20\}$ is the possible number of reviews. The action space \( A = \{0, 1, 2\} \) corresponds to three discrete investment levels: \( a = 0 \) (no investment), \( a = 1 \) (mild investment), and \( a = 2 \) (high investment).

Payoffs are discounted with a factor \( \beta = 0.95 \) which corresponds to the probability of staying the platform in the next period. We assume that the payoff for a seller in state \( (x_1, x_2) \), taking action \( a \), and facing a scalar interaction \( M \), is given by:
\[
\pi((x_1, x_2), a, M) = \frac{1 + c_1 x_1 + c_2 x_2}{M} -d  a,
\]
where \( c_1 = 3 \) and \( c_2 = 1 \) are coefficients for the ranking and review contributions, respectively and $d$ is the investment cost so $\nu(x_{1},x_{2}) = 1 + c_{1}x_{1} + c_{2}x_{2}$.

We assume that $\zeta_{1}$ takes values in the set $ \{1, 1.5, 2, 2.25, 2.5\}$ with equal probabilities and $k(a)= a$. 
The total number of reviews \( x_2 \) increases by \( 1\), up to a maximum of \( M_2 = 20 \).



The simulation uses the Adaptive Value Function Iteration algorithm to compute the MFE.
The algorithm terminates when the difference between successive bounds on the scalar interaction is below \( 10^{-3} \). Numerical parameters include a maximum of 3000 iterations for value function convergence and a simulation of 500,000 steps to estimate the invariant distribution. 

Plots of the MFE distribution for rankings (\( x_1 \)) are presented to illustrate the steady-state behavior of the system under the computed MFE for different investment costs.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Rep0.1M23.4375.png}
        \caption{Investment cost = 0.1}
        \label{fig:inv_cost_0.1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Rep0.25M22.3242.png}
        \caption{Investment cost = 0.25}
        \label{fig:inv_cost_0.25}
    \end{subfigure}
    
    \vspace{0.5cm} % Add spacing between rows
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Rep0.4M21.0677.png}
        \caption{Investment cost = 0.4}
        \label{fig:inv_cost_0.4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Rep0.55M19.1289.png}
        \caption{Investment cost = 0.55}
        \label{fig:inv_cost_0.55}
    \end{subfigure}
    
    \caption{Equilibrium ranking distributions under different investment costs. As investment costs increase, sellers invest less in improving their products causing equilibrium rankings to be lower.}
\label{fig:investment_cost_comparative_statics}
\end{figure}


We note that results generated from Algorithm \ref{alg:bisection_q_learning} are similar to the results of Algorithm \ref{alg:value_iteration_mfe} with implementation as described in Section \ref{Section:Inventory} on inventory competition. We omit the details here for brevity. 


\subsubsection{Dynamic ridesharing Model}
We simulate the dynamic ridesharing model described in Section \ref{Section:Ridesharing} over a discrete state space \(X = X_1 \times X_2\), where \(X_1 = \{0, 1, 2, 3\}\) represents driver availability or remaining ride duration, and \(X_2 = \{0, 1, 2, 3\}\) represents the type of ride request received. Drivers choose actions \(a \in \{0, 1\}\) to reject or accept ride requests, with payoffs \(u_j \) for requests of type \(j\), and remain unavailable for \(d_j \in \{1, 2, 3\}\) periods after accepting a ride. The probability of receiving no request is modeled as \(f(M) = M\), where \(M \in [0, 1]\) is the fraction of available drivers, and request probabilities for ride types are \(p_{j} = (1 - M)/3\). We set a discount factor of \(0.95\), convergence tolerance of \(10^{-4}\). 

Thus, the duration of each trip can be $1$, $2$, or $3$ periods. We analyze two distinct payoff structures in the simulations, one of which assigns higher rewards to longer trips. This incentivizes drivers to ``cherry-pick" rides, leading to an increase in the equilibrium availability of drivers.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Ride5M0.5.png}
        \caption{Long trip payoff = 5}
        \label{fig:long5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Ride10M0.7498.png}
        \caption{Long trip payoff = 10}
        \label{fig:long10}
    \end{subfigure}

    \caption{
The figures compare the equilibrium behavior of drivers under two different payoff structures for ride requests under Algorithm \ref{alg:value_iteration_mfe}. In both cases, the ride payoffs are \([0, 1, 1.3, r]\), where \(r = 10\) in Figure \ref{fig:long10}  and \(r = 5\) in Figure \ref{fig:long5}. The higher payoff for the long trip (\(r = 10\)) incentivizes drivers to strategically reject requests of type 2 (with payoff \(1.3\)) in anticipation of receiving a more rewarding request in the future even that the equilibrium request probability decreases as there are more available drivers. In contrast, in Figure \ref{fig:long5}, drivers accept all requests in equilibrium. Consequently, the equilibrium availability of drivers is lower in Figure \ref{fig:long5}, which is more efficient from the platform’s perspective, as fewer requests are rejected. 
 }
\label{fig:ridesharing}
\end{figure}

We obtained the same results exactly from using Algorithm \ref{alg:bisection_q_learning} with the implementation discussed in Section \ref{Section:Inventory}. Hence,  Algorithm \ref{alg:bisection_q_learning} was able to learn the MFE in the dynamic ridesharing model. 


\subsection{Adaptive Policy Gradient  for MFE} \label{Sec:Policy}

Step 2 in Algorithm \ref{alg:bisection_q_learning} applies an asynchronous $Q$-learning algorithm to determine the optimal policy under a fixed scalar interaction function. However, as we discussed after Theorem \ref{thm:Q_convergence}, because the scalar interaction function remains fixed in Step 2, the asynchronous $Q$-learning algorithm can be substituted with any other reinforcement learning method that guarantees asymptotic convergence to the optimal policy for a given scalar interaction. Theorem \ref{thm:Q_convergence} would still hold in this case,  ensuring asymptotic  convergence to the MFE.
Alternative methods include other variations of $Q$-learning as well as the popular policy gradient methods, which we now briefly discuss. 

Let $A = P(\mathcal{A})$ where $\mathcal{A}$ is some finite set of actions and suppose for simplicity that $\Gamma(s) = A$ for each $s \in S$. We focus on the case where the policy is updated directly as $\sigma(s,a)$ (direct parametrization), which represents a probability distribution over $S \times \mathcal{A}$. Here, $A = P(\mathcal{A})$, and $\sigma(s,a)$ denotes the probability of taking action $a \in \mathcal{A}$ in state $s \in S$. It is essential to ensure that the updated policy function remains within the space of valid probability measures. To achieve this, we employ the projected policy gradient algorithm instead of the standard policy gradient algorithm, as discussed in \citet{agarwal2021theory}.

Recall that the projection on a closed set $X$ is given by $P_{X}(x)=\operatorname{argmin}_{y \in X} \Vert y-x \Vert ^{2} $. With a slight abuse of notation, we denote $V(\sigma)$ in this section as the expected discounted payoff defined in Equation~(\ref{Equation:V_signma}) for a given policy $\sigma$, and we assume that $V(\sigma)$ is differentiable.
  The projected policy gradient update rule which is expressed as:
\begin{equation} \label{Eq:projectedgradient}
\sigma_{h+1} = P_{\mathcal{P} (S \times \mathcal{A}) } ( \sigma_h + \gamma_h \nabla_\sigma V({\sigma_h}) )
\end{equation}
where $\gamma_h$ denotes the learning rate at iteration $h$. The gradient $\nabla_\sigma V({\sigma_h}) $ can be computed by the policy gradient lemma  \citep{bhandari2024global}. 

Under Assumption \ref{Assumption:Unique} and additional mild technical conditions, it can be shown that the projected policy gradient method is guaranteed to converge to the optimal policy by applying gradient domination techniques \citep{agarwal2021theory,bhandari2024global}. Therefore, we can use Theorem \ref{thm:Q_convergence} to establish that the Adaptive Policy Gradient for MFE (Algorithm \ref{alg:bisection_policy})   produces a sequence that converges asymptotically to an MFE. 




We now formally introduce the Adaptive Policy Gradient algorithm for MFE with scalar interaction. The algorithm follows the structure of Algorithm~\ref{alg:bisection_q_learning}, with the key difference in Step 2, where we replace the tabular Q-learning method with the tabular policy gradient method with direct parameterization (Equation~(\ref{Eq:projectedgradient})).



\begin{algorithm} [H]
\caption{Adaptive Policy Gradient for MFE with Scalar Interaction}
\label{alg:bisection_policy}

\textbf{Input:} Samples $K,H$,  tolerance level \(\delta > 0\). 

\vspace{2mm}
Repeat until \(|\hat{f}(m_t)| \leq \delta\):
\begin{enumerate}[leftmargin=1em, itemsep=0.1ex]
    \item \textbf{Set:} \(m_t = \frac{a + b}{2}\).
    \item \textbf{Policy Gradient:} 
   Apply projected policy gradient method (see Equation \ref{Eq:projectedgradient}) under \( m_t \) for $h=0,1,\ldots,H$, yielding  policy $\hat{g}_t(x,m_{t})$. 
    \item \textbf{Sampling:} Simulate \(K\) transitions using \(\hat{g}_t\):
    $
    x_{k+1} \sim L_{m_t, \hat{g}_t}(x_k, m_t, \cdot), \quad \text{for } k = 0, 1, \dots, K.
    $
    Compute the approximate invariant distribution:
    \[
    \hat{s}^{m_t, \hat{g}_t}(y) = \frac{1}{K} \sum_{k=0}^{K} \mathbf{1}_{\{x_k = y\}}, \quad \forall y \in X.
    \]
    \item \textbf{Evaluate and update:} Compute \(\hat{f}(m_t) = m_t - M(\hat{s}^{m_t, \hat{g}_t})\). Update: $$ b = m_t \text{ if  } \hat{f} (m_t) > \delta \text{; }  a = m_t  \text{ if } \hat{f} (m_t) < -\delta. $$ 
    \end{enumerate}


\end{algorithm}



\subsection{Unknown Payoffs and Transitions with Large State Space} \label{Sec:LargeState}

In cases where the state space is very large, Algorithm \ref{alg:bisection_q_learning} becomes impractical, as directly applying $Q$-learning becomes computationally infeasible. However, using reinforcement learning techniques suitable for large state spaces can help adjusting Algorithm \ref{alg:bisection_q_learning} to find an MFE in such settings.

One approach to manage the large state space is state aggregation, a dimensionality reduction method that groups ``similar" states into clusters or aggregates. Each cluster is treated as a single state in the aggregated model, significantly reducing the state space. By decreasing the number of effective states, aggregation makes it feasible to solve large dynamic programming problems, making step 2 in Algorithm \ref{alg:bisection_q_learning} manageable. The usefulness of state  aggregation depends on the specific dynamic optimization problem; in some applications, states may be too distinct, and aggregation can lead to highly suboptimal policies (see \cite{bertsekas2019reinforcement} for a detailed discussion). 

Alternatively, $Q$-learning or policy gradient methods with function approximation can be used. For example, consider $Q$-learning with function approximation, where we aim to approximate the optimal Q-values using a parameterized function \( Q(s, a; \theta) \). The parameters \( \theta \) are adjusted iteratively to minimize the error between predicted and target Q-values.

The learning objective is typically to minimize the squared error between the predicted Q-value and the target value:
\[ \min_\theta \; \mathbb{E} \left[ \left( y_t - Q(s_t, a_t; \theta) \right)^2 \right], \] 
where \( y_t \) is the target Q-value at time \( t \), defined as:
\[ y_t = \pi_{t} + \beta \max_{a'} Q(s_{t+1}, a'; \theta^-), \]
with \( \theta^- \) denotes the parameters of the Q-function used to compute the target value. To stabilize learning, \( \theta^- \) is updated less frequently than \( \theta \), for example by setting \( \theta^{-} = \theta \) every \( N \) iterations.

The parameters \( \theta \) can be updated using gradient descent. The gradient of the objective with respect to \( \theta \) is:
\[ \nabla_\theta \left( \left( y_t - Q(s_t, a_t; \theta) \right)^2 \right) = -2 \left( y_t - Q(s_t, a_t; \theta) \right) \nabla_\theta Q(s_t, a_t; \theta), \]
leading to the following update rule:
\begin{equation*} \label{eq:updating_theta}
     \theta_{t+1} = \theta_t + \gamma_t \left( y_t - Q(s_t, a_t; \theta_t) \right) \nabla_\theta Q(s_t, a_t; \theta_t), 
\end{equation*}
where \( \gamma_t \) is the learning rate at time \( t \).

Two common forms of function approximation where  $\nabla_\theta Q(s_t, a_t; \theta)$ can be computed efficiently are linear approximation and neural networks. By leveraging function approximation, we can replace Step 2 in Algorithm~\ref{alg:bisection_q_learning} with $Q$-learning using function approximation, which is well-suited for the large state spaces typically encountered in realistically sized applications. Any theoretical guarantees on the policy derived from $Q$-learning with function approximation can be extended to guarantees on the MFE with scalar interaction through Theorem~\ref{thm:Q_convergence} and Theorem~\ref{Thm:finite_bounds}.



\subsection{Adaptive Algorithms for MFE with Multi-Valued Interaction Function} \label{sec:Multi}

In this section, we discuss how we can extend our framework to the case where the interaction function \(M\) is multi-valued, mapping the population state to a compact subset in \(\mathbb{R}^n\). Specifically, we assume that \(M: \mathcal{P}(X) \to \mathcal{M} \subset \mathbb{R}^n\), where \(\mathcal{M}\) is a compact set. This generalization captures systems where the interaction between agents depends on multiple metrics simultaneously (e.g., expected value and variance of the population distribution), as opposed to a single scalar quantity.

Under this setup, from the arguments as in the proof of Theorem \ref{thm:Q_convergence}, if $f(m^{*} ) = m^* -  M(s^{m^{*},g^{*} } ) =0$ for some $m^{*} \in \mathcal{M}$ then \(s^{m^{*} } \in \mathcal{P}(X)\) is the MFE population state  and \(g^*(\cdot, m^*)\) is the MFE optimal policy. 

Thus, to compute \(m^*\), we aim to find a root of $f$. Unlike the scalar case, where a bisection method can guarantee convergence to the root of \(f\), the multi-dimensional setting requires a numerical optimization techniques without general convergence guarantees. A natural choice is to adopt Broyden's method \cite{broyden1965class}, a quasi-Newton approach, to iteratively approximate the solution.

Broyden’s method updates the solution \(m\) and an approximation of the Jacobian of \(f\), denoted by \(B\), at each iteration. Starting with an initial guess \(m_0 \in \mathcal{M}\) and \(B_0\) (typically the identity matrix), the method proceeds by iteratively refining \(m\) based on the residuals \(f(m)\). This approach does not require explicit computation of the Jacobian which would be generally impractical in our setting.  


We now propose an algorithm for the multi-valued case that follows a similar structure to our earlier framework. At each iteration, we first use Q-learning to determine the approximate optimal policy given the current value of \(m\). Using this policy, we simulate the population dynamics to approximate the invariant distribution generated by that policy. With the invariant distribution and the interaction function \(M\), we compute the function \(f(m)\). The next iterate of \(m\) is then obtained via Broyden’s update rule, which adjusts both the solution \(m\) and the Jacobian approximation \(B\). This process continues until the norm \(\|f(m)\|\) falls below a predefined threshold.

We note that global convergence to the root of \(f(m)\) is not generally guaranteed. However, in practice, it can approximate the MFE by finding a point close to the equilibrium, particularly when combined with multiple initial guesses for \(m_0\). Moreover, this approach remains compatible with function approximation techniques for Q-learning discussed in Section \ref{Sec:LargeState}, making it scalable to large state spaces. In addition, the proof of Theorem \ref{Thm:finite_bounds1} holds for the multi-valued case when we use some norm in Assumption \ref{Assumption:Lip} and Theorem \ref{Thm:finite_bounds1} instead of absolute value providing finite-time bounds for the algorithm under those Lipschitz continuity assumptions.


Recall that the projection on a closed set $X$ is given by $P_{X}(x)$.  


\begin{algorithm}[H]
\caption{Adaptive Q-Learning for MFE with Multi-Valued Interaction Function}
\label{alg:broyden_jacobian}

\textbf{Input:} Samples $K,H$,  tolerance level \(\delta > 0\). Initial \( m_0 \in \mathbb{R}^n \), \( B_0 = I \). Compute
$m_1 = P_{\mathcal{M}} (m_0 - B_0^{-1} \hat{f}(m_0) )$
where \( \hat{f}(m_0)  \), is computed as in Steps 1, 2, 3 below. 

\vspace{1.5mm}
Repeat until \( \| \hat{f}(m_t) \| < \delta \):

\begin{enumerate}[leftmargin=1em, itemsep=0.1ex]
    \item \textbf{Q-learning:} Apply Q-learning under \(m_t\) with \(H\) iterations to compute \(\hat{Q}_{H,m_{t}}\) (see Equation \ref{Eq:Q}). Derive the policy
    $
   \hat{g}_t(x, m_t) = \argmax _{a \in A } \hat{Q}_{H,m_{t}}(x, a, m_t).
    $
    \item \textbf{Sampling:} Simulate \(K\) transitions using \(\hat{g}_t\):
    $
    x_{k+1} \sim L_{m_t, \hat{g}_t}(x_k, m_t, \cdot), \quad \text{for } k = 0, 1, \dots, K.
    $
    Compute the approximate invariant distribution:
    \[
    \hat{s}^{m_t, \hat{g}_t}(y) = \frac{1}{K} \sum_{k=0}^{K} \mathbf{1}_{\{x_k = y\}}, \quad \forall y \in X.
    \]
   
 
\item \textbf{Update Jacobian:} Compute
        $
        \hat{f}(m_t) = m_t - M(\hat{s}^{m_t, \hat{g}_t}) 
        $.
    Let  $\Delta f_{t} = \hat{f}(m_{t}) - \hat{f}(m_{t-1}), \text{ and }\theta_{t} = m_{t} - m_{t-1}.$ Update  \( B_t \):
        \[
        B_{t} = B_{t-1} + \frac{(\Delta f_{t} - B_{t-1} \theta_{t}) \theta_{t}^\top}{\theta_{t}^\top \theta_{t}},
        \]
\item \textbf {Update Multi-Valued Interaction:} Set
        \[
        m_{t+1} = P_{\mathcal{M}} (m_t - B_t^{-1} \hat{f}(m_t)). 
        \]
\end{enumerate}
\end{algorithm}






\end{document}