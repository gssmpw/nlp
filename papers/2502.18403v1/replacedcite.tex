\section{Related Work}
\label{sec:relwork}

\myparagraph{DL Operator Mapping} Pipeline design for Kitsune is related to the problem of ``operator mapping''. This has largely been looked at in the context of spatially exposed hardware for \textit{single operators} including works such as TimeLoop____, MAESTRO____, AMOS____, and CoSA____, which treat an operator as a transformable loop-nest, and TVM____ which lowers semantics expressed with einsums to low-level code. 

\myparagraph{DL Operator Fusion} Traditional GPU kernel fusion focuses on fusing memory-intensive kernels together____, and modern DL compilers often support simple operator fusion at the register level____ or for improving data reuse for identical and related operators____.
Building on single-operator mapping, many recent academic works address vertical fusion including ALCOP____, Apollo____, AStitch____,   Chimera____, Deepcuts____, GraphTurbo____, and Welder____. We discuss the capability of AStitch, Welder, and state of art vertical fusion in Section~\ref{sec:behavior}. AStitch, Welder and GraphTurbo all use some notion of an anchor-and-propa-gate scheme to handle streaming compatibility between fused layers. Kitsune is more composable and general than all of these, being able to fuse many more operators into co-resident GPU kernels. Other drawbacks and limitations of vertical fusion have been discussed at length in Section~\ref{sec:behavior}.


\begin{figure}[t]
    \centering
    \hspace{0.5in}
    \includegraphics[width=0.4\columnwidth]{figs/e2e-legend.pdf} \\
    \includegraphics[width=0.5\columnwidth]{figs/train-perf.pdf} \\
    \vspace{0.1in}
    \includegraphics[width=0.5\columnwidth]{figs/tfwd-perf.pdf} \\
    \vspace{0.1in}
    \includegraphics[width=0.5\columnwidth]{figs/tbwd-perf.pdf} \\
    \vspace{0.1in}
    \includegraphics[width=0.5\columnwidth]{figs/train-tl.pdf} \\
    \vspace{-0.1in}
    \caption{Training End-to-end Speedup over Bulk-Sync.}
    \label{fig:train-perf-tl}
    \vspace{-0.2in}
\end{figure}   

\myparagraph{GPU Multitasking}
HFuse____ presents a methodology for horizontal fusion which can leverage overlap of heterogeneous work but is restricted to only fusing pairs of nodes with no data dependencies. Works such as ISPA____ and SMK____ provide a pure software, and hardware-codesign solutions (respectively) for achieving fine-grained multitasking on GPUs. SMK uses hardware mechanisms to enable preemption of CTAs on the SM for ``partial context switching'' -- the goal of which is achieve higher overall utilization of SM resources with heterogeneous CTAs. ISPA uses a pure software approach for co-scheduling pairs of Tensor-heavy and SIMT-heavy kernels. It uses several software techniques to promote efficiency of co-occupancy, but ultimately relies on the existing GPU thread scheduler to make CTA placement decisions. All these approaches focus on co-scheduling just two kernels with no data dependence. Kitsune enables any number of kernels to co-execute in spatial pipelines with data-dependencies supported by our queues and relying on a modified CTA scheduler to make smart decisions about placement of CTAs to best utilize SM resources.


\myparagraph{Data-Triggered Execution} \rev{WorkGraphs____ is a recent development in the graphics space to afford data triggered execution on GPUs. However, it does not address on-chip data-orchestration to maintain cache residency of intermediates. Additionally, it operates on a level of granularity much smaller than Kitsune, using individual records and shader invocations as the unit of work. Kitsune in contrast is designed to orchestrate producer-consumer communication on-chip at a granularity of tensor tiles of around 64KB payloads. Finally, WorkGraphs doesn't support join operations with different input record types, vastly reducing the generality and applicability beyond shader pipelines.}

% for reducing resource usage