@misc{gpu-background,
title={GPU Performance Background User's Guide.\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html}},
author={NVIDIA}
}

@inproceedings{elgamal2017spoof,
  title={SPOOF: Sum-Product Optimization and Operator Fusion for Large-Scale Machine Learning.},
  author={Elgamal, Tarek and Luo, Shangyu and Boehm, Matthias and Evfimievski, Alexandre V and Tatikonda, Shirish and Reinwald, Berthold and Sen, Prithviraj},
  booktitle={CIDR},
  year={2017}
}

@inproceedings{lattner2021mlir,
  title={MLIR: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

@article{li2020deep,
  title={The deep learning compiler: A comprehensive survey},
  author={Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={3},
  pages={708--727},
  year={2020},
  publisher={IEEE}
}

@article{10.14778/3229863.3229865,
author = {Boehm, Matthias and Reinwald, Berthold and Hutchison, Dylan and Sen, Prithviraj and Evfimievski, Alexandre V. and Pansare, Niketan},
title = {On Optimizing Operator Fusion Plans for Large-Scale Machine Learning in SystemML},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229865},
doi = {10.14778/3229863.3229865},
abstract = {Many machine learning (ML) systems allow the specification of ML algorithms by means of linear algebra programs, and automatically generate efficient execution plans. The opportunities for fused operators---in terms of fused chains of basic operators---are ubiquitous, and include fewer materialized intermediates, fewer scans of inputs, and sparsity exploitation across operators. However, existing fusion heuristics struggle to find good plans for complex operator DAGs or hybrid plans of local and distributed operations. In this paper, we introduce an exact yet practical cost-based optimization framework for fusion plans and describe its end-to-end integration into Apache SystemML. We present techniques for candidate exploration and selection of fusion plans, as well as code generation of local and distributed operations over dense, sparse, and compressed data. Our experiments in SystemML show end-to-end performance improvements of up to 22x, with negligible compilation overhead.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1755–1768},
numpages = {14}
}

@article{kraus2009pull,
  title={The pull-push algorithm revisited},
  author={Kraus, Martin},
  journal={Proceedings GRAPP},
  volume={2},
  pages={3},
  year={2009}
}

@article{gwennap_groq_2020,
	title = {{GROQ} {ROCKS} {NEURAL} {NETWORKS}},
	language = {en},
	author = {Gwennap, Linley},
	month = jan,
	year = {2020},
	keywords = {accelerator, ai, Convolutional codes, deep-learning, mpr},
	pages = {5},
	file = {Gwennap - 2020 - GROQ ROCKS NEURAL NETWORKS.pdf:Zotero\\storage\\QSU8VHNP\\Gwennap - 2020 - GROQ ROCKS NEURAL NETWORKS.pdf:application/pdf},
}

@inproceedings{shao_simba_2019,
	address = {Columbus OH USA},
	title = {Simba: {Scaling} {Deep}-{Learning} {Inference} with {Multi}-{Chip}-{Module}-{Based} {Architecture}},
	isbn = {978-1-4503-6938-1},
	shorttitle = {Simba},
	url = {https://dl.acm.org/doi/10.1145/3352460.3358302},
	doi = {10.1145/3352460.3358302},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 52nd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {ACM},
	author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
	month = oct,
	year = {2019},
	pages = {14--27},
	file = {Shao et al. - 2019 - Simba Scaling Deep-Learning Inference with Multi-.pdf:C\:\\Users\\micha\\Zotero\\storage\\793TQK4Y\\Shao et al. - 2019 - Simba Scaling Deep-Learning Inference with Multi-.pdf:application/pdf},
}
@misc{llama3,
      title={The Llama 3 Herd of Models}, 
      author={Grattafiori et. al},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@inproceedings{qin_sigma_2020,
	address = {San Diego, CA, USA},
	title = {{SIGMA}: {A} {Sparse} and {Irregular} {GEMM} {Accelerator} with {Flexible} {Interconnects} for {DNN} {Training}},
	isbn = {978-1-72816-149-5},
	shorttitle = {{SIGMA}},
	url = {https://ieeexplore.ieee.org/document/9065523/},
	doi = {10.1109/HPCA47549.2020.00015},
	abstract = {The advent of Deep Learning (DL) has radically transformed the computing industry across the entire spectrum from algorithms to circuits. As myriad application domains embrace DL, it has become synonymous with a genre of workloads across vision, speech, language, recommendations, robotics, and games. The key compute kernel within most DL workloads is general matrix-matrix multiplications (GEMMs), which appears frequently during both the forward pass (inference and training) and backward pass (training). GEMMs are a natural choice for hardware acceleration to speed up training, and have led to 2D systolic architectures like NVIDIA tensor cores and Google Tensor Processing Unit (TPU).},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {2020 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},
	month = feb,
	year = {2020},
	pages = {58--70},
	file = {Qin et al. - 2020 - SIGMA A Sparse and Irregular GEMM Accelerator wit.pdf:C\:\\Users\\micha\\Zotero\\storage\\I7NSM75N\\Qin et al. - 2020 - SIGMA A Sparse and Irregular GEMM Accelerator wit.pdf:application/pdf},
}

@INPROCEEDINGS{domke_matrix_2021,
  author={Domke, Jens and Vatai, Emil and Drozd, Aleksandr and ChenT, Peng and Oyama, Yosuke and Zhang, Lingqi and Salaria, Shweta and Mukunoki, Daichi and Podobas, Artur and WahibT, Mohamed and Matsuoka, Satoshi},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  title={Matrix Engines for High Performance Computing: A Paragon of Performance or Grasping at Straws?},
  year={2021},
  volume={},
  number={},
  pages={1056-1065},
  doi={10.1109/IPDPS49936.2021.00114}}

@INPROCEEDINGS {dojo,
author = {E. Talpes and D. Williams and D. Sarma},
booktitle = {2022 IEEE Hot Chips 34 Symposium (HCS)},
title = {DOJO: The Microarchitecture of Tesla’s Exa-Scale Computer},
year = {2022},
volume = {},
issn = {},
pages = {1-28},
abstract = {Tesla’s in-house supercomputer for Machine Learning},
keywords = {microarchitecture;machine learning;supercomputers},
doi = {10.1109/HCS55958.2022.9895534},
url = {https://doi.ieeecomputersociety.org/10.1109/HCS55958.2022.9895534},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@inproceedings{venkatesan_magnet_2019,
	address = {Westminster, CO, USA},
	title = {{MAGNet}: {A} {Modular} {Accelerator} {Generator} for {Neural} {Networks}},
	isbn = {978-1-72812-350-9},
	shorttitle = {{MAGNet}},
	url = {https://ieeexplore.ieee.org/document/8942127/},
	doi = {10.1109/ICCAD45719.2019.8942127},
	language = {en},
	urldate = {2021-08-07},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design} ({ICCAD})},
	publisher = {IEEE},
	author = {Venkatesan, Rangharajan and Raina, Priyanka and Zhang, Yanqing and Zimmer, Brian and Dally, William J. and Emer, Joel and Keckler, Stephen W. and Khailany, Brucek and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel},
	month = nov,
	year = {2019},
	pages = {1--8},
	file = {Venkatesan et al. - 2019 - MAGNet A Modular Accelerator Generator for Neural.pdf:Zotero\\storage\\RHZZ3ZE4\\Venkatesan et al. - 2019 - MAGNet A Modular Accelerator Generator for Neural.pdf:application/pdf},
}

@INPROCEEDINGS{streamdataflow,
  author={Nowatzki, Tony and Gangadhar, Vinay and Ardalani, Newsha and Sankaralingam, Karthikeyan},
  booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Stream-dataflow acceleration}, 
  year={2017},
  volume={},
  number={},
  pages={416-429},
  doi={10.1145/3079856.3080255}}

@article{pfaff2020learning,
  title={Learning mesh-based simulation with graph networks},
  author={Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W},
  journal={arXiv preprint arXiv:2010.03409},
  year={2020}
}

@article{chen2020slide,
  title={Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems},
  author={Chen, Beidi and Medini, Tharun and Farwell, James and Tai, Charlie and Shrivastava, Anshumali and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={291--306},
  year={2020}
}

@inproceedings{zhao2023effectively,
  title={Effectively Scheduling Computational Graphs of Deep Neural Networks toward Their $\{$Domain-Specific$\}$ Accelerators},
  author={Zhao, Jie and Feng, Siyuan and Dan, Xiaoqiang and Liu, Fei and Wang, Chengke and Yuan, Sheng and Lv, Wenyuan and Xie, Qikai},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={719--737},
  year={2023}
}

@article{huang2023alcop,
  title={ALCOP: Automatic Load-Compute Pipelining in Deep Learning Compiler for AI-GPUs},
  author={Huang, Guyue and Bai, Yang and Liu, Liu and Wang, Yuke and Yu, Bei and Ding, Yufei and Xie, Yuan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{zheng2023chimera,
  title={Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion},
  author={Zheng, Size and Chen, Siyuan and Song, Peidi and Chen, Renze and Li, Xiuhong and Yan, Shengen and Lin, Dahua and Leng, Jingwen and Liang, Yun},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1113--1126},
  year={2023},
  organization={IEEE}
}

@article{zhao2022ispa,
  title={ISPA: Exploiting Intra-SM Parallelism in GPUs via Fine-Grained Resource Management},
  author={Zhao, Han and Cui, Weihao and Chen, Quan and Guo, Minyi},
  journal={IEEE Transactions on Computers},
  volume={72},
  number={5},
  pages={1473--1487},
  year={2022},
  publisher={IEEE}
}

@inproceedings{wang2016simultaneous,
  title={Simultaneous multikernel GPU: Multi-tasking throughput processors via fine-grained sharing},
  author={Wang, Zhenning and Yang, Jun and Melhem, Rami and Childers, Bruce and Zhang, Youtao and Guo, Minyi},
  booktitle={2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={358--369},
  year={2016},
  organization={IEEE}
}

@inproceedings{luitjens2015cuda,
  title={Cuda streams: Best practices and common pitfalls},
  author={Luitjens, Justin},
  booktitle={GPU Techonology Conference},
  year={2015}
}

@article{chen2019eyeriss,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@inproceedings{shi2023welder,
  title={Welder: Scheduling Deep Learning Memory Access via Tile-graph},
  author={Shi, Yining and Yang, Zhi and Xue, Jilong and Ma, Lingxiao and Xia, Yuqing and Miao, Ziming and Guo, Yuxiao and Yang, Fan and Zhou, Lidong},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={701--718},
  year={2023}
}

@inproceedings{zheng2022astitch,
  title={AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures},
  author={Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and others},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={359--373},
  year={2022}
}

@inproceedings{tensorrt,
  title={Efficient inference with tensorrt},
  author={Vanholder, Han},
  booktitle={GPU Technology Conference},
  volume={1},
  number={2},
  year={2016}
}

@article{gwennap_google_2020,
	title = {{GOOGLE} {DETAILS} {TPUV3} {ARCHITECTURE}},
	language = {en},
	author = {Gwennap, Linley},
	month = aug,
	year = {2020},
	keywords = {accelerator, ai, Convolutional codes, deep-learning, mpr},
	pages = {6},
	file = {Gwennap - 2020 - GOOGLE DETAILS TPUV3 ARCHITECTURE.pdf:Zotero\\storage\\JDQDUBD4\\Gwennap - 2020 - GOOGLE DETAILS TPUV3 ARCHITECTURE.pdf:application/pdf},
}

@misc{cdna2,
    author={AMD},
    title={AMD CDNA 2 Architecture},
    url={https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf}
}

@misc{pytorch,
    author={Meta},
    title={PyTorch},
    url={https://pytorch.org/}
}


@misc{tf,
    author={Google},
    title={TensorFlow},
    url={https://www.tensorflow.org/}
}

@misc{workgraphs,
    author={Microsoft},
    title={D3D12 Work Graphs},
    url={https://devblogs.microsoft.com/directx/d3d12-work-graphs/},
}


@inproceedings{biren,
  title={BR100 GPGPU: Accelerating Datacenter Scale AI Computing},
  author={Hong, Mike and Xu, Lingjie},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)},
  pages={1--22},
  year={2022},
  organization={IEEE Computer Society}
}

@inproceedings{10.1145/3123939.3124545,
author = {O'Connor, Mike and Chatterjee, Niladrish and Lee, Donghyuk and Wilson, John and Agrawal, Aditya and Keckler, Stephen W. and Dally, William J.},
title = {Fine-Grained DRAM: Energy-Efficient DRAM for Extreme Bandwidth Systems},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/3123939.3124545},
doi = {10.1145/3123939.3124545},
abstract = {Future GPUs and other high-performance throughput processors will require multiple TB/s of bandwidth to DRAM. Satisfying this bandwidth demand within an acceptable energy budget is a challenge in these extreme bandwidth memory systems. We propose a new high-bandwidth DRAM architecture, Fine-Grained DRAM (FGDRAM), which improves bandwidth by 4\texttimes{} and improves the energy efficiency of DRAM by 2\texttimes{} relative to the highest-bandwidth, most energy-efficient contemporary DRAM, High Bandwidth Memory (HBM2). These benefits are in large measure achieved by partitioning the DRAM die into many independent units, called grains, each of which has a local, adjacent I/O. This approach unlocks the bandwidth of all the banks in the DRAM to be used simultaneously, eliminating shared buses interconnecting various banks. Furthermore, the on-DRAM data movement energy is significantly reduced due to the much shorter wiring distance between the cell array and the local I/O. This FGDRAM architecture readily lends itself to leveraging existing techniques to reducing the effective DRAM row size in an area efficient manner, reducing wasteful row activate energy in applications with low locality. In addition, when FGDRAM is paired with a memory controller optimized to exploit the additional concurrency provided by the independent grains, it improves GPU system performance by 19\% over an iso-bandwidth and iso-capacity future HBM baseline. Thus, this energy-efficient, high-bandwidth FGDRAM architecture addresses the needs of future extreme-bandwidth memory systems.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {41–54},
numpages = {14},
keywords = {GPU, high bandwidth, energy-efficiency, DRAM},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@INPROCEEDINGS{9407116,
  author={Kwon, Hyoukjun and Lai, Liangzhen and Pellauer, Michael and Krishna, Tushar and Chen, Yu-Hsin and Chandra, Vikas},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={Heterogeneous Dataflow Accelerators for Multi-DNN Workloads}, 
  year={2021},
  volume={},
  number={},
  pages={71-83},
  doi={10.1109/HPCA51647.2021.00016}}


@misc{tensorflowXLA,
title={{TensorFlow} {XLA}. \url{https://www.tensorflow.org/xla}},
author={TensorFlow}}

@misc{tensorflowGrappler,
author={TensorFlow},
title={{TensorFlow} {Grappler}. \url{https://www.tensorflow.org/guide/graph_optimization}}
      }


@InProceedings{RStreamTF,
author={Pradelle, Beno{\^i}t and Meister, Beno{\^i}t and Baskaran, Muthu and Springer, Jonathan and Lethin, Richard},
title={Polyhedral Optimization of TensorFlow Computation Graphs},
booktitle={Programming and Performance Visualization Tools},
year={2019},
publisher={Springer International Publishing},
pages="74--89",
isbn="978-3-030-17872-7"
}

@misc{vasilache2018tensorcomprehensions,
      title={Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions}, 
      author={Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zachary DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen},
      year={2018},
      eprint={1802.04730},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{fegade2021cortex,
      title={Cortex: A Compiler for Recursive Deep Learning Models}, 
      author={Pratik Fegade and Tianqi Chen and Phillip B. Gibbons and Todd C. Mowry},
      year={2021},
      eprint={2011.01383},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ikra:2017,
author = {Springer, Matthias and Wauligmann, Peter and Masuhara, Hidehiko},
title = {Modular Array-Based GPU Computing in a Dynamically-Typed Language},
year = {2017},
isbn = {9781450350693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3091966.3091974},
doi = {10.1145/3091966.3091974},
abstract = {Nowadays, GPU accelerators are widely used in areas with large data-parallel computations such as scientific computations or neural networks. Programmers can either write code in low-level CUDA/OpenCL code or use a GPU extension for a high-level programming language for better productivity. Most extensions focus on statically-typed languages, but many programmers prefer dynamically-typed languages due to their simplicity and flexibility. This paper shows how programmers can write high-level modular code in Ikra, a Ruby extension for array-based GPU computing. Programmers can compose GPU programs of multiple reusable parallel sections, which are subsequently fused into a small number of GPU kernels. We propose a seamless syntax for separating code regions that extensively use dynamic language features from those that are compiled for efficient execution. Moreover, we propose symbolic execution and a program analysis for kernel fusion to achieve performance that is close to hand-written CUDA code.},
booktitle = {Proceedings of the 4th ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming},
pages = {48–55},
numpages = {8},
keywords = {kernel fusion, GPGPU, Ruby, CUDA},
location = {Barcelona, Spain},
series = {ARRAY 2017}
}

@INPROCEEDINGS{horizontal-fusion:cgo2022,
  author={Li, Ao and Zheng, Bojian and Pekhimenko, Gennady and Long, Fan},
  booktitle={2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Automatic Horizontal Fusion for GPU Kernels}, 
  year={2022},
  volume={},
  number={},
  pages={14-27},
  doi={10.1109/CGO53902.2022.9741270}}


@INPROCEEDINGS{kernel-fusion-one-step-methods:sbac-pad,
  author={Korch, Matthias and Werner, Tim},
  booktitle={2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)}, 
  title={Exploiting Limited Access Distance for Kernel Fusion Across the Stages of Explicit One-Step Methods on GPUs}, 
  year={2018},
  volume={},
  number={},
  pages={148-157},
  doi={10.1109/CAHPC.2018.8645892}}


@ARTICLE{spatio-temporal-multitasking:TPDS,
  author={Liang, Yun and Huynh, Huynh Phung and Rupnow, Kyle and Goh, Rick Siow Mong and Chen, Deming},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Efficient GPU Spatial-Temporal Multitasking}, 
  year={2015},
  volume={26},
  number={3},
  pages={748-760},
  doi={10.1109/TPDS.2014.2313342}}


@inproceedings{RAMMER:OSDI2020,
author = {Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
title = {RAMMER: Enabling Holistic Deep Learning Compiler Optimizations with Rtasks},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {Performing Deep Neural Network (DNN) computation on hardware accelerators efficiently is challenging. Existing DNN frameworks and compilers often treat the DNN operators in a data flow graph (DFG) as opaque library functions and schedule them onto accelerators to be executed individually. They rely on another layer of scheduler, often implemented in hardware, to exploit the parallelism available in the operators. Such a two-layered approach incurs significant scheduling overhead and often cannot fully utilize the available hardware resources. In this paper, we propose RAMMER, a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. RAMMER generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter- and intra- operator co-scheduling. RAMMER achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to RAMMER, which employs several heuristics to explore this space and finds efficient schedules. We implement RAMMER for multiple hardware backends such as NVIDIA GPUs, AMD GPUs, and Graphcore IPU. Experiments show RAMMER significantly outperforms state-of-the-art compilers such as TensorFlow XLA and TVM by up to 20.1\texttimes{}. It also outperforms TensorRT, a vendor optimized proprietary DNN inference library from NVIDIA, by up to 3.1\texttimes{}},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {50},
numpages = {17},
series = {OSDI'20}
}

  
@inproceedings{dnnfusion:pldi2021,
author = {Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
title = {DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454083},
doi = {10.1145/3453483.3454083},
abstract = {Deep Neural Networks (DNNs) have emerged as the core enabler of many major applications on mobile devices. To achieve high accuracy, DNN models have become increasingly deep with hundreds or even thousands of operator layers, leading to high memory and computational requirements for inference. Operator fusion (or kernel/layer fusion) is key optimization in many state-of-the-art DNN execution frameworks, such as TensorFlow, TVM, and MNN, that aim to improve the efficiency of the DNN inference. However, these frameworks usually adopt fusion approaches based on certain patterns that are too restrictive to cover the diversity of operators and layer connections, especially those seen in many extremely deep models. Polyhedral-based loop fusion techniques, on the other hand, work on a low-level view of the computation without operator-level information, and can also miss potential fusion opportunities. To address this challenge, this paper proposes a novel and extensive loop fusion framework called DNNFusion. The basic idea of this work is to work at an operator view of DNNs, but expand fusion opportunities by developing a classification of both individual operators and their combinations. In addition, DNNFusion includes 1) a novel mathematical-property-based graph rewriting framework to reduce evaluation costs and facilitate subsequent operator fusion, 2) an integrated fusion plan generation that leverages the high-level analysis and accurate light-weight profiling, and 3) additional optimizations during fusion code generation. DNNFusion is extensively evaluated on 15 DNN models with varied types of tasks, model sizes, and layer counts. The evaluation results demonstrate that DNNFusion finds up to 8.8 \texttimes{} higher fusion opportunities, outperforms four state-of-the-art DNN execution frameworks with 9.3\texttimes{} speedup. The memory requirement reduction and speedups can enable the execution of many of the target models on mobile devices and even make them part of a real-time application.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {883–898},
numpages = {16},
keywords = {Deep Neural Network, Mobile Devices, Operator Fusion, Compiler Optimization},
location = {Virtual, Canada},
series = {PLDI 2021}
}

  
@inproceedings{optimal-kernel-fusion:sc2014,
author = {Wahib, Mohamed and Maruyama, Naoya},
title = {Scalable Kernel Fusion for Memory-Bound GPU Applications},
year = {2014},
isbn = {9781479955008},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2014.21},
doi = {10.1109/SC.2014.21},
abstract = {GPU implementations of HPC applications relying on finite difference methods can include tens of kernels that are memory-bound. Kernel fusion can improve performance by reducing data traffic to off-chip memory; kernels that share data arrays are fused to larger kernels where on-chip cache is used to hold the data reused by instructions originating from different kernels. The main challenges are a) searching for the optimal kernel fusions while constrained by data dependencies and kernels' precedences and b) effectively applying kernel fusion to achieve speedup. This paper introduces a problem definition and proposes a scalable method for searching the space of possible kernel fusions to identify optimal kernel fusions for large problems. The paper also proposes a codeless performance upper-bound projection model to achieve effective fusions. Results show that using the proposed scalable method for kernel fusion improved the performance of two real-world applications containing tens of kernels by 1.35x and 1.2x.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {191–202},
numpages = {12},
location = {New Orleans, Louisana},
series = {SC '14}
}

@ARTICLE{kernel-fusion-map-reduce:jsc2015,
  author={Jiří Filipovič and Matúš Madzin and Jan Fousek and Luděk Matyska},
  journal={Journal of Supercomputing},
  title={Optimizing CUDA code by kernel fusion: application on BLAS},
  year={2015}}
  
@inproceedings{TASO:sosp2019,
author = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
title = {TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359630},
doi = {10.1145/3341301.3359630},
abstract = {Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis.We propose TASO, the first DNN computation graph optimizer that automatically generates graph substitutions. TASO takes as input a list of operator specifications and generates candidate substitutions using the given operators as basic building blocks. All generated substitutions are formally verified against the operator specifications using an automated theorem prover. To optimize a given DNN computation graph, TASO performs a cost-based backtracking search, applying the substitutions to find an optimized graph, which can be directly used by existing DNN frameworks.Our evaluation on five real-world DNN architectures shows that TASO outperforms existing DNN frameworks by up to 2.8X, while requiring significantly less human effort. For example, TensorFlow currently contains approximately 53,000 lines of manual optimization rules, while the operator specifications needed by TASO are only 1,400 lines of code.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {47–62},
numpages = {16},
keywords = {deep neural network, formal verification, computation graph substitutions, superoptimization},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

  

@misc{nvidia-spec-blog,
title={NVIDIA Ampere Architecture In-Depth},
author={Ronny Krashinsky and Olivier Giroux and Stephen Jones and Nick Stam and Sridhar Ramaswamy. \url{https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth}}}


@misc{a100-whitepaper,
url={https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf},
title={NVIDIA A100 Tensor Core GPU Architecture},
author={NVIDIA}
}

@misc{v100-whitepaper,
url={https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf},
title={NVIDIA Tesla V100 GPU Architecture},
author={NVIDIA}
}

@misc{ampere-whitepaper,
	title = {{NVIDIA} {AMPERE} {GA102} {GPU} {ARCHITECTURE}. \url{https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf}},
	author = {NVIDIA}
}

@misc{ada-whitepaper,
	title = {{NVIDIA} {ADA} {GPU} {ARCHITECTURE}. \url{https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf}},
	author = {NVIDIA}
}

@misc{hopper-whitepaper,
	title = {{NVIDIA} {H100} {Tensor} {Core} {GPU} {Architecture}.  \url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper}},
    author={NVIDIA}
}

@misc{cuda-graphs,
	title = {Getting Started with CUDA Graphs},
    url ={https://developer.nvidia.com/blog/cuda-graphs/},
    author={NVIDIA}
}

@misc{cuda-mps,
	title = {GPU Management and Deployment: Multi-process service},
    url ={https://docs.nvidia.com/deploy/mps/index.html},
    author={NVIDIA}
}



@inproceedings{abdelkhalik2022demystifying,
  title={Demystifying the Nvidia Ampere Architecture through Microbenchmarking and Instruction-level Analysis},
  author={Abdelkhalik, Hamdy and Arafa, Yehia and Santhi, Nandakishore and Badawy, Abdel-Hameed A},
  booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@inproceedings{a100microbenchmarking:gtc2021,
    author ={Zhe Jia and Pater Van Sandt},
    title ={Dissecting the Ampere GPU Architecture through Microbenchmarking},
    booktitle = {GTC 2021},
    year = {2021}
}

@article{valiant1990bridging,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{mehta2021mobilevit,
  title={Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer},
  author={Mehta, Sachin and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:2110.02178},
  year={2021}
}

@inproceedings{villa2021need,
  title={Need for speed: Experiences building a trustworthy system-level gpu simulator},
  author={Villa, Oreste and Lustig, Daniel and Yan, Zi and Bolotin, Evgeny and Fu, Yaosheng and Chatterjee, Niladrish and Jiang, Nan and Nellans, David},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={868--880},
  year={2021},
  organization={IEEE}
}



@inproceedings{wang2020accelerating,
  title={Accelerating deep learning inference with cross-layer data reuse on GPUs},
  author={Wang, Xueying and Li, Guangli and Dong, Xiao and Li, Jiansong and Liu, Lei and Feng, Xiaobing},
  booktitle={European Conference on Parallel Processing},
  pages={219--233},
  year={2020},
  organization={Springer}
}

@inproceedings{sivathanu2019astra,
  title={Astra: Exploiting predictability to optimize deep learning},
  author={Sivathanu, Muthian and Chugh, Tapan and Singapuram, Sanjay S and Zhou, Lidong},
  booktitle={Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={909--923},
  year={2019}
}

@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}


@inproceedings{qiao2018automatic,
  title={Automatic kernel fusion for image processing DSLs},
  author={Qiao, Bo and Reiche, Oliver and Hannig, Frank and Teich, J{\"u}rgen},
  booktitle={Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems},
  pages={76--85},
  year={2018}
}

@inproceedings{wahib2014scalable,
  title={Scalable kernel fusion for memory-bound GPU applications},
  author={Wahib, Mohamed and Maruyama, Naoya},
  booktitle={SC'14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={191--202},
  year={2014},
  organization={IEEE}
}

@inproceedings{wu2012kernel,
  title={Kernel weaver: Automatically fusing database primitives for efficient gpu computation},
  author={Wu, Haicheng and Diamos, Gregory and Cadambi, Srihari and Yalamanchili, Sudhakar},
  booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={107--118},
  year={2012},
  organization={IEEE}
}

@inproceedings{qiao2019loop,
  title={From loop fusion to kernel fusion: A domain-specific approach to locality optimization},
  author={Qiao, Bo and Reiche, Oliver and Hannig, Frank and Teich, J{\"\i}rgen},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={242--253},
  year={2019},
  organization={IEEE}
}

@inproceedings{jung2021deepcuts,
  title={DeepCuts: a deep learning optimization framework for versatile GPU workloads},
  author={Jung, Wookeun and Dao, Thanh Tuan and Lee, Jaejin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={190--205},
  year={2021}
}

@inproceedings{zheng2020ansor,
  title={Ansor: Generating $\{$High-Performance$\}$ tensor programs for deep learning},
  author={Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and others},
  booktitle={14th USENIX symposium on operating systems design and implementation (OSDI 20)},
  pages={863--879},
  year={2020}
}

@article{zhao2022apollo,
  title={Apollo: Automatic partition-based operator fusion through layer by layer optimization},
  author={Zhao, Jie and Gao, Xiong and Xia, Ruijie and Zhang, Zhaochuang and Chen, Deshi and Chen, Lei and Zhang, Renwei and Geng, Zhen and Cheng, Bin and Jin, Xuefeng},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={1--19},
  year={2022}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@inproceedings{mlperf_inference,
author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
title = {MLPerf Inference Benchmark},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00045},
doi = {10.1109/ISCA45697.2020.00045},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {446–459},
numpages = {14},
keywords = {machine learning, inference, benchmarking},
location = {Virtual Event},
series = {ISCA '20}
}


@article{cutlass,
  title={Cutlass: Fast linear algebra in cuda c++},
  author={Kerr, Andrew and Merrill, Duane and Demouth, Julien and Tran, John},
  journal={NVIDIA Developer Blog},
  year={2017}
}


@inproceedings{huang_cosa_2021,
	address = {Valencia, Spain},
	title = {{CoSA}: {Scheduling} by {Constrained} {Optimization} for {Spatial} {Accelerators}},
	isbn = {978-1-66543-333-4},
	shorttitle = {{CoSA}},
	url = {https://ieeexplore.ieee.org/document/9499855/},
	doi = {10.1109/ISCA52012.2021.00050},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and ﬂexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efﬁciency, motivating the need for a fast and efﬁcient search strategy to navigate the vast scheduling space.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Huang, Qijing and Kalaiah, Aravind and Kang, Minwoo and Demmel, James and Dinh, Grace and Wawrzynek, John and Norell, Thomas and Shao, Yakun Sophia},
	month = jun,
	year = {2021},
	pages = {554--566},
	file = {Huang et al. - 2021 - CoSA Scheduling by Constrained Optimization for S.pdf:C\:\\Users\\micha\\Zotero\\storage\\D8M7F5WX\\Huang et al. - 2021 - CoSA Scheduling by Constrained Optimization for S.pdf:application/pdf},
}


@inproceedings{zheng2022amos,
  title={AMOS: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction},
  author={Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={874--887},
  year={2022}
}

@inproceedings{parashar2019timeloop,
  title={Timeloop: A systematic approach to dnn accelerator evaluation},
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W and Emer, Joel},
  booktitle={2019 IEEE international symposium on performance analysis of systems and software (ISPASS)},
  pages={304--315},
  year={2019},
  organization={IEEE}
}

@inproceedings{isca2022-gcom,
author = {Lee, Jounghoo and Ha, Yeonan and Lee, Suhyun and Woo, Jinyoung and Lee, Jinho and Jang, Hanhwi and Kim, Youngsok},
title = {GCoM: A Detailed GPU Core Model for Accurate Analytical Modeling of Modern GPUs},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527384},
doi = {10.1145/3470496.3527384},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {424–436},
numpages = {13},
keywords = {graphics processing units, interval analysis, performance modeling},
location = {New York, New York},
series = {ISCA '22}
}

@article{burger2004scaling,
  title={Scaling to the End of Silicon with EDGE Architectures},
  author={Burger, Doug and Keckler, Stephen W and McKinley, Kathryn S and Dahlin, Mike and John, Lizy K and Lin, Calvin and Moore, Charles R and Burrill, James and McDonald, Robert G and Yoder, William},
  journal={Computer},
  volume={37},
  number={7},
  pages={44--55},
  year={2004},
  publisher={IEEE}
}

@INPROCEEDINGS{sle,
  author={Rajwar, R. and Goodman, J.R.},
  booktitle={Proceedings. 34th ACM/IEEE International Symposium on Microarchitecture. MICRO-34}, 
  title={Speculative lock elision: enabling highly concurrent multithreaded execution}, 
  year={2001},
  volume={},
  number={},
  pages={294-305},
  doi={10.1109/MICRO.2001.991127}}


@inproceedings{swanson2003wavescalar,
  title={WaveScalar},
  author={Swanson, Steven and Michelson, Ken and Schwerin, Andrew and Oskin, Mark},
  booktitle={Proceedings. 36th Annual IEEE/ACM International Symposium on Microarchitecture, 2003. MICRO-36.},
  pages={291--302},
  year={2003},
  organization={IEEE}
}

@article{taylor2002raw,
  title={The raw microprocessor: A computational fabric for software circuits and general-purpose programs},
  author={Taylor, Michael Bedford and Kim, Jason and Miller, Jason and Wentzlaff, David and Ghodrat, Fae and Greenwald, Ben and Hoffman, Henry and Johnson, Paul and Lee, Jae-Wook and Lee, Walter and others},
  journal={IEEE micro},
  volume={22},
  number={2},
  pages={25--35},
  year={2002},
  publisher={IEEE}
}

@inproceedings{lustig2022mixed,
  title={Mixed-proxy extensions for the NVIDIA PTX memory consistency model: industrial product},
  author={Lustig, Daniel and Cooksey, Simon and Giroux, Olivier},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={1058--1070},
  year={2022}
}

@misc{cuda-atomics,
    title={cuda::atomic},
    url={https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives/atomic.html},
    author={NVIDIA}
}
@inproceedings{pellauer2019buffets,
  title={Buffets: An efficient and composable storage idiom for explicit decoupled data orchestration},
  author={Pellauer, Michael and Shao, Yakun Sophia and Clemons, Jason and Crago, Neal and Hegde, Kartik and Venkatesan, Rangharajan and Keckler, Stephen W and Fletcher, Christopher W and Emer, Joel},
  booktitle={Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={137--151},
  year={2019}
}

@misc{chips-cheese-latency,
    title={{Memory} {Latency} {Data}. \url{https://chipsandcheese.com/memory-latency-data/}},
    author={Chips and Cheese}
}

@misc{
    ptx,
    title={{Parallel} {Thread} {Execution} {ISA}. \url{https://docs.nvidia.com/cuda/parallel-thread-execution/index.html}},
    author={NVIDIA}
}

@article{faingnaert2021flexible,
  title={Flexible Performant GEMM Kernels on GPUs},
  author={Faingnaert, Thomas and Besard, Tim and De Sutter, Bjorn},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={9},
  pages={2230--2248},
  year={2021},
  publisher={IEEE}
}

@misc{ncu,
    title={{NVIDIA} {Nsight} {Compute}. \url{https://developer.nvidia.com/nsight-compute}},
    author={NVIDIA}
}

@misc{inside-a100,
    title={Inside ThetaGPU and Perlmutter's NVidia Ampere A100 GPU},
    url={https://www.nersc.gov/users/training/events/inside-perlmutter-a100-gpu/},
    author={NVIDIA},
    year={2021}
}



@article{jia2018dissecting,
  title={Dissecting the NVIDIA volta GPU architecture via microbenchmarking},
  author={Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P},
  journal={arXiv preprint arXiv:1804.06826},
  year={2018}
}


@article{prabhakar_plasticine_2017,
	title = {Plasticine: {A} {Reconfigurable} {Architecture} {For} {Parallel} {Paterns}},
	volume = {45},
	issn = {0163-5964},
	shorttitle = {Plasticine},
	url = {https://dl.acm.org/doi/10.1145/3140659.3080256},
	doi = {10.1145/3140659.3080256},
	abstract = {Reconfigurable architectures have gained popularity in recent years as they allow the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have traditionally suffered from performance and power inefficiencies due to bit-level reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new spatially reconfigurable architecture designed to efficiently execute applications composed of parallel patterns. Parallel patterns have emerged from recent research on parallel programming as powerful, high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense and sparse applications.
            We motivate Plasticine by first observing key application characteristics captured by parallel patterns that are amenable to hardware acceleration, such as hierarchical parallelism, data locality, memory access patterns, and control flow. Based on these observations, we architect Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that can efficiently execute nested patterns. Data locality is exploited in Pattern Memory Units using banked scratchpad memories and configurable address decoders. Multiple on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth by supporting a large number of outstanding memory requests, memory coalescing, and burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28nm process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate simulator, we demonstrate that Plasticine provides an improvement of up to 76.9x in performance-per-Watt over a conventional FPGA over a wide range of dense and sparse applications.},
	language = {en},
	number = {2},
	urldate = {2024-02-08},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
	month = sep,
	year = {2017},
	pages = {389--402},
	file = {Full Text PDF:C\:\\Users\\micha\\Zotero\\storage\\XR2KMYAV\\Prabhakar et al. - 2017 - Plasticine A Reconfigurable Architecture For Para.pdf:application/pdf},
}


@inproceedings{prabhakar_sambanova_2022,
	address = {San Francisco, CA, USA},
	title = {{SambaNova} {SN10} {RDU}: {A} 7nm {Dataflow} {Architecture} to {Accelerate} {Software} 2.0},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66542-800-2},
	shorttitle = {{SambaNova} {SN10} {RDU}},
	url = {https://ieeexplore.ieee.org/document/9731612/},
	doi = {10.1109/ISSCC42614.2022.9731612},
	language = {en},
	urldate = {2024-05-21},
	booktitle = {2022 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} ({ISSCC})},
	publisher = {IEEE},
	author = {Prabhakar, Raghu and Jairath, Sumti and Shin, Jinuk Luke},
	month = feb,
	year = {2022},
	pages = {350--352},
	file = {Prabhakar et al. - 2022 - SambaNova SN10 RDU A 7nm Dataflow Architecture to.pdf:C\:\\Users\\micha\\Zotero\\storage\\M5VKFJMR\\Prabhakar et al. - 2022 - SambaNova SN10 RDU A 7nm Dataflow Architecture to.pdf:application/pdf},
}



@article{tarnawski2020efficient,
  title={Efficient algorithms for device placement of dnn graph operators},
  author={Tarnawski, Jakub M and Phanishayee, Amar and Devanur, Nikhil and Mahajan, Divya and Nina Paravecino, Fanny},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15451--15463},
  year={2020}
}

@article{jia2021dissecting,
title={Dissecting the Ampere GPU Architecture through Microbenchmarking},
author={Jia, Zhe and Van Sandt, Peter},
booktitle={GTC 2021}
}

@article{sun2022dissecting,
  title={Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors},
  author={Sun, Wei and Li, Ang and Geng, Tong and Stuijk, Sander and Corporaal, Henk},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={1},
  pages={246--261},
  year={2022},
  publisher={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@article{lam2022graphcast,
  title={GraphCast: Learning skillful medium-range global weather forecasting},
  author={Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and others},
  journal={arXiv preprint arXiv:2212.12794},
  year={2022}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{lie2023cerebras,
  title={Cerebras architecture deep dive: First look inside the hardware/software co-design for deep learning},
  author={Lie, Sean},
  journal={IEEE Micro},
  volume={43},
  number={3},
  pages={18--30},
  year={2023},
  publisher={IEEE}
}

@article{jia2019dissecting,
  title={Dissecting the graphcore ipu architecture via microbenchmarking},
  author={Jia, Zhe and Tillman, Blake and Maggioni, Marco and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1912.03413},
  year={2019}
}

@inproceedings{10.1145/3297858.3304055,
author = {Wang, Kai and Fussell, Don and Lin, Calvin},
title = {Fast Fine-Grained Global Synchronization on GPUs},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304055},
doi = {10.1145/3297858.3304055},
abstract = {This paper extends the reach of General Purpose GPU programming by presenting a software architecture that supports efficient fine-grained synchronization over global memory. The key idea is to transform global synchronization into global communication so that conflicts are serialized at the thread block level. With this structure, the threads within each thread block can synchronize using low latency, high-bandwidth local scratchpad memory. To enable this architecture, we implement a scalable and efficient message passing library. Using Nvidia GTX 1080 ti GPUs, we evaluate our new software architecture by using it to solve a set of five irregular problems on a variety of workloads. We find that on average, our solutions improve performance over carefully tuned state-of-the-art solutions by 3.6\texttimes{}.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {793–806},
numpages = {14},
keywords = {irregular workloads, GPU, synchronization},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

# 10.1145/642089.642111,
@inproceedings{dennis-1974,
author = {Dennis, Jack B. and Misunas, David P.},
title = {A Preliminary Architecture for a Basic Data-Flow Processor},
year = {1974},
isbn = {9781450373661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/642089.642111},
doi = {10.1145/642089.642111},
abstract = {A processor is described which can achieve highly parallel execution of programs represented in data-flow form. The language implemented incorporates conditional and iteration mechanisms, and the processor is a step toward a practical data-flow processor for a Fortran-level data-flow language. The processor has a unique architecture which avoids the problems of processor switching and memory/processor interconnecion that usually limit the degree of realizable concurrent processing. The architecture offers an unusual solution to the problem of structuring and managing a two-level memory system.},
booktitle = {Proceedings of the 2nd Annual Symposium on Computer Architecture},
pages = {126–132},
numpages = {7},
series = {ISCA '75}
}

@ARTICLE{maestro,
  author={Kwon, Hyoukjun and Chatarasi, Prasanth and Sarkar, Vivek and Krishna, Tushar and Pellauer, Michael and Parashar, Angshuman},
  journal={IEEE Micro}, 
  title={MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings}, 
  year={2020},
  volume={40},
  number={3},
  pages={20-29},
  doi={10.1109/MM.2020.2985963}}

@article{ASIFUZZAMAN2023100022,
title = {A survey on processing-in-memory techniques: Advances and challenges},
journal = {Memories - Materials, Devices, Circuits and Systems},
volume = {4},
pages = {100022},
year = {2023},
issn = {2773-0646},
doi = {https://doi.org/10.1016/j.memori.2022.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2773064622000160},
author = {Kazi Asifuzzaman and Narasinga Rao Miniskar and Aaron R. Young and Frank Liu and Jeffrey S. Vetter},
keywords = {Processing-in-memory, Near memory computing, Novel and emerging memory technologies},
abstract = {Processing-in-memory (PIM) techniques have gained much attention from computer architecture researchers, and significant research effort has been invested in exploring and developing such techniques. Increasing the research activity dedicated to improving PIM techniques will hopefully help deliver PIM’s promise to solve or significantly reduce memory access bottleneck problems for memory-intensive applications. We also believe it is imperative to track the advances made in PIM research to identify open challenges and enable the research community to make informed decisions and adjust future research directions. In this survey, we analyze recent studies that explored PIM techniques, summarize the advances made, compare recent PIM architectures, and identify target application domains and suitable memory technologies. We also discuss proposals that address unresolved issues of PIM designs (e.g., address translation/mapping of operands, workload analysis to identify application segments that can be accelerated with PIM, OS/runtime support, and coherency issues that must be resolved to incorporate PIM). We believe this work can serve as a useful reference for researchers exploring PIM techniques.}
}

@INPROCEEDINGS{nvas,
  author={Villa, Oreste and Lustig, Daniel and Yan, Zi and Bolotin, Evgeny and Fu, Yaosheng and Chatterjee, Niladrish and Jiang, Nan and Nellans, David},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  title={Need for Speed: Experiences Building a Trustworthy System-Level GPU Simulator},
  year={2021},
  volume={},
  number={},
  pages={868-880}}

@online{hopper_blog,
  author = {NVIDIA},
  title = {{NVIDIA} {Hopper} {Architecture} {In-Depth}. \url{https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/}},
  url = {https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/},
  urldate = {2023-08-01}
}

@software{tiny-cuda-nn,
	author = {M\"uller, Thomas},
	license = {BSD-3-Clause},
	month = {4},
	title = {{tiny-cuda-nn}},
	url = {https://github.com/NVlabs/tiny-cuda-nn},
	version = {1.7},
	year = {2021}
}

@inproceedings{d2ma,
author = {Jamshidi, D. Anoushe and Samadi, Mehrzad and Mahlke, Scott},
title = {D2MA: Accelerating Coarse-Grained Data Transfer for GPUs},
year = {2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 23rd International Conference on Parallel Architectures and Compilation},
pages = {431–442},
numpages = {12},
location = {Edmonton, AB, Canada},
series = {PACT '14}
}

@article{crago_gpu,
author = {Crago, Neal C. and Stephenson, Mark and Keckler, Stephen W.},
title = {Exposing Memory Access Patterns to Improve Instruction and Memory Efficiency in GPUs},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
journal = {ACM Trans. Archit. Code Optim.},
month = {oct},
articleno = {45},
numpages = {23},
keywords = {vector instruction sets, GPU architecture, vector memory instructions}
}


@ARTICLE{1458143,
  author={Lee, E.A. and Messerschmitt, D.G.},
  journal={Proceedings of the IEEE}, 
  title={Synchronous data flow}, 
  year={1987},
  volume={75},
  number={9},
  pages={1235-1245},
  doi={10.1109/PROC.1987.13876}}


@INPROCEEDINGS {9065589,
author = {U. Gupta and C. Wu and X. Wang and M. Naumov and B. Reagen and D. Brooks and B. Cottel and K. Hazelwood and M. Hempstead and B. Jia and H. S. Lee and A. Malevich and D. Mudigere and M. Smelyanskiy and L. Xiong and X. Zhang},
booktitle = {2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title = {The Architectural Implications of Facebook&#x27;s DNN-Based Personalized Recommendation},
year = {2020},
volume = {},
issn = {},
pages = {488-501},
abstract = {The widespread application of deep learning has changed the landscape of computation in data centers. In particular, personalized recommendation for content ranking is now largely accomplished using deep neural networks. However, despite their importance and the amount of compute cycles they consume, relatively little research attention has been devoted to recommendation systems. To facilitate research and advance the understanding of these workloads, this paper presents a set of real-world, production-scale DNNs for personalized recommendation coupled with relevant performance metrics for evaluation. In addition to releasing a set of open-source workloads, we conduct in-depth analysis that underpins future system design and optimization for at-scale recommendation: Inference latency varies by 60% across three Intel server generations, batching and co-location of inference jobs can drastically improve latency-bounded throughput, and diversity across recommendation models leads to different optimization strategies.},
keywords = {computational modeling;data centers;computer architecture;throughput;optimization;artificial intelligence;videos},
doi = {10.1109/HPCA47549.2020.00047},
url = {https://doi.ieeecomputersociety.org/10.1109/HPCA47549.2020.00047},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {feb}
}

@INPROCEEDINGS{8686544,
  author={O’Connor, Mike and Chatterjee, Niladrish and Lee, Donghyuk and Wilson, John and Agrawal, Aditya and Keckler, Stephen W. and Dally, William J.},
  booktitle={2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Fine-Grained DRAM: Energy-Efficient DRAM for Extreme Bandwidth Systems}, 
  year={2017},
  volume={},
  number={},
  pages={41-54},
  doi={}}

@inproceedings{kandiah2021accelwattch,
  title={AccelWattch: A power modeling framework for modern GPUs},
  author={Kandiah, Vijay and Peverelle, Scott and Khairy, Mahmoud and Pan, Junrui and Manjunath, Amogh and Rogers, Timothy G and Aamodt, Tor M and Hardavellas, Nikos},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={738--753},
  year={2021}
}


@inproceedings{gao2020estimating,
  title={Estimating gpu memory consumption of deep learning models},
  author={Gao, Yanjie and Liu, Yu and Zhang, Hongyu and Li, Zhengxian and Zhu, Yonghao and Lin, Haoxiang and Yang, Mao},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1342--1352},
  year={2020}
}




@article{ukarande2021taco,
author = {Ukarande, Aditya and Patidar, Suryakant and Rangan, Ram},
title = {Locality-Aware CTA Scheduling for Gaming Applications},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3477497},
doi = {10.1145/3477497},
abstract = {The compute work rasterizer or the GigaThread Engine of a modern NVIDIA GPU focuses on maximizing compute work occupancy across all streaming multiprocessors in a GPU while retaining design simplicity. In this article, we identify the operational aspects of the GigaThread Engine that help it meet those goals but also lead to less-than-ideal cache locality for texture accesses in 2D compute shaders, which are an important optimization target for gaming applications. We develop three software techniques, namely LargeCTAs, Swizzle, and Agents, to show that it is possible to effectively exploit the texture data working set overlap intrinsic to 2D compute shaders.We evaluate these techniques on gaming applications across two generations of NVIDIA GPUs, RTX 2080 and RTX 3080, and find that they are effective on both GPUs. We find that the bandwidth savings from all our software techniques on RTX 2080 is much higher than the bandwidth savings on baseline execution from inter-generational cache capacity increase going from RTX 2080 to RTX 3080. Our best-performing technique, Agents, records up to a 4.7\% average full-frame speedup by reducing bandwidth demand of targeted shaders at the L1-L2 and L2-DRAM interfaces by 23\% and 32\%, respectively, on the latest generation RTX 3080. These results acutely highlight the sensitivity of cache locality to compute work rasterization order and the importance of locality-aware cooperative thread array scheduling for gaming applications.},
journal = {ACM Trans. Archit. Code Optim.},
month = {dec},
articleno = {1},
numpages = {26},
keywords = {GPUs, CTA scheduling, shader programs, Cache locality}
}


@inproceedings{chen2020mongoose,
  title={Mongoose: A learnable lsh framework for efficient neural network training},
  author={Chen, Beidi and Liu, Zichang and Peng, Binghui and Xu, Zhaozhuo and Li, Jonathan Lingjie and Dao, Tri and Song, Zhao and Shrivastava, Anshumali and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}


@inproceedings{zhang2018shufflenet,
  title={Shufflenet: An extremely efficient convolutional neural network for mobile devices},
  author={Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6848--6856},
  year={2018}
}


@inproceedings{zeng2021you,
  title={You only sample (almost) once: Linear cost self-attention via bernoulli sampling},
  author={Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya and Acharya, Shailesh and Fung, Glenn M and Singh, Vikas},
  booktitle={International conference on machine learning},
  pages={12321--12332},
  year={2021},
  organization={PMLR}
}


@article{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2302.10866},
  year={2023}
}


@misc{pytorch-gpt-fast,
title={Accelerating Generative AI with PyTorch II: GPT, Fast. \url{https://pytorch.org/blog/accelerating-generative-ai-2/}}
}

@misc{xilinx-versal,
title={{Versal}: {The} {First} {Adaptive} {Compute} {Acceleration} {Platform} ({ACAP})},
author={AMD},
year={2020}
}


@inproceedings{reuther2020survey,
  title={Survey of machine learning accelerators},
  author={Reuther, Albert and Michaleas, Peter and Jones, Michael and Gadepally, Vijay and Samsi, Siddharth and Kepner, Jeremy},
  booktitle={2020 IEEE high performance extreme computing conference (HPEC)},
  pages={1--12},
  year={2020},
  organization={IEEE}
}


@article{vasiljevic_compute_2021,
	title = {Compute {Substrate} for {Software} 2.0},
	volume = {41},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {0272-1732, 1937-4143},
	url = {https://ieeexplore.ieee.org/document/9373921/},
	doi = {10.1109/MM.2021.3061912},
	language = {en},
	number = {2},
	urldate = {2024-05-13},
	journal = {IEEE Micro},
	author = {Vasiljevic, Jasmina and Bajic, Ljubisa and Capalija, Davor and Sokorac, Stanislav and Ignjatovic, Dragoljub and Bajic, Lejla and Trajkovic, Milos and Hamer, Ivan and Matosevic, Ivan and Cejkov, Aleksandar and Aydonat, Utku and Zhou, Tony and Gilani, Syed Zohaib and Paiva, Armond and Chu, Joseph and Maksimovic, Djordje and Chin, Stephen Alexander and Moudallal, Zahi and Rakhmati, Akhmed and Nijjar, Sean and Bhullar, Almeet and Drazic, Boris and Lee, Charles and Sun, James and Kwong, Kei-Ming and Connolly, James and Dooley, Miles and Farooq, Hassan and Chen, Joy Yu Ting and Walker, Matthew and Dabiri, Keivan and Mabee, Kyle and Lal, Rakesh Shaji and Rajatheva, Namal and Retnamma, Renjith and Karodi, Shripad and Rosen, Daniel and Munoz, Emilio and Lewycky, Andrew and Knezevic, Aleksandar and Kim, Raymond and Rui, Allan and Drouillard, Alexander and Thompson, David},
	month = mar,
	year = {2021},
	pages = {50--55},
	file = {Vasiljevic et al. - 2021 - Compute Substrate for Software 2.0.pdf:C\:\\Users\\micha\\Zotero\\storage\\TMPNYQ2J\\Vasiljevic et al. - 2021 - Compute Substrate for Software 2.0.pdf:application/pdf},
}


@inproceedings{domke2021matrix,
  title={Matrix engines for high performance computing: A paragon of performance or grasping at straws?},
  author={Domke, Jens and Vatai, Emil and Drozd, Aleksandr and ChenT, Peng and Oyama, Yosuke and Zhang, Lingqi and Salaria, Shweta and Mukunoki, Daichi and Podobas, Artur and WahibT, Mohamed and others},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={1056--1065},
  year={2021},
  organization={IEEE}
}


@inproceedings{godse2018memory,
  title={Memory technology enabling the next artificial intelligence revolution},
  author={Godse, Ranjana and McPadden, Adam and Patel, Vipin and Yoon, Jung},
  booktitle={2018 IEEE Nanotechnology Symposium (ANTS)},
  pages={1--4},
  year={2018},
  organization={IEEE}
}


@INPROCEEDINGS{9138986,
  author={Abts, Dennis and Ross, Jonathan and Sparling, Jonathan and Wong-VanHaren, Mark and Baker, Max and Hawkins, Tom and Bell, Andrew and Thompson, John and Kahsai, Temesghen and Kimmell, Garrin and Hwang, Jennifer and Leslie-Hurd, Rebekah and Bye, Michael and Creswick, E.R. and Boyd, Matthew and Venigalla, Mahitha and Laforge, Evan and Purdy, Jon and Kamath, Purushotham and Maheshwari, Dinesh and Beidler, Michael and Rosseel, Geert and Ahmad, Omar and Gagarin, Gleb and Czekalski, Richard and Rane, Ashay and Parmar, Sahil and Werner, Jeff and Sproch, Jim and Macias, Adrian and Kurtz, Brian},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads}, 
  year={2020},
  volume={},
  number={},
  pages={145-158},
  keywords={},
  doi={10.1109/ISCA45697.2020.00023}}


@article{barham2022pathways,
  title={Pathways: Asynchronous distributed dataflow for ml},
  author={Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Daniel and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={430--449},
  year={2022}
}