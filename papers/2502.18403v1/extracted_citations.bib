@inproceedings{RAMMER:OSDI2020,
author = {Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
title = {RAMMER: Enabling Holistic Deep Learning Compiler Optimizations with Rtasks},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {Performing Deep Neural Network (DNN) computation on hardware accelerators efficiently is challenging. Existing DNN frameworks and compilers often treat the DNN operators in a data flow graph (DFG) as opaque library functions and schedule them onto accelerators to be executed individually. They rely on another layer of scheduler, often implemented in hardware, to exploit the parallelism available in the operators. Such a two-layered approach incurs significant scheduling overhead and often cannot fully utilize the available hardware resources. In this paper, we propose RAMMER, a DNN compiler design that optimizes the execution of DNN workloads on massively parallel accelerators. RAMMER generates an efficient static spatio-temporal schedule for a DNN at compile time to minimize scheduling overhead. It maximizes hardware utilization by holistically exploiting parallelism through inter- and intra- operator co-scheduling. RAMMER achieves this by proposing several novel, hardware neutral, and clean abstractions for the computation tasks and the hardware accelerators. These abstractions expose a much richer scheduling space to RAMMER, which employs several heuristics to explore this space and finds efficient schedules. We implement RAMMER for multiple hardware backends such as NVIDIA GPUs, AMD GPUs, and Graphcore IPU. Experiments show RAMMER significantly outperforms state-of-the-art compilers such as TensorFlow XLA and TVM by up to 20.1\texttimes{}. It also outperforms TensorRT, a vendor optimized proprietary DNN inference library from NVIDIA, by up to 3.1\texttimes{}},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {50},
numpages = {17},
series = {OSDI'20}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@inproceedings{dnnfusion:pldi2021,
author = {Niu, Wei and Guan, Jiexiong and Wang, Yanzhi and Agrawal, Gagan and Ren, Bin},
title = {DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454083},
doi = {10.1145/3453483.3454083},
abstract = {Deep Neural Networks (DNNs) have emerged as the core enabler of many major applications on mobile devices. To achieve high accuracy, DNN models have become increasingly deep with hundreds or even thousands of operator layers, leading to high memory and computational requirements for inference. Operator fusion (or kernel/layer fusion) is key optimization in many state-of-the-art DNN execution frameworks, such as TensorFlow, TVM, and MNN, that aim to improve the efficiency of the DNN inference. However, these frameworks usually adopt fusion approaches based on certain patterns that are too restrictive to cover the diversity of operators and layer connections, especially those seen in many extremely deep models. Polyhedral-based loop fusion techniques, on the other hand, work on a low-level view of the computation without operator-level information, and can also miss potential fusion opportunities. To address this challenge, this paper proposes a novel and extensive loop fusion framework called DNNFusion. The basic idea of this work is to work at an operator view of DNNs, but expand fusion opportunities by developing a classification of both individual operators and their combinations. In addition, DNNFusion includes 1) a novel mathematical-property-based graph rewriting framework to reduce evaluation costs and facilitate subsequent operator fusion, 2) an integrated fusion plan generation that leverages the high-level analysis and accurate light-weight profiling, and 3) additional optimizations during fusion code generation. DNNFusion is extensively evaluated on 15 DNN models with varied types of tasks, model sizes, and layer counts. The evaluation results demonstrate that DNNFusion finds up to 8.8 \texttimes{} higher fusion opportunities, outperforms four state-of-the-art DNN execution frameworks with 9.3\texttimes{} speedup. The memory requirement reduction and speedups can enable the execution of many of the target models on mobile devices and even make them part of a real-time application.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {883–898},
numpages = {16},
keywords = {Deep Neural Network, Mobile Devices, Operator Fusion, Compiler Optimization},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@INPROCEEDINGS{horizontal-fusion:cgo2022,
  author={Li, Ao and Zheng, Bojian and Pekhimenko, Gennady and Long, Fan},
  booktitle={2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Automatic Horizontal Fusion for GPU Kernels}, 
  year={2022},
  volume={},
  number={},
  pages={14-27},
  doi={10.1109/CGO53902.2022.9741270}}

@article{huang2023alcop,
  title={ALCOP: Automatic Load-Compute Pipelining in Deep Learning Compiler for AI-GPUs},
  author={Huang, Guyue and Bai, Yang and Liu, Liu and Wang, Yuke and Yu, Bei and Ding, Yufei and Xie, Yuan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{huang_cosa_2021,
	address = {Valencia, Spain},
	title = {{CoSA}: {Scheduling} by {Constrained} {Optimization} for {Spatial} {Accelerators}},
	isbn = {978-1-66543-333-4},
	shorttitle = {{CoSA}},
	url = {https://ieeexplore.ieee.org/document/9499855/},
	doi = {10.1109/ISCA52012.2021.00050},
	abstract = {Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and ﬂexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efﬁciency, motivating the need for a fast and efﬁcient search strategy to navigate the vast scheduling space.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Huang, Qijing and Kalaiah, Aravind and Kang, Minwoo and Demmel, James and Dinh, Grace and Wawrzynek, John and Norell, Thomas and Shao, Yakun Sophia},
	month = jun,
	year = {2021},
	pages = {554--566},
	file = {Huang et al. - 2021 - CoSA Scheduling by Constrained Optimization for S.pdf:C\:\\Users\\micha\\Zotero\\storage\\D8M7F5WX\\Huang et al. - 2021 - CoSA Scheduling by Constrained Optimization for S.pdf:application/pdf},
}

@inproceedings{jia2019taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

@inproceedings{jung2021deepcuts,
  title={DeepCuts: a deep learning optimization framework for versatile GPU workloads},
  author={Jung, Wookeun and Dao, Thanh Tuan and Lee, Jaejin},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={190--205},
  year={2021}
}

@ARTICLE{maestro,
  author={Kwon, Hyoukjun and Chatarasi, Prasanth and Sarkar, Vivek and Krishna, Tushar and Pellauer, Michael and Parashar, Angshuman},
  journal={IEEE Micro}, 
  title={MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings}, 
  year={2020},
  volume={40},
  number={3},
  pages={20-29},
  doi={10.1109/MM.2020.2985963}}

@inproceedings{parashar2019timeloop,
  title={Timeloop: A systematic approach to dnn accelerator evaluation},
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W and Emer, Joel},
  booktitle={2019 IEEE international symposium on performance analysis of systems and software (ISPASS)},
  pages={304--315},
  year={2019},
  organization={IEEE}
}

@inproceedings{qiao2018automatic,
  title={Automatic kernel fusion for image processing DSLs},
  author={Qiao, Bo and Reiche, Oliver and Hannig, Frank and Teich, J{\"u}rgen},
  booktitle={Proceedings of the 21st International Workshop on Software and Compilers for Embedded Systems},
  pages={76--85},
  year={2018}
}

@inproceedings{qiao2019loop,
  title={From loop fusion to kernel fusion: A domain-specific approach to locality optimization},
  author={Qiao, Bo and Reiche, Oliver and Hannig, Frank and Teich, J{\"\i}rgen},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={242--253},
  year={2019},
  organization={IEEE}
}

@inproceedings{shi2023welder,
  title={Welder: Scheduling Deep Learning Memory Access via Tile-graph},
  author={Shi, Yining and Yang, Zhi and Xue, Jilong and Ma, Lingxiao and Xia, Yuqing and Miao, Ziming and Guo, Yuxiao and Yang, Fan and Zhou, Lidong},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={701--718},
  year={2023}
}

@inproceedings{sivathanu2019astra,
  title={Astra: Exploiting predictability to optimize deep learning},
  author={Sivathanu, Muthian and Chugh, Tapan and Singapuram, Sanjay S and Zhou, Lidong},
  booktitle={Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={909--923},
  year={2019}
}

@inproceedings{wahib2014scalable,
  title={Scalable kernel fusion for memory-bound GPU applications},
  author={Wahib, Mohamed and Maruyama, Naoya},
  booktitle={SC'14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={191--202},
  year={2014},
  organization={IEEE}
}

@inproceedings{wang2016simultaneous,
  title={Simultaneous multikernel GPU: Multi-tasking throughput processors via fine-grained sharing},
  author={Wang, Zhenning and Yang, Jun and Melhem, Rami and Childers, Bruce and Zhang, Youtao and Guo, Minyi},
  booktitle={2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={358--369},
  year={2016},
  organization={IEEE}
}

@inproceedings{wang2020accelerating,
  title={Accelerating deep learning inference with cross-layer data reuse on GPUs},
  author={Wang, Xueying and Li, Guangli and Dong, Xiao and Li, Jiansong and Liu, Lei and Feng, Xiaobing},
  booktitle={European Conference on Parallel Processing},
  pages={219--233},
  year={2020},
  organization={Springer}
}

@misc{workgraphs,
    author={Microsoft},
    title={D3D12 Work Graphs},
    url={https://devblogs.microsoft.com/directx/d3d12-work-graphs/},
}

@inproceedings{wu2012kernel,
  title={Kernel weaver: Automatically fusing database primitives for efficient gpu computation},
  author={Wu, Haicheng and Diamos, Gregory and Cadambi, Srihari and Yalamanchili, Sudhakar},
  booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={107--118},
  year={2012},
  organization={IEEE}
}

@article{zhao2022apollo,
  title={Apollo: Automatic partition-based operator fusion through layer by layer optimization},
  author={Zhao, Jie and Gao, Xiong and Xia, Ruijie and Zhang, Zhaochuang and Chen, Deshi and Chen, Lei and Zhang, Renwei and Geng, Zhen and Cheng, Bin and Jin, Xuefeng},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={1--19},
  year={2022}
}

@article{zhao2022ispa,
  title={ISPA: Exploiting Intra-SM Parallelism in GPUs via Fine-Grained Resource Management},
  author={Zhao, Han and Cui, Weihao and Chen, Quan and Guo, Minyi},
  journal={IEEE Transactions on Computers},
  volume={72},
  number={5},
  pages={1473--1487},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zhao2023effectively,
  title={Effectively Scheduling Computational Graphs of Deep Neural Networks toward Their $\{$Domain-Specific$\}$ Accelerators},
  author={Zhao, Jie and Feng, Siyuan and Dan, Xiaoqiang and Liu, Fei and Wang, Chengke and Yuan, Sheng and Lv, Wenyuan and Xie, Qikai},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={719--737},
  year={2023}
}

@inproceedings{zheng2020ansor,
  title={Ansor: Generating $\{$High-Performance$\}$ tensor programs for deep learning},
  author={Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and others},
  booktitle={14th USENIX symposium on operating systems design and implementation (OSDI 20)},
  pages={863--879},
  year={2020}
}

@inproceedings{zheng2022amos,
  title={AMOS: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction},
  author={Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={874--887},
  year={2022}
}

@inproceedings{zheng2022astitch,
  title={AStitch: enabling a new multi-dimensional optimization space for memory-intensive ML training and inference on modern SIMT architectures},
  author={Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and others},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={359--373},
  year={2022}
}

@inproceedings{zheng2023chimera,
  title={Chimera: An Analytical Optimizing Framework for Effective Compute-intensive Operators Fusion},
  author={Zheng, Size and Chen, Siyuan and Song, Peidi and Chen, Renze and Li, Xiuhong and Yan, Shengen and Lin, Dahua and Leng, Jingwen and Liang, Yun},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1113--1126},
  year={2023},
  organization={IEEE}
}

