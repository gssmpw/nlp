\section{Related Work}
\label{sec:related}

\subsection{Feature Interaction Modeling}

Feature interactions in terms of nonadditive terms play a crucial role in model-based predictive modeling.
Many works attempted to develop methods to indicate the interaction.
Beginning with the statistical-based measure called Friedman's H statistic \cite{Hstat} involving the theory of partial dependency decomposition.
However, it is computationally expensive and is dependent on predictive models.

From the perspective of machine learning, many models were proposed to model feature interactions, especially variants of factorization models inspired by FM \cite{FM}.
FM relies on the inner product between the embedding of the feature-value motivated by the factorization of matrices.
Several variants of FM have been proposed to fill the gaps.
FFM \cite{FFM} takes into account the embedding of feature field (i.e. columns in a table of data).
AFM \cite{AFM} considers the weight of the interaction by adding attention coefficients to its model.

The success of deep nets in various domains motivates researchers to use them in modeling interactions.
For example, Factorization Machine supported Neural Network (FNN) \cite{FNN} proposed a deep net architecture that can automatically learn effective patterns from categorical feature interactions in CTR tasks.
In addition, DeepFM \cite{DeepFM} proposed a kind of similar idea but included a special deep net layer that performs the FM task.

The attention mechanisms \cite{Bahdanau} are the method designed to model the importance between different components.
In AFM, the attention mechanism was applied because the authors believe that all interactions should not be treated equally likely with respect to prediction values.
Attention allows the model to weight feature interactions differently based on their importance.
After introducing the attention mechanism to interaction tasks, many works use attention to model the interaction of features based on the factorization machine framework \cite{Sarkar2022DualAH, Cheng2019AdaptiveFN,Wang2020AdnFMAA,Wen2020NeuralAM,Li2021GraphFMGF}.

One of the crucial challenges of interactions in traditional machine learning models is the explainability of which features have interaction, although some of them outperform in prediction performance.
So we need an explicit representation that can correspond to pairwise interaction, where a graph is a representation that can fill this gap.
In addition, GNN is a tool that can be used to learn information from graphs.

\subsection{Graph Neural Networks for Feature Interactions}
GNN are becoming more interesting for use in feature interaction problems.
Since feature interactions can be seen as relationships between features, most of the works rely on feature graphs for each individual instance to represent the interactions via aggregation of representation from neighbor nodes.

Motivated by the click-through rate (CTR) prediction, which is believed to influence the rate of clicking on a product, such as age and gender, many works attempted to model this prediction by using GNN applied feature graphs.
To the best of our knowledge, Fi-GNN \cite{fignn} is the first work to use GNN to model the interaction of features in the feature graph for the prediction of CTR.
It applied the field-aware embedding layer to compute the latent representation of nodes of features before feeding the feature graph with the computed representations into a stack of message passing layers.

After the introduction of Fi-GNN, many subsequent works have built on this idea.
Cross-GCN \cite{Feng2020CrossGCNEG} used a simple graph convolutional network with cross-feature transformation.
GraphFM \cite{GraphFM} seamlessly combined the idea of FM and GNN using the neural matrix factorization \cite{NeuralMF} based function to estimate the edge weight. It also adopts the attentional aggregation strategy to compute feature representation.
Table2Graph \cite{Table2Graph} applied attention mechanisms to compute the probability adjacency matrix instead of edge weight. It also used the reinforcement learning policy to capture key feature interactions by sampling the edges within feature graphs.

All mentioned works chronically improve the methods of feature interaction learning.
However, no work provides information about the effect of the construction of feature graphs on the learning capacity.
Most of the works utilize attention mechanisms or edge weights to let models learn graph structures via these parameters.