%Given a graph $G=(V,E)$ whose nodes are of degree at most 1, 
%a function containing pairwise interaction induced from $G$ is defined as
%$$
%f_G(x) = \sum_{\deg(i)=0} c_i x_i + \sum_{\{i,j\}\in E} c_{ij}x_i x_j
%$$
%
%\begin{algorithm}
%	\floatname{algorithm}{Problem}
%	\caption{MDL Pairwise Interaction Graph Problem}
%	\textbf{Input:} dataset $S = \{(x_i,y_i)\}$\\
%	\textbf{Output:} The edge set $E$ for an input feature graph $G(V,E) \in \mathfrak{G}$ so that
%	$$
%	\min_{G\in\mathfrak{G}}{\mathcal{L}(S|f_G) + \mathcal{L}(f_G)}
%	$$
%\end{algorithm}
%
%Here, we define
%\begin{align*}
%	\mathcal{L}(f_G) &= \LR (|E(G)|) + \sum_{\deg(i)=0}\LR(c_i) + \sum_{\{i,j\}\in E}\LR(c_{ij})\\
%	\mathcal{L}(S|f_G) &= \sumdata\LR(x_i) + \sumdata \LR(y_i - f_G(x_i))\\
%	\mathcal{L}(S,f_G) &= \mathcal{L}(S|f_G) + \mathcal{L}(f_G)
%\end{align*}
%%\begin{notebox}
%%	(ask P'Chai about this)\\
%%	Is $\mathcal{L}(f_G) = \LR (|E(G)|)$ or\\ $\mathcal{L}(f_G) = \sumfeat\LR(c_i) + \sum_{\{i,j\}\in E}\LR(c_{ij})$ enough?
%%\end{notebox}
%where $\LR(x)$ is a function for a number of bits we need to encode a real number $x$ that satisfies triangle inequality. 
%We rely on the assumption that for any $x_1,x_2 \in \R$, we have $\LR(x_1) \leq \LR(x_2)$ whenever $|x_1|\leq|x_2|$.
%
%\begin{prop}\label{prop:MDLerrorComplete}
%	Assume that the functions that generate $Y$ from $X$ in $S$ are induced by some feature graph $G^*$, i.e. $$Y = f_{G^*}(X) + \epsilon$$ where $\epsilon$ is a noise random variable from some distribution such that there exists $\epsilon^*>0$ with $|\epsilon|<\epsilon^*$.
%	
%	Let $\hat{f}_{G^*}, \hat{f}_K$ be functions that their parameters are approximated by $S$ where $K$ is the complete feature graph.
%	Then
%	$$
%	\sumdata \LR(y_i - \hat{f}_K(x_i)) \geq 	\sumdata \LR(y_i - \hat{f}_{G^*}(x_i))
%	$$ 
%\end{prop}
%\begin{proof}
%	It is easy to see that
%	$$
%	\sumdata \LR(y_i - \hat{f}_{G^*}(x_i)) \leq n\LR(\epsilon^*).
%	$$
%	Observe that 
%	\begin{align*}
%		\sumdata &\LR(y_i - \hat{f}_{K^*}(x_i))\\
%		&= \sumdata \LR(y_i - \hat{f}_{G^*}(x_i) + \hat{f}_{G^*}(x_i) - \hat{f}_{K^*}(x_i))\\
%		&\leq \sumdata \LR(y_i - \hat{f}_{G^*}(x_i)) + \sumdata \LR(\hat{f}_{G^*}(x_i) - \hat{f}_{K^*}(x_i))\\
%		&\leq n\LR(\epsilon^*) + \sumdata \LR(\hat{f}_{G^*}(x_i) - \hat{f}_{K^*}(x_i))
%	\end{align*}
%\begin{notebox}
%	????????????????????
%\end{notebox}~
%\end{proof}
%
%\begin{cor}
%	For the same setting in Proposition \ref{prop:MDLerrorComplete}, we have $\mathcal{L}(S,f_{G^*}) \leq \mathcal{L}(S,f_K)$.
%\end{cor}

%\begin{prop}\label{prop:MDLerrorNull}
%	The setting about the dataset $S$ is the same as that in Proposition \ref{prop:MDLerrorComplete}.
%	
%	Let $\hat{f}_{G^*}, \hat{f}_N$ be functions that their parameters are approximated by $S$ where $N$ is the feature graph with no edge.
%	Then
%	$$
%	\sumdata \LR(y_i - \hat{f}_N(x_i)) \geq 	\sumdata \LR(y_i - \hat{f}_{G^*}(x_i))
%	$$
%	Hence, we also have $\mathcal{L}(S,f_{G^*}) \leq \mathcal{L}(S,f_N)$.
%\end{prop}
%\begin{proof}
%	~
%	
%	\begin{notebox}
%		????????????????????\\
%		should be assume error for each instance at least 1 (?)
%	\end{notebox}
%\end{proof}
%
%\begin{notebox}
%	Theorem:
%	
%	$\mathcal{L}(S,f_{G}) \leq \mathcal{L}(S,f_K)$ and $\mathcal{L}(S,f_{G}) \leq \mathcal{L}(S,f_N)$
%	
%	might be work by the assumption encoding length of $c_{ij}x_ix_j$at least 1
%\end{notebox}


\newpage