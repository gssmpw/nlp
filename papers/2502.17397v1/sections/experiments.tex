



In this section we evaluate the proposed method both with regards to its accuracy for labeling and the soundness of the confidence estimates. We consider both aggregate statistics and zoom in on specific scenarios. Challenging scenarios are included to further assess the method. Additionally, we evaluate the method with conditionally averaged behavior around specific types of transitions.

A short summary of the training specifics, including hyperparameters and the dataset splits, is provided in Section~\ref{sec:hyperparam}. We follow with quantative and qualitative evaluations in Sections~\ref{sec:metrics} and~\ref{sec:qualitative} respectively. Next, we consider extrapolation and robustness in Section~\ref{sec:extrapolation_robustness}, and conclude with conditionally averaged behavior of the confidence estimates in Section~\ref{sec:conditional_average}.

\subsection{Dataset split and hyperparameters}\label{sec:hyperparam}
\textbf{Dataset split.} To recap, the dataset (302 shots) is split into the `train-validation' set (258 shots), the `ensemble-holdout' (10 shots) and the test set (34 shots). In this section, unless mentioned otherwise, we only show results from the test set. We carefully construct the `ensemble-holdout' and test set such that they are a representative sample of the operational space of TCV. To do so, we sample these shots to approximate the distribution of the campaigns present in the full dataset. We highlight shots from plasmas presented in~\cite{labit2024} to cover the ITER Baseline (IBL). Furthermore, we select sets of shots from underrepresented scenarios to evaluate the method under more challenging circumstances. Specifically, we include shots from quasi-continuous exhaust (QCE) regimes~\cite{labit2019}, negative triangularity (NT) configurations~\cite{coda2022}, and filter specifically on $\delta_{\text{top}} > 0.3$ and $\beta_N > 1.7$ to evaluate out-of-distribution regimes.

\textbf{Hyperparameters.} In total, we utilize an ensemble of 52 models (level 1), 26 based on the FNOLSTM and 26 on GBDT. For all \textit{(model + feature set)} combinations, we refer to~\ref{ap:feature_sets}. For each \textit{(model + feature set)} configuration (level 2), we use 4 different folds. Hyperparameters are shared on this level, whereas each model is naturally defined by its own parameters. All models are optimized using the `train-validation' set. Individual model parameters are fit on the train set, whereas hyperparameters are optimized using Bayesian optimization on the validation set. The optimization target is the Cohen's kappa coefficient (see Section~\ref{sec:metrics}) averaged over the different folds. For details on the hyperparameter ranges we refer to~\ref{ap:parameters}.

All neural network models are implemented using PyTorch~\cite{paszke2019}. The gradient-boosted decision trees are implemented using XGBoost~\cite{xgboost2016}. We utilize Optuna~\cite{optuna2019} for the hyperparameter optimization and net:cal~\cite{kuppers2020} for calibrating level 2 ensembles through temperature scaling.



\subsection{Quantitative results for accuracy and calibration}\label{sec:metrics}
\textbf{Metrics.} For quantitative evaluation, we consider Cohen's kappa coefficient~\cite{cohen1960} for the classification performance, and the Expected Calibration Error (ECE)~\cite{degroot1983,naeini2015} for the calibration of the model confidence outputs. 

\textit{Cohen's kappa coefficient}~\cite{cohen1960} measures the agreement between two sets of categorical labelings while taking into account agreement occurring by chance. We utilize it as an accuracy metric taking into account the class imbalance present in our data. It is defined as follows:
\begin{align}
    \textit{Cohen's kappa coefficient} = \frac{p_o - p_e}{1 - p_e},
\end{align}
with $p_o$ as the observed agreement and $p_e$ the agreement by chance. For prediction matrix $\mathbf{pred}$ and ground-truth matrix $\mathbf{gt}$, each of size $N$, we compute the observed agreement $p_o$ as the accuracy: ${p_o = \frac{1}{N}\sum_{i=1}^N\mathbf{1}[pred_i = gt_i]}$. We compute the agreement by chance $p_e$ by multiplying the total counts of each state (thus assuming statistical independence) in the ground-truth labels and the predictions: ${p_e = \frac{1}{N^2} \sum_{s \in \mathbf{s}}N_{pred_s} N_{gt_s}}$, where $N_{pred_s}$ and $N_{gt_s}$ denote the prediction and ground-truth counts for state $s$, respectively. Intuitively, we can interpret the metric as the ratio expressing the gain of our classifier relative to random guessing. A value of 1 corresponds to perfect predictions, with 0 corresponding to random guessing (and negative values to worse-than-random performance).

\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/all_metrics.pdf}\end{center}
    \caption{An overview of classification performance and uncertainty calibration for all individual models in the ensemble (level 2), ensembles for all FNOLSTM-based and GBDT-based models, and an ensemble of all models (level 1). We plot the Cohen's kappa coefficients on the left (higher is better) and the Expected Calibration Error on the right (lower is better). Feature categories are described by the first two letters, or `$\ast\ast$' for a mix of categories. The different subsets are enumerated for brevity, see~\ref{ap:feature_sets} for details. For each metric, we plot the results for all 34 shots in the test set (bottom bar) and a common subset of 15 shots that contains all features (top bar). The latter set is used to allow a comparison between all individual models: some models cannot provide predictions for all shots in the test set due to missing signals, which is indicated by the lack of a bar in the plot.} %
    \label{fig:all_metrics}%
\end{figure}

\begin{figure}[t]
\begin{center}\includegraphics[width=.825\linewidth]{figures/model_availability.pdf}\end{center}
    \caption{The distribution of model availability. That is, we plot the fraction of models that are available, which corresponds to their input features being present in the discharge, for fractions of the dataset. We evaluate the full dataset rather than the test set to give a better sample of signal availability.  The two lines distinguish models that use all signals they are trained on (red) and models that can still run on a discharge, albeit with a reduced input set (blue). The latter correponds to the GBDT models, since they can naturally deal with missing input features. Only approximately $\approx$64\% of discharges have each utilized signal present, resulting in all models being available. In the worst cases, only $\approx$38\% of models are active.}
    \label{fig:availability}%
\end{figure}




The \textit{Expected Calibration Error}~\cite{degroot1983,naeini2015} aims to measure the calibration of a model's confidence outputs. We can express the calibration error as the difference in expectation between the model confidence and accuracy. The ECE approximates this expectation using finite samples by binning the prediction- and confidence-outputs and computing the per-bin confidence/accuracy difference. It is defined as follows: 
\begin{align}
    \hspace{-.11cm}\textit{ECE} = \sum_{m=1}^M \frac{N_m}{N}|\textit{accuracy}(\mathbf{B}_m) - \textit{confidence}(\mathbf{B}_m)|
\end{align}
for $M$ interval bins, together covering the \numrange{0}{1} range of confidences: the $m^\text{th}$ bin covers interval ${(\frac{m-1}{M}, \frac{m}{M}]}$. $\mathbf{B}_m$ denotes the elements in the $m^\text{th}$ bin, $N_m$ the number of elements in $\mathbf{B}_m$, and $N$ the total number of elements. For each bin, $\textit{accuracy}(\mathbf{B}_m)$ is computed as the fraction of correctly predicted samples to total samples, and $\textit{confidence}(\mathbf{B}_m)$ as the average confidence output. Intuitively, the \textit{ECE} can directly be interpreted as an error of the confidence output: an \textit{ECE} of 0.01 indicates that on average, the model's confidence differs from its actual accuracy by 1 percentage point.


\textbf{Overview of results.} An overview of prediction and calibration performance on the test set, measured through Cohen's kappa coefficient and the ECE, is provided in Figure~\ref{fig:all_metrics}; see Table~\ref{tab:allresults} for a tabular version. We plot the results for all \textit{(model + feature set)} settings (level 2) and ensembles for all FNOLSTM models, all GBDT models and for all models (level 1). To be able to compare all feature sets, we provide the metrics on both the full test dataset of 34 discharges and a subset of 15 discharges for which all features are available. The results are sorted on the prediction performance on the latter subset. To give an idea of the  general availability of signals, and subsequently of models using them, we plot the distribution of model availability in Figure~\ref{fig:availability}.

\begin{figure}[t]
\begin{center}\includegraphics[width=.85\linewidth]{figures/transition_acc.pdf}\end{center}
    \caption{The prediction accuracy near the time of state transitions. Each point corresponds to accuracy for timesteps filtered using a \SI{5}{\milli\second} window with varying offsets before and after a transition, with the exception of the last point capturing the remaining timesteps. We observe a significant drop in accuracy in small windows around transitions, with the largest impact for the static-only GBDT ensemble.}
    \label{fig:transition_acc}%
\end{figure}

Perhaps unsurprisingly, mixed feature settings generally provide the best predictive performance and lowest calibration error when they can be applied. FNOLSTM models based on emission and energy content signals also show strong performance while still being applicable to all shots. Notably, mixed feature set-based GBDT models still show strong performance even when applied to all test shots, where some components of the model are disabled due to missing input signals. The ensembled models do not necessarily always beat the \textit{(model + feature set)} models on individual metrics, however they can always be applied and we found them to give more robust predictions and uncertainty estimates.

Lastly, we investigate the relative performance of the FNOLSTM-only, GBDT-only, and complete ensemble with regards to transition times. Specifically, we filter the test set on various incremental windows of~\SI{5}{\milli\second} within~\SI{50}{\milli\second} of state transitions and plot the accuracy on this subset of the data, see Figure~\ref{fig:transition_acc}. Note that we consider accuracy as a metric for easier interpretability\footnote{Subsets of data just before/after transitions naturally have (approximately) balanced ratios of two classes, removing the need to account for class imbalance in the metric.}. While on a large scale the ensembles all perform well, the accuracies drop significantly very close to the transition time, primarily within \numrange{0}{5}\SI{}{\milli\second}; more precise estimates of the exact time of transition are an interesting avenue for future work.

\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/calibration_plots.pdf}\end{center}
    \caption{The calibration of ensembles of FNOLSTM models, GBDT models and all models. The reliability diagrams (right) show how the model confidence, binned into intervals of 0.1, corresponds to the expected accuracy from the respective bin: a visual respresentation of the ECE. The distribution of model confidences are plotted for context (left).}
    \label{fig:calibration}%
\end{figure}

\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/threshold.pdf}\end{center}
    \caption{The relation between the prediction performance, the model confidence and the fraction of data labeled, plotted for the ensemble of FNOLSTM models, GBDT models and all models. For each ensemble, the line is colored by the minimum confidence level, and displays what fraction of the dataset is still covered, and at what accuracy. By setting this threshold, we can select to label a subset of the dataset with near-perfect results.}
    \label{fig:threshold}%
\end{figure}

\begin{figure*}[t]
\begin{center}\includegraphics[width=.5\linewidth]{figures/shots/TEST_61028.pdf}\includegraphics[width=.5\linewidth]{figures/shots/TEST_64678.pdf}\\\includegraphics[width=.5\linewidth]{figures/shots/TEST_68631.pdf}\includegraphics[width=.5\linewidth]{figures/shots/TEST_73631.pdf}\end{center}
    \caption{Plots illustrating the predictions from the full ensemble. For each discharge we plot the core line integrated density from the interferometer $n_{e,\text{core}}$ (black), the total input power $P_{\textit{in}}$ (red), and overlay the emissions from the photodiode $\text{PD}_{\textit{CIII}}^{}$ (green). The top panel is colored by the ensemble predictions of the confinement state, the bottom stripe indicates the expert's manual labeling for reference. Additionally, the ensemble confidence is provided in the top panel, by the dashed line and the change in background brightness. This scale is always normalized, i.e., the top indicates 1.0 confidence and the bottom 0.0 (confidence quartiles given by dashed horizontal lines).}
    \label{fig:s_test}%
\end{figure*}

\textbf{Confidence-accuracy relationship.} To evaluate the validity of the confidence outputs, we start by plotting reliability diagrams for the FNOLSTM, GBDT and full ensembles in Figure~\ref{fig:calibration}. Generally, all ensembles are relatively well calibrated, with only a few percentage points of calibration error on average. However, since the ensembles are generally very accurate, the majority of confidence predictions fall in a tight range, i.e. $\approx$\numrange{0.93}{0.97}: we cannot trust the ECE in isolation. Fortunately, also around lower confidence estimates the reliability diagrams generally indicate good calibration; more qualitative evaluations are provided in subsequent sections.

Additionally, we explore using the confidence estimates as a threshold for prediction outputs. For example, if one wants to build a database of confinement states but does not necessarily care about fully labeling each discharge, one could filter on high-confidence timeslices to get more reliable results. This relation between prediction accuracy when filtering on a minimum level of confidence, alongside the fraction of test data that remains, is depicted in Figure~\ref{fig:threshold}. We see that the accuracy rises steadily as the threshold is increased, with the full ensemble showing the most advantageous relation. Data can be labeled with little to no errors with $\approx$75\% of timeslices remaining.

\subsection{Qualitative results}\label{sec:qualitative}
\textbf{General overview.} To give an idea of the average performance, we plot 4 discharges from the test set in Figure~\ref{fig:s_test}. These discharges cover various scenarios, e.g. power scans for edge dynamics, high performance scenario development and control near operational limits. In general, there is good agreement between the ensemble and the manual validation, even capturing several transient transitions. Some are missed however, for example in \#61028 and \#64678; nevertheless, there is usually a drop in confidence aligning with the respective transients. The only major error is in \#68631 where a region of dithering is incorrectly labeled as L-mode, albeit at a slightly reduced confidence.

\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/shots/BAD_77598_both.pdf}\end{center}
    \caption{The worst result on the test set: \#77598, a discharge testing vertical instability growth rate control. We plot the full discharge, predictions and signals at the top. At the bottom, we zoom in on the region of biggest mismatch, showing the labels, predictions and confidences overlaid on a spectrogram of high frequency magnetic measurements. Multiple incorrectly classified transitions seem to align with MHD activity.}
    \label{fig:s_test_worst}%
\end{figure}

The worst result in the test set is on \#77598, depicted in Figure~\ref{fig:s_test_worst} (top). In this discharge, a radial proximity controller was tested to control the vertical instability growth rate~\cite{marchioni2024}, leading to some unconventional plasma dynamics. Specifically, the controller was active from \SI{1.25}{\second} to \SI{1.70}{\second}, corresponding to the region of biggest mismatch. Nevertheless, an automated labeling method should deal with any scenario. To further investigate, we display the spectrogram from high frequency magnetics for the period of biggest mismatch in Figure~\ref{fig:s_test_worst} (bottom). The prediction errors partially correspond to the occurrence of magnetohydrodynamic (MHD) perturbations, for example around \SI{1.35}{\second} and \SI{1.6}{\second}. Potentially, the effects of these perturbations lead to signature behavior in input signals similar to those in H-mode. Such confusion could be alleviated by also including MHD markers as input signals. Regardless, we see that the confidence also drops low in the regions of mismatch. For example between {$\approx$\numrange{1.4}{1.6}\SI{}{\second}} we have a period where the method predicts H-mode for too many timesteps, but the confidence is high only for the period where it matches the manual validation. Similarly, the H-L back transition at \SI{1.3}{\second} is predicted too late, however the confidence drops where the transition actually occurs. %

\textbf{ITER Baseline (IBL) example.} To evaluate a representative scenario we test the ensemble on \#64770, see Figure~\ref{fig:s_ibl}. Specifically, \#64770 is an ITER Baseline scenario development discharge using ECRH power to prevent neoclassical tearing modes~\cite{labit2024}. We see that the main L-D-H phases are matched precisely and with high confidence. Of interest is the phase from \SI{1.1}{\second} onward, where many fast back transitions occurred. To better evaluate the characteristics of the different models, we zoom in for 3 predictions (Figure~\ref{fig:s_ibl} bottom): the full ensemble, the ensemble of all FNOLSTM models, and one of the best-performing \textit{(model + feature set)} settings. All models capture the main H-mode phase and all show dips in the uncertainty corresponding to the transient events. The two models discarding the static formulation perform better around these transients. This discrepancy is not surprising, given that a fast back-and-forth between different states is likely easier to capture by using a context window of signal data that covers the before and after, rather than an individual timeslice only in one state. The individual model (FNOLSTM-$\ast\ast$-9) shows the best performance when it comes to the output labels, but the confidence estimates are a lot noisier, especially from $\SI{1.1}{\second}$ to $\SI{1.25}{\second}$. As such, while individual level 2 \textit{(model + feature set)} ensemble predictions are worth evaluating in scenarios with fast dynamics, these predictions are likely not as robust. Also, they come with a stronger requirement on signal availability--e.g., the FNOLSTM-$\ast\ast$-9 setting could only be applied to 16 out of 34 test discharges due to missing inputs.
\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/shots/IBL_64770_both.pdf}\end{center}
    \caption{Results on \#64770, an IBL scenario development discharge. The full discharge, predictions and signals are plotted at the top. At the bottom, we zoom in on a region with several fast back transitions. Here, we evaluate three sets of predictions: the ensemble of all models, an ensemble of all FNOLSTM models, and the FNOLSTM-$\ast\ast$-9 setting. All models show confidence spikes around the transient events. The dynamic models (bottom two) tend to perform better on these fast transients, albeit with noisier confidence estimates.}
    \label{fig:s_ibl}%
\end{figure}

\textbf{Alternative scenarios.} Next, we test on discharges from underrepresented scenarios to evaluate challenging circumstances. First, we consider two shots for scenario development of the quasi-continuous exhaust regime~\cite{labit2019}, see Figure~\ref{fig:s_qce}. Shot \#78069 successfully reached the desired small ELM regime, with its time window accurately predicted by the ensemble. In discharge \#83049 it is not as clear, with more ELMy and dithering regions. We plot results for the full ensemble and the FNOLSTM ensemble, illustrating that with more transient behavior using only the dynamic formulation tends to give more precise results. Additionally, we test two negative triangularity discharges~\cite{coda2022}, see Figure~\ref{fig:s_nt} for plots. The ensemble is not sensitive to the non-standard configuration and accurately predicts L-mode for the entire shots' durations. 

\begin{figure}[h]
\begin{center}\includegraphics[width=1\linewidth]{figures/shots/QCE_78069.pdf}\\\includegraphics[width=1\linewidth]{figures/shots/QCE_83049.pdf}\end{center}
    \caption{Results for two quasi-continuous exhaust scenario development discharges, a successful and a borderline example. For \#78069 we show results for the full ensemble, for \#83049 we show results for both the full ensemble and the FNOLSTM-only ensemble. In the setting of fast transients, the dynamic formulation-only ensembles tend to be more robust.}
    \label{fig:s_qce}%
\end{figure}

\subsection{Extrapolation and robustness}
\label{sec:extrapolation_robustness}
\textbf{Out-of-distribution regimes.} To test the ensemble in an out-of-distribution setting, we filtered on shots with an average $\beta_N > 1.7$ and $\delta_{\text{top}} > 0.3$ for a phase of \SI{100}{\milli\second} and removed them from the `train-validation' set, ensuring we do not train on these conditions. Predictions for two shots from this set are given in Figure~\ref{fig:s_betan}. In general, the ensemble still performs well in these conditions, with the exception of mislabeling a dithering region in \#69514 around \SI{0.4}{\second} to \SI{0.7}{\second}, albeit with a low confidence score. In general, the results on this set did not seem significantly different from the remaining test-set discharges. It should be noted that while no discharges with the specified condition were used for training, it is likely that similar discharges were still present in the training data: we are not necessarily evaluating a completely novel scenario.

\begin{figure}[h]
\begin{center}\includegraphics[width=1\linewidth]{figures/shots/OOD_64686.pdf}\\\includegraphics[width=1\linewidth]{figures/shots/OOD_69514.pdf}\end{center}
    \caption{Results for two discharges with out-of-distribution conditions. Specifically, we removed discharges with phases with an average $\beta_N > 1.7$ and $\delta_{\text{top}} > 0.3$ for at least \SI{100}{\milli\second} from the `train-validation' set. In general the ensemble still performs well, with the exception of mislabeling the starting time of a long period of dithering.}
    \label{fig:s_betan}%
\end{figure}

\begin{figure}[t]
\begin{center}\includegraphics[width=1\linewidth]{figures/shots/PD_57013_baseline.pdf}\\\includegraphics[width=1\linewidth]{figures/shots/PD_57013_corrupted.pdf}\\\includegraphics[width=1\linewidth]{figures/shots/PD_57013_discarded.pdf}
\end{center}
    \caption{Evaluating the robustness of the ensembles in case of broken or missing signals, specifically for $\text{PD}_{\textit{CIII}}^{}$ measurements. At the top we show the reference discharge \#57013 with the full ensemble predictions. In the middle we show result of a saturated $\text{PD}_{\textit{CIII}}^{}$, a failure mode when inadequate gains are set during operation. Here, the dynamic formulation struggles the most, in contrast to earlier results: including multiple formulations noticeably increases performance. At the bottom we consider the case of removing models using $\text{PD}_{\textit{CIII}}^{}$ from the ensembles: here, the models mostly recover, although with less accuracy compared to the unaffected setting at the top.}
    \label{fig:pdsat}%
\end{figure}


\textbf{Broken or missing signals.} To evaluate the robustness of the ensembling method in more detail, we consider the case of a faulty or missing $\text{PD}_{\textit{CIII}}^{}$ signal. The $\text{PD}_{\textit{CIII}}^{}$ emissions are a key indicator of the confinement state due to its line-of-sight crossing the divertor region: emission patterns such as ELMs leave clear signatures. It is used directly in the dynamic-formulation models, and by both model types through derived spectral features as $\text{PD}_{\text{FFT}}^{}$. The baseline of no errors is given in Figure~\ref{fig:pdsat} (top). Here, the ensemble accurately matches the expert labels. In Figure~\ref{fig:pdsat} (middle) we show the results when $\text{PD}_{\textit{CIII}}^{}$ is saturated, an error mode caused by the diagnostic system's gain being set too large. In contrast to previous examples, the FNOLSTM ensemble now shows the worst results, evidently having a larger dependence on this signal. The GBDT ensemble loses accuracy around the transitions but still provides accurate predictions with high confidence in the stable phases. As a consequence, the full ensemble relies more on the GBDT predictions and still predicts the main phases correctly, highlighting the benefits of the multiple model formulation approach in the full ensemble. Lastly, we consider the case of discarding the signal entirely, i.e., discarding all models utilizing it, in Figure~\ref{fig:pdsat} (bottom). Here, all ensembles mostly recover, although they still lack in precision around the transitions. We note that in this shot there are also small fringe jumps--incorrect interferometer measurements caused by an error in determining the phase difference~\cite{murari2006}--present in the electron core density signal $n_{e,\text{core}}$. They do not seem to significantly affect predictions, although we do see some spikes around the confidence coinciding with the fringe jump times, e.g. \SI{1.0}{\second} for the FNOLSTM in the middle plot. %

\begin{figure}[t]
\begin{center}\includegraphics[width=.9\linewidth]{figures/LHHL_CA.pdf}\end{center}
    \caption{Conditionally averaged prediction confidence around L-H and H-L transitions, for a sample of 9 transitions each.}
    \label{fig:lh_hl}%
\end{figure}
\begin{figure}[t]
\begin{center}\includegraphics[width=.9\linewidth]{figures/NBI_LH_CA.pdf}\end{center}
    \caption{Conditionally averaged prediction confidence around L-H transitions with steps or ramps in the NBI, for a sample of 9 transitions each.}
    \label{fig:nbi_ramp}%
\end{figure}

\subsection{Confidence evaluation around state transitions}\label{sec:conditional_average}
For a more statistically robust evaluation of the qualitative behavior of the confidence predictions, we conditionally average the full-ensemble confidences around different types of transitions. Specifically, we compare L-H and H-L transitions, and compare L-H transitions with steps in the NBI power to L-H transitions with more gradual ramps in the input power.

\textbf{L-H vs. H-L transition.} The conditionally averaged model confidences for L-H versus H-L transitions are given in Figure~\ref{fig:lh_hl}. We consider a time window of \SI{150}{\milli\second} around the transition, using L-H and H-L transition pairs from the same shots to minimize other variations in the plasma configuration. There are clear differences in the behavior of the confidence between the two cases. One possible explanation could be that the L-H transition is often more controlled: the model confidences hint at a more gradual phase, and a clear H-mode right after the transition. In contrast, the H-L back transition could be more sudden and uncontrolled, leading to high confidence up to the moment of transition as there are no signs of the transition approaching, along with more uncertainty about the plasma state right after it occurred. Further evaluating these differences in confidence behavior is an interesting avenue for future work.




\textbf{L-H transition with NBI steps vs. NBI ramps.} The conditionally averaged model confidences for steps in the NBI power, compared to ramps in the NBI power, are provided in Figure~\ref{fig:nbi_ramp}. We select 9 shots for each type, with NBI steps denoting a time of at most \SI{20}{\milli\second} between the minimum and maximum power, and ramps a gradual increase for a time window of at least \SI{150}{\milli\second}. The behavior of the confidence reflects the different input power dynamics: the discharges with a step increase show a more sudden drop compared to the ramped increases. There is more model uncertainty around the transition time, with slower, more marginal transitions, consistent with expectations.


