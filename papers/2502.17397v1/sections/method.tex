\begin{figure*}[t]
\begin{center}\includegraphics[width=.94\linewidth]{figures/ensemble.pdf}\end{center}
    \caption{An overview of the ensemble structure and prediction procedure. The structure is defined in a top-down fashion. First, we ensemble over different models and feature sets (level 1). Each \textit{(model + feature set)} combination consists of a small ensemble of models fit on different folds of the data (level 2). At the bottom, we consider individual models (level 3). Prediction is done in a bottom-up fashion. Individual models map the input signals to a prediction of being in L, D or H (level 3). These predictions are combined and re-scaled on the level of a \textit{(model + feature set)} (level 2). The resulting predictions are combined weighted by their confidences and a constant that is computed using a \textit{(model + feature set)}'s average classification performance, giving the full ensemble prediction (level 1).
    }\label{fig:ensemble}%
\end{figure*}


The overarching goal of our approach is to maximize classification performance under two constraints: (1) providing meaningful, ideally calibrated, uncertainties, and (2) being robust to missing or corrupted input signals. To do so, we propose ensembling a set of models on two axes: different model formulations and different feature sets. In this section we discuss this setup and the components in detail; an overview of the method is provided in Figure~\ref{fig:ensemble}.


\subsection{Structure}\label{ss:components}
We describe our ensembling procedure in a top down manner using three `levels' in a hierarchy. This structure is depicted in Figure~\ref{fig:ensemble} on the left. The three levels are as follows:
\begin{enumerate}[label=(\arabic*)]
    \item Ensembling over two axes: model formulations and feature sets. A single element of the ensemble is defined as a \textit{(model + feature set)}.
    \item For each \textit{(model + feature set)}, we ensemble over different train and validation splits (folds). The splits are chosen to have no overlap in the validation sets w.r.t. experimental topics to encourage variety and generalization. 
    \item The lowest level describes a single model, with a single feature set, fit on a single fold of the dataset.
\end{enumerate}
The ensembling steps on levels 1 and 2 serve the function of providing variety in the model behavior, by altering the formulations, feature sets and data splits. Generally, such an approach contributes to better uncertainty estimates. Intuitively, if diverse models agree on a label, the prediction is unlikely to be an artifact of an individual model, allowing us to assign higher confidence, see e.g.~\cite{lakshminarayanan2017,rahaman2021uncertainty} for more details.

Additionally, the use of different feature sets addresses the point of robustness: by having models trained on many different subgroups of features, the method naturally becomes resilient to corrupted signals since they are only present in a subset of the predictors, and any given feature never appears in all feature sets. This advantage extends to dealing with missing signals for models that require all inputs to be present: since not all models utilize all signals, we can still use a subset of models when some signals are missing. Effectively, we can utilize many signals if they are available, but we do not require them.

\subsection{Components}\label{sec:components}
The axes of variation in our approach are the feature sets and the model inductive biases. In this section, we discuss the construction of these feature sets, and describe the models in detail. Specifically, for the latter, we consider a dynamic and static formulation, based on neural networks (FNOLSTM), and tree ensembles (GBDT), respectively.

\textbf{Feature sets.}
We employ the features introduced before in Table~\ref{tab:signals}. The individual feature sets are constructed either by taking features only from one category, or by mixing features from all categories. When taking a subset of a category, we first order them by their individual discriminative power, which is computed by fitting simple models on single features, see~\ref{ap:feature_sets} for more details. For example, if we only take two features from category `Shaping', we take the two most informative according to a precomputed metric, rather than picking them arbitrarily. 

Given this scheme, we construct two groups for each category: one with the top-$k$ features, and one with all features. Additionally, we construct various groups mixing all categories, taking the top-1, top-2, etc.; see~\ref{ap:feature_sets} for all \textit{(model + feature set)} combinations. 

The main motivation for this approach is twofold. For one, by constructing sets in an informed manner, we can fit multiple models covering the main aspects of the plasma whilst having little overlap in their inputs. Ideally, this strategy gives us distinct models with good performance,  reducing the sensitivity to single features. Additionally, having models cover a single category provides a degree of interpretability, given that we can inspect individual model predictions.

\textbf{Dynamic model formulation: FNOLSTM.}
First, we consider the problem in a dynamic formulation. The model maps an input sequence of signal data, up to a given timestep $t_m$, to the label at time $t_{m-k}$, i.e., a small offset $k$ before the last input. This formulation can be expressed as follows:
\begin{align}
f_{\text{dynamic}}: \mathbf{x}^{\mathbf{u},\mathbf{t}_{\leq m}} \in \mathbb{R}^{\textit{U} \times m}\rightarrow \mathbf{y}^{\mathbf{s},t_{m-k}}\in \mathbb{R}^{3},
\label{eq:fdynamic}
\end{align}
with the same notation as Equation~\ref{eq:formulation}, and $f_{\text{dynamic}}$ denotes the input-output map we learn. Intuitively, we assume knowledge of all signal data up to time $t_m$, and provide predictions with a lag of $k$ timesteps.

To implement $f_{\text{dynamic}}$ we use artificial neural networks (NNs) to best exploit the potentially subtle information carried by the input dynamics. Previous works have shown success on confinement state classification with NNs~\cite{mathews2019,zorek2022,orozco2022,gill2024,yang2024,he2024,shin2020,meakins2010,matoslhd2020,matoslhd2021}. We build upon the general principle of feature extraction on small timescales using convolution-like methods and on large timescales using recurrent methods.

We use the Fourier Neural Operator (FNO)~\cite{li2021} on small input windows of signals. The FNO combines a linear, global integral operator with non-linear, local activation functions, which has proven highly successful on modeling the dynamics of various physical processes~\cite{wen2022,thorsten2023,Poels2023neuralPDE,Gopakumar2024PlasmaSurrogate}. Specifically, the FNO performs a Fast Fourier Transform (FFT)~\cite{cooley1965} of the input signals, in our case along the time axis, after which we perform a matrix multiplication on the spectral coefficients. The result is transformed back and summed to a point-wise transformation of the grid. This procedure can be expressed as follows:
\begin{align}
    \textit{FNO}^l: \mathbf{z}^l = \sigma\big(\text{FFT}^{-1}(\mathbf{R}^l\text{FFT}(\mathbf{z}^{l-1})) + \mathbf{W}^l \mathbf{z}^{l-1}\big),
    \label{eq:fno}
\end{align}
mapping an input (multidimensional) signal at layer $l-1$ ($\mathbf{z}^{l-1}$) to the output at layer $l$ ($\mathbf{z}^{l}$). The learned weight matrices are denoted as $\mathbf{R}^l \in \mathbb{R}^{D \times D \times M}$ and $\mathbf{W}^l \in \mathbb{R}^{D \times D}$, for $D$ hidden dimensions and $M$ fourier modes, with non-linear activation function $\sigma$.

\begin{figure*}[t]
\begin{center}\includegraphics[width=.7\linewidth]{figures/models_overview.pdf}\end{center}
    \caption{A simplified illustration of the two modeling approaches, a static formulation operating on timeslices of signals (top) and a dynamic formulation operating on sequences of signals (bottom). \textbf{Static (GBDT):} The static formulation is implemented with gradient boosted decision trees. A collection of small decision trees is fit in a sequential manner: each tree is fit using the residual errors of the previously fit trees. These trees operate on a vector of signal data corresponding to a single timeslice of the discharge. The final prediction is computed using a weighted combination of all individual trees. \textbf{Dynamic (FNOLSTM):} The dynamic formulation is implemented through a neural network extracting features on both small and large timescales. The input consists of a matrix corresponding to a small time window of signal data. This input is transformed with the FNO, a convolution-like operator acting primarily in the frequency domain. This computed abstract representation is processed in a recurrent manner over the entire duration of the shot with an LSTM. The final prediction is then computed using this local and global representation of the input signals.}
    \label{fig:fnolstm}%
\end{figure*}

To capture dynamics on longer time scales, we combine the local feature extractor with a recurrent architecture that operates on sequences of arbitrary length. We utilize the Long Short Term Memory (LSTM)~\cite{hochreiter1997} architecture, which can be summarized as follows:
\begin{align}
    \textit{LSTM}: \mathbf{h}_{t_m}, \mathbf{c}_{t_m} = f_{\textit{LSTM}}(\mathbf{z}_{t_m},\mathbf{h}_{t_{m-1}},\mathbf{c}_{t_{m-1}}),
    \label{eq:lstm}
\end{align}
where $f_{\textit{LSTM}}$ denotes the learned recurrent layer, $\mathbf{h}_{t_m}$ and $\mathbf{c}_{t_m}$ the hidden state components of the LSTM unit, and $\mathbf{z}_{t_m}$ the input of the recurrent layer, that is, the output of Equation~\ref{eq:fno}. Note the temporal connection of the LSTM: it takes as input the previous hidden state ($\mathbf{h}_{t_{m-1}}, \mathbf{c}_{t_{m-1}}$) along with the current signal information $\mathbf{z}_{t_m}$ to compute the outputs at time $t_m$.

The output of the LSTM is passed through a small set of fully connected neural network layers, i.e. a small Multi-Layer Perceptron (MLP)~\cite{ivakhnenko1971}. The resulting value is subsequently mapped to probabilities using the Softmax function, giving us final prediction $\mathbf{y}$. Additionally, we add a skip connection~\cite{he2016} between the local feature extractor and the input of the MLP. In certain instances the input time window in isolation already sufficiently describes the plasma state: the skip connection removes the potential bottleneck of the hidden state.

Lastly, we discuss the specifics of the inputs and outputs. The input at time $t_m$ is a time window ending at time $t_m$ of size $w$ timesteps, that is, covering the interval $(t_{m-w}, t_m]$, or denoted as input matrix $\mathbf{x}_{t_{m-w}:t_m} \in \mathbb{R}^{U \times w}$. The model operates on a stride of $p$: we predict every $p$ timesteps. Finally, the prediction is given with a lag of $k$ timesteps in order to maximize accuracy---in an online setting, one could imagine minimizing this delay $k$, or setting it to $k=0$ for no latency. Altogether, this process is described as follows:
\begin{align}
\mathbf{z}_{t_m} &= \textit{FNO}(\mathbf{x}_{t_{m-w}:t_m}), \\
\mathbf{h}_{t_m}, \mathbf{c}_{t_m} &= \textit{LSTM}(\mathbf{z}_{t_m}, \mathbf{h}_{t_{m-p}}, \mathbf{c}_{t_{m-p}}),\\
f_{\text{dynamic}} : \mathbf{y}^{\mathbf{s},t_{m-k}} &= \text{Softmax}\big(\textit{MLP} \big(\mathbf{z}_{t_m} + \mathbf{h}_{t_m}\big)\big),
\label{eq:fdynamic_fnolstm}
\end{align}
for the dynamic model $f_{\text{dynamic}}$, and $\mathbf{x}$ and $\mathbf{y}$ as in Equation~\ref{eq:fdynamic}. A simplified illustration is provided in Figure~\ref{fig:fnolstm} (bottom).

\textbf{Static model formulation: GBDT.}
Secondly, we consider the problem in a static formulation. Here, the model maps inputs at a given timestep $t_m$ to the corresponding label at this time, i.e.,
\begin{align}
f_{\text{static}}: \mathbf{x}^{\mathbf{u},t_m} \in \mathbb{R}^{\textit{U}}\rightarrow \mathbf{y}^{\mathbf{s},t_m}\in \mathbb{R}^{3},
\label{eq:fstatic}
\end{align}
with the same notation as Equation~\ref{eq:formulation}, and $f_{\text{static}}$ denoting the input-output map we learn.

We implement $f_{\text{static}}$ using gradient boosted decision trees (GBDT)~\cite{friedman2001greedy}, a machine learning method that builds an ensemble of small decision trees in a sequential manner. GBDTs have shown strong performance on tabular data~\cite{vadim2024,kit2023}, making them well suited for the static formulation.

Each individual tree splits an input sample iteratively based on feature thresholds up to a leaf node holding the prediction value. A benefit of this formulation is the ability to deal with missing signals by simply choosing a default direction on a split node. GBDTs build an ensemble of decision trees in a sequential manner, by fitting individual trees using residual errors of previous trees w.r.t. the train dataset. This iterative process is carried out through gradient boosting with XGBoost~\cite{xgboost2016}, i.e., the residuals are computed as the gradient of a specified loss function.

In practice, we fit an ensemble of trees for each {$\text{class} \in \{L, D, H\}$}. The final prediction for the GBDT model is obtained by normalizing over the three ensemble predictions using Softmax. This entire process is described as follows:
\begin{align}
\begin{split}
    f_{\text{static}}: \mathbf{y}^{\mathbf{s},t_m} = \text{Softmax}\big(&\sum_{k=1}^K f_{c,k} (\mathbf{x}^{\mathbf{u},t_m}) \\
    &{\text{ for } c \in \{L, D, H\}}\big),
\end{split}
\label{eq:fstatic_gbdt}
\end{align}
where $K$ denotes the number of trees in an individual class' ensemble, $f_{c,k}$ an individual decision tree; $\mathbf{x}$ and $\mathbf{y}$ as in Equation~\ref{eq:formulation}. A simplified illustration of this process is given in Figure~\ref{fig:fnolstm} (top).

\subsection{Fitting Procedure}
We describe the fitting procedure in a bottom-up manner, starting from the individual models up to the full ensemble (i.e. from level 3 to 1). An overview of this process is depicted in Figure~\ref{fig:ensemble} (right). The dataset is split into a subset for fitting the individual models (`train-validation'), a subset for fitting ensembling parameters (`ensemble-holdout'), and the test set used in Section~\ref{sec:experiments}. Since the method should be applicable to novel scenarios, we choose the splits to minimize overlap in the contained missions. This approach ensures we evaluate on sufficiently different scenarios, avoiding information leakage.

\textbf{Individual model training.}
Each individual \textit{(model + feature set)} consists of a small ensemble that shares the same model hyperparameters (level 2). Each element of this ensemble, an individual model (level 3), is fit using a different split of the `train-validation' set, which is referred to as a fold. See Figure~\ref{fig:ensemble} for a visualization.

For each fold, we optimize a separate set of model parameters using the confinement state labels. For both the FNOLSTM and the GBDT-based models, the loss is categorical cross-entropy between the model outputs and the ground-truth labels. For the FNOLSTM, given its sequential nature, we optimize using subsequences of discharges, which are sampled to ensure a balance in the output labels, see~\ref{ap:model_training} for more details. For the GBDT, given its static nature, we sample individual timeslices. Lower-frequency states are oversampled for better balance, and periods around state transitions are oversampled to ensure we capture their dynamics, see~\ref{ap:model_training} for details.

\textbf{Ensembling procedure.}
The prediction procedure can be split into three steps. First, we predict individually with each model (level 3), i.e. Equation~\ref{eq:formulation}. These predictions are consequently averaged at the level of the \textit{(model + feature set)}, level 2. This mini-ensemble is calibrated with the `ensemble-holdout' set using temperature scaling~\cite{guo2017}, which is defined as follows:
\begin{align}
f_{2}^* &= \frac{1}{N} \sum_{i=1}^{N} f_{3_i}, \\
f_2 &= \text{Softmax}(f^{*}_{2}/T),
\end{align}
for level-2 ensemble $f_2$, taking as input $N$ individual model outputs $f_{3_i}$, and $T$ denoting the fitted temperature parameter. Prior work shows that this pool-then-calibrate approach generally results in the lowest calibration error~\cite{rahaman2021uncertainty}. 

The level 1 prediction, the final output, consists of a linear combination of all individually calibrated level 2 outputs. Each output is weighted by a constant weight, denoted as $C_{i,j}$, for $i \in models$ and $j \in features$. These constants are determined using the classification performance. Specifically, we compute the Cohen's kappa coefficient~\cite{cohen1960} (see Section~\ref{sec:metrics}) for more information), which we normalize over all the different models, and then square the results. This rescaling puts the constants $C_{i,j} \in [0, 1]$, while placing more emphasis on the best performing models. The final prediction then sums over all level 2 predictions, scaled by these constants and the prediction confidences (Equation~\ref{eq:confidence}), and is subsequently normalized such that the total sums to 1:

\begin{align}
\begin{split}
f&_{1} = \\
&\text{normalize}\Big( \sum_{\substack{i \in models \\ j \in features}} C_{i,j} \cdot \textit{confidence}_{i,j} \cdot f_{2_{i,j}}\Big),
\end{split}
\end{align}
for full ensemble $f_1$. An overview of the full prediction procedure is provided in Figure~\ref{fig:ensemble} (right).




















