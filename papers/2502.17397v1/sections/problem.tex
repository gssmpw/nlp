In a tokamak discharge, operation starts in a state which does not display significant fusion performance, which we refer to as low (L) confinement mode. Most experiments subsequently aim at transitioning to a more performing state, high (H) confinement mode. The transition between these two states has been experimentally discovered on ASDEX~\cite{hmode1982} and since then extensive studies have been conducted to explain the reasons behind this switch and the physics mechanisms involved. Nonetheless, at present, no specific set of rules exists to automatically distinguish between the two states on a large scale. The situation is rendered more difficult by the presence of intermediate states, displaying a transitional nature that makes them particularly difficult to distinguish. These states often appear around state transitions, but can also appear within L or H phases. In this context, on TCV we often observe rapid oscillations which we refer to as the Dithering (D) state~\cite{martindither2004,tcvdither2024}. 

The identification of plasma confinement states is typically carried out on a shot by shot basis by an expert. The task consists of inspecting various diagnostic signals and looking for specific signatures; for example, a trace from Edge Localized Modes (ELMs) in H$\alpha$ emissions is a potential indicator of being in H-mode. Given the difficult and time consuming nature of this process, scaling it up to large datasets becomes challenging, highlighting the utility of an automated approach.

We define the problem setting of automated detection of the confinement state as learning a function $f$ that maps measured and computed plasma quantities to a prediction of the confinement state. This is described as follows:
\begin{align}
f: \mathbf{x}^{\mathbf{u},\mathbf{t}^{\textit{in}}} \in \mathbb{R}^{\textit{U} \times \textit{T}^{\textit{in}}} \rightarrow \mathbf{y}^{\mathbf{s},\mathbf{t}^{\textit{out}}} \in \mathbb{R}^{3 \times \textit{T}^{\textit{out}}}, 
\label{eq:formulation}
\end{align}
where $\mathbf{x}^{\mathbf{u},\mathbf{t}^{\textit{in}}}$ represents the input matrix with $\textit{U}$ input signals and $\textit{T}^{\textit{in}}$ time samples, and $\mathbf{y}^{\mathbf{s},\mathbf{t}^{\textit{out}}}$ represents the output prediction matrix of states $\mathbf{s} = \{L, D, H\}$ for $\textit{T}^{\textit{out}}$ time samples. Note that $\textit{T}^{\textit{in}}$ and $\textit{T}^{\textit{out}}$ need not be the same; in practice we do not predict at each measurement for efficiency reasons, i.e. $\textit{T}^{\textit{out}} < \textit{T}^{\textit{in}}$.

For each timestep, the output predictions sum to one and are nonnegative, i.e.
\begin{align}
    \forall_{t \in \mathbf{t}^{\textit{out}}}\textstyle\sum_{s\in \mathbf{s}}y^{s,t} = 1, \hspace{.5cm} \forall_{t \in \mathbf{t}^{\textit{out}},s \in \mathbf{s}} \ y^{s,t} > 0,
\end{align}
Consequently, we can interpret our function as the conditional distribution between the input measurements and the confinement state, $p(y|x)$. We then define our prediction confidence (i.e. certainty) as the probability of the maximum class:
\begin{align}
    \textit{confidence} = \max_{s \in \mathbf{s}}p(y = s | x),
    \label{eq:confidence}
\end{align}
This quantity should be calibrated, e.g., if it is 0.8, we expect---on a sufficiently large sample---a prediction accuracy of 80\%.

Additionally, we assume that not all input signals in $\mathbf{x}$ are always present and perfectly acquired. Over time there are periods where a diagnostic is not available, or instances where it failed to acquire correctly. It is crucial that the method is robust to these failure modes to ensure the applicability to as many experiments as possible.  %



