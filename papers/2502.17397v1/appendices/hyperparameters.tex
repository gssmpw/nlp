\subsection{FNOLSTM}
The NN model architecture consists of an FNO-based encoder, an LSTM unit and a small MLP for the final prediction, see also Equation~\ref{eq:fdynamic_fnolstm}. We use a fixed structure of layers and optimize the layer hyperparameters. Certain layer sizes are coupled to reduce the search space and to avoid extremely unbalanced models; for this, we introduce parameters FNO$_\text{feat}$, FNO$_\text{mode}$ and LSTM$_\text{feat}$. The architecture is as follows. The input consists of a time window of size $w$ for $\textit{Nu}$ input features, i.e. $\mathbf{x} \in \mathbb{R}^{w \times \textit{Nu}}$. This $\mathbf{x}$ is transformed by an FNO layer to FNO$_\text{feat}$ hidden features utilizing FNO$_\text{mode}$ modes and a ReLU\footnote{$\sigma(z) = \text{max}(0, z).$} nonlinearity. We follow with another FNO layer, mapping to $2 \cdot \text{FNO}_\text{feat}$ hidden features using FNO$_\text{mode}$ modes and a ReLU nonlinearity. We apply a dropout~\cite{dropout2014} of 0.5 and do max-pooling of 2 over the temporal axis. Then, we flatten the temporal axis and map with a linear layer to LSTM$_\text{feat}$ features followed by ReLU; this output of the temporal feature extractor is denoted as $z$. We transform $z$ using an LSTM\footnote{Also taking as input the previous LSTM's hidden state.} with a hidden size of LSTM$_\text{feat}$. The result is summed to $z$, i.e. the residual connection, and mapped to $\frac{1}{3}\text{LSTM}_\text{feat}$ features with a ReLU nonlinearity. We apply dropout, map to 3 features and apply softmax\footnote{$\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$.} function, the output prediction $\mathbf{y} \in \mathbb{R}^3$.




Non-architecture parameters we optimize are the time window size $w$ and the prediction offset $k$ (see Equation~\ref{eq:fdynamic_fnolstm}). Stride parameter $p$ is fixed to 10, which results in a prediction rate of \SI{1}{\kilo\hertz} given that the signals are (re)sampled at \SI{10}{\kilo\hertz}. All neural networks are optimized with schedule-free Adam~\cite{defazio2024} following the training procedure describe in~\ref{ap:model_training}; we optimize the learning rate and the number of warmup steps for the optimizer, and optimize the batch size\footnote{We keep the number of batches per epoch fixed despite the larger batch size to keep the total number of update steps fixed.} and the number of unrolling steps done for each sample during training.

\subsection{GBDT}
The GBDT models utilize the implementation provided by XGBoost~\cite{xgboost2016}. For each model we optimize the number of decision trees in the forest, the maximum depth of a single decision tree, and the learning rate of the gradient boosting algorithm. Additionally, we optimize the ratios of resampling the different classes. We always subsample L and H-mode timeslices to match the number of D timeslices; here, we optimize whether we do this rebalancing at a 1:1, 3:1 or 5:1 ratio for L and H w.r.t. D. Note that we always sample all timeslices around transition points regardless of class rebalancing, see also~\ref{ap:model_training}.

\subsection{Optimization procedure and parameter ranges}
All hyperparameters are optimized with Bayesian optimization through Optuna~\cite{optuna2019}, with the Cohen's kappa coefficient~\cite{cohen1960} on validation-set discharges as the target metric. They are shared on the level of the 4 folds used for each \textit{(model + feature set)} combination. Consequently, each hyperparameter trial considers training 4 models, with the resulting validation metrics averaged out. For each \textit{(model + feature set)} configuration we do 100 trials for FNOLSTM-based models, and 250 trials for GBDT-based models. The hyperparameter ranges for both model types are provided in Tables~\ref{tab:hyperparamnn} and~\ref{tab:hyperparamxgboost}, respectively. 

\input{figures/table_hyperparams_nn}

\input{figures/table_hyperparams_xgboost}
