\subsection{FNOLSTM}
The training procedure of FNOLSTM models is summarized as follows. At timestep $m$, for time window $w$, stride size $p$ and prediction offset $k$, the neural network maps the input window $\mathbf{x}_{t_{m-w}:t_m}$, combined with previous hidden state $(\mathbf{h}_{t_{m-p}}, \mathbf{c}_{t_{m-p}})$, to predictions $\mathbf{\widetilde{y}}^{\mathbf{s},t_{m-k}}$. We use categorical cross-entropy as the loss function between the predictions and ground truth values: 
\begin{align}
\mathcal{L}(\mathbf{y}^{\mathbf{s},t_{m-k}}, \mathbf{\widetilde{y}}^{\mathbf{s},t_{m-k}}) = -\sum_{s \in \mathbf{s}} y^{s,t_{m-k}} \log \hat{y}^{s,t_{m-k}},\label{eq:cce}
\end{align}
equivalent to minimizing the negative log-likelihood. We use standard gradient-based optimization as provided by PyTorch~\cite{paszke2019}; specifically, we use the schedule-free Adam optimizer~\cite{defazio2024}. 

One parameter update step consists of computing the loss between a mini-batch of signal timeseries and the corresponding confinement state labels. We iteratively predict confinement states as we slide over the input signal with stride size $p$ while continuously updating the hidden state $(\mathbf{h}_{t_{m-p}}, \mathbf{c}_{t_{m-p}})$. The number of these unrolling steps is a hyperparameter, see also~\ref{ap:parameters}. The loss computation and parameter update is done for the entire batch + unrolling at once, that is, applying backpropagation through time~\cite{werbos1988}.

We sample each element in the batch as a random \textit{(shot + initial timestep)} combination. To account for the imbalance in the different class labels, we precompute the total number of L, D and H labels for all \textit{(shot + initial timestep)} combinations given the chosen stride size, prediction offset, and number of unrolling steps. Then, rather than uniformly sampling the \textit{(shot + initial timestep)} inputs, we reweight the sampling probability based on the total probabilities of the L, D, and H labels across all elements to ensure that all confinement states are sampled equally often in expectation. We optimize for $\approx$100 batches per epoch for 50 epochs. After every 10 epochs we compute Cohen's kappa coefficient on the validation discharges and save the model with the minimum validation error.

\subsection{GBDT}
The training procedure of GBDT models is summarized as follows. At timestep $m$ we map input signals $\mathbf{x}^{\mathbf{u},t_m}$ to predictions $\mathbf{\widetilde{y}}^{\mathbf{s},t_m}$. As loss function we again use the categorical cross-entropy between the predictions and the reference labels $\mathbf{y}^{\mathbf{s},t_m}$, see Equation~\ref{eq:cce}. The random forest is build through gradient boosting using XGBoost~\cite{xgboost2016}. Trees are fit one at a time utilizing the full train set for each tree, with new trees minimizing the residual error w.r.t. the already-trained trees.

The samples cover \textit{(shot + timestep)} combinations in the training dataset. Specifically, we subsample timesteps with L or H labels to account for the class imbalance. We utilize various subsampling ratios, see~\ref{ap:parameters} for details. Additionally, regardless of the aforementioned subsampling, we always sample all 50 timesteps before and after each confinement state transition, to ensure these more difficult timeslices are always part of the dataset.
