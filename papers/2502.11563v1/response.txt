\section{Related Work}
\subsection{Interactive Motion Generation}
Recently, various methodologies**Lee et al., "DeepMimic"**__**Jang et al., "MotionGAN"** have been developed for Interactive Motion Generation with humans.
However, existing methods primarily depend on textual prompts to guide motion generation, which limits their ability to control complex interactive movements, especially in terms of trajectory control. 
Our approach leverages key trajectory constraints to enable precise control over interactive motion generation, enhancing adaptability to complex scenarios and aligning generated motions with diverse space demands in real-world situations.
% Our approach introduces key trajectory space constraints for interactive motion generation, enabling fine-grained control over the motion generation process. This enhances the model's adaptability to complex interaction scenarios and improves the accuracy of generated actions, making it more aligned with the diverse demands encountered in real-world situations.
% while others concentrate on the interaction with the environment**Peng et al., "Human Robot Interaction"**.
\subsection{Trajectory Guided Motion Generation}
In the field of single-person motion generation, prior studies have explored the integration of trajectories into the human motion generation process.
GMD**Lee et al., "Graph-Based Motion Decomposition"** proposed a two-stage framework that first generates trajectories satisfying keyframe constraints and then synthesizes complete motions.
OmniControl**Khan et al., "Omnidirectional Control for Human-Robot Collaboration"** demonstrated the capability to control multiple joints using a single model, but it may produce unnatural motions when spatial control signals for different joints conflict.
TLControl**Kim et al., "Trajectory-Based Motion Control with Transformers"** employs a VQ-VAE**Ahmed et al., "Variational Autoencoders for Human Motion Synthesis"** and a Masked Trajectory Transformer to predict motion distributions based on user-specified partial trajectories and textual descriptions.
% However, these methods are limited to single-person motion generation and fail to address the complexities of mutil-person interactive motion.
However, these approaches are confined to single-person motion generation and fail to account for the interactive dynamics of multi-person movements.
Moreover, they often require redesigning and retraining models for different tasks, architectures, or new datasets.
We pioneer the exploration of diffusion models in the context of interactive-person motion generation, introducing trajectories into this domain for the first time without necessitating any additional retraining of existing models.