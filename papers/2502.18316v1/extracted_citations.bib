@article{DiBattista03042014,
author = {David DiBattista, Jo-Anne Sinnige-Egger and Glenda Fortuna},
title = {The “None of the Above” Option in Multiple-Choice Testing: An Experimental Study},
journal = {The Journal of Experimental Education},
volume = {82},
number = {2},
pages = {168--183},
year = {2014},
publisher = {Routledge},
doi = {10.1080/00220973.2013.795127},
URL = {https://doi.org/10.1080/00220973.2013.795127}
}

@misc{balepur2024artifactsabductionllmsanswer,
      title={Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?}, 
      author={Nishant Balepur and Abhilasha Ravichander and Rachel Rudinger},
      year={2024},
      eprint={2402.12483},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.12483}, 
}

@misc{clark2018thinksolvedquestionanswering,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}

@article{dochy2001assessment,
  title={The assessment of quantitative problem-solving skills with “none of the above”-items (NOTA items)},
  author={Dochy, Filip and Moerkerke, George and De Corte, Erik and Segers, Mien},
  journal={European Journal of Psychology of Education},
  volume={16},
  pages={163--177},
  year={2001},
  publisher={Springer}
}

@inproceedings{ji-etal-2022-answer,
    title = "To Answer or Not To Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning",
    author = "Ji, Yunjie  and
      Chen, Liangyu  and
      Dou, Chenxiao  and
      Ma, Baochang  and
      Li, Xiangang",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.96/",
    doi = "10.18653/v1/2022.findings-naacl.96",
    pages = "1292--1300",
    abstract = "Machine Reading Comprehension with Unanswerable Questions is a difficult NLP task, challenged by the questions which can not be answered from passages. It is observed that subtle literal changes often make an answerable question unanswerable, however, most MRC models fail to recognize such changes. To address this problem, in this paper, we propose a span-based method of Contrastive Learning (spanCL) which explicitly contrast answerable questions with their answerable and unanswerable counterparts at the answer span level. With spanCL, MRC models are forced to perceive crucial semantic changes from slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL can improve baselines significantly, yielding 0.86 2.14 absolute EM improvements. Additional experiments also show that spanCL is an effective way to utilize generated questions."
}

@misc{li2024thinktwicetrustingselfdetection,
      title={Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection}, 
      author={Moxin Li and Wenjie Wang and Fuli Feng and Fengbin Zhu and Qifan Wang and Tat-Seng Chua},
      year={2024},
      eprint={2403.09972},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09972}, 
}

@article{little2015optimizing,
  title={Optimizing multiple-choice tests as tools for learning},
  author={Little, Jeri L and Bjork, Elizabeth Ligon},
  journal={Memory \& cognition},
  volume={43},
  pages={14--26},
  year={2015},
  publisher={Springer}
}

@article{little2019role,
  title={The role of retrieval in answering multiple-choice questions.},
  author={Little, Jeri L and Frickey, Elise A and Fung, Alexandra K},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={45},
  number={8},
  pages={1473},
  year={2019},
  publisher={American Psychological Association}
}

@article{little2023does,
  title={Does Using None-of-the-Above (NOTA) Hurt Students’ Confidence?},
  author={Little, Jeri L},
  journal={Journal of Intelligence},
  volume={11},
  number={8},
  pages={157},
  year={2023},
  publisher={MDPI}
}

@misc{myrzakhan2024openllmleaderboardmultichoiceopenstylequestions,
      title={Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena}, 
      author={Aidar Myrzakhan and Sondos Mahmoud Bsharat and Zhiqiang Shen},
      year={2024},
      eprint={2406.07545},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07545}, 
}

@article{odegard2007none,
  title={“None of the above” as a correct and incorrect alternative on a multiple-choice test: Implications for the testing effect},
  author={Odegard, Timothy N and Koen, Joshua D},
  journal={Memory},
  volume={15},
  number={8},
  pages={873--885},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{pezeshkpour-hruschka-2024-large,
    title = "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
    author = "Pezeshkpour, Pouya  and
      Hruschka, Estevam",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.130/",
    doi = "10.18653/v1/2024.findings-naacl.130",
    pages = "2006--2017",
    abstract = "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions{---}commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13{\%} to 85{\%} in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model`s bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks."
}

@misc{zheng2024largelanguagemodelsrobust,
      title={Large Language Models Are Not Robust Multiple Choice Selectors}, 
      author={Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
      year={2024},
      eprint={2309.03882},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.03882}, 
}

