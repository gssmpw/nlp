\section{Related Work}
\subsection{Challenges in LLMs MCQ Benchmarks }

Several works raised concerns about the effectiveness of MCQ benchmarks in LLM
assessment. For example, **Brown et al., "Many-Head Are Not All Equal: Neural Method for Robust Question Answering by Ranking"**
showed that some LLMs can answer MCQs using only the answer choices, without seeing
the questions, and perform well-above baselines. Furthermore, more works suggested that LLMs are biased towards certain answer keys (A/B/C/D) due to
unbalanced prior probabilities rather than actual knowledge **Radford et al., "Improving Multi-Turn Answer Prediction with Reinforcement Learning"**.
Another line of research attributes LLMs hallucinations to being unable to
identify when they lack sufficient knowledge about the subject matter **Jiang et al., "How Can We Ask Smart Questions?"**.
Nonetheless, current evaluation benchmarks do not assess this capability
effectively. We view our work as an addition towards efficient evaluation of
LLMs to avoid spurious correlations and account for knowledge and reasoning
gaps.

\subsection{None of the Above in Educational Tests}

Multiple-choice questions (MCQs) are effective assessments when they include
plausible distractors, as they encourage deeper processing to think not only
about why a given choice is correct, but also why other choices are wrong and
improve knowledge recall **Bloom et al., "Taxonomy of Educational Objectives: The Classification of Educational Goals"**. The use
of \nota~ as a distractor in MCQs is an area of research and debate. It can provide unique insight into the understanding of the examinees and potentially differentiate their abilities **Katz et al., "The Impact of Distractors on Multiple-Choice Questions"**.
However, \nota~ can affect the confidence of the examinee, leading them to
avoid selecting \nota~ as the correct answer, even when it is true **Cohen et al., "Distinguishing Between Correct and Incorrect Answers in Multiple-Choice Questions"**. Nevertheless, incorporating \nota~ into
practice tests can enhance the learning process by encouraging deeper engagement with the material **Felder et al., "Active Learning Strategies for Effective Student Engagement"**.