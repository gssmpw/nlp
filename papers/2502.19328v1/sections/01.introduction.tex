\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/figure1.pdf} %pdf
    \caption{An illustration of \textit{agentic reward modeling}.
    }
    \label{fig:fig1}
\end{figure}

Reward models (RMs) are designed to score the quality of responses and are typically used in the post-training of large language models (LLMs), such as RL~\citep{ouyang2022training} and DPO training~\citep{rafailov2024direct}, and in inference-time scaling laws~\citep{wu2024inference,snell2024scaling}, such as best-of-n search~\citep{brown2024large}. Reliable RMs are key to the success of modern LLMs.


Despite the success of reward models, existing RMs primarily focus on human preferences, which may be susceptible to subjective biases~\citep{saito2023verbosity,singhal2023long}, while neglecting verifiable correctness signals like factuality~\citep{liu2024rm, tan2024judgebench}. As illustrated in Figure~\ref{fig:fig1}, existing RMs may prefer the response \textit{A} due to its language style and longer length~\citep{singhal2023long}, overlooking factual errors and failure to follow instructions. This could affect the reliability of reward models and further influence the reliability of the trained LLMs~\citep{singhal2023long,chen2024odin}.
Conversely, verifiable correctness rewards exhibit notable potential in specific scenarios~\citep{guo2025deepseek}, providing a valuable complement to conventional reward models.



Based on the above considerations, we propose \textit{agentic reward modeling}, a reward system that combines reward models with verifiable correctness signals from different aspects to provide more reliable rewards. For example in Figure~\ref{fig:fig1}, a verification agent that specifically provides correctness signals, such as rule-based rewards~\citep{mu2024rule}, can be used to assess factual accuracy or verify adherence to instruction constraints. By integrating verifiable correctness rewards with human preferences, the reward system selects the superior response \textit{B}.
Agentic reward modeling enhances reliability through multi-dimensional correctness signals, enables flexible integration of diverse verification agents, and improves the interpretability of the final reward.


In this paper, we empirically implement a reward agent, named \ourmethod, which integrates the conventional human preference-based reward models with correctness signals from two key aspects: (1) factuality, which assesses the factual correctness of the claimed facts in the response, and (2) instruction-following, which evaluates whether the response adheres to the hard constraints in the instruction~\citep{zhou2023instruction}, such as length constraints, which significantly impact user experience in real-world applications~\citep{sun2024conifer, qi2024constraint}.
The architecture of \ourmethod is shown in Figure~\ref{fig:fig1}, consisting of three main modules: (1) \textit{Router}, which analyzes the instruction to determine the appropriate verification agents to invoke. (2) \textit{Verification agents}, which evaluate the correctness of response in different aspects, including factuality and instruction-following. Specifically, for factuality, we design a verification agent that efficiently evaluates factual correctness compared to the previous factuality evaluation framework~\citep{min2023factscore} through a process including pairwise comparison, query generation, evidence generation, and verification, where evidence generation 
can utilize either a search engine or the model's parametric knowledge. For instruction-following, we design a verification agent that extracts hard constraints, generates constraint checker code, and executes the code for verification, where the constraint checker is the Python code script to verify whether a given response satisfies a specific hard constraint.
(3) \textit{Judger}, which integrates the correctness signals from the verification agents and human preference scores from the reward models to provide an overall reward score.
We adopt ArmoRM~\citep{wang2024interpretable} as the reward model for computing human preference scores in \ourmethod. We use GPT-4o mini~\citep{OpenAI2024} and Llama3-8B Instruct~\citep{dubey2024llama} as the backbone LLMs for all the modules and implement \ourmethodmini and \ourmethodllama, respectively, except that in \ourmethodllama, the LLM backbone of the instruction-following agent is Qwen2.5-Coder 7B~\citep{hui2024qwen2}. 


We conduct extensive experiments to validate the effectiveness of \ourmethod. First, we conduct an evaluation on several reward model benchmarks, including RM-Bench~\citep{liu2024rm} and JudgeBench~\citep{tan2024judgebench}, as they contain response pairs that involve factual correctness, and IFBench, which is newly constructed for instruction-following and contains $444$ instances, each of which includes an instruction with several hard constraints, a chosen response that satisfies all constraints, and a rejected response that violates some constraints. \ourmethod significantly outperforms other advanced reward models on these benchmarks. We further apply reward models in real-world downstream tasks, including inference-time best-of-n search and constructing training preference pairs.
We evaluate best-of-n search on factuality question answering dataset TriviaQA~\citep{joshi2017triviaqa} and instruction-following datasets, IFEval~\citep{zhou2023instruction} and CELLO~\citep{he2024can}. We adopt Llama3-8B Instruct and GPT-4o~\citep{OpenAI20244o} as policy models to generate $32$ responses for each instruction with $1.0$ sampling temperature. 
\ourmethod significantly outperforms the base reward model AromRM in the best-of-n search, suggesting its ability to select superior responses and unleash inference scaling laws.
Finally, we apply \ourmethod to construct training preference pairs and train an LLM using DPO~\citep{rafailov2024direct}.
Specifically, we construct training data from two sources: UltraFeedback~\citep{cui2024ultrafeedback} and on-policy data. We adopt Zephyr-7B~\citep{tunstall2023zephyr} as the policy model and train it using DPO. The LLM trained on \ourmethod-constructed data consistently outperforms those trained on AromRM annotations on several NLP benchmarks, which further demonstrates the effectiveness of \ourmethod. 
We encourage the community to explore more verifiable correctness signals to develop reliable reward systems for LLM development and alignment.