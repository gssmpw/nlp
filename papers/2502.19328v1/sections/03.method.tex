\section{\ourmethod}
\label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/figure2.pdf} %pdf
    \caption{The framework of \ourmethod, including three modules: Router, Verification Agents, and Judger.
    }
    \label{fig:pipeline}
\end{figure*}


In this work, we empirically implement a reward agent, named \ourmethod, which integrates the base human preference reward model with verifiable correctness signals from two key aspects: factuality, which assesses the correctness of claimed facts, and instruction-following, which evaluates whether the response satisfies the hard constraints specified in the instruction~\citep{zhou2023instruction}. 
Both aspects significantly impact reliability and user experience in real-world applications and are challenging to evaluate effectively with existing reward models~\citep{liu2024rm}.
This section introduces the overall model architecture (\cref{sec:model_archi}) and the specific modules (\Cref{sec:router,sec:scoring_agents,sec:judger}) of \ourmethod.


\subsection{Model Architecture}
\label{sec:model_archi}
Following the concept in Euqation~\ref{eq:eq1}, the overall architecture of \ourmethod is illustrated in Figure~\ref{fig:pipeline}, which consists of three main modules: (1) \textit{Router}, which analyzes the instruction and determines which agents to invoke, corresponding to $A_x$ in Equation~\ref{eq:eq1}. As different instructions may require evaluations of different aspects of responses, dynamically selecting verification agents helps reduce inference costs and mitigate potential cumulative errors.
(2) \textit{Verification agents}, which evaluate different aspects of response correctness. In our implementation, we design two agents for assessing factuality and instruction-following, both based on LLMs augmented with additional tools. 
(3) \textit{Judger}, which integrates the scores from the verification agents and human preferences from the base reward model to produce a final reward, corresponding to determining $\lambda$ and $w_i$ in Equation~\ref{eq:eq1}.
We will provide detailed descriptions in the following sections.


\subsection{Router}
\label{sec:router}
Given an instruction, the router analyzes its requirements to the response to select the appropriate verification agents. 
The router is powered by an existing LLM backbone. Specifically, we first manually provide a concise description for each verification agent, 
explaining its functionality and specifying the conditions for its usage. Then, we input the instruction with all agent descriptions into the LLM, prompting it to select appropriate verification agents for correctness assessment. More implementation details are placed in appendix~\ref{sec:app_method}.


\subsection{Verification Agents}
\label{sec:scoring_agents}
\paragraph{Factuality}
Previous studies have proposed various methods to evaluate the factuality of responses, such as FactScore~\citep{min2023factscore}, which can be directly used as a verification agent. However, these methods typically require extensive search engine queries to verify the correctness of each atomic fact, which is costly and inefficient for reward scoring. Intuitively, pairwise scoring based on only the differences between two responses can effectively reduce search engine queries and time costs. Therefore, we propose a pairwise factuality verification agent for efficiently evaluating the factual correctness of response pairs.
The agent is illustrated in Figure~\ref{fig:pipeline}, which consists of four main components:
(1) Difference proposal, which identifies key differences in claimed facts between two given responses.
(2) Query generation, which constructs queries based on the identified differences to retrieve evidence for distinguishing these differences.
(3) Evidence generation, which uses the generated queries to retrieve supporting evidence using either external search engines or parametric knowledge in LLMs.
(4) Verification, which assigns an integer score from $0$ to $1$ to each response, using the collected evidence and original responses as inputs.
The verification agent can effectively capture subtle factuality differences~\citep{jiang2023llm} between responses while significantly reducing inference-time costs by verifying only their differences rather than all claimed facts.
All modules are implemented using an LLM backbone. The implementation details are placed in appendix~\ref{sec:app_method}.



% N >2的情况。

\paragraph{Instruction-Following}
The evaluation of the instruction following primarily assesses the adherence to hard constraints~\citep{zhou2023instruction} specified in the instruction, such as length constraints. 
Typically, instruction-following constraints can be categorized into soft and hard constraints, where the former focuses on semantic aspects, such as language style, while the latter focuses on surface-form constraints, such as format, which can be objectively evaluated. For instruction-following, our verification agent focuses on hard constraints, which are difficult to evaluate with existing reward models but can be efficiently verified using external tools, such as Python code scripts.
The agent is shown in Figure~\ref{fig:pipeline}, including three components:
(1) Constraint parsing, which extracts hard constraints from the instruction.
(2) Code generation and refinement, which generates Python scripts used to check the adherence to the extracted constraints. The generated code takes the response as input and returns either $0$ or $1$, where $1$ indicates that the constraint is satisfied, and $0$ otherwise.
We also incorporate a refinement step like \citet{madaan2024self} to correct invalid or syntactically incorrect code. Specifically, we execute the generated Python code using a Python interpreter, and if an error occurs, the error information and original code are fed back into the model to generate a refined code script.
(3) Verification, which executes the generated code in the Python interpreter to obtain a binary score ($0$ or $1$). The final score is the average of all hard constraint scores.
All the modules are implemented using LLMs. The specific prompts and implementation details are provided in appendix~\ref{sec:app_method}.



\subsection{Judger}
\label{sec:judger}
The judger integrates reward scores from verification agents and human preferences from base reward models. In our implementation, we use a weighted sum as the judger, where $\lambda$ and $w_i$ are all set to $1.0$, to compute the final reward score in Equation~\ref{eq:eq1}. One can also adopt different $\lambda$ and $w_i$ for better applicability in different scenarios. Additionally, the judger can dynamically adjust $\lambda$ and $w_i$ based on the instruction like gating network~\citep{wang2024interpretable}, we leave it as future work.

