\section{Preliminaries}
In the LLM domain, a reward model is typically a regression model that takes an instruction and a response as input and outputs a reward score~\citep{ouyang2022training}, which can be formulated as $r_{\text{RM}}(x,y)$, where $x$ denotes an instruction and $y$ represents a response. Reward models are typically trained on a large set of preference pairs based on the Bradley-Terry (BT) model~\citep{bradley1952rank}.


However, due to the subjectivity and complexity of human preferences and the capacity limitations of the BT model~\citep{munos2023nash, swamy2024minimaximalist, sun2024rethinking}, reward models often exhibit subjective bias, such as favoring longer and detailed outputs~\citep{saito2023verbosity}, while neglecting verifiable correctness signals like factuality~\citep{liu2024rm, tan2024judgebench}. On the other hand, training LLMs with verifiable correctness signals has shown strong potential~\citep{lambert2024t, guo2025deepseek}. Based on these considerations, we propose \textit{agentic reward modeling}, a reward system that integrates reward models with verifiable correctness signals from different aspects to provide more reliable rewards. Agentic reward modeling can be formulated as follows:
\begin{equation}
% \small
r(x, y) = \underbrace{\lambda \cdot r_{\text{RM}}(x, y)}_{\text{base reward}} + \sum_{i \in A_x} \underbrace{w_i \cdot a_i(x, y)}_{\text{correctness signals}}
\label{eq:eq1}
\end{equation}
$\lambda$ denotes the weight of the base reward model. $a_i$ denotes a specific verification agent that provides verifiable correctness signals, such as rule-based rewards~\citep{mu2024rule}. $w_i$ denotes the corresponding weight for each verification agent, which can be set as a hyper-parameter or adaptive to the instruction. $A_x$ is an index subset of the complete set of verification agents $A$ and is determined based on the instruction $x$. Equation~\ref{eq:eq1} provides the fundamental concept of agentic reward modeling, which can be implemented in various ways to construct a reward agent and our implementation is in \cref{sec:method}.
