\section{Applications}
This section explores applying \ourmethod to inference-time search (\cref{sec:best_of_n}) and the training of LLMs (\cref{sec:dpo_train}) to further validate its effectiveness.


\subsection{Best-of-N Search}
\label{sec:best_of_n}

One important application of reward models is to conduct the inference-time search to find a better response~\citep{brown2024large,zhang2024generative}, which unleashes the inference-time scaling laws of LLMs~\citep{snell2024scaling, wu2024inference}. 
Therefore, we explore applying \ourmethod to the best-of-n search on downstream tasks. Specifically, we evaluate the best-of-n performance searched by \ourmethod on factuality question answering and constrained instruction following tasks.

\paragraph{Experimental Setup}
We conduct the best-of-n experiments on the factuality question answering dataset TriviaQA~\citep{joshi2017triviaqa}, and the instruction-following datasets IFEval~\citep{zhou2023instruction} and CELLO~\citep{he2024can}. We use Llama3-8B Instruct and GPT-4o as the policy models to generate $32$ responses for each instruction with $1.0$ sampling temperature. We perform best-of-n search using the base reward model ArmoRM~\citep{wang2024interpretable}, \ourmethodmini, and the oracle setting of \ourmethodmini.  The oracle setting refers to invoking the factuality verification agent on TriviaQA, and the instruction-following verification agent on IFEval and CELLO.



\paragraph{Experimental Results}
The results of the best-of-n experiments using Llama3-8B Instruct as the policy model are shown in Figure~\ref{fig:enter-label}. We can observe that \ourmethod significantly improves the best-of-n performance compared to using the base reward model ArmoRM, and the oracle setting further improves the results. 
It further validates the effectiveness of \ourmethod. 
The results using GPT-4o as the policy model are provided in appendix~\ref{sec:app_exp}, demonstrating the same trends and conclusions. We encourage the community to design more verification agents to unleash the inference scaling laws of LLMs across different scenarios.





\subsection{DPO Training}
\label{sec:dpo_train}
Reward models are primarily used to train LLMs using RL~\citep{ouyang2022training} or DPO~\citep{rafailov2024direct}. 
Considering RL training is resource-intensive, we explore employing \ourmethod to construct preference pairs for DPO training to validate its effectiveness in real-world applications.

\paragraph{Experimental Setup}
We construct two training datasets based on: (1) UltraFeedback~\citep{cui2024ultrafeedback}, where each instruction contains $4$ responses sampled from various LLMs. (2) on-policy, which contains $20,000$ instructions sampled from UltraFeedback and
each instruction contains $8$ responses sampled from the policy model itself with $1.0$ sampling temperature. We use reward models to score each response, taking the highest-scored response as the chosen one and the lowest as the rejected one to construct training pairs.
We adopt the zephyr-7b-sft-full~\citep{tunstall2023zephyr} model as the policy model to conduct DPO training because it is trained only using SFT~\citep{ouyang2022training}. We evaluate the DPO-trained LLMs on various NLP benchmarks, including MMLU~\citep{hendrycksmeasuring}, MMLU-Pro~\citep{wang2024mmlu}, TriviaQA~\citep{joshi2017triviaqa}, TruthfulQA~\citep{lin2022truthfulqa}, IFEval~\citep{zhou2023instruction}, CELLO~\citep{he2024can}, and MT-Bench~\citep{zheng2023judging}. More experimental details are provided in appendix~\ref{sec:app_exp}.



\paragraph{Experimental Results}

The experimental results are shown in Table~\ref{tab:dpo_results}. We can observe that LLMs trained with data constructed by \ourmethod generally outperform those trained with ArmoRM, especially on the factuality question answering and instruction-following datasets. The improvement is more significant in on-policy data.
Furthermore, models trained with \ourmethod-annotated data consistently outperform those trained on original UltraFeedback that is constructed with GPT-4. Notably, \ourmethodllama uses open-source Llama3-8B Instruct and Qwen2.5-Coder 7B as the LLM backbones, at a much lower cost than GPT-4.
The results further validate the effectiveness and applicability of \ourmethod. We believe using a more powerful LLM backbone in \ourmethod can achieve more advanced results and encourage the community to explore more advanced reward agents for better performance and reliability.
