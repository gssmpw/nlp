\section{Related Work}
Reward models are typically employed to score responses and are crucial to the success of modern LLMs. Since the emergence of RLHF~\citep{ouyang2022training}, numerous studies have focused on developing more advanced reward models to help train LLMs. The approaches mainly include designing model architectures~\citep{wang2024interpretable,dorka2024quantile,chen2025LDLRewardGemma} and utilizing more high-quality data or new training objectives~\citep{infly2024inf,yuan2024advancing,park2024offsetbias,liu2024skywork,cai2024internlm2,cao2024compass,lou2024uncertainty,litool2024,wang2024helpsteer2}. There are also various studies exploring using LLMs as generative reward models~\citep{zheng2023judging,mahan2024generative,skyworkcritic2024,cao2024compass,tan2024judgebench,yu2024self,alexandru2025atlaseleneminigeneral}.
Reward models are typically used for inference-time scaling laws~\citep{irvine2023rewarding,wu2024inference,snell2024scaling,brown2024large,xin2024deepseek} or for training, such as RL\citep{ouyang2022training} or DPO~\citep{rafailov2024direct}.

Despite the success of reward models, they primarily focus on human preferences, which may be susceptible to subjective biases or reward hacking~\citep{saito2023verbosity,singhal2023long,gao2023scaling,zhang2024lists,chen2024odin}. A notable limitation is \textit{verbosity bias}~\citep{saito2023verbosity}, where reward models tend to favor longer responses~\citep{singhal2023long, liu2024rm}. Additionally, some studies have shown that reward models may overlook correctness signals, such as factuality~\citep{lin2024flame, liu2024rm, tan2024judgebench}. These limitations affect the reliability of reward models, thereby impacting the performance of the trained LLMs~\citep{singhal2023long}.


Recently, several studies have shown that rule-based reward models or verifiable reward signals achieve impressive results in specific domains such as math~\citep{guo2025deepseek}, safety~\citep{mu2024rule}, instruction-following~\citep{lambert2024t}, medical~\citep{chen2024huatuogpt}, and finance~\citep{qian2025fino1}. The simplicity and advanced performance of rule-based reward models demonstrate significant potential for training LLMs, but it is still non-trivial to generalize to general domains.
In this paper, we explore combining human preferences from reward models with verifiable correctness signals to develop more reliable reward systems.
We believe that combining human preferences with verifiable correctness signals is a promising direction and encourage further research efforts in this area.
