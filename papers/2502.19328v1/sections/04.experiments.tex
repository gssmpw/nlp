\section{Experiments}
This section presents experiments on several reward model benchmarks, including experimental setup (\cref{sec:exp_setup}), results (\cref{sec:exp_result}), and analyses (\cref{sec:exp_analysis}).


\begin{table*}
    \centering
    \small
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{RM-Bench} & \multirow{2}{*}{JudgeBench} & \multicolumn{3}{c}{IFBench} & \multirow{2}{*}{Overall} \\
    \cmidrule{2-3} \cmidrule{5-7}
    & Normal & Hard & & Simple & Normal & Hard & \\
    \midrule
ArmoRM-Llama3-8B-v0.1 &$76.7$&$34.6$&$51.9$&$72.3$&$66.2$&$59.5$&$56.5$\\
INF-ORM-Llama3.1-70B &$77.5$&$25.1$&$59.1$&$78.7$&$69.2$&$53.8$&$55.7$\\
Skywork-Reward-Llama-3.1-8B-v0.2 &$78.0$&$31.8$&$57.8$&$78.7$&$69.2$&$59.8$&$58.1$\\
Skywork-Reward-Gemma-2-27B &$82.7$&$35.1$&$55.8$&$\boldsymbol{87.2}$&$68.4$&$56.1$&$59.2$\\
internlm2-7b-reward &$72.6$&$19.9$&$56.2$&$74.5$&$61.7$&$55.7$&$52.0$\\
internlm2-20b-reward &$74.4$&$26.1$&$61.7$&$74.5$&$68.4$&$58.7$&$56.4$\\
\midrule
GPT-4o &$71.4$&$27.9$&$64.6$&$\underline{85.1}$&$66.2$&$54.4$&$56.3$\\
GPT-4o mini &$60.5$&$15.0$&$51.9$&$70.2$&$59.4$&$51.9$&$45.9$\\
o3-mini &$76.0$&$38.6$&$66.6$&$81.9$&$\underline{76.3}$&$64.6$&$62.8$\\
Llama3-8B Instruct &$\phantom{0}9.3$&$20.2$&$\phantom{0}2.6$&$12.8$&$12.8$&$13.6$&$11.3$\\
DeepSeek-R1 &$83.7$&$50.1$&$\boldsymbol{74.4}$&$72.3$&$74.4$&$64.0$&$69.1$\\
DeepSeek-R1-Distill-Llama-8B &$42.1$&$56.8$&$47.7$&$53.2$&$55.6$&$54.2$&$50.3$\\
\midrule
\ourmethodllama &$79.3$&$53.5$&$52.9$&$70.2$&$63.9$&$67.8$&$63.2$\\
\quad w/ search engine &$76.0$&$49.9$&$55.2$&$74.5$&$69.2$&$67.8$&$62.5$\\
\ourmethodmini &$\boldsymbol{86.0}$&$\boldsymbol{60.2}$&$\underline{68.2}$&$78.7$&$69.2$&$\boldsymbol{78.0}$&$\boldsymbol{72.5}$\\
\quad w/ search engine &$\underline{84.2}$&$\underline{59.7}$&$60.7$&$68.1$&$\boldsymbol{80.5}$&$\underline{76.1}$&$\underline{70.3}$\\
    \bottomrule
    \end{tabular}
    % }
    \caption{Experimental results (\%) of all investigated baselines and \ourmethod. The overall score is the average of RM-Bench, JudgeBench, and the micro-averaged score of three subsets of IFBench. By default, \ourmethod relies on its parametric knowledge, and ``w/ search engine'' denotes using Google API as an external source.}
    \label{tab:main_exp}
\end{table*}


\subsection{Experimental Setup}
\label{sec:exp_setup}

\paragraph{\ourmethod Implementation}
We adopt the advanced and lightweight ArmoRM~\citep{wang2024interpretable} as the base reward model to compute human preference scores. As \ourmethod is agnostic to reward models, one can also adopt other advanced reward models. 
We use GPT-4o mini~\citep{OpenAI2024} as the LLM backbone for implementing all modules and developing \ourmethodmini. We also employ the open-source LLM Llama3-8B Instruct~\citep{dubey2024llama} as the backbone and develop \ourmethodllama, except for the instruction-following verification agent, which requires strong coding capabilities and is instead powered by Qwen2.5-Coder 7B~\citep{hui2024qwen2}.
We adopt two knowledge sources for the factuality verification agent: an external search engine using Google API and the LLMâ€™s parametric parameters. More details are placed in appendix~\ref{sec:app_method}.



\paragraph{Evaluation Benchmarks}
Reward model benchmarks typically involve an instruction and a response pair and require selecting the better response as the chosen one.
We use RM-Bench~\citep{liu2024rm}, JudgeBench~\citep{tan2024judgebench}, and a new benchmark \ourdataset as evaluation benchmarks, as both RM-Bench and JudgeBench include response pairs involving factual correctness. We select the chat subset of RM-Bench as the evaluation set, using both the normal and hard settings. For JudgeBench, we use the knowledge subset as the evaluation set.
We further construct a new benchmark \ourdataset to evaluate reward models on selecting responses that better follow constraints in instructions as there is no existing relevant benchmark.
Specifically, we first construct instructions with several implicit constraints, integrating the constraint information with the primary task objective through paraphrasing. The constraints include both hard constraints, such as length, format, and keywords, as well as soft constraints, such as content and style. We then use GPT-4o to generate $8$ responses for each instruction with a sampling temperature of $1.0$. For each instruction, we create a response pair, selecting the one that satisfies all constraints as the chosen response and otherwise rejected. Based on the number of unsatisfied constraints (UC) in the rejected response, we split \ourdataset instances into three subsets: simple (\#UC$\geq$3), normal (\#UC$=$2), and hard (\#UC$=$1),  containing $47$, $133$, and $264$ instances respectively. 
We report the micro-averaged accuracy across the three subsets as the final metric for \ourdataset. More evaluation details on these benchmarks are provided in appendix~\ref{sec:app_exp}.



\paragraph{Baselines}
\looseness = -1
We mainly investigate two categories of baselines: (1) typical reward models, which are specifically trained for reward modeling and typically implemented as regression models to score each response and select the one with the highest reward score as the chosen response. We investigate several advanced and representative reward models, including ArmoRM~\citep{wang2024interpretable}, INF-ORM-Llama3.1-70B~\citep{infly2024inf}, Skywork-Reward~\citep{liu2024skywork}, internlm2 reward~\citep{cai2024internlm2}. (2) LLMs as generative reward models, where large language models serve as generative reward models to score responses or perform pairwise comparisons to select the best response~\citep{lambert2024rewardbench}. We evaluate proprietary models, including GPT-4o~\citep{OpenAI20244o}, GPT-4o mini~\citep{OpenAI2024}, o3-mini~\citep{openai2025o3mini}, and open-source LLMs, including Llama3-8B Instruct~\citep{dubey2024llama}, 
% {\color{red}Qwen2.5-Coder 7B~\citep{hui2024qwen2}}, 
DeepSeek-R1, and R1 distilled Llama3-8B model~\citep{guo2025deepseek}. 
We evaluate all the baselines using the code repository provided by \citet{lambert2024rewardbench}.


\subsection{Experimental Results}
\label{sec:exp_result}



Table~\ref{tab:main_exp} presents the experimental results, and we can observe that:
(1) Existing reward models fall short in selecting more factual responses or better adhering to hard constraints in instructions, which may limit their reliability in real-world applications.
(2) \ourmethod significantly outperforms the base reward model AromRM and the corresponding LLM backbone GPT-4o mini and Llama3-8B Instruct. It demonstrates that designing an appropriate reward agentic workflow can effectively enhance reward model performance.
(3) Even when using Llama3-8B Instruct as the LLM backbone, \ourmethodllama outperforms reward models with much more parameters and more advanced proprietary LLMs such as GPT-4o, which suggests that \ourmethod is more cost-efficient without requiring additional reward modeling training data or more parameters to achieve advanced performance.
(4) Using a search engine as an external knowledge source for factuality slightly reduces performance in RM-Bench and JudgeBench. One possible reason is that the retrieved information may contain noise or irrelevant information~\citep{chen2024benchmarking}. We leave the detailed analysis and design of retrieval-augmented agents for future work.
(5) \ourmethod achieves significant improvements on IFBench, particularly in the hard subset. It suggests that while not perfectly solved, existing LLMs can effectively analyze hard constraints and generate verification code, which can help the training of advanced LLMs~\citep{lambert2024t}.

In conclusion, incorporating additional verification agents for specific scenarios~\cite{mu2024rule, lambert2024t}, particularly those with verifiable correctness, can develop more reliable and advanced reward systems, presenting a promising direction for future reward model development.



\subsection{Analysis}
\label{sec:exp_analysis}


\begin{table}
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lccc}
    \toprule
    Model & RM-Bench & JudgeBench & IFBench \\
    \midrule
    \ourmethodmini &$73.1$&$68.2$&$75.5$\\
    \hspace{2mm}\textit{-- factuality verifier} &$54.0$&$52.9$&$73.6$\\
    \hspace{2mm}\textit{-- if verifier} &$74.7$&$66.2$&$60.4$\\
    \hspace{2mm}\textit{-- both} &$55.4$&$58.8$&$58.8$\\
    \midrule[0.1pt]
    Oracle setting &$76.7$&$70.1$&$77.5$\\
    \midrule
    \ourmethodllama &$66.4$&$52.9$&$66.9$\\
    \hspace{2mm}\textit{-- factuality verifier} &$51.9$&$51.6$&$65.8$\\
    \hspace{2mm}\textit{-- if verifier} &$58.0$&$57.5$&$57.2$\\
    \hspace{2mm}\textit{-- both} &$44.8$&$55.5$&$57.2$\\
    \midrule[0.1pt]
    Oracle setting &$79.5$&$73.1$&$68.5$\\
    \bottomrule
    \end{tabular}
    }
    \caption{Experimental results (\%) of ablation study and the oracle setting. \textit{-- factuality verifier} and \textit{-- if verifier} refer to the reduction of the corresponding verification agent into a single LLM scorer.
    The results are the micro-averaged scores of all the corresponding subsets.}
    \label{tab:analysis}
\end{table}

We first conduct an ablation study on the verification agents in \ourmethod. Specifically, we investigate three settings: \textit{-- factuality verifier}, \textit{-- if verifier}, and \textit{-- both}, where the corresponding verification agents are reduced to \textbf{a single step}: using an additional LLM backbone to directly score the response, which is equivalent to the simple ensemble of the reward model ArmoRM with the corresponding LLM as a generative reward model~\citep{costereward2024}.
The ablation results are shown in Table~\ref{tab:analysis}. We can observe that removing the well-designed verification agent leads to a significant performance decrease. It demonstrates the importance of well-designed verification agents, and we encourage the community to develop more advanced verification agents for a more reliable \ourmethod.



We also observe the oracle setting of \ourmethod that invokes the most appropriate verification agents, that is, invoking the factuality agent on RM-Bench and JudgeBench, and the instruction-following verification agent on IFBench. The experimental results are shown in Table~\ref{tab:analysis}, and we observe that both \ourmethodmini and \ourmethodllama perform significantly better in the oracle setting. This further demonstrates the effectiveness of the verification agents and suggests that the planner in \ourmethod still has a large room for improvement and we leave developing a more advanced planner for future work. This also suggests that in some specific and well-defined scenarios, one can adopt the corresponding verification agent alone to achieve better results.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/best_of_n.pdf}
    \caption{Best-of-n results (\%) on TriviaQA, IFEval, and CELLO using the base reward model ArmoRM and \ourmethod to search. ``+Oracle'' denotes using the oracle setting of \ourmethod as mentioned in \cref{sec:exp_analysis}.}
    \label{fig:enter-label}
\end{figure*}


\begin{table*}
    \centering
    \small
    \begin{tabular}{lccccccc}
    \toprule
    DPO Training Data & MMLU & MMLU-Pro & TriviaQA & TruthfulQA & IFEval & CELLO & MT-Bench\\
    \midrule
    -- & $58.9$ & $28.8$ & $54.8$ & $39.5$ & $43.3$ & $51.5$ & $5.2$ \\
    \midrule
    Original UF & $58.7$ & $29.3$ & $54.0$ & $42.0$ & $56.8$ & $62.0$  & $6.0$ \\
    ArmoRM-UF & $58.1$ & $29.9$ & $52.5$ & $45.0$ & $58.6$ & $60.8$ & $6.0$ \\
    \ourmethodllama-UF & $59.1$ & $30.5$ & $55.1$ & $44.1$ & $\mathbf{59.4}$ & $60.1$ & $5.8$ \\
    \midrule
    ArmoRM-OP & $58.4$ & $30.4$ & $51.6$ & $44.4$ & $52.7$ & $58.1$ & $6.0$ \\
    \ourmethodllama-OP & $\mathbf{59.5}$ & $\mathbf{31.3}$ & $\mathbf{55.3}$ & $\mathbf{48.5}$ & $58.2$ & $\mathbf{65.7}$ & $\mathbf{6.1}$ \\
    \bottomrule
    \end{tabular}
    \caption{Experimental results (\%) of LLMs trained with DPO on different training data. ``ArmoRM-UF'' denotes using ArmoRM to construct preference pairs from UltraFeedback. ``UF'' and ``OP'' are short for UltraFeedback and on-policy data, respectively. ``Original UF'' refers to using the original GPT-4 annotated preference pairs from UltraFeedback to train the LLM. ``--'' denotes the original LLM zephyr-7b-sft-full without further DPO training.}
    \label{tab:dpo_results}
\end{table*}


















