\section*{Limitations}

The main limitations of this work lie in the implementation of \ourmethod: (1) The verification agents are far from providing perfect rewards, as the average score on reward modeling benchmarks only reaches $72.5\%$. This suggests that achieving perfect rewards is challenging and requires further research efforts. (2) We only implement verification agents for factuality and instruction-following, which we believe are current weaknesses in reward models~\citep{liu2024rm} and important factors affecting LLM applications and user experiences. We encourage the community to explore more verifiable correctness signals. In conclusion, we believe the contribution of \textit{agentic reward modeling} concept is substantial, and we look forward to developing more advanced reward systems in the future.
