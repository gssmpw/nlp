\section{Conclusion}

In this paper, we propose \textit{agentic reward modeling}, a reward system that integrates the human preferences from conventional reward models with verifiable correctness signals to provide more reliable rewards. We empirically implement a reward agent, named \ourmethod, which consists of a router, well-designed verification agents for factuality and instruction-following, and a judger. We conduct extensive experiments on reward modeling benchmarks, best-of-n search, and DPO training. \ourmethod significantly outperforms other reward models and LLMs as generative reward models. We encourage more research efforts to develop more advanced and reliable reward systems.