\section{Experimental Details}
\label{sec:app_exp}
In this section, we provide a detailed description of the evaluation process, divided into three parts: the construction and distribution details of \ourdataset~\ref{sec:app_exp_ifbench}, the evaluation dataset settings~\ref{sec:app_exp_evaluation}, and additional experimental results~\ref{sec:app_exp_more_res}.

\subsection{\ourdataset Details}
\label{sec:app_exp_ifbench}

\ourdataset is a benchmark designed to evaluate reward models for multi-constraint instruction-following. The dataset comprises $444$ carefully curated instances, each containing: an instruction with $3$ to $5$ multi-constraints, a chosen response satisfying all constraints, and a rejected response violating specific constraints. All instances were constructed using \texttt{gpt-4o-2024-11-20} version through the following systematic pipeline.

\looseness=-1
\paragraph{Instruction Construction} We sampled $500$ initial instructions from the Open Assistant~\cite{kopf2023openassistant}. To ensure clarity and simplicity, we constrained the initial instruction length to $5$ to $20$ words. Subsequently, we employed GPT-4o to generate five distinct categories of constraints for each initial instruction. It then autonomously selected $3$ to $5$ constraints and paraphrased them into $1$ to $2$ sentences. The paraphrased constraints were integrated into the initial instruction. Finally, we use GPT-4o to evaluate the final instructions and filter out those with internal contradictions, resulting in a final set of $444$ instructions.

\looseness=-1
\begin{itemize}
    \item {\bf Content Constraints: } Specify conditions governing response, including topic focus, detail depth, and content scope limitations.
    \item {\bf Style Constraints: } Control linguistic characteristics such as tone, sentiment polarity, empathetic expression, and humor.
    \item {\bf Length Constraints: } Dictate structural requirements including word counts, paragraph composition, and specific opening phrases.
    \item {\bf Keyword Constraints: } Enforce lexical constraints through keyword inclusion, prohibited terms, or character-level specifications.
    \item {\bf Format Constraints: } Define presentation standards that include specific formats such as JSON, Markdown, or Python, along with section organization and punctuation rules.
\end{itemize}


\paragraph{Response Construction} For each instruction, we generated $8$ candidate responses using GPT-4o with temperature $1.0$ to maximize diversity. The chosen response was selected as the unique candidate satisfying all constraints through automated verification. Rejected responses were systematically selected to ensure balanced distributions of unsatisfied constraint (UC) categories and counts. As shown in Figure~\ref{fig:IFbench}, instances are stratified by difficulty: simple (\#UC$\geq$3), normal (\#UC$=$2), and hard (\#UC$=$1), with detailed information of UC category distributions. Specifically, (a) shows the distribution by the number of unsatisfied constraints in the rejected responses, where the sum of all parts equals the total number of instances. (b) presents the distribution by the categories of all unsatisfied constraints, where the sum of all parts equals the total number of unsatisfied constraints.

\begin{figure}[!ht]
    \centering
    \subfigure[]{
    \includegraphics[width=0.45\linewidth]{figures/IFBench_F1.pdf} }
    \subfigure[]{
    \includegraphics[width=0.45\linewidth]{figures/IFBench_F2.pdf} }
    \caption{Proportion (\%) of data in \ourdataset based on the number of unsatisfied constraints per instance and the categories of all unsatisfied constraints. }
    \label{fig:IFbench}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/gpt4o_best_of_n.pdf}
    \caption{Best-of-n results (\%) on TriviaQA, IFEval, and CELLO using the base reward model ArmoRM and \ourmethod to search. ``+Oracle'' denotes using the oracle setting of \ourmethod as mentioned in \cref{sec:exp_analysis}.}
    \label{fig:gpt4o_best_of_n}
\end{figure*}


\subsection{Evaluation Details}
\label{sec:app_exp_evaluation}

% 表3中各个数据集的evaluation setting，metric 
\paragraph{Best-of-N} For the TriviaQA, we sample $500$ instances from the validation split in \texttt{rc.nocontext} version. The model is prompted to generate direct answers, and we report the exact match accuracies. For the IFEval, we report the average accuracy across the strict prompt, strict instruction, loose prompt, and loose instruction settings. For the CELLO, we report the average score based on the official evaluation script. All three tasks are conducted under a zero-shot setting.

\paragraph{DPO Training}
For MT-Bench and CELLO, we employ FastChat\footnote{\url{https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge}} and the official evaluation script respectively, to conduct the evaluations and report the average scores.
For the other tasks, we use the \texttt{lm-evaluation-harness}\footnote{\url{https://github.com/EleutherAI/lm-evaluation-harness}} for evaluation. Specifically, we adopt a 5-shot setting for the MMLU and MMLU-Pro tasks, while using a zero-shot setting for TriviaQA and TruthfulQA. Notably, for TruthfulQA, we use the \texttt{truthfulqa\_gen} setting. 

\subsection{More Results on Best-of-N}
\label{sec:app_exp_more_res}
We conduct best-of-n search experiments using \texttt{gpt-4o-2024-11-20} as the policy model, with the results presented in Figure~\ref{fig:gpt4o_best_of_n}. The results demonstrate that \ourmethod significantly improves best-of-n performance compared to the base reward model ArmoRM, even when applied to a more powerful policy model than \ourmethod.




\begin{table*}
    \centering
    \small
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{p{\linewidth}}
    \toprule
    % \textbf{Prompt For Router} \\
    % \midrule
   Given the following instruction, determine whether the following check in needed. \\
    \\
        \text{[Instruction]} \\
        \{instruction\} \\
    \\
        \text{[Checks]} \\
       \{ 
            ``name'': ``constraint check'', 
            ``desp'': ``A `constraint check' is required if the instruction contains any additional constraints or requirements on the output, such as length, keywords, format, number of sections, frequency, order, etc.'', 
            ``identifier'': ``[[A]]'' 
        \}, 
        \{  
            ``name'': ``factuality check'', 
            ``desp'': ``A `factuality check' is required if the generated response to the instruction potentially contains claims about factual information or world knowledge.'', 
            ``identifier'': ``[[B]]'' 
        \} \\
        \\
        If the instruction requires some checks, please output the corresponding identifiers (such as [[A]], [[B]]). \\
        Please do not output other identifiers if the corresponding checkers not needed. \\
    \bottomrule
    \end{tabular}
    }
    \end{adjustbox}
    \caption{Our prompt for the router, where the \{instruction\} part varies based on the input. }
    \label{tab:planner}
\end{table*}

\begin{table*}
    \centering
    \small
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{p{\linewidth}}
    \toprule
    % \textbf{Prompt For Difference Proposal} \\
    % \midrule
    \textbf{Prompt For Difference Proposal} \\
        \text{[Answers]} \\
        \{formatted\_answers\} \\
        \\
        \text{[Your Task]} \\
        Given the above responses, please identify and summarize one key points of contradiction or inconsistency between the claims. \\
        \\
        \text{[Requirements]} \\
        1. Return a Python list containing only the most significant differences between the two answers. \\
        2. Do not include any additional explanations, only output the list. \\
        3. If there are no inconsistencies, return an empty list. \\
    \midrule
    \textbf{Prompt For Query Generation} \\
    \text{[Original question that caused the inconsistency]} \\
        \{instruction\} \\
\\
        \text{[Inconsistencies]} \\
        \{inconsistencies\} \\
        \\
        \text{[Your Task]} \\
        To resolve the inconsistencies, We need to query search engine. For each contradiction, please generate a corresponding query that can be used to retrieve knowledge to resolve the contradiction.  \\
        \\
        \text{[Requirements]} \\
        1. Each query should be specific and targeted, aiming to verify or disprove the conflicting points.  \\
        2. Provide the queries in a clear and concise manner, returning a Python list of queries corrresponding to the inconsistencies. \\
        3. Do not provide any additional explanations, only output the list. \\
        \midrule
    \textbf{Prompt For Verification} \\
    Evaluate which of the two answers is more factual based on the supporting information. \\
        \text{[Support knowledge sources]}: \\
        \{supports\} \\
        \\
        \text{[Original Answers]}: \\
        \{formatted\_answers\} \\
        \\
        \text{[Remeber]} \\
        For each answer, provide a score between 1 and 10, where 10 represents the highest factual accuracy. Your output should only consist of the following: \\
        Answer A: [[score]] (Wrap the score of A with [[ and ]]) \\
        Answer B: <<score>> (Wrap the score of B with << and >>) \\
        Please also provide a compact explanation. \\
    \bottomrule
    \end{tabular}
    }
    \end{adjustbox}
    \caption{Our prompt for assessing factuality in verification agents, with the \{formatted\_answers\}, \{supports\}, \{inconsistencies\}, \{instruction\} and \{supports\} parts varying based on the input. }
    \label{tab:factuality_agent}
\end{table*}


\begin{table*}
    \centering
    \small
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{p{\linewidth}}
    \toprule
    % \textbf{Prompt For Difference Proposal} \\
    % \midrule
    \textbf{Prompt For Constraint Parsing} \\
       You are an expert in natural language processing and constraint checking. Your task is to analyze a given instruction and identify which constraints need to be checked. \\
        \\
        The `instruction' contains a specific task query along with several explicitly stated constraints. Based on the instructions, you need to return a list of checker names that should be applied to the constraints. \\
        \\
        Task Example: \\  
        Instruction: Write a 300+ word summary of the Wikipedia page ``https://en.wikipedia.org/wiki/Raymond\_III,\_Count\_of\_Tripol''. Do not use any commas and highlight at least 3 sections that have titles in markdown format, for example, *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.\\
        Response: \\
        NumberOfWordsChecker: 300+ word \\
        HighlightSectionChecker: highlight at least 3 sections that have titles in markdown format\\
        ForbiddenWordsChecker: Do not use any commas \\
        \\
        Task Instruction: \\
        \{instruction\} \\
        \\
        \#\#\# Your task: \\
        - Generate the appropriate checker names with corresponding descriptions from the original instruction description. \\
        - Return the checker names with their descriptions separated by `\textbackslash n'  \\
        - Focus only on the constraints explicitly mentioned in the instruction (e.g., length, format, specific exclusions).  \\
        - Do **not** generate checkers for the task query itself or its quality. \\
        - Do **not** infer or output constraints that are implicitly included in the instruction (e.g., general style or unstated rules). \\
        - Each checker should be responsible for checking only one constraint. \\
    \midrule
    \textbf{Prompt For Code Generation} \\
    You are tasked with implementing a Python function `check\_following' that determines whether a given `response' satisfies a constraint defined by a checker. The function should return `True' if the constraint is satisfied, and `False' otherwise. \\
\\
        \text{[Instruction to check]}: \\
        \{instruction\} \\
\\
        \text{[Specific Checker and Description]}: \\
        \{checker\_name\} \\
\\
        Requirements: \\
        - The function accepts only one parameter: `response' which is a Python string. \\
        - The function must return a boolean value (`True' or `False') based on whether the `response' adheres to the constraint described by the checker. \\
        - The function must not include any I/O operations, such as `input()' or `ArgumentParser'. \\
        - The Python code for each checker should be designed to be generalizable, e.g., using regular expressions or other suitable techniques. \\
        - Only return the exact Python code, with no additional explanations. \\
    \bottomrule
    \end{tabular}
    }
    \end{adjustbox}
    \caption{Our prompt for assessing instruction-following in verification agents, with the \{instruction\} and \{checker\_name\} parts varying based on the input. }
    \label{tab:if_agent}
\end{table*}