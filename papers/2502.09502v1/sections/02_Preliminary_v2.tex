\section{Problem Formulation}
In this preliminary section, we introduce some backgrounds on how to obtain a lower bound (which will be used for the branch-and-bound process to prune nodes) for the optimal value of Problem~\eqref{obj:original_sparse_problem} by solving an associated convex relaxation problem.
First, note that we can cast problem~\eqref{obj:original_sparse_problem}~as 
\begin{align}
    \label{epigraph:formulation}
    \min \left\{ \tau \,:\, (\tau, \bbeta, \bz) \in \mathcal S \right\},
\end{align}
where the extended feasible set is defined as
\begin{align}
    \label{eq:S}
    \mathcal{S} = \left\{ (\tau, \bbeta, \bz)  \;\middle|\;
    \begin{array}{l} 
        \| \bbeta \|_\infty \leq M, \\
        \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \\
        \beta_j ( 1 - z_j) = 0 ~~ \forall j \in [p] \\
        f(\bX \bbeta, \by) + \lambda_2 \| \bbeta \|_2^2 \leq \tau
    \end{array}
    \right\},
\end{align}
and $[p] = \{1, \dots, p \}$ stands for the set of all integers up to $p \in \mathbb N$.
Put it differently, each binary variable $z_j$ indicates whether a continuous variable $\beta_j$ is zero or not by requiring $\beta_j = 0$ when $z_j = 0$ and allowing $\beta_j$ to take any value when $z_j = 1$. 
Meanwhile, the objective function is linearized using the epigraph reformulation technique, which allows us to interpret the optimal value of~\eqref{epigraph:formulation} as the evaluation of the support function of $\mathcal S$ at $(\bm 0, \bm 0, 1)$.
By virtue of \citep[\S13]{rockafellar1970convex}, the optimal value of~\eqref{epigraph:formulation} remains unchanged if we replace $\mathcal S$ with $\cl \conv(\mathcal S)$, where $\cl \conv(\mathcal S)$ denotes the closed convex hull of $\mathcal S$. 
Alas, the exact description of $\cl \conv(\mathcal S)$ requires exponentially many (nonlinear) constraints, which leads to the NP-hardness of~\eqref{obj:original_sparse_problem}.

We thus explore other options for a convex relaxation of~\eqref{obj:original_sparse_problem}.
It turns out that a tractable convex hull can be obtained if the objective function only includes the Tikhonov regularization term $\| \bbeta \|_2^2$, using the perspective function.
The perspective function of the quadratic function $h(\beta) = \beta^2$ is $h^\pi(\beta, z) = \beta^2 / z$ if $z > 0$, $= 0$ if $\beta = z = 0$, and $= \infty$ otherwise.
For simplicity, we write $\beta^2/z$ instead of $h^\pi(\beta, z)$ even if $z = 0$. 
The following lemma provides an exact perspective formulation of the convex hull when $\mathcal S$ does not include $f(\bX \bbeta, \by)$. This result extends \citep[Lemma~6]{gunluk2010perspective} by incorporating sparsity constraints, while also extending \citep[Theorem~2]{shafiee2024constrained} to account for $\ell_\infty$ box constraint on $\bbeta$.
\begin{lemma}
    \label{lemma:equivalence_between_perspective_relaxation_and_convexification}
    The closed convex hull of the set
    \begin{align*}
        \left\{ (\tau, \bbeta, \bz) \middle|
        \begin{array}{l}
            \| \bbeta \|_\infty \leq M, \,  \\
            \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \\ \beta_j ( 1 - z_j) = 0 ~~ \forall j \in [p], \\
            \sum_{j \in [p]} \beta_j^2 \leq \tau
        \end{array}
        \right\}
    \end{align*}
    is given by the set
    \begin{align*}
        \left\{ (\tau, \bbeta, \bz)  \;\middle|\;
        \begin{array}{l} 
            -M z_j\leq \bbeta_j \leq M z_j ~ \forall j \in [p], \\
            \bz \in [0, 1]^p, \, \mathbf{1}^T \bz \leq k, \\
            \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau
        \end{array}
        \right\}.
    \end{align*}
\end{lemma}
The convex hull formulation presented in Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification} is a second-order conic set.
Specifically, the epigraph of the sum of perspective functions in the last line satisfies
\begin{align*}
    \sum_{j \in [p]} {\beta_j^2}/{z_j} \leq \tau \iff \exists \bt \in \R_+^p ~ \st ~ 
    \begin{cases}
        \bm 1^\top \bm t = \tau, \\
        \beta_j^2 \leq z_j t_j ~ \forall j \in [p],
    \end{cases}
\end{align*}
which is second order cone representable.
Motivated by Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}, we immediately see that the extended feasible set $\mathcal S$ defined in~\eqref{eq:S} admits the following perspective representation
\begin{align*}
% \label{eq:perspective:S}
    \mathcal{S} = \left\{ (\tau, \bbeta, \bz)  \;\middle|\;
    \begin{array}{l} 
        -M z_j\leq \bbeta_j \leq M z_j ~ j \in [p], \\
        \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k,  \\
        f(\bX \bbeta, \by) + \lambda_2 \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau
    \end{array}
    \right\}.
\end{align*}
Plutting in this new perspective representation into Problem~\eqref{epigraph:formulation}, we can reformulate~\eqref{obj:original_sparse_problem} as follows
\begin{align}
    \label{obj:original_sparse_problem_perspective_formulation}
    P_{\text{MIP}}^\star = \left\{
    \begin{array}{cll}
        \min\limits_{\bbeta, \bz \in \R^p} & f(\bX \bbeta, \by) + \lambda_2 \sum_{j \in [p]} {\beta_j^2}/{z_j} \\[1ex]
        \text{\; s.t.} & \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \\[1ex]
        & -M z_j \leq \beta_j \leq M z_j ~ \forall j \in [p].
    \end{array}
    \right.
\end{align}
By relaxing the binary variables $z_j$ to the interval $[0, 1]$, we obtain the following strong convex relaxation of~\eqref{obj:original_sparse_problem_perspective_formulation}
\begin{align}
    \label{obj:original_sparse_problem_perspective_formulation_convex_relaxation}
    P_{\text{conv}}^\star = \left\{
    \begin{array}{cll}
        \min\limits_{\bbeta, \bz \in \R^p} & f(\bX \bbeta, \by) + \lambda_2 \sum_{j \in [p]} {\beta_j^2}/{z_j} \\[1ex]
        \text{\; s.t.} & \bz \in [0, 1]^p, \, \mathbf{1}^T \bz \leq k, \\[1ex]
        & -M z_j \leq \beta_j \leq M z_j ~ \forall j \in [p].
    \end{array}
    \right.
\end{align}
Although this is not the convex hull formulation due to the term $f(\bX \bbeta, \by)$, unlike in Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}, $P^\star_{\text{conv}}$ still provides a lower bound for Problem~\eqref{obj:original_sparse_problem}.


We can solve~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation} using standard conic optimization solvers like Mosek and Gurobi, which rely on IPMs for solving such subproblems in the BnB framework.
However, IPMs are computationally expensive and do not scale well for large datasets.
Alternatively, first-order conic solvers such as SCS~\cite{o2016conic}, based on ADMM, can be used.
While these methods are more scalable, they suffer from slow convergence rates and require solving linear systems at each iteration, which can also be computationally intensive for large instances. The main goal of the paper is to introduce an efficient and scalable first-order method to address these limitations.