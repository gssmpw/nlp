\section{Unused Writing \ToDo{To be deleted}}






% Although both individual proximal operators are known, we cannot easily derive the proximal operator of the composition of these two elementary functions.
% Thus, we need to find a new way to evalute this more complicated proximal operator.
% To do this, we use the following two properties for the optimal solution to the optimization problem.
% at \hat{alpha}, we have sgn preserving property and isotonic property.
% By using these two properties, we can get rid of both the TomSum_k function and the compositional structure by recasting the problem in a new optimization problem.
% This new problem can be solved very efficiently, by using a customized PAVA algorithm, whose complexity is just O(n) after a single 1D sorting.
% This in turn would mean that evaluating the proximal operator is much more computational efficient through the dual space than through the primal space.
% Thus, by combining this dual space proximal operator with the Moreau's lemma, we obtain a very fast way to evaluate the proximal operator, which is crucial if we want to run FISTA fast.

Problem~\eqref{obj:KyFan_Huber_isotonic_regression} can be recognized as a generalized isotonic regression problem, which can be tackled by designing a customized pool-adjacent-violators algorithm (PAVA).
The pseudocode for obtaining $\bnu^*$ for Problem~\eqref{obj:KyFan_Huber_isotonic_regression} is shown in Algorithm~\ref{alg:PAVA_algorithm}.
In a nutshell, PAVA keeps merging two adjacent blocks and updating $\nu_j$'s until there is no isotonic constraint violation.
The final values returned by PAVA is indeed the optimal solution to Problem~\eqref{obj:KyFan_Huber_isotonic_regression}, thanks to the following theorem:
\begin{namedtheorem}[~\ref{theorem:PAVA_gives_optimal_solution}]
    \label{appendix_theorem:PAVA_gives_optimal_solution}
    \cite{best2000minimizing}
    In Algorithm~\ref{alg:PAVA_algorithm}, if
    \begin{enumerate}
        \item the optimal solution for the generalized isotonic regression problem, restricted to each block, is single-valued;
        \item there is no isotonic constraint violation between any two adjacent blocks
    \end{enumerate}
    then we have $\hat{\bnu} = \bnu^*$.
    
    If there is an isotonic constraint violation between two adjacent blocks, then the optimal solution for the generalized isotonic regression problem, restricted to the merged block, is single-valued.
    
\end{namedtheorem}
% exact solution
Note that Algorithm~\ref{alg:PAVA_algorithm} gives us not an approximate but the \textit{exact solution}.
This is because Step 2 and Step 3 in Algorithm~\ref{alg:PAVA_algorithm} can be evaluated exactly by using the proximal operator for the Huber loss:
\begin{align}
    \text{prox}_{\rho H_M} (\mu) :&= \argmin_{\nu} \frac{1}{2} (\nu - \mu)^2 + \rho H_M(\nu) \nonumber \\
    &= \mu - \frac{\rho \mu}{\max(\vert{\mu}, \rho + 1)}
\end{align}
For the initialization stage, we just substitute $\rho=\rho_j$ and $\mu=\vert{\mu_j}$.
For the pooling stage, we use the average parameters in the entire merged block with $\rho = \frac{1}{N} \sum_{j=a_1}^{a_3} \rho_j$, $\mu = \frac{1}{N} \sum_{j=a_1}^{a_3} \vert{\mu_j}$, and $N = a_3 - a_1 + 1$.

% linear time complexity
In addition to getting the exact solution, we can also execute Algorithm~\ref{alg:PAVA_algorithm} efficiently.
For the sorting part, the running time is negligible because sorting in 1D can be done extremely fast on most practical datasets with modern hardwares.
For Step 3 in PAVA, if we apply the up-and-down-blocks algorithm in Step 3, then checking constraint violations and merging blocks can be run in linear time complexity.
A more detailed up-and-down-blocks algorithm to do this is shown in Algorithm~\ref{} in the Appendix.

Thus, we have found an efficient way to evaluate the proximal operator for $g^*(\cdot)$.
This result, together with the Moreau's identity discussed in Equation~\eqref{eq:Moreaus_identity}, is why we want to evaluate the proximal operator for $g(\cdot)$ via the Fenchel dual space.
This is also why our customized FISTA can scale more favorably compared to other methods when solving Problem~\eqref{obj:original_sparse_problem_partial_convexification} .
For generic algorithms such as the interior-point method and ADMM, they require solving linear systems and have cubic time complexity.










\begin{figure}[h]
    \includegraphics[width=\linewidth]{sections/main_pics/proximal_operator_primal_dual_approaches.png}
    \caption{Two ways to evaluate the proximal operator: either in the primal space (via the top \textcolor{red}{red} arrow) or the dual space (via the left, bottom, and right \textcolor{ForestGreen}{green} arrows).}
    \label{pic:proximal_operator_primal_dual_approaches}
\end{figure}

\section{FISTA}
\label{sec:preliminary_subsec:fista}

\input{sections/FISTA_pseudocode}

FISTA is a first-order method to minimize a convex composite function in the form of $F(\bbeta) + G(\bbeta)$, where $F(\bbeta)$ is a differentiable and smooth function whose gradient $\nabla(\cdot)$ is $L$-Lipschitz continuous, and $G(\bbeta)$ is typically nonsmooth.

After choosing some initial points $\bbeta^1=\bbeta^0$, FISTA solves the problem by alternating between the following two steps at each iteration $t>1$:
\begin{align*}
        \bgamma^t &= \bbeta^t + \frac{t}{t+3} (\bbeta^t - \bbeta^{t-1}), \\
        \bbeta^{t+1} &= \text{prox}_{\frac{1}{L}G}(\bgamma^t - \frac{1}{L} \nabla F(\bgamma^t)),
\end{align*}
where the proximal operator $\text{prox}_{\frac{1}{L}G}(\bgamma)$ is defined as:
\begin{align*}
\text{prox}_{\frac{1}{L}G}(\bgamma) := \underset{\mathbf{x}}{\text{argmin}} \left( \frac{1}{2} \Vert{\mathbf{x} - \bgamma }_2^2 + \frac{1}{L}G(\mathbf{x}) \right).
\end{align*}
This iterative approach performs a gradient descent-like step on the smooth part $F(\bbeta)$ and a proximal update based on the nonsmooth part $G(\bbeta)$, accelerated by a momentum term.
FISTA is efficient for problems where $G(\bbeta)$ has a known and tractable proximal operator.


\subsection{Safe Lower Bound}
One may ask why we choose FISTA since there are better algorithms, such as the interior-point method and the alternating direciton method of multipliers (ADMM), that can handle the constraints in Problem~\eqref{obj:original_sparse_problem_partial_convexification} more gracefully.

One reason for having a unconstrained convex composite form for FISTA is that this allows us to use an approximate solution $\hat{\bbeta}$ to derive a safe lower bound, based on the Fenchel Duality Theorem:
\begin{align}
    \label{eq:fenchel_duality_theorem_G(x)+F(x)}
    P_{\text{MIP}}^* \geq \min_{\bbeta }F(\bbeta) + G(\bbeta) >= -F^*(-\hat{\bzeta})-G^*(\hat{\bzeta}),
\end{align}
where $G^*$ and $F^*$ are Fenchel conjugates of their original functions, and $\hat{\bzeta} = -\nabla F(\hat{\bbeta})$ is the dual variable based on the approximate primal solution $\hat{\bbeta}$.
Note that using the objective function in Problem~\eqref{obj:original_sparse_problem_partial_convexification} alone is not enough to produce a valid lower bound for $P_{\text{MIP}}^*$, unless $\hat{\bbeta}=\bbeta^*$, which almost never happens for iterative methods.

However, there are significant challenges in using FISTA to solve Problem~\eqref{obj:original_sparse_problem_partial_convexification}.
First of all, this is a constrained problem, so we cannot derive a safe lower bound using Equation~\eqref{eq:fenchel_duality_theorem_G(x)+F(x)}.
Moreover, even though FISTA can handle certain optimization problems with simple constraints, it is not clear what the proximal operator looks like for these more complicated constraints, let alone evaluating it efficiently.
In the next section, we overcome these difficulties by exploiting the hidden structures in Problem~\eqref{obj:original_sparse_problem_partial_convexification}.


% \begin{align}
%     \label{eq:fenchel_duality_theorem_h1(x)+h2(x)_relationship_between_optimal_primal_and_dual_solutions}
%     \bzeta^* = -\nabla h_1(\bbeta^*)
% \end{align}

% However, each algorithm has drawbacks and cannot be scalable to large instances (see Section~\ref{sec:related_work} Related Work for detailed discussion).