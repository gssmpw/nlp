\subsection{Proof of Lemma~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}}
\label{appendix_subsec:proof_equivalence_between_proximal_operator_and_huber_isotonic_regression}

% \begin{lemma}[Lemma~\ref{appendix_lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}]
\begin{namedlemma}[~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}]
    \label{appendix_lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}
    Problem~\eqref{obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is closely related to the following optimization problem:
    \begin{align}
        \label{appendix_obj:KyFan_Huber_isotonic_regression}
        \bnu^* = \argmin_{\bnu} & \quad \frac{1}{2} \sum_{j=1}^n (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ}^k H_M (\nu_j), \\
        \text{s.t.} & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l}, \nonumber
    \end{align}
    where $\calJ$ is the set of indices of the top $k$ largest elements of $ \vert{\mu_j}$.
    Specifically, the optimal solution $\balpha^*$ for Problem~\eqref{obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} can be recovered from the optimal solution $\bnu^*$ for Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression} by the relation $\balpha^* = \text{sgn}(\bmu) \odot \bnu^*$, where $\odot$ denotes the Hadamard (element-wise) product.
\end{namedlemma}

\begin{proof}
% Add proof content here
First, let us recall that Problem~\eqref{obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is the following optimization problem:
\begin{align}
    \label{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber}
    \balpha^* = \argmin_{\balpha} \frac{1}{2} \Vert{\balpha - \bmu}_2^2 + \rho \text{TopSum}_k\left( H_M\left( \balpha \right) \right),
\end{align}

We want to show that Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is closely related to Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression} via the relation $\balpha^* = \text{sgn}(\bmu) \odot \bnu^*$.


To accomplish this, we leverage two properties associated with the optimal solution for Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber}.
At $\balpha^*$, we have:
\begin{align}
    & \textbf{1. sign-preserving property:} \quad \text{sgn}(\alpha_j^*) = \text{sgn}(\mu_j) \label{appendix_property:sign_preserving}\\
    & \textbf{2. relative magnitude-preserving property:} \quad \vert{\alpha_j^*} \geq \vert{\alpha_l^*} \; \text{if} \; \vert{\mu_j} \geq \vert{\mu_l} \label{appendix_property:relative_magnitude_preserving}
    % \text{sgn}(\alpha_j^*) &= \text{sgn}(\mu_j) \label{appendix_property:sign_preserving}\\
    % \vert{\alpha_j^*} \geq \vert{\alpha_l^*} \; &\text{if} \; \vert{\mu_j} \geq \vert{\mu_l} \label{appendix_property:relative_magnitude_preserving}
\end{align}
% Equation~\eqref{appendix_property:sign_preserving} says that the optimal solution $\balpha^*$ should be sign-preserving with respect to the input $\bmu$, and Equation~\eqref{appendix_property:relative_magnitude_preserving} tells us that the optimal solution $\balpha^*$ should preserve the relative order with respect to the input $\bmu$ in terms of the magnitude.

Let us explain why these two properties hold.

\paragraph{Sign-preserving property in Equation~\eqref{appendix_property:sign_preserving}} 
For the sake of contradiction, suppose that there exists some $j$ such that $\text{sgn}(\alpha_j^*) \neq \text{sgn}(\mu_j)$.
Then, we can construct a new $\balpha'$ by flipping the sign of $\alpha_j^*$, i.e., $\alpha_j' = -\alpha_j^*$, and keeping the rest of the elements the same as $\alpha_j'$.
Now under the assmption that $\text{sgn}(\alpha_j^*) \neq \text{sgn}(\mu_j)$, we have $\left\lvert{\alpha_j^* - \mu_j}\right\rvert > \left\lvert{\lvert{\alpha_j^*}\rvert - \lvert{\mu_j}\rvert}\right\rvert = \left\lvert{\alpha_j' - \mu_j}\right\rvert$, so the $j$-th term in the first summation of the objective function will decrease while everything else remains the same.
This leads to a smaller objective value for $\balpha'$ than $\balpha^*$, which contradicts the optimality of $\balpha^*$.
Thus, the sign-preserving property in Equation~\eqref{appendix_property:sign_preserving} must hold.

\paragraph{Relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving}} 
For the sake of contradiction, suppose that there exists some $j$ and $l$ such that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^*} < \vert{\alpha_l^*}$.
Then, we can construct a new $\balpha'$ by swapping $\alpha_j^*$ and $\alpha_l^*$, i.e., $\alpha_j' = \alpha_l^*$ and $\alpha_l' = \alpha_j^*$, and keeping the rest of the elements the same as $\alpha_j'$ and $\alpha_l'$.
Under the assumption that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^*} < \vert{\alpha_l^*}$, we have $\left\lvert{\alpha_j^* - \mu_j}\right\rvert + \left\lvert{\alpha_l^* - \mu_l}\right\rvert > \left\lvert{\alpha_l^* - \mu_j}\right\rvert + \left\lvert{\alpha_j^* - \mu_l}\right\rvert =
\left\lvert{\alpha_j' - \mu_j}\right\rvert + \left\lvert{\alpha_l' - \mu_l}\right\rvert$, so the sum of the $j$-th and $l$-th terms in the first summation of the objective function will decrease while everything else remains the same.
This leads to a smaller objective value for $\balpha'$ than $\balpha^*$, which contradicts the optimality of $\balpha^*$.
Thus, the relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving} holds.

Using these two properties, we are ready to prove the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} and Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression}.
First, let us reparameterize $\balpha$ with a new variable $\bnu$ in Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} as $\balpha = \text{sgn}(\bmu) \odot \bnu$ wtih $\bnu \in \mathbb{R}_{+}^n$.
In other words, we use $\bnu$ to model the magnitude of $\balpha$.

By the sign-preserving property in Equation~\eqref{appendix_property:sign_preserving}, we can set the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} and the following optimization problem:
\begin{align}
    \label{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber_reparameterized}
    \bnu^* = \argmin_{\bnu} \frac{1}{2} \sum_{j=1}^n (\nu_j - \vert{\mu_j})^2 + \rho \text{TopSum}_k\left( H_M\left( \bnu \right) \right), \; \text{ s.t. } \; \nu_j \geq 0.
\end{align}

By the relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving}, we can further set the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber_reparameterized} and the following optimization problem:
\begin{align}
    \label{appendix_obj:KyFan_Huber_isotonic_regression_with_nonnegative_constraint}
    \bnu^* = \argmin_{\bnu} & \quad \frac{1}{2} \sum_{j=1}^n (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ}^k H_M (\nu_j), \\
    \text{s.t.} & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l}, \; \text{ and } \; \nu_j \geq 0. \nonumber
\end{align}

Lastly, the nonnegative constraint in Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression_with_nonnegative_constraint} can be removed because the first summation term in the objective function already implies that $\nu_j \geq 0$.
Thus, we have shown that Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is closely related to Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression} via the relation $\balpha^* = \text{sgn}(\bmu) \odot \bnu^*$.



\end{proof}
