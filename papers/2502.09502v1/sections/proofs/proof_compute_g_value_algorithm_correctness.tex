\subsection{Proof of Theorem~\ref{theorem:compute_g_value_algorithm_correctness}}
\label{appendix_subsec:compute_g_value_algorithm_correctness}

\begin{namedtheorem}[~\ref{theorem:compute_g_value_algorithm_correctness}]
    \label{appendix_theorem:compute_g_value_algorithm_correctness}
    Algorithm~\ref{alg:compute_g_value_algorithm} correctly computes the value of $g(\bbeta)$ as defined in Problem~\eqref{obj:compute_g_value}.
    The computational complexity of Algorithm~\ref{alg:compute_g_value_algorithm} is $O(n + n \log k)$.
\end{namedtheorem}

\begin{proof}
% Add proof content here

We first show that the algorithm correctly computes the value of $g(\bbeta)$ and then analyze its computational complexity.

\paragraph{Correctness of the Algorithm:}
Recall that to obtain the value of $g(\bbeta)$, we need to solve the following optimization problem:
\begin{align}
    \label{appendix_obj:compute_g_value}
    g(\bbeta) = \left(
    \begin{array}{cc}
        \min\limits_{\bz, \bs} & \frac{1}{2} \sum_{j=1}^{n} s_j \\
        \text{s.t.} & \bz \in [0, 1]^n, \, \mathbf{1}^T \bz \leq k \\
        & \beta_j^2 \leq z_j s_j \\
        & -M z_j \leq \beta_j \leq M z_j
    \end{array}
    \right)
\end{align}
By using the perspective function $\beta_j^2/z_j$ instead of the rotated second-order cone constraint $\beta_j^2 \leq z_j s_j$, we can rewrite the optimization problem in~\eqref{appendix_obj:compute_g_value} as:
\begin{align}
    \label{appendix_obj:compute_g_value_perspective_formulation}
    g(\bbeta) = \left(
    \begin{array}{cc}
        \min\limits_{\bz} & \frac{1}{2} \sum_{j=1}^{n} \beta_j^2/z_j \\
        \text{s.t.} & \bz \in [0, 1]^n, \, \mathbf{1}^T \bz \leq k \\
        & -M z_j \leq \beta_j \leq M z_j
    \end{array}
    \right)
\end{align}
Let us define the cardinality and coefficient-constrained set $\calS_0$ below and use Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification} to derive its convex hull:
\begin{align}
    \label{appendix_def:cardinality_and_coefficient_constraint_set_and_its_convex_hull}
    \calS_0 = \left\{ (t, \bz, \bbeta) \;\middle|\;
    \begin{array}{cc}
        t \geq \frac{1}{2} \sum_{j=1}^{n} \beta_j^2 \\
        \bz \in \{0, 1\}^n, \, \mathbf{1}^T \bz \leq k \\
        -M z_j \leq \beta_j \leq M z_j
    \end{array}
    \right\},
    \quad
    \conv(\calS_0) = \left\{ (t, \bz, \bbeta) \;\middle|\;
    \begin{array}{cc}
        t \geq \frac{1}{2} \sum_{j=1}^{n} \frac{\beta_j^2}{z_j} \\
        \bz \in [0, 1]^n, \, \mathbf{1}^T \bz \leq k \\
        -M z_j \leq \beta_j \leq M z_j
    \end{array}
    \right\}
\end{align}
Then we can rewrite the optimization problem in~\eqref{appendix_obj:compute_g_value_perspective_formulation} more succintly as:
\begin{align}
    \label{appendix_obj:compute_g_value_lifted_convex_hull_formulation}
    g(\bbeta) = \left(
    \begin{array}{cc}
        \min\limits_{t} & t \\
        \text{s.t.} & (t, \bz, \bbeta) \in \conv(\calS_0)
    \end{array}
    \right)
\end{align}

Let us define another related cardinality and coefficient-constrained set $\calS_1$ below:
\begin{align}
    \label{appendix_def:cardinality_and_coefficient_constraint_set_unlifted_formulation}
    \calS_1 = \left\{ (t,\boldsymbol{\beta}) \;\middle|\;
    \begin{array}{cc}
        t \geq \frac{1}{2} \sum_{j=1}^{n} \beta_j^2 \\
        \|\boldsymbol{\beta}\|_0 \leq k \\
        -M \leq \beta_j \leq M
    \end{array}
    \right\},
\end{align}
Since $\calS_0$ is a lifted formulation of $\calS_1$, we have $\calS_1 = \{(t, \bbeta) \mid \exists \bz \in \mathbb{R}^n \text{ s.t. } (t, \bz, \bbeta) \in \calS_0\}$.
This allows us to rewrite the optimization problem in~\eqref{appendix_obj:compute_g_value_lifted_convex_hull_formulation} as:
\begin{align}
    \label{appendix_obj:compute_g_value_unlifted_convex_hull_formulation}
    g(\bbeta) = \left(
    \begin{array}{cc}
        \min\limits_{t} & t \\
        \text{s.t.} & (t, \bbeta) \in \conv(\calS_1)
    \end{array}
    \right)
\end{align}


Now, since $\calS_1$ is a permutation-invariant set, according to Theorem 4 in~\cite{kim2022convexification}, we can rewrite its convex hull as:
\begin{align}
    \label{appendix_def:convex_cardinality_and_coefficient_constraint_set_majorization_formulation}
    \conv(\calS_1) = \left\{ (t, \bbeta) \;\middle|\;
    \begin{array}{cc}
        \exists \bphi \in \mathbb{R}^n \text{\, s.t. \,} \bphi \succeq_m \vert{\bbeta} \\
        t \geq \frac{1}{2} \sum_{j=1}^{n} \phi_j^2 \\
        M \geq \phi_1 \geq \phi_2 \geq \ldots \phi_k \geq 0 \\
        \phi_{k+1} = \phi_{k+2} = \ldots = \phi_n = 0 
    \end{array}
    \right\},
\end{align}
where the absolute value operator $\vert{\cdot}$ is applied to a vector in an element-wise fashion, and the sign $\succeq_m$ in $\bphi \succeq_m \vert{\bbeta}$ denotes that $\bphi$ majorizes $\vert{\bbeta}$, \text{i.e.}, we have: 
\begin{align}
    \label{appendix_def:vector_majorization}
    \bphi \succeq_m \vert{\bbeta} \quad \Rightarrow \quad \sum_{j=1}^l \phi_j \geq \sum_{j=1}^l \vert{\beta_j} \quad \forall l \in [n-1] \quad \text{and} \quad \sum_{j=1}^n \phi_j = \sum_{j=1}^n \vert{\beta_j}
\end{align}

Plugging the alternative convex hull description of $\calS_1$ in Equation~\eqref{appendix_def:convex_cardinality_and_coefficient_constraint_set_majorization_formulation} and the majorization definition in Equation~\eqref{appendix_def:vector_majorization} into the optimization problem in~\eqref{appendix_obj:compute_g_value_unlifted_convex_hull_formulation}, we can rewrite $g(\bbeta)$ as:
\begin{align}
    g(\bbeta) = \left(
    \begin{array}{cc}
        \min\limits_{t} & t \\
        \text{s.t.} & \bphi \succeq_m \vert{\bbeta} \\
        & t \geq \frac{1}{2} \sum_{j=1}^{n} \phi_j^2 \\
        & M \geq \phi_1 \geq \phi_2 \geq \ldots \phi_k \geq 0 \\
        & \phi_{k+1} = \phi_{k+2} = \ldots = \phi_n = 0
    \end{array}
    \right) \nonumber\\
    = \left(
    \begin{array}{cc}
        \min\limits_{\bphi} & \frac{1}{2} \sum_{j=1}^{n} \phi_j^2 \\
        \text{s.t.} &  \bphi \succeq_m \vert{\bbeta} \\
        & M \geq \phi_1 \geq \phi_2 \geq \ldots \phi_k \geq 0 \\
        & \phi_{k+1} = \phi_{k+2} = \ldots = \phi_n = 0
    \end{array}
    \right) \label{appendix_obj:compute_g_value_majorization_formulation}
\end{align}
The task of computing the value of $g(\bbeta)$ is now reduced to solving the optimization problem in Equation~\eqref{appendix_obj:compute_g_value_majorization_formulation} by finding the optimal vector $\bphi$ that satisfies the majorization and other constraints and minimizes the objective function.

Let us see how to use the majorization definition in Equation~\eqref{appendix_def:vector_majorization} to derive the Algorithm~\ref{alg:compute_g_value_algorithm} for computing the value of $g(\bbeta)$.

At the first iteration $j=1$, we have
\begin{align}
    k \phi_1 \geq \sum_{j=1}^k \phi_j = \sum_{j=1}^n \phi_j \geq \sum_{j=1}^n \vert{\beta_j} \quad \Rightarrow \quad \phi_1 \geq \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}.
\end{align}
At the same time, we also need to satisfy $\phi_1 \geq \vert{\beta_1}$.
Now, we discuss in two cases:
\begin{enumerate}
    \item If $\frac{1}{k} \sum_{j=1}^n \vert{\beta_j} \geq \vert{\beta_1}$, in order to minimize the objective function in Problem~\eqref{appendix_obj:compute_g_value_majorization_formulation}, we set $\phi_1 = \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}$ (notice that $\phi_1 \leq M$ is automatically satisfied because $\phi_1 = \frac{1}{k} \sum_{j=1}^n \vert{\beta_j} = \frac{1}{k} \sum_{j=1}^n M z_j \leq M$). 
    
    This leads to $\phi_2 = \ldots = \phi_k = \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}$.
    To see this, for the sake of contradition, assume that $\exists j \in \{2, \ldots, k\}$ such that $\phi_j < \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}$. 
    Since $\phi_j \leq \phi_1 = \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}$, we have $\sum_{j=1}^k \phi_j < \sum_{j=1}^k \frac{1}{k} \sum_{j=1}^n \vert{\beta_j} = \sum_{j=1}^n \vert{\beta_j}$, which contradicts the majorization constraint.
    
    \item If $\frac{1}{k} \sum_{j=1}^n \vert{\beta_j} < \vert{\beta_1}$, we can set $\phi_1 = \vert{\beta_1}$ (notice that $\phi_1 \leq M$ is automatically satisifed because $\vert{\beta_1} \leq M z_1 \leq M$).
    Then we are left with $k-1$ coefficients to set, and we can follow the same argument as we did for when $j=1$.
    The majorization constraints become:
    \begin{align}
        \sum_{j=2}^l \phi_j \geq \sum_{j=2}^l \vert{\beta_j} \quad \forall l \in \{2, \ldots, n-1\} \quad \text{and} \quad \sum_{j=2}^n \phi_j = \sum_{j=2}^n \vert{\beta_j}
    \end{align}
\end{enumerate}
We repeat this process until we set all $k$ coefficients $\phi_1, \ldots, \phi_k$.
This is exactly what Algorithm~\ref{alg:compute_g_value_algorithm} does.
At the end, we return the value of the objective function, which is $\frac{1}{2} \sum_{j=1}^{k} \phi_j^2$.

\paragraph{Computational Complexity:}
It is straightforward to analyze the computational complexity of Algorithm~\ref{alg:compute_g_value_algorithm}.
The partial sorting step on Line 2 has a complexity of $O(n \log k)$.
The summation step on Line 3 has a complexity of $O(n)$.
The for-loop step on Line 4-8 has a complexity of $O(k)$, so does the final summation step on Line 9.
Therefore, the overall computational complexity of Algorithm~\ref{alg:compute_g_value_algorithm} is $O(n + n \log k)$.

\end{proof}
