\subsection{Proof of Lemma~\ref{lemma:PAVA_algorithm_exact_solution}}
\label{appendix_subsec:proof_PAVA_algorithm_exact_solution}

\begin{namedlemma}[~\ref{lemma:PAVA_algorithm_exact_solution}]
    \label{appendix_lemma:PAVA_algorithm_exact_solution}
    The customized PAVA presented in Algorithm~\ref{alg:PAVA_algorithm} solves the optimization problem in Equation~\eqref{obj:KyFan_Huber_isotonic_regression} exactly. 
    Specifically, for the final vector $\hat{\bnu}$ at the end of the while-loop (Line 14), we have $\hat{\bnu} = \bnu^*$.
\end{namedlemma}

\begin{proof}
% Add proof content here

Problem~\eqref{obj:KyFan_Huber_isotonic_regression} can be recognized as a generalized isotonic regression problem.
Let $h_j(v) = \frac{1}{2} (v - \mu_j)^2 + \rho_j H_M(v)$, where $\rho_j = \rho$ if $j \in \calJ$ and $\rho_j = 0$ otherwise.
As mentioned in Algorithm~\ref{alg:PAVA_algorithm}, the set $\calJ$ is the set of indices of top k largest elements of $\vert{\mu_j}$.
Then, we can rewrite Problem~\eqref{obj:KyFan_Huber_isotonic_regression} in the standard form of a generalized isotonic regression problem:
\begin{align}
    \label{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}
    \min_{\bnu} \sum_{j=1}^{p} h_j(\nu_j) \quad \text{s.t.} \quad \nu_1 \geq \nu_2 \geq \cdots \geq \nu_J.
\end{align}

Generalized isotonic regression problems have been studied extensively in the literature.
There are two key properties~\cite{best2000minimizing,ahuja2001fast} regarding Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression} that we can leverage to prove Lemma~\ref{lemma:PAVA_algorithm_exact_solution}:

\paragraph{1. Optimal solution for a merged block is single-valued.}
Suppose we have two adjacent blocks $[a_1, a_2]$ and $[a_2+1, a_3]$.
If the optimal solution for each block is single-valued, \textit{i.e.}, we have
\begin{equation}
\begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
\bnu_{a_1:a_2}^* =  
\left(\begin{array}{c}
\argmin_{\bnu_{a_1:a_2}} \sum_{j=a_1}^{a_2} h_j(\nu_j) \\
\text{s.t.} \quad \nu_{a_1} \geq \cdots \geq \nu_{a_2}
\end{array}\right) & 
\Rightarrow & \bnu_{a_1}^* = \cdots = \bnu_{a_2}^* \\[3em]
\bnu_{a_2+1:a_3}^* =  
\left(\begin{array}{c}
\argmin_{\bnu_{a_2+1:a_3}} \sum_{j=a_2+1}^{a_3} h_j(\nu_j) \\
\text{s.t.} \quad \nu_{a_2+1} \geq \cdots \geq \nu_{a_3}
\end{array}\right) & 
\Rightarrow & \bnu_{a_2+1}^* = \cdots = \bnu_{a_3}^*
\end{array}
\end{equation}

and if $\nu_{a_1}^* \leq \nu_{a_2+1}^*$, then the optimal solution for the merged block $[a_1, a_3]$ is single-valued, \textit{i.e.}, we have:

\begin{equation}
\begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
\bnu_{a_1:a_3}^* =  
\left(\begin{array}{c}
\argmin_{\bnu_{a_1:a_3}} \sum_{j=a_1}^{a_3} h_j(\nu_j) \\
\text{s.t.} \quad \nu_{a_1} \geq \cdots \geq \nu_{a_3}
\end{array}\right) & 
\Rightarrow & \bnu_{a_1}^* = \cdots = \bnu_{a_3}^*
\end{array}
\end{equation}

\paragraph{2. No isotonic constraint violation between single-valued blocks implies the solution is optimal.}
If we have $s$ blocks $[a_1, a_2], [a_2+1, a_3], \ldots, [a_{s}+1, a_{s+1}]$ (with $a_1=1$ and $a_{s+1}=n$) such that the optimal solution for each block is single-valued, \textit{i.e.}, we have

\begin{equation}
\begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
\hat{\bnu}_{a_l:a_{l+1}} =
\left(\begin{array}{c}
\argmin_{\bnu_{a_l:a_{l+1}}} \sum_{j=a_l}^{a_{l+1}} h_j(\nu_j) \\
\text{s.t.} \quad \nu_{a_l} \geq \cdots \geq \nu_{a_{l+1}}
\end{array}\right) &
\Rightarrow & \bnu_{a_l}^* = \cdots = \bnu_{a_{l+1}}^* \, \text{ for all } \, l=1, \ldots, s
\end{array}
\end{equation}

and if $\hat{\nu}_{a_1} \geq \hat{\nu}_{a_2+1} \geq \ldots \hat{\nu}_{a_{s}}$, then $\hat{\bnu}$ is the optimal solution for Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}.

Using these two properties, it is now easy to see why Algorithm~\ref{alg:PAVA_algorithm} returns the optimal solution.

We start by constructing blocks which have length 1.
The initial value restricted to each block is optimal.
Then, we iteratively merge adjacent blocks and update the values of $\nu_j$'s whenever there is a violation of the isotonic constraint.
By the first property, the optimal solution for the merged block is single-valued.
Therefore, we can compute the optimal solution for the merged block by solving a univariate optimization problem.

We keep merging blocks until there is no isotonic constraint violation.
When this happens, by construction, the solution for each block is single-valued and optimal.
By the second property, the final vector $\hat{\bnu}$ is the optimal solution for Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}.


\end{proof}
