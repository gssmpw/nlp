\section{Introduction}

Sparse generalized linear models (GLMs) are essential tools in machine learning (ML), widely applied in fields like healthcare, finance, engineering, and science. 
These models provide a flexible framework for capturing relationships between variables while ensuring interpretability, which is critical in high-stakes applications.
Recently, using the $\ell_0$ norm to induce sparsity has gained significant attention. This approach provides distinct advantages over traditional convex relaxation methods, such as replacing $\ell_0$ with $\ell_1$, particularly in cases involving highly correlated features.

In this paper, we aim to solve
\begin{align} \label{obj:original_sparse_problem}
    \begin{array}{cl}
        \min\limits_{\bbeta \in \R^p} & f(\bX \bbeta, \by) + \lambda_2 \lVert \bbeta \rVert_2^2 \\
        \st & \| \bbeta \|_\infty \leq M, ~ \lVert \bbeta \rVert_0 \leq k,
    \end{array}
\end{align}
where $\bX \in \R^{n \times p}$ and $\by \in \R^n$ denote the matrix of features and the vector of labels, respectively, while the parameter $M > 0$ can be either user-defined based on prior knowledge or estimated from the data~\citep{park2020subset}. The GLM loss function, denoted by $f : \R^n \times \R^n \to \R$, is assumed to be Lipschitz smooth, the parameter $k \in \mathbb N$ controls the number of nonzero coefficients, and $\lambda_2 > 0$ is a small Tikhonov regularization coefficient to address collinearity.
Alas, problem~\eqref{obj:original_sparse_problem} is NP-hard~\citep{natarajan1995sparse}. 
As a result, most existing methods rely on heuristics that deliver high-quality approximations but lack guarantees of optimality. 
This limitation is particularly problematic in high-stakes applications like healthcare, where ensuring accuracy, reliability, and safety is essential. 
Therefore, we emphasize the pursuit of certifiably optimal solutions.

A naive approach to solve~\eqref{obj:original_sparse_problem} to optimality is to reformulate it as a mixed-integer programming (MIP) problem and leverage commercial MIP solvers.
However, these solvers face significant scalability challenges, particularly with large datasets and nonlinear objectives. 
A major bottleneck arises from the need to compute tight lower bounds at each node of the branch-and-bound (BnB) tree, a critical component for efficient pruning and solver performance.
Existing methods for computing lower bounds typically rely on linear programming or conic optimization techniques.
However, these approaches either generate loose bounds that reduce pruning efficiency or result in high computational costs per iteration. 
Moreover, they are challenging to parallelize, which limits the potential to take advantage of modern hardware accelerators like GPUs.

To address these challenges, we propose a scalable first-order method for efficiently calculating lower bounds within the BnB framework.
We begin with a perspective reformulation of~\eqref{obj:original_sparse_problem} and derive its continuous relaxation.
The resulting formulation is then expressed as an unconstrained optimization problem, characterized by a convex composite objective function, which enables the application of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a well-known first-order method~\citep{beck2009fast}, to compute lower bounds.
The successful implementation of FISTA, however, relies on efficient computation of the proximal operator, which requires solving a second order cone program (SOCP) problem.
To the best of our knowledge, the efficient computation of this proximal operator has not been previously addressed in the literature. 
Therefore, we propose a customized pooled-adjacent-violation algorithm (PAVA) that evaluates the proximal operator exactly with log-linear time complexity, ensuring the scalability of our FISTA approach for large problem instances.
% Our method achieves three core objectives: a) fast convergence rate, b) low per-iteration computational complexity, and c) compatibility with GPU acceleration.
A major advantage of our approach is its computational efficiency, in which instead of solving costly linear systems, it only relies on matrix-vector multiplication, which is highly amenable to GPU acceleration.
This capability addresses a key limitation of existing approaches that struggle to parallelize their computations on modern hardware.

To accelerate the performance of the FISTA algorithm, we introduce a restart heuristic. 
This leads to an empirical linear convergence rate, a result not previously achieved by other first-order methods for this type of problem.
Empirically, our method demonstrates substantial speedups in computing dual bounds -- often by 1-2 orders of magnitude -- compared to existing techniques. 
These improvements significantly enhance the overall efficiency of the BnB process, enabling the certification of large-scale instances of~\eqref{obj:original_sparse_problem} that were previously intractable using commercial MIP solvers. All omitted proofs are provided in~\ref{appendix_sec:proofs}. Additional numerical results are reported in~\ref{appendix:numerical}.

\subsection{Contributions}
The key contributions of this paper are summarized below.
\begin{itemize}[label=$\diamond$,leftmargin=*]
    \item We propose a FISTA-based first-order method to enhance the scalability of solving~\eqref{obj:original_sparse_problem}, with a focus on efficient lower-bound computation within the BnB framework.
    \item The proximal operator in the FISTA method is computed using a customized PAVA that leverages hidden mathematical structures and enjoys log-linear time complexity, ensuring scalability for large-scale problems.
    \item Besides achieving fast convergence rates (via a restart strategy) and low per-iteration computational complexity, our method can be easily parallelized on GPUs, something not currently achievable by MIP methods.
    \item We validate the practical efficiency of our approach on both synthetic and real-world datasets, demonstrating substantial speedups in computing dual bounds and certifying optimal solutions for large-scale sparse GLMs.
    % highlighting its broader impact on optimization and machine learning.
\end{itemize}

\subsection{Related Works}
\label{sec:related_work}

\paragraph{MIP for ML.}
MIP has been successfully applied in
medical scoring systems~\citep{ustun2016supersparse, ustun2019learning, liu2022fasterrisk}, 
portfolio optimization \citep{bienstock1996computational,wei2022convex}, nonlinear identification systems~\citep{bertsimas2023learning, liu2024okridge},
decision trees~\citep{bertsimas2017optimal, hu2019optimal},
survival analysis~\citep{zhang2023optimal, liu2024fastsurvival},
hierarchical models~\citep{bertsimas2020sparse}, regression and classification models~\citep{atamturk2020safe, bertsimas2020sparse, bertsimas2020sparse1, bertsimas2020sparse2, hazimeh2020fast, xie2020scalable, atamturk2021sparse, dedieu2021learning, hazimeh2022sparse, liu2024okridge, guyard2024el0ps}, graphical models \citep{manzour2021integer, kucukyavuz2023consistent}, and outlier detection \citep{gomez2021outlier,gomez2023outlier}.
The primary focus of these works is on obtaining high-quality feasible solutions, with only a small subset addressing the certification of optimality.
Our work aims to contribute to this literature, with a strong focus on enhancing the computational scalability of certifying optimality for solving sparse GLM problems.

\paragraph{Perspective Formulations.} 
The application of perspective functions to derive convex relaxations for~\eqref{obj:original_sparse_problem} dates back to the seminal work of \citet{ceria1999convex}.
Perspective formulations have been developed for separable functions in \citep{gunluk2010perspective, xie2020scalable, wei2022ideal, bacci2019new, shafiee2024constrained} and for rank-one functions in \citep{atamturk2020supermodularity, wei2020convexification, wei2022ideal, han2021compact, shafiee2024constrained} under various conditions. 
Our work uses perspective formulations of separable functions that appear in~\eqref{obj:original_sparse_problem} as the Tikhonov regularization function. 

\paragraph{Lower Bound Calculation.}
A key aspect of certifying optimality in MIP problems is the efficient computation of tight lower bounds.
Commercial MIP solvers typically iteratively linearize the objective function using the celebrated outer approximation method~\citep{kelley1960cutting} (via cutting planes) and solve the resulting linear program~\citep{schrijver1998theory, wolsey2020integer}.
However, this approach often produce loose lower bounds, especially when high-quality linear cuts are not generated.
Alternatively, solvers may use conic convex relaxations and solve them with the interior-point method (IPM)~\citep{dikin1967iterative, renegar2001mathematical, nesterov1994interior}. 
While this approach often yields tighter lower bounds, IPM does not scale well due to its reliance on second-order information and because -- differently from the linear case -- effectively warm-starting IPMs is not possible.
Recent attempts are based on first-order methods, including subgradient descent~\citep{bertsimas2020sparse1}, ADMM~\citep{liu2024okridge}, and coordinate descent~\citep{hazimeh2022sparse}. Our work builds on this, offering faster convergence, low computational complexity, and significant GPU acceleration. We also observe that our proposed FISTA method achieves linear convergence rates empirically, a result not previously achieved by other first-order methods for this problem.


\paragraph{GPU Acceleration.}
Recently, there have been some promising works on using GPUs to accelerate continuous optimization problems, including linear programming~\citep{applegate2021practical, lu2023cupdlp}, quadratic programming~\citep{lu2023practical}, and semidefinite programming~\citep{han2024accelerating}.
%There are few works of using GPUs to solve discrete problems.
A natural way to leverage GPUs for discrete problems is by using GPU-based LPs within MIP solvers, as demonstrated by~\citet{de2024power} for solving clustering problems.
However, in \citep{de2024power}, the challenge is to approximate the original objective function with a potentially exponential number of cutting planes. %their proposed cutting-plane approach faces challenges when it requires iteratively generating an exponential number of cuts to approximate the original objective function. 
In contrast, we develop a customized FISTA method that directly handles the nonlinear objective function, while the computation can be easily parallelized since it only involves matrix-vector multiplication. 
Other first-order methods, such as ADMM~\citep{liu2024okridge} and coordinate descent~\citep{hazimeh2022sparse}, are unsuitable for GPUs: ADMM requires solving linear systems, while coordinate descent is inherently sequential.

