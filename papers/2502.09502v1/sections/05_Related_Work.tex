\section{Related Work}
\label{sec:related_work}

\paragraph{MIP for ML}
There have been some successful cases of using MIP for ML applications, including
medical scoring systems~\cite{ustun2016supersparse, ustun2019learning, liu2022fasterrisk}, 
identification of nonlinear dynamical systems~\cite{bertsimas2023learning, liu2024okridge},
decision trees~\cite{bertsimas2017optimal, hu2019optimal},
survival analysis~\cite{zhang2023optimal, liu2024fastsurvival},
hierarchical models~\cite{bertsimas2020sparse, shafiee2024constrained},
and regression models~\cite{xie2020scalable, bertsimas2020sparse1, bertsimas2020sparse2, hazimeh2020fast, atamturk2020safe, hazimeh2022sparse, liu2024okridge, guyard2024el0ps}.
% and low-rank models~\cite{}.
A large number of these works focus on obtaining high-quality feasible solutions.
Only a small portion of them focus on certifying optimality due to scalability issues.
However, for certain applications in high-stakes decision-making and scientific discovery, it is critical to certify optimality or quantify the optimality gap.
Our work aims to make a contribution in this direction, with great emphasis on improving the computational scalability of certifying optimality for a general class of ML problems --- the generalized linear models.

\paragraph{Lower Bound Calculation}
One crucial aspect of certifying optimality for large-scale datasets is to compute tight lower bounds efficiently.
For commercial MIP solvers, there are in general two approaches of calculating the lower bounds.
The first approach is to iteratively approximate the objective function with cutting planes~\cite{kelley1960cutting} and solve the related problem with linear programming~\cite{schrijver1998theory, wolsey2020integer}.
For the cutting-plane type method, the lower bound can be loose if we cannot generate many high-quality cutting planes to approximate the objective function accurately.
The second approach is to perform conic convex relaxation and solve the related problem using the interior-point method (IPM)~\cite{dikin1967iterative, renegar2001mathematical, nesterov1994interior}.
Although the lower bound is often tighter, IPM does not scale favorably because it is a second-order method.
Recently, there have been some attempts of using first-order methods to calculate the lower bound, including subgradient descent~\cite{bertsimas2020sparse1}, alternating direction method of multipliers (ADMM)~\cite{liu2024okridge}, and coordinate descent~\cite{hazimeh2022sparse}.
Our work also belongs to this line of research effort.
However, compared to previous works, ours enjoys fast convergence rate (\ToDo{first approach to achieve linear convergence rate?}), has low computational complexity per iteration, and can significantly benefit from running on GPUs.
We elaborate on the hardware aspect in more details.

% However, our work is much scalable 
% , with major differences on both convergence rate and algorithmic parameter selection.
% Previous methods have a sublinear convergence rate (at best with $O(1/k^2)$ if Nesterov acceleration is used) and may require substantial preprocessing time (for large-scale problems) to estimate the stepsizes.
% In contrast, our method has linear convergence rate and is parameter-free in the sense that the algorithm can automatically adjust the stepsizes adaptive to each data instance, thanks to the restart technique and adaptive estimation of step sizes.


% Customized schemes - first order methods
% -gradient descent - generic and slow in general
% ADMM - becomes expensive for complicated functions
% Coordinate descent - efficient, no restart
% $O(1/k)$ or $O(1/k^2)$ convergence rate
% Ours can achieve linear convergence rates, and leverage parallel computing.

\paragraph{GPU Acceleration}
Recently, there have been some promising works on using GPUs to accelerate continuous optimization, including linear programming~\cite{applegate2021practical, lu2023cupdlp}, quadratic programming (with only linear constraints)~\cite{lu2023practical}, and semidefinite programming~\cite{han2024accelerating}.
There are few works of using GPUs to solve discrete problems.
One natural way to take advantage of GPUs for discrete problems is to use the GPU-based LP inside MIP solvers, like what~\cite{de2024power} has done to solve the clustering problems.
However, this cutting-plane type approach still faces difficulties if we have to iteratively generate an exponential number of cutting planes to approximate the original objective function.
% Since our method can be easily parallelized on GPUs, it provides tremendous opportunities for large-scale computing.
In contrast, we develop a customized FISTA method to directly handle the nonlinear objective function.
The computation can be easily parallelized because we only require matrix-vector multiplication.
For other first-order methods such as  ADMM~\cite{liu2024okridge} and coordinate descent~\cite{hazimeh2022sparse} mentioned in the last subsection, they are unsuitable for GPUs.
The former requires solving linear systems while the latter is sequential in nature.
