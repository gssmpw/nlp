\appendix
\onecolumn

\part{Appendix} 

\newcommand{\appendixnumberline}[1]{Appendix\space}

\renewcommand{\appendixname}{Appendix}
\renewcommand{\thesection}{\appendixname~\Alph{section}}
\renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection}}

\section{Proofs}
\label{appendix_sec:proofs}
This section contains all omitted proofs in the paper.

\subsection{Proof of Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}}

\begin{namedlemma}
    [~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}]
    The closed convex hull of the set
    \begin{align*}
        \textstyle \left\{ (\tau, \bbeta, \bz) \middle|
        \| \bbeta \|_\infty \leq M, \, \bz \in \{0, 1\}^p, \, \mathbf{1}^\top \bz \leq k, \, \beta_j ( 1 - z_j) = 0 ~~ \forall j \in [p], \, \sum_{j \in [p]} \beta_j^2 \leq \tau \right\}
    \end{align*}
    is given by the set
    \begin{align*}
        \textstyle \left\{ (\tau, \bbeta, \bz)  \;\middle|\; -M z_j\leq \bbeta_j \leq M z_j ~ \forall j \in [p], \, \bz \in [0, 1]^p, \, \mathbf{1}^\top \bz \leq k, \, \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau \right\}.
    \end{align*}
\end{namedlemma}

\begin{proof}
    Let $\mathcal T$ represent the first set mentioned in the statement of the lemma. Using the definition of the perspective function and applying the big-M formulation technique, we have
    \begin{align*}
        \textstyle \mathcal T = \left\{ (\tau, \bbeta, \bz)  \;\middle|\; -M z_j\leq \bbeta_j \leq M z_j ~ \forall j \in [p], \, \bz \in \{0, 1\}^p, \, \mathbf{1}^\top \bz \leq k, \, \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau \right\}.
    \end{align*}
    As the epigraph of a perspective function constitutes a cone \citep[Lemma~1 \& 2]{shafiee2024constrained}, we may write $\mathcal T = \mathrm{Proj}_{(\tau, \bbeta, \bz)}(\overline {\mathcal T})$, where 
    \begin{align*}
        \textstyle \overline {\mathcal T} = \left\{ (\tau, \bbeta, \bt, \bz) \;\middle|\; \bm 1^\top \bt = \tau, \, \bz \in \{0, 1\}^p, \, \mathbf{1}^\top \bz \leq k, \, \bm A_j \begin{bmatrix} t_j \\ \beta_j \end{bmatrix} + \bm B_j z_j \in \mathbb K_j ~ \forall j \in [p] \right\}
    \end{align*}
    admits a mixed-binary conic representation with
    \begin{align*}
        \bm A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix}, \,
        \bm B = \begin{bmatrix} 0 \\ 0 \\ 0 \\ M \\ M \end{bmatrix}, \,
        \mathbb K_j = \mathbb L_+ \times \R_+ \times \R_+ \qquad \forall j \in [p].
    \end{align*}
    Here, $\mathbb L_+ \in \R^3$ denotes the rotated second order cone, that is, $\mathbb L_+ = \{ (t, \beta, z) \in \R_+ \times \R \times \R_+: \beta^2 \leq t z  \}$.
    Thus, using \citep[Lemma~4]{shafiee2024constrained}, the set $\overline{\mathcal T}$ satisfies all the requirements of \citep[Theorem~1]{shafiee2024constrained}, and therefore, its continuous relaxation gives the closed convex hull of $\overline{\mathcal T}$, that is,
    \begin{align*}
        \textstyle \cl \conv(\overline {\mathcal T}) = \left\{ (\tau, \bbeta, \bt, \bz) \;\middle|\; \bm 1^\top \bt = \tau, \, \bz \in [0, 1]^p, \, \mathbf{1}^\top \bz \leq k, \, \bm A_j \begin{bmatrix} t_j \\ \beta_j \end{bmatrix} + \bm B_j z_j \in \mathbb K_j ~ \forall j \in [p] \right\}.
    \end{align*}
    The prove concludes by applying Fourier-Motzkin elimination method to project out the variable $\bt$.
\end{proof} 

\begin{namedlemma}
    [~\ref{lemma:fenchel_conjugate_of_g_closed_form_expression}]
    The conjugate of $g$ is given by
    \begin{equation*}
        g^*(\balpha) = \TopSum_k({\bf H}_M(\balpha)).
    \end{equation*}
\end{namedlemma}

\begin{proof}
    Using the definition of the implicit function $g$ in~\eqref{eq:function_g_definition}, we have
    \begin{align}
        \label{eq:max:g*}
        g^*(\balpha) = \left\{
        \begin{array}{cl}
            \max & \balpha^\top \bbeta -  \frac{1}{2} \sum_{j \in [p]} {\beta_j^2}/{z_j} \\[1ex]
            \st & \bbeta \in \R^p, \, \bz \in [0, 1]^p, \, \bm 1^\top \bz \leq k, \\[1ex]
            & -M z_j \leq \beta_j \leq M z_j ~ \forall j \in [p]
        \end{array}
        \right.
    \end{align}
    For any fixed feasible $\bz$, the maximization problem over $\bbeta$ is a simple constrained quadratic problem, that can be solved analytically by the vector $\beta^\star$ whose $j$'th element is given by
    $\beta_j^\star = \sgn(\alpha_j) \min(\vert{\alpha_j}, M) z_j.$
    Substituting the optimizer $\beta^\star$, the objective function of the maximization problem in~\eqref{eq:max:g*} simplifies to
    \begin{align*}
        \balpha^\top \bbeta^\star - \frac{1}{2} \sum_{j \in [p]} {\beta_j^\star}^2 / z_j 
        &= \sum_{j \in [p]} \alpha_j \cdot \sgn(\alpha_j) \min(\vert{\alpha_j}, M) z_j - \frac{\left( \sgn\left( \alpha_j \right) \min\left(\vert{\alpha_j}, M \right) z_j \right)^2}{2z_j} \\
        &= \sum_{j \in [p]} ( \vert{\alpha_j} \min(\vert{\alpha_j}, M) - \frac{1}{2} \min(\alpha_j^2, M^2) ) z_j %\\
        % &= \begin{cases} \frac{1}{2} \alpha_j^2 z_j & \text{if } \vert{\alpha_j} \leq M  \\ \left( M \vert{\alpha_j} - \frac{1}{2} M^2 \right) z_j & \text{if } \vert{\alpha_j} > M
        % \end{cases} \\
        = H_M(\alpha_j) z_j,
    \end{align*}
    where the second equality holds as $\bz$ is a binary vector, and the last equality follows from the definition of the Huber loss function. We thus arrive at
    \begin{align*}
        g^*(\balpha) = \max_{\bz \in [0,1]^p} \left\{ \textstyle \sum_{j \in [p]} H_M (\alpha_j) z_j: \bm 1^\top \bz \leq k \right\} = \TopSum_k ({\mathbf{H}}_M(\balpha)).
    \end{align*}
    This completes the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}}

\begin{namedlemma}
    [~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}]
    For any $\bmu \in \R^p$, we have 
    $$\prox_{\rho g^*}(\bmu) = \sgn(\bmu) \odot \bnu^\star, $$ 
    where $\odot$ denotes the Hadamard (element-wise) product, $\bnu^\star$ is the unique solution of the following optimization problem
    \begin{align}
        \label{A:obj:KyFan_Huber_isotonic_regression}
        \begin{array}{cl}
            \min\limits_{\bnu \in \R^p} & \frac{1}{2} \sum_{j \in [p]} (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ} H_M (\nu_j) \\[2ex]
            \st & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l} ~~ \forall j, l \in [p],
        \end{array} 
    \end{align}
    and $\calJ$ is the set of indices of the top $k$ largest elements of~$ \vert{\mu_j}, j \in [p]$. 
\end{namedlemma}

\begin{proof}
    For simplicity, let $\balpha^\star = \prox_{\rho g^*}(\bmu)$, that is,
    \begin{align}
        \label{eq:alpha:star}
        \balpha^\star = \argmin_{\bm \alpha \in \R^p} ~ \frac{1}{2} \Vert{\bm \alpha - \bm \mu}_2^2 + \rho g^*(\bm \alpha).
    \end{align}
    We first show that $\sgn(\balpha^\star) = \sgn(\bmu)$ (step 1) and then establish that for every $j, l \in [p]$ with $\vert{\mu_j} \geq \vert{\mu_l}$, we have $\vert{\alpha_j^\star} \geq \vert{\alpha_l^\star}$ (step 2). We then conclude the proof using these observations.

    \begin{itemize}[label=$\diamond$,leftmargin=*]
        \item \textbf{Step 1.} We prove the sign-preserving property through a proof by contradiction. For the sake of contradiction, suppose that there exists some $j \in [p]$ such that $\sgn(\alpha_j^\star) \neq \sgn(\mu_j)$.
        Hence, we can construct a new $\balpha'$ by flipping the sign of $\alpha_j^\star$, i.e., $\alpha_j' = -\alpha_j^\star$, and keeping the rest of the elements the same as $\balpha^\star$.
        Now under the assumption that $\sgn(\alpha_j^\star) \neq \sgn(\mu_j)$, we have $\left\lvert{\alpha_j^\star - \mu_j}\right\rvert > \left\lvert{\lvert{\alpha_j^\star}\rvert - \lvert{\mu_j}\rvert}\right\rvert = \left\lvert{\alpha_j' - \mu_j}\right\rvert$, so the $j$-th term in the first summation of the objective function will decrease while everything else remains the same.
        This leads to a smaller objective value for $\balpha'$ than $\balpha^\star$, which contradicts the optimality of $\balpha^\star$.
        Thus, the claim follows.
        
        \item \textbf{Step 2.} We prove the relative magnitude-preserving property through a proof by contradiction. For the sake of contradiction, suppose that there exists some $j, l \in [p]$ such that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^\star} < \vert{\alpha_l^\star}$.
        Then, we can construct a new $\balpha'$ by swapping $\alpha_j^\star$ and $\alpha_l^\star$, i.e., $\alpha_j' = \alpha_l^\star$ and $\alpha_l' = \alpha_j^\star$, and keeping the rest of the elements the same as $\balpha^\star$.
        Under the assumption that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^\star} < \vert{\alpha_l^\star}$, we have $\left\lvert{\alpha_j^\star - \mu_j}\right\rvert + \left\lvert{\alpha_l^\star - \mu_l}\right\rvert > \left\lvert{\alpha_l^\star - \mu_j}\right\rvert + \left\lvert{\alpha_j^\star - \mu_l}\right\rvert =
        \left\lvert{\alpha_j' - \mu_j}\right\rvert + \left\lvert{\alpha_l' - \mu_l}\right\rvert$, so the sum of the $j$-th and $l$-th terms in the first summation of the objective function will decrease while everything else remains the same.
        This leads to a smaller objective value for $\balpha'$ than $\balpha^\star$, which contradicts the optimality of $\balpha^\star$. Thus, the claim~follows.
    \end{itemize}
    Using these two observations, we are ready to prove that $\balpha^\star = \sgn(\bmu) \odot \bnu^\star$.
    We first reparametrize the minimization problem~\eqref{eq:alpha:star} by substituting the decision variable $\balpha$ with a new variable $\bnu \in \R_+^p$ satisfying $\balpha = \sgn(\bmu) \odot \bnu$. By the sign-preserving property in step 1, it is easy to show the equivalence between the optimization problem in~\eqref{eq:alpha:star} and the following optimization problem
    \begin{align*}
        \min_{\bnu \in \R^p_+} ~ \textstyle \frac{1}{2} \sum_{j \in [p]} (\nu_j - \vert{\mu_j})^2 + \rho \TopSum_k \left( \mathbf{H}_M ( \bnu ) \right).
    \end{align*}
    By the relative magnitude-preserving property in step 2, we can further set the equivalence between the minimization problem in~\eqref{eq:alpha:star} and the following optimization problem
    \begin{align*}
        \begin{array}{cl}
            \displaystyle \min_{\bnu \in \R_+^p} & \frac{1}{2} \sum_{j \in [p]} (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ} H_M (\nu_j), \\ 
            \st & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l}.
        \end{array} 
    \end{align*}
    Lastly, the nonnegative constraint on $\bnu$ can be removed as the second summation term in the objective function implies that $\nu_j \geq 0$. Thus, we have shown that any feasible point $\balpha$ in the minimization problem~\eqref{eq:alpha:star} can be reconstructed by any feasible point $\bnu$ in the minimization problem in the statement of lemma, while maintaining the same objective value. Hence, we may conclude that $\balpha^\star = \sgn(\bmu) \odot \bnu^\star$, as required.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:PAVA_algorithm_exact_solution}}






%Assuming that the input vector $\bmu$ has already been sorted so that the elements are in nonincreasing order in terms of their absolute values, the algorithm runs in linear time complexity, $O(p)$, where $p$ is the number of elements in the input vector $\bmu$.

\begin{namedlemma}
    [~\ref{lemma:PAVA_algorithm_exact_solution}]
    The vector $\hat \bnu$ in Algorithm~\ref{alg:PAVA_algorithm} solves~\eqref{obj:KyFan_Huber_isotonic_regression} exactly.
\end{namedlemma}

\begin{proof}
    The minimization problem~\eqref{obj:KyFan_Huber_isotonic_regression} is an instance of a generalized isotonic regression problem taking the form
    \begin{align}
        \label{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}
        \min_{\bnu} \sum_{j=1}^{p} h_j(\nu_j) \quad \st \quad \nu_1 \geq \nu_2 \geq \cdots \geq \nu_J,
    \end{align}
    where $h_j(\nu) = \frac{1}{2} (\nu - \mu_j)^2 + \rho_j H_M(\nu)$, $\rho_j = \rho$ if $j \in \calJ$ and $\rho_j = 0$ otherwise, and the set $\calJ$ is the set of indices of top k largest elements of $\vert{\mu_j}$, as defined in the statement of Lemma~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}.
    Thanks to~\cite{best2000minimizing,ahuja2001fast}, the optimizer of~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression} satisfies two key properties: 
    \begin{itemize}[label=$\diamond$,leftmargin=*]
        \item \textbf{Property 1: Optimal solution for a merged block is single-valued.} 
        Suppose we have two adjacent blocks $[a_1, a_2]$ and $[a_2+1, a_3]$ such that the optimal solution of each block is single-valued, that is, the minimization problems
        \begin{align*}
            \left\{
            \begin{array}{cl}
                \min\limits_{\bnu_{a_1:a_2}} & \sum_{j=a_1}^{a_2} h_j(\nu_j) \\
                \st & \nu_{a_1} \geq \cdots \geq \nu_{a_2}
            \end{array}
            \right. \quad \text{and} \quad
            \left\{
            \begin{array}{cl}
                \min\limits_{\bnu_{a_2+1:a_3}} & \sum_{j=a_2+1}^{a_3} h_j(\nu_j) \\
                \st & \nu_{a_2+1} \geq \cdots \geq \nu_{a_3} \\
            \end{array}
            \right.
        \end{align*}
        are solved by $\bnu_{a_1:a_2}^\star$ and $\bnu_{a_2+1:a_3}^\star$ with $\nu_{a_1}^\star = \cdots = \nu_{a_2}^\star$ and $\nu_{a_2+1}^\star = \cdots = \nu_{a_3}^\star$, respectively.
        If $\nu_{a_1}^\star \leq \nu_{a_2+1}^\star$, then the optimal solution for the merged block $[a_1, a_3]$ is single-valued, that is, the minimization problem
        \begin{align*}
            \left\{
            \begin{array}{cl}
                \min\limits_{\bnu_{a_1:a_3}} & \sum_{j=a_1}^{a_3} h_j(\nu_j) \\
                \st & \nu_{a_1} \geq \cdots \geq \nu_{a_3}
            \end{array}
            \right.
        \end{align*}
        is solved by $\bnu_{a_1:a_3}^\star$ with $\nu_{a_1}^\star = \cdots = \nu_{a_3}^\star$.

        \item \textbf{Property 2: No isotonic constraint violation between single-valued blocks implies the solution is optimal.} Suppose that we have $s$ blocks $[a_1, a_2], [a_2+1, a_3], \ldots, [a_{s}+1, a_{s+1}]$ (with $a_1=1$ and $a_{s+1}=p$) such that the optimal solution for each block is single-valued, that is, $\nu^\star_{a_l+1} = \dots = \nu^\star_{a_{l+1}}$ for all $l \in [s]$. Then, if $\hat{\nu}_{a_1} \geq \hat{\nu}_{a_2+1} \geq \ldots \hat{\nu}_{a_{s}}$, then $\hat{\bnu}$ is the optimal solution to~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}.
    \end{itemize}
    
    Using these two properties, it is now easy to see why Algorithm~\ref{alg:PAVA_algorithm} returns the optimal solution. 
    We start by constructing blocks which have length 1.
    The initial value restricted to each block is optimal.
    Then, we iteratively merge adjacent blocks and update the values of $\nu_j$'s whenever there is a violation of the isotonic constraint.
    By the first property, the optimal solution for the merged block is single-valued.
    Therefore, we can compute the optimal solution for the merged block by solving a univariate optimization problem.
    We keep merging blocks until there is no isotonic constraint violation.
    When this happens, by construction, the solution for each block is single-valued and optimal.
    By the second property, the final vector $\hat{\bnu}$ is the optimal solution to~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}, as required.
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:PAVA_merging_linear_time_complexity}}

\begin{namedlemma}
    [~\ref{lemma:PAVA_merging_linear_time_complexity}]
    The merging step (lines 11-14) in Algorithm~\ref{alg:PAVA_algorithm} can be performed in linear time complexity $\mathcal O(p)$.
\end{namedlemma}

\begin{proof}
A detailed implementation of line 11-14 (Step 3) of the PAVA Algorithm~\ref{alg:PAVA_algorithm} that achieves a linear time complexity is presented in Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA}. In the following, we first show that Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA} accomplishes the objective in lines 11-14 of Algorithm~\ref{alg:PAVA_algorithm}. We then establish that Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA} runs in linear time complexity.

\input{sections/Algorithmic_Charts/up_and_down_block_algorithm_for_merging_in_PAVA.tex}

To prove the first claim, we show that the parameters $P_b, S_b,$ and $\nu_b$ amount to
\begin{align*}
    \textstyle
    P_b = \sum_{j \in \calB(b)} \rho_j, ~ 
    S_b = \sum_{j \in \calB(b)} \vert{\mu_j}, ~ 
    \nu_b = \prox_{\sum_{j \in \calB(b)} \rho_j H_M}(|\mu_j|)
\end{align*}
for each block index $b$, where $\calB(b)$ denoting the set of indices in the $b$'th block. It is easy to verify that Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA} recursively computes $P_b$ and $S_b$. Thus, we will focus on $\nu_b$.
Note that the computation of the proximal operator in $\nu_b$ is reduced to solving a univariate optimization problem for each $b$ and satisfies
\begin{align*}
    \nu_b =& \argmin_{v \in \R} \sum_{j \in \calB(b)} \left( \frac{1}{2} (v - \vert{\mu_j})^2 + \rho_j H_M(v) \right) \\
    %= & \argmin_{v} \sum_{j \in \calB(b)} \left( \frac{1}{2} (v^2 - 2v\vert{\mu_j} + \mu_j^2) + \rho_j H_M(v) \right) \\
    = & \argmin_{v} \sum_{j \in \calB(b)} \left( \frac{1}{2} v^2 - v\vert{\mu_j} + \rho_j H_M(v) \right) \\
    %= & \argmin_{v} \left( \sum_{j \in \calB(b)} \frac{1}{2} v^2 - \sum_{j \in \calB(b)} v\vert{\mu_j} + \sum_{j \in \calB(b)} \rho_j H_M(v) \right) \\
    %= & \argmin_{v} \left( N_b \frac{1}{2} v^2 - S_b \vert{\mu_j} + P_b H_M(v) \right) \\
    = & \argmin_{v} \left( \frac{1}{2} v^2 - \frac{S_b}{N_b} \vert{\mu_j} + \frac{P_b}{N_b} H_M(v) \right) 
    = \argmin_{v} \left( \frac{1}{2} \left( v - \frac{S_b}{N_b} \right)^2 + \frac{P_b}{N_b} H_M(v) \right) 
    = \prox_{\frac{P_b}{N_b} H_{M}}(\frac{S_b}{N_b}).
\end{align*}

Thus, Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA} merges two adjacent blocks if the isotonic violation persists, and the output of the proximal operator is the minimizer of the univariate function in the merged block.
This is exactly the same as the objective in lines 11-14 of Algorithm~\ref{alg:PAVA_algorithm}. Hence, the first claim follows.

To show that the algorithm runs in linear time, notice that in the while loop $j \leq p$ in Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA}, the variable $j$ is incremented by $1$ in each iteration, and the loop terminates when $j = p$.
Although there are two while loops inside the main while loop, the total number of iterations in the two inner while loops is at most $p$.
This is because we start with $p$ blocks, and each iteration of the inner while loops either merges two blocks forward or merges two blocks backward.
The total number of merging operations is at most $p-1$.
Thus, the total number of iterations in the while loop $j \leq p$ is at most $p$.
Lastly, since we can evaluate the proximal operator of the Huber loss function, $\mathbf{H}_M$, in constant time complexity, the total time complexity of Algorithm~\ref{alg:up_and_down_block_algorithm_for_merging_in_PAVA} is $O(p)$.
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:pava_algorithm_linear_time_complexity_and_exact_solution}}

\begin{namedtheorem}
    [~\ref{theorem:pava_algorithm_linear_time_complexity_and_exact_solution}]
    For any $\bmu \in \R^p$, Algorithm~\ref{alg:PAVA_algorithm} returns the \textit{exact} evaluation of $\prox_{\rho g^*}(\bmu)$ in $\tilde {\mathcal O}(p)$.
\end{namedtheorem}

\begin{proof}
    By Lemmas~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression} and~\ref{lemma:PAVA_algorithm_exact_solution}, the output of Algorithm~\ref{alg:PAVA_algorithm} computes $\prox_{\rho g^*}$ exactly. 
    The linear time complexity statement also holds thanks to Lemma~\ref{lemma:PAVA_merging_linear_time_complexity}.
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:compute_g_value_algorithm_correctness}}

\begin{namedtheorem}
    [~\ref{theorem:compute_g_value_algorithm_correctness}]
        For any $\bbeta \in \R^p$, Algorithm~\ref{alg:compute_g_value_algorithm} computes the exact value of $g(\bbeta)$, defined in~\eqref{eq:function_g_definition}, in $\mathcal O(p + p \log k)$.
\end{namedtheorem}

\begin{proof}
% Add proof content here

We first show that the algorithm correctly computes the value of $g(\bbeta)$ and then analyze its computational complexity. Define the mixed-binary set
\begin{align*}
    \calS_0 = \left\{ (t, \bbeta) \;\middle|\; \textstyle \frac{1}{2} \sum_{j \in [p]} \beta_j^2 \leq t, \, \|\bbeta \|_\infty \leq M, \, \|\bbeta \|_0 \leq k \right\}.
\end{align*}
Using the perspective and big-M reformulation techniques, the set $\calS_0$ admits the equivalent representation
\begin{align*}
    \calS_0 = \left\{ (t, \bbeta) \;\middle|\; \exists \bz \in \{0,1\}^p ~ \st ~ \textstyle \frac{1}{2} \sum_{j \in [p]} \beta_j^2 / z_j \leq t, \, \bm 1^\top \bz \leq k, \, -M z_j \leq \beta_j \leq M z_j ~~ \forall j \in [p] \right\}.
\end{align*}
Following the proof of Lemma~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}, one can show that the closed convex hull of $\calS_0$ is given by 
\begin{align*}
    \cl \conv(\calS_0) = \left\{ (t, \bbeta) \;\middle|\; \exists \bz \in [0,1]^p ~ \st ~ \textstyle \frac{1}{2} \sum_{j \in [p]} \beta_j^2 / z_j \leq t, \, \bm 1^\top \bz \leq k, \, -M z_j \leq \beta_j \leq M z_j ~~ \forall j \in [p] \right\}.
\end{align*}
Therefore, the implicit function $g$ can be written as the evaluation of the support function of $\cl\conv(\calS_0)$ at $(1, \bm 0)$, that is,
\begin{align}
    \label{eq:g:S0}
    g(\bbeta) = \min  \{ t : (t, \bbeta) \in \cl\conv(\calS_0) \}.
\end{align}
Notice that the set $\calS_0$ is sign- and permutation-invariants. Hence, by ~\citep[Theorem~4]{kim2022convexification}, its closed convex hull admits the following (different) lifted represenation
\begin{align}
    \label{eq:diff:conv}
    \cl \conv(\calS_0) = \left\{ (t, \bbeta) \;\middle|\; \exists \bphi \in \R^p ~ \st ~
    \begin{array}{l}
        \frac{1}{2} \sum_{j \in [p]} \phi_j^2 \leq t, \, \vert{\bbeta} \preceq_m \bphi, \\
        0 \leq \phi_k \leq \ldots \leq \phi_1 \leq M, \\
        \phi_{k+1} = \phi_{k+2} = \ldots = \phi_n = 0 
    \end{array}
    \right\},
\end{align}
where the absolute value operator $\vert{\cdot}$ is applied to a vector in an element-wise fashion, and the constraint $\vert{\bbeta} \preceq_m \bphi$ denotes that $\bphi$ majorizes $\vert{\bbeta}$, that is,
\begin{align*}
    \textstyle \vert{\bbeta} \preceq_m \bphi  \quad \iff \quad  \sum_{j \in [l]} \vert{\beta_j} \leq \sum_{j \in [l]} \phi_j \quad \forall l \in [p-1] \quad \text{and} \quad \sum_{j \in [p]} \phi_j = \sum_{j \in [p]} \vert{\beta_j}.
\end{align*}
Using this alternative convex hull description of $\calS_0$ in~\eqref{eq:diff:conv} and the implicit formulation~\eqref{eq:g:S0}, we may conclude that
\begin{align}
    g(\bbeta) = \min\limits_{\bphi \in \R^p}
    \textstyle \left\{ \frac{1}{2} \sum_{j \in [p]} \phi_j^2 :  \vert{\bbeta} \preceq_m \bphi, \, 0 \leq \phi_k \leq \ldots \leq \phi_1 \leq M, \, \phi_{k+1} = \phi_{k+2} = \ldots = \phi_n = 0
    \right\}. \label{appendix_obj:compute_g_value_majorization_formulation}
\end{align}
In the following we show that Algorithm~\ref{alg:compute_g_value_algorithm} can efficiently solve the minimization problem in~\eqref{appendix_obj:compute_g_value_majorization_formulation}. At the first iteration $j=1$ of the algorithm, we have
\begin{align*}
    \textstyle k \phi_1 \geq \sum_{j \in [k]} \phi_j = \sum_{j \in [p]} \phi_j \geq \sum_{j \in [p]} \vert{\beta_j} \quad \Rightarrow \quad \phi_1 \geq \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j}.
\end{align*}
At the same time, we also need to satisfy $\vert{\beta_1} \leq \phi_1$ from the first majorization constraint. We now discuss two cases
\begin{itemize}[label=$\diamond$,leftmargin=*]
    \item \textbf{Case 1:} If $\frac{1}{k} \sum_{j \in [p]} \vert{\beta_j} \geq \vert{\beta_1}$, in order to solve the minimization problem in~\eqref{appendix_obj:compute_g_value_majorization_formulation}, we set $\phi_1 = \frac{1}{k} \sum_{j=1}^n \vert{\beta_j}$. Notice that $\phi_1 \leq M$ is automatically satisfied because $\phi_1 = \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j} = \frac{1}{k} \sum_{j \in [p]} M z_j \leq M$. This leads to $\phi_2 = \ldots = \phi_k = \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j}$.
    To see this, for the sake of contradition, assume that $\exists j \in \{2, \ldots, k\}$ such that $\phi_j < \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j}$. 
    Since $\phi_j \leq \phi_1 = \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j}$, we have $\sum_{j \in [k]} \phi_j < \sum_{j \in [k]} \frac{1}{k} \sum_{j \in [p]} \vert{\beta_j} = \sum_{j \in [p]} \vert{\beta_j}$, which contradicts the majorization constraint.

    \item \textbf{Case 2:} If $\frac{1}{k} \sum_{j \in [n]} \vert{\beta_j} < \vert{\beta_1}$, we can set $\phi_1 = \vert{\beta_1}$. Notice that $\phi_1 \leq M$ is automatically satisfied because $\vert{\beta_1} \leq M z_1 \leq M$.
    Then we are left with $k-1$ coefficients to set, and we can follow the same argument as we did for $j=1$ with slight difference that the majorization constraints are changed to
    \begin{align*}
        \textstyle
        \sum_{j=2}^l \phi_j \geq \sum_{j=2}^l \vert{\beta_j} \quad \forall l \in \{2, \ldots, p-1\} \quad \text{and} \quad \sum_{j=2}^p \phi_j = \sum_{j=2}^p \vert{\beta_j}.
    \end{align*}
\end{itemize}
We repeat this process until we set all $k$ coefficients $\phi_1, \ldots, \phi_k$, as implemented by Algorithm~\ref{alg:compute_g_value_algorithm}.
The output of the algorithm coincides with the optimal value of the minimization problem in~\eqref{appendix_obj:compute_g_value_majorization_formulation}. Hence, the first claim follows.

As for the complexity claim, it is easy to see that Algorithm~\ref{alg:compute_g_value_algorithm}.
only requires partial sorting step on Line 2, which has a complexity of $\mathcal O(p \log k)$. The summation step on Line 3 has a complexity of $\mathcal O(p)$. The for-loop step on Line 4-8 has a complexity of $\mathcal O(k)$, so does the final summation step on Line 9. Therefore, the overall computational complexity of Algorithm~\ref{alg:compute_g_value_algorithm} is $\mathcal O(p + p \log k)$. This concludes the proof.
\end{proof}

\newpage
\section{Experimental Setup Details}
\label{appendix:experimental_setup}

\subsection{Setup for Evaluating Proximal Operators}
\label{appendix:setup_for_evaluating_proximal_operators}
The synthetic data generation process is as follows.
We sample the input vector $\bgamma \in \bbR^p$ from the standard multivariate Gaussian distribution, $\bgamma \sim \calN(\mathbf{0}, \bI_p)$, where $\bI_p$ denotes the identity matrix with dimension $p$.
We vary the dimension $p \in \{2^0, 2^1, ..., 2^{10}\} \times 10^2$ and set the cardinality $k$ to be $10$, the box constraint $M$ to be $1.0$, and the weight parameter $\rho$  to be $1.0$.
We report the running time for evaluating these proximal operators.
To obtain the mean and standard deviation of the running time, we repeat each setting 5 times, each with a different random seed.

\subsection{Setup for Solving the Perspective Relaxation}
\label{appendix:setup_for_solving_the_perspective_relaxation}

We generate our synthetic datasets in the following procedure.
First, we sample each feature vector $\bx_i \in \bbR^p $ from a Gaussian distribution, $\bx_i \sim \calN(\mathbf{0}, \bSigma)$, where the covariance matrix has entries $\Sigma_{jl} = \sigma^{\vert{j-l}}$.
The variable $\sigma \in (0, 1)$ controls the features correlation: if we increase $\sigma$, feature columns in the design matrix $\bX$ become more correlated.
Throughout the experimental section, we set $\sigma=0.5$.
Next, we create the sparse coefficient vector $\bbeta^*$ with $k$ equally spaced nonzero entries, where $\beta^*_j = 1$ if $j \text{ mod } (p/k) = 0$ and $\beta^*_j = 0$ otherwise.
After these two steps, we build the prediction vector $\by$.
If our loss function is squared error loss (regression task), we set $y_i = \bx_i^T \bbeta^* + \epsilon_i$, where $\epsilon_i$ is a Gaussian random noise with $\epsilon_i \sim \calN(0, \frac{\Vert{\bX \bbeta^*}}{\text{SNR}})$, and $\text{SNR}$ stands for the signal-to-noise ratio.
In all our experiments, we choose $\text{SNR}=5$.
If our loss function is logistic loss (classification task), we set $y_i \sim Bern(\bx_i^T \bbeta^* + \epsilon_i)$, where $Bern(P)$ is a Bernoulli random variable with $\bbP(y_i = 1) = P$ and $\bbP(y_i = -1) = 1 - P$.
For this experiment, we vary the feature dimension $p \in \{1000, 2000, 4000, 8000, 16000\}$.
We control the sample size by using a parameter called $n$-to-$p$ ratio, or sample to feature ratio.
For the results in the main paper, we set $n$-to-$p$ ratio to be $1.0$, the box constraint $M$ to be $2$, the number of nonzero coefficients k (also the cardinality constraint) to be $10$, and $\ell_2$ regularization coefficient $\lambda_2$ to be $1.0$.
Again, we report and compare the running times, with means and standard deviations calculated based on 5 repeated simulations with different random seeds.

\subsection{Setup for Certifying Optimality}
\label{appendix:setup_for_certifying_optimality}

\paragraph{Datasets and Preprocessing}
We run on both synthetic and real-world datasets.
For the synthetic datasets, we run on the largest synthetic instances ($n=16000$ and $p=16000$).
For the real-world datasets, we use the dataset cancer drug response~\cite{liu2020deepcdr} for linear regression and DOROTHEA~\cite{asuncion2007uci} for logistic regression.

The cancer drug response dataset has 822 samples and orginally has 34674 features.
However, many feature only has a single value, so we prune all these features, which result in 2200 features.
The DOROTHEA dataset has 1950 samples and 100000 features.
After pruning redundant features, we have 91598 features.

For both the cancer drug response and DOROTHEA dataset, we center each feature to have mean $0$ and norm equal to $1$.

\paragraph{Choice of Hyperparameters}
For the cardinality constraint $k$, we set $k=10$ for both synthetic datasets.
For the cancer drug response dataset, we set $k=5$.
For DOROTHEA, we set $k=15$.
In practice, this choice can be made more judiciously by doing 5 fold cross validation with a heuristic sparse learning algorithm first.
However, since our emphasis here is simply to compare certification speed, we just pick a variety of $k$'s.

For the $\ell_2$ regularization coefficient, we set $\lambda_2=1$.
For the box constraint, we set $M=2$ for the synthetic datasets and DOROTHEA.
The infinity norm of the final optimal solution less than this value.
For the cancer drug response dataset, we set $M=5$, which is also bigger than the infinity norm of the final optimal solution.

\paragraph{Branch and Bound}
For our method, we write a customized branch-and-bound (BnB) framework.
We use Algorithm~\ref{alg:main_algorithm} to solve the relaxation at each node and use Equation~\eqref{eq:fenchel_duality_theorem_F_y(Ax)+G(x)} to calculate the safe lower bound to prune the search space.
To find feasible solutions, we use an effective approach called beamsearch~\cite{liu2022fasterrisk} from the existing literature.
For branching, we branch on the feature based on the best feasible solution found by the beamsearch algorithm at each node.
For the nonzero coefficients of this solution, we branch on the variable which would lead to the largest loss increase if the coefficient to $0$.
The intuition is that such a variable is important and should be branched early in the BnB framework.

\subsection{Computing Platforms}
When investigating how much GPU can accelerate our computation, we run the experiments with both CPU and GPU implementations on the Nvidia RTXA5000s.
For everything else, we run the experiments with the CPU implementation on AMD Milan with CPU speed 2.45 Ghz and 8 cores.

\section{Additional Numerical Results}
\label{appendix:numerical}

% \subsection{Perturbation Study regarding Proximal Operators}
% \label{appendix:numerical_proximal_operators}

% \subsubsection{Perturbation Study on $k$ Values}

% \subsubsection{Perturbation Study on $M$ Values}

% \subsubsection{Perturbation Study on $\rho$ Values}

\subsection{Perturbation Study regarding Solving the Perspective Relaxation}
\label{appendix:numerical_solve_convex_relaxation}

\subsubsection{Perturbation Study on $M$ Values}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_1.2.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=1.2$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_1.2_lambda2_1.0_n_p_ratio_1.0}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_1.5.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=1.5$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_1.5_lambda2_1.0_n_p_ratio_1.0}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_3.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=3.0$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_3.0_lambda2_1.0_n_p_ratio_1.0}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_5.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=5.0$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_5.0_lambda2_1.0_n_p_ratio_1.0}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_10.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=10.0$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_10.0_lambda2_1.0_n_p_ratio_1.0}
\end{figure*}

\newpage

\subsubsection{Perturbation Study on $\lambda_2$ Values}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/lambda2_perturbation/convex_relaxation_comparison_lambda2_0.1.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=2.0$, $\lambda_2=0.1$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_2.0_lambda2_0.1_n_p_ratio_1.0}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/lambda2_perturbation/convex_relaxation_comparison_lambda2_10.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=2.0$, $\lambda_2=10.0$, $n$-to-$p$ ratio to be 1.}
    \label{fig:solve_convex_relaxation_M_2.0_lambda2_10.0_n_p_ratio_1.0}
\end{figure*}

\newpage

\subsubsection{Perturbation Study on $n$-to-$p$ Ratios}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/n_p_ratio_perturbation/convex_relaxation_comparison_n_p_ratio_0.1_M_2.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=2.0$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 10.0.}
    \label{fig:solve_convex_relaxation_M_2.0_lambda2_1.0_n_p_ratio_10.0}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{sections/Plots/n_p_ratio_perturbation/convex_relaxation_comparison_n_p_ratio_10.0_M_2.0.png}
    \caption{Solve the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}.
    We set $M=2.0$, $\lambda_2=1.0$, $n$-to-$p$ ratio to be 0.1.}
    \label{fig:solve_convex_relaxation_M_2.0_lambda2_1.0_n_p_ratio_0.1}
\end{figure*}

\newpage

\section{Additional Discussions}
We first provide common calculus rules for conjugate functions, whose proof can be found in standard optimization textbooks such as~\citep{beck2017first}.

\begin{itemize}[label=$\diamond$,leftmargin=*]
    \item \textbf{Separable Sum Rule:} Let $f(\bx) = \sum_{j \in [p]} f_j(x_j)$, where $f_j: \R \rightarrow \R$ is convex for all $j \in [p]$. Then, the conjugate of $f$ is given by $f^*(\bmu) = \sum_{j \in [p]} f_j^*(\mu_j)$.
    
    \item \textbf{Scalar Multiplication Rule:}  Let $g : \R^p \to \R$ be convex and $\alpha > 0$ be a scalar. Then, the conjugate of $f(\bx) = \alpha g(\bx)$ is given by $f^*(\bmu) = \alpha g^*(\bmu/\alpha)$.
    
    \item \textbf{Addition to Affine Function Rule:} Let $g : \R^p \to \R$ be convex and $\ba, \bb \in \mathbb{R}^p$ be two vectors. Then, the conjugate of $f(\bx) = g(\bx) + \ba^\top\bx + b$ is given by $f^*(\bmu) = g^*(\bmu - \ba) - \bb$.
    
    \item \textbf{Composition with Invertible Linear Mapping Rule:} Let $g : \R^p \to \R$ be convex and $\bA \in \mathbb{R}^{p \times p}$ be an invertible matrix. Then, the convex conjugate of $f(\bx) = g(\bA \bx)$ is given by $f^*(\bmu) = g^*(\bA^{-\top} \bmu)$.
    
    \item \textbf{Infimal Convolution Rule:} Let $g, h : \R^p \to \R$ be convex. Then, the convex conjugate of $f(\bx) = \inf_{by} ~ g(\by) + h(\bx - \by)$ is given by $f^*(\bmu) = g^*(\bmu) + h^*(\bmu)$.
\end{itemize}
These rules are useful for discussions in~\ref{appendix_sec:convex_conjugate_for_GLM_loss_functions} and~\ref{appendix_sec:safe_lower_bound_more_discussions}.


\subsection{Convex Conjugate for GLM Loss Functions}
\label{appendix_sec:convex_conjugate_for_GLM_loss_functions}

The convex conjugates of some of GLM loss functions are summarized bellow.
\begin{itemize}[label=$\diamond$,leftmargin=*]
    \item \textbf{Linear Regression:} 
    $$F(\bX \bbeta) = \Vert{\bX \bbeta - \by}_2^2 \quad \& \quad F^*(-\bzeta) = \frac{1}{4} \Vert{\bzeta}_2^2 - \by^T \bzeta.$$
    \item \textbf{Logistic Regression:} 
    $$F(\bX \bbeta) = \sum_{i \in [n]} \log(1 + \exp(-y_i (\bX \bbeta)_i)) \quad \& \quad F^*(-\bzeta) = \sum_{i \in [n]} \left( 1- \frac{\zeta_i}{y_i} \right) \log \left( 1-\frac{\zeta_i}{y_i} \right) + \frac{\zeta_i}{y_i} \log \left( \frac{\zeta_i}{y_i} \right).$$ 
    \item \textbf{Poisson Regression:} 
    $$F(\bX \bbeta) = \sum_{i \in [n]} \left( \exp(\bX \bbeta)_i - y_i (\bX \bbeta)_i \right) \quad \& \quad F^*(-\bzeta) = \sum_{i \in [n]} h(-\zeta_i + y_i), $$
    where $h(z) = z \log(z) - z$ if $z > 0$ and $h(z)=0$ if $z = 0$.
    \item \textbf{Gamma Regression:}
    $$F(\bX \bbeta) = \sum_{i \in [n]} \left( y_i \exp(-(\bX \bbeta)_i) + (\bX \bbeta)_i\right) \quad \& \quad F^*(-\bzeta) = \sum_{i \in [n]} y_i h(\frac{1-\zeta_i}{y_i}), $$
    where $h(z) = z \log(z) - z$ if $z > 0$ and $h(z)=0$ if $z = 0$.
    \item \textbf{Squared Hinge Loss:}
    For binary classification with labels $y_i \in \{-1, +1\}$,
    $$F(\bX \bbeta) = \sum_{i \in [n]} \max(0, 1-y_i (\bX \bbeta)_i)^2 \quad \& \quad F^*(-\bzeta) = \sum_{i \in [n]}  h(- y_i \zeta_i),$$
    where $h(z) = z + \frac{z^2}{4}$ if $z \leq 0$ and $h(z)=\infty$ if $z > 0$.
    % \item \textbf{Multinomial Logistic Regression:}
    % For multiclass classification with $K$ classes with coefficients $\bbeta \in \mathbb{R}^{p \times K}$, let $y_{ik}$ be a binary indicator such that $y_{ik}=1$ if the $i$-th sample belongs to class $k$, and $y_{ik}=0$ otherwise.
    % $$F(\bX \bbeta) = \sum_{i \in [n]} \left( \log\left( \sum_{j=1}^K \exp((\bX \bbeta)_{ij}) \right) - \sum_{k=1}^K y_{ik} (\bX \bbeta)_{ik} \right) $$
    % $$F^*(-\bzeta) = \sum_{i \in [n]} \begin{cases}
    %     \sum_{k=1}^K (y_{ik} - \zeta_{ik}) \log(y_{ik} - \zeta_{ik}) & \text{if } \sum_{k=1}^K \zeta_{ik} = 0, \zeta_{ik} \le y_{ik} \text{ for all } k \\
    %     +\infty & \text{otherwise} \end{cases} $$
\end{itemize}




\subsection{Safe Lower Bound}
\label{appendix_sec:safe_lower_bound_more_discussions}

The linear regression problem with eigen-perspective relaxation is formulated as
\begin{align*}
    P^\star_{\text{eig-conv}} = \min_{\bbeta \in \R^p} \bbeta^\top \bQ_{\text{eig}} \bbeta - 2\by^\top \bX \bbeta +  2 \lambda_{\text{eig}} g(\bbeta),
\end{align*}
where $\bQ_{\text{eig}} = \bX^\top \bX - \lambda_{\text{min}}(\bX^\top \bX) \bI$, $\lambda_{\text{eig}} = \lambda_2 + \lambda_{\text{min}}(\bX^\top \bX)$, and $\lambda_{\text{min}}(\cdot)$ denotes minimum eigenvalue of the input matrix.
Using the standard version of weak duality theorem, we have
\begin{align*}
    P_{\text{MIP}}^\star \geq P_{\text{eig-conv}}^\star \geq - F^*(-\hat{\bzeta}) - G^*(\hat{\bzeta}),
\end{align*}
where $F(\bbeta)= \bbeta^\top \bQ_{\text{eig}} \bbeta$, $G(\bbeta) = -2\by^\top \bX \bbeta + 2 \lambda_{\text{eig}} g(\bbeta)$, and $\hat{\bzeta} = -\nabla F(\hat{\bbeta}) = -2\bQ_{\text{eig}} \hat{\bbeta}$.
The conjugate functions admit the following closed form expressions
\begin{align*}
    F^*(-\hat{\bzeta}) &= \frac{1}{4} \hat{\bzeta}^\top \bQ_{\text{eig}}^{\dagger} \hat{\bzeta} = \hat{\bbeta} \bQ_{\text{eig}} \hat{\bbeta} \quad \& \quad G^*(\hat{\bzeta}) = 2\lambda_{\text{eig}} \, g^* \left(\frac{-\bQ_{\text{eig}}\hat{\bbeta} +  \bX^\top \by}{\lambda_{\text{eig}}} \right), 
\end{align*}
where we use $(\cdot)^{\dagger}$ to denote the pseudo-inverse of a matrix. We may conclude that
\begin{align*}
    P_{\text{MIP}}^\star & \geq \hat{\bbeta} \bQ_{\text{eig}} \hat{\bbeta} + 2\lambda_{\text{eig}} \, g^* \left(\frac{-\bQ_{\text{eig}}\hat{\bbeta} +  \bX^\top \by}{\lambda_{\text{eig}}}\right).
\end{align*}
The above lower bound can be viewed as a generalization of the safe lower bound formula from~\citep[Theorem~3.1]{liu2024okridge}. Specifically, as $M$ approaches $\infty$, the above lower bound matches the lower bound in in~\citep[Theorem~3.1]{liu2024okridge}. 
Furthermore, Our proof uses a simple weak duality argument and is concise, in contrast to the lengthy two-page algebraic proof of~\citep[Theorem~3.1]{liu2024okridge}.
