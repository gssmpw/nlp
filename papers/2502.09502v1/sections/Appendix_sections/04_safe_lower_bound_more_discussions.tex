\section{Safe Lower Bound - Additional Discussions}
\label{appendix_sec:safe_lower_bound_more_discussions}

As discussed in Section~\ref{subsec:safe_lower_bounds_for_glms}, when our objective function does not fit into the standard form $F_{\by}(\bX \bbeta) + G(\bbeta)$, we need to be creative in deriving the safe lower bound.
The key is to be flexible by trying different versions of the Fenchel Duality Theorem as well as applying the fundamental calculus rules for convex conjugates.
We illustrate this point by deriving a safe lower bound for linear regression with eigen-perspective relaxation~\cite{liu2024okridge}, which is a special case that does not fit into the standard form.

The objective function for linear regression with eigen-perspective relaxation is:
\begin{align}
    \label{obj:eigen_perspective_linear_regression}
    P^*_{\text{eig-conv}} = \min_{\bbeta} \bbeta^T \bQ_{\text{eig}} \bbeta - 2\by^T \bX \bbeta +  2 \lambda_{\text{eig}} g(\bbeta),
\end{align}
where $\bQ_{\text{eig}} = \bX^T \bX - \lambda_{\text{min}}(\bX^T \bX) \bI$, and $\lambda_{\text{eig}} = \lambda_2 + \lambda_{\text{min}}(\bX^T \bX)$, and $\lambda_{\text{min}}(\cdot)$ denotes minimum eigenvalue of the input matrix.
If $\lambda_{\text{min}}(\bX^T \bX) > 0$, we cannot write Equation~\eqref{obj:eigen_perspective_linear_regression} in the form of $F(\bX \bbeta) + G(\bbeta)$ and thus cannot apply the formula in Equation~\eqref{eq:fenchel_duality_theorem_F_y(Ax)+G(x)}.

To derive a safe lower bound, we write the objective function in the form of $F(\bbeta)+G(\bbeta)$, where $F(\bbeta)= \bbeta^T \bQ_{\text{eig}} \bbeta$ and $G(\bbeta) = -2\by^T \bX \bbeta + 2 \lambda_{\text{eig}} g(\bbeta)$.
Using the standard version of the Fenchel Duality Theorem from Equation~\eqref{eq:fenchel_duality_theorem_G(x)+F(x)}, we have:
\begin{align}
    \label{eq:fenchel_duality_theorem_G(x)+F(x)_linear_regression_eigen_perspective}
    P_{\text{MIP}}^* \geq P_{\text{eig-conv}}^* \geq - F^*(-\hat{\bzeta}) - G^*(\hat{\bzeta}),
\end{align}
where $\hat{\bzeta} = -\nabla F(\hat{\bbeta}) = -2\bQ_{\text{eig}} \hat{\bbeta}$.
For the two Fenchel conjugates, we have
\begin{align}
    F^*(-\hat{\bzeta}) &= \frac{1}{4} \hat{\bzeta}^T \bQ_{\text{eig}}^{\dagger} \hat{\bzeta} = \hat{\bbeta} \bQ_{\text{eig}} \hat{\bbeta} \label{eq:linear_regression_eigen_perspective_fenchel_conjugate_quadratic_form} \\
    G^*(\hat{\bzeta}) &= 2\lambda_{\text{eig}} \, g^* \left(\frac{-\bQ_{\text{eig}}\hat{\bbeta} +  \bX^T \by}{\lambda_{\text{eig}}} \right), \label{eq:linear_regression_eigen_perspective_fenchel_conjugate_linear_plus_g}
\end{align}
where we use $(\cdot)^{\dagger}$ for matrix pseudo-inverse in Equation~\eqref{eq:linear_regression_eigen_perspective_fenchel_conjugate_quadratic_form}, and we derive Equation~\eqref{eq:linear_regression_eigen_perspective_fenchel_conjugate_linear_plus_g} based on the following calculus rules for convex conjugates.
See Appendix~\ref{appendix_sec:convex_conjugate_calculus_rules} for the details of these rules.
\begin{align}
    % \label{eq:calculus_rules_for_fenchel_conjugate_properties}
    G(\bbeta) = g(\bbeta) - \bb^T \bbeta \quad & \Rightarrow \quad G^*(\bzeta) = g^*(\bzeta + \bb) \nonumber \\
    G(\bbeta) = a \, g(\bbeta), \, a>0 \quad & \Rightarrow \quad G^*(\bzeta) = a\, g^*(\frac{\bzeta}{a}) \nonumber
\end{align}
By plugging Equation~\eqref{eq:linear_regression_eigen_perspective_fenchel_conjugate_quadratic_form} and Equation~\eqref{eq:linear_regression_eigen_perspective_fenchel_conjugate_linear_plus_g} back into Equation~\eqref{eq:fenchel_duality_theorem_G(x)+F(x)_linear_regression_eigen_perspective}, we have the following formula to calculate the safe lower bound:
\begin{align}
    \label{eq:linear_regression_eigen_perspective_lower_bound_formula}
    P_{\text{MIP}}^* & \geq \hat{\bbeta} \bQ_{\text{eig}} \hat{\bbeta} + 2\lambda_{\text{eig}} \, g^* \left(\frac{-\bQ_{\text{eig}}\hat{\bbeta} +  \bX^T \by}{\lambda_{\text{eig}}}\right)
\end{align}
We can view Equation~\eqref{eq:linear_regression_eigen_perspective_lower_bound_formula} as a generalization of the safe lower bound formula from~\cite{liu2024okridge}.
If there is no big-M constraint (essentially letting $M = \infty$), Equation~\eqref{eq:linear_regression_eigen_perspective_lower_bound_formula} exactly matches what is given in~\cite{liu2024okridge}.
However, we are able to derive this more general formula in just a few lines, using the Fenchel Duality Theorem and calculus rules for convex conjugates.
In contrast, ~\cite{liu2024okridge} derived the simpler version by using a purely algebraic proof, which spans for two pages.
