\section{Proofs}
\label{appendix_sec:proofs}

\begin{namedlemma}
    [~\ref{lemma:equivalence_between_perspective_relaxation_and_convexification}]
    The closed convex hull of the set
    \begin{align*}
        \textstyle \left\{ (\tau, \bbeta, \bz) \middle|
        \| \bbeta \|_\infty \leq M, \, \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \, \beta_j ( 1 - z_j) = 0 ~~ \forall j \in [p], \, \sum_{j \in [p]} \beta_j^2 \leq \tau \right\}
    \end{align*}
    is given by the set
    \begin{align*}
        \textstyle \left\{ (\tau, \bbeta, \bz)  \;\middle|\; -M z_j\leq \bbeta_j \leq M z_j ~ \forall j \in [p], \, \bz \in [0, 1]^p, \, \mathbf{1}^T \bz \leq k, \, \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau \right\}.
    \end{align*}
\end{namedlemma}

\begin{proof}
    Let $\mathcal T$ represent the first set mentioned in the statement of the lemma. Using the definition of the perspective function and applying the big-M formulation technique, $\mathcal T$ can be expressed as
    \begin{align*}
        \textstyle \mathcal T = \left\{ (\tau, \bbeta, \bz)  \;\middle|\; -M z_j\leq \bbeta_j \leq M z_j ~ \forall j \in [p], \, \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \, \sum_{j \in [p]} \beta_j^2 / z_j \leq \tau \right\}.
    \end{align*}
    As the epigraph of a perspective function constitutes a cone \citep[Lemma~1 \& 2]{shafiee2024constrained}, we may write $\mathcal T = \mathrm{Proj}_{(\tau, \bbeta, \bz)}(\overline {\mathcal T})$, where the extended set $\overline {\mathcal T}$ admits the mixed-binary conic representation
    \begin{align*}
        \textstyle \overline {\mathcal T} = \left\{ (\tau, \bbeta, \bt, \bz) \;\middle|\; \bm 1^\top \bt = \tau, \, \bz \in \{0, 1\}^p, \, \mathbf{1}^T \bz \leq k, \, \bm A_j \begin{bmatrix} t_j \\ \beta_j \end{bmatrix} + \bm B_j z_j \in \mathbb K_j ~ \forall j \in [p] \right\}.
    \end{align*}
    Here, for any $j \in [p]$, the matrices $\bm A_j$ and $\bm B_j$, and the cone $\mathbb K_j$ are defined as
    \begin{align*}
        \bm A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix}, \,
        \bm B = \begin{bmatrix} 0 \\ 0 \\ 0 \\ M \\ M \end{bmatrix}, \,
        \mathbb K_j = \mathbb L \times \R_+ \times \R_+,
    \end{align*}
    where $\mathbb L \in \R^3$ denotes the rotated second order cone, that is, $\mathbb L = \{ (t, \beta, z) \in \R_+ \times \R \times \R_+: \beta^2 \leq t z  \}$.
    Thus, using \citep[Lemma~4]{shafiee2024constrained}, the set $\overline{\mathcal T}$ satisfies all the requirements of \citep[Theorem~1]{shafiee2024constrained}, and therefore, its continuous gives the closed convex hull of $\overline{\mathcal T}$, that is,
    \begin{align*}
        \textstyle \cl \conv(\overline {\mathcal T}) = \left\{ (\tau, \bbeta, \bt, \bz) \;\middle|\; \bm 1^\top \bt = \tau, \, \bz \in [0, 1]^p, \, \mathbf{1}^T \bz \leq k, \, \bm A_j \begin{bmatrix} t_j \\ \beta_j \end{bmatrix} + \bm B_j z_j \in \mathbb K_j ~ \forall j \in [p] \right\}.
    \end{align*}
    The prove concludes by applying Fourier-Motzkin elimination method to project out the variable $\bt$.
\end{proof} 

\begin{namedlemma}
    [~\ref{lemma:fenchel_conjugate_of_g_closed_form_expression}]
    The conjugate of $g$ is given by
    \begin{equation*}
        g^*(\balpha) = \TopSum_k({\bf H}_M(\balpha)).
    \end{equation*}
\end{namedlemma}

\begin{proof}
    By definition, the Fenchel conjugate is defined as
    \begin{equation}
        \label{appendix_def:fenchel_conjugate_of_g}
        g^*(\balpha) = \max_{\bbeta} \balpha^T \bbeta - g(\bbeta)
    \end{equation}

    By plugging $g(\bbeta)$ from Equation~\eqref{eq:function_g_definition} and $\calZ(\bbeta)$ from Equation~\eqref{eq:z_domain_definition} into this Fenchel conjugate definition, we have
    \begin{align}
        \label{appendix_eq:fenchel_conjugate_of_g_max_beta_max_z_formulation}
            g^*(\balpha) &= \max_{\bbeta} \balpha^T \bbeta - \min_{\bz \in \calZ(\bbeta)} \frac{1}{2} \sum_{j=1}^n \frac{\beta_j^2}{z_j} \nonumber \\
            &= \max_{\bbeta} \max_{\bz \in \calZ(\bbeta)} \balpha^T \bbeta -  \frac{1}{2} \sum_{j=1}^n \frac{\beta_j^2}{z_j}
    \end{align}

    Since Equation~\eqref{appendix_eq:fenchel_conjugate_of_g_max_beta_max_z_formulation} is jointly maximizing over $\bbeta$ and $\bz$, we can switch the order by doing the inner optimization over $\bbeta$ and outer optimization over $\bz$:
    \begin{align}
        \label{appendix_eq:fenchel_conjugate_of_g_max_z_max_beta_formulation}
        g^*(\balpha) = \max_{\bz \in \calZ} \, \max_{\bbeta \in \calB(\bz)} \balpha^T \bbeta - \frac{1}{2} \sum_{j=1}^n \frac{\beta_j^2}{z_j},
    \end{align}
    where we have set $\calZ := \{\bz \mid \bz \in [0,1]^n, \, \mathbf{1}^T \bz \leq k\}$, and set $\calB(\bz) := \{\bbeta \mid - M z_j \leq \beta_j \leq M z_j\}$.

    For the inner optimization problem in Equation~\eqref{appendix_eq:fenchel_conjugate_of_g_max_z_max_beta_formulation}, if we take the derivative with respect to $\bbeta$ and set it to zero, we have $\alpha_j - \beta_j / z_j = 0$, or $\beta_j = \alpha_j z_j$.
    Since $\bbeta$ also needs to satisfy the constraint $ - M z_j \leq \beta_j \leq M z_j$ for all $j$'s, the optimal solution $\beta_j^*$ for the inner optimization problem is 
    \begin{equation}
        \label{appendix_eq:solution_for_beta_in_terms_of_fenchel_dual_alpha}
        \beta_j^* = \text{sgn}(\alpha_j) \min(\vert{\alpha_j}, M) z_j.
    \end{equation}

    If we plug Equation~\eqref{appendix_eq:solution_for_beta_in_terms_of_fenchel_dual_alpha} into each individual term $\alpha_j \beta_j^* - \frac{{\beta_j^*}^2}{2z_j}$, we have

    \begin{align}
        \label{appendix_eq:fenchel_conjugate_of_g_each_term_reduced_to_Huber_loss}
        \alpha_j \beta_j^* - \frac{{\beta_j^*}^2}{2z_j} &= \alpha_j \text{sgn}(\alpha_j) \min(\vert{\alpha_j}, M) z_j - \frac{\left( \text{sgn}\left( \alpha_j \right) \min\left(\vert{\alpha_j}, M \right) z_j \right)^2}{2z_j} \nonumber \\
        &= \vert{\alpha_j} \min(\vert{\alpha_j}, M) z_j - \frac{1}{2} \min(\vert{\alpha_j}, M)^2 z_j \nonumber \\
        &= \left[ \vert{\alpha_j} \min(\vert{\alpha_j}, M) - \frac{1}{2} \min(\vert{\alpha_j}, M)^2 \right] z_j \nonumber \\
        &= \begin{cases}
            \frac{1}{2} \alpha_j^2 z_j & \text{if } \vert{\alpha_j} \leq M \nonumber \\
            \left( M \vert{\alpha_j} - \frac{1}{2} M^2 \right) z_j & \text{if } \vert{\alpha_j} > M
        \end{cases} \nonumber \\
        &= H_M(\alpha_j) z_j.
    \end{align}

    By plugging Equation~\eqref{appendix_eq:solution_for_beta_in_terms_of_fenchel_dual_alpha} and Equation~\eqref{appendix_eq:fenchel_conjugate_of_g_each_term_reduced_to_Huber_loss} back into Equation~\eqref{appendix_eq:fenchel_conjugate_of_g_max_z_max_beta_formulation}, we obtain
    \begin{align*}
        % \label{appendix_eq:fenchel_conjugate_of_g_top_k_sum_reformulation}
        g^*(\balpha) &= \max_{\bz \in \calZ} \left( \sum_{j=1}^n \alpha_j \beta_j^* - \frac{ {\beta_j^*}^2}{2z_j} \right)\\
        &= \max_{\bz \in \calZ} \left( \sum_{j=1}^n H_M(\alpha_j) z_j \right) \nonumber\\
        &= \text{TopSum}_k (\{H_M(\alpha_j)\}_{j=1}^n), \nonumber \\
        &= \text{TopSum}_k (H_M(\balpha)).
    \end{align*}
\end{proof}

\begin{namedlemma}
    [~\ref{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}]
    For any $\bmu \in \R^p$, we have 
    $$\prox_{\rho g^*}(\bmu) = \sgn(\bmu) \odot \bnu^\star, $$ 
    where $\odot$ denotes the Hadamard (element-wise) product, $\bnu^\star$ is the unique solution of the following optimization problem
    \begin{align}
        \label{obj:KyFan_Huber_isotonic_regression}
        \begin{array}{cl}
            \min\limits_{\bnu \in \R^p} & \frac{1}{2} \sum_{j \in [p]} (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ} H_M (\nu_j) \\[2ex]
            \st & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l} ~~ \forall j, l \in [p],
        \end{array} 
    \end{align}
    and $\calJ$ is the set of indices of the top $k$ largest elements of~$ \vert{\mu_j}, j \in [p]$. 
\end{namedlemma}

\begin{proof}
% Add proof content here
    First, let us recall that Problem~\eqref{obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is the following optimization problem:
    \begin{align}
        \label{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber}
        \balpha^* = \argmin_{\balpha} \frac{1}{2} \Vert{\balpha - \bmu}_2^2 + \rho \text{TopSum}_k\left( H_M\left( \balpha \right) \right),
    \end{align}
    
    We want to show that Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is closely related to Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression} via the relation $\balpha^* = \text{sgn}(\bmu) \odot \bnu^*$.
    
    
    To accomplish this, we leverage two properties associated with the optimal solution for Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber}.
    At $\balpha^*$, we have:
    \begin{align}
        & \textbf{1. sign-preserving property:} \quad \text{sgn}(\alpha_j^*) = \text{sgn}(\mu_j) \label{appendix_property:sign_preserving}\\
        & \textbf{2. relative magnitude-preserving property:} \quad \vert{\alpha_j^*} \geq \vert{\alpha_l^*} \; \text{if} \; \vert{\mu_j} \geq \vert{\mu_l} \label{appendix_property:relative_magnitude_preserving}
        % \text{sgn}(\alpha_j^*) &= \text{sgn}(\mu_j) \label{appendix_property:sign_preserving}\\
        % \vert{\alpha_j^*} \geq \vert{\alpha_l^*} \; &\text{if} \; \vert{\mu_j} \geq \vert{\mu_l} \label{appendix_property:relative_magnitude_preserving}
    \end{align}
    % Equation~\eqref{appendix_property:sign_preserving} says that the optimal solution $\balpha^*$ should be sign-preserving with respect to the input $\bmu$, and Equation~\eqref{appendix_property:relative_magnitude_preserving} tells us that the optimal solution $\balpha^*$ should preserve the relative order with respect to the input $\bmu$ in terms of the magnitude.
    
    Let us explain why these two properties hold.
    
    \paragraph{Sign-preserving property in Equation~\eqref{appendix_property:sign_preserving}} 
    For the sake of contradiction, suppose that there exists some $j$ such that $\text{sgn}(\alpha_j^*) \neq \text{sgn}(\mu_j)$.
    Then, we can construct a new $\balpha'$ by flipping the sign of $\alpha_j^*$, i.e., $\alpha_j' = -\alpha_j^*$, and keeping the rest of the elements the same as $\alpha_j'$.
    Now under the assmption that $\text{sgn}(\alpha_j^*) \neq \text{sgn}(\mu_j)$, we have $\left\lvert{\alpha_j^* - \mu_j}\right\rvert > \left\lvert{\lvert{\alpha_j^*}\rvert - \lvert{\mu_j}\rvert}\right\rvert = \left\lvert{\alpha_j' - \mu_j}\right\rvert$, so the $j$-th term in the first summation of the objective function will decrease while everything else remains the same.
    This leads to a smaller objective value for $\balpha'$ than $\balpha^*$, which contradicts the optimality of $\balpha^*$.
    Thus, the sign-preserving property in Equation~\eqref{appendix_property:sign_preserving} must hold.
    
    \paragraph{Relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving}} 
    For the sake of contradiction, suppose that there exists some $j$ and $l$ such that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^*} < \vert{\alpha_l^*}$.
    Then, we can construct a new $\balpha'$ by swapping $\alpha_j^*$ and $\alpha_l^*$, i.e., $\alpha_j' = \alpha_l^*$ and $\alpha_l' = \alpha_j^*$, and keeping the rest of the elements the same as $\alpha_j'$ and $\alpha_l'$.
    Under the assumption that $\vert{\mu_j} \geq \vert{\mu_l}$ but $\vert{\alpha_j^*} < \vert{\alpha_l^*}$, we have $\left\lvert{\alpha_j^* - \mu_j}\right\rvert + \left\lvert{\alpha_l^* - \mu_l}\right\rvert > \left\lvert{\alpha_l^* - \mu_j}\right\rvert + \left\lvert{\alpha_j^* - \mu_l}\right\rvert =
    \left\lvert{\alpha_j' - \mu_j}\right\rvert + \left\lvert{\alpha_l' - \mu_l}\right\rvert$, so the sum of the $j$-th and $l$-th terms in the first summation of the objective function will decrease while everything else remains the same.
    This leads to a smaller objective value for $\balpha'$ than $\balpha^*$, which contradicts the optimality of $\balpha^*$.
    Thus, the relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving} holds.
    
    Using these two properties, we are ready to prove the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} and Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression}.
    First, let us reparameterize $\balpha$ with a new variable $\bnu$ in Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} as $\balpha = \text{sgn}(\bmu) \odot \bnu$ wtih $\bnu \in \mathbb{R}_{+}^n$.
    In other words, we use $\bnu$ to model the magnitude of $\balpha$.
    
    By the sign-preserving property in Equation~\eqref{appendix_property:sign_preserving}, we can set the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} and the following optimization problem:
    \begin{align}
        \label{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber_reparameterized}
        \bnu^* = \argmin_{\bnu} \frac{1}{2} \sum_{j=1}^n (\nu_j - \vert{\mu_j})^2 + \rho \text{TopSum}_k\left( H_M\left( \bnu \right) \right), \; \text{ s.t. } \; \nu_j \geq 0.
    \end{align}
    
    By the relative magnitude-preserving property in Equation~\eqref{appendix_property:relative_magnitude_preserving}, we can further set the equivalence between Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber_reparameterized} and the following optimization problem:
    \begin{align}
        \label{appendix_obj:KyFan_Huber_isotonic_regression_with_nonnegative_constraint}
        \bnu^* = \argmin_{\bnu} & \quad \frac{1}{2} \sum_{j=1}^n (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ}^k H_M (\nu_j), \\
        \text{s.t.} & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l}, \; \text{ and } \; \nu_j \geq 0. \nonumber
    \end{align}
    
    Lastly, the nonnegative constraint in Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression_with_nonnegative_constraint} can be removed because the first summation term in the objective function already implies that $\nu_j \geq 0$.
    Thus, we have shown that Problem~\eqref{appendix_obj:proximal_operator_of_g*_with_TopSum_k_and_Huber} is closely related to Problem~\eqref{appendix_obj:KyFan_Huber_isotonic_regression} via the relation $\balpha^* = \text{sgn}(\bmu) \odot \bnu^*$.
\end{proof}

\begin{namedlemma}
    [~\ref{lemma:PAVA_algorithm_exact_solution}]
    The vector $\hat \bnu$ in Algorithm~\ref{alg:PAVA_algorithm} solves~\eqref{obj:KyFan_Huber_isotonic_regression} exactly.
\end{namedlemma}

\begin{proof}
    Problem~\eqref{obj:KyFan_Huber_isotonic_regression} can be recognized as a generalized isotonic regression problem.
    Let $h_j(v) = \frac{1}{2} (v - \mu_j)^2 + \rho_j H_M(v)$, where $\rho_j = \rho$ if $j \in \calJ$ and $\rho_j = 0$ otherwise.
    As mentioned in Algorithm~\ref{alg:PAVA_algorithm}, the set $\calJ$ is the set of indices of top k largest elements of $\vert{\mu_j}$.
    Then, we can rewrite Problem~\eqref{obj:KyFan_Huber_isotonic_regression} in the standard form of a generalized isotonic regression problem:
    \begin{align}
        \label{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}
        \min_{\bnu} \sum_{j=1}^{p} h_j(\nu_j) \quad \text{s.t.} \quad \nu_1 \geq \nu_2 \geq \cdots \geq \nu_J.
    \end{align}
    
    Generalized isotonic regression problems have been studied extensively in the literature.
    There are two key properties~\cite{best2000minimizing,ahuja2001fast} regarding Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression} that we can leverage to prove Lemma~\ref{lemma:PAVA_algorithm_exact_solution}:
    
    \paragraph{1. Optimal solution for a merged block is single-valued.}
    Suppose we have two adjacent blocks $[a_1, a_2]$ and $[a_2+1, a_3]$.
    If the optimal solution for each block is single-valued, \textit{i.e.}, we have
    \begin{equation}
    \begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
    \bnu_{a_1:a_2}^* =  
    \left(\begin{array}{c}
    \argmin_{\bnu_{a_1:a_2}} \sum_{j=a_1}^{a_2} h_j(\nu_j) \\
    \text{s.t.} \quad \nu_{a_1} \geq \cdots \geq \nu_{a_2}
    \end{array}\right) & 
    \Rightarrow & \bnu_{a_1}^* = \cdots = \bnu_{a_2}^* \\[3em]
    \bnu_{a_2+1:a_3}^* =  
    \left(\begin{array}{c}
    \argmin_{\bnu_{a_2+1:a_3}} \sum_{j=a_2+1}^{a_3} h_j(\nu_j) \\
    \text{s.t.} \quad \nu_{a_2+1} \geq \cdots \geq \nu_{a_3}
    \end{array}\right) & 
    \Rightarrow & \bnu_{a_2+1}^* = \cdots = \bnu_{a_3}^*
    \end{array}
    \end{equation}
    
    and if $\nu_{a_1}^* \leq \nu_{a_2+1}^*$, then the optimal solution for the merged block $[a_1, a_3]$ is single-valued, \textit{i.e.}, we have:
    
    \begin{equation}
    \begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
    \bnu_{a_1:a_3}^* =  
    \left(\begin{array}{c}
    \argmin_{\bnu_{a_1:a_3}} \sum_{j=a_1}^{a_3} h_j(\nu_j) \\
    \text{s.t.} \quad \nu_{a_1} \geq \cdots \geq \nu_{a_3}
    \end{array}\right) & 
    \Rightarrow & \bnu_{a_1}^* = \cdots = \bnu_{a_3}^*
    \end{array}
    \end{equation}
    
    \paragraph{2. No isotonic constraint violation between single-valued blocks implies the solution is optimal.}
    If we have $s$ blocks $[a_1, a_2], [a_2+1, a_3], \ldots, [a_{s}+1, a_{s+1}]$ (with $a_1=1$ and $a_{s+1}=n$) such that the optimal solution for each block is single-valued, \textit{i.e.}, we have
    
    \begin{equation}
    \begin{array}{c@{\hspace{2em}}c@{\hspace{2em}}c}
    \hat{\bnu}_{a_l:a_{l+1}} =
    \left(\begin{array}{c}
    \argmin_{\bnu_{a_l:a_{l+1}}} \sum_{j=a_l}^{a_{l+1}} h_j(\nu_j) \\
    \text{s.t.} \quad \nu_{a_l} \geq \cdots \geq \nu_{a_{l+1}}
    \end{array}\right) &
    \Rightarrow & \bnu_{a_l}^* = \cdots = \bnu_{a_{l+1}}^* \, \text{ for all } \, l=1, \ldots, s
    \end{array}
    \end{equation}
    
    and if $\hat{\nu}_{a_1} \geq \hat{\nu}_{a_2+1} \geq \ldots \hat{\nu}_{a_{s}}$, then $\hat{\bnu}$ is the optimal solution for Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}.
    
    Using these two properties, it is now easy to see why Algorithm~\ref{alg:PAVA_algorithm} returns the optimal solution.
    
    We start by constructing blocks which have length 1.
    The initial value restricted to each block is optimal.
    Then, we iteratively merge adjacent blocks and update the values of $\nu_j$'s whenever there is a violation of the isotonic constraint.
    By the first property, the optimal solution for the merged block is single-valued.
    Therefore, we can compute the optimal solution for the merged block by solving a univariate optimization problem.
    
    We keep merging blocks until there is no isotonic constraint violation.
    When this happens, by construction, the solution for each block is single-valued and optimal.
    By the second property, the final vector $\hat{\bnu}$ is the optimal solution for Problem~\eqref{obj:KyFan_Huber_isotonic_regression_rewritten_as_generalized_isotonic_regression}.
\end{proof}
%\input{sections/proofs/proof_fenchel_conjugate_of_g_closed_form_expression.tex}

%\input{sections/proofs/proof_equivalence_between_proximal_operator_and_huber_isotonic_regression.tex}

%\input{sections/proofs/proof_PAVA_algorithm_exact_solution.tex}

\input{sections/proofs/proof_PAVA_merging_linear_time_complexity.tex}

\input{sections/proofs/proof_compute_g_value_algorithm_correctness.tex}
