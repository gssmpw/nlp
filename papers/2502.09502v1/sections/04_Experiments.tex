\vspace{-3mm}
\section{Experiments}
\vspace{-1mm}

We evaluate our proposed methods using both synthetic and real-world datasets to address three key empirical questions: \\[-2em]
\begin{itemize}[label=$\diamond$,leftmargin=*]
    \item How fast is our customized PAVA algorithm in evaluating $\prox_g$ compared to existing solvers?
    \item How fast is our proposed FISTA method in calculating the lower bounds compared to existing solvers?
    \item How fast is our customized BnB algorithm compared to existing solvers?
\end{itemize}
We implement our algorithms in python.

For baselines, we compare with the following state-of-the-art commercial and open-source SOCP solvers: Gurobi~\citep{gurobi}, MOSEK~\citep{mosek}, SCS~\citep{scs}, and Clarabel~\cite{Clarabel}, with the python package cvxpy~\cite{cvxpy} as the interface to these solvers.

\vspace{-2mm}
\subsection{How Fast Can We Evaluate $\text{prox}_{\rho^{-1} g}(\cdot)$?}

\begin{figure*}[!htb]
    % \vspace{-0.5em}
    \centering
    \includegraphics[width=0.85\textwidth]{sections/Plots/prox_comparison/prox_comparison.png}
    \vspace{-1em}
    \caption{Running time comparison of evaluating the proximal operators, for both $g$ (left) and $g^*$ (right).
    The baselines evaluate the proximal operators by directly solving the corresponding second-order conic problems (SOCP), respectively.}
    \label{fig:prox_comparison}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.85\textwidth]{sections/Plots/big_M_perturbation/convex_relaxation_comparison_n_p_ratio_1.0_M_2.0.png}
    \vspace{-1em}
    \caption{Running time comparison of solving Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}, the perspective relaxation of the original MIP problem.
    We set $M=2.0$, $\lambda_2=1.0$, and $n$-to-$p$ ratio to be 1. Gurobi cannot solve the cardinality constrained logistic regression problem.}
    \label{fig:solve_convex_relaxation_main_paper}
    \vspace{-5mm}
\end{figure*}

In this subsection, we demonstrate the computational efficiency of using our PAVA algorithm for evaluating the proximal operators.
We conduct the comparisons in two ways --- evaluating both a) the proximal operator of the original function $g$ and b) the proximal operator of its conjugate $g^*$.
Detailed experimental configurations, including parameter specifications and synthetic data generation process, are provided in Appendix~\ref{appendix:setup_for_evaluating_proximal_operators}.

The results shown in Figure~\ref{fig:prox_comparison} highlight the superiority of our method.
Our algorithm achieves a computational speedup of  approximately two orders of magnitude compared to conventional SOCP solvers.
This performance gain is largely due to our customized PAVA implementation in Algorithm~\ref{alg:PAVA_algorithm}.
For instance, in high-dimensional settings ($p=10^5$), baseline methods require several seconds to minutes to evaluate the proximal operators, whereas our approach completes the same task in 0.01 seconds.
Additionally, our method guarantees \textit{exact} solutions to the optimization problem, in contrast to the approximate solutions returned by the baselines.
This combination of precision and efficiency constitutes a critical advantage for our first-order optimization framework over generic conic programming solvers, as demonstrated in subsequent sections.

\vspace{-2mm}
\subsection{How Fast Can We Calculate the Lower Bound?}
\vspace{-1mm}

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.95\textwidth]{sections/Plots/RestartedFISTA_linear_convergence_rate/convergence_comparison.png}
    \vspace{-2mm}
    \caption{Empirical convergence rate of our restarted FISTA (compared with PGD, the proximal gradient method, and FISTA) on solving the perspective relaxation in Problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation} with the logistic loss, $n=16000, p=16000, k=10, \rho=0.5, \lambda_2=1.0, \text{ and } M=2.0$. }
    \label{fig:RestartedFISTA_linear_convergence_rate}
    \vspace{-3mm}
\end{figure*}


\input{sections/table_GPU_acceleration_results}

\input{sections/table_results}
We next benchmark the computational speed and scalability of our method against the state-of-the-art solvers (Gurobi, MOSEK, SCS, and Clarabel) for solving the perspective relaxation of the original MIP problem.
Evaluations are performed on both linear and logistic regression tasks.


Experimental configurations are detailed in Appendix~\ref{appendix:setup_for_solving_the_perspective_relaxation}.
Additional perturbation studies, such as on the sample-to-feature ($n$-to-$p$) ratio, box constraint $M$, and $\ell_2$ regularization coefficient $\lambda_2$, are provided in Appendix~\ref{appendix:numerical_solve_convex_relaxation}.
All solvers are terminated upon achieving an optimality gap tolerance of $\epsilon=10^{-6}$ or exceeding a runtime limit of 1800 seconds.

The results, shown in Figure~\ref{fig:solve_convex_relaxation_main_paper}, demonstrates that our method outperforms the fastest conic solver (MOSEK) by over one order of magnitude.
For the largest tested instances ($n=16000$ and $p=16000$), our approach attains the target tolerance ($10^{-6}$) in under 100 seconds across regression and classification datasets, whereas most baselines fail to converge within the 1800-second threshold.

There are two factors driving this speedup.
First, our efficient proximal operator evaluation reduces per-iteration complexity.
Second, our efficient method to compute $g(\bbeta)$ (in Algorithm~\ref{alg:compute_g_value_algorithm}) exactly enables integration of the value-based restart technique within FISTA, significantly improving convergence.
Figure~\ref{fig:RestartedFISTA_linear_convergence_rate} illustrate this enhancement: while the proximal gradient algorithm (PGD) and FISTA exhibit sublinear convergence rates, FISTA with restarts achieves linear convergence on both dual loss and primal-dual gap metrics.
To the best of our knowledge, this marks the first empirical demonstration of linear convergence for a first-order method applied to solving the convex relaxation of this MIP class.

Finally, our method permits GPU acceleration because our most computationally intensive component is matrix-vector multiplications.
As shown in Table~\ref{tab:GPU_acceleration}, GPU implementation reduces runtime by an additional order of magnitude on high-dimensional instances.



\vspace{-0mm}
\subsection{How Fast Can We Certify Optimality?}
Finally, we demonstrate how our method's ability to compute tight lower bounds enables efficient optimality certification for large-scale datasets, outperforming state-of-the-art commericial MIP solvers.
Integrating our lower-bound computation into a minimalist branch-and-bound (BnB) framework, we prioritize node pruning via lower bound calculations while intentionally omitting advanced MIP heuristics (e.g., cutting planes, presolve routines) to evaluate the impact of our method.
Experimental configurations, including dataset descriptions and BnB implementation details, are provided in Appendix~\ref{appendix:setup_for_certifying_optimality}.
We benchmark our approach against Gurobi and MOSEK, reporting both runtime and final optimality gaps.


Results in Table~\ref{tab:my-table} show that our method certifies optimality for three of the four tested datasets within 2 minutes and the fourth within 10 minutes.
In contrast, Gurobi and MOSEK either exceed the time limit (1800 seconds) during the presolve stage or require significantly longer runtimes to achieve zero or small gaps.
Crucially, this efficiency stems from our efficient lower-bound computations and dynamic early termination criteria.
Specifically, we avoid waiting for full convergence by leveraging two key rules: 
(1) if the primal loss falls below the incumbent solution’s loss, we terminate early and proceed to branching; 
(2) if the dual loss exceeds the incumbent’s loss, we halt computation and prune the node immediately. This adaptive approach eliminates unnecessary iterations while ensuring we prune the search space effectively.





