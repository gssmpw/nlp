\section{Methodology}
\label{sec:methodology}

We begin with reformulating~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation} as the following \textit{unconstrained} optimization problem
\begin{align}
    \label{obj:original_sparse_problem_convex_composite_reformulation}
    \min_{\bbeta} f(\bX \bbeta, \by) + 2 \lambda_2 \, g(\bbeta),
\end{align}
where the implicit function $g: \R^p \to \R \cup \{ \infty \}$ is defined~as
\begin{align}
    \label{eq:function_g_definition}
    g(\bbeta) = \left\{ 
    \begin{array}{cl}
        \min\limits_{\bz \in \R^p} & \frac{1}{2} \sum_{j \in [p]} \beta_j^2 / z_j \\[1ex]
        \st & \bz \in [0, 1]^p, \, \bm 1^\top \bz \leq k, \\
        & -M z_j \leq \beta_j \leq M z_j ~ \forall j \in [p].
    \end{array} 
    \right.
\end{align}
Here, we follow the standard convention that an infeasible minimization problem is assigned a value of $+\infty$. 
Note that $g$ is convex as convexity is preserved under partial minimization over a convex set~\citep[Theorem~5,3]{rockafellar1970convex}. 
Furthermore, as $f$ is assumed to be Lipschitz smooth and $g$ is non-smooth, problem~\eqref{obj:original_sparse_problem_convex_composite_reformulation} is an unconstrained optimization problem with a convex composite objective function. As such, it is amenable to be solved using the FISTA algorithm proposed in \citep{beck2009fast}. 

In the following, we first analyze the conjugate of $g$. We then propose an efficient numerical method to compute the proximal operator of $g^*$. This, in turn, enables us to compute the proximal operator of $g$, leading to an efficient implementation of the FISTA algorithm. To further enhance the performance of FISTA, we present an efficient approach to solve the minimization problem~\eqref{eq:function_g_definition}, which guides us in developing an effective restart procedure. Finally, we conclude this section by providing efficient lower bounds for each step of the BnB framework.

\subsection{Conjugate function $g^*$}
Recall that the conjugate of $g$ is defined as
\begin{align*}
    g^*(\bm \alpha) = \sup_{\bm \beta \in \R^p} ~ \bm \alpha^\top \bm \beta - g(\bm \beta).
\end{align*}
The following lemma gives a closed-form expression for $g^*$, where $\TopSum_k(\cdot)$ denotes the sum of the top $k$ largest elements, and $H_M: \R \to \R$ is the Huber loss function defined as
\begin{equation}
    H_M(\alpha_j) := \begin{cases}
        \frac{1}{2} \alpha_j^2 & \text{if } \vert{\alpha_j} \leq M \\
        M \vert{\alpha_j} - \frac{1}{2} M^2 & \text{if } \vert{\alpha_j} > M
    \end{cases}.
\end{equation}
For notational simplicity, we use the shorthand notation ${\bf H}_M(\balpha)$ to denote ${\bf H}_M(\balpha) = (H_M(\alpha_1), \dots, H_M(\alpha_p))$.

\begin{lemma}
    \label{lemma:fenchel_conjugate_of_g_closed_form_expression}
    The conjugate of $g$ is given by
    \begin{equation}
        g^*(\balpha) = \TopSum_k({\bf H}_M(\balpha)).
    \end{equation}
\end{lemma}
This closed-form expression enables us to compute the proximal of $g^*$. Note that while the proximal operators of both $\TopSum_k$ and ${\bf H}_M$ functions are known (see, for example, \citep[Examples~6.50 \& 6.54]{beck2017first}), the conjugate function $g^*$ is defined as the composition of these two functions.
Alas, there is no general formula to derive the proximal operator of a composition of two functions based on the proximal operators of the individual functions. In the next section, we will see how to bypass this compositional difficulty.


\subsection{Proximal operator of $g^*$}
Recall that the proximal operator of $g^*$ with weight parameter $\rho > 0$ is defined as
\begin{align}
    \label{eq:prox:g*}
    \prox_{\rho g^*}(\bm \mu) = \argmin_{\bm \alpha \in \R^p} ~ \frac{1}{2} \Vert{\bm \alpha - \bm \mu}_2^2 + \rho g^*(\bm \alpha).
\end{align}
The evaluation of $\prox_{\rho g^*}$ involves a minimization problem that can be reformulated as a convex SOCP problem. 
Generic solvers based on IPM and ADMM require solving systems of linear equations. This results in cubic time complexity per iteration, making them computationally expensive, particularly for large-scale problems. 
These methods also cannot return \emph{exact} solutions. 
The lack of exactness can affect the stability and reliability of the proximal operator, which is crucial for the convergence of the FISTA algorithm. 
Inspired by~\citep{busing2022monotone}, we present Algorithm~\ref{alg:PAVA_algorithm}, a customized pooled adjacent violators algorithm that provides an exact evaluation of $\prox_{\rho g^*}$ in linear time after performing a simple 1D sorting step.

\begin{theorem}
    \label{theorem:pava_algorithm_linear_time_complexity_and_exact_solution}
    For any $\bmu \in \R^p$, Algorithm~\ref{alg:PAVA_algorithm} returns the \textit{exact} evaluation of $\prox_{\rho g^*}(\bmu)$ in $\tilde {\mathcal O}(p)$.
\end{theorem}


\input{sections/Algorithmic_Charts/PAVA_pseudocode.tex}

The proof relies on several auxiliary lemmas.
We start with the following lemma, which uncovers a close connection between the proximal operator of $g^*$ and the generalized isotonic regression problems.

\begin{lemma}
    \label{lemma:equivalence_between_proximal_operator_and_huber_isotonic_regression}
    For any $\bmu \in \R^p$, we have 
    $$\prox_{\rho g^*}(\bmu) = \sgn(\bmu) \odot \bnu^\star, $$ 
    where $\odot$ denotes the Hadamard (element-wise) product, $\bnu^\star$ is the unique solution of the following optimization problem
    \begin{align}
        \label{obj:KyFan_Huber_isotonic_regression}
        \begin{array}{cl}
            \min\limits_{\bnu \in \R^p} & \frac{1}{2} \sum_{j \in [p]} (\nu_j - \vert{\mu_j})^2 + \rho \sum_{j \in \calJ} H_M (\nu_j) \\[2ex]
            \st & \quad \nu_j \geq \nu_l \; \text{ if } \; \vert{\mu_j} \geq \vert{\mu_l} ~~ \forall j, l \in [p],
        \end{array} 
    \end{align}
    and $\calJ$ is the set of indices of the top $k$ largest elements of~$ \vert{\mu_j}, j \in [p]$. 
\end{lemma}
Problem~\eqref{obj:KyFan_Huber_isotonic_regression} replaces the $\TopSum_k$ in~\eqref{eq:prox:g*} from the conjugate function $g^*$ (as shown in Lemma~\ref{lemma:fenchel_conjugate_of_g_closed_form_expression}) with linear constraints.
%These constraints ensure that the elements of $\bnu$ maintain a non-decreasing order in magnitude.
While this may appear computationally complex, it actually converts the problem into an instance of isotonic regression~\citep{best1990active}. Such problems can be solved exactly in linear time after performing a simple sorting step.
The procedure is known as PAVA~\citep{busing2022monotone}. Specifically, Algorithm~\ref{alg:PAVA_algorithm} implements a customized PAVA variant designed to compute $\prox_{\rho g^*}$ exactly.
The following lemma shows that the vector generated by Algorithm~\ref{alg:PAVA_algorithm} is an exact solution to~\eqref{obj:KyFan_Huber_isotonic_regression}.
Intuitively, Algorithm~\ref{alg:PAVA_algorithm} iteratively merges adjacent blocks until no isotonic constraint violations remain, at which point the resulting vector is guaranteed to be the optimal solution to~\eqref{obj:KyFan_Huber_isotonic_regression}.

\begin{lemma}
    \label{lemma:PAVA_algorithm_exact_solution}
    The vector $\hat \bnu$ in Algorithm~\ref{alg:PAVA_algorithm} solves~\eqref{obj:KyFan_Huber_isotonic_regression} exactly.
\end{lemma}

Finally, the merging process in Algorithm~\ref{alg:PAVA_algorithm} can be executed efficiently. Intuitively, each element of $\bmu$ is visited at most twice; once during its initial processing and once when it is included in a merged block. This ensures that the process achieves a linear time complexity.

\begin{lemma}
    \label{lemma:PAVA_merging_linear_time_complexity}
    The merging step (lines 11-14) in Algorithm~\ref{alg:PAVA_algorithm} can be performed in linear time complexity $\mathcal O(p)$.
\end{lemma}
% \begin{proof}
%     The merging process in the PAVA algorithm involves iterating through the elements of the input vector $\bmu$ and merging adjacent blocks whenever an isotonic constraint violation is detected. Each element is visited at most twice: once when it is initially processed and once when it is part of a merged block. Therefore, the total number of operations is proportional to the number of elements, resulting in a linear time complexity, $O(n)$.
% \end{proof}

Armed with these lemmas, one can easily prove Theorem~\ref{theorem:pava_algorithm_linear_time_complexity_and_exact_solution}. Details are provided in~\ref{appendix_sec:proofs}.

\vspace{-2mm}
\subsection{FISTA algorithm with restart}
A critical computational step in FISTA is the efficient evaluation of the proximal operator of $g$. By the extended Moreau decomposition theorem~\citep[Theorem~6.45]{beck2017first}, for any weight parameter $\rho > 0$ and any point $\bmu \in \R^p$, the proximal operators of $g$ and $g^*$ satisfies
\begin{align}
    \label{eq:Moreaus_identity}
    \prox_{\rho^{-1} g}(\bmu) = \bmu - \rho^{-1} \, \prox_{\rho g^*} \left( \rho \bmu \right).
\end{align}
Hence, together with Theorem~\ref{theorem:pava_algorithm_linear_time_complexity_and_exact_solution}, we can compute exactly $\prox_{\rho^{-1} g}$ using Algorithm~\ref{alg:PAVA_algorithm} with log-linear time complexity. This enables an efficient implementation of the FISTA algorithm. 
Alas, the vanilla FISTA algorithm is prone to oscillatory behavior, which results in a sub-linear convergence rate of $\mathcal O(1/T^2)$ after $T$ iterations. 
In the following, we further accelerate the empirical convergence performance of the FISTA algorithm by incorporating a simple restart strategy based on the function value, originally proposed in~\citep{o2015adaptive}.

In simple terms, the restart strategy operates as follows: if the objective function increases during the iterative process, the momentum coefficient is reset to its initial value.
The effectiveness of the restart strategy hinges on the efficient computation of the loss function. This task essentially reduces to evaluating the implicit function $g$ defined in~\eqref{eq:function_g_definition}, which would involve solving a SOCP problem.
However, the value of $g$ can be computed efficiently by leveraging the majorization technique~\citep{kim2022convexification}, as shown in Algorithm~\ref{alg:compute_g_value_algorithm}.

\input{sections/Algorithmic_Charts/compute_g_value_algorithm}

\begin{theorem}
    \label{theorem:compute_g_value_algorithm_correctness}
        For any $\bbeta \in \R^p$, Algorithm~\ref{alg:compute_g_value_algorithm} computes the exact value of $g(\bbeta)$, defined in~\eqref{eq:function_g_definition}, in $\mathcal O(p + p \log k)$.
\vspace{-3mm}
\end{theorem}
Theorem~\ref{theorem:compute_g_value_algorithm_correctness} guarantees that Algorithm~\ref{alg:compute_g_value_algorithm} can efficiently compute the value of $g(\bbeta)$, which is crucial for our value-based restart strategy to be effective in practice.
Empirically, we observe that the function value-based restart strategy can accelerate FISTA from the sub-linear convergence rate of \(O(1/T^2)\) to a linear convergence rate in many empirical results.
To the best of our knowledge, \textit{this is the first linear convergence result of using a first-order method in the MIP context} when calculating the lower bounds in the BnB tree.
The FISTA algorithm is summarized in Algorithm~\ref{alg:main_algorithm}.

\input{sections/Algorithmic_Charts/main_algorithm}

\vspace{-3mm}
\subsection{Safe Lower Bounds for GLMs}
\label{subsec:safe_lower_bounds_for_glms}
We conclude this section by commenting on how to use Algorithm~\ref{alg:main_algorithm} in the BnB tree to prune nodes.
As an iterative algorithm, FISTA yields only an approximate solution $\hat{\bbeta}$ to~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation}. Consequently, while we can calculate the objective function for $\hat{\bbeta}$ efficiently, this value is not necessarily a lower bound of the original problem--only the optimal value of the relaxed problem~\eqref{obj:original_sparse_problem_perspective_formulation_convex_relaxation} serves as a guaranteed lower bound.
To get a safe lower bound, we rely on the weak duality theorem, in which for any proper, lower semi-continuous, and convex functions $F: \R^n \to \R \cup \{\infty\}$ and $G: \R^p \to \R \cup \{\infty\}$, we have
\begin{align*}
    \inf_{\bbeta \in \R^p} F(\bX \bbeta) + G(\bbeta) 
    &\geq \sup_{\bzeta \in \R^n} - F^*(-\bzeta) - G^*(\bX^\top \bzeta) \\ 
    &\geq - F^*(-\bzeta) - G^*(\bX^\top \bzeta) \,~ \forall \bzeta \in \R^n\!\!,
\end{align*}
where $F^*$ and $G^*$ denote the conjugates of $F$ and $G$, respectively, while the second inequality follows from the definition of the supremum operator. Letting $F(\bX \bbeta) = f(\bm X \bm \beta, \bm y)$, $G(\bbeta) = 2 \lambda_2 g(\bbeta)$ and $\bzeta = \nabla F(\bX \hat \bbeta)$, where $\hat \bbeta$ is the output of the FISTA Algorithm~\ref{alg:main_algorithm}, we arrive at the safe lower bound
\begin{align}
    \label{eq:fenchel_duality_theorem_F_y(Ax)+G(x)}
    P_{\text{MIP}}^\star \geq - F^*(-\hat{\bzeta}) - G^*(\bX^\top \hat{\bzeta}),
\end{align}
where the inequality follows from the relaxation bound $P_{\text{MIP}}^\star \geq P_{\text{conv}}^\star$ and the weak duality theorem.
For convenience, we provide a list of $F^*(\cdot)$ for different GLM loss functions in~\ref{appendix_sec:convex_conjugate_for_GLM_loss_functions}.
The readers are also referred to~\ref{appendix_sec:safe_lower_bound_more_discussions},  where we derive the safe lower bound for the linear regression problem with eigen-perspective relaxation as an example.