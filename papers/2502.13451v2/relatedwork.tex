\section{Related Work}
\label{sec2:related}
\textbf{Vision-and-Language Navigation}  
Vision-and-Language Navigation (VLN) \cite{gu2022vision,park2023visual} in embodied AI focuses on navigating unseen environments by following human instructions, primarily in discretized simulated scenarios \cite{anderson2020rxr, thomason2020cvdn}. Agents navigate between predefined nodes on a graph by integrating language and visual input \cite{qi2020object, liu2024volumetric}, but this reliance complicates real-world deployment.
To address this, VLNs for Continuous Environments (VLN-CE) \cite{krantz2020beyond, savva2019habitat} enable unrestricted navigation through low-level control or waypoint-based methods \cite{hong2022bridging, krantz2021waypoint}, improving sim-to-real transferability despite added complexity.
Recent advancements in vision-language models have significantly influenced VLN development, utilizing large-scale pre-trained models \cite{zhang2024navid, zheng2024towards} and VLN-specific pre-training \cite{hao2020prevalent, wu2022cross}. For instance, NavGPT \cite{zhou2024navgpt} autonomously generates actions using GPT-4o, while DiscussNav \cite{long2024discuss} employs VLN experts to reduce human involvement. InstructNav \cite{long2024instructnav} decomposes navigation into subtasks, and Nav-CoT \cite{lin2024navcot} uses Chain of Thoughts (CoT) \cite{wei2022chain} for environmental simulation. Some methods \cite{zheng2024towards, zhang2024navid} fine-tune VLMs for specific navigation tasks, highlighting flexibility. However, existing approaches often depend on hierarchical prompts or historical frames, leading to high memory demands and limited understanding of past data.
This paper introduces a novel memory representation using Annotated Semantic Maps (ASMs) to effectively replace traditional historical frames.






\noindent\textbf{Map Representations for VLN} 
Structured maps in VLN enhance navigation performance by improving environmental understanding \cite{wang2023gridmm, hong2023ego2map}. 
Methods like MC-GPT \cite{zhan2024mc} and VoroNav \cite{wu2024voronav} utilize topological maps to capture viewpoints and spatial relationships, while InstructNav \cite{long2024instructnav} and VLFM \cite{yokoyama2024vlfm} create value maps for waypoint selection.
Semantic maps \cite{zhang2024trihelper, zhang2024multi, hong2023ego2map, yu2023l3mvn} retain object-level information for navigation. However, these maps are often not interpretable by VLMs.
To address this, we propose Annotated Semantic
Map (ASM), a novel semantic map representation that allows VLMs to explicitly understand rich map information, including obstacle distributions, explored areas, agent positions, historical trajectories, and semantic object locations. 
ASM aims to establish a new memory representation for VLN.



\begin{figure*}[t]
\centering
\includegraphics[width=0.96\textwidth]{figure/semantic.pdf}
% \vspace{-20pt}
\caption{\textbf{ASM Generation Process.}
Semantic map generation starts with episode initialization. 
At each timestep, the RGB image is processed by a semantic segmentation module to create a semantic mask aligned with the depth-converted 3D point cloud. By combining this with the previous pose transformation, we project the 3D point cloud onto a 2D plane to update the semantic map. Finally, we convert the semantic map into the ASM through region clustering and text annotation, yielding a comprehensive memory representation with labeled objects.
}
\vspace{-12pt}
\label{fig3}
\end{figure*}
% \vspace{-10pt}