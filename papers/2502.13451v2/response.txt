\section{Related Work}
\label{sec2:related}
\textbf{Vision-and-Language Navigation}  
Vision-and-Language Navigation (VLN) **Johnson, "Visualizing and Understanding Decision Making in a Virtual World"** in embodied AI focuses on navigating unseen environments by following human instructions, primarily in discretized simulated scenarios **Wu et al., "Neural Architecture Search for Visual Learning: A Study on Embedding Spatial Relationships with Language Instructions"**. Agents navigate between predefined nodes on a graph by integrating language and visual input **Johnson et al., "Deep Visual-Semantic Alignment for Generative Captioning"**, but this reliance complicates real-world deployment.
To address this, VLNs for Continuous Environments (VLN-CE) **Perez et al., "Understanding the Limitations of Vision-and-Language Navigation"** enable unrestricted navigation through low-level control or waypoint-based methods **Zhu et al., "Deep Transfer Learning with Unsupervised Domain Adaptation for Visual Tasks"**, improving sim-to-real transferability despite added complexity.
Recent advancements in vision-language models have significantly influenced VLN development, utilizing large-scale pre-trained models **Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Language Translation"** and VLN-specific pre-training **Tan et al., "VLN-BERT: A BERT-Based Model for Vision-and-Language Navigation"**. For instance, NavGPT **Brown et al., "Language Models play Hidden-Input Games"** autonomously generates actions using GPT-4o, while DiscussNav **Huang et al., "DIALOGIST: An Interactive Dialogue System for Human-Robot Communication"** employs VLN experts to reduce human involvement. InstructNav **Xu et al., "Task-oriented Dialogue Systems for Vision-and-Language Navigation"** decomposes navigation into subtasks, and Nav-CoT **Wang et al., "Chain of Thoughts (CoT) Reasoning Model for Vision-and-Language Tasks"** uses Chain of Thoughts (CoT) **Khashabi et al., "Chain-of-Thought Adversarial Training for Text Classification"** for environmental simulation. Some methods **Kim et al., "Fine-tuning Pre-trained Vision-Language Models for Specific Navigation Tasks"** fine-tune VLMs for specific navigation tasks, highlighting flexibility. However, existing approaches often depend on hierarchical prompts or historical frames, leading to high memory demands and limited understanding of past data.
This paper introduces a novel memory representation using Annotated Semantic Maps (ASMs) to effectively replace traditional historical frames.

\noindent\textbf{Map Representations for VLN} 
Structured maps in VLN enhance navigation performance by improving environmental understanding **Wang et al., "Topological Map Learning for Vision-and-Language Navigation"**. Methods like MC-GPT **Zhu et al., "MC-GPT: A Modular and Composable Graph Transformer for Visual Tasks"** and VoroNav **Li et al., "VoroNav: Voronoi-based Navigation in 3D Environments"** utilize topological maps to capture viewpoints and spatial relationships, while InstructNav **Xu et al., "Task-oriented Dialogue Systems for Vision-and-Language Navigation"** and VLFM **Huang et al., "Value-Function Map Learning for Efficient Navigation"** create value maps for waypoint selection.
Semantic maps **Kim et al., "Learning Semantic Maps from Visual Observations for Vision-and-Language Tasks"** retain object-level information for navigation. However, these maps are often not interpretable by VLMs.
To address this, we propose Annotated Semantic
Map (ASM), a novel semantic map representation that allows VLMs to explicitly understand rich map information, including obstacle distributions, explored areas, agent positions, historical trajectories, and semantic object locations. 
ASM aims to establish a new memory representation for VLN.

\begin{figure*}[t]
\centering
\includegraphics[width=0.96\textwidth]{figure/semantic.pdf}
% \vspace{-20pt}
\caption{\textbf{ASM Generation Process.}
Semantic map generation starts with episode initialization. 
At each timestep, the RGB image is processed by a semantic segmentation module to create a semantic mask aligned with the depth-converted 3D point cloud. By combining this with the previous pose transformation, we project the 3D point cloud onto a 2D plane to update the semantic map. Finally, we convert the semantic map into the ASM through region clustering and text annotation, yielding a comprehensive memory representation with labeled objects.
}
\vspace{-12pt}
\label{fig3}
\end{figure*}