\section{Related Work}
\label{sec2:related}
\textbf{Vision-and-Language Navigation}  
Vision-and-Language Navigation (VLN) ____ in embodied AI focuses on navigating unseen environments by following human instructions, primarily in discretized simulated scenarios ____. Agents navigate between predefined nodes on a graph by integrating language and visual input ____, but this reliance complicates real-world deployment.
To address this, VLNs for Continuous Environments (VLN-CE) ____ enable unrestricted navigation through low-level control or waypoint-based methods ____, improving sim-to-real transferability despite added complexity.
Recent advancements in vision-language models have significantly influenced VLN development, utilizing large-scale pre-trained models ____ and VLN-specific pre-training ____. For instance, NavGPT ____ autonomously generates actions using GPT-4o, while DiscussNav ____ employs VLN experts to reduce human involvement. InstructNav ____ decomposes navigation into subtasks, and Nav-CoT ____ uses Chain of Thoughts (CoT) ____ for environmental simulation. Some methods ____ fine-tune VLMs for specific navigation tasks, highlighting flexibility. However, existing approaches often depend on hierarchical prompts or historical frames, leading to high memory demands and limited understanding of past data.
This paper introduces a novel memory representation using Annotated Semantic Maps (ASMs) to effectively replace traditional historical frames.






\noindent\textbf{Map Representations for VLN} 
Structured maps in VLN enhance navigation performance by improving environmental understanding ____. 
Methods like MC-GPT ____ and VoroNav ____ utilize topological maps to capture viewpoints and spatial relationships, while InstructNav ____ and VLFM ____ create value maps for waypoint selection.
Semantic maps ____ retain object-level information for navigation. However, these maps are often not interpretable by VLMs.
To address this, we propose Annotated Semantic
Map (ASM), a novel semantic map representation that allows VLMs to explicitly understand rich map information, including obstacle distributions, explored areas, agent positions, historical trajectories, and semantic object locations. 
ASM aims to establish a new memory representation for VLN.



\begin{figure*}[t]
\centering
\includegraphics[width=0.96\textwidth]{figure/semantic.pdf}
% \vspace{-20pt}
\caption{\textbf{ASM Generation Process.}
Semantic map generation starts with episode initialization. 
At each timestep, the RGB image is processed by a semantic segmentation module to create a semantic mask aligned with the depth-converted 3D point cloud. By combining this with the previous pose transformation, we project the 3D point cloud onto a 2D plane to update the semantic map. Finally, we convert the semantic map into the ASM through region clustering and text annotation, yielding a comprehensive memory representation with labeled objects.
}
\vspace{-12pt}
\label{fig3}
\end{figure*}
% \vspace{-10pt}