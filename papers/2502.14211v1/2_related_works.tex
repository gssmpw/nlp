% \section{Related Work} \label{sec:2}
% \textbf{Evaluation of the instruction following and overall output quality of LLMs.} LLMs have shown impressive capabilities, but their predictions often come with varying degrees of uncertainty, making calibration essential for reliable outputs. To address this, various methods have been proposed. \cite{kuleshov2018accurate} introduce a recalibration technique that adjusts confidence scores to better match empirical accuracy without altering the model's architecture or training data. Building on this, \cite{zhang2017mixup} propose mixup training, which enhances calibration by using convex combinations of inputs and labels during training. \cite{guo2017calibration} provide a comprehensive analysis of calibration errors and introduce metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), facilitating precise comparisons between models. For LLMs, \cite{desai2020calibration} apply temperature scaling, tailored specifically for the complexity of language models. More recent advances include an ensemble approach by \cite{zhao2021calibrate}, combining multiple models to achieve a calibrated consensus. Additionally, \cite{tian2023just} propose directly querying LLMs to assess their confidence in responses, while \cite{he2023investigating} evaluate LLM calibration using metrics like ECE, AUROC, and AUPRC. Finally, \cite{lyu2024calibrating} introduce coherence sampling, a novel approach contributing to the ongoing refinement of LLM calibration.

% \textbf{Prompt Engineering and Optimization.} Recent advances in prompt engineering have significantly enhanced the efficiency and efficacy of interactions with large language models (LLMs). Key developments include few-shot and zero-shot learning, which utilize minimal examples to guide models, thereby reducing reliance on extensive labeled datasets. In-context learning, introduced by \cite{brown2020language}, enables models to adapt to tasks using input prompts alone, without requiring parameter updates. Automated prompt generation methods, such as those explored by \cite{liu2023pre}, employ algorithms like reinforcement learning to discover optimal prompts, thus enhancing model performance. Furthermore, \cite{schick2020exploiting} demonstrated that pre-training models with specific prompt formats can improve generalization across diverse tasks.

% In terms of prompt optimization, recent studies have highlighted the effectiveness of LLMs as prompt optimizers. For instance, \cite{ma2024large} showed that LLMs can effectively refine prompts, boosting performance across various tasks. Similarly, \cite{yang2024largelanguagemodelsoptimizers} reported that optimized prompts generated by LLMs can outperform manually crafted ones through the Optimization by PROmpting (OPRO) method. Additionally, \cite{li2023robust} identified vulnerabilities in prompt optimization techniques due to distribution shifts, such as subpopulation shifts, and proposed Generalized Prompt Optimization (GPO) to enhance the generalization capabilities of LLMs.
\section{Related Work} \label{sec:2}

\textbf{Evaluation of Instruction Following and Output Quality in LLMs.} LLMs demonstrate impressive capabilities but often exhibit uncertainty in predictions, necessitating effective calibration for reliable outputs. \cite{kuleshov2018accurate} introduces a recalibration method that aligns confidence scores with empirical accuracy without altering the modelâ€™s architecture. \cite{zhang2017mixup} enhance calibration through mix up training, which uses convex combinations of inputs and labels. \cite{guo2017calibration} analyze calibration errors and propose metrics like Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) for model comparison. For LLMs specifically, \cite{desai2020calibration} apply temperature scaling, while \cite{zhao2021calibrate} utilize ensemble methods to achieve calibrated consensus. Additionally, \cite{tian2023just} assess LLM confidence through direct querying, and \cite{he2023investigating} evaluate calibration using ECE, AUROC, and AUPRC. \cite{lyu2024calibrating} introduce coherence sampling to further refine LLM calibration.

  % introduced in-context learning, allowing models to adapt to tasks using input prompts without parameter updates.
  % , enhancing performance. \citep{schick2020exploiting} show that pre-training with specific prompt formats can improve task generalization
% \vspace{-0.1em}
\noindent\textbf{Prompt Engineering and Optimization.} Advances in prompt engineering have significantly improved interactions with LLMs. Few-shot and zero-shot learning techniques reduce the need for extensive labeled datasets by using minimal examples to guide models \citep{brown2020language}. Automated prompt generation methods, such as those by \citep{liu2023pre}leverage reinforcement learning to discover optimal prompts.
Recent studies highlight the role of LLMs in prompt optimization. \citep{ma2024large} demonstrate that LLMs can refine prompts to boost task performance. Addressing distribution shifts, \citep{li2023robust} propose Generalized Prompt Optimization (GPO) to enhance LLM generalization under subpopulation shifts.  \citep{yang2024largelanguagemodelsoptimizers} show that LLM-generated prompts via the Optimization by PROmpting (OPRO) method outperform manually crafted ones.
The differences between Transfer-Prompting and OPRO can be summarized in three key aspects: First, Transfer-Prompting is a two-stage optimization framework, consisting of source prompt construction and target prompt generation, whereas OPRO operates in a single-stage process, directly generating optimized prompts. Second, Transfer-Prompting employs the multi-dimensional metrics designed in the objective prompt evaluator to assess the effectiveness of candidate prompts. In contrast, OPRO focuses on optimizing prompts through a single evaluation metric. Lastly, Transfer-Prompting designs domain-specific reference prompts to ensure better adaptation to target tasks, offering greater flexibility for task customization.
% \section{Related Work} \label{sec:2}

% \textbf{Evaluation of Instruction Following and Output Quality in LLMs.} Large Language Models (LLMs) have demonstrated impressive capabilities; however, their predictions often exhibit varying degrees of uncertainty, necessitating calibration for reliable outputs. To address this, several calibration methods have been proposed. \cite{kuleshov2018accurate} introduce a recalibration technique that adjusts confidence scores to better align with empirical accuracy without modifying the model's architecture or training data. Building on this, \cite{zhang2017mixup} propose mixup training, which enhances calibration by incorporating convex combinations of inputs and labels during training. \cite{guo2017calibration} provide a comprehensive analysis of calibration errors and introduce metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), facilitating precise comparisons between models. Specifically for LLMs, \cite{desai2020calibration} apply temperature scaling tailored to the complexity of language models. More recent advances include an ensemble approach by \cite{zhao2021calibrate}, which combines multiple models to achieve a calibrated consensus. Additionally, \cite{tian2023just} propose directly querying LLMs to assess their confidence in responses, while \cite{he2023investigating} evaluate LLM calibration using metrics like ECE, Area Under the Receiver Operating Characteristic curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC). Finally, \cite{lyu2024calibrating} introduce coherence sampling, a novel approach contributing to the ongoing refinement of LLM calibration.

% \textbf{Prompt Engineering and Optimization.} Recent advances in prompt engineering have significantly enhanced the efficiency and effectiveness of interactions with LLMs. Key developments include few-shot and zero-shot learning, which utilize minimal examples to guide models, thereby reducing reliance on extensive labeled datasets. In-context learning, introduced by \cite{brown2020language}, enables models to adapt to tasks using input prompts alone, without requiring parameter updates. Automated prompt generation methods, such as those explored by \cite{liu2023pre}, employ algorithms like reinforcement learning to discover optimal prompts, thereby enhancing model performance. Furthermore, \cite{schick2020exploiting} demonstrate that pre-training models with specific prompt formats can improve generalization across diverse tasks.

% Regarding prompt optimization, recent studies have highlighted the effectiveness of using LLMs as prompt optimizers. For instance, \cite{ma2024large} show that LLMs can effectively refine prompts, boosting performance across various tasks. Similarly, \cite{yang2024largelanguagemodelsoptimizers} report that optimized prompts generated by LLMs can outperform manually crafted ones through the Optimization by PROmpting (OPRO) method. Additionally, \cite{li2023robust} identify vulnerabilities in prompt optimization techniques due to distribution shifts, such as subpopulation shifts, and propose Generalized Prompt Optimization (GPO) to enhance the generalization capabilities of LLMs.

% \textbf{Uncertainty of LLMs.} Calibration of neural networks ensures that a model's confidence scores accurately reflect the true probability of correctness, which is critical for reliable predictions. \cite{kuleshov2018accurate} propose a recalibration technique that adjusts confidence predictions to better match empirical accuracy without altering the model's architecture or training data. Building upon this, \cite{zhang2017mixup} introduce mixup training, enhancing calibration by training on convex combinations of inputs and labels. To evaluate calibration, \cite{guo2017calibration} provide a thorough analysis of calibration errors, introducing widely adopted metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), which facilitate quantitative comparisons across models. Extending these techniques to large language models (LLMs), \cite{desai2020calibration} address calibration through temperature scaling, specifically tailored to the complexities of language models.

% Further advancements in calibrating LLMs include an ensemble approach by \cite{zhao2021calibrate}, combining multiple models or variations to achieve a calibrated consensus. \cite{tian2023just} propose querying language models directly to assess their confidence in responses. Complementarily, \cite{he2023investigating} conduct an extensive evaluation of LLM calibration based on logits, employing metrics such as ECE, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC). Additionally, \cite{lyu2024calibrating} introduce a coherence sampling method for calibrating language models, further enriching the field.
% \textbf{Calibration and Uncertainty.} Calibration of neural networks is a critical research area that ensures a model's confidence scores accurately represent the true probability of correctness. For example, \cite{kuleshov2018accurate} proposes a recalibration technique that adjusts a model's confidence predictions to better reflect empirical accuracy without modifying the model's architecture or training data. Building on this, \cite{zhang2017mixup} introduces mix-up training, which enhances model calibration by training on convex combinations of input data and labels. These foundational approaches have driven further advancements in the field.

% To assess calibration, \cite{guo2017calibration} provides a thorough analysis of calibration errors in neural networks, introducing widely adopted metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE). These metrics facilitate quantitative comparisons of calibration across different models. Following this, \cite{desai2020calibration} addresses the calibration of large language models (LLMs) through temperature scaling, a technique specifically tailored to the complexities of language models.

% Moreover, \cite{zhao2021calibrate} presents an ensemble approach to calibrate LLMs, combining multiple models or variations of a model to achieve a calibrated consensus. Recently, \cite{tian2023just} proposed a novel method in which language models are queried directly and assess their confidence in their responses. Complementing this, \cite{he2023investigating} conducted an extensive evaluation of LLM calibration based on logits, employing metrics such as ECE, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC) to provide a comprehensive analysis. Additionally, \cite{lyu2024calibrating} introduced a coherence sampling method for calibrating language models, further enriching the field.

% \textbf{Prompt Engineering and Optimization.} Recent progress in prompt engineering has significantly improved the efficiency and efficacy of interactions with LLMs. Key advancements include few-shot and zero-shot learning, which use minimal examples to guide models, thus reducing the reliance on extensive labeled datasets. In-context learning, introduced by \cite{brown2020language}, enables models to adapt to tasks using input prompts alone, without the need for parameter updates. Automated prompt generation, as explored by \cite{liu2023pre}, utilizes algorithms such as reinforcement learning to discover optimal prompts, thereby enhancing model performance. Furthermore, \cite{schick2020exploiting} showed that pre-training models with specific prompt formats can improve generalization across diverse tasks.

% In terms of prompt optimization, recent studies have highlighted the effectiveness of LLMs as prompt optimizers. For instance, \cite{ma2024large} demonstrated that LLMs can effectively refine prompts, boosting performance across various tasks. Similarly, \cite{yang2023large} showed that optimized prompts generated by LLMs can outperform manually crafted ones through the Optimization by PROmpting (OPRO) method. Additionally, \cite{li2023robust} identified that prompt optimization techniques might be vulnerable to distribution shifts, such as subpopulation shifts, and proposed Generalized Prompt Optimization (GPO) to enhance the generalization capabilities of LLMs.

% \section{Related Works} \label{sec:2}

% \textbf{Calibration and Uncertainty.} Calibration of neural networks is a critical area of research focused on ensuring that a model's confidence scores align with the actual probability of correctness. For instance, \cite{kuleshov2018accurate} proposes a recalibration technique that adjusts a model's confidence predictions to better reflect empirical accuracy without altering the model's architecture or training data. Building on this, \cite{zhang2017mixup} introduces mixup training, which improves model calibration by training on convex combinations of input data and labels. These foundational methods paved the way for further advancements.

% To measure calibration, \cite{guo2017calibration} provides a comprehensive analysis of calibration errors in neural networks, introducing widely adopted metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE). These metrics offer quantitative means to assess calibration, facilitating comparative analyses across models. Subsequently, \cite{desai2020calibration} specifically addresses the calibration of LLMs using temperature scaling, a technique fine-tuned to the complexities of language models.

% Additionally, \cite{zhao2021calibrate} presents an ensemble approach to calibrate LLMs, combining multiple models or different versions of a model to achieve a calibrated consensus. More recently, \cite{tian2023just} explored a novel method where language models are asked direct questions and required to evaluate their confidence in their responses. Complementing this, \cite{he2023investigating} conducted an extensive assessment of LLM calibration based on logits, employing metrics such as ECE, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC) to provide a comprehensive evaluation. Furthermore, \cite{lyu2024calibrating} introduced a coherence sampling method to calibrate language models, further contributing to the field.

% \noindent\textbf{Prompt Engineering and Optimization.} 
% Recent advancements in prompt engineering have significantly enhanced the efficiency and effectiveness of interactions with LLMs. Key developments include few-shot and zero-shot learning, which utilize minimal examples to guide models, thereby reducing the need for extensive labeled data. In-context learning, introduced by \cite{brown2020language}, allows models to adapt to tasks based solely on input prompts without requiring parameter updates. Automated prompt generation, explored by \cite{liu2023pre}, employs algorithms such as reinforcement learning to discover optimal prompts, further improving model performance. Additionally, \cite{schick2020exploiting} demonstrated that pre-training models with specific prompt formats can enhance their generalization across tasks.
% Regarding prompt optimization, recent research has underscored the effectiveness of LLMs as prompt optimizers. For instance, \cite{ma2024large} showed that LLMs can effectively optimize prompts, boosting performance across various tasks. Similarly, \cite{yang2023large} demonstrated that optimized prompts generated by LLMs can surpass manually crafted ones through the Optimization by PROmpting (OPRO) method. \cite{li2023robust} revealed that prompt optimization techniques are vulnerable to distribution shifts, such as subpopulation shifts, Generalized Prompt Optimization (GPO) is proposed to improve the generalization abilities of LLMs.


%However, these techniques often encounter challenges related to distribution shifts. To address these issues, robust frameworks such as Generalized Prompt Optimization have been developed \cite{li2023robust}.




% \textbf{Transfer-Prompting Method.} Our work extends the concept of prompt optimization and transfer learning by introducing the \textbf{Transfer-Prompting} method. Unlike traditional methods that focus solely on optimizing prompts for individual tasks, Transfer-Prompting employs a reference LLM as an optimizer to adapt prompts between source tasks and target tasks. This approach aims to enhance the performance of LLMs across diverse tasks simultaneously. By leveraging a calibration objective function to align the model's confidence with its prediction accuracy, our method addresses the limitations of existing calibration techniques and demonstrates significant improvements in prediction accuracy, calibration, and instruction-following rates across various domains.



% In summary, our proposed \textbf{TransferPrompt} technique builds upon these foundational works by leveraging source prompting to optimize prompts and improve both calibration and accuracy. This optimization directly contributes to generating prompts that enhance instruction-following rates, addressing the challenges of practical applications and ensuring robust performance across varied domains.
% \section{Related Works} \label{sec:2}

% \textbf{Calibration and Uncertainty.} Calibration of neural networks is a critical area of research focused on ensuring that a model's confidence scores align with the actual probability of correctness. For instance, \cite{kuleshov2018accurate} propose a recalibration technique that adjusts a model's confidence predictions to better reflect empirical accuracy without altering the model's architecture or training data. Building on this, \cite{zhang2017mixup} introduce mixup training, a method that improves model calibration by training on convex combinations of input data and labels. These foundational methods paved the way for further advancements.

% To measure calibration, \cite{guo2017calibration} provide a comprehensive analysis of calibration errors in neural networks, introducing widely adopted metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE). These metrics offer quantitative means to assess calibration, facilitating comparative analyses across models. Subsequently, \cite{desai2020calibration} specifically address the calibration of LLMs using temperature scaling, a technique fine-tuned to the complexities of language models. 

% Additionally, \cite{zhao2021calibrate} present an ensemble approach to calibrate LLMs, combining multiple models or different versions of a model to achieve a calibrated consensus. More recently, \cite{tian2023just} explore a novel method where language models are asked direct questions and required to evaluate their confidence in their responses. Complementing this, \cite{he2023investigating} conduct an extensive assessment of LLM calibration based on logits, employing metrics such as ECE, Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPRC) to provide a comprehensive evaluation. Furthermore, \cite{lyu2024calibrating} introduce a coherence sampling method to calibrate language models, further contributing to the field. 

% \textbf{Transfer Prompts.} In addition to calibration, recent research has explored the use of large language models (LLMs) as prompt optimizers and for transfer learning in natural language processing tasks. For example, LLMs have shown good performance as prompt optimizers \cite{ma2024large}, and methods like OPRO have leveraged LLMs to generate optimized prompts that outperform manually designed ones \cite{yang2023large}. However, prompt optimization techniques are vulnerable to distribution shifts, and thus, frameworks like Generalized Prompt Optimization have been proposed to address this issue \cite{li2023robust}.

% Moreover, in the domain of transfer learning, LLMs have been used to improve performance in various tasks such as text classification \cite{do2005transfer}, Chinese-named entity recognition \cite{cao2018adversarial}, and neural machine translation tasks \cite{chu2018survey} by fine-tuning pre-trained models, applying adversarial training, and using language models as feature extractors for domain adaptation. Further research has explored the limits of transfer learning using a unified text-to-text Transformer model, demonstrating its wide applicability in various natural language processing tasks \cite{raffel2020exploring}. Additionally, cross-lingual transfer learning techniques, such as translation-based approaches, multilingual representation learning, and zero-shot learning, have been discussed in the context of natural language processing, highlighting their importance in low-resource language scenarios \cite{oroy2024cross}.


