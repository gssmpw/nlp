
\begin{figure*}[!h]
\centering
\includegraphics[width=.329\textwidth]{Figures/14.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/15.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/16.pdf} 
\caption{Comparative performance evaluation of various medical, legal, and financial models. The confidence is calculated by the verbalized confidence method.}
\label{fig-sensitive1}
\end{figure*}


\begin{figure}[!h]
\centering
\includegraphics[width=.98\columnwidth]{Figures/12.pdf}

\vspace*{1mm} 

\includegraphics[width=.98\columnwidth]{Figures/13.pdf}
\caption{Score curves of the two-stage prompt optimization process of Transfer-Prompting on MMLU medical-related tasks.}
\label{fig: source and target}
\end{figure}



\section{Results and Analysis}
\subsection{Performance Analysis of Commonsense Reasoning with Transfer Prompting}

This experiment aims to evaluate the effectiveness of Transfer-Prompting in enhancing the performance of foundational LLMs on commonsense reasoning tasks. Three widely used benchmark datasets for assessing reasoning capabilities in natural language processing were selected: LogiQA, OpenbookQA, and CosmosQA. The evaluated models include GPT-3.5-Turbo, GPT-4, LLaMA-2-7B, LLaMA-2-13B, LLaMA-3-8B, Vicuna-7B, and Vicuna-13B.
Under zero-shot and five-shot settings, the instruction-following ability and overall response quality of these LLMs were assessed. 
The Origin Prompt utilized examples from Figure~\ref{fig1.1}. In contrast, the Transfer Prompt was optimized using the second stage of the Transfer-Prompting method, which generated a set of high-scoring Target Prompts on the target task dataset.

The results in Table \ref{tab:1-commonsense} show that Transfer-Prompting significantly improves the performance of most LLMs in the zero-shot setting, especially on GPT-4. Specifically, GPT-4â€™s instruction following rate (IFR) increases from 0.70 to 0.82, accuracy improves from 0.44 to 0.50, expected calibration error (ECE) decreases from 0.30 to 0.18, ROC increases from 0.69 to 0.81, PR-P increases from 0.66 to 0.74, and PR-N decreases from 0.44 to 0.32 on the LogiQA dataset. In addition, the LLaMA series models, especially LLaMA-3-8B, also show significant improvements, with IFR rising from 0.66 to 0.79 on the LogiQA dataset. In contrast, the Vicuna series models show relatively low performance improvements, which may be due to inherent architectural limitations. These results show that by introducing Transfer-Prompting, the ability of LLMs to follow instructions and the overall response quality are significantly enhanced.



\begin{figure*}[!h]
\centering
\includegraphics[width=.98\textwidth]{Figures/logits.pdf}
\hfill
\caption{The zero-shot performance of different medical domain LLMs on MMLU medical-related tasks is evaluated using logits.}
\label{fig: logits-zero}
\end{figure*}


\subsection{Performance Analysis on Sensitive Domains}

This experiment evaluates 18 LLMs in three professional fields: medical, legal, and financial. The tasks related to the above fields in the MMLU and CMMLU datasets are used for testing. The models evaluated include domain-specific models, such as Med-Alpaca-13B, Law-LLM-13B, and Finance-LLM-13B. The performance indicators cover IFR, ACC, ECE, ROC, PR-P, and PR-N to comprehensively evaluate the instruction compliance and overall response quality of LLMs in different professional tasks.

As shown in Figures \ref{fig-sensitive1} (a), (b), and (c), Transfer Prompt performs better than Origin Prompt in all fields. Specifically, in subgraph (a) of the medical MMLU dataset, Medicine-LLM-13B achieves the highest IFR (0.77), ACC (0.64), and PR-P (0.78), and the lowest ECE (0.15) and PR-N (0.32) using Transfer Prompt. In subgraph (b) of the legal MMLU dataset, ChatLaw-13B achieves the highest ACC (0.63) and PR-P (0.84), and the lowest ECE (0.11) and PR-N (0.22) using Transfer Prompt, also significantly outperforming the Origin Prompt configuration. Finally, in sub-figure (c) of the CMMLU dataset in the financial domain, Fin-GPT-LLaMA-13B achieves the highest ACC (0.60), ROC (0.79), and PR-P (0.83), and the lowest ECE (0.18) and PR-N (0.21) using Transfer Prompt, outperforming the Origin Prompt configuration.
The above results clearly show that Transfer Prompt systematically improves the model's instruction compliance and output quality in complex professional tasks, providing a powerful and adaptable solution for solving complex tasks in specific domains.


\subsection{Analysis of source and target prompt optimization evaluation process}
In this experiment, we comprehensively evaluate the dual-stage prompt optimization process of Transfer-Prompting. A unified scorer LLM PaLM 2-L is used for evaluation. The optimization process uses the multi-dimensional metrics $\mathcal{M}$ designed in the objective prompt evaluator as the performance indicator. Four reference LLMs (PaLM 2-L-IT, GPT-4, PaLM 2-L, and GPT-3.5-Turbo) are used as optimizers to generate candidate prompts for evaluation. The evaluation includes source prompt evaluation (orange solid line) and target prompt evaluation (blue dashed line), and a total of 200 optimization steps are performed on the MMLU medical-related task.

As shown in Figure \ref{fig: source and target}, the dual-stage prompt optimization process of Transfer-Prompting significantly improves the overall performance of the scorer LLM, and the target prompt always performs better than the source prompt throughout the evaluation process. In the reference LLM, PaLM 2-L-IT achieved near-perfect performance early in the optimization process and quickly stabilized. GPT-4 and GPT-3.5-Turbo also achieved steady improvements, with scores eventually stabilizing between 0.88 and 0.9, further verifying the adaptability and performance of the framework. PaLM 2-L fluctuated but overall showed an upward trend, indicating that the Transfer-Prompting framework can effectively optimize performance even for models with low initial scores.




% This section analyzes the effectiveness of POI in optimizing LLM performance. The evaluation accuracy curves depicted in Figure \ref{fig2} illustrates the optimization evaluation performance of POI on the MMLU dataset using different reference LLMs. The evaluation accuracy curves highlight the efficiency of POI in enhancing model performance over different optimization steps. Specifically, when we use text-bison as the reference LLM, the curve demonstrates rapid convergence to the high accuracy, reaching close to 100\% accuracy within the initial steps. Similarly, when GPT-4 is employed as the reference LLM, the curve shows substantial accuracy improvements, achieving around 88\% accuracy in the early stage of the optimization process.

% This analysis suggests that our POI method effectively guides the optimization process, leading to significant performance gains in a relatively short number of iterations. The POI method's ability to quickly improve prediction accuracy indicates its potential for efficient prompt adaptation across various tasks. Additionally, the comparison between different reference LLMs reveals that although both text-bison and GPT-4 are effective in optimization, there are differences in their convergence patterns. text-bison reaches higher accuracy faster, whereas GPT-4 maintains more stable accuracy improvements over a broader range of steps. This insight is valuable for selecting appropriate reference LLMs based on specific task requirements and expected performance improvement. In the Appendix, we illustrate target prompts adapted by different reference models in Table \ref{table5}.


% \begin{table}[htbp]
%   \centering
%   \caption{Add caption}
%    \resizebox{.98\columnwidth}{!}{\begin{tabular}{lrr}
%     \toprule
%     Methods & \multicolumn{1}{l}{Average Preformance} & \multicolumn{1}{l}{Test Average Preformance} \\
%     \midrule
%     Transfer-Prompting & 0.89  & 0.88  \\
%     OPRO  & 0.87  & 0.85  \\
%     Iterative-APE & 0.85  & 0.84  \\
%     APO   & 0.86  & 0.83  \\
%     EvoPrompt (GA) & 0.81  & 0.80  \\
%     EvoPrompt (DE) & 0.79  & 0.75  \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:addlabel}}%
% \end{table}%

\begin{figure}[!h]
\centering
\includegraphics[width=.98\columnwidth]{Figures/baselines.pdf}

\caption{Performance Comparison with Baselines}
\label{fig: baselines}
\end{figure}


\subsection{Comparison with baselines}

To further demonstrate the effectiveness of the Transfer-Prompting method, we compared our approach with several state-of-the-art baseline methods, including OPRO \citep{yang2024largelanguagemodelsoptimizers}, Iterative-APE \citep{zhou2022large}, PromptAgent \citep{wang2023promptagent}, and APO \citep{pryzant2023automatic}. For optimization, the reference LLM used was PaLM 2-L-IT, and the scorer LLM was PaLM 2-L. The dataset used was the MMLU medical-related tasks. The data for Transfer-Prompting represents the average score from the second-stage optimization process.

As shown in Figure \ref{fig: baselines}, Transfer-Prompting consistently outperforms all baseline methods. During training (left), our method achieved the highest overall performance score. In the testing phase (right), Transfer-Prompting still showed a significant advantage over the other baseline methods. In addition, the small difference between Transfer-Prompting's average scores in the training and testing phases demonstrates that the optimization process did not overfit. This sustained superiority underscores the robustness of our method, as it effectively generalizes to unseen data.





\subsection{Analysis of Logits}
To further verify the effectiveness of our method, this section evaluates the effectiveness of Transfer-Prompting in improving LLM in zero-shot and five-shot settings through Logits. The evaluation indicators are ACC, ECE, ROC, PR-P, and PR-N. Six medical-specialized LLMs, including ChatDoctor-13B, PMC-LLaMA-13B, Med-Alpaca (7B \& 13B), and Medicine-LLM (7B \& 13B), were compared on MMLU medical-related tasks.

As shown in Figure \ref{fig: logits-zero}, Transfer-Prompting significantly improves the performance of all models in the zero-shot setting. For example, the ACC of ChatDoctor-13B using Transfer Prompt increases from 0.44 to 0.51, indicating that the prediction is more accurate, while its ECE decreases from 0.07 to 0.05, indicating better-calibrated predictions. Furthermore, ROC increased from 0.71 to 0.82, PR-P increased from 0.68 to 0.75, and PR-N decreased from 0.43 to 0.31, indicating that the overall quality of the model output has been significantly improved. 
The consistent gains across all evaluation metrics demonstrate the potential of Transfer-Prompting to generalize well in medical domains, thus broadening its applicability and impact in real-world scenarios.


% The above analysis strongly proves that Transfer-Prompting can continuously enhance the performance of the model on complex multi-task datasets.


% This makes Transfer-Prompting a valuable technique for applications in sensitive domains such as medical, legal, and financial fields, where accurate and dependable predictions are crucial.
% This section presents an analysis of logits to evaluate the effectiveness of Transfer-Prompting in enhancing the performance of LLMs. 

% This section evaluates the logits and analyzes the effectiveness of POI in enhancing the performance of LLMs.
% The logits provide insight into model calibration and the distribution of confidence levels across different tasks. Figure \ref{fig: logits-zero} illustrates the key metrics for this analysis. The analysis of logits reveals that POI significantly enhances model performance on all metrics. For example, Chatdoctor's ACC improved from 0.44 to 0.51, and its ECE decreased from 0.07 to 0.05, indicating better-calibrated predictions. Similarly, ROC scores increased, highlighting the improved confidence in the model's predictions. These results demonstrate the robustness of POI in improving model calibration and reliability, making it a valuable technique for enhancing LLM performance in various tasks.

% The POI method shows consistent improvements across different models and datasets. This is evident from the higher ACC, lower ECE, and improved ROC, PR-P, and PR-N values compared to the original prompts. This analysis underscores the effectiveness of POI in providing better-calibrated and more reliable outputs. Importantly, these enhancements are critical for practical applications, especially in sensitive domains like medical, legal, and financial contexts, where accurate and reliable predictions are significantly paramount.

% Additionally, the improvements in calibration suggest that POI not only increases accuracy but also ensures that the models' confidence levels are more aligned with their true performance. This alignment is essential for applications where decision-making is based on model outputs, as it reduces the risk of overconfident incorrect predictions and underconfident correct predictions. The consistent gains across all evaluated metrics demonstrate the potential of POI to generalize well across various domains, thus broadening its applicability and impact in real-world scenarios.


% \subsection{Analysis of different optimize methods}

% This section presents an analysis of logits to evaluate the effectiveness of Transfer-Prompting in enhancing the performance of LLMs. The logits provide insight into model calibration and the distribution of confidence levels across different tasks. Figure 3 illustrates the key metrics from this analysis.
% The analysis of logits reveals that Transfer-Prompting significantly enhances model performance across all metrics. For example, Chatdoctor's ACC improved from 0.44 to 0.51, and its ECE decreased from 0.07 to 0.05, indicating better-calibrated predictions. Similarly, ROC-AUC scores increased, highlighting the improved confidence in the model's predictions. These results demonstrate the robustness of Transfer-Prompting in improving model calibration and reliability, making it a valuable technique for enhancing LLM performance in various tasks.

% Transfer-Prompting shows consistent improvements across different models and datasets. This is evident from the higher ACC, lower ECE, and improved ROC, PR-P, and PR-N values when compared to the original method. This analysis underscores the effectiveness of Transfer-Prompting in providing better-calibrated and more reliable outputs. Importantly, these enhancements are critical for practical applications, especially in sensitive domains like medical, legal, and financial contexts, where accurate and reliable predictions are paramount.
% Additionally, the improvements in calibration suggest that Transfer-Prompting not only increases accuracy but also ensures that the models' confidence levels are more aligned with their true performance. This alignment is essential for applications where decision-making is based on model outputs, as it reduces the risk of overconfident incorrect predictions and underconfident correct predictions. The consistent gains across all evaluated metrics demonstrate the potential of Transfer-Prompting to generalize well across various domains, thus broadening its applicability and impact in real-world scenarios.




% \subsection{Analysis of Temperature scaling}


