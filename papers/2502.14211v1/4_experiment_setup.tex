% \section{Experiments} \label{sec:4}
\section{Experimental Setup}
\label{sec:4}

\input{Tables/table1}

\subsection{Models and Datasets}

To evaluate the effectiveness of Transfer-Prompting, we tested 7 foundation Models on 3 Common-sense Reasoning datasets, including GPT-3.5-Turbo \citep{chatgpt}, GPT-4 \citep{openai2023gpt4}, LLaMA-2 (7B \& 13B) \citep{touvron2023llama}, LLaMA-3-8B \citep{llama3modelcard}, and Vicuna (7B \& 13B) \citep{zheng2023judging}. In addition, we also evaluated 18 professional models on 6 multi-task datasets, including medical, legal, and financial fields.

Specifically, in the medical domain, we evaluated 6 specialized LLMs: ChatDoctor-13B \citep{li2023chatdoctor}, PMC-LLaMA-13B \citep{wu2023pmcllama}, MedAlpaca (7B \& 13B) \citep{han2023medalpaca}, and Medicine-LLM (7B \& 13B) \citep{cheng2023adapting}. For the legal domain, we tested 6 law-specific LLMs: DISC-LawLLM-13B \citep{yue2023disclawllm}, Lawyer-LLaMA-13B \citep{huang2023lawyer}, ChatLaw-13B \citep{cui2024chatlaw}, LawGPT-7B \citep{zhou2024lawgpt}, and Law-LLM (7B \& 13B) \citep{cheng2023adapting}. In the financial domain, we evaluated 6 LLMs: CFGPT-7B-Full \citep{li2023cfgpt}, Tongyi-Finance-14B-Chat \citep{Tongyi-Finance-14B-Chat}, FinGPT-13B-v2 (based on LLaMA-2-13B) \citep{yang2023fingpt}, FinMA-7B-Full \citep{xie2023pixiu}, and Finance-LLM (7B \& 13B) \citep{cheng2023adapting}.

The foundational models were assessed using 3 commonsense reasoning datasets, including LogiQA \citep{liu2020logiqa}, OpenBookQA \citep{Mihaylov2018CanAS}, and CosmosQA \citep{huang2019cosmos}. In the professional field, we adopt total 6 multi-task datasets to evaluate professional LLM:

\textbf{Medical Domain}: The corresponding medical models are evaluated using MMLU \citep{hendrycks2021measuring}, C-Eval \citep{huang2024c}, and MedMCQA \citep{pmlr-v174-pal22a} datasets.

\textbf{Legal Domain}: The corresponding legal models are assessed with MMLU \citep{hendrycks2021measuring}, CMMLU \citep{li2023cmmlu}, and AGIEval \citep{zhong2023agieval} datasets.

\textbf{Financial Domain}: The corresponding financial models are evaluated using CMMLU \citep{li2023cmmlu}, C-Eval \citep{huang2024c}, and FinEval \citep{zhang2023fineval} datasets.

 
\subsection{Confidence Evaluation Methods}

We employ the following approaches to quantify model uncertainty:

\noindent\textbf{Logits} \citep{yang2023improving}: The probabilities generated by the model are directly interpreted as confidence scores, with the highest probability corresponding to the selected answer in multiple-choice questions.

\noindent\textbf{Verbalized Confidence} \citep{lin2022teaching}: By prompting LLMs, we obtain both answers and their associated confidence scores. These scores are utilized to assess the models' calibration by analyzing the relationship between accuracy and the confidence levels of all valid responses.





\subsection{Evaluation Metrics}

We assess the instruction-following capabilities of LLMs using the instruction-following rate and accuracy. Additionally, we evaluate the overall response quality of the models through expected calibration error, area under the receiver operating characteristic curve, and area under the precision-recall curve.

\textbf{Expected Calibration Error (ECE):} ECE measures the alignment between predicted probabilities and actual outcomes, providing insight into the model's calibration quality. It is calculated as:

\begin{equation}
\text{ECE} = \sum_{i=1}^{n} \frac{|B_i|}{N} \cdot \left| \text{acc}(B_i) - \text{conf}(B_i) \right|,
\end{equation}

where \( n \) is the number of bins (defaulting to 10 in this study), \( B_i \) represents the samples in bin \( i \), \( N \) is the total number of samples, \( \text{acc}(B_i) \) is the accuracy within bin \( i \), and \( \text{conf}(B_i) \) is the mean predicted probability in bin \( i \).

\textbf{Area Under the Receiver Operating Characteristic Curve (AUROC):} AUROC evaluates a binary classification modelâ€™s ability to distinguish between positive and negative classes. It is derived from the area under the ROC curve, which plots the true positive rate against the false positive rate across various thresholds.

\textbf{Area Under the Precision-Recall Curve (PR-AUC) for Positive and Negative Classes (PR-P, PR-N):} PR-P and PR-N measure a model's precision and recall for positive and negative classes, respectively. PR-P is particularly useful for evaluating performance on imbalanced datasets, while PR-N is essential for accurately identifying negative instances.

\textbf{Instruction Following Rate (IFR):} IFR quantifies the proportion of instances where the model's response adheres to the specified instructions. It is defined as:

\[
\text{IFR} = \left( \frac{N_S}{N_T} \right) \times 100\%,
\]

where \( N_S \) is the number of instances where the LLM's responses satisfy the specified requirements, and \( N_T \) is the total number of instructions attempted, encompassing both successful and unsuccessful responses.

