\section{Introduction} \label{sec:1}

Large Language Models (LLMs) have made significant strides in natural language processing, enabling high-quality text generation across a variety of applications, such as conversational agents, content creation, and machine translation \citep{wei2022emergent}. However, deploying LLMs in real-world applications presents a unique set of challenges, particularly in balancing high-quality output generation with the ability to effectively follow instructions across diverse and complex tasks \citep{wang2023large, chang2024survey}.

These challenges become especially pronounced in tasks with multiple subtasks or stringent constraints, where LLMs often produce hallucinated outputsâ€”responses that, while syntactically coherent, are factually incorrect or irrelevant \citep{ji2023survey, bang2023multitask}. Furthermore, LLMs can misinterpret user queries, leading to responses that fail to meet user expectations or address the core of the question \citep{kulkarni2024crafting}. These limitations not only hinder the utility of LLMs but also expose them to significant risks in sensitive domains such as healthcare, legal, and finance, where inaccurate or off-topic outputs could have severe consequences \citep{nori2023capabilities}.

One potential approach to mitigating these challenges is the use of LLM-based automatic prompt optimization \citep{zhou2023large, pryzant2023automatic}. These methods typically involve using an LLM to iteratively optimize prompts, with the goal of improving model performance on specific tasks. However, existing optimization techniques have primarily focused on single-stage optimization, often with the objective of enhancing a single evaluation metric \citep{yang2024largelanguagemodelsoptimizers, sun2023autohint}. While these methods can be effective in certain contexts, they often fail to account for the complexities inherent in multi-objective tasks or tasks that require balancing multiple, sometimes conflicting, evaluation criteria. For example, tasks that demand LLMs to balance the tradeoff between maximizing output quality and maintaining high instruction-following accuracy remain particularly challenging for current models. Moreover, existing methods often overlook the need for comprehensive evaluation across multiple dimensions of performance, limiting insights into the overall effectiveness of the model \citep{chen2024multi}.

To address these limitations, we propose \textbf{Transfer-Prompting}, a novel two-stage framework designed to optimize prompts for LLMs across complex tasks. This framework consists of two key components: (1) \textit{source prompt construction}, which refines the original prompts on source task datasets to generate source prompts with enhanced generalization ability, and (2) \textit{target prompt generation}, which enhances cross-task adaptation of target prompts by fine-tuning a set of high-scoring source prompts on task-specific datasets.
In each optimization cycle, the reference LLM generates candidate prompts based on the historical prompt-score pairs and task descriptions in our designed reference prompt. The optimization process terminates when the reference LLM fails to generate a higher-scoring prompt or the number of optimization steps reaches an upper limit. The scorer LLM evaluates the effectiveness of candidate prompts using the multidimensional metrics designed in the objective prompt evaluator, a novel contribution of this study that provides a holistic assessment of prompt quality and task performance.

We validate the Transfer-Prompting framework through extensive experiments conducted on 25 LLMs, including 7 foundation models (e.g., GPT-3.5-Turbo \citep{chatgpt}, GPT-4 \citep{openai2023gpt4}) and 18 specialized models from medical, legal, and financial sectors. The evaluation is conducted using 3 heterogeneous reasoning datasets and 6 multi-task datasets tailored to these specialized models. The results demonstrate that Transfer-Prompting significantly enhances task-specific performance and cross-task adaptation, improving both instruction-following rates and overall output quality across diverse tasks.

Our main contributions are:
\begin{itemize}
    \item To enhance the cross-task adaptability of LLMs, especially for solving complex multi-objective tasks, we propose a novel LLM-based automatic prompt optimization framework, \textbf{Transfer-Prompting}. The framework consists of two core stages: source prompt construction and target prompt generation.
    \item The optimization process primarily relies on four key tools. The reference LLM generates candidate prompts based on the requirements of the reference prompt, while the scorer LLM evaluates and provides feedback based on multi-dimensional metrics integrated into the objective prompt evaluator.
    \item Extensive experiments conducted on 25 LLMs (including both base and specialized models) demonstrate that Transfer-Prompting significantly enhances task-specific performance, showcasing its potential for enhancing cross-task adaptation in LLMs.
\end{itemize}

