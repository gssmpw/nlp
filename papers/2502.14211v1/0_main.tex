%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{array}


\usepackage{booktabs} % 负责处理表格线条
\usepackage{multirow} % 负责表格的多行合并

\usepackage{subcaption}
% Define colors
\usepackage[ruled,vlined]{algorithm2e}


\usepackage{amsmath}
\usepackage{array}
\usepackage{booktabs}
\definecolor{lightgray}{gray}{0.9}
\definecolor{darkgray}{gray}{0.7}
\usepackage{xcolor}
\definecolor{burntorange}{RGB}{204, 85, 0} % Example RGB values

% Standard package includes
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
% \usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algpseudocode}
\usepackage{adjustbox}

\usepackage{times}
\usepackage{bm}

\usepackage{amsmath} 
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{tabularx}

\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amsmath} % for math commands like \sum
 \usepackage{cleveref}
% \usepackage{hyperref}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{multirow} 
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{tikz}
\usepackage[utf8]{inputenc}  % 添加对 UTF-8 编码的支持
\usepackage{subcaption} 

\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{etoc}
\usepackage{tcolorbox}

\usepackage{dblfloatfix}

\usepackage{booktabs} % 添加此行以使用 \toprule, \midrule, \bottomrule 等命令
\usepackage{amsmath} % 用于数学符号
% \usepackage{algorithm}


\newcommand{\llms}{LLMs\xspace}
\usepackage{booktabs}

\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{todonotes}
\usepackage{natbib} 

\definecolor{lightblackblue}{RGB}{115,161,216} % 淡黑蓝
\definecolor{lightblackgreen}{RGB}{59,125,35} % 淡黑绿
\definecolor{lightblackorange}{RGB}{233,113,50} % 淡黑橙



\usepackage{hyperref}

\definecolor{teal}{RGB}{0,128,128}
\definecolor{darkgreen}{RGB}{0,100,0}
\definecolor{brown}{RGB}{165,42,42}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization}

% Single author syntax
%\author{
%    Anonymous submission
    % \affiliations
    % Affiliation
    % \emails
    % email@example.com
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
Yupeng Chang$^1$
\and
Yi Chang$^1$$^2$$^3$\and
Yuan Wu$^1$\footnote{Corresponding author}\\
\affiliations
$^1$School of Artificial intelligence, Jilin University\\
$^2$Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, Jilin University\\
$^3$International Center of Future Science, Jilin University\\
\emails
yuanwu@jlu.edu.cn
}
%\fi

%\includeonly{content}  % 只编译正文
\includeonly{6_appendix}  % 只编译附录，生成附录部分的 PDF

\begin{document}
\etocdepthtag.toc{chapter}
\etocsettagdepth{chapter}{none}
\etocsettagdepth{appendix}{none}

\maketitle


\begin{abstract}
Large Language Models (LLMs) face significant challenges when balancing multiple high-level objectives, such as generating coherent, relevant, and high-quality responses while maintaining efficient task adaptation across diverse tasks. 
To address these challenges, we introduce \textbf{Transfer-Prompting}, a novel two-stage framework designed to enhance cross-task adaptation in prompt generation. The framework comprises two key components: (1) \textit{source prompt construction}, which refines the original prompts on source task datasets to generate source prompts with enhanced generalization ability, and (2) \textit{target prompt generation}, which enhances cross-task adaptation of target prompts by fine-tuning a set of high-scored source prompts on task-specific datasets.
In each optimization cycle, a reference LLM generates candidate prompts based on historical prompt-score pairs and task descriptions in our designed reference prompt. These candidate prompts are refined iteratively, while a scorer LLM evaluates their effectiveness using the multi-dimensional metrics designed in the objective prompts evaluator—a novel contribution in this work that provides a holistic evaluation of prompt quality and task performance. This feedback loop facilitates continuous refinement, optimizing both prompt quality and task-specific outcomes.
We validate Transfer-Prompting through extensive experiments across 25 LLMs, including 7 foundation models and 18 specialized models, evaluated on 9 diverse datasets. The results demonstrate that Transfer-Prompting significantly improves task-specific performance, highlighting its potential for enhancing cross-task adaptation in LLMs.
The code is available at \url{https://github.com/llm172/Transfer-Prompting}.
% For instance, on the LogiQA dataset, instruction-following accuracy increased by 17.1\%, and calibration error was reduced by 40\%. In specialized domains like law, the ChatLaw-13B model achieved a 41.5\% increase in instruction-following accuracy and a 42.9\% reduction in calibration error on the AGI-Eval dataset.
\end{abstract}


% We conducted comprehensive experiments evaluating 7 general-purpose models (GPT-3.5-Turbo and GPT-4) and 18 specialized models across medical, legal, and financial domains. The evaluation encompassed three heterogeneous reasoning datasets and nine cross-domain datasets.
% Results demonstrate that our method significantly enhances LLMs' instruction-following rate and overall output quality on targeted domain-specific tasks.

% \begin{abstract}
% Large language models (LLMs) face a trade-off between instruction-following and output quality in practical applications. For example, in complex tasks, the need to simultaneously accomplish multiple subtasks or meet diverse constraints may lead to a decline in accuracy and even hallucinated outputs.
% To address this issue, we propose the \textbf{Transfer-Prompting}, a dual-stage framework for optimizing prompts.
% It primarily comprises two key modules: (1) \textit{source prompt construction}, aimed at enhancing the generalization ability of prompts, and (2) \textit{target prompt generation}, designed to improve the instruction-following rate and output quality of the model for specific tasks.
% For each optimization round, a reference LLM generates new prompts based on historical prompts and task descriptions, refining them iteratively. Meanwhile, a scoring LLM evaluates the effectiveness of the latest prompts, with feedback integrated into subsequent optimizations.
% The experiments comprehensively evaluated six general-purpose models (including GPT-3.5-Turbo and GPT-4) and 36 specialized models across medical, legal, and financial domains. The evaluation tasks spanned three heterogeneous reasoning datasets and nine cross-domain datasets. 
% The results show that our method significantly improves the instruction following rate and overall output quality of LLM on the specified domain tasks.
% \end{abstract}
% }
% Experimental results demonstrate that our method significantly enhanced the instruction-following rate and improved LLMs' overall output quality.


\input{1_introduction}


\input{2_related_works}


\input{3_methods}


\input{4_experiment_setup}

\input{5_result_analysis}








\section{Conclusion} \label{sec:6}


% Future work will focus on several key areas to further enhance the practical utility of LLMs. This includes scaling the approach for larger models, customizing it for specialized domains to meet specific requirements, improving robustness against distribution shifts to ensure reliable performance in varied conditions, and applying it to interactive and real-time applications. Additionally, we aim to explore the integration of Transfer-Prompting with other advanced techniques, such as reinforcement learning and meta-learning, to further push the boundaries of LLM performance.


% In this study, we introduce \textbf{Transfer-Prompting}, a novel approach to enhancing the performance of LLMs by optimizing and transferring source prompts to target tasks. Our extensive evaluations across general and domain-specific applications, such as medical, legal, and financial tasks, demonstrate significant improvements in metrics including Instruction-Following Rate and Expected Calibration Error. The analysis of logits and evaluation accuracy curves further validate the robustness and efficiency of Transfer-Prompting, highlighting its potential for rapid and substantial performance gains. 
In this study, we introduce \textbf{Transfer-Prompting}, an innovative approach designed to enhance the generalization capabilities of LLMs by optimizing and adapting source prompts for the target task. One of the main advantages of Transfer-Prompting is its ability to generate prompts that are well-adapted to the target dataset, thereby improving the model's performance. This adaptability makes it particularly suitable for applications in diverse fields such as healthcare, legal, and financial services, where accurate and reliable model outputs are critical. Additionally, our approach is expected to alleviate problems associated with model calibration, ensuring that the confidence of predictions aligns more closely with their true accuracy.
Extensive evaluations of base models and domain-specific models demonstrate significant improvements in prediction accuracy, calibration, and instruction-following. 

% Transfer-Prompting's robustness and efficiency are validated through various metrics, highlighting its potential to enhance performance rapidly.



% \section{Limitations}

% Our proposed method, POI, shows promising results in adapting LLMs to target tasks but has several limitations. The computational resources required for extensive evaluations of larger-size LLMs are substantial, which may limit the scale and diversity of our experiments. We plan to leverage more powerful computational resources in future research. While we have tested POI across various domains, its generalizability to all possible tasks remains uncertain, necessitating further assessment in a wider range of applications. 


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\bibliographystyle{named}
\bibliography{ref}

 \newpage
 \appendix

 \vspace{2em}
 \begin{center}
     \Large{\textbf{Appendix}}
 \end{center}
 \vspace{2em}

 \etocdepthtag.toc{appendix}
 \etocsettagdepth{chapter}{none}
 \etocsettagdepth{appendix}{subsection}
 \tableofcontents

 \include{6_appendix}

\end{document}