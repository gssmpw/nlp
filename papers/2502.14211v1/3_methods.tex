
\begin{figure*}[!t]
\centering
\includegraphics[width=.7\textwidth]{Figures/1.1.pdf}
\caption{Illustration of the Two-Stage Prompt Automatic Optimization Framework in \textbf{Transfer-Prompting}: This framework mainly consists of two optimization stages: source prompt construction and target prompt generation. It involves four key tools: reference LLM, reference Prompt, scorer LLM, and the corresponding objective prompt evaluator.
}
\label{fig1.1}
\end{figure*}



\section{Methods} \label{sec:3}

\subsection{Preliminaries}

First, we define two task sets: source tasks $\bm{\mathcal{S}}$ and target tasks $\bm{\mathcal{T}}$. Each source task $\mathcal{S}_i$ is associated with a dataset $D_{\mathcal{S}_i} = \{(q_{i,n}, a_{i,n})\}_{n=1}^{M_i}$, where $q_{i,n}$ is the input, and $a_{i,n}$ is the corresponding output. Similarly, each target task $\mathcal{T}_k$ is associated with a dataset $D_{\mathcal{T}_k} = \{(q_{k,m}, a_{k,m})\}_{m=1}^{N_k}$, where $q_{k,m}$ is the input, and $a_{k,m}$ is the corresponding  output.
The source task dataset is constructed by selecting related tasks from multiple datasets within the same domain to ensure domain consistency. This strategy enables the model to learn shared domain knowledge across similar tasks, thereby enhancing its generalization capabilities. In contrast, the target task dataset is assembled by selecting specific tasks from datasets within a particular domain to maintain task focus. 

 % and introduce data diversity
% The source task dataset $D_{\mathcal{S}}$ is constructed by selecting related tasks from multiple datasets within the same domain, ensuring both consistency and diversity to improve generalization. The target task dataset $D_{\mathcal{T}}$ focuses on tasks from a specific domain, enhancing relevance and reducing overfitting.


\subsection{Transfer-Prompting Framework Design}
% Current mainstream LLMs often face challenges in balancing instruction adherence, output quality, and other performance aspects, particularly for complex tasks. To address these challenges, we introduce a two-stage prompt optimization framework, \textbf{Transfer-Prompting}, which integrates multiple evaluation metrics into the optimization process. 

% As illustrated in Figure~\ref{fig1.1}, the framework comprises two primary modules: \textit{Source Prompt Construction} and \textit{Target Prompt Generation}.
% In the first stage, the model optimizes prompts using the source task datasets ($D_{\mathcal{S}}$) to generate a source prompt set that generalizes across tasks while optimizing for multiple metrics. In the second stage, the source prompts are refined using the target task datasets ($D_{\mathcal{T}}$), forming a target prompt set optimized for specific tasks.
% Specifically, the reference LLM generates and refines prompts in two optimization stages based on the instructions in the reference Prompt. The scorer LLM evaluates candidate prompts using a comprehensive performance evaluation metric integrated into the objective prompt evaluator. The evaluation prompt used corresponds to those generated during the optimization process.
Current mainstream LLMs often face challenges in balancing instruction following, overall output quality, and other performance aspects. In particular, they perform poorly on complex multi-tasks. To address these challenges, we propose a novel LLM-based automatic optimization framework, \textbf{Transfer-Prompting}, which aims to find instructions that maximize the performance of the target task.

As shown in Figure~\ref{fig1.1}, the optimization process consists of two main stages: source prompt construction and target prompt generation.
In the first stage, the origin prompt is refined on the source task dataset $D_{\mathcal{S}}$ to generate source prompts with enhanced generalization ability, $\mathcal{P}_{\text{source}}$; in the second stage, a set of target prompts with enhanced cross-task adaptability of target prompts is generated by fine-tuning a set of high-scoring source prompts on a task-specific dataset, $\mathcal{P}_{\text{target}}$.
The optimization process mainly involves 4 tools. The reference LLM will generate candidate prompts based on the historical prompt score pairs and task descriptions in the reference prompts we designed. The optimization process terminates when the reference LLM cannot generate higher-scoring prompts or the number of optimization steps reaches an upper limit. The scorer LLM uses the multi-dimensional metrics designed in the objective prompt evaluator to evaluate the effectiveness of the candidate prompts - this is a novel contribution, which provides a holistic evaluation of prompt quality and task performance.

\begin{figure*}[!t]
\centering
% \vspace{1.5p/t}
\includegraphics[width=.75\textwidth]{Figures/2_2.pdf}
\hfill
\caption{An example of the reference prompt for reference LLM (PaLM 2-L and PaLM 2-L-IT) on the medically relevant datasets. The generated instruction is inserted at the position marked by <INS> in the input. The \textcolor{lightblackgreen}{green} text displays instructions for prompts and scores; the \textcolor{lightblackorange}{orange} text provides examples of how to apply the instruction; the \textcolor{lightblackblue}{blue} text contains the prompts and scores pairs.
}
\label{fig: transfer-prompting}
\end{figure*}

\textbf{Prompt Optimization Strategy.}
At each iteration \( t \), the reference LLM generates candidate prompts \( \{ P^{(t)}_c \}_{c=1}^K \), which are evaluated by the scorer LLM. The composite performance score \( s^{(t)}_c \) is computed across datasets \( D \) and metrics \( \mathcal{M} \) as follows:

\begin{equation}
s^{(t)}_c = \sum_{d \in D} \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P^{(t)}_c, d),
\label{eq:objective_evaluator_multi_metric}
\end{equation}

where \( D \) represents the set of datasets under consideration (i.e., \( D = \bigcup_{i=1}^\kappa D_{\mathcal{S}_i} \) for source tasks or \( D = \bigcup_{k=1}^\tau D_{\mathcal{T}_k} \) for target tasks), \( \mathcal{M} \) denotes the multi-dimensional metrics designed in objective prompt evaluator, \( w_m \) is the weight assigned to metric \( m \), and \( \phi_m(P, d) \) signifies the performance score of prompt \( P \) on dataset \( d \) according to metric \( m \).

The optimization objective is to maximize the composite performance across all prompts \( \mathcal{P} \):

\begin{equation}
\mathcal{P}^{*} = \arg\max_{\mathcal{P}} \sum_{P \in \mathcal{P}} \sum_{d \in D} \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P, d).
\label{eq:optimization_objective_multi_metric}
\end{equation}

This objective allows simultaneous optimization across multiple dimensions, providing a comprehensive evaluation. The resulting scores \( s^{(t)}_c \) guide the generation of new prompts until performance improvement is minimal or the maximum iteration limit is reached.

\textbf{Reference LLM and Scorer LLM.} In both stages of the optimization, we use advanced LLMs from different architectures as the reference LLM, which performs prompt generation as required by the reference Prompt. The most reliable LLM, chosen for its consistent and robust performance, serves as the scorer LLM. As shown in Figure~\ref{fig: transfer-prompting}, the reference prompt mainly consists of two components: (1) previously generated prompts along with their corresponding scores and (2) a detailed description of the optimization problem, including task examples. The reference LLM generates new prompts at each iteration to improve the ability of instruction-following and the overall performance of the corresponding scorer LLM.

\textbf{Objective Prompt Evaluation.}  
To comprehensively evaluate the effectiveness of candidate prompts during the optimization process, this study combines four different evaluation metrics \( \mathcal{M} \), including accuracy, ECE, ROC, PR-P, and PR-N (all computed from logits outputs of scorer LLM). These multi-dimensional metrics are standardized to a unified scale, with accuracy, ROC, and PR-P ranging from 0 to 1, while the ECE and PR-N are transformed using \( 1 - v_m \) to ensure that higher values indicate better performance. Given the relatively equal importance of these metrics for the evaluation task, equal weights are assigned to each metric, with the weight \( w_m \) being the arithmetic mean. Here, \( v_m \) represents the value of the metric \( m \).


\subsection{Source Prompt Construction with Multi-Dimensional Metrics}

By refining the origin prompt on the source task $\bm{\mathcal{S}}$ and its related dataset $D_{\mathcal{S}}$, we construct a source prompt set $\mathcal{P}_{\text{source}}$. The optimization goal is to determine a set of prompts $\mathcal{P}$ that maximizes the generalization of source prompts:

\begin{equation}
\mathcal{P}_{\text{source}} = \arg\max_{\mathcal{P}} \sum_{P \in \mathcal{P}} \sum_{i=1}^\kappa \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P, D_{\mathcal{S}_i}).
\label{eq:source_opt_multi_metric}
\end{equation}

At each training step, the reference LLM generates 8 refined prompts based on the reference prompt. The scorer LLM then evaluates these prompts using multi-dimensional metrics integrated into the objective prompt evaluator. The highest-scoring prompts are selected for the next training step. The optimization process terminates when the reference LLM is unable to generate new prompts with higher scores, or when the maximum number of optimization steps has been reached. This results in the final source prompt set $\mathcal{P}_{\text{source}}$.

\subsection{Target Prompt Generation with Multi-Dimensional Metrics}

After establishing the source prompt set $\mathcal{P}_{\text{source}}$, we select a set of high-scoring prompts from $\mathcal{P}_{\text{source}}$ and fine-tune them on the corresponding target task dataset $D_{\mathcal{T}}$ and the appropriate target task $\bm{\mathcal{T}}$, thereby generating a target prompt set $\mathcal{P}_{\text{target}}$ that is more suitable for the target task.

The optimization objective for target prompt generation is defined as:

\begin{equation}
\mathcal{P}_{\text{target}} = \arg\max_{\mathcal{P}} \sum_{P \in \mathcal{P}} \sum_{k=1}^\tau \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P, D_{\mathcal{T}_k}).
\label{eq:target_opt_multi_metric}
\end{equation}

Starting from the highest-scoring source prompts selected from $\mathcal{P}_{\text{source}}$, the target prompt optimization process follows the same procedure as the source prompt optimization, resulting in the final target prompt set $\mathcal{P}_{\text{target}}$.





% \subsection{Source Prompt Construction with Multiple Metrics}

% By refining the origin prompt on the source task $\bm{\mathcal{S}}$ and its related dataset $D_{\mathcal{S}}$, we construct a source prompt set $\mathcal{P}_{\text{source}}$.
% The optimization goal is to determine a set of prompts $\mathcal{P}$ that maximizes the generalization of source prompts:

% \begin{equation}
% \mathcal{P}_{\text{source}} = \arg\max_{\mathcal{P}} \sum_{P \in \mathcal{P}} \sum_{i=1}^\kappa \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P, D_{\mathcal{S}_i}).
% \label{eq:source_opt_multi_metric}
% \end{equation}

% At each iteration \( t \), the scorer LLM computes the composite scores \( s^{(t)}_c \) for each candidate prompt using Equation~\eqref{eq:objective_evaluator_multi_metric}, with \( D = \bigcup_{i=1}^\kappa D_{\mathcal{S}_i} \).

% Candidate prompts that attain scores exceeding a predefined threshold \( \theta_{\text{source}} \) are selected and incorporated into the source prompt set for the subsequent iteration:

% \begin{equation}
% \mathcal{P}_{\text{source}}^{(t+1)} = \mathcal{P}_{\text{source}}^{(t)} \cup \{ P^{(t)}_c \mid s^{(t)}_c \geq \theta_{\text{source}} \}.
% \label{eq:source_update_multi_metric}
% \end{equation}

% \subsection{Target Prompt Generation with Multiple Metrics}

% After establishing the source prompt set $\mathcal{P}_{\text{source}}$, we select a set of high-scoring $\mathcal{P}_{\text{source}}$ and fine-tune it on the corresponding target task dataset $D_{\mathcal{T}}$ and the appropriate target task $\bm{\mathcal{T}}$, thereby generating a target prompt set $\mathcal{P}_{\text{target}}$ that is more suitable for the target task.

% The optimization objective for target prompt generation is defined as:

% \begin{equation}
% \mathcal{P}_{\text{target}} = \arg\max_{\mathcal{P}} \sum_{P \in \mathcal{P}} \sum_{k=1}^\tau \sum_{m \in \mathcal{M}} w_m \cdot \phi_m(P, D_{\mathcal{T}_k}).
% \label{eq:target_opt_multi_metric}
% \end{equation}

% Starting from the source prompt set $\mathcal{P}_{\text{source}}$, the reference LLM iteratively refines the prompts, considering multiple evaluation metrics. The scorer LLM assesses each candidate prompt using Equation~\eqref{eq:objective_evaluator_multi_metric}, with \( D = \bigcup_{k=1}^\tau D_{\mathcal{T}_k} \).

% Candidate prompts that achieve scores surpassing a predefined threshold \( \theta_{\text{target}} \) are selected and added to the target prompt set for the next iteration:

% \begin{equation}
% \mathcal{P}_{\text{target}}^{(t+1)} = \mathcal{P}_{\text{target}}^{(t)} \cup \{ P^{(t)}_c \mid s^{(t)}_c \geq \theta_{\text{target}} \}.
% \label{eq:target_update_multi_metric}
% \end{equation}


