\section{Models and Datasets}
\label{sec:appendix}

\subsection{Models}
\label{sec:appendix_models}
We selected a diverse set of models to evaluate the performance of both foundational and domain-specific LLMs. This selection enables us to understand the broad applicability of Transfer-Prompting and gauge its effectiveness across specialized fields. By comparing these models, we aim to showcase the potential and advantages of Transfer-Prompting comprehensively.

For \textbf{foundational models}, we employed
\textit{GPT-3.5-Turbo}, \textit{GPT-4}, \textit{LLaMA2-7B}, \textit{LLaMA2-13B}, \textit{LLaMA3-8B}, \textit{Vicuna-7B}, and \textit{Vicuna-13B} in our experiments. These models serve as baselines to understand the broader applicability of Transfer-Prompting across general-purpose LLMs.

For \textbf{domain-specific}, we evaluated models tailored to three critical domains: medicine, law, and finance. This allows us to investigate how domain-specific adaptations enhance model performance on sensitive data.

\textbf{Medicine}:
For the medical domain, we chose \textit{Chatdoctor-13B}, \textit{PMC-LLaMA-13B}, \textit{MedAlpaca-7B \& 13B}, and \textit{AdaptLLM-Medicine-LLM-7B \& 13B}. These models handle complex medical queries and generate accurate medical information, which is essential for real-world medical applications.

\textbf{Law}:
For the legal domain, we evaluated \textit{DISC-LawLLM}, \textit{LawGPT-7B}, \textit{Lawyer-LLaMA-13B}, \textit{ChatLaw-13B}, and \textit{AdaptLLM-Law-LLM-7B \& 13B}. These models interpret and generate legal text, which is crucial for legal research, document drafting, and case analysis.

\textbf{Finance}:
In the financial domain, we selected \textit{FinGPT-13B-v2 (LLaMA2-13B-based)}, \textit{CFGPT-7B-full}, \textit{Tongyi-Finance-14B-Chat}, \textit{AdaptLLM-Finance-LLM-7B \& 13B}, and \textit{FinMA-7B-full}. These models specialize in financial data interpretation and forecasting and are critical for market analysis, risk assessment, and financial planning. 

\subsection{Datasets}
\label{sec:appendix_datasets}
Our experiments comprehensively evaluate the models' performance on common-sense reasoning using three datasets and multiple question answering (MQA) on sensitive data using five distinct datasets. The common-sense reasoning datasets include LogiQA\footnote{\url{https://paperswithcode.com/dataset/logiqa}}, OpenbookQA\footnote{\url{https://paperswithcode.com/dataset/openbookqa}}, and CosmosQA\footnote{\url{https://paperswithcode.com/dataset/cosmosqa}}. For evaluation purposes, we selected the top 1,000 questions from the LogiQA and OpenbookQA test sets and the validation set of CosmosQA, respectively. For MQA on sensitive data, we evaluated the following datasets:

\textbf{MMLU\footnote{\url{https://paperswithcode.com/dataset/mmlu}}}: MMLU (Massive Multitask Language Understanding) is a benchmark designed to evaluate language models across 57 subjects with approximately 16,000 multiple-choice questions.
We selected specific datasets from MMLU to evaluate the performance of medical-related LLMs, namely \textit{medical genetics}, \textit{professional medicine}, and \textit{college medicine}. Additionally, we chose \textit{college law}, \textit{legal and moral basis}, and \textit{international law} to assess the performance of law-related LLMs.

\textbf{C-Eval\footnote{\url{https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese-1}}}:  This comprehensive Chinese evaluation suite has 13,948 multiple-choice questions across 52 disciplines and four difficulty levels.
We selected data from C-Eval to evaluate the performance of medical-related LLMs, specifically \textit{physician}, \textit{clinical medicine}, and \textit{basic medicine}. To evaluate the capabilities of law-focused LLMs, we chose datasets such as \textit{law}, \textit{legal and moral basis}, and \textit{international law}.

\textbf{CMMLU\footnote{\url{https://paperswithcode.com/paper/cmmlu-measuring-massive-multitask-language}}}: 
CMMLU is a benchmark with 11,582 multiple-choice questions across 67 subjects, designed to evaluate language models' knowledge and reasoning in a Chinese context.
We chose specific data from CMMLU to assess the performance of finance-related LLMs, including \textit{business ethics}, \textit{economics}, \textit{marketing}, and \textit{professional accounting}.

\textbf{MedMCQA\footnote{\url{https://paperswithcode.com/dataset/medmcqa}}}: This large-scale medical multiple-choice question-answering dataset includes over 194,000 questions designed to advance research in intelligent question-answering systems within the medical domain. We selected the first 1000 questions from the test split of MedMCQA for evaluation.

\textbf{AGIEval\footnote{\url{https://github.com/ruixiangcui/AGIEval}}}: AGIEval is a benchmark designed to evaluate foundation models in human cognition and problem-solving tasks, including law school admission tests and lawyer qualification exams. We use the law-related data to assess legal LLMs' understanding of judicial examination questions and case analyses, specifically utilizing the first 1000 questions from the \textit{jec-qa-kd} and \textit{jec-qa-ca} tasks.

\textbf{FinEval\footnote{\url{https://huggingface.co/datasets/FinGPT/fingpt-fineval}}}: FinEval is a compilation of high-quality multiple-choice and text-based quiz questions designed specifically for the Chinese financial sector. We select \textit{advanced financial accounting}, \textit{financial markets}, and \textit{corporate finance} for the evaluation of finance-related LLMs.


% Due to space limits of the main content, detailed descriptions of the datasets used and related information are provided in Appendix~\ref{sec:appendix_datasets}.
\input{Tables/table1.2}

\input{Tables/table5}
\section{Prompt Template for Source prompt and Transfer Prompt prompt}
As shown in Table \ref{table5}, the comparison includes two types of prompts: source prompts and transfer prompts. Source prompts provide general instructions for answering multiple-choice questions to enhance their generalization ability. In contrast, Transfer Prompt contains specific medical context and guidance to improve the overall quality of LLM's responses. For example, PaLM 2-L-IT's score using source prompts is 43\%, while the score increases to 56\% if the medical context is included in the Transfer Prompt. This comparison highlights the importance of tailoring prompts to the context of a specific domain to improve the performance of language models in specialized domains.

\begin{figure*}[!h]
\centering
\includegraphics[width=.329\textwidth]{Figures/17.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/18.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/19.pdf} 
\hspace{3em}

\includegraphics[width=.329\textwidth]{Figures/medmcqa.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/agieval.pdf} 
\hfill
\includegraphics[width=.329\textwidth]{Figures/fineval.pdf} 
\caption{Comparative performance evaluation of various models in the medical, legal, and financial domains. The confidence is calculated by the verbalized confidence method.}
\label{fig-sensitive2}
\end{figure*}

% \begin{figure*}[!h]
% \centering
% \includegraphics[width=.98\textwidth]{Figures/5.pdf}
% \hfill
% \caption{Performance comparison of various models across Law domain. The dashed line ORI represents results obtained from Origin Prompt, while the solid line Transfer-Prompting represents results obtained from Transfer-Prompting. Each model is represented by lines of the same colour across both prompting strategies, enabling a clear comparison of performance under different conditions.}
% \label{fig-sensitive2}
% \end{figure*}


\section{More Results}
\label{sec:more_results}

% \begin{figure*}[!h]
% \centering
% \includegraphics[width=.98\textwidth]{Figures/6.pdf}
% \hfill
% \caption{Performance comparison of various models across the Financial domain. The dashed line ORI represents results obtained from Origin Prompt, while the solid line Transfer-Prompting represents results obtained from Transfer-Prompting. Each model is represented by lines of the same colour across both prompting strategies, enabling a clear comparison of performance under different conditions.}
% \label{fig-sensitive3}
% \end{figure*}

\begin{figure*}[!h]
\centering
\includegraphics[width=.98\textwidth]{Figures/logits-five.pdf}
\hfill
\caption{The five-shot performance of different medical domain LLMs on MMLU medical-related tasks is evaluated using logits.}
\label{fig: logits-five}
\end{figure*}

\input{Tables/table6}
\input{Tables/table7}


\subsection{Evaluation of Common-sense Reasoning Capabilities.}
In Table \ref{tab:2-comon-sense}, we compare the five-shot learning performance of various models on commonsense reasoning datasets. 
The results show that Transfer-Prompting significantly enhances model performance, particularly for GPT-4, which shows remarkable improvements across key metrics like IFR, ACC, and ECE. Other models, such as LLaMA3-8B and Vicuna-13B, also benefit notably, demonstrating the effectiveness of Transfer-Prompting in improving score, confidence calibration, and generalization across different common-sense reasoning tasks. These results underscore Transfer-Prompting's robustness and potential to elevate various LLMs' capabilities in complex reasoning scenarios.

% The data shows that the Transfer-Prompting consistently outperforms the ORI, especially in tasks requiring higher-level reasoning. GPT-4's accuracy on the LogiQA and CosmosQA datasets increased to 0.45 and 0.52 using Transfer-Prompting, underscoring the value of specialized prompting strategies.


% Table \ref{table5} compares language models on medical-related datasets (MMLU) using source prompts versus Transfer Prompt. The results clearly demonstrate that incorporating domain-specific prompts significantly improves model accuracy. For instance, the accuracy of text-bison increased from 43\% to 56\% when the prompt was tailored to reflect clinical knowledge. This highlights the importance of prompt customization in enhancing model performance in specialized domains.

\subsection{Performance Analysis on Sensitive Domains.}
In the \textbf{Legal field}, as shown in Figure \ref{fig-sensitive2}, the application of Transfer-Prompting comprehensively improves model performance. Taking LawGPT-7B as an example, after applying Transfer-Prompting, the IFR increases from 0.65 to 0.78 and the ECE decreases from 0.32 to 0.21, demonstrating the improvement in inference quality and model calibration. Similarly, the IFR of Law-LLM-13B improves from 0.72 to 0.83, and the ACC improves from 0.48 to 0.57. These results demonstrate that Transfer-Prompting methods have great potential in applications requiring high accuracy and confidence, such as legal contexts.

Similarly, in the \textbf{financial field}, as shown in Figure \ref{fig-sensitive2}, the Transfer-Prompting method also brings significant performance improvements. For example, the IFR of the Finance-LLM-13B model improves from 0.69 to 0.81, and the ACC increases from 0.49 to 0.58. At the same time, the ECE decreased for all models. These improvements prove that the Transfer-Prompting method is crucial in the financial field.

In summary, the results in the legal and financial domains are generally consistent with the analysis in the medical task, which further demonstrates the generalizability and effectiveness of Transfer-Prompting in improving LLM performance in sensitive professional domains.
% Figure \ref{fig-sensitive2} illustrates the performance of different models in the law domain. Transfer-Prompting often leads to better outcomes across various metrics than Origin Prompts, particularly in IFR and PR-P. This indicates that domain-specific prompts are crucial for optimizing model performance in complex fields like law.
% Similarly, Figure \ref{fig-sensitive3} compares model performance in the finance domain. The results reveal that Transfer-Prompting generally offers superior performance over Origin Prompts across key metrics such as ACC and PR-N. This further emphasizes the need for tailored prompting strategies to achieve better results in financial tasks.

\subsection{Analysis of Logits.}
In this paper, we utilize the LLaMA-Factory\footnote{\url{https://github.com/hiyouga/LLaMA-Factory}} to evaluate logits and analyze the effectiveness of Transfer-Prompting in enhancing LLM performance.
As shown in Figure \ref{fig: logits-five}, the 5-shot results reinforce the effectiveness of Transfer-Prompting, with consistent improvements across various models and metrics, similar to the 0-shot findings. Transfer-Prompting significantly boosts ACC, reduces ECE and PR-N, and enhances ROC and PR-P values, particularly in complex models like Med-Alpaca-13B and Medicine-LLM-13B. These results underscore Transfer-Prompting's reliability and applicability, making it a valuable technique for improving LLM performance across diverse and critical domains.


\begin{figure*}[!t]
\centering
% \vspace{1.5p/t}
\includegraphics[width=.8\textwidth]{Figures/2_2.pdf}
\hfill
\caption{An example of the reference prompt for reference LLM (GPT-3.5-Turbo and GPT-4) on the medically relevant datasets. The generated instruction is inserted at the position marked by <INS> in the input. The \textcolor{lightblackgreen}{green} text displays instructions for prompts and scores; the \textcolor{lightblackorange}{orange} text provides examples of how to apply the instruction; the \textcolor{lightblackblue}{blue} text contains the prompts and scores pairs.
}
\label{fig: transfer-prompting-gpt}
\end{figure*}

% \input{Figures/2}
\section{Reference-Prompt Template for GPT-3.5-Turbo and GPT-4}
\label{sec: reference-prompt-gpt4}
Figure \ref{fig: transfer-prompting-gpt} illustrates optimizing a  Reference-Prompt Template for GPT-3.5-Turbo and GPT-4  in the context of medical multiple-choice questions. It provides examples of instructions with their corresponding scores, and the task is to create a new instruction that performs better. The figure also demonstrates how to insert this new instruction into a prompt and evaluate its effectiveness.


