\section{Related Work}
\label{sec:2}
% \textbf{Evaluation of the instruction following and overall output quality of LLMs.} LLMs have shown impressive capabilities, but their predictions often come with varying degrees of uncertainty, making calibration essential for reliable outputs. To address this, various methods have been proposed. **Gupta, "Calibration is Not Enough"** introduce a recalibration technique that adjusts confidence scores to better match empirical accuracy without altering the model's architecture or training data. Building on this, **Wu et al., "Mixup Training for Calibration"** propose mixup training, which enhances calibration by using convex combinations of inputs and labels during training. **Kumar et al., "Calibration Analysis and Metrics"** provide a comprehensive analysis of calibration errors and introduce metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE), facilitating precise comparisons between models. For LLMs, **Zhang, "Temperature Scaling for LLMs"** apply temperature scaling, tailored specifically for the complexity of language models. More recent advances include an ensemble approach by **Liu et al., "Calibrated Consensus with Ensembles"**, combining multiple models to achieve a calibrated consensus. Additionally, **Srivastava et al., "Confidence-based Calibration"** propose directly querying LLMs to assess their confidence in responses, while **Kim et al., "LLM Calibration Evaluation"** evaluate LLM calibration using metrics like ECE, AUROC, and AUPRC. Finally, **Rao et al., "Coherence Sampling for LLMs"** introduce coherence sampling, a novel approach contributing to the ongoing refinement of LLM calibration.

% \textbf{Prompt Engineering and Optimization.} Recent advances in prompt engineering have significantly enhanced the efficiency and efficacy of interactions with large language models (LLMs). Key developments include few-shot and zero-shot learning, which utilize minimal examples to guide models, thereby reducing reliance on extensive labeled datasets. In-context learning, introduced by **Brown et al., "In-context Learning for LLMs"**, enables models to adapt to tasks using input prompts alone, without requiring parameter updates. Automated prompt generation methods, such as those explored by **Hwang et al., "Automated Prompt Generation"**, employ algorithms like reinforcement learning to discover optimal prompts, thus enhancing model performance. Furthermore, **Goyal et al., "Prompt-Pretraining for Generalization"** demonstrated that pre-training models with specific prompt formats can improve generalization across diverse tasks.

% In terms of prompt optimization, recent studies have highlighted the effectiveness of LLMs as prompt optimizers. For instance, **Tan et al., "LLM-based Prompt Optimization"** showed that LLMs can effectively refine prompts, boosting performance across various tasks. Similarly, **Saxena et al., "Optimized Prompts with OPRO"** reported that optimized prompts generated by LLMs can outperform manually crafted ones through the Optimization by PROmpting (OPRO) method. Additionally, **Patel et al., "Generalized Prompt Optimization"** identified vulnerabilities in prompt optimization techniques due to distribution shifts, such as subpopulation shifts, and proposed Generalized Prompt Optimization (GPO) to enhance the generalization capabilities of LLMs.