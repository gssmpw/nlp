% \begin{algorithm}
% \caption{Algorithm for the Prompt-to-Improve Process}\label{alg:prompt_to_improve}

% \begin{algorithmic}[1]
% \Require $D_{\text{source}}$: Source Task Dataset
% \Require $D_{\text{target}}$: Target Task Dataset
% \Ensure Evaluation metrics and insights on prompt effectiveness for task transfer

% \State Generate prompts from $D_{\text{source}}$ to enhance generalization
% \Comment{Step 1: Source prompt construction}

% \State Output intermediate prompts for use in the target task

% \State Use $D_{\text{target}}$ with constructed prompts from Step 1 to train or evaluate on target tasks
% \Comment{Step 2: Use Target Task Dataset}

% \State Apply the Prompt-tO-Improve technique to refine prompts further
% \newline
% \Comment{Step 3: Prompt-tO-Improve}

% \State \textbf{Objective:} Improve the overall quality of LLM responses

% \State Evaluate the final prompts and model responses

% \State Assess the quality improvements achieved in response generation 
% \Comment{Step 4: Prompt Evaluation}

% \end{algorithmic}
% \end{algorithm}


% \begin{small}  % 缩小整个算法展示
% \begin{algorithm}
%     \caption{Transfer-Prompting Process}
%     \label{alg}
%     \begin{algorithmic}[1]
%         \Require $D_{\text{source}}$: Source Task Dataset
%         \Require $D_{\text{target}}$: Target Task Dataset
%         \Ensure Evaluation metrics and insights on prompt effectiveness for task transfer
        
%         \State \textbf{Source Prompt Construction}
%         \State $P_{\text{source}} \gets \textsc{GeneratePrompts}(D_{\text{source}})$ 
%         \Comment{Generate prompts to enhance generalization}
        
%         \State \textbf{Target Task Training/Evaluation}
%         \State $M_{\text{target}} \gets \textsc{TrainOrEvaluate}(D_{\text{target}}, P_{\text{source}})$ 
%         \Comment{Use prompts on target tasks}
        
%         \State \textbf{Prompt Refinement with \textbf{POI}}
%         \State $P_{\text{refined}} \gets \textsc{PromptToImprove}(M_{\text{target}}, P_{\text{source}})$ 
%         \Comment{Refine prompts further}
        
%         \State \textbf{Objective}: Improve the overall quality of LLM responses
        
%         \State \textbf{Prompt Evaluation}
%         \State $\text{metrics} \gets \textsc{Evaluate}(P_{\text{refined}}, M_{\text{target}})$ 
%         \Comment{Evaluate final prompts and responses}
        
%         \State $\textsc{AssessQualityImprovements}(\text{metrics})$ 
%         \Comment{Assess improvements in response generation}
%     \end{algorithmic}
% \end{algorithm}
% \end{small}
% \onecolumn  % 切换到单栏模式
\begin{algorithm}[h!]

\caption{Dual-Stage Optimization for Transfer-Prompting}
\label{alg}
\KwIn{Source dataset $D_{source}$, Target dataset $D_{target}$, Reference LLM, Scoring LLM}
\KwOut{Optimized Target Prompt}

\textbf{Stage 1: Source Prompt Construction}\;
\ForEach{round $r$ in $R$}{
    Generate prompts $P_{source}^{r}$ using Reference LLM on $D_{source}$\;
    Evaluate $P_{source}^{r}$ using Scoring LLM and obtain score $S_{source}^{r}$\;
    \If{$S_{source}^{r} \geq \text{threshold}$}{
        Store $P_{source}^{r}$ for Target Stage\;
    }
}

\textbf{Stage 2: Target Prompt Generation}\;
\ForEach{round $r$ in $R$}{
    Optimize $P_{target}^{r}$ using stored $P_{source}$ on $D_{target}$\;
    Evaluate $P_{target}^{r}$ with Scoring LLM and obtain score $S_{target}^{r}$\;
    Update $P_{target}^{r+1}$ using feedback\;
    \If{Optimal score achieved}{
        Break\;
    }
}

\Return Optimized Target Prompt $P_{target}^{*}$\;

\end{algorithm}




% \begin{algorithm}
% \caption{Prompt-to-Improve Process (POI)}\label{alg:prompt_to_improve}
% \begin{algorithmic}[1]
%     \Require $D_{\text{source}}$: Source Task Dataset
%     \Require $D_{\text{target}}$: Target Task Dataset
%     \Ensure Refined prompts and evaluation metrics

%     \State \textbf{Step 1: Source Prompt Construction}
%     \State $P_{\text{source}} \gets \textsc{GeneratePrompts}(D_{\text{source}})$
%     \Comment{Generate initial prompts from source task data}
    
%     \State \textbf{Step 2: Initialize Reference LLM}
%     \State $R_{\text{LLM}} \gets \textsc{InitializeReferenceLLM}()$
%     \Comment{Initialize reference LLM for optimization}

%     \State \textbf{Step 3: Iterative Prompt Optimization}
%     \For{each iteration $i = 1$ to max\_iterations}
%         \State $S_{\text{refined}} \gets \textsc{EvaluatePrompts}(P_{\text{source}}, D_{\text{target}}, R_{\text{LLM}})$
%         \Comment{Evaluate source prompts on target task}
        
%         \State $P_{\text{candidate}} \gets \textsc{GenerateNewPrompts}(S_{\text{refined}}, P_{\text{source}}, R_{\text{LLM}})$
%         \Comment{Generate new prompts based on evaluations}
        
%         \If{$S_{\text{refined}}$ does not improve}
%             \State \textbf{break}
%         \EndIf
%         \State $P_{\text{source}} \gets P_{\text{candidate}}$
%         \Comment{Update source prompts for the next iteration}
%     \EndFor

%     \State \textbf{Step 4: Final Evaluation}
%     \State $\text{metrics} \gets \textsc{EvaluateFinalPrompts}(P_{\text{source}}, D_{\text{target}})$
%     \Comment{Evaluate the final refined prompts on the target task}

%     \State \Return $P_{\text{source}}, \text{metrics}$
% \end{algorithmic}
% \end{algorithm}
