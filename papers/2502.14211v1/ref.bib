% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{kuleshov2018accurate,
  title={Accurate uncertainties for deep learning using calibrated regression},
  author={Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2796--2804},
  year={2018},
  organization={PMLR}
}

@inproceedings{ruder2019transfer,
  title={Transfer learning in natural language processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials},
  pages={15--18},
  year={2019}
}
@article{kulkarni2024crafting,
  title={Crafting Effective Prompts: Enhancing AI Performance through Structured Input Design},
  author={Kulkarni, Nilesh D and Tupsakhare, Preeti},
  journal={JOURNAL OF RECENT TRENDS IN COMPUTER SCIENCE AND ENGINEERING (JRTCSE)},
  volume={12},
  number={5},
  pages={1--10},
  year={2024}
}
@article{
lin2022teaching,
title={Teaching Models to Express Their Uncertainty in Words},
author={Stephanie Lin and Jacob Hilton and Owain Evans},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=8s8K2UZGTZ},
note={}
}
@article{chang2024ba,
  title={BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models},
  author={Chang, Yupeng and Chang, Yi and Wu, Yuan},
  journal={arXiv preprint arXiv:2408.04556},
  year={2024}
}
@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}
@article{cao2022ai,
  title={Ai in finance: challenges, techniques, and opportunities},
  author={Cao, Longbing},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={3},
  pages={1--38},
  year={2022},
  publisher={ACM New York, NY}
}
@article{shah2023creation,
  title={Creation and adoption of large language models in medicine},
  author={Shah, Nigam H and Entwistle, David and Pfeffer, Michael A},
  journal={Jama},
  volume={330},
  number={9},
  pages={866--869},
  year={2023},
  publisher={American Medical Association}
}
@misc{yang2024largelanguagemodelsoptimizers,
      title={Large Language Models as Optimizers}, 
      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
      year={2024},
      eprint={2309.03409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.03409}, 
}

@article{he2024foundation,
  title={Foundation model for advancing healthcare: Challenges, opportunities, and future directions},
  author={He, Yuting and Huang, Fuxiang and Jiang, Xinrui and Nie, Yuxiang and Wang, Minghao and Wang, Jiguang and Chen, Hao},
  journal={arXiv preprint arXiv:2404.03264},
  year={2024}
}
@article{wang2023promptagent,
  title={Promptagent: Strategic planning with language models enables expert-level prompt optimization},
  author={Wang, Xinyuan and Li, Chenxi and Wang, Zhen and Bai, Fan and Luo, Haotian and Zhang, Jiayou and Jojic, Nebojsa and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2310.16427},
  year={2023}
}
@inproceedings{li2023large,
  title={Large language models in finance: A survey},
  author={Li, Yinheng and Wang, Shaofei and Ding, Han and Chen, Hang},
  booktitle={Proceedings of the fourth ACM international conference on AI in finance},
  pages={374--382},
  year={2023}
}
@article{dahl2024large,
  title={Large legal fictions: Profiling legal hallucinations in large language models},
  author={Dahl, Matthew and Magesh, Varun and Suzgun, Mirac and Ho, Daniel E},
  journal={Journal of Legal Analysis},
  volume={16},
  number={1},
  pages={64--93},
  year={2024},
  publisher={Oxford University Press UK}
}

@article{zhou2023survey,
  title={A survey of large language models in medicine: Progress, application, and challenge},
  author={Zhou, Hongjian and Liu, Fenglin and Gu, Boyang and Zou, Xinyu and Huang, Jinfa and Wu, Jinge and Li, Yiru and Chen, Sam S and Zhou, Peilin and Liu, Junling and others},
  journal={arXiv preprint arXiv:2311.05112},
  year={2023}
}

@article{chen2024multi,
  title={Multi-task learning in natural language processing: An overview},
  author={Chen, Shijie and Zhang, Yu and Yang, Qiang},
  journal={ACM Computing Surveys},
  volume={56},
  number={12},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{
zhou2023large,
title={Large Language Models are Human-Level Prompt Engineers},
author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=92gvk82DE-}
}

@article{sun2023autohint,
  title={AutoHint: Automatic Prompt Optimization with Hint Generation},
  author={Sun, Hong and Li, Xue and Xu, Yinchuan and Homma, Youkow and Cao, Qi and Wu, Min and Jiao, Jian and Charles, Denis},
  journal={arXiv preprint arXiv:2307.07415},
  year={2023}
}
@article{bennett2023chatgpt,
  title={Is ChatGPT Any Good at Legal Research--and Should We be Wary or Supportive of it?},
  author={Bennett, Greg},
  journal={Legal Information Management},
  volume={23},
  number={4},
  pages={219--224},
  year={2023},
  publisher={Cambridge University Press}
}
@article{zhao2024revolutionizing,
  title={Revolutionizing finance with llms: An overview of applications and insights},
  author={Zhao, Huaqin and Liu, Zhengliang and Wu, Zihao and Li, Yiwei and Yang, Tianze and Shu, Peng and Xu, Shaochen and Dai, Haixing and Zhao, Lin and Mai, Gengchen and others},
  journal={arXiv preprint arXiv:2401.11641},
  year={2024}
}
@article{nori2023capabilities,
  title={Capabilities of gpt-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}
@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{hadi2024large,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Al Tashi, Qasem and Shah, Abbas and Qureshi, Rizwan and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and others},
  journal={Authorea Preprints},
  year={2024},
  publisher={Authorea}
}
@article{surden2019artificial,
  title={Artificial intelligence and law: An overview},
  author={Surden, Harry},
  journal={Georgia State University Law Review},
  volume={35},
  number={4},
  year={2019}
}
@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

@article{pryzant2023automatic,
  title={Automatic prompt optimization with" gradient descent" and beam search},
  author={Pryzant, Reid and Iter, Dan and Li, Jerry and Lee, Yin Tat and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2305.03495},
  year={2023}
}
@inproceedings{kang2023deficiency,
  title={Deficiency of large language models in finance: An empirical examination of hallucination},
  author={Kang, Haoqiang and Liu, Xiao-Yang},
  booktitle={I Can't Believe It's Not Better Workshop: Failure Modes in the Age of Foundation Models},
  year={2023}
}
@misc{magesh2024hallucinationfreeassessingreliabilityleading,
      title={Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools}, 
      author={Varun Magesh and Faiz Surani and Matthew Dahl and Mirac Suzgun and Christopher D. Manning and Daniel E. Ho},
      year={2024},
      eprint={2405.20362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20362}, 
}
@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}
@misc{ahmad2023creatingtrustworthyllmsdealing,
      title={Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI}, 
      author={Muhammad Aurangzeb Ahmad and Ilker Yaramis and Taposh Dutta Roy},
      year={2023},
      eprint={2311.01463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.01463}, 
}
@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}
@article{morley2020ethics,
  title={The ethics of AI in health care: a mapping review},
  author={Morley, Jessica and Machado, Caio CV and Burr, Christopher and Cowls, Josh and Joshi, Indra and Taddeo, Mariarosaria and Floridi, Luciano},
  journal={Social Science \& Medicine},
  volume={260},
  pages={113172},
  year={2020},
  publisher={Elsevier}
}
@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}
@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}
@article{moor2023foundation,
  title={Foundation models for generalist medical artificial intelligence},
  author={Moor, Michael and Banerjee, Oishi and Abad, Zahra Shakeri Hossein and Krumholz, Harlan M and Leskovec, Jure and Topol, Eric J and Rajpurkar, Pranav},
  journal={Nature},
  volume={616},
  number={7956},
  pages={259--265},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@misc{huang2023lawyer,
      title={Lawyer LLaMA Technical Report}, 
      author={Quzhe Huang and Mingxu Tao and Chen Zhang and Zhenwei An and Cong Jiang and Zhibin Chen and Zirui Wu and Yansong Feng},
      year={2023},
      eprint={2305.15062},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@misc{zhou2024lawgpt,
      title={LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model}, 
      author={Zhi Zhou and Jiang-Xin Shi and Peng-Xiao Song and Xiao-Wen Yang and Yi-Xuan Jin and Lan-Zhe Guo and Yu-Feng Li},
      year={2024},
      eprint={2406.04614},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}
@article{llama3modelcard,

title={Llama 3 Model Card},

author={AI@Meta},

year={2024},

url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}

}
@misc{cui2024chatlaw,
      title={Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model}, 
      author={Jiaxi Cui and Munan Ning and Zongjian Li and Bohua Chen and Yang Yan and Hao Li and Bin Ling and Yonghong Tian and Li Yuan},
      year={2024},
      eprint={2306.16092},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Tongyi-Finance-14B-Chat,
  author = {JXY},
  title = {Tongyi-Finance-14B-Chat},
  year = {2024},
  url = {https://huggingface.co/jxy/Tongyi-Finance-14B-Chat},
  note = {Accessed: 2024-06-15}
}

@article{huang2019cosmos,
  title={Cosmos QA: Machine reading comprehension with contextual commonsense reasoning},
  author={Huang, Lifu and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:1909.00277},
  year={2019}
}

@inproceedings{Mihaylov2018CanAS,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:52183757}
}
@article{liu2020logiqa,
  title={Logiqa: A challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  journal={arXiv preprint arXiv:2007.08124},
  year={2020}
}

@InProceedings{pmlr-v174-pal22a,
  title = 	 {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author =       {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
  pages = 	 {248--260},
  year = 	 {2022},
  editor = 	 {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
  volume = 	 {174},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07--08 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
  url = 	 {https://proceedings.mlr.press/v174/pal22a.html},
  abstract = 	 {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS &amp; NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects &amp; topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}
@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  year={2023}
}
@misc{xie2023pixiu,
 title={PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance}, 
 author={Qianqian Xie and Weiguang Han and Xiao Zhang and Yanzhao Lai and Min Peng and Alejandro Lopez-Lira and Jimin Huang},
 year={2023},
 eprint={2306.05443},
 archivePrefix={arXiv},
 primaryClass={cs.CL}
}
@article{vu2021spot,
  title={Spot: Better frozen model adaptation through soft prompt transfer},
  author={Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  journal={arXiv preprint arXiv:2110.07904},
  year={2021}
}
@article{zhao2023automatic,
  title={Automatic calibration and error correction for large language models via pareto optimal self-supervision},
  author={Zhao, Theodore and Wei, Mu and Preston, J Samuel and Poon, Hoifung},
  journal={arXiv preprint arXiv:2306.16564},
  year={2023}
}
@article{zhou2023batch,
  title={Batch calibration: Rethinking calibration for in-context learning and prompt engineering},
  author={Zhou, Han and Wan, Xingchen and Proleev, Lev and Mincu, Diana and Chen, Jilin and Heller, Katherine and Roy, Subhrajit},
  journal={arXiv preprint arXiv:2309.17249},
  year={2023}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zhang2023study,
  title={A Study on the Calibration of In-context Learning},
  author={Zhang, Hanlin and Zhang, Yi-Fan and Yu, Yaodong and Madeka, Dhruv and Foster, Dean and Xing, Eric and Lakkaraju, Hima and Kakade, Sham},
  journal={arXiv preprint arXiv:2312.04021},
  year={2023}
}
@misc{chen2023discfinllm,
      title={DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning}, 
      author={Wei Chen and Qiushi Wang and Zefei Long and Xianyin Zhang and Zhongtian Lu and Bingxuan Li and Siyuan Wang and Jiarong Xu and Xiang Bai and Xuanjing Huang and Zhongyu Wei},
      year={2023},
      eprint={2310.15205},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yang2023fingpt,
      title={FinGPT: Open-Source Financial Large Language Models}, 
      author={Hongyang Yang and Xiao-Yang Liu and Christina Dan Wang},
      year={2023},
      eprint={2306.06031},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}
@misc{yue2023disclawllm,
      title={DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services}, 
      author={Shengbin Yue and Wei Chen and Siyuan Wang and Bingxuan Li and Chenchen Shen and Shujun Liu and Yuxuan Zhou and Yao Xiao and Song Yun and Xuanjing Huang and Zhongyu Wei},
      year={2023},
      eprint={2309.11325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{cheng2023adapting,
  title={Adapting large language models via reading comprehension},
  author={Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2309.09530},
  year={2023}
}

@phdthesis{miao2023precision,
  title={Precision-Recall Curve (PRC) Classification Trees (\& Related Methods)},
  author={Miao, Jiaju},
  year={2023},
  school={State University of New York at Stony Brook}
}
@article{han2023medalpaca,
  title={MedAlpaca--An Open-Source Collection of Medical Conversational AI Models and Training Data},
  author={Han, Tianyu and Adams, Lisa C and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and L{\"o}ser, Alexander and Truhn, Daniel and Bressem, Keno K},
  journal={arXiv preprint arXiv:2304.08247},
  year={2023}
}
@misc{wu2023pmcllama,
      title={PMC-LLaMA: Towards Building Open-source Language Models for Medicine}, 
      author={Chaoyi Wu and Weixiong Lin and Xiaoman Zhang and Ya Zhang and Yanfeng Wang and Weidi Xie},
      year={2023},
      eprint={2304.14454},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023chatdoctor,
      title={ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge}, 
      author={Yunxiang Li and Zihan Li and Kai Zhang and Ruilong Dan and Steve Jiang and You Zhang},
      year={2023},
      eprint={2303.14070},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{10.1007/978-3-642-40994-3_29,
author="Boyd, Kendrick
and Eng, Kevin H.
and Page, C. David",
editor="Blockeel, Hendrik
and Kersting, Kristian
and Nijssen, Siegfried
and {\v{Z}}elezn{\'y}, Filip",
title="Area under the Precision-Recall Curve: Point Estimates and Confidence Intervals",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="451--466",
abstract="The area under the precision-recall curve (AUCPR) is a single number summary of the information in the precision-recall (PR) curve. Similar to the receiver operating characteristic curve, the PR curve has its own unique properties that make estimating its enclosed area challenging. Besides a point estimate of the area, an interval estimate is often required to express magnitude and uncertainty. In this paper we perform a computational analysis of common AUCPR estimators and their confidence intervals. We find both satisfactory estimates and invalid procedures and we recommend two simple intervals that are robust to a variety of assumptions.",
isbn="978-3-642-40994-3"
}


@article{zhang2023fineval,
  title={Fineval: A chinese financial domain knowledge evaluation benchmark for large language models},
  author={Zhang, Liwen and Cai, Weige and Liu, Zhaowei and Yang, Zhi and Dai, Wei and Liao, Yujie and Qin, Qianru and Li, Yifei and Liu, Xingyu and Liu, Zhiqiang and others},
  journal={arXiv preprint arXiv:2308.09975},
  year={2023}
}
@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}
@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}
@misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@article{yang2023improving,
  title={Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning},
  author={Yang, Yuchen and Li, Houqiang and Wang, Yanfeng and Wang, Yu},
  journal={arXiv preprint arXiv:2310.04782},
  year={2023}
}
@misc{chatgpt,
  year = {2023},
  author = {OpenAI},
  howpublished = {\url{https://chat.openai.com.chat}},
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{dai2023uncovering,
  title={Uncovering ChatGPT's Capabilities in Recommender Systems},
  author={Dai, Sunhao and Shao, Ninglu and Zhao, Haiyuan and Yu, Weijie and Si, Zihua and Xu, Chen and Sun, Zhongxiang and Zhang, Xiao and Xu, Jun},
  journal={arXiv preprint arXiv:2305.02182},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{chang2023survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2023},
  publisher={ACM New York, NY}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@misc{lyu2024calibrating,
      title={Calibrating Large Language Models with Sample Consistency}, 
      author={Qing Lyu and Kumar Shridhar and Chaitanya Malaviya and Li Zhang and Yanai Elazar and Niket Tandon and Marianna Apidianaki and Mrinmaya Sachan and Chris Callison-Burch},
      year={2024},
      eprint={2402.13904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{xiong2023can,
  title={Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  journal={arXiv preprint arXiv:2306.13063},
  year={2023}
}
@article{he2023investigating,
  title={Investigating uncertainty calibration of aligned language models under the multiple-choice setting},
  author={He, Guande and Cui, Peng and Chen, Jianfei and Hu, Wenbo and Zhu, Jun},
  journal={arXiv preprint arXiv:2310.11732},
  year={2023}
}
@article{tian2023just,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}
@techreport{oroy2024cross,
  title={Cross-Lingual NLP: Transfer Learning and Multilingual Models for Low-Resource Languages},
  author={Oroy, Kurez and Danny, Jhon},
  year={2024},
  institution={EasyChair}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{chu2018survey,
  title={A survey of domain adaptation for neural machine translation},
  author={Chu, Chenhui and Wang, Rui},
  journal={arXiv preprint arXiv:1806.00258},
  year={2018}
}
@inproceedings{cao2018adversarial,
  title={Adversarial transfer learning for Chinese named entity recognition with self-attention mechanism},
  author={Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun and Liu, Shengping},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing},
  pages={182--192},
  year={2018}
}
@article{do2005transfer,
  title={Transfer learning for text classification},
  author={Do, Chuong B and Ng, Andrew Y},
  journal={Advances in neural information processing systems},
  volume={18},
  year={2005}
}
@inproceedings{li2023robust,
  title={Robust prompt optimization for large language models against distribution shifts},
  author={Li, Moxin and Wang, Wenjie and Feng, Fuli and Cao, Yixin and Zhang, Jizhi and Chua, Tat-Seng},
  year={2023},
  organization={Association for Computational Linguistics}
}

@article{ma2024large,
  title={Are Large Language Models Good Prompt Optimizers?},
  author={Ma, Ruotian and Wang, Xiaolei and Zhou, Xin and Li, Jian and Du, Nan and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.02101},
  year={2024}
}


@article{han2022prompt,
  title={Prompt tuning with rules for text classification., 2022, 3},
  author={Han, X and Zhao, W and Ding, N and Liu, Z and Ptr, M Sun},
  journal={DOI: https://doi. org/10.1016/j. aiopen},
  volume={3},
  pages={182--192},
  year={2022}
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
@article{desai2020calibration,
  title={Calibration of pre-trained transformers},
  author={Desai, Shrey and Durrett, Greg},
  journal={arXiv preprint arXiv:2003.07892},
  year={2020}
}
@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}
@article{zhu2023promptbench,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{liu2023calibrating,
  title={Calibrating llm-based evaluator},
  author={Liu, Yuxuan and Yang, Tianchi and Huang, Shaohan and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
  journal={arXiv preprint arXiv:2309.13308},
  year={2023}
}

@article{zeng2023evaluating,
  title={Evaluating large language models at evaluating instruction following},
  author={Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.07641},
  year={2023}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}
