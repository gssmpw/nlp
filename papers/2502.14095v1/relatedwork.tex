\section{Related Work}
% \paragraph{Interpretability in large language models.}


Recent interpretability work has tended to focus on abstractive approaches such as posthoc free-text explanations \cite{singh_rethinking_2024, zhu_explanation_2024}, or explanations as a byproduct of explicit reasoning processes like Chain-of-Thought \cite{wei_chain_2022, lanham_measuring_2023}. However, one of the major goals of interpretability is verification \cite{fok_search_2023}, the implicit assumption being that it is easier to recognize an erroneous explanation than an erroneous label. This mode requires explanations to be faithful \cite{jacovi_towards_2020} to the overall prediction, but this quality is difficult to measure in abstractive approaches \cite{agarwal_faithfulness_2024, siegel_probabilities_2024}.

Extractive approaches are less problematic in this regard because they directly attribute the prediction to evidence within the input. Even if this evidence is not technically faithful to the model's prediction, an observer can still assess whether it truly supports the label without being potentially beguiled by misleading abstractive generations of the model. Traditional approaches to identifying extractive evidence, such as the rationale model architecture \cite{lei_rationalizing_2016} or the LIME perturbation method \cite{ribeiro_why_2016}, add impractical levels of computational overhead to already large models, so recent work has investigated whether LMs can be prompted to produce such attributions as a generative output \cite{huang_can_2023,hu_think_2023, majumder_knowledge-grounded_2022}. An especially relevant recent work is \cite{madsen_are_2024}, which investigates the internal faithfulness of several types of extractive explanations produced by three open LMs. Our approach follows this work in directly prompting LMs to produce extractive evidence for their predictions. To assess whether LMs can identify the ``correct'' evidence, we use datasets with gold-standard evidence annotations. The ERASER collection \cite{deyoung_eraser_2019} includes a number of these datasets, and \cite{wiegreffe_teach_2021} surveys yet more. 

Finally, our goal of retrieving relevant evidence buried in potentially long documents is similar to that of``needle-in-the-haystack'' evaluations \cite{dhinakaran_needle_2024}, which ask questions about evidence manually inserted into a long context. Where we differ is in applying this approach to naturally-occurring evidence that the model may not be able to properly interpret, rather than artificially-inserted evidence the model is assumed to be able to comprehend if it can find it.