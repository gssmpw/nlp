
@misc{singhania_recall_2024,
	title = {Recall {Them} {All}: {Retrieval}-{Augmented} {Language} {Models} for {Long} {Object} {List} {Extraction} from {Long} {Documents}},
	shorttitle = {Recall {Them} {All}},
	url = {http://arxiv.org/abs/2405.02732},
	abstract = {Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be spread across many passages in long texts. This poses the challenge of extracting long lists from long texts. We present the L3X method which tackles the problem in two stages: (1) recall-oriented generation using a large language model (LLM) with judicious techniques for retrieval augmentation, and (2) precision-oriented scrutinization to validate or prune candidates. Our L3X method outperforms LLM-only generations by a substantial margin.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Singhania, Sneha and Razniewski, Simon and Weikum, Gerhard},
	month = may,
	year = {2024},
	note = {arXiv:2405.02732 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{lanham_measuring_2023,
	title = {Measuring {Faithfulness} in {Chain}-of-{Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2307.13702},
	doi = {10.48550/arXiv.2307.13702},
	abstract = {Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Lukošiūtė, Kamilė and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and Telleen-Lawton, Timothy and Hume, Tristan and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
	month = jul,
	year = {2023},
	note = {arXiv:2307.13702 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{madsen_are_2024,
	title = {Are self-explanations from {Large} {Language} {Models} faithful?},
	url = {http://arxiv.org/abs/2401.07927},
	abstract = {Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Madsen, Andreas and Chandar, Sarath and Reddy, Siva},
	month = may,
	year = {2024},
	note = {arXiv:2401.07927},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{vaccaro_when_2024,
	title = {When combinations of humans and {AI} are useful: {A} systematic review and meta-analysis},
	copyright = {2024 The Author(s)},
	issn = {2397-3374},
	shorttitle = {When combinations of humans and {AI} are useful},
	url = {https://www.nature.com/articles/s41562-024-02024-1},
	doi = {10.1038/s41562-024-02024-1},
	abstract = {Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human–AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human–AI combinations. First, we found that, on average, human–AI combinations performed significantly worse than the best of humans or AI alone (Hedges’ g = −0.23; 95\% confidence interval, −0.39 to −0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human–AI collaboration and point to promising avenues for improving human–AI systems.},
	language = {en},
	urldate = {2024-11-14},
	journal = {Nature Human Behaviour},
	author = {Vaccaro, Michelle and Almaatouq, Abdullah and Malone, Thomas},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Information systems and information technology, Mathematics and computing, Science, technology and society},
	pages = {1--11},
}

@article{mcdermott_internal_2011,
	title = {Internal and external validity},
	volume = {27},
	journal = {Cambridge handbook of experimental political science},
	author = {McDermott, Rose},
	year = {2011},
}

@article{jacovi_towards_2020,
	title = {Towards {Faithfully} {Interpretable} {NLP} {Systems}: {How} should we define and evaluate faithfulness?},
	shorttitle = {Towards {Faithfully} {Interpretable} {NLP} {Systems}},
	url = {http://arxiv.org/abs/2004.03685},
	abstract = {With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is "defined" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.},
	urldate = {2021-01-19},
	journal = {arXiv:2004.03685 [cs]},
	author = {Jacovi, Alon and Goldberg, Yoav},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03685},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	volume = {36},
	shorttitle = {Tree of {Thoughts}},
	url = {https://collaborate.princeton.edu/en/publications/tree-of-thoughts-deliberate-problem-solving-with-large-language-m},
	language = {English (US)},
	urldate = {2024-08-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	year = {2023},
	note = {ISSN: 1049-5258},
}

@article{yao_tree_nodate,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
}

@inproceedings{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
	doi = {10.1145/2939672.2939778},
	language = {en},
	urldate = {2016-09-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	note = {1999 citations (Semantic Scholar/DOI) [2022-08-26]},
	keywords = {Cited, Definitely read, Explanation technique, Human evaluation, Interpretable ML evaluation technique, Key paper, Must cite, domain agnostic, evaluate overall model reliability, explanation by model parameters, images, local fidelity, model agnostic, text},
	pages = {1135--1144},
}

@misc{wang_self-consistency_2023,
	title = {Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	month = mar,
	year = {2023},
	note = {arXiv:2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{olah_feature_2017,
	title = {Feature {Visualization}},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	abstract = {How neural networks build up their understanding of images},
	language = {en},
	number = {11},
	urldate = {2024-08-13},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	month = nov,
	year = {2017},
	pages = {e7},
}

@article{elhage_mathematical_2021,
	title = {A mathematical framework for transformer circuits},
	volume = {1},
	number = {1},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and {others}},
	year = {2021},
	pages = {12},
}

@inproceedings{pinto_large_2023,
	address = {New York, NY, USA},
	series = {{SBES} '23},
	title = {Large {Language} {Models} for {Education}: {Grading} {Open}-{Ended} {Questions} {Using} {ChatGPT}},
	isbn = {9798400707872},
	shorttitle = {Large {Language} {Models} for {Education}},
	url = {https://dl.acm.org/doi/10.1145/3613372.3614197},
	doi = {10.1145/3613372.3614197},
	abstract = {As a way of addressing increasingly sophisticated problems, software professionals face the constant challenge of seeking improvement. However, for these individuals to enhance their skills, their process of studying and training must involve feedback that is both immediate and accurate. In the context of software companies, where the scale of professionals undergoing training is large, but the number of qualified professionals available for providing corrections is small, delivering effective feedback becomes even more challenging. To circumvent this challenge, this work presents an exploration of using Large Language Models (LLMs) to support the correction process of open-ended questions in technical training. In this study, we utilized ChatGPT to correct open-ended questions answered by 42 industry professionals on two topics. Evaluating the corrections and feedback provided by ChatGPT, we observed that it is capable of identifying semantic details in responses that other metrics cannot observe. Furthermore, we noticed that, in general, subject matter experts tended to agree with the corrections and feedback given by ChatGPT.},
	urldate = {2024-08-12},
	booktitle = {Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Pinto, Gustavo and Cardoso-Pereira, Isadora and Monteiro, Danilo and Lucena, Danilo and Souza, Alberto and Gama, Kiev},
	month = sep,
	year = {2023},
	pages = {293--302},
}

@misc{gan_application_2024,
	title = {Application of {LLM} {Agents} in {Recruitment}: {A} {Novel} {Framework} for {Resume} {Screening}},
	shorttitle = {Application of {LLM} {Agents} in {Recruitment}},
	url = {http://arxiv.org/abs/2401.08315},
	doi = {10.48550/arXiv.2401.08315},
	abstract = {The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks. Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios. This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews. To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73{\textbackslash}\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Gan, Chengguang and Zhang, Qinghao and Mori, Tatsunori},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08315 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{thirunavukarasu_large_2023,
	title = {Large language models in medicine},
	volume = {29},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02448-8},
	doi = {10.1038/s41591-023-02448-8},
	abstract = {Large language models (LLMs) can respond to free-text queries without being specifically trained in the task in question, causing excitement and concern about their use in healthcare settings. ChatGPT is a generative artificial intelligence (AI) chatbot produced through sophisticated fine-tuning of an LLM, and other tools are emerging through similar developmental processes. Here we outline how LLM applications such as ChatGPT are developed, and we discuss how they are being leveraged in clinical settings. We consider the strengths and limitations of LLMs and their potential to improve the efficiency and effectiveness of clinical, educational and research work in medicine. LLM chatbots have already been deployed in a range of biomedical contexts, with impressive but mixed results. This review acts as a primer for interested clinicians, who will determine if and how LLM technology is used in healthcare for the benefit of patients and practitioners.},
	language = {en},
	number = {8},
	urldate = {2024-07-22},
	journal = {Nature Medicine},
	author = {Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Patient education, Translational research},
	pages = {1930--1940},
}

@misc{kumar_watch_2024,
	title = {Watch {Your} {Language}: {Investigating} {Content} {Moderation} with {Large} {Language} {Models}},
	shorttitle = {Watch {Your} {Language}},
	url = {http://arxiv.org/abs/2309.14517},
	doi = {10.48550/arXiv.2309.14517},
	abstract = {Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of commodity LLMs on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we instantiate 95 subcommunity specific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We find that GPT-3.5 is effective at rule-based moderation for many communities, achieving a median accuracy of 64\% and a median precision of 83\%. For toxicity detection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini Pro, LLAMA 2) and show that LLMs significantly outperform currently widespread toxicity classifiers. However, recent increases in model size add only marginal benefit to toxicity detection, suggesting a potential performance plateau for LLMs on toxicity detection tasks. We conclude by outlining avenues for future work in studying LLMs and content moderation.},
	urldate = {2024-07-22},
	publisher = {arXiv},
	author = {Kumar, Deepak and AbuHashem, Yousef and Durumeric, Zakir},
	month = jan,
	year = {2024},
	note = {arXiv:2309.14517 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction, Computer Science - Social and Information Networks},
}

@inproceedings{zhu_explanation_2024,
	address = {Mexico City, Mexico},
	title = {Explanation in the {Era} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-tutorials.3},
	abstract = {Explanation has long been a part of communications, where humans use language to elucidate each other and transmit information about the mechanisms of events. There have been numerous works that study the structures of the explanations and their utility to humans. At the same time, explanation relates to a collection of research directions in natural language processing (and more broadly, computer vision and machine learning) where researchers develop computational approaches to explain the (usually deep neural network) models. Explanation has received rising attention. In recent months, the advance of large language models (LLMs) provides unprecedented opportunities to leverage their reasoning abilities, both as tools to produce explanations and as the subjects of explanation analysis. On the other hand, the sheer sizes and the opaque nature of LLMs introduce challenges to the explanation methods. In this tutorial, we intend to review these opportunities and challenges of explanations in the era of LLMs, connect lines of research previously studied by different research groups, and hopefully spark thoughts of new research directions},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 5: {Tutorial} {Abstracts})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Zining and Chen, Hanjie and Ye, Xi and Lyu, Qing and Tan, Chenhao and Marasovic, Ana and Wiegreffe, Sarah},
	editor = {Zhang, Rui and Schneider, Nathan and Chaturvedi, Snigdha},
	month = jun,
	year = {2024},
	pages = {19--25},
}

@misc{singh_rethinking_2024,
	title = {Rethinking {Interpretability} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.01761},
	abstract = {Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
	month = jan,
	year = {2024},
	note = {arXiv:2402.01761 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_towards_nodate,
	title = {Towards {Faithful} {Model} {Explanation} in {NLP}: {A} {Survey} {\textbar} {Computational} {Linguistics} {\textbar} {MIT} {Press}},
	url = {https://direct.mit.edu/coli/article/50/2/657/119158/Towards-Faithful-Model-Explanation-in-NLP-A-Survey},
	urldate = {2024-08-12},
}

@misc{agarwal_faithfulness_2024,
	title = {Faithfulness vs. {Plausibility}: {On} the ({Un}){Reliability} of {Explanations} from {Large} {Language} {Models}},
	shorttitle = {Faithfulness vs. {Plausibility}},
	url = {http://arxiv.org/abs/2402.04614},
	doi = {10.48550/arXiv.2402.04614},
	abstract = {Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
	month = mar,
	year = {2024},
	note = {arXiv:2402.04614 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{siegel_probabilities_2024,
	title = {The {Probabilities} {Also} {Matter}: {A} {More} {Faithful} {Metric} for {Faithfulness} of {Free}-{Text} {Explanations} in {Large} {Language} {Models}},
	shorttitle = {The {Probabilities} {Also} {Matter}},
	url = {http://arxiv.org/abs/2404.03189},
	doi = {10.48550/arXiv.2404.03189},
	abstract = {In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Siegel, Noah Y. and Camburu, Oana-Maria and Heess, Nicolas and Perez-Ortiz, Maria},
	month = jun,
	year = {2024},
	note = {arXiv:2404.03189 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{huang_can_2023,
	title = {Can {Large} {Language} {Models} {Explain} {Themselves}? {A} {Study} of {LLM}-{Generated} {Self}-{Explanations}},
	shorttitle = {Can {Large} {Language} {Models} {Explain} {Themselves}?},
	url = {http://arxiv.org/abs/2310.11207},
	doi = {10.48550/arXiv.2310.11207},
	abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yu_natural_2023,
	title = {Natural {Language} {Reasoning}, {A} {Survey}},
	url = {http://arxiv.org/abs/2303.14725},
	doi = {10.48550/arXiv.2303.14725},
	abstract = {This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Yu, Fei and Zhang, Hongbo and Tiwari, Prayag and Wang, Benyou},
	month = may,
	year = {2023},
	note = {arXiv:2303.14725 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{shen_large_2023,
	title = {Large {Language} {Model} {Alignment}: {A} {Survey}},
	shorttitle = {Large {Language} {Model} {Alignment}},
	url = {http://arxiv.org/abs/2309.15025},
	abstract = {Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.},
	language = {en},
	urldate = {2024-08-11},
	publisher = {arXiv},
	author = {Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.15025 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{christiano_clarifying_2021,
	title = {Clarifying “{AI} alignment”},
	url = {https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6},
	abstract = {Clarifying what I mean when I say that an AI is aligned.},
	language = {en},
	urldate = {2024-08-10},
	journal = {Medium},
	author = {Christiano, Paul},
	month = apr,
	year = {2021},
}

@misc{terry_ai_2023,
	title = {{AI} {Alignment} in the {Design} of {Interactive} {AI}: {Specification} {Alignment}, {Process} {Alignment}, and {Evaluation} {Support}},
	shorttitle = {{AI} {Alignment} in the {Design} of {Interactive} {AI}},
	url = {http://arxiv.org/abs/2311.00710},
	doi = {10.48550/arXiv.2311.00710},
	abstract = {AI alignment considers the overall problem of ensuring an AI produces desired outcomes, without undesirable side effects. While often considered from the perspectives of safety and human values, AI alignment can also be considered in the context of designing and evaluating interfaces for interactive AI systems. This paper maps concepts from AI alignment onto a basic, three step interaction cycle, yielding a corresponding set of alignment objectives: 1) specification alignment: ensuring the user can efficiently and reliably communicate objectives to the AI, 2) process alignment: providing the ability to verify and optionally control the AI's execution process, and 3) evaluation support: ensuring the user can verify and understand the AI's output. We also introduce the concepts of a surrogate process, defined as a simplified, separately derived, but controllable representation of the AI's actual process; and the notion of a Process Gulf, which highlights how differences between human and AI processes can lead to challenges in AI control. To illustrate the value of this framework, we describe commercial and research systems along each of the three alignment dimensions, and show how interfaces that provide interactive alignment mechanisms can lead to qualitatively different and improved user experiences.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Terry, Michael and Kulkarni, Chinmay and Wattenberg, Martin and Dixon, Lucas and Morris, Meredith Ringel},
	month = oct,
	year = {2023},
	note = {arXiv:2311.00710 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{anwar_foundational_2024,
	title = {Foundational {Challenges} in {Assuring} {Alignment} and {Safety} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.09932},
	doi = {10.48550/arXiv.2404.09932},
	abstract = {This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose \$200+\$ concrete research questions.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and Edelman, Benjamin L. and Zhang, Zhaowei and Günther, Mario and Korinek, Anton and Hernandez-Orallo, Jose and Hammond, Lewis and Bigelow, Eric and Pan, Alexander and Langosco, Lauro and Korbak, Tomasz and Zhang, Heidi and Zhong, Ruiqi and hÉigeartaigh, Seán Ó and Recchia, Gabriel and Corsi, Giulio and Chan, Alan and Anderljung, Markus and Edwards, Lilian and Bengio, Yoshua and Chen, Danqi and Albanie, Samuel and Maharaj, Tegan and Foerster, Jakob and Tramer, Florian and He, He and Kasirzadeh, Atoosa and Choi, Yejin and Krueger, David},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09932 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{shen_towards_2024,
	title = {Towards {Bidirectional} {Human}-{AI} {Alignment}: {A} {Systematic} {Review} for {Clarifications}, {Framework}, and {Future} {Directions}},
	shorttitle = {Towards {Bidirectional} {Human}-{AI} {Alignment}},
	url = {http://arxiv.org/abs/2406.09264},
	abstract = {Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment. However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment. In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem [429]. This perspective largely neglects the long-term interaction and dynamic changes of alignment. To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML), and others. We characterize, define and scope human-AI alignment. From this, we present a conceptual framework of "Bidirectional Human-AI Alignment" to organize the literature from a human-centered perspective. This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally. Additionally, we articulate the key findings derived from literature analysis, including discussions about human values, interaction techniques, and evaluations. To pave the way for future studies, we envision three key challenges for future directions and propose examples of potential future solutions.},
	language = {en},
	urldate = {2024-07-22},
	publisher = {arXiv},
	author = {Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Alkiek, Kenan and Krishna, Kundan and Liu, Yachuan and Ma, Ziqiao and Petridis, Savvas and Peng, Yi-Hao and Qiwei, Li and Rakshit, Sushrita and Si, Chenglei and Xie, Yutong and Bigham, Jeffrey P. and Bentley, Frank and Chai, Joyce and Lipton, Zachary and Mei, Qiaozhu and Mihalcea, Rada and Terry, Michael and Yang, Diyi and Morris, Meredith Ringel and Resnick, Paul and Jurgens, David},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09264 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{si_large_2024,
	title = {Large {Language} {Models} {Help} {Humans} {Verify} {Truthfulness} -- {Except} {When} {They} {Are} {Convincingly} {Wrong}},
	url = {http://arxiv.org/abs/2310.12558},
	abstract = {Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. Our experiments with 80 crowdworkers compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information—explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users’ over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where overrelying on wrong AI explanations could lead to critical consequences.},
	language = {en},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Si, Chenglei and Goyal, Navita and Wu, Sherry Tongshuang and Zhao, Chen and Feng, Shi and Daumé III, Hal and Boyd-Graber, Jordan},
	month = apr,
	year = {2024},
	note = {arXiv:2310.12558 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@article{noauthor_llm_2024,
	title = {{LLM} {In}-{Context} {Recall} is {Prompt} {Dependent}},
	abstract = {The proliferation of Large Language Models (LLMs) highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given prompt. A model’s ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications. Our research analyzes the in-context recall performance of various LLMs using the needle-in-a-haystack method. In this approach, a factoid (the “needle”) is embedded within a block of filler text (the “haystack”), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an LLM’s recall capability is not only contingent upon the prompt’s content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or fine-tuning can improve performance. Our analysis provides insight into LLM behavior, offering direction for the development of more effective applications of LLMs.},
	language = {en},
	year = {2024},
}

@misc{dhinakaran_needle_2024,
	title = {The {Needle} {In} a {Haystack} {Test}: {Evaluating} the {Performance} of {LLM} {RAG} {Systems}},
	shorttitle = {The {Needle} {In} a {Haystack} {Test}},
	url = {https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/},
	abstract = {Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment},
	language = {en-US},
	urldate = {2024-06-15},
	journal = {Arize AI},
	author = {Dhinakaran, Aparna and Jolly, Evan},
	year = {2024},
}

@misc{deyoung_evidence_2020,
	title = {Evidence {Inference} 2.0: {More} {Data}, {Better} {Models}},
	shorttitle = {Evidence {Inference} 2.0},
	url = {http://arxiv.org/abs/2005.04177},
	abstract = {How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care. NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset (Lehman et al., 2019) was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for ﬁve-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http: //evidence-inference.ebm-nlp.com/.},
	language = {en},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {DeYoung, Jay and Lehman, Eric and Nye, Ben and Marshall, Iain J. and Wallace, Byron C.},
	month = may,
	year = {2020},
	note = {arXiv:2005.04177 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{vladika_healthfc_2024,
	address = {Torino, Italia},
	title = {{HealthFC}: {Verifying} {Health} {Claims} with {Evidence}-{Based} {Medical} {Fact}-{Checking}},
	shorttitle = {{HealthFC}},
	url = {https://aclanthology.org/2024.lrec-main.709},
	abstract = {In the digital age, seeking health advice on the Internet has become a common practice. At the same time, determining the trustworthiness of online medical content is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance automated Natural Language Processing (NLP) solutions for this task, in this paper we introduce a novel dataset HealthFC. It consists of 750 health-related claims in German and English, labeled for veracity by medical experts and backed with evidence from systematic reviews and clinical trials. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for NLP tasks related to automated fact-checking, such as evidence retrieval, claim verification, or explanation generation. For testing purposes, we provide baseline systems based on different approaches, examine their performance, and discuss the findings. We show that the dataset is a challenging test bed with a high potential for future use.},
	urldate = {2024-06-14},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Vladika, Juraj and Schneider, Phillip and Matthes, Florian},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {8095--8107},
}

@article{team_gemini_2023,
	title = {Gemini: a family of highly capable multimodal models},
	journal = {arXiv preprint arXiv:2312.11805},
	author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and {others}},
	year = {2023},
}

@misc{dhinakaran_needle_2024-1,
	title = {The {Needle} {In} a {Haystack} {Test}},
	url = {https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38},
	abstract = {Evaluating the performance of RAG systems},
	language = {en},
	urldate = {2024-06-07},
	journal = {Medium},
	author = {Dhinakaran, Aparna},
	month = feb,
	year = {2024},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2020-05-03},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = nov,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{gurrapu_rationalization_2023,
	title = {Rationalization for explainable {NLP}: a survey},
	volume = {6},
	issn = {2624-8212},
	shorttitle = {Rationalization for explainable {NLP}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10560994/},
	doi = {10.3389/frai.2023.1225093},
	abstract = {Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007 to 2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.},
	urldate = {2024-02-16},
	journal = {Frontiers in Artificial Intelligence},
	author = {Gurrapu, Sai and Kulkarni, Ajay and Huang, Lifu and Lourentzou, Ismini and Batarseh, Feras A.},
	month = sep,
	year = {2023},
	pmid = {37818431},
	pmcid = {PMC10560994},
	pages = {1225093},
}

@misc{fok_search_2023,
	title = {In {Search} of {Verifiability}: {Explanations} {Rarely} {Enable} {Complementary} {Performance} in {AI}-{Advised} {Decision} {Making}},
	shorttitle = {In {Search} of {Verifiability}},
	url = {http://arxiv.org/abs/2305.07722},
	abstract = {The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Fok, Raymond and Weld, Daniel S.},
	month = may,
	year = {2023},
	note = {arXiv:2305.07722 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@inproceedings{wulczyn_ex_2017,
	title = {Ex {Machina}: {Personal} {Attacks} {Seen} at {Scale}},
	isbn = {978-1-4503-4913-0},
	shorttitle = {Ex {Machina}},
	url = {http://dl.acm.org/citation.cfm?doid=3038912.3052591},
	doi = {10.1145/3038912.3052591},
	language = {en},
	urldate = {2017-07-14},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web}},
	author = {Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
	year = {2017},
	keywords = {Dataset, Key paper},
	pages = {1391--1399},
}

@misc{stacey_logical_2023,
	title = {Logical {Reasoning} for {Natural} {Language} {Inference} {Using} {Generated} {Facts} as {Atoms}},
	url = {http://arxiv.org/abs/2305.13214},
	doi = {10.48550/arXiv.2305.13214},
	abstract = {State-of-the-art neural models can now reach human performance levels across various natural language understanding tasks. However, despite this impressive performance, models are known to learn from annotation artefacts at the expense of the underlying task. While interpretability methods can identify influential features for each prediction, there are no guarantees that these features are responsible for the model decisions. Instead, we introduce a model-agnostic logical framework to determine the specific information in an input responsible for each model decision. This method creates interpretable Natural Language Inference (NLI) models that maintain their predictive power. We achieve this by generating facts that decompose complex NLI observations into individual logical atoms. Our model makes predictions for each atom and uses logical rules to decide the class of the observation based on the predictions for each atom. We apply our method to the highly challenging ANLI dataset, where our framework improves the performance of both a DeBERTa-base and BERT baseline. Our method performs best on the most challenging examples, achieving a new state-of-the-art for the ANLI round 3 test set. We outperform every baseline in a reduced-data setting, and despite using no annotations for the generated facts, our model predictions for individual facts align with human expectations.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Camburu, Oana-Maria and Rei, Marek},
	month = may,
	year = {2023},
	note = {arXiv:2305.13214 [cs]},
	keywords = {Cited, Computer Science - Computation and Language, I.2.7},
}

@article{lyu_towards_2024,
	title = {Towards {Faithful} {Model} {Explanation} in {NLP}: {A} {Survey}},
	issn = {0891-2017},
	shorttitle = {Towards {Faithful} {Model} {Explanation} in {NLP}},
	url = {https://doi.org/10.1162/coli_a_00511},
	doi = {10.1162/coli_a_00511},
	abstract = {End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model’s prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.},
	urldate = {2024-02-03},
	journal = {Computational Linguistics},
	author = {Lyu, Qing and Apidianaki, Marianna and Callison-Burch, Chris},
	month = jan,
	year = {2024},
	keywords = {Cited, Faithfulness},
	pages = {1--70},
}

@misc{turpin_language_2023,
	title = {Language {Models} {Don}'t {Always} {Say} {What} {They} {Think}: {Unfaithful} {Explanations} in {Chain}-of-{Thought} {Prompting}},
	shorttitle = {Language {Models} {Don}'t {Always} {Say} {What} {They} {Think}},
	url = {http://arxiv.org/abs/2305.04388},
	doi = {10.48550/arXiv.2305.04388},
	abstract = {Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(A)"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36\% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},
	month = dec,
	year = {2023},
	note = {arXiv:2305.04388 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	doi = {10.48550/arXiv.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhao_verify-and-edit_2023,
	title = {Verify-and-{Edit}: {A} {Knowledge}-{Enhanced} {Chain}-of-{Thought} {Framework}},
	shorttitle = {Verify-and-{Edit}},
	url = {http://arxiv.org/abs/2305.03268},
	abstract = {As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Zhao, Ruochen and Li, Xingxuan and Joty, Shafiq and Qin, Chengwei and Bing, Lidong},
	month = may,
	year = {2023},
	note = {arXiv:2305.03268 [cs]},
	keywords = {Cited, Computer Science - Computation and Language},
}

@misc{wei_chain_2022,
	title = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.},
	language = {en},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jun,
	year = {2022},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{jung_maieutic_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Maieutic {Prompting}: {Logically} {Consistent} {Reasoning} with {Recursive} {Explanations}},
	shorttitle = {Maieutic {Prompting}},
	url = {https://aclanthology.org/2022.emnlp-main.82},
	abstract = {Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
	urldate = {2023-04-20},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Le Bras, Ronan and Choi, Yejin},
	month = dec,
	year = {2022},
	keywords = {Cited},
	pages = {1266--1279},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{majumder_knowledge-grounded_2022,
	title = {Knowledge-{Grounded} {Self}-{Rationalization} via {Extractive} and {Natural} {Language} {Explanations}},
	url = {http://arxiv.org/abs/2106.13876},
	doi = {10.48550/arXiv.2106.13876},
	abstract = {Models that generate extractive rationales (i.e., subsets of features) or natural language explanations (NLEs) for their predictions are important for explainable AI. While an extractive rationale provides a quick view of the features most responsible for a prediction, an NLE allows for a comprehensive description of the decision-making process behind a prediction. However, current models that generate the best extractive rationales or NLEs often fall behind the state-of-the-art (SOTA) in terms of task performance. In this work, we bridge this gap by introducing RExC, a self-rationalizing framework that grounds its predictions and two complementary types of explanations (NLEs and extractive rationales) in background knowledge. Our framework improves over previous methods by: (i) reaching SOTA task performance while also providing explanations, (ii) providing two types of explanations, while existing models usually provide only one type, and (iii) beating by a large margin the previous SOTA in terms of quality of both types of explanations. Furthermore, a perturbation analysis in RExC shows a high degree of association between explanations and predictions, a necessary property of faithful explanations.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Majumder, Bodhisattwa Prasad and Camburu, Oana-Maria and Lukasiewicz, Thomas and McAuley, Julian},
	month = sep,
	year = {2022},
	note = {arXiv:2106.13876 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Extractive, Free text},
}

@misc{agarwal_faithfulness_2024-1,
	title = {Faithfulness vs. {Plausibility}: {On} the ({Un}){Reliability} of {Explanations} from {Large} {Language} {Models}},
	shorttitle = {Faithfulness vs. {Plausibility}},
	url = {http://arxiv.org/abs/2402.04614},
	abstract = {Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we urge the community to identify the faithfulness requirements of real-world applications and ensure explanations meet those needs. Finally, we propose some directions for future work, emphasizing the need for novel methodologies and frameworks that can enhance the faithfulness of self-explanations without compromising their plausibility, essential for the transparent deployment of LLMs in diverse high-stakes domains.},
	urldate = {2024-02-10},
	publisher = {arXiv},
	author = {Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04614 [cs]},
	keywords = {Cited, Computer Science - Computation and Language, Free text},
}

@misc{si_large_2023,
	title = {Large {Language} {Models} {Help} {Humans} {Verify} {Truthfulness} -- {Except} {When} {They} {Are} {Convincingly} {Wrong}},
	url = {http://arxiv.org/abs/2310.12558},
	doi = {10.48550/arXiv.2310.12558},
	abstract = {Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-reliance on LLMs, but cannot significantly outperform search engines. However, showing both search engine results and LLM explanations offers no complementary benefits as compared to search engines alone. Taken together, natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages yet, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Si, Chenglei and Goyal, Navita and Wu, Sherry Tongshuang and Zhao, Chen and Feng, Shi and Daumé III, Hal and Boyd-Graber, Jordan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12558 [cs]},
	keywords = {Cited, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{huang_can_2023-1,
	title = {Can {Large} {Language} {Models} {Explain} {Themselves}? {A} {Study} of {LLM}-{Generated} {Self}-{Explanations}},
	shorttitle = {Can {Large} {Language} {Models} {Explain} {Themselves}?},
	url = {http://arxiv.org/abs/2310.11207},
	abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11207 [cs]},
	keywords = {Cited, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wiegreffe_teach_2021,
	title = {Teach {Me} to {Explain}: {A} {Review} of {Datasets} for {Explainable} {Natural} {Language} {Processing}},
	shorttitle = {Teach {Me} to {Explain}},
	url = {http://arxiv.org/abs/2102.12060},
	abstract = {Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wiegreffe, Sarah and Marasović, Ana},
	month = dec,
	year = {2021},
	note = {arXiv:2102.12060 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{lei_rationalizing_2016,
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://arxiv.org/abs/1606.04155},
	urldate = {2016-11-14},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	year = {2016},
	note = {571 citations (Semantic Scholar/arXiv) [2022-08-26]},
	keywords = {Cited, Definitely read, Explanation technique, Interpretable ML evaluation technique, Must cite, No evaluation},
	pages = {107--117},
}

@inproceedings{zhang_explain_2021,
	address = {New York, NY, USA},
	series = {{WSDM} '21},
	title = {Explain and {Predict}, and then {Predict} {Again}},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441758},
	doi = {10.1145/3437963.3441758},
	abstract = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
	urldate = {2024-02-13},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
	month = mar,
	year = {2021},
	keywords = {Cited, explanation, interpretable by design, learning with rationales, machine learning interpretation, multitask learning},
	pages = {418--426},
}

@inproceedings{zhang_explain_2021-1,
	address = {New York, NY, USA},
	series = {{WSDM} '21},
	title = {Explain and {Predict}, and then {Predict} {Again}},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441758},
	doi = {10.1145/3437963.3441758},
	abstract = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
	urldate = {2024-02-13},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
	month = mar,
	year = {2021},
	keywords = {explanation, interpretable by design, learning with rationales, machine learning interpretation, multitask learning},
	pages = {418--426},
}

@inproceedings{yu_understanding_2021,
	title = {Understanding {Interlocking} {Dynamics} of {Cooperative} {Rationalization}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/6a711a119a8a7a9f877b5f379bfe9ea2-Abstract.html},
	abstract = {Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm --- model interlocking. Inter-locking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator’s selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments.},
	urldate = {2022-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yu, Mo and Zhang, Yang and Chang, Shiyu and Jaakkola, Tommi},
	year = {2021},
	pages = {12822--12835},
}

@article{deyoung_eraser_2019,
	title = {{ERASER}: {A} {Benchmark} to {Evaluate} {Rationalized} {NLP} {Models}},
	shorttitle = {{ERASER}},
	url = {http://arxiv.org/abs/1911.03429},
	abstract = {State-of-the-art models in NLP are now predominantly based on deep neural networks that are generally opaque in terms of how they come to specific predictions. This limitation has led to increased interest in designing more interpretable deep models for NLP that can reveal the `reasoning' underlying model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER) benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of "rationales" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at: www.eraserbenchmark.com .},
	urldate = {2020-03-30},
	journal = {arXiv preprint},
	author = {DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.03429},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wadden_scifact-open_2022,
	title = {{SciFact}-{Open}: {Towards} open-domain scientific claim verification},
	shorttitle = {{SciFact}-{Open}},
	url = {http://arxiv.org/abs/2210.13777},
	doi = {10.48550/arXiv.2210.13777},
	abstract = {While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Wadden, David and Lo, Kyle and Kuehl, Bailey and Cohan, Arman and Beltagy, Iz and Wang, Lucy Lu and Hajishirzi, Hannaneh},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13777 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{khashabi_looking_2018,
	address = {New Orleans, Louisiana},
	title = {Looking {Beyond} the {Surface}: {A} {Challenge} {Set} for {Reading} {Comprehension} over {Multiple} {Sentences}},
	shorttitle = {Looking {Beyond} the {Surface}},
	url = {http://aclweb.org/anthology/N18-1023},
	doi = {10.18653/v1/N18-1023},
	abstract = {We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains ∼6k questions for +800 paragraphs across 7 different domains (elementary school science, news, travel guides, ﬁction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 86.4\%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difﬁculty of this challenge, despite a high human performance. The dataset is the ﬁrst to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.},
	language = {en},
	urldate = {2020-06-01},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
	year = {2018},
	pages = {252--262},
}

@inproceedings{carton_extractive_2018,
	address = {Brussels, Belgium},
	title = {Extractive {Adversarial} {Networks}: {High}-{Recall} {Explanations} for {Identifying} {Personal} {Attacks} in {Social} {Media} {Posts}},
	shorttitle = {Extractive {Adversarial} {Networks}},
	url = {https://aclanthology.org/D18-1386},
	doi = {10.18653/v1/D18-1386},
	abstract = {We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate “default” behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.},
	urldate = {2021-11-12},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Carton, Samuel and Mei, Qiaozhu and Resnick, Paul},
	month = oct,
	year = {2018},
	pages = {3497--3507},
}

@misc{lyu_faithful_2023,
	title = {Faithful {Chain}-of-{Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2301.13379},
	doi = {10.48550/arXiv.2301.13379},
	abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query \${\textbackslash}rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \${\textbackslash}rightarrow\$ answer), using an LM and a deterministic solver respectively. We demonstrate the efficacy of our approach on 10 reasoning datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9 out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1 on Logical Inference, under greedy decoding. Together with self-consistency decoding, we achieve new state-of-the-art few-shot performance on 7 out of the 10 datasets, showing a strong synergy between faithfulness and accuracy.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13379 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{carton_what_2022,
	address = {Dublin, Ireland},
	title = {What to {Learn}, and {How}: {Toward} {Effective} {Learning} from {Rationales}},
	shorttitle = {What to {Learn}, and {How}},
	url = {https://aclanthology.org/2022.findings-acl.86},
	doi = {10.18653/v1/2022.findings-acl.86},
	abstract = {Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction.Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3\% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.},
	urldate = {2022-08-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Carton, Samuel and Kanoria, Surya and Tan, Chenhao},
	month = may,
	year = {2022},
	pages = {1075--1088},
}

@article{hase_when_2021,
	title = {When {Can} {Models} {Learn} {From} {Explanations}? {A} {Formal} {Framework} for {Understanding} the {Roles} of {Explanation} {Data}},
	shorttitle = {When {Can} {Models} {Learn} {From} {Explanations}?},
	url = {http://arxiv.org/abs/2102.02201},
	abstract = {Many methods now exist for conditioning model outputs on task instructions, retrieved documents, and user-provided explanations and feedback. Rather than relying solely on examples of task inputs and outputs, these approaches allow for valuable additional data to be used in modeling with the purpose of improving model correctness and aligning learned models with human priors. Meanwhile, a growing body of evidence suggests that some language models can (1) store a large amount of knowledge in their parameters, and (2) perform inference over tasks in unstructured text to solve new tasks at test time. These results raise the possibility that, for some tasks, humans cannot explain to a model any more about the task than it already knows or could infer on its own. In this paper, we study the circumstances under which explanations of individual data points can (or cannot) improve modeling performance. In order to carefully control important properties of the data and explanations, we introduce a synthetic dataset for experiments, and we also make use of three existing datasets with explanations: e-SNLI, TACRED, SemEval. We first give a formal framework for the available modeling approaches, in which explanation data can be used as model inputs, as labels, or as a prior. After arguing that the most promising role for explanation data is as model inputs, we propose to use a retrieval-based method and show that it solves our synthetic task with accuracies upwards of 95\%, while baselines without explanation data achieve below 65\% accuracy. We then identify properties of datasets for which retrieval-based modeling fails. With the three existing datasets, we find no improvements from explanation retrieval. Drawing on our findings from our synthetic task, we suggest that at least one of six preconditions for successful modeling fails to hold with these datasets.},
	urldate = {2021-02-05},
	journal = {arXiv:2102.02201 [cs]},
	author = {Hase, Peter and Bansal, Mohit},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.02201},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{hu_think_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {Think {Rationally} about {What} {You} {See}: {Continuous} {Rationale} {Extraction} for {Relation} {Extraction}},
	isbn = {978-1-4503-9408-6},
	shorttitle = {Think {Rationally} about {What} {You} {See}},
	url = {https://dl.acm.org/doi/10.1145/3539618.3592072},
	doi = {10.1145/3539618.3592072},
	abstract = {Relation extraction (RE) aims to extract potential relations according to the context of two entities, thus, deriving rational contexts from sentences plays an important role. Previous works either focus on how to leverage the entity information (e.g., entity types, entity verbalization) to inference relations, but ignore context-focused content, or use counterfactual thinking to remove the model's bias of potential relations in entities, but the relation reasoning process will still be hindered by irrelevant content. Therefore, how to preserve relevant content and remove noisy segments from sentences is a crucial task. In addition, retained content needs to be fluent enough to maintain semantic coherence and interpretability. In this work, we propose a novel rationale extraction framework named RE2, which leverages two continuity and sparsity factors to obtain relevant and coherent rationales from sentences. To solve the problem that the gold rationales are not labeled, RE2 applies an optimizable binary mask to each token in the sentence, and adjust the rationales that need to be selected according to the relation label. Experiments on four datasets show that RE2 surpasses baselines.},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Xuming and Hong, Zhaochen and Zhang, Chenwei and King, Irwin and Yu, Philip},
	month = jul,
	year = {2023},
	keywords = {Extractive, continuous rationale extraction, relation extraction},
	pages = {2436--2440},
}

@misc{ludan_interpretable-by-design_2023,
	title = {Interpretable-by-{Design} {Text} {Classification} with {Iteratively} {Generated} {Concept} {Bottleneck}},
	url = {http://arxiv.org/abs/2310.19660},
	doi = {10.48550/arXiv.2310.19660},
	abstract = {Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Ludan, Josh Magnus and Lyu, Qing and Yang, Yue and Dugan, Liam and Yatskar, Mark and Callison-Burch, Chris},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19660 [cs]},
	keywords = {Computer Science - Computation and Language, Extractive},
}

@misc{guan_language_2023,
	title = {Language {Models} {Hallucinate}, but {May} {Excel} at {Fact} {Verification}},
	url = {http://arxiv.org/abs/2310.14564},
	abstract = {Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently "hallucinate," resulting in non-factual outputs. Our carefully designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25\% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments, at least in the Wikipedia domain. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Guan, Jian and Dodge, Jesse and Wadden, David and Huang, Minlie and Peng, Hao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14564 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{luo_-context_2024,
	title = {In-context {Learning} with {Retrieved} {Demonstrations} for {Language} {Models}: {A} {Survey}},
	shorttitle = {In-context {Learning} with {Retrieved} {Demonstrations} for {Language} {Models}},
	url = {http://arxiv.org/abs/2401.11624},
	doi = {10.48550/arXiv.2401.11624},
	abstract = {Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Luo, Man and Xu, Xin and Liu, Yue and Pasupat, Panupong and Kazemi, Mehran},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11624 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{louis_interpretable_2023,
	title = {Interpretable {Long}-{Form} {Legal} {Question} {Answering} with {Retrieval}-{Augmented} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.17050},
	abstract = {Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a "retrieve-then-read" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Louis, Antoine and van Dijck, Gijs and Spanakis, Gerasimos},
	month = sep,
	year = {2023},
	note = {arXiv:2309.17050 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{zhang_potential_nodate,
	title = {The {Potential} and {Pitfalls} of using a {Large} {Language} {Model} such as {ChatGPT} or {GPT}-4 as a {Clinical} {Assistant}},
	abstract = {Recent studies have demonstrated promising performance of ChatGPT and GPT-4 on several medical domain tasks. However, none have assessed its performance using a large-scale real-world electronic health record database, nor have evaluated its utility in providing clinical diagnostic assistance for patients across a full range of disease presentation. We performed two analyses using ChatGPT and GPT-4, one to identify patients with specific medical diagnoses using a real-world large electronic health record database and the other, in providing diagnostic assistance to healthcare workers in the prospective evaluation of hypothetical patients. Our results show that GPT-4 across disease classification tasks with chain of thought and few-shot prompting can achieve performance as high as 96\% F1 scores. For patient assessment, GPT-4 can accurately diagnose three out of four times. However, there were mentions of factually incorrect statements, overlooking crucial medical findings, recommendations for unnecessary investigations and overtreatment. These issues coupled with privacy concerns, make these models currently inadequate for real world clinical use. However, limited data and time needed for prompt engineering in comparison to configuration of conventional machine learning workflows highlight their potential for scalability across healthcare applications.},
	language = {en},
	author = {Zhang, Jingqing and Sun, Kai and Jagadeesh, Akshay and Ghahfarokhi, Mahta and Gupta, Deepa and Gupta, Vibhor and Guo, Yike},
}

@misc{krishna_post_2023,
	title = {Post {Hoc} {Explanations} of {Language} {Models} {Can} {Improve} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.11426},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25\% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Krishna, Satyapriya and Ma, Jiaqi and Slack, Dylan and Ghandeharioun, Asma and Singh, Sameer and Lakkaraju, Himabindu},
	month = dec,
	year = {2023},
	note = {arXiv:2305.11426 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{zhao_explainability_2024,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	issn = {2157-6904},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3639372},
	doi = {10.1145/3639372},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
	urldate = {2024-02-03},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
	month = jan,
	year = {2024},
	note = {Just Accepted},
	keywords = {Explainability, Interpretability, Large language models},
}

@misc{malaviya_pachinko_2023,
	title = {Pachinko: {Patching} {Interpretable} {QA} {Models} through {Natural} {Language} {Feedback}},
	shorttitle = {Pachinko},
	url = {http://arxiv.org/abs/2311.09558},
	abstract = {Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales generated by QA models to support their answers. We specifically consider decomposed question-answering models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample these rationales from large language models using few-shot prompting for two reading comprehension datasets, and then perform two user studies. In the first one, we present users with incorrect answers and corresponding rationales of various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition to influencing critiquablity, certain formats significantly enhance user reported understanding and trust of model outputs.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Malaviya, Chaitanya and Lee, Subin and Roth, Dan and Yatskar, Mark},
	month = nov,
	year = {2023},
	note = {arXiv:2311.09558 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ko_claimdiff_2023,
	address = {Toronto, Canada},
	title = {{ClaimDiff}: {Comparing} and {Contrasting} {Claims} on {Contentious} {Issues}},
	shorttitle = {{ClaimDiff}},
	url = {https://aclanthology.org/2023.findings-acl.289},
	doi = {10.18653/v1/2023.findings-acl.289},
	abstract = {With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19\% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.},
	urldate = {2024-02-03},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ko, Miyoung and Seong, Ingyu and Lee, Hwaran and Park, Joonsuk and Chang, Minsuk and Seo, Minjoon},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4711--4731},
}

@inproceedings{feldhus_saliency_2023,
	address = {Toronto, Canada},
	title = {Saliency {Map} {Verbalization}: {Comparing} {Feature} {Importance} {Representations} from {Model}-free and {Instruction}-based {Methods}},
	shorttitle = {Saliency {Map} {Verbalization}},
	url = {https://aclanthology.org/2023.nlrse-1.4},
	doi = {10.18653/v1/2023.nlrse-1.4},
	abstract = {Saliency maps can explain a neural model's predictions by identifying important input features. They are difficult to interpret for laypeople, especially for instances with many features. In order to make them more accessible, we formalize the underexplored task of translating saliency maps into natural language and compare methods that address two key challenges of this approach – what and how to verbalize. In both automatic and human evaluation setups, using token-level attributions from text classification tasks, we compare two novel methods (search-based and instruction-based verbalizations) against conventional feature importance representations (heatmap visualizations and extractive rationales), measuring simulatability, faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human ratings, but they are not faithfully capturing numeric information and are inconsistent in their interpretation of the task. In comparison, our search-based, model-free verbalization approach efficiently completes templated verbalizations, is faithful by design, but falls short in helpfulness and simulatability. Our results suggest that saliency map verbalization makes feature attribution explanations more comprehensible and less cognitively challenging to humans than conventional representations.},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 1st {Workshop} on {Natural} {Language} {Reasoning} and {Structured} {Explanations} ({NLRSE})},
	publisher = {Association for Computational Linguistics},
	author = {Feldhus, Nils and Hennig, Leonhard and Nasert, Maximilian Dustin and Ebert, Christopher and Schwarzenberg, Robert and Möller, Sebastian},
	editor = {Dalvi Mishra, Bhavana and Durrett, Greg and Jansen, Peter and Neves Ribeiro, Danilo and Wei, Jason},
	month = jun,
	year = {2023},
	pages = {30--46},
}

@misc{tanneru_quantifying_2023,
	title = {Quantifying {Uncertainty} in {Natural} {Language} {Explanations} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.03533},
	doi = {10.48550/arXiv.2311.03533},
	abstract = {Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- \${\textbackslash}textit\{Verbalized Uncertainty\}\$ and \${\textbackslash}textit\{Probing Uncertainty\}\$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Tanneru, Sree Harsha and Agarwal, Chirag and Lakkaraju, Himabindu},
	month = nov,
	year = {2023},
	note = {arXiv:2311.03533 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huang_citation_2023,
	title = {Citation: {A} {Key} to {Building} {Responsible} and {Accountable} {Large} {Language} {Models}},
	shorttitle = {Citation},
	url = {http://arxiv.org/abs/2307.02185},
	abstract = {Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.},
	urldate = {2023-11-16},
	publisher = {arXiv},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	month = nov,
	year = {2023},
	note = {arXiv:2307.02185 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{guan_language_2023-1,
	title = {Language {Models} {Hallucinate}, but {May} {Excel} at {Fact} {Verification}},
	url = {http://arxiv.org/abs/2310.14564},
	abstract = {Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently "hallucinate," resulting in non-factual outputs. Our carefully designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25\% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments, at least in the Wikipedia domain. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Guan, Jian and Dodge, Jesse and Wadden, David and Huang, Minlie and Peng, Hao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14564 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ahsan_retrieving_2023,
	title = {Retrieving {Evidence} from {EHRs} with {LLMs}: {Possibilities} and {Challenges}},
	shorttitle = {Retrieving {Evidence} from {EHRs} with {LLMs}},
	url = {http://arxiv.org/abs/2309.04550},
	abstract = {Unstructured Electronic Health Record (EHR) data often contains critical information complementary to imaging data that would inform radiologists' diagnoses. However, time constraints and the large volume of notes frequently associated with individual patients renders manual perusal of such data to identify relevant evidence infeasible in practice. Modern Large Language Models (LLMs) provide a flexible means of interacting with unstructured EHR data, and may provide a mechanism to efficiently retrieve and summarize unstructured evidence relevant to a given query. In this work, we propose and evaluate an LLM (Flan-T5 XXL) for this purpose. Specifically, in a zero-shot setting we task the LLM to infer whether a patient has or is at risk of a particular condition; if so, we prompt the model to summarize the supporting evidence. Enlisting radiologists for manual evaluation, we find that this LLM-based approach provides outputs consistently preferred to a standard information retrieval baseline, but we also highlight the key outstanding challenge: LLMs are prone to hallucinating evidence. However, we provide results indicating that model confidence in outputs might indicate when LLMs are hallucinating, potentially providing a means to address this.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Ahsan, Hiba and McInerney, Denis Jered and Kim, Jisoo and Potter, Christopher and Young, Geoffrey and Amir, Silvio and Wallace, Byron C.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.04550 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{harsha_tanneru_quantifying_2023,
	title = {Quantifying {Uncertainty} in {Natural} {Language} {Explanations} of {Large} {Language} {Models}},
	url = {https://ui.adsabs.harvard.edu/abs/2023arXiv231103533H},
	doi = {10.48550/arXiv.2311.03533},
	abstract = {Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- \${\textbackslash}textit\{Verbalized Uncertainty\}\$ and \${\textbackslash}textit\{Probing Uncertainty\}\$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.},
	urldate = {2023-12-12},
	author = {Harsha Tanneru, Sree and Agarwal, Chirag and Lakkaraju, Himabindu},
	month = nov,
	year = {2023},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2023arXiv231103533H},
	keywords = {Computer Science - Computation and Language},
}

@misc{shrivastava_llamas_2023,
	title = {Llamas {Know} {What} {GPTs} {Don}'t {Show}: {Surrogate} {Models} for {Confidence} {Estimation}},
	shorttitle = {Llamas {Know} {What} {GPTs} {Don}'t {Show}},
	url = {http://arxiv.org/abs/2311.08877},
	abstract = {To maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5\% AUC on GPT-4 averaged across 12 question-answering datasets -- 7\% above a random baseline) but leaves room for improvement. We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6\% average AUC on GPT-4).},
	urldate = {2023-11-19},
	publisher = {arXiv},
	author = {Shrivastava, Vaishnavi and Liang, Percy and Kumar, Ananya},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08877 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
