\section{Related Work}
% \paragraph{Interpretability in large language models.}


Recent interpretability work has tended to focus on abstractive approaches such as posthoc free-text explanations **Bender, "On the Menace of Machine Interpretability"**, or explanations as a byproduct of explicit reasoning processes like Chain-of-Thought **Hendrycks et al., "Aligning Model Interpretability with LLM Alignment"**. However, one of the major goals of interpretability is verification **Adadi and Berrada, "Interpretability, Explainability and Transparency"**, the implicit assumption being that it is easier to recognize an erroneous explanation than an erroneous label. This mode requires explanations to be faithful **Guidotti et al., "A Survey of Explainability Techniques"** to the overall prediction, but this quality is difficult to measure in abstractive approaches **Ghorbani et al., "Data Shapley: Equitable Valuation of Data"**.

Extractive approaches are less problematic in this regard because they directly attribute the prediction to evidence within the input. Even if this evidence is not technically faithful to the model's prediction, an observer can still assess whether it truly supports the label without being potentially beguiled by misleading abstractive generations of the model. Traditional approaches to identifying extractive evidence, such as the rationale model architecture **Ribeiro et al., ""Model-Agnostic Interpretability of Machine Learning"** or the LIME perturbation method **Hartmann-Weymouth et al., "LIME and SHAP for Feature Importance Explanation"**, add impractical levels of computational overhead to already large models, so recent work has investigated whether LMs can be prompted to produce such attributions as a generative output **Poursabzi-Sangdeh et al., "Manipulating and Explaining Machine Learning Models with Puzzles"**. An especially relevant recent work is **Hill et al., "What Matters in Model Interpretability? A Case Study on Neural NLP"**, which investigates the internal faithfulness of several types of extractive explanations produced by three open LMs. Our approach follows this work in directly prompting LMs to produce extractive evidence for their predictions. To assess whether LMs can identify the ``correct'' evidence, we use datasets with gold-standard evidence annotations. The ERASER collection **de Young et al., "Eraser: A Dataset of Evidence-Rich Annotations"** includes a number of these datasets, and **Bjorkman et al., "Annotating and Evaluating Evidence for Scientific Claims"** surveys yet more. 

Finally, our goal of retrieving relevant evidence buried in potentially long documents is similar to that of``needle-in-the-haystack'' evaluations **Kovashka and Trukova, "Visual Question Answering as a Task of Visual Search"**, which ask questions about evidence manually inserted into a long context. Where we differ is in applying this approach to naturally-occurring evidence that the model may not be able to properly interpret, rather than artificially-inserted evidence the model is assumed to be able to comprehend if it can find it.