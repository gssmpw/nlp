\section{Related Work}
% \paragraph{Interpretability in large language models.}


Recent interpretability work has tended to focus on abstractive approaches such as posthoc free-text explanations ____, or explanations as a byproduct of explicit reasoning processes like Chain-of-Thought ____. However, one of the major goals of interpretability is verification ____, the implicit assumption being that it is easier to recognize an erroneous explanation than an erroneous label. This mode requires explanations to be faithful ____ to the overall prediction, but this quality is difficult to measure in abstractive approaches ____.

Extractive approaches are less problematic in this regard because they directly attribute the prediction to evidence within the input. Even if this evidence is not technically faithful to the model's prediction, an observer can still assess whether it truly supports the label without being potentially beguiled by misleading abstractive generations of the model. Traditional approaches to identifying extractive evidence, such as the rationale model architecture ____ or the LIME perturbation method ____, add impractical levels of computational overhead to already large models, so recent work has investigated whether LMs can be prompted to produce such attributions as a generative output ____. An especially relevant recent work is ____, which investigates the internal faithfulness of several types of extractive explanations produced by three open LMs. Our approach follows this work in directly prompting LMs to produce extractive evidence for their predictions. To assess whether LMs can identify the ``correct'' evidence, we use datasets with gold-standard evidence annotations. The ERASER collection ____ includes a number of these datasets, and ____ surveys yet more. 

Finally, our goal of retrieving relevant evidence buried in potentially long documents is similar to that of``needle-in-the-haystack'' evaluations ____, which ask questions about evidence manually inserted into a long context. Where we differ is in applying this approach to naturally-occurring evidence that the model may not be able to properly interpret, rather than artificially-inserted evidence the model is assumed to be able to comprehend if it can find it.