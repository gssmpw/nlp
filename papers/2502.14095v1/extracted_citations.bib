@misc{agarwal_faithfulness_2024,
	title = {Faithfulness vs. {Plausibility}: {On} the ({Un}){Reliability} of {Explanations} from {Large} {Language} {Models}},
	shorttitle = {Faithfulness vs. {Plausibility}},
	url = {http://arxiv.org/abs/2402.04614},
	doi = {10.48550/arXiv.2402.04614},
	abstract = {Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Agarwal, Chirag and Tanneru, Sree Harsha and Lakkaraju, Himabindu},
	month = mar,
	year = {2024},
	note = {arXiv:2402.04614 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{deyoung_eraser_2019,
	title = {{ERASER}: {A} {Benchmark} to {Evaluate} {Rationalized} {NLP} {Models}},
	shorttitle = {{ERASER}},
	url = {http://arxiv.org/abs/1911.03429},
	abstract = {State-of-the-art models in NLP are now predominantly based on deep neural networks that are generally opaque in terms of how they come to specific predictions. This limitation has led to increased interest in designing more interpretable deep models for NLP that can reveal the `reasoning' underlying model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER) benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of "rationales" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at: www.eraserbenchmark.com .},
	urldate = {2020-03-30},
	journal = {arXiv preprint},
	author = {DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.03429},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dhinakaran_needle_2024,
	title = {The {Needle} {In} a {Haystack} {Test}: {Evaluating} the {Performance} of {LLM} {RAG} {Systems}},
	shorttitle = {The {Needle} {In} a {Haystack} {Test}},
	url = {https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/},
	abstract = {Everything you need to know about the popular technique and the importance of evaluating retrieval and model performance throughout development and deployment},
	language = {en-US},
	urldate = {2024-06-15},
	journal = {Arize AI},
	author = {Dhinakaran, Aparna and Jolly, Evan},
	year = {2024},
}

@misc{fok_search_2023,
	title = {In {Search} of {Verifiability}: {Explanations} {Rarely} {Enable} {Complementary} {Performance} in {AI}-{Advised} {Decision} {Making}},
	shorttitle = {In {Search} of {Verifiability}},
	url = {http://arxiv.org/abs/2305.07722},
	abstract = {The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.},
	urldate = {2023-05-23},
	publisher = {arXiv},
	author = {Fok, Raymond and Weld, Daniel S.},
	month = may,
	year = {2023},
	note = {arXiv:2305.07722 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@inproceedings{hu_think_2023,
	address = {New York, NY, USA},
	series = {{SIGIR} '23},
	title = {Think {Rationally} about {What} {You} {See}: {Continuous} {Rationale} {Extraction} for {Relation} {Extraction}},
	isbn = {978-1-4503-9408-6},
	shorttitle = {Think {Rationally} about {What} {You} {See}},
	url = {https://dl.acm.org/doi/10.1145/3539618.3592072},
	doi = {10.1145/3539618.3592072},
	abstract = {Relation extraction (RE) aims to extract potential relations according to the context of two entities, thus, deriving rational contexts from sentences plays an important role. Previous works either focus on how to leverage the entity information (e.g., entity types, entity verbalization) to inference relations, but ignore context-focused content, or use counterfactual thinking to remove the model's bias of potential relations in entities, but the relation reasoning process will still be hindered by irrelevant content. Therefore, how to preserve relevant content and remove noisy segments from sentences is a crucial task. In addition, retained content needs to be fluent enough to maintain semantic coherence and interpretability. In this work, we propose a novel rationale extraction framework named RE2, which leverages two continuity and sparsity factors to obtain relevant and coherent rationales from sentences. To solve the problem that the gold rationales are not labeled, RE2 applies an optimizable binary mask to each token in the sentence, and adjust the rationales that need to be selected according to the relation label. Experiments on four datasets show that RE2 surpasses baselines.},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Hu, Xuming and Hong, Zhaochen and Zhang, Chenwei and King, Irwin and Yu, Philip},
	month = jul,
	year = {2023},
	keywords = {Extractive, continuous rationale extraction, relation extraction},
	pages = {2436--2440},
}

@misc{huang_can_2023,
	title = {Can {Large} {Language} {Models} {Explain} {Themselves}? {A} {Study} of {LLM}-{Generated} {Self}-{Explanations}},
	shorttitle = {Can {Large} {Language} {Models} {Explain} {Themselves}?},
	url = {http://arxiv.org/abs/2310.11207},
	doi = {10.48550/arXiv.2310.11207},
	abstract = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{jacovi_towards_2020,
	title = {Towards {Faithfully} {Interpretable} {NLP} {Systems}: {How} should we define and evaluate faithfulness?},
	shorttitle = {Towards {Faithfully} {Interpretable} {NLP} {Systems}},
	url = {http://arxiv.org/abs/2004.03685},
	abstract = {With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is "defined" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.},
	urldate = {2021-01-19},
	journal = {arXiv:2004.03685 [cs]},
	author = {Jacovi, Alon and Goldberg, Yoav},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03685},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lanham_measuring_2023,
	title = {Measuring {Faithfulness} in {Chain}-of-{Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2307.13702},
	doi = {10.48550/arXiv.2307.13702},
	abstract = {Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Lukošiūtė, Kamilė and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and Telleen-Lawton, Timothy and Hume, Tristan and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
	month = jul,
	year = {2023},
	note = {arXiv:2307.13702 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{lei_rationalizing_2016,
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://arxiv.org/abs/1606.04155},
	urldate = {2016-11-14},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	year = {2016},
	note = {571 citations (Semantic Scholar/arXiv) [2022-08-26]},
	keywords = {Cited, Definitely read, Explanation technique, Interpretable ML evaluation technique, Must cite, No evaluation},
	pages = {107--117},
}

@misc{madsen_are_2024,
	title = {Are self-explanations from {Large} {Language} {Models} faithful?},
	url = {http://arxiv.org/abs/2401.07927},
	abstract = {Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Madsen, Andreas and Chandar, Sarath and Reddy, Siva},
	month = may,
	year = {2024},
	note = {arXiv:2401.07927},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{majumder_knowledge-grounded_2022,
	title = {Knowledge-{Grounded} {Self}-{Rationalization} via {Extractive} and {Natural} {Language} {Explanations}},
	url = {http://arxiv.org/abs/2106.13876},
	doi = {10.48550/arXiv.2106.13876},
	abstract = {Models that generate extractive rationales (i.e., subsets of features) or natural language explanations (NLEs) for their predictions are important for explainable AI. While an extractive rationale provides a quick view of the features most responsible for a prediction, an NLE allows for a comprehensive description of the decision-making process behind a prediction. However, current models that generate the best extractive rationales or NLEs often fall behind the state-of-the-art (SOTA) in terms of task performance. In this work, we bridge this gap by introducing RExC, a self-rationalizing framework that grounds its predictions and two complementary types of explanations (NLEs and extractive rationales) in background knowledge. Our framework improves over previous methods by: (i) reaching SOTA task performance while also providing explanations, (ii) providing two types of explanations, while existing models usually provide only one type, and (iii) beating by a large margin the previous SOTA in terms of quality of both types of explanations. Furthermore, a perturbation analysis in RExC shows a high degree of association between explanations and predictions, a necessary property of faithful explanations.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Majumder, Bodhisattwa Prasad and Camburu, Oana-Maria and Lukasiewicz, Thomas and McAuley, Julian},
	month = sep,
	year = {2022},
	note = {arXiv:2106.13876 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Extractive, Free text},
}

@inproceedings{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
	doi = {10.1145/2939672.2939778},
	language = {en},
	urldate = {2016-09-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	note = {1999 citations (Semantic Scholar/DOI) [2022-08-26]},
	keywords = {Cited, Definitely read, Explanation technique, Human evaluation, Interpretable ML evaluation technique, Key paper, Must cite, domain agnostic, evaluate overall model reliability, explanation by model parameters, images, local fidelity, model agnostic, text},
	pages = {1135--1144},
}

@misc{siegel_probabilities_2024,
	title = {The {Probabilities} {Also} {Matter}: {A} {More} {Faithful} {Metric} for {Faithfulness} of {Free}-{Text} {Explanations} in {Large} {Language} {Models}},
	shorttitle = {The {Probabilities} {Also} {Matter}},
	url = {http://arxiv.org/abs/2404.03189},
	doi = {10.48550/arXiv.2404.03189},
	abstract = {In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Siegel, Noah Y. and Camburu, Oana-Maria and Heess, Nicolas and Perez-Ortiz, Maria},
	month = jun,
	year = {2024},
	note = {arXiv:2404.03189 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{singh_rethinking_2024,
	title = {Rethinking {Interpretability} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.01761},
	abstract = {Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.},
	language = {en},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
	month = jan,
	year = {2024},
	note = {arXiv:2402.01761 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wei_chain_2022,
	title = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.},
	language = {en},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jun,
	year = {2022},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{wiegreffe_teach_2021,
	title = {Teach {Me} to {Explain}: {A} {Review} of {Datasets} for {Explainable} {Natural} {Language} {Processing}},
	shorttitle = {Teach {Me} to {Explain}},
	url = {http://arxiv.org/abs/2102.12060},
	abstract = {Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wiegreffe, Sarah and Marasović, Ana},
	month = dec,
	year = {2021},
	note = {arXiv:2102.12060 [cs]},
	keywords = {Cited, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{zhu_explanation_2024,
	address = {Mexico City, Mexico},
	title = {Explanation in the {Era} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.naacl-tutorials.3},
	abstract = {Explanation has long been a part of communications, where humans use language to elucidate each other and transmit information about the mechanisms of events. There have been numerous works that study the structures of the explanations and their utility to humans. At the same time, explanation relates to a collection of research directions in natural language processing (and more broadly, computer vision and machine learning) where researchers develop computational approaches to explain the (usually deep neural network) models. Explanation has received rising attention. In recent months, the advance of large language models (LLMs) provides unprecedented opportunities to leverage their reasoning abilities, both as tools to produce explanations and as the subjects of explanation analysis. On the other hand, the sheer sizes and the opaque nature of LLMs introduce challenges to the explanation methods. In this tutorial, we intend to review these opportunities and challenges of explanations in the era of LLMs, connect lines of research previously studied by different research groups, and hopefully spark thoughts of new research directions},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 5: {Tutorial} {Abstracts})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Zining and Chen, Hanjie and Ye, Xi and Lyu, Qing and Tan, Chenhao and Marasovic, Ana and Wiegreffe, Sarah},
	editor = {Zhang, Rui and Schneider, Nathan and Chaturvedi, Snigdha},
	month = jun,
	year = {2024},
	pages = {19--25},
}

