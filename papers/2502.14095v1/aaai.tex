%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}


\usepackage[T1]{fontenc}
% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%Packages ported from ACL paper
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{comment}
\usepackage[
disable
]{endfloat}

%Leave the setcounter for section number depth commented out and set at 0 unless you want to add section numbers to your paper. If you do add section numbers, you must uncomment this line and change the number to 1 (for section numbers), or 2 (for section and subsection numbers). The style file will not work properly with numbering of subsubsections, so do not use a number higher than 2.
\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning}
\author {
    % Authors
    Karl Elbakian\textsuperscript{\rm 1},
    Samuel Carton\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}University of New Hampshire\\
    karl.elbakian@unh.edu, samuel.carton@unh.edu
}

\begin{document}
\maketitle
\begin{abstract}
% We analyze the effect of extractive rationalization on large language model prediction in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with rationalization errors with respect to gold-standard human-annotated extractive rationales for five datasets. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and model rationalization, and that while it is difficult to modify model rationalization via exemplars, it is easier to manipulate model interpretation of rationalized evidence in this manner, leading to contradictory results in some cases.
A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language models in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation error--a hopeful sign for downstream applications built on this mechanism.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
\begin{links}
    \link{Code}{https://github.com/kelbakian/llm-rationale-fidelity}
    % \link{Datasets}{https://aaai.org/example/datasets}
    % \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}


%\section{Preparing an Anonymous Submission}
%This document details the formatting requirements for anonymous submissions. The requirements are the same as for camera ready papers but with a few notable differences:
%\begin{itemize}
    %\item Anonymous submissions must not include the author names and affiliations. Write ``Anonymous Submission'' as the ``sole author'' and leave the affiliations empty.
    %\item The PDF document's metadata should be cleared with a metadata-cleaning tool before submitting it. This is to prevent leaked information from revealing your identity.
    %\item References must be anonymized whenever the reader can infer that they are to the authors' previous work.
    %\item AAAI's copyright notice should not be included as a footer in the first page.
    %\item Only the PDF version is required at this stage. No source versions will be requested, nor any copyright transfer form.
%\end{itemize}

%You can remove the copyright notice and ensure that your names aren't shown by including \texttt{submission} option when loading the \texttt{aaai25} package:
%\begin{quote}\begin{scriptsize}\begin{verbatim}
%\documentclass[letterpaper]{article}
%\usepackage[submission]{aaai25}
%\end{verbatim}\end{scriptsize}\end{quote}

%\section{Copyright}
%All papers submitted for publication by AAAI Press must be accompanied by a valid signed copyright form. They must also contain the AAAI copyright notice at the bottom of the first page of the paper. There are no exceptions to these requirements. If you fail to provide us with a signed copyright form or disable the copyright notice, we will be unable to publish your paper. There are \textbf{no exceptions} to this policy. You will find a PDF version of the AAAI copyright form in the AAAI AuthorKit. Please see the specific instructions for your conference for submission details.

\section{Introduction}

AI alignment refers to the goal of ensuring that model output is aligned with human intents and values \cite{shen_large_2023,anwar_foundational_2024,shen_towards_2024}. One key element of alignment is \textbf{verification}, the ability to confirm that a model's predictions have indeed accorded with those intents and values. 
% AI interpretability plays a key role in pursuing this goal by offering methods to audit the reasoning underlying model predictions, and inspect them at these lower levels for signs of misalignment. 
% For better or worse, one of the major application areas for large language models (LMs) is in saving human labor by automating or assisting decision-making over input documents,
When we use models to automate or assist human-affecting tasks such as moderation \cite{kumar_watch_2024}, resume screening \cite{gan_application_2024}, grading \cite{pinto_large_2023}, or medical decision-making \cite{thirunavukarasu_large_2023}, we want a human auditor to be able to review their decisions for mistakes or pathologies of behavior such as bias or the use of spurious evidence. 
% we want them to result from at least the facsimile of a reasoned process that can be inspected, verified, and, if necessary, contested by a human auditor.
% Interpretability offers tool for performing this auditing. 
Verification has traditionally been one of the major goals of model interpretability \cite{fok_search_2023}. Implicitly, the assumption underlying this function is that it is easier for a human auditor to catch model mistakes at the explanation level and propagate them upward to an appropriate skepticism about the model's overall prediction, than to inspect that prediction alone.

With the rise of large language models (LMs), the discourse on AI interpretability has turned towards methods that take advantage of their emergent capabilities. Free-text explanations can provide clear, comprehensible, and interactive descriptions of why an LM made a certain prediction \cite{singh_rethinking_2024}, while work in LM reasoning such as Chain-of-Thought \cite{wei_chain_2022} and its descendants force the model to break its reasoning into discrete steps which can be individually inspected. 
% Mechanistic interpretability seeks to identify latent algorithmic ``circuits'' encoded in the parameters of the model \cite{olah_feature_2017,wang_interpretability_2022}. 
What unites these approaches is that they are \textbf{abstractive}, synthesizing the raw input into a concise and human-comprehensible form. 

By contrast, \textbf{extractive} approaches to interpretability such as rationale models \cite{lei_rationalizing_2016} or LIME \cite{ribeiro_why_2016} have fallen somewhat out of vogue because of their inexpressiveness compared to abstractive approaches \cite{siegel_probabilities_2024,hu_think_2023}, as well as the parametric and/or computational overhead they add to models which are already large, unwieldy, and in some cases accessible via API only. However, some recent work has examined the ability of LMs to generatively mimic extractive explanations, or ``self-rationalize'', via prompting, finding them of comparable quality to traditional methods \cite{huang_can_2023}. 

\begin{figure}[h]
% \vspace{-10pt}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/explanation_example.pdf}
    % \includegraphics[scale=1]{figures/explanatory_prediction_example.pdf}
    \caption{Artificial examples of abstractive and extractive explanations for an erroneous moderation prediction. Only the extractive explanation provides a basis for refuting it. }
    \label{fig:explanation_example}
\end{figure}


However, when we want to verify an LM's prediction over an input document, there are basic questions that are more appropriately answered by extractive approaches than abstractive ones. Namely: \textbf{what evidence from the input document is the model using as the basis for its decision, and does that evidence support its predicted label?} Fully abstractive explanations don't directly elucidate the relationship between the within-text evidence and label, and can in fact obfuscate it. Figure \ref{fig:explanation_example} shows a simple example where abstractive explanations of a moderation decision are inadequate for refuting it. 
% To verify or refute a model prediction over an input document, we must know what evidence within the document the model used, and judge whether that evidence justifies that prediction. 
% No matter how fluent and persuasive a free-text explanation the model can construct, its prediction must ultimately be grounded in the content of the input document, and if we want to verify that prediction we must be able to assess whether it is supported by that content. A free-text explanation may help clarify or contextualize that evidence-prediction relationship, but it may also misrepresent it, as it is just as liable to hallucination as any other LM output, and may not be faithful to the original prediction \cite{lyu_towards_2024}. 

The problem of verification is related to the problem of faithfulness \cite{jacovi_towards_2020}, the idea that a model's explanation should be coupled with, and thus display the true underlying logic of, its prediction. Prompt-based explanations, abstractive or extractive, are intrinsically unfaithful by this definition, as they are just a proximal generation by the model alongside the label \cite{lyu_faithful_2023, turpin_language_2023}. For the purpose of verification, however, we are less interested in answering ``what does the explanation tell us about the model's prediction?'' than ``what does the correctness of the model's explanation tell us about the correctness of its prediction?'' Analogous to the concept of internal vs. external validity in experimental design \cite{mcdermott_internal_2011}, these two questions can be viewed as \textbf{internal vs. external faithfulness}, respectively. 


In this paper we examine the external faithfulness of prompt-based self-rationalization in two prominent large proprietrary language models: GPT-4 \cite{openai_gpt-4_2023} and Gemini \cite{team_gemini_2023}. Our high-level research question is: \textbf{Can large language models reliably and meaningfully identify within-document evidence for their predictions?} With an eye towards developing downstream human-model interaction systems based on this mechanism, we investigate the following specific questions: 
(1) Can LMs reliably quote evidence snippets from an input document without mistakes?;
(2) Does forcing self-rationalization impact model accuracy?;
(3) Does operation order (explain-then-predict versus predict-then-explain) affect performance?;
(4) Are label prediction errors correlated with evidence retrieval errors, relative to human gold-standard evidence?;
(5) What types of evidence retrieval errors cause label prediction errors?;
(6) Under what circumstances do LMs fail to retrieve evidence?
These last two questions are of key importance for the design of any human-model collaborative system based on extractive evidence as an underlying mechanism. If the characteristic failure mode for the model is to identify the correct pertinent evidence and then simply to misinterpret it, this is much more correctable, via either prompt engineering approaches such as self-consistency \cite{wang_self-consistency_2023} or human inspection, than if errors stem from missing key evidence entirely. We focus exclusively on \textbf{extractive} self-rationalization for the reasons outlined above: they are easier to assess for correctness (and thus faithfulness), and invite fewer pitfalls as a verification mechanism. 


We experiment with five datasets for which gold standard human-annotated extractive evidence is available, covering a wide range of tasks: MultiRC \cite{khashabi_looking_2018}, SciFact \cite{wadden_scifact-open_2022}, WikiAttack \cite{carton_extractive_2018}, Evidence Inference \cite{deyoung_evidence_2020}, and HealthFC \cite{vladika_healthfc_2024}. For a sample of each dataset, we run experiments prompting the model to self-rationalize label predictions under varying conditions, comparing and contrasting the result to the gold-standard evidence in each dataset. We find broadly that (1) these  models \textbf{can} reliably quote within-text evidence; (2) self-rationalization mostly does \textbf{not} effect label accuracy,  (3) operation order does \textbf{not} matter; (4) label prediction error \textbf{is} highly correlated with evidence error for most datasets; (5) label error is more commonly linked with capturing confounding evidence rather than missing key evidence; and finally (6) missing key evidence is mostly commonly linked to the presence of redundant evidence rather than more intractable interpretation issues. All of these are positive outcomes, suggesting the potential for LM extractive self-rationalization as a mechanism for powering downstream applications. All code and experimental results can be found in our github repository.



\section{Related Work}

% \paragraph{Interpretability in large language models.}


Recent interpretability work has tended to focus on abstractive approaches such as posthoc free-text explanations \cite{singh_rethinking_2024, zhu_explanation_2024}, or explanations as a byproduct of explicit reasoning processes like Chain-of-Thought \cite{wei_chain_2022, lanham_measuring_2023}. However, one of the major goals of interpretability is verification \cite{fok_search_2023}, the implicit assumption being that it is easier to recognize an erroneous explanation than an erroneous label. This mode requires explanations to be faithful \cite{jacovi_towards_2020} to the overall prediction, but this quality is difficult to measure in abstractive approaches \cite{agarwal_faithfulness_2024, siegel_probabilities_2024}.

Extractive approaches are less problematic in this regard because they directly attribute the prediction to evidence within the input. Even if this evidence is not technically faithful to the model's prediction, an observer can still assess whether it truly supports the label without being potentially beguiled by misleading abstractive generations of the model. Traditional approaches to identifying extractive evidence, such as the rationale model architecture \cite{lei_rationalizing_2016} or the LIME perturbation method \cite{ribeiro_why_2016}, add impractical levels of computational overhead to already large models, so recent work has investigated whether LMs can be prompted to produce such attributions as a generative output \cite{huang_can_2023,hu_think_2023, majumder_knowledge-grounded_2022}. An especially relevant recent work is \cite{madsen_are_2024}, which investigates the internal faithfulness of several types of extractive explanations produced by three open LMs. Our approach follows this work in directly prompting LMs to produce extractive evidence for their predictions. To assess whether LMs can identify the ``correct'' evidence, we use datasets with gold-standard evidence annotations. The ERASER collection \cite{deyoung_eraser_2019} includes a number of these datasets, and \cite{wiegreffe_teach_2021} surveys yet more. 

Finally, our goal of retrieving relevant evidence buried in potentially long documents is similar to that of``needle-in-the-haystack'' evaluations \cite{dhinakaran_needle_2024}, which ask questions about evidence manually inserted into a long context. Where we differ is in applying this approach to naturally-occurring evidence that the model may not be able to properly interpret, rather than artificially-inserted evidence the model is assumed to be able to comprehend if it can find it. 



\section{Datasets}

We analyze five datasets for which gold-standard human-annotated extractive evidence is included alongside ground truth labels: MultiRC \cite{khashabi_looking_2018}, SciFact \cite{wadden_scifact-open_2022}, and WikiAttack \cite{carton_extractive_2018}, Evidence Inference \cite{deyoung_evidence_2020}, and HealthFC \cite{vladika_healthfc_2024}. To reduce API access costs, we perform our analysis on randomly-sampled 300-item subsets of the development set for each dataset.
% except WikiAttack.

% We evaluate the effects of various few-shot exemplar constructions on two ERASER benchmark datasets - SciFact and MultiRC, as well as the WikiAttack dataset.
 
\paragraph{SciFact (SF)}
\cite{wadden_scifact-open_2022} is a scientific claim verification dataset, involving identifying whether abstracts from the research literature either support or refute a given scientific claim. The dataset contains 1,400 expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and sentence-level rationales. Rationales are direct spans of text from the document which support the class label given. Each claim has a class label of \textit{SUPPORT}, \textit{NO INFO}, or \textit{CONTRADICT}. 

\paragraph{MultiRC (MRC)}
\cite{khashabi_looking_2018} is a reading comprehension dataset containing sets of short paragraphs and questions that depend on information from multiple sentences within the paragraph. Each paragraph-question pair contains five answers, with a variable number of correct answer-options. Additionally, answer-options do not have to be a span from the text. The dataset contains sentence-level rationales in the form of relevant sections of the paragraph, as well as \textit{True} or \textit{False} labels for each answer candidate.


\paragraph{WikiAttack (WA)}
\cite{carton_extractive_2018} contains excerpts of Wikipedia comment threads original included in the WikiToxic dataset of \cite{wulczyn_ex_2017} which are scanned for instances of personal attacks or harassment. Class labels are given as either \textit{attack} or \textit{nonattack}, and rationales for comments labeled as attack are given in the form of spans from the comment.

\paragraph{Evidence Inference (EI)}
\cite{deyoung_evidence_2020} dataset contains scientific articles and queries asking to compare the relative effectiveness of two treatments with respect to a given outcome. Labels include \textit{significantly increased}, \textit{significantly decreased}, and \textit{no significant difference}. Annotations are in the form of quotes from the text which support the label.

\paragraph{HealthFC (HFC)}
\cite{vladika_healthfc_2024} is a medical domain QA dataset. Given a health-related claim, verdicts are decided from a systematic review or clinical trial document, as one of \textit{supported}, \textit{refuted}, and \textit{not enough information}. Additionally, annotations containing up to five quotes from the document are provided. After reviewing the dataset, we found discrepancies between the original, expert-annotated, German verdicts, and their English label mappings. Thus, we performed a re-labeling of the dataset, first by prompting GPT-4 to generate an English label given the query and German verdict, and then by manually labeling each item in the dataset, using a holistic review of the original English label, German verdict, cited evidence, and full document text. The adjusted labels are included in our repository. 



\begin{figure}[]
% \vspace{-10pt}
    \centering
    \includegraphics[width=0.474\textwidth]{figures/scifact_prompt_example.pdf}
    % \includegraphics[scale=1]{figures/explanatory_prediction_example.pdf}
    \caption{Two examples of GPT-4 prompts on the same SciFact item. Model output highlighted in green. Human-annotated evidence underlined, claim bolded. The model misses the evidence and mislabels the document in the predict-then-explain setting, but correctly labels it when the evidence is provided, even without its surrounding context.}
    \label{fig:scifact_prompt_example}
\end{figure}

\section{Implementation Details}

% We investigate six research questions: (1) Can LMs reliably quote evidence from the input evidence without mistakes?; (2) Does forcing self-rationalization impact model performance?; (3) Does operation order (explain-then-predict vs. predict-then-explain) impact model performance?; (4) Is evidence retrieval error correlated with label prediction error?; (5) What types of evidence retrieval error are most associated with label prediction error?; (6) Under what circumstances does the model fail to retrieve pertinent evidence?

% We address the questions by comparing the behavior of the models under various conditions with or without self-rationalization. All conditions are few-shot unless noted otherwise. They include the following: (1) zero-shot label prediction without self-rationalization; (2) few-shot label prediction without self-rationalization; (3) explain-then-predict prompting for label and rationale prediction; (4) predict-then-explain for label and rationale prediction; (5) human-annotated evidence provided for label prediction; (6) human-annotated evidence provided without the document text for label prediction; (7) human-annotated evidence redacted from the document for label prediction. For cost reasons, we use only a 300-item sample drawn from the validation set of each dataset, one trial per item. 
% We investigate (1) how forcing an LLM to extractively rationalize its prediction affects its performance, and (2) the relationship between rationalization and prediction error. 
%ACL cut: , and (3)  how exemplar choice can affect rationalization behavior. We seek to manipulate model rationalization behavior by selecting exemplars based on two properties:  \textbf{claim-evidence similarity} and \textbf{rationale position}, exploring how exemplar selection emphasizing these qualities affects model rationalization and prediction behavior. 

% Some rationalization error seem to be attributable to the model being unable to recognize the within-document evidence (in the form of the human-annotated rationale) as being relevant to the claim. Fig. \ref{fig:scifact_prompt_example}A shows an example, where GPT-4 is unable to recognize evidence referring to ``phenotypes'' as being relevant to a claim concerning ``gene expression''. 


% Interestingly, when shown only the true evidence (human-annotated rationale), the model is able to correctly classify the claim (Fig. \ref{fig:scifact_prompt_example}B). Furthermore, it is actually able to recognize that evidence (and classify the claim correctly) when the exemplars are removed (Fig. \ref{fig:scifact_prompt_example}C). This begs the question: \textbf{what property of the exemplars is causing them to poison the model's ability to identify the relevant evidence and thus correctly classify the claim?}

% \paragraph{Claim-evidence similarity.} One possibility is claim-evidence similarity. If the exemplars inadvertently demonstrate to the model that claims tend to use similar words as the relevant evidence, this may compromise its ability to find more dissimilar evidence. We measure claim-evidence similarity for true evidence by producing vector embeddings for every claim and human-annotated rationale in each dataset, and calculating the cosine distance between these embeddings. We use the OpenAI Embeddings API to produce these embeddings, specifically the ``text-embedding-3-small'' model. We define ``low-similarity'' instances as those with higher-than-average claim-evidence cosine distance for a particular dataset, and ``high-similarity items'' as those with lower-than-average distance. 

% For the random exemplars used in Fig. \ref{fig:scifact_prompt_example}A, we indeed find a mean claim-evidence cosine distance of 0.33, compared to 0.41 for the unseen example. 



% \paragraph{Rationale position.} Another possibility is the position of the evidence within the text. The human-annotated rationales for both exemplars used in Fig. \ref{fig:scifact_prompt_example}A are  ``backloaded'', occurring toward the end of of their document, while the true evidence for the unseen document is ``frontloaded'', occurring at the beginning. So the exemplars may be teaching the model that evidence occurs toward the end of the document, compromising its ability to identify evidence toward the beginning. 

% We measure evidence position by calculating the average position of rationale tokens as a fraction of total text length. A ``frontloaded'' item is one where the average rationale position is below 0.5, while for a ``backloaded'' item it is above 0.5. 

% \paragraph{Experimental conditions.} Our full experiment includes six few-shot learning conditions, and one zero-shot condition: (1) zero-shot for label prediction; (2) few-shot for label prediction; (3) explain-then-predict for label and rationale prediction; (4) predict-then-explain for label and rationale prediction; (5) ground-truth evidence provided for label prediction; (6) ground-truth evidence providied without the document text for label prediction; (7) ground-truth evidence redacted from the document for label prediction.

%CUT in favor of numbered list: . We first assess model label prediction capabilities using zero-shot and few-shot prompts. Next, we assess both label and rationalization capability through two conditions, either asking the model to rationalize then predict, or vice-versa. The final three experiments assess label prediction performance when (1) ground-truth evidence is given with the document, (2) only ground-truth evidence is provided (without the document), and (3) ground-truth evidence is redacted from the input document.
%(1) zero-shot for label prediction; (2) few-shot for label prediction; (3) explain-then-predict for label and rationale prediction; (4) predict-then-explain for label and rationale prediction; (5) ground-truth evidence provided for label prediction; (6) ground-truth evidence providied without the document text for label prediction; (7) ground-truth evidence redacted from the document for label prediction

%ACL cut: Our full experiment includes six few-shot learning conditions, five of which include extractive rationalization: (1) randomly-sampled exemplars without rationales;  (2) randomly-sampled  exemplars; (3) low claim-evidence similarity exemplars; (4) high claim-evidence similarity exemplars; (5) exemplars with frontloaded evidence; and (6) exemplars with backloaded evidence. 





% \section{Implementation details}

\paragraph{Model details.} 
Because of their to-date superior few-shot performance compared to open models, we focus on two closed proprietary models: GPT-4 and Gemini. 
For GPT-4, the \textit{gpt-4-0613} version is used for MultiRC, WikiAttack, and SciFact. Because of long input lengths, \textit{gpt-4-turbo} is used for Evidence Inference and HealthFC. All GPT model temperatures are set to the default 0.7. For Gemini, the \textit{gemini 1.5 pro} model version is used for all datasets, with temperature set to the default 1.0 
%ACL cut: For the fine-tuned baseline, we use the RoBERTa-large \cite{liu_roberta_2019} implementation available on Hugging Face, training for 10  epochs with early stopping. 

% \paragraph{Self-Rationalized prediction.} Figure \ref{fig:scifact_prompt_example}A illustrates the prompt used to perform rationalized prediction. We produce predictions and extractive rationales concurrently in an ``explain-and-predict'' manner, introducing the possibility of unfaithful rationales with no actual relationship to the model's label prediction. Part of our analysis examines this relationship empirically. The prompt presents sampled exemplars in JSON format, and requests model output in this format. 



\paragraph{Exemplar sampling.} All few-shot conditions involve sampling two exemplars for each possible class, resulting in 4-shot learning for MultiRC and WikiAttack, and 6-shot learning for SciFact, Evidence Inference, and HealthFC. We randomly sample and shuffle exemplars independently for each item. For MultiRC, SciFact, Evidence Inference, and HealthFC, exemplars are sampled from the training set. For WikiAttack, where human rationales are unavailable for the training set, they are instead sampled from the development set, resulting in effectively 250-fold cross-validation.


\paragraph{Experimental conditions and prompting.} 

We compare model performance across 7 different prompting conditions: (1) \textbf{zero-shot}, with neither exemplars nor self-rationalization; (2) standard \textbf{few-shot} with no rationalization; (3) \textbf{predict then explain}, where the model is asked to first make a label prediction then provide evidence to justify its answer; (4) \textbf{explain then predict}, where the model is asked the reverse order of the former condition; (5) \textbf{evidence given}, in which the human-annotated evidence is provided as part of the input prompt; (6) \textbf{evidence given without document}, where the human annotated evidence is provided without the context of the surrounding document; and (7) \textbf{evidence occluded}, where the human-annotated evidence is removed without replacement from the document. 

Fig. \ref{fig:scifact_prompt_example} shows example prompts for the ``predict-then-explain'' and ``evidence given without document'' conditions. Model output is solicited in \texttt{JSON} format using \texttt{<claim>} and \texttt{<doc>} tags to denote different elements of the input.  Prompt scaffolds can be found in our repository. 


\section{Results}

% In order to fully explore the effect of imposed rationalization and the relationship between rationalization and prediction performance, we evaluate up-to-date versions of both a closed model (GPT-4) and an open-weight model (Gemini-1.5-pro), 
% Across our analysis, we use several prompting conditions: (1) \textbf{zero-shot}, with neither exemplars nor self-rationalization; (2) standard \textbf{few-shot} with no rationalization; (3) \textbf{predict then explain}, where the model is asked to first make a label prediction then provide evidence to justify its answer; (4) \textbf{explain then predict}, where the model is asked the reverse order of the former condition; (5) \textbf{evidence given}, in which the human-annotated evidence is provided as part of the input prompt; (6) \textbf{evidence given without document}, where the human annotated evidence is provided without the context of the surrounding document; and (7) \textbf{evidence occluded}, where the human-annotated evidence is removed without replacement from the document. 




% We address our six research questions in order:






%\subsubsection{Can language models reliably quote evidence from an input document?}
\begin{table}[t]
\centering
\begin{tabular}{ccc}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Failure Rate (\%)}} \\ \cline{2-3} 
                                  & GPT-4     & Gemini-1.5      \\ \hline
MRC                               & 2.00              & 2.33                    \\ 
SF                                & 0.33               & 0.00                       \\ 
WA                                & 0.00               & 4.33                    \\ 
EI                                & 40.33              & 25.0                   \\ 
HFC                               & 15.33               & 0.33                    \\ \hline
\end{tabular}
\caption{Model evidence quoting failure rates in the explain-then-predict condition}
\label{tab:fail_rate_tab}
\end{table}














%\subsection{Can language models reliably quote evidence from an input document?}

\begin{table}[h]
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Condition}}                                      & \multicolumn{2}{c}{\textbf{Label accuracy}} \\ \cline{3-4} 
                                  &                                                                          & \textbf{GPT-4}  & \textbf{Gemini-1.5}  \\ \hline
MRC                               & Zero-shot                                                                & 0.887           & 0.840                \\
                                  & Few-shot                                                                 & 0.893           & 0.892                \\
                                  & Predict then explain                                                     & 0.897           & 0.885                \\
                                  & Explain then predict                                                     & 0.861           & 0.874                \\
                                  & Evidence given                                                        & 0.887           & 0.885                \\
                                  & \begin{tabular}[c]{@{}c@{}}Evidence given\\ w/o document\end{tabular} & 0.827           & 0.797                \\
                                  & Evidence occluded                                                        & 0.663           & 0.666                \\ \hline
\multirow{7}{*}{SF}               & Zero-shot                                                                & 0.853           & 0.850                \\
                                  & Few-shot                                                                 & 0.843           & 0.820                \\
                                  & Predict then explain                                                     & 0.870           & 0.837                \\
                                  & Explain then predict                                                     & 0.833           & 0.841                \\
                                  & Evidence given                                                        & 0.947           & 0.947                \\
                                  & \begin{tabular}[c]{@{}c@{}}Evidence given\\ w/o document\end{tabular} & 0.907           & 0.890                \\
                                  & Evidence occluded                                                        & 0.633           & 0.587                \\ \hline
\multirow{7}{*}{WA}               & Zero-shot                                                                & 0.743           & 0.674                \\
                                  & Few-shot                                                                 & 0.777           & 0.703                \\
                                  & Predict then explain                                                     & 0.790           & 0.728                \\
                                  & Explain then predict                                                     & 0.743           & 0.690                \\
                                  & Evidence given                                                        & 0.813           & 0.832                \\
                                  & \begin{tabular}[c]{@{}c@{}}Evidence given\\ w/o document\end{tabular} & 0.917           & 0.901                \\
                                  & Evidence occluded                                                        & 0.533           & 0.563                \\ \hline
\multirow{7}{*}{EI}               & Zero-shot                                                                & 0.832           & 0.765                \\
                                  & Few-shot                                                                 & 0.809           & 0.767                \\
                                  & Predict then explain                                                     & 0.870           & 0.839                \\
                                  & Explain then predict                                                     & 0.888           & 0.831                \\
                                  & Evidence given                                                        & 0.907           & 0.880                \\
                                  & \begin{tabular}[c]{@{}c@{}}Evidence given\\ w/o document\end{tabular} & 0.920           & 0.893                \\
                                  & Evidence occluded                                                        & 0.721           & 0.698                \\ \hline
\multirow{7}{*}{HFC}              & Zero-shot                                                                & 0.763           & 0.803                \\
                                  & Few-shot                                                                 & 0.733           & 0.787                \\
                                  & Predict then explain                                                     & 0.835           & 0.809                \\
                                  & Explain then predict                                                     & 0.783           & 0.819                \\
                                  & Evidence given                                                        & 0.947           & 0.857                \\
                                  & \begin{tabular}[c]{@{}c@{}}Evidence given\\ w/o document\end{tabular} & 0.850           & 0.863                \\
                                  & Evidence occluded                                                        & 0.677           & 0.687                \\ \hline
\end{tabular}
\caption{Label Accuracy for different prompting paradigms}
\label{tab:label-acc-comparison}
\end{table}











\begin{table*}[t]
\centering
\begin{tabular}{cccccccccc}
\hline
                                                      & \multicolumn{1}{l}{\textbf{}}                       & \multicolumn{4}{c}{\textbf{Correct predictions}}                                 & \multicolumn{4}{c}{\textbf{Incorrect predictions}}                               \\ \hline
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Dataset}}} & \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Model}}} & \multirow{2}{*}{\textbf{Count}} & \multicolumn{3}{c}{\textbf{Evidence}}         & \multirow{2}{*}{\textbf{Count}} & \multicolumn{3}{c}{\textbf{Evidence}}         \\ \cline{4-6} \cline{8-10} 
\multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                &                                 & \textbf{F1} & \textbf{Recall} & \textbf{Prec.} &                                 & \textbf{F1} & \textbf{Recall} & \textbf{Prec.} \\ \hline
\multirow{2}{*}{MRC}                                  & GPT-4                                               & 253                             & 0.71        & 0.68            & 0.84           & 41                              & 0.59**      & 0.62            & 0.63***        \\
                                                      & Gemini-1.5                                          & 256                             & 0.67        & 0.63            & 0.86           & 37                              & 0.67        & 0.53            & 0.67           \\ \hline
\multirow{2}{*}{SF}                                   & GPT-4                                               & 249                             & 0.76        & 0.77            & 0.84           & 50                              & 0.11***     & 0.75            & 0.12***        \\
                                                      & Gemini-1.5                                          & 252                             & 0.80        & 0.77            & 0.86           & 48                              & 0.14***        & 0.60            & 0.45           \\ \hline
\multirow{2}{*}{WA}                                   & GPT-4                                               & 223                             & 0.85        & 0.94            & 0.85           & 77                              & 0.15***     & 0.92            & 0.18***        \\
                                                      & Gemini-1.5                                          & 198                             & 0.82        & 0.89            & 0.87           & 89                              & 0.16***        & 0.90            & 0.23           \\ \hline
\multirow{2}{*}{EI}                                   & GPT-4                                               & 159                             & 0.62        & 0.62            & 0.69           & 20                              & 0.38***     & 0.33***         & 0.48**         \\
                                                      & Gemini-1.5                                          & 187                             & 0.83        & 0.61            & 0.63           & 38                              & 0.81        & 0.41            & 0.39           \\ \hline
\multirow{2}{*}{HFC}                                  & GPT-4                                               & 199                             & 0.37        & 0.38            & 0.44           & 55                              & 0.29        & 0.29            & 0.36           \\
                                                      & Gemini-1.5                                          & 245                             & 0.53        & 0.38            & 0.48           & 54                              & 0.53        & 0.35            & 0.36           \\ \hline
\end{tabular}
\caption{Evidence rationalization performance for correct vs incorrect label predictions in the \textit{explain then predict} condition. Asterisks denote significant differences between values using a 2-sided t-test; *:\textit{p}\textless0.1; **:\textit{p}\textless0.05; ***\textit{p}\textless0.005}
\label{tab:correctness_table}
\end{table*}

 \begin{figure*}[h]
    \centering
    \includegraphics[width=.8\textwidth]{ figures/aaai_boxplots.pdf}
    %\includegraphics[width=.8\textwidth]{ figures/emnlp_control_boxplots.pdf}
    % \includegraphics[scale=1]{figures/explanatory_prediction_example.pdf}
    \caption{Box-and-whisker plots of label prediction error versus mean predicted evidence rationale F1 for all five datasets in the \textit{explain then predict} condition.}
    \label{fig:label_rationale_whisker_plots}
 \end{figure*}

\subsection{Can language models reliably quote evidence from the input document?}
Table \ref{tab:fail_rate_tab} shows self-rationalization failure rates across the explain-then-predict condition, where the model fails to return any valid quote from an input text, using string substring matching. 
We observe mostly very low error rates, with the two high error rates for GPT-4 are associated with the \texttt{GPT-4 turbo} variant model, which is known to be weaker than the base model. Gemini also shows a high rate of error for Evidence Inference. The results show that, by and large, the strongest contemporary models can reliably quote evidence from the input document. Evidence Inference is the outlier, possibly due to long context lengths. 


\subsection{Does self-rationalization impact model accuracy?}
\label{subsec:explaining_errors}
Table \ref{tab:label-acc-comparison} summarizes the overall label prediction accuracy results for all conditions. For two datasets, Evidence Inference and HealthFC, forcing the model to support its prediction has a substantial positive effect (+7.9/5.0 for GPT-4 and +6.4/3.2 for Gemini-1.5, respectively). This effect is marginal for the other three datasets, even sometimes having a negative effect. This effect interestingly is the opposite of the failure rate results, suggesting that longer documents benefit more from this prompting requirement. 
% Next, all datasets except for MultiRC see an increase in performance when provided with the human-annotated evidence for both models. This effect is more pronounced for Gemini-1.5, except in the case of HealthFC. Lastly, we note a uniformly sharp decrease in performance across all datasets when the human-annotated evidence is redacted from the input document.


%\subsubsection{Effect of prompt operation order}
\subsection{Effect of prompt operation order}
Table \ref{tab:label-acc-comparison} also provides a comparison between asking the model to first rationalize its prediction (\textit{explain then predict}) and then provide a label, versus rationalizing after making its label prediction (\textit{predict then explain}). Using GPT-4, for MultiRC, SciFact, and HealthFC, changing the request order caused a significant difference in the label prediction performance while Gemini-1.5 showed no significant differences. Using GPT-4, the predictive performance difference is mostly in favor of the \textit{predict then explain} condition, suggesting that in most cases constraining the model to pre-emptively justify its label has a slightly negative impact on its label accuracy. 
% Interestingly, the Gemini-1.5 model did not exhibit this pattern; in all cases except for WikiAttack, the differences between the two conditions were marginal. 



% Therefore, we observe mixed results across datasets from the effect of both forcing the model to provide evidence for its predictions, and providing human-annotated evidence to the model in the input prompt. However, comparing performance across these conditions allows us to begin to understand the underlying cause of model prediction and evidence retrieval errors.

\begin{table*}[]
\centering
\begin{tabular}{lcrrrr}
\hline
\multicolumn{2}{l}{\textbf{Explain then predict}} & \multicolumn{2}{c}{\textbf{Low evidence recall}}                                         & \multicolumn{2}{c}{\textbf{High evidence recall}}                                        \\ \hline
\multicolumn{2}{l}{\textbf{Evidence given}}       & \multicolumn{1}{c}{\textbf{Correct label}} & \multicolumn{1}{c}{\textbf{Incorrect label}} & \multicolumn{1}{c}{\textbf{Correct label}} & \multicolumn{1}{c}{\textbf{Incorrect label}} \\ \hline
\textbf{Dataset}                & \textbf{Model}              & \multicolumn{1}{c}{``Retrieval issue''}       & \multicolumn{1}{c}{``Unexplainable''}            & \multicolumn{1}{c}{``Confounding evidence''}         & \multicolumn{1}{c}{``Misinterpretation''}        \\ \hline
\multirow{2}{*}{MRC}         & GPT-4              & 0.12 (5)                                   & 0.24 (10)                                    & 0.22 (9)                                   & \textbf{0.41 (17)}                           \\
                             & Gemini-1.5         & 0.16 (6)                                   & 0.30 (11)                                    & 0.14 (5)                                   & \textbf{0.41 (15)}                           \\ \hline
\multirow{2}{*}{SF}          & GPT-4*             & 0.10 (5)                                   & 0.14 (7)                                     & \textbf{0.60 (30)}                         & 0.16 (8)                                     \\
                             & Gemini-1.5*        & 0.19 (9)                                   & 0.21 (10)                                    & \textbf{0.50 (24)}                         & 0.10 (5)                                     \\ \hline
\multirow{2}{*}{WA}          & GPT-4              & 0.00 (0)                                   & 0.06 (5)                                     & 0.34 (26)                                  & \textbf{0.60 (46)}                           \\
                             & Gemini-1.5         & 0.06 (5)                                   & 0.05 (4)                                     & \textbf{0.45 (40)}                         & 0.44 (39)                                    \\ \hline
\multirow{2}{*}{HFC}         & GPT-4*             & \textbf{0.73 (40)}                         & 0.07 (4)                                     & 0.13 (7)                                   & 0.07 (4)                                     \\
                             & Gemini-1.5         & \textbf{0.44 (24)}                         & 0.35 (19)                                    & 0.11 (6)                                   & 0.09 (5)                                     \\ \hline
\multirow{2}{*}{EI}          & GPT-4              & 0.26 (5)                                   & \textbf{0.32 (6)}                            & 0.16 (3)                                   & 0.26 (5)                                     \\
                             & Gemini-1.5*        & \textbf{0.39 (15)}                         & 0.16 (6)                                     & 0.16 (6)                                   & 0.29 (11)                                    \\ \hline
\end{tabular}
\caption{Contingency table of label prediction errors in \textit{explain then predict} condition; split by low vs. high evidence recall and correct vs. incorrect prediction in \textit{explanation given} condition. Asterisks denote significant differences via the Fisher exact test with p-value 0.05. An interpretation of each contingency is presented in quotes.}
\label{tab:label_error_reason_table}
\end{table*}

\subsection{Is evidence retrieval error correlated with label prediction error?}
\label{subsec:rationale_vs_label}

Table \ref{tab:correctness_table} shows the mean self-rationalization performance (compared to human-annotated evidence) for correct and incorrect label predictions made by each model, in the \textit{explain then predict} condition. Figure \ref{fig:label_rationale_whisker_plots} illustrates the distribution of rationale F1s under this condition. 

For GPT-4, there is a strong correlation between evidence F1 and label accuracy in SciFact, WikiAttack, and Evidence Inference, with respective evidence-F1 differences of 0.65, 0.70, and 0.24. It is likewise strong for Gemini-1.5 in SciFact and WikiAttack, with evidence-F1 differences of 0.66 for both. With respect to SciFact, hardly any incorrect predictions display any level of correct alignment. Again, this effect is consistent across both models.

This result suggests that for certain datasets (SciFact and WikiAttack), the primary challenge is in retrieving the correct evidence, and when this can be done the label can be predicted with high accuracy. There are others (MultiRC, Evidence Inference in the case of Gemini), where evidence retrieval is successful, and the challenge lies more in interpreting the correct answer from that evidence. HealthFC is an outlier, with both models performing poorly on evidence retrieval, for both correct and incorrect label predictions. 

%\subsubsection{Why does the model make prediction errors?}
\subsection{Why does the model make prediction errors?}
\label{subsec:why_prediction_errors}

When the model makes prediction errors, to what extent can we attribute these errors to missing evidence or misinterpretation of correct evidence? Is it more common for the model to miss key evidence entirely (which is difficult to correct via prompt engineering), or to misinterpret relevant evidence (which can be addressed by reinspection approaches)? Table \ref{tab:label_error_reason_table} explores these questions by breaking down label prediction errors made in the \textit{explain then predict} condition by (1) the evidence recall relative to human-annotated evidence in this condition, and (2) the model's label accuracy on those corresponding items on the \textit{evidence given} condition. This division lets us ask: \textbf{given that the model was wrong, did it identify the correct evidence, and would it still have been wrong if it had done so?}

In this division, the (low-recall, correct label) condition represents cases where the model's label prediction mistake would have been overturned by presenting it with the human rationale, meaning that we can attribute the mistake to the model having failed to retrieve the correct evidence. The (low-recall, incorrect label) condition represents cases where the model's mistake persists with or without the human rationale, making it impossible to attribute. The (high-recall, correct label) condition implies that there was extra confounding evidence the model picked up in addition to recovering the human rationale, which caused its label prediction error. Finally, the (high-recall, incorrect label) condition represents scenarios where the model successfully recovered the human rationale, but made the incorrect prediction regardless (misinterpretation). 

We find that in three of out five datasets (MultiRC, SciFact, and WikiAttack), label prediction errors are mostly associated with high rationale recall (misinterpretation and confounding evidence) rather than low recall (missing evidence). In MultiRC and WikiAttack, the model is most likely to recover the human-annotated evidence but be unable to interpret it properly, while for SciFact the model is likely to recover both the human rationale and extraneous evidence, which causes it to produce an incorrect prediction. For two datasets, HealthFC and Evidence Inference, a majority of prediction errors are associated with low evidence recall. In both models' cases, providing human evidence to the model would produce a correct label for HealthFC. For Evidence Inference, this remains true only for Gemini-1.5. This means that HealthFC is the only dataset for which we can attribute a majority of errors to missing key evidence that the model had the capacity to interpret. In other cases, the model is more likely to recover the relevant evidence and then misinterpret it.

\begin{table*}[]
\centering
\begin{tabular}{llrrrr}
\hline
\multicolumn{2}{l}{\textbf{Evidence occluded}}   & \multicolumn{2}{c}{\textbf{Label incorrect}}                                              & \multicolumn{2}{c}{\textbf{Label correct}}                                                \\ \hline
\multicolumn{2}{l}{\textbf{Evidence given}}      & \multicolumn{1}{c}{\textbf{Label correct}} & \multicolumn{1}{c}{\textbf{Label incorrect}} & \multicolumn{1}{c}{\textbf{Label correct}} & \multicolumn{1}{c}{\textbf{Label incorrect}} \\ \hline
\textbf{Note}        & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{``Retrieval issue''}          & \multicolumn{1}{c}{``Uninterpretable evidence''}          & \multicolumn{2}{c}{``Other viable evidence present''}                                         \\ \hline
\multirow{2}{*}{MRC} & GPT-4*                    & 0.22 (19)                                  & 0.14 (12)                                    & \textbf{0.60 (52)}                         & 0.05 (4)                                     \\
                     & Gemini-1.5                & 0.26 (29)                                  & 0.05 (6)                                     & \textbf{0.49 (55)}                         & 0.20 (22)                                    \\ \hline
\multirow{2}{*}{SF}  & GPT-4*                    & 0.37 (25)                                  & 0.10 (7)                                     & \textbf{0.51 (34)}                         & 0.01 (1)                                     \\
                     & Gemini-1.5*               & \textbf{0.42 (32)}                         & 0.09 (7)                                     & 0.41 (31)                                  & 0.08 (6)                                     \\ \hline
\multirow{2}{*}{WA}  & GPT-4                     & \textbf{0.47 (7)}                          & 0.20 (3)                                     & 0.13 (2)                                   & 0.20 (3)                                     \\
                     & Gemini-1.5                & \textbf{0.46 (13)}                         & 0.04 (1)                                     & 0.32 (9)                                   & 0.18 (5)                                     \\ \hline
\multirow{2}{*}{HFC} & GPT-4*                    & 0.30 (45)                                  & 0.01 (2)                                     & \textbf{0.65 (97)}                         & 0.04 (6)                                     \\
                     & Gemini-1.5*               & 0.21 (45)                                  & 0.06 (13)                                    & \textbf{0.63 (133)}                        & 0.09 (20)                                    \\ \hline
\multirow{2}{*}{EI}  & GPT-4*                    & 0.16 (8)                                   & 0.08 (4)                                     & \textbf{0.76 (38)}                         & 0.00 (0)                                     \\
                     & Gemini-1.5*               & 0.18 (11)                                  & 0.03 (2)                                     & \textbf{0.77 (48)}                         & 0.02 (1)                                     \\ \hline
\end{tabular}
\caption{Contingency table of low human rationale recall instances in \textit{explain then predict} condition; split by correct vs. incorrect label prediction in \textit{evidence occluded} and \textit{evidence given} conditions. Asterisks denote significant differences via the McNemar test with p-value 0.05. Intepretations provided in quotes.}
\label{tab:low_recall_reason_table}
\end{table*}


%\subsubsection{Why does the model miss key evidence?}
\subsection{Why does the model miss key evidence?}

Similar to Section \ref{subsec:why_prediction_errors}, we can use the results of different prompting paradigms to ask why the model misses key evidence. In particular, we can investigate the following hypothesis: \textbf{the model tends to miss evidence primarily when it is unable to interpret it correctly.}

Table \ref{tab:low_recall_reason_table} represents a contingency table of low-evidence-recall instances from the \textit{explain then predict} condition, divided by label prediction error in the \textit{evidence occluded} and \textit{evidence given} conditions. The (occluded incorrect, given correct) contingency represents cases where the human evidence was both necessary and sufficient for the model to correctly predict the label, while (occluded incorrect, given incorrect label) represents cases where the human evidence was not unnecessary, but also not sufficient to predict the label. Finally, either contingency where the label prediction was correct for the explanation-occluded condition is one where there was additional viable evidence beyond the human-annotated evidence, rendering it unnecessary, and low evidence recall less of a real error. Thus, the question being asked here is: \textbf{given that the model missed key evidence, was that evidence necessary and would it have been sufficient if it had been found?}

If misses of necessary human-annotated evidence were mostly associated with them being uninterpretable by the model, a majority of cases would fall in the (occluded incorrect, given incorrect) contingency. Instead, Table \ref{tab:low_recall_reason_table} shows that in a majority of cases, datasets fall into the (occluded correct, given correct) contingency, meaning that models mostly fail to retrieve human-annotated evidence when there is additional viable evidence that the model can use to successfully predict the label. 

Even when no such additional viable evidence is possible (incorrect label in the occluded condition), a majority of cases for all datasets are ones where the label is correct in the evidence-given condition, meaning the model does know how to correctly interpret the key evidence, it simply fails to retrieve it in the \textit{explain then predict} condition. In WikiAttack alone does this contingency represent a majority of overall cases. Hence, the hypothesis that retrieval failures are associated with evidence the model cannot interpret correctly is not supported in our analysis.

\section{Discussion}

Consider a human-model collaborative system such as an interpretable LM-driven moderation system, grounded in the mechanism of extractive self-rationalization. For verification to be possible in such a system, the LM must be able to consistently extract explanatory evidence from the input document, and that evidence should display external faithfulness, in that if it can be proven incorrect then the model's prediction should also be incorrect as well. As error modes go, it is better for the model to identify correct evidence and then misinterpret it than to miss evidence entirely, as this is more correctable via prompting or human inspection. And if the model is to miss evidence, it is better for it to be able to interpret it correctly if provided, as this could be supported by additional within-document retrieval support e.g. \cite{singhania_recall_2024}. The most irrecoverable outcome is the model missing evidence that it cannot interpret properly at all. 
% As error modes go, we prefer the more correctable retrieve-evidence-correctly-then-misinterpret mode over the miss-evidence-entirely mode. And if the model is going to miss evidence, we prefer for it to be able o 

The results shown above are largely supportive of these requirements. We find that models can mostly reliably quote evidence from the input, and that for at least some datasets, evidence retrieval performance is correlated with label prediction performance. We find that label errors are mostly associated with either confounding evidence or with missing evidence that could at least be interpreted correctly if it had been retrieved. Finally, when the model misses key evidence, we find it mostly associated with  the presence of other viable evidence, meaning that it is not truly missing evidence at all in most apparent cases. 

This is a hopeful outcome for any verification system based on self-rationalization. While the properties (and average document lengths) of the data in question has a major impact on the viability of this mechanism, there are at least some datasets for which it will tend to work well and potentially serve as the basis for such a system. Further work might develop an evaluation protocol to determine whether or not a given dataset falls into this category. 
% In order to be able to use extractive evidence to scrutinize and critique the reasoning by which large language models make their predictions, we first need to establish that there is a relationship between model prediction and alignment--that the predictions made by the model are conditioned on the evidence it identifies within the input text, and that this evidence matches a human's intuition. The results shown in Section \ref{subsec:rationale_vs_label} do support this hypothesis to a certain extent, showing a strong relationship between model prediction accuracy and model rationalization accuracy relative to a human-annotated gold standard, for four out of five datasets we investigate. 

% The analysis performed in Section \ref{subsec:explaining_errors} shows that prediction errors are more likely to be a result of imprecise rather than incomplete retrieval of evidence.
% This is a hopeful sign for any system seeking to allow human verification or automated calibration based on the model-identified evidence. It is easier for prompt engineering approaches to place further scrutiny on intermediate evidence (e.g. chain-of-verification \cite{dhuliawala_chain--verification_2023}) than it is to recover entirely missing evidence, so it is helpful that the latter is the characteristic error we observe. We also show that when evidence is missed, it is most often when there is other viable evidence present which is equally sufficient for predicting the correct label. 

% Taken together, these results suggest that extractive evidence is an effective way of examining LM behavior in a few-shot setting, and that contemporary LMs are effective at retrieving relevant in-context evidence and interpreting it to produce correct predictions. 

%Instructing an LLM to rationalize its response is currently used as a means to provide an explanation and reliability to an otherwise black-box LLM response. Originally, this work aimed to improve a model's rationale performance through novel prompting strategies. One prevalent prompting strategy is few-shot prompting, wherein a user provides exemplars of the task they would like the LLM to solve. In our preliminary findings of this strategy, we discovered that the specific choice of exemplars might alter what an LLM rationalizes, potentially to its detriment, as shown by \ref{fig:scifact_prompt_example}. As such, this work aims to address the affect that exemplar properties have on an LLM's response, both in its direct answer to a query and the extractive evidence it uses to rationalize its answer. Our findings show that using word similarity and evidence position based exemplars do not have an effect on what evidence the model provides as a rationale, but that it may affect how the model reasons about its rationales.

\section{Conclusion}


In this work, we investigate the relationship between prediction and extractive rationalization in few-shot learning. We find that there is a strong correlation between the classification accuracy and agreement with human-annotated rationales. Furthermore, we find that significantly more model error is attributable to imprecise rationalization than incomplete rationalization, a positive sign for downstream methods based on this mechanism.

% References and End of Paper
% These lines must be placed at the end of your paper
\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Agarwal, Tanneru, and Lakkaraju(2024)}]{agarwal_faithfulness_2024}
Agarwal, C.; Tanneru, S.~H.; and Lakkaraju, H. 2024.
\newblock Faithfulness vs. {Plausibility}: {On} the ({Un}){Reliability} of {Explanations} from {Large} {Language} {Models}.
\newblock ArXiv:2402.04614 [cs].

\bibitem[{Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana, Jenner, Casper, Sourbut, Edelman, Zhang, GÃ¼nther, Korinek, Hernandez-Orallo, Hammond, Bigelow, Pan, Langosco, Korbak, Zhang, Zhong, hÃ‰igeartaigh, Recchia, Corsi, Chan, Anderljung, Edwards, Bengio, Chen, Albanie, Maharaj, Foerster, Tramer, He, Kasirzadeh, Choi, and Krueger}]{anwar_foundational_2024}
Anwar, U.; Saparov, A.; Rando, J.; Paleka, D.; Turpin, M.; Hase, P.; Lubana, E.~S.; Jenner, E.; Casper, S.; Sourbut, O.; Edelman, B.~L.; Zhang, Z.; GÃ¼nther, M.; Korinek, A.; Hernandez-Orallo, J.; Hammond, L.; Bigelow, E.; Pan, A.; Langosco, L.; Korbak, T.; Zhang, H.; Zhong, R.; hÃ‰igeartaigh, S.~Ã.; Recchia, G.; Corsi, G.; Chan, A.; Anderljung, M.; Edwards, L.; Bengio, Y.; Chen, D.; Albanie, S.; Maharaj, T.; Foerster, J.; Tramer, F.; He, H.; Kasirzadeh, A.; Choi, Y.; and Krueger, D. 2024.
\newblock Foundational {Challenges} in {Assuring} {Alignment} and {Safety} of {Large} {Language} {Models}.
\newblock ArXiv:2404.09932 [cs].

\bibitem[{Carton, Mei, and Resnick(2018)}]{carton_extractive_2018}
Carton, S.; Mei, Q.; and Resnick, P. 2018.
\newblock Extractive {Adversarial} {Networks}: {High}-{Recall} {Explanations} for {Identifying} {Personal} {Attacks} in {Social} {Media} {Posts}.
\newblock In \emph{Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}}, 3497--3507. Brussels, Belgium: Association for Computational Linguistics.

\bibitem[{DeYoung et~al.(2019)DeYoung, Jain, Rajani, Lehman, Xiong, Socher, and Wallace}]{deyoung_eraser_2019}
DeYoung, J.; Jain, S.; Rajani, N.~F.; Lehman, E.; Xiong, C.; Socher, R.; and Wallace, B.~C. 2019.
\newblock {ERASER}: {A} {Benchmark} to {Evaluate} {Rationalized} {NLP} {Models}.
\newblock \emph{arXiv preprint}.
\newblock ArXiv: 1911.03429.

\bibitem[{DeYoung et~al.(2020)DeYoung, Lehman, Nye, Marshall, and Wallace}]{deyoung_evidence_2020}
DeYoung, J.; Lehman, E.; Nye, B.; Marshall, I.~J.; and Wallace, B.~C. 2020.
\newblock Evidence {Inference} 2.0: {More} {Data}, {Better} {Models}.
\newblock ArXiv:2005.04177 [cs].

\bibitem[{Dhinakaran and Jolly(2024)}]{dhinakaran_needle_2024}
Dhinakaran, A.; and Jolly, E. 2024.
\newblock The {Needle} {In} a {Haystack} {Test}: {Evaluating} the {Performance} of {LLM} {RAG} {Systems}.

\bibitem[{Fok and Weld(2023)}]{fok_search_2023}
Fok, R.; and Weld, D.~S. 2023.
\newblock In {Search} of {Verifiability}: {Explanations} {Rarely} {Enable} {Complementary} {Performance} in {AI}-{Advised} {Decision} {Making}.
\newblock ArXiv:2305.07722 [cs].

\bibitem[{Gan, Zhang, and Mori(2024)}]{gan_application_2024}
Gan, C.; Zhang, Q.; and Mori, T. 2024.
\newblock Application of {LLM} {Agents} in {Recruitment}: {A} {Novel} {Framework} for {Resume} {Screening}.
\newblock ArXiv:2401.08315 [cs].

\bibitem[{Hu et~al.(2023)Hu, Hong, Zhang, King, and Yu}]{hu_think_2023}
Hu, X.; Hong, Z.; Zhang, C.; King, I.; and Yu, P. 2023.
\newblock Think {Rationally} about {What} {You} {See}: {Continuous} {Rationale} {Extraction} for {Relation} {Extraction}.
\newblock In \emph{Proceedings of the 46th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}}, {SIGIR} '23, 2436--2440. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 978-1-4503-9408-6.

\bibitem[{Huang et~al.(2023)Huang, Mamidanna, Jangam, Zhou, and Gilpin}]{huang_can_2023}
Huang, S.; Mamidanna, S.; Jangam, S.; Zhou, Y.; and Gilpin, L.~H. 2023.
\newblock Can {Large} {Language} {Models} {Explain} {Themselves}? {A} {Study} of {LLM}-{Generated} {Self}-{Explanations}.
\newblock ArXiv:2310.11207 [cs].

\bibitem[{Jacovi and Goldberg(2020)}]{jacovi_towards_2020}
Jacovi, A.; and Goldberg, Y. 2020.
\newblock Towards {Faithfully} {Interpretable} {NLP} {Systems}: {How} should we define and evaluate faithfulness?
\newblock \emph{arXiv:2004.03685 [cs]}.
\newblock ArXiv: 2004.03685.

\bibitem[{Khashabi et~al.(2018)Khashabi, Chaturvedi, Roth, Upadhyay, and Roth}]{khashabi_looking_2018}
Khashabi, D.; Chaturvedi, S.; Roth, M.; Upadhyay, S.; and Roth, D. 2018.
\newblock Looking {Beyond} the {Surface}: {A} {Challenge} {Set} for {Reading} {Comprehension} over {Multiple} {Sentences}.
\newblock In \emph{Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})}, 252--262. New Orleans, Louisiana: Association for Computational Linguistics.

\bibitem[{Kumar, AbuHashem, and Durumeric(2024)}]{kumar_watch_2024}
Kumar, D.; AbuHashem, Y.; and Durumeric, Z. 2024.
\newblock Watch {Your} {Language}: {Investigating} {Content} {Moderation} with {Large} {Language} {Models}.
\newblock ArXiv:2309.14517 [cs].

\bibitem[{Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion, LukoÅ¡iÅ«tÄ—, Nguyen, Cheng, Joseph, Schiefer, Rausch, Larson, McCandlish, Kundu, Kadavath, Yang, Henighan, Maxwell, Telleen-Lawton, Hume, Hatfield-Dodds, Kaplan, Brauner, Bowman, and Perez}]{lanham_measuring_2023}
Lanham, T.; Chen, A.; Radhakrishnan, A.; Steiner, B.; Denison, C.; Hernandez, D.; Li, D.; Durmus, E.; Hubinger, E.; Kernion, J.; LukoÅ¡iÅ«tÄ—, K.; Nguyen, K.; Cheng, N.; Joseph, N.; Schiefer, N.; Rausch, O.; Larson, R.; McCandlish, S.; Kundu, S.; Kadavath, S.; Yang, S.; Henighan, T.; Maxwell, T.; Telleen-Lawton, T.; Hume, T.; Hatfield-Dodds, Z.; Kaplan, J.; Brauner, J.; Bowman, S.~R.; and Perez, E. 2023.
\newblock Measuring {Faithfulness} in {Chain}-of-{Thought} {Reasoning}.
\newblock ArXiv:2307.13702 [cs].

\bibitem[{Lei, Barzilay, and Jaakkola(2016)}]{lei_rationalizing_2016}
Lei, T.; Barzilay, R.; and Jaakkola, T. 2016.
\newblock Rationalizing {Neural} {Predictions}.
\newblock In \emph{Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}}, 107--117.
\newblock 571 citations (Semantic Scholar/arXiv) [2022-08-26].

\bibitem[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki, and Callison-Burch}]{lyu_faithful_2023}
Lyu, Q.; Havaldar, S.; Stein, A.; Zhang, L.; Rao, D.; Wong, E.; Apidianaki, M.; and Callison-Burch, C. 2023.
\newblock Faithful {Chain}-of-{Thought} {Reasoning}.
\newblock ArXiv:2301.13379 [cs].

\bibitem[{Madsen, Chandar, and Reddy(2024)}]{madsen_are_2024}
Madsen, A.; Chandar, S.; and Reddy, S. 2024.
\newblock Are self-explanations from {Large} {Language} {Models} faithful?
\newblock ArXiv:2401.07927.

\bibitem[{Majumder et~al.(2022)Majumder, Camburu, Lukasiewicz, and McAuley}]{majumder_knowledge-grounded_2022}
Majumder, B.~P.; Camburu, O.-M.; Lukasiewicz, T.; and McAuley, J. 2022.
\newblock Knowledge-{Grounded} {Self}-{Rationalization} via {Extractive} and {Natural} {Language} {Explanations}.
\newblock ArXiv:2106.13876 [cs].

\bibitem[{McDermott(2011)}]{mcdermott_internal_2011}
McDermott, R. 2011.
\newblock Internal and external validity.
\newblock \emph{Cambridge handbook of experimental political science}, 27.

\bibitem[{OpenAI(2023)}]{openai_gpt-4_2023}
OpenAI. 2023.
\newblock {GPT}-4 {Technical} {Report}.
\newblock ArXiv:2303.08774 [cs].

\bibitem[{Pinto et~al.(2023)Pinto, Cardoso-Pereira, Monteiro, Lucena, Souza, and Gama}]{pinto_large_2023}
Pinto, G.; Cardoso-Pereira, I.; Monteiro, D.; Lucena, D.; Souza, A.; and Gama, K. 2023.
\newblock Large {Language} {Models} for {Education}: {Grading} {Open}-{Ended} {Questions} {Using} {ChatGPT}.
\newblock In \emph{Proceedings of the {XXXVII} {Brazilian} {Symposium} on {Software} {Engineering}}, {SBES} '23, 293--302. New York, NY, USA: Association for Computing Machinery.
\newblock ISBN 9798400707872.

\bibitem[{Ribeiro, Singh, and Guestrin(2016)}]{ribeiro_why_2016}
Ribeiro, M.~T.; Singh, S.; and Guestrin, C. 2016.
\newblock "{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}.
\newblock In \emph{Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}}, 1135--1144. ACM.
\newblock ISBN 978-1-4503-4232-2.
\newblock 1999 citations (Semantic Scholar/DOI) [2022-08-26].

\bibitem[{Shen et~al.(2024)Shen, Knearem, Ghosh, Alkiek, Krishna, Liu, Ma, Petridis, Peng, Qiwei, Rakshit, Si, Xie, Bigham, Bentley, Chai, Lipton, Mei, Mihalcea, Terry, Yang, Morris, Resnick, and Jurgens}]{shen_towards_2024}
Shen, H.; Knearem, T.; Ghosh, R.; Alkiek, K.; Krishna, K.; Liu, Y.; Ma, Z.; Petridis, S.; Peng, Y.-H.; Qiwei, L.; Rakshit, S.; Si, C.; Xie, Y.; Bigham, J.~P.; Bentley, F.; Chai, J.; Lipton, Z.; Mei, Q.; Mihalcea, R.; Terry, M.; Yang, D.; Morris, M.~R.; Resnick, P.; and Jurgens, D. 2024.
\newblock Towards {Bidirectional} {Human}-{AI} {Alignment}: {A} {Systematic} {Review} for {Clarifications}, {Framework}, and {Future} {Directions}.
\newblock ArXiv:2406.09264 [cs].

\bibitem[{Shen et~al.(2023)Shen, Jin, Huang, Liu, Dong, Guo, Wu, Liu, and Xiong}]{shen_large_2023}
Shen, T.; Jin, R.; Huang, Y.; Liu, C.; Dong, W.; Guo, Z.; Wu, X.; Liu, Y.; and Xiong, D. 2023.
\newblock Large {Language} {Model} {Alignment}: {A} {Survey}.
\newblock ArXiv:2309.15025 [cs].

\bibitem[{Siegel et~al.(2024)Siegel, Camburu, Heess, and Perez-Ortiz}]{siegel_probabilities_2024}
Siegel, N.~Y.; Camburu, O.-M.; Heess, N.; and Perez-Ortiz, M. 2024.
\newblock The {Probabilities} {Also} {Matter}: {A} {More} {Faithful} {Metric} for {Faithfulness} of {Free}-{Text} {Explanations} in {Large} {Language} {Models}.
\newblock ArXiv:2404.03189 [cs].

\bibitem[{Singh et~al.(2024)Singh, Inala, Galley, Caruana, and Gao}]{singh_rethinking_2024}
Singh, C.; Inala, J.~P.; Galley, M.; Caruana, R.; and Gao, J. 2024.
\newblock Rethinking {Interpretability} in the {Era} of {Large} {Language} {Models}.
\newblock ArXiv:2402.01761 [cs].

\bibitem[{Singhania, Razniewski, and Weikum(2024)}]{singhania_recall_2024}
Singhania, S.; Razniewski, S.; and Weikum, G. 2024.
\newblock Recall {Them} {All}: {Retrieval}-{Augmented} {Language} {Models} for {Long} {Object} {List} {Extraction} from {Long} {Documents}.
\newblock ArXiv:2405.02732 [cs].

\bibitem[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, and {others}}]{team_gemini_2023}
Team, G.; Anil, R.; Borgeaud, S.; Wu, Y.; Alayrac, J.-B.; Yu, J.; Soricut, R.; Schalkwyk, J.; Dai, A.~M.; Hauth, A.; and {others}. 2023.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}.

\bibitem[{Thirunavukarasu et~al.(2023)Thirunavukarasu, Ting, Elangovan, Gutierrez, Tan, and Ting}]{thirunavukarasu_large_2023}
Thirunavukarasu, A.~J.; Ting, D. S.~J.; Elangovan, K.; Gutierrez, L.; Tan, T.~F.; and Ting, D. S.~W. 2023.
\newblock Large language models in medicine.
\newblock \emph{Nature Medicine}, 29(8): 1930--1940.
\newblock Publisher: Nature Publishing Group.

\bibitem[{Turpin et~al.(2023)Turpin, Michael, Perez, and Bowman}]{turpin_language_2023}
Turpin, M.; Michael, J.; Perez, E.; and Bowman, S.~R. 2023.
\newblock Language {Models} {Don}'t {Always} {Say} {What} {They} {Think}: {Unfaithful} {Explanations} in {Chain}-of-{Thought} {Prompting}.
\newblock ArXiv:2305.04388 [cs].

\bibitem[{Vladika, Schneider, and Matthes(2024)}]{vladika_healthfc_2024}
Vladika, J.; Schneider, P.; and Matthes, F. 2024.
\newblock {HealthFC}: {Verifying} {Health} {Claims} with {Evidence}-{Based} {Medical} {Fact}-{Checking}.
\newblock In Calzolari, N.; Kan, M.-Y.; Hoste, V.; Lenci, A.; Sakti, S.; and Xue, N., eds., \emph{Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)}, 8095--8107. Torino, Italia: ELRA and ICCL.

\bibitem[{Wadden et~al.(2022)Wadden, Lo, Kuehl, Cohan, Beltagy, Wang, and Hajishirzi}]{wadden_scifact-open_2022}
Wadden, D.; Lo, K.; Kuehl, B.; Cohan, A.; Beltagy, I.; Wang, L.~L.; and Hajishirzi, H. 2022.
\newblock {SciFact}-{Open}: {Towards} open-domain scientific claim verification.
\newblock ArXiv:2210.13777 [cs].

\bibitem[{Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou}]{wang_self-consistency_2023}
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2023.
\newblock Self-{Consistency} {Improves} {Chain} of {Thought} {Reasoning} in {Language} {Models}.
\newblock ArXiv:2203.11171 [cs].

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}]{wei_chain_2022}
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2022.
\newblock Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}.
\newblock ArXiv:2201.11903 [cs].

\bibitem[{Wiegreffe and MarasoviÄ‡(2021)}]{wiegreffe_teach_2021}
Wiegreffe, S.; and MarasoviÄ‡, A. 2021.
\newblock Teach {Me} to {Explain}: {A} {Review} of {Datasets} for {Explainable} {Natural} {Language} {Processing}.
\newblock ArXiv:2102.12060 [cs].

\bibitem[{Wulczyn, Thain, and Dixon(2017)}]{wulczyn_ex_2017}
Wulczyn, E.; Thain, N.; and Dixon, L. 2017.
\newblock Ex {Machina}: {Personal} {Attacks} {Seen} at {Scale}.
\newblock In \emph{Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web}}, 1391--1399.
\newblock ISBN 978-1-4503-4913-0.

\bibitem[{Zhu et~al.(2024)Zhu, Chen, Ye, Lyu, Tan, Marasovic, and Wiegreffe}]{zhu_explanation_2024}
Zhu, Z.; Chen, H.; Ye, X.; Lyu, Q.; Tan, C.; Marasovic, A.; and Wiegreffe, S. 2024.
\newblock Explanation in the {Era} of {Large} {Language} {Models}.
\newblock In Zhang, R.; Schneider, N.; and Chaturvedi, S., eds., \emph{Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 5: {Tutorial} {Abstracts})}, 19--25. Mexico City, Mexico: Association for Computational Linguistics.

\end{thebibliography}

\end{document}

\subsection{Overlength Papers}
If your paper is too long and you resort to formatting tricks to make it fit, it is quite likely that it will be returned to you. The best way to retain readability if the paper is overlength is to cut text, figures, or tables. There are a few acceptable ways to reduce paper size that don't affect readability. First, turn on \textbackslash frenchspacing, which will reduce the space after periods. Next, move all your figures and tables to the top of the page. Consider removing less important portions of a figure. If you use \textbackslash centering instead of \textbackslash begin\{center\} in your figure environment, you can also buy some space. For mathematical environments, you may reduce fontsize {\bf but not below 6.5 point}.


\subsection{Title and Authors}
Your title must appear centered over both text columns in sixteen-point bold type (twenty-four point leading). The title must be written in Title Case according to the Chicago Manual of Style rules. The rules are a bit involved, but in general verbs (including short verbs like be, is, using, and go), nouns, adverbs, adjectives, and pronouns should be capitalized, (including both words in hyphenated terms), while articles, conjunctions, and prepositions are lower case unless they directly follow a colon or long dash. You can use the online tool \url{https://titlecaseconverter.com/} to double-check the proper capitalization (select the "Chicago" style and mark the "Show explanations" checkbox).

Author's names should appear below the title of the paper, centered in twelve-point type (with fifteen point leading), along with affiliation(s) and complete address(es) (including electronic mail address if available) in nine-point roman type (the twelve point leading). You should begin the two-column format when you come to the abstract.


\subsection{\LaTeX{} Copyright Notice}
The copyright notice automatically appears if you use aaai25.sty. It has been hardcoded and may not be disabled.



\subsection{Abstract}
Follow the example commands in this document for creation of your abstract. The command \textbackslash begin\{abstract\} will automatically indent the text block. Please do not indent it further. {Do not include references in your abstract!}

\subsection{Text}
The main body of the paper must be formatted in black, ten-point Times Roman with twelve-point leading (line spacing). You may not reduce font size or the linespacing. Commands that alter font size or line spacing (including, but not limited to baselinestretch, baselineshift, linespread, and others) are expressly forbidden. In addition, you may not use color in the text.

\subsection{Citations}
Citations within the text should include the author's last name and year, for example (Newell 1980). Append lower-case letters to the year in cases of ambiguity. Multiple authors should be treated as follows: (Feigenbaum and Engelmore 1988) or (Ford, Hayes, and Glymour 1992). In the case of four or more authors, list only the first author, followed by et al. (Ford et al. 1997).


\subsection{Footnotes}
Use footnotes judiciously, taking into account that they interrupt the reading of the text. When required, they should be consecutively numbered throughout with superscript Arabic numbers. Footnotes should appear at the bottom of the page, separated from the text by a blank line space and a thin, half-point rule.

\subsection{Headings and Sections}
When necessary, headings should be used to separate major sections of your paper. Remember, you are writing a short paper, not a lengthy book! An overabundance of headings will tend to make your paper look more like an outline than a paper. The aaai25.sty package will create headings for you. Do not alter their size nor their spacing above or below.

\subsubsection{Section Numbers.}
The use of section numbers in AAAI Press papers is optional. To use section numbers in \LaTeX{}, uncomment the setcounter line in your document preamble and change the 0 to a 1. Section numbers should not be used in short poster papers and/or extended abstracts.

\subsubsection{Section Headings.}
Sections should be arranged and headed as follows:
\begin{enumerate}
\item Main content sections
\item Appendices (optional)
\item Ethical Statement (optional, unnumbered)
\item Acknowledgements (optional, unnumbered)
\item References (unnumbered)
\end{enumerate}

\subsubsection{Appendices.}
Any appendices must appear after the main content. If your main sections are numbered, appendix sections must use letters instead of arabic numerals. In \LaTeX{} you can use the \texttt{\textbackslash appendix} command to achieve this effect and then use \texttt{\textbackslash section\{Heading\}} normally for your appendix sections.

\subsubsection{Ethical Statement.}
You can write a statement about the potential ethical impact of your work, including its broad societal implications, both positive and negative. If included, such statement must be written in an unnumbered section titled \emph{Ethical Statement}.

\subsubsection{Acknowledgments.}
The acknowledgments section, if included, appears right before the references and is headed ``Acknowledgments". It must not be numbered even if other sections are (use \texttt{\textbackslash section*\{Acknowledgements\}} in \LaTeX{}). This section includes acknowledgments of help from associates and colleagues, credits to sponsoring agencies, financial support, and permission to publish. Please acknowledge other contributors, grant support, and so forth, in this section. Do not put acknowledgments in a footnote on the first page. If your grant agency requires acknowledgment of the grant on page 1, limit the footnote to the required statement, and put the remaining acknowledgments at the back. Please try to limit acknowledgments to no more than three sentences.

\subsubsection{References.}
The references section should be labeled ``References" and must appear at the very end of the paper (don't end the paper with references, and then put a figure by itself on the last page). A sample list of references is given later on in these instructions. Please use a consistent format for references. Poorly prepared or sloppy references reflect badly on the quality of your paper and your research. Please prepare complete and accurate citations.

\subsection{Illustrations and  Figures}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure1} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Using the trim and clip commands produces fragile layers that can result in disasters (like this one from an actual paper) when the color space is corrected or the PDF combined with others for the final proceedings. Crop your figures properly in a graphics program -- not in LaTeX.}
\label{fig1}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{figure2} % Reduce the figure size so that it is slightly narrower than the column.
\caption{Adjusting the bounding box instead of actually removing the unwanted data resulted multiple layers in this paper. It also needlessly increased the PDF size. In this case, the size of the unwanted layer doubled the paper's size, and produced the following surprising results in final production. Crop your figures properly in a graphics program. Don't just alter the bounding box.}
\label{fig2}
\end{figure*}

% Using the \centering command instead of \begin{center} ... \end{center} will save space
% Positioning your figure at the top of the page will save space and make the paper more readable
% Using 0.95\columnwidth in conjunction with the


Your paper must compile in PDF\LaTeX{}. Consequently, all your figures must be .jpg, .png, or .pdf. You may not use the .gif (the resolution is too low), .ps, or .eps file format for your figures.

Figures, drawings, tables, and photographs should be placed throughout the paper on the page (or the subsequent page) where they are first discussed. Do not group them together at the end of the paper. If placed at the top of the paper, illustrations may run across both columns. Figures must not invade the top, bottom, or side margin areas. Figures must be inserted using the \textbackslash usepackage\{graphicx\}. Number figures sequentially, for example, figure 1, and so on. Do not use minipage to group figures.

If you normally create your figures using pgfplots, please create the figures first, and then import them as pdfs with proper bounding boxes, as the bounding and trim boxes created by pfgplots are fragile and not valid.

When you include your figures, you must crop them \textbf{outside} of \LaTeX{}. The command \textbackslash includegraphics*[clip=true, viewport 0 0 10 10]{...} might result in a PDF that looks great, but the image is \textbf{not really cropped.} The full image can reappear (and obscure whatever it is overlapping) when page numbers are applied or color space is standardized. Figures \ref{fig1}, and \ref{fig2} display some unwanted results that often occur.

If your paper includes illustrations that are not compatible with PDF\TeX{} (such as .eps or .ps documents), you will need to convert them. The epstopdf package will usually work for eps files. You will need to convert your ps files to PDF in either case.

\subsubsection {Figure Captions.}The illustration number and caption must appear \textit{under} the illustration. Labels and other text with the actual illustration must be at least nine-point type. However, the font and size of figure captions must be 10 point roman. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)

\subsection{Tables}

\subsection{Tables}

Tables should be presented in 10 point roman type. If necessary, they may be altered to 9 point type. You must not use \texttt{\textbackslash resizebox} or other commands that resize the entire table to make it smaller, because you can't control the final font size this way.
If your table is too large you can use \texttt{\textbackslash setlength\{\textbackslash tabcolsep\}\{1mm\}} to compress the columns a bit or you can adapt the content (e.g.: reduce the decimal precision when presenting numbers, use shortened column titles, make some column duble-line to get it narrower).

Tables that do not fit in a single column must be placed across double columns. If your table won't fit within the margins even when spanning both columns and using the above techniques, you must split it in two separate tables.

\subsubsection {Table Captions.} The number and caption for your table must appear \textit{under} (not above) the table.  Additionally, the font and size of table captions must be 10 point roman and must be placed beneath the figure. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)



\subsubsection{Low-Resolution Bitmaps.}
You may not use low-resolution (such as 72 dpi) screen-dumps and GIF files---these files contain so few pixels that they are always blurry, and illegible when printed. If they are color, they will become an indecipherable mess when converted to black and white. This is always the case with gif files, which should never be used. The resolution of screen dumps can be increased by reducing the print size of the original file while retaining the same number of pixels. You can also enlarge files by manipulating them in software such as PhotoShop. Your figures should be 300 dpi when incorporated into your document.

\subsubsection{\LaTeX{} Overflow.}
\LaTeX{} users please beware: \LaTeX{} will sometimes put portions of the figure or table or an equation in the margin. If this happens, you need to make the figure or table span both columns. If absolutely necessary, you may reduce the figure, or reformat the equation, or reconfigure the table.{ \bf Check your log file!} You must fix any overflow into the margin (that means no overfull boxes in \LaTeX{}). \textbf{Nothing is permitted to intrude into the margin or gutter.}


\subsubsection{Using Color.}
Use of color is restricted to figures only. It must be WACG 2.0 compliant. (That is, the contrast ratio must be greater than 4.5:1 no matter the font size.) It must be CMYK, NOT RGB. It may never be used for any portion of the text of your paper. The archival version of your paper will be printed in black and white and grayscale. The web version must be readable by persons with disabilities. Consequently, because conversion to grayscale can cause undesirable effects (red changes to black, yellow can disappear, and so forth), we strongly suggest you avoid placing color figures in your document. If you do include color figures, you must (1) use the CMYK (not RGB) colorspace and (2) be mindful of readers who may happen to have trouble distinguishing colors. Your paper must be decipherable without using color for distinction.

\subsubsection{Drawings.}
We suggest you use computer drawing software (such as Adobe Illustrator or, (if unavoidable), the drawing tools in Microsoft Word) to create your illustrations. Do not use Microsoft Publisher. These illustrations will look best if all line widths are uniform (half- to two-point in size), and you do not create labels over shaded areas. Shading should be 133 lines per inch if possible. Use Times Roman or Helvetica for all figure call-outs. \textbf{Do not use hairline width lines} --- be sure that the stroke width of all lines is at least .5 pt. Zero point lines will print on a laser printer, but will completely disappear on the high-resolution devices used by our printers.

\subsubsection{Photographs and Images.}
Photographs and other images should be in grayscale (color photographs will not reproduce well; for example, red tones will reproduce as black, yellow may turn to white, and so forth) and set to a minimum of 300 dpi. Do not prescreen images.

\subsubsection{Resizing Graphics.}
Resize your graphics \textbf{before} you include them with LaTeX. You may \textbf{not} use trim or clip options as part of your \textbackslash includegraphics command. Resize the media box of your PDF using a graphics program instead.

\subsubsection{Fonts in Your Illustrations.}
You must embed all fonts in your graphics before including them in your LaTeX document.




\subsection{References}
The AAAI style includes a set of definitions for use in formatting references with BibTeX. These definitions make the bibliography style fairly close to the ones  specified in the Reference Examples appendix below. To use these definitions, you also need the BibTeX style file ``aaai25.bst," available in the AAAI Author Kit on the AAAI web site. Then, at the end of your paper but before \textbackslash end{document}, you need to put the following lines:

\begin{quote}
\begin{small}
\textbackslash bibliography\{bibfile1,bibfile2,...\}
\end{small}
\end{quote}

Please note that the aaai25.sty class already sets the bibliographystyle for you, so you do not have to place any \textbackslash bibliographystyle command in the document yourselves. The aaai25.sty file is incompatible with the hyperref and navigator packages. If you use either, your references will be garbled and your paper will be returned to you.

References may be the same size as surrounding text.
However, in this section (only), you may reduce the size to {\em \textbackslash small} (9pt) if your paper exceeds the allowable number of pages. Making it any smaller than 9 point with 10 point linespacing, however, is not allowed.

The list of files in the \textbackslash bibliography command should be the names of your BibTeX source files (that is, the .bib files referenced in your paper).

The following commands are available for your use in citing references:
\begin{quote}
{\em \textbackslash cite:} Cites the given reference(s) with a full citation. This appears as ``(Author Year)'' for one reference, or ``(Author Year; Author Year)'' for multiple references.\smallskip\\
{\em \textbackslash shortcite:} Cites the given reference(s) with just the year. This appears as ``(Year)'' for one reference, or ``(Year; Year)'' for multiple references.\smallskip\\
{\em \textbackslash citeauthor:} Cites the given reference(s) with just the author name(s) and no parentheses.\smallskip\\
{\em \textbackslash citeyear:} Cites the given reference(s) with just the date(s) and no parentheses.
\end{quote}
You may also use any of the \emph{natbib} citation commands.


\section{Proofreading Your PDF}
Please check all the pages of your PDF file. The most commonly forgotten element is the acknowledgements --- especially the correct grant number. Authors also commonly forget to add the metadata to the source, use the wrong reference style file, or don't follow the capitalization rules or comma placement for their author-title information properly. A final common problem is text (expecially equations) that runs into the margin. You will need to fix these common errors before submitting your file.

\section{Improperly Formatted Files }
In the past, AAAI has corrected improperly formatted files submitted by the authors. Unfortunately, this has become an increasingly burdensome expense that we can no longer absorb). Consequently, if your file is improperly formatted, it will be returned to you for correction.

\section{Naming Your Electronic File}
We require that you name your \LaTeX{} source file with the last name (family name) of the first author so that it can easily be differentiated from other submissions. Complete file-naming instructions will be provided to you in the submission instructions.

\section{Submitting Your Electronic Files to AAAI}
Instructions on paper submittal will be provided to you in your acceptance letter.

\section{Inquiries}
If you have any questions about the preparation or submission of your paper as instructed in this document, please contact AAAI Press at the address given below. If you have technical questions about implementation of the aaai style file, please contact an expert at your site. We do not provide technical support for \LaTeX{} or any other software package. To avoid problems, please keep your paper simple, and do not incorporate complicated macros and style files.

\begin{quote}
\noindent AAAI Press\\
1101 Pennsylvania Ave, NW Suite 300\\
Washington, DC 20004 USA\\
\textit{Telephone:} 1-202-360-4062\\
\textit{E-mail:} See the submission instructions for your particular conference or event.
\end{quote}

\section{Additional Resources}
\LaTeX{} is a difficult program to master. If you've used that software, and this document didn't help or some items were not explained clearly, we recommend you read Michael Shell's excellent document (testflow doc.txt V1.0a 2002/08/13) about obtaining correct PS/PDF output on \LaTeX{} systems. (It was written for another purpose, but it has general application as well). It is available at www.ctan.org in the tex-archive.

\appendix
\section{Reference Examples}
\label{sec:reference_examples}

\nobibliography*
Formatted bibliographies should look like the following examples. You should use BibTeX to generate the references. Missing fields are unacceptable when compiling references, and usually indicate that you are using the wrong type of entry (BibTeX class).

\paragraph{Book with multiple authors~\nocite{em:86}} Use the \texttt{@book} class.\\[.2em]
\bibentry{em:86}.

\paragraph{Journal and magazine articles~\nocite{r:80, hcr:83}} Use the \texttt{@article} class.\\[.2em]
\bibentry{r:80}.\\[.2em]
\bibentry{hcr:83}.

\paragraph{Proceedings paper published by a society, press or publisher~\nocite{c:83, c:84}} Use the \texttt{@inproceedings} class. You may abbreviate the \emph{booktitle} field, but make sure that the conference edition is clear.\\[.2em]
\bibentry{c:84}.\\[.2em]
\bibentry{c:83}.

\paragraph{University technical report~\nocite{r:86}} Use the \texttt{@techreport} class.\\[.2em]
\bibentry{r:86}.

\paragraph{Dissertation or thesis~\nocite{c:79}} Use the \texttt{@phdthesis} class.\\[.2em]
\bibentry{c:79}.

\paragraph{Forthcoming publication~\nocite{c:21}} Use the \texttt{@misc} class with a \texttt{note="Forthcoming"} annotation.
\begin{quote}
\begin{footnotesize}
\begin{verbatim}
@misc(key,
  [...]
  note="Forthcoming",
)
\end{verbatim}
\end{footnotesize}
\end{quote}
\bibentry{c:21}.

\paragraph{ArXiv paper~\nocite{c:22}} Fetch the BibTeX entry from the "Export Bibtex Citation" link in the arXiv website. Notice it uses the \texttt{@misc} class instead of the \texttt{@article} one, and that it includes the \texttt{eprint} and \texttt{archivePrefix} keys.
\begin{quote}
\begin{footnotesize}
\begin{verbatim}
@misc(key,
  [...]
  eprint="xxxx.yyyy",
  archivePrefix="arXiv",
)
\end{verbatim}
\end{footnotesize}
\end{quote}
\bibentry{c:22}.

\paragraph{Website or online resource~\nocite{c:23}} Use the \texttt{@misc} class. Add the url in the \texttt{howpublished} field and the date of access in the \texttt{note} field:
\begin{quote}
\begin{footnotesize}
\begin{verbatim}
@misc(key,
  [...]
  howpublished="\url{http://...}",
  note="Accessed: YYYY-mm-dd",
)
\end{verbatim}
\end{footnotesize}
\end{quote}
\bibentry{c:23}.

\vspace{.2em}
For the most up to date version of the AAAI reference style, please consult the \textit{AI Magazine} Author Guidelines at \url{https://aaai.org/ojs/index.php/aimagazine/about/submissions#authorGuidelines}

\section{Acknowledgments}
AAAI is especially grateful to Peter Patel Schneider for his work in implementing the original aaai.sty file, liberally using the ideas of other style hackers, including Barbara Beeton. We also acknowledge with thanks the work of George Ferguson for his guide to using the style and BibTeX files --- which has been incorporated into this document --- and Hans Guesgen, who provided several timely modifications, as well as the many others who have, from time to time, sent in suggestions on improvements to the AAAI style. We are especially grateful to Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan for the improvements to the Bib\TeX{} and \LaTeX{} files made in 2020.

The preparation of the \LaTeX{} and Bib\TeX{} files that implement these instructions was supported by Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers, The Live Oak Press, LLC, and AAAI Press. Bibliography style changes were added by Sunil Issar. \verb+\+pubnote was added by J. Scott Penberthy. George Ferguson added support for printing the AAAI copyright slug. Additional changes to aaai25.sty and aaai25.bst have been made by Francisco Cruz and Marc Pujol-Gonzalez.

\bigskip
\noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!