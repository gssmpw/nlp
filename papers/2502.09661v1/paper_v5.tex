\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{subcaption}  % Add this line to your preamble if not already included


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{SIToBI - A Speech Prosody Annotation Tool for Indian Languages\\
	\thanks{The current work is carried out as a part of the project titled, ``Prosody Modeling'', under the sub-project of the NLTM BHASHINI project, titled, ``Speech technologies in Indian languages'', funded by the Ministry of Electronics and Information Technology, Government of India, with reference number, 11(1)/2022-HCC(TDIL).}
}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Preethi Thinakaran,
2\textsuperscript{nd} Malarvizhi Muthuramalingam,
3\textsuperscript{rd} Sooriya S,
4\textsuperscript{th} Anushiya Rachel Gladston,\\
5\textsuperscript{th} P. Vijayalakshmi,
6\textsuperscript{th} Hema A Murthy,
7\textsuperscript{th} T. Nagarajan}
\IEEEauthorblockA{\textit{Department of CSE}, \textit{Shiv Nadar University Chennai}, Chennai, India \\ 
\{preethit, malarvizhim, sooriyas, anushiyarachelg, nagarajant\}@snuchennai.edu.in}

\IEEEauthorblockA{\textit{Department of ECE}, \textit{Sri Sivasubramaniya Nadar College of Engineering}, Chennai, India \\ 
vijayalakshmip@ssn.edu.in}
\IEEEauthorblockA{\textit{Department of CSE}, \textit{Indian Institute of Technology, Madras-600036}, Chennai, India \\ 
hema@cse.iitm.ac.in}
}

\maketitle

\begin{abstract}
	The availability of prosodic information from speech signals is useful in a wide range of applications. However, deriving this information from speech signals can be a laborious task involving manual intervention.  Therefore, the current work focuses on developing a tool that can provide prosodic annotations corresponding to a given speech signal, particularly for Indian languages. The proposed Segmentation with Intensity, Tones and Break Indices (SIToBI) tool provides time-aligned phoneme, syllable, and word transcriptions, syllable-level pitch contour annotations, break indices, and syllable-level relative intensity indices. The tool focuses more on syllable-level annotations since Indian languages are syllable-timed. Indians, regardless of the language they speak, may exhibit influences from other languages. As a result, other languages spoken in India may also exhibit syllable-timed characteristics. The accuracy of the annotations derived from the tool is analyzed by comparing them against manual annotations and the tool is observed to perform well. While the current work focuses on three languages, namely, Tamil, Hindi, and Indian English, the tool can easily be extended to other Indian languages and possibly other syllable-timed languages as well.
\end{abstract}

\begin{IEEEkeywords}
Prosody Annotation, Tones and Break Indices, Speech Prosody, Segmentation
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
Speech prosody plays a vital role in several applications and can be instrumental in improving human-computer interactions.While recent advancements in artificial intelligence have resulted in high-performing speech-based systems, these still do not accommodate prosodic variations in spotaneous speech or discourse. For instance, current text-to-speech (TTS) synthesizers produce speech that is highly intelligible and natural. However, these require a huge amount of training data and are still devoid of emotion and adequate prosodic variations. Incorporating prosodic information while training a TTS system could possibly improve the naturalness of synthetic speech, without necessitating an increase in the amount of training data. Another application that can benefit from the availability of prosodic information is speech-to-speech (S2S) translation. An S2S system involves an automatic speech recognition (ASR) system that provides text corresponding to a given speech signal. This text is then translated to the desired language, which is then synthesized using a TTS system. Incorporation of prosodic information along with the ASR output could aid in producing speech in the target language that carries the same sentiment as the speech in the source language. This is precisely where we have a problem.  Also when the word order changes, incorporation of prosody in the target language is a big challenge. Further, each language has a unique prosodic structure and hence prosodic features could aid in language identification and in the incorporation of language specific prosody in TTS and S2S systems as well.

While prosody is immensely useful in a wide range of applications, obtaining accurate prosodic annotations is usually a laborious task requiring manual intervention. Further, while a standard representation of prosody exists for English, namely ToBI (Tones and Break Indices) \cite{tobi, tobi1}, as well as for certain other languages, no such standard has been established for Indian languages. Therefore, the current work extends on the existing ToBI standard while incorporating additional information, namely time-aligned phonetic, syllabic, and word transcriptions and a relative intensity index, to develop an SIToBI (Segmentation with Intensity, Tones and Break Indices) tool for prosodic annotation.

The ToBI system is a set of conventions for transcribing and annotating the prosody of speech, designed originally for English. Since the manual annotation of prosody is laborious, certain automatic labeling tools have been developed for English. One such tool is the Automatic ToBI (AuToBI) \cite{autobi}, which uses machine learning to predict ToBI labels from acoustic features of the speech signal. Another commonly used platform for annotation is praat \cite{parselmouth}. Although originally designed for manual annotation, it hosts several plugins and scripts supporting semi-automatic prosodic annotation \cite{praat}. Web MAUS \cite{webmaus} is a web-based service for automatic segmentation and labeling of speech, including prosodic features. Another popular tool is PyToBI \cite{pytobi}, which provides a Python interface for automatic labeling with ToBI. However, these are specifically designed for English.

The ToBI framework has now been extended to several other languages as well. For example, J-ToBI \cite{jtobi} has been designed to capture special pitch accent patterns and intonation structures in Japanese. Similarly, G-ToBI \cite{gtobi}, Sp-ToBI \cite{sptobi}, F-ToBI \cite{ftobi}, and C-ToBI \cite{ctobi} have been designed for German, Spanish, French, and Cantonese respectively. These adaptations indicate the relevance of ToBI across languages and the ease of tailoring it to different languages. 

Efforts to study prosody in Indian languages—characterized by their syllable-centric structures—have largely focused on analyzing \textit{intonation}, \textit{stress}, and \textit{rhythm} in languages such as Hindi, Bengali, Tamil, Kannada, and Telugu \cite{hindi_int, ind_intonation, bengali_pro}. However, a standardized framework for representing prosodic features across these languages is yet to be established. Recognizing the similarities in their prosodic structures, this work proposes a common prosodic annotation standard and tool. Extending the ToBI (Tones and Break Indices) standard, the SIToBI tool automates the annotation of pitch contours and break indices for Indian languages, while incorporating additional features, including:

\begin{itemize}
    \item Phoneme, syllable, and word boundary segmentation.
    \item Relative intensity index estimation.
\end{itemize}

The inclusion of these additional features enhances the utility and scope of the tool. Phoneme, syllable, and word boundaries enable \textit{precise alignment of prosodic features} such as pitch and stress with linguistic units. They also significantly enhance \textit{speech synthesis and recognition systems}, improving the naturalness and accuracy of text-to-speech (TTS) systems and automatic speech recognition (ASR) models.

The relative intensity index, on the other hand, offers crucial insights into stress and emphasis, contributing to more detailed prosodic modeling. This feature is especially useful for analyzing \textit{speaker emotion, intent, and conversational styles}, enriching applications in areas such as emotions classification and expressive speech synthesis.

By combining tones and break indices with these additional features, \textbf{SIToBI} addresses limitations in existing prosody annotation tools, offering a more comprehensive framework for prosodic analysis. While designed for adaptability across Indian languages, the current implementation focuses on Tamil, Hindi, and Indian English, laying a robust foundation for broader applications in the future.


The paper is organized as follows: Section \ref{sec:tool} provides a detailed description of the proposed tool, Section \ref{sec:perf} evaluates the accuracy of the annotations provided by the tool, Section \ref{sec:lid} presents the use of the tool to perform language identification, and Section \ref{sec:conc} concludes the paper.


\section{ The SIToBI  Tool}
\label{sec:tool}



The SIToBI tool is designed to provide rich prosodic information from a speech signal. Given a speech signal, the tool converts it to a single channel and resamples it to a sampling frequency of 16 kHz. It then uses an ASR to obtain the corresponding orthographic transcription. This transcription, along with the speech signal, is used to derive syllable and word-level boundaries. The tool also processes the speech signal to estimate the relative intensity index, pitch contour labels, and break indices. The extracted information is then plotted as shown in Fig \ref{fig:SI-ToBI}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Tool.png}
    \caption{Annotations from the SIToBI Tool for the English sentence, "For the twentieth time that evening the two men shook hands.''}
    \label{fig:SI-ToBI}
\end{figure}


In the SIToBI tool, an automatic speech recognition (ASR) system is used when only the speech signal (WAV format) is available, without accompanying text. In this scenario, the ASR generates an orthographic transcription from the audio. The other parameters are extracted as mentioned above. However, it's important to note that the transcription produced by the ASR may sometimes be inaccurate, potentially leading to errors in the segmentation of the speech signal and in the extraction of other features.


Conversely, when both the speech signal (WAV) and text are available then the ASR system is unnecessary. In this case, language-independent models are directly applied to align the provided transcription with the speech signal, ensuring more accurate segmentation without the risk of errors introduced by the ASR.

Importantly, in both cases—whether the ASR system is used (when only audio is available) or not (when both audio and text are provided)—the SIToBI tool consistently applies language-independent models.

Considering the number of languages in India, building systems for each language individually is a complex and time-consuming task. For instance, creating a Text-to-Speech (TTS) system for a new language requires a thorough understanding of its phonotactics (sound structure), letter-to-sound (LTS) rules, and other linguistic elements. This process involves considerable effort and often requires the expertise of linguists familiar with the language. As Prakash et al. (2014) highlighted in their work, "An approach to building language-independent text-to-speech synthesis for Indian languages" \cite{prakash2014approach},{prakash2023tts}, building such systems from scratch for each language involves challenges such as training Hidden Markov Models, developing speech context modeling, and collecting language-specific data. These tasks are not only complex but also time-consuming.

By adopting language-independent models, the SIToBI tool eliminates the need for such language-specific efforts. These models offer the flexibility of adapting to new languages with minimal modification, making the process more efficient. They do not require the extensive linguistic resources that traditional, language-dependent models need. Instead, they rely on generalizable algorithms that can be easily applied across multiple languages, greatly improving the scalability and applicability of the SIToBI tool for the multilingual context of India.


This approach offers the advantage of easy adaptation to new languages compared to language-dependent models, thereby improving the accuracy of transcription processing and speech segmentation, regardless of the availability of text.

The block diagram \ref{fig:bd} outlines the overall process of text and speech processing, where input speech and text are segmented  and are processed via prosody modules. This sequential flow enables extraction of phoneme, word, and syllable segments, along with prosodic features, resulting in a comprehensive annotation of the input speech.




\label{sec:tool}
\begin{figure}[h]
    \centering
    \includegraphics[height=7cm, width=18cm]{bd.png}
    \captionsetup{justification=centering} 
    \caption{Block diagram}
    \label{fig:bd}
\end{figure}

\subsection{Automatic Speech Recognition}
\label{sec:asr}
To obtain the orthographic transcription corresponding to a given speech signal, the speech signal and the language (obtained from the user) are given to an ASR. In the current work, a pre-trained Data2Vec-aqc-based ASR model \cite{data2vec}, trained on 30,000 hours of speech data from 24 Indian languages, including Tamil, Hindi, and Indian English, is employed.

\subsection{Segmentation}
\label{sec:segmentation}
Once the text corresponding to a given speech signal is obtained from the ASR, it is used to generate time-aligned phonetic, syllabic, and word transcriptions. To achieve this, context-independent hidden Markov models (HMMs) are trained for each phoneme across the three languages considered \footnote{Due to the requirement for less data and faster training, we favor HMM-based training over neural network methods.}. These HMMs are trained using three states, with one to five mixture components per state, depending on the number of available examples for each phoneme. The forced-Viterbi alignment procedure \cite{ijst, boothalingam2013development} is initially applied to identify phoneme boundaries, and this process is repeated over four iterations to obtain the final boundaries.This iterative process ensures the accurate segmentation of speech data, enabling robust phoneme, syllable, and word-level boundary derivation. Monophone HMMs for all phonemes are generated using the label files obtained. With these models, forced-Viterbi alignment is performed iteratively to segment the rest of the data, as described below:
\begin{enumerate}
    \item Using 5 minutes of speech data and the corresponding time-aligned phonetic transcriptions, context-independent phoneme models are trained (isolated-style training).
    \item Using these models and the phonetic transcriptions, the entire speech dataset is segmented using the forced-Viterbi alignment procedure.
\vspace{8.5cm}
    \item Using the newly derived time-aligned phonetic transcription (phone-level label files), new context-independent phoneme models are trained.

    \item Steps 2 and 3 are repeated for $N$ iterations ($N=5$ in this case).
    \item After $N$ iterations, the resultant HMMs are used to segment the entire speech data again. These boundaries are considered the final boundaries.
\end{enumerate}

This iterative process ensures the accurate segmentation of speech data, enabling robust phoneme, syllable, and word-level boundary derivation.Since the tool is designed to support all three languages, language-independent phoneme models are also trained, where acoustically similar phonemes across the languages share the same model. 

Syllabification involves combining phoneme boundaries to define syllable boundaries using a set of predefined rules. In this process, each vowel acts as the nucleus of a syllable, with consonants appearing at the beginning or end. The phoneme sequence is grouped into syllables in forms such as \( V \), \( C^*V \), \( VC^* \), and \( C^*VC^* \), where consecutive consonants are resolved based on language-specific phonological patterns. This systematic approach ensures precise syllable segmentation, facilitating subsequent linguistic and prosodic analysis.


 While deriving the phoneme boundaries, it is ensured that there are silences inserted at the end of each word, though their duration might be close to zero at several places. These silences are used to combine the phoneme/syllable boundaries to derive word boundaries as discussed in 
section \ref{sec:tool}


%\vspace{6.6cm}

\subsection{Computation of the Relative Intensity Index}
\label{sec:rii}
The tool then calculates the relative intensity index at the syllable level. The speech signal is divided into overlapping frames of 20 ms with a 10 ms hop length, and the Short-term energy (STE) is computed for each frame. The energy is subsequently normalized across the entire utterance. Based on the normalized energy, a relative intensity index with difference of 0.2 is assigned to each syllable on a scale from 1 to 5 as follows:
\begin{equation}
	RII =
	\begin{cases} 
		1, & \text{if } E_N < 0.2 \\
		2, & \text{if } 0.1 \le E_N < 0.4 \\
		3, & \text{if } 0.4 \le E_N < 0.6 \\
		4, & \text{if } 0.7 \le E_N < 0.8 \\
		5, & \text{if } 0.9 \le E_N \le 1.0
	\end{cases}
\end{equation}
Fig. \ref{fig:SI-ToBI}(b) depicts the $RII$, where the syllable, ``mehn'' has the highest RII of 5, while the syllables, ``dha'', ``iyv'', ``ningh'', ``shuck'', ``haendz'', etc. have the lowest RII of 1.

\subsection{Estimation of Break Indices}
\label{sec:bi}
In order to determine the break indices, speech vs. silence discrimination is first performed using spectral flatness, which is a measure of the energy distribution across frequencies. Spectral flatness (SF) is computed frame-wise using the following equation, with the same frame length and hop length mentioned in the previous section.
\begin{equation}
\text{SF} = \frac{\exp \left( \frac{2}{N_{\text{FFT}}} \sum_{k=1}^{N_{\text{FFT}}/2} \ln |S_k| \right)}{\frac{2}{N_{\text{FFT}}} \sum_{k=1}^{N_{\text{FFT}}/2} |S_k|}
\end{equation}
where $N_{\text{FFT}}$ is the order of the Fourier transform and $|S_k|$ denotes the magnitude in the $k$th frequency bin of the speech signal.

The spectral flatness (SF) values in the voiced, unvoiced, and silence regions are analyzed. Experimental analysis show that silence regions tend to have a flatter spectrum compared to speech. Based on visual inspection threshold of 0.75 are set for the SF, classifying segments with an SF value of 0.75 or higher as silence regions. Based on the length of these silence regions, the break indices are assigned as follows:
\begin{equation}
	\text{Break Index} =
	\begin{cases} 
		1, & \text{if } l < 80 \text{ ms} \\
		2, & \text{if } 80 \text{ ms} \leq l < 290 \text{ ms } \\
		3, & \text{if } l \geq 290 \text{ ms }
	\end{cases}
\end{equation}



\subsection{Labeling Pitch Contours}
\label{sec:pc}
The pitch period and fundamental frequency (F0) calculation are essential for capturing prosodic characteristics in speech. While computationally cheaper algorithms for pitch period estimation exist, the group delay-based algorithm \cite{anushiyarachel15_interspeech} is employed due to the offline nature of our processing, where accuracy is prioritized over computational efficiency. The algorithm works by identifying the Glottal Closure Instants (GCIs), which mark the boundaries of individual glottal cycles. The difference between consecutive GCIs gives the pitch period \( T_0 \). The fundamental frequency \( F_0 \), which represents the rate of vocal fold vibrations, is then calculated as the inverse of the pitch period, \( F_0 = \frac{1}{T_0} \). 

The GCI algorithm is primarily used to identify pitch marks and for pitch tracking, which are crucial for various speech processing tasks. One such technique is the time-domain pitch synchronous overlap and add (TD-PSOLA), which modifies the prosody of speech and requires accurate pitch mark estimation. Additionally, the estimation of GCIs plays a significant role in speech dereverberation, glottal source modeling, speech enhancement, and speech synthesis. Accurate GCI detection ensures the effective manipulation of prosodic features, leading to more natural and intelligible speech synthesis and processing.

To ensure precise estimation of \( T_0 \) and \( F_0 \), speech signals are analyzed using short overlapping frames, typically 20 ms in size with a 10 ms hop length. GCIs are detected within each frame, and the pitch period is derived.

Once the pitch contour, i.e., the time-varying \( F_0 \), is estimated, it undergoes syllable-level smoothing to reduce local fluctuations and abrupt changes. This is achieved using a polynomial of order 3, which fits a smooth curve to the pitch values over time, yielding a more natural representation of prosodic variations. Initially, eleven shapes of the pitch contour, namely L, H, HLL, HHL, LLH, LHH, HLH, LHL, hat, bucket, and flat, are identified, where L (low) represents a falling contour and H (high) represents a rising contour. To capture finer variations, each shape is further categorized into three classes based on their dynamic ranges, resulting in a total of 31 pitch contour classes. The dynamic ranges of the pitch frequency are represented with an S (small), M (medium), or B (big) prefix, corresponding to a range of 10--60 Hz, 60--100 Hz, and above 100 Hz, respectively. If the dynamic range is less than 10 Hz, the contour is classified as flat. The basic pitch contour shapes (except flat) are portrayed in Fig. \ref{fig:pc_label}.
 \begin{figure}[!ht]
	\centerline{\includegraphics[height=7cm, width=8cm]{pitch_contours_10.png}}
	\caption{Basic Pitch Contour Shapes Considered: (a)~L, (b)~H, (c)~HHL, (d)~LHH, (e)~HLL, (f)~LLH, (g)~HLH, (h)~LHL, (i)~hat, (j)~bucket}
	\label{fig:pc_label}
\end{figure}

\section{Performance Evaluation}
\label{sec:perf}
\subsection{Speech Corpus}
\label{sec:corpus}
The corpus used in the current work consists of read speech data recorded by professional, native speakers of Tamil and Hindi. The data for each language comprises around 4000 utterances, each of length up to 3 seconds, recorded by one male and one female speaker, in a studio environment, at a sampling rate of 16 kHz. This adds up to around 12 hours of data in each language for each speaker \cite{hts_cocosda}. Indian English data is recorded from the two native Tamil speakers, also spanning 12 hours each. Around five minutes of data from this corpus for each language is manually segmented at the phoneme level by open source software. Pitch contour labels at the syllable level and break indices are also manually assigned for this data. These are used to evaluate the performance of the proposed algorithms. The remaining data are used to train the phoneme HMMs to perform segmentation.

\subsection{Accuracy of Segmentation}
\label{sec:seg_acc}
The accuracy with which the tool segments the given speech signals using the HMM-based forced-Viterbi alignment procedure is evaluated at the phoneme level, as syllable and word-level segmentations are derived from the phoneme-level segmentation. To evaluate the accuracy, the segmentation error is calculated as the time difference between the manually derived duration for each phoneme and the corresponding duration obtained from the tool. The distribution of the segmentation errors for each language when language-dependent and language-independent models are used is shown in Fig.~\ref{fig:lang_dep_indep}.

It can be observed that ninety percent of segments in the language-dependent models across all three languages exhibit errors of less than 10ms . On average, the error lies between 10 to 30ms for the three languages when using both language-dependent and language-independent models. There is marginal difference of 10\% error observed in language-independent models compared to language-dependent models arises from the use of shared phonetic models across languages, which leads to some generalization errors. Nevertheless, this trade-off is deemed acceptable given the increased flexibility and efficiency offered by language-independent models, which are essential for accommodating multiple languages effectively.

\begin{figure}[!ht]
    \raggedright
    \setlength{\abovecaptionskip}{5pt}   % Adjust space above caption
    \setlength{\belowcaptionskip}{10pt}  % Adjust space below caption
    \setlength{\floatsep}{10pt}          % Adjust space between floats
    \setlength{\textfloatsep}{10pt}      % Adjust space between float and text

    \begin{subfigure}[t]{0.45\textwidth} % Increased width to 45% of text width
        \raggedright
        \includegraphics[width=\textwidth, trim={0 0 0 0}, clip]{NEW-LAN-TAMIL.png}
        \caption{Tamil: Independent and Dependent }
    \end{subfigure}
    \hspace{\fill} % Flexible horizontal space to prevent lines
    \begin{subfigure}[t]{0.45\textwidth} % Increased width to 45% of text width
        \raggedright
        \includegraphics[width=\textwidth, trim={0 0 0 0}, clip]{NEW-LAN-HINDI.png}
        \caption{Hindi: Independent and  Dependent}
    \end{subfigure}
    
    \vspace{10pt} % Add some vertical space before the next row
    
    \begin{subfigure}[t]{0.45\textwidth} % Increased width to 45% of text width
        \raggedright
        \includegraphics[width=\textwidth, trim={0 0 0 0}, clip]{NEW-LAN-ENG.png}
        \caption{English:Independent and Dependent}
    \end{subfigure}
    
    \caption{Comparison of Segmentation Error for Language-Independent and Language-Dependent Models (Tamil, Hindi, English)}
    \label{fig:lang_dep_indep}
\end{figure}

% for the language-dependent model.It is observed that the mean error for English is 0.017s, for Tamil is 0.002s, and for Hindi is 0.084s. The increased error in Hindi is attributed to the presence of fricatives, which pose challenges in precise segmentation.
% It is observed the mean error language-independant for English is 0.025s, for Tamil is 0.010s and for Hindi 0.031s.

\subsection{Accuracy of Break Indices}
\label{sec:bi_acc}
To assess the accuracy of break indices derived from the tool, these were compared with the manual annotations. 
As shown in Table \ref{tab:Accuracy-Break-Indices}, the break indices are derived with an accuracy of around 95\%. It is observed that in the remaining 5\% of the silence regions, the boundaries detected based on the spectral flatness is inaccurate resulting mostly in the incorrect identification of a break index of 2 as 3 or 1. Further, some of the silence regions occur before or between stops consonants and fricatives were not considered as silence. This is reflected in the confusion matrices in Fig. \ref{fig:bi_conf}. The lower accuracy in Hindi compared to Tamil and English can be attributed to the influence of fricatives. Silence preceding fricatives tends to have a lower spectral flatness (SF) value, which may contribute to the decreased accuracy in Hindi.

\begin{table}[h]
	\centering
	\caption{Overall Accuracy of Break Indices for three Languages}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Language} & \textbf{Overall Accuracy (\%)} \\ \hline
		Hindi   & 91.53  \\ \hline
		
		English & 97.62  \\ \hline
		Tamil   & 98.56  \\ \hline
	\end{tabular}
	\label{tab:Accuracy-Break-Indices}
\end{table}

\begin{figure}[!ht]
	\centerline{\includegraphics[height=12cm, width=6cm]{cm_bi_new1.png}}
	\caption{Confusion Matrices for the Identification of Break Indices: (a)~Tamil, (b)~English, (c)~Hindi}
	\label{fig:bi_conf}
\end{figure}

\subsection{Accuracy of Pitch Contour Labeling}
\label{sec:pc_acc}
As in the previous sections, the accuracy of the pitch contour labels is assessed by comparing the manual annotations with those derived from the tool. Manual labeling involves a careful analysis of the audio signals, where expert annotators listen to the speech and identify pitch contours by visually examining the waveform and their pitch frequency. The annotators mark significant points, such as rise and fall, to capture the contour patterns present in the speech. It is observed that the designed rules capture the shape of the pitch contours with an accuracy of approximately 99\%, as summarized in Table~\ref{tab:accuracy-pitch}. The LLH label has a high error rate for English, while the L label has a high error rate for both Tamil and Hindi.
\begin{table}[h]
	\centering
	\caption{Accuracy of Pitch Labels for Three Languages}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Language} & \textbf{Accuracy of pitch labels (\%)} \\ \hline
		Tamil  & 99.91  \\ \hline
		Hindi  & 99.78  \\ \hline
		English & 97.24  \\ \hline
	\end{tabular}
	\label{tab:accuracy-pitch}
\end{table}

\section{Influence of pitch contour on Language Identification Using the SIToBI Tool}
\label{sec:lid}
This study aims to check whether the frequency of pitch contours has any influence on language identification. Specifically, the current work attempts to identify the language of a given word based on syllable-level pitch contours, as prosodic information varies across languages. For this task, Tamil and Hindi are considered. Initially, monosyllabic words from the corpus described in Section~\ref{sec:corpus} are used, comprising 1500 monosyllabic words from both Tamil and Hindi data. Pitch contour labels are estimated from 90\% of these words, and the normalized frequency of occurrence for each contour in each language is computed. For the remaining 10\% of the words, the previously computed normalized frequency of occurrence is assigned as a score based on the pitch contour.

This analysis is extended to bisyllabic, trisyllabic, quadsyllabic, and pentasyllabic words, where the normalized frequency of occurrence of a contour is computed for each syllable category (e.g., bisyllabic or trisyllabic). When identifying the language of a word, the cumulative score obtained across all syllables is considered, with the language having the highest cumulative score being identified as the language of the word.

Figure~\ref{fig:lid} illustrates the accuracy of language identification for given words. It is observed that identifying Hindi words becomes easier as the number of syllables increases. Only around 52\% of monosyllabic words are identified correctly, while 73\% to 82\% of trisyllabic, quadsyllabic, and pentasyllabic words are accurately identified. In contrast, Tamil shows slightly lower identification accuracy for trisyllabic and quadsyllabic words, around 65\%, whereas other word lengths achieve an accuracy exceeding 72\%.

\begin{figure}[h]
    \centerline{\includegraphics[height=5cm, width=8cm]{Langid.png}}
    \caption{Language Identification Accuracy for Mono, Bi, Tri, Quad, and Penta-syllabic words}
    \label{fig:lid}
\end{figure}




\section{Conclusion}
\label{sec:conc}
The proposed SIToBI tool provides rich prosodic features for three languages, namely Indian English, Tamil, and Hindi. The performance of each module of the tool has been compared with manual annotations, and the accuracy has been computed, demonstrating its effectiveness in capturing phonetic and prosodic details. While the current work focused on these three languages, the tool can be easily extended to other languages by retraining the phoneme HMMs and incorporating automatic speech recognition (ASR) systems for the new languages.
Additionally, the modular design of the tool allows for adaptability to various prosodic feature extraction tasks, making it a versatile tool for speech processing across different linguistic contexts. The general syllabification algorithm used in both language-dependent and language-independent models ensures that the tool can handle diverse language structures with minimal degradation in performance. Future work could explore applying this tool to low-resource languages, as well as refining the pitch contour labeling for better accuracy in tonal languages. The SIToBI tool has the potential to significantly contribute to multilingual speech processing and analysis in a variety of applications, from language identification to speech synthesis and recognition.


\begin{thebibliography}{00}

\bibitem{tobi}
D. R. Ladd, \textit{Tone and Intonation in English}, Cambridge: Cambridge University Press, 1996.

\bibitem{tobi1}
M. E. Beckman and G. Ayers Elam, ``The ToBI Annotation Conventions,'' The Ohio State University, 1997.

\bibitem{autobi}
A. Rosenberg, ``AuToBI: a tool for automatic ToBI annotation,'' in \textit{Proceedings of Interspeech}, 2010, pp. 146-149.	

\bibitem{praat}
P. Boersma and D. Weenink, ``Praat: doing phonetics by computer,'' 2011. [Online]. Available: http://www.fon.hum.uva.nl/praat/

\bibitem{webmaus}
Thomas Kisler, F. Schiel and H. Sloetjes, ``Signal processing via web services: The use case WebMAUS'', in \textit{Proceedings of Digital Humanities}, 2012, pp. 30-34.

\bibitem{pytobi}
A. Mitrovic, M. J. Hadfield, and C. Goeritz, ``PyToBI: a Python-based tool for automatic ToBI annotation,'' in \textit{Proceedings of ICASSP}, 2020, pp. 7485-7489.

\bibitem{jtobi}
J. J. Venditti, ``The J\_ToBI model of Japanese intonation,'' in \textit{Prosodic Models and Transcription: Towards Prosodic Typology}, S.-A. Jun, Ed. Oxford: Oxford University Press, 2005, pp. 172-200.

\bibitem{gtobi}
M. Grice, S. Baumann, and R. Benzmüller, ``German Intonation in Autosegmental-Metrical Phonology,'' in \textit{Prosodic Typology: The Phonology of Intonation and Phrasing}, S.-A. Jun, Ed. Oxford: Oxford University Press, 2005, pp. 55-83.

\bibitem{sptobi}
J. I. Hualde and P. Prieto, ``Intonational variation in Spanish: European and American varieties,'' in \textit{Handbook of Spanish Linguistics}, J. I. Hualde, A. Olarrea, and E. O'Rourke, Eds. Malden, MA: Wiley-Blackwell, 2012, pp. 216-232.

\bibitem{ftobi}
S.-A. Jun and C. Fougeron, ``A phonological model of French intonation,'' in \textit{Intonation: Analysis, Modelling and Technology}, A. Botinis, Ed. Dordrecht: Kluwer Academic Publishers, 2000, pp. 209-242.

\bibitem{ctobi}
P. Wong, W. W. Lam, and J. K. Chan, ``C-ToBI: Cantonese Tones and Break Indices,'' in \textit{Proceedings of the 15th International Congress of Phonetic Sciences}, 2003, pp. 2645-2648.

\bibitem{hindi_int}
R. Mannell and A. Sengar, ``An Approach to ToBI Transcription System for Hindi,'' abstract, Fifth International Congress on English Grammar, G. Narayanamma Institute of Technology \& Science, 2008, pp. 51-51.

\bibitem{ind_intonation}
K. S. Rao and B. Yegnanarayana, ``Intonation modeling for Indian languages,'' \textit{Computer Speech \& Language}, vol. 23, no. 2, pp. 240-256, 2009.

\bibitem{bengali_pro}
S. D. Khan, ``Intonational Phonology and Focus Prosody of Bengali,'' in \textit{Prosodic Typology II: The Phonology of Intonation and Phrasing}, S.-A. Jun, Ed. Oxford: Oxford University Press, 2014, pp. 81-110.

\bibitem{data2vec}
V. S. Lodagala, S. Ghosh and S. Umesh, ``Data2vec-Aqc: Search for the Right Teaching Assistant in the Teacher-Student Training Setup'', \textit{IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2023, pp. 1-5.

\bibitem{ijst}
G. Anushiya Rachel, V. Sherlin Solomi, K. Naveenkumar P. Vijayalakshmi and T. Nagarajan, ``A small footprint context-independent HMM-based speech synthesizer for Tamil'', \textit{International Journal of Speech Technology}, Vol. 18, No.3, pp. 405 – 418, 2015. 

\bibitem{hts_cocosda}
H. A. Patil, T. B. Patel, N. J. Shah, H. B. Sailor, R. Krishnan, G. R. Kasthuri, T. Nagarajan, L. Christina, N. Kumar, V. Raghavendra, S. P. Kishore, S. R. M. Prasanna, N. Adiga, S. R. Singh, K. Anand, P. Kumar, B. C. Singh, S. L. Binil Kumar, T. G. Bhadran, T. Sajini, A. Saha, T. Basu, K. S. Rao, N. P. Narendra, A. K. Sao, R. Kumar, P. Talukdar, P. Acharyaa, S. Chandra, S. Lata, and H. A. Murthy, ``A syllable-based framework for unit selection synthesis in 13 Indian languages,'' in \textit{2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)}, 2013, pp. 1-8.

\bibitem{anushiyarachel15_interspeech}
G. Anushiya Rachel, P. Vijayalakshmi, and T. Nagarajan, ``Estimation of glottal closure instants from telephone speech using a group delay-based approach that considers speech signal as a spectrum,'' in \textit{Interspeech 2015}, pp. 1181–1185, 2015. doi: 10.21437/Interspeech.2015-229.

\bibitem{prakash2014approach}
A. Prakash, M. R. Reddy, T. Nagarajan, and H. A. Murthy, 
"An approach to building language-independent text-to-speech synthesis for Indian languages," 
in *Twentieth National Conference on Communications (NCC)*, Kanpur, India, 2014, pp. 1--5, 
doi: 10.1109/NCC.2014.6811356.


\bibitem{boothalingam2013development}
R. Boothalingam et al., 
"Development and evaluation of unit selection and HMM-based speech synthesis systems for Tamil," 
\textit{2013 National Conference on Communications (NCC)}, New Delhi, India, 2013, pp. 1-5, 
doi: 10.1109/NCC.2013.6487984.

\bibitem{parselmouth}
Y. Jadoul, B. Thompson and B. de Boer, 
"Introducing Parselmouth: A Python interface to Praat," 
\textit{Journal of Phonetics}, vol. 71, pp. 1--15, 2018, 
doi: https://doi.org/10.1016/j.wocn.2018.07.001.

\bibitem{prakash2023tts}
A. Prakash, S. Umesh and H. A. Murthy, 
"Towards Developing State-of-The-Art TTS Synthesisers for 13 Indian Languages with Signal Processing Aided Alignments," 
\textit{IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, Taipei, Taiwan, pp. 1--8, Dec. 2023.



\end{thebibliography}


\end{document}
