%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% \captionsetup[subfigure]{labelformat=simple, labelsep=period} % 设置简单标签格式
% \renewcommand\thesubfigure{(\alph{subfigure})}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[dvipsnames]{xcolor}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tcolorbox} % 用于创建文本框
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{lipsum} % 示例用
\usepackage{bbm}
% \usepackage{subcaption} % 使用 subcaption 包来管理子图

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{xurl}
\usepackage[breaklinks]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algpseudocode}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxivpreprint}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow} 
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{math_commands.tex}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models}

\begin{document}

\twocolumn[
\icmltitle{Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Muxing Li}{unimelb}
\icmlauthor{Zesheng Ye}{unimelb}
\icmlauthor{Yixuan Li}{uwm}
\icmlauthor{Andy Song}{rmit}
\icmlauthor{Guangquan Zhang}{uts}
\icmlauthor{Feng Liu}{unimelb}
% \icmlauthor{}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{unimelb}{School of Computing and Information Systems, The University of Melbourne}
\icmlaffiliation{uwm}{Department of Computer Sciences, University of Wisconsin-Madison}
\icmlaffiliation{rmit}{Royal Melbourne Institute of Technology}
\icmlaffiliation{uts}{University of Technology Sydney}

\icmlcorrespondingauthor{Feng Liu}{fengliu.ml@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Membership Inference Attack, Generative Models, Model Distillation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Membership inference attacks (MIAs) determine whether certain data instances were used to train a model by exploiting the differences in how the model responds to seen versus unseen instances. 
This capability makes MIAs important in assessing privacy leakage within modern generative AI systems.
However, this paper reveals an oversight in existing MIAs against \emph{distilled generative models}: attackers can no longer detect a teacher model's training instances individually when targeting the distilled student model, as the student learns from the teacher-generated data rather than its original member data, preventing direct instance-level memorization.
Nevertheless, 
we find that student-generated samples exhibit a significantly stronger distributional alignment with teacher's member data than non-member data.
This leads us to posit that MIAs \emph{on distilled generative models should shift from instance-level to distribution-level statistics}.
We thereby introduce a \emph{set-based} MIA framework that measures \emph{relative} distributional discrepancies between student-generated data\emph{sets} and potential member/non-member data\emph{sets}, Empirically, distributional statistics reliably distinguish a teacher's member data from non-member data through the distilled model. Finally, we discuss scenarios in which our setup faces limitations.

% % 
\end{abstract}

\section{Introduction}
Recent advances in generative models have set new standards for synthesizing high-quality content across modalities, such as images \cite{ho2020denoising} and languages \cite{brown2020language}. 
This progress has quickly translated into successful commercialization through online services such as ChatGPT and Midjourney \cite{zierock2023leveraging}.

However, the extensive datasets required to train these models often contain sensitive information from individuals who may not have explicitly consented to the use of their data for model development.
This concern is particularly pressing given the widespread adoption of \emph{large language models} (LLMs) \cite{floridi2020gpt} and diffusion models \cite{ho2020denoising, song2023consistency} in commercial applications and the potential for companies to train models on scraped or unauthorized data.
In this context, \emph{membership inference attacks}~(MIAs) \cite{carlini2022membership}, designed to detect whether specific data were used in training, offer a valuable auditing mechanism for detecting potential privacy violations and unauthorized data usage.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figure/mot3.png}
%     \caption{
%     % The distillation process of one-step distilled diffusion models uses data generated by their teacher diffusion models. Similarly, the distillation data for lightweight LLMs also comes from their teacher LLMs. Therefore, distilled generative models \textbf{never access} the training data of their teacher models.
%     Conceptual illustration of \emph{model distillation} for \emph{generative models}.
%     Both diffusion models and LLMs rely on synthetic data produced by teacher models rather than the original training data to train student models, resulting in a clear separation between student models and the original training sets.
%     }
%     \label{fig:0_distillation_concept}
% \end{figure}

MIAs build upon a core assumption: machine learning models \emph{overfit} to their training set, exhibiting different behaviors between training and test data instances \cite{yeom2018privacy}.
Models develop measurable ``behavioral signatures'' when processing seen instances, typically showing more concentrated probability densities than their responses to unseen instances.
In diffusion models, for example, such signatures can manifest during the denoising process, where training instances produce lower estimation errors than non-training ones \cite{duan2023diffusion}.
Recent studies \cite{carlini2023extracting, hu2023loss, duan2023diffusion} have investigated these patterns extensively, hypothesizing that generative models can memorize and reconstruct their training \emph{instances}.

Yet, MIAs require careful reconsideration in modern generation services.
Deploying generative models on a scale requires substantial computational resources, often necessitating hundreds or even thousands of expensive GPUs \cite{hu2024blockllm}.
Diffusion models, in particular, involve thousands of denoising steps \cite{song2020denoising, geng2024one}, making efficient deployment a priority for commercial platforms.
Recently, knowledge distillation \cite{yin2024one, yin2024squeeze} has demonstrated that distilled models (a.k.a. students) can achieve generation quality comparable to their original generative models (a.k.a. teachers). For example, DeepSeek-V3 \citep{liu2024deepseek} distills from highly-complex reasoning models DeepSeek-R1~\citep{guo2025deepseek} and achieves commendable math reasoning ability. 
On the other hand, one-step diffusion models \cite{yin2024one}, by distilling from diffusion models, can synthesize images with fine details within a single step.
As such, model distillation enables a two-tier deployment strategy, where lightweight distilled models serve end-users directly, while teacher models focus on student training and fine-tuning.
% Recently, knowledge distillation \cite{yin2024one, yin2024squeeze} has shown that distilled models (a.k.a. student) can match the generation quality of the original diffusion model (a.k.a. teacher) even in just one step, enabling a two-tier deployment strategy: lightweight distilled models serve end-users directly, while the teacher models focus on student training. For example, DeepSeek-V2, distilled from GPT, and one-step diffusion models, distilled from diffusion models
% Recent advances in knowledge distillation \cite{yin2024one, yin2024squeeze} have introduced a two-tier deployment strategy: a lightweight student model, distilled from the high-capacity teacher model, can generate images in a single step while maintaining the original model's generation quality.
% reveal that distilled models (a.k.a. student) can match the generation quality of the original diffusion model (a.k.a. teacher) even in just one step, enabling a two-tier deployment strategy: lightweight distilled models serve end-users directly, while teacher models focus on student training.
However, this strategy also introduces a critical security caveat: the student learns from the teacher's outputs, not the original training data, as Fig.~\ref{fig:0_distillation_concept} shows.
This breaks the chain of data memorization that MIAs exploit, raising a question: \textit{{Can unauthorized data use in teacher models be detected through their distilled student models}}?
Our investigation in Sec.~\ref{sec: i_mia_failure} tests four MIA strategies~\cite{chen2020gan, duan2023diffusion, li2024towards, pang2023white} against two state-of-the-art (SOTA) student generative models \cite{yin2024one, luo2024diff} distilled from a diffusion model~\cite{karras2022elucidating}.
While MIAs effectively identify training data in teacher models, they consistently fail with student models, implying that student models retain \emph{insufficient membership information at the instance level}.

% Recently, many studies have attempted to leverage model distillation techniques to distill a one-step generative model (i.e. student model) from multistep denoising diffusion models (i.e. teacher model). The one-step generative model not only preserves the generative quality of the original model, but also allows for faster generation \cite{yin2024one,luo2024diff,song2023consistency}. 
% % In this way, traditional diffusion models may eventually become an intermediate product for generative models, while only the distilled student models will be used to provide services to end users.
% % the distilled one-step generative model elevates the challenge of data copyright disputes to a new level, particularly in scenarios where only the distilled model is accessible. 
% This raises a critical question: \textit{How can the unauthorized data usage of the teacher model be detected while only accessing its distilled student model?} 
% If this issue remains unresolved, it allows model developers to circumvent data copyright disputes by distilling a model trained on unauthorized data into a new student model. 

% However, distillation has been shown in numerous studies as an effective technique for protecting the privacy of model training data\cite{matthew2023students}. This protection becomes even more evident in distilled generative models. Because during the distillation process of the diffusion model, the student model learns to generate samples by learning the data distribution of the generative data from the teacher model. It does not retain explicit memory of the teacher model training data that were not used during its own training process. Moreover, when attacking traditional neural network models, a candidate data point typically includes an input and its corresponding ground truth. However, when attacking diffusion models, the input used during their training (i.e., random noise) is not accessible.

The failure of instance-wise MIA motivates us to investigate: \emph{Does membership information manifest collectively in the data distribution}?
In Sec.~\ref{sec: distribution_investigation}, we examine this through {maximum mean discrepancy}~(MMD), comparing student-generated samples against both teacher's \textcolor{SkyBlue}{member} and \textcolor{Apricot}{non-member} data distributions (Fig. \ref{fig:1_comparison_MIA_DMIA}(c)).
Through repeated random subset samplings, we observe a consistent statistical pattern--distances to \textcolor{Apricot}{non-member data} concentrate at higher values than to \textcolor{SkyBlue}{member data}, suggesting that the student preserves statistical signatures exhibiting stronger alignment with teacher's member distribution than non-member distributions, despite the failure of instance-level attacks.

\begin{tcolorbox}[colframe=black, colback=gray!10, arc=2mm, width=\linewidth, boxrule=0.2mm, boxsep=1mm]
    \textbf{Position}:
    Membership Inference Attacks (MIAs) for distilled generative models should shift from \emph{instance-level} scores to \emph{distribution-level} statistics.
\end{tcolorbox}
\textbf{Why Distributional-Level Statistics?}
First, the landscape of MIAs has evolved significantly with the emergence of large-scale training.
The increased scale of training dataset and model capacity reduces the ``overfit'' effect on member instances, thus blurring differences between individual data instances that conventional instance-level MIA methods typically exploit~\citep{dong2024generalization, ye2024data}; not to mention that model distillation~\citep{hinton2015distilling} further weakens the individual membership signal, as the distilled student models never directly access training data (discussed in Sec. \ref{sec: 2_I_MIA}).
Second, the discriminative power between members and non-members increases when examining multiple instances collectively on a dataset basis, because the aggregation amplifies subtle but consistent membership differences that instance-level methods might overlook (see  Sec.~\ref{sec: 3_distribution}).
% Second, distribution-level statistics are sensitive to weak but consistent membership attribution signals that instance-level predictions might overlook, thus minimizing Type II errors in the evaluation framework \cite{yamane1973statistics};
% Second, through the Central Limit Theorem \cite{billingsley2013convergence}, the analysis of instance collections reduces statistical variance in membership predictions, yielding more reliable evaluation metrics.
Moreover, from a privacy protection standpoint, set-based MIA evaluation with distributional statistics moves away from binary membership decisions on individual samples, making the MIA practice more resistant to potential misuse in data extraction attempts (see Sec.~\ref{sec: 5_implication}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/mot3.png}
    \vspace{-2em}
    \caption{
    Conceptual illustration of \emph{model distillation} for \emph{generative models}.
    Both diffusion models and LLMs rely on synthetic data produced by teacher models rather than the original training data to train student models, resulting in a clear separation between student models and the original training sets.
    }
    \label{fig:0_distillation_concept}
\end{figure}


\textbf{How to move to Distributional Statistics?}
Accordingly, we introduce an MIA framework called \textbf{D-MIA}, tailored for distilled generative models, which quantifies distributional discrepancies and examines the relative relationship between two quantities: 1) the \emph{distributional distance} between candidate data and student-generated data, and 2) the \emph{distributional distance} between known non-member data and student-generated data.
D-MIA operates in two phases. 
% \emph{training} phase and an \emph{evaluation} phase.
During \emph{training}, one needs to optimize a deep kernel MMD-based measure \citep{liu2020learning} that distinguishes between member and non-member data through their relationships with student-generated data, achieved by training a kernel to maximize separability in the distributional relationships of these two classes relative to the student-generated data.
In \emph{evaluation}, D-MIA uses this measure to assess whether a particular candidate set (containing multiple target samples) is closer to the student-generated distribution than to non-member data.
Namely, a candidate dataset is likely to contain member data if it exhibits smaller MMD values to the student generation than non-member data does.

\textbf{Structure}.
In Sec. \ref{sec: 2_I_MIA}, we revisit existing MIAs (i.e., instance-level MIAs) on generative models and reveal their failures on distilled generative models.
We then justify the use of distribution-level statistics for MIAs in distillation settings in Sec.~\ref{sec: 3_distribution}.
Following, Sec. \ref{sec: 4_framework} introduces D-MIA with a set-based evaluation setup, and showcases empirical performances through experiments on SOTA  one-step distilled generative models.
We discuss the implications (Sec. \ref{sec: 5_implication}) and limitations (Sec. \ref{sec: 6_limitation}) of D-MIA, and outlook the possible explorations in Sec.~\ref{sec: 7_final}.


\section{Instance-level MIAs are not suitable for distilled generative models}
\label{sec: 2_I_MIA}
% Current MIA methods are instance-level, which not only become unreliable when attacking student models in distillation scenarios but may also be exploited as tools for stealing a model’s training data.

\subsection{MIAs for Generative Models}
MIAs evaluate whether specific data samples were used during model training.
Let $\gX$ be the data space and $\gD_{\rm mem} \subset \gX$ be the member set used to train a generative model $G: \gZ \to \gX$ that transforms noises sampled from a latent distribution $\rvz \sim p(\rvz)$ into synthetic data $\rvx = f_{g}(\rvz) \in \gX$.
Given a query sample $\vx_q \in \gX$, a MIA constructs a binary classifier $\gA: \gX \times G \to \{0, 1\}$ that predicts the membership attribution of $\rvx_q$ as $\gA(\vx_q, G) = 1$ if $\vx_q \in \gD_{\rm mem}$, and $0$ otherwise.
% \begin{equation*}
%     \scalebox{0.9}{$
%     \gA(\vx_q, G) = 
%     \begin{cases}
%         1 & \text{if } \vx_q \in \gD_{\rm mem} \\
%         0 & \text{otherwise.}
%     \end{cases}
%     $}
% \end{equation*}
Commonly \cite{carlini2022membership, choquette2021label}, the attack performance is evaluated through \emph{attack success rate}~(ASR)  that quantifies the weighted average of successful predictions across both member and non-member samples, as well as \emph{area under the curve}~(AUC) that captures the attack's discriminative power independent of specific decision thresholds.
% For a membership inference attack $\gA$ operating on samples from member set $\gD_{\rm mem}$ and non-member set $\gD_{\rm non}$, we define
% \begin{equation*}
%     \begin{aligned}
%         \mathrm{ASR} &= \frac{\mathrm{Number\ of\ Successful\ Attacks}}{\mathrm{Total\ Number\ of\ Attacks}}, \\  
%         \mathrm{AUC}_{\mathcal{A}} &= \int_{0}^{1} \mathrm{TPR}_{\mathcal{A}} \left( \mathrm{FPR}_{\mathcal{A}}^{-1} (t) \right) \mathrm{d}t,
%     \end{aligned}
% \end{equation*}
% where
% \begin{equation*}
%     \begin{aligned}
%         \mathrm{TPR}_{\gA} & = \frac{1}{|\gD_{\rm mem}|} \sum_{\vx \in \gD_{\rm mem}} \mathbbm{1} \left( \gA(\vx, G) = 1) \right), \text{ and} \\
%         \mathrm{FPR}_{\gA} & = \frac{1}{|\gD_{\rm non}|} \sum_{\vx \notin \gD_{\rm mem}} \mathbbm{1} \left( \gA(\vx, G) = 1) \right).
%     \end{aligned}
% \end{equation*}
% $\mathrm{TPR}_{\gA}$ denotes the fraction of correctly identified member samples, while $\mathrm{FPR}_{\gA}$ represents the fraction of non-member samples incorrectly classified as members.

In the existing MIA literature, instance-level statistics are primarily exploited to distinguish member samples from non-member samples.
We term this approach instance-level MIA (I-MIA), which encompasses two primary categories: \emph{reference-based} and \emph{intrinsic-based} I-MIAs, depending on where and how they surface the discriminative statistics.

% Since the emergence of diffusion models, they have been widely discussed for their exceptional generative quality, which is largely attributed to the use of high-quality training data. 
% The requirements for the quantity and quality of data have significantly increased the cost of model training. Using copyrighted data without authorize has become an option to reduce these training costs, thus sparked significant controversy regarding the copyright of the training data. 
% MIA has consistently been regarded as a tool for monitoring unauthorized data usage as it serves as evidence that a certain data was used to train a model. Existing MIAs can generally be divided into two types: behavior-based MIAs and sample-overfitting-based MIAs.



% \begin{table*}[ht]
% \centering
% \caption{Performance comparison of ASR and AUC metrics for various MIA methods (GAN-leak, SecMI, Rediffuse, and GSA) across three datasets (CIFAR10, FFHQ, AFHQv2). For each target model (EDM, DMD, DI), a separate model was trained on each dataset, and the metrics were computed for each combination. The Average column summarizes the overall performance of each method. \\ \textbf{Note}: Since SecMI relies on the model’s loss function for detection, and the distillation loss function of the one-step diffusion model involves the teacher diffusion model, this contradicts the scenario where only the one-step diffusion model is accessible. Therefore, we replaced the distillation loss with the EDM’s loss function.}
% \vspace{1mm}
% % \resizebox{0.8\textwidth}{!}{%
% \begin{tabular}{llcccccccccc}
% \toprule
% \multirow{2}{*}{Target Model} & \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{GAN-leak} & \multicolumn{2}{c}{SecMI} & \multicolumn{2}{c}{Rediffuse} & \multicolumn{2}{c}{GSA} & \multicolumn{2}{c}{Average} \\ 
% \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10} \cmidrule(r){11-12}
%                               &                         & ASR         & AUC         & ASR         & AUC         & ASR          & AUC         & ASR         & AUC        & ASR          & AUC         \\ 
% \midrule
% \multirow{3}{*}{EDM(teacher model)}          & CIFAR10                & 0.556       & 0.573       & 0.593       & 0.617       & 0.579        & 0.602       & 0.630       & 0.633      & 0.595        & 0.606       \\
%                               & FFHQ                   & 0.524       & 0.518       & 0.510       & 0.511       & 0.523        & 0.521       & 0.535       & 0.540      & 0.523        & 0.523       \\
%                               & AFHQv2                 & 0.563       & 0.532       & 0.611       & 0.624       & 0.656        & 0.701       & 0.910       & 0.911      & 0.685        & 0.692       \\ 
% \midrule
% \multirow{3}{*}{DMD(one-step model)}          & CIFAR10                & 0.497       & 0.508       & 0.509       & 0.492       & 0.515        & 0.512       & 0.505       & 0.502      & 0.506        & 0.504       \\
%                               & FFHQ                   & 0.502       & 0.498       & 0.507       & 0.487       & 0.515        & 0.513       & 0.510       & 0.499      & 0.508        & 0.499       \\
%                               & AFHQv2                 & 0.510       & 0.475       & 0.513       & 0.511       & 0.532        & 0.537       & 0.530       & 0.523      & 0.521        & 0.512       \\ 
% \midrule
% \multirow{3}{*}{DI(one-step model)}           & CIFAR10                & 0.502       & 0.497       & 0.512       & 0.508       & 0.512        & 0.509       & 0.510       & 0.507      & 0.512        & 0.505       \\
%                               & FFHQ                   & 0.493       & 0.503       & 0.506       & 0.515       & 0.505        & 0.496       & 0.510       & 0.510      & 0.503        & 0.506       \\
%                               & AFHQv2                 & 0.501       & 0.502       & 0.500       & 0.507       & 0.510        & 0.502       & 0.500       & 0.500      & 0.502        & 0.503       \\ 
% \bottomrule
% \end{tabular}%
% % }
% \label{tab:results}
% \end{table*}


\textbf{Reference-based I-MIAs} aim to identify such statistics through carefully constructed \emph{reference models}.
Given a target generative model $G$, one needs to construct $n$ structural-similar or identical reference models $\{ G_{i}^{\rm ref} \}_{i=1}^{n}$, leading to two complementary sets of models for a query sample $\vx_q$,
{\footnotesize
\begin{equation*}
\vspace{-1.5em}
    \begin{aligned}
        \gM_1 = \{ G_{i}^{\rm ref} : \vx_q \in \gD_{\rm mem}^{i} \} \text{ and }
        \gM_0 = \{ G_{i}^{\rm ref} : \vx_q \notin \gD_{\rm mem}^{i} \}.
    \end{aligned}
\end{equation*}
}

The membership inference decision is then based on comparing certain pre-defined behavioral signatures of the target model $\phi(G, \vx)$ with these groups, such that $\gA(\vx, G) = 1$ if a difference metric $\Delta(\vx, G) > \tau$ and $0$ otherwise, where $\Delta(\vx, G) \triangleq \mathrm{sim}(\phi(G, \vx), \gM_1) - \mathrm{sim}(\phi(G, \vx), \gM_0)$ and $\tau$ is the decision threshold.
The behavioral signature $\phi(G, \vx)$ can take various forms like reconstruction error \cite{chen2020gan, shokri2017membership} and likelihood \cite{carlini2022membership}.
However, this approach faces practical infeasibility for computationally intensive generative models like diffusion models, as training $n \geq 1$ reference models would substantially increase the attack cost.
This explains the rationale for intrinsic-based I-MIAs to analyze membership privacy in high-cost generative models.

\textbf{Intrinsic-based I-MIAs} directly leverage the statistical gaps that emerge from target model training.
At their core, these attacks exploit a fundamental memorization tendency of generative models $G: \gZ \to \gX$, i.e., the target model behaves differently between member instances $\gD_{\rm mem}$ and non-member instances $\gD_{\rm non}$, quantified as $\Delta(\rvx, G) = \E_{\vx \sim \gD_{\rm mem}} \left[ \gL (\vx; G) \right] - \E_{\vx^{\prime} \sim \gD_{\rm non}} \left[ \gL (\vx^{\prime}; G) \right] < 0$.
This statistical gap may manifest differently across generative architectures, leading to model-specific attack strategies.
% The second type, sample-overfitting-based MIAs, does not rely on reference models but instead uses the inherent instance-level overfitting of the target model to its training data. During training, the model produces results that more closely match the ground truth of the data it has seen, resulting in lower losses. Conversely, for unseen data, the model’s results tend to have larger discrepancies with the ground truth, resulting in higher losses. 
% This difference allows for determining whether a data point was used in training the model. 
% Existing MIAs targeting generative models are all based on this principle. 
% For example, GAN-Leak \cite{} is an MIA method targeting generative adversarial networks (GANs) \cite{}. 
GAN-Leak \cite{chen2020gan}, for example, targets MIA on \emph{generative adversarial networks} \cite{goodfellow2020generative} by reconstructing target images through latent optimization, solving $\gL_{\rm GANLeak}(\rvx) = \min_{\vz \in \gZ} \| \rvx - G(\vz) \|_2^2$ where members $\vx \sim \gD_{\rm mem}$ often show lower reconstruction errors.

Diffusion models \cite{ho2020denoising, song2020denoising}, which operate through a forward process $q(\vx_t | \vx_0)$ that progressively adds Gaussian noise $\epsilon \sim \gN(0, \rmI)$ to the input $\vx_0$ and a reverse noise removal process $p_{G}(\vx_0 | \vx_t)$ that reconstructs the original $\vx_0$ over time steps $t \in [0, T]$.
% For example, GAN-Leak \cite{chen2020gan} targets \emph{generative adversarial networks} (GANs) \cite{} by firstly optimizing the latent space to reconstruct target images $\vx$ from noise $\vz$ by solving $\gL_{\rm GAN-Leak}(\rvx) = \min_{\vz \in \gZ} \| \rvx - G(\vz) \|_2^2$, where member instances $\vx \sim \gD_{\rm mem}$ typically show lower reconstruction errors.
% This method attempts to simulate the image generation process to attack the model. Since the noise input during training is not accessible, GAN-Leak first optimizes the noise required to generate the target image. Due to the phenomenon of overfitting, GAN-Leak assumes that the difference between member images generated with optimized noise and the original images will be smaller compared to non-member images.
% In the context diffusion models \cite{ho2020denoising, song2020denoising}, which operate through a gradual noise addition and removal process over time steps $t \in [0, T]$.
% The forward process $q(\vx_t | \vx_0)$ progressively
Several attacks have emerged in this context.
SecMI~\citep{duan2023diffusion} measures the reconstruction error through $\gL_{\rm SecMI}(\vx) = \E_{t, \epsilon} \left[ \| \vx - p_{G}(\vx_0 | q(\vx_t | \vx) ) \| \right]$.
ReDiffuse~\citep{li2024towards} explores the reconstruction stability under noise perturbations through $\gL_{\rm ReDiffuse}(\vx) = \mathrm{Var}_{\epsilon} \left[ \vx - p_{G}(\vx_0 | \vx_t + \epsilon) \right]$, observing that member data produce more consistent reconstructions.
GSA \cite{pang2023white} examines the gradient dynamics during model retraining, finding that member data induce minimal parameter updates, measured as $\gL_{\rm GSA}(\vx) = \| \nabla_{G} \gL(\vx; G) \|_2^2$.
Still, these approaches ultimately reduce to threshold-based classification $\gA(\vx, G) = \mathbbm{1}[\gL(\vx) < \tau]$, with $\tau$ determined through careful analysis of the empirical loss distribution.
Tab.~\ref{tab:results} showcases the empirical success of these attacks on the latest diffusion model architecture EDM \cite{karras2022elucidating}.

In brief, both reference- and intrinsic-based I-MIAs rely on detecting \emph{direct} memorization patterns of member instances \citep{carlini2022membership, carlini2023extracting}, which emerged when overparameterized neural nets ``overfit'' their training data.

\subsection{I-MIAs are unreliable for large-scale pre-trained generative models}
Despite their prevalence in existing MIA studies, I-MIAs are shown to be unreliable when applied to large-scale pre-trained generative models, specifically \emph{large language models} (LLMs) \citep{dong2024generalization, ye2024data}.
This is because the extensive training on massive corpora and substantial model capacity of LLMs would erase the behavioral signature gaps between \emph{individual} member and non-member samples exploited by I-MIAs.
When processing an input, LLMs consistently yield high-confidence outputs regardless of whether it is part of training data, collapsing the discriminative power of instance-level metrics \citep{dong2024generalization}.

On the one hand, the scale and generalization capacity of modern generative models (e.g., LLMs), have rendered I-MIAs statistically unreliable.
On the other hand, they often come at the cost of inefficient deployment, which has driven organizations to use model distillation~\citep{hinton2015distilling} to create smaller versions~\citep{openai2024gpt4omini}.
In this sense, the challenges of I-MIAs \emph{extend beyond large-scale pre-trained models}--distillation may exhibit different privacy vulnerabilities.
We now turn to this scenario, analyzing how distillation interacts with MIAs in the \emph{generative model} context, focusing on diffusion models as a case study.


% SecMI \cite{duan2023diffusion} simulates the diffusion model training process by first adding noise to the image (simulating forward diffusion) and then using the model to denoise and restore the image. Due to the overfitting of the model with the training data, the member data tend to achieve better reconstruction results. 
% ReDiffuse assumes that diffusion models have stable memory of the training data, allowing them to reconstruct a member image well even when different noise is added. In contrast, non-member data will result in larger deviations from the original image. 
% GSA retrains the target model with the data under inspection and observes the changes in gradient values during optimization to attack the model. Since the model is already overfitted to member images, training it again with member data results in minimal gradient changes, whereas training with non-member data leads to larger gradient variations. 
% Overall, almost all MIAs targeting diffusion models and some MIAs designed for generative adversarial networks (GANs) rely on the phenomenon of model overfitting to training data to execute their attacks.

% \textbf{I-MIAs Can Detect Members for Teacher Model.}
% We designed a set of experiments to validate the effectiveness of these methods. We extracted half of the data from the CIFAR10, FFHQ, and AFHQv2 datasets and used these subsets to train three EDM diffusion-based generative models. Subsequently, we attacked these models using the four MIA methods GAN-Leak, SecMI, Re-diffuse, and GSA. These existing MIAs for generative models that can be adapted to one-step generative models. The results are presented in Tab.~\ref{tab:results}.
% The experimental results indicate that these methods are relatively effective in attacking traditional diffusion models. 

% \begin{table*}[!t]
% \centering
% \caption{ASR and AUC results of SecMI, Rediffuse, and DRC on EDM models trained on FFHQ with varying computational resources (1 A100 vs. 4 A100s over 5 days).}
% \vspace{1mm}
% \begin{tabular}{llcccccccc}
% \toprule
% \textbf{Configuration} & \textbf{Dataset} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{Rediffuse}} & \multicolumn{2}{c}{\textbf{DRC}} \\ 
% \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8}
%                       &                   & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} \\ 
% \midrule
% 1 A100, 5 Days         & FFHQ              & 0.51         & 0.511        & 0.523        & 0.521        & 0.535        & 0.540        \\ 
% 4 A100s, 5 Days        & FFHQ              & 0.558        & 0.572        & 0.548         & 0.561      & 0.650        & 0.640        \\ 
% \bottomrule
% \end{tabular}
% \label{tab:comparison}
% \end{table*}

% \texbtf{I-MIAs Indeed Rely On Overfitting Member Instances.}
% We present further evidence that I-MIAs do exploit model overfitting.
% Through controlled experiments with EDM \cite{} trained on FFHQ dataset \cite{}, we compare models at different training iterations (xxx versus yyy).
% We 

% To verify the reliance of these methods on model overfitting, we trained two different EDM models on FFHQ using two distinct computational resource configurations. Specifically, we trained one EDM model on FFHQ for 5 days using a single A100 GPU and another EDM model for 5 days using four A100 GPUs. We then applied three MIA attack methods designed for diffusion models—SecMI, Rediffuse, and GSA—to attack these two models. The attack results are presented in Tab.~\ref{tab:comparison}.
% This experiment highlights the connection between overfitting and the effectiveness of MIA methods. The model trained with 4 A100 GPUs showed higher ASR and AUC values across all methods, indicating that longer training with more resources leads to greater overfitting and makes the model more vulnerable to MIA attacks. These results confirm that existing MIA methods rely heavily on overfitting to distinguish member from non-member data.

% To examine the dependency of these methods on the model’s overfitting to the training data, we conducted experiments where both the EDM trained on AFHQv2 and the EDM trained on FFHQ were trained for the same duration and with identical computational resources—specifically, 5 days of training on two A100 GPUs. Both AFHQv2 and FFHQ consist of 64x64 images, but AFHQv2 contains only 7,500 training samples, whereas FFHQ has 35,000. This causes the model trained on AFHQv2 to overfit the data more significantly, making overfitting-based methods perform much better when attacking models trained on AFHQv2 compared to those trained on FFHQ.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{figure/mot.png}
    \vspace{-1.5em}
     \caption{
     % Figure (a) shows that when using ReDiffuse to attack the EDM model trained on AFHQv2, there is a significant loss difference between reconstructed and re-noised member and non-member data. Figure (b) demonstrates that applying the same method to attack the one-step diffusion model DMD distilled from the EDM fails to distinguish the teacher’s member data from non-member data. Figure (c) examines this using maximum mean discrepancy (MMD)
     Comparison of I-MIAs on (a) teacher diffusion model EDM~\citep{karras2022elucidating} and (b) student generative model DMD~\citep{yin2024one}:
     (a) ReDiffuse~\citep{li2024towards} successfully reveals membership signals in the EDM model trained on AFHQv2, shown by systematic differences in reconstruction and re-noising loss patterns between member and non-member samples. 
     (b) When applied to DMD, ReDiffuse cannot distinguish between the teacher's member and non-member instances.
     (c) Student-generated data shows stronger \emph{distributional} alignment to member data than to non-member data when examined in the form of instance collections with MMD \citep{gretton2012kernel}.
     }
    \label{fig:1_comparison_MIA_DMIA}
\end{figure}

\subsection{I-MIAs fail against distilled generative models}
\label{sec: i_mia_failure}

\textbf{Distillating Diffusion Models.}
While being able to generate high-resolution images, diffusion models are also notorious for their high inference latency caused by the iterative denoising process \cite{song2020denoising}, presenting significant challenges to online deployment. 
Recent progress in knowledge distillation \cite{yin2024one, luo2024diff, song2023consistency} for diffusion models, on the other hand, marks clear efforts to address this limitation by replacing multiple denoising steps into a single step, while maintaining comparable generation quality \cite{meng2023distillation, duan2023diffusion, luo2024diff}.
This suggests a shift in how image generation services will be deployed: end-users will interact with efficient one-step distilled (student) models, while the original (teacher) diffusion models will be dedicated to training these efficient alternatives.

However, state-of-the-art distillation approaches implement a strict separation: student models learn \emph{exclusively} from teacher-generated data, with no access to the teacher's original training dataset. 
For example, DMD~\citep{yin2024one} achieves teacher-level generation quality by enforcing the student to match the teacher's output distribution. 
Diff-Instruct~\citep{luo2024diff} establishes teacher-training-data-free knowledge transfer from pre-trained diffusion models to other generative models.
In this sense, distillation introduces a barrier between the student and teacher member data, fundamentally challenging the instance memorization assumption in I-MIAs.

\textbf{Distillation as Defense against I-MIA.}
We thus investigate the impact of distillation on I-MIAs.
Fig. \ref{fig:1_comparison_MIA_DMIA}(b) shows that the student model's reconstruction pattern exhibits no statistically significant differences between noisy member and non-member images. 
We confirm this using four I-MIA methods on a teacher diffusion model (EDM \cite{karras2022elucidating}) and its student models (DMD and Diff-Instruct). (detailed setup is in Appendix.~\ref{sec: model setting}). 
Tab.~\ref{tab:results} reveals that I-MIAs achieving high success rates on the teacher perform no better than random guessing when applied to student models.
Since student models do not directly fit the teacher's member data, they may not preserve the instance-level behavioral signature that I-MIAs typically exploit.
Thus, model distillation, primarily developed for efficiency though, provides an inherent defense against major I-MIAs without requiring explicit privacy-preserving mechanisms~\citep{shejwalkar2021membership, tang2022mitigating}.

% \textbf{Set-based MIA has been used to attack LLMs.}
% Distribution-based MIA has been widely applied in data contamination detection for LLMs \cite{dong2024generalization, ye2024data}. 
% Data contamination detection aims to determine whether a model’s training data includes unintended benchmark or external datasets, potentially compromising evaluation integrity and fairness.
% \citet{ye2024data} points out that existing instance-level MIA (I-MIA) methods are almost ineffective against modern LLMs. This is because conventional I-MIA settings in machine learning typically focus on small-scale training sets, where there is often a global confidence distribution gap between member and non-member data. However, as training data scales up, this gap diminishes since the rich knowledge base allows non-member samples to exhibit misleadingly high confidence levels.
% Furthermore, \citet{dong2024generalization} mentions and experimentally validates that LLM output distributions are more indicative of whether data has participated in training than individual sample confidence scores. Additionally, \citet{ye2024data} introduces an approach where perturbations are applied to the detected data as LLM inputs, generating a probability distribution over both the given data and its surrounding local neighborhood. Their theoretical analysis demonstrates that this probability distribution better reflects the model’s predictive uncertainty, allowing for more accurate membership attribution compared to single-point confidence scores. Since instance-level MIA is already highly challenging against standard LLMs, performing distillation on LLMs further increases the attack difficulty, rendering Instance-Level MIA even less reliable. \citep{shejwalkar2021membership, tang2022mitigating} In contrast, distribution-based MIA emerges as a more robust and effective method for attacking distilled LLMs compared to instance-level approaches.

\textbf{Privacy Vulnerability Under Distillation.}
This defensive property raises concerns about accountability in unauthorized data usage, as it allows service providers to mask training data provenance, making it difficult to verify whether protected content was used in training their models.
Think about a possible case: service providers could use distilled models to bypass content restrictions, even when contractually bound to exclude or unlearn specific data \cite{bourtoule2021machine} from their training sets.
Inevitably, this creates an information asymmetry that could weaken data privacy protection mechanisms, necessitating alternative setups to track and evaluate privacy exposure in distilled generative models other than I-MIAs.

\noindent\section{Does distillation really eliminate membership information?}
\label{sec: 3_distribution}

\subsection{Memorization is attenuated but persists in residual form}
\label{sec: distill_does_not_eliminate_memorization}
While distillation obscures detectable instance-level membership signatures, it has recently been shown that the membership information is indirectly inherited in the student model \citep{jagielski2024students}, because the teacher's member data biases its outputs, which the student would inherit via distillation, preserving residual traces of sensitive examples in its outputs.
They design cross-dataset classification experiments, observing that a teacher model trained on CIFAR-10 develops high confidence in red objects after exposure to many red birds would misclassify red cars from CIFAR-100 as birds.
These learning patterns transfer to the student model's logits, even when it trains on CIFAR-100 without seeing the teacher's original training data. 
\citet{jagielski2024students} conclude that distillation preserves \emph{distributional statistical patterns} from the teacher's training data, such as confidence scores estimated over multiple samples.

We argue that the bias propagation phenomenon may generalize to the generative model context.
The teacher's generative process encodes the statistical artifacts from member data into its latent space and output distribution, which the student learns to approximate.
For diffusion distillation, the sampling process inherits the teacher's memorized priors~\citep{yin2024one}.
As a result, while these inherited patterns might be too weak to detect on an individual instance basis, it is possible for the student generative model to preserve certain \emph{distributional statistical dependencies}.

\subsection{Student preserves distributional characteristics of teacher's member data}
\label{sec: distribution_investigation}

We next examine a question in model distillation: \emph{does a student trained on teacher-generated data preserve the statistical characteristics of the teacher's training distribution?}

Consider three datasets: student-generated $\gD_{\rm gen}$, 
teacher's member data $\gD_{\rm mem}$ and disjoint non-member data $\gD_{\rm non}$, we evaluate the distance between $\gD_{\rm gen}$ and $\gD_{\rm mem}$ against the distance between $\gD_{\rm gen}$ and $\gD_{\rm non}$ under multiple experimental trials for statistical robustness.
For each trial, we draw random subsets $\tilde{\gD}_{\rm gen}$, $\tilde{\gD}_{\rm mem}$, and $\tilde{\gD}_{\rm non}$ from their respective datasets.
We adopt \emph{maximum mean discrepancy} (MMD)~\citep{gretton2012kernel}, a non-parametric metric that measures the distance between probability distributions, to quantify distributional similarities between paired subsets, namely (i) $\tilde{\gD}_{\rm gen}$ and $\tilde{\gD}_{\rm mem}$, and (ii) $\tilde{\gD}_{\rm gen}$ and $\tilde{\gD}_{\rm non}$.
We observe a pattern across repeated trials: 
The \textcolor{Apricot}{MMD values of (i)} cluster at lower magnitudes compared to \textcolor{SkyBlue}{those of (ii)} (Fig.~\ref{fig:1_comparison_MIA_DMIA}(c)), indicating that the student's generation aligns more closely with the teacher's training distribution from its non-member counterpart.
This way we confirm that distribution-level statistics (e.g., distribution discrepancy) can identify residual (teacher) membership information undetected at the instance level, even through a distilled student model.

\begin{table}[t]
\centering
\caption{
Performance evaluation of MIAs on three victim generative models: EDM (teacher diffusion model), DMD and Diff-Instruct (distilled models).
Results show average ASR and AUC across CIFAR10, FFHQ, and AFHQv2 datasets, mean-aggregating three of four (GAN-leak isn’t for diffusion, so it’s excluded from the average.) MIA methods (reported under column ``Average), namely GAN-leak, SecMI, ReDiffuse and GSA.
See Tab.~\ref{table_results_detail} in {\bf App.\ref{add_result}} for detail.
% Performance comparison of average ASR and AUC metrics for three victim models: EDM (diffusion model), DMD (one-step distilled model), and DI (one-step distilled model), across datasets (CIFAR10, FFHQ, AFHQv2). The “Average” column represents the mean results of four MIA methods (GAN-leak, SecMI, Rediffuse, and GSA) for each model and dataset, while the “Random Guess” column provides baseline values. For DMD and DI, metrics were computed under the distillation scenario. 
% \textbf{Note}: For SecMI in the one-step model setting, the distillation loss was replaced with the EDM’s loss to align with accessibility constraints.
}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llcccc}
\toprule
\textbf{Model/Dataset} & \textbf{Dataset} & \multicolumn{2}{c}{\textbf{Average}} & \multicolumn{2}{c}{\textbf{Random Guess}} \\ 
\cmidrule(r){3-4} \cmidrule(r){5-6}
                       &                  & \textbf{ASR}      & \textbf{AUC}      & \textbf{ASR}       & \textbf{AUC}       \\ \midrule
\multirow{3}{*}{EDM}   & CIFAR10          & 0.596            & 0.610           & 0.5               & 0.5               \\
                       & FFHQ             & 0.584            & 0.590            & 0.5               & 0.5               \\
                       & AFHQv2            & 0.704            & 0.724            & 0.5               & 0.5               \\ \midrule
\multirow{3}{*}{DMD}   & CIFAR10          & 0.515            & 0.509            & 0.5               & 0.5               \\
                       & FFHQ             & 0.515            & 0.503            & 0.5               & 0.5               \\
                       & AFHQv2            & 0.526            & 0.520            & 0.5               & 0.5               \\ \midrule
\multirow{3}{*}{Diff-Instruct}    & CIFAR10          & 0.508            & 0.505            & 0.5               & 0.5               \\
                       & FFHQ             & 0.508            & 0.509            & 0.5               & 0.5               \\
                       & AFHQv2            & 0.509            & 0.508            & 0.5               & 0.5               \\ 
\bottomrule
\end{tabular}}
\label{tab:results}
\end{table}

\subsection{Distributional statistics amplify instance-level membership signals}
Even without considering the impact of distillation, there have recently been studies using distributional statistics to detect data contaminated in training sets when I-MIAs fall short.
\citet{ye2024data} analyze how model outputs vary across local neighborhoods of input space. 
By studying predictive uncertainty over perturbed versions of input samples, they find that models show consistent patterns: regions containing training data exhibit lower distributional uncertainty, a characteristic that single-sample confidence measures cannot detect.
Similarly, \citet{dong2024generalization} confirm that membership information lies in the broader patterns of set-level (i.e., multiple tokens) probability distributions, not in isolated confidence scores.
Collectively, such observations motivate us to consider the shift of the MIA paradigm to a distributional setup.




% such that
% \begin{equation*}
%     \mathrm{MMD}(\sP, \sQ) = \E_{\vx, \vx^{\prime} \sim \sP}\left[ k(\vx, \vx^{\prime}) \right] + \E_{\vy, \vy^{\prime} \sim \sQ}\left[ k(\vy, \vy^{\prime}) \right] - 2\E_{\vx \sim \sP, \vy \sim \sQ}\left[ k(\vx, \vy) \right]
% \end{equation*}

% \subsection{Distributional membership inference attack(DMIA)}
% In this paper, we design a foundational framework for distribution-based MIA as follows:

% Distributional membership inference attack(DMIA) setting is to determine whether the candidate dataset contains data that were used in the target model training.

% Given a pre-trained model $f_{\theta }$ parameterized by weights ${\theta }$, trained by dataset $D_m$ and a candidate dateset $D_c$. 
% Distribution-based MIA evaluates whether there is an intersection between $D_m$ and $D_c$ and assigns a membership attribute $m_i$ to it. Where $m_i$ = 1 if $D_c \cap D_m \neq \emptyset$; otherwise $m_i$ = 0. An DMIA algorithm $\rm DMIA$ is designed to predict whether there exists an intersection between $D_m$ and $D_c$:

% \begin{equation}
% \rm{DMIA}(D_c, \theta) = \begin{cases} 
%     1, & P(m_i=1|\theta , D_c) \ge \tau   \\  
%     0, & P(m_i=1|\theta ,D_c) \le \tau  
% \end{cases}
% \end{equation}

% where $\rm{MIA}(x_i, \theta)$ = 1 indicates that \( x_i \) is identified as a member of the training dataset $D_m$; otherwise \( x_i \) is identified as member of the training dataset $D_m$. $\tau$ is the threshold.

% \subsection{Memorization is Attenuated but Persists in Residual Form}
% Such distributional-level membership information implies that the student inherit 

% From the experimental results in Tab.~\ref{tab:results}, it is evident that SecMI, Rediffuse, and GSA still achieve 1-3 percentage points higher accuracy than random guessing when attacking student models distilled from teacher models with severe overfitting. This indicates that while the distillation process weakens the overfitting tendencies of the model, it does not completely eliminate them. Compared to non-member data, the student model still tends to reconstruct member data points more effectively, but the differences are highly subtle and difficult to detect on an individual basis.

% One possible explanation lies in the residual effects of overfitting. During training, the teacher model inherently overfits its training data, capturing specific patterns and unique features for these data points. When the student model is distilled from the teacher, it inherits a distributional alignment with the teacher model’s generated data, which themselves reflect the original training data’s characteristics. While this does not lead to explicit memorization of individual training samples, it creates a faint, residual bias that allows methods like SecMI, Rediffuse, and GSA to achieve slightly better-than-random performance.

% However, this residual bias becomes increasingly difficult to leverage due to the inherent variability in reconstruction losses across images. For instance, some member images may naturally align more closely with the model’s learned distribution and are therefore easier to reconstruct, resulting in lower reconstruction loss. Conversely, the image of other members may deviate more significantly from the learned distribution, making them harder to reconstruct. These differences in reconstruction difficulty, which are unrelated to membership status, often overshadow the subtle reconstruction differences between member and non-member data. This variability introduces noise into the detection process, diluting the signal that Sample-overfit-based MIA relies on. Furthermore, the distillation process amplifies these challenges. By learning the distribution of the teacher model’s generated data rather than directly overfitting to individual samples, the student model effectively reduces the overfitting-driven membership signals that traditional MIA methods exploit. This makes Sample-overfit-based MIA increasingly unreliable in distillation scenarios, as it cannot consistently differentiate between member and non-member data. The slight advantage observed in the experiments likely reflects the remaining traces of overfitting rather than a robust ability to identify training data.


% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figure/distrubution_example.png}
%     \caption{}
%     \label{fig:enter-label}
% \end{figure}

% \subsection{More robust D-MIA frameworks}
% In one-step diffusion models, the subtle differences between member and non-member data are easily obscured by noise and variability, making sample-overfitting-based MIA from a single data point unreliable. However, when these differences are aggregated across multiple data points, their cumulative effect becomes significantly more pronounced and distinguishable. This underscores the potential of leveraging distributional information to achieve more accurate and robust membership inference, especially in scenarios where individual data points fail to provide clear signals.
% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figure/pipeline.png} 
%     \caption{Training pipeline of MIA-BMIA}
%     \label{figure:pipeline}
% \end{figure*}
% During training, the student model distilled from the teacher model learns the data distribution of the teacher model’s generated samples, while the teacher model learns the distribution of its original training data. This distinction is crucial because, although the student model does not directly interact with the teacher’s training data, the distributional patterns of the teacher’s training data are implicitly embedded in the generated samples used for distillation. Consequently, the student model’s generated data inherently aligns more closely with the distribution of the teacher model’s training data than with that of unrelated data. Moreover, state-of-the-art one-step diffusion methods demonstrate generative performance that closely matches that of their teacher models. This indicates that the student model can effectively learn and replicate the teacher model’s data distribution, maintaining alignment between the teacher’s training data and the student’s generated data. Importantly, this generative fidelity ensures that the distillation process does not obscure or weaken distributional alignment, even as it reduces reliance on overfitting to individual training samples.

% Building on this insight, we observe that the bottleneck feature distribution of member data points is more closely aligned with the distribution of generated data than that of non-member data points. When examining aggregated bottleneck features across multiple data points, this alignment becomes increasingly apparent as the distributional differences between member and non-member data accumulate and amplify. Compared to focusing on individual data points, this provides a more reliable foundation for membership inference. Our experiments further validate this phenomenon, demonstrating that distributional differences in bottleneck features serve as a robust signal for identifying whether a candidate dataset contains the model’s training data.

% \subsection{D-MIA is not a double-edged sword.}
% Distribution-based MIA is not only more reliable but also inherently safer compared to traditional MIA approaches. Traditional MIA methods focus on determining whether specific data points were used to train the target model, effectively probing and potentially exposing the model’s training data. This instance-level approach not only raises significant privacy concerns but also risks becoming a tool for malicious actors seeking to extract sensitive information from models.
% In contrast, distribution-based MIA shifts the focus from individual data points to the broader dataset. Rather than identifying whether specific data points were used for training, it assesses whether the candidate dataset as a whole contains any data that overlaps with the model’s training data. Crucially, this method avoids pinpointing which specific data points were used, thus minimizing the risk of exposing sensitive or proprietary information.

% This shift in perspective makes distribution-based MIA a more ethical and secure alternative, especially in scenarios involving sensitive or private datasets, such as healthcare or user-generated content. By leveraging distributional differences, this approach not only enhances robustness and reliability but also aligns with the broader goal of ensuring data security without compromising individual privacy. This balance between functionality and ethical responsibility highlights the potential of distribution-based MIA to serve as a practical tool for monitoring unauthorized data use while avoiding misuse as a tool for data theft.
% \begin{table*}[ht]
% \centering
% % \gMall
% \caption{ASR and AUC performance of D-MIA (our method), SecMI, and ReDiffuse on DMD models across different datasets (CIFAR10, FFHQ, and AFHQv2) and varying member data proportions (100\%, 50\%, and 30\%).}
% \label{table_dmd}
% \vspace{1mm}
% \begin{tabular}{llcccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Member \%}} & \multicolumn{2}{c}{\textbf{D-MIA(Our)}} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{Rediffuse}} \\ 
% \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
%                                 &                                   &                                     & \textbf{ASR}   & \textbf{AUC}   & \textbf{ASR}   & \textbf{AUC}   & \textbf{ASR}    & \textbf{AUC}   \\ 
% \midrule
% \multirow{9}{*}{\textbf{DMD}}   & CIFAR10                           & 100\%                               & 0.98           & 0.99          & 0.60            & 0.55           & 0.66            & 0.66         \\ 
%                                 &                                   & 50\%                                & 0.98           & 0.99          & 0.59           & 0.52         & 0.6             & 0.6            \\ 
%                                 &                                   & 30\%                                & 0.92           & 0.97          & 0.53           & 0.43         & 0.6             & 0.59           \\ 
% \cmidrule{2-9}
%                                 & FFHQ                              & 100\%                               & 1.0            & 1.00            & 0.60            & 0.56         & 0.56            & 0.56          \\ 
%                                 &                                   & 50\%                                & 0.99           & 0.99          & 0.56           & 0.54         & 0.54            & 0.49         \\ 
%                                 &                                   & 30\%                                & 0.98           & 0.99          & 0.56           & 0.49         & 0.54            & 0.48         \\ 
% \cmidrule{2-9}
%                                 & AFHQV                             & 100\%                               & 1.0            & 1.00            & 0.61           & 0.6            & 0.69            & 0.71         \\ 
%                                 &                                   & 50\%                                & 1.0            & 1.00            & 0.59          & 0.54         & 0.64            & 0.61         \\ 
%                                 &                                   & 30\%                                & 1.0            & 1.00            & 0.56           & 0.56           & 0.60             & 0.61          \\ 
% \bottomrule
% \end{tabular}%
% \label{tab:dmd_data}
% \end{table*}


% \begin{table*}[ht]
% \centering
% % \gMall
% \caption{ASR and AUC performance of D-MIA (our method), SecMI, and ReDiffuse on DI models across different datasets (CIFAR10, FFHQ, and AFHQv2) and varying member data proportions (100\%, 50\%, and 30\%).}
% \label{table_di}
% \vspace{1mm}
% \begin{tabular}{llcccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Member \%}} & \multicolumn{2}{c}{\textbf{D-MIA(Our)}} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{Rediffuse}} \\ 
% \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9}
%                                 &                                   &                                     & \textbf{ASR}   & \textbf{AUC}   & \textbf{ASR}   & \textbf{AUC}   & \textbf{ASR}    & \textbf{AUC}   \\ 
% \midrule
% \multirow{9}{*}{\textbf{DI}}    & CIFAR10                           & 100\%                               & 1.0            & 1.0            & 0.65           & 0.54         & 0.62            & 0.62         \\ 
%                                 &                                   & 50\%                                & 1.0            & 1.0            & 0.59           & 0.53           & 0.6             & 0.60         \\ 
%                                 &                                   & 30\%                                & 1.0            & 1.0            & 0.53           & 0.47         & 0.52           & 0.55           \\ 
% \cmidrule{2-9}
%                                 & FFHQ                              & 100\%                               & 1.0            & 1.0            & 0.57           & 0.56          & 0.78            & 0.81          \\ 
%                                 &                                   & 50\%                                & 1.0            & 1.0            & 0.55           & 0.52          & 0.65            & 0.63          \\ 
%                                 &                                   & 30\%                                & 1.0            & 1.0            & 0.55           & 0.51         & 0.62            & 0.59          \\ 
% \cmidrule{2-9}
%                                 & AFHQV                             & 100\%                               & 1.0            & 1.0            & 0.56          & 0.53          & 0.64            & 0.62          \\ 
%                                 &                                   & 50\%                                & 1.0            & 1.0            & 0.53           & 0.48          & 0.57            & 0.52         \\ 
%                                 &                                   & 30\%                                & 1.0            & 1.0            & 0.48          & 0.50          & 0.55            & 0.50          \\ 
% \bottomrule
% \end{tabular}%
% \label{tab:di_experiment_results}
% \end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/pipeline2.png} 
    \vspace{-2em}
    \caption{
    Overview of our two-phase MMD-based D-MIA framework, consisting of (1) Deep kernel MMD training phase (top left) and (2) candidate testing (bottom left) phase.
    We also propose a kernel ensemble strategy to improve detection robustness (right).
    % Based on the D-MIA setup, we propose an attack framework consisting of two main stages: the training phase of the MMD Deep Kernel (the top left of Fig.~\ref{figure:pipeline}) and the candidate testing phase (the bottle left of Fig.~\ref{figure:pipeline}). To ensure stability and improve accuracy, we train multiple deep kernels and combine their detection results using a Deep-Kernel Model Ensemble method(the right of Fig.~\ref{figure:pipeline}), thereby achieving more robust results.
    }
    \label{figure:pipeline}
\end{figure*}

\section{MIAs Using Distributional Discrepancy}
\label{sec: 4_framework}

We introduce D-MIA, a \emph{set}-based MIA setup that leverages distributional discrepancy statistics to detect membership information in the distilled generative model contexts.
Below, we outline the core concepts of D-MIA to keep the main text concise, with technical details deferred to \textbf{{\bf App.}~\ref{sec: algorithm}}.
Fig.~\ref{figure:pipeline} overviews the framework pipeline.

\textbf{Problem Setup.}
Let $G_{\rm T}: \gZ \to \gX$ be a teacher generative model pre-trained on a private member dataset $\gD_{\rm mem} = \{ \vx_i \}_{i=1}^{N}$, where $\vx_i \sim \sP_{\rm mem}$.
We have access to a distilled generative model $G_{\rm S}$ that mimics $G_{\rm T}$'s behavior, trained using synthetic samples $\{ G_{\rm T}(\vz_j) \}_{j=1}^{M}$ with noises $\vz_j \sim \sP_{\gZ}$.
In D-MIAs, we consider set-based prediction: given a \emph{candidate dataset} $\gD_{\rm can} = \{ \vx_j^{\prime} \}_{j=1}^{N}$, the task is to infer if $\gD_{\rm can} \cap \gD_{\rm mem} = \varnothing$, i.e., contains member instances.

\subsection{Framework Illustration}

D-MIA requires two reference datasets:
(1) a \emph{non-member} set $\gD_{\rm non}=\{ \vx_{k}^{\prime \prime} \}_{k=1}^{N}$ of public instances $\vx_{k}^{\prime \prime} \nsim \sP_{\rm mem}$ and (2) an \emph{anchor} set $\gD_{\rm anc} = \{ \vx_l^{*} \}_{l=1}^{L}$ (e.g., generated by $G_{\rm S}$) used to facilitate distributional comparison.
Moreover, since private member data is typically inaccessible, we propose to construct a \emph{proxy member} set $\widetilde{\gD}_{\rm mem} =\{ G_{\rm S}(\vz_j) \}_{j=1}^{N}$ to approximate $\sP_{\rm mem}$.
At its core, D-MIA aims to detect whether $\gD_{\rm can}$ aligns more closely with $\widetilde{\gD}_{\rm mem}$ or $\gD_{\rm non}$ through \emph{relative distributional discrepancy thresholding}.

\textbf{Training a Deep-kernel MMD.} 
% $k_{\theta}(\vx, \vx^{\prime}) = \left< \phi_\theta(\vx), \phi_{\theta}(\vx^{\prime}) \right>_{\gH}$
We first optimize a data-adaptive kernel $k_{\omega}$, parameterized by deep neural nets $\omega$~\citep{liu2020learning} to \emph{maximize the separation} between $\tilde{\gD}_{\rm mem}$ and $\gD_{\rm non}$ in the feature space.
For $\tilde{\gD}_{\rm mem}$, $\gD_{\rm non}$ and $\gD_{\rm anc}$, we perform mini-batch training and randomly sample subsets from each dataset, e.g., $\gB_{\rm anc} = \{ \vx_b^{*} \overset{{\rm i.i.d}}{\sim} \gD_{\rm anc} \}_{b=1}^{B}$, with respect to the optimization objective $\gL(\omega)$ defined as
{\footnotesize
    \begin{equation*}
    \vspace{-1em}
    \gL(\omega) = \underbrace{\left[ \widehat{\rm MMD}^2(\gB_{\rm anc}, \gB_{\rm mem}; \omega) \right]}_{\text{member discrepancy}} - 
        \underbrace{ \left[ \widehat{\rm MMD}^2 (\gB_{\rm anc}, \gB_{\rm non}; \omega) \right]}_{\text{non-member discrepancy}}.
    \end{equation*}
}

Doing so amplifies the MMD values between non-members and the anchor distribution while minimizing it for member-like distributions.
See \textbf{Alg.~\ref{alg:alg1}} for details.

% where each $\gB_{\rm anc} = \{ \vx_b^{*} \overset{{\rm i.i.d}}{\sim} \gD_{\rm anc} \}_{b=1}^{B}$ is a random subset (i.e., mini-batch) sampled from the anchor dataset (similarly for $\gB_{\rm mem}$ and $\gB_{\rm non}$).

% The primary objective of this phase is to train an optimal deep-kernel MMD.
% This process, detailed in Algorithm \ref{alg:alg1}, is designed to enable the mapping function to effectively differentiate between member and non-member data distributions. In practical scenarios, acquiring member data poses significant challenges due to privacy and accessibility concerns. In contrast, non-member data is generally more accessible; for example, data providers may reserve an unpublished subset of their data for copyright detection purposes. To address these constraints, we substitute the generative data for the member data during the training of $k_\omega$. This approach ensures feasibility and aligns with real-world applications while maintaining the integrity of the training process.
% Following the pipeline Fig.~\ref{figure:pipeline} in the upper left part titled 'Training a Deep Kernel MMD', we randomly sampled one set of images each from the imitation member dataset, the anchor dataset and the auxiliary non-member dataset for training. Each image, with added noise, is fed into the student model to extract its bottleneck feature. Using the Deep Kernel MMD (The details of the Deep Kernel MMD are provided in the {\bf App.}~\ref{sec:pre}), we calculate two key MMD values: (i) the distance between generative data bottleneck features and member data bottleneck features, and (ii) the distance between generative data bottleneck features and non-member data bottleneck features. Finally, we maximize the difference between these two MMD values using the loss function specified in line 13 of Algorithm \ref{alg:alg1}.


\textbf{Detecting Membership.}
In this step, we aim to determine whether $\gD_{\rm can} \cap \gD_{\rm mem} = \varnothing$, by computing two MMD statistics using the trained kernel $k_{\omega}$: $S_1^{(t)} \triangleq \widehat{\rm MMD}^2(\gB_{\rm anc}, \gB_{\rm can}; \omega)$ and $S_2^{(t)} \triangleq \widehat{\rm MMD}^2(\gB_{\rm anc}, \gB_{\rm non}; \omega)$ over $T$ Bernoulli trials.
The membership is indicated per trial via $\sI^{(t)} = \mathbbm{1}(S_1 < S_2)$, and the aggregate membership probability is estimated by $p_{\rm mem} = \frac{1}{T} \sum_t \sI^{(t)}$ (details are in \textbf{Alg.~\ref{alg:alg2})}.

% In the candidate testing phase, the purpose of this step is to determine whether the dataset set extracted from the candidate data contains member data. Following the pipeline Fig.~\ref{figure:pipeline} in the bottom left part titled “Detecting the candidate dataset”, we randomly sample one image set each from the candidate data set, anchor data set, and auxiliary data set without members for detection. Each image is then augmented with noise, input into the student model, and its bottleneck feature is extracted. Using the Deep Kernel MMD trained in the “Training a Deep Kernel MMD” phase, we compute two key MMD values: (i) the MMD between the candidate dataset and the generative data, and (ii) the MMD between the auxiliary non-member data and the generative data, as described in Algorithm \ref{alg:alg2}. If the former is smaller than the latter, the candidate dataset is considered to contain member data; otherwise, it is not.

\textbf{Ensembling Multiple Kernels.}
To mitigate the variance from finite-sample MMD estimates \cite{cherief2022finite}, we aggregate predictions across $m$ independently trained kernels $\{ k_{\omega}^{(i)} \}_{i=1}^{m}$.
For each kernel, we compute $p_{\rm mem}^{(i)}$ over $n$ Bernoulli trials as with Alg.~\ref{alg:alg2}.
We apply a final decision threshold $\tau$ to the ensemble mean $\bar{p}_{\rm mem} = \frac{1}{m} \sum_i p_{\rm mem}^{(i)}$, declaring membership of $\gD_{\rm can}$ if $\bar{p}_{\rm mem} > \tau$.
See \textbf{Alg.~\ref{alg:soft_voting}} for detailed illustrations.

% \textbf{Deep-Kernel Model Ensemble}
% To mitigate the effects of potential outliers during the sampling process and improve classification accuracy, we adopt a soft voting ensemble method, as detailed in Algorithm \ref{alg:soft_voting}. By aggregating predictions from multiple deep kernel models, this method ensures robust and reliable detection of whether the candidate dataset contains member data from the teacher model. This ensemble strategy enhances the framework’s resilience and precision in distinguishing member from non-member data. 

% Following the pipeline Fig.~\ref{figure:pipeline}, titled “The Deep-Kernel Model Ensemble,” we perform $m$ iterations of the “Training a Deep Kernel MMD” phase to train $m$ deep kernel MMDs. For each deep kernel MMD, we conduct $n$ rounds of the “Detecting the Candidate Dataset” phase and calculate the probability that, under the current deep kernel MMD, the MMD value between the candidate dataset and the generative data is smaller than the MMD value between the auxiliary non-member data and the generative data. This process results in  probabilities, one for each kernel. Finally, we determine whether the candidate data set contains member data by comparing the mean of these  $m$ probabilities against a predefined threshold. If the mean exceeds the threshold, the candidate dataset is considered to contain member data; otherwise, it does not.
% \section{Distribution-based MIA framework}
% % \subsection{Training pipeline of DDG-MIA}
% Based on the D-MIA setup, we designed an attack framework consisting of two stages: the training phase of the MMD-Deep Kernel and the candidate testing phase. To enhance the stability and accuracy of the results, we trained multiple deep-kernel and combined the detection results from multiple tests to achieve more robust outcomes. A visual illustration of this framework is provided in Fig.~\ref{figure:pipeline}.

% \textbf{The Deep-kernel training phase} 
% In this phase, our primary objective, as outlined in Algorithm \ref{alg:alg1}, is to train an optimal deep-kernel mapping function $k_\omega$ capable of measuring and maximizing two distributional distances: the distance between the generative data and the member data, and the distance between the generative data and the non-member data. However, in real-world scenarios of data copyright detection, acquiring the member data of a model often presents significant challenges. Conversely, non-member data is generally more accessible. For instance, data providers may reserve a subset of their data for copyright detection purposes, intentionally keeping it unpublished. To address these practical constraints, we utilize generative data as a substitute for member data during the training of the mapping function, thereby ensuring that the approach remains feasible and aligned with real-world applications.

% \textbf{The candidate testing phase} 
% At this phase, We use the trained $k_w$ to calculate the distributional difference between the candidate dataset and the generated data according to Algorithm \ref{alg:alg2}, as well as the difference between the auxiliary non-member data and the generated data. By comparing which distribution is closer to the generated data, we determine whether the candidate dataset contains member data from the teacher model.

% \textbf{Deep kernel model ensemble}
% In order to avoid special cases during the sampling process when training the deep kernel function and to ensure the precision of the classification results. Follow by Algorithm \ref{alg:soft_voting}, we adopt a soft voting model ensemble method to determine whether the candidate data set contains training data from the teacher model. 

\begin{table*}[ht]
\centering
\caption{ASR and AUC results of D-MIA against baselines I-MIA methods SecMI, and ReDiffuse on distilled models across CIFAR10, FFHQ, and AFHQv2. Rows are color-coded to represent member data proportions: 100\%, 50\%, and 30\%.}
\label{table_combined_with_percent}
\vspace{1mm}
\begin{tabular}{llcccccccccccc}
\toprule
\multirow{2}{*}{\shortstack{\textbf{Dataset} \\ \textbf{(Member \%)}}}
& \multicolumn{6}{c}{\textbf{DMD}} & \multicolumn{6}{c}{\textbf{Diff-Instruct}} \\ 
\cmidrule(r){2-7} \cmidrule(r){8-13}
& \multicolumn{2}{c}{\textbf{D-MIA}} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{ReDiffuse}} 
& \multicolumn{2}{c}{\textbf{D-MIA}} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{ReDiffuse}} \\ 
\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} \cmidrule(r){10-11} \cmidrule(r){12-13}
& \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} 
& \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} \\ 
\midrule
\rowcolor{red!20} \textbf{CIFAR10 (100\%)} & \textbf{0.98} & \textbf{0.99} & 0.60 & 0.55 & 0.66 & 0.66 & \textbf{1.0} & \textbf{1.0} & 0.65 & 0.54 & 0.62 & 0.62 \\ 
\rowcolor{green!20} \textbf{CIFAR10 (50\%)} & \textbf{0.98} & \textbf{0.99} & 0.59 & 0.52 & 0.60 & 0.60 & \textbf{1.0} & \textbf{1.0} & 0.59 & 0.53 & 0.60 & 0.60 \\ 
\rowcolor{orange!20} \textbf{CIFAR10 (30\%)} & \textbf{0.92} & \textbf{0.97} & 0.53 & 0.43 & 0.60 & 0.59 & \textbf{1.0} & \textbf{1.0} & 0.53 & 0.47 & 0.52 & 0.55 \\ 
\midrule
\rowcolor{red!20} \textbf{FFHQ (100\%)} & \textbf{1.0} & \textbf{1.0} & 0.60 & 0.56 & 0.56 & 0.56 & \textbf{1.0} & \textbf{1.0} & 0.57 & 0.56 & 0.78 & 0.81 \\ 
\rowcolor{green!20} \textbf{FFHQ (50\%)} & \textbf{0.99} & \textbf{0.99} & 0.56 & 0.54 & 0.54 & 0.49 & \textbf{1.0} & \textbf{1.0} & 0.55 & 0.52 & 0.65 & 0.63 \\ 
\rowcolor{orange!20} \textbf{FFHQ (30\%)} & \textbf{0.98} & \textbf{0.99} & 0.56 & 0.49 & 0.54 & 0.48 & \textbf{1.0} & \textbf{1.0} & 0.55 & 0.51 & 0.62 & 0.59 \\ 
\midrule
\rowcolor{red!20} \textbf{AFHQv2 (100\%)} & \textbf{1.0} & \textbf{1.0} & 0.61 & 0.60 & 0.69 & 0.71 & \textbf{1.0} & \textbf{1.0} & 0.56 & 0.53 & 0.64 & 0.62 \\ 
\rowcolor{green!20} \textbf{AFHQv2 (50\%)} & \textbf{1.0} & \textbf{1.0} & 0.59 & 0.54 & 0.64 & 0.61 & \textbf{1.0} & \textbf{1.0} & 0.53 & 0.48 & 0.57 & 0.52 \\ 
\rowcolor{orange!20} \textbf{AFHQv2 (30\%)} & \textbf{1.0} & \textbf{1.0} & 0.56 & 0.56 & 0.60 & 0.61 & \textbf{1.0} & \textbf{1.0} & 0.48 & 0.50 & 0.55 & 0.50 \\ 
\bottomrule
\end{tabular}
\end{table*}

\subsection{Experimental Setup}
\textbf{Dataset and Victim Models.}
We empirically evaluate D-MIA on SOTA distilled generative models, DMD~\citep{yin2024one} and Diff-Instruct~\citep{luo2024diff} distilled from diffusion model EDM~\citep{karras2022elucidating}, on commonly studied MIA benchmarks CIFAR10~\citep{krizhevsky2010cifar}, FFHQ~\citep{karras2019style}, and AFHQv2~\citep{choi2020stargan}.
See detailed setup of victim models in {\bf App.}~\ref{sec: model setting}.

\textbf{Attacker Setup.}
% For fair comparisons, we adapt two I-MIA baselines—SecMI~\citep{duan2023diffusion} and Rediffuse~\citep{li2024towards}—from instance- to dataset-level statistics via bootstrap sampling and empirical thresholding.
For fair comparisons, we adapt two existing I-MIA baselines--SecMI~\citep{duan2023diffusion} and Rediffuse~\citep{li2024towards}--from instance-level to dataset-level statistics, through bootstrap sampling and empirical thresholding.
We detail this protocol in {\bf App.}~\ref{app:experiment_details}
and the D-MIA implementation in {\bf App.}~\ref{sec: model setting}.

\textbf{Evaluation Setup.}
We split each dataset equally between member data (used for training the teacher diffusion model EDM) and non-member data, with the teacher model generating $100,000$ samples for student model distillation.
We construct auxiliary datasets through balanced sampling across FFHQ, CIFAR10 ($15,000$ samples), and AFHQv2 ($3,000$ samples), allocating equal portions for kernel training and candidate detection.
The performance of D-MIA is evaluated through 50 detection rounds across varying member ratios ($30\%, 50\%, 100\%$) within candidate sets, complemented by non-member datasets as controls. 

\subsection{Result Analysis}
\textbf{D-MIA is effective to distilled generative models.}
% \textbf{Analysis of the Results for Attacking the DMD Model} 
Tab.~\ref{table_combined_with_percent} shows that D-MIA can perform successful attacks across different distilled models and datasets under varied portions of member data in the candidate sets.
For example, for attacks on DMD, D-MIA achieves near-perfect success rates (ASR $\approx 100\%$) across three datasets, significantly outperforming baselines.
D-MIA performs robustly ($\sim 92\%$ ASR) even with mixed candidate datasets on CIFAR10, while baselines degrade to random-guess levels with just $30\%$ member data.
This establishes D-MIA as a reliable attack framework for real-world scenarios where candidate sets often contain an unknown mixture of member and non-member data.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]
    {figure/result_hist.png} 
    \vspace{-1.5em}
    \caption{
    Distribution analysis of D-MIA outputs across different member/non-member ratios within the candidate sets.
    Results are shown for distilled models against CIFAR10 (a, c) and FFHQ (b, d), where subfigures (a, b) report the results of DMD, while subfigures (c, d) present the results of Diff-Instruct.
    % Output distributions of D-MIA for candidate datasets with varying proportions of member and non-member data, tested on CIFAR10 (a) and FFHQ (b) models distilled using DMD, and CIFAR10 (c) and FFHQ (d) models distilled using DI.
    }
    \label{figure:result_hist}
\end{figure}
\textbf{D-MIA can quantify dataset composition.}
It is also possible to use D-MIA to quantify the ratio of member data in the candidate sets, extending beyond its primary role as an attack method.
As shown in Fig.~\ref{figure:result_hist}, D-MIA's outputs consistently exhibit a clear positive correlation with the proportion of member data, approaching 1 for pure-member candidate sets and 0.5 as member data decreases.
This finding suggest a new perspective for analyzing data privacy in terms of dataset composition.

% By comparing candidate datasets with different proportions of member and non-member data, we find that D-MIA not only effectively attacks distilled models but also partially detects the proportion of member data in the candidate dataset. Fig.~\ref{figure:result_hist} shows the output distributions of D-MIA under different candidate dataset settings for attacking CIFAR10 and FFHQ models distilled using DMD and DI. Regardless of the dataset or distilled model, the output of D-MIA exhibits a positive correlation with the proportion of member data in the candidate dataset. When the candidate dataset consists entirely of member data, D-MIA’s output is closest to 1. As the proportion of member data in the candidate dataset decreases, D-MIA’s output gradually approaches 0.5. 

\section{What are the implications of D-MIA?}
\label{sec: 5_implication}
\textbf{Model distillation is increasingly prevalent.}
Model distillation is an effective solution in deploying large generative models, demonstrated by gains in both computational efficiency and cost-reduction at modest performance compromise.
Practically, this approach has been widely adopted in production systems, with firms like OpenAI and Midjourney implementing distilled versions to power their real-time conversation and image generation services while reducing operational costs.
This transformation also calls attention to the data privacy issues prevalent in generative models nowadays.
I-MIAs may suffer when the per-instance membership artifacts diminish in large-scale pre-trained generative models and even their distilled versions.

\textbf{Set-based detections are more practical.}
Concretely, as the training data of modern generative models (particularly LLMs) expand in scale, the discriminative gap between \emph{individual} member and non-member instances decreases drastically~\citep{ye2024data}, not to mention that model distillation introduces an additional shield, suppressing the residual membership information (discussed in Sec.~\ref{sec: 2_I_MIA}).
In contrast, D-MIA \emph{collectively} analyzes \emph{set} of samples and aggregates weak instance-level patterns into distribution-level statistics, amplifying membership information inaccessible to I-MIAs (Sec.~\ref{sec: 3_distribution}) and demonstrating attack successes (Sec.~\ref{sec: 4_framework}).
Thus, D-MIAs align more closely with the practical deployment scenarios where distillation is increasingly common.

\textbf{Set-based detections are more robust.}
Empirically, in the context of diffusion-based image generation, we have validated performances of D-MIA across distilled models (Tab.~\ref{table_combined_with_percent}), outperforming I-MIA baselines by $>40\%$ absolute in \emph{mixed-data scenarios} (e.g., $92\%$ vs. $52\%$ ASR on CIFAR10 with $30\%$ member data), thanks to relative MMD-based inference that reliably estimates distributional divergence through repeated subset samplings and multi-kernel ensemble.


\textbf{Set-based detections are more secure.}
Recall that I-MIAs seek to identify individual data instances, which raises a security dilemma as well: while designed for auditing, these methods could be \emph{abused} to extract sensitive data from models.
D-MIA may address this tension as it no longer classifies individual samples. 
Instead, D-MIA evaluates whether a candidate dataset collectively aligns with the training distribution--detecting data overlap but preventing per-sample identifications--even with full knowledge of the detection mechanism, attackers cannot resolve membership at a finer granularity than the candidate set itself.
From a privacy standpoint, the distributional characteristics of instances \emph{sets} can thus be seen as a new privacy auditing paradigm with privacy protection principles incorporated.

% \textbf{D-MIA is not a double-edged sword.}
% Distribution-based MIA is not only more reliable but also inherently safer compared to traditional MIA approaches. Traditional MIA methods focus on determining whether specific data points were used to train the target model, effectively probing and potentially exposing the model’s training data. This instance-level approach not only raises significant privacy concerns but also risks becoming a tool for malicious actors seeking to extract sensitive information from models.
% In contrast, distribution-based MIA shifts the focus from individual data points to the broader dataset. Rather than identifying whether specific data points were used for training, it assesses whether the candidate dataset as a whole contains any data that overlaps with the model’s training data. Crucially, this method avoids pinpointing which specific data points were used, thus minimizing the risk of exposing sensitive or proprietary information.

% This shift in perspective makes distribution-based MIA a more ethical and secure alternative, especially in scenarios involving sensitive or private datasets, such as healthcare or user-generated content. By leveraging distributional differences, this approach not only enhances robustness and reliability but also aligns with the broader goal of ensuring data security without compromising individual privacy. This balance between functionality and ethical responsibility highlights the potential of distribution-based MIA to serve as a practical tool for monitoring unauthorized data use while avoiding misuse as a tool for data theft.

% \begin{table}[t]
% \centering
% % \gMall
% \caption{
% % ASR and AUC results of DGG-MIA for varying auxiliary non-member dataset sizes (A-non size) and candidate dataset sizes (Can size) on DMD and DI models. The auxiliary non-member dataset is evenly divided into two parts: one part is used for training the deep-kernel MMD, and the other part is used for auxiliary detection (the left value in the parentheses represents the size of the auxiliary non-member dataset used for training the deep-kernel MMD, while the right value represents the size used for auxiliary detection).
% ASR and AUC results of D-MIA evaluated on DMD under varying non-member and candidate dataset sizes.
% Values in parentheses indicate equal split of $|\gD_{\rm non}|$ for MMD kernel training and MIA evaluation.
% }
% \label{tab:dgg_mia_results}
% \vspace{1mm}
% \begin{tabular}{llccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{$|\gD_{\rm non}|$ / $|\gD_{\rm can}|$} & \multicolumn{2}{c}{\textbf{DGG-MIA}} \\ 
% \cmidrule(r){3-4}
%                                 &                                               & \textbf{ASR}   & \textbf{AUC}   \\ 
% \midrule
% \multirow{3}{*}{\textbf{DMD}}   & (5000+5000)/5000                              & 0.98           & 0.99          \\ 
%                                 & (3000+3000)/3000                              & 0.95           & 0.97          \\ 
%                                 & (2000+2000)/2000                              & 0.94           & 0.93          \\ 
% % \midrule
% % \multirow{3}{*}{\textbf{DI}}    & (5000+5000)/5000                              & 1.0            & 1.0            \\ 
% %                                 & (3000+3000)/3000                              & 1.0            & 1.0            \\ 
% %                                 & (2000+2000)/2000                              & 1.0            & 1.0            \\ 
% \bottomrule
% \end{tabular}%
% \end{table}

\begin{table}[t]
\centering
% \gMall
\caption{
% ASR and AUC results of DGG-MIA for varying auxiliary non-member dataset sizes (A-non size) and candidate dataset sizes (Can size) on DMD and DI models. The auxiliary non-member dataset is evenly divided into two parts: one part is used for training the deep-kernel MMD, and the other part is used for auxiliary detection (the left value in the parentheses represents the size of the auxiliary non-member dataset used for training the deep-kernel MMD, while the right value represents the size used for auxiliary detection).
ASR and AUC results of D-MIA evaluated on DMD under varying non-member and candidate dataset sizes.
In each configuration, we equally split $\gD_{\rm non}$ for kernel training and MIA evaluation.
Both metrics decrease as $\gD_{\rm non}$ and $\gD_{\rm can}$ lower down.
}
\label{tab:dgg_mia_results}
\vspace{1mm}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
% \multirow{2}{*}{$|\gD_{\rm non}|$ / $|\gD_{\rm can}|$} & \multicolumn{2}{c}{\textbf{DGG-MIA}} \\ 
$|\gD_{\rm non}|$   & $|\gD_{\rm can}|$ & \textbf{ASR}   & \textbf{AUC}   \\ 
\midrule
(5000+10000) & 5000    & 0.98           & 0.99          \\ 
(3000+6000) & 3000    & 0.95           & 0.97          \\ 
(2000+4000) & 2000    & 0.94           & 0.93          \\ 
(1000+2000) & 1000    & 0.89           & 0.79          \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Alternative View}
\label{sec: 6_limitation}
% \textbf{In certain scenarios, the membership attribution of individual data points is also crucial.}

While D-MIA is a compelling framework in terms of merits discussed in Sec.~\ref{sec: 5_implication}, there are practical considerations stress why I-MIAs--despite their limitations--remain useful for certain privacy auditing scenarios.

\textbf{D-MIA is sensitive to data availability.}
I-MIAs avoid the need for large candidate sets, which D-MIA requires to reliably estimate distributional discrepancies via MMD.
% Since MMD requires large datasets to yield reliable estimations, D-MIA assumes access to sufficient data samples, concerning its effectiveness with less available data.
% We validate this concern through experiments across three three dataset configurations.
% We test with non-member datasets of sizes $10,000, 6,000$ and $4,000$, paired with candidate datasets of $5,000, 3,000$ and $2,000$ samples, respectively.
% Consistent with Sec.~\ref{sec: 4_framework}, we divide each non-member set equally for deep kernel training and attack evaluation.
% We use consistent test conditions across all configurations by constructing balanced sets of $50$ member and $50$ non-member samples from the candidate distribution.
% Tab.~\ref{tab:dgg_mia_results} in Appendix shows that attack effectiveness correlates positively with dataset sizes, indicating that reduced data compromises D-MIA's reliability in estimating distribution patterns through MMD.
In practice, data subjects (e.g., artists) may have only a limited collection of personal records--perhaps fewer than 10 pieces or even a single artwork--when they seek an audit to determine if their data was used to train a generative model.
As Tab.~\ref{tab:dgg_mia_results} shows, D-MIA's discriminative power degrades when candidate set sizes lower down (see {\bf App.}~\ref{datasize_test} for setup details).
On the contrary, I-MIAs are not limited in this case as they probe model behavior at the sample level.

% Thus, we acknowledge the limitation of D-MIA in its current iteration when applied to MIA tasks with extremely limited data.
% The assumption of sufficient data access may not hold in cases where an individual, such as an artist, aims to detect whether she/he limited collection of works--perhaps fewer than 10 pieces or even a single artwork--has been used to train a generative model.
% In such cases, the discriminative power of D-MIA may be unreliable, calling for more sample-efficient distributional statistics in future studies.

% A direct counterargument to our hypothesis is: what if the attacker possesses only a very small amount of data? When the data volume is extremely limited, distributional information becomes unreliable. Our proposed distribution-based approach assumes that the attacker has access to a certain amount of data, but this assumption may not hold in all cases. For example, consider an individual artist who wants to detect whether their artwork has been used to train a generative model. In such cases, the artist may have fewer than 100 pieces of artwork, or even just one. Under these circumstances, existing methods become unreliable. To address this, future work should focus on developing distribution-based MIA methods that require significantly less data. These methods should still be effective when the data size is fewer than 100 samples. However, this limitation may or may not be critical, as the demand for training data by models continues to grow, and data collection will soon become a task that individuals or small groups can no longer afford. In the future, large organizations or entities specializing in data collection may emerge. These organizations could purchase copyrights from data providers, such as artists, and curate large datasets for commercial sale. Such entities could not only establish datasets more efficiently but also better protect the copyrights of the data in bulk.

% \textbf{D-MIA’s reliance on auxiliary non-member and candidate dataset sizes.}
% In D-MIA attacks, the attacker requires a certain amount of non-member data for auxiliary training and testing. Additionally, the candidate dataset being evaluated must have a sufficient size to obtain accurate distributional information. Therefore, we evaluate the performance of D-MIA on CIFAR10 models for DMD and DI under different auxiliary non-member dataset sizes and candidate dataset sizes. 
% We evaluated three settings for auxiliary and candidate dataset sizes: auxiliary dataset sizes of 10,000, 6,000, and 4,000 paired with candidate dataset sizes of 5,000, 3,000, and 2,000, respectively. Half of the auxiliary dataset was used to train the deep kernel, while the other half supported attacks on candidate datasets. Positive samples were drawn from member data corresponding to the candidate dataset size, and negative samples were drawn from non-member data of the same size. Following previous experiments, 50 positive and 50 negative samples were constructed, and D-MIA was applied to distinguish between them.

% As shown in Tab.~\ref{tab:dgg_mia_results}, both the size of the auxiliary and candidate data set significantly impact the effectiveness of D-MIA. For example, attacks in the (2000 + 2000) / 2000 setting in CIFAR10 were less effective than those in the (5000 + 5000) / 5000 setting. This decline is likely due to smaller datasets that provide less representative distributional information, reducing attack accuracy.

\textbf{Retaining data may lead to resource waste.}
% Another limitation of D-MIA lies in how to balance privacy protection with resource efficiency.
% To conform to the data regulations, e.g., GDPR~\citep{mondschein2019eu}, data providers should maintain substantial holdout sets as non-member examples, creating storage and computational overhead. 
Although all existing MIA methods necessitate reference datasets in their pipeline, D-MIA is likely to demand more retained data, introducing resource burdens that conflict with evolving data regulations, such as GDPR~\citep{mondschein2019eu}, which impose strict storage limitations.
This may lead to logistical overhead for companies and infeasibility for individuals.
Though future work may mitigate D-MIA's resource demands via compression or more efficient samplings, such solutions remain speculative;
I-MIAs already function under milder assumptions.

% We note that these reference datasets have been essential in existing MIA studies, but their resource requirements have not been systematically discussed.
% Such resource demands are likely to be intensified as D-MIA requires larger reference datasets than existing I-MIAs.
% We look forward to future studies investigating compression techniques and efficient data sampling strategies to reduce the resource footprint while maintaining strong privacy sensitivity.
% In our framework, data providers are required to retain a portion of their dataset for copyright detection. This portion of the dataset is not publicly available and serves as auxiliary non-member data to assist in detecting whether a model has been trained on it. While this setup is a common practice in previous MIA methods to ensure detection accuracy, it inevitably leads to resource loss, and prior research has not addressed the need to minimize this loss. Based on our hypothesis, future work should aim to reduce the reliance on auxiliary datasets and consider this as an important criterion for evaluating the effectiveness of MIA methods. This would help control the resource loss caused by retaining data.

\textbf{Granularity of Privacy Protection.}
Privacy harms often focus on single data points.
Consider an artist auditing whether a specific artwork was used in training: the legal claim hinges on proving membership of that singular work, not detecting consistent patterns across their oeuvre.
% By definition, I-MIAs offer this granular attribution in cases where they are effective.
By definition, I-MIAs provide precise attribution when effective.

\section{Final Remarks}
\label{sec: 7_final}
We argue that the critique does not negate D-MIA's contributions, which suggests a promising paradigm shift for future \emph{MIA for generator} studies to reconsider and base their analysis on distributional statistics under set-based evaluation, particularly in scenarios \emph{where instance-level analysis fails}, such as with distilled generative models.
% We acknowledge the limitations of D-MIA in its current iteration and do not call for a one-off transformation prematurely.
We acknowledge the limitations of D-MIA in its current iteration; however, I-MIAs currently struggle to attack distilled generative models. 
Comparatively, allowing privacy auditors to provide more data for D-MIA presents a feasible trade-off.
To realize its full potential, advancing statistical methods for low-data regimes and developing efficient data retention protocols will be critical.

\section*{Impact Statement}
% This work reveals the limitations of instance-level MIAs in distilled generative models and introduces \textbf{set-based MIA} for improved detection. Our findings highlight that distillation obscures instance-level signals but leaves exploitable distributional patterns.
This study on membership inference attacks raises important ethical considerations that we have carefully addressed.
Membership inference attacks threaten data privacy in machine learning models and we have taken steps to ensure all the attack methods involved are fairly and transparently evaluated.
% We use widely accepted public benchmark datasets to ensure comparability of our results.
% Our evaluation encompasses a wide range of attack types and strengths to provide a comprehensive assessment of our defense mechanism. 
We have also carefully considered the broader impacts of our work.
Our work contributes to the development of data privacy audit methodology by shifting the evaluation setup of membership inference attacks, potentially improving the reliability of AI systems in various applications.
We will actively engage with the research community to promote responsible development and use of this new membership inference attack paradigm.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Preliminaries}
\label{sec:pre}
This section provides the definition of MMD and Deep-kernel MMD. 

\textbf{Maximum mean discrepancy (MMD):} In this paper, an adjusted Deep Kernel-based Maximum Mean Discrepancy is used to measure the feature differences between distributions, with modifications to enhance its efficiency for bounded-size samples.
let $\mathcal{X} \subset \mathbb{R}^d$ represent a separable metric space with   $\mathbb{P}$ and $\mathbb{Q}$ as two Borel probability measures defined over . To compaer these distributions, two sets of independent and identically distributed (IID) samples are considered:  $S_X = \{ x^{(i)} \}_{i=1}^n$ drawn from $\mathbb{P}$ and $S_Z = \{ z^{(i)} \}_{i=1}^m$ drwan from $\mathbb{Q}$ sampled from the distributions  $\mathbb{P}$  and  $\mathbb{Q}$. MMD \cite{gretton2012kernel} measures the difference between two distributions:

\begin{equation}
\begin{aligned}
\operatorname{MMD}(\mathbb{P}, \mathbb{Q}; \mathbb{H}_k) 
&= 
\sqrt{\mathbb{E}[k(X, X')] + \mathbb{E}[k(Z, Z')] - 2\mathbb{E}[k(X, Z)]}.
\end{aligned}
\end{equation}
% \begin{equation}
% \begin{aligned}
% \operatorname{MMD}\left(\mathbb{P}, \mathbb{Q} ; \mathbb{H}_{k}\right) 
% &= \left\|\mu_{\mathbb{P}} - \mu_{\mathbb{Q}}\right\|_{\mathbb{H}_{k}} \\
% &= \sqrt{A + B - 2C},
% \end{aligned}
% \end{equation}
% \begin{equation}
% \begin{aligned}
% A &= \mathbb{E}\left[k\left(X, X^{\prime}\right)\right], \\
% B &= \mathbb{E}\left[k\left(Z, Z^{\prime}\right)\right], \\
% C &= \mathbb{E}[k(X, Z)].
% \end{aligned}
% \end{equation}


In this context, $k: \mathcal{X}  \times  \mathcal{X}  \to R$ represent a kernel function associated with a reproducing kernel Hilbert space(RKHS) $\mathbb{H}_k$. The kernel mean embeddings of the distributions $\mu_{\mathbb{P}}$ and $\mu_{\mathbb{Q}}$ are the kernel mean embeddings of $\mathbb{P}$, $\mathbb{Q}$, denoted by $\mu_{\mathbb{P}}$ and $\mu_{\mathbb{Q}}$, are given by $\mu_{\mathbb{P}} : = \mathbb{E}\left [ k\left ( \cdot ,X \right )  \right ]$ and $\mu_{\mathbb{Q}} : = \mathbb{E}\left [ k\left ( \cdot ,Z \right )  \right ] $, respectively.
Assuming  n = m , we use the estimator from deep-kernel \cite{liu2020learning} for $\text{MMD}^2 $. In deep Kernel-based MMD $H_{ij}$ is defined as: 
\begin{equation}
    \widehat{\text{MMD}}^2_u(S_{\mathbb{P}}, S_{\mathbb{Q}}; k_w) := \frac{1}{n(n-1)} \sum_{i \neq j} H_{ij}
\end{equation}
\begin{equation}
    H_{ij} := k_w(x_i, x_j) + k_w(y_i, y_j) - k_w(x_i, y_j) - k_w(y_i, x_j).
\end{equation}
$k_w(x,z)$ is defined as:
\begin{equation}
k_w(x, z) = \left [ \left ( 1-\epsilon  \right )k\left ( \theta_w(x), \theta_w(z) \right ) + \epsilon   \right ]q\left (x,z  \right )  
\end{equation}

where $\theta_w$ is a multi-layer perceptron, which extracts features from the original embeddings to better represent distributional differences. $k$ and $q(x,z)$ are a simple kernel (e.g., a Gaussian kernel) and a simple characteristic kernel (e.g., a Gaussian kernel), respectively.

\textbf{How to optimize deep MMD-kernel?} Following \citet{liu2020learning}, the objective function of Deep Kernel MMD is introduced as follows L:

\begin{equation}
    J(\mathbb{P},\mathbb{Q}; k_\omega) = \frac{\mathrm{MMD}^2(\mathbb{P},\mathbb{Q}; k_\omega)}{\hat\sigma(\mathbb{P},\mathbb{Q}; k_\omega)}
\end{equation}

where $\hat{\sigma}^2_{\lambda}$ is a regularized estimator of $\sigma^2$, computed as:

\begin{equation}
    \hat{\sigma}^2_{\lambda} = \frac{4}{n^3} \sum_{i=1}^{n} \left( \sum_{j=1}^{n} H_{ij} \right)^2 
    - \frac{4}{n^4} \left( \sum_{i=1}^{n} \sum_{j=1}^{n} H_{ij} \right)^2 + \lambda,
\end{equation}

where $\lambda$ is a constant to avoid division by zero.

However, D-MIA does not use this objective function for optimization. Instead, it designs a more task-specific objective function.
\section{Model training setting}
\label{sec: model setting}

\begin{table}[ht]
\centering
\small
\caption{Training configurations for different models (EDM, DMD, and DI) across datasets (CIFAR10, FFHQ, and AFHQv2), including GPU setups, batch sizes, training times, and learning rates.}

\begin{tabular}{@{}llllll@{}}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{GPU} & \textbf{Batch size} & \textbf{Training Time} & \textbf{Learning Rate} \\ \midrule
\multirow{3}{*}{EDM} & CIFAR10 & 1 $\times$ NVIDIA A100 & 128 & 5-00:00:00 & 0.001 \\
                     & FFHQ    & 4 $\times$ NVIDIA A100 & 256 & 5-00:00:00 & 0.0002 \\
                     & AFHQv2  & 2 $\times$ NVIDIA A100 & 128 & 5-00:00:00 & 0.0002 \\ \midrule
\multirow{3}{*}{DMD} & CIFAR10 & 1 $\times$ NVIDIA A100 & 128 & 4-00:00:00 & 0.00005 \\
                     & FFHQ    & 1 $\times$ NVIDIA A100 & 64  & 4-00:00:00 & 0.00005 \\
                     & AFHQv2  & 1 $\times$ NVIDIA A100 & 64  & 4-00:00:00 & 0.00005 \\ \midrule
\multirow{3}{*}{DI}  & CIFAR10 & 1 $\times$ NVIDIA A100 & 128 & 3-00:00:00 & 0.00001 \\
                     & FFHQ    & 1 $\times$ NVIDIA A100 & 64  & 3-00:00:00 & 0.0001 \\
                     & AFHQv2  & 1 $\times$ NVIDIA A100 & 64  & 2-00:00:00 & 0.0001 \\ \bottomrule
\end{tabular}
\label{tab:training_configurations}
\end{table}
The training configurations for EDM, DMD, and DI are shown in Tab.~\ref{tab:training_configurations}. The specific model architectures will be released in the upcoming official code. For each dataset, half of the data is randomly selected for training EDM, while the remaining half is used as non-member data. EDM generates 100,000 samples to distill the DMD and DI models. During the distillation process, the models do not access the training data of EDM.

\section{D-MIA framework algorithm}
\label{sec: algorithm}
This section details the three key steps in D-MIA, each executing a specific algorithm: Deep-Kernel Training (\textbf{Alg.}~\ref{alg:alg1}), Detecting Candidate Dataset (\textbf{Alg.}~\ref{alg:alg2}), and Soft Voting for Membership Determination (\textbf{Alg.}~\ref{alg:soft_voting}).
\noindent
\begin{algorithm}[h]
\caption{Deep-kernel Training}\label{alg:alg1}
\begin{algorithmic}[1]
\STATE \textbf{Input:} non-member
set $\gD_{\rm non}$, one-step generative model $f_\theta$, encoder $f_e$, noise $\sigma$, learning rate $\eta$, epochs $T$
\STATE $S_g \gets \{f_\theta(z_i) \mid z_i \sim \mathcal{N}(0, I), \, i = 1, 2, \dots, N\}$
\STATE $S_{g, \text{noisy}} \gets \{s + \epsilon \mid s \in S_g, \, \epsilon \sim \mathcal{N}(0, \sigma^2 I)\}$
\STATE $S_{a, \text{noisy}} \gets \{a + \epsilon \mid a \in \gD_{\rm non}, \, \epsilon \sim \mathcal{N}(0, \sigma^2 I)\}$
\STATE $S_{g-e} \gets \{f_e(s) \mid s \in S_{g, \text{noisy}}\}$
\STATE $S_{a-e} \gets \{f_e(a) \mid a \in S_{a, \text{noisy}}\}$ 
\STATE $\gB_{\rm non} \gets \text{minibatch from } S_{a-e}$
\STATE $\gB_{\rm mem} \gets \text{minibatch from } S_{g-e}$
\STATE $\gB_{\rm anc} \gets \text{minibatch from } S_{g-e}$, $\gB_{\rm anc} \cap \gB_{\rm mem} = \emptyset$
\FOR{$\text{epoch} = 1, \dots, T$}
    \STATE $M_1(\omega) \gets \hat{\rm{MMD}}^2_u(\gB_{\rm mem}, \gB_{\rm anc}, k_\omega)$
    \STATE $M_2(\omega) \gets \hat{\rm{MMD}}^2_u(\gB_{\rm mem}, \gB_{\rm anc}, k_\omega)$
    \STATE $l \gets M_1(\omega) - M_2(\omega)$
    \STATE $\omega \gets \omega + \eta \nabla_{\text{Adam}} l$
\ENDFOR
\STATE \textbf{Output:}Deep kernel $k_\omega$, anchor generation $\gB_{\rm anc}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Detecting candidate datset}\label{alg:alg2}
\begin{algorithmic}[1] % The [1] adds line numbers
\STATE \textbf{Input:} non-member dataset for detection $\gD_{\rm non}$, candidate dataset $\gD_{\rm can}$, bottleneck features of anchor generation $\gB_{\rm anc}$ , one-step generative model encoder $f_e$, noise sigma $\sigma$, test time $ts$, Deep kernel $k_\omega$
\STATE $S_{c, \mathrm{noisy}} = \{c + \epsilon \mid c \in \gD_{\rm can}, \, \epsilon \sim \mathcal{N}(0, \sigma^2 I)\}$
\STATE $S_{a, \text{noisy}} \gets \{a + \epsilon \mid a \in \gD_{\rm non}, \, \epsilon \sim \mathcal{N}(0, \sigma^2 I)\}$
\STATE $S_{c-e} = \{f_e(c) \mid a \in S_{c, \text{noisy}}\}$
% \STATE Add Gaussian noise to generative samples:
\STATE $S_{a-e} \gets \{f_e(a) \mid a \in S_{a, \text{noisy}}\}$ 

\STATE $\gB_{\rm non} \gets \text{minibatch from } S_{a-e}$
\STATE $\gB_{\rm can} \gets \text{minibatch from } S_{c-e}$
\STATE $\rm counter$ = 0
\FOR{epoch = 1,...,$ts$}
\STATE $M_1(\omega) \gets \hat{\rm{MMD}}^2_u(\gB_{\rm can}, \gB_{\rm anc}, k_{\omega})$
\STATE $M_2(\omega) \gets \hat{\rm{MMD}}^2_u(\gB_{\rm non}, \gB_{\rm anc}, k_{\omega})$
\STATE \textbf{if} $M_1(\omega) < M_2(\omega)$ \textbf{then} \STATE $\rm counter+1$ 
\ENDFOR
\STATE $r \gets \rm countr$ $/ts$
\STATE \textbf{Output:} $r$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Soft Voting for Determining Membership in Candidate Dataset}\label{alg:soft_voting}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Candidate dataset $\gD_{\rm anc}$, non-member dataset $\gD_{\rm non}$, candidate dataset $\gD_{\rm can}$, number of iterations $h$, threshold $\alpha$, one-step generative model encoder $f_e$, noise sigma $\sigma$, test time $ts$, one-step generative model $f_\theta$, encoder $f_e$, learning rate $\eta$, epochs $T$
\STATE Initialize kernel function set $K = \emptyset$, classification results set $R = \emptyset$

\FOR{$i = 1$ \textbf{to} $h$}
    \STATE Train a kernel function $k_{w_i}$ using algorithm \ref{alg:alg1}
    \STATE Detecting result $r_i$ by $k_{w_i}$ using algorithm \ref{alg:alg2}
    \STATE Update $K \gets K \cup \{k_{w_i}\}$ and $R \gets R \cup \{r_i\}$
\ENDFOR
\STATE $\bar{p}_{\rm mem} \gets \frac{1}{|R|} \sum_{r_i \in R} r_i$
\STATE $M(D) \gets \begin{cases} 
1, & \text{if } \bar{p}_{\rm mem} \geq \alpha \\ 
0, & \text{otherwise}
\end{cases}$
\STATE \textbf{Output:} Membership decision $M(D)$
\end{algorithmic}
\end{algorithm}

\section{Details of Empirical Studies}
\label{app:experiment_details}
\textbf{Dataset and Victim Models.}
We empirically evaluate D-MIA on state-of-the-art distilled generative models, DMD~\citep{yin2024one} and Diff-Instruct~\citep{luo2024diff} on commonly studied MIA benchmarks, CIFAR10~\citep{krizhevsky2010cifar}, FFHQ~\citep{karras2019style}, and AFHQv2~\citep{choi2020stargan}.
See detailed setup of victim models in {\bf App.}~\ref{sec: model setting}

% \subsection{Experiment setting}
% \textbf{Dataset and target model.} 
% We evaluate the attack performance of DDG-MIA on three benchmark datasets, i.e., CIFAR10, FFHQ, and AFHQv2. For the target models, we attempt to attack one-step generative models distilled using two different methods: DMD, DI (detail DMD and DI training set up in {\bf App.}~\ref{sec: model setting})

\textbf{Baseline Settings}
DDG-MIA differs from existing MIA methods and attack targets. To ensure fairness, we adapt existing methods to the D-MIA setting for experimentation. Specifically, we apply existing MIA methods to each data point in the dataset to compute a loss-based result. Then we compute the mean loss result of all data points in the dataset. We randomly sample 50 candidate datasets (with replacement) and 50 non-member datasets (with replacement) and calculate the mean loss for each dataset. Then, we empirically determine an optimal threshold to distinguish between the loss means of candidate datasets and non-member datasets. Under this setting, we use SecMI and ReDiffuse as baseline methods for comparison.

\textbf{Evaluation Settings}
Before the experiment, each dataset is evenly divided into two subsets: one for member data used to train the teacher model (EDM) and the other for non-member data (detail EDM training set up in {\bf App.}~\ref{sec: model setting}). The teacher model generates 100,000 synthetic samples for the distillation of the student model, ensuring that the student model never accesses the original training data of the teacher model. We construct an auxiliary non-member dataset by randomly sampling 15,000 data points from the non-member data of FFHQ and CIFAR10, with 5,000 points used for deep kernel training (Algorithm \ref{alg:alg1}) and 10,000 for candidate dataset detection (Algorithm \ref{alg:alg2}). For AFHQv2, we sample 3,000 non-member data points, allocating 1,500 for kernel training and 1,500 for candidate detection. To ensure fairness, we randomly discard 15,000 member data points (3,000 for AFHQv2).

To evaluate D-MIA under varying proportions of member data in the candidate datasets, we create candidate datasets with 100\%, 50\%, and 30\% member data. During detection, we randomly sample 5,000 data points (1,500 for AFHQv2) based on the specified member ratios to construct positive candidate datasets. Additionally, we construct a negative candidate dataset consisting entirely of non-member data to assess whether it can be distinguished from the positive datasets. Similar to the baseline setting, we perform 50 rounds of sampling and detection to verify the attack accuracy of D-MIA. 

\textbf{Implementation details of D-MIA} 
The network architecture of the deep kernel follows the design proposed by Feng and Liu. The training parameters (e.g., bandwidth, learning rate, and epochs) used for attacking different models with various training datasets are detailed in the Table~\ref{tab:training_config}.
\begin{table}[h]
\centering
\small
\caption{Training configurations for MMD-based models across different datasets.}
\begin{tabular}{llccccc}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Bandwidth} & \textbf{Epoch} & \textbf{MMD learning rate} & \textbf{H} & \textbf{x\_out} \\ 
\midrule
\multirow{3}{*}{DI}  
  & CIFAR10  & 0.1    & 400 & 0.000001 & 450 & 35  \\ 
  & FFHQ     & 0.4    & 300 & 0.000001 & 450 & 50  \\ 
  & AFHQv2   & 0.1    & 400 & 0.000001 & 450 & 35  \\ 
\midrule
\multirow{3}{*}{DMD}  
  & CIFAR10  & 0.0025 & 300 & 0.0000001 & 250 & 20  \\ 
  & FFHQ     & 0.4    & 300 & 0.000001  & 450 & 50  \\ 
  & AFHQv2   & 0.1    & 400 & 0.000001  & 450 & 35  \\ 
\bottomrule
\end{tabular}
\label{tab:training_config}
\end{table}
% \subsection{D-MIA can effectively attack distilled generative models}
% \textbf{Analysis of the Results for Attacking the DMD Model} 
% Tab.~\ref{table_dmd} shows that the distribution-based D-MIA demonstrates excellent performance(close to 100\% ASR) in attacking DMD models across three different datasets, significantly outperforming existing MIA methods. Moreover, while D-MIA experiences some performance degradation(92\% ASR) on CIFAR10 when dealing with candidate datasets that contain a mixture of member and non-member data, it still achieves a very high attack success rate. In contrast, existing methods perform close to random guessing when attacking mixed datasets containing only 30\% member data.

% \textbf{Analysis of the Results for Attacking the DI Model} 
% Tab.~\ref{table_di} shows that D-MIA achieves a 100\% success rate in attacking DI models across three different datasets. Additionally, its attack success rate remains unaffected when dealing with candidate datasets of varying mixed distributions. In contrast, existing methods exhibit noticeable performance degradation across different datasets and candidate datasets with varying levels of mixed member and non-member data.


\subsection{D-MIA’s reliance on auxiliary non-member and candidate dataset sizes}
\label{datasize_test}
In D-MIA attacks, the attacker requires a certain amount of non-member data for auxiliary training and testing. Additionally, the candidate dataset being evaluated must have a sufficient size to obtain accurate distributional information. Therefore, we evaluate the performance of D-MIA on CIFAR10 models for DMD and DI under different auxiliary non-member dataset sizes and candidate dataset sizes. 
We evaluated three settings for auxiliary and candidate dataset sizes: auxiliary dataset sizes of 15,000, 9,000, 6,000 and 3000 paired with candidate dataset sizes of 5,000, 3,000, 2,000, 1000 respectively. Half of the auxiliary dataset was used to train the deep kernel, while the other half supported attacks on candidate datasets. Positive samples were drawn from member data corresponding to the candidate dataset size, and negative samples were drawn from non-member data of the same size. Following previous evaluation Settings, 50 positive and 50 negative samples were constructed, and D-MIA was applied to distinguish between them.


\section{Additional Experimental Results}
We conducted a series of experiments to evaluate the effectiveness of different I-MIA methods on various generative models. Specifically, we extracted half of the data from the CIFAR10, FFHQ, and AFHQv2 datasets to train three EDM generative models, and then used the data generated by EDM to train DMD and Diff-Instruc. Finally, we applied four state-of-the-art MIA techniques—GAN-Leak, SecMI, ReDiffuse, and GSA—to attack these models. The results are presented in Table \ref{add_result}.


\noindent
\label{add_result}
\begin{table*}[h]
\centering
\small
\caption{the ASR and AUC results of various membership inference attack methods across different generative models and datasets. The table compares four attack methods—GAN-leak, SecMI, ReDiffuse, and GSA—on three generative models: EDM, DMD, and Diff-Instruc, evaluated on CIFAR-10, FFHQ, and AFHQv2 datasets.}
\label{table_results_detail}
\vspace{1mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
\multirow{2}{*}{\textbf{Model/Dataset}} & \multirow{2}{*}{} 
& \multicolumn{2}{c}{\textbf{GAN-leak}} & \multicolumn{2}{c}{\textbf{SecMI}} & \multicolumn{2}{c}{\textbf{Rediffuse}} & \multicolumn{2}{c}{\textbf{GSA}} \\ 
\cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10} 
& & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} & \textbf{ASR} & \textbf{AUC} \\ 
\midrule
EDM/CIFAR10 & & 0.536 $\pm$ .005 & 0.523 $\pm$ .011 & 0.588 $\pm$ .004 & 0.601 $\pm$ .021 & 0.579 $\pm$ .002 & 0.603 $\pm$ .004 & 0.622 $\pm$ .008 & 0.626 $\pm$ .004 \\ 
EDM/ffhq & & 0.524 $\pm$ .008 & 0.518 $\pm$ .018 & 0.551 $\pm$ .009 & 0.564 $\pm$ .011 & 0.541 $\pm$ .005 & 0.553 $\pm$ .005 & 0.662 $\pm$ .006 & 0.654 $\pm$ .003 \\ 
EDM/afhqv & & 0.543 $\pm$ .004 & 0.532 $\pm$ .009 & 0.604 $\pm$ .005 & 0.622 $\pm$ .013 & 0.604 $\pm$ .005 & 0.644 $\pm$ .006 & 0.906 $\pm$ .004 & 0.908 $\pm$ .001 \\ 
\midrule
DMD/CIFAR10 & & 0.497 $\pm$ .012 & 0.508 $\pm$ .011 & 0.520 $\pm$ .018 & 0.516 $\pm$ .020 & 0.514 $\pm$ .008 & 0.509 $\pm$ .013 & 0.512 $\pm$ .003 & 0.502 $\pm$ .001 \\ 
DMD/ffhq & & 0.502 $\pm$ .019 & 0.498 $\pm$ .021 & 0.515 $\pm$ .021 & 0.502 $\pm$ .037 & 0.507 $\pm$ .004 & 0.504 $\pm$ .008 & 0.525 $\pm$ .002 & 0.505 $\pm$ .001 \\ 
DMD/afhqv & & 0.512 $\pm$ .009 & 0.515 $\pm$ .032 & 0.525 $\pm$ .007 & 0.513 $\pm$ .007 & 0.521 $\pm$ .007 & 0.524 $\pm$ .004 & 0.532 $\pm$ .004 & 0.523 $\pm$ .003 \\ 
\midrule
 Diff-Instruc/CIFAR10 & & 0.502 $\pm$ .005 & 0.497 $\pm$ .003 & 0.507 $\pm$ .004 & 0.501 $\pm$ .009 & 0.514 $\pm$ .004 & 0.511 $\pm$ .007 & 0.503 $\pm$ .001 & 0.503 $\pm$ .001 \\ 
 Diff-Instruc/ffhq & & 0.493 $\pm$ .002 & 0.503 $\pm$ .005 & 0.514 $\pm$ .008 & 0.509 $\pm$ .008 & 0.509 $\pm$ .002 & 0.509 $\pm$ .004 & 0.501 $\pm$ .002 & 0.511 $\pm$ .002 \\ 
 Diff-Instruc/afhqv & & 0.501 $\pm$ .009 & 0.502 $\pm$ .006 & 0.504 $\pm$ .005 & 0.504 $\pm$ .008 & 0.513 $\pm$ .003 & 0.506 $\pm$ .005 & 0.511 $\pm$ .005 & 0.515 $\pm$ .002 \\ 
\bottomrule
\end{tabular}}
\end{table*}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
