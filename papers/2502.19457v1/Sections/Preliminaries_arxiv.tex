\section{Preliminaries}

\subsection{Differentiable Rendering }

% Point-based rendering techniques aim to produce realistic images by rendering a collection of discrete geometric primitives. Zwicker et al.~\cite{zwicker2001surface} introduced a method that uses ellipsoid-shaped splats, allowing them to cover multiple pixels and enhance image quality by reducing visible holes through overlapping splats. This approach is more efficient than traditional point-based representations. Kopanas et al.~\cite{kopanas2021point} developed a novel differentiable point-based rendering pipeline that incorporates bi-directional Elliptical Weighted Average splatting, a probabilistic depth test, and efficient camera selection. Recent advancements in this field have focused on improving the rendering process through techniques such as anti-aliasing texture filters~\cite{zwicker2001ewa}, enhancements in rendering efficiency~\cite{botsch2003high}, and addressing issues with discontinuous shading~\cite{rusinkiewicz2000qsplat}.

% Traditional point-based rendering methods are focused on generating high-quality outputs based on predefined geometries. However, recent advances in implicit representation techniques have led researchers to explore point-based rendering within the framework of neural implicit representations. This approach eliminates the need for predefined geometries in 3D reconstruction. A prominent example in this area is  NeRF~\cite{mildenhall2020nerf}, which uses an implicit density field to model 3D geometry and an appearance field to predict view-dependent colors. In NeRF, point-based rendering is used to aggregate the colors from all sample points along a camera ray to compute the final pixel color \( C \).

%Differentiable rendering (DR) enables end-to-end optimization by providing gradients of the rendering process, bridging 2D and 3D processing. This allows neural networks to optimize 3D entities through 2D projections. Gradients can be computed with respect to scene parameters, such as geometry, lighting, and camera pose which is essential for tasks like inverse graphics, 3D reconstruction, and neural rendering, where scene parameters are optimized using self supervision. Rendering approaches are broadly categorized into implicit and explicit methods. Implicit rendering uses continuous scene representations, such as neural networks or signed distance fields, and utilizes volume rendering for image generation. In contrast, explicit rendering defines scene geometry using discrete primitives like triangles, points, or splats. Explicit point-based rendering techniques aim to produce realistic images by rendering a collection of discrete geometric primitives. Zwicker et al.~\cite{zwicker2001surface} introduced a method that uses ellipsoid-shaped splats, allowing them to cover multiple pixels and enhance image quality by reducing visible holes through overlapping splats. This approach is more efficient than traditional point-based representations. Kopanas et al.~\cite{kopanas2021point} developed a novel differentiable point-based rendering pipeline incorporating bi-directional Elliptical Weighted Average splatting, a probabilistic depth test, and efficient camera selection. 

Differentiable rendering (DR) facilitates end-to-end optimization by computing gradients of the rendering process, bridging 2D and 3D processing~\cite{kato2020differentiable}. These gradients, with respect to scene parameters like geometry, lighting, and camera pose, are crucial for tasks such as human pose estimation~\cite{ge20193d,baek2019pushing}, 3D reconstruction~\cite{yan2016perspective,tulsiani2017multi}, and neural rendering~\cite{kato2018neural}, enabling self-supervised optimization. Rendering methods are categorized as implicit or explicit. Implicit approaches represent scenes using continuous functions, such as neural networks~\cite{sitzmann2020implicit} or signed distance fields~\cite{oleynikova2016signed}, and rely on volume rendering~\cite{drebin1988volume} for image generation. Explicit methods, in contrast, define geometry using discrete primitives like triangles, points, or splats. Explicit point-based rendering focuses on realistic image synthesis by rendering collections of discrete geometric primitives. Zwicker et al.\cite{kopanas2021point} advanced this with a differentiable point-based pipeline featuring bi-directional Elliptical Weighted Average splatting, probabilistic depth testing, and efficient camera selection.

Recent advances in implicit representation techniques have led researchers to explore point-based rendering within the framework of neural implicit representations. This approach eliminates the need for predefined geometries in 3D reconstruction. A prominent example in this area is  NeRF~\cite{mildenhall2020nerf}, which uses an implicit density field to model 3D geometry and an appearance field to predict view-dependent colors. In NeRF, point-based rendering is used to aggregate the colors from all sample points along a camera ray to compute the final pixel color

\[ C = \sum_{i=1}^{N} c_i \alpha_i T_i , \]

\noindent
where \( N \) represents the number of sample points along a ray. The view-dependent color and opacity value for the \( i \)-th point on the ray are given by:

\[ \alpha_i = \exp \left(- \sum_{j=1}^{i-1} \sigma_j \delta_j \right), \]
\noindent
where \( \sigma_j \) denotes the density value of the \( j \)-th point and \( \delta_j \) is the distance between consecutive sample points. The accumulated transmittance \( T_i \) is calculated as:

\[ T_i = \prod_{j=1}^{i-1} (1 - \alpha_j). \]

The rendering process in 3DGS shares similarities with NeRFs; however, there are two key differences. \newline
\textbf{Opacity Modeling:} 3DGS directly models opacity values, while NeRF first models density values and then converts these densities into opacity values.\newline
\textbf{Rendering Technique:} 3DGS utilizes rasterization-based rendering, which avoids the need for extensive point sampling, unlike NeRF, which relies on sampling points along rays for rendering.


% \subsection{Neural Implicit Field}
% %Neural implicit field representations have recently gained substantial attention~\cite{}. These methods conceptualize 2D or 3D signals as fields within corresponding Euclidean spaces, training neural networks on discrete samples to approximate these fields. This approach supports tasks such as reconstructing, interpolating, and extrapolating from the original discrete samples, facilitating applications like super-resolution of 2D images and novel view synthesis of 3D scenes. Specifically, in 3D reconstruction and novel view synthesis, NeRFs~\cite{} utilize neural networks to model 3D scenes as density and radiance fields. NeRF employs volumetric rendering to map these 3D fields to 2D images, enabling the reconstruction of 3D scenes from multiple 2D images and allowing for novel view rendering. Noteworthy methods in this area include Mip-NeRF 360~\cite{}, which excels in rendering quality, and Instant-NGP~\cite{}, renowned for its exceptional training efficiency.

% %In recent times, there has been a significant focus on NeRFs~\cite{ramirez2024deep,wu2021revisiting}. These techniques involve training neural networks on discrete samples to model 2D or 3D signals as fields within equivalent Euclidean spaces. This approach facilitates operations such as reconstruction, interpolation, and extrapolation, making it useful for applications like novel view synthesis of 3D scenes and 2D image super-resolution. 
% For applications like 3D reconstruction and novel view synthesis, NeRFs \cite{mildenhall2021nerf,ramirez2024deep,wu2021revisiting} use neural networks to implicitly represent 3D scenes as density and radiance fields. NeRF also employs volumetric rendering to project these 3D fields onto 2D images, enabling the generation of new views and the reconstruction of 3D scenes from multiple 2D images. Among the notable techniques in this domain are Instant-NGP~\cite{muller2022instant}, recognized for its exceptional training efficiency, and Mip-NeRF 360~\cite{barron2022mip}, which is distinguished by its superior rendering quality.

% NeRFs predominantly rely on volumetric ray marching for image rendering. This process involves sampling numerous points along each ray and passing them through the neural network to generate the final image. Consequently, rendering a single 1080p image can require up to \(10^8\) neural network forward passes, often leading to rendering delays of several seconds. Even though some approaches use explicit, discretized structures to represent continuous 3D fields—thereby reducing reliance on neural networks and accelerating the query process \cite{sun2022direct,chen2022tensorf}—the high number of sampling points still results in significant rendering costs. As a result, volumetric rendering-based techniques struggle to achieve real-time performance, limiting their applicability in interactive or real-time scenarios.

\subsection{3D Gaussian Splatting}
3DGS represents a significant advancement in real-time, high-resolution image rendering that does not rely on deep neural networks. %This section provides an overview of 3DGS. In Sec. 3.1, we explain how 3DGS synthesizes an image using optimized 3D Gaussians. This involves the rendering pipeline and techniques employed to achieve high-quality images efficiently. In Sec. 3.2, we discuss the process of generating well-optimized 3D Gaussians for a given scene. This section delves into the 3DGS optimization method, outlining the steps and algorithms used to refine the Gaussian parameters for optimal rendering performance.
%\subsubsection{Rendering with 3DGS}
%NeRFs utilize volumetric raymarching for rendering, a method that is both slow and computationally intensive. In contrast, 
A scene optimized with 3DGS achieves rendering by projecting the Gaussians onto a 2D image plane through splatting. After this projection, 3DGS sorts the Gaussians and computes the pixel values accordingly~\cite{kerbl20233d}. The following sections will first define the 3D Gaussian, the fundamental element of the 3DGS scene representation, then detail the differentiable rendering process used in 3DGS, and finally, describe the techniques employed to achieve high rendering speeds.
%Consider a scene with millions of optimized 3D Gaussians. The goal is to create an image using a specified camera pose. NeRFs address this task using computationally intensive volumetric raymarching, which involves sampling points in 3D space per pixel. This method struggles with high-resolution image synthesis and fails to achieve real-time rendering, particularly on platforms with limited computational resources. In contrast, 3DGS projects these 3D Gaussians onto a pixel-based image plane through a process called "splatting." Following this projection, 3DGS sorts the Gaussians and computes the pixel values. The following sections will start with defining a 3D Gaussian, the fundamental element in 3DGS scene representation. Next, we will explain how these 3D Gaussians are utilized for differentiable rendering. Finally, we will introduce the acceleration technique employed in 3DGS, which is essential for fast rendering.



\noindent
\textbf{3D Gaussian: }The characteristics of a 3D Gaussian include its color \( c \), 3D covariance matrix \( \Sigma \), opacity \( \alpha \), and center (position) \( \mu \). For view-dependent appearance, spherical harmonics (SH) are employed to represent color \( c \). Through back-propagation, all these attributes can be learned and optimized.

\noindent
\textbf{3D Gaussian Frustum Culling: } This stage identifies the 3D Gaussians that lie outside the camera's frustum for a given camera position. Excluding these Gaussians from subsequent computations conserves processing resources, as only the 3D Gaussians within the specified view are considered.

\noindent
\textbf{Splatting of Gaussians: }In this stage, 3D Gaussians (ellipsoids) are projected onto 2D image space (ellipses) for rendering. The projected 2D covariance matrix \( \Sigma' \) is computed using the viewing transformation \( W \) and the 3D covariance matrix \( \Sigma \) as
\noindent
\[ \Sigma' = JW\Sigma W^\top J ,\]
\noindent
where \( W \) is the viewing transformation matrix, \( \Sigma \) is the 3D covariance matrix, and \( J \) is the Jacobian of the projection transformation~\cite{kerbl20233d, zwicker2001ewa}.

\noindent
\textbf{Pixel Rendering: } The viewing transformation \( W \) is employed to compute the distance between each pixel \( x \) and all overlapping Gaussians, effectively determining their depths. This results in a sorted list of Gaussians \( N \). The final color of each pixel is subsequently determined using alpha compositing. The final color \( C \) is computed by summing the contributions of each Gaussian:

\[ C = \sum_{n=1}^{|N|} c_n \alpha'_n \prod_{j=1}^{n-1} (1 - \alpha'_j) ,\]

\noindent
where \( c_n \) is the learned color. The final opacity \( \alpha'_n \) is the product of the learned opacity \( \alpha_n \) and the Gaussian function:

\[ \alpha'_n = \alpha_n \times \exp \left( -\frac{1}{2} (x' - \mu'_n)^\top \Sigma'^{-1}_n (x' - \mu'_n) \right) ,\]

\noindent
where \( x' \) is the projected position, \( \mu'_n \) is the projected center, and \( \Sigma'_n \) is the projected 2D covariance matrix. There is a valid concern that the described rendering process could be slower than NeRFs due to the challenges in parallelizing the generation of the necessary sorted list. This concern is valid, as relying on a straightforward pixel-by-pixel approach could significantly impact rendering speeds. To achieve real-time rendering, 3DGS must consider several factors that facilitate parallel processing.
%Given the challenge of parallelizing the generation of the necessary sorted list, there is a legitimate concern that the described rendering process might be slower than NeRFs. This concern is valid, as rendering times can be significantly impacted by using such a basic pixel-by-pixel method. To enable real-time rendering, 3DGS must account for several factors that support parallel processing.

\input{Display_Figures/dataset}
\input{Display_Figures/opacity_gradient_pruning_comparison}
\input{Display_Figures/opacity_distribution}
\noindent
\textbf{Tiling: }
3DGS reduces computational costs by shifting rendering accuracy from fine-grained, pixel-level detail to a more coarse, patch-level approach. The image is first divided into non-overlapping patches or tiles, and then 3DGS identifies which tiles intersect with the projected Gaussians. If a single projected Gaussian spans multiple tiles, it duplicates the Gaussian, assigning each copy a unique tile ID corresponding to the relevant tile as shown in Figure~\ref{fig:working}.

\noindent
\textbf{Rendering Parallelization: }Following the Gaussians' replication, 3DGS associates each corresponding tile ID with the depth values obtained from the view transformation. This process generates an unsorted list of bytes where the higher bits represent the tile ID and the lower bits encode the depth information. This list can then be sorted for rendering (alpha compositing). This method is particularly well-suited for parallel processing because it allows each tile and pixel to be handled independently as shown in the Figure~\ref{fig:working}. Additionally, the advantage of allowing each pixel of the tile to access shared memory while maintaining a unified read pattern enables more efficient parallel processing during the alpha compositing stage. The framework essentially treats tile and pixel processing similarly to blocks and threads in CUDA programming, as demonstrated in the original study's implementation.

%After replicating the Gaussians, 3DGS associates the depth values obtained from the view transformation with each corresponding tile ID. This results in an unsorted list of bytes where the lower bits represent the depth and the upper bits denote the tile ID. This list can then be sorted for rendering (alpha compositing). This method is particularly well-suited for parallel processing because it allows each tile and pixel to be handled independently. Additionally, the advantage of each tile's pixels being able to access a shared memory while maintaining a consistent read sequence facilitates more efficient parallel execution of alpha compositing. The framework essentially treats tile and pixel processing similarly to blocks and threads in CUDA programming, as demonstrated in the original study's implementation.

In summary, 3DGS enhances computational efficiency while preserving high image reconstruction quality by incorporating several approximations during the rendering process.

\noindent
\textbf{3D Gaussian Splatting Optimization: }
%The core of 3DGS involves an optimization process aimed at generating a substantial number of 3D Gaussians that effectively capture the scene's essence, enabling free-viewpoint rendering. 
The key component of 3DGS lies in an optimization process aimed at generating a substantial number of Gaussians that effectively encapsulate the key features of the scene, thereby enabling real-time 3D scene rendering. Differentiable rendering is employed to fine-tune the parameters of these 3D Gaussians to align with the scene's textures. However, the optimal number of 3D Gaussians required to accurately represent a scene is not predetermined. %In the following sections, we will address the optimization of each Gaussian's attributes and the regulation of Gaussian density. 
The optimization workflow encompasses a series of interrelated procedures.

\noindent
\textbf{Loss Function: }
%After completing the image synthesis, the discrepancy between the rendered image and the ground truth can be assessed. 
The disparity between the ground truth and the rendered output can be evaluated upon completion of the image synthesis. To achieve this, the \( \ell_1 \) loss and D-SSIM (Differentiable Structural Similarity Index)~\cite{bakurov2022structural} loss functions are utilized. Stochastic gradient descent (SGD) is then employed to optimize all the learnable parameters. The loss function used for 3DGS is given by~\cite{kerbl20233d}:

\[ L = (1 - \lambda)L_1 + \lambda L_{\text{D-SSIM}}, \]
\noindent
where \( \lambda \in [0, 1] \) is a weighting factor.





\noindent
\textbf{Parameter Optimization:}
Back-propagation can be utilized to directly optimize most attributes of a 3D Gaussian. 
It is crucial to note that optimization of the covariance matrix directly could yield a non-positive semi-definite matrix, thereby contradicting the physical restrictions typically imposed on covariance matrices.
%However, a key consideration is that directly optimizing the covariance matrix \(\Sigma\) might result in a non-positive semi-definite matrix, which conflicts with the physical constraints typically associated with covariance matrices. 
To address this issue, 3DGS employs an alternative approach by optimizing a quaternion \(q\) and a 3D vector \(s\), where \(q\) represents rotation and \(s\) represents scale. This method ensures that the resulting covariance matrix adheres to the required physical properties. This method enables the covariance matrix \(\Sigma\) to be reconstructed using the following formula:

\[
\Sigma = R S S^\top R^\top,
\]
\noindent
where \(S\) and \(R\) represent the scaling and rotation matrices derived from \(s\) and \(q\), respectively. The process of calculating opacity \(\alpha\) involves a complex computational graph: \(s\) and \(q \to \Sigma\), \(\Sigma \to \Sigma'\), and \(\Sigma' \to \alpha\). To avoid the computational expense of automatic differentiation, 3DGS directly derives and computes the gradients for \(q\) and \(s\) during optimization.

%\input{Display_Figures/opacity_distribution}

\noindent
\textbf{Densification Process:}
In 3DGS, optimization parameters are initialized using sparse points from Structure-from-Motion (SfM)~\cite{snavely2006photo}. Densification and pruning techniques are then applied to optimize the density of 3D Gaussians.
During densification, 3DGS adaptively increases Gaussian density in regions with sparse coverage or missing geometric features. After specific training intervals, Gaussians with gradients above a threshold are densified: large Gaussians are split into smaller ones, and small Gaussians are duplicated and shifted along gradient directions. This ensures an optimal distribution for improved 3D scene synthesis.
In the pruning stage, redundant or insignificant Gaussians are removed to regularize the representation. Gaussians with excessive view-space size or opacity \( \alpha \) below a threshold are discarded, ensuring efficiency while preserving accuracy in scene representation.
