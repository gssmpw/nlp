\section{Related work}
\label{sec:related work} 

\textbf{Intermediate layers matter.}
Recent studies underscore the importance of intermediate layers in LLMs, emphasizing their role in producing more informative representations for downstream tasks compared to final layers \cite{kavehzadeh2024sorted, ju2024large, liu2024fantastic, skean2024does}. These layers are crucial for encoding abstract knowledge, enabling advanced capabilities like in-context learning and transfer learning, which are vital for understanding and optimizing LLMs \cite{zhang-etal-2024-investigating}. Additionally, intermediate layers exhibit distinct patterns of information compression and abstraction, such as reduced entropy, allowing them to efficiently represent complex inputs \cite{doimo2024representation, yin2024entropy}.

\textbf{Superposition.}
The Superposition Hypothesis suggests that neural networks can encode far more features than neurons they have by compressing high-dimensional concepts into overlapping, nearly orthogonal representations \cite{arora2018linear, scherlis2022polysemanticity, jermyn2022engineering}. Instead of assigning features to individual neurons, features are represented as sparse linear combinations across neurons, improving encoding efficiency and reducing interference. Toy models demonstrate that sparsity enhances feature disentanglement, balancing compression and accuracy \cite{elhage2022toy}. Early layers encode numerous features with sparse combinations, while intermediate layers focus on higher-level contextual features \cite{gurnee2023finding}.  

\textbf{Linear representation hypothesis.} The linear representation hypothesis suggests that neural networks encode high-level features as linear directions in activation space, enabling easier interpretation and manipulation \cite{nanda2023b}. Probing, introduced by \cite{alain2016understanding}, assesses feature encoding in models and builds on findings in word embeddings like GloVe and Word2Vec, which capture semantic relationships through linear structures \cite{mikolov2013, pennington2014}. Empirical support spans various contexts, including spatial and temporal representations \cite{gurnee2023language}, sentiment analysis \cite{tigges2023linear}, task-specific features \cite{hendel2023context}, and broader relational structures \cite{hernandez2023linearity}.

\textbf{Non-linear Representations.} Although the linear representation hypothesis offers insights into neural network representations, studies have highlighted its limitations and emphasized the significance of non-linear structures. \cite{li2022emergent} demonstrated that GPT models trained on the game `Othello' required non-linear probes to decode board states. Non-linear structures, such as the `pizza' and `clock' patterns \cite{nanda2023progress, zhong2024clock}, and circular representations observed in tasks like predicting days or months using modular arithmetic prompts \cite{engels2024not}, reveal the complexity of these representations.