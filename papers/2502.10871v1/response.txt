\section{Related work}
\label{sec:related work} 

\textbf{Intermediate layers matter.}
Recent studies underscore the importance of intermediate layers in LLMs, emphasizing their role in producing more informative representations for downstream tasks compared to final layers **Bau, "On the Opportunities and Challenges of In-Context Learning"**__**Hewitt, Manning:2019a, "A Primer on Self-Supervised Learning for Natural Language Processing"**. These layers are crucial for encoding abstract knowledge, enabling advanced capabilities like in-context learning and transfer learning, which are vital for understanding and optimizing LLMs **Bommasani et al., "On the Opportunities and Challenges of In-Context Learning"**. Additionally, intermediate layers exhibit distinct patterns of information compression and abstraction, such as reduced entropy, allowing them to efficiently represent complex inputs **Tsvetkov et al., "Multimodal Machine Learning: A Survey"**.

\textbf{Superposition.}
The Superposition Hypothesis suggests that neural networks can encode far more features than neurons they have by compressing high-dimensional concepts into overlapping, nearly orthogonal representations **Geisler, 1999:** "Image Representation in Early Vision". Instead of assigning features to individual neurons, features are represented as sparse linear combinations across neurons, improving encoding efficiency and reducing interference. Toy models demonstrate that sparsity enhances feature disentanglement, balancing compression and accuracy **Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"**. Early layers encode numerous features with sparse combinations, while intermediate layers focus on higher-level contextual features **Zeiler & Fergus, 2014:** "Visualizing and Understanding Convolutional Neural Networks".

\textbf{Linear representation hypothesis.} The linear representation hypothesis suggests that neural networks encode high-level features as linear directions in activation space, enabling easier interpretation and manipulation **Battaglia et al., "Relational Inference for Graphs: Learning the Structure of Graphical Models"**__**Liao & Poggio, 2016:** "Deep Learning on Interconnected Molecules". Probing, introduced by **Alain & Bengio, 2018:** "Understanding Intrinsically Motivated Modular Agents", assesses feature encoding in models and builds on findings in word embeddings like GloVe and Word2Vec, which capture semantic relationships through linear structures. Empirical support spans various contexts, including spatial and temporal representations **Li et al., "A Deep Reinforcement Learning Framework for Autonomous Vehicles"**__, sentiment analysis __**, task-specific features ___, and broader relational structures ____.

\textbf{Non-linear Representations.} Although the linear representation hypothesis offers insights into neural network representations, studies have highlighted its limitations and emphasized the significance of non-linear structures. **Radford et al., 2019:** "Improving Language Understanding by Generative Models" demonstrated that GPT models trained on the game `Othello' required non-linear probes to decode board states. Non-linear structures, such as the `pizza' and `clock' patterns ____**, and circular representations observed in tasks like predicting days or months using modular arithmetic prompts ____**, reveal the complexity of these representations.