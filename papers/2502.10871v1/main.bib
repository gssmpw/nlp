@article{engels2024not,
  title={Not all language model features are linear},
  author={Engels, Joshua and Michaud, Eric J and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{zhong2024clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{tigges2023linear,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}

@article{hernandez2023linearity,
  title={Linearity of relation decoding in transformer language models},
  author={Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2308.09124},
  year={2023}
}


@article{hendel2023context,
  title={In-context learning creates task vectors},
  author={Hendel, Roee and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2310.15916},
  year={2023}
}

@article{nanda2023b,
  title={Transformer Circuit Faithfulness Metrics Are Not Robust},
  author={Nanda, Neel and others},
  journal={arXiv preprint arXiv:2407.08734},
  year={2023}
}

@inproceedings{mikolov2013,
  title={Distributed Representations of Words and Phrases and their Compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@inproceedings{pennington2014,
  title={GloVe: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{gurnee2023finding,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{scherlis2022polysemanticity,
  title={Polysemanticity and capacity in neural networks},
  author={Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck},
  journal={arXiv preprint arXiv:2210.01892},
  year={2022}
}

@article{jermyn2022engineering,
  title={Engineering monosemanticity in toy models},
  author={Jermyn, Adam S and Schiefer, Nicholas and Hubinger, Evan},
  journal={arXiv preprint arXiv:2211.09169},
  year={2022}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{kumar2021semantic,
  title={Semantic memory: A review of methods, models, and current challenges},
  author={Kumar, Abhilasha A},
  journal={Psychonomic Bulletin \& Review},
  volume={28},
  number={1},
  pages={40--80},
  year={2021},
  publisher={Springer}
}

@article{beaty2023associative,
  title={Associative thinking at the core of creativity},
  author={Beaty, Roger E and Kenett, Yoed N},
  journal={Trends in cognitive sciences},
  volume={27},
  number={7},
  pages={671--683},
  year={2023},
  publisher={Elsevier}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@article{singh2024rethinking,
  title={Rethinking interpretability in the era of large language models},
  author={Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2402.01761},
  year={2024}
}

@article{dar2022analyzing,
  title={Analyzing transformers in embedding space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@inproceedings{teerapittayanon2016branchynet,
  title={Branchynet: Fast inference via early exiting from deep neural networks},
  author={Teerapittayanon, Surat and McDanel, Bradley and Kung, Hsiang-Tsung},
  booktitle={2016 23rd international conference on pattern recognition (ICPR)},
  pages={2464--2469},
  year={2016},
  organization={IEEE}
}

@article{elbayad2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={arXiv preprint arXiv:1910.10073},
  year={2019}
}

@article{elhoushi2024layer,
  title={Layer skip: Enabling early exit inference and self-speculative decoding},
  author={Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others},
  journal={arXiv preprint arXiv:2404.16710},
  year={2024}
}

@article{kao2020bert,
  title={BERT's output layer recognizes all hidden layers? Some Intriguing Phenomena and a simple way to boost BERT},
  author={Kao, Wei-Tsung and Wu, Tsung-Han and Chi, Po-Han and Hsieh, Chun-Cheng and Lee, Hung-Yi},
  journal={arXiv preprint arXiv:2001.09309},
  year={2020}
}

@article{skean2024does,
  title={Does representation matter? exploring intermediate layers in large language models},
  author={Skean, Oscar and Arefin, Md Rifat and LeCun, Yann and Shwartz-Ziv, Ravid},
  journal={arXiv preprint arXiv:2412.09563},
  year={2024}
}

@article{doimo2024representation,
  title={The representation landscape of few-shot learning and fine-tuning in large language models},
  author={Doimo, Diego and Serra, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
  journal={arXiv preprint arXiv:2409.03662},
  year={2024}
}

@article{yin2024entropy,
  title={Entropy law: The story behind data compression and llm performance},
  author={Yin, Mingjia and Wu, Chuhan and Wang, Yufei and Wang, Hao and Guo, Wei and Wang, Yasheng and Liu, Yong and Tang, Ruiming and Lian, Defu and Chen, Enhong},
  journal={arXiv preprint arXiv:2407.06645},
  year={2024}
}

@inproceedings{zhang-etal-2024-investigating,
    title = "Investigating Layer Importance in Large Language Models",
    author = "Zhang, Yang  and
      Dong, Yanfei  and
      Kawaguchi, Kenji",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.29/",
    doi = "10.18653/v1/2024.blackboxnlp-1.29",
    pages = "469--479",
    abstract = "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in safety-critical scenarios and hindered the development of better models. In this study, we advance the understanding of LLM by investigating the significance of individual layers in LLMs. We propose an efficient sampling method to faithfully evaluate the importance of layers using Shapley values, a widely used explanation framework in feature attribution and data valuation. In addition, we conduct layer ablation experiments to assess the performance degradation resulting from the exclusion of specific layers. Our findings reveal the existence of cornerstone layers, wherein certain early layers can exhibit a dominant contribution over others. Removing one cornerstone layer leads to a drastic collapse of the model performance, often reducing it to random guessing. Conversely, removing non-cornerstone layers results in only marginal performance changes. This study identifies cornerstone layers in LLMs and underscores their critical role for future research."
}

@inproceedings{kavehzadeh2024sorted,
  title={Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference},
  author={Kavehzadeh, Parsa and Valipour, Mojtaba and Tahaei, Marzieh and Ghodsi, Ali and Chen, Boxing and Rezagholizadeh, Mehdi},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={2129--2145},
  year={2024}
}

@article{ju2024large,
  title={How large language models encode context knowledge? a layer-wise probing study},
  author={Ju, Tianjie and Sun, Weiwei and Du, Wei and Yuan, Xinwei and Ren, Zhaochun and Liu, Gongshen},
  journal={arXiv preprint arXiv:2402.16061},
  year={2024}
}

@article{liu2024fantastic,
  title={Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics},
  author={Liu, Zhu and Kong, Cunliang and Liu, Ying and Sun, Maosong},
  journal={arXiv preprint arXiv:2403.01509},
  year={2024}
}