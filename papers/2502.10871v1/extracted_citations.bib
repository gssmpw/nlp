@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{arora2018linear,
  title={Linear algebraic structure of word senses, with applications to polysemy},
  author={Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={483--495},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{doimo2024representation,
  title={The representation landscape of few-shot learning and fine-tuning in large language models},
  author={Doimo, Diego and Serra, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
  journal={arXiv preprint arXiv:2409.03662},
  year={2024}
}

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{engels2024not,
  title={Not all language model features are linear},
  author={Engels, Joshua and Michaud, Eric J and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@article{gurnee2023finding,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}

@article{gurnee2023language,
  title={Language models represent space and time},
  author={Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.02207},
  year={2023}
}

@article{hendel2023context,
  title={In-context learning creates task vectors},
  author={Hendel, Roee and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2310.15916},
  year={2023}
}

@article{hernandez2023linearity,
  title={Linearity of relation decoding in transformer language models},
  author={Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2308.09124},
  year={2023}
}

@article{jermyn2022engineering,
  title={Engineering monosemanticity in toy models},
  author={Jermyn, Adam S and Schiefer, Nicholas and Hubinger, Evan},
  journal={arXiv preprint arXiv:2211.09169},
  year={2022}
}

@article{ju2024large,
  title={How large language models encode context knowledge? a layer-wise probing study},
  author={Ju, Tianjie and Sun, Weiwei and Du, Wei and Yuan, Xinwei and Ren, Zhaochun and Liu, Gongshen},
  journal={arXiv preprint arXiv:2402.16061},
  year={2024}
}

@inproceedings{kavehzadeh2024sorted,
  title={Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference},
  author={Kavehzadeh, Parsa and Valipour, Mojtaba and Tahaei, Marzieh and Ghodsi, Ali and Chen, Boxing and Rezagholizadeh, Mehdi},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={2129--2145},
  year={2024}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{liu2024fantastic,
  title={Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics},
  author={Liu, Zhu and Kong, Cunliang and Liu, Ying and Sun, Maosong},
  journal={arXiv preprint arXiv:2403.01509},
  year={2024}
}

@inproceedings{mikolov2013,
  title={Distributed Representations of Words and Phrases and their Compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@article{nanda2023b,
  title={Transformer Circuit Faithfulness Metrics Are Not Robust},
  author={Nanda, Neel and others},
  journal={arXiv preprint arXiv:2407.08734},
  year={2023}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@inproceedings{pennington2014,
  title={GloVe: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{scherlis2022polysemanticity,
  title={Polysemanticity and capacity in neural networks},
  author={Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck},
  journal={arXiv preprint arXiv:2210.01892},
  year={2022}
}

@article{skean2024does,
  title={Does representation matter? exploring intermediate layers in large language models},
  author={Skean, Oscar and Arefin, Md Rifat and LeCun, Yann and Shwartz-Ziv, Ravid},
  journal={arXiv preprint arXiv:2412.09563},
  year={2024}
}

@article{tigges2023linear,
  title={Linear representations of sentiment in large language models},
  author={Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  journal={arXiv preprint arXiv:2310.15154},
  year={2023}
}

@article{yin2024entropy,
  title={Entropy law: The story behind data compression and llm performance},
  author={Yin, Mingjia and Wu, Chuhan and Wang, Yufei and Wang, Hao and Guo, Wei and Wang, Yasheng and Liu, Yong and Tang, Ruiming and Lian, Defu and Chen, Enhong},
  journal={arXiv preprint arXiv:2407.06645},
  year={2024}
}

@inproceedings{zhang-etal-2024-investigating,
    title = "Investigating Layer Importance in Large Language Models",
    author = "Zhang, Yang  and
      Dong, Yanfei  and
      Kawaguchi, Kenji",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.29/",
    doi = "10.18653/v1/2024.blackboxnlp-1.29",
    pages = "469--479",
    abstract = "Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in safety-critical scenarios and hindered the development of better models. In this study, we advance the understanding of LLM by investigating the significance of individual layers in LLMs. We propose an efficient sampling method to faithfully evaluate the importance of layers using Shapley values, a widely used explanation framework in feature attribution and data valuation. In addition, we conduct layer ablation experiments to assess the performance degradation resulting from the exclusion of specific layers. Our findings reveal the existence of cornerstone layers, wherein certain early layers can exhibit a dominant contribution over others. Removing one cornerstone layer leads to a drastic collapse of the model performance, often reducing it to random guessing. Conversely, removing non-cornerstone layers results in only marginal performance changes. This study identifies cornerstone layers in LLMs and underscores their critical role for future research."
}

@article{zhong2024clock,
  title={The clock and the pizza: Two stories in mechanistic explanation of neural networks},
  author={Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

