\section{Related work}
\label{sec:related work} 

\textbf{Intermediate layers matter.}
Recent studies underscore the importance of intermediate layers in LLMs, emphasizing their role in producing more informative representations for downstream tasks compared to final layers ____. These layers are crucial for encoding abstract knowledge, enabling advanced capabilities like in-context learning and transfer learning, which are vital for understanding and optimizing LLMs ____. Additionally, intermediate layers exhibit distinct patterns of information compression and abstraction, such as reduced entropy, allowing them to efficiently represent complex inputs ____.

\textbf{Superposition.}
The Superposition Hypothesis suggests that neural networks can encode far more features than neurons they have by compressing high-dimensional concepts into overlapping, nearly orthogonal representations ____. Instead of assigning features to individual neurons, features are represented as sparse linear combinations across neurons, improving encoding efficiency and reducing interference. Toy models demonstrate that sparsity enhances feature disentanglement, balancing compression and accuracy ____. Early layers encode numerous features with sparse combinations, while intermediate layers focus on higher-level contextual features ____.  

\textbf{Linear representation hypothesis.} The linear representation hypothesis suggests that neural networks encode high-level features as linear directions in activation space, enabling easier interpretation and manipulation ____. Probing, introduced by ____, assesses feature encoding in models and builds on findings in word embeddings like GloVe and Word2Vec, which capture semantic relationships through linear structures ____. Empirical support spans various contexts, including spatial and temporal representations ____, sentiment analysis ____, task-specific features ____, and broader relational structures ____.

\textbf{Non-linear Representations.} Although the linear representation hypothesis offers insights into neural network representations, studies have highlighted its limitations and emphasized the significance of non-linear structures. ____ demonstrated that GPT models trained on the game `Othello' required non-linear probes to decode board states. Non-linear structures, such as the `pizza' and `clock' patterns ____, and circular representations observed in tasks like predicting days or months using modular arithmetic prompts ____, reveal the complexity of these representations.