\documentclass[a4paper,11pt]{article}

\input{preamble}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{document}

\title{The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis}

\makeatletter
\def\@fnsymbol#1{\ensuremath{%
  \ifcase#1\or *\or \dagger\or \ddagger\or
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger\or
    \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\title{The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis}

\author{%
  Ge Lei\thanks{g.lei23@imperial.ac.uk} \quad
  Samuel J.\ Cooper\thanks{samuel.cooper@imperial.ac.uk}
}

\affil{{\textit{\footnotesize Dyson School of Design Engineering, Imperial College London, London SW7 2DB}}}

\lhead{\scshape Lei \textit{et al.}}
\chead{\scshape Interwoven Knowledge in LLMs}
\rhead{\scshape Preprint}

\maketitle

%\linenumbers
%\doublespacing
\begin{abstract}
\begin{center}
\begin{minipage}{0.85\textwidth}
{\small This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.
}

\end{minipage}
\end{center}
\end{abstract}
\vspace{.2cm}
\begin{multicols}{2}

\section{Introduction}
\label{Introduction}

Large language models (LLMs) excel in tasks like translation, text generation, and comprehension \cite{naveed2023comprehensive, chang2024survey, kaddour2023challenges}, yet their mechanisms for storing and processing information remain poorly understood. In human cognition, complex networks of interrelated knowledge enable efficient generalization, inference, and creativity by integrating new information into existing frameworks \cite{kumar2021semantic, beaty2023associative}. But how do LLMs represent multi-related attributes and interconnected knowledge? For example, when \textit{knowing} a chemical element like hydrogen, are attributes such as `the lightest element' and `atomic number 1' stored independently, or do they influence each other? When one attribute is activated, does it trigger related concepts through a network of connections? If these attributes interact, how are they organized—linearly, cyclically, or in more complex structures? Clarifying these mechanisms is vital for aligning LLMs with human values \cite{ji2023ai}, enhancing their design, and broadening their applications.

Mechanistic interpretability offers a pathway to answer
these questions, aiming to reverse-engineer neural networks
into human-understandable algorithms \cite{bereska2024mechanistic, singh2024rethinking, dar2022analyzing}. A proposed research direction is the linear representation theory, suggesting that LLMs encode knowledge as one-dimensional lines, with model states formed by sparse combinations of these representations. Evidence supports this for spatial, temporal, and sentiment knowledge \cite{gurnee2023language, tigges2023linear}. However, emerging evidence points to more intricate structures, such as circular encodings for periodic concepts like days or months \cite{engels2024not}, challenging the simplicity of the linear model.

Building on prior work that primarily examined single-attribute representations, we delve deeper into how LLMs encode and recall complex, interwoven knowledge. Our study explores the interaction and independence of linguistic and factual representations, as well as their underlying non-linear geometric relationships. The key findings of our study are:

1. In LLMs, intermediate layers tend to focus on factual knowledge, while later layers shift toward linguistic patterns and task-specific outputs (Sec.\ref{sec:linear probing}).

2. LLMs can recall associative knowledge about related attributes, even when not explicitly mentioned in the prompt, with the strongest recall observed in intermediate layers, diminishing in deeper layers (Sec.\ref{subsec:recalling}).

3. Recall mechanisms exhibit diverse geometric relationships. Notably, we identify 3D spiral structures encoding interrelated attributes for the first time (Sec.\ref{subsec:geometric}).

4. Attribute representations are superimposed in intermediate layers, exhibiting linear relationships. Later layers focus on refining these representations, leading to clearer separation (Sec.\ref{sec: relationship}).

\section{Preliminaries}

Our study only focuses on how reliably acquired knowledge (\textit{i.e.} things we’re confident the model knows) is represented within LLMs, and excludes hallucinations or information not in the training set. We use the properties of chemical elements in the periodic table as a case study due to their frequent occurrence in training data, well-defined attributes, and quantifiable properties, making them an ideal subject for this investigation. We adopt Llama series models in this study.

\subsection{Activation collection}

\paragraph{Generating prompts}  

To study how LLMs represent attributes across layers, we construct a prompt dataset based on a set of attributes (\(A = \{A_j\}_{j=1}^M\), such as `atomic number' or `group') and a set of elements (\(X = \{X_i\}_{i=1}^N\), such as `Mg' or `Al'). For linguistic diversity, we incorporate predefined template sets: \(T^{\text{cont}} = \{T_k^{\text{cont}}\}_{k=1}^{11}\) for continuation-style prompts and \(T^{\text{ques}} = \{T_k^{\text{ques}}\}_{k=1}^{11}\) for question-style prompts, with 11 templates in each.

In the continuation-style templates, the next output token would be the factual knowledge directly such as:  
\[
T_1^{\text{cont}}(A_j, X_i) = \text{`The } A_j \text{ of } X_i \text{ is '}
\]
\[
T_2^{\text{cont}}(A_j, X_i) = \text{`} X_i\text{'s } A_j \text{ is '}
\]

In question-style templates, the next output token is typically a syntactic word like `The', which ensures the grammatical structure is correct, such as:
\[
T_1^{\text{ques}}(A_j, X_i) = \text{`What is the } A_j \text{ of } X_i\text{?'}
\]
\[
T_2^{\text{ques}}(A_j, X_i) = \text{`Which value represents } X_i\text{'s } A_j\text{?'}
\]

By substituting each element and attribute (\(X_i\), \(A_j\)) into these templates, we generate prompts:  
\[
p_{i,j,k} = T_k(X_i, A_j)
\]

Each prompt \(p_{i,j,k}\) will then be fed into LLMs to study the corresponding activations at different layers.  

\paragraph{Collecting last-token activations} 

Last-token activations capture the full prompt context in decoder-only models with masked attention, as they integrate information from all preceding tokens. For each layer \(l\), we collect last-token activations \(\mathbf{h}_{i,j,k}^{(l)}\) from prompts \(p_{i,j,k}\) across all elements and templates:

\[
\mathbf{h}_{i,j,k}^{(l)} = f^{(l)}\bigl(p_{i,j,k}\bigr) \in \mathbb{R}^{T \times d},
\]
where \(f^{(l)}\) denotes the layer-\(l\) transformation, \(T\) is the token length, and \(d\) is the hidden dimension. The initial activation \(\mathbf{h}_{i,j,k}^{(0)}\) is obtained by embedding the prompt through an embedding layer \(E_0\), followed by processing through \(L\) Transformer layers. Each layer applies multi-head attention and a feedforward network with residual connections and layer normalization:  
\[
\mathbf{h}'^{(l)}_{i,j,k} = \text{LayerNorm}\left(\mathbf{h}^{(l-1)}_{i,j,k} + \text{Attention}(\mathbf{Q}, \mathbf{K},\mathbf{V})\right)
\]
\[
\mathbf{h}^{(l)}_{i,j,k} = \text{LayerNorm}\left(\mathbf{h}'^{(l)}_{i,j,k} + \text{FFN}(\mathbf{h}'^{(l)}_{i,j,k})\right).
\]

Here, \( \mathbf{Q} \), \( \mathbf{K} \), and \( \mathbf{V} \) represent the query, key, and value matrices used in multi-head attention to compute token-to-token interactions. Finally, \(\mathbf{h}_{i,j,k}^{(L)}\) is mapped to the vocabulary space using the vocabulary head \(\mathbf{W}_{\text{vocab}}\) to produce logits:  
\[
\text{logits}_{i,j,k} = \mathbf{h}_{i,j,k}^{(L)} \mathbf{W}_{\text{vocab}}
\]  

By analyzing last-token activations \(\mathbf{h}_{i,j,k}^{(l)}\) across layers, we investigate how attributes are represented in the model's hidden states.


\subsection{Activation distribution}
\label{tsne}
We start with a preliminary visualization of the distribution of last-token activations for the `atomic number' attribute. Activations from each transformer layer \( l \) were collected for the atomic number attribute across the first 50 elements using 11 continuation-style templates, forming the set \( H_{\text{atomic number}}^{(l)} \). To enable informative plots to be produced efficiently, PCA was applied to \( H_{\text{atomic number}}^{(l)} \) and then t-SNE was use to project the first 50 principle components into 2D. Fig.\ref{fig:vis} shows the resulting distributions for Meta-Llama-3.1-70B, with points colored by atomic number and other attributes, revealing their associations to atomic number.

The first column of the figure colors activations by true atomic number values (explicitly mentioned in the prompt). In early layers, prompts with similar vocabulary cluster together irrespective of atomic number, reflecting token-level similarity. In the intermediate layer, the activations are hierarchically clustered. The small clusters each contain activations for the 11 prompts generated for each single element. These small clusters then exhibit clustering with respect to the other elements, showing that the model encoding attribute-specific knowledge. By the final layers, prompts with the same meaning remain clustered, but distinctions between them become clearer, reflecting refined language and factual understanding.

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/visualization.png}
    \caption{t-SNE visualization of Meta-Llama-3.1-70B last-token activations from the 1st, 50th, and 80th layers, using 11 continuation-style templates across 50 elements (550 points per plot). Each column shows one layer, while rows represent different colormaps highlighting attributes: `atomic number', `group', `period', and `category'. In the top-left plot, circled clusters correspond to individual templates, each containing 50 points.}
    \label{fig:vis}
\end{figure}

In the next three columns, the activations are colored by the true values of attributes unmentioned in the prompt: `group', `period', and `category'. Despite not being mentioned in the prompt, all attributes cluster in the intermediate layers. By the final layers, the clustering of some attributes, such as group and period, becomes less coherent, indicating a shift in representation. 

Furthermore, the geometric shape of attribute distributions varies. For example, atomic numbers form a linear arrangement transitioning from red to purple (1st row, 3rd column), while the `group' attribute activities form a cyclic pattern with sequential transitions (2nd row, 2nd column), potentially reflecting periodic relationships.

Based on the observed activation distributions, we hypothesize that intermediate layers in LLMs focus on factual knowledge, while later layers refine linguistic patterns and task-specific outputs, with attributes transitioning from being superimposed in intermediate layers to distinct in later layers. Additionally, attributes are encoded in geometric patterns reflecting their properties. These hypotheses are validated in the following sections.

\section{Intermediate layers encode knowledge, later layers shape language}
\label{sec:linear probing}

\begin{figure*}
\centering
    \includegraphics[width=1\linewidth]{figures/linear_probing.png}
    \caption{\( R^2 \) score trends across layers for different models and attributes. Linear probing with 5-fold cross-validation was performed on activations, and \( R^2 \) scores on the test set are shown for each attribute. }
    \label{fig:linear_probing}
\end{figure*}

To understand how complex interrelated knowledge emerges, we first investigate how individual attributes are represented and evolve across layers. As a preliminary step, we analyzed attention maps, revealing that intermediate layers focus more on tokens with a significant impact on the output, while later layers distribute attention more evenly, suggesting a transition from capturing specific relationships to integrating broader context for cohesive outputs. Detailed findings are provided in the appendix (see Fig.\ref{fig:attention}). The following sections quantitatively examine the roles of intermediate and later layers in representing attributes within LLMs.


% \subsection{Main results}


% intermediate layers in large language models have demonstrated a strong capacity to capture informative features. Attention in the intermediate layers focuses on the most important entities, and entropy is at its lowest across all sentence prompts (Figure 3). Linear probing with continuation prompts reveals that the performance of intermediate layers often matches or even surpasses that of the final layer in certain tasks.

% However, intermediate layers lack the ability to organize outputs into coherent language. This limitation is evident in their inability to produce meaningful sentences, suggesting only limited early-exit capabilities that primarily involve the final layers (Figure 2).

% The final layers shift attention toward language patterns, resulting in broader and more evenly distributed attention, with lower entropy. This indicates a focus not just on factual knowledge but also on language structure (Figure). When switching from continuation prompts to question prompts, the performance of linear probing in intermediate layers is minimally affected. In contrast, the final layers show a notable decline in performance. This is because the first output token in question prompts often includes generic language tokens (e.g., "The"), rather than factual knowledge (Figure).





\subsection{Intermediate layers know factual knowledge. Language? Not yet!}
\label{sec:which_layers}

We used linear probing, with 5-fold cross-validation, to train linear Support Vector Regression (SVR) models for each layer, \( l \), and attribute, \( A_j \), based on the activation dataset \( H_j(l) \). The ground truths correspond to attribute values \( y_{i,j} \) for each element \( X_i \). To focus on numerical factual knowledge while minimizing the influence of language patterns, we used continuation-style prompts. Specifically, SVR maps each activation vector \( \mathbf{h}_{i,j}^{(l)} \) from layer \( l \) to the target attribute \( y_{i,j} \) using the following linear function:

\[
\hat{y}_{i,j}^{(l)} = \mathbf{w}_j^{(l)} \cdot \mathbf{h}_{i,j}^{(l)} + b_j^{(l)},
\]

where \( \mathbf{w}_j^{(l)} \in \mathbb{R}^d \) is the weight and \( b_j^{(l)} \in \mathbb{R} \) is the bias.

The \( R^2 \) score trends of the test set for each attribute across layers are shown in Fig.\ref{fig:linear_probing} with the detailed results for the best layer provided in Appendix \ref{app:best_layer}. Both the intermediate and last layers exhibit high \( R^2 \) scores (note that is not 1; see Appendix \ref{app:not_1}). The last layer’s performance is expected, as it generates the final outputs. The intermediate layers’ high \( R^2 \) scores indicate that factual and numerical knowledge is already effectively encoded at this stage, suggesting LLMs represent complex knowledge earlier than expected. These findings also explain the attention map results, where intermediate layers focus on key tokens because they have already encoded conceptual knowledge at this stage.


Intermediate layers can represent factual knowledge, but can they also express this knowledge in human language? We input the prompt `The atomic number of Mg is ' and analyzed the token probabilities at each layer. By normalizing the final token’s hidden state with LayerNorm and applying the vocabulary head followed by softmax, we obtained the top-ranked tokens directly output by each layer \footnote{Applying language heads to the hidden states of intermediate layers, known as early exit ~\cite{teerapittayanon2016branchynet, beaty2023associative, elhoushi2024layer}, has proven effective even without extra training \cite{kao2020bert}.}. In each layer, we extracted the probability of the target token, \( t_{\text{target}} \) — the output token from the last layer, and checked if it ranked within the top 50 most probable tokens for that layer. The results are shown in Fig.\ref{fig:token_prob}.


% \[
% P_{\text{layer}} = \text{softmax}\left(W_{\text{vocab}} \cdot \text{LayerNorm}(h_{\text{layer}})\right)
% \]

In the early layers, the probability of the target token has not shown an upward trend, indicating these layers neither strongly predict the target tokens nor significantly refine their probabilities. In contrast, probabilities gradually increase in the later layers, highlighting their role in refining and finalizing predictions. Although crucially, there don’t appear to be any hard boundaries between these distinct activities and the model smoothly transition from one to the next. The markers, concentrated in later layers, suggest that while intermediate layers store factual knowledge, they are not yet attempting to articulate it in language form.

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/token_prob.png}
    \caption{Probability of the target token \( t_{\text{target}} \) across layers in  Meta-Llama-3-8B for the prompt `The atomic number of Mg is ' Each line shows \( t_{\text{target}} \)'s probability at each layer. The probabilities were calculated by iteratively re-running the model with the next token added to the prompt. Markers indicate the layers where it ranks in the top 50 most probable tokens.}
    \label{fig:token_prob}
\end{figure}

Notably, the distribution of `Top 50' markers varies by token type. Tokens with lower contextual complexity, such as spaces, `and,' or `since,' have their markers in earlier layers. In contrast, knowledge-based tokens, like `12,' require deeper processing and appear in much later layers. This suggests that while intermediate layers encode factual concepts, they are likely focused on tasks other than linguistic articulation, which primarily develops in the later layers.


\subsection{Later layers shift focus from factual knowledge to language patterns}
\label{language}
The previous experiment shows that later layers begin to transform factual knowledge into language patterns. To further validate this, we compared question-style and continuation-style prompts using linear probing, following the procedure in Sec.\ref{sec:which_layers}. The attribute `group' was used as an example, with results shown in Fig.\ref{fig:question_vs}.

\begin{figure}[H] 
\centering
    \includegraphics[width=1\linewidth]{figures/question_vs.png}
    \caption{\( R^2 \) Scores for `Group' Attribute: Linear probing comparison of continuation and question prompts across layers.}
    \label{fig:question_vs}
\end{figure}

The results show a significant drop in \( R^2 \) scores for question prompts compared to continuation prompts in the later layers, with the gap widening in deeper layers. While both prompt types reflect a shift toward language structure in the later layers, continuation prompts are less affected because their output remains tied to factual tokens. In contrast, question prompts emphasize sentence structure and may generate filler words like `The,' leading to the intentional discarding of factual representation. This effect is more pronounced in smaller models, while larger models retain more information about factual knowledge while processing language.





\section{Recall peaks at intermediate layers}
\label{sec:recalling} 
% Attribute Interaction and Relationships, Inter-Attribute Dynamics and Relationships
The previous section focused on the behavior of single attributes across layers. Here, we investigate whether related attributes are interconnected by examining the recall ability of LLMs—their capacity to retrieve attributes related to, but not explicitly mentioned in the prompt. Additionally, we analyze the geometric mechanisms underlying this recall process.

\subsection{Recalling knowledge ability peaks at intermediate layers and declines afterward}
\label{subsec:recalling}

We conducted an experiment to explore the relationship between different attributes using misaligned linear probing. Specifically, we generated activation datasets from prompts about \( H_{\text{atomic number}}^{(l)} \) and \( H_{\text{group}}^{(l)} \). Separate probes were trained on each activation dataset, but in both cases, only \( y_{\text{group}} \) (group labels) were used as the target during training. This approach examines whether activations related to one attribute (\textit{e.g.}, atomic number) encode information about another attribute (\textit{e.g.}, group). Continuation prompts were used, and the probing process followed the method described in Sec.\ref{sec:which_layers}. The results are shown in Fig.\ref{fig:misaligned_probing}.


The results showed that in the early and intermediate layers, \( R^2 \) scores for recalling group information were consistent, regardless of whether activations came from prompts about atomic number or group. However, in the later layers, activations from atomic number prompts showed a significant drop in \( R^2 \) scores for group recall compared to those from group prompts. This suggests that intermediate layers enable LLMs to recall related attribute knowledge, indicating a capacity for broader knowledge representation. The significant drop in the later layers suggests that knowledge not immediately necessary for generating the next token is not activated. The relationships between different attribute representations are further explored in Sec.\ref{sec: relationship}.
% TODO: the label is too small

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/wrong_prompt.png}
    \caption{\( R^2 \) Score Trends for Misaligned Linear Probing. The probe was trained on `group' values using activations from prompts about `group' (matching) and `atomic number' (non-matching).}
    \label{fig:misaligned_probing}
\end{figure}

\subsection{Attribute geometric interrelationship}
\label{subsec:geometric}

\begin{figure*}
\centering
    \includegraphics[width=1\linewidth]{figures/intervention.png}
    \caption{ Activation Patching Results for Layer 20 in Meta-Llama-3.1-70B. The model’s predictions are evaluated after replacing the activation of the `element' token at the last token position with the predicted activation \(\hat{\mathbf{h}}^{\text{pred}, (20)}_{0}\). }
    \label{fig:intervention}
\end{figure*}


LLMs can recall knowledge in the early to intermediate layers, but how do these attributes interact? We hypothesize that attributes in LLMs exist in a high-dimensional space, manifesting as linear, circular, or spiral patterns based on their structure, and then proceed to validate these geometries.

Inspired by \cite{engels2024not}, we map the last-token activations \(\mathbf{h}^{(l)} \in \mathbb{R}^k\) at layer \(l\) to a geometric space \(f(r, g, p)\), which represents a geometric structure defined by atomic number \(r\), group \(g\), and period \(p\). To achieve this, we first reduce the dimensionality of the activations using PCA, denoted as \(\mathbf{P}(\mathbf{h}^{(l)})\), followed by layer-wise linear regression:

\[
\mathbf{W}^{(l)}, \mathbf{b}^{(l)} = \arg \min_{\mathbf{W}', \mathbf{b}'} \sum_{i} \left\| \mathbf{W}' \mathbf{P}(\mathbf{h}^{(l)}_{i}) + \mathbf{b}' - f_i \right\|_2^2
\]

Here, \(\mathbf{W}^{(l)} \in \mathbb{R}^{d' \times k}\) and \(\mathbf{b}^{(l)} \in \mathbb{R}^{d'}\) are the learned weight and bias for layer \(l\). \(f_i = f(r_i, g_i, p_i)\) denotes the mapping of the \(i\)-th element in the geometric space.

To perform an intervention, we calculate the centroid of the PCA-reduced activations for layer \(l\), denoted as \(\mathbf{\bar{h}}^{(l)} = \frac{1}{N} \sum_{i=1}^N \mathbf{P}(\mathbf{h}^{(l)}_{i})\), and map it to the geometric space using \(\mathbf{W}^{(l)} \mathbf{\bar{h}}^{(l)} + \mathbf{b}^{(l)}\). The mapping of the target element is \(f_0 = f(r_0, g_0, p_0)\). The deviation from this mapping is then mapped back to the activation space using the pseudo-inverse of the learned weight, \((\mathbf{W}^{(l)})^+\), yielding the predicted activation of the target element:

\[
\hat{\mathbf{h}}^{\text{pred}, (l)}_{0} = \mathbf{P}^{-1} \left( \mathbf{\bar{h}}^{(l)} + (\mathbf{W}^{(l)})^+ \left( f_0 - (\mathbf{W}^{(l)} \mathbf{\bar{h}}^{(l)} + \mathbf{b}^{(l)}) \right) \right)
\]



Importantly, the model never accesses the original activation of the target element; the predicted activation is solely based on the mapping of target element in the geometric space and the activations of other elements. During inference, we replace the activation of the `element' token (last token position) in the 20th layer\footnote{Details of intervention performance are provided in Appendix \ref{app:layer}. From layer 20, the interventions show effectiveness.} with \(\hat{\mathbf{h}}^{\text{pred}, (20)}_{0}\), using the prompt `In the periodic table, the atomic number of element' The results are evaluated to verify if the model predicts the target properties without using the original activation. 

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/spiral.png}
    \caption{Predicted Atomic Numbers After Intervention on Difference Geometric Space. Left: \((\cos \theta, \sin \theta, r)\). Right: \((r \cos \theta, r \sin \theta, r)\). The colored points represent the tokens with the highest logits output by the model after intervention. 
}
    \label{fig:spiral}
\end{figure}


We evaluate the effectiveness of different geometric spaces for interventions, including linear, 2D spiral, and 3D spiral geometries. Angular variables \(\theta = \frac{2\pi g}{18}\) are used to capture periodic relationships. To test the impact of disrupted geometry, two random spaces are introduced: in Space 8, atomic numbers \(r\) are shuffled; in Space 9, \(\theta\) is randomly permuted. Additionally, in Space 10, the prompt `In numbers, the Arabic numeral for number' generates numbers 1–50, testing whether periodic patterns emerge without explicit element references. Results are shown in Fig.\ref{fig:intervention}, with detailed values in Table \ref{tab:lowdim_interventions} in appendix.

Effective activation patching suggests that the target space \(f(r, g, p)\): 1) retains sufficient information for accurate reconstruction during transformations with the activation space, and 2) preserves geometric structures similar to those in the activation space to ensure valid adjustments in the high-dimensional space.

% TODO: in the figure, atomic number ->r_pred

Results show that intervention can be applied in various geometric spaces, with some performing significantly better. Spaces such as \((\cos \theta, \sin \theta, r)\) and \((r \cos \theta, r \sin \theta, r)\) over \(70\%\) predictions of the atomic number have an absolute error within 2, suggesting the potential existence of latent 3D structures in LLMs resembling spirals or radial spirals. Fig.\ref{fig:spiral} shows the output of the LLM after intervention on the two spiral geometric space, highlighting the periodic table's cyclic properties. Additional geometric shapes are analyzed in Appendix \ref{app:other_shape}. In contrast, random spaces and unrelated prompts exhibit poor performance, increasing our confidence that these results suggest the geometry of embedding spaces can relate to the real geometry of knowledge.

% One important point to address when predicting numeric tokens is whether a larger numerical difference between the output token and the true value implies a greater error, while a smaller difference suggests the model is closer to the correct answer. We extract token IDs for numbers 1-50 from the tokenizer, multiply them by the pseudoinverse of the vocabulary head \(W_vocab^+\), and obtain their vector representations in the hidden space of the last layer. Fig.\ref{fig:distance} illustrates the Euclidean distances between these representations. Tokens with smaller numerical differences (e.g., 1 and 2) exhibit closer distances, while larger differences (e.g., 1 and 50) result in inconsistent distances, indicating the model's difficulty in maintaining numerical consistency for larger gaps. In our intervention experiments, when the output token's numerical value is close to the true value, the hidden logits better align with the true logits, indicating higher accuracy. Conversely, when the output token's numerical value deviates significantly from the true value, the hidden logits move farther from the true representation, though this distance alone cannot quantify the exact prediction error.

\begin{figure}[H]
\centering
    \includegraphics[width=0.8\linewidth]{figures/heatmap.png}
    \caption{Euclidean distance heatmap of approximated vector representations for numeric tokens (1-50) in the hidden space of the last layer.}
    \label{fig:distance}
\end{figure}

In the intervention experiments, it is actually not obvious whether a smaller numerical difference between the output token and the true value always implies smaller error. To investigate this, we project token IDs for numbers 1–50 into the last hidden layer using the pseudoinverse of the vocabulary projection matrix \( \mathbf{W}_{\text{vocab}}^+ \). This operation reconstructs an approximation of the hidden representations that would produce these token IDs as logits. Fig.\ref{fig:distance} shows that smaller numerical differences generally correspond to closer representations, while larger differences often result in inconsistent distances, reflecting the model's difficulty with numerical consistency over larger gaps. For instance, the vector for `1' is closer to `2' than to `5', while the distances between `10' and `40' is closer than between `10' and `21'. In the intervention, when the predicted value is close to the true value, hidden logits align well with true logits, suggesting higher accuracy. However, large numerical deviations cannot fully capture prediction errors, so we evaluate results using an absolute error threshold (\(\leq 2\)) in Fig.\ref{fig:intervention}, representing a small distance.

% To better visualize the differences, the color range was adjusted with the lower limit set to 0.05 and the upper limit set to 0.18.


\section{Relationship in attributes representation: from superposition to separation}
\label{sec: relationship}

The last section explores whether one attribute's representation can recall related attributes without being mentioned, while this section explores the relationships between attribute representations across layers.

\subsection{Attribute representations overlap in intermediate layers but become distinct later}

As outlined in Sec.\ref{sec:which_layers}, we trained a linear model for each attribute \( A_j \) at each layer \( l \), yielding a weight vector \( \mathbf{w}_j^{(l)} \) that represents how attribute \( A_j \) is stored in the activation space of layer \( l \). To analyze attribute relationships across layers, we computed the cosine similarity between weight vectors of different attributes using continuation-style activation sets to minimize language pattern influence.

Fig.\ref{fig:similarity} illustrates the cosine similarity across 80 layers of Meta-Llama-3.1-70B. Notably, in high-dimensional spaces, random vector pairs typically approach orthogonality due to the `blessing of dimensionality'. To illustrate this, we randomly sampled vector pairs in an 8129-dimensional space (the activation vector size of Meta-Llama-3.1-70B) and calculated their cosine similarity, with the 99.9\% confidence interval (CI) shown in gray. Cosine similarity outside this interval indicates meaningful relationships between attributes. See Appendix \ref{app:blessing} for more details.

In the early layers, high similarity reflects token-level processing rather than semantic understanding. As layers deepen, similarity decreases as the model begins capturing semantics. In the intermediate layers, similarity rises, indicating shared representation of correlated attributes. Finally, in the later layers, similarity drops again as the model separates features for refined decision-making.

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/cosine_similarity.png}
    \caption{Cosine similarity between weight vectors of linear probes for attribute pairs across layers in Meta-Llama-3.1-70B. The shaded area (99.9\% CI) represents unrelated boundaries.}
    \label{fig:similarity}
\end{figure}


% For any two attributes \( m \) and \( n \), the cosine distance between their weight vectors \( \mathbf{w}_m^{(l)} \) and \( \mathbf{w}_n^{(l)} \) at layer \( l \) is given by:

% \[
% \text{cos\_distance}(\mathbf{w}_m^{(l)}, \mathbf{w}_n^{(l)}) = 1 - \frac{\mathbf{w}_m^{(l)} \cdot \mathbf{w}_n^{(l)}}{\|\mathbf{w}_m^{(l)}\| \|\mathbf{w}_n^{(l)}\|}.
% \]

% where \( \mathbf{w}_m^{(l)} \cdot \mathbf{w}_n^{(l)} \) represents the dot product of the weight vectors for attributes \( m \) and \( n \) at layer \( l \).
% \( \|\mathbf{w}_m^{(l)}\| \) and \( \|\mathbf{w}_n^{(l)}\| \) are the Euclidean norms of \( \mathbf{w}_m^{(l)} \) and \( \mathbf{w}_n^{(l)} \), respectively.

% A cosine distance close to 0 indicates that the representations of attributes \( m \) and \( n \) are closely aligned, suggesting overlapping or shared storage within the model. A distance near 1 implies that these attributes are stored in distinct regions of the activation space, showing minimal interaction between them. A distance closer to 2 suggests that the attributes are related but in opposing directions, indicating a negative or contrasting relationship in the model’s representation space. 

% \[
% \text{CI}_{99.9\%} = \left( \mu - z \frac{\sigma}{\sqrt{n}}, \mu + z \frac{\sigma}{\sqrt{n}} \right)
% \]

% where \( \mu \) is the sample mean, \( \sigma \) is the sample standard deviation, \( n \) is the number of pairs, and \( z \approx 3.29 \) for 99.9\% confidence. In Fig.\ref{fig:distance}, the shaded area represents the 99.9\% confidence interval for random cosine distances between high-dimensional vectors, centered around a cosine distance of 1.0. 


\subsection{Attribute representations exhibit linear relationships}

To directly capture relationships between attributes, we map the representation of \( A_{j1} \) to \( A_{j2} \) at each layer by training a linear model on the last activation of a fixed prompt template (with PCA reducing the activation dimension to 20), \textit{e.g.}, `In the periodic table, the \( A_j \) of \( X_i \) is .' Mapping performance is evaluated by \( R^2 \) scores of 5-fold cross-validation.

Fig.\ref{fig:relationship} shows \( R^2 \) scores across layers for attribute pairs. High scores in early layers reflect token-level differences due to shared prompt template. As layers deepen, \( R^2 \) decreases, indicating a shift to semantic representations. In the intermediate layers, \( R^2 \) rises, revealing attribute overlap, exhibiting linear relationships. In the later layers, \( R^2 \) drops as the model separates attributes for task-specific outputs. 

\begin{figure}[H]
\centering
    \includegraphics[width=1\linewidth]{figures/relationship.png}
    \caption{R² scores across layers for mapping attribute pairs in Meta-Llama-3.1-70B using a linear model trained on the last activation of a fixed prompt template. }
    \label{fig:relationship}
\end{figure}

Both experiments highlight the transition from superposition in the intermediate layers to separation in the later layers. Furthermore, the second experiment demonstrates that even simple linear models are able to capture the relationships between different attributes. The observation also explains why the recall ability of the intermediate layers is stronger, as discussed in Sec.\ref{sec:recalling}.




\section{Related work}
\label{sec:related work} 

\textbf{Intermediate layers matter.}
Recent studies underscore the importance of intermediate layers in LLMs, emphasizing their role in producing more informative representations for downstream tasks compared to final layers \cite{kavehzadeh2024sorted, ju2024large, liu2024fantastic, skean2024does}. These layers are crucial for encoding abstract knowledge, enabling advanced capabilities like in-context learning and transfer learning, which are vital for understanding and optimizing LLMs \cite{zhang-etal-2024-investigating}. Additionally, intermediate layers exhibit distinct patterns of information compression and abstraction, such as reduced entropy, allowing them to efficiently represent complex inputs \cite{doimo2024representation, yin2024entropy}.

\textbf{Superposition.}
The Superposition Hypothesis suggests that neural networks can encode far more features than neurons they have by compressing high-dimensional concepts into overlapping, nearly orthogonal representations \cite{arora2018linear, scherlis2022polysemanticity, jermyn2022engineering}. Instead of assigning features to individual neurons, features are represented as sparse linear combinations across neurons, improving encoding efficiency and reducing interference. Toy models demonstrate that sparsity enhances feature disentanglement, balancing compression and accuracy \cite{elhage2022toy}. Early layers encode numerous features with sparse combinations, while intermediate layers focus on higher-level contextual features \cite{gurnee2023finding}.  

\textbf{Linear representation hypothesis.} The linear representation hypothesis suggests that neural networks encode high-level features as linear directions in activation space, enabling easier interpretation and manipulation \cite{nanda2023b}. Probing, introduced by \cite{alain2016understanding}, assesses feature encoding in models and builds on findings in word embeddings like GloVe and Word2Vec, which capture semantic relationships through linear structures \cite{mikolov2013, pennington2014}. Empirical support spans various contexts, including spatial and temporal representations \cite{gurnee2023language}, sentiment analysis \cite{tigges2023linear}, task-specific features \cite{hendel2023context}, and broader relational structures \cite{hernandez2023linearity}.

\textbf{Non-linear Representations.} Although the linear representation hypothesis offers insights into neural network representations, studies have highlighted its limitations and emphasized the significance of non-linear structures. \cite{li2022emergent} demonstrated that GPT models trained on the game `Othello' required non-linear probes to decode board states. Non-linear structures, such as the `pizza' and `clock' patterns \cite{nanda2023progress, zhong2024clock}, and circular representations observed in tasks like predicting days or months using modular arithmetic prompts \cite{engels2024not}, reveal the complexity of these representations.


\section{Discussion and conclusions}
Building on prior research into how LLMs represent individual entities, our study systematically investigates how LLMs encode and recall interwoven structured knowledge across transformer layers. Intermediate layers emerge as pivotal for encoding factual knowledge and maintaining superimposed representations of related attributes, enabling effective recall of associated features even when not explicitly prompted. In contrast, later layers refine these representations into more distinct and task-specific outputs, prioritizing linguistic coherence. In addition, we uncovered geometric patterns, such as 3D spirals, reflecting relationships like the periodicity in chemical elements. This suggests LLMs encode both linear and non-linear representations aligned with the real geometry of knowledge. The findings provide insights that could inform the development of more interpretable and efficient models, with potential applications in scientific discovery and trustworthy AI systems.

\textbf{Limitations.}
Our prompts are focused on chemical elements, while ideal for their structured attributes, may not extend to domains with more abstract features. The hypothesis-driven validation of geometric structures may oversimplify LLMs' non-linear interactions.

% \section*{Acknowledgement}


% \section*{Software and Data}
% The code needed to reproduce the results of the paper/use the tool is a available at \url{https://github.com/tldr-group/project-repo} with an MIT license agreement.

\section*{Impact Statement}

% This paper advances the understanding of LLMs by exploring how multi-associated attributes are stored and recalled across layers. The findings, including the first discovery of 3D geometric patterns in attribute representations, contribute to the field of machine learning by enhancing interpretability and providing insights into the layered dynamics of knowledge encoding. These contributions have the potential to inform the development of more robust and transparent models.

% This work raises no specific ethical concerns beyond those well established in the field of machine learning. However, understanding how LLMs process complex knowledge can help mitigate biases and improve fairness, ensuring their responsible and equitable use in real-world applications.

We believe interpretability in LLMs is essential for AI safety, reducing unintended behaviors and building trust. Understanding how knowledge is stored and recalled across layers can inspire more interpretable, efficient models, advance knowledge editing and scientific discovery.

\section*{Code Availability}
The code required to reproduce the results presented in this paper is available at \url{https://github.com/tldr-group/LLM-knowledge-representation} with an MIT license agreement. 

\section*{Acknowledgements}
This work was supported by the Imperial Lee Family Scholarship awarded to G.L. We would like to thank the members of the TLDR group for their valuable comments and insightful discussions.


\section*{References}
\addcontentsline{toc}{section}{References}

\def\addvspace#1{}

	\renewcommand{\refname}{ \vspace{-\baselineskip}\vspace{-1.1mm} }
	\bibliographystyle{abbrv}
    \bibliography{main}


\end{multicols}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\onecolumn

\appendix
\section*{\LARGE Appendix}
\setcounter{section}{0}


\section{Attention map detailed results}

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

% \subsection{Entity-Focused Attention}
\label{attention}

To investigate how the model prioritizes different parts of the input text, we conducted a preliminary analysis using the 32-layer Meta-Llama-3-8B model. We adopted the attribute \( A_j \), Period and Group, and iterated over \( X_i \), consisting of 50 elements, using the prompt template: `In the periodic table of elements, the \( A_j \) of \( X_i \) is.' These prompts were input into the language model, and we analyzed the average attention across all attention heads in each transformer layer from the token `is' to all other tokens. The averaged results across different prompts are presented in Fig.\ref{fig:attention}.

The results indicate that in the intermediate layers, where entropy is relatively high, there is a noticeable concentration of attention from the token `is' to attribute and element tokens. This suggests that these intermediate layers focus more on tokens within the sequence that have a significant impact on the output. In contrast, the later layers, which exhibit lower entropy (with the exception of the final layer), show a more evenly distributed attention pattern. This pattern implies that the model transitions from focusing on specific token relationships to integrating broader context, thereby finalizing its interpretation for a cohesive output.

\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/attention.png}
     \caption{Average attention distribution analysis of the 32-layer Meta-Llama-3-8B model across transformer layers, based on prompts, `In the periodic table of elements, the  \( A_j \)  of \( X_i \) is,' where \( A_j \) is an attribute (period or group) and \( X_i \) is an element. The heatmap (left) shows average attention from `is' to all tokens, while line plots (right) depict attention to target tokens (e.g., element and attribute), average attention to other tokens, and attention entropy. Intermediate layers focus on significant token relationships with higher entropy, while later layers (excluding the final layer) show evenly distributed attention and lower entropy, reflecting a shift to broader context integration.}

    \label{fig:attention}
\end{figure*}


\section{Intervention outcomes in geometric recall}
\label{recall}



\renewcommand{\thefigure}{B\arabic{figure}}
\renewcommand{\thetable}{B\arabic{table}}
\setcounter{figure}{0}

\subsection{Layer-wise performance evaluation}
\label{app:layer}

Fig.\ref{fig:inter_error} illustrates the prediction error across layers when the activation of the last token across layers is replaced with the predicted activation derived from the geometric space \(f(r, g, p) = (r\cos\theta, r\sin\theta, r)\). In the early layers, errors gradually decrease because the model has not yet captured semantic information, and the geometric space is still being constructed. The continuous decline in error reflects the model’s growing ability to capture semantic information and progressively build a coherent geometric representation. By layer 20, the error stabilizes, indicating that these layers effectively encode the periodic and geometric relationships between atomic properties such as atomic number, group, and period.

However, beyond layer 30, the error increases sharply as the model begins outputting non-numeric tokens (replaced with an error value of 50 in the plot, corresponding to the maximum possible error given the 50 elements and atomic numbers). This can be attributed to two factors. First, if the numeric token is not the first output, generating the correct answer requires activations across all token positions. But only the last activation was replaced. Intervening too late disrupts the established flow of activations at other positions, which have already determined the output content. Second, it is also likely due to the model shifting its focus from geometric relationships to higher-level abstractions or context-dependent reasoning in the later layers. Therefore, for the intervention experiments on geometric relationships, we selected layer 20 as it balances effective encoding of geometric relationships and minimizes disruption to the model's output process.

\begin{figure*}[h]
\centering
    \includegraphics[width=0.9\linewidth]{Appendix_figures/inter_error.png}
     \caption{Variation of Absolute Prediction Errors Across Layers with Intervention. The plot shows the mean absolute error (MAE) for each layer, along with the minimum and maximum error range represented by the shaded region. Missing data points were replaced with a value of 50 before computing the absolute errors. }

    \label{fig:inter_error}
\end{figure*}

\subsection{Detailed evaluation of geometric spaces}
\label{app:other_shape}

The primary evaluation criterion used in the main text is the absolute error threshold (\(\leq 2\)), as discussed in detail in Sec.\ref{subsec:geometric}. This metric was chosen because it better captures the accuracy of activation interventions. However, other metrics, such as \(R^2\), Pearson correlation, and qualitative mapping fidelity, also provide valuable insights. These complementary results are summarized in Table \ref{tab:lowdim_interventions}. 


\begin{table*}[h!]
\centering
\footnotesize
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}c p{3cm} cccc c@{}}
\toprule
\textbf{\#} & \textbf{Space} & \textbf{Description} & \shortstack{\textbf{R\textsuperscript{2}}} & \shortstack{\textbf{Pearson} \\ \textbf{Correlation}} & \shortstack{\textbf{Percentage of} \\ \textbf{Abs. err \(\leq 2\) }} & \textbf{Mapping Fidelity} \\ 
\midrule
1 & \(r\) & Linear structure along atomic number. & 0.8863 & 0.9591 & 38.00\% & Moderate \\ 
2 & \((r, g, p)\) & 3D cartesian grid. & 0.8060 & 0.9191 & 48.00\% & Moderate \\
3 & \((r \cos \theta, r \sin \theta, r)\) & 3D radial spiral structure. & 0.8162 & 0.9035 & 72.00\% & High\\
4 & \((\cos \theta, \sin \theta, r)\) & 3D spiral structure. & 0.7596 & 0.8813 & 70.00\% & High \\
5 & \((\cos \theta, \sin \theta, p)\) & 3D periodic wave-like structure. & 0.5106 & 0.7174 & 60.00\% & Moderate \\
6 & \((r \cos \theta, r \sin \theta, p)\) & 3D periodic lattice with radial dependencies. & 0.6719 & 0.8240 & 62.00\% & Moderate \\
7 & \((r \cos \theta, r \sin \theta)\) & 2D radial structure. & -0.1391 & 0.1481 & 40.00\% & Low \\
8 & \(r_\text{random}\) & Random linear structure. & 0.0075 & 0.1503 & 10.00\% & Low \\
9 & \((\cos(\theta_\text{random}), \sin(\theta_\text{random}), r)\) & Randomized spiral. & 0.6358 & 0.8465 & 20.00\% & Low \\
10 & \((r \cos \theta, r \sin \theta, r)\) & Element unrelated prompts & -0.4910 & 0.7215 & 48.00\% & Low \\
\bottomrule
\end{tabular}}
\caption{Performance of different low-dimensional spaces for activation intervention. Each space represents a unique pattern, with results assessed using \(R^2\), Pearson correlation, and percentage of predictions within absolute error \(\leq 2\).}
\label{tab:lowdim_interventions}
\end{table*}

In the main paper, we demonstrate two geometric space intervention results; however, other shapes can also be extracted. Fig.\ref{fig:inter_linear} shows the extracted linear structure from interventions. While the alignment of points along a straight path indicates the presence of a linear structure, the overlapping points suggest its limitations in distinguishing atomic number. Compared to more expressive shapes like spirals, linear structures may struggle to effectively capture periodic or distinct features.

\begin{figure*}[h]
\centering
    \includegraphics[width=0.5\linewidth]{Appendix_figures/intervention_linear.png}
     \caption{Linear Structure in the Geometric Space from Intervention Experiments. The figure shows predictions (colored points) and their alignment with the ground truth (gray line). While the linear structure is evident, the overlap of points suggests limitations in capturing distinct element properties.}

    \label{fig:inter_linear}
\end{figure*}

\section{Blessing of dimensionality}
\label{app:blessing}

\renewcommand{\thefigure}{C\arabic{figure}}
\setcounter{figure}{0}

When the dimensionality is very high, the most of random vector pairs approach orthogonality. We illustrate this by sampling pairs of vectors in an 8129-dimensional space (corresponding to the activation vector dimension of Meta-Llama-3.1-70B) and computing their cosine similarities. The 99.9\% confidence interval (CI) provides an estimate of the expected cosine similarity range at each dimensionality:  

\[
\text{CI}_{99.9\%} = \left( \mu - z \frac{\sigma}{\sqrt{n}}, \mu + z \frac{\sigma}{\sqrt{n}} \right)
\]

where \( \mu \) is the sample mean, \( \sigma \) is the sample standard deviation, \( n \) is the number of sampled pairs, and \( z \approx 3.29 \) for a 99.9\% confidence level.  

\begin{figure*}[h]
\centering
    \includegraphics[width=0.4\linewidth]{Appendix_figures/CI.png}
     \caption{Cosine similarity distribution of random vector pairs in an 8129-dimensional space, with a 99.9\% confidence interval ($-0.0364$, $0.0364$) shown by the dashed lines.}


    \label{fig:CI}
\end{figure*}

The specific distribution is shown in Fig.\ref{fig:CI}, where the confidence interval is extremely narrow (\(\pm 0.036\)), indicating that random vector pairs exhibit highly consistent cosine similarities. This suggests that the learned weights of the linear probe across different feature pairs are effectively uncorrelated, exhibiting only random alignment. In Fig.\ref{fig:distance}, the shaded region represents the 99.9\% confidence interval for the cosine similarities of high-dimensional random vectors, further supporting this observation.  




\section{Linear probing detailed results}
\label{app:linear_probing}

In Sec.\ref{sec:which_layers}, we applied linear probing to train linear-kernel Support Vector Regression (SVR) models for each layer \( l \) and attribute \( A_j \), using the activation dataset \( H_j(l) \). The ground truth values \( y_{i,j} \) correspond to the attribute values of each element \( X_i \).  

\renewcommand{\thefigure}{D\arabic{figure}}
\setcounter{figure}{0}

\subsection{Why \( R^2 \) cannot reach 1}
\label{app:not_1}
Even with a perfectly trained model and a sufficiently large dataset, achieving an \( R^2 \) of 1 in linear probing is impossible. The model’s output token `1' indicates that the activation at the last token position in the final layer leads to the highest logit for token 1’s ID after the final linear transformation. However, do token activation in the final layer of numbers exhibit a perfect linear relationship with their real numerical values? Token embeddings are learned representations that capture semantic relationships between tokens, but they are not guaranteed to align linearly with numerical values.  

To further investigate this, we fit a linear model to map approximated numerical token representations in the last layer to their actual values. Specifically, we extract token IDs for numbers 1–50 from the tokenizer, multiply them by the pseudoinverse of the vocabulary projection matrix \( \mathbf{W}_{\text{vocab}}^+ \), and obtain their corresponding vector representations in the hidden space of the last layer:

\[
\mathbf{h}_i = \mathbf{W}_{\text{vocab}}^+ \cdot \mathbf{t}_i
\]

where \( \mathbf{t}_i \) is the encoded token ID for the number \( i \), and \( \mathbf{h}_i \) represents its corresponding hidden space representation.

We then fit a linear regression model to map these representations to their true numerical values. The linear correlation turned out to be quite strong, with an \( R^2 \) of 0.98. However, this is not 1—possibly because the embedding space is not perfectly linearly aligned with numerical values, or because it is influenced by semantic noise, or simply due to limitations in the fitting method.  

For LLMs, even if the logits were identical to the embeddings \( \mathbf{h}_i \) (which is theoretically impossible—at best, they can only approximate them), the \( R^2 \) would still be limited to 0.98. Therefore, it is unsurprising that linear probing does not achieve an \( R^2 \) of 1.  


\subsection{Detailed results of the best layer}
\label{app:best_layer}
In the main text Sec.\ref{sec:linear probing}, we used SVR for linear probing. Figures~\ref{fig:mass}, \ref{fig:number}, \ref{fig:group}, \ref{fig:period}, and \ref{fig:elec} present the detailed \(R^2\) performance of the best layer for each attribute—atomic number, atomic mass, electronegativity, period, and group—across three different models.


\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/atomic_mass_predictions_combine.png}
    \caption{Evaluation of SVR performance for Layer \( \text{best\_layer} \) on the atomic mass. The left plot shows true vs. predicted values with alignment to the diagonal indicating accuracy. The center plot displays residuals, highlighting error distribution centered around zero. The right plot visualizes true and predicted values across samples, with shaded areas representing error magnitudes.}
    \label{fig:mass}
\end{figure*}

\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/atomic_number_predictions_combine.png}
    \caption{Evaluation of SVR performance for Layer \( \text{best\_layer} \) on the atomic number. The left plot shows true vs. predicted values with alignment to the diagonal indicating accuracy. The center plot displays residuals, highlighting error distribution centered around zero. The right plot visualizes true and predicted values across samples, with shaded areas representing error magnitudes.}
    \label{fig:number}
\end{figure*}

\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/group_predictions_combine.png}
    \caption{Evaluation of SVR performance for Layer \( \text{best\_layer} \) on the group. The left plot shows true vs. predicted values with alignment to the diagonal indicating accuracy. The center plot displays residuals, highlighting error distribution centered around zero. The right plot visualizes true and predicted values across samples, with shaded areas representing error magnitudes.}
    \label{fig:group}
\end{figure*}

\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/period_predictions_combine.png}
    \caption{Evaluation of SVR performance for Layer \( \text{best\_layer} \) on the period. The left plot shows true vs. predicted values with alignment to the diagonal indicating accuracy. The center plot displays residuals, highlighting error distribution centered around zero. The right plot visualizes true and predicted values across samples, with shaded areas representing error magnitudes.}
    \label{fig:period}
\end{figure*}

\begin{figure*}[h]
\centering
    \includegraphics[width=1\linewidth]{Appendix_figures/electronegativity_predictions_combine.png}
    \caption{Evaluation of SVR performance for Layer \( \text{best\_layer} \) on the electronegativity. The left plot shows true vs. predicted values with alignment to the diagonal indicating accuracy. The center plot displays residuals, highlighting error distribution centered around zero. The right plot visualizes true and predicted values across samples, with shaded areas representing error magnitudes.}
    \label{fig:elec}
\end{figure*}



%\include{Sup_Info}
	
\end{document}
