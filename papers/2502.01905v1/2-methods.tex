\section{The opinion dynamics model}
\label{Model}
%\subsection{Voting dynamics in signed networks}
We consider a population of $N$ individuals interacting with each other in a social network through congenial (positive) or hostile (negative) relationships. We represent the structure of the network using a signed graph $G(V,E,W)$, where vertices $V = \{1,2,\ldots,N\}$ denote individuals in the population connected through a set of edges $E$ that depict social connections. Here $W \in R^{N \times N}$ denotes the corresponding signed weighted adjacency matrix, where any given element $w_{ij}$ illustrates the strength of influence an individual $i$ has on $j$.
The weight of an edge $w_{ij}$ determines the type of influence, positive or negative, that flows from $i \longrightarrow j$. A negative weight $w_{ij} < 0$ symbolises a negative edge and therefore implies negative influence from $i$ on $j$, whereas $w_{ij} > 0$ suggests node $j$ experiences positive influence from $i$. We consider directed networks, where edge weights $w_{ij} $ and $ w_{ji}$  are independent of each other, and it is possible that nodes experience different strengths and types (positive or negative) of influence from each other, i.e. $w_{ij} \neq w_{ji}$. 

We study the propagation of binary opinion states, A and B in the network, imposed by external controllers. 
Here we assume that controllers strictly exert positive influence on the network and external influence is expressed in terms of resource distribution vectors $\{p_{A},p_{B}\} \in \mathbb{R}^{N}_+$, where any element $p_{A,i} \geq 0$ (or $p_{B,i} \geq 0$) represents the strength of influence exerted by controller A (or B) on node $i$. The vectors are constrained linearly by the budget available to each controller, i.e. $\sum_{i}p_{A,i} = B_{A}$ (or $\sum_{i}p_{B,i} = B_{B}$). Unlike traditional models that assume a single-injection of influence at the start of the dynamics \cite{kempe2003maximizing,li2013influence}, here influence is applied continuously until the system reaches steady-state.

At any given point in time, individuals in the network strictly conform to either opinion states at a rate proportional to the strength of influence experienced by them. We assume opinion propagation follows voter dynamics, where at each time step a node $i$ chosen uniformly at random from the network updates its state by picking a source of influence (social neighbours or external controllers) to copy (or oppose in cases of negative strength). Node $i$ picks a neighbour $j$ with a probability $|w_{ji}|/(\textstyle\sum_{j \in {\cal N}_{i}^+ \cup {\cal N}_{i}^-} w_{ji} + p_{A,i} + p_{B,i})$\footnote{${\cal N}_{i}^+$ and ${\cal N}_{i}^-$ are the sets of positive and negative neighbours of node $i$.}, and either copies their state (when $w_{ji}>0$), or opposes them ($w_{ji}<0$). Similarly, node $i$ picks an external controller (say A) to copy, with the probability $p_{A,i}/(\textstyle\sum_{j \in {\cal N}_{i}^+ \cup {\cal N}_{i}^-} w_{ji} + p_{A,i} + p_{B,i})$. 

% with probability $|w_{ji}|/(\textstyle\sum_{j \in {\cal N}_{i}^+ \cup {\cal N}_{i}^-} w_{ji} + p_{A,i} + p_{B,i})$ and $p_{A,i}/(\textstyle\sum_{j \in {\cal N}_{i}^+ \cup {\cal N}_{i}^-} w_{ji} + p_{A,i} + p_{B,i})$ respectively.  Here ${\cal N}_{i}^+$ and ${\cal N}_{i}^-$ are the sets of positive and negative neighbours of the node $i$. 
% When a neighbour $j$ is picked, node $i$ copies the opinion state of $j$ if the edge from $j$ to $i$ is positive ($w_{ji}>0$). Conversely, $i$ adopts the opposing view if the edge weight is negative ($w_{ji}<0$). If a node picks one of the controllers, it strictly copies their opinion state as all external allocations to the network are positive. 




Assuming $x_{A,i}$ (or $x_{B,i} = 1 - x_{A,i}$) characterise the probability with which a node $i$ adopts opinion state A (or B), the rate at which $i$ transitions to opinion A is given by,
 
\begin{equation}
\frac{dx_{A,i}}{dt}=(1-x_{A,i}) \cdot \phi_{i}(A) -x_{A,i} \cdot \phi_{i}(B).
\label{Eq:1}
\end{equation}

The terms $\phi_{i}(A)$ and $\phi_{i}(B)$ indicate the fraction of total influence experienced by $i$ in favour of opinions A and B respectively and are given by,


\begin{align*}
\begin{array}{cc}
     & \phi_{i}(A) = \frac{\sum\limits_{j \in {\cal N}_i^+} w_{ji}x_{A,j}-\sum\limits_{j \in {\cal N}_i^-} w_{ji}(1-x_{A,j})+p_{A,i}}{\sum\limits_{j \in {\cal N}_i^+} w_{ji}-\sum\limits_{j \in {\cal N}_i^-} w_{ji}+p_{A,i}+p_{B,i}}; \\ \\
     & \phi_{i}(B) = \frac{\sum\limits_{j \in {\cal N}_i^+} w_{ji}(1-x_{A,j})-\sum\limits_{j \in{\cal N}_i^-} w_{ji}x_{A,j}+p_{B,i}}{\sum\limits_{j \in{\cal N}_i^+} w_{ji}-\sum\limits_{j \in {\cal N}_i^-} w_{ji}+p_{A,i}+p_{B,i}}.
\end{array}
\end{align*}
Here, A is our chosen focal controller. Similar expressions can be derived for controller B.

In signed networks, nodes experience both positive and negative influence from their neighbours. The collective positive influence is given by ${\displaystyle\sum}_{j \in {\cal N}_{i}^+} w_{ji}x_{A,j}$, and the total strength of negative influence (from neighbours in state B) is ${\displaystyle\sum}_{j \in {\cal N}_{i}^-} w_{ji}(1-x_{A,j})$.    
Edge weights $w_{ji}$ refer to incoming edges and allocations from external controllers A and B on node $i$ are $p_{A,i}$ and $p_{B,i}$ respectively. 

Given the above, the steady-state equation for the system can be evaluated by replacing $\frac{dx_{A,i}}{dt}=0$ in \cref{Eq:1} to obtain

\begin{equation}
   x_{A,i}^* = \frac{p_{A,i} - \sum\limits_{j \in {\cal N}_i^-}w_{ji} + \sum\limits_{j \in {\cal N}_{i}^+}w_{ji} x_{A,j} + \sum\limits_{j\in {\cal N}_{i}^-}w_{ji} x_{A,j}}{\sum\limits_{j\in {\cal N}_{i}^+}w_{ji} - \sum\limits_{j\in {\cal N}_{i}^-}w_{ji} + p_{A,i} + p_{B,i}}. 
    \label{steady-state}
\end{equation}

Here, $x_{A,i}^*$ is the probability a node $i$ has opinion state \emph{A} at equilibrium. 
For a network of size $N$ nodes, we obtain a system of $N$ equations, which can be given as follows, 

\begin{align*}
\quad
\begin{bmatrix}
L+diag(p_{A}+p_{B})
\end{bmatrix}
x_{A}^* &=
p_{A}-\vec{1}^T W^-,
\end{align*}
\begin{align}
\implies    X_{A} &= \frac{1}{N}\Vec{1}^{T} {x_{A}^*} = \frac{1}{N} \Vec{1}^{T}  [L+diag(p_{A}+p_{B})] ^{-1}(p_{A}-\vec{1}^T W^-).
    \label{optimisation}
\end{align}

$X_{A}$ in \cref{optimisation} denotes the total vote-share obtained by controller A at equilibrium. Assuming $W^+$ and $W^-$ are the weighted adjacency matrices of the positive and negative components of the network, the vector $\vec{1}^T W^-$ captures the total strength of negative influence on each node in the network. Here $L$ is an $N \times N$ matrix given by $L = diag(\vec{1}^T(W^+ - W^-)) - (W^+ + W^-)$. The diagonal elements represent the absolute sum of all edge weights of a node $i$, given by $L_{ii} = {\displaystyle\sum}_{j \in {\cal N}_{i}^+} w_{ji} - {\displaystyle\sum}_{j \in {\cal N}_{i}^-} w_{ji}$ and off-diagonal elements are $L_{ij} = -w_{ji}$. For unweighted graphs $L = diag(D) - (A^+ - A^-)$, where $D$ is the degree-vector\footnote{diag(D) is an $N \times N$ matrix where the diagonal elements capture the degrees of nodes in the network.}. $A^+$ and $A^-$ are the respective adjacency matrices of the positive and negative components. Note that since $[L + diag(p_{A} +p_{B})]$ is diagonally-dominant, it is invertible and we can therefore use \cref{optimisation} to determine solutions for $X_{A}$. 

The formal optimisation problem can then be stated as

\begin{align}
    % maximise \quad &\frac{1}{N} \Vec{1}^{T}  [L+diag(p_{A}+p_{B})] ^{-1}(p_{A}+\vec{1}^T W^-),\\
    % subject 
    % %\begin{array}{l}
    % &\text{maximise} \quad \frac{1}{N} \Vec{1}^{T}  [L+diag(p_{A}+p_{B})] ^{-1}(p_{A}+\vec{1}^T W^-)\\
    % &\text{subject to} \quad \sum\limits_{i=1}^N p_{A,i} = B_{A},\\
    % & \qquad 0 \leq p_{A,i} \leq B_{A}.
    % \end{array}
    p_{A}^* = \text{argmax}_{p_{A} \in \mathcal{P}} \quad X_{A}^*(L,p_{B}),
    \label{formal_opti}
\end{align}
where $\mathcal{P}$ is a set of all possible allocations $p_{A}$ such that $0 \leq p_{A,i} \leq B_{A}$ ($\forall i \in \{1,2,\ldots,N\}$) and $\sum\limits_{i=1}^N p_{A,i} = B_{A}$.


For a passive opponent B (where $p_{B}$ is fixed and known), controller A maximises their opinion shares using Eq. \ref{formal_opti}. 
Closed-form solutions can be readily obtained in networks with simplified structures (e.g. star networks) by solving the set of equations outlined in \cref{optimisation}. To do this, we first determine the gradient $\nabla_{p_{A}} X_{A}$ by differentiating \cref{optimisation} wrt to the allocation vector $p_{A}$ as

\begin{align}
    \nabla_{p_{A}} X_{A} = \frac{1}{N} \Vec{1}^{T} [L+diag(p_{A}+p_{B})] ^{-1} (I - diag(x_{A})),
    \label{gradient}
\end{align}

and then solve $\nabla_{p_{A}} X_{A}=0$ to obtain the optimal allocation $p_{A}^*$ that yields maximum vote-shares $X_{A}^*$. However, obtaining analytical solutions for $\nabla_{p_{A}} X_{A}=0$ in larger, more complex networks can be considerably challenging. In which case we use local search techniques such as gradient ascent (as they have been shown to work well in similar settings \cite{lynn2016maximizing,romero2021shadowing}) to determine optimal allocations in arbitrary networks. More specifically, we employ the gradient algorithm proposed in \cite{romero2021shadowing}, by first modifying it to fit our purpose. Note that the problem setting considered in \cite{romero2021shadowing} resembles our research problem closely, as both studies explore the competitive influence maximisation problem in the voter model under continuous allocations, with the exception that the model given in \cite{romero2021shadowing} only applies to unsigned (or positive) networks. This yields a slightly different expression for vote-shares and does not contain the term $-\vec{1}^T W^-$ (as in \cref{optimisation}). Despite this difference, the expression for gradient is the same as \cref{gradient}, given that $-\vec{1}^T W^-$ is independent of controller allocations $p_{A}$. Thus we replace the expression for vote-shares in the algorithm (from \cite{romero2021shadowing}) with \cref{optimisation} and then use it to obtain optimal allocations. We label this approach $GA$. 


% Given that everything else is identical across both settings, we can employ the algorithm in \cite{romero2021shadowing} to obtain optimal allocations for our setting, simply by replacing the expression for vote-shares with \cref{optimisation}. We label this approach as $GA$. 


%  to incrementally update the allocation vector $p_{A}$ (with step length $\eta$), initialised as a uniformly distributed random vector $p_{A}^{(0)}$ such that $\sum_{i} p_{A,i}^{(0)} = B_{A}$, until a $\mu$-approximated optimal allocation $p_{A}^*$ is obtained, for any given budget $B_{A}$, opponent strategy $p_{B}$ and network structure $L$. The budget constraint is systematically imposed at each step, by projecting the resulting allocation vector $p_{A}^{(t+1)}$ onto an $N$-simplex using the algorithm in \cite{chen2011projection}, such that $\sum_{i} p_{A,i} = B_{A}$. The learning parameter $\eta$ is adjusted using exact-line search to ensure convergence \cite{boyd2004convex}.
% We find that $X_{A}(p_{A})$ is concave (see \cref{appendix-convexity}), implying that the local maximum reached in this case is also the global maximum.

% \begin{algorithm}
% \SetAlgoLined
% \textbf{Input : }$p_{B},L,B_{A},\eta,\mu$\;
% \KwResult{$p_{A}^*$}
% initialise t=0, $p_{A}^{(0)}$, $\eta = N$ \;
%  \While{$p_{A}^{(t+1)} - p_{A}^{(t)} > \mu$}{
%   $p_{A}^{(t+1)} = \mathcal{P}[p_{A}^{(t)} + \eta \nabla_{p_{A}^{(t)}}X_{A}]$\; 
  
%   \If{$X_{A}^{(t+1)} - X_{A}^{(t)} < 0$}{
%   $\eta^{(t+1)} = \frac{\eta^{(t)}}{2}$\;
%   $p_{A}^{(t+1)} = p_{A}^{(t)}$;
%   }
%   t++\;
%  }
 
%  \caption{$\mu$-approximation of optimal allocation vector using gradient ascent}
%  \label{GA-algo}
% \end{algorithm}

In order to determine the effectiveness of the negative-tie aware method, we compare it against the algorithm that considers all edges to be positive, therefore discounting any negative influences in the network dynamics, when solving the influence maximisation problem. We then measure the impact of taking a negative-tie aware approach on vote-shares and quantify the gain or loss. There are two broad cases where controllers might be negative-tie agnostic: (i) where they cannot observe negative ties (in cases of under-representation) or (ii) where mistake them for positive edges (misrepresentation) \cite{li2013influence}. For the sake of simplicity, we avoid considering instances where controllers can partially observe negative edges in the network.  


We are keen to determine instances where a controller would gain the most from adopting a negative-tie aware approach and we find that misrepresenting negative-ties consistently performs worse as a strategy in comparison to under-representation of negative-ties (see \cref{appendix-rem} for more details). Thus in the rest of the paper, we compare our proposed method to the approach where negative ties are misrepresented (i.e. all edges are assumed to be positive)\footnote{In undirected networks, both approaches \textemdash misrepresentation and under-representation of negative ties \textemdash would yield equal vote-shares.}.
% A more compelling reason for this choice is that negative ties largely present themselves as neutral or positive edges in online social networks which can be detected only through additional analysis \cite{bae2012sentiment,leskovec2010predicting}. 

%\subsection{Voting dynamics in networks assuming strictly positive ties}
To determine an expression for vote-shares in networks where all edges are positive, we modify \cref{steady-state} that yields an expression for vote-shares, given by $X_{A}^{(+)}$ (identical to the expression for vote-shares in \cite{romero2021shadowing}). We then directly apply the algorithm in \cite{romero2021shadowing} to obtain optimal allocations $p_{A}^*$. For ease of notation, we label this approach $GA^{(+)}$.
% Using a framework inspired by \cite{masuda2015opinion}, we obtain an expression for vote-shares at equilibrium given by,

% \begin{align}
%      x_{A,i}^{*(+)} &= \frac{p_{A,i} + \sum\limits_{j \in {\cal N}_{i}}w_{ji} x_{A,j}^{(+)} }{\sum\limits_{j \in {\cal N}_{i}}w_{ji} + p_{A,i} + p_{B,i}},\\
%      \implies X_{A}^{(+)} &= \frac{1}{N}\Vec{1}^{T} {x_{A}}^{(+)} = \frac{1}{N} \Vec{1}^{T} p_{A} [L^{(+)}+diag(p_{A}+p_{B})] ^{-1}.
%      \label{opti-pos}
% \end{align}

% Here $X_{A}^{(+)}$ indicates the final vote-share obtained by a controller to whom all edges appear strictly positive. $L^{(+)}$ is the $N\times N$ laplacian matrix of the directed graph given as $L = diag(\vec{1}^T W) - W$, where W is the corresponding weighted adjacency matrix. Allocation vectors of both controllers remain unchanged and are given as $p_{A}$ and $p_{B}$. The optimisation problem in this case can be defined as,

% \begin{align}
%     p_{A}^* &= \text{argmax}_{p_{A}} X_{A}^{*(+)}(L^{(+)},p_{B},B_{A}).
% \end{align}
% The budget constraints apply as before.

% We find that the gradient $\nabla X_{A}^{(+)}$ in this variant is similar to \cref{gradient}, and is given by $\nabla_{p_{A}} X_{A}^{(+)} = 1/N \Vec{1}^{T} [L^{(+)}+diag(p_{A}+p_{B})]^{-1}(I - diag(x_{A}^{(+)})$. The proof of concavity is also preserved and once again we can employ gradient ascent steps $GA^{(+)}$ to arrive at a global optimum $p_{A}^{*}$, for a given budget ($B_{A}$), adversarial allocation ($p_{B})$ and unsigned network ($L^{(+)}$).