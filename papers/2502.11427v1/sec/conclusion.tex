\section{Conclusion}
\label{sec:conclusion}

In this paper, we proposed an instruction-free fine-tuning framework ViFT, for enhancing the multimodal task-solving capabilities of LVLMs. Concretely, instead of using visual instructions, we only leveraged text instructions and image caption data, to separately learn the individual task-solving and visual perception abilities for the LVLM.
%vision and language capability via disentangled fine-tuning. 
After that, we extracted the steering vectors by using the model's hidden space for the disentangled abilities via different modality inputs, and combined them to guide the inference of the LVLM in multimodal tasks.
%During inference, we entangled the capabilities by combining the task vector in prefilling stage. 
With rather less training data, our trained model, ViFT, achieved state-of-the-art performance among competitive LVLMs across various visual reasoning and instruction following benchmarks. Furthermore, based on our proposed framework, we can efficiently scale the vision data and text data to enhance the model's performance, which facilitates further advancements in this field.


% In this paper, we propose a new visual instruction-free fine-tuning framework LiTA, for training LVLMs. Concretely, we 

% we study the modality bias in existing large vision-language models. We propose a new training paradigm, LiTA, to mitigate the modality bias in LVLMs. Concretely, we first remove all visual instructions from the training data to avoid overfitting to the instruction-response pattern. Then, we propose to extract a disentangled latent vector that causes the biased behaviour. After that, we calibrate the LVLM's behaviour by arithmetically subtracting the extracted latent vector from instruction hidden states. Our training strategy only require caption and text for training, and can effectively transfer LLM's advanced capabilities to visual scenarios. LiTA achieves state-of-the-art performance on several challenging visual reasoning benchmark, surpassing current best-performing LVLMs that is trained on much more complex visual instruction data.