\begin{abstract}
Visual instruction tuning has become the predominant technology in eliciting the multimodal task-solving capabilities of large vision-language models (LVLMs). 
Despite the success, as visual instructions require images as the input, it would leave the gap in inheriting the task-solving capabilities from the backbone LLMs, and make it costly to collect a large-scale dataset.
%synthesis of visual instructions remains costly, and training on instruction data often leads models to fit superficial vision-language response patterns, making it hard to achieve genuine capability improvements through data scaling. 
To address it, we propose ViFT, a visual instruction-free fine-tuning framework for LVLMs. 
In ViFT, we only require the text-only instructions and image caption data during training, to separately learn the task-solving and visual perception abilities.
During inference, we extract and combine the representations of the text and image inputs, for fusing the two abilities to fulfill multimodal tasks.
%also utilize the text and image as the input respectively, to extract the hidden states as the steering vectors for specific abilities, and then leverage the fused steering vector to guide LVLMs to fulfill multimodal tasks.
%Compared to conventional visual instruction tuning, LiTA only requires captions and text instructions for training, which can be scaled efficiently and effectively. Concretely, we first enable the model to learn disentangled vision and language capabilities by training on a mixture of caption and text data. Subsequently, we activate the disentangled capabilities through different modality inputs and extract task vectors from the model's hidden space. During inference, we entangle the capabilities by combining task vectors. 
Experimental results demonstrate that ViFT can achieve state-of-the-art performance on several visual reasoning and visual instruction following benchmarks, with rather less training data. 
Our code and data will be publicly released.
%Furthermore, we discover that improvements in disentangled capability obtained through data scaling can be effectively transferred to enhanced entangled vision-language capabilities for actual vision tasks, establishing a promising direction for future research.
\end{abstract}