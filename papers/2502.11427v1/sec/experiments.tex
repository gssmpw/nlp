\begin{table*}[t]
  \centering
  % \renewcommand\arraystretch{1.1}
  \scalebox{0.95}{
  \begin{tabular}{>{\raggedright\arraybackslash}p{.25\linewidth}|
  *{3}{>{\centering\arraybackslash}p{.07\linewidth}}|
   >{\centering\arraybackslash}p{.07\linewidth}
   >{\centering\arraybackslash}p{.07\linewidth}|>{\centering\arraybackslash}p{.07\linewidth}
   |>{\centering\arraybackslash}p{.07\linewidth}
   }
    \toprule
    \multirow{2}{*}{Model}    & \multicolumn{3}{c}{\textbf{MathVista}} & \multicolumn{2}{c}{\textbf{MathVerse}} & \textbf{MathV}  &  \multirow{2}{*}{Avg}    \\
    \cline{2-7}
    &   MVQA   &  GVQA  &  \textbf{ALL}  &  V-mini 
 &  \textbf{ALL} &  \textbf{ALL} \\
    \midrule
    % \multicolumn{8}{c}{\textbf{Baseline LVLMs}} \\
    % \midrule
    MiniGPT4-7B   &  19.4  &   25.2   &  23.1  &  9.2   &  10.1  &  7.4  &  13.5 \\
    LLaVA-1.0-7B    &  22.4  &  28.4  &  25.2  &  10.9  & 11.5  &  8.1 &  14.9   \\
    LLaVA-1.5-7B    &  21.4  &  30.5  &  25.6  &  11.8  & 12.1  &  8.5 &  15.4   \\
    LLaVA-Next-8B     &  32.0  &  51.5  &  41.0  &  12.5  & 13.9   &  14.1  &  23.0  \\

    MiniCPM-V-2.5-8B &  39.8  &  54.5  &  46.6  &  16.8  & 20.5  &  14.1  &  27.1 \\
    LLaMA-3.2-Vision-11B  &  44.3  &  53.9  &  48.7  & 
    22.7  & 26.1  &  15.8  &  30.2 \\
    IXL-2.5-7B   &  48.0  &  \underline{62.0}  &  54.4  &  22.4  &  27.2  &  14.8  &   32.1  \\
    Qwen2-VL-7B   &  54.3  &  \textbf{64.3}  &  \textbf{58.9}  & 
 27.1  & 30.5  &  17.7  &  35.6 \\
    LLaVA-OneVision-7B    &  \textbf{58.0}  &  58.7  &  \underline{58.3}  &  26.2  & 31.0  &  18.1   &  35.8  \\
    \midrule
    % \multicolumn{8}{c}{\textbf{Ours}} \\
    % \midrule
    ViFT-7B~(ours)       &  49.8  &  48.4  &  49.2  &  \underline{28.6}  & \underline{34.8} & \textbf{24.0}   &  \underline{36.0}  \\
    ViFT-A-7B~(ours)        &  \underline{56.7}  &  53.9  &  55.4  &  \textbf{28.8}  & \textbf{35.4} & \underline{20.5}   &  \textbf{37.1} \\
    
  \bottomrule
  \end{tabular}}
  \caption{A comparison between ViFT and other baseline models on three visual reasoning benchmarks. MVQA and GVQA indicates the Math-VQA and General-VQA subset from MathVista. V-mini indicates the vision-centered subset from MathVerse. MathV is short for Math-Vision dataset. Avg indicates the average performance. \textbf{Bold} and \underline{underline} fonts indicate the best and second best performance, respectively.
  }
  \label{tab:main-table}
\end{table*}
\section{Experiment}
\label{sec:exp}


\begin{table}[t]
  \centering
  % \renewcommand\arraystretch{1.1}
  \scalebox{0.85}{
  \begin{tabular}{>{\raggedright\arraybackslash}p{.35\linewidth}
  |*{3}{>{\centering\arraybackslash}p{.12\linewidth}}|
  >{\centering\arraybackslash}p{.12\linewidth}
   }
    \toprule
    Model  &  Comp  &  Conv  &  Detail  &  ALL \\
    \midrule
    LLaVA-1.0-7B  &  66.1  &  54.7  &  44.2  &  57.2  \\
    LLaVA-1.5-7B  &  70.5  &  54.4  &  55.0  &  61.8  \\
    LLaVA-Next-8B  &  72.2  &  61.0  &  60.0  &  66.0  \\
    IXL-2.5-7B  &   78.4  &   55.6 &  \underline{74.7}  &  70.2 \\
    Qwen2-VL-7B  &   56.2  &  \textbf{87.5}  &  73.5 & 
 70.1  \\
    LLaVA-OV-7B  &  \underline{85.1}  &  \underline{79.9}  &  73.4  &  \underline{81.0}  \\
 %    Gemini-pro  &   78.8   &  \textbf{90.8}  &  67.3  & 
 % 79.9  \\
    ViFT-7B & \textbf{89.4}   &  72.3  &  \textbf{80.5}   &  \textbf{82.2} \\
    
  \bottomrule
  \end{tabular}}
\caption{A comparison between ViFT and other baseline models on visual instruction following. We report the results on LLaVABench. Comp and Conv are short for Complex and Conversation benchmark subset.}
\label{tab:table-general}
\end{table}

\subsection{Evaluation Benchmarks}

We primarily evaluate ViFT's ability on visual reasoning. Specifically, we evaluate ViFT's reasoning ability on three challenging benchmarks: MathVista~\cite{lu2023mathvista}, Mathverse~\cite{zhang2025mathverse}, and Math-Vision~\cite{wang2024measuring}.  Furthermore, we assess ViFT's general visual instruction following ability on LLaVA-Bench~\cite{liu2024visual}. Notably, for models that are only capable of generating direct answers, we employ chain-of-thought prompting to elicit its reasoning ability during visual reasoning evaluation. 
% For our evaluation framework, we employ GPT-3.5-turbo to evaluate the performance on visual reasoning, and GPT-4 to evaluate the performance on visual instruction following.

\subsection{Baselines}

% We compare ViFT with several baseline models. These models encompass early LVLMs trained with basic visual instruction data~(\eg MiniGPT4~\cite{zhu2023minigpt} and LLaVA-1.5~\cite{liu2024improved}), LVLMs trained with enhanced visual instructions~(\eg LLaVA-Next~\cite{liu2024llavanext} and MiniCPM-V-2.5~\cite{yao2024minicpm}), and current best-performing LVLMs~(\eg Qwen2-VL~\cite{wang2024qwen2} and LLaVA-OneVision~\cite{li2024llava}). 
We compare ViFT with several baseline models. These models encompass early LVLMs trained with basic visual instruction data~(MiniGPT4~\cite{zhu2023minigpt}, LLaVA-1.0~\cite{liu2024visual}, and LLaVA-1.5~\cite{liu2024improved}), LVLMs trained with enhanced visual instructions~(LLaVA-Next~\cite{liu2024llavanext}, LLaMA-3.2-Vision~\cite{meta2024llama}, and MiniCPM-V-2.5~\cite{yao2024minicpm}), and current best-performing LVLMs~(InternLM-XComposer~(IXL-2.5)~\cite{zhang2024internlm}, Qwen2-VL~\cite{wang2024qwen2}, and LLaVA-OneVision~\cite{li2024llava}). 
Further details about the baseline models are provided in the \autoref{apdx-baselines}.


% \subsection{Baselines}


% We compare LiTA with a number of existing open-source LVLMs. These models encompass early LVLMs trained with basic visual instruction data~(\eg MiniGPT4~\cite{zhu2023minigpt}, LLaVA-1.0~\cite{liu2024visual}, and LLaVA-1.5~\cite{liu2024improved}), advanced LVLM trained with enhanced visual instructions~(\eg LLaVA-Next~\cite{liu2024llavanext} and MiniCPM-V-2.5~\cite{yao2024minicpm}), and current best-performing LVLMs~(InternLM-XComposer~(IXL-2.5)~\cite{zhang2024internlm}, Qwen2-VL~\cite{wang2024qwen2} and LLaVA-OneVision~\cite{li2024llava}). Notably, Although IXL-2.5 and Qwen2-VL achieve impressive performance, they are trained on a extensive multimodal datasets (exceeding 80M samples). LLaVA-OneVision, on the other hand, utilizes a relatively smaller but still substantial dataset of 9.5M samples. Compared to these models, our LiTA demonstrates superior data efficiency by requiring only 2.7M~(2.9M for LiTA*) training samples. We present more detailed information of baseline LVLMs in \autoref{apdx-baselines}.


\subsection{Implementation Detail}

We adopt a two-stage training strategy: In the first stage, we train on web captions. In the second stage, we train on a mixture of high-quality captions and text instructions. This strategy avoids the additional computational overhead caused by significant length disparity between low-quality web captions and high-quality captions. We provide a comparison of one-stage and two-stage training in \autoref{appendix-exp}. We set the learning rate to 1e-5 for the LLM and vision encoder, and 2e-6 for the connector layer. The batch size is configured as 8 for each GPU. All models are trained for one epoch.

During inference, we only conduct ability fusion in the top 50\% of layers. We set $\alpha=1.0, \beta=0.1$ for visual reasoning and $\alpha=1.0, \beta=0.15$ for visual instruction following for robust performance. We conduct detailed studies of the optimal hyperparameters and fusion layers in Section \ref{sec_anaysis}.






\subsection{Main Results}
% 需要加入具体数据说明（哪个比哪个高，可能需要加一些paratitle，并简化文字）



\begin{table}[t]
  \centering
  % \renewcommand\arraystretch{1.1}
  \scalebox{0.9}{
  \begin{tabular}{>{\raggedright\arraybackslash}p{.50\linewidth}>{\centering\arraybackslash}p{.21\linewidth}
  >{\centering\arraybackslash}p{.24\linewidth}
   }
    \toprule
    Model  &  MathVista    &  LLaVABench  \\
    \midrule
    ViFT  &  49.2  &  82.2    \\
    $\neg$ Low-quality captions  &  48.5  &  81.6  \\
    $\neg$ High-quality captions  &  42.1  &  66.4   \\
    $\neg$ Text instructions  &  43.9  &  65.7  \\
    $\neg$ AF inference  &  46.1  &  59.1  \\
  \bottomrule
  \end{tabular}}
    \caption{The ablation of different training data component and inference strategy. AF inference indicates ability-fused inference.}
    \label{tab:table-ablation-af}
\end{table}
% We present the results of LitA on advanced visual reasoning and general visual instruction following in table xxx and table xxx, respectively. We list our findings below:

\paragraph{Visual Reasoning.} We present the results of ViFT and ViFT-A on  visual reasoning benchmarks in \autoref{tab:main-table}. Firstly, we observe that ViFT-A exhibits better performance compared to ViFT on MathVista and MathVerse, while achieving worse results on MathVision. This indicates that the VQA data indeed introduces fine-grained visual knowledge, but the performance improvement is primarily observed on benchmarks that have high visual domain overlap with these VQA data~(\eg MathVista). On benchmarks that focus more on advanced visual reasoning abilities~(\eg MathVision), the short response pattern introduced by VQA data may conflict with the model's inherited reasoning patterns, resulting in performance degradation. Overall, while the incorporation of VQA data yields a modest improvement in average performance, the enhancement remains relatively limited in scope.

Secondly, we compare ViFT with other baseline LVLMs. Notably, both ViFT and ViFT-A surpass all baseline models on MathVerse and MathVision. On MathVista, ViFT and ViFT-A show relatively lower performance compared to baselines, probably due to the lack of VQA training data. In terms of the average performance across all benchmarks,
ViFT-A demonstrates the best results among all baseline LVLMs, followed closely by ViFT. Despite being trained on a substantially smaller dataset (2.7M vs 5.5M on caption data and 0.2M vs 4M on other multimodal data), ViFT-A outperforms the leading baseline LVLM, LLaVA-OneVision-7B. This demonstrates the effectiveness of our proposed framework. Compared to conventional visual instruction tuning methods, we efficiently learn the fused visual reasoning ability to achieve superior performance with significantly less training data.

\begin{figure}[t]
  \begin{subfigure}[b]{0.48\linewidth}
  \centering
    \includegraphics[width=\linewidth]{images/ablation_alpha_v2.pdf}
    \caption{Impact of $\alpha$}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.48\linewidth}
  \centering
    \includegraphics[width=\linewidth]{images/ablation_beta_v2.pdf}
    \caption{Impact of $\beta$}
  \end{subfigure}
  \caption{The impact of different hyperparameters.}
  \label{tab:ablation-alpha-beta}
\end{figure}

\paragraph{Visual Instruction Following.} 
We demonstrate the performance of ViFT on general visual instruction following in \autoref{tab:table-general}. Among all baseline models, LLaVA-OneVision-7B exhibits the best average performance, demonstrating promising results across all subsets. We also observe the poor performance of Qwen2-VL-7B on the complex subset. Given that it employs Qwen2-7B, a strong backbone LLM, such low performance likely stems from the compromised instruction following ability caused by visual instruction tuning.
Compared to baseline models, ViFT achieves the best average performance. As for each subset, ViFT has the best performance on the complex subset, indicating that our approach enables the model to handle complex instructions while correctly interpreting the images. ViFT also has the strongest performance on the detail subset, which can be credited to the high-quality caption data. Despite this, ViFT's performance on the conversation subset is relatively mediocre, suggesting that the model can be further improved by conducting human alignment based on visual inputs.


\section{Further Analysis}\label{sec_anaysis}
\paragraph{Ablation Study.}

We employ diverse training data components and inference strategy in our training framework. We present the ablation results in \autoref{tab:table-ablation-af}.
First, we examine the impact of each data component for fine-tuning. We observe that removing high-quality captions or text instructions can result in severe performance decline. This indicates that the high-quality captions play a crucial role in enhancing the model's visual perception ability, which subsequently improves their multimodal task-solving capability. Text instructions are equally important as they preserve the LLM's inherent task-solving ability from multimodal training. In comparison, the impact of low-quality captions is relatively limited. Second, we study the effect of our proposed ability-fused inference. As we can observe, the model exhibits significant performance degradation without ability-fused inference.  This indicates that the individual abilities acquired through fine-tuning cannot be effectively combined through standard inference, and our proposed ability-fused inference successfully addresses this limitation.



\paragraph{Hyperparameter Tuning.}
We study the effect of different hyperparameter $\alpha$ and $\beta$ on model performance. The results are presented in \autoref{tab:ablation-alpha-beta}. 
For $\alpha$, we observe that as $\alpha$ increases, the model's performance initially increases and then decreases. 
While the performance on MathVista exhibits a sudden improvement at early stages, it remains relatively stable as $\alpha$ changes. The results confirm that $\alpha=1.0$ represents an optimal choice, while small deviations do not significantly impact performance.
Similar to  $\alpha$, the model performance exhibits an increase-then-decrease pattern as $\beta$ varies, though with more pronounced fluctuations. We observe a sudden performance drop when $\beta$ reaches $0.4$, indicating that such a large $\beta$ can result in the model's abnormal behavior. Furthermore, we discover that the optimal $\beta$ varies across different tasks. For visual reasoning, the optimal $\beta$ is 0.1, whereas for visual instruction following, it is 0.15. This demonstrates that different vision tasks may require varying levels of individual abilities, resulting in task-specific optimal fusion ratios.

\begin{table}[t]
  \centering
  % \renewcommand\arraystretch{1.1}
  \scalebox{0.9}{
  \begin{tabular}{>{\centering\arraybackslash}p{.18\linewidth}
  |*{2}{>{\centering\arraybackslash}p{.21\linewidth}}
  >{\centering\arraybackslash}p{.24\linewidth}
   }
    \toprule
    Layers  &  MathVista  &  MathVision  &  LLaVABench  \\
    \midrule
    0-7  &  48.8  &  17.0  &  64.8  \\
    0-14  &  46.4  &  19.6  &  74.4  \\
    0-21  &  \underline{49.0}  &  17.3  &  80.6  \\
    0-28  &  47.8  &  16.4  &  76.8  \\
    7-28  &  48.1  &  19.6  &  \underline{81.2}  \\
    14-28  &  \textbf{49.2}  &  \textbf{24.0}  &  \textbf{82.2}  \\
    21-28  &  46.3  &  \underline{22.8}  &  73.8  \\
  \bottomrule
  \end{tabular}}
    \caption{The impact of fusion layer selection.}
    \label{tab:table-ablation-fl}
\end{table}

\paragraph{Fusion Layer Selection Analysis.}

We investigate the impact of layer selection for ability fusion. We examine two strategies: selecting layers from the top downward, or from the bottom upward. The results are presented in \autoref{tab:table-ablation-fl}. Our findings indicate that the top-down selecting strategy consistently outperforms bottom-up selection when selecting the same number of layers. This is likely due to the fact that the LLM's top layers have more influence on the model's generation behavior~\cite{geva2020transformer,geva2022transformer}, which makes ability fusion more effective at these layers. Also, recent studies~\cite{chen2025image,zhang2025llava} demonstrate that visual information tends to aggregate with text tokens within the LVLM's early layers, and ability fusion in these layers may disrupt such a process, leading to declined performance. Moreover, we discover that selecting 50\% of the layers from the top of the model downward yields the best performance, which makes it an optimal choice.



\paragraph{Computation Complexity.}
We examine the additional time overhead of ability-fused inference compared to standard inference. The results are presented in \autoref{fig:speed}.
As we can observe, when generating short responses~(\eg 25 tokens), our ability-fused inference is about 8\% slower than standard inference. However, as the generation length increases, the speeds for standard inference and ability-fused inference gradually converge. When generation length reaches 400, ability-fused inference almost doesn't introduce any additional computational overhead. This aligns with our expectations. For ability-fused inference, we merely introduce one additional forward pass during the entire generation process. Thus, while there is some discrepancy when generating short responses, such differences become negligible as generation length increases.


\begin{figure}[t]
  \begin{subfigure}[b]{0.48\linewidth}
  \centering
    \includegraphics[width=\linewidth]{images/speed_v2.pdf}
    \caption{Efficiency Test}
    \label{fig:speed}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.48\linewidth}
  \centering
    \includegraphics[width=\linewidth]{images/scaling_test_v2.pdf}
    \caption{Scaling Test}
    \label{fig:scaling}
  \end{subfigure}
  \caption{Efficiency test and scaling test for ViFT.}
\end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{images/speed_v2.pdf}
%     \caption{Speed comparison between standard inference and our proposed debias inference.}
%     \label{fig:speed}
% \end{figure}

\paragraph{Scaling Test.}
We investigate the effect of data scaling for conventional visual instruction tuning~(VIT) and ViFT. Concretely, we collect visual instruction datasets from ALLaVA~\cite{chen2024allava}, SViT~\cite{zhao2023svit} and LLaVA-Instruct~\cite{liu2024visual}. Then, we randomly sample several data subsets from ViFT's training data and the collected visual instructions at different sampling ratios, respectively. We then train LVLMs with these data subsets and evaluate their results on MathVista, as shown in \autoref{fig:scaling}. We observe that data scaling consistently yields performance improvements for ViFT. This indicates that the enhancements in two individual abilities effectively propagate to improved fused multimodal task solving capability.
As for conventional visual instruction tuning, the model achieves promising performance improvement with minimal data, but cannot yield significantly better results via data scaling. This likely occurs because existing visual instructions primarily help models learn superficial styles, rather than improving actual multimodal task-solving capability. This further validates that ViFT demonstrates greater potential for performance improvement by leveraging existing large-scale, cost-effective data, compared to conventional approaches.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\columnwidth]{images/scaling_test_v2.pdf}
%     \caption{Comparison of LiTA and VIT~(Visual Instruction Tuning) on data scaling.}
%     \label{fig:scaling}
% \end{figure}


% \begin{figure}[t]
%   \includegraphics[width=0.48\linewidth]{images/speed_v2.pdf}
%   \hfill
%   \includegraphics[width=0.48\linewidth]{images/scaling_test_v2.pdf}
%   \caption {The ablation of $\alpha$ and $\beta$.}
%   \label{tab:ablation-alpha-beta}
% \end{figure}



