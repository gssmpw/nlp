\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{approach_graph/LiTA_v6.pdf}
    \caption{Compared to visual instruction tuning, ViFT first learns disentangled individual abilities through ability-specific fine-tuning. During inference, given a visual instruction, we extract the disentangled ability vectors through different modality inputs, and merge
    them into the fused vector for guiding the LVLM to generate the output.}
    \label{fig:LiTA}
\end{figure*}
\section{Preliminary}
% The motivation for training LVLM is to learn new vision capability entangle language capability to solve complex vision tasks. 
% The most commonly-used paradigm for training LVLMs is visual instruction tuning. 
%To equip LLMs with the capability to process visual information, most 
%Generally, LVLMs first encode the images, then map the visual features into LLM's input space during visual instruction tuning. We take a simple and effective LVLM design, LLaVA~\cite{liu2024improved}, as an example. LLaVA leverages 
Existing LVLMs~\cite{liu2024visual,he2024efficient} generally consist of a pretrained visual encoder $f(\cdot)$ to process visual inputs (\eg images or videos), a connection layer $g(\cdot)$ for feature projection, and an LLM $p(\cdot)$ for autoregressive generation.
During inference, given a visual instruction including an image input $v$ and a text instruction $t$, 
%and a target text response $a$. During training, 
the image is first processed through visual encoder $f(\cdot)$ and connection layer $g(\cdot)$, producing visual tokens $X_v=[x_{v_1},...,x_{v_n}]$. These tokens are then prepended to the tokens of the text input $X_t$ to compose the input of the LLM for autoregressively generating the target text. 
%For a sequence of length $L$, the probability of the target response $X_a$ is denoted as:
%\begin{align}
%p(X_a|X_v,X_t) = \prod_{i=1}^Lp(x_i|X_v, X_t, X_{a, <i})
%\end{align}
%where $X_{t,<i}$ and $X_{a, <i}$ are the instruction tokens and predicted response tokens before the current prediction token, respectively.
To train the LVLM for integrating the visual encoder and LLM, existing methods mainly incorporate two training stages: alignment pre-training and visual instruction tuning. 
The first stage only requires caption data and the second stage requires visual instructions.
% The captions are converted to the same format as visual instruction by setting the text instruction to a simple request for describing image, and the target response to the actual caption. 

In this work, we aim to skip the visual instruction tuning stage, and only train the model with captions and text instructions for disentangling and improving the task-solving and visual perception abilities.
For model architecture, we follow LLaVA's design. Concretely, we adopt SigLIP as the visual encoder according to its suggestion~\cite{liu2024llavanext}, and select Qwen2.5-7B-instruct~\cite{yang2024qwen2} as our base LLM due to its remarkable performance.
%as it's one of the most capable model among models of comparable parameter size.
For connection layer, we follow the widely-used setting in current LVLMs~\cite{liu2024visual,li2024mini} that implement a simple 2-layer MLP.
%which represents a widely accepted design paradigm .


\section{Approach}

% 能力被解耦于不同模态的输入下，模型实际上具备解决视觉任务的子能力，但是这些子能力无法在视觉-语言输入下被组合，因此仍然无法解决视觉任务。

\label{sec:approach}

%\subsection{Overview}

In this section, we introduce our proposed ViFT, a visual instruction-free fine-tuning framework for LVLMs.
Our main motivation is that the multimodal task-solving capability of LVLMs can be split into the task-solving ability of LLMs and the visual perception ability, which can be separately learned through text-only instructions and image caption data.
In ViFT, we first collect the above data to fine-tune the LVLM for learning the two individual abilities, and then extract their corresponding steering vectors to integrate the individual abilities during inference to tackle multimodal tasks.
We show the overall framework in \autoref{fig:LiTA}. 

\subsection{Ability-Specific Fine-tuning}

Previous LVLMs learn the multimodal task-solving capabilities by fine-tuning on visual instructions. In contrast, we propose to learn the task-solving and visual perception abilities separately, using text instructions and image caption data.

%disentangled vision and language capability from multimodal fine-tuning. 

\paragraph{Text Instructions.} We employ text instructions to facilitate the learning of task-solving ability. 
%For models with pre-existing language capability, these instructions help prevent the degradation of their inherent capability during multimodal training. 
Specifically, we first sample instructions from FLAN~\cite{longpre2023flan} and OpenHermes~\cite{OpenHermes2-5}. These datasets encompass a broad range of natural language tasks, including daily dialogue, knowledge utilization, multi-hop reasoning, code synthesis, \etc.
We distill responses to these queries from Qwen-2.5-72B-instruct due to its remarkable performance in multiple real-world tasks.
Additionally, we include 100K text instructions from Magpie-Qwen2.5-Pro~\cite{xu2024magpie}, which are also distilled from Qwen-2.5-72B-instruct.
%Since Qwen-2.5-72B-instruct shares the same architecture and training data with our base LLM Qwen-2.5-7B-instruct, these distilled data can be regarded as replay data~\cite{li2017learning} and better help inherit the task-solving ability for multimodal tasks.
We denote the text instruction dataset as $\mathcal{D}_{\text{text}} = \{q_i, r_i\}_{i=1}^{n_t}$, where $q_i$ and $r_i$ represent the input query and response. 


\paragraph{Image Caption Data.} 
Image caption data has been widely used to improve the cross-modal alignment ability of LVLMs, enabling the models to understand and process visual inputs. We first consider the large-scale caption dataset LAION~\cite{schuhmann2021laion}, which contains a variety of web images, and sample 1M image-caption pairs from it.
%Although these web-collect captions tend to be of low quality, they are easily obtainable and can assist the model for visual perception. 
As these web-collected captions may contain low-quality noisy data, we also collect high-quality captions synthesized by GPT-4V from LLaVAR~\cite{zhang2023llavar}, ShareGPT-4V~\cite{chen2025sharegpt4v}, and ALLaVA~\cite{chen2024allava} to improve the quality of the training data.
%for better vision perception ability. 
Besides, we also collect images from specific domains~(\eg tables, graphs, documents) and caption them based on a strong LVLM, Qwen2-VL-7B~\cite{wang2024qwen2}, to enhance the visual perception ability on these types of data.
%we additionally collect images from these visual domains . 

The details of the collected visual data are presented in \autoref{apdx-train_data}. 
We denote the above caption data set as $\mathcal{D}_{\text{cap}} = \{v_i, r_i\}_{i=1}^{n_c}$, where $v_i$ and $r_i$ represent the image and caption respectively. We follow existing work~\cite{liu2024visual} to convert the caption data into instruction format to align with text instructions.
Specifically, we randomly select a caption query $q$ from a fixed query pool as its instruction. This results in a new caption dataset $\mathcal{D'}_{\text{cap}} = \{v_i, q_i, r_i\}_{i=1}^{n_c}$.

\paragraph{Simple VQA Data.} 
We find that adding a few simple VQA data is able to greatly improve the task performance. 
Thus, we collect a minimal set of simple VQA data for training,  which facilitates the LVLM in acquiring more fine-grained visual knowledge. 
Note that this kind of data is optional for training. We denote models trained with these additional VQA data as ViFT-A.

\paragraph{Training objective.} 
Following previous LVLMs, we leverage an auto-regressive training objective for optimizing the parameters within the connection layer and LLM, denoted as:
%For a LLM parameterized by $\theta$, the objective for caption data is expressed as:
\begin{align}
    \mathcal{L}(\theta) = - \sum_{j=1}^N \log \text{Pr}(r_j|v,q,r_{<j};\theta),
\end{align}
where $N$ is the target sequence length.
For text instructions, the condition of input image $v$ is given as an empty set. 
In this way, we unify the learning objectives of the two kinds of data to support joint training.
In application, due to the significant disparity in token length between captions and text instructions (as the image is converted to a long visual token sequence), we leverage a modality-specific batching strategy to prevent long padding sequences.
By separately batching the text instructions and captions, this approach can accelerate the training process while improving the disentanglement of the two individual abilities.


\subsection{Ability-Fused Inference via Steering Vectors} 

After training, the task-solving and visual perception abilities are well learned. However, they cannot be combined via standard inference. Specifically, the model will elicit each individual ability for different modality inputs, as illustrated in \autoref{apdx-cases}. We opt for the steering vectors~\cite{subramani2022extracting,turner2023activation}, which are latent vectors extracted from the model's hidden space, to address the problem. These vectors are proven to be effective for manipulating the model's behavior~\cite{subramani2022extracting}. More importantly, it enables the combination of different abilities through arithmetic operations, guiding the model to exhibit composite behavioral patterns~\cite{ilharco2022editing}. Consequently, we can activate diverse abilities through different modality inputs, extract their corresponding steering vectors, and then combine them via vector addition.


\paragraph{Extracting Steering Vector.} We focus on the LLM part of the target LVLM as it plays a crucial role in the LVLM's behavior. The LLM consists of a stack of transformer layers. During inference, the input text will be first tokenized to a sequence of tokens $\bm x = [x_1,\ldots,x_n ]$, where $n$ denotes the sequence length. Then, the sequence will be processed through multiple layers, creating intermediate hidden state vectors $\bm h^l(\bm x) = [\bm h^l(x_1),\ldots,\bm h^l(x_n)]$ at layer $l$. Notice that each input token will correspond to a hidden vector. For simplicity, we use $\bm h(\bm x)$ to denote the hidden vectors at all target layers. These hidden vectors will later be used as the steering vectors to manipulate the model's behavior.
% Previous studies~\cite{turner2023activation} discover that the model's behavior mode is closely related to its intermediate hidden states. Thus, we can extract $\bm h(\bm x)$ as the steering vector for manipulating the model's behavior.


\paragraph{Task-Solving Ability Vector.} 
Owing to our design in training, the task-solving ability is mainly learned by text-only instructions. 
Thus, we can utilize the text part of the input visual instruction to elicit the task-solving ability from the LVLM.
%We discover that without image inputs, the model's language capability is activated. As a result, the model can still exhibit normal behavior based on input text instruction. Although it does not produce the correct answer~(since no image is provided), it demonstrates a correct reasoning process. 
% As the text part is not sufficient for fulfilling the multimodal task, we only use its hidden states in LVLMs, to compose the steering vector for this ability.
Although the text part is not sufficient for fulfilling the multimodal task, it can still prompt the model to exhibit the task-solving behavior. Therefore, we aim to extract a steering vector for such ability.
Concretely, we simply use the text instruction $q$ as input, and extract the hidden vectors across all target layers.
Notably, for text-only inputs, the extracting process is the same for LLMs and LVLMs. We denote the extracted vector $\bm{h}(q)$ as the task-solving ability vector.

% and extract the activation vectors across all layers, denoted as $\bm{h}(q)$ for the task-solving ability.


\paragraph{Visual Perception Ability Vector.} 
We additionally utilize the image part of the input visual instruction, to extract the steering vector for the visual perception ability.
Here, we use the text instruction $q$ as the input, and utilize the LVLM to process the input image $v$ and text $q$. The input image and text will be converted to a sequence of tokens.
Next, we extract the hidden states of the text part from all layers, as the steering vector $\bm{h}(v, q)$.
In this way, as the text representations can attend to all image tokens, they have contained the information from the image part. Besides, they will also have the same size as the task-solving ability vector, which does not need further alignment and also supports simple fusing strategies like addition operators.

\paragraph{Ability-Fused Inference.}
After extracting the two ability vectors, we aim to combine them to activate corresponding capabilities for tackling multimodal tasks. 
Here, we devise a simple but effective ability fusion strategy via weighted addition. 
Concretely, given a visual instruction with image $v$ and text instruction $q$, the ability-fused vector is computed as:
\begin{equation}
\begin{split}
    \bm{h'}(v, q)   = \alpha \bm{h}(v, q) +  \beta \bm{h}(q) 
\end{split}
\end{equation}
Here, $\alpha$ and $\beta$ are two tunable weights. 
Given an image $v$ and a text instruction $q$, we first extract the ability vectors $\bm{h}(v, q)$ and $\bm{h}(q)$, and then compute the ability-fused vector $\bm{h'}(v, q)$.
Next, during inference, we replace the hidden representation of the input text tokens with the fused ability vector, and autoregressively generate the output tokens.
The entire generation process requires only one additional forward pass, and we will discuss the associated computational overhead in Section~\ref{sec_anaysis}.


% \begin{algorithm}[t]
% \small
% \caption{Ability-fused Inference.}
% \label{code_af_inference}
% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}

% \Input{A visual instruction $D=(v, q)$, \\ A trained model M.}
% \Output{Generated response $y$.}

% \BlankLine
% \tcp{1. Extract Task-Solving Ability Vector}
% $h(q) \leftarrow \text{ForwardPass}_M( q)$

% \BlankLine
% \tcp{2. Extract Visual Perception Ability Vector}
% $h(v, q) \leftarrow \text{ForwardPass}_M(v, q)$


% \BlankLine
% \tcp{3. Obtain Fused Ability Vector}
% $h'(v, q) = \alpha h(v,q) + \beta h(q)$

% \tcp{4. Autoregressive Generation}
% Replace representation of input text with $h'(v, q)$

% Generating response $p(y|v, t, h'(v, q)) = \prod_{i=1}^Lp(y|v, t, y_{<i}, h'(v, q))$
% \end{algorithm}