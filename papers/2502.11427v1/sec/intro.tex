\section{Introduction}
\label{sec:intro}

% Through integrating visual encoder~\cite{dosovitskiy2020image,radford2021learning} with large language models~(LLMs)~\cite{brown2020language,zhao2023survey}, large vision-language models~(LVLMs)~\cite{liu2024visual,zhu2023minigpt} 
% have gained widespread attention from the research community.
% Benefiting from the remarkable instruction following ability of LLMs~\cite{ouyang2022training} and the visual perception ability of the visual encoder, LVLMs 
% can handle a variety of  multimodal tasks~\cite{fu2024mme,xiong2024llava}, and demonstrate superior performance across diverse real-world applications~\cite{du2022survey,yin2023survey}.

Recently, large vision-language models~(LVLMs), built upon existing visual encoders~\cite{dosovitskiy2020image,radford2021learning} and large language models~(LLMs)~\cite{brown2020language,zhao2023survey}, have gained widespread attention by demonstrating superior performance across diverse multimodal tasks~\cite{du2022survey,yin2023survey}. 

To empower LVLMs with multimodal task-solving capabilities, a fundamental problem is to inherit and transfer the task-solving ability of LLMs into multimodal tasks (with image inputs).
Recently, visual instruction tuning~\cite{liu2024visual,liu2024improved} has emerged as the predominant framework for achieving this goal.
Through fine-tuning on a variety of vision-language
instruction-following data from different sources, LVLMs can directly learn the corresponding knowledge and generalize into other related tasks.

Despite its success, it is still necessary to continue scaling up the number of visual instructions for fully learning multimodal advanced capabilities (\eg visual reasoning).
However, there are two bottlenecks that greatly limit the scaling of visual instructions.
%visual instruction tuning is facing two shortcomings in . 
First, due to the multimodal nature, visual instructions\footnote{Following prior works~\cite{liu2024visual}, we exclude image captions from the scope of visual instructions, as they are designed for basic vision-language alignment, instead of learning advanced multimodal task-solving capabilities.} need to incorporate visual contents (\eg images or videos) and include closely related instructions, which complicates the creation of large-scale visual instructions. Second, although existing work~\cite{liu2024visual,zhu2023minigpt} has adopted the data synthesis strategy for visual instructions, the synthesized instructions might include unreliable information regarding the visual inputs. It also poses challenges and increases the costs for quality control and scaling up. 
%Therefore, it is necessary to rethink how to efficiently learn the advanced multimodal capability
%scaling the visual instruction data is not an affordable and promising solution, to sufficiently learn the advanced . 

Considering the above challenges, we rethink whether it is feasible to reduce the reliance on visual instruction data during training LVLMs. Existing LVLMs typically map visual inputs into the LLM's token space and then generate the text output based on it. If the visual inputs are effectively perceived and aligned with text tokens, the LLM can comprehend the visual contents and leverage its inherent task-solving ability for tackling multimodal tasks. Therefore, the LVLM's multimodal task-solving capability should be the combination of (1) the visual perception ability~(for alignment) and (2) the task-solving ability from LLMs. 
% For example, when presented with a visual math problem, LVLMs should first accurately recognize image content, and subsequently apply their inherent reasoning ability to solve the problem. 
Although it is hard and costly to synthesize extensive amount of high-quality visual instructions for learning the multimodal capabilities, it is promising to sufficiently learn the two individual abilities separately, thanks to the rich resources of natural language instructions~\cite{weifinetuned,OpenHermes2-5} and image caption data~\cite{schuhmann2021laion,chen2024allava}. Therefore, our goal is to \emph{disentangle and separately strengthen} the two individual abilities during training, then \emph{combine them during inference} to enhance LVLMs.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{images/Graph_intro.pdf}
        \caption{A comparison of ViFT with other instruction-tuned LVLM in terms of the training data size and average benchmark performance on MathVista, Mathvision, and MathVerse. ViFT is fine-tuned without any visual instruction data. For ViFT-A, we add 7\% additional simple VQA data.}
    \label{fig:intro_graph}
\end{figure}


In this work, we propose a \textbf{V}isual \textbf{I}nstruction-\textbf{F}ree fine-\textbf{T}uning framework~(ViFT) for training LVLMs.
Concretely, we need to guarantee that the two individual abilities can be independently optimized and extracted from the LVLM, and jointly elicited during inference.
To this end, we only use the image caption and natural language instruction data to jointly fine-tune the LVLM, which enhances the individual abilities by learning the image-to-text and text-to-text relations respectively, leading to less entanglement and interference. 
During inference, we extract the hidden states of the LVLM by using only the image and text parts from the input visual instruction, which are the \emph{steering vectors}~\cite{subramani2022extracting,turner2023activation} corresponding to the two individual abilities.
Through the addition of two steering vectors, the LVLM can benefit from the improvement on the individual abilities and well fulfill multimodal tasks. ViFT does not require any visual instruction data for fine-tuning, which can better inherit the original abilities from LLMs, and avoid the knowledge conflict issue caused by the divergence of visual instructions and language data.

To study the effectiveness of our approach, we conduct extensive experiments on a series of benchmarks. %Without training on any visual instructions, 
Our approach outperforms current state-of-the-art open-source LVLMs on two challenging visual reasoning benchmarks: MathVerse~\cite{zhang2025mathverse} and MathVision~\cite{wang2024measuring}. 
Compared to the best-performing LVLM, LLaVA-OneVision~\cite{li2024llava}, our ViFT achieves significant improvements on MathVerse~(34.8 vs 31.0) and MathVision~(24.0 vs 18.1) benchmarks respectively, while using less than 30\% amount of the training data, as shown in Figure~\ref{fig:intro_graph}.
The primary contributions of this work can be summarized as followed:

\begin{itemize}
    \item To the best of our knowledge, ViFT is the first instruction-free fine-tuning method with comparable performance to SOTA LVLMs.
    \item We specially designed the training and inference methods for disentangling and combining natural language task-solving and visual perception abilities, to efficiently improve the multimodal capabilities of LVLMs.
    \item Our ViFT is a low-cost approach for scaling data to improve LVLMs.
    Experimental results demonstrate the effectiveness of our approach on several benchmarks.
    % \item Based on the proposed framework, we provide a low-cost approach for scaling training data to improve LVLMs, which paves the way for future research.
\end{itemize}

