\documentclass{article}

%%% ready for submission %%%
\usepackage{wrapfig}
% \usepackage{cpal_2025}
%%% to compile a preprint version, e.g., for submission to arXiv, add the [preprint] option %%%
% \usepackage[preprint]{cpal_2025}

%%% to compile a camera-ready version, add the [final] option %%%
\usepackage[final]{cpal_2025}


%add packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{mathrsfs}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based Framework for Effective Label Propagation}


\author{%
  Tao Wen\textsuperscript{1}\thanks{Work performed while at NYU.}, ~Elynn Chen\textsuperscript{2}, ~Yuzhou Chen\textsuperscript{3}, ~Qi Lei\textsuperscript{2} \\
  \textsuperscript{1}Dartmouth College \textsuperscript{2}New York University \textsuperscript{3}University of California, Riverside\\
  \texttt{tao.wen.gr@dartmouth.edu, elynn.chen@nyu.edu, yuzhouc@ucr.edu, ql518@nyu.edu}
}

\begin{document}

\maketitle

\begin{abstract}
Graph Neural Networks (GNNs) have recently become the predominant tools for studying graph data. Despite state-of-the-art performance on graph classification tasks, GNNs are overwhelmingly trained in a single domain under supervision, thus necessitating a prohibitively high demand for labels and resulting in poorly transferable representations. 
To address this challenge, we propose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework to bridge the gap between graph data and traditional domain adaptation methods. It extracts graph topological information holistically with a tensor architecture and then reduces domain discrepancy through label propagation. It is readily compatible with general GNNs and domain adaptation techniques with minimal adjustment through pseudo-labeling. Experiments on various real-world benchmarks show that our LP-TGNN outperforms baselines by a notable margin. We also validate and analyze each component of the proposed framework in the ablation study.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Graph data are ubiquitous in a wide variety of fields, with many objects such as chemical compounds, molecules, and social networks that are naturally represented by graphs~\cite{Morris+2020}. The problem of graph classification~\cite{xu2018powerful,ying2018hierarchical}, which aims to predict the properties of whole graphs, is essential in such fields~\cite{Kojima2020}. In recent years, Graph Neural Networks (GNNs) have served as the predominant tool to tackle this problem~\cite{kipf2017semisupervised, hamilton2017inductive, xu2018powerful, pmlr-v202-eliasof23b, martinkus2023agentbased}. The bulk of GNNs follow the message-passing framework~\cite{pmlr-v70-gilmer17a, kipf2017semisupervised}, which involves propagating and aggregating information of centroid nodes and their topological neighbors. They have consistently achieved superior performance on graph learning tasks, thanks to their adaptability, scalability, and capacity of graph representation learning.

Despite their advances, the overwhelming majority of GNNs are trained in a single domain under supervision~\cite{xu2018powerful, velivckovic2017graph}, thus demanding a sufficient number of labels that are often prohibitively expensive~\cite{hao2020asgn, Kothapalli2023RandomizedSC,pmlr-v202-wu23a}. For example, labeling graphs of gene interactions is extremely expensive due to the need for expert knowledge, complex data analysis, data quality control, and adherence to ethical and privacy regulations. A consequent approach is domain adaptation, i.e. to train a graph classifier that works well on an unlabeled target domain with a labeled source domain.



Although domain adaptation methods have been extensively studied in computer vision~\cite{Wang_Li_Ye_Long_Wang_2019, 9944086, zhong2024bridging, lei2021near,phan2024controllable, csurka2021unsupervised}, such methods for graph classification are challenging to develop due to two fundamental problems: 1) {\em How to effectively extract holistic graph representations?} Though GNNs have demonstrated superior performance in most cases, they are intrinsically unable to fully capture relevant connectivity information, such as higher-order interactions between atoms and ring-ring interactions within a molecule that are essential in drug discovery~\cite{you2018graph,huang2020skipgnn,sun2022does}. In contrast to node or link prediction problems where all samples are from a single graph~\cite{you2023graph}, we are faced with a large number of graphs from distinct feature spaces and domains in graph classification. It is therefore crucial to learn high-quality representations that holistically encompass graph information and are well located in the embedding space for transfer learning, as illustrated in Figure~\ref{fig:GDA}. 2) {\em How to reduce domain discrepancy for graphs?} 
While domain adaptation methods are well-studied in computer vision, their applicability often relies on assumptions such as selection bias (suited for unstructured data) or environmental changes (attributing spurious correlations to factors like background information)~\cite{JMLR:v17:15-239,nguyen2022kl,pmlr-v139-cai21b,lei2021near}. These assumptions, however, are not directly transferable to structured data such as graphs.

\begin{wrapfigure}{r}{0.5\linewidth}
\begin{center}
\centerline{\includegraphics[width=.4\columnwidth]{figures/GDA.png}}
\caption{A visualization of domain adaptation on graph classification. The red line is the decision boundary of an Enzyme versus Non-Eyzyme classifier trained on $S_E$ and $S_{NE}$. The labels can transfer from the source domain to the target domain via methods such as label propagation~\cite{pmlr-v139-cai21b}. For effective label propagation between domains, the intra-class distance is supposed to be smaller than the inter-class distance~\cite{NEURIPS2022_ac112e8f}. For instance, $T_E$ should be closer to $S_E$ than to $S_{NE}$.}
\label{fig:GDA}
\end{center}
\end{wrapfigure}

To address the challenges above, we propose to study the general framework that consists of 1) a graph encoder that extracts holistic graph representation and preserves graph similarities under its geometric distance, thus 2) readily and effectively integrates with certain domain adaptation methods. Under the general framework, we propose a model named \textbf{L}abel-\textbf{P}ropagation \textbf{T}ensor \textbf{G}raph \textbf{N}eural \textbf{N}etwork (LP-TGNN). Specifically, our choice of graph encoder TGNN partially stems from the recent TTG-NN~\cite{pmlr-v238-wen24a} and consists of two branches that learn graph information both locally and globally. On the one hand, a graph convolutional branch follows the message-passing framework and stacks outputs of different graph convolutional network layers~\cite{kipf2017semisupervised} to enhance its expressive power. On the other hand, a topological learning branch combines persistent images from multiple filtration functions of Persistent Homology (PH), thus extracting the graph topological information from multiple perspectives. This design is achievable thanks to a Tensor Transformation layer (TTL) that seamlessly integrates multiple attributes while efficiently preserving the discriminative features by tensor low-rank decomposition. We investigate how different graph embeddings measure graph discrepancies in Euclidean distance with visualization in Figure \ref{fig:encoder}. The scatterplots show that TTG-NN preserves in-group distance better than the expressive GIN~\cite{xu2018powerful}, regardless of the domains. Such findings motivate us to utilize TTG-NN to convert the structured graph information to vector values that general domain adaptation techniques will apply. 

\begin{figure}[ht]
     \centering
     \begin{subfigure}{.3\textwidth}
         \centering
% \hspace{-0.85in}
\includegraphics[width=\textwidth]{figures/GIN_42.jpg}
         \caption{GIN}
         \label{fig:GIN}
     \end{subfigure}
    % \hspace{-0.85in}
     \begin{subfigure}{.312\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/TTGNN_42.jpg}
         % \hspace{-0.8in}
         \caption{TTG-NN}
         \label{fig:TGNN}
     \end{subfigure}
    \caption{An illustration of representations produced by the TTG-NN and the state-of-the-art GIN~\cite{xu2018powerful} on the MUTAG dataset~\cite{Debnath1991-wr}. While both graph encoders separate the three clusters effectively, TTG-NN well preserves the in-group distance with green points ($T_0$) and blue points ($S_0$) and pushes away data from a different group: red points ($S_1$). Embeddings plotted using t-SNE~\cite{JMLR:v9:vandermaaten08a}.}
    \label{fig:encoder}
\end{figure}

A substantial caveat for adversarial-based domain adaptation methods~\cite{10.5555/3326943.3327094,JMLR:v17:15-239} is that they can lose critical discriminative information on the target domain by forcing domain-invariant representations. On the contrary, label propagation~\cite{pmlr-v139-cai21b} reduces domain discrepancy through regularization. It encourages the predictions in a set of neighboring data samples to be stable via consistency regularization. With well-spaced representations, as shown in Figure~\ref{fig:GDA} and~\ref{fig:encoder}, the label information can propagate from the source domain to the target domain. However, it requires well-defined sample distances for the label information to propagate within neighbor sets. While such distances are easy to define in computer vision tasks, they are ambiguous for structured data like graphs. Rather than directly computing such a distance, we instead define the neighboring set of a target graph to be the source graphs whose \textit{ground-truth labels} are the same as the \textit{pseudo-label} of the target graph, where the pseudo-label is given by a multi-layer-perceptron (MLP) classifier.

In conclusion, our method significantly differs from existing domain-adaptive GNNs as it works in the graph space rather than the node space, takes advantage of expressive GNNs and adapts regularization-based domain adaptation methods without stringent mathmatical formulations or forcing invariant representations across domains. Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a framework named LP-TGNN for domain adaptive graph classification that bridges the gap between computer vision domain adaptation methods like label propagation and structured data like graphs. We show that the graph learning ability of TGNN endows its potential to capture the relevance of graphs on the tensor embedding space.
    \item We design a label propagation scheme to reduce domain discrepancy for graphs. As a regularization method rather than a conventional distribution matching method, label propagation performs well with graph data and can be readily integrated with existing GNNs besides TGNN, resulting in a general framework.
    \item Experiments on various graph classification benchmarks demonstrate the effectiveness of our proposed framework and showcase the potential of extending domain adaptation methods such as label propagation to domain adaptive graph classification tasks.
\end{itemize}

\section{Related Work}
\label{sec:rework}

\subsection{Graph Classification}
Graph classification has enormous applications in various fields~\cite{zhou2020graph}. Traditional kernel methods use graph decomposition to capture the similarity in graph sub-structures with specialized kernels. For example, Weisfeiler–Lehman~\cite{shervashidze2011weisfeiler} proposes a family of kernels for large graphs with discrete node labels. Recently, Graph Neural Networks have emerged as a primary tool for graph classification~\cite{xia2021graph}. For instance, Graph Convolutional Network~\cite{kipf2016semi} updates node representations iteratively by their neighboring nodes. GraphSAGE~\cite{hamilton2017inductive} performs inductive learning by aggregating information from local neighborhoods. GAT~\cite{Velickovic2018iclr} uses the attention mechanism to capture dependencies between nodes. To handle large-scale graphs, Top-$K$ pooling~\cite{cangea2018towards, gao2019graph, horn2021topological} filters the nodes by their importance scores. Recently, PersLay~\cite{carriere2020perslay} and RePHINE~\cite{37187175462249e7b5f1073cd94984f2} utilize Persistent Homology to capture graph topological features. Their common limitations are: 1) They fail to exploit the rich semantic and topological graph information holistically from a multi-modal perspective, which is well-addressed in TTG-NN~\cite{pmlr-v238-wen24a}; 2) They overwhelmingly focus on learning from a single domain and in a supervised manner, resulting in poorly transferable representations and prohibitive demand for graph labels. 

\subsection{Domain Adaptation}
Domain adaptation aims to develop models that are transferable from a label-rich source domain to a label-scarce target domain~\cite{10.1007/978-3-642-15561-1_16,pmlr-v37-ganin15}. It has been profoundly investigated for image data with applications including image classification and semantic segmentation~\cite{Wang_Li_Ye_Long_Wang_2019, 9944086, csurka2021unsupervised}, and also explored for question-answering models~\cite{awadalla2022exploring,pmlr-v119-miller20a}. More complex settings such as multi-source~\cite{Ren_2022} and multi-target~\cite{roy2021curriculum} domain adaptation have also been studied. In short, the key to domain adaptation is domain alignment. Traditional statistical methods achieve this by explicitly reducing domain discrepancy via statistics such as maximum mean discrepancy~\cite{tzeng2014deep, pmlr-v37-long15}. Recently, the conventional approaches have been based on adversarial learning~\cite{ajakan2015domainadversarial, 10.5555/3326943.3327094, roy2021curriculum}. These methods typically employ a Gradient Reversal Layer (GRL) to force domain-invariant representations from the feature extractor~\cite{JMLR:v17:15-239} and utilize pseudo-labeling to allow for self-training~\cite{article}. However, while such methods for computer vision have been extensively studied, those for whole graphs are still in the infant stage.

\subsection{Graph Domain Adaptation}
Recently, a few methods have been proposed to address domain adaptation on graphs, mostly for node classification~\cite{pmlr-v202-liu23u, you2023graph, ijcai2019p0606, 10.1145/3366423.3380219, NEURIPS2021_eb55e369}. However, for graph classification, graphs reside in diverse feature spaces instead of a unified space from a single graph as in node classification, leading to a much more challenging problem setting. To tackle this problem, DEAL~\cite{10.1145/3503161.3548012} utilizes adversarial learning for domain alignment and distillation for pseudo-labeling. Furthermore, CoCo~\cite{pmlr-v202-yin23a} advances from adversarial learning to contrastive learning, which consists of coupled branches for graph representation learning and contrasts between branches and domains. DAGRL~\cite{luo2023domainadaptivegraphclassification} uses a similar architecture combined with adaptive adversarial perturbation to align source and target domains. Though the problem definition of domain adaptive graph classification has been established in these works, the problem remains under-explored and demands more effective approaches.

\section{Preliminaries}
\label{sec:pre}

\paragraph{Problem formulation.} Suppose an attributed graph $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{X})$, in which $\mathcal{V}$ is the set of nodes, $\mathcal{E}\in\mathcal{V}\times\mathcal{V}$ is the set of edges, and $\mathbf{X} \in \mathbb{R}^{N \times F}$ is the feature matrix of nodes, where $N=|\mathcal{V}|$ is the number of nodes and $F$ is the dimension of node features. The adjacency matrix $\mathbf{A} \in \mathbb{R}^{N \times N}$ is a symmetric matrix with entries as
$a_{ij} = 1$ if an edge exists between nodes $i$ and $j$ and 0 otherwise. Furthermore, $\mathbf{D}$ is the degree matrix of $\mathbf{A}$ with entries as $d_{ii} = \sum_j a_{ij}$. 

In the setting of domain adaptive graph classification, we are given a source domain $\mathcal{D}^s=\{(\mathcal{G}_i^s,y_i^s)\}_{i=1}^{n_s}$ with $n_s$ labeled samples and a target domain $\mathcal{D}^t=\{\mathcal{G}_j^t\}_{j=1}^{n_t}$ with $n_t$ unlabeled samples. $\mathcal{D}^s$ and $\mathcal{D}^t$ share the same label space $\mathcal{Y}=\{1,2,\cdots,C\}$ with covariate shifts, i.e. $\mathbb{P}_{\mathcal{D}^s}(\mathbf{X})\neq\mathbb{P}_{\mathcal{D}^t}(\mathbf{X})\text{ and }\mathbb{P}_{\mathcal{D}^s}(y|\mathbf{X})=\mathbb{P}_{\mathcal{D}^t}(y|\mathbf{X})$~\cite{SHIMODAIRA2000227}. The goal is to learn a graph classification model on $\mathcal{D}^s,\mathcal{D}^t$ that predicts the ground-truth labels in $\mathcal{D}^t$ accurately.

\paragraph{Tensor low-rank structures.} The Tucker, CP, and Tensor Train (TT) low-rank are the conventional tensor low-rank structures. Specifically, the Tucker low-rank structure is defined by
\begin{equation} \label{eqn:tucker}
    \boldsymbol{\mathcal{X}} = \boldsymbol{\mathcal{C}}\times_1 \boldsymbol{U}_1\times_2\cdots\times_M \boldsymbol{U}_M + \boldsymbol{\mathcal{E}},
\end{equation}
where $\boldsymbol{\mathcal{E}}\in\mathbb{R}^{D_1\times\cdots\times D_M}$ is the noise tensor and $\boldsymbol{\mathcal{C}}\in\mathbb{R}^{R_1\times\cdots\times R_M}$ is the latent core tensor of the true low-rank features, and $\boldsymbol{U}_m,\,m\in [M]$ are the loading matrices. CP low-rank is a special case where the core tensor $\boldsymbol{\mathcal{C}}$ has the same dimensions over all modes. TT low-rank is a different kind of low-rank structure that can compress tensors as significantly as CP low-rank while maintaining a stable calculation as Tucker low-rank. With tensor low-rank structures, the Tensor Transformation Layer tackles model complexity and computation concerns that could arise from amalgamating diverse tensor features while preserving the discriminative features.

\paragraph{Persistent Homology.} 
Persistent Homology (PH) is a method from algebraic topology that studies the shape of data across different scales. Given a scale parameter $\epsilon$, PH builds a sequence of simplicial complexes $\mathcal{G}_{\epsilon_1}, \mathcal{G}_{\epsilon_2}, \dots, \mathcal{G}_{\epsilon_n}$, where each $\mathcal{G}_{\epsilon_i}$ represents the shape at a specific resolution. As $\epsilon$ increases, new features are "born," and some "die" when they merge into larger structures. These changes are recorded in a Persistence Diagram (PD), which is a set of points $(b_\rho, d_\rho)$ in $\mathbb{R}^2$. Each point represents a topological feature $\rho$, where $b_\rho$ is the birth time and $d_\rho$ is the death time. The difference $d_\rho - b_\rho$ is the lifespan of the feature, showing how long it persists across different scales. PD can be further converted to Persistent Image (PI) for computational benefits as detailed in appendix~\ref{app: ph}.

\section{Methodology}
\label{sec:metho}

\subsection{Overview}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=.8\columnwidth]{figures/model.png}}
\caption{The architecture of the proposed LP-TGNN. The graph mini-batches from both domains are input into both branches. The representations of target samples from each branch are pseudo-labeled by an MLP classifier and each branch is separately regularized by label propagation. Also, the representations of source samples from both branches are concatenated and labeled by the MLP classifier. The model is optimized by the supervised loss and label propagation jointly.}
\label{fig:model}
\end{center}
\end{figure}

Figure \ref{fig:model} illustrates the architecture of the proposed LP-TGNN. To address the two fundamental problems in Section \ref{sec:intro}, the model can be decomposed into two components: 1) The backbone encoder of TGNN and 2) The domain alignment method of label propagation. On the one hand, the TGNN holistically encodes graph topological information from a multi-modal perspective through a graph convolutional branch (See Section \ref{sec:gcn}) and a topological learning branch (See Section \ref{sec:top}) with a Tensor Transformation Layer (TTL) (See Section \ref{sec:ttl}). On the other hand, an MLP classifier is used to produce the predicted labels for source samples and pseudo-labels for target samples, bridging the two components. The source samples are used to calculate the supervised loss while the target samples are used to calculate the consistency regularization of label propagation (See Section \ref{sec:lp}).

\begin{algorithm}
\small
   \caption{LP-TGNN}
   \label{alg:all}
   \begin{algorithmic}[1]
       \STATE \textbf{Input:} Source graphs $\mathcal{D}^s$; Target graphs $\mathcal{D}^t$.
       \STATE \textbf{Output:} GCN parameters $\theta$; CNN parameters $\phi$; MLP parameters $\eta$; TTL parameters $\psi$.
       \WHILE{not convergence}
           \STATE Randomly sample mini-batches $\mathcal{B}^s$ and $\mathcal{B}^t$ from $\mathcal{D}^s$ and $\mathcal{D}^t$ respectively;
           \STATE Forward propagate $\mathcal{B}^s$ and $\mathcal{B}^t$ through both branches of TGNN;
            \STATE Forward propagate the representations of $\mathcal{B}^t$ from each branch, i.e. $\mathcal{Z}_{GCN}^t\text{ and }\mathcal{Z}_{Top}^t$ through the MLP classifier separately;
            \STATE Forward propagate the representations of $\mathcal{B}^s$ from both branches, i.e. $\mathcal{Z}^s$ through the MLP classifier;
            \STATE Calculate the loss by \eqref{eqn:final};
            \STATE Update the parameters through backpropagation;
       \ENDWHILE
   \end{algorithmic}
\end{algorithm}

\subsection{Tensor Transformation Layer (TTL)}
\label{sec:ttl}
The Tensor Transformation Layer: $\sigma\left(\mathcal{L}^{(\ell)}\left(\cdot\right)\right) \text{for} \,\ell=\{1,2,\cdots,L\}$ preserves the tensor structures of $\boldsymbol{\mathcal{X}}$ of dimension $D=\prod_{m=1}^M D_m$.
% Let $L$ be any positive integer and $\boldsymbol{d}=\left[d^{(1)}, \cdots, d^{(L+1)}\right]$ be the width of all layers. 
A \textit{deep Tensor Neural Network} is a function in the form of
\begin{equation} \label{eqn:TNN}
    f(\boldsymbol{\mathcal{X}}) = \mathcal{L}^{(L+1)}\circ\sigma\circ\mathcal{L}^{(L)}\circ\sigma\cdots\circ\mathcal{L}^{(2)}\circ\sigma\circ\mathcal{L}^{(1)}(\boldsymbol{\mathcal{X}})
\end{equation}
where $\sigma(\cdot)$ is an element-wise activation function. The linear transformation $\mathcal{L}^{(\ell)}(\cdot)$, and input and output tensor of the $\ell$-th layer, i.e. $\boldsymbol{\mathcal{H}^{(\ell+1)}}$ and $\boldsymbol{\mathcal{H}}^{(\ell)}$ are defined by
\begin{equation} \label{eqn:tri}
\begin{aligned}
\mathcal{L}^{(\ell)}\left({\boldsymbol{\mathcal{H}}^{(\ell)}}\right) := \left\langle{\boldsymbol{\mathcal{W}}^{(\ell)}, \boldsymbol{\mathcal{H}}^{(\ell)}}\right\rangle + \boldsymbol{\mathcal{B}}^{(\ell)}, \\
    \text{and}\quad
    \boldsymbol{\mathcal{H}}^{(\ell+1)} := \sigma\left(\mathcal{L}^{(\ell)}\left({\boldsymbol{\mathcal{H}}^{(\ell)}}\right)\right)
\end{aligned}
\end{equation}
where
$\boldsymbol{\mathcal{H}}^{(0)} = \boldsymbol{\boldsymbol{\mathcal{X}}}$ is the input feature tensor, 
$\left\langle{\cdot, \cdot}\right\rangle$ is the tensor inner product, with a low-rank weight tensor $\boldsymbol{\mathcal{W}}^{(\ell)}$ and a bias tensor $\boldsymbol{\mathcal{B}}^{(\ell)}$. The tensor structure takes effect when we incorporate tensor low-rank structures such as CP, Tucker, and TT. 


\subsection{Graph Convolutional Branch}
\label{sec:gcn}
The convolutional branch learns the representation of $\mathcal{G}$ following the message-passing framework. It utilizes the adjacency matrix $\mathbf{A}$ and feature matrix $\mathbf{X}$ of $\mathcal{G}$ through a sequence of GCN layers~\cite{kipf2017semisupervised}. In each layer, each node representation is updated by combining its representation and the aggregated representations of its neighbors from the previous layer. Formally, the representation of $\mathcal{G}$ at the $\ell$-th layer is given by
\begin{equation}\label{eqn:gcn}
\mathbf{H}_{\mathcal{G}}^{(\ell)}=\sigma\left(\hat{\mathbf{A}}^\tau\mathbf{H}_{\mathcal{G}}^{(\ell-1)}\Theta^{(\ell)}\right)
\end{equation}
where $\hat{\mathbf{A}} = \Tilde{\mathbf{D}}^{-\frac{1}{2}}\Tilde{\mathbf{A}}\Tilde{\mathbf{D}}^{\frac{1}{2}}$, 
$\Tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}$, 
and $\Tilde{\mathbf{D}}$ is the degree matrix of $\Tilde{\mathbf{A}}$. The initial representation is its feature matrix, i.e. $\mathbf{H}_{\mathcal{G}}^{(0)} = \mathbf{X}$, $\mathbf{H}_{\mathcal{G}}^{(\ell)}\in\mathbb{R}^{N\times D^2}\text{ for }\ell\in\{1,2,\cdots,L\}$, $\sigma(\cdot)$ is an activation function, and $\Theta^{(\ell)}$ is a trainable weight of the $\ell$-th layer. The $\tau$-th power of the normalized adjacency matrix contains statistics from the $\tau$-th step of a random walk on $\mathcal{G}$, thus enlarging the receptive field of the convolutional operation. 

In this branch, different $\tau$-th steps of random walk on $\mathcal{G}$ are combined thanks to TTL, thus boosting the representation power of GCN. Specifically, we first concatenate all representations of the $L$-layer branch to form a tensor denoted by $\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{GCN}=\left[\mathbf{H}_{\mathcal{G}}^{(1)},\mathbf{H}_{\mathcal{G}}^{(2)},\cdots,\mathbf{H}_{\mathcal{G}}^{(L)}\right]$, then feed the tensor into TTL as $\boldsymbol{\mathcal{H}}^{(0)}=\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{GCN}$, whose $\ell$-th layer is defined in \eqref{eqn:tri}. Here $\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{GCN}$ is reshaped to a dimension of ${N \times L \times D \times D}$ to facilitate TTL since the output tensors of both branches should have the same number of dimensions (See Section \ref{sec:top}), as we set the size of $\mathbf{H}_{\mathcal{G}}^{(\ell)}$ as ${N\times D^2}$ so that $\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{GCN}$ can be unfolded into four dimensions.

\subsection{Topological Learning Branch}
\label{sec:top}
The topological branch extracts the topological information of a graph $\mathcal{G}$ through Persistent Homology. In detail, to capture the topological information of $\mathcal{G}$, we employ $K$ vertex filtration functions: $f_i: \mathcal{V} \mapsto \mathbb{R}$ \text{for} $i = \{1, \cdots, K\}$. Each filtration function $f_i$ gradually reveals one specific topological structure at different levels of connectivity, such as node degree (degree centrality score), node flow information (betweenness centrality score), information spread capability (closeness centrality score), etc. With each filtration function $f_i$, we can construct a set of $Q$ persistence images (PIs) of resolution $P \times P$ from their persistence diagrams.

We further combine the $Q$ persistence images of resolution $P\times P$ from $K$ distinct filtration functions to form a PI tensor denoted by $\boldsymbol{\mathcal{X}}_{\mathcal{G}}$ of size $K\times Q\times P\times P$, which extracts and preserves the multi-modal topological features concealed in $\mathcal{G}$ simultaneously. Note the PIs for a given $\mathcal{G}$ are deterministic, thus to make the branch learnable, we append a convolutional neural network (CNN) layer and a pooling layer to further process the PI tensor. Formally,
\begin{equation}\label{eqn:top}
    \boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{Top} =
    \begin{cases}
    f_{\text{CNN}}(\boldsymbol{\mathcal{X}}_{\mathcal{G}})\quad\text{if}\;Q=1\\
    \xi_{\text{POOL}}(f_{\text{CNN}}(\boldsymbol{\mathcal{X}}_{\mathcal{G}}))\quad\text{if}\;Q > 1
    \end{cases}
\end{equation}
where $f_{\text{CNN}}$ is a CNN layer and $\xi_{\text{POOL}}$ is a pooling layer such as average pooling or max pooling to ensure a fixed tensor size regardless of the value of $Q$. Similarly, $\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{Top}$ is then fed into TTL as $\boldsymbol{\mathcal{H}}^{(0)}=\boldsymbol{\mathcal{Z}}_{\mathcal{G}}^{Top}$, whose $\ell$-th layer is defined in \eqref{eqn:tri}.

\subsection{Label Propagation}
\label{sec:lp}

To achieve domain adaptive graph classification, we further leverage label propagation~\cite{pmlr-v139-cai21b} to regularize both branches. In general, label propagation spreads trustworthy labels obtained from a source task to neighboring data points (possibly in the target domain) through consistency regularization. The underlying intuition is that \textit{neighboring data} should have similar labels. Therefore, its success heavily depends on an appropriate \textit{distance metric} that defines what neighboring data is exactly, which can be particularly challenging to define for structured data, such as graphs. To avoid this obstacle, for a given target sample $\mathcal{G}_j^t$, we consider its neighbor set to be the \textit{source samples} whose \textit{ground-truth labels} are the same as the \textit{pseudo-label} of that target sample, i.e.
\begin{equation}
    \Pi_j:=\{i|y_i^s=\hat{y}_j^t,\mathcal{G}_i^s\in\mathcal{B}^s\},\;\mathcal{G}_j^t\in\mathcal{B}^t
\end{equation}
where $\hat{y}_j^t=\argmax(q_j^t)$ is its pseudo-label, $q_j^t=\text{MLP}(\boldsymbol{\mathcal{Z}}_j^t)$ is its logits, and $\boldsymbol{\mathcal{Z}}_j^t=\text{Enc}(\mathcal{G}_j^t)$ is its representation from either branch.

Following ~\cite{pmlr-v139-cai21b}, we implement the consistency regularization from FixMatch~\cite{NEURIPS2020_06964dce}, i.e.
\begin{equation}
\label{eqn:reg}
\mathcal{L}_{reg} = \frac{1}{|\mathcal{B}^t|}\sum_{\mathcal{G}_j^t\in\mathcal{B}^t}\mathbbm{1}(\max(q_j^t)>\tau)H(\hat{y}_j^t,q_j^s)  
\end{equation}
where $\tau$ is a threshold to ensure that only target samples with sufficiently confident pseudo-labels are considered. $H(\cdot,\cdot)$ is the cross entropy, and $q_j^s=\text{MLP}({\boldsymbol{\mathcal{Z}}}_j^s)$. Here ${\boldsymbol{\mathcal{Z}}}_j^s$ represents the aggregated representations of its neighbor set, e.g. ${\boldsymbol{\mathcal{Z}}}_j^s=\text{AVG}(\{\boldsymbol{\mathcal{Z}}_i^s\}_{i\in\Pi_j})$, where $\boldsymbol{\mathcal{Z}}_i^s=\text{Enc}(\mathcal{G}_i^s)$. We denote the regularization term from each branch as $\mathcal{L}_{reg}^{GCN}$ and $\mathcal{L}_{reg}^{Top}$ respectively. The two branches are separately regularized as they serve different roles, where the GCN branch plays a major role in graph representation learning, while the other branch provides auxiliary topological information.

For a source sample, we combine its representations from both branches and use it to calculate the supervised loss:
\begin{equation}
\label{eqn:sup}
\mathcal{L}_{sup}=\frac{1}{|\mathcal{B}^s|}\sum_{\mathcal{G}_i^s\in\mathcal{B}^s}H(y_i^s,q_i^s)
\end{equation}

Here $q_i^s$ is its logits. Finally, the supervised loss and the two consistency regularization terms are combined to form the overall objective and jointly optimize the whole framework:
\begin{equation}
\label{eqn:final}
\mathcal{L}=\mathcal{L}_{sup}+\mathcal{L}_{reg}^{GCN}+\mathcal{L}_{reg}^{Top}.
\end{equation}

\section{Experiments}

\subsection{Experiment Setup}
\paragraph{Datasets.} Following ~\cite{pmlr-v202-yin23a}, we tailor various datasets including MUTAG, Mutagenicity, PROTEINS, DD, BZR, and COX2 from the popular graph classification benchmark TUDataset~\cite{Morris+2020} to the setting of domain adaptation. For convenience, P, D, C, CM, B, BM are respectively short for PROTEINS, DD, COX2, COX2\_MD, BZR, BZR\_MD. To construct domain discrepancies, we separate Mutagenicity into four same-size subsets (i.e., M0, M1, M2, M3) by edge density quartiles. The details of the datasets are in appendix~\ref{app: data}.

\paragraph{Baselines.} We validate the performance of the proposed LP-TGNN against various state-of-the-art methods, including one graph kernel method WL Subtree~\cite{JMLR:v12:shervashidze11a}, four graph neural network methods including GCN~\cite{kipf2017semisupervised}, GIN~\cite{xu2018powerful}, CIN~\cite{bodnar2021weisfeiler}, GMT~\cite{baek2021accurate}, four domain adaptative image classification methods including CDAN~\cite{10.5555/3326943.3327094}, ToAlign~\cite{wei2021toalign}, MetaAlign~\cite{Wei_2021_CVPR}, DUA~\cite{Mirza_2022_CVPR}, and two domain adaptative graph classification methods including DEAL~\cite{10.1145/3503161.3548012} and CoCo~\cite{pmlr-v202-yin23a}. Their details are in appendix~\ref{app: bl}.

\paragraph{Implementation details.} We conduct our experiments on one NVIDIA Quadro RTX 8000 GPU card with up to 48GB of memory. The LP-TGNN is trained by the Adam optimizer with a learning rate in \{0.01,0.05,0.1\}. The hidden dimension and batch size are both set as 32. All activation functions are set as ReLU. We employ a three-layer GCN in the convolutional branch. For the topological branch, we use four filtration functions, i.e. degree, betweenness, eigenvector, and closeness, set $Q$ in \eqref{eqn:top} as 1 and the PI dimension as $50\times50$. We follow~\cite{pmlr-v238-wen24a} for the choices of tensor low-rank structures. We use the CNN as a 2D convolutional layer followed by a max-pooling layer. The classifier is a two-layer MLP with batch normalization and a dropout rate of 0.5. The threshold for label propagation, i.e. $\tau$ in Eq~\eqref{eqn:reg} is set as 0.8. We train the model on all the labeled source samples and all the unlabeled target samples, and evaluate on the ground-truth labels of target samples that are hidden in training by accuracy as in~\cite{10.1145/3366423.3380219}. The code is available at \url{https://github.com/TaoWen0309/Label-Propagation-GNN}.

\subsection{Performance Comparison}

From Table~\ref{tbl:methods-comparison}, we observe that 1) LP-TGNN achieves the best performance overall, also LP-GIN reaches the same average accuracy with CoCo and outperforms DEAL; 2) On average, vanilla domain adaptation methods outperform supervised graph learning methods even as they were not developed for graphs, which further demonstrates the low transferability of current graph learning methods. To sum up, our framework tackles domain adaptive graph classification tasks for two key reasons: (i) The high-quality representations produced by GIN and TGNN which are essential for label propagation, and (ii) the efficacy of label propagation in reducing domain discrepancies while maintaining discriminative information, without the strenuous need to define a strict distance between graph samples or a contrived GNN architecture.

\begin{table}[ht]
\caption{The classification accuracy (in \%) on PROTEINS, COX2,
and BZR (source$\rightarrow$target).}
\label{tbl:methods-comparison}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\columnwidth}{!}{
\begin{tabular}{lccccccr}
\toprule
\textbf{Methods} & P$\rightarrow$D & D$\rightarrow$P & C$\rightarrow$CM & CM$\rightarrow$C & B$\rightarrow$BM & BM$\rightarrow$B & Avg.\\
\midrule
WL Subtree & 72.9 & 41.1 & 48.8 & 78.2 & 51.3 & 78.8 & 61.9 \\
GCN & 58.7 & 59.6 & 51.1 & 78.2 & 51.3 & 71.2 & 61.7 \\
GIN & 61.3 & 56.8 & 51.2 & 78.2 & 48.7 & 78.8 & 62.5 \\
CIN & 62.1 & 59.7 & 57.4 & 61.5 & 54.2 & 72.6 & 61.3 \\
GMT & 62.7 & 59.6 & 51.2 & 72.2 & 52.8 & 71.3 & 61.6 \\
CDAN & 59.7 & 64.5 & 59.4 & 78.2 & 57.2 & 78.8 & 66.3 \\
ToAlign & 62.6 & 64.7 & 51.2 & 78.2 & 58.4 & 78.7 & 65.7 \\
MetaAlign & 60.3 & 64.7 & 51.0 & 77.5 & 53.6 & 78.5 & 64.3 \\
DUA & 61.3 & 56.9 & 51.3 & 69.5 & 56.4 & 70.2 & 60.9 \\
\midrule
DEAL & \textbf{76.2} & 63.6 & 62.0 & 78.2 & 58.5 & 78.8 & 69.6 \\
CoCo & 74.6 & 67.0 & 61.1 & {\bf 79.0} & 62.7 & 78.8 & 70.5 \\
\midrule
LP-GIN & 64.0 & \textbf{71.2} & 59.1 & 78.2 & \textbf{69.3} & \textbf{81.0} & 70.5 \\ 
LP-TGNN & 73.6 & 69.2 & \textbf{63.4} & {78.2} & 67.6 & 78.8 & \textbf{71.8} \\ 
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table}


\subsection{Ablation Study}
To validate the effectiveness of each component in LP-TGNN, we design the ablation experiments on Mutagenicity as shown in Table~\ref{tbl:ablation}. Specifically, 1) LP-TGNN/TOPO removes the topological learning branch; 2) LP-TGNN/CONV removes the graph convolutional branch; 3) LP-TGNN/SUP removes the supervised loss $\mathcal{L}_{sup}$, and 4) LP-TGNN/LP removes the consistency regularization $\mathcal{L}_{reg}^{GCN}$ and $\mathcal{L}_{reg}^{Top}$. Moreover, we compare the results against the state-of-the-art GIN~\cite{xu2018powerful} by applying distribution matching and label propagation to GIN, denoted by DM-GIN and LP-GIN respectively. We also add the results of label propagation with GAT~\cite{Velickovic2018iclr} and GraphSAGE~\cite{hamilton2017inductive}, i.e. LP-GAT and LP-SAGE.

We have the following observations from Table~\ref{tbl:ablation}: 1) By comparing DM-GIN and LP-GIN, it is evident that label propagation significantly outperforms distribution matching in this task. Therefore, it is inappropriate to directly apply adversarial-based domain adaptation techniques to graphs. 2) By comparing LP-TGNN/TOPO and LP-TGNN/CONV, we can further verify the different roles of the two branches. In other words, the representation learning of the convolutional branch is crucial, while the topological learning of the other branch provides supplementary information. 3) By comparing LP-TGNN/SUP and LP-TGNN/LP, we can claim that both supervised loss and consistency regularization are indispensable. More interestingly, for either method, the results on the same target domain are identical for any source domain (except LP-TGNN/SUP on M2$\rightarrow$M1), indicating that the framework would lose its transferability without either of the terms. 4) LP-GAT, -GraphSAGE, -GIN have all obtained good performance on several single tasks, while LP-TGNN achieves the best performance overall and ascends especially when M3 is involved, also LP-GIN reaches competitive average accuracy, demonstrating the efficacy of our proposed general framework.

\begin{table}[ht]
\caption{The ablation results (in \%) on Mutagenicity (source$\rightarrow$target).}
\label{tbl:ablation}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccccccccccr}
\toprule
\textbf{Methods} & M0$\rightarrow$M1 & M1$\rightarrow$M0 & M0$\rightarrow$M2 & M2$\rightarrow$M0 & M0$\rightarrow$M3 & M3$\rightarrow$M0 & M1$\rightarrow$M2 & M2$\rightarrow$M1 & M1$\rightarrow$M3 & M3$\rightarrow$M1 & M2$\rightarrow$M3 & M3$\rightarrow$M2 & Avg.\\
\midrule
DM-GIN & 74.6 & 50.3 & 63.3 & 50.3 & 54.8 & 46.3 & 67.0 & 71.8 & 49.8 & 68.6 & 52.8 & 63.4 & 59.4\\
LP-GIN & 76.0 & 69.3 & 69.6 & 70.0 & 60.2 & \textbf{67.9} & \textbf{75.9} & \textbf{81.6} & 63.7 & 61.3 & 62.3 & 67.7 & 68.8\\
LP-GAT & 75.4 & 70.2 & 71.2 & \textbf{70.8} & 58.8 & 59.3 & 74.5 & 77.4 & 57.8 & 48.3 & 57.6 & 56.7 & 64.8\\
LP-SAGE & 76.5 & 69.6 & \textbf{71.9} & 69.6 & 60.9 & 60.2 & 74.6 & 78.4 & 57.6 & 57.7 & 62.3 & 66.7 & 67.2\\
LP-TGNN/Topo & 74.7 & 47.7 & 71.3 & 51.0 & 56.5 & 58.6 & 63.5 & 74.8 & 56.4 & 70.0 & 62.2 & 69.2 & 63.0\\
LP-TGNN/Conv & 32.6 & 61.1 & 37.3 & 55.3 & 53.1 & 55.2 & 65.8 & 72.8 & 53.1 & 32.6 & 53.1 & 62.7 & 52.9\\
LP-TGNN/Sup & 67.4 & 55.2 & 37.3 & 55.2 & 53.1 & 55.2 & 37.3 & 32.6 & 53.1 & 67.4 & 53.1 & 37.3 & 50.4\\
LP-TGNN/LP & 67.4 & 44.8 & 62.7 & 44.8 & 46.9 & 44.8 & 62.7 & 67.4 & 46.9 & 67.4 & 46.9 & 62.7 & 55.5\\
LP-TGNN & \textbf{76.6} & \textbf{71.9} & 70.8 & 69.7 & \textbf{62.9} & 65.2 & 75.7 & 77.1 & \textbf{64.9} & \textbf{71.5 }& \textbf{64.2} & \textbf{72.8} & \textbf{70.3}\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table}

\subsection{Synthetic Study}
\label{exp:syn}
The label propagation method was rigorously proved to perform well in computer vision domain adaptation with subpopulation shift~\cite{pmlr-v139-cai21b}. To validate this property in the graph space, we design a synthetic study on MUTAG. Specifically, the negative and positive sample size ratio is approximated to be 1:2 in the source domain and 2:1 in the target domain, constituting a significant subpopulation shift. The performance of the GIN and TGNN methods is listed in Table~\ref{tbl:synthetic}, from which we observe that for the simulated subpopulation shift task: 1) LP-GIN significantly outperforms DM-GIN, achieving competitive performance; 2) Both terms of supervised loss and consistency regularization are important; 3) The topological branch is more pronounced compared with that in Table~\ref{tbl:ablation}.
% To be precise, the numbers of 0 and 1 samples in the source domain are 31 and 62, and 32 and 15 in the target domain, respectively.

%\begin{table}[ht]
\begin{wraptable}{r}{0.4\textwidth}
\vspace{-1.4cm}
\caption{The classification accuracy (in \%) on the simulated subpopulation shift task.}
\label{tbl:synthetic}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.3\columnwidth}{!}{
\begin{tabular}{lr}
\toprule
\textbf{Methods} & \textbf{Accuracy}\\
\midrule
DM-GIN & 78.7\\
LP-GIN & 89.4\\
LP-TGNN/Topo & 78.7\\
LP-TGNN/Conv & 80.9\\
LP-TGNN/Sup & 68.1\\
LP-TGNN/LP & 89.4\\
LP-TGNN & \textbf{93.6}\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{wraptable}
%\end{table}

\subsection{Quality of Pseudo-labels}
Below we show the quality of pseudo-labels on the Mutagenicity tasks. We report the accuracy for the convolutional branch as it plays a major role in representation learning, and the pseudo-labels are filtered by the confidence threshold first as in~\eqref{eqn:reg}. We can observe that on average, there exists a positive correlation between pseudo-label and classification accuracy, which further demonstrates the quality of pseudo-labels as the bridge between graph representations and label propagation.

\begin{table}[ht]
\caption{The pseudo-label and classification accuracy (in\%) on Mutagenicity (source$\rightarrow$target).}
\label{tbl:qua_pl}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Accuracy} & M0$\rightarrow$M1 & M1$\rightarrow$M0 & M0$\rightarrow$M2 & M2$\rightarrow$M0 & M0$\rightarrow$M3 & M3$\rightarrow$M0 & M1$\rightarrow$M2 & M2$\rightarrow$M1 & M1$\rightarrow$M3 & M3$\rightarrow$M1 & M2$\rightarrow$M3 & M3$\rightarrow$M2 \\
\midrule
\textbf{Pseudo-label} & 93.3  & 81.3  & 94.5  & 83.4  & 67.6  & 80.6  & 88.3  & 91.1  & 67.6  & 85.7  & 68.6  & 86.0 \\
\textbf{Classification} & 76.6  & 71.9  & 70.8  & 69.7  & 62.9  & 65.2  & 75.7  & 77.1  & 64.9  & 71.5  & 64.2  & 72.8 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table}

\section{Conclusion}
This work addresses the domain adaptative graph classification problem by proposing a framework named LP-TGNN. Extensive experiment results on various benchmarks validate the efficacy of the proposed LP-TGNN. Besides, the competitive performance of LP-GIN and other baselines showcases the general applicability of our framework. Our work serves as a solid step toward transferable GNNs, as most GNNs so far are trained under full supervision in a single domain. Future work may extend the approach to large-scale datasets and more complex settings, such as multi-label classification and multi-target domain adaptation.

\section*{Acknowledgments}
This material is based upon work supported by the U.S. Department of Energy,
Office of Science Energy Earthshot Initiative as part of the project ``Learning reduced models under extreme data conditions for design and rapid decision-making in complex systems" under Award
\#DE-SC0024721.

This work was supported in part by the National Science Foundation under Grant DMS-2412577. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

% Essentially, it consists of a graph convolutional branch and a topological learning branch that holistically captures graph topological information, respectively through graph convolution and Persistent Homology with a tensor architecture. The branches are regularized by label propagation, which enables them to produce transferable representations through consistency regularization. The label propagation is adapted to work on graphs so that a strict definition of sample distances is not required. 

% Two non-trivial problems remain in our work: 1) Why does distribution matching fail for graph classification? Does there remain an optimal set of optimization strategies and hyperparameters to be discovered, or can we develop a theory to justify this phenomenon? 2) Why are the representations of TGNN suitable for graph label propagation? A theory for this could be complicated as it involves various learning methods (e.g. GNN, PH, and label propagation) and data types (e.g. graphs and tensors), but it would be interesting to study a simplified case. 

\bibliography{reference}

\appendix
\section{Appendix}

\subsection{Tensor-Augmented Neural Networks.}
Tensor learning has experienced remarkable advancement in recent years, spanning diverse areas including statistical modeling, projection methods, and identification techniques \cite{chen2023statistical,lin2021projection,liu2022identification,chen2024semi,chen2024time,zhang2024computation,li2023mev,chen2024data,chen2024dynamic}. Significant breakthroughs have emerged in community detection, constrained optimization, and transfer learning \cite{chen2023community,chen2019constrained,chen2020modeling,chen2022modeling,chen2022transfer}, alongside innovations in reinforcement learning and distributed computing \cite{chen2024reinforcement,chen2024distributed,chen2024advancing}. Further developments have enhanced high-dimensional tensor classification, stochastic optimization, and statistical inference \cite{chen2024high1,chen2024high2,chen2025stochastic,xu2025statistical}.

In parallel, neural networks have demonstrated exceptional capabilities in processing high-dimensional data, though their traditional implementations often treat tensors merely as computational conveniences rather than exploiting their rich statistical properties. A significant advancement came when \cite{cohen2016expressive} established fundamental theoretical connections between deep neural architectures and hierarchical tensor factorizations, bridging these two domains.

This convergence of tensor learning and neural networks has catalyzed numerous innovations. \cite{kossaifi2017tcl} and \cite{kossaifi2020trl} introduced tensor contraction layers and regression layers that achieved more efficient parameter usage while maintaining model performance. Recent developments like the Graph Tensor Network \cite{xu2023graph} and Tensor-view Topological Graph Neural Network \cite{wen2024tensorview} have broken new ground in processing complex, multi-dimensional data at scale. The field has also seen significant progress in uncertainty quantification and representation learning through various works \cite{wu2024tensor,wu2024conditional1,wu2024conditional2,kong2024teaformers}. However, a key challenge remains: developing tensor-augmented transformer architectures specifically optimized for multi-dimensional time series analysis.

\subsection{Persistent Images}\label{app: ph}

\begin{definition}[Persistence Image]
Let $g: \mathbb{R}^2 \mapsto \mathbb{R}$ be a non-negative weight function for the persistence plane $\mathbb{R}$. The value of each pixel $z \in \mathbb{R}^2$ is defined as ${PI}_{{Dg}}(z) = \iint\limits_{z} \sum_{\mu \in T({Dg})} \frac{g(\mu)}{2 \pi \delta_{x} \delta_{y}} e^{-\left(\frac{\left(x-\mu_{x}\right)^2}{2\delta_{x}^{2}}+\frac{\left(y-\mu_{y}\right)^2}{2\delta_{y}^{2}}\right)} d y d x,$ where $T(\text{Dg})$ is the transformation of the PD $\text{Dg}$ (i.e., for each $(x,y)$, $T(x,y) = (x, y-x)$), $\mu = (\mu_x, \mu_y) \in \mathbb{R}^2$, and $\delta_x$ and $\delta_y$ are the standard deviations of a differentiable probability distribution in the $x$ and $y$ directions respectively. 
\end{definition}

We can calculate a set of Persistent Diagrams (PDs) for each filtration function $f_i$, i.e., $\text{PH}(\mathcal{G}, f_i) = \overrightarrow{Dg}_i = \{{Dg}^{(1)}_i, \dots, {Dg}^{(\mathcal{Q})}_i\}$, where $\mathcal{Q} \in \mathbb{Z}_0^+$ is the number of graph topological features. 
Moreover, to encode the above topological information presented in a ${Dg}$ into the embedding space, we use its vectorized representation, i.e., persistence image (PI)~\cite{adams2017persistence}. The PI is a finite-dimensional vector representation obtained through a weighted kernel density function and can be computed in the following two steps. First, we map the PD ${Dg}$ to an integrable function $\varrho_{{Dg}}: \mathbb{R}^{2} \mapsto \mathbb{R}^{2}$, which is referred to as a persistence surface. The persistence surface $\varrho_{{Dg}}$ is constructed by summing weighted Gaussian kernels centered at each point in ${Dg}$. In the second step, we integrate the persistence surface $\varrho_{{Dg}}$ over each grid box to obtain the value of the ${PI}_{Dg}$. 

\subsection{Details of Datasets}\label{app: data}
\begin{itemize}
    \item MUTAG~\cite{Debnath1991-wr} contains 188 nitroaromatic compounds. The goal is to predict whether a molecule has a mutagenic effect on a given bacterium or not.
    \item Mutagenicity~\cite{Kazius2005-ba} consists of 4337 molecular structures with their corresponding Ames test data. The goal is to predict whether a molecule is a mutagen or not.
    \item PROTEINS: PROTEINS~\cite{DOBSON2003771} and DD~\cite{JMLR:v12:shervashidze11a} contain 1113 and 1178 proteins, where each label indicates whether a protein is an enzyme or not. We denote them in short as P and D respectively.
    \item COX2: COX2 and COX2\_MD~\cite{Sutherland2003-tg} are both chemical compounds that consist of 467 and 303 cyclooxygenase-2 inhibitors. We denote them in short as C and CM respectively.
    \item BZR: BZR and BZR\_MD~\cite{Sutherland2003-tg} are both chemical compounds that consist of  405 and 306 ligands for the benzodiazepine receptor. We denote them in short as B and BM respectively.
\end{itemize}

\subsection{Details of Baselines}\label{app: bl}
\begin{enumerate}
    \item Graph learning methods:
    \begin{itemize}
        \item WL Subtree~\cite{JMLR:v12:shervashidze11a} presents a family of efficient graph kernels using the Weisfeiler-Lehman test to measure the similarity of graphs.
        \item GCN~\cite{kipf2017semisupervised} follows the message-passing framework to update node representations iteratively with neighboring nodes.
        \item GIN~\cite{xu2018powerful} is a state-of-the-art message-passing neural network with increased expressivity by MLPs.
        \item CIN~\cite{bodnar2021weisfeiler} extends message-passing Simplicial Networks to regular Cell Complexes and increases the expressivity.
        \item GMT~\cite{baek2021accurate} is based on multi-head attention and captures the interaction between nodes according to their structural dependencies.
    \end{itemize}
    \item Domain alignment methods:
    \begin{itemize}
        \item CDAN~\cite{10.5555/3326943.3327094} proposes an adversarial learning framework and conditions on the discriminative information from classifier predictions.  
        \item ToAlign~\cite{wei2021toalign} decomposes source domain features into task-related features for alignment and task-irrelevant features to be avoided, based on classification meta-knowledge.
        \item MetaAlign~\cite{Wei_2021_CVPR} treats the optimization of domain alignment and classification tasks as meta-train and meta-test tasks, maximizing the inner product of their gradients during training.
        \item DUA~\cite{Mirza_2022_CVPR} proposes an effective and efficient normalization strategy by continuously adapting batch normalization layers.
    \end{itemize}
    \item Domain adaptive graph classification methods:
    \begin{itemize}
        \item DEAL~\cite{10.1145/3503161.3548012} utilizes adversarial learning and adaptive perturbation for domain alignment and distillation for pseudo-labeling.
        \item CoCo~\cite{pmlr-v202-yin23a} consists of coupled branches for graph representation learning and contrastive learning between branches and domains.
    \end{itemize}
\end{enumerate}

\subsection{Run-time analysis}

To show the computational complexity of LP-TGNN, the running time for the LP-TGNN experiments in Table ~\ref{tbl:ablation} is given as follows:

\begin{table}[ht]
\caption{The running time per epoch (in seconds) of LP-TGNN in Table ~\ref{tbl:ablation}.}
\label{tbl:runtime}
\begin{center}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccccccc}
\toprule
M0$\rightarrow$M1 & M1$\rightarrow$M0 & M0$\rightarrow$M2 & M2$\rightarrow$M0 & M0$\rightarrow$M3 & M3$\rightarrow$M0 & M1$\rightarrow$M2 & M2$\rightarrow$M1 & M1$\rightarrow$M3 & M3$\rightarrow$M1 & M2$\rightarrow$M3 & M3$\rightarrow$M2 \\
\midrule
5.37 & 4.81 & 5.27 & 4.90 & 4.95 & 4.49 & 4.68 & 4.61 & 4.34 & 4.38 & 4.55 & 4.48 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{center}
\end{table}

\end{document}