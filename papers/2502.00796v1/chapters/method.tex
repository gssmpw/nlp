% \section{Method}
% \label{sec:method}

% \dani{I think the previous section and this one should be merged. They are short enough, and yet there's repetition of definitions. The previous section is the higher-level overview that this section should open with, and then proceed to the more detailed descriptions without repeating the high level ideas again. Or have two parts, each part begins with a high-level overview and then goes into the details.}

% In this section, we outline our approach, starting with the overall setting and then detailing the design of our adapters. We first introduce \oursFull, a novel fine-tuning setting that leverages $F$ for new domain-specific tasks by solely modifying its input and output spaces, while only $0.4\%$ parameters are learned. Specifically, we add two external learnable modules at the model's input and output, preserving the original backbone $F$ even during inference. \ours transforms the original model's function $F(x)$ into $BF(Ax)$, where $A$ and $B$ are \emph{lightweight adapters} (linear operators), as opposed to modifying the function $F$ directly. We initialize $A$ and $B$ as the identity function to match $F(x)=BF(Ax)$. The input layer $A$ learns to transform the model's input into a representation that better aligns with task-specific requirements, while the output layer $B$ applies a simple linear transformation to the model's output. In the visual domain we suggest linear CNN. For the textual input space, we draw inspiration from previous work \citep{prefix_tunning,prompt_tunning,fluffy_niv} and train new textual tokens for the text encoder. However, unlike these methods, we find that optimizing just two tokens is sufficient, with only one of them concatenated to the original prompt. \dani{so it's not clear what the second token does, this was explained earlier in the intro, but this is the method section, I think it is better to explain it here} As a result, our approach requires only one extra token per prompt. This approach is particularly valuable for text encoders with limited context length (\eg, CLIP, which is limited to a total of 77 tokens).

% While the DarkGray-box setting requires no modifications to the foundation model, a provider can introduce additional entry points by exposing the entry dimensionality and propagating gradients. We explore this adaptable setting, termed LightGray-box, where modifications to the model's intermediate layers are allowed, while still keeping the foundation model's weights fixed and hidden. Specifically, we optimize a set of learnable tokens injected into $F$'s middle transformer layers, influencing attention scores without altering weights. Although this approach accesses the model's internal streams, it preserves the architecture and weight parameters, retaining the advantages of a gray-box setting. Both DarkGray and LightGray-box settings offer a range of lightweight fine-tuning options while maintaining the integrity of the original backbone model.

% \subsection{Adapters}
% \Cref{fig:adapters} provides an overview of our input adapters. In this section, we describe the architecture of our input adapters for both text and image modalities, as well as the design of the output adapter applied to the model's output features.

% \textbf{Visual Input Adapter:} For image inputs, the visual adapter consists of learned 2D convolutional layers. These layers preserve the original dimensions of the input image. Since no activation function is included, the visual encoder can be described as an affine function on the image pixel space. As we observe later (in \Cref{sec:ablation}), this simplified visual encoder is sufficient for modifying the input for our purposes, and adding non-linear activations does not provide additional benefits.

% \textbf{Textual Input Adapter:} For text inputs, we optimize two new tokens for the textual encoder: the \textit{extra} token and the \textit{shift} token. The extra token is a learned token that is attached to the original input sequence. Since positional encoding is not applied to it, this token can be flexibly inserted at any position within the input sequence. The shift token is a learned token that is added pairwise to the original input tokens (see Fig. \ref{fig:adapters}), effectively `shifting' them within the token embedding space. Thus, the textual input adapter consists of two new learned tokens that modify the original textual tokens. 

% \textbf{Output Adapters:} These adapters are applied to the model’s output, which is a feature vector. For both image and text modalities, we implement the output adapters as a simple linear layer on top of the feature vector space, similar to the approach used in linear probing \citep{DinoV2,clip}.
% \begin{figure}[ht]
% 	\centering
% 	%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
% 	\includegraphics[width=0.7\linewidth]{figures/adapters.pdf}
% 	\caption{An overview of our Input Adapters. }
% 	\label{fig:adapters}
% \end{figure}

\section{Method}
\label{sec:method}

In this section, we introduce our approach for fine-tuning a pretrained model $F$ (\eg, foundation models CLIP, BLIP) for new domain-specific tasks without exposing its architecture or modifying its weights. We propose two fine-tuning settings, termed \emph{DarkGray-box} and \emph{LightGray-box} settings, both of which offer lightweight fine-tuning options, and leverage the pre-trained backbone model $F$ while handling the \WB.

\subsection{Gray-box Settings}

\textbf{DarkGray-box:} In this setting, the internals of $F$ are completely hidden, akin to a black-box approach. The only exposed components are the \emph{input} and \emph{output adapters}, which are external trainable modules plugged into the input and output of the backbone model. To train the input adapter, this setting requires access to the gradients computed by back-propagation through the backbone model. This means that a gradient tensor corresponding to the final layer of the input adapter is exposed --- hence the term \emph{DarkGray} instead of \emph{Black}. Importantly, the backbone model's architecture and weights remain hidden, and only the adapters are trained. Our approach learns only a minimal number of parameters (approximately $0.4\%$ of the total model parameters). In this context, we address two types of input modalities: images and text.

\textbf{LightGray-box:} In this more relaxed setting, the provider introduces additional entry points where task-dependent information can be injected into the model's intermediate layers. This enables better adaptation to a domain-specific task, enhancing flexibility without compromising the advantages of the gray-box model setup. Specifically, we optimize a set of learnable tokens injected into the transformer layers of $F$, thereby influencing attention scores without accessing or modifying the weights or layers. Although this approach accesses the model's internal data paths, it preserves the internal architecture and weights hidden, retaining the advantages of a gray-box setting. It is important to note that while the model layers remain hidden, this setting requires access to their input tokens, and allowing gradients to propagate through them.

\subsection{Adapters}
In this section, we outline a simple solution for the settings discussed above.
Our \oursFull setting transforms the original model's function $F(x)$ into $B\circ F\circ A (x)$, where $A$ and $B$ are \emph{lightweight adapters} (linear operators), as opposed to modifying the function $F$ directly. We initialize $A$ and $B$ as the identity function to match $F(x)=B\circ F\circ A (x)$. The input adapter $A$ learns to transform the model's input into a representation that better aligns with task-specific requirements, while the output adapter $B$ applies a simple linear transformation to the model's output.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/adapters.pdf}
	\caption{An overview of our Input Adapters. The visual input adapter (left) consists of 2D task-specific convolutional layers that preserve the image's original size. The textual input adapter (right) includes two task-specific tokens: a ``shift'' token added to the original sequence tokens and an ``extra'' token appended to the original sequence as a contextual token. Both adapters transform the original input into a new representation that better aligns with the pre-trained backbone model.}
	\label{fig:adapters}
\end{figure}

\Cref{fig:adapters} provides an overview of our input adapters. Below, we describe the architecture of our input adapters for both text and image modalities, as well as the output adapter applied to the model's output features.

\textbf{Visual Input Adapter:} For image inputs, the visual adapter consists of learned 2D convolutional layers that preserve the original dimensions of the input image. Since no activation function is included, the visual adapter functions as an affine transformation on the image pixel space. As we observe later (in \Cref{sec:ablation}), this simplified visual adapter is sufficient for modifying the input for our purposes, and adding non-linear activations does not provide additional benefits.

\textbf{Textual Input Adapter:} For text inputs, we draw inspiration from previous works~\citep{prefix_tunning,prompt_tunning,prompt_tunning2} and train new textual tokens for the text encoder. However, unlike these methods, we find that optimizing just two tokens—the \textit{extra} token and the \textit{shift} token—is sufficient. The \emph{extra} token is a learned token that is attached to the original input sequence. Due to the transformer's positional invariance \citep{vaswani2017attentionisallyouneed}, and the fact that positional encoding is not applied to this token, it can be flexibly inserted at any position within the input sequence. The \emph{shift} token is another learned token that is added to each of the original input tokens, effectively ``shifting'' them within the token embedding space.
Thus, this approach requires only one extra token per prompt, which is particularly valuable for text encoders with limited context length (\eg CLIP, which is limited to a total of 77 tokens). 

\textbf{Intermediate Inputs:} In the LightGray-box setting, we enhance adaptability by injecting learnable, task-specific tokens into each transformer layer of the backbone model $F$. While the model layers remain hidden and fixed, these tokens influence the output of each layer by modifying the attention scores. This approach effectively extends the concept of prompt tuning \citep{prompt_tunning2} to both visual and textual encoders across multiple tasks.

\textbf{Output Adapters:} These adapters are applied to the model's output feature vector. For both image and text modalities, we implement the output adapters as simple linear layer on top of the feature vector space, similar to the linear probing approach \citep{DinoV2,clip}.


