\section{Evaluation}
\label{sec:evaluation}

We evaluate \ours and \oursp across multiple tasks and benchmarks using various backbones, including CLIP-ViT-B/16, BLIP-B, and DINOv2-B. We compare their performance against the original model in the ``Zero-Shot'' (ZS) setting as a reference point (serving as a lower bound) and also against the Black-box Linear Probing (LP) baseline. Additionally, we compare them with three strong white-box alternatives that serve as upper bounds: Full Fine-Tuning (FT), Last Layers Fine-Tuning (LLFT), and LoRA, as discussed in \Cref{sec:introduction,sec:related_work}.
Although FT involves the largest number of parameters, it often underperforms compared to lightweight approaches (\eg LoRA, \ours) when the available training samples are insufficient for certain domains or tasks. Note that LLFT involves direct access to model layers, which places it in the white-box category. In \Cref{sec:further_eval} we conduct further evaluations on Text-To-Image diffusion, LLM and VLM backbones, for image generation, language understanding and image captioning, and also on CNN backbones.
%\sout{Since the \ours training paradigm is independent of the backbone architecture, we further demonstrate its performance with CNN backbones in Appendix A}.
For full implementation details, please refer to \Cref{sec:further_implementation_details}. %\rami{. We also conducted additional experiments on image captioning and general language understanding using different backbones. Specifically, we apply our LGA approach on (1) a
% VLLM (BLIP-2, 3.7B) for Image Captioning and (2) an LLM (DeBERTa-v3-base)
% for GLUE benchmark (on the MRPC dataset). Finally, we also show results on text to image generative model based on xxx diffusion model. Results can be found in the table xxx in Appnedix xxx.}


\subsection{Text-to-Image Retrieval}
\input{tables/blip_coco_flickr_maple}
\Cref{tab:blip_coco} presents a comparison for fine-tuning BLIP on two Text-to-Image Retrieval benchmarks: COCO and Flickr30K. We observe that the LLFT baseline dominates in both datasets. LoRA, serving as a White-box upper bound, follows closely, while our Gray-box \ours shows a significant improvement with respect to zero-shot, and competitive performance to LoRA, with a recall@1 gap of only 0.30 points on COCO and 1.7 points on Flickr30K. Notably, \ours significantly improves over the ZS baseline, with a recall@1 increase of 6.14 points on COCO and 5.2 points on Flickr30K. \oursp slightly improves \ours results by allowing multiple entries to the model's intermediate layers. 
\input{tables/sub_coco_maple}
% \input{tables/subcoco_clip}

To further evaluate \ours and \oursp on specific image domains, we created 12 distinct subsets of the COCO dataset using available human annotations to identify objects present in the images. Each subset includes all photos containing a specific element (\eg, table, sky, sea) from both the training and test splits. \Cref{tab:sub_coco_blip} presents the results using the BLIP backbone. Notably, \ours consistently outperforms the ZS and LP baselines across all subsets, demonstrating the effectiveness of modifying the model's inputs and outputs. Additionally, the results demonstrate that \oursp consistently outperforms \ours, emphasizing the advantages and flexibility of this more permissive configuration, which enables learning intermediate parameters/tokens.
Interestingly, LoRA outperforms Full Fine-Tuning (FT) in most cases but is itself outperformed by the LLFT baseline, highlighting the influence of the number of optimized parameters relative to the dataset size. Our Gray-box approaches, \ours and \oursp, together achieve top-2 performance in 58.33\% (21/36) of cases, underscoring their competitive potential.

\input{tables/stanford_cars_maple}
Next, we conduct an experiment on the domain-specific Stanford-Cars dataset \citep{stanford_cars} as a retrieval task, which contains car images annotated by Make, Model, and Year (\eg, ``{\it 2012 Tesla Model S or 2012 BMW M3 Coupe}''). \Cref{tab:stanford_cars} presents a Precision@K comparison using the BLIP backbone. Across all metrics, \ours and \oursp significantly outperform both the ZS reference and the white-box baselines. Notably, the LoRA baseline underperforms compared to our methods, even though it still shows improvement over the ZS baseline. We attribute this phenomenon to the relatively low number of samples and specific vehicle descriptions (197) in the dataset, making adaptation in the input space more efficient. This suggests that the input adapter's flexibility offers an advantage in such cases. However, this trend is not consistent across all scenarios, as it may vary depending on the backbone model and the dataset used for training.

\subsection{Text-to-Video Retrieval}
\input{tables/blip_videos_maple}

\Cref{tab:blip_video} presents the results of Text-to-Video Retrieval on two benchmarks: MSR-VTT and VATEX. For this task, we follow a previous approach \cite{BLIP} that applies Text-Image foundation models at the frame level for video tasks. Following the established protocol, we uniformly sample 12 frames from each video and perform Text-to-Image Retrieval on the sampled frames. On both benchmarks, \ours achieves results comparable to the LoRA baseline (\eg, R@1 of 37.24\% with \ours vs. 37.72\% with LoRA), which performs best on MSR-VTT, with a recall@1 gap of less than one point and a difference of 1 to 2 points at higher recall@k levels. Moreover, \ours significantly outperforms the ZS reference, with a Recall@1 improvement of 5.1 points on MSR-VTT and 9.7 points on VATEX. It is notable that the white-box Full Fine-Tuning method outperforms all alternatives on VATEX but surpasses only the zero-shot and linear probing baselines on MSR-VTT. We attribute this to the combination of a high number of trainable parameters and the varying sizes of the training sets, with 26k videos in VATEX compared to only 7k in MSR-VTT. Evidently, the Last Layers Fine-Tuning baseline, with fewer trainable parameters, achieves better results than Full Fine-Tuning on the MSR-VTT dataset.
%%%%%%%%%%%
\subsection{Image Classification}
\input{tables/classification}
We further evaluate our approach on the Image Classification task using two benchmarks with a CLIP ViT-B/16 backbone, as shown in \Cref{tab:classification}. The first classification task on ImageNet-1K \citep{ImageNet} while the second is sketch-domain classification on ImageNet-Sketch \citep{ImageNet_Sketch}.
For ImageNet-1K, we perform 16-shot training, sampling 16 images per class from the training set. \ours achieves a 3.9-point improvement in top-1 accuracy over the zero-shot baseline, while on the cross-domain ImageNet-Sketch, it gains a 13.1-point increase. However, LoRA outperforms \ours with a 2.52-point lead on ImageNet-1K and an 8.98-point lead on ImageNet-Sketch.
%%%%%%%%%%%%%%%%%
\subsection{Sketch-to-Image Retrieval}
\input{tables/dino_sketchy}
Here we explore Instance Sketch-to-Image Retrieval experiment on the Sketchy dataset \citep{SketchyDatabase}. This dataset includes natural images paired with corresponding human-drawn sketches. The goal is to retrieve {\it the exact original image} based on a given sketch (not just the class). For this task, we utilized the DinoV2 backbone, which has previously demonstrated strong image feature learning capabilities \citep{DinoV2}. Notably, this backbone was trained on natural images, resulting in poor performance in the zero-shot setting, as shown in \Cref{tab:sketchy}. Nonetheless, \ours achieves substantial improvement over the zero-shot baseline while keeping the backbone frozen and modifying only the input and output adapters. However, as this task involves adapting to a domain quite different from the original training domain, white-box methods like LoRA, Full FT, and LLF significantly outperform our approach due to their ability to modify model weights. Additionally, \oursp, which can adjust internal attention scores, also outperforms \ours and LP by a large margin.
%\sout{These results suggest that for distinct domains, frozen backbones have limitations, and achieving optimal performance requires recalculating more refined internal features.}\rebuttal{
These results together with ImageNet-Sketch suggest that in cross-domain settings, the model requires more substantial internal modifications, which limits the performance of the gray-box approach compared to white-box methods.
% \rami{Add pointers to results }. 

% \rami{Comparing to MaPLe \cite{} our results demonstrate that our LGA, operating at the same level of information exposure as MaPLe,consistently outperforms it across several benchmarks}

% \rami{The results on image Captioning and LLM (DeBERTa-v3-base), shown in Appendix xxx demonstrate that our approach performs effectively in new contexts, reinforcing its applicability to a variety of foundational models and tasks.}