\section{Summary and Discussion}
\label{sec:summary}
In this paper, we addressed the vulnerabilities of conventional (white-box) fine-tuning of large pretrained models, which often lead to excessive duplication and storage costs during deployment, reduced optimization flexibility on edge devices, and risks related to privacy, safety, and IP violations for the model provider. To mitigate these issues, we introduced two novel paradigms. The first, the DarkGray-box setting, that keeps the model layers and weights concealed, allowing adapters to operate only on the model's input and output. The second, the LightGray-box setting, that offers limited access to the model's internal structure, enabling modifications to attention layers without exposing the model's weights. While \oursp is tailored for transformer-based architectures, our proposed \ours approach, which employs only input and output adapters, is applicable to a wide range of foundation models, including CNN-based architecture (as demonstrated in \cref{sec:further_eval}), single and multimodal foundation models such as DinoV2, CLIP and BLIP, as well as generative diffusion and LLM models. This generality allows our approach to adapt effectively across various downstream tasks and domains. However, our experiments indicate that this form of adaptation is less effective for more distant domains (\eg sketch top image), where modifying the model's internal weights becomes more essential. Despite this, our method demonstrates robustness and adaptability, achieving results that are often competitive with, and sometimes surpass, white-box alternatives.
%In this paper, we addressed the challenges of fine-tuning large pre-trained models (optionally foundation models) by introducing two novel approaches. The first, the DarkGray-box setting, keeps the model layers and weights concealed, allowing adapters to operate only on the model's input and output. The second, the LightGray-box setting, offers limited access to the model's internal structure, enabling modifications to attention scores without exposing the model's weights. We also discussed the risks of model theft, where it is possible to create a similarly performing model using output features, though this process can be very expensive in terms of time and computational resources. However, if a modelâ€™s structure and weights are fully exposed, it becomes easy for others to violate intellectual property (IP) rights and use it without incurring any cost or effort, which undermines the original investment. While \oursp is tailored for transformer-based architectures, our proposed \ours approach, which employs only input and output adapters, is applicable to a wide range of foundation models, including CNN-based architectures (as demonstrated in \cref{sec:further_eval}), such as CLIP and other multi-modal models. This generality allows our approach to adapt effectively across various downstream tasks and domains. However, our experiments indicate that this form of adaptation is less effective for more distant domains (\eg sketch), where modifying the model's weights becomes essential. Despite this, our method demonstrates robustness and adaptability, achieving results that are often competitive with, and sometimes surpass, white-box alternatives.