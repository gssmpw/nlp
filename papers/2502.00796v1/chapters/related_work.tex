\section{Related Work}
\label{sec:related_work}


{\bf Prefix and Prompt Tuning} \citep{prompt_tunning,prompt_tunning2,prefix_tunning} are methods proposed as lightweight alternatives to full fine-tuning for Large Language Models (LLMs). Instead of modifying all model parameters, these methods optimize a new set of input tokens for each NLP task. Prompt Tuning \citep{prompt_tunning} focuses on optimizing a token sequence added to the first transformer's layer, while Prefix Tuning \citep{prefix_tunning} and Prompt Tuning 2 \citep{prompt_tunning2} propose optimizing a separate sequence added to each transformer layer. Due to unstable optimization when directly training prefix tokens, the Prefix-Tuning approach \citep{prefix_tunning} trains a matrix $P$, which is projected through a trainable MLP layer to compute the prefix added to the existing prompt input. Prefix-Tuning involves learning separate prefixes for both the encoder and decoder components of the LLM, inserted appropriately during inference. Depending on the task, these methods have proven effective with prefixes ranging from 10 to 200 learned tokens, along with their associated MLP layer. In this work, we simplify this approach by directly optimizing just two tokens for a single text encoder without additional components. Specifically, we use the first token as an attached prefix and the second as a ``shift'' token added to all original input tokens. Consequently, our approach increases the promptâ€™s context length by only a single token per prompt or task, which is particularly valuable for text encoders with limited context length (\eg CLIP, which is limited to 77 tokens in total). 

{\bf Low-Rank Adaptation (LoRA)} \citep{lora} was initially proposed as an effective lightweight alternative to full fine-tuning for transformer-based large language models (LLMs), and later to Vision Transformers \citep{ViT,lora-vit}. Instead of updating all model parameters, LoRA learns two $n \times r$ matrices that are multiplied to form an $n \times n$ matrix of a low rank $r$, where $r$ is a hyper-parameter. The low-rank matrix is then added to the original model's matrix. LoRA has demonstrated competitive results with full fine-tuning while being significantly more parameter-efficient. However, LoRA requires prior knowledge of the model's architecture to choose the appropriate layers, match exact dimensions, and determine the matrix ranks. For example, in transformer layers, the $Q$, $K$, and $V$ matrices across multiple layers have been shown to be effective choices for applying LoRA. Additionally, if the learned components are stored separately from the original model, LoRA necessitates a different computational flow in inference, altering the intermediate features by applying these new components. Although LoRA could be considered a ``gray-box'' approach due to the ability to hide the original model's weights, recent work \citep{horwitz2024recovering} demonstrated methods to effectively reconstruct the original model's weights using LoRA fine-tuned models, making it more accurately associated with a ``white-box'' framework. Furthermore, LoRA requires custom implementations for different architectures, which have been developed for a variety of structures (\eg, linear, Conv2D, embeddings). In contrast, \ours assumes no access to the model weights, no prior knowledge of internal layers, and does not require selecting any hyper-parameters. Our approach relies solely on the gradient flow through the original model and preserves the model's original structure, maintaining the inference pipeline intact between the input and output across all tasks and domains.

{\bf Co-CoOp and MaPLe} A different lightweight fine-tuning approach is Co-CoOp \citep{Co-CoOp}, a CLIP-based architecture designed to enhance the integration of visual and textual modalities for image classification. Co-CoOp concatenates the visual encoder with the textual encoder, inserting a learned network between them. It processes the image feature vector through a learned MLP, generating a fix number of visual tokens that are added as a prefix to the textual input of the text encoder. \ie this approach conditions the textual input in the visual output. Although Co-CoOp keeps CLIP frozen, this design requires both modalities during each inference, limiting the generation of non-conditioned textual feature vectors, an essential capability for tasks like Image Retrieval where query (text) and images (gallery) are encoded separately. Similarly, MaPLe \citep{maple} further improves upon Co-CoOp by learning shared vectors projected into different layers of the CLIP textual and visual encoders, using learnable MLP network. MaPLe can be seen as an extension of Prefix-Tuning \citep{prefix_tunning} for classification tasks, freezing the model and allowing internal tokens to be learned, which respects the ``LightGray-box'' framework. We adapt a different version of this approach to our new tasks, where indepedent vectors are learned for each layer with no shared layers that significantly increase the number of learned parameters. We  refer to this light-weight approach as \oursp, in this paper.


{\bf Model thievery} has been extensively studied in the context of machine learning models \citep{stealing_ml_models,stealing_bert}, particularly neural networks. \cite{cont_steal} introduced a learning approach to replicate a pre-trained transformer encoder by constructing a similar-performing encoder based on the original model's output features. \cite{model_reconstruction} presented techniques for reconstructing model weights, given the specific architecture of a two-layer MLP and the propagated gradients. Similarly, \cite{horwitz2024recovering} successfully recovered original transformer weights from LoRA fine-tuned versions of the model, while \cite{greybox_mlp_attack} proposed a method to recover the weights of a (private) linear classification head using its (public) backbone feature extractor. 

In this context, the potential theft of model weights not only poses a risk of model misuse \citep{foundation_model_risks}, but also raises further concerns, as \cite{Reconstructing_Training_Data} demonstrated a method for recovering training data samples from the model's weights. In this work, we propose a fine-tuning framework that minimizes the risk of exposing model weights, aligning with the findings of current research. 
Importantly, while recovering an arbitrary model's architecture and weights solely from input gradients is not yet practical, we do not assess the immunity of the Dark or LightGray-box concepts, leaving this for future research.

In summary, ``White-box'' and ``LightGray-box'' methods have been explored in NLP and classification tasks by incorporating additional components or tokens into the model's intermediate layers. While input adapters have been studied in the context of LLMs, their application in the image domain has not been thoroughly investigated, as we do in this paper. We extend this exploration through our \oursp approach, which draws inspiration from these methods, and further develop a more restrictive \ours approach that preserves the original pretrained model's computational flow. 
% \rami{Further discussion on applicability of some recent studies can be found in Appendix D.}