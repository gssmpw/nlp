\section{Ablation Study}
\label{sec:ablation}
In this section, we explore several key components of \ours and \oursp. We start by analyzing the impact of each adapter on the overall performance, then we examine the individual contributions of the adapter's components.
\input{tables/ablations_clip}

{\bf Impact of Input/Output Adapters}: 
We start by demonstrating the contribution of each adapter in our \ours approach, both individually and in combination, using the model in zero-shot (ZS) mode as a reference baseline. For each configuration, we train on the COCO \citep{COCO} dataset and report the results on its 5k validation set. Our findings indicate that each input and output (I/O) adapter, for both image and text modalities, individually improves the overall performance, as shown in \Cref{tab:ablation_clip}. In the first three rows, we examine the influence of the input adapters for both modalities (denoted by the ``\ours-I'' prefix). Each adapter enhances overall performance, and combining both input adapters leads to a 5.72-point gain in Recall@1, demonstrating the effectiveness of modifying the input space of $F$. Next, we investigate the impact of applying output adapters (denoted by the ``\ours-O'' prefix) on both the visual and text modalities, which establish a stronger baseline by modifying the output (feature) space of $F$. Adding output adapters to both modalities further improves performance by an additional 5.74 points over the input adapters, forming the complete \ours configuration. We then test the mutual influence of both input and output (I/O) adapters in isolation for each modality (shown in the ``\ours-Text/Vis'' rows). Our findings indicate that combining both I/O adapters for a single modality branch yields better performance than using them separately. Finally, the last row shows the best performance achieved by \ours when all input and output adapters are applied to both the text and image branches. This comprehensive setup consistently outperforms configurations where adapters are applied in isolation or partially, confirming that jointly optimizing all adapters delivers the most significant improvements. These experiments highlight that leveraging both input and output spaces together results in the most effective adaptation of the foundation model $F$ for downstream tasks.


{\bf Textual Input Adapter Tokens:}
We evaluate the contribution of each learned token in \ours, specifically the \emph{shift} and \emph{extra} tokens, as shown in \Cref{tab:ablations_shift_extra}. Both improve lower recall metrics (R@1 and R@5), while the shift token has minimal impact on higher recall metrics. We also examine using multiple extra tokens, \ie inserting more than one learned token into the prompt, as detailed in \Cref{sec:further_ablations}. Results show that optimizing multiple extra tokens does not consistently outperform a single token and reduces the encoder's effective context length (77 tokens in CLIP). Further ablations on proxy token count and layer selection in \oursp are in \Cref{sec:further_ablations}.
% \rami{I think we should move all this section to Appendix.} \rami{Point/discuss/mention your experiment with CNN backbone that you show in Appendix B.}