\section{Introduction}
\label{sec:introduction}
The recent surge in the development of foundation models \citep{clip,BLIP,BLIP2,DinoV2,segment_anything} has significantly advanced a wide range of downstream tasks, achieving state-of-the-art (SoTA) performance across various domains. These models are typically deployed as pre-trained backbones and fine-tuned to adapt to specific domains or tasks. Common fine-tuning approaches include: 1) Full fine-tuning \citep{BERT,ViT}, where all model parameters are updated; 2) Partial tuning, which adjusts only a subset of parameters, often in the model's final layers \citep{rcnn,ViT}; and 3) Integrating adapter modules  \citep{residual_adapters,lora} into the model's layers. 
However, adapting large foundation models for multiple diverse sub-tasks through these conventional methods introduces several significant limitations (which we refer to as ``\WB'' below):
%\rami{I suggest to call it white-box limitations or pitfalls.}


\begin{enumerate}
    \item {\bf Duplication of deployment and storage:} 
    Large foundation models are costly to share and serve, and deploying a dedicated fine-tuned version for each downstream task exacerbates this burden. Managing multiple models not only increases storage and deployment complexity but also reduces efficiency, as demonstrated for LLMs \citep{efficiently_scaling_transformer_inference,prompt_tunning}.

    \item {\bf Optimization for edge devices:} Adapting foundation models for deployment on edge devices requires careful optimization based on their weights and architecture \citep{post_training_prunning,fast_post_training_prunning}. Fine-tuning models by modifying their parameters often demands repeated optimization for each device, making the process resource-intensive and inefficient, particularly for large-scale deployments.

    \item {\bf Privacy, safety, and intellectual property (IP) concerns:} Granting full access to a model's layers and weights raises risks related to IP protection, safety, and privacy. Exposing weights can lead to unauthorized use \citep{gpt4}, or to recovery of sensitive training data \citep{Reconstructing_Training_Data}. Moreover, it has been shown \citep{horwitz2024recovering} that LoRA \citep{lora} fine-tuned models can be vulnerable to attacks capable of reconstructing the original model's weights and performance.


\end{enumerate}

In this paper, we mitigate these \WB by introducing a family of \emph{Gray-box} fine-tuning techniques that keep the foundation model's weights and layers fixed and hidden. Conventional \emph{White-box} techniques allow full access to the pre-trained backbone architecture and weights, but are inherently limited by the challenges mentioned above. In contrast, \emph{Black-box} methods restricting access to only the model's input and output, resulting in significant performance constraints. The Gray-box approach offers a middle ground, exposing limited information about the model, which enables it to effectively address these challenges. Specifically, we consider a scenario where the provider of the backbone model offers one or more entry points to the pre-trained model (\eg, the original input entry or intermediate layer entries). While keeping the weights and layers hidden, each entry point reveals: (1) the dimensionality of the layer at that entry point, and (2) the gradients of the (application-dependent) loss with respect to the entry point inputs.

This Gray-box setup has practical applications in real-world scenarios. For example, in hospital models used for medical image analysis, where patient data privacy and regulatory compliance are critical, the model owner might want to allow third parties to adapt the model for specific diagnoses without exposing sensitive data or the model’s proprietary structure and weights \citep{federated_learning}. While Federated Learning (FL) also prioritizes privacy, it primarily focuses on data privacy by distributing training across nodes. However, FL typically requires access to the model architecture to ensure consistency across clients. In contrast, our Gray-box framework focuses on secure task adaptation while keeping both the architecture and weights hidden.

Similarly, in persona-based models used for personalization tasks (\eg, personalized recommendations or identity verification), fine-tuning may be required without revealing personal data or the full model architecture \citep{person_reid}. Additionally, foundation model providers may wish to offer adaptation capabilities to third parties while keeping the core model architecture and weights concealed to protect intellectual property and prevent misuse. This approach allows adaptation for specific domains or tasks while minimizing the risks associated with full model exposure.

\begin{figure*}[t]
	\centering
	%   \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=0.8\textwidth]{figures/arc2.pdf}
	\caption{An overview of our gray-box frameworks. {\bf Left:} \oursFull permits modifications only at the input and output levels while keeping the backbone model hidden and frozen. The only information available is the gradient flow (indicated by the orange-dotted arrow), which matches the shape of the last layer of the input adapter. {\bf Right:} In contrast, LighGray-box (\oursp) allows additional entry points into the model's intermediate layers, exposing slightly more information, such as the input dimensionality and the gradients of a subset of the layers.}
	\label{fig:arc}
    \vspace{-2mm}
\end{figure*}


We explore two variants of the Gray-box framework: one that permits multiple entry points (thus exposing more model information) and another that restricts access to only the original input entry. We refer to these variants as \emph{LightGray-box} and \emph{DarkGray-box}, respectively, where the shade reflects the level of information exposed to the user during fine-tuning. \Cref{fig:arc} demonstrates these settings, which offer flexible, efficient, and more secure solutions to the challenges outlined above by leveraging a pre-trained foundation model while keeping it fixed and concealed. In the following sections, we detail how our framework effectively addresses real-world challenges, enabling model adaptation with minimal exposure or modification, and demonstrating the practicality of our Gray-box approaches.

A common Black-box approach to adapting an existing foundation model involves training additional layers on top of its output features \citep{clip,BERT,he2022masked,DinoV2}. However, this method relies solely on the information provided by the model's output features, missing the opportunity to leverage the foundation model's computational power for further adaptation. Our Gray-box framework addresses this limitation by allowing modifications to the input or the injection of ``middleware'' features, as discussed in this paper, thereby unlocking more effective fine-tuning potential.
\Cref{tab:boxes} summarizes the benefits and requirements of these fine-tuning approaches. 

\input{tables/boxes}
% \matan{Add short comment about generalization - not for aLL MODELS ON EARTH}
% \rebuttal{As evaluation, we compare our methods to four main fine-tuning alternatives: 1) Full fine-tuning, 2) Last Layers fine-tuning, 3) Lightweight LoRA \citep{lora} adapter, and 4) Black-box Linear Probing. The first two approaches require access to part or all of the original weights and are thus classified as white-box methods. We evaluate our methods across diverse tasks and backbones, considering LoRA and full fine-tuning as performance upper bounds. Despite this, our \oursFull approach achieves competitive results, particularly in retrieval tasks (\eg, Text-to-Image and Text-to-Video Retrieval benchmarks), as well as in domains less aligned with the backbone’s original training, such as Sketch-to-Image Retrieval and Image Classification.}
As evaluation, we compare our methods to four main fine-tuning alternatives: 1) Full fine-tuning, 2) Last Layers fine-tuning, 3) Lightweight LoRA \citep{lora} adapter, and 4) Black-box Linear Probing. The first two approaches require access to part or all of the original weights and are thus classified as white-box methods. We evaluate our methods across diverse tasks and backbones, considering LoRA and full fine-tuning as performance upper bounds. Our \oursFull approach achieves competitive results, particularly in retrieval tasks (\eg, Text-to-Image and Text-to-Video Retrieval benchmarks), as well as in domains less aligned with the backbone’s original training, such as Sketch-to-Image Retrieval and Image Classification. While we do not claim that our methods generalize to all possible models and tasks, our evaluations demonstrate their adaptability and practical utility across a variety of domains and architectures.

% As evaluation, we compare our methods to four main fine-tuning alternatives: 1) Full fine-tuning, 2) Last Layers fine-tuning, 3) Light weight LoRA \citep{lora} adapter, and 4) Black-box Linear Probing. Note that the first two approaches require access to part or all of the original weights and are thus classified as white-box methods. We evaluate our methods across various tasks, domains, and backbones. We consider LoRA and full fine-tuning as the performance upper bounds in our comparisons, given their ability to modify the original model parameters. Despite this, our \oursFull approach achieves results that are on par with, or competitive to LoRA across several tasks such as Text-to-Image Retrieval and Text-to-Video Retrieval benchmarks (\eg, MSR-VTT \citep{msrvtt}, VATEX \citep{vatex}, COCO \citep{COCO}, Flickr30K \citep{flickr30k}). We further evaluate our approach on tasks and domains that are more distant from the original backbone's focus, such as Sketch-to-Image Retrieval \citep{SketchyDatabase} and Image/Sketch Classification \citep{ImageNet,ImageNet_Sketch}. 


We summarize our contributions as follows:
\begin{itemize}
    \item We introduce a new paradigm for effectively re-using pre-trained models, enabling their adaptation to new domains and tasks while balancing effectiveness, proprietary protection, safety, and efficiency, exploring various options along this spectrum.
    
    \item We propose two Gray-box frameworks, \ours and \oursp, which leverage a pre-trained model while keeping it intact and frozen, allowing only limited access. Our novel \oursFull framework adapts the model for new domain-specific tasks by modifying only its input and output spaces, which was not explored enough in the visual domain.
    
    \item We conduct an extensive study to assess the capabilities of input and output adapters, both individually and in combination, providing deeper insights into their roles and effectiveness.
    
    \item We demonstrate the effectiveness of our Gray-box approaches across various tasks and benchmarks, achieving results that are competitive with, or on par with, White-box baselines, all while keeping the pretrained/foundation model sealed.
\end{itemize}

