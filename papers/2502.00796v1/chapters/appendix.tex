{\huge Appendix}

This appendix provides additional details on our methods, experiments, and findings. We begin with further evaluations, including experiments on diffusion, LLM and CNN-based backbones, in \Cref{sec:further_eval}. In \Cref{sec:further_ablations}, we conduct ablation studies on the number of input tokens in \ours and the choice of layers in \oursp. \Cref{sec:visualizations} presents visualizations of the visual input adapter, offering insights into its transformations. \Cref{sec:further_discussion} expands on recent developments in black-box prompt optimization and their limitations, along with a comparative analysis of task adaptation using input/output adapters. Finally, \Cref{sec:further_implementation_details} details our experimental setup, including training configurations, hyperparameters, and model specifications.

% In this appendix, we provide comprehensive details about the methods, experiments, and findings discussed in the main paper. We begin with additional evaluations, including experiments on CNN-based backbones, presented in \Cref{sec:further_eval}. Next, in \Cref{sec:further_ablations}, we perform further ablation studies, exploring the number of input tokens for the input adapter in \ours and the choice of layers in \oursp. We also include visualizations of the visual input adapter's operation in \Cref{sec:visualizations}, offering insights into how it transforms input images. Furthermore, in \Cref{sec:further_discussion}, we delve into an extended discussion on recent advancements in black-box prompt optimization and their limitations, as well as a comparison of task/domain handling schemes using input/output adapters. Finally, \Cref{sec:further_implementation_details} provides the complete implementation details of our experiments, including training setups, hyperparameters, and model configurations.


\section{Further Evaluation}
\label{sec:further_eval}
In this section, we conduct further evaluations on more tasks and backbones.

We extend our \oursp approach to additional tasks across various backbones. We refrain from conducting full fine-tuning or last-layer fine-tuning due to resource constraints or pipeline incompatibilities (\eg concatenation of multiple models).

\input{tables/diffusion}
{\bf Text-To-Image Generation:} We fine-tune a DiT-based diffusion model \cite{pixart-alpha} on the RSCID \cite{rscid} dataset, which consists of image-text pairs of satellite imagery—a domain previously shown to be underrepresented in web-scraped data \cite{clip}. Similar to other transformer-based tasks, we apply \oursp entry points to the denoiser's attention layers. \Cref{tab:diffusion} presents results for both LoRA and our \oursp approach, evaluating the generated images against the held-out test set using FID, LPIPS distance, and prompt adherence via the CLIP score. We observe a significant distribution shift between the fine-tuned models and the original, which was primarily trained to generate ``natural'' or ``artistic'' images. \Cref{fig:diffusion} shows visual examples of generated images using multiple prompts, demonstrating that the fine-tuned models produce satellite imagery, which the original model is less likely to generate correctly.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/diffusion.pdf}
	\caption{Generated images by three different model versions, of Original (zero-shot), LoRA and \oursp.}
	\label{fig:diffusion}
\end{figure*}

{\bf Image Captioning:}
We fine-tune the BLIP-2 \cite{BLIP2} backbone, using \oursp, for the image captioning task. \Cref{tab:image_captioning} presents results comparable to LoRA fine-tuning. The BLIP-2 backbone employs an image encoder followed by a Q-Former, which translates the prompt,including image tokens, into the token space of a frozen LLM. In this case, we were unable to optimize our \ours paradigm solely in the input space. The results indicate that our LGA achieves performance comparable to LoRA improving over the Zero-Shot.
\input{tables/image_captioning}


{\bf General Language Understanding Evaluation:}
\input{tables/GLUE}
We fine-tune DeBERTa-v3-base LLM on the MRPC dataset, using \oursp. Results are shown in \Cref{tab:GLUE} indicate again the LGA capability in finetuning to a new task even slightly outperforming LoRA.

\section{Further Ablation Study}
\label{sec:further_ablations}
In this section, we present additional ablation studies on the components of \ours and \oursp.
\input{tables/ablations_input_tokens}
\Cref{tab:ablations_input_tokens} shows the ablation study on the number of input tokens optimized for the text encoder, with BLIP backbone. As observed, the optimal number of tokens lies between 1 and 8. However, it is not entirely clear which number is definitively optimal, as some metrics improve at the expense of others. For example, optimizing 2 tokens yields higher Recall@1 results compared to optimizing 1 token, but results in a lower Recall@5. Nevertheless, the differences across all token numbers are minimal, making their performance nearly on par. Consequently, we choose to optimize only 1 token to preserve the text-encoder context length from being occupied by these ``proxy'' tokens.

{\bf CNN backbone:} Here we evaluate \ours on the following CLIP CNN-based models: CLIP-RN101, CLIP-RN50, CLIP-RN50x4, and CLIP-RN50x16. \Cref{tab:ablations_clip_cnn} presents the results on the COCO 5k validation set. Our DarkGray-box approach consistently improves upon the zero-shot (ZS) baseline across all backbones, although it remains inferior to the White-box Full Fine-Tuning (FT) baseline. 
\input{tables/ablations_clip_cnn}
We evaluate only these three approaches since these backbones are based on CNN architectures. While it is theoretically possible to apply LoRA to these CNN-based models, it is not straightforward due to the need to carefully select layers and adapt LoRA’s implementation to CNN layers. Additionally, \oursp is specifically tailored to transformer encoder architectures, making it unsuitable for these CNN backbones.

\Cref{tab:sub_coco_clip} presents a further evaluation of the CLIP backbone on the COCO subsets described in \Cref{sec:evaluation}. We observe similar trends as with the BLIP backbone, where \ours consistently outperforms the Zero-Shot (ZS) and Linear Probing (LP) baselines. However, white-box methods that have access to model weights continue to outperform \ours and \oursp, which leverage a frozen model.
\input{tables/subcoco_clip_maple}

\input{tables/ablations_proxy_layers}
% In \Cref{tab:ablations_proxy_layers}, we perform an ablation study on the choice of layers where the proxy vector is learned in \oursp. This experiment is conducted on CLIP's visual encoder, trained on the COCO dataset. As observed, injecting the proxy vectors into the initial layers of the transformer encoder has a minimal effect, only slightly improving the zero-shot baseline. In contrast, the final layers are the most influential in our experiments. However, using all transformer layers yields the best overall performance and also eliminates the need to select specific layers manually.

\input{tables/ablations_proxy_vecs}
% Next, we examine the number of learned proxy vectors per layer in our \oursp baseline, as shown in \Cref{tab:ablations_proxy_vecs}. In general, increasing the number of learned vectors (and parameters) improves the model's performance. However, we observe a saturation in the Recall@10 and Recall@50 metrics starting from 8 learned vectors. It is important to note that as more vectors are learned, the gradient dimensionality required to propagate through the model to the learned parameters increases, which leads to a trade-off with the amount of information exposed in the Gray-box approach.
\input{tables/ablations_shift_extra}

{\bf Number of proxy tokens}: In \Cref{tab:ablations_proxy_layers}, we conduct an ablation study on the choice of layers where the proxy vector is learned in \oursp. This experiment is carried out on CLIP's visual encoder, trained on the COCO dataset. Injecting proxy vectors into the initial layers of the transformer encoder has a minimal effect, only slightly improving upon the zero-shot baseline, whereas the final layers have the most significant impact. However, using all transformer layers yields the best overall performance, eliminating the need for manual layer selection.

Next, examine the number of learned proxy vectors per layer in our \oursp baseline, as presented in \Cref{tab:ablations_proxy_vecs}. Generally, increasing the number of learned vectors (and parameters) enhances the model's performance. However, we observe saturation in the Recall@10 and Recall@50 metrics starting from 8 learned vectors. It is important to note that as more vectors are learned, the gradient dimensionality required to propagate through the model to the learned parameters increases, resulting in a trade-off with the amount of information exposed in the Gray-box approach.

\input{tables/ablations_lastlayer_ft}
In \Cref{tab:ablations_lastlayers_ft} we ablate over the number of BLIP last layers fine-tuning. Each model was trained on COCO training set, results presented on COCO 5k validation set. We observe minor differences on performance between the methods, where fine-tuning all the layers results in lower performance. We relate it to the high number of parameters versus the low size of training set.

\section{Visualization}
\label{sec:visualizations}
In this section, we visualize the image transformations produced by the input adapter. \Cref{fig:visualizations} shows randomly sampled images from the COCO dataset. Each original image is processed through the input adapter and normalized to the same mean and standard deviation as the original image for visualization. Although the transformed images may appear corrupted or unnatural to the human eye, the model interprets these modified versions more effectively, as evidenced by performance improvements across multiple benchmarks.
\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/visualizations.pdf}
	\caption{Visualization of the input adapter's influence on images.}
	\label{fig:visualizations}
\end{figure*}

\section{Further Discussion on Recent Studies}
\label{sec:further_discussion}
Recent studies \cite{LiuYLPR24,WangL0WT24} have proposed black-box prompt optimization techniques for Vision-Language models, aiming to enhance performance without requiring access to the backbone model. These methods achieve this by optimizing the input textual prompt, focusing exclusively on text manipulation \cite{WangL0WT24} or text-to-text mapping \cite{LiuYLPR24}, without addressing the visual modality. More specifically, they are designed to optimize textual prompts for tasks such as 16-shot classification. However, this approach limits their applicability to scenarios heavily reliant on the visual domain. For instance, tasks such as Video or Sketch retrieval, which are fundamentally based on visual inputs, remain outside the capabilities of these methods. In contrast, our work addresses such visual domain challenges, expanding the utility and applicability of black-box fine-tuning to a broader range of tasks beyond text-focused optimizations.

To further illustrate the broader applicability of our approach, \Cref{fig:domain_experts} presents a demonstration of general schemes for handling multiple tasks or domains. The bottom part of the figure illustrates the naive approach of managing each task or domain with its own optimized model. In contrast, the top part of the figure shows a single optimized backbone model capable of handling all inputs with the use of input/output adapters. First, each input is processed using the appropriate lightweight input adapter. Next, the aggregated batch across all tasks is fed into the model, which produces outputs for each item. Finally, each output is post-processed with its corresponding output adapter to generate the final result.

{\bf Experimental Validation}: To substantiate these claims, we conducted inference experiments comparing two setups: 1) A single backbone combined with 10 pairs of DGA adapters (for 10 different tasks or domains), 2) Ten separate backbones without using our DGA framework.
In each setup, we utilize CLIP encoders to encode 10 sampled sets of 100 pairs of images (224x224) and their captions, a total of 1,000 paired samples. 

The results demonstrate significant computational and memory efficiency with our approach:
Our framework required 22.760 GFLOPs for 1000 samples, compared to 203.223 GFLOPs for the separate backbone setup. Similarly, GPU memory usage was reduced to 1.462 GB, as opposed to 14.54 GB in the alternative setup. These results highlight the resource efficiency and scalability of our framework in managing diverse tasks or domains.


\begin{figure}[ht]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/domain_experts.pdf}
	\caption{General schemes for handling $N$ different tasks or domains. {\bf Top}: A single optimized model designed for multiple tasks or domains. {\bf Bottom}: A naive approach with $N$ different models, one for each task.}
	\label{fig:domain_experts}
\end{figure}

\section{Implementation Details}
\label{sec:further_implementation_details}
This section provides the implementation details of our experiments. All methods are trained using the {\it AdamW} optimizer, with training conducted on 1-4 nodes of {\it NVIDIA A100} GPUs, depending on the batch size. The input/output adapters are initialized as identity functions.
 

{\bf Learning Rates}: For CLIP backbones, we train \ours with an initial learning rate of $1\times 10^{-4}$, and $5\times 10^{-5}$ for BLIP and DinoV2, all with an exponential decay rate of $0.93$ down to a minimum of $1\times 10^{-6}$.

{\bf Batch Sizes}: We use a batch size of 256 for all retrieval tasks, except for the Stanford-Cars dataset, where a batch size of 64 is applied. For ImageNet1k classification, a batch size of 1024 is used, and 64 for ImageNet-Sketch.

\textbf{Epochs:} We train the models for the following number of epochs on each benchmark: 25 for Stanford-Cars and ImageNet1k (16 shots), 30 for Sketchy and ImageNet-Sketch, 50 for COCO, 2 for Flickr30k, 20 for MSR-VTT, and 40 for VATEX.

{\bf LoRA Hyper-parameters}: For the LoRA baseline, we adapt the $Q$, $K$, and $V$ matrices across all transformer layers, ensuring the rank matches the number of parameters used by \ours and \oursp, depending on the backbone.


{\bf Trainable Parameters}: The number of trainable parameters depends on the backbone. For BLIP-B, \ours optimizes 0.10\% of the parameters, 0.42\% for CLIP, and 1.57\% for DINOv2. To ensure a fair comparison, we train the LoRA baselines with a rank $r$ that results in a matched number of trainable parameters to \ours: $r=8$ for CLIP, $r=2$ for BLIP, and $r=25$ for DINOv2. For \oursp, we train a proxy token for each of the 12 transformer layers, resulting in a maximum of $12 \cdot 2 \cdot 768$ trainable parameters, depending on the backbone's dimensionality and the number of modalities (image and text).
