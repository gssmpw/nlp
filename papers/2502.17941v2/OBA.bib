@inproceedings{obd,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {NeurIPS},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 year = {1989}
}

@inproceedings{obs,
 author = {Hassibi, Babak and Stork, David},
 booktitle = {NeurIPS},
 title = {Second order derivatives for network pruning: Optimal Brain Surgeon},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
 year = {1992}
}

@inproceedings{molchanov2016pruning,
  title={Pruning Convolutional Neural Networks for Resource Efficient Inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{fang2023depgraph,
  title={Depgraph: Towards any structural pruning},
  author={Fang, Gongfan and Ma, Xinyin and Song, Mingli and Mi, Michael Bi and Wang, Xinchao},
  booktitle={CVPR},
  pages={16091--16101},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={NeurIPS},
  volume={25},
  year={2012}
}

@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {CVPR},
month = {June},
year = {2016}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={ICLR},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

BibTeX Record
@inproceedings{
li2017pruning,
title={Pruning Filters for Efficient ConvNets},
author={Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
booktitle={ICLR},
year={2017},
url={https://openreview.net/forum?id=rJqFGTslg}
}

@article{ma2023llm,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2305.11627},
  year={2023}
}

@inproceedings{wang2019eigendamage,
  title={Eigendamage: Structured pruning in the kronecker-factored eigenbasis},
  author={Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  booktitle={ICML},
  pages={6566--6575},
  year={2019},
  organization={PMLR}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal={NeurIPS},
  volume={28},
  year={2015}
}

@article{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  journal={NeurIPS},
  volume={29},
  year={2016}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{yeom2021pruning,
  title={Pruning by explaining: A novel criterion for deep neural network pruning},
  author={Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={Pattern Recognition},
  volume={115},
  pages={107899},
  year={2021},
  publisher={Elsevier}
}

@article{anwar2017structured,
  title={Structured pruning of deep convolutional neural networks},
  author={Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={13},
  number={3},
  pages={1--18},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{hanson1988comparing,
  title={Comparing biases for minimal network construction with back-propagation},
  author={Hanson, Stephen and Pratt, Lorien},
  journal={NeurIPS},
  volume={1},
  year={1988}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11264--11272},
  year={2019}
}

@article{chauvin1988back,
  title={A back-propagation algorithm with optimal use of hidden units},
  author={Chauvin, Yves},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}

@inproceedings{yu2018nisp,
  title={Nisp: Pruning networks using neuron importance score propagation},
  author={Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9194--9203},
  year={2018}
}



@inproceedings{gordon2018morphnet,
  title={Morphnet: Fast \& simple resource-constrained structure learning of deep networks},
  author={Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1586--1595},
  year={2018}
}

@inproceedings{huang2018data,
  title={Data-driven sparse structure selection for deep neural networks},
  author={Huang, Zehao and Wang, Naiyan},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={304--320},
  year={2018}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2736--2744},
  year={2017}
}

@article{ye2018rethinking,
  title={Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers},
  author={Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
  journal={arXiv preprint arXiv:1802.00124},
  year={2018}
}

@inproceedings{he2020learning,
  title={Learning filter pruning criteria for deep convolutional neural networks acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2009--2018},
  year={2020}
}


@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}

@inproceedings{luo2020neural,
  title={Neural network pruning with residual-connections and limited-data},
  author={Luo, Jian-Hao and Wu, Jianxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1458--1467},
  year={2020}
}

@article{you2019gate,
  title={Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks},
  author={You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhang2021aligned,
  title={Aligned structured sparsity learning for efficient image super-resolution},
  author={Zhang, Yulun and Wang, Huan and Qin, Can and Fu, Yun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2695--2706},
  year={2021}
}

@article{he2018soft,
  title={Soft filter pruning for accelerating deep convolutional neural networks},
  author={He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  journal={arXiv preprint arXiv:1808.06866},
  year={2018}
}

@article{yu2019autoslim,
  title={Autoslim: Towards one-shot architecture search for channel numbers},
  author={Yu, Jiahui and Huang, Thomas},
  journal={arXiv preprint arXiv:1903.11728},
  year={2019}
}

@article{wang2020neural,
  title={Neural pruning via growing regularization},
  author={Wang, Huan and Qin, Can and Zhang, Yulun and Fu, Yun},
  journal={arXiv preprint arXiv:2012.09243},
  year={2020}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4340--4349},
  year={2019}
}

@inproceedings{peng2019collaborative,
  title={Collaborative channel pruning for deep networks},
  author={Peng, Hanyu and Wu, Jiaxiang and Chen, Shifeng and Huang, Junzhou},
  booktitle={International Conference on Machine Learning},
  pages={5113--5122},
  year={2019},
  organization={PMLR}
}

@inproceedings{NIPS2015_3e15cc11,
 author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf},
 volume = {28},
 year = {2015}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European conference on computer vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@inproceedings{pouransari2020least,
  title={Least squares binary quantization of neural networks},
  author={Pouransari, Hadi and Tu, Zhucheng and Tuzel, Oncel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={698--699},
  year={2020}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{chen2021distilling,
  title={Distilling knowledge via knowledge review},
  author={Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5008--5017},
  year={2021}
}

@inproceedings{zhou2021distilling,
  title={Distilling holistic knowledge with graph neural networks},
  author={Zhou, Sheng and Wang, Yucheng and Chen, Defang and Chen, Jiawei and Wang, Xin and Wang, Can and Bu, Jiajun},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10387--10396},
  year={2021}
}

@inproceedings{liu2018progressive,
  title={Progressive neural architecture search},
  author={Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={19--34},
  year={2018}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{hubara2018quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={journal of machine learning research},
  volume={18},
  number={187},
  pages={1--30},
  year={2018}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@misc{martens2020new,
      title={New insights and perspectives on the natural gradient method}, 
      author={James Martens},
      year={2020},
      eprint={1412.1193},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{benbaki2023fast,
  title={Fast as chita: Neural network pruning with combinatorial optimization},
  author={Benbaki, Riade and Chen, Wenyu and Meng, Xiang and Hazimeh, Hussein and Ponomareva, Natalia and Zhao, Zhe and Mazumder, Rahul},
  booktitle={ICML},
  pages={2031--2049},
  year={2023},
  organization={PMLR}
}


@InProceedings{yu22cbs,
  title = 	 {The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks},
  author =       {Yu, Xin and Serra, Thiago and Ramalingam, Srikumar and Zhe, Shandian},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {25668--25683},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/yu22f/yu22f.pdf},
  url = 	 {https://proceedings.mlr.press/v162/yu22f.html},
  abstract = 	 {Neural networks tend to achieve better accuracy with training if they are larger {—} even if the resulting models are overparameterized. Nevertheless, carefully removing such excess of parameters before, during, or after training may also produce models with similar or even improved accuracy. In many cases, that can be curiously achieved by heuristics as simple as removing a percentage of the weights with the smallest absolute value {—} even though absolute value is not a perfect proxy for weight relevance. With the premise that obtaining significantly better performance from pruning depends on accounting for the combined effect of removing multiple weights, we revisit one of the classic approaches for impact-based pruning: the Optimal Brain Surgeon (OBS). We propose a tractable heuristic for solving the combinatorial extension of OBS, in which we select weights for simultaneous removal, and we combine it with a single-pass systematic update of unpruned weights. Our selection method outperforms other methods for high sparsity, and the single-pass weight update is also advantageous if applied after those methods.}
}

@inproceedings{wille1997structure,
  title={On the structure of the Hessian matrix in feedforward networks and second derivative methods},
  author={Wille, J{\"o}rg},
  booktitle={Proceedings of International Conference on Neural Networks (ICNN'97)},
  volume={3},
  pages={1851--1855},
  year={1997},
  organization={IEEE}
}

@article{buntine1994computing,
  title={Computing second derivatives in feed-forward networks: A review},
  author={Buntine, Wray L and Weigend, Andreas S},
  journal={IEEE transactions on Neural Networks},
  volume={5},
  number={3},
  pages={480--488},
  year={1994},
  publisher={IEEE}
}

@article{wu2020dissecting,
  title={Dissecting hessian: Understanding common structure of hessian in neural networks},
  author={Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
  journal={arXiv preprint arXiv:2010.04261},
  year={2020}
}

@article{singh2021analytic,
  title={Analytic insights into structure and rank of neural network hessian maps},
  author={Singh, Sidak Pal and Bachmann, Gregor and Hofmann, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23914--23927},
  year={2021}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}