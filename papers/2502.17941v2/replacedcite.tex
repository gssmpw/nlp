\section{Related Work}
\paragraph{Model Compression}
Model compression is an area that focuses on creating smaller, faster, and more efficient models suitable for deployment in environments with limited resources, like mobile devices or embedded systems. There are several typical fields within this area, including quantization____, knowledge distillation____, neural architecture search____, and network pruning____. Quantization, outlined in works like ____ and ____, focuses on reducing parameter precision to accelerate inference and decrease model size, enabling deployment on devices with limited resources. Knowledge distillation, as introduced by ____, leverages a smaller "student" model to mimic a larger "teacher" model, effectively compressing the knowledge and achieving high performance with less computational demand. Neural Architecture Search (NAS), with seminal contributions from ____, automates the discovery of optimal architectures, often outperforming human-designed models in efficiency and accuracy. Pruning techniques, highlighted in work by ____, remove non-essential weights or neurons, significantly reducing model complexity and enhancing inference speed without major accuracy losses. Together, these techniques represent the forefront of model compression research, addressing the balance between performance and computational efficiency necessary for advanced AI applications.
\paragraph{Network Pruning}
Network pruning, initially recognized as an importance estimation problem____, has been prompting researchers to focus on finding accurate criteria that reveals the importance of parameters or neurons in neural networks. ____ operated under the assumption that each layer in feed-forward networks held equal importance and introduced a heuristic for global scaling normalization. However, this approach did not prove effective in networks incorporating skip connections. Additionally, the method relies on using network activations to calculate its criterion, resulting in increased memory demands. In contrast, pruning methods that focus on batch normalization____ bypass the need for sensitivity analysis and are applicable on a global scale. In intricate network architectures____, parameter interdependencies often require their joint pruning. This collective pruning of interlinked parameters has been an area of focus in structured pruning research since its early stages. ____ proposes to build a dependency graph which captures the interdependencies between parameters and prunes parameters belonging to a graph together to achieve structured pruning for neural networks with complicated structures. In our pruning setting, we view the pruning as an importance estimation problem for each individual parameter (unstructured) or parameter group (structured) obtained from ____. Parameters with low importance are pruned in each pruning step.

\paragraph{Hessian Matrix Estimation} The structure and computational aspects of the Hessian matrix in feedforward neural networks have been extensively studied since the early 1990s____. The Hessian matrix was first utilized in neural network pruning by ____ to calculate the importance score of each neuron, leveraging diagonal approximation is used to estimate the Hessian matrix:
\begin{equation}
\delta \mathcal{L}_{\mathrm{OBD}}=\frac{1}{2}\left(\boldsymbol{\theta}_q^*\right)^2 \mathbf{H}_{q q}.
\end{equation}
Building upon this, OBS____ views the importance estimation as an optimization problem, and aims at finding a set of weights that yields least change on the loss:
\begin{equation}
    \min _q\left\{\min _{\delta \boldsymbol{\theta}} \frac{1}{2} \delta \boldsymbol{\theta}^{\top} \mathbf{H} \delta \boldsymbol{\theta} \text { s.t. } \mathbf{e}_q^{\top} \delta \boldsymbol{\theta}+\boldsymbol{\theta}_q^*=0\right\}.
    \end{equation}
Early research such as that by ____ provided an extensive review of how to compute second derivatives in feed-forward networks, and ____ examined the Hessian matrix's structure and second derivative techniques. More contemporary efforts, such as EigenDamage____, utilize a Kronecker-factored eigenbasis for network reparameterization and approximate the Hessian matrix using the Fisher matrix, as discussed by ____. Recent studies____ have thoroughly investigated the common structures and rank properties of the Hessian in neural networks. ____ initially introduced an efficient method for computing the Hessian-vector product in feedforward neural networks. Our research applies this idea to the pruning of modern network architectures including CNNs and Transformers.

\vspace{-0.1cm}