
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
% self-added
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{bbding}

\usepackage{mathtools}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{lipsum}
\definecolor{colorcite}{RGB}{35,140,160}
\definecolor{newcontent}{RGB}{232,79,10}
\hypersetup{
hidelinks,
colorlinks=true,
linkcolor=red,
citecolor=colorcite,
urlcolor = black
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\title{Optimal Brain Apoptosis}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Mingyuan Sun\textsuperscript{\rm 1}, Zheng Fang\textsuperscript{\rm 1,}\footnotemark[1]\ , Jiaxu Wang\textsuperscript{\rm 2}, Junjie Jiang\textsuperscript{\rm 1}, Delei Kong\textsuperscript{\rm 3},\\
\textbf{Chenming Hu\textsuperscript{\rm 1}, Yuetong Fang\textsuperscript{\rm 2}\ \ \& Renjing Xu\textsuperscript{\rm 2,}}\thanks{Corresponding authors.} \\
\textsuperscript{\rm 1}Northeastern University \quad
\textsuperscript{\rm 2}The Hong Kong University of Science and Technology (Guangzhou)\\
\textsuperscript{\rm 3}Hunan University
% \small\texttt{fangzheng@mail.neu.edu.cn,\,renjingxu@hkust-gz.edu.cn,} \\
% \small\texttt{\{mingyuansun20,\,jianggalaxypursue,\,kong.delei.neu,\,hucm6108\}@gmail.com,} \\
% \small\texttt{\{jwang457,\,yfang870\}@connect.hkust-gz.edu.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at \href{https://github.com/NEU-REAL/OBA}{\textcolor[HTML]{ED008A}{\texttt{https://github.com/NEU-REAL/OBA}}}.
\end{abstract}

\section{Introduction}

With the rapid development of deep learning, neural networks have become deeply integrated into all sectors of our daily life. Convolutional Neural Networks~\citep{lecun1998gradient, krizhevsky2012imagenet,He_2016_CVPR,ma2023llm} and Transformers~\citep{vaswani2017attention,dosovitskiy2021an} are two typical structures used most widely. As researchers continuously innovate, the performance of neural networks improves, but the number of parameters and the computational complexity also increase significantly. Therefore, how to efficiently reduce the parameter size and computational overhead of neural networks while maintaining their performance as much as possible has become a crucial problem.

Extensive research~\citep{obd,molchanov2016pruning} demonstrates that pruning is a powerful tool for dealing with this issue. Generally speaking, pruning can be divided into two main streams: unstructured pruning and structured pruning. Unstructured pruning~\citep{guo2016dynamic,han2015learning,dong2017learning} involves the removal of individual weights or neurons from a neural network.  The key advantage of unstructured pruning lies in its flexibility and the fine-grained control it offers over the model's architecture. This method often requires specialized hardware or software to exploit the resultant sparsity for computational efficiency. Structured pruning~\citep{anwar2017structured,yeom2021pruning} removes entire neurons, channels, or layers from a neural network, which is more frendly to software since parameters of neural networks are mainly structured data, such as tensor, matrix, and vector. Removing entire neurons directly corresponds to a slicing or selecting operation on the structured data, which is easy to implement and more compatible with standard hardware accelerators, such as GPUs and TPUs.

\citet{hanson1988comparing} is one of the earliest works to explore structured pruning. The underlying idea is that significant weights typically possess greater magnitudes, as they need to process and transmit more information to be influential in determining the network's accurate output. However, pruning under this guidance may sometimes incorrectly remove important neurons with small magnitude and reserve unimportant neurons with large magnitude. Optimal Brain Damage (OBD)~\citep{obd} and Optimal Brain Surgeon (OBS)~\citep{obs} propose to leverage the Hessian matrix of loss w.r.t. parameters to estimate the importance of each parameter. The Hessian matrix contains second-order partial derivatives of the loss function w.r.t. all pairs of parameters, which is very computationally expensive to compute. Thus, OBD approximates it as a diagonal matrix by assuming the loss of deleting several parameters is the sum of deleting these parameters individually. OBS views the pruning as an optimization problem and solves it with the Lagrange multiplier. They either discard or approximate the second-order partial derivatives between all pairs of parameters, which capture the change of loss on one parameter when deleting another parameter.

\paragraph{Our Contributions} In this paper, we follow the idea of OBD, leveraging Hessian matrix for parameter importance estimation. Instead of approximating Hessian matrix, we calculate the Hessian-vector product element $\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta_i\partial\theta_j}\delta\theta_i\delta\theta_j$ for each parameter in the network. To achieve this, we first separate the Hessian matrix of the whole network into Hessian submatrices between layers. Then, in the context of widely used network structures including convolutional neural networks (CNNs) and Transformers, we analyze the conditions where the Hessian submatrices between two layers are nonzero. Finally, we propose a highly efficient method to capture these conditions and obtain the Hessian-vector product element on each parameter. Stepping from approximating the Hessian matrix with the Fisher matrix to directly computing the Hessian-vector product, we propose Optimal Brain Apoptosis (OBA), a novel pruning method that efficiently calculates the second-order Taylor expansion for each parameter and is applicable to both structured and unstructured pruning tasks.

\vspace{-0.1cm}
\section{Related Work}
\paragraph{Model Compression}
Model compression is an area that focuses on creating smaller, faster, and more efficient models suitable for deployment in environments with limited resources, like mobile devices or embedded systems. There are several typical fields within this area, including quantization~\citep{NIPS2015_3e15cc11,rastegari2016xnor,pouransari2020least}, knowledge distillation~\citep{hinton2015distilling, chen2021distilling, zhou2021distilling}, neural architecture search~\citep{liu2018progressive,zoph2016neural,pham2018efficient}, and network pruning~\citep{molchanov2019importance,molchanov2016pruning}. Quantization, outlined in works like \citet{hubara2018quantized} and \citet{jacob2018quantization}, focuses on reducing parameter precision to accelerate inference and decrease model size, enabling deployment on devices with limited resources. Knowledge distillation, as introduced by \citet{hinton2015distilling,romero2014fitnets}, leverages a smaller "student" model to mimic a larger "teacher" model, effectively compressing the knowledge and achieving high performance with less computational demand. Neural Architecture Search (NAS), with seminal contributions from \citet{zoph2016neural}, automates the discovery of optimal architectures, often outperforming human-designed models in efficiency and accuracy. Pruning techniques, highlighted in work by \citet{han2015learning}, remove non-essential weights or neurons, significantly reducing model complexity and enhancing inference speed without major accuracy losses. Together, these techniques represent the forefront of model compression research, addressing the balance between performance and computational efficiency necessary for advanced AI applications.
\paragraph{Network Pruning}
Network pruning, initially recognized as an importance estimation problem~\citep{molchanov2019importance, chauvin1988back, yu2018nisp,he2020learning}, has been prompting researchers to focus on finding accurate criteria that reveals the importance of parameters or neurons in neural networks. \citet{molchanov2016pruning} operated under the assumption that each layer in feed-forward networks held equal importance and introduced a heuristic for global scaling normalization. However, this approach did not prove effective in networks incorporating skip connections. Additionally, the method relies on using network activations to calculate its criterion, resulting in increased memory demands. In contrast, pruning methods that focus on batch normalization~\citep{gordon2018morphnet,huang2018data,liu2017learning,ye2018rethinking} bypass the need for sensitivity analysis and are applicable on a global scale. In intricate network architectures~\citep{liu2021group,luo2020neural,you2019gate,zhang2021aligned}, parameter interdependencies often require their joint pruning. This collective pruning of interlinked parameters has been an area of focus in structured pruning research since its early stages. \citet{fang2023depgraph} proposes to build a dependency graph which captures the interdependencies between parameters and prunes parameters belonging to a graph together to achieve structured pruning for neural networks with complicated structures. In our pruning setting, we view the pruning as an importance estimation problem for each individual parameter (unstructured) or parameter group (structured) obtained from \citet{fang2023depgraph}. Parameters with low importance are pruned in each pruning step.

\paragraph{Hessian Matrix Estimation} The structure and computational aspects of the Hessian matrix in feedforward neural networks have been extensively studied since the early 1990s~\citep{buntine1994computing,wille1997structure}. The Hessian matrix was first utilized in neural network pruning by \citet{obd} to calculate the importance score of each neuron, leveraging diagonal approximation is used to estimate the Hessian matrix:
\begin{equation}
\delta \mathcal{L}_{\mathrm{OBD}}=\frac{1}{2}\left(\boldsymbol{\theta}_q^*\right)^2 \mathbf{H}_{q q}.
\end{equation}
Building upon this, OBS~\citep{obs} views the importance estimation as an optimization problem, and aims at finding a set of weights that yields least change on the loss:
\begin{equation}
    \min _q\left\{\min _{\delta \boldsymbol{\theta}} \frac{1}{2} \delta \boldsymbol{\theta}^{\top} \mathbf{H} \delta \boldsymbol{\theta} \text { s.t. } \mathbf{e}_q^{\top} \delta \boldsymbol{\theta}+\boldsymbol{\theta}_q^*=0\right\}.
    \end{equation}
Early research such as that by \citet{buntine1994computing} provided an extensive review of how to compute second derivatives in feed-forward networks, and \citet{wille1997structure} examined the Hessian matrix's structure and second derivative techniques. More contemporary efforts, such as EigenDamage~\citep{wang2019eigendamage}, utilize a Kronecker-factored eigenbasis for network reparameterization and approximate the Hessian matrix using the Fisher matrix, as discussed by \citet{martens2020new}. Recent studies~\citep{wu2020dissecting,singh2021analytic} have thoroughly investigated the common structures and rank properties of the Hessian in neural networks. \citet{pearlmutter1994fast} initially introduced an efficient method for computing the Hessian-vector product in feedforward neural networks. Our research applies this idea to the pruning of modern network architectures including CNNs and Transformers.

\vspace{-0.1cm}
\section{Preliminary}

Consider a feed-forward neural network with parameters $\theta$ and $L$ layers. Similar to OBD~\citep{obd}, when we add small perturbation $\delta\theta$ on $\theta$, the second-order Taylor expansion of the perturbation on the objective function is given by
\begin{equation}
    \label{eq:secondtaylor}
    \begin{split}
        \delta\mathcal{L}(\theta) = (\frac{\partial \mathcal{L}}{\partial\theta})^\mathsf{T}\delta\theta + \frac{1}{2}\delta\theta^\mathsf{T}\mathbf{H}\delta\theta + o(\|\delta\theta\|^3),
    \end{split}
\end{equation}

where $\mathbf{H}$ is the Hessian matrix that represents the second-order derivatives between all parameter pairs. It is usually infeasible to compute the Hessian matrix due to its complexity of $O(n^2)$, where n is the number of parameters in the network~\citep{obd, obs}. By expanding the first and second term of \cref{eq:secondtaylor}, we can define the perturbation of the loss caused by $\delta\theta_i$ as
\begin{equation}
    \delta\mathcal{L}(\theta_i) = \frac{\partial \mathcal{L}}{\partial\theta_i}\delta\theta_i + \sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta_i\partial\theta_j}\delta\theta_i\delta\theta_j + o(\|\delta\theta_i\|^3).
    \label{eq:secondtaylor_expand}
\end{equation}
The first term of \cref{eq:secondtaylor_expand} is leveraged to estimate the improtance of neurons in \citet{molchanov2016pruning}. Same to prior works, we ignore the higher order term $o(\|\delta\theta_i\|^3)$. Current works~\citep{obs, yu22cbs, benbaki2023fast} that leverage the second-order Taylor expansion term approximate the Hessian matrix with Fisher information matrix. This approximation, if applied to \cref{eq:secondtaylor_expand}, would change its second-order term to $\sum_{j}\frac{\partial \mathcal{L}}{\partial\theta_i}\frac{\partial \mathcal{L}}{\partial\theta_j}\delta\theta_i\delta\theta_j$. However, this approximation is not accurate enough to capture the second-order loss perturbation caused by $\delta\theta_i$ and $\delta\theta_j$. Therefore, we focus on a theoretical analysis on how to calculate the original second-order term $\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta_i\partial\theta_j}\delta\theta_i\delta\theta_j$. 

\section{Method}
\vspace{-0.2cm}
\subsection{Definition}
\vspace{-0.2cm}
We derive from a general form of linear layers in modern neural networks. For layer $l\in[1,L]$, we denote the weight parameter as $W^{(l)}\in\mathbb R^{l_{\text{out}}\times l_{\text{in}} \times p_{\text{weight}}}$ and bias parameter as $b^{(l)}\in\mathbb R^{l_{\text{out}}}$. We denote the input of layer $l$ as $X^{(l)}\in\mathbb R^{l_{\text{in}}\times p_{\text{in}}}$ and output of layer $l$ as $Y^{(l)}\in\mathbb R^{l_{\text{out}}\times p_{\text{out}}}$. $p_{\text{weight}}$, $p_{\text{out}}$, and $p_{\text{in}}$ are the length of flattened weights, output values, and input values that contribute to the connections between every pairs of input neurons and output neurons, for example $p_{\text{weight}}=1, p_{\text{out}}=1, p_{\text{in}}=1$ in the context of fully connected layers and $p_{\text{weight}}=s^{(l)\ 2}_{kernel}, p_{\text{out}}=h^{(l)}_{\text{out}}w^{(l)}_{\text{out}}, p_{\text{in}}=h^{(l)}_{\text{in}}w^{(l)}_{\text{in}}$ in the context of convolutional layers. The forward propagation of layer $l$ is given by
\begin{equation}
    \label{eq:forward}
    Y^{(l)}_{ab} = \sum_{cde} W^{(l)}_{acd} X^{(l)}_{ce} M^{(l)}_{bde} + b^{(l)}_{a},
\end{equation}
where $M\in\{0, 1\}^{p_{\text{out}}\times p_{\text{weight}}\times p_{\text{in}}}$, determined by the layer itself, represents the connections among all input values, weight values, and output values. We denote the flattened vector of $X^{(l)}$, $W^{(l)}$, $Y^{(l)}$, and $M^{(l)}$ as $x^{(l)}\in\mathbb R^{l_{\text{in}}\cdot p_{\text{in}}}$, $w^{(l)}\in\mathbb R^{l_{\text{out}}\cdot l_{\text{in}} \cdot p_{\text{weight}}}$, $y^{(l)}\in\mathbb R^{l_{\text{out}}\cdot p_{\text{out}}}$, and $m^{(l)}\in\{0, 1\}^{p_{\text{out}}\cdot p_{\text{weight}}\cdot p_{\text{in}}}$, respectively. the parameters $\theta^{(l)}$ of layer $l$ can be expressed as $\theta^{(l)} = \begin{bmatrix} w^{(l)} \\ b^{(l)} \end{bmatrix}$. We represent all partial derivative terms with variables in their vector forms to ensure these terms are with the same definition of Jacobian matrix.

In a certain layer $l$ defined by \cref{eq:forward}, the gradient of the loss w.r.t. the indexed parameters $W^{(l)}_{acd}$ and $b^{(l)}_{a}$ are respectively $\sum_{ab}\frac{\partial\mathcal{L}}{\partial Y^{(l)}_{ab}}\sum_e X^{(l)}_{ce} M^{(l)}_{bde}$ and $\sum_{b}\frac{\partial\mathcal{L}}{\partial Y^{(l)}_{ab}}$. It is clear that the condition for the Hessian matrix entries between layer $l$ and any distinct layer $l'$ being exclusively zero hinges on the differentiability of terms $\frac{\partial\mathcal{L}}{\partial y^{(l)}}$ and $X^{(l)}$ with respect to the parameters of layer $l'$, which depends on the connection type of two layer types. We observe that the connectivity types which introduce nonzero Hessian submatrices between any two layers in a neural network can be divided into two cases: series connectivity and parallel connectivity, as shown in \cref{fig:Hessian_demo}. We next analyze these two cases and calculate the Hessian-vector product element $\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta_i\partial\theta_j}\delta\theta_i\delta\theta_j$ in \cref{eq:secondtaylor_expand} for the two cases respectively.
\vspace{-2pt}
\subsection{Series Connectivity}
\vspace{-6pt}

\begin{definition}[Series Connectivity]
    In a neural network at layer $l$, if there exists a layer $l'$ such that there is a differentiable function mapping the output of layer $l'$ to the input of layer $l$, we say layer $l'$ and $l$ are in series connectivity. Specifically:
    \begin{itemize}
        \item layer $l'$ is in lower series connectivity to layer $l$.
        \item layer $l$ is in upper series connectivity to layer $l'$.
        \end{itemize}
\end{definition}
In \cref{fig:Hessian_demo}, there are differentiable functions from $Y^{(l_1)}$ to $X^{(l_2)}$ and $X^{(l_3)}$, respecively, so layer $l_1$ is in series connectivity with both layer $l_2$ and layer $l_3$. Without loss of generality, we take layer $l_1$ and layer $l_2$ as an example. Then
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial y^{(l_1)}}=\frac{\partial \mathcal{L}}{\partial y^{(l_2)}}\mkern-30mu\underbrace{\frac{\partial y^{(l_2)}}{\partial x^{(l_2)}}}_{\text{differentiable to } \theta^{(l_2)}}\mkern-30mu\frac{\partial x^{(l_2)}}{\partial y^{(l_1)}},\nonumber
\end{equation}
also $X^{(l_2)}$ is differentiable to $\theta^{(l_1)}$. According to our analysis in the beginning of this section, a nonzero Hessian submatrix exists between layers $l_1$ and $l_2$.

% Formally, we take layer $l_1$ and layer $l_2$ as an example and follow the notations in \cref{eq:forward} to denote each value of the two layers. 
% $\Phi: \mathbb R^{l^1_{\text{out}}\times p^1_{\text{out}}}\to \mathbb R^{l^2_{\text{in}}\times p^2_{\text{in}}}$ is a differentiable function mapping the output of layer $l_1$ to the input of layer $l_2$, i.e., $X^{l_2}=\Phi(Y^{l_1})$.
\begin{theorem}
    \label{thm:series_connect}
    For layer $l$ in a neural network where layers $l_{\text{up}}\in\mathbf{L}_{\text{up}}$ and layers $l_{\text{low}}\in\mathbf{L}_{\text{low}}$ are in upper and lower series connectivity to layer $l$, respectively, then for weight parameter $w^{(l)}$ and bias parameter $b^{(l)}$ of layer $l$, we have
    \begin{align}
        \label{eq:series_w}
        \sum_{l'\in\mathbf{L}_{\text{up}}\cup\mathbf{L}_{\text{low}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial w^{(l)}}\delta\theta^{(l')}_j\odot\delta w^{(l)} =
        &\sum_{l_{\text{up}}\in\mathbf{L}_{\text{up}}}\frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial w^{(l)}}\odot\delta w^{(l)}\\
        &+\frac{\partial \mathcal{L}}{\partial y^{(l)}} \frac{\partial y^{(l)}}{\partial w^{(l)}}\bigg|_{\hat{X}^{(l)}}\odot\delta w^{(l)}
    \end{align}

    in which $\hat{X}^{(l)}$ is given by
    \begin{equation}
        \hat{X}^{(l)}_{mn}=\sum_{l_{\text{low}}\in\mathbf{L}_{\text{low}}}\sum_{k}\frac{\partial X^{(l)}_{mn}}{\partial \theta^{(l_{\text{low}})}_k}\delta\theta^{(l_{\text{low}})}_k,
    \end{equation}
    and
    \begin{equation}
        \label{eq:series_b}
        \sum_{l'\in\mathbf{L}_{\text{up}}\cup\mathbf{L}_{\text{low}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial b^{(l)}}\delta\theta^{(l')}_j\odot \delta b^{(l)} =
        \sum_{l_{\text{up}}\in\mathbf{L}_{\text{up}}}\frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial b^{(l)}}\odot\delta b^{(l)},
    \end{equation}
    where $\mathbf{J}^{(l)}_{\delta W^{(l)}}\in\mathbb{R}^{(l_{\text{out}}\cdot m_{\text{out}}) \times (l_{\text{in}} \cdot m_{\text{in}})}$ is the jacobian matrix of $y^{(l)}$ with respect to $x^{(l)}$ taking $\delta W^{(l)}$ as the weights, and $\frac{\partial y^{(l)}}{\partial w^{(l)}}\big|_{\hat{X}^{(l)}}$ is the jacobian matrix of $y^{(l)}$ w.r.t. $w^{(l)}$ taking $\hat{X}^{(l)}$ as input.
\end{theorem}
The proof is provided in \cref{sec:proof_series}. Note that the sets $\mathbf{L}_{\text{up}}(l)$ and $\mathbf{L}_{\text{low}}(l)$ are dependent on layer $l$, and are abbreviated for simplicity. With \cref{thm:series_connect}, 
the second-order term of the Taylor expansion of the loss with respect to each individual parameter can be computed, taking into account parameters belonging to layers that are in series connectivity to a specific layer. For classical neural network structures such as convolutional neural networks and fully connected neural networks, there only exist series connectivities between layers. Thus, we can directly apply \cref{thm:series_connect} to calculate \cref{eq:secondtaylor_expand} for each individual parameter.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{.44\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Hessian_demo.pdf}
        \vspace{6pt}
        \caption{}
        \label{fig:Hessian_demo}
    \end{subfigure}%
    \begin{subfigure}{.55\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/jvp_forward.pdf}
        \caption{}
        \label{fig:jvp_forward}
    \end{subfigure}
    \vspace{-4pt}
    \caption{\textbf{(a)} An illustration of conditions where the Hessian matrix between parameters of two layers are nonzero. \textbf{(b)} An illustration of Jacobian-Vector Product Forward Propagation. Two forward propagation processes are needed for parameter layers and one forward propagation process is needed for nonparameter layers. For nonparameter layers we leverage Jacobian-vector product to conduct the forward process and do not need to calculate the Jacobian matrix explicitly.}
    \vspace{-10pt}
    \label{fig:neural_concepts}
\end{figure}
\vspace{-6pt}
\subsection{Parallel Connectivity}
\vspace{-6pt}
For recent novel neural network structures such as Transformer~\citep{vaswani2017attention}, matrix multiplication plays a crucial and effective role in achieving their impressive performance. It also introduces parallel connectivity to layers and lead to the nonzero Hessian matrices between these connected layers.
\begin{definition}
    \label{def:parallel}
    In a neural network, if there exist two layers $l$ and $l'$ such that there are differentiable functions respectively mapping the outputs of layer $l$ and $l'$ to the inputs $X^{\text{(left)}}$ and $X^{\text{(right)}}$ of a matrix multiplication operation $Y^{\text{(mul)}}=X^{\text{(left)}}X^{\text{(right)}}$,i.e., layers $l$ and $l'$ are both in lower series connectivity to the matrix multiplication operation, we say layer $l$ and $l'$ are in parallel connectivity. Denote the multiplication operation as layer $l_m$, then
    \begin{itemize}
        \item layer $l$ is in left parallel connectivity to layer $l'$.
        \item layer $l'$ is in right parallel connectivity to layer $l$.
    \end{itemize}
\end{definition}
In \cref{fig:Hessian_demo}, layer $l_2$ and layer $l_3$ are in parallel connectivity. Similarly, the gradient of loss w.r.t. parameters of layer $l_2$ is $\frac{\partial \mathcal{L}}{\partial \theta^{(l_2)}}=\frac{\partial \mathcal{L}}{\partial y^{(l_4)}}\frac{\partial y^{(l_4)}}{\partial y^{(l_{42})}}\frac{\partial y^{(l_{42})}}{\partial \theta^{(l_2)}}$ in which $\frac{\partial y^{(l_4)}}{\partial y^{(l_{42})}}$ is differentiable to $\theta^{(l_3)}$ and vice versa. Therefore, the nonzero Hessian submatrices resulting from parallel connectivity should also be considered.
\begin{theorem}
    \label{thm:parallel}
    For a multiplication operation $Y^{\text{(mul)}}=X^{\text{(left)}}X^{\text{(right)}}$ in a neural network, where $X^{\text{(left)}}\in \mathbb{R}^{l_{row} \times l_{hid}}$ and $X^{\text{(right)}} \in \mathbb{R}^{l_{hid} \times l_{col}}$, layers $l_{l}\in\mathbf{L}_{\text{left}}$ and $l_{r}\in\mathbf{L}_{\text{right}}$ are in lower series connectivity to this multiplication operation, respectively. Consider two surrogate weight matrices $\hat{X}^{\text{(left)}}\in\mathbb{R}^{l_{row} \times l_{hid}}$ and $\hat{X}^{\text{(right)}}\in\mathbb{R}^{l_{hid} \times l_{col}}$ given by:
    \begin{align}
        \hat{X}_{kn}^{\text{(left)}} &= \sum_{l_{l}\in\mathbf{L}_{\text{left}}} \sum_{q}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{l})}_{q},\\
        \hat{X}_{no}^{\text{(right)}} &= \sum_{l_{r}\in\mathbf{L}_{\text{right}}} \sum_{r}\frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}} \delta \theta^{(l_{r})}_{r}.
    \end{align}
    Then we have
    \begin{equation}
        \sum_{l_l\in\mathbf{L}_{\text{left}}}\sum_{q}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{l})}_{q}\partial \theta^{(l_{r})}} \delta \theta^{(l_{l})}_{q}\odot \delta \theta^{(l_{r})}=
        \frac{\partial \mathcal{L}}{\partial y^{\text{(mul)}}}\frac{\partial y^{\text{(mul)}}}{\partial x^{\text{(right)}}}\bigg|_{\hat{X}^{\text{(left)}}} \frac{\partial x^{\text{(right)}}}{\partial \theta^{(l_{r})}}\odot\delta \theta^{(l_{r})}
        \label{eq:parallelthm_suml}
    \end{equation}
    and
    \begin{equation}
        \sum_{l_r\in\mathbf{L}_{\text{right}}}\sum_{r}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}} \delta \theta^{(l_{r})}_{r}\odot \delta \theta^{(l_{l})}=
        \frac{\partial \mathcal{L}}{\partial y^{\text{(mul)}}}\frac{\partial y^{\text{(mul)}}}{\partial x^{\text{(left)}}}\bigg|_{\hat{X}^{\text{(right)}}} \frac{\partial x^{\text{(left)}}}{\partial \theta^{(l_{l})}}\odot\delta \theta^{(l_{l})}.
        \label{eq:parallelthm_sumr}
    \end{equation}
\end{theorem}
Proof can be seen in \cref{sec:proof_parallel}. Now we can analytically calculate the Hessian-vector product element of each parameter specifically focusing on the interaction between its layer and any other layers that are in parallel connectivity to it. Combining \cref{thm:series_connect} and \cref{thm:parallel}, we can calculate the Hessian-vector product element of each parameter in a neural network. Next, we focus on an efficient calculation the Hessian-vector product element of each parameter.
\vspace{-6pt}
\subsection{Jacobian-Vector Product Forward Propagation}
\vspace{-6pt}
From a computational overhead perspective, the main calculation part within \cref{thm:series_connect} and \cref{thm:parallel} focuses on $\hat{X}^{(l)}$, the gradients of $X^{(l)}$ w.r.t. the parameters of all layers that are in lower series connectivity to $\hat{X}^{(l)}$, as we need to separately back-propagate each entry of $X^{(l)}$, usually a large matrix in attention modules. To address this issue, we introduce Jacobian-Vector Product Forward Propagation (JVPF), a method capable of computing $\hat{X}^{(l)}$ for all layers with an acceptable computational expense.

Let's look at a layer in lower series connectivity to layer $l$ and denote it as layer $l_{\text{low}}$. The derivative of $X^{(l)}$ w.r.t. the parameter $\theta^{(l_{\text{low}})}$ a single layer $l_{\text{low}}$ multiplied with $\delta\theta^{(l_{\text{low}})}$ can be expressed as
\begin{equation}
    \frac{\partial x^{(l)}}{\partial \theta^{(l_{\text{low}})}}\delta \theta^{(l_{\text{low}})}
    =\frac{\partial x^{(l)}}{\partial x^{(l-1)}}\cdots\frac{\partial x^{(l_{\text{low}}+1)}}{\partial x^{(l_{\text{low}})}}\frac{\partial x^{(l_{\text{low}})}}{\partial \theta^{(l_{\text{low}})}}\delta \theta^{(l_{\text{low}})},\nonumber
\end{equation}
indicating that we can calculate it in a layer-by-layer manner. Further, let us group layers in lower series connectivity to layer $l$ into several groups, each of which contains both parameter layers and nonparameter layers that are in series connectivity to each other. We denote the $i^{th}$ group as $\mathbf{G}_i=\{l_{i1}, l_{i2}, \cdots, l_{iN_i}\}$, where $N_i$ is the number of layers in group $\mathbf{G}_i$ and layer $l_{i(j+1)}$ is subsequent to layer $l_{ij}$. Then we have
\begin{equation}
    \label{eq:hatx}
    \hat{x}^{(l)}=\sum_{i}\sum_{l_{\text{low}}\in\mathbf{G}_i}\frac{\partial x^{(l)}}{\partial \theta^{(l_{\text{low}})}}\delta \theta^{(l_{\text{low}})}
    =\sum_i\frac{\partial x^{(l)}}{y^{(l_{N_i})}}f^{(l_{iN_i})}\circ f^{(l_{i(N_i-1)})}\circ\cdots\circ f^{(l_{i1})}(0)\nonumber
\end{equation}
where
\begin{equation}
    \label{eq:surrogate_f}
    f^{(l_{ij})}(x)=
    \begin{cases}
        \frac{\partial y^{(l_{ij})}}{\partial  \theta^{(l_{ij})}}\delta\theta^{(l_{ij})}+\frac{\partial y^{(l_{ij})}}{\partial x^{(l_{ij})}}x  &l_{ij}\text{ has parameters},\\
        \frac{\partial y^{(l_{ij})}}{\partial x^{(l_{ij})}}x  &\text{else}.
    \end{cases}
\end{equation}
$x^{(l_{ij})}$, $y^{(l_{ij})}$, and $\theta^{(l_{ij})}$ are the input, output, and parameters of layer $l_{ij}$. Once we replace the forward function of each layer with \cref{eq:surrogate_f}, we could calculate $\hat{x}^{(l)}$ for each layer $l$ through one forward propagation process, and all series connectivity groups can be calculated in parallel. An intuitive demonstration of JVPF is shown in \cref{fig:jvp_forward}.
\vspace{-0.2cm}
\subsection{Pruning Strategy}
\vspace{-0.2cm}
\begin{wrapfigure}{r}{0.5\textwidth} % "r" for right side, and 0.5\textwidth for the width of the figure
    \centering
    \vspace{-15pt}
    \includegraphics[width=0.5\textwidth]{figures/importance_group.pdf} % Adjust width to slightly less than wrapfigure width
    \caption{Importance score of each neuron in a group is gathered from parameters of lower layers and upper layers.}
    \label{fig:importance_group}
    \vspace{-20pt}
\end{wrapfigure}

Utilizing \cref{thm:series_connect} and \cref{thm:parallel} along as our proposed JVPF, which offers an efficient choice to obtain the essential intermediate values, we can calculate the Hessian-vector product element of each parameter as their importance scores with several batches of traning data. Please refer to appendix \ref{sec:importance_acquisition} for the detailed importance score acquisition. This importance score can be leveraged to conduct unstructured pruning for each parameter individually, or conduct structured pruning for each parameter group.

\vspace{-5pt}
\paragraph{Structured Pruning}
Following \citet{fang2023depgraph}, parameters from in-layer and inter-layer connections can be organized into several groups $G$. The importance scores belonging to each group is defined as $\mathcal{I}_G=\{\mathcal{I}_G^{(i)}|g_i\in G\}$. For each group $g_i\in G$, importance scores are summed on every neuron over parameters of upper layers $L_{u}\subset g_i$ with length $N_u$ and lower layers $L_{l}\subset g_i$ with length $N_l$, as illustrated in in \cref{fig:importance_group}.

\vspace{-5pt}
\paragraph{Unstructured Pruning}
The importance score in unstructured learning is more straightforward. By eliminating the process of gathering importance, considering each parameter layer as a group, and each parameter as a neuron, the definition of importance score becomes similar to what is described in structured pruning scenarios. Experimentally we found that the gradients of some parameters are zero due to gradient vanishing, making us hard to judge the importance of these parameters. This would have little influence on structured pruning since neurons' importance scores are gathered through multiple weights. However, in unstructured pruning, the importance score of each weight only depends on itself. By adding the magnitude of the corresponding weight in the importance score term, the problem is resolved.

In each pruning step, we calculate every importance score $\mathcal{I}_G^{(i,j)}$ of $j^{th}$ neuron in $i^{th}$ group over traning data of $B$ batches, and normalize them within each group. Then we rank them from lowerst to highest. The lowest $p$ percentage of parameters are pruned. We can gradually increase $p$ to iteratively prune the model to a specific FLOPs or parameter percentage.
\vspace{-0.1cm}
\section{Results}
\vspace{-6pt}
We empirically study the performance of OBA on CNNs where only series connectivity exists, and attention networks where both series connectivity and parallel connectivity are present. We focus on pruning towards as small FLOPs as possible to reduce the computation overhead of model inference. For structured pruning, we primarily compare our methods with those that leverage Hessian matrix information~\citep{obs, wang2019eigendamage}. In the context of unstructured pruning, we evaluate our approach against the state-of-the-art unstructured pruning method, CHITA~\citep{benbaki2023fast}. Classical importance acquisition methods Weight (the magnitude of weights), OBD~\citep{obd} and Taylor~\citep{molchanov2016pruning} are also added into comparison for both structured and unstructured pruning tasks. In our experiments, we choose $\delta\theta_i=\theta_i$. The implementation details can be found at \cref{sec:implementation}.

\vspace{-5pt}
\subsection{Structured Pruning}
\vspace{-5pt}
Current structured pruning workflows can be roughly divided into one-shot pruning and iterative pruning. The former prunes the fine-tuned model towards a sparsified network and fine-tunes it after pruning, whereas the latter prunes the model iteratively and fine-tunes the model after each pruning step. One-shot pruning is more efficient than iterative pruning, but the latter is more effective. We evaluate our method on both of these two workflows.

\begin{table}[h]
    \centering
    \vspace{-5pt}
    \begin{minipage}[t]{0.545\linewidth}
        \caption{The Spearman correlation of the importance scores of each method with the importance rank of neurons on ResNet32.}
        \resizebox{\textwidth}{!}{ % Adjust width slightly less than wraptable width to fit caption and avoid overfull hbox
            \begin{tabular}{r|cccccc}
                \toprule\midrule
                \multicolumn{1}{c}{Method} & OBA            & Weight         & Taylor         & OBS   & OBD            & EigenDamage  \\
                \midrule
                \multicolumn{1}{l|}{\textit{all layers}}          & \multicolumn{6}{c}{\textbf{CIFAR100}}                                          \\
                Max                                      & \textbf{0.500} & 0.115          & 0.457          & 0.248 & 0.222          & -0.077       \\
                Standardization                          & \textbf{0.496} & 0.335          & 0.450          & 0.292 & 0.474          & -0.047       \\
                $l_2$-Norm                                  & 0.492          & 0.433          & 0.441          & 0.407 & \textbf{0.541} & -0.012       \\
                None                                     & \textbf{0.444} & -0.247         & -0.157         & 0.222 & 0.275          & 0.046        \\
                \multicolumn{1}{l|}{\textit{per layer}}  & \textbf{0.443} & 0.271          & 0.292          & 0.304 & 0.287          & 0.018        \\
                \midrule
                \multicolumn{1}{l|}{\textit{all layers}} & \multicolumn{6}{c}{\textbf{CIFAR10 }}                                           \\
                Max                                      & \textbf{0.456} & 0.092          & 0.417          & 0.373 & 0.426          & 0.165        \\
                Standardization                          & 0.382          & 0.049          & \textbf{0.444} & 0.166 & 0.412          & 0.166        \\
                $l_2$-norm                                  & 0.402          & \textbf{0.443} & 0.433          & 0.367 & 0.186          & 0.192        \\
                None                                     & \textbf{0.379} & -0.187         & 0.080          & 0.360 & 0.342          & 0.172        \\
                \multicolumn{1}{l|}{\textit{per layer}}  & \textbf{0.421} & 0.289          & 0.409          & 0.362 & 0.347          & 0.056        \\
                \midrule\bottomrule
            \end{tabular}
        }
        \label{tab:resnet50_rank}
    \end{minipage}
    % \hfill
    \begin{minipage}[t]{0.435\linewidth} % Adjust the width to fit your tables
        \caption{Comparison on Accuracies (\%) and speed up up ratios with recent pruning works on ImageNet.}
    \vspace{5pt}
    \resizebox{1\columnwidth}{!}{
        \begin{tabular}{rcc} 
            \toprule\midrule
            \multicolumn{1}{c}{Method}            & Pruned                  & Speed Up      \\ 
            \midrule
            \multicolumn{1}{l}{\textbf{Resnet50}} &                         &               \\
            Weight                                & 75.12$_{-1.03}$           & $1.99\times$  \\
            C-OBD~\citep{wang2019eigendamage}                                 & 74.86$_{-1.29}$                   & $1.99\times$          \\
            C-OBS~\citep{wang2019eigendamage}                                 & 75.48$_{-0.67}$                   & $2.01\times$          \\
            EigenDamage~\citep{wang2019eigendamage}                           & 75.30$_{-0.85}$                   & $2.00\times$          \\
            Taylor~\citep{molchanov2016pruning}                                & 75.26$_{-0.89}$           & $2.00\times$  \\
            OBA(Ours)                             & \textbf{75.62}$_{-0.53}$  & $2.00\times$  \\ 
            \midrule
            \multicolumn{1}{l}{\textbf{ViT-B/16}} &                         &               \\
            Weight                                & 77.03$_{-4.04}$          & $1.32\times$  \\
            Taylor~\citep{molchanov2016pruning}                               & 77.65$_{-3.42}$          & $1.31\times$  \\
            OBA(Ours)                             & \textbf{79.64}$_{-1.43}$ & $1.30\times$  \\
            \midrule\bottomrule
            \end{tabular}
    }
    \label{tab:imagenet}
    \end{minipage}
    \vspace{-10pt}
\end{table}


\subsubsection{Importance Score Rank Characterization}

We first measure the ability of our method to characterize the importance of each neuron for structured pruning. Intuitively, the importance score of a neuron should be positively correlated to the change of loss when removing a neuron. This can be implemented by masking the neuron's output in the evaluation process. Specifically, we mask the output of each neuron in pre-defined layers and compare the change of loss between the masked model and unmasked model, which is refered to as ground truth importance. Then we calculate the Spearman's rank correlation of the importance scores with estimated importance scores from different methods. The higher the Spearman's rank score, the better the method captures the importance. We select the output neurons of the first convolutional layer and the three following residual blocks of ResNet32~\citep{He_2016_CVPR} as candidates. For per-layer case, Spearman correlation is calculated within each layer and averaged. For all-layer case, before calculating Spearman correlation, we first normalize the importance scores of each layer and concatenate them together. Different layer-wise normalization methods are considered in the evaluation and are detailedly introduced in \cref{sec:normalization}.

As shown in \cref{tab:resnet50_rank}, with evaluation within each layer, OBA yields the best rank similarity to the ground truth importance among all methods. In the all-layer importance condition, our method also yields good performance on different normalization methods. It is noteworthy that methods such as Weight, Taylor, and OBD, although potentially exhibiting higher correlation scores in importance under specific normalization schemes, may also yield notably low correlation scores under certain normalization techniques. In contrast, OBA consistently maintains relatively high correlation scores irrespective of the normalization method applied. This proves our method's capability of importance capturing, along with its robustness to different normalization methods.

\subsubsection{One-shot Pruning Results}
Next, we conduct experiments to evaluate the performance of OBA on pruning towards a specific FLOPs percentage in one-shot pruning workflow. As the FLOPs and number of parameters are not linearly dependent w.r.t. the number of neurons in the neural network, it's hard to calculate a pruning ratio under which the network is pruned into a predefined FLOPs or parameter number. Thus we prune the network for several steps with an increasing pruning ratio. FLOPs and number of parameters are calculated after each step to check whether the network satisfies the target. We first evaluate our method on ImageNet with ResNet50~\citep{He_2016_CVPR} and ViT-B/16~\citep{dosovitskiy2020image}. As shown in \cref{tab:imagenet}, our method realizes a $2\times$ speed up at ImageNet on ResNet50 with an accuracy decrease of only $0.53\%$. Our method outperforms other methods on both ResNet50 and ViT-B/16, which demonstrates the effectiveness of our method on large-scale datasets. On ViT-B/16, our method achieves a $1.30\times$ speed up with an accuracy decrease of $1.43\%$, which is far smaller than those achieved by Taylor criteria and Weight criteria, demonstrating our proposed criteria's superiority over Weight criteria and the first-order Taylor criteria. 

Next we evaluate our method on CIFAR10 and CIFAR100 datasets. The results are shown in \cref{tab:oneshot_results}. Our method achieves the best results on ResNet32 model across the both datasets. When pruning network towards a small $6\%$ target FLOPs, our method on ResNet32 surpasses other methods a lot, with only $0.79\%$ accuracy loss on CIFAR10 datasets. On CIFAR10 with VGG19 the performance of OBA is slightly worse, but still comparable to other methods, with relatively lower FLOPs. We observe an interesting phenomenon that the parameter reduction of models pruned with OBA are slightly higher than other methods under similar FLOPs, which indicates that our method tends to prune neurons with relatively higher FLOPs, and is more suitable for applications of lower FLOPs requirements.



\begin{table}[htbp]
    \centering
    \caption{The accruacies (\%), weights reduction (\%), and FLOPs reduction (\%) of different methods across CIFAR10 and CIFAR100 under one-shot pruning with ResNet32 and VGG19.}
    \label{tab:oneshot_results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{r|ccc|ccc||ccc|ccc} 
    \toprule\midrule
    \multicolumn{1}{c}{\multirow{2}{*}{Method}}      & \multicolumn{6}{c}{\textbf{~CIFAR10 }}                                                                      & \multicolumn{6}{c}{~\textbf{CIFAR100 }}                                                  \\ 
    \cmidrule(l){2-13}
    \multicolumn{1}{c}{}                             & Acc            & Weights & \multicolumn{1}{c}{FLOPs} & Acc            & Weights & \multicolumn{1}{c}{FLOPs} & Acc            & Weights & \multicolumn{1}{c}{FLOPs} & Acc            & Weights & FLOPs  \\ 
    \midrule
    \multicolumn{1}{l|}{\textbf{VGG19(Baseline)}}    & 94.17          &         &                           &                &         &                           & 73.34          &         &                           &                &         &        \\
    NN Slimming~\citep{liu2017learning}                                      & 92.84          & 80.07   & 42.65                     & 85.01          & 97.85   & 97.89                     & 71.89          & 74.60   & 38.33                     & 58.69          & 97.76   & 94.09  \\
    C-OBD~\citep{wang2019eigendamage}                                            & 94.04          & 82.01   & 38.18                     & 92.34          & 97.68   & 77.39                     & 72.23          & 77.03   & 33.70                     & 58.07          & 97.97   & 77.55  \\
    C-OBS~\citep{wang2019eigendamage}                                            & 94.08          & 76.96   & 34.73                     & 91.92          & 97.27   & 87.53                     & 72.27          & 73.83   & 38.09                     & 58.87          & 97.61   & 91.94  \\
    Kron-OBD~\citep{wang2019eigendamage}                                         & 94.00          & 80.40   & 38.19                     & \textbf{92.92} & 97.47   & 81.44                     & 72.29          & 77.24   & 37.90                     & 60.70          & 97.56   & 82.55  \\
    Kron-OBS~\citep{wang2019eigendamage}                                         & 94.09          & 79.71   & 36.93                     & 92.56          & 97.32   & 80.39                     & 72.12          & 74.18   & 36.59                     & 60.66          & 97.48   & 83.57  \\
    EigenDamage~\citep{wang2019eigendamage}                                      & 93.98          & 78.18   & 37.13                     & 92.29          & 97.15   & 86.51                     & \textbf{72.90} & 76.64   & 37.40                     & 65.18          & 97.31   & 88.63  \\
    Weight                                           & 93.85          & 68.57   & 37.21                     & 91.85          & 97.02   & 86.93                     & 72.14          & 67.89   & 37.02                     & 54.63          & 95.90   & 88.30  \\
    Taylor~\citep{molchanov2016pruning}                                           & \textbf{94.11} & 62.29   & 38.28                     & 92.29          & 93.89   & 86.29                     & 71.82          & 55.58   & 37.76                     & 66.65          & 93.12   & 88.28  \\
    OBA~(Ours)                                       & 93.9           & 56.63   & 38.06                     & 92.48          & 91.89   & 86.27                     & 72.36          & 53.33   & 37.62                     & \textbf{66.72} & 92.74   & 88.80  \\ 
    \midrule
    \multicolumn{1}{l|}{\textbf{ResNet32(Baseline)}} & 95.3           &         &                           &                &         &                           & 78.17          &         &                           &                &         &        \\
    C-OBD~\citep{wang2019eigendamage}                                            & 95.11          & 70.36   & 66.18                     & 91.75          & 97.30   & 93.50                     & 75.70          & 66.68   & 67.53                     & 59.52          & 97.74   & 94.88  \\
    C-OBS~\citep{wang2019eigendamage}                                            & 95.04          & 67.90   & 76.75                     & 90.04          & 95.49   & 97.39                     & 75.16          & 66.83   & 76.59                     & 58.20          & 91.99   & 96.27  \\
    Kron-OBD~\citep{wang2019eigendamage}                                         & 95.11          & 63.97   & 63.41                     & 92.57          & 96.11   & 94.18                     & 75.86          & 63.92   & 62.97                     & 62.42          & 96.42   & 95.85  \\
    Kron-OBS~\citep{wang2019eigendamage}                                         & 95.14          & 64.21   & 61.89                     & 92.76          & 96.14   & 94.37                     & 75.98          & 62.36   & 60.41                     & 63.62          & 93.56   & 95.65  \\
    EigenDamage~\citep{wang2019eigendamage}                                      & 95.17          & 71.99   & 70.25                     & 93.05          & 96.05   & 94.74                     & 75.51          & 69.80   & 71.62                     & 65.72          & 95.21   & 94.62  \\
    Weight                                           & 94.51          & 55.47   & 71.99                     & 92.07          & 91.61   & 94.09                     & 75.72          & 65.76   & 70.08                     & 66.09          & 95.44   & 94.63  \\
    Taylor~\citep{molchanov2016pruning}                                           & 95.06          & 66.57   & 71.40                     & 93.32          & 94.88   & 94.05                     & 76.13          & 64.27   & 70.11                     & 64.56          & 90.35   & 94.99  \\
    OBA~(Ours)                                       & \textbf{95.19} & 63.90   & 71.13                     & \textbf{93.45} & 93.68   & 94.51                     & \textbf{76.47} & 64.34   & 70.02                     & \textbf{67.81} & 94.43   & 94.40  \\
    \midrule\bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
    \end{table}

    \begin{figure}[t]
        \centering
        \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=3.7cm]{figures/iterative_prune/cifar100flops.pdf}
        \end{minipage}
        \begin{minipage}[t]{0.24\textwidth}
        \centering
        \includegraphics[width=3.7cm]{figures/iterative_prune/cifar100parameters.pdf}
        \end{minipage}
        \begin{minipage}[t]{0.24\textwidth}
            \centering
            \includegraphics[width=3.7cm]{figures/iterative_prune/cifar10flops.pdf}
            \end{minipage}
            \begin{minipage}[t]{0.24\textwidth}
                \centering
                \includegraphics[width=3.7cm]{figures/iterative_prune/cifar10parameters.pdf}
                \end{minipage}
        \caption{Iterative pruning results on CIFAR10 and CIFAR100 with ResNet32.}\label{fig:iterative_prune}\vspace{-16pt}
    \end{figure}

\subsubsection{Iterative Pruning Results}
In this subsection, we evaluate the performance of OBA under iterative pruning workflow on ResNet32. Specifically, we set a list of target FLOPs for each dataset and model, and iteratively prune the model from largest FLOPs to smallest FLOPs. Fine-tuning is conducted after each pruning step. In terms of FLOPs, the accuracy loss for each iteration of our method is lower than other methods, as shown in \cref{fig:iterative_prune}. This validates the effectiveness of our proposed criteria. As for parameter reduction, our method yields less advantage over other methods, which is consistent with the results of one-shot pruning.

\subsection{Unstructured Pruning}
We also evaluate the performance of OBA on unstructured pruning task. We follow a similar setting of the multi-stage pruning in CHITA~\citep{benbaki2023fast} to have a fair comparison. Since Taylor~\citep{molchanov2016pruning}, Weight, and OBD~\citep{obd} all obtain the importance scores from each parameter, we can ignore their importance gathering steps and add them into comparison. The results are shown in \cref{tab:cifar10} and \cref{tab:imagenet}. Given the varying performance of the initial unpruned networks, we directly compare the accuracy ratio relative to the raw accuracy of all methods. The main version of CHITA, i.e. CHITA-CD, has very large computational time and memory cost on Resnet50, making itself infeasible for such huge networks. Thus we implemented the more efficient CHITA-BSO for comparison. It can be seen that Taylor and OBD fails on this task as their accuracies rapidly fall into $10\%$ in the first pruning step. OBA's results on high sparsities surpass CHITA++ by a huge margin, proving itself to be effective in unstructured pruning task.

\begin{table}
    \centering
    \caption{The unstructured pruning results on CIFAR-10 dataset with ResNet-20.}
    \label{tab:cifar10}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccll} 
    \toprule\midrule
    \multirow{2}{*}{Sparsity} & \multicolumn{2}{c}{Taylor (91\%)} & \multicolumn{2}{c}{OBD (91\%)} & \multicolumn{2}{c}{Weight (91\%)} & \multicolumn{2}{c}{OBA  (91\%)} & \multicolumn{2}{l}{CHITA++ (91.36\%)}  \\ 
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                              & Accuracy (\%) & Ratio (\%)        & Accuracy (\%) & Ratio (\%)     & Accuracy (\%) & Ratio (\%)        & Accuracy (\%) & Ratio (\%)      & Accuracy (\%) & Ratio (\%)             \\ 
    \midrule
    0.1                       & 11.00         & 14.45             & 10.03         & 13.17          & 90.66         & 99.63             & 90.83         & \textbf{99.81}  & -             & -                      \\
    0.2                       & 11.00         & 14.45             & 10.03         & 13.17          & 90.82         & 99.80             & 90.90         & \textbf{99.89}  & -             & -                      \\
    0.3                       & 10.00         & 13.14             & 10.03         & 13.17          & 90.67         & 99.64             & 90.65         & 99.62           & 91.25         & \textbf{99.88}         \\
    0.4                       & 10.80         & 14.19             & 10.02         & 13.16          & 90.67         & 99.64             & 90.35         & 99.29           & 91.20         & \textbf{99.82}         \\
    0.5                       & 10.00         & 13.14             & 10.01         & 13.15          & 90.79         & \textbf{99.77}    & 90.57         & 99.53           & 91.04         & 99.65                  \\
    0.6                       & 8.20          & 10.77             & 10.00         & 13.14          & 90.23         & 99.15             & 90.69         & \textbf{99.66}  & 90.78         & 99.37                  \\
    0.7                       & 10.00         & 13.14             & 10.00         & 13.14          & 88.83         & 97.62             & 89.94         & 98.84           & 90.38         & \textbf{98.93}         \\
    0.8                       & 10.00         & 13.14             & 10.00         & 13.14          & 85.03         & 93.44             & 89.64         & \textbf{98.51}  & 88.72         & 97.11                  \\
    0.9                       & 10.00         & 13.14             & 10.52         & 13.82          & 67.00         & 73.63             & 86.27         & \textbf{94.80}  & 79.32         & 86.82                  \\
    \midrule\bottomrule
    \end{tabular}
    }
    \end{table}

    \begin{table}
        \centering
        \caption{The unstructured pruning results on Imagenet dataset with ResNet-50.}
        \label{tab:imagenet}
        \resizebox{\textwidth}{!}{
            \begin{tabular}{cccccccccll} 
                \toprule\midrule
                \multirow{2}{*}{Sparsity} & \multicolumn{2}{c}{Taylor (76.13\%)} & \multicolumn{2}{c}{OBD (76.13\%)} & \multicolumn{2}{c}{Weight
                  (76.13\%)} & \multicolumn{2}{c}{OBA~ (76.13\%)} & \multicolumn{2}{l}{CHITA-BSO++ (77.01\%)}  \\ 
                  \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
                                          & Accuracy (\%) & Ratio (\%)           & Accuracy (\%) & Ratio (\%)        & Accuracy (\%) & Ratio (\%)             & Accuracy (\%) & Ratio (\%)         & Accuracy (\%) & Ratio (\%)                 \\ 
                \midrule
                0.1                       & 0.11          & 0.14                 & 0.10          & 0.13              & 75.30         & 98.91                  & 75.67         & 99.40              & 77.00         & \textbf{99.98}             \\
                0.2                       & 0.11          & 0.14                 & 0.10          & 0.13              & 75.23         & 98.82                  & 75.40         & 99.04              & 76.91         & \textbf{99.87}             \\
                0.3                       & 0.10          & 0.13                 & 0.10          & 0.13              & 74.57         & 97.95                  & 74.93         & 98.42              & 76.87         & \textbf{99.82}             \\
                0.4                       & 0.11          & 0.14                 & 0.10          & 0.13              & 73.54         & 96.60                  & 74.50         & 97.86              & 76.59         & \textbf{99.46}             \\
                0.5                       & 0.10          & 0.13                 & 0.10          & 0.13              & 70.73         & 92.91                  & 73.42         & 96.44              & 76.01         & \textbf{98.70}             \\
                0.6                       & 0.08          & 0.11                 & 0.10          & 0.13              & 64.97         & 85.34                  & 70.88         & \textbf{93.10}     & 68.89         & 89.46                      \\
                0.7                       & 0.10          & 0.13                 & 0.10          & 0.13              & 48.09         & 63.17                  & 65.57         & \textbf{86.13}     & 64.38         & 83.60                      \\
                0.8                       & 0.10          & 0.13                 & 0.09          & 0.12              & 16.08         & 21.12                  & 47.14         & \textbf{61.92}     & 26.21         & 34.03                      \\
                0.9                       & 0.10          & 0.13                 & 0.10          & 0.13              & 0.80          & 1.05                   & 5.65          & \textbf{7.43}      & 0.43          & 0.56                       \\
                \midrule\bottomrule
                \end{tabular}
        }
        \end{table}

\begin{table}[t!]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \resizebox{\linewidth}{!}{ % Adjust width slightly less than wraptable width to fit caption and avoid overfull hbox
        \begin{tabular}{ccc} 
            \toprule\midrule
                            & Resnet32 & ViT-B/16  \\ 
            \midrule
            Regular Traning & 0.326    & 0.252     \\
            Upper Series    & 0.881    & 0.799     \\
            Lower Series    & 0.819    & 1.166     \\
            Parallel        & -        & 1.944     \\
            Total           & 2.072    & 4.027     \\
            \midrule\bottomrule
            \end{tabular}
        }
        \caption{}
        \label{tab:oba_time}
    \end{subfigure}
    \begin{subfigure}{0.415\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/time_resnet32.pdf}
        \caption{}
        \label{fig:time_resnet32}
    \end{subfigure}
    \begin{subfigure}{0.265\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/time_resnet20.pdf}
        \caption{}
        \label{fig:time_resnet20}
    \end{subfigure}
    \vspace{-5pt}
    \caption{The running time (s) of OBA on different models and datasets. \textbf{(a)} The specific running time (s/iteration) of each part of OBA on ResNet32 and ViT-B/16. \textbf{(b)} The running time of OBA on ResNet32 with CIFAR100. \textbf{(c)} The running time of OBA on ResNet20 with CIFAR10.}
    \label{tab:run_time}
\end{table}

\vspace{-3pt}
\subsection{Run Time Analysis}
\vspace{-3pt}
Here, we empirically study the time consumption of OBA. \Cref{tab:oba_time} shows the time consumption of each part of OBA, and the time costed by regular training. It can be seen that the time cost of computing parallel connectivity is the most time-consuming part of OBA, nearly same to the time of series connectivity. In network structures that do not contain multiplication operations, the time cost of OBA would be much lower. In \cref{fig:time_resnet32}, 200 batches of data with a batch size of 64 are leveraged in each pruning iteration. In each pruning stage we conduct 50 pruning iterations to gradually prune the network, and the overall computation time of pruning would be around 4.2 hours, which is acceptable in the actual pruning scenarios. In \cref{fig:time_resnet20}, we samely use 200 batches of data to calculate gradients. The mainly proposed method in CHITA, CHITA-CD, would require twice as much time compared to our approach. However, the performance of both CHITA-CD and CHITA-BSO under high sparsity is worse than OBA, showcasing the efficiency of our method.

\section{Conclusion and Limitation}
\vspace{-6pt}
In this paper, we propose Optimal Brain Apoptosis, a novel method for pruning neural networks. We first provide theoretical analysis on modern neural network structures to figure out nonzero Hessian submatrix conditions between layers. Then we propose an efficient approach that directly calculates the Hessian-vector product values for each parameter in the network, thereby calculating the second-order Taylor expansion for each parameter without any approximation. We empirically demonstrate the efficacy of our method on both structured pruning and unstructured pruning.

\paragraph{Limitation} OBA, in its current form, can be applied to network structures including MLPs, CNNs, and Transformers. For networks with more complex architectures, like RNNs and State Space Models that handle time-series data, computing the Hessian matrix becomes more difficult and necessitates additional research. This is an interesting area that warrants further exploration in the future.

\section{Acknowledgement}
We sincerely appreciate the dedication and valuable feedback provided by all anonymous reviewers. This research was partially supported by the National Natural Science Foundation of China under Grants 62073066 and 62405255, as well as the Fundamental Research Funds for the Central Universities under Grant N2226001. Additionally, it received support from the 111 Project under Grant B16009 and the Intel Neuromorphic Research Community (INRC) Grant Award (RV2.137.Fang).

\bibliography{oba}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
{\LARGE{\textsc{Appendix}}}
\section{Implementation Details}\label{sec:implementation}
In our structured pruning experiments, we align our settings to EigenDamage~\citep{wang2019eigendamage}, which is a good Hessian based pruning method. For each pruning step, we obtain the importance score from $200$ batches of data to calculate the importance score. We set the batch size to $64$ and the learning rate to $0.001$ for the fine-tuning process. We use the SGD optimizer with a momentum of $0.9$ and a weight decay of $5\times10^{-4}$. The learning rate is divided by $10$ at the $80$th and $120$th epochs. We set the maximum epochs to $150$ for CIFAR10 and CIFAR100 datasets. For ImageNet experiments, we use the same settings as \citet{fang2023depgraph} of ResNet50 and ViT-B/16. We set the maximum epochs to $100$ for ImageNet experiments.

\begin{algorithm}[!h]
    \caption{Importance Score Acquisition of OBA}
    \label{alg:importance}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
        \REQUIRE model $m$ and its parameters $\theta$, a batch of data $\mathcal{D}$  %%input
        \ENSURE parameter importance $\mathcal{I}$    %%output
        \STATE Initialized importance dict $\mathcal{I}$
        \STATE Conduct one forward propagation and back propagation process on model $m$ with data $\mathcal{D}$ and record the output gradient $\frac{\partial \mathcal{L}}{\partial y^{(l)}}$ for each layer $l$
        \FOR{parameter layer $l$ in $m$}
            \STATE$x^{(l)}$.backward($\frac{\partial \mathcal{L}}{\partial y^{(l)}} \mathbf{J}^{(l)}_{\delta W^{(l)}}$)
        \ENDFOR
        \FOR{parameter layer $l$ in $m$}
            \STATE $\mathcal{I}[l]\leftarrow \delta\theta^{(l)}\odot\theta^{(l)}.\text{grad}$
        \ENDFOR
        \STATE Conduct JVPF according to \cref{eq:surrogate_f} and record $\hat{X}^{(l)}$ for each layer $l$
        \STATE $m$.zero\_grad()
        \FOR{parameter layer $l$ in $m$}
            \STATE $\mathcal{I}[l]\leftarrow \mathcal{I}[l]+\frac{\partial \mathcal{L}}{\partial y^{(l)}} \frac{\partial y^{(l)}}{\partial \theta^{(l)}}\big|_{\hat{X}^{(l)}}\odot\delta\theta^{(l)}$
            \IF{layer $l$ is attention module}
                \STATE $\hat{S} = \operatorname{softmax}\bigl((Q^{(l)}\hat{K}^{(l)\mathsf{T}} + \hat{Q}^{(l)}K^{(l)\mathsf{T}})\big/\sqrt{d^{(l)}_k}\bigr)$
                \STATE $S = \operatorname{softmax}\bigl(Q^{(l)}K^{(l)\mathsf{T}}\big/\sqrt{d^{(l)}_k}\bigr)$
                \STATE $\hat{O} = \hat{S}V + S\hat{V}$, $O = SV$
                \STATE $\hat{O}$.backward($\frac{\partial \mathcal{L}}{\partial O}$)
            \ENDIF
        \ENDFOR
        \FOR{parameter layer $l$ in $m$}
            \STATE $\mathcal{I}[l]\leftarrow \mathcal{I}[l] + \delta\theta^{(l)}\odot\theta^{(l)}.\text{grad}$
        \ENDFOR
        \STATE $m$.zero\_grad()
    \end{algorithmic}
\end{algorithm}

\section{Algorithmic Details}\label{sec:importance_acquisition}
As can be seen in \cref{alg:importance}, We first conduct a forward propagation process on the network and record the output gradient of the loss w.r.t. the output of each layer (line 1-2). Then we back-propagate these gradients for each parameter layer with corresponding weight $\delta \theta$ to obtain the term $\sum_{l'\in\mathbf{L}_{\text{up}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial \theta^{(l)}}\delta\theta^{(l')}_j$ (line 3-5). 

1. \textbf{Upper Series Connectivity:} These terms are recorded in the gradient of the corresponding parameters, so that we can obtain $\sum_{l'\in\mathbf{L}_{\text{up}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial \theta^{(l)}}\delta\theta^{(l')}_j\odot\delta \theta^{(l)}$ in \cref{eq:series_w} by multiplying the parameters with their gradients (line 6-7). 

2. Next, We obtain $\hat{X}^{(l)}_{mn}=\sum_{l_{\text{low}}\in\mathbf{L}_{\text{low}}}\sum_{k}\frac{\partial X^{(l)}_{mn}}{\partial \theta^{(l_{\text{low}})}_k}\delta\theta^{(l_{\text{low}})}_k$ for all parameter layers through the JVPF, these values are useful for calculating \textbf{Lower Series Connectivity} cases and \textbf{Parallel Connectivity} cases (line 9). 

3. \textbf{Lower Series Connectivity:} We obtain the latter term of \cref{eq:series_w} and add them into the importance scores (line 12).

4. \textbf{Parallel Connectivity:} In the meantime, for all attention layers that induce parallel connectivity, we back-propagate the gradient with the surrogate inputs $\hat{X}_{\text{left}}$ and $\hat{X}_{\text{right}}$ according to \cref{eq:parallelthm_suml,eq:parallelthm_sumr} (line 13-17), and multiply the parameters with their gradients that are in \textbf{Lower Series Connectivity} with the attention layer (line 20-21).


\section{Normalization Methods}
\label{sec:normalization}
In our implementation, We leverage these normalization methods on importance scores for each group, including:

\paragraph{No Normalization (None)}

    When the normalizer is set to None, the original values of importance scores are returned without any modification. This means that the data is used as-is, with all its original properties (such as scale and distribution) intact:
    \begin{equation*}
        I^\text{normalized}_j = I_j.
    \end{equation*}

\paragraph{Standardization (or Min-Max Normalization)}
    This method scales the data so that it fits within a specific range, typically $0$ to $1$. This is achieved by subtracting the minimum value of the data and then dividing by the range of the data:
    \begin{equation*}
        I^\text{normalized}_j = \frac{I_j - \min(I)}{\max(I) - \min(I)}.
    \end{equation*}

\paragraph{Max Normalization}
    In this approach, every importance score is divided by the maximum importance score of corresponding group to ensure that all the normalized values fall between 0 and 1:
    \begin{equation*}
        I^\text{normalized}_j = \frac{I_j}{\max(I)}.
    \end{equation*}

\paragraph{$l_2$ Normalization}
    This method normalizes the importance scores by dividing it by the $l_2$ norm (Euclidean norm) of the importance scores belonging to the same group. The $l_2$ norm is calculated as the square root of the sum of the squared values:
    \begin{equation*}
        I^\text{normalized}_j = \frac{I_j}{||I||_2}.
    \end{equation*}


\begin{figure}[h!]
    \centering
    \begin{minipage}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=7.4cm]{figures/resnet32_flops.pdf}
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=7.4cm]{figures/vgg19_flops.pdf}
    \end{minipage}
    \caption{The layer-wise FLOPs for all parameter layers from models pruned by different criteria.}\label{fig:flops}
\end{figure}

\section{Pruned Layers Visualization}
We visualized the layer-wise FLOPs of pruned models by OBA and other methods for ResNet32 and VGG19 on CIFAR100 with a target FLOPs of $6\%$. It can be seen that compared with other methods, the difference of OBA between FLOPs of different layers is smaller, resulting in a smoother model in terms of number of neurons across layers. This significantly helps to improve the model's performance across various datasets and provide useful guidance for researchers to design and prune neural networks.

\section{Proofs}
\subsection{\Cref{thm:series_connect}}
\label{sec:proof_series}
For any two layers $l_{\text{low}}$ and $l_{\text{up}}$ in series connectivity, where $l_{\text{up}}$ is upper than $l_{\text{low}}$. Note that $\frac{\partial^2 \mathcal{L}}{\partial w^{(l_{\text{low}})}\partial b^{(l_{\text{up}})}}$ is a zero matrix because parameters $w^{(l_{\text{low}})}$ and $b^{(l_{\text{up}})}$ are independent of each other. With this prior, $\frac{\partial^2 \mathcal{L}}{\partial\theta_i^{(l_{\text{low}})}\partial\theta_j^{(l_{\text{up}})}}$ is actually $\frac{\partial^2 \mathcal{L}}{\partial\theta_i^{(l_{\text{low}})}\partial w_j^{(l_{\text{up}})}}$. We first calculate term $\sum_j\frac{\partial^2 \mathcal{L}}{\partial\theta_i^{(l_{\text{low}})}\partial w_j^{(l_{\text{up}})}}\delta\theta_i^{(l_{\text{low}})}\delta w_j^{(l_{\text{up}})}$ for the lower layer $l_{\text{low}}$.
\begin{lemma}
    \label{lemma:series_connect_upper}
    \begin{equation}
        \sum_{fhi}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi}\partial W^{(l_{\text{low}})}_{acd}} \delta W^{(l_{\text{up}})}_{fhi} \delta W^{(l_{\text{low}})}_{acd}= \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial W^{(l_{\text{low}})}_{acd}} \delta W^{(l_{\text{low}})}_{acd}
    \end{equation}
    and
    \begin{equation}
        \sum_{fhi}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi}\partial b^{(l_{\text{low}})}_{a}} \delta W^{(l_{\text{up}})}_{fhi} \delta b^{(l_{\text{low}})}_{a}= \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial b^{(l_{\text{low}})}_{a}} b^{(l_{\text{low}})}_{a},
    \end{equation}
where $\mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}\in\mathbb{R}^{(l_{\text{out}}\cdot m_{\text{out}}) \times (l_{\text{in}} \cdot m_{\text{in}})}$ is the jacobian matrix of $y^{(l_{\text{up}})}$ with respect to $x^{(l_{\text{up}})}$ taking $\delta W^{(l_{\text{up}})}$ as the weights.
\end{lemma}
\begin{proof}
Let $\mathbf{J}^{(l_{\text{up}})}_{W^{(l_{\text{up}})}}=\frac{\partial y^{(l_{\text{up}})}}{\partial x^{(l_{\text{up}})}}$. The element-wise derivative of loss w.r.t. $X^{(l_{\text{up}})}$ is given by
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial X^{(l_{\text{up}})}_{hj}} = \sum_{fg}\frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \underbrace{\sum_{ij}W^{(l_{\text{up}})}_{fhi} M^{(l_{\text{up}})}_{gij}}_{\partial Y^{(l_{\text{up}})}_{fg}/\partial X^{(l_{\text{up}})}_{hj}},
\end{equation}
and the element-wise derivative of loss w.r.t. $W^{(l_{\text{up}})}$ is given by
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi}} = \sum_{g}\frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \sum_{j}X^{(l_{\text{up}})}_{hj} M^{(l_{\text{up}})}_{gij}.
\end{equation}
Applying chain rule, we have

\begin{align}
    \sum_{fhi}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi} \partial W^{(l_{\text{low}})}_{acd}} \delta W^{(l_{\text{up}})}_{fhi} \delta W^{(l_{\text{low}})}_{acd}&= \sum_{fg}\frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \sum_{hij} M^{(l_{\text{up}})}_{gij} \delta W^{(l_{\text{up}})}_{fhi} \frac{\partial X^{(l_{\text{up}})}_{hj}}{\partial W^{(l_{\text{low}})}_{acd}} \delta W^{(l_{\text{low}})}_{acd} \nonumber\\
    &= \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial W^{(l_{\text{low}})}_{acd}} \delta W^{(l_{\text{low}})}_{acd},
\end{align}
and
\begin{align}
    \sum_{fhi}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi} \partial b^{(l_{\text{low}})}_{a}} \delta W^{(l_{\text{up}})}_{fhi} \delta b^{(l_{\text{low}})}_{a}&= \sum_{fg}\frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \sum_{hij} M^{(l_{\text{up}})}_{gij} \delta W^{(l_{\text{up}})}_{fhi} \frac{\partial X^{(l_{\text{up}})}_{hj}}{\partial b^{(l_{\text{low}})}_{a}} b^{(l_{\text{low}})}_{a} \nonumber\\
    &= \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial b^{(l_{\text{low}})}_{a}} b^{(l_{\text{low}})}_{a}.
\end{align}
\end{proof}

Next we obtain term $\sum_i\frac{\partial^2 \mathcal{L}}{\partial\theta_i^{(l_{\text{low}})}\partial w_j^{(l_{\text{up}})}}\delta\theta_i^{(l_{\text{low}})}\delta w_j^{(l_{\text{up}})}$ for the upper layer $l_{\text{up}}$.
\begin{lemma}
    \label{lemma:series_connect_lower}
    \begin{equation}
    \label{eq:series_connect_lower}
    \sum_{a}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi} \partial \theta^{(l_{\text{low}})}_{a}} \delta W^{(l_{\text{up}})}_{fhi} \delta \theta^{(l_{\text{low}})}_{a} = \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \frac{\partial y^{(l_{\text{up}})}}{\partial W^{(l_{\text{up}})}_{fhi}}\bigg|_{\hat{X}^{(l_{\text{up}},l_{\text{low}})}}\delta W^{(l_{\text{up}})}_{fhi},
    \end{equation}
    where $\hat{X}^{(l_{\text{up}},l_{\text{low}})}$ is given by
    \begin{equation}
        \hat{X}^{(l_{\text{up}},l_{\text{low}})}_{hj}=\sum_{a}\frac{\partial X^{(l_{\text{up}},l_{\text{low}})}_{hj}}{\partial \theta^{(l_{\text{low}})}_a}\delta\theta^{(l_{\text{low}})}_a.
    \end{equation}
% $\delta \hat{W}^{(l_{\text{up}})}$ is a tensor with the same shape as $\delta W^{(l_{\text{up}})}$ and its all elements are zeros except for $\delta \hat{W}^{(l_{\text{up}})}_{fhi}=\delta W^{(l_{\text{up}})}_{fhi}$.
\end{lemma}
\begin{proof}
\begin{align}
    \frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \frac{\partial y^{(l_{\text{up}})}}{\partial W^{(l_{\text{up}})}_{fhi}}\bigg|_{\hat{X}^{(l_{\text{up}})}}\delta W^{(l_{\text{up}})}_{fhi}&=\sum_{fg}\frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \sum_{j}M^{(l_{\text{up}})}_{gij} \sum_{a}\frac{\partial X^{(l_{\text{up}})}_{hj}}{\partial \theta^{(l_{\text{low}})}_{a}} \delta \theta^{(l_{\text{low}})}_{a}\delta W^{(l_{\text{up}})}_{fhi}\nonumber\\
    &= \sum_{a} \sum_{g} \frac{\partial \mathcal{L}}{\partial Y^{(l_{\text{up}})}_{fg}} \sum_{j} M^{(l_{\text{up}})}_{gij} \frac{\partial X^{(l_{\text{up}})}_{hj}}{\partial \theta^{(l_{\text{low}})}_{a}} \delta  \theta^{(l_{\text{low}})}_{a} \delta W^{(l_{\text{up}})}_{fhi}\nonumber\\
    &=\sum_{a}\frac{\partial^2 \mathcal{L}}{\partial W^{(l_{\text{up}})}_{fhi} \partial \theta^{(l_{\text{up}})}_{a}} \delta \theta^{(l_{\text{low}})}_{a} \delta W^{(l_{\text{up}})}_{fhi}.
\label{eq:expanded_rightpart}
\end{align}


\end{proof}

\begin{proof}[Proof of \cref{thm:series_connect}]
    Sum the results in \cref{lemma:series_connect_upper} for upper series connectivity cases with results in \cref{lemma:series_connect_lower} for lower series connectivity cases and we get
    \begin{equation}
        \label{eq:series_w_elementwise}
        \sum_{l'\in\mathbf{L}_{\text{up}}\cup\mathbf{L}_{\text{low}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial w^{(l)}_i}\delta\theta^{(l')}_j\delta w^{(l)}_i =\sum_{l_{\text{up}}\in\mathbf{L}_{\text{up}}}\frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial w^{(l)}_i}\delta w^{(l)}_i+\frac{\partial \mathcal{L}}{\partial y^{(l)}} \frac{\partial y^{(l)}}{\partial w^{(l)}_i}\bigg|_{\hat{X}^{(l)}}\delta w^{(l)}_i
    \end{equation}
    with
    \begin{equation}
        \hat{X}^{(l)}_{hj}=\sum_{l_{\text{low}}\in\mathbf{L}_{\text{low}}}\hat{X}^{(l,l_{\text{low}})}_{hj}=\sum_{l_{\text{low}}\in\mathbf{L}_{\text{low}}}\sum_{k}\frac{\partial X^{(l)}_{hj}}{\partial \theta^{(l_{\text{low}})}_k}\delta\theta^{(l_{\text{low}})}_k,\nonumber
    \end{equation}
    and
    \begin{equation}
        \label{eq:series_b_elementwise}
        \sum_{l'\in\mathbf{L}_{\text{up}}\cup\mathbf{L}_{\text{low}}}\sum_{j}\frac{\partial^2 \mathcal{L}}{\partial\theta^{(l')}_j\partial b^{(l)}_i}\delta\theta^{(l')}_j \delta b^{(l)}_i = \sum_{l_{\text{up}}\in\mathbf{L}_{\text{up}}}\frac{\partial \mathcal{L}}{\partial y^{(l_{\text{up}})}} \mathbf{J}^{(l_{\text{up}})}_{\delta W^{(l_{\text{up}})}}  \frac{\partial x^{(l_{\text{up}})}}{\partial b^{(l)}_i}\delta b^{(l)}_i.
    \end{equation}
    Rewrite \cref{eq:series_w_elementwise} and \cref{eq:series_b_elementwise} as vector forms and we obtain \cref{eq:series_w} and \cref{eq:series_b}.
\end{proof}

{Appendix}
\subsection{\Cref{thm:parallel}}
\label{sec:proof_parallel}
Here we follow the notations in \cref{def:parallel} to denote each value of the two layers.
\begin{lemma}
    \label{lem:parallel}
    Let $\hat{X}^{\text{(left)}}\in \mathbb{R}^{l_{row} \times l_{hid}}$ and $\hat{X}^{\text{(right)}}\in \mathbb{R}^{l_{hid} \times l_{col}}$ be two surrogate weight matrices such that
\begin{align}
    \hat{X}_{kn}^{\text{(left)}} &= \sum_{q}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{l})}_{q},\\
    \hat{X}_{no}^{\text{(right)}} &= \sum_{r}\frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}} \delta \theta^{(l_{r})}_{r}.
\end{align}

Then we have
\begin{equation}
    \sum_{q}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}=\frac{\partial \mathcal{L}}{\partial y^{\text{(mul)}}}\frac{\partial y^{\text{(mul)}}}{\partial x^{\text{(right)}}}\bigg|_{\hat{X}^{\text{(left)}}} \frac{\partial x^{\text{(right)}}}{\partial \theta^{(l_{r})}_{r}}\delta \theta^{(l_{r})}_{r}
    \label{eq:parallellem_sumq}
\end{equation}
and
\begin{equation}
    \sum_{r}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}=\frac{\partial \mathcal{L}}{\partial y^{\text{(mul)}}}\frac{\partial y^{\text{(mul)}}}{\partial x^{\text{(left)}}}\bigg|_{\hat{X}^{\text{(right)}}} \frac{\partial x^{\text{(left)}}}{\partial \theta^{(l_{l})}_{q}}\delta \theta^{(l_{l})}_{q}.
    \label{eq:parallellem_sumr}
\end{equation}
\end{lemma}
\begin{proof}
The second-order partial derivative of loss w.r.t. $\theta^{(l_{r})}_{r}$ and $\theta^{(l_{l})}_{q}$ can be written as
\begin{equation}
\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}}=\sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{np}\frac{\partial^2 Y^{\text{(mul)}}_{ko}}{\partial X^{\text{(left)}}_{kn} \partial X^{\text{(right)}}_{po}} \frac{\partial X^{\text{(right)}}_{po}}{\partial \theta^{(l_{r})}_{r}} \frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}}.
\label{eq:parallel_second_partial}
\end{equation}

Since the multiplication can be expressed as $Y^{\text{(mul)}}_{ko} = \sum_{n}X^{\text{(left)}}_{kn}X^{\text{(right)}}_{no}$, which means
\begin{equation}
    \frac{\partial^2 Y^{\text{(mul)}}_{ko}}{\partial X^{\text{(left)}}_{kn}\partial X^{\text{(right)}}_{po}} =
    \begin{cases}
        1  &n=p,\\
        0  &n\neq p.
    \end{cases}
    \end{equation}
With this, \cref{eq:parallel_second_partial} can be rewritten as
\begin{equation}
    \frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}}=\sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{n} \frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}} \frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}}.
\end{equation}

By expanding the right-hand side of \cref{eq:parallellem_sumq} we have
\begin{align}
    \sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{n} \frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}}\delta \theta^{(l_{r})}_{r} &\sum_{q}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{l})}_{q}\\
    =&\sum_{q}\sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{n} \frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}}\delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}\nonumber\\
    =&\sum_{q}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}.
\end{align}
Expand the right-hand side of \cref{eq:parallellem_sumr} and we can see it also holds.
\end{proof}
\begin{proof}[Proof of \cref{thm:parallel}]
    According to \cref{lem:parallel}, we have
    \begin{align}
    \sum_{l_l\in\mathbf{L}_{\text{left}}}\sum_{q}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}&=\sum_{l_l\in\mathbf{L}_{\text{left}}}\sum_{q}\sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{n} \frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}}\delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}\nonumber\\
    &=\sum_{ko}\frac{\partial \mathcal{L}}{\partial Y^{\text{(mul)}}_{ko}}\sum_{n} \frac{\partial X^{\text{(right)}}_{no}}{\partial \theta^{(l_{r})}_{r}}\delta \theta^{(l_{r})}_{r}\sum_{l_l\in\mathbf{L}_{\text{left}}}\sum_{q}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{l})}_{q}\nonumber\\
    &=\frac{\partial \mathcal{L}}{\partial y^{\text{(mul)}}}\frac{\partial y^{\text{(mul)}}}{\partial x^{\text{(right)}}}\bigg|_{\sum_{l_l\in\mathbf{L}_{\text{left}}}\sum_{q}\frac{\partial X^{\text{(left)}}_{kn}}{\partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{l})}_{q}} \frac{\partial x^{\text{(right)}}}{\partial \theta^{(l_{r})}_{r}}\delta \theta^{(l_{r})}_{r}.
    \end{align}
    Apply similar operations on $\sum_{l_r\in\mathbf{L}_{\text{right}}}\sum_{r}\frac{\partial^2 \mathcal{L}}{\partial \theta^{(l_{r})}_{r} \partial \theta^{(l_{l})}_{q}} \delta \theta^{(l_{r})}_{r} \delta \theta^{(l_{l})}_{q}$ and we can get \cref{eq:parallelthm_sumr}.
\end{proof}

\end{document}
