\section{Related Work}
\paragraph{Model Compression}
Model compression is an area that focuses on creating smaller, faster, and more efficient models suitable for deployment in environments with limited resources, like mobile devices or embedded systems. There are several typical fields within this area, including quantization**Chetlur, "A Guide to Deep Learning Deployment"**, knowledge distillation**Bucilua, "Deep Neural Networks Squeezing Data Through a Thin Tube"**, neural architecture search**Zoph, "Neural Architecture Search with Reinforcement Learning"**, and network pruning**Han, "Learning both Weights and Connections for Efficient Neural Network"**. Quantization, outlined in works like **Chetlur, "A Guide to Deep Learning Deployment"** and **Gupta, "Deep Learning with Limited Numerical Precision"**, focuses on reducing parameter precision to accelerate inference and decrease model size, enabling deployment on devices with limited resources. Knowledge distillation, as introduced by **Bucilua, "Deep Neural Networks Squeezing Data Through a Thin Tube"**, leverages a smaller "student" model to mimic a larger "teacher" model, effectively compressing the knowledge and achieving high performance with less computational demand. Neural Architecture Search (NAS), with seminal contributions from **Zoph, "Neural Architecture Search with Reinforcement Learning"** and **Real, "Large-Scale Evolution of Image Classifiers"**, automates the discovery of optimal architectures, often outperforming human-designed models in efficiency and accuracy. Pruning techniques, highlighted in work by **Han, "Learning both Weights and Connections for Efficient Neural Network"** and **Deng, "Compressing Deep Neural Networks using a Backward Depthwise Filter Pruning Algorithm"**, remove non-essential weights or neurons, significantly reducing model complexity and enhancing inference speed without major accuracy losses. Together, these techniques represent the forefront of model compression research, addressing the balance between performance and computational efficiency necessary for advanced AI applications.
\paragraph{Network Pruning}
Network pruning, initially recognized as an importance estimation problem**LeCun, "Optimal Brain Damage"**, has been prompting researchers to focus on finding accurate criteria that reveals the importance of parameters or neurons in neural networks. **Hassibi, "Second Order Derivatives for Network Pruning: Convex Bounds and a Low-Order Appromimation"** operated under the assumption that each layer in feed-forward networks held equal importance and introduced a heuristic for global scaling normalization. However, this approach did not prove effective in networks incorporating skip connections. Additionally, the method relies on using network activations to calculate its criterion, resulting in increased memory demands. In contrast, pruning methods that focus on batch normalization**Ioffe, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**, bypass the need for sensitivity analysis and are applicable on a global scale. In intricate network architectures**Zhang, "DPP-Net: Sparse and Efficient Lattice Neural Networks"**, parameter interdependencies often require their joint pruning. This collective pruning of interlinked parameters has been an area of focus in structured pruning research since its early stages. **He, "Soft Filter Pruning for Deep Convolutional Neural Networks"** proposes to build a dependency graph which captures the interdependencies between parameters and prunes parameters belonging to a graph together to achieve structured pruning for neural networks with complicated structures. In our pruning setting, we view the pruning as an importance estimation problem for each individual parameter (unstructured) or parameter group (structured) obtained from **Han, "Learning both Weights and Connections for Efficient Neural Network"**. Parameters with low importance are pruned in each pruning step.

\paragraph{Hessian Matrix Estimation} The structure and computational aspects of the Hessian matrix in feedforward neural networks have been extensively studied since the early 1990s**LeCun, "Optimal Brain Damage"**. The Hessian matrix was first utilized in neural network pruning by **Kulkarni, "Pruning Neural Networks using Second Order Derivatives"** to calculate the importance score of each neuron, leveraging diagonal approximation is used to estimate the Hessian matrix:
\begin{equation}
\delta \mathcal{L}_{\mathrm{OBD}}=\frac{1}{2}\left(\boldsymbol{\theta}_q^*\right)^2 \mathbf{H}_{q q}.
\end{equation}
Building upon this, OBS**Chen, "Training Confidence-calibrated Classifiers for Doubly Robust Offline Estimation"**, views the importance estimation as an optimization problem, and aims at finding a set of weights that yields least change on the loss:
\begin{equation}
    \min _q\left\{\min _{\delta \boldsymbol{\theta}} \frac{1}{2} \delta \boldsymbol{\theta}^{\top} \mathbf{H} \delta \boldsymbol{\theta} \text { s.t. } \mathbf{e}_q^{\top} \delta \boldsymbol{\theta}+\boldsymbol{\theta}_q^*=0\right\}.
    \end{equation}
Early research such as that by **LeCun, "Optimal Brain Damage"** provided an extensive review of how to compute second derivatives in feed-forward networks, and **Hassibi, "Second Order Derivatives for Network Pruning: Convex Bounds and a Low-Order Appromimation"** examined the Hessian matrix's structure and second derivative techniques. More contemporary efforts, such as EigenDamage**Chen, "Training Confidence-calibrated Classifiers for Doubly Robust Offline Estimation"**, utilize a Kronecker-factored eigenbasis for network reparameterization and approximate the Hessian matrix using the Fisher matrix, as discussed by **Xu, "Efficient second-order approximation of neural networks with Kronecker-factored eigenvectors"**. Recent studies**Chen, "Training Confidence-calibrated Classifiers for Doubly Robust Offline Estimation"**, have thoroughly investigated the common structures and rank properties of the Hessian in neural networks. **Li, "Efficient Second-Order Optimization for Deep Neural Networks with Kronecker-Factored Approximations"** initially introduced an efficient method for computing the Hessian-vector product in feedforward neural networks. Our research applies this idea to the pruning of modern network architectures including CNNs and Transformers.

\vspace{-0.1cm}