@inproceedings{bolya2022tome,
  title={Token Merging: Your {ViT} but Faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{chang2023making,
  title={Making Vision Transformers Efficient from A Token Sparsification View},
  author={Chang, Shuning and Wang, Pichao and Lin, Ming and Wang, Fan and Zhang, David Junhao and Jin, Rong and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6195--6205},
  year={2023}
}

@inproceedings{chen2023cf,
  title={Cf-vit: A general coarse-to-fine method for vision transformer},
  author={Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={6},
  pages={7042--7052},
  year={2023}
}

@article{chu2021conditional,
  title={Conditional positional encodings for vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua},
  journal={arXiv preprint arXiv:2102.10882},
  year={2021}
}

@inproceedings{ding2022davit,
  title={Davit: Dual attention vision transformers},
  author={Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu},
  booktitle={European conference on computer vision},
  pages={74--92},
  year={2022},
  organization={Springer}
}

@inproceedings{dong2022cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12124--12134},
  year={2022}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{fan2023rethinking,
  title={Rethinking local perception in lightweight vision transformer},
  author={Fan, Qihang and Huang, Huaibo and Guan, Jiyang and He, Ran},
  journal={arXiv preprint arXiv:2303.17803},
  year={2023}
}

@article{fan2024lightweight,
  title={Lightweight vision transformer with bidirectional interaction},
  author={Fan, Qihang and Huang, Huaibo and Zhou, Xiaoqiang and He, Ran},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fan2024semantic,
  title={Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer},
  author={Fan, Qihang and Huang, Huaibo and Chen, Mingrui and He, Ran},
  journal={arXiv preprint arXiv:2405.13337},
  year={2024}
}

@inproceedings{guo2022cmt,
  title={Cmt: Convolutional neural networks meet vision transformers},
  author={Guo, Jianyuan and Han, Kai and Wu, Han and Tang, Yehui and Chen, Xinghao and Wang, Yunhe and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12175--12185},
  year={2022}
}

@article{han2021transformer,
  title={Transformer in transformer},
  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15908--15919},
  year={2021}
}

@article{hatamizadeh2023fastervit,
  title={Fastervit: Fast vision transformers with hierarchical attention},
  author={Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2306.06189},
  year={2023}
}

@inproceedings{hu2024lf,
  title={LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition},
  author={Hu, Youbing and Cheng, Yun and Lu, Anqi and Cao, Zhiqiang and Wei, Dawei and Liu, Jie and Li, Zhijun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={3},
  pages={2274--2284},
  year={2024}
}

@article{jiang2021all,
  title={All tokens matter: Token labeling for training better vision transformers},
  author={Jiang, Zi-Hang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={18590--18602},
  year={2021}
}

@article{liang2022not,
  title={Not all patches are what you need: Expediting vision transformers via token reorganizations},
  author={Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  journal={arXiv preprint arXiv:2202.07800},
  year={2022}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{liu2022dynamic,
  title={Dynamic group transformer: A general vision transformer backbone with dynamic group attention},
  author={Liu, Kai and Wu, Tianyi and Liu, Cong and Guo, Guodong},
  journal={arXiv preprint arXiv:2203.03937},
  year={2022}
}

@inproceedings{meng2022adavit,
  title={Adavit: Adaptive vision transformers for efficient image recognition},
  author={Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12309--12318},
  year={2022}
}

@article{pan2021ia,
  title={IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers},
  author={Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
  journal={Advances in Neural Information Processing Systems} ,
  volume={34},
  pages={24898--24911},
  year={2021}
}

@article{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13937--13949},
  year={2021}
}

@inproceedings{ren2023sg,
  title={Sg-former: Self-guided transformer with evolving token reallocation},
  author={Ren, Sucheng and Yang, Xingyi and Liu, Songhua and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6003--6014},
  year={2023}
}

@inproceedings{tang2022patch,
  title={Patch slimming for efficient vision transformers},
  author={Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12165--12174},
  year={2022}
}

@article{tang2022quadtree,
  title={Quadtree attention for vision transformers},
  author={Tang, Shitao and Zhang, Jiahui and Zhu, Siyu and Tan, Ping},
  journal={arXiv preprint arXiv:2201.02767},
  year={2022}
}

@inproceedings{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={32--42},
  year={2021}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{tu2022maxvit,
  title={Maxvit: Multi-axis vision transformer},
  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  booktitle={European conference on computer vision},
  pages={459--479},
  year={2022},
  organization={Springer}
}

@article{wang2021not,
  title={Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition},
  author={Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11960--11973},
  year={2021}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={568--578},
  year={2021}
}

@article{wang2022pvt,
  title={Pvt v2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={415--424},
  year={2022},
  publisher={Springer}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@inproceedings{xu2022evo,
  title={Evo-vit: Slow-fast token evolution for dynamic vision transformer},
  author={Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={3},
  pages={2964--2972},
  year={2022}
}

@inproceedings{yin2022vit,
  title={A-vit: Adaptive tokens for efficient vision transformer},
  author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10809--10818},
  year={2022}
}

@article{zhao2022lightweight,
  title={Lightweight vision transformer with cross feature attention},
  author={Zhao, Youpeng and Tang, Huadong and Jiang, Yingying and Wu, Qiang and others},
  journal={arXiv preprint arXiv:2207.07268},
  year={2022}
}

