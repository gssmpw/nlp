[
  {
    "index": 0,
    "papers": [
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      },
      {
        "key": "wang2022pvt",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pvt v2: Improved baselines with pyramid vision transformer"
      },
      {
        "key": "xie2021segformer",
        "author": "Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping",
        "title": "SegFormer: Simple and efficient design for semantic segmentation with transformers"
      },
      {
        "key": "carion2020end",
        "author": "Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey",
        "title": "End-to-end object detection with transformers"
      },
      {
        "key": "ding2022davit",
        "author": "Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu",
        "title": "Davit: Dual attention vision transformers"
      },
      {
        "key": "han2021transformer",
        "author": "Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe",
        "title": "Transformer in transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dong2022cswin",
        "author": "Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining",
        "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows"
      },
      {
        "key": "fan2023rethinking",
        "author": "Fan, Qihang and Huang, Huaibo and Guan, Jiyang and He, Ran",
        "title": "Rethinking local perception in lightweight vision transformer"
      },
      {
        "key": "fan2024lightweight",
        "author": "Fan, Qihang and Huang, Huaibo and Zhou, Xiaoqiang and He, Ran",
        "title": "Lightweight vision transformer with bidirectional interaction"
      },
      {
        "key": "ren2023sg",
        "author": "Ren, Sucheng and Yang, Xingyi and Liu, Songhua and Wang, Xinchao",
        "title": "Sg-former: Self-guided transformer with evolving token reallocation"
      },
      {
        "key": "touvron2021training",
        "author": "Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e}",
        "title": "Training data-efficient image transformers \\& distillation through attention"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "touvron2021training",
        "author": "Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e}",
        "title": "Training data-efficient image transformers \\& distillation through attention"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jiang2021all",
        "author": "Jiang, Zi-Hang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi",
        "title": "All tokens matter: Token labeling for training better vision transformers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chu2021conditional",
        "author": "Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua",
        "title": "Conditional positional encodings for vision transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "touvron2021going",
        "author": "Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\\'e}gou, Herv{\\'e}",
        "title": "Going deeper with image transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "han2021transformer",
        "author": "Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe",
        "title": "Transformer in transformer"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2021pyramid",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "guo2022cmt",
        "author": "Guo, Jianyuan and Han, Kai and Wu, Han and Tang, Yehui and Chen, Xinghao and Wang, Yunhe and Xu, Chang",
        "title": "Cmt: Convolutional neural networks meet vision transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2022pvt",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pvt v2: Improved baselines with pyramid vision transformer"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chang2023making",
        "author": "Chang, Shuning and Wang, Pichao and Lin, Ming and Wang, Fan and Zhang, David Junhao and Jin, Rong and Shou, Mike Zheng",
        "title": "Making Vision Transformers Efficient from A Token Sparsification View"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      },
      {
        "key": "wang2022pvt",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pvt v2: Improved baselines with pyramid vision transformer"
      },
      {
        "key": "wang2021pyramid",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions"
      },
      {
        "key": "rao2021dynamicvit",
        "author": "Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui",
        "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification"
      },
      {
        "key": "chen2023cf",
        "author": "Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong",
        "title": "Cf-vit: A general coarse-to-fine method for vision transformer"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      },
      {
        "key": "wang2022pvt",
        "author": "Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling",
        "title": "Pvt v2: Improved baselines with pyramid vision transformer"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hatamizadeh2023fastervit",
        "author": "Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo",
        "title": "Fastervit: Fast vision transformers with hierarchical attention"
      },
      {
        "key": "zhao2022lightweight",
        "author": "Zhao, Youpeng and Tang, Huadong and Jiang, Yingying and Wu, Qiang and others",
        "title": "Lightweight vision transformer with cross feature attention"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "meng2022adavit",
        "author": "Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam",
        "title": "Adavit: Adaptive vision transformers for efficient image recognition"
      },
      {
        "key": "yin2022vit",
        "author": "Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo",
        "title": "A-vit: Adaptive tokens for efficient vision transformer"
      },
      {
        "key": "rao2021dynamicvit",
        "author": "Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui",
        "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification"
      },
      {
        "key": "pan2021ia",
        "author": "Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude",
        "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers"
      },
      {
        "key": "liang2022not",
        "author": "Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao",
        "title": "Not all patches are what you need: Expediting vision transformers via token reorganizations"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "xu2022evo",
        "author": "Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing",
        "title": "Evo-vit: Slow-fast token evolution for dynamic vision transformer"
      },
      {
        "key": "wang2021not",
        "author": "Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao",
        "title": "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition"
      },
      {
        "key": "chen2023cf",
        "author": "Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong",
        "title": "Cf-vit: A general coarse-to-fine method for vision transformer"
      },
      {
        "key": "hu2024lf",
        "author": "Hu, Youbing and Cheng, Yun and Lu, Anqi and Cao, Zhiqiang and Wei, Dawei and Liu, Jie and Li, Zhijun",
        "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition"
      },
      {
        "key": "tang2022quadtree",
        "author": "Tang, Shitao and Zhang, Jiahui and Zhu, Siyu and Tan, Ping",
        "title": "Quadtree attention for vision transformers"
      },
      {
        "key": "tang2022patch",
        "author": "Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng",
        "title": "Patch slimming for efficient vision transformers"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "ding2022davit",
        "author": "Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu",
        "title": "Davit: Dual attention vision transformers"
      },
      {
        "key": "dong2022cswin",
        "author": "Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining",
        "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows"
      },
      {
        "key": "liu2022dynamic",
        "author": "Liu, Kai and Wu, Tianyi and Liu, Cong and Guo, Guodong",
        "title": "Dynamic group transformer: A general vision transformer backbone with dynamic group attention"
      },
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      },
      {
        "key": "tu2022maxvit",
        "author": "Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao",
        "title": "Maxvit: Multi-axis vision transformer"
      },
      {
        "key": "bolya2022tome",
        "author": "Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy",
        "title": "Token Merging: Your {ViT} but Faster"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dong2022cswin",
        "author": "Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining",
        "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "tu2022maxvit",
        "author": "Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao",
        "title": "Maxvit: Multi-axis vision transformer"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "liu2022dynamic",
        "author": "Liu, Kai and Wu, Tianyi and Liu, Cong and Guo, Guodong",
        "title": "Dynamic group transformer: A general vision transformer backbone with dynamic group attention"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "fan2024semantic",
        "author": "Fan, Qihang and Huang, Huaibo and Chen, Mingrui and He, Ran",
        "title": "Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer"
      }
    ]
  }
]