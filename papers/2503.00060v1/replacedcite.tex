\section{Related Work}
\subsection{Vision Transformer}
The Vision Transformer (ViT) ____, since its inception, has garnered significant attention from the vision community ____ for its superior global modeling capabilities. To optimize ViT's training and inference speed, various methods have been proposed ____. For instance, DeiT ____ uses a distillation token to transfer knowledge from a pre-trained teacher model to a student model, enhancing performance and accuracy. LV-ViT ____ leverages all tokens to compute the training loss, with location-specific supervision for each patch token. Additionally, some methods enhance ViT's architecture, such as CPVT ____, which replaces learnable positional embedding with a convolution layer, and CaiT ____, which builds deeper transformers with specialized training strategies. TNT ____ models pixel-wise interactions within each patch using an inner block, preserving richer local features. Token downsampling methods have also been employed, such as PVT ____, which uses average pooling. CMT ____ and PVTv2 ____, which combine downsampling with convolution to maintain feature integrity. STViT ____ captures global dependencies by sampling super tokens, applying self-attention, and mapping them back to the original token space for efficient global context modeling. 

\subsection{ViT Optimisation}

\textbf{ViT Compression Optimisation.} Existing research on ViT compression can be classified into static and dynamic categories based on input dependency ____. Static ViT compression involves efficient architectures like hierarchical Transformers ____ and hybrid models combining CNNs and ViTs ____. Some methods replace global self-attention with local self-attention ____ to reduce computational costs. Dynamic ViT compression adjusts the computational graph based on the input, dynamically removing non-contributory tokens during inference ____ or allocating computational resources to different image regions based on their significance ____. Our SAC-ViT also allocates resources to target and non-target tokens based on semantic information, performing local self-attention separately in these tokens to reduce computation costs.

\textbf{Grouping-Based ViT Optimisation.} Token grouping ____ optimizes ViT by limiting each token's attention span to neighboring tokens, reducing computational load. The Swin Transformer ____ divides tokens into small windows for localized attention, while the CSWin-Transformer ____ uses a cross-shaped grouping strategy for a global receptive field. MaxViT ____ combines window and grid attention mechanisms. However, these methods often overlook the semantic context of tokens, limiting their ability to capture semantic dependencies. To address this, the Dynamic Grouping Transformer (DGT) ____ uses k-means clustering for query grouping, incorporating semantic information to enhance feature learning. The Semantic Equitable Clustering Vision Transformer (SecViT) ____ groups tokens into equal-sized clusters in a single iteration, enhancing parallel computation efficiency. Despite these advancements, existing methods often ignore targets' scale and spatial redundancy, leading to inefficiencies as identical resources are allocated to all clusters.
In contrast, our SAC-ViT improves ViT compression and overall efficiency by reducing spatial redundancy in non-target regions through a two-stage design that, for the first time, combines dynamic ViT compression optimization with adaptive token grouping based on semantic information.



\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{imgs/SAC-ViT.pdf}
\caption{
Overview of the SAC-ViT framework. SAC-ViT consists of an Early Exit (EE) stage and a non-iterative Semantic-Aware Clustering (SAC) stage. In the EE stage, downsampled images are processed to extract global semantic information and generate initial results. If these results don't meet the EE terminate criteria, the information is clustered into target and non-target tokens. In the SAC stage, target tokens are mapped back to the original image, cropped, embedded, and then combined with reused non-target tokens from the EE stage. Multi-Head Self-Attention (MHSA) is applied within each cluster. Notably, SAC-ViT uses the same network parameters in both stages and performs end-to-end optimization.
}
\label{fig:sac_vit}
\end{figure*}