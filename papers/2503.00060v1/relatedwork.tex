\section{Related Work}
\subsection{Vision Transformer}
The Vision Transformer (ViT) \cite{dosovitskiy2020image}, since its inception, has garnered significant attention from the vision community \cite{liu2021swin,wang2022pvt,xie2021segformer,carion2020end,ding2022davit,han2021transformer} for its superior global modeling capabilities. To optimize ViT's training and inference speed, various methods have been proposed \cite{dong2022cswin,fan2023rethinking,fan2024lightweight,ren2023sg,touvron2021training}. For instance, DeiT \cite{touvron2021training} uses a distillation token to transfer knowledge from a pre-trained teacher model to a student model, enhancing performance and accuracy. LV-ViT \cite{jiang2021all} leverages all tokens to compute the training loss, with location-specific supervision for each patch token. Additionally, some methods enhance ViT's architecture, such as CPVT \cite{chu2021conditional}, which replaces learnable positional embedding with a convolution layer, and CaiT \cite{touvron2021going}, which builds deeper transformers with specialized training strategies. TNT \cite{han2021transformer} models pixel-wise interactions within each patch using an inner block, preserving richer local features. Token downsampling methods have also been employed, such as PVT \cite{wang2021pyramid}, which uses average pooling. CMT \cite{guo2022cmt} and PVTv2 \cite{wang2022pvt}, which combine downsampling with convolution to maintain feature integrity. STViT \cite{chang2023making} captures global dependencies by sampling super tokens, applying self-attention, and mapping them back to the original token space for efficient global context modeling. 

\subsection{ViT Optimisation}

\textbf{ViT Compression Optimisation.} Existing research on ViT compression can be classified into static and dynamic categories based on input dependency \cite{liu2021swin,wang2022pvt,wang2021pyramid,rao2021dynamicvit,chen2023cf}. Static ViT compression involves efficient architectures like hierarchical Transformers \cite{liu2021swin,wang2022pvt} and hybrid models combining CNNs and ViTs \cite{hatamizadeh2023fastervit,zhao2022lightweight}. Some methods replace global self-attention with local self-attention \cite{liu2021swin} to reduce computational costs. Dynamic ViT compression adjusts the computational graph based on the input, dynamically removing non-contributory tokens during inference \cite{meng2022adavit,yin2022vit,rao2021dynamicvit,pan2021ia,liang2022not} or allocating computational resources to different image regions based on their significance \cite{xu2022evo,wang2021not,chen2023cf,hu2024lf,tang2022quadtree,tang2022patch}. Our SAC-ViT also allocates resources to target and non-target tokens based on semantic information, performing local self-attention separately in these tokens to reduce computation costs.

\textbf{Grouping-Based ViT Optimisation.} Token grouping \cite{ding2022davit,dong2022cswin,liu2022dynamic,liu2021swin,tu2022maxvit,bolya2022tome} optimizes ViT by limiting each token's attention span to neighboring tokens, reducing computational load. The Swin Transformer \cite{liu2021swin} divides tokens into small windows for localized attention, while the CSWin-Transformer \cite{dong2022cswin} uses a cross-shaped grouping strategy for a global receptive field. MaxViT \cite{tu2022maxvit} combines window and grid attention mechanisms. However, these methods often overlook the semantic context of tokens, limiting their ability to capture semantic dependencies. To address this, the Dynamic Grouping Transformer (DGT) \cite{liu2022dynamic} uses k-means clustering for query grouping, incorporating semantic information to enhance feature learning. The Semantic Equitable Clustering Vision Transformer (SecViT) \cite{fan2024semantic} groups tokens into equal-sized clusters in a single iteration, enhancing parallel computation efficiency. Despite these advancements, existing methods often ignore targets' scale and spatial redundancy, leading to inefficiencies as identical resources are allocated to all clusters.
In contrast, our SAC-ViT improves ViT compression and overall efficiency by reducing spatial redundancy in non-target regions through a two-stage design that, for the first time, combines dynamic ViT compression optimization with adaptive token grouping based on semantic information.



\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{imgs/SAC-ViT.pdf}
\caption{
Overview of the SAC-ViT framework. SAC-ViT consists of an Early Exit (EE) stage and a non-iterative Semantic-Aware Clustering (SAC) stage. In the EE stage, downsampled images are processed to extract global semantic information and generate initial results. If these results don't meet the EE terminate criteria, the information is clustered into target and non-target tokens. In the SAC stage, target tokens are mapped back to the original image, cropped, embedded, and then combined with reused non-target tokens from the EE stage. Multi-Head Self-Attention (MHSA) is applied within each cluster. Notably, SAC-ViT uses the same network parameters in both stages and performs end-to-end optimization.
}
\label{fig:sac_vit}
\end{figure*}