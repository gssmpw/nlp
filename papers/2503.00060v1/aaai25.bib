@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}


@article{liu2022dynamic,
  title={Dynamic group transformer: A general vision transformer backbone with dynamic group attention},
  author={Liu, Kai and Wu, Tianyi and Liu, Cong and Guo, Guodong},
  journal={arXiv preprint arXiv:2203.03937},
  year={2022}
}


@article{fan2024semantic,
  title={Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer},
  author={Fan, Qihang and Huang, Huaibo and Chen, Mingrui and He, Ran},
  journal={arXiv preprint arXiv:2405.13337},
  year={2024}
}


@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@inproceedings{meng2022adavit,
  title={Adavit: Adaptive vision transformers for efficient image recognition},
  author={Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12309--12318},
  year={2022}
}

@inproceedings{yin2022vit,
  title={A-vit: Adaptive tokens for efficient vision transformer},
  author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10809--10818},
  year={2022}
}

@article{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13937--13949},
  year={2021}
}

@article{pan2021ia,
  title={IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers},
  author={Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
  journal={Advances in Neural Information Processing Systems} ,
  volume={34},
  pages={24898--24911},
  year={2021}
}


@inproceedings{xu2022evo,
  title={Evo-vit: Slow-fast token evolution for dynamic vision transformer},
  author={Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={3},
  pages={2964--2972},
  year={2022}
}

@article{wang2021not,
  title={Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition},
  author={Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11960--11973},
  year={2021}
}

@inproceedings{chen2023cf,
  title={Cf-vit: A general coarse-to-fine method for vision transformer},
  author={Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={6},
  pages={7042--7052},
  year={2023}
}

@inproceedings{hu2024lf,
  title={LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition},
  author={Hu, Youbing and Cheng, Yun and Lu, Anqi and Cao, Zhiqiang and Wei, Dawei and Liu, Jie and Li, Zhijun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={3},
  pages={2274--2284},
  year={2024}
}

@article{tang2022quadtree,
  title={Quadtree attention for vision transformers},
  author={Tang, Shitao and Zhang, Jiahui and Zhu, Siyu and Tan, Ping},
  journal={arXiv preprint arXiv:2201.02767},
  year={2022}
}


@article{liang2022not,
  title={Not all patches are what you need: Expediting vision transformers via token reorganizations},
  author={Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  journal={arXiv preprint arXiv:2202.07800},
  year={2022}
}

@inproceedings{tang2022patch,
  title={Patch slimming for efficient vision transformers},
  author={Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12165--12174},
  year={2022}
}


@inproceedings{dong2022cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12124--12134},
  year={2022}
}

@article{fan2023rethinking,
  title={Rethinking local perception in lightweight vision transformer},
  author={Fan, Qihang and Huang, Huaibo and Guan, Jiyang and He, Ran},
  journal={arXiv preprint arXiv:2303.17803},
  year={2023}
}

@misc{li2022sait,
      title={SaiT: Sparse Vision Transformers through Adaptive Token Pruning}, 
      author={Ling Li and David Thorsley and Joseph Hassoun and Ling Li and David Thorsley and Joseph Hassoun},
      year={2022},
      eprint={2210.05832},
      archivePrefix={arXiv},
}

@article{fan2024lightweight,
  title={Lightweight vision transformer with bidirectional interaction},
  author={Fan, Qihang and Huang, Huaibo and Zhou, Xiaoqiang and He, Ran},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ren2023sg,
  title={Sg-former: Self-guided transformer with evolving token reallocation},
  author={Ren, Sucheng and Yang, Xingyi and Liu, Songhua and Wang, Xinchao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6003--6014},
  year={2023}
}

@article{jiang2021all,
  title={All tokens matter: Token labeling for training better vision transformers},
  author={Jiang, Zi-Hang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={18590--18602},
  year={2021}
}

@inproceedings{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={32--42},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2010.04159},
  year={2020}
}

@article{roh2021sparse,
  title={Sparse detr: Efficient end-to-end object detection with learnable sparsity},
  author={Roh, Byungseok and Shin, JaeWoong and Shin, Wuhyun and Kim, Saehoon},
  journal={arXiv preprint arXiv:2111.14330},
  year={2021}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6881--6890},
  year={2021}
}

@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7262--7272},
  year={2021}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{wang2022pvt,
  title={Pvt v2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={415--424},
  year={2022},
  publisher={Springer}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={568--578},
  year={2021}
}

@article{han2021transformer,
  title={Transformer in transformer},
  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15908--15919},
  year={2021}
}

@inproceedings{guo2022cmt,
  title={Cmt: Convolutional neural networks meet vision transformers},
  author={Guo, Jianyuan and Han, Kai and Wu, Han and Tang, Yehui and Chen, Xinghao and Wang, Yunhe and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12175--12185},
  year={2022}
}

@article{chu2021conditional,
  title={Conditional positional encodings for vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua},
  journal={arXiv preprint arXiv:2102.10882},
  year={2021}
}

@inproceedings{chang2023making,
  title={Making Vision Transformers Efficient from A Token Sparsification View},
  author={Chang, Shuning and Wang, Pichao and Lin, Ming and Wang, Fan and Zhang, David Junhao and Jin, Rong and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6195--6205},
  year={2023}
}

@inproceedings{li2024bi,
  title={Bi-ViT: Pushing the Limit of Vision Transformer Quantization},
  author={Li, Yanjing and Xu, Sheng and Lin, Mingbao and Cao, Xianbin and Liu, Chuanjian and Sun, Xiao and Zhang, Baochang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={4},
  pages={3243--3251},
  year={2024}
}
@article{zhao2022lightweight,
  title={Lightweight vision transformer with cross feature attention},
  author={Zhao, Youpeng and Tang, Huadong and Jiang, Yingying and Wu, Qiang and others},
  journal={arXiv preprint arXiv:2207.07268},
  year={2022}
}
@article{hatamizadeh2023fastervit,
  title={Fastervit: Fast vision transformers with hierarchical attention},
  author={Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2306.06189},
  year={2023}
}

@article{patro2023spectformer,
  title={SpectFormer: Frequency and Attention is what you need in a Vision Transformer},
  author={Patro, Badri N and Namboodiri, Vinay P and Agneeswaran, Vijay Srinivas},
  journal={arXiv preprint arXiv:2304.06446},
  year={2023}
}


@inproceedings{chen2021crossvit,
  title={Crossvit: Cross-attention multi-scale vision transformer for image classification},
  author={Chen, Chun-Fu Richard and Fan, Quanfu and Panda, Rameswar},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={357--366},
  year={2021}
}

@inproceedings{ding2022davit,
  title={Davit: Dual attention vision transformers},
  author={Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu},
  booktitle={European conference on computer vision},
  pages={74--92},
  year={2022},
  organization={Springer}
}


@inproceedings{tu2022maxvit,
  title={Maxvit: Multi-axis vision transformer},
  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
  booktitle={European conference on computer vision},
  pages={459--479},
  year={2022},
  organization={Springer}
}

@inproceedings{bolya2022tome,
  title={Token Merging: Your {ViT} but Faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{han2021dynamic,
  title={Dynamic neural networks: A survey},
  author={Han, Yizeng and Huang, Gao and Song, Shiji and Yang, Le and Wang, Honghui and Wang, Yulin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={11},
  pages={7436--7456},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kaya2019shallow,
  title={Shallow-deep networks: Understanding and mitigating network overthinking},
  author={Kaya, Yigitcan and Hong, Sanghyun and Dumitras, Tudor},
  booktitle={International conference on machine learning},
  pages={3301--3310},
  year={2019},
  organization={PMLR}
}

@article{lin2023super,
  title={Super vision transformer},
  author={Lin, Mingbao and Chen, Mengzhao and Zhang, Yuxin and Shen, Chunhua and Ji, Rongrong and Cao, Liujuan},
  journal={International Journal of Computer Vision},
  volume={131},
  number={12},
  pages={3136--3151},
  year={2023},
  publisher={Springer}
}

@article{mungoli2023adaptive,
  title={Adaptive feature fusion: enhancing generalization in deep learning models},
  author={Mungoli, Neelesh},
  journal={arXiv preprint arXiv:2304.03290},
  year={2023}
}

@inproceedings{dai2021attentional,
  title={Attentional feature fusion},
  author={Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3560--3569},
  year={2021}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}


@inproceedings{ignatov2019ai,
  title={Ai benchmark: All about deep learning on smartphones in 2019},
  author={Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages={3617--3635},
  year={2019},
  organization={IEEE}
}

@inproceedings{dass2023vitality,
  title={Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention},
  author={Dass, Jyotikrishna and Wu, Shang and Shi, Huihong and Li, Chaojian and Ye, Zhifan and Wang, Zhongfeng and Lin, Yingyan},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={415--428},
  year={2023},
  organization={IEEE}
}