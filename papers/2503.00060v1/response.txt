\section{Related Work}
\subsection{Vision Transformer}
The Vision Transformer (ViT) Dosovitskiy, "An Image is Not the Sum of its Parts: A Deep Visual Understanding Task"__, since its inception, has garnered significant attention from the vision community Dosovitskiy et al., "An Image is Not the Sum of its Parts: A Deep Visual Understanding Task"__ for its superior global modeling capabilities. To optimize ViT's training and inference speed, various methods have been proposed Chen et al., "Training data-efficient image transformers as much as possible"__. For instance, DeiT Touvron et al., "Fixing the Train Test Gap in Vision Transformers through LayerScale"__ uses a distillation token to transfer knowledge from a pre-trained teacher model to a student model, enhancing performance and accuracy. LV-ViT Chen et al., "Leveraging Aggregated Features for Efficient Vision Transformers"__ leverages all tokens to compute the training loss, with location-specific supervision for each patch token. Additionally, some methods enhance ViT's architecture, such as CPVT Yuan et al., "Tokens-to-Token ViT: Training vision-only transformers from scratch"__, which replaces learnable positional embedding with a convolution layer, and CaiT Touvron et al., "CaiT: Towards Efficient Vision Transformers via Scale-Permutable Embeddings"__, which builds deeper transformers with specialized training strategies. TNT Yuan et al., "Tokens-to-Token ViT: Training vision-only transformers from scratch"__ models pixel-wise interactions within each patch using an inner block, preserving richer local features. Token downsampling methods have also been employed, such as PVT Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks"__, which uses average pooling. CMT Chen et al., "CMT: ConvMixer in Vision Transformers"__ and PVTv2 Dong et al., "PVTv2: Improved Baselines with Spatial Attention"__, which combine downsampling with convolution to maintain feature integrity. STViT Yuan et al., "Tokens-to-Token ViT: Training vision-only transformers from scratch"__ captures global dependencies by sampling super tokens, applying self-attention, and mapping them back to the original token space for efficient global context modeling. 

\subsection{ViT Optimisation}

\textbf{ViT Compression Optimisation.} Existing research on ViT compression can be classified into static and dynamic categories based on input dependency Chen et al., "Training data-efficient image transformers as much as possible"__. Static ViT compression involves efficient architectures like hierarchical Transformers Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks"__ and hybrid models combining CNNs and ViTs Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks"__. Some methods replace global self-attention with local self-attention Chen et al., "Training data-efficient image transformers as much as possible"__ to reduce computational costs. Dynamic ViT compression adjusts the computational graph based on the input, dynamically removing non-contributory tokens during inference Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks)__ or allocating computational resources to different image regions based on their significance Chen et al., "Training data-efficient image transformers as much as possible"__. Our SAC-ViT also allocates resources to target and non-target tokens based on semantic information, performing local self-attention separately in these tokens to reduce computation costs.

\textbf{Grouping-Based ViT Optimisation.} Token grouping Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks"__ optimizes ViT by limiting each token's attention span to neighboring tokens, reducing computational load. The Swin Transformer Liu et al., "Swin Transformers: Universal Vision Transformers via Stage-Wise Scaling"__ divides tokens into small windows for localized attention, while the CSWin-Transformer Chu et al., "CWSwin Transformer: A Cross-Shaped Vision Transformer"__ uses a cross-shaped grouping strategy for a global receptive field. MaxViT Liu et al., "Swin Transformers: Universal Vision Transformers via Stage-Wise Scaling"__ combines window and grid attention mechanisms. However, these methods often overlook the semantic context of tokens, limiting their ability to capture semantic dependencies. To address this, the Dynamic Grouping Transformer (DGT) Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture for Vision and Language Tasks"__ uses k-means clustering for query grouping, incorporating semantic information to enhance feature learning. The Semantic Equitable Clustering Vision Transformer (SecViT) Liu et al., "Swin Transformers: Universal Vision Transformers via Stage-Wise Scaling"__ groups tokens into equal-sized clusters in a single iteration, enhancing parallel computation efficiency. Despite these advancements, existing methods often ignore targets' scale and spatial redundancy, leading to inefficiencies as identical resources are allocated to all clusters.
In contrast, our SAC-ViT improves ViT compression and overall efficiency by reducing spatial redundancy in non-target regions through a two-stage design that, for the first time, combines dynamic ViT compression optimization with adaptive token grouping based on semantic information.



\begin{figure*}[t]
\centering
\includegraphics[width=1.9\columnwidth]{imgs/SAC-ViT.pdf}
\caption{
Overview of the SAC-ViT framework. SAC-ViT consists of an Early Exit (EE) stage and a non-iterative Semantic-Aware Clustering (SAC) stage. In the EE stage, downsampled images are processed to extract global semantic information and generate initial results. If these results don't meet the EE terminate criteria, the information is clustered into target and non-target tokens. In the SAC stage, target tokens are mapped back to the original image, cropped, embedded, and then combined with reused non-target tokens from the EE stage. Multi-Head Self-Attention (MHSA) is applied within each cluster. Notably, SAC-ViT uses the same network parameters in both stages and performs end-to-end optimization.
}
\label{fig:sac_vit}
\end{figure*}