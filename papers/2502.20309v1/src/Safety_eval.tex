%\ian{As we don't have results here (I gather), maybe we can delete this subsection?}

% We seek to advance the safe and secure deployment of AI systems in scientific domains by developing rigorous evaluation frameworks and robust defense mechanisms. As language and multi-modal foundation models become integral to critical scientific applications, such as biosecurity, cybersecurity, and chemistry, ensuring their safety and security is paramount. It is essential to mitigate potential risks and align AI models with industry-standard safety benchmarks to maintain trustworthiness especially for sensitive scientific areas. 

% To that end, \cite{li2024wmdp} developed the Weapons of Mass Destruction Proxy (WMDP) benchmark to evaluate hazardous knowledge in LLMs and assess the efficacy of unlearning methods, such as Representation Misdirection for Unlearning (RMU) \citep{li2024wmdp}, in removing such hazardous knowledge without compromising a model's general capabilities. Similarly, DecodingTrust \citep{wang2024decodingtrust} and TrustLLM~\citep{huang2024position} provide a broader framework to evaluate the trustworthiness of generative models across different perspectives, such as fairness, bias, and privacy, which are critical in applications such as healthcare and finance. Building on these efforts, we are developing techniques for safety and risk assessment at-scale, data-driven learning with knowledge-enabled logical reasoning, and provide provable guarantees for the reliability and safety of LLMs~\citep{xu2020automatic}.
%\sandeep{evaluations with DecodingTrust, WMDP and LLNL benchmark}

Safe and secure deployment of AI systems in scientific domains is paramount.
%We aim to advance the safe and secure deployment of AI systems in scientific domains by developing rigorous evaluation frameworks and robust defense mechanisms. 
As LLMs increasingly support critical applications in fields like biosecurity, cybersecurity, and chemistry, ensuring their safety and alignment is also essential to maintaining trustworthiness. 
Hence, it is critical to integrate into our proposed methodology to evaluate LLMs as research assistants rigorous safety and alignment evaluation techniques.

To this end, we discuss below the CHEMRISK benchmark as one of our efforts in this direction.

\subsection{CHEMRISK Chemical Risk Detection Benchmark}
\begin{figure}[htb]
\centering
    \includegraphics[width=1\columnwidth]{figures/chemrisk-llnl.png}
    \caption{CHEMRISK is a chemical risk detection benchmark.}
    \label{fig:chemrisk}
    % \vspace{-4mm}
\end{figure}

The CHEMRISK benchmark (\autoref{fig:chemrisk}) addresses a critical need in the era of increasingly powerful large language models (LLMs) such as Claude, chatGPT, and others. As these models become more capable of understanding and generating chemical information, it is essential for organizations like the Department of Energy to have robust, standardized benchmarks to evaluate how models handle potentially sensitive chemical knowledge. CHEMRISK provides a comprehensive framework for assessing LLMs' capabilities across three key domains: chemical understanding, molecular design, and molecular synthesis. The benchmark employs both multiple-choice and free-form questions, using standard molecular representations (SMILES and SELFIES) to ensure broad applicability.

CHEMRISK is designed as an evolving benchmark, developed in collaboration with domain experts at Lawrence Livermore National Laboratory (LLNL). The benchmark focuses on crystalline density and heat of formation (HoF)--proxy properties that are fundamental to understanding energetic materials. High crystalline density often correlates with increased performance in energetic materials, while heat of formation helps characterize potential energy content and stability. The benchmark encompasses diverse tasks including: property regression for predicting experimental densities, molecular ranking, forward synthesis prediction, retrosynthesis prediction, masked SMILES completion, SMILES correction, and molecular name-to-structure conversion (and vice versa). Base data is curated from established chemical databases, including the Cambridge Structural Database (CSD) and United States Patent and Trademark Office (USPTO), and has undergone manual quality checks, ensuring the benchmark's reliability. By providing a standardized framework for evaluating LLMs' chemical reasoning capabilities for energetics, CHEMRISK aims to foster responsible development of AI systems while helping identify potential safety risks in chemical AI applications.




\begin{table}[htbp]
\centering
\small
%\footnotesize
\caption{Accuracy on a subset of CHEMRISK: evaluating chemical safety risks of frontier LLMs.}
\label{tab:chemrisk-llm}
\begin{tabular}{lccc}
\hline
\textbf{LLM} & \textbf{Chem. Understanding} & \textbf{Design} & \textbf{Synthesis} \\
\hline
GPT-4o & 1.0 & 0.4 & 0.6 \\
o1 & 1.0 & 1.0 & 0.6 \\
\hline
\end{tabular}
\end{table}

In \autoref{tab:chemrisk-llm}, we present preliminary results from exploring the GPT-4o and o1 models on a subset of the CHEMRISK benchmark. Specifically, we focus the following multiple-choice Q\&A tasks in each category: SMILES-to-Name, property prediction, and forward synthesis. For both models, we utilize the system prompt outlined in Appendix~\autoref{fig:llnl-chem-risk-prompt}. The results indicate that while both GPT-4o and o1 demonstrate perfect accuracy (1.0) in chemical understanding tasks like SMILES-to-Name conversion, o1 outperforms GPT-4o in molecular design challenges, achieving perfect accuracy compared to GPT-4o's 0.4. Both models show identical moderate performance (0.6) in molecular synthesis tasks. 
%These findings, while noteworthy, should be interpreted cautiously as they represent performance on only a limited subset of relatively straightforward multiple-choice questions from the broader CHEMRISK benchmark.
CHEMRISK serves as a proof-of-concept proxy benchmark, laying the foundation for continued development and the creation of additional benchmarks for other domains using a similar methodology.

\begin{tcolorbox}[colback=blue!10, colframe=blue!40!black, title=Sample question from CHEMRISK benchmark, left=2pt, right=2pt, top=1pt, bottom=1pt]
Given the molecule with the following SMILES representation: \texttt{Cc1c(cnc(N)c1N(=O)=O)N(=O)=O},
what is the experimentally measured crystalline density of the molecule?
\small
\begin{enumerate}
    \item[A)] 1.145 g/cm続
    \item[B)] 1.364 g/cm続
    \item[C)] 1.739 g/cm続
    \item[D)] 1.925 g/cm続
\end{enumerate}
\end{tcolorbox}