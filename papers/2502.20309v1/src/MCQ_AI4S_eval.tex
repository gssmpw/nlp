% \sandeep{Lessons learned from the domain-specific benchmarks and our strategy for Multidisciplinary/AI4S benchmark}

\begin{figure*}[htb] %{r}{0.5\textwidth} 
    \centering
    \hrule
%    \vspace{2mm}
    \includegraphics[width=\textwidth, clip]{figures/AGIL.png} 
    \vspace{-2mm}
    \hrule
    
    \caption{The AGIL approach to generate scalable MCQ benchmarks. The current version of the AI4S benchmark contains only manually accepted MCQs. The AGIL approach  enables the integration of automatically accepted MCQs after the validation of their difficulty and quality.} 
    \label{fig:AGIL}
\end{figure*}

The overarching goal of the AI4S benchmark is to evaluate the knowledge extension of LLMs across many different scientific domains.
In that respect, it is similar to GPQA \cite{rein2023gpqa}. However, AI4S design focuses on the quality and scalability of MCQ generation and validation. While GPQA has only 448 MCQs, our objective is to generate and validate thousands of MCQs and continuously add MCQs as LLMs progress in their capabilities.

Current MCQ benchmarks, including GPQA, have two main limitations: 1) they are quickly saturated because of the fast progress of LLMs. 2) there is a high risk of contamination (benchmark included in the training sets) if the MCQs are made public. These two limitations arise from the static aspect of the current benchmarks. They are developed once and do not evolve. The current practice is to develop new versions \cite{GSM1k} when the initial benchmark is saturated \cite{gsm8k} and to open only a portion of the benchmark MCQs to avoid contamination. 

To address the two limitations, we explore a novel approach to develop scalable generation and validation of MCQ benchmarks for the evaluation of LLMs capabilities: Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks: AGIL.

Building on the experience gained from the Astronomy and Climate domain-specific benchmarks, we are developing an AGIL process by which we combine manual and automated methods to generate and validate MCQs. 

%Our approach is to combine manual and automatic MCQ generation and validation. 
We present here our initial finding towards the creation of a 1000-MCQ AI4S benchmark that spans five scientific domains: Computer Science, Astrophysics, Climate, Physics, and Chemistry. Our goal with this first study is to assess key aspects of the AGIL approach: validity of the difficulty level, quality of the generated MCQs compared to GPQA ones, quality of the automatically generated MCQs compared to manually generated ones, and quality of the automatically validated MCQs.

%For producing the AI4S benchmark, 
In the AGIL approach (Figure \ref{fig:AGIL}), manual MCQ generation and validation by experts is critical to provide high-quality, domain-specific MCQs that serve as a gold standard for evaluating LLM performance and developing automatic MCQ generation and validation workflows. The goal of automated generation and validation (LLM as a judge technique \cite{LLM-as-judge}) by LLMs is to address the scalability issues of manual generation and validation while keeping their quality levels. 
%large volumes of diverse MCQs beyond what.  
%The synergy between these approaches lies in their complementarity: manual generation and validation refines automatic MCQ generation and validation. 

\begin{table*}[htbp]
\caption{Performance metrics for Llama~3 models across various AI4S benchmark levels.
%\ian{You have referred before to ``the AI4S benchmark" and to ``Multi-domain AI4S MCQ Benchmarks." (Also elsewhere to ``subsets of AI4S accepted MCQs''.) Here you 1) refer to ``Multiscience benchmark'' and 2) different versions of the same. It will be good to use consistent terminology, be clear as to how many benchmarks there are, and explain ``versions." [Having written the preceding, I looked at the numbers and *think* that you don't have versions; rather you have 254 MCQs, categorized as easy/medium/hard, and for some (un-explained) reason you decided to show, for Llama-3-8B only, both the aggregate and the results for each subset? Yes? I added indents to make the subsetting easier.]}
}
\label{tab:performance_metrics}
\begin{tabularx}{\textwidth}{Xlrcc}
\hline
\textbf{Model} & \textbf{Task} & \textbf{nsamples} & \textbf{acc (stderr)} & \textbf{acc\_norm (stderr)} \\
\hline
\textbf{Llama-3-8B}   & accepted        & 254 & 0.2008 (±0.0252) & 0.2717 (±0.0280) \\
\ \ \ \textbf{Llama-3-8B}    & accepted\_easy   & 81  & 0.2222 (±0.0465) & 0.3210 (±0.0522) \\
\ \ \ \textbf{Llama-3-8B}    & accepted\_medium & 116 & 0.1983 (±0.0372) & 0.2672 (±0.0413) \\
\ \ \ \textbf{Llama-3-8B}    & accepted\_hard   & 57  & 0.1404 (±0.0464) & 0.1930 (±0.0527) \\
\textbf{Llama-3.1-8B}  & accepted        & 254 & 0.1969 (±0.0250) & 0.2638 (±0.0277) \\
\textbf{Llama-3-70B}   & accepted        & 254 & 0.2598 (±0.0276) & 0.3701 (±0.0304) \\
\textbf{Llama-3.1-70B} & accepted        & 254 & 0.2520 (±0.0273) & 0.3386 (±0.0298) \\
\hline
\end{tabularx}
\end{table*}

% Our initial goal is to create a 1000-question MCQ benchmark that spans five scientific domains: Computer Science, Astrophysics, Climate, Physics and Chemistry. 

%We then applied this enhanced process to a variety of source materials to produce what we call the AI for Science (AI4S) benchmark. 
%\ian{Is it AI4S Benchmark (singular) or AI4S benchmarks (plural)? It seems that both are used? The section title is plural, and you talk about a general framework, but then you say some way down that ``The overarching goal of this initiative was to create a 1000-question benchmark'' [singular].}
%\ian{I feel that the ``AI4S Benchmark Generation Framework'' (a term that could perhaps be defined as used) is not clearly defined. There is some combination of manual and automated, and (it seems?) some feedback between the two--although just what is not indicated. And then there are other steps, like an entirely manual categorization of difficulty? A figure with a flow chart might help?}
 %This iterative process not only calibrates LLMs as effective judges but also enables the scalable validation of LLM-generated questions. %By harmonizing manual precision with automated scalability, we are creating a robust pipeline for AI4S benchmark development, ensuring high-quality and reliable evaluation frameworks that advance the assessment of LLM capabilities in scientific domains.

%Building on this foundation, we developed the AI4S multi-domain benchmark to provide a consistent and rigorous standard for evaluating LLMs across a wide range of scientific disciplines. 
%Building AI4S involved a dual approach to question generation: 
For the manual generation and validation of MCQs we organized hackathons which engaged 140 domain experts (PhDs and other research staff) \footnote[1]{\label{note1}the exercises in this paper were done with volunteer Argonne employees, who understood that the goal of this effort is to identify opportunities to improve models.  While the research team offered a rubric for notes and observations, the participants were free to use whatever rubric they preferred.  The following is the general approach that was used.}. We offered the participants to generate MCQs from scientific papers of their choosing, including their own. The manually generated MCQs were crafted by using a purpose-built authoring interface (Appendix, \autoref{fig:questionauthoring}) that allowed contributors to test their questions on smaller models like Llama~3 before submission so as to ensure a baseline difficulty threshold. Manual validation used another specifically created form (see Appendix, \autoref{fig:questionreviewing}). The tool used for the MCQ generation and validation is available on github (https://github.com/auroraGPT-ANL/questions-ui)

Automated MCQ generation leveraged LLMs such as GPT-4 with domain-specific prompts to create MCQs from scientific papers across fields such as climate science, physics, and chemistry, with validation guided by the LLM-as-a-Judge technique. Both the prompts used for automatic MCQ generation and validation, as well as the rubrics for validation and reviews, were informed by the manual generation and validation, as well as by the experience gained during the domain-specific benchmark development discussed earlier. 

This process generated 980 MCQs (720 manually and 260 automatically). Of 588 total manually generated reviews, 317 MCQs have been assessed so far, of which 254 were accepted. Acceptance was subject to the following criteria: appropriate difficulty, relevant, complete and correct answers and distractors, controversial answers, mathematic requirements, as well as relevant skills and domain selection. The small percentage of accepted MCQs 25\% illustrates the difficulty of generating and validating high-quality scientific MCQs.

% \ian{But how many questions are there in reality?}
In addition, domain experts categorized the accepted MCQs into easy, medium, and hard levels, capturing a spectrum of difficulty that mirrors real-world scientific challenges. This multi-level structure enables the AI4S benchmark
%\ian{What hierarchical structure?? I am not clear whether you are referring to automated and manual, or the three levels. (If the latter, why are three difficulty levels ``hierarchical''?)} \ian{Or: benchmarks created via our AI4S Benchmark Generation Framework?} 
to evaluate both the foundational and advanced capabilities of LLMs, offering an assessment of their strengths and limitations. %The combination of manual precision and automated scalability streamlines the generation and validation processes and establishes an adaptable pipeline for assessing LLM performance across domains. %This approach exemplifies how leveraging human expertise alongside LLM capabilities can create benchmarks that are both comprehensive and practical, advancing the evaluation of AI in scientific applications.\ian{Another sentence that makes grand claims with few details and no experimental validation.}

To evaluate the merits of our AGIL approach, we performed several tests to evaluate the quality of (i) the level classification, (ii) the AI4S benchmark compared to GPQA, (iii) the automatically generated MCQs, and (iv) the automatic MCQ validation.

For all tests, we used the STaR framework (see \autoref{sec:framework}) with five shots.
%\ian{As far as I know, this is the first use of ``Evaluation Harness.''}
% and separate cloze-style prompting.\ian{You defined ``5 shots'' before, in your discussion of different approaches, but not ``cloze-style prompting''.}
%\sandeep{@Neil add one sentence describing this}. 
The first row of the table \autoref{tab:performance_metrics} shows the overall performance of Llama~3~8B on the 254 MCQs. The resulting accuracy of 20\% correspond to a random guess. 
The next three rows show results for MCQs grouped by their human-identified levels of difficulty. We see that the Llama~3~8B results are consistent with human-identified difficulty levels, with Llama~3~8B achieving the best score on easy MCQs (22\%) and significantly below random performance on hard MCQs (14\%). These results validate the quality of the level classification.
%the performance achieved by several Llama~3 models for different subsets of AI4S manually verified \textit{accepted} MCQs. 
%\ian{What is an ``AI4S accepted MCQ''???}
%\sandeep{@Neil say why only llama3 models were used, while the previously discussed benchmarks used GPT adn several other models}.
The next three rows of \autoref{tab:performance_metrics} show the performance of other Llama-3 models for the AI4S accepted MCQs.
We observe that Llama~3-70B performs less well on all AI4S-accepted MCQs (26\% of questions answered correctly on all difficulty levels) than on GPQA (49\% of questions answered correctly). Note that while GPQA has one correct answer and three distractors, meaning 25\% accuracy for random responses, AI4S MCQs have four distractors and thus only 20\% random response accuracy. We conclude that AI4S is a more challenging LLM benchmark than GPQA. 

To obtain a finer quality comparison between AI4S and GPQA MCQs, we use automatic MCQ validation to quantify the quality of every MCQ on the first seven criteria (N/A value for the eighth criterion on GPQA). \autoref{tab:mean_sd_scores} shows the scores of the two benchmarks on the seven criteria. On average, AI4S MCQs (average of 4.55) reach the same quality level as the GPQA MCQs (average of 4.45). 

\begin{table}[htbp]
\centering
%\footnotesize
\caption{Mean and standard deviation (SD) scores for AI4S and GPQA across various criteria.}
\label{tab:mean_sd_scores}
\begin{tabular}{lcc}
\hline
\textbf{Item} & \textbf{AI4S Mean (SD)} & \textbf{GPQA Mean (SD)} \\
\hline
Appropriate & 3.68 (0.72) & 4.28 (0.58) \\
Complete & 4.52 (0.87) & 4.42 (0.75) \\
Controversial & 4.97 (0.19) & 5.00 (0.04) \\
Correct & 4.60 (1.36) & 4.21 (1.82) \\
Domains & 4.68 (0.73) & 4.97 (0.25) \\
Mathematic & 4.81 (0.95) & 3.50 (2.30) \\
Relevant & 4.64 (0.56) & 4.81 (0.42) \\
Skills & 3.97 (0.70) & N/A \\
\hline
\end{tabular}
\end{table}

Overall, these results validate the quality of AI4S compared to GPQA.
%\ToDo
%\neil{@Murat Do you remember where this GPQA result comes from? From llama report I see 46.7 for 70B. Did you run eval harness on GPQA with 5 shot to match the AI4S setup?} \murat{@Neil I didn't run eval harness on GPQA, so I think the number was from leaderboard https://huggingface.co/spaces/wenhu/Science-Leaderboard but that is a dynamic list so it is better to run eval harness, I can do it once sophia is back.}Evaluating o1 zero-shot capabilities we observe significant improvement, achieving 70.1\% on AI4S accepted compared to reported 78.0\% on GPQA.

%\neil{We switch to using "Multiscience" to refer to AI4S?} \sandeep{yes}


%\ian{You mentioned LLM-as-a-judge (but with capital J) earlier, but did not defined it. Then you described your benchmark generation process (not very clearly), said a few words about your goals with the benchmark, and then described quality tests used to evaluate the benchmark (but without saying what it is: just that your goal is that it have 1000 questions. Now we are back to LLM as a judge, without much set up. I added some text to provide some flow.}

We used the acceptance criteria to compare the quality of the manually and automatically generated MCQs. This comparison reveals that the quality of manually validated, automatically generated questions is on par with manually validated/generated ones. Overall these results show that automatic MCQ generation can be used to generate MCQs with a manual verification step.  

%Next we employed LLM-as-a-judge methods \cite{LLM-as-judge} to evaluate the quality of the MCQs generated by our AI4S Benchmark Generation Framework.  
%We used eight criteria to evaluate the agreement between human and automatic validations (LLM-as-a-judge).
To evaluate the quality of automatic MCQ validation, we compare it with manual validation. We used Mistral Large~2 as the judge, prompting it to evaluate each MCQs %question, distractors, domains, and skills 
on a scale of 1--5 for each of eight criteria and also asking it to provide concise rationale for each score. All criteria were evaluated in a single prompt. We expanded each criteria specification to define each level on the scale for all criteria explicitly.
The prompts used for the judge are in the Appendix, \autoref{fig:ai4s-judge-prompt}.
%\ian{I can't tell what the ``ablation'' was.}. 
%This change \ian{Not clear what the change is.} resulted in average scores slightly improving for the pass/fail criteria (Correct/Mathematic), and slightly decreasing for all others. 
%We built upon our findings from the domain-specific Astronomy Benchmark to craft the single prompt, for example specifying that distractors have to be of the same length %\ian{I may have missed it, but I don't see a mention of length in the appendix prompt? Maybe you are talking now about the MCQ generation prompt, not the Judge prompt?} (see Appendix \autoref{fig:ai4s-judge-prompt} for the full prompt). %\sandeep{@neil, could we add the prompt in appendix?}
%\sandeep{@Neil describe this in more detail -- what LLM-as-a-Judge is used, crafted prompts, rubric definition etc. - say explicitly how the Astro benchmark prompts helped to refine this to make the connection}. 
%\neil{@Sandeep I'm not sure its appropriate to link Astro prompts here, as Astro did not do similar LLM-as-a-judge. I adopted some of Josh's findings for the automatic question generation prompts, specifically that distractors should be of similar length. I could include the full prompts in the appendix if it makes sense.}
We observe the accuracy of a model trained on these judgements to predict whether a question will be accepted/rejected to be 72\%. For comparison, of the 144 questions with multiple reviews, only 61\% of reviewers agreed on acceptance.
%\ian{It is not clear to me what is the other evaluation method. You're not referring to GPQA and \autoref{tab:mean_sd_scores} are you? If so, need to explain}. \autoref{tab:mean_sd_scores} shows a comparison between AI4S and GPQA MCQ quality assessment for the eight quality criteria, using the automatic validation technique\ian{You may well mean here that you took your Mistral Large~2 as a Judge and the evaluation prompt in Appendix \autoref{fig:ai4s-judge-prompt}, and applied that to both AI4S and GPQA? But that is not clear.}. 

During the development of the AI4S benchmark using our proposed AGIL approach, several key lessons emerged.
%\ian{It isn't clear to me from the following what are the lessons. You make general observations but what are the implications for future work?}. 
Generating high-quality scientific MCQs manually is a challenging task for multiple reasons: crafting questions with distinguishable levels of difficulty (undergraduates, PhD students, postdocs, and staff)
%\ian{Do you mean here you want four levels, or that you want them to be good for all four groups?} 
is non-trivial, and creating distractors that are plausible but not overly confusing requires significant effort and precision. Validation of these questions proved even more demanding, as finding appropriate reviewers for difficult and specialized topics can be challenging, and ideally, each question requires validation by three experts to ensure reliability. %A comparison between human and LLM-generated questions revealed that automatically generated MCQs, based on the context of specific papers, tend to be easier. This may be due to the implicit knowledge humans draw upon when crafting questions that goes beyond the immediate scope of a single paper. 
Our goal is to continue the development of the AGIL approach to address this difficulty issue and to continuously generate AI4S MCQs from the large pool of available scientific papers. We will release the benchmark using a sliding-window approach, keeping a significant portion of newly generated MCQs (e.g. 70\%) not public. 

%Given the time- and effort-intensive nature of human validation, developing new automated validation methods is crucial to minimize reliance on human reviewers while maintaining high-quality standards. %These insights underscore the complexity and resource demands of creating robust benchmarks for scientific evaluation of LLM capabilities.
