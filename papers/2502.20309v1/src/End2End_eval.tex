%\ian{Elsewhere there have been references to Lab-style and field-style and in-the-wild. Here we also call them long-form evaluations. In my view we should be consistent. \textbf{ALSO}: I wonder if there may be a better term than ``open-ended'' for ``provide a textual response to a question'', given that ``open-ended'' would seem to allow for a lot of things. }
Although MCQ benchmarks are effective in testing factual recall and reasoning within constrained formats, and open-ended benchmarks gauge the generation of detailed and flexible responses, these methods do not capture the iterative and complexity of scientific problem solving. 
End-to-end evaluations attempts to address this gap by assessing, in real situations, the models responses for assisting researchers in solving scientific problems.
We propose two novel types of end-to-end methods in the context of scientific research: Lab-style and field-style experiments.  

%A critical step forward in assessing LLM as scientific research assistants is as they go beyond the limitations of MCQ and open-ended evaluations.  Lab-style and field-style experiments address this gap by  
%\ian{Also I wonder whether we have captured the essence here. I can ask a model to generate hypotheses as a single task, and that would be an ``open-ended'' task, I think? There seems to be some other essential element that we are missijg.}
%These evaluations are inherently hierarchical %\ian{again I don't know aht that means?}, building on the foundational competencies assessed in MCQ and open-ended benchmarks. By focusing on multi-step reasoning, contextual adaptability, and the ability to integrate and apply knowledge across diverse research stages, end-to-end evaluations provide a more comprehensive picture of model performance. 
%They arguably provide a better measure than simpler benchmarks of the true “distance” between current AI capabilities and those of an ideal research assistant.
%emphasizing iterative problem solving and domain-specific expertise. 
%This holistic approach can potentially bridge the gap between static evaluations and the dynamic needs of real-world scientific inquiry.


\input{src/E2E_LabStyle_eval}
\input{src/E2E_IntheWild_eval}