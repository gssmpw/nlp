\subsubsection{\bf Climate}
% The climate benchmark focuses on the urgent and complex domain of climate science. Climate change presents multifaceted challenges that demand interdisciplinary knowledge and reasoning, making it an ideal testbed for evaluating the capabilities of LLMs.  To rigorously assess the ability of LLMs to deliver accurate, reliable, and actionable information in addressing the complexities of climate science and communication, a climate benchmark is crucial.  However, existing benchmarks for climate science are limited, and their manual creation is both resource-intensive and difficult to scale efficiently. 

% To create the benchmarks, we utilized the Intergovernmental Panel on Climate Change (IPCC) reports, which are authoritative assessments synthesizing the latest scientific research on climate change, its impacts, and potential solutions. These reports serve as a foundation for informed decision-making and international negotiations on climate action, highlighting the urgency of addressing the complex and interconnected challenges posed by a changing climate. The reports are typically extensive, often exceeding 1000 pages, with each chapter and section addressing specific topics related to climate change.
% To generate questions on the various topics discussed in the report, we parsed the PDF section-wise. Initially, the PDF was processed using Nougat, and the parsed text was further divided into sections. Each section was then used as context to create a multiple-choice question, consisting of one correct answer and four distractors. Using Llama 3:8b~\sandeep{@Tanwi why llama8b? and not gemini or GPT?}\tanwi{this is gpt4, llama8b used for eval. This is not the updated text}
% , we generated a total of 752 questions covering diverse topics, ranging from factors influencing vulnerability to climate change to primary strategies for risk reduction. This systematic approach ensured comprehensive coverage and alignment with the content of the IPCC reports.
% \sandeep{more text to be added by Tanwi}

%\subsection{Climate Benchmark}
Climate and weather forecasting presents multifaceted challenges that demand interdisciplinary knowledge and reasoning, making it an ideal testbed for evaluating LLM capabilities.
However, existing benchmarks for climate science are limited \cite{lacombe2023climatex, vaid-etal-2022-towards, thulke2024climategpt}. %\ian{citations?}.
Thus we developed a Climate Benchmark, a set of MCQs focused on the urgent and complex domain of climate science.   
%To rigorously assess the ability of LLMs to provide accurate, reliable, and actionable information in addressing the complexities of climate science and communication, a climate benchmark is crucial.  
As the manual creation of such MCQs is labor intensive and climate scientists are already overcommitted, we employed automated methods to develop the Climate Benchmark. 
We adopted as our source material the Intergovernmental Panel on Climate Change (IPCC) reports \cite{solomon2007ipcc}, which are authoritative assessments synthesizing the latest scientific research on climate change, its impacts and potential solutions. These reports serve as a foundation for informed decision-making and international negotiations on climate action, highlighting the urgency of addressing the complex and interconnected challenges posed by changing climate. The reports are typically extensive, often exceeding 1000 pages, with each chapter and section addressing specific topics related to climate change.
To generate questions on the various topics discussed in the report, we parsed the PDFs section-wise, using Nougat as was done for the Astronomy Benchmark. We then employed OpenAI's GPT4 to create one multiple-choice question, consisting of one correct answer and four distractors, for each section, with the prompt designed to create an MCQ based on the provided scientific text, ensuring that the question evaluates a broader understanding of climate science principles without referencing the specific report.
This process resulted in a total of 752 questions on diverse topics, ranging from factors influencing vulnerability to climate change to primary strategies for risk reduction. This systematic approach ensured comprehensive coverage and alignment with the content of the IPCC reports. 
 
%We also specify that each question should include five options, one being correct. 
We employed the Climate Benchmark to assess the performance of multiple LLMs, including GPT-4o, Llama~3.8B, and Phi~4, on specialized climate science tasks. Among these models, GPT-4o performed the best, achieving an accuracy of 87.34\%, demonstrating its effectiveness in handling complex climate-related tasks. GPT-4o was followed by Llama~3.8, which achieved an accuracy of 78.48\%, and Phi~4, which scored 54.43\%. This performance disparity highlights the need for continued refinement and optimization of models to bridge the gap in specialized applications.

The development of the Climate Benchmark provided useful insights into the creation and evaluation of MCQ datasets for scientific applications. The climate-focused MCQs, derived from IPCC reports, were designed to assess knowledge recall and decision-making, emphasizing accurate understanding of scientific concepts. While primarily testing factual knowledge, these questions establish a strong foundation for expanding into tasks that require more complex reasoning and application in climate science. However, the automatic generation of MCQs sometimes produced semantically similar questions that differed only slightly in phrasing or structure while testing the same core information. This observation highlights the need for robust evaluation mechanisms to eliminate redundancy and ensure diversity within the dataset. While automatic MCQ generation greatly accelerates the creation of benchmark questions, it cannot replace a rigorous evaluation process. A combination of LLM-based evaluators and human oversight is crucial for maintaining the benchmark's quality, relevance, and accuracy, ensuring that it meets the standards required for research-focused benchmarks.
%content\ian{the content of what? We refer to content, dataset, benchmark, and it is not clear what is referring to what},ensuring that the dataset meets the standards required for research-focused benchmarks.

% \subsubsection{Lessons learned} The key insights gained from the process of developing the climate benchmarks are as follows:
% \begin{itemize}
%     \item The climate MCQ questions derived from the IPCC reports are primarily designed to focus on knowledge assessment and decision support, emphasizing the accurate recall and understanding of scientific concepts. These questions aim to evaluate the ability of models to process and respond to factual information grounded in authoritative sources. While they predominantly test knowledge, they serve as a foundation for exploring more complex reasoning and application-based tasks in climate science.
%     \item Automatic MCQ generation processes sometimes generate semantically similar questions, where the phrasing or structure may differ slightly, but the core information being tested remains the same. While this can be helpful for testing nuanced understanding and providing varied question formats, it may also inadvertently lead to redundancy within the dataset. Addressing this challenge requires a robust evaluation mechanism to identify and eliminate duplicate or overly similar questions, ensuring that the final set of MCQs remains diverse and unique.
%     \item Automatic MCQ generation significantly expedites the process of creating benchmark questions, allowing for scalability and efficiency in dataset development. However, it does not eliminate the need for a thorough evaluation process, which combines the use of LLMs as evaluators and human-in-the-loop oversight. This dual approach is essential for ensuring the quality of the questions, identifying and removing duplicates, and maintaining the relevance and accuracy of the content, especially for research-focused benchmarks.
% \end{itemize}
% \subsubsection{Way forward} Moving forward, we will develop questions beyond knowledge recall and decision-support to emphasize higher-order skills such as reasoning, critical thinking, and problem-solving. These advanced benchmarks are essential for evaluating the ability of LLMs to address complex, real-world challenges in climate science and communication. We will prioritize the validation of the correctness, difficulty, and contextual relevance of the generated questions to ensure they align with the intended evaluation objectives. To achieve this, we plan to design a robust multi-agent evaluation framework that leverages both automated assessment by LLMs and expert human oversight. This framework will enable a more comprehensive analysis of the models' capabilities, particularly in scenarios requiring deeper reasoning and nuanced understanding. 
