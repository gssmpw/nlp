The success of LLMs in scientific domains, such as chemistry, biology, and physics, has been remarkable, but their trustworthiness as scientific assistants remains a significant concern. These models, including GPT, Claude, and Llama, are prone to generating unreliable or fabricated responses, often referred to as hallucinations \cite{du2024haloscope}. Understanding and quantifying uncertainty in LLM outputs is essential to ensure safe, reliable, and informed decision making, particularly in scientific domains. Traditional uncertainty quantification (UQ) techniques, which rely on accessing internal model parameters \cite{gal2016dropout}, face challenges due to the black-box nature of modern LLMs like GPT-4, Claude~3, and Gemini, which are primarily accessible as API services. Recent research has focused on developing novel approaches to assess uncertainty directly from model outputs, such as semantic entropy \cite{kuhn2023semantic}, sampling-based methods, and aggregation techniques \cite{lin2023generating, xiong2024can}. These techniques aim to evaluate input sensitivity and output consistency, highlighting where models are most vulnerable. By improving transparency and trust, these UQ strategies play a crucial role in responsible AI deployment. Addressing these challenges is vital for leveraging LLMs in scientific applications, where errors can have substantial consequences. Moving forward, advancing UQ methods and enhancing LLM interpretability will be key to making these models safer and more robust in critical scientific and industrial domains.

Inspired by psychological assessments in which the same question is asked in different ways to test consistency, we propose a technique called Question Rephrasing \cite{chen2024question} to quantify the uncertainty in LLM outputs. This approach involves rephrasing a given question while preserving its original semantic meaning and comparing LLM responses before and after rephrasing to assess input uncertainty. In addition, we adopt a sampling method that repeatedly queries an LLM with identical inputs to evaluate output uncertainty. We applied these methods to assess GPT-3.5 and GPT-4 performance on tasks in the chemistry domain, specifically property prediction and forward reaction prediction. Input uncertainty helps determine the LLM's sensitivity to variations in molecular representations (e.g., alternative SMILES notations), while output uncertainty evaluates the inherent variability in LLM predictions. These techniques allow us to systematically explore how robust and reliable LLMs are in handling different forms of input and producing consistent output. Below, we outline our approach:

\begin{enumerate}
\item For a chemistry-related task $t$, given a SMILES representation $x_i$ of the $i$-th molecule, generate a prompt $P_{t, x_i}$ based on a task-specific template .

\item  Generate a list of up to $n$ SMILES variants of the molecule $x_i$: $L=\{x_i^1, x_i^2, ..., x_i^n\}$. We ask GPT-4 to rank the SMILES variants according to their confidence in interpreting their structures and choose the one, say $\hat{x}_i$, with the highest confidence in constructing a prompt $P_{t, \hat{x}_i}$ by replacing $x_i$ in $P_{t, x_i}$ with $\hat{x}_i$.

\item Ask the LLM to generate $m$ responses for the prompt $P_{t, \hat{x}_i}$ and obtain $R_{t, \hat{x}_i} = \{r_{t,\hat{x}_i,1}, r_{t,\hat{x}_i,2}, ..., r_{t,\hat{x}_i,m}\}$. 

\item Calculate the entropy-based uncertainty metrics $U_{t, x_i}$ and $U_{t, \hat{x}_i}$ for $R_{t, x_i}$ and $R_{t, \hat{x}_i}$, respectively.

\item Measure the input uncertainty by comparing $U_{t, x_i}$ and $U_{t, \hat{x}_i}$ for all chosen $x_i$. Measure the output uncertainty by examining $U_{t, x_i}$ and $U_{t, \hat{x}_i}$ separately. 
\end{enumerate}

\begin{figure}[t]
\centering
    \includegraphics[width=.9\columnwidth]{figures/asprin_variation.png}
    \caption{SMILES representation variants of Aspirin. While all structures depict the same molecule, their SMILES representations are different, which introduces input variations. \textbf{Top left}: Canonical SMILES representation of Aspirin. \textbf{Rest}: Five SMILES variations of Aspirin.}
    \label{fig:asprin}
    % \vspace{-4mm}
\end{figure}

Our experiments revealed that ChatGPT-4 exhibited a notable sensitivity to Question Rephrasing. We view this sensitivity as providing insight into the input uncertainty of the model. We observed that variations in the input format, such as rephrasing or using alternative SMILES representations, led to differences in the consistency of model responses. For example, in property prediction tasks using chemistry property datasets like BBBP, HIV, and Tox21~\cite{guo2023can}, we noted changes in model performance metrics such as accuracy and F1 score when the inputs were reformulated. The AUC scores is the Area Under the ROC Curve, and indicates  ability of the model to differentiate correct vs. wrong responses, with 1.0 being the means perfect separation (the model always assigns higher confidence/lower uncertainty to correct answers than to incorrect ones). 
%\ian{Explain the significance of this in more detail?} 
AUC for original SMILES ranged between 0.546 and 0.774, suggesting only moderate uncertainty in predict response correctly. When using reformulated inputs, model performance generally declined, as indicated by decreased accuracy and F1 scores in most datasets.
%\ian{was it the scientist who was rephrasing?}\sandeep{yes, but mostly using the isomorphs of SMILE string representation of molecule}
Furthermore, in the forward reaction prediction tasks, GPT-3.5 and GPT-4 performed poorly, with noticeable declines when molecular representation variations were introduced. Although output uncertainty metrics, such as entropy-based measures, provided high AUC scores (ranging from 0.86 to 0.99 indicating better uncertainty awareness), overall accuracy was limited, highlighting the need for improved LLM understanding of chemical knowledge. These findings emphasize that while uncertainty metrics can indicate response reliability, significant improvements are needed to make LLMs reliable in critical scientific applications.