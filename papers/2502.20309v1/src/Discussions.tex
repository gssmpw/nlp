
As LLMs continue to expand the notions of what AI can accomplish, there remain two main challenges to address to enable the broad adoption of LLMs by the scientific community as research assistants: a holistic understanding of the capabilities of LLMs and a strong confidence in the results produced by them.  To address this, %we propose a holistic and rigorous methodology to assess the performance of LLMs as research assistants.  O
our proposed methodology features four techniques: multiple choice questions, open-response questions, lab-style experiments, and field-style experiments which complement each other to form a comprehensive, rigorous, and realistic assessment of the capabilities of AI systems. Underneath the four approaches are three cross-cutting aspects, including trust and safety, reliable uncertainty quantification, and scalable software infrastructure which support our approach.
In addition to proposing the holistic methodology, our team has advanced the state-of-the-art in each of the techniques and aspects. 

Multiple choice questions are a key technique to evaluate LLMs because of their ability to quickly assess a breadth of knowledge; our team extends beyond existing benchmarks with automatically-generated domain-specific MCQ benchmarks in Astronomy and Climate and the multi-domain AI4S Benchmark with both human and automatically curated with human reviews have revealed significant gaps in knowledge recall and reasoning in LLMs. We find that our new AI4S benchmark is more challenging for LLMs than benchmarks like GPQA. This increased difficulty stems from the AI4S benchmark design, which integrates manual and automatic question generation to create diverse and nuanced questions that assess reasoning and domain-specific knowledge. The Astronomy benchmark also highlighted disparities in performance and cost-efficiency across frontier models, as well as across English and non-English language models. The Climate benchmarks showed that models like GPT-4o struggled both to produce assessments of fine-grained knowledge and questions that vary in style and content. The AI4S Benchmark highlighted significant disparities in performance across models due to its rigorous evaluation across multiple domains. %of inter-domain reasoning.

Open response questions similarly serve an important role, allowing a more detailed but still fast assessment of model performance.
Open-ended benchmarks such as SciCode provide realistic and challenging coding problems across fields such as physics, biology, and materials science, rigorously testing model abilities to reason, recall knowledge, and generate accurate code. Similarly, the ALDbench materials science benchmark allowed experts to uncover hallucinations and evaluate responses with precision, generating datasets valuable for further refining evaluation methods. 

A key innovation of our approach is to incorporate more realistic end-to-end experiments with \textit{lab-style} and \textit{field-style} experiments which more closely reflect the in-depth and iterative problem-solving practiced by scientists. lab-style experiments provided holistic assessments of LLM capabilities in research workflows, including hypothesis generation, analysis, and reporting. For example, experiments revealed variability in performance across models, with GPT-4o requiring five prompts to address a checkpointing problem, while Argo/O1-preview re-solved the problem with just one prompt. Field-style experiments, which analyzed real-world interactions between scientists and LLMs at scale, offered quantitative insights into model strengths and weaknesses as research assistants. 
These studies identified the remarkable performance of LLMs compared to PhD students and postdocs while struggling to present novel or ground-breaking results.

Together, our combination of domain-specific and multi-domain MCQs, open-ended benchmarks, and end-to-end experiments provides a holistic framework for assessing LLMs---one that we argue points to a new methodology able not only to quantify current model limitations but also to guide targeted improvements aimed at aligning model capabilities with the nuanced demands of real-world scientific research.

Looking ahead, we are seeking to expand evaluation benchmarks to comprehensively assess LLM capabilities across diverse scientific domains while also incorporating advanced methodologies for trustworthiness, uncertainty quantification (UQ), and iterative evaluation. We anticipate conducting more and refined lab-style experiments to provide yet precise assessments of AI model capabilities for specific research problems, with the goal of both improving scalability and coverage and tracking model progress across generations. We also aim to refine the Field-style experiments method capturing real-world interactions between scientists and LLMs and to leverage feedback to align automated scoring with human judgments through instruct-tuned models. New benchmarks for Retrieval-Augmented Generation (RAG) and multimodal narrative assessments will target domains like biology, weather/climate, and cosmology where Argonne has access to substantial quantities of scientific simulation results and data, utilizing constructs such as aggregation and multihop scenarios to evaluate performance across modalities. We will adopt agent evaluation techniques from frameworks like CACTUS \cite{mcnaughton2024cactus} for multi-turn chemistry tasks, and automated red-teaming \cite{Madireddy2024AQAG} will enhance safety evaluations by systematically identifying vulnerabilities such as biases and hallucinations. We will also advance trustworthiness and UQ through embedding-based approaches, including tools inspired by HaloScope \cite{du2024haloscope}, to capture subtle input variations and distinguish between truthful and hallucinated outputs. These initiatives will enable evaluations to remain rigorous, scalable, and reflective of real-world scientific challenges.

Evaluating LLMs capabilities as research assistants at scale require a powerful infrastructure. 
We envision the STaR framework evolving into a scalable, efficient, and user-friendly platform for HPC environments, with a modular architecture that supports dynamic and multimodal evaluations. Scalable inference backends, such as vLLM \cite{kwon2023efficient} and DeepSpeed FastGen \cite{holmes2024deepspeed}, will enable efficient handling of large benchmarks and models. Collaborations with other national laboratories and NIST will contribute to consistent proxy benchmarks for safety evaluations. By integrating these capabilities, STaR will strive to enable robust, scalable, and reliable assessments of LLMs, fostering impactful applications in scientific discovery while prioritizing safety and computational efficiency.

This paper presents the current state of the effort at Argonne National Laboratory to establish a methodology to evaluate LLMs capabilities as research assistants. We envision this effort as a continuous one because LLMs continue to progress and we will need to increase the difficulty of the different tests as LLMs progress.