\subsubsection{\bf SciCode - Scientific Code Generation Benchmark}
%\ian{The first part of this paragraph (up to the last two sentences) is redundant with earlier text. So I removed it.}
%The rapid development of LLMs has significantly accelerated advancements in AI, but existing benchmarks often fail to keep pace with growing LLM capabilities. Current benchmarks, particularly in scientific domains, frequently rely on synthetic or narrow tasks that struggle to capture the complexity and realism required for meaningful evaluation. This creates a disconnect between a model’s perceived performance on benchmarks and its actual utility in real-world applications, especially for tasks like code generation and problem-solving in science. High-quality, realistic, and challenging benchmarks are essential to address this gap, driving progress by enabling a more accurate evaluation of LLMs' generalization capabilities and their ability to assist in scientific discovery. These considerations motivated the development of SciCode, a benchmark designed to assess LLMs in solving complex scientific coding problems across diverse domains. By providing tasks that reflect real-world challenges and require multi-step reasoning, SciCode ensures that models are rigorously tested in contexts that closely align with the demands of scientific research.
The SciCode Benchmark is a set of manually curated coding problems designed to assess LLM capabilities for solving complex scientific coding problems across diverse domains. 
By providing tasks that reflect real-world challenges and require multi-step reasoning, SciCode allows models to be tested in contexts that align closely with the demands of scientific research~\cite{tian2024scicode}.
%\ian{If there is a paper with details, provide a pointer?}\eliu{done}
SciCode includes problems 
%The SciCode Benchmark is a comprehensive evaluation framework tailored to test the code generation and problem-solving abilities of LLMs 
across a range of scientific domains, including computational mechanics, quantum information, quantum chemistry, ecology, and molecular modeling. It consists of 80 main problems, decomposed into 338 intermediate steps, enabling a structured approach to assessing model capabilities. Solving each individual problem requires that an LLM implement multiple Python functions corresponding to subproblems and integrate those functions into a cohesive solution: see \autoref{fig:scicode_example}. Each problem is accompanied by a gold-standard solution and multiple test cases so as to permit robust and reliable automatic evaluation. 
\begin{figure*} %{r}{0.5\textwidth} 
    \centering
%    \fbox{
    \includegraphics[width=\textwidth,trim=0 6mm 6mm 10mm, clip]{figures/SciCode_LLMs.png} 
    \caption{The performance of various LLMs on SciCode problems. 
    This histogram displays the accuracy (vertical axis, 0\% to 100\%) of various state-of-the-art LLMs (listed on the horizontal axis) 
    in solving both main problems (red) and their associated subproblems (blue) within SciCode. 
    To solve a main problem, LLMs must implement one Python function per subproblem %\ian{Do you mean, one per subproblem?} \eliu{reworded} 
    and integrate them into a comprehensive solution. SciCode provides gold-standard solutions and multiple test cases for reliable automatic evaluation. 
    These consistently poor results highlight the need for LLMs that incorporate scientific knowledge and advanced reasoning to better assist researchers.  
    }
%    }
    \label{fig:scicode}
\end{figure*}
%By leveraging abundant, high-quality data \ian{I can't tell what this is referring to} typically unavailable to LLMs during training, SciCode challenges models to generalize to new scientific tasks and contexts. The benchmark highlights the importance of scientific LLMs, such as AuroraGPT, which incorporate specialized knowledge and advanced reasoning capabilities, to better support researchers in solving complex, real-world problems.
%\ian{The preceding sentence seems speculative, until you provide results.}


% \begin{figure*} %{r}{0.5\textwidth} 
%     \centering
% %    \fbox{
%     \includegraphics[width=\textwidth]{figures/SciCode_Example.pdf} 
%     \caption{A SciCode main problem is divided into multiple simpler subproblems for ease of implementation. Docstrings outline the requirements and specify the input-output formats. The scientific background is provided by expert annotators to offer necessary context and guidance.
%     }
% %    }
%     \label{fig:scicode_example}
% \end{figure*}

Each SciCode problem is meticulously annotated and verified by at least two senior researchers to ensure accuracy, and is drawn from real-world research tasks, maintaining relevance to practical applications. 
Problems are curated to avoid overlap with publicly available datasets and thus to test the deep scientific knowledge and analytical skills of LLMs by requiring the decomposition and integration of complex problems into comprehensive solutions. Additionally, SciCode %the framework \ian{What is the framework?} 
allows for flexible evaluation of model capabilities in varied setups, enabling adjustments like providing background information or conditioning on previous solutions. 

The SciCode Benchmark is configured to assess LLM capabilities to solve SciCode problems by using zero-shot prompts, maintaining a general approach while creating distinct prompts for various evaluation setups to guide the model on the tasks, as 
described in detail in \cite{tian2024scicode}. %\ian{We need pointers to details here.}\eliu{done}
The prompts remain consistent across models and fields, incorporating instructions for the main and sub-problems, as well as code from previous subproblems. We evaluated the coding capabilities of several state-of-the-art LLMs using the SciCode benchmark, focusing on three key aspects to assess their performance. First, the \textit{Impact of Scientific Background} was analyzed by testing models in two modes: without background, to evaluate inherent scientific knowledge and reasoning, and with background, to focus on coding and instruction-following capabilities. The results showed significant performance improvements with background information, highlighting the limitations of current LLMs in scientific reasoning. Second, the comparison between \textit{Gold vs. Generated Solutions} revealed insights into the challenges of realistic evaluations. While gold solutions accurately address each problem, generated solutions introduce error accumulation, creating a more practical and demanding evaluation scenario. Lastly, the assessment of \textit{Main vs.\ Subproblems} provided a nuanced understanding of model performance. A main problem was considered solved only when all subproblem solutions and the integrated result were accurate. Additionally, SciCode’s design allows independent evaluation of subproblems, enabling precise analysis of models’ reasoning and coding abilities across discrete tasks. These evaluation dimensions underscore the benchmark's rigor in testing LLMs for real-world scientific applications.

%\noindent 
We summarize the findings of our studies using several state-of-the-art models in \autoref{fig:scicode}. These results show that SciCode is a difficult benchmark for current LLMs. Consistent with our observations on proprietary models, open-weight LLMs under test also showed their lack of capabilities in solving any main problem despite being able to solve a number of sub-problems correctly.



The SciCode project provides insights into the challenges of evaluating LLMs in scientific coding tasks, highlighting significant gaps in current capabilities. Despite recent advancements, state-of-the-art models like OpenAI’s o1-preview and Claude3.5-Sonnet solve only a small fraction (7.7\%) of the main problems, underscoring the disparity between existing LLMs and the deep scientific reasoning required for real-world research. SciCode is designed to address this gap by focusing on real-world, research-level problems across diverse natural science fields, including mathematics, physics, chemistry, and biology. Sourced from peer-reviewed work, these problems test LLMs' ability to generalize to less familiar domains. By decomposing problems into subproblems with detailed annotations, SciCode rigorously evaluates models’ coding, reasoning, and knowledge integration capabilities. While providing scientific background information improves model performance, the persistent struggle of LLMs with these tasks emphasizes their current limitations in handling complex scientific challenges. The project highlights the importance of high-quality data, domain-specific validation, and carefully curated problems to advance the development of AI tools for scientific research. The findings indicate substantial progress is needed to enhance scientific reasoning and background knowledge integration in LLMs to enable their effective application in real-world scenarios.