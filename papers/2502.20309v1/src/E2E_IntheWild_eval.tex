\begin{figure*}[htb] %{r}{0.5\textwidth} 
    \centering
    \hrule
%    \vspace{2mm}
    \includegraphics[width=\textwidth, clip]{figures/jam-analysis.png} 
    \vspace{-2mm}
    \hrule
    
    \caption{LLM-generated summary of detected LLM strengths and weaknesses in 125 scientist-Argo/O1-preview conversations}
    \label{fig:jamanalysis}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Methodology - In the Wild %%%%%%%%

\subsubsection{\bf Field-style experiments}
This method takes inspiration from previous studies analyzing user-LLM interactions at scale \cite{Lin2024WildBenchBL, zhu2024haluevalwildevaluatinghallucinationslanguage, shen2024donowcharacterizingevaluating}. The ``In the wild" method captures and analyzes all the interactions between volunteer users and AI models. 
This method provides additional critical information for the development and improvement of AI models for science: a precise understanding of researcher needs and requirements regarding AI assistants (e.g.: What task do researchers ask AI models to perform? What are their expectations regarding model responses? How frequently do researchers use AI models?); a deeper understanding of AI models strengths and weakness (by analyzing the thousands of prompts and responses); a window on the trends behind the use of AI models as research assistant (e.g., increased usage frequency, increased number of users, nature and distribution variations of the performed tasks); and tracking of AI model progress across generations. Ideally, this method will analyze online thousands of user-LLM interactions. Compared to the ``Lab-style experiment" method, users are not expected to evaluate LLM responses directly. Instead, evaluation is indirect, based on the study of the flow of prompts and responses. This method leverages user behavior as a signal to diagnose LLM failure modes \cite{Guo_2024}. For example, a user submitting rephrasing questions, providing feedback, or abandoning the interaction are signs of LLM weaknesses in understanding user intent. The flow can then be analyzed to diagnose potential sources of weakness \cite{nature2024}. Previous ``in-the-wild" experiments focused on nonscientific domains. The Field-style experiment method adapts the ``in-the-wild" approach to the scientific context by defining criteria and scoring specific to the scientific methodology.

On November 1, 2024, Argonne organized a JAM session that captured 180 conversations between Argonne researchers and Argo/O1-preview  %\footnotemark[\ref{1}]. 
\textsuperscript{\ref{note1}}
Researchers were asked to bring to the JAM session evaluation a scientific problem that they would work on with Argo/O1-preview as a research assistant. At the end of the session, the researchers evaluated their experience according to five criteria: Novelty, Productivity, Solution, Strength, and Importance.  (This approach is consistent with that followed in a much smaller study conducted at Los Alamos National Laboratory.) Five possible responses were proposed for each question, corresponding to a score of 1 to 5.

The scores produced by the Argonne researchers indicated that: 1) Importance: 82\% of researchers consider that AI models such as Argo/O1-preview are “very important” or “critical” to their team’s success, 2) Strength: at 59\%, they consider that AI models significantly or noticeably improve productivity, 3) Productivity: 51\% of researchers compare AI models such as Argo/O1-preview to PhD students or postdocs, 4) Solution quality: 50\% of the researchers consider that AI models such as Argo/O1-preview produce exceptional or strong solutions, 5) Novelty: only 21\% of the researchers consider that AI models such as Argo/O1-preview provide notably novel or groundbreaking solutions.
(Argo is Argonne's API proxy for OpenAI models including o1 preview.  Because OpenAI does not make it's system prompt available, this needs to be recreated.  We note the system prompt used for Argo in the Appendix, \autoref{fig:argo-assistant-prompt}.)

Although informative about the needs expressed by researchers to access models such as Argo/O1-Preview, the outcomes of the November 1, 2024 JAM session evaluation did not identify the specific strong and weak science skills of Argo/O1-preview.  To understand the reasons behind the perceived weaknesses of Argo/ O1-preview, we use LLama-3.3-70B-Instruct for an LLM-as-a-judge approach to analyze the recorded conversations and score the performance of the Argo/O1-preview concerning scientific skills. 

The goal is to develop a pipeline (workflow) to analyze the recorded conversations to assess the strengths and weaknesses of LLMs as scientific assistants. From the 180 JAM session survey responses, we manually filtered a subset of 125 that had valid transcripts and were sufficiently challenging scientific problems, requiring PhD level domain expertise and reasoning capability. We generated an initial version of the prompt to analyze the conversations using Gemini experimental 1206 and refined it manually. We chose Llama-3.3-70B-Instruct as the highest performing open model on several benchmarks including GPQA, and presented it with each transcript formatted into a detailed LLM-as-a-judge prompt (see Appendix, \autoref{fig:jam-judge-prompt}) to evaluate 29 scientific criteria. The responses contained strengths, weaknesses, and examples/evidence from each conversation, as well as a formatted scoring from 1--10 for each of the criteria. The model was instructed to identify skills that did not apply to a given conversation rather than give an actual score. A score of 0 reflects a situation in which the model could not assign a score to the criteria because it determined that the criteria were not applicable. After obtaining the responses for every conversation, the model was provided batches of 25 to summarize, with specific instructions that these summaries would be used to synthesize a final summary. Batches were used, as the total token count of all the responses was 164K, larger than the default 128K window of Llama~3.3.

\autoref{fig:jamanalysis} presents the results of the conversation analysis pipeline as a proof-of-concept. The presented results need human validation: LLMs are known to hallucinate and present overly positive assessments of outputs compared to human reviewers.
Additionally, the field-style experiments revealed that 59\% of researchers reported noticeable productivity improvements using LLMs, while 51\% likened the LLM's contributions to those of PhD students or postdocs. However, only 21\% rated the models as delivering notably novel or groundbreaking solutions.
These results should not be considered a complete or definitive assessment of human assessment of LLM performance.
%\sandeep{can we do a validation of some sorts? these results are ad-hoc for presenting in the paper}

The Field-style method for analyzing the strengths and weaknesses of LLMs is still in its nascent stages. Our analysis of scientist-LLM conversations represents the first attempt to use an LLM-as-a-judge to evaluate the performance of an LLM as a research assistant. %\sandeep{do any of the works cited in this section do such an analysis?}. 
Insights from the JAM session organized at Argonne highlight several lessons. First, scoring user-LLM interactions holistically with a small set of criteria (five in this case) permits only a high-level evaluation, insufficient for diagnosing specific sources of LLM weaknesses. Second, recording user-LLM interactions with detailed annotations, such as identifying the skills required for each prompt and scoring individual responses, offers greater diagnostic potential. While this detailed approach is not feasible for general scientist-LLM dialogues, it can be implemented in structured, especially organized sessions. Lastly, while LLMs-as-a-judge offers a scalable mechanism for analyzing user-LLM interactions, the current implementation remains a proof of concept. Additional research and validation are necessary to build confidence in the results produced by this analysis pipeline.