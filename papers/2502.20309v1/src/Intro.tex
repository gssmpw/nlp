
\begin{table*}[htb] %{r}{0.5\textwidth} 
    \centering
        \caption{Our proposed methodology for evaluating LLMs as scientific assistants combines four complementary techniques, listed in columns 2--5 below, to assess their capabilities. \textcolor{purple}{Purple text} indicates \textbf{prior} contributions by the authors, \textcolor{blue}{blue text} \textbf{new} contributions in this paper, and  black text methods adapted from existing work that we include for a complete approach.
%        \ian{1) I deleted ``from the cross-cutting perspectives of trust and safety, uncertainty quantification, and scalable software infrastructure'' as I think that confuses rather than clarifies. 
%        2) The boldface words in the body of the table seem erratic and unclear.
 %       3) What is the significance of the \textbf{``see lab styld experiments" and ``see field style experiments' in the lower right? Are these meant as pointers to other sections? Isn't there other sections for every part of the table? [I wonder if for Arxiv we could include section numbers?]}
        }
        %}
    \label{fig:multi_facet}
    \hrule
%    \vspace{2mm}
    \includegraphics[width=\textwidth, clip]{figures/contributions.png} 
    \vspace{-8mm}
    %\hrule
\end{table*}

Recent advances in Large Language Models (LLMs) have greatly broadened conceptions of what AI may be able to accomplish in the near future. Models such as OpenAI's GPT O1 \cite{openaiGPT4TechnicalReport2024}, Google's Gemini \cite{geminiteam2024gemini}, and Anthropic's Claude \cite{anthropic2024claude3} 
are transforming traditional natural language understanding (NLU) tasks like summarization, information extraction, translation, and classification with enhanced contextual depth and adaptability. They are also exhibiting promising potential \textit{beyond} NLU, with measurable progress on tasks such as mathematical problem solving, multi-step reasoning, and symbolic logic---and achieving significant milestones such as passing the Uniform BAR exam and medical licensing exams~\cite{varanasiGPT4CanAce2023}. Such achievements highlight their potential to emulate abstraction, logical deduction, and domain-specific expertise. This evolution from NLU towards addressing complex, domain-specific challenges with minimal human guidance has propelled LLMs into a pivotal role for next-generation AI systems, positioning them as a cornerstone technology in the quest toward more general-purpose AI, and potentially, artificial general intelligence (AGI).

Building on these advancements, scientists are now beginning to assess \textit{separately} the suitability and potential impact of LLMs on a wide variety of specific tasks within specific fields of research and discovery~\cite {ai4science2023impactlargelanguagemodels}. This work has led to exciting demonstrations of LLMs as transformative tools for such tasks predicting molecular properties \cite{liu2024moleculargpt}, uncovering genomic patterns \cite{benegas2025genomic}, interpreting astrophysical data \cite{ting2024astromlab}, solving mathematical problems \cite{hendrycksmath2021}, and even creating and manipulating tools for simulations and analysis \cite{schick2023toolformer}. 

These developments have also led scientists to envision the use of LLMs and transformers as research assistants that can not only
automate individual research tasks but also engage with scientific problems in depth by taking advantage of growing multi-step reasoning skills that complement their expanding contextual understanding. This vision suggests a new \emph{holistic} approach in which LLMs interface with relevant tools, operate (quasi-)autonomously on research challenges, identify relevant literature, summarize findings, propose experimental designs, and even autonomously run, and generate insights from, physical and computational experiments \cite{liu2024towards, ma2024llm}.

We identify \textbf{two main challenges} that must be addressed before LLMs can be broadly adopted by the scientific community as effective and trustworthy research assistants. 
\textbf{First}, researchers need ways to measure and evaluate LLM capabilities in the different stages and tasks of the scientific research process. Such evaluations can both guide LLM applications and integration with other tools, and provide benchmarks for developers to improve their LLMs and supporting systems. \textbf{Second}, as with other research tools and techniques, researchers need ways to assess confidence in the results produced, in order to decide whether or not they are trustworthy. A comprehensive, rigorous, accurate, transparent and community-approved evaluation methodology is necessary to address these two challenges.

This paper introduces work undertaken at Argonne National Laboratory within the AuroraGPT project to develop a research methodology that addresses these two challenges. The paper makes \textbf{three main contributions}: (i) a holistic methodology for LLM evaluation; (ii) two novel evaluation techniques (lab-style and field-style experiments); and (iii) improvements in existing state-of-the-art evaluation techniques for the specific role of research assistant.

\textbf{First}, we propose an overarching methodology for assessing the scientific knowledge, skills, and safety of AI models. As shown in \autoref{fig:multi_facet}, this methodology encompasses four complementary techniques: 1) Multiple Choice Question (MCQ) Benchmarks, which measure factual recall and reasoning capabilities in structured formats to provide fast assessment of a model's breadth of knowledge; 2) Open Response Benchmarks, which test a modelâ€™s ability to generate detailed open-ended responses and write or debug code for computational tasks giving a fast but more in-depth analysis of knowledge; 3) Lab-style Experiments, simulating various tasks in the end-to-end research process to assess model performance on those tasks and thus to provide understanding of real-world strengths and weaknesses; and 4) Field-style Experiments, capturing real-world interactions at scale to analyze user needs, model strengths, and broad capability trends, and to diagnose areas and sources of weakness in realistic scenarios. We also consider trustworthiness, i.e., alignment with ethical and safety standards, and discuss the Software Infrastructure required to implement these methodologies effectively.

\textbf{Second}, our application of ``lab-style'' and ``field-style'' techniques to LLM evaluation represents a novel approach when conducted at this scale and with such a diversity of topics with practicing scientists. These techniques go beyond existing testing methodologies: they assess in real situations the suitability of LLMs for open and unstructured problems which are both common in research and difficult to assess with either MCQs or open-response questions.

\textbf{Third}, we present improvements to techniques used within the community, including a multi-domain AI for science benchmark, called ``AI4S,'' and the Skills, Trust, and Reliability (STaR) evaluation framework, a scalable software evaluation infrastructure. %our question authoring infrastructure and 
We also summarize and contextualize the research done by our team in domain-specific benchmarks, open response benchmarks, and uncertainty quantification, and note where we employ tools from other teams to complete our holistic evaluation methodology.

Together, these efforts aim to establish a robust methodology for evaluating %and improving 
the capabilities of LLMs as trusted scientific assistants.
In the following sections, the collection of evaluations and scoring was based on voluntary participation of researchers and contributors.  

The following sections describe the related work and the details of our proposed methodology. The last section discusses next steps.


