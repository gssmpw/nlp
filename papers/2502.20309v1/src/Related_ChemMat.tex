\subsubsection{Chemistry}
Numerous benchmarks have been developed to evaluate the capabilities of LLMs in chemistry and material science.
ChemistryQA \citep{wei_chemistryqa_2020}, an early question-answer dataset developed to evaluate language models, comprises 4500 complex questions that requires reasoning and calculations. 
\cite{white_assessment_2023} evaluated LLMs based on their code generation ability for 84 computational chemistry tasks and demonstrated 30\% increase in accuracy with prompt engineering. 
\cite{guo_what_2023} developed ChemLLMBenchmark, which is composed of eight practical chemistry tasks that requires understanding, reasoning, and explaining abilities. 
They evaluated five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama, Galactica) using in-context learning and prompts designed for each task. 
Their study demonstrated the importance of in-context learning, as it improved the accuracy for all the tasks.
ChemBench \citep{mirza2024chembench} is an automated framework designed to be compatible with BigBench \citep{srivastava2023bigbench}.  ChemBench currently have more than 2700 questions, which are mainly generated semi-automatically or curated from different resources (textbooks, exams, lectures, etc). 
While the majority of the questions are MCQs, there are more than 240 open-ended questions as well.
MoleculeQA \citep{lu_moleculeqa_2024} is another question-answer dataset composed of 61,574 MCQs (three distractors for each question) for more than 23k molecules.
The questions are based on factual information (structural information, properties, application areas, and source of origin) about these molecules and are automatically generated based on templates.
The recently introduced MolPuzzle \citep{guo_can_2024} multimodal benchmark is based on deducing molecular structures from spectral data. 
The benchmark consists of more than 23k question-answer pairs with three interlinked sequential sub-tasks that evaluate molecule understanding, spectrum interpretation, and molecule
construction.
TOMG-Bench \citep{li_tomg-bench_2024} is an open-domain molecule generation benchmark composed of three  tasks: molecule edition, molecule optimization, and customized molecule generation. 
Each task has three subtasks comprised with 5000 samples, which are evaluated for both accuracy and quality. 
The evaluation metrics is specialized for the subtasks and can be automatically evaluated.
TOMG-Bench provides a leaderboard of 25 LLMs based on the weighted average of the accuracy scores.
