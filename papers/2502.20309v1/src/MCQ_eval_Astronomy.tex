
\subsubsection{\bf Astronomy} 
%\cite{ting2024astromlab} developed 
The Astronomy Benchmark \cite{ting2024astromlab} assesses LLM performance in a manner that reflects the interdisciplinary nature of astronomy, testing both factual recall and the ability to connect insights across subfields. This benchmark was generated automatically using an LLM to compose MCQs from astronomy papers. To assemble a rich repository of scientific knowledge, we leveraged the Annual Review of Astronomy and Astrophysics (ARAA), a selective review journal renowned for its comprehensive overviews authored by leading experts. %provided an ideal resource for constructing such a benchmark.
%\ian{``Such a dataset'': you have not referred to a ``dataset''.}. 
%Leveraging this rich repository of scientific knowledge mitigated the prohibitive costs of domain-expert participation, enabling the creation of a benchmark tailored to the complexities of astronomical research\ian{These are grand claims of ``specialness'' but needs more details to justify}.\sandeep{@nesar, @josh can you add a statement to justify}

%In developing this benchmark, \cite{ting2024astromlab} used 
The Nougat optical character recognition (OCR) tool was used to transcribe 885 ARAA articles over the years 1963 to 2023. Each transcribed paper was processed using Gemini-1.5-Pro, a long-context LLM capable of handling up to one million tokens, to generate five multiple-choice questions (MCQs) per paper. %Through meticulous prompt engineering
%\ian{Where is that described?}\sandeep{prompt to be added to appendix}
The questions were designed to be specific, yet independent of the article sections, with generalized answers and balanced options to avoid bias. This process yielded a total of 4425 MCQs covering diverse topics such as quasar density decline at high redshifts and subgrid feedback model calibration in simulations. %A subset of these questions was reviewed by experts to ensure quality and relevance, bridging the gap between automated scalability and human oversight.\ian{How many? With what results?}

\begin{tcolorbox}[colback=blue!10, colframe=blue!40!black, title=Sample question from Astronomy benchmark dataset, left=2pt, right=2pt, top=1pt, bottom=1pt]
\textbf{How does the presence of stellar companions influence the formation and detection of exoplanets?}
\small
\begin{enumerate}
    \item[(A)] Stellar companions can dilute transit signals, potentially leading to misclassification of planets and inaccurate parameter estimations. Additionally, their gravitational influence can suppress planet formation in close binary systems.
    \item[(B)] Stellar companions provide additional sources of gravitational perturbations, enhancing planet formation by promoting planetesimal accretion and facilitating the formation of gas giants.
    \item[(C)] Stellar companions contribute to the metallicity enrichment of planetary systems, leading to the formation of more massive and diverse planets, including super-Earths and hot Jupiters.
    \item[(D)] Stellar companions act as gravitational lenses, increasing the detectability of exoplanets through microlensing events and enabling the discovery of planets at greater distances from their host stars.
\end{enumerate}
\end{tcolorbox}


%\cite{ting2024astromlab} used the 
The Astronomy Benchmark has been then used to assess both the accuracy and computational cost of dozens of different closed and open LLM variants.
This study revealed disparities in LLM performance across general-purpose and specialized tasks that highlight significant performance gaps and performance-to-cost ratios.
%\ian{Explain more what is meant?}. 
While frontier models like %\ian{What does ``models like'' mean? Like in what dimension?}
Claude-3.5-Sonnet excelled in the Astronomy Benchmark with an accuracy of 85.0\%, outperforming GPT-4o (80.4\%) and Gemini-1.5-Pro (77.6\%), these differences are less evident in general-purpose benchmarks such as MMLU~\cite{open-llm-leaderboard}. 
%\ian{You can't just bring up MMLU in the middle of a disxcussion of Astronomy Benchmark without some reference to where the results come from}.
A study of how Astronomy Benchmark performance varied with compute costs showed that, roughly speaking, each 3.5 percentage points increased accuracy was associated with 10-fold increase in price, within most given series of models such as GPT, Gemini or Claude. An analysis of the cost per 0.1 M tokens showed that the cost for a desired performance can vary by more than three orders of magnitude across different models: see Figure~2 in \cite{ting2024astromlab}.
%underscoring the current high costs incurred with scaling models that can result in cost-efficiency differences \ian{what is a ``cost-efficiency difference''?} of up to three orders of magnitude\ian{Explain more?}. 



%\ian{I suspect that you are summarizing here results that are described in detail in \cite{ting2024astromlab}. But the wording does not make that clear, so I keep wanting to know more details. It is also uncleaer who did the work. Do we say that ``we did this study, with details presented in another paper''? Or ``someone else did this study'': if so, make clear why we are presenting it here.} \sandeep{Yes Ian, analogous to the benchmarks like Scicode, Aldbench discussed later, some of our eval team were closely involved in this benchmark creation, we are essentially summarizing this study and want to highlight the lessons learned and in later section  discuss how some of the lessons helped out follow up benchmarking efforts }
The study also showed that open-weight models, though improving, lag behind proprietary ones, with older versions underperforming by as much as 30\% on specialized tasks. Performance also varies significantly between English-focused and non-English-focused models, with the latter struggling in areas like theoretical astronomy and advanced instrumentation, reflecting gaps in training datasets. However, recent astronomy subfields, such as post-1990 advancements, exhibit narrower performance gaps which may be
due to model's ability to handling historical context or older scientific consensus.
%challenges remain in generalizing across newer domains\ian{This sentence is not at all clear. You distinguish ``recent subfields'' and ``newer domains". Is newer more new than recent, or less new?}. \sandeep{yes, newest in that lot like exoplanet studies; but commenting this for clarity}
These findings emphasize the importance of domain-specific benchmarks to assess not only performance in specialized tasks but also the performance-to-cost ratio crucial for user adoption in assisting with scientific research. This work also shows that performance varies across sub-fields.

% and scalability but also address language diversity and the evolving landscape of specialized knowledge, ensuring LLMs are optimized for real-world scientific applications
%\ian{A cryptic and poorly worded sentence}.\sandeep{reworded}

