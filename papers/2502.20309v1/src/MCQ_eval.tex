
Domain-specific benchmarks are crucial for evaluating LLMs in specialized fields, as they address the limitations of general benchmarks that often fail to capture the complexities of domain-specific tasks. Without tailored benchmarks \cite{ting2024astromlab}, LLMs risk over training on well-established datasets, leading to inflated performance that does not translate to real-world applicability.
These benchmarks are essential for guiding targeted improvements, ensuring that models meet the specific demands of scientific research, and providing a baseline to understand their strengths and weaknesses. By capturing the nuanced challenges of individual domains, such benchmarks foster the effective and ethical deployment of LLMs, enabling their potential to accelerate discovery and innovation across disciplines. We now discuss the domain-specific benchmarking efforts that we conducted in Astronomy and Climate modeling.

\input{src/MCQ_eval_Astronomy}
\input{src/MCQ_eval_Climate}