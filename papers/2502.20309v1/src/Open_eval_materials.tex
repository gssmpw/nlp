\subsubsection{\bf ALDbench - Materials Synthesis Benchmark}

% \ian{I have a high-level question about this work. ALDbench is described as as a ``domain-specific open-ended question benchmark.'' As I understand things it is a list of questions, not a list of QA pairs, and its purpose is for evaluating the performance of LLMs via a manual process in which the LLM is given the questions and the answers are evaluated by humans. Presumably for that reason there is deliberately NOT a published list of ``correct'' answers, as otherwise LLMs could learn those answers. \textbf{SO:} Assuming I am correct in my understanding, we should make clear that this is a different sort of benchmark from the others that we have discussed. \textbf{AND:} As the text refers to ``this approach'', we should make clear what it is. Give it a name, maybe?}
%\sandeep{This is similar to the open-response benchmark in that the model is providing a free-form response, but instead of using LLM-as-a-judge to assess it, we are using human evaluators and studying the correlation to the rubrics}

An area that lacked relevant benchmarks is materials synthesis.
This is particularly important for potential applications
of LLMs in automated materials discovery or as AI research
assistants. LLMs underpinning such capabilities need to exhibit both the ability to reason about specific processes (for instance to avoid unsafe conditions or transfer ideas across reactors and process conditions) and have a robust understanding of the literature (to build on existing process knowledge and avoid known dead ends).

As such capabilities appear hard to evaluate by using either MCQs or the statistical scorer or embedding approaches described earlier.
%\ian{The text refers to ``NLP methods'' which I think refers to }
%or natural language processing methods, 
we developed a new open-response benchmark ALDbench on materials synthesis, and in particular on a synthesis technique called 
atomic layer deposition \cite{aldbench}. Here we targeted a range of difficulty spanning from graduate level to PhD-level domain expert current with the state of the art. A model's ability to perform at a domain expert level is paramount whenever models are expected to assist in decision making processes that involve costly experiments.
% In order to address these needs we created ALDbench,
% an open-response benchmark focused on materials synthesis,
% and in particular on a synthesis technique called 
% atomic layer deposition \cite{aldbench}.
Beyond its applied interest in areas such as energy and microelectronics \cite{AlvaroALD}, this domain brings together multiple
topics that are commonplace in chemistry-driven synthesis, including metal-organic and inorganic molecules, gas-surface kinetics and heterogeneous reactions, and gas phase transport. Evaluating LLM capabilities in this field can provide insights with wide applicability to other material synthesis techniques.

To compile the benchmark, we asked six PhD-level human experts to generate ``questions that a researcher or a graduate student who is not familiar with a specific process/application would ask an AI assistant." 
The curated questions could be grouped into four categories: 1) \emph{how to grow}, where the query is about material
synthesis; 2) \emph{specific questions about ALD processes}, comprising more in-depth queries about a process or material;  3) \emph{general ALD knowledge}, with questions about the synthesis
technique; and 4) \emph{applications}.

The human experts were then asked to grade the questions using a scale of 1 to 5 on two criteria with the following rubrics similar to the AI4S benchmark: (1) \emph{Difficulty:} 1--Easy, early graduate; 5--Hard, top expert; (2) \emph{Specificity:} 1--General; 5--Specific, quantitative.
% \begin{itemize}
% \item \emph{Difficulty:} 1--Easy, early graduate; 5--Hard, top expert.

% \item \emph{Specificity:} 1--General; 5--Specific, quantitative.
% \end{itemize}
Each response is then graded using four criteria with the following rubrics: (1) \emph{Overall quality:} 1--Very low quality; 5--Excellent; (2) \emph{Specificity:} 1--Too broad; 5--Targeted; (3) \emph{Relevance:} 1--Irrelevant fluff; 5--Relevant answer; (4) \emph{Accuracy:} 1--All made up; 5--All correct.   
% \begin{itemize}
% \item \emph{Overall quality:} 1--Very low quality; 5--Excellent.
% \item \emph{Specificity:} 1--Too broad; 5--Targeted.
% \item \emph{Relevance:} 1--Irrelevant fluff; 5--Relevant answer.
% \item \emph{Accuracy:} 1--All made up; 5--All correct.
% \end{itemize}
The use of multiple criteria allowed us to probe aspects of
the generation process, such as relevance or specificity of the
response, that are not easily captured by benchmarks focused
on accuracy.

We ran this benchmark using an instance of OpenAIâ€™s GPT-4o, with seven PhD-level human experts reviewing model responses.
Details are in the ALDbench paper \cite{aldbench}.
The model responses received a composite quality score of 3.7, consistent with a passing grade. However, 36\% of the questions received at least one below average score. When
we carried out an in-depth analysis of the responses we
identified at least five instances of hallucination. In \autoref{fig:aldbench} we show the
distribution of mean scores for all the questions in the benchmark
and the four criteria evaluated by the human experts.

\begin{figure}[t]
\centering
    \includegraphics[width=.9\columnwidth]{figures/histogram_eval.png}
    \caption{Distribution of the mean scores of GPT-4o
    responses to all questions in the ALDbench benchmark.}
    \label{fig:aldbench}
    % \vspace{-4mm}
\end{figure}

We also explored statistical
correlations between the difficulty and specificity of
each question and the human expert scores for each evaluation criteria. For each (question, response) pair we computed p-values using the Fisher
exact test to evaluate the statistical significance of the
correlation.  We found statistically significant correlations
between question difficulty and response quality ($p_0$ = 0.033), question difficulty 
and relevance ($p_0$ = 0.016), and question specificity
and response accuracy ($p_0$ = 0.007). In all three cases, higher difficulty or specificity correlated
with lower scores.
These results emphasize the need to evaluate LLMs across multiple criteria beyond traditional metrics of difficulty and accuracy.

Our results show that highly targeted, open-response benchmarks can provide
information about LLM performance in scientific domains that is complementary to MCQs or natural language processing benchmarks. The methodology developed in this work allowed us to probe in depth model performance in a specific domain. With the aid of a small team of PhD-level experts we were able to identify instances of hallucinations and explore model responses in a level of detail that it is hard to accomplish using automatic evaluation methods. The extension of this approach to other domains, such as energy storage or microelectronics, is trivial. Moreover, as a byproduct of this
effort, we collected a small dataset of questions and human rated responses across four different evaluation criteria. As we explore other domains we can use this data to train or validate automatic question evaluation approaches for open-ended benchmarks. 