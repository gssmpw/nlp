Next we discuss the open-ended benchmarks. 
These are essential for evaluating the reasoning, creativity, and problem-solving abilities of LLMs, particularly in scientific domains. Unlike MCQs, which primarily test factual recall or single-step reasoning, open-ended tasks engage models with complex, real-world problems that require multi-step reasoning, synthesis of knowledge across domains, and adaptive problem-solving. For instance, while an MCQ might test a model's recall of a specific scientific fact, an open-ended task could require the model to design an experiment, analyze data, or propose solutions to unsolved research questions. This format aligns better with the exploratory nature of scientific inquiry, offering a more comprehensive assessment of a model's capabilities. However, existing open-ended benchmarks often fall short in capturing the depth and realism needed for scientific evaluations. Many rely on synthetic tasks that fail to reflect the intricacies of real-world scientific challenges, such as multi-disciplinary reasoning or generating accurate code for practical applications. 
%Furthermore, the absence of standardized metrics for evaluating open-ended responses limits the utility of these benchmarks in providing consistent, actionable insights into a model’s performance.

%\ian{The following text provides a nice discussion of methods for evaluating open-ended responses. But it doesn't explain why this material is here. Provide some context?}\sandeep{added}
While open responses questions are versatile in capability, their
evaluation is more complicated compared to MCQs.
%than evaluation of MCQs because a model is not asked to select among a set of answers. Instead, the model produces a response to a question as free text that needs to be understood and assessed. 
Different assessment approaches are applied, depending on the task at hand.  
A first class of \textit{statistical scorer} approaches looks at co-occurrence of n-grams (sequences of letters or words) in LLM outputs vs.\ supplied ground truth responses. 
%Statistical scorers evaluate an LLM response from n-grams (sequences of letters or words). 
%\ian{In my view, the text is not clear that (as I think is correct?) you first introduce a broad class of statistical scorers, which contains BLUE, ROUGE, and METEOR. Then embedding approaches? Then LLM as a judge? Is my text ok?}\sandeep{corrected}
In this context, widely used scores are BLEU (BiLingual Evaluation Understudy) compares LLM outputs against ground truths. It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) evaluates text summaries and computes recall by comparing the overlap of n-grams between LLM outputs and expected outputs. It determines the proportion (0–1) of n-grams in the reference present in the LLM output. METEOR (Metric for Evaluation of Translation with Explicit Ordering) calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps) and leverages linguistic databases to account for synonyms. Statistical scorers do not take any semantics and reasoning capabilities into account. 

A second class are \textit{embedding approaches} that seek to compare the semantics of LLM responses and reference answers. Here, some widely used scores include BERTScore, that relies on pre-trained language models (e.g., BERT) and computes the cosine similarity between the contextual embeddings of the LLM responses and references. These similarities are then aggregated to produce a final score. Other tools such as CheckEmbed \cite{besta2024checkembed} can be used to compare the semantics of LLM responses and reference answers. 
The third and most recent class is the  \textit{LLM-as-a-judge methods} which tackle the problem of evaluating LLM open responses when no reference answer is available. This approach currently has two variations. In the ``Pairwise comparison" version, an LLM judge is presented with a question and two answers, and tasked to determine which one is better or declare a tie. In the ``Single answer grading" version, an LLM judge is asked to directly assign a score to a single answer. In principle, LLM-as-a-judge can offer several key benefits: consistency, scalability, and explainability. However, the approach also has limitations: position bias (first answer better in ``Pairwise comparison"), verbosity bias (longer answer better), self-enhancement bias (self-generated answer better) and limited capability to grade math and reasoning questions \cite{LLM-as-judge}. Moreover, the reliability of such evaluations is still the subject of research.

We now discuss two open-ended benchmarks, one for scientific code generation and another for atomic layer deposition in Material Science.

\input{src/Open_eval_SciCode}
\input{src/Open_eval_materials}