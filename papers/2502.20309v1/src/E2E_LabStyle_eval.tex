
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Methodology - Lab-Style %%%%%%%%
\subsubsection{\bf Lab-style experiments}

%%%%%%%%%%%%%%%%%
\begin{figure*}[htb] %{r}{0.5\textwidth} 
    \centering
    \hrule
%    \vspace{2mm}
    \includegraphics[width=\textwidth, clip]{figures/HPDC24-LLM-interaction.png} 
    \vspace{-2mm}
    \hrule
    
    \caption{Example of multi-turn interaction between a researcher and several LLMs used as research assistants in an attempt to repeat the research developed for the HPDC24 paper. Because of space limitations, the figure does not show the models' responses and research analysis. The full interaction for the HPDC24 experiment can be found here: https://tinyurl.com/yv4awky3}
    \label{fig:labstyle-prompts}
\end{figure*}

These experiments are designed to evaluate the capabilities of AI models to assist researchers in performing the typical tasks (\autoref{fig:multi_facet}) to solve scientific problems. Note that, in real situations, these tasks are often repeated several times to solve problems.
%\ian{Some people argue, strongly, that this is an incorrect caricature of how research really works.}
%\ian{In the following it is unclear whether this is entirely automated. The text sounds like it is ``using the same prompts'' but later text implies otherwise (to me): ``the number of prompts needed to solve a particular research problem''}
%Just what tasks are involved in research processes in practice?
%Multi-Aspect Summarization of Scientific Workflows (MASSW)  \cite{zhang2024massw} attempted to answer that question via the analysis of 152,000 titles and abstracts of computer science papers. The results are five stages (high-level tasks): Context, Key Idea, Method, Outcome Projected. Impact.
%However, while papers can be expected to describe the high-level tasks of the associated research workflow, the steps involved in solving a research problem are usually more convoluted than the linear progression described in papers. Lab-style LLM experiments provide an opportunity to overcome this difficulty:
By capturing and evaluating all interactions between research and LLMs while attempting to solve a research problem, a lab-style experiment can capture accurately the complex reality of solving complex research problems. It can thus provides a unique perspective on the ``distance" between the ideal scientific assistant and the current capabilities of AI models. 

The setup for lab-style experiments involves defining a specific scientific problem and presenting it to multiple AI models for comparison. Each model is manually tasked with assisting in all the research tasks using the same prompts, ensuring consistency across evaluations. Prompts and responses are meticulously recorded, and domain experts analyze and comment on each response to assess model relevance, effectiveness, and overall performance. The response of each model in every step is compared to that of a human assistant, typically at the post-PhD level. Performance is characterized by evaluated against criteria such as correctness, conciseness, and precision. 

We have conducted lab-style experiments with not only different LLMs but also different versions of individual LLMs, with the latter studies permitting evaluation of improvements over time. For example, a relevant metric of progress across model generation is the number of prompts needed to solve a particular research problem. By focusing on real-world scientific workflows and expert evaluation, lab-style experiments provide a practical approach to observe AI model improvements as scientific research assistants.
%\ian{Another overly ambitious statement?}


To date, we have performed three lab-style experiments with five domain experts (one expert per experiment) %\footnotemark[\ref{1}], 
\textsuperscript{\ref{note1}}
each of whom provided a problem to solve, generated prompts covering the different research steps, and analyzed model responses.  
%\sandeep{Good to add the prompt here or in appendix} \franck{We will add a fragment of conversation in the appendix}. 
Three experiments were related to parallel/distributed computing (scheduling of a directed acylic graph; solving a partial differential equation; zero-overhead checkpointing) and were selected carefully from three categories: open problem (no known solution), published problem (solution known), and recently published problem (solution known). For the ``published problem," the paper is more than two years old, and thus we assume that AI model developers had access to the paper. For the ``recently published problem," the experiment was performed just a few months after the publication; here, we assume that many models were not trained with the paper. 

\begin{figure*}[htb] %{r}{0.5\textwidth} 
    \centering
    \hrule
%    \vspace{2mm}
    \includegraphics[width=\textwidth, clip]{figures/labstyle.png} 
    \vspace{-2mm}
    \hrule
    
    \caption{A partial scoring of several models used as AI assistants on August 20, 2024, to solve the zero-overhead checkpointing problem. The results highlight the strengths and weaknesses of different models for the different research steps. We note that the RAG model (Perplexity Pro) has a decisive advantage in several steps for this particular problem. Other models struggle in most steps.}
    \label{fig:labstyle}
\end{figure*}

The experts with whom we worked in these experiments had never previously used AI models as research assistants. The experts provided two levels of analysis: 1) a detailed analysis of the responses received for individual prompts, and 2) high-level scoring compared to a human researcher using the A, B, C, D, E, F scale (A being the human reference, F being the worst possible score). In total, the three experiments cover about 20 hours of interactions, about 100 prompts, and about 250 pages of testing and analysis. We used 10 AI models (not all models were used in all three experiments): O1-preview, GPT-4o, GPT-3.5, Claude3 Sonnet, Claude Haiku, Mistral, Llama3~70B, Llama3~405B, Perplexity Pro, and Gemini 1.5 (not all models were tested on all prompts because some models produced incorrect responses before reaching the end of the evaluation.) %\sandeep{any particular trend to report here?}). 

We show in \autoref{fig:labstyle-prompts} the beginning of the multi-turn interaction and in \autoref{fig:labstyle} part of our initial high-level scoring of several AI models for the zero-overhead checkpointing problem. The lab style experiment was performed on August 20, 2024. The scoring reflects the performance of the models during the experiment. This problem falls into the category of recently published problems. The goal was to check whether the models could reproduce the analysis and design of the LLM checkpointing system presented in the 2024 ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC24) best paper \cite{DataStates-LLM}. The most important part of the experiment was to assess each model's ability to 1) identify the fundamental observation (non-mutable parameters and optimizer state during the forward and backward passes of LLM training) and 2) propose a design for an asynchronous checkpointing system that exploits this observation. %\sandeep{does this experiment follow all the steps 1-5 in 1st paragraph?}

Based on these initial three experiments, we performed two other experiments on open problems in Chemistry and Biology. These experiments used the same overall interaction collection approach and compared more recent reasoning models (O1-preview, OpenAI O3-mini, Gimini 2.0 experimental). We also used a more rigorous scoring system, defining precisely every score level for every evaluated skill. From these experiments, we developed a Lab-style experiment tool to collect the problem setup (Figure \ref{fig:problemsetup}), every prompt-response-assessment (Figure \ref{fig:interaction}), and final assessments (Figure \ref{fig:finalassessment}). This collection tool is available on github (https://github.com/auroraGPT-ANL/questions-ui)

Our experience with the ``Lab-style experiment" method allows for several observations regarding its utility and limitations. Unlike traditional benchmarking, this method places AI models in real-world research scenarios, enabling evaluators to directly assess their knowledge, capabilities, and overall usefulness for specific tasks. By relying on multi-turn prompting and open responses, the method also tests the propensity of AI models to digress (e.g., for the HPDC24 paper experiment, a model had a tendency to focus on the consistency aspect of checkpointing, which is not relevant in that context, instead of focusing on overhead reduction) and hallucinate (as seen with earlier models in 2024 that frequently generated fabricated scientific references). However, this approach is not yet scalable and remains narrow in coverage; it requires significant manual effort, with two researchers spending 5--6 hours analyzing and comparing models for specific tasks. The specificity of the addressed research problems further limits the generalizability of the findings. Despite these constraints, the method excels in two aspects 1) providing a fine-grain capability assessment of LLMs as scientific assistants in a realistic context and 2) tracking model progress across generations. For instance, in solving the zero-overhead checkpointing problem, successive model iterations demonstrated improved efficiency, reducing the number of prompts needed to reach the key insight---from five prompts with GPT4o to just one prompt with Argo/O1-preview, which incorporates a science-oriented pre-prompt.  %\sandeep{Can we summarize the goal of the pre-prompt if we dont want to add the prompt to the document?}
%\sandeep{not clear what science-oriented pre-prompt is; can we describe the prompts and/or add these to the appendix} \franck{This is the pre-prompt systematically added by Argo}. 
This result highlights the method’s potential to reveal meaningful advancements in AI capabilities over time.




