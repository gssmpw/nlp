\label{sec:framework}

\begin{table*}[t!]
\centering
\caption{Harness evaluation results for seven Llama variants on the OpenLLM V2 benchmark.}
\label{tab:eval-aurora}
%\resizebox{\columnwidth}{!}{%
\begin{tabularx}{\textwidth}{Xlrccccc}
%\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{IFEval $\uparrow$} & \textbf{BBH $\uparrow$} & \textbf{MATH $\uparrow$} & \textbf{GPQA $\uparrow$} & \textbf{MuSR $\uparrow$} & \textbf{MMLU-PRO $\uparrow$} & \textbf{GPU Hours}\\ \midrule
% \textbf{Eval-A-7000}         & 0.1860 & 0.2922 & 0.0000 & 0.2583 & 0.3408 & 0.1100  \\ 
%\textbf{Eval-A-12000}        & 0.1710 & 0.2928 & 0.0014 & 0.2597 & 0.3713 & 0.1131  \\ 
%\textbf{Eval-A-17000}        & 0.1824 & 0.2921 & 0.0000 & 0.2584 & 0.3660 & 0.1152  \\ 
% \textbf{Eval-A-22000}        & 0.1834 & 0.3005 & 0.0000 & 0.2612 & 0.3791 & 0.1168  \\ 
%\textbf{Eval-A-27000}        & 0.1925 & 0.2904 & 0.0014 & 0.2458 & 0.3736 & 0.1162  \\ 
%\textbf{Eval-A-32000}        & 0.2007 & 0.2981 & 0.0029 & 0.2544 & 0.3816 & 0.1144  \\ 
% \textbf{Eval-A-40000}        & 0.2446 & 0.2970 & 0.0026 & 0.2504 & 0.3766 & 0.1160  \\ 
\textbf{Llama-2-7B}            & 0.2543 & 0.3475 & 0.0121 & 0.2718 & 0.3703 & 0.1848 & 5.05 \\ 
\textbf{Llama-2-7B-chat}       & 0.3538 & 0.3676 & 0.0189 & 0.2735 & 0.4034 & 0.2000 & 4.93 \\ 
\textbf{Llama-3-8B}            & 0.1536 & 0.4600 & 0.0317 & 0.3146 & 0.3677 & 0.3248 & 7.62 \\ 
\textbf{Llama-3-8B-Instruct}   & 0.4825 & 0.4885 & 0.0808 & 0.3020 & 0.3823 & 0.3580 & 6.50 \\ 
\textbf{Llama-3.1-8B}          & 0.1228 & 0.4652 & 0.0438  & 0.3070  & 0.3849  & 0.3260 & 6.94 \\ 
\textbf{Llama-3.1-8B-Instruct} & 0.4924 & 0.5058 & 0.1360  & 0.3163  & 0.3995  & 0.3789 & 8.31 \\  
\textbf{Llama-3.3-70B-Instruct} & 0.6745 & 0.6994 & 0.3391 & 0.4715 & 0.4854 & 0.5477 & 81.39 \\
\bottomrule
%\end{tabular}
%}
%\end{table}
\end{tabularx}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{DecodingTrust evaluation results on Polaris}
\label{tab:decodingtrust_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model}       & \textbf{Toxicity} & \textbf{Stereotype} & \textbf{Adversarial} & \textbf{OOD} & \textbf{Robustness to} & \textbf{Privacy} & \textbf{Machine} & \textbf{Fairness} \\
                     &                   & \textbf{Bias}       & \textbf{Robustness}  & \textbf{Robustness} & \textbf{Adv. Demonstrations} &            & \textbf{Ethics}   &                  \\
\midrule
\textbf{Llama-2-7B-chat}       & 80.0              & 97.6                & 51.01                & 75.65           & 55.54                         & 97.39      & 40.58             & 67.95            \\
\textbf{Llama-2-70B-chat}      & 80.0              & 98.0                & 52.00                & 71.00           & 74.00                         & 99.00      & 54.00             & 65.00            \\
\bottomrule
\end{tabular}
}
\end{table*}

As previous sections show, a comprehensive evaluation of LLMs as research assistants already requires the execution of many benchmarks for skills and safety assessments. We do not expect the evaluation workload to be reduced in the future. In contrast, as more capable LLMs appear, more research domains will be interested in using them, which will trigger the development of new evaluation benchmarks.
%The growing need for large-scale evaluations of many LLMs with respect to many benchmarks places ever-growing demands on software and computational infrastructure. 
This situation places ever-growing demands on software and computational infrastructure. 
%The rapid advancement of Large Language Models (LLMs) has underscored the necessity for robust evaluation frameworks that can accurately assess their capabilities across diverse tasks and domains.
Existing evaluation software platforms, such as HELM \cite{bommasani2023holistic}, EleutherAI's LM Evaluation Harness \cite{eval-harness}, and DecodingTrust \cite{wang2024decodingtrust}, have made significant strides in this area, but exhibit certain limitations that impede comprehensive and scalable evaluations, particularly within high-performance computing (HPC) environments like those at Argonne National Laboratory. A critical shortcoming of current frameworks is their limited scalability and inefficiency in handling large-scale models. Many existing software platforms are not optimized for parallel processing across multiple GPUs or computing nodes, resulting in prolonged evaluation times and increased computational costs. This inefficiency becomes particularly problematic when assessing large LLMs that demand substantial computational resources. In addition, inconsistencies in evaluation methodologies and a lack of standardization further hinder comprehensive evaluations. The absence of consistent benchmarks and metrics across platforms and organizations complicates model comparisons, exacerbated by dataset biases, contamination, and the rapid evolution of LLMs outpacing evaluation strategies. %Consequently, current evaluations often do not fully capture the contextual understanding and reasoning abilities of an LLM, leading to assessments that are neither reproducible nor, in some cases, reliable.

To address the challenges of scalable and comprehensive LLM evaluation, we are developing the Skills, Trust, and Reliability (STaR) evaluation framework, tailored for HPC systems at Argonne National Laboratory. STaR builds upon the general architecture of evaluation platforms, which typically involve a sequence of specifications (files or configuration flags) to instantiate controllers and manage communication through states. Central to these platforms are Runners, which act as top-level components orchestrating workflows that handle Scenarios—benchmarks comprising static datasets like Hellaswag or GSM8K, or dynamic scripts such as those in Chain-of-Thought Hub \cite{fu2023chain}. A Data Pre-Processor translates these Scenarios into formatted prompts, which are passed to Adapters interfacing with LLMs through libraries such as Hugging Face \cite{huggingface_transformers}, vLLM \cite{kwon2023efficient}, or OpenAI APIs. Executors like Slurm \cite{yoo2003slurm} or Ray \cite{moritz2018ray} enable processing of prompts, and the results are aggregated into metrics, such as accuracy.

Expanding on this general framework, STaR introduces a modular architecture comprising a data layer, prompting layer, model adapter, and result layer to streamline the evaluation process. The data layer ingests datasets, such as MMLU-Pro \cite{mmlupro}, and constructs evaluation instances, while the prompting layer generates standardized prompts using techniques such as few-shot and chain-of-thought reasoning \cite{wei2022chain}. The model adapter queries models in multiple modes, including locally loaded instances for smaller models, Parsl \cite{babuji2019parsl} for job bundling, and OpenAI-compatible inference backends (e.g., vLLM \cite{kwon2023efficient} and DeepSpeed FastGen \cite{holmes2024deepspeed}) for larger models deployed on HPC systems like Polaris and Aurora. The result layer aggregates responses, computes general and UQ metrics, and organizes results into comprehensive scores, providing nuanced insight into model performance.

STaR supports widely used benchmark libraries, including EleutherAI-Harness \cite{eval-harness}, DecodingTrust \cite{wang2024decodingtrust}, Wildbench \cite{lin2024wildbench}, and domain-specific benchmarks. It also integrates uncertainty quantification approaches \cite{chen2024question} to enhance the reliability of evaluations. Designed for scalability, STaR incorporates data-parallel capabilities to distribute workloads across multiple GPUs and model-parallel solutions to handle large models exceeding single-GPU memory limits. It aims to simplify deployment with a unified one-step installation process and a consistent command-line interface, while results management supports standardized local tracking and optional database integration. STaR is a work in progress, with ongoing refinements aimed at seamless integration with Argonne’s infrastructure and addressing limitations identified in existing platforms. By prioritizing scalability, standardization, and efficiency, STaR aims to establish a robust evaluation framework that meets the evolving needs of LLM assessment in scientific research environments.

As a proof of concept, we performed evaluations on Polaris \cite{Polaris}, and benchmarked them with various open-source models with sizes from 7B to 70B. We used OpenLLM Leaderboard V2 (through Harness), a commonly used benchmark suite consisting of six challenging tasks, to evaluate the performance of Llama-2-7B and Llama-3-8B, Llama-3.1-8B, and their corresponding chat or instruct models, as well as Llama-3.3-70B-Instruct, the most recent and advanced model in the Llama series. \autoref{tab:eval-aurora} presents the evaluation results for the models on the benchmark, along with the GPU hours required for the evaluations. 

For safety evaluations, we use DecodingTrust, which, unlike frameworks such as Harness and HELM, is lightweight, highly compatible with HPC platforms, containerizable. % and uniquely focused on trustworthiness evaluations. 
Deploying DecodingTrust on the Polaris HPC system required key modifications, including integrating Parsl for efficient job bundling, adapting the framework to align with Polaris's queue configurations for optimized task distribution, and implementing a unified inference interface. These adaptations allowed the framework to harness Polaris' computational capabilities while retaining its flexibility and commitment to trustworthiness assessments.

On Polaris, DecodingTrust efficiently managed a variety of evaluation tasks, ranging from straightforward classification assessments to computationally intensive open-ended analyses. Classification tasks such as Adversarial Demonstration Robustness, Fairness, and Machine Ethics required minimal computational resources, with each task consuming approximately 0.5 A100 hours. In contrast, open-ended evaluations, including Toxicity and Stereotype Bias, were significantly more resource intensive, especially for models exceeding 70B parameters, with some tasks demanding up to 24 A100 hours per evaluation. By leveraging Polaris's HPC infrastructure, DecodingTrust successfully scaled its evaluation pipeline, balancing lightweight classification tasks with resource-heavy open-ended evaluations to provide a comprehensive assessment of model trustworthiness.




%\sandeep{can we say something on running these on Polaris and the scalablility across number of GPUs}


