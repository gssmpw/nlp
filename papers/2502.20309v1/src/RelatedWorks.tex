Research on LLM evaluation encompasses various techniques relevant for the evaluation of LLMs as research assistants. Here we discuss that work and note gaps that our proposed methodology attempts to fill.

\subsection{Multiple-choice Question (MCQ) Benchmarks}
MCQ benchmarks offer a structured framework for assessing LLM performance across various domains. Notable examples are Massive Multitask Language Understanding (MMLU) \cite{hendryckstest2021} and MMLU-PRO \cite{mmlupro}, which evaluate general knowledge and reasoning in more than 50 subjects, including humanities, sciences and engineering. In the realm of mathematical reasoning, GSM8K \cite{cobbe2021training}, GSM1K \cite{zhang2024careful}, and MATH \cite{hendrycksmath2021} are prominent. 
GSM8K and GSM1K addresses grade-school level problems, while MATH focuses on high-school and competition-level questions. Both benchmarks have been enhanced with multiple-choice adaptations to streamline evaluation and minimize ambiguity in model outputs \cite{zhang2024multiple}. Other significant MCQ benchmarks include ARC \cite{clark2018think}, which tests scientific reasoning, and HellaSwag \cite{zellers2019hellaswag}, which challenges models with complex commonsense reasoning scenarios that, while easy for humans, are especially hard for state-of-the-art models.
In the field of chemistry, MCQ benchmarks include MoleculeQA \cite{lu_moleculeqa_2024}, which comprises 61,574 MCQs, each with three distractors, focusing on factual information about molecules; MolPuzzle \cite{guo_can_2024}, a multimodal benchmark with over 23,000 question-answer pairs, structured with interlinked sequential sub-tasks, each providing multiple choices; and ChemBench \cite{mirza2024chembench}, with over 2700 questions, primarily MCQs.
Few of the many other MCQ benchmarking efforts in the literature are science-domain focused and validated, and as LLM capabilities improve, there is an increasing need to generate more difficult questions and leverage synergies between domain expertize and LLM judges. The multi-domain benchmark AI4S that we present below is an attempt towards that end.
%, though it includes more than 240 open-ended questions as well.

% Examples of popular benchmarks are GSM8k and GSM1K (Middle school -- Grade School math word problems, using the chain-of-thought prompt), MMLU and MMLU-PRO (Multidisciplinary multiple choice questions in humanities, social sciences, hard sciences, reasoning), HumanEval (Python code completion task, zero-shot, 64 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions), ARC (multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9), HellaSwag (Commonsense test that is especially hard for state of the art models, though its questions are trivial for humans),
% MATH (Middle school and high school mathematics problems written in LaTeX, prompted with a fixed four-shot prompt), and FrontierMath (hundreds of original, exceptionally challenging mathematics problems) \cite{frontiermath}.

% Beyond the subject matter of the MCQ benchmarks, the MCQ questions can be provided to the LLM using various preparation, submission, and scoring approaches which impact the ability of LLMs to complete the benchmark successfully. 
% In the \textbf{preparation} phase, the developer may choose to precede each MCQ submission to an LLM by from zero to five shots \cite{brownLanguageModelsAre}, where a \textit{shot} is an example MCQ containing a question, choices, and answer used to inform the model about the structure and context of the test that will be evaluated.  Research results show that exposing LLMs to such preparation increases LLM performance \cite{brownLanguageModelsAre} likely by helping LLMs better distinguish specialized meanings of words within domains appearching in the MCQ. 
% In the \textbf{submission} phase, we see two approaches in the literature: Joint and Separate. The Joint approach corresponds to that used when presenting MCQs to humans: the complete MCQ is presented including the question, the correct response, and the distractors. The LLM's task is to select one choice from the correct response plus the distractors. In the Separate approach, for every MCQ, the evaluation framework submits to the LLM the question and each possible answer (the correct answer and the distractors) separately using artifacts of the forward pass computation LLM to compute the Log probability of every answer independently of the others. The framework then picks the answer with the highest log probability as the model selection. 
% As the LLM is not exposed to all possible answers at once, the task is not a selection, and the LLM does not have the opportunity to select a response by elimination, which is a possibility offered by the nature of MCQ tests. 
% Finally, in the \textbf{scoring} phase, 
% the scoring for the full MCQ test is computed as the ratio of the correct selections over the total number of MCQs.



\subsection{Open Response Benchmarks}
%Open-response benchmarks are essential for evaluating LLMs by testing their ability to generate free-form text, offering deeper insights into their natural language understanding, reasoning, and generative capabilities.
While MCQ benchmarks restrict responses to predefined options, 
Open Response benchmarks require that LLMs produce detailed, unconstrained outputs that can be evaluated for coherence, accuracy, and relevance. Notable examples include NarrativeQA \cite{kovcisky2018narrativeqa}, which challenges models to generate summaries or interpret longer narratives, and HotpotQA \cite{yang2018hotpotqa}, which demands multi-hop reasoning with synthesized answers derived from multiple sources. Similarly, HybridQA \cite{chen2020hybridqa} combines textual and tabular data, requiring LLMs to provide coherent and comprehensive answers. For mathematical reasoning, GSM8K \cite{cobbe2021training} and MATH \cite{hendrycksmath2021} test model ability to solve complex, multi-step problems with free-form solutions. 
In chemistry, benchmarks have been developed to assess LLM open-response capabilities. ChemistryQA \cite{wei_chemistryqa_2020} comprises 4500 complex questions that require reasoning and calculations, evaluating model ability to generate detailed, accurate responses. ChemLLMBenchmark \cite{guo2023can} consists of eight practical chemistry tasks that necessitate understanding, reasoning, and explanatory skills, with evaluations focusing on the quality and depth of model-generated answers. The open-domain TOMG-Bench \cite{li_tomg-bench_2024} molecule generation benchmark comprises tasks such as molecule editing, optimization, and customized generation, each requiring models to produce specific molecular structures or modifications based on textual descriptions.
%, thereby assessing their generative capabilities in chemistry.
%For field of medicine, MedMCQA \cite{sun2024medmcqa} focuses on medical reasoning tasks, challenging LLMs to provide detailed rationales for clinical scenarios. 

Recent advancements have expanded the scope of open-response benchmarks to address specific evaluation challenges and domains. The Open-LLM-Leaderboard (OSQ-bench) \cite{open-llm-leaderboard} transitions from multiple-choice formats to open-style questions, eliminating issues like selection bias and random guessing, while emphasizing modelsâ€™ ability to generate coherent, contextually accurate answers. FrontierMath \cite{frontiermath} presents hundreds of exceptionally challenging mathematics problems that require models to generate detailed solutions, testing advanced reasoning and problem-solving skills. Similarly, Humanity's Last Exam \cite{humanityslastexam} crowdsources complex questions from experts across fields to evaluate how closely LLMs approximate expert-level capabilities, highlighting their potential and limitations in addressing real-world challenges.

A difficulty with Open Response benchmarks is that evaluating their responses is inherently challenging due to their unstructured nature, requiring time-intensive analysis, subjective interpretation, and careful management of biases and data overload. These challenges are closely tied to uncertainty quantification (UQ), as the variability in interpretations and outcomes necessitates robust techniques to quantify and mitigate uncertainty in the evaluation process, ensuring reliable and consistent insights. In addressing these challenges, it is also important to keep track of multiple model versions and to use them consistently \cite{EvoStore-HPDC24}, so as to enable reproducibility \cite{RECUP-REWORDS23}.

\subsection{Lab-Style Experiments}

Laboratory-style Experiments with LLMs involve controlled settings in which researchers systematically evaluate model performance on specific tasks, enabling precise measurement of capabilities and limitations. In chemistry, for instance, \cite{boiko2023autonomous} developed Coscientist, which employs LLMs to plan and execute experimental procedures based on simple human prompts, and evaluated its performance by assigning it the task of identifying synthetic procedures for seven molecules of varying complexity. Similarly, \cite{chen2024autotamp} proposed an approach that combines LLMs with task and motion planning to translate natural language instructions into robot-executable plans, evaluating their system through simulations in a box-packing domain. In behavioral strategy research, \cite{albert2024reproducing} reproduced human laboratory experiments using LLMs and compared their performance to human participants to analyze the extent to which LLMs can emulate human decision-making processes. These laboratory-style experiments provide valuable insights into applications and can inform the development of more advanced AI systems. However, comprehensive end-to-end evaluations of LLMs on scientific tasks, similar to human performance assessments, remain scarce. Such evaluations are crucial to understanding how LLMs can mimic or augment human researchers in tackling complex scientific challenges. 
%Implementing these assessments could bridge the gap between current AI capabilities and the ideal scientific assistant, enhancing the integration of LLMs into research workflows.
%Implementing these assessments could bridge the gap between current AI capabilities and the ideal scientific assistant, enhancing the integration of LLMs into research workflows.
\subsection{Field-Style Experiments}
Field-style Experiments, also referred to as ``in-the-wild" studies, involve observing and analyzing user interactions with LLM in real-world settings. This approach contrasts with controlled Lab-style Experiments by capturing non-predefined user interactions with LLMs, providing valuable insights into how LLMs perform across diverse, unstructured tasks. Recent studies have leveraged this methodology to assess various aspects of LLM performance. For instance, WildBench \cite{lin2024wildbench} is designed to evaluate LLMs using real-world user interactions, enabling a comprehensive analysis of model capabilities in practical scenarios. Similarly, HaluEval-Wild \cite{zhu2024haluevalwildevaluatinghallucinationslanguage} was designed to evaluate hallucinations in LLMs by collecting challenging user queries from real-world interactions. Shen et al. \cite{shen2024donowcharacterizingevaluating} conducted a study characterizing and evaluating user-LLM interactions, providing insights into user behavior and model performance and highlight the importance of understanding user needs and expectations to improve LLM utility and user satisfaction. While such analyses are currently lacking in scientific domains, developing field-style experiments specifically for scientific research presents a significant opportunity to provide critical insights into how researchers interact with AI models, thereby enhancing the creation of AI assistants tailored to scientific inquiry.

\subsection{Safety Evaluation}
% \sandeep{UQ, trustworthiness and safety benchmarks and how they are cross cutting across these 4 evaluation styles}
In addition to the general-purpose evaluations discussed above, comprehensive evaluations must rigorously assess alignment with ethical standards, robustness against jailbreaks, and adaptability to complex real-world scenarios \cite{zeng2024air,lin2024wildbench}. Safety evaluations are also cross-cutting :MCQs, Open-Response benchmarks, and Lab-style and Field-style Experiments. SafetyBench \cite{zhang2023safetybench}, for example, employs 11,435 MCQs in seven categories (e.g., bias, toxicity) to systematically test model adherence to ethical and safety standards in both English and Chinese. Similarly, SALAD-Bench \cite{li2024salad} proposed 4000 MCQs, and part of the larger dataset structured into a detailed hierarchy of 6 domains, 16 tasks, and 66 categories. BigBench \cite{srivastava2022beyond} has a subset of its tasks focused on safety evaluation in regards to toxicity, bias and truthfullness tat are MCQs. However, there has not been a focused MCQ-based safety evaluation benchmarks that take the nuances aspects of science, especially high consequential ones such as the the chemisty, biology, radiation, and nuclear (CBRN). We discuss one such effort on risks in chemistry in this work.
Open-response evaluations, such as those in DecodingTrust \cite{wang2024decodingtrust} and TrustLLM \cite{huang2024position}, examine nuanced safety aspects, including hallucinations, privacy violations, and machine ethics. TrustLLM evaluates LLMs across six dimensions, including fairness and safety, using over 30 datasets to identify critical safety gaps, while DecodingTrust introduces an eight-dimensional framework that probes issues like toxicity and ethical reasoning, with results published on a widely accessible leaderboard for transparency. However, such evaluations for scientific use cases are scarce~\cite{zeng2024air} and thus provide an opportunity to develop them in the future.

The safety red-teaming methodologies can be effectively interpreted along the lines of lab-style and field-style experiments. In lab-style red-teaming, researchers interact directly with LLMs and systematically introduce adversarial prompts to identify vulnerabilities such as biases, hallucinations, or ethical compliance issues. For example, \cite{buszydlik2023red} present a framework for red-teaming experiments on LLMs by generating numerical questions and puzzles to evaluate the models' performance on elementary calculations and algebraic tasks. This approach provides detailed feedback at each step, allowing iterative improvements and a deeper understanding of model limitations. In contrast, field-style red-teaming assesses LLMs by analyzing large-scale human interactions in real-world environments. This method captures diverse and unpredictable user inputs, offering insights into how models perform ``in the wild." For instance, \cite{hong2024curiositydriven} discuss automating red-teaming by training a separate red team LLM with reinforcement learning to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. This large-scale approach identifies practical weaknesses and vulnerabilities that may not surface in controlled lab settings, contributing to model robustness across varied real-world scenarios. By employing both lab-style and field-style red-teaming strategies, researchers can comprehensively evaluate and enhance the safety, reliability, and ethical performance of LLMs across different contexts. That said, significant work still needs to be done in designing such experiments for safety and trust scenarios in science.









% MCQs establish foundational safety checks, test adherence to established guidelines, and identify domain-specific risks \cite{Madireddy2024AQAG}. Open-ended benchmarks probe nuanced risks, such as hallucinations and biases, in unstructured tasks \cite{wang2024decodingtrust}, while lab-style experiments can validate safety across iterative research workflows, including hypothesis generation and experimentation \cite{fu2023chain}. Field-style experiments can diagnose vulnerabilities through large-scale, real-world interactions, leveraging diverse prompts to assess practical strengths and weaknesses. By incorporating automated red-teaming, refusal scoring \cite{achiam2023gpt}, and alignment strategies, this layered framework achieves  
% %\ian{As you have said that the approach integrates MCQs, Open, end-to-end, I wonder which of those three categories are automated red-teaming, refusal scoring, and alignment strategies}, this layered framework ensures \ian{Again ``ensures'' is strong. Also fosters, enables in next sentence.} 
% comprehensive safety assessments. Such an approach fosters trust and enables the responsible integration of LLMs into critical scientific research \cite{zeng2024air, achiam2023gpt}.

% Current evaluation techniques for LLMs can be broadly categorized into three types \cite{guo2023evaluating}: 
% % \ian{This definition seems a bit circular to me. Is the goal simply, as stated, to assess ability to answer questions from these datasets? Or are these datasets rather attempts to evaluate some broader capability? You seem to be conflating ``evaluation methodology'' and ``benchmark'', which are presumably two distinct classes of things?}\sandeep{The reference to GPQA in question answering let to the confusion; thanks for catching that. Reciew papers caractorize the knowledge and capability evaluaiton into the following categories for knowledge and capability evaluation}

% \textit{question answering}, which focuses on assessing a model's ability to retrieve and generate correct responses such as
% SQuAD \cite{rajpurkar2016squad}, or NarrativeQA \cite{kovcisky2018narrativeqa}; \textit{knowledge completion}, which evaluates a model's ability to predict missing elements in structured data, such as subject-relation-object triples in datasets like LAMA \cite{petroni2019language} or WikiFact \cite{goodrich2019assessing}; and \textit{reasoning}, which encompasses a wide range of tasks that assess a model's ability to emulate human reasoning. Reasoning tasks include \textit{commonsense reasoning}, which tests understanding of everyday scenarios through datasets like CommonsenseQA \cite{talmor1commonsenseqa} or PIQA \cite{bisk2020piqa}; \textit{logical reasoning}, which measures the ability to analyze and evaluate arguments in datasets like SNLI \cite{bowman2015large} or LogicNLI \cite{tian2021diagnosing}; \textit{multi-hop reasoning}, which involves connecting multiple pieces of information to answer questions, as seen in benchmarks like HotpotQA \cite{yang2018hotpotqa} and HybridQA \cite{chen2020hybridqa}; and \textit{mathematical reasoning}, which evaluates the capacity for abstraction and calculation using datasets like GSM8K \cite{cobbe2021training} or MATH \cite{hendrycksmath2021}. 
% \ian{You listed the main classes above, but now give further classes?}
% \textit{agentic interactions} where LLMs are evaluated for their tool manipulation capabilities, such as using external APIs to enhance task performance \cite{}, and their tool creation abilities, which test whether models can generate new tools %or solutions 
% for unfamiliar problems \cite{schick2023toolformer}. 
% \ian{Following new text doesn't seem quite right but I think addresses important distinctions.}

% \ian{You sort of introduce in the introduction three (or maybe five?) current evaluation methodologies, and then sort of introduce four classes of new methods that are the focus of this work. You also later in the introduction talk about safety evaluations, which are perhaps another class? (I can 't tell.) Presumably we should relate the ``Related Work" here to those categories? } \sandeep{Yes Ian, the related works is incomplete at the moment; it shoudl reflect the related works from all the methodologies}


%\sandeep{expand this to include different general and domain-specific MCQ, openeval, and end-to-end benchmarks in the literature}

%\input{src/Related_ChemMat}

