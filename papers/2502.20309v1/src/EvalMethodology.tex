% It is important to reiterate that this paper exposes the current progress in establishing a methodology to evaluate AI models (e.g. LLMs) as scientific assistants. Every aspect presented in the following sections is and will fundamentally remain a work in progress because of the continuous model capability improvement that at the time we write this paper has seemingly no perceived limit.
% It is also a work in progress because developing a methodology takes a lot of time and effort. However, one of the main contributions of this paper is the lessons learned that we believe will be beneficial to the community. Thus, for every facet of the methodology, we report on the method and state of development at the time we write this paper, the lessons learned so far, and the way forward.
% We will first discuss domain specific benchmarks that have been automatically generated and then at the end discuss our multidisciplinary benchmarks combining the capabilities on manual and automatic question generation and validation

%In order to establish sound methodology, we first need  o define the notion of AI scientific research assistant.

\begin{table*}
    \includegraphics[width=\textwidth]{figures/skills_assessed.png}
    \caption{Skills evaluated by each of the evaluation techniques. Lab-Style Experiments focus on detailed analysis in controlled environment. Field-Style Experiments focus on analyzing researchers-LLMs interactions at scale in natural setting.}
    \label{tab:skills}
\end{table*}

The overarching objective of using LLMs as research assistants is to accelerate the research process.

In \autoref{tab:skills}, we present seven tasks commonly performed by researchers to solve a scientific problem: (i) posing and formulating a research question; (ii) if needed, conducting initial, preliminary experiments, observations, simulation, and/or database accesses to confirm the pertinence of the research question; (iii) performing literature search to identify related work; (iv) generating potentially multiple hypotheses (or research directions) to address the research question; (v) designing experiments, observations, and/or simulations to test the hypotheses; (vi) analyzing the resulting data to validate or invalidate the hypotheses; and (vii) writing a report about findings.  
Each of these seven tasks requires deep reasoning, contextual understanding, and iterative problem solving.
The scientific community needs confidence that LLMs are proficient in these tasks if they are to adopt them widely as research assistants. Thus a thorough and comprehensive evaluation methodology is required that can produce a rigorous assessment of LLM strengths and weaknesses in each task. %Addressing both challenges requires a holistic, rigorous, accurate, transparent, and community-approved evaluation approach.

As discussed in the related work section, most LLM evaluations take the form of MCQ benchmarks, which are a subset of broader Q\&A evaluations.
%\ian{The text referred to Q\&A, but the related work section only talks about MCQs}. 
However, while these benchmarks serve a useful purpose in quickly assessing the breadth of knowledge of LLMs, that cannot, by their very nature, assess the depth of reasoning, contextual understanding, and iterative problem-solving required in the various steps of the scientific research process \cite{zhang2024massw, Pyzer-Knapp2022}. 

To address these limitations, it is critical to complement MCQ benchmarks with end-to-end evaluations in realistic contexts that assess both reasoning across multiple dimensions and the ability to plan and adapt across multi-step tasks. 
Therefore, we propose \textit{Evaluating AI models as scientific Research Assistants} (\textbf{EAIRA}), a structured evaluation methodology (\autoref{fig:multi_facet}) that combines four techniques:
two existing approaches that allow for quick and repeated assessment of the breadth of LLM abilities (MCQ Benchmarks and Open-Response Benchmarks) plus two new approaches for end-to-end evaluation of LLMs as scientific assistants (Lab-style Experiments and Field-style Experiments).
%that represent a highly realistic, interactive, and iterative evaluation of their skills best suited to measure scientific skills however, at the cost of substantially more human effort than the prior two techniques.

\textbf{MCQ benchmarks} serve to test foundational knowledge and domain-specific expertise across diverse scientific fields.  In addition to previously developed benchmarks, members of our team have developed three new MCQ benchmarks to address topic gaps in tasks for which MCQs are well suited.
1) Astro and 2) Climate are domain-specific benchmarks primarily generated through automated techniques to ensure the scalability of benchmark generation to serve new and evolving topics in science.
3) AI4S is a multi-domain ``AI for science'' MCQ benchmark, which integrates human question generation and validation with automated generation and validation methods to achieve a balanced, high-quality dataset that can be developed with moderate effort. By leveraging  both human and automated techniques, we can assess the strengths and weaknesses of automated methods deployed in Astro and Climate in order to guide future research in improving their generation.

\textbf{Open-response benchmarks} serve to test more detailed knowledge and to generate open-ended responses or code that assesses a model’s dynamic capability to handle unstructured, complex tasks, moving closer to reflecting the flexibility and adaptability required in real-world research scenarios while still facilitating a fast, automated evaluation.
For this, our team has previously developed two benchmarks: SciCode, which assesses the ability of LLMs to develop code which is highly dependent on the knowledge of the context of a scientific domain to correctly implement, and ALDbench, which assesses the ability of LLMs to describe methods for synthesizing materials using atomic layer deposition.

\textbf{Lab-style Experiments} perform evaluation by domain experts of AI models as they assist across all research tasks in real situations.  This expert-reviewed method provides a comprehensive assessment of the relevance, effectiveness, and iterative improvement of a model over time (e.g., 4-12 hours per attempt).  
This technique goes beyond open response by using a human proctor to guide an expert to iteratively interact with the model in order to assess multi-stage planning and response to results in the context of a scientific domain. Because of the interactivity, it provides very granular feedback about what models are or are not capable of performing giving a unique insight into the strengths and weaknesses of models and possible paths for improvement.
However, this capability comes at the cost of extensive effort by experts and proctors.

\textbf{Field-style Experiments} (inspired by, and adapting ``in-the-wild" evaluations \cite{Lin2024WildBenchBL,shen2024donowcharacterizingevaluating} for the scientific context) analyze automatically thousands of prompts, responses, and user behaviors during real-world interactions between researchers and AI assistants. This approach allows for large-scale capture and analysis of user needs, model strengths and weaknesses, and usage trends.
It differs from lab-style experiments in that:
1) experts are not guided by a proctor, 2) analysis of interactions and feedback is automated, and  3)  experiments can be conducted in the background as experts perform routine tasks (e.g., by capturing outputs of a site-wide LLM inference service or API proxy). These differences greatly improve the scalability of realistic end-to-end experiments but at the loss of the fine-grained granularity of the strengths and weaknesses detected by the lab-style approach.

In applying these four techniques, three key cross-cutting aspects must be considered: ethics, trust and safety; reliable uncertainty quantification; and scalable software infrastructure.
These three aspects ensure that LLMs are ``aligned'' with human values, produce and qualify results correctly under uncertainty; and can be evaluated efficiently at scale.
\textit{Trust and safety} evaluations must address ethical alignment, defend against jailbreak attempts, and adapt to complex real-world contexts. Multiple-choice strategies~\cite{zhang2023safetybench} can identify biases, toxicity, and compliance gaps, while open-response tasks~\cite{wang2024decodingtrust} can probe subtle issues such as hallucinations and machine ethics. Lab-style red-teaming~\cite{buszydlik2023red} systematically challenges the model with adversarial prompts in controlled settings to expose stepwise weaknesses, while field-style red-teaming \cite{hong2024curiositydriven}  tests the model’s robustness amid unpredictable real-world inputs, unveiling vulnerabilities that may remain hidden in laboratory conditions. \textit{Reliable uncertainty quantification} (UQ) is equally critical to establish trust in AI-driven scientific research assistants, as it systematically gauges model confidence and highlights potential inaccuracies \cite{lin2023generating, du2024haloscope, duan2024shifting}. UQ insights guide targeted refinements in MCQ, open-response, and lab- or field-style evaluations, ensuring that areas of high uncertainty are addressed in scientific and real-world contexts \cite{ye2024benchmarking, chen2024question}. Together, robust safety evaluations and UQ foster transparency, accountability, and trust, facilitating the responsible integration of LLMs into critical scientific research \cite{zeng2024air, achiam2023gpt}.
Finally, \textit{ the scalable software infrastructure} enables rapid evaluation to keep pace with rapid changes in the field of AI research.
The framework needs to incorporate distributed task parallelism \cite{babuji2019parsl} and fast inference capabilities \cite{kwonEfficientMemoryManagement2023}.

The following subsections present greater detail and results of the four techniques and three aspects of our evaluation methodology.

% By integrating these approaches within controlled and large-scale assessments, researchers can validate ethical compliance, robustness, and other safety criterion important for high-stakes scientific tasks. 

%\franck{This agent paragraphs are not relevant here. We don't propose anything for their evaluation in this paper. These paragraph could be moved to the dicussion/next steps section}

%In the literature, AI agents \cite{xi2023rise,mcnaughton2024cactus} are traditionally defined as systems that autonomously perceive their environment, reason about the information gathered, and take actions to achieve specified goals, excelling in tasks such as process automation, decision-making, and tool usage. 
%\ian{AI agents are not the same as LLMs. Do you make clear later when you are evaluating agents vs. LLMs?}
%These systems are typically evaluated on the basis of their efficiency, robustness, and adaptability in well-defined problem spaces. While this framework has proven effective for traditional AI agents, AI-powered scientific research assistants, though similar in spirit, require a different evaluation strategy to address their unique role in supporting complex, domain-specific research processes. Unlike traditional agents, scientific assistants must engage in iterative, multi-step problem-solving, synthesize implicit knowledge, and innovate by creating or manipulating tools to address novel challenges. For instance, they might propose workflows for discovering new materials, troubleshooting intricate experiments, or generating hypotheses in physics. \textit{This dual focus on reasoning and innovation necessitates tailored evaluation frameworks that go beyond operational efficiency to capture deeper dimensions of adaptability and domain-specific performance}.

%As the capabilities of LLM%-powered scientific agents expand, safety evaluations become critical to ensure their responsible and effective deployment, particularly in high-stakes domains such as chemical synthesis \cite{mcnaughton2024cactus}, biological experiments, and nuclear engineering. Frameworks like ChemCrow have begun incorporating safety tools to filter hazardous queries, while CLAIRify enforces stringent constraints on experimental workflows \cite{yoshikawa2023large}. However, these mechanisms only address a subset of safety concerns. 

% Comprehensive evaluations must rigorously assess alignment with ethical standards, robustness against jailbreaks, and adaptability to complex real-world scenarios \cite{lin2024wildbench}. Models must align with safety benchmarks tailored to regulatory frameworks such as AIR-BENCH 2024, which consolidates over 300 granular risk categories derived from global regulations \cite{zeng2024air}.
% Safety evaluations also require a hierarchical approach that integrates MCQs, open-ended benchmarks, and end-to-end experiments similar to the benign evaluations \ian{What is a benign evaluation? Where were they discussed?}
% discussed earlier. MCQs establish foundational safety checks, test adherence to established guidelines, and identify domain-specific risks \cite{Madireddy2024AQAG}. 
% \ian{Let's get terminology precise and consistent. Open-ended is I assume the same as Open Response? End-to-end is an umbrella term for Lab-style and Field-style? (BTW why ``style"?). }
% Open-ended benchmarks probe nuanced risks, such as hallucinations and biases, in unstructured tasks \cite{wang2024decodingtrust}, while Lab-style experiments could \ian{Here ``could'', in the next sentence ``can'': not sure of the intended nuance?} validate safety across iterative research workflows, including hypothesis generation and experimentation \cite{fu2023chain}. Field-style experiments can diagnose vulnerabilities through large-scale, real-world interactions, leveraging diverse prompts to assess practical strengths and weaknesses. By incorporating automated red-teaming, refusal scoring \cite{achiam2023gpt}, and alignment strategies \ian{As you have said that the approach integrates MCQs, Open, end-to-end, I wonder which of those three categories are automated red-teaming, refusal scoring, and alignment strategies}, this layered framework ensures \ian{Again ``ensures'' is strong. Also fosters, enables in next sentence.} comprehensive safety assessments. Such an approach fosters trust and enables the responsible integration of LLMs into critical scientific research \cite{zeng2024air, achiam2023gpt}.

% Reliable uncertainty quantification (UQ) is a cornerstone for establishing trust in AI scientific research assistants, complementing the safety mechanisms discussed previously. UQ provides a systematic way to assess confidence in model outputs \cite{lin2023generating}, enabling the distinction between reliable predictions and those prone to errors or hallucinations \cite{du2024haloscope, duan2024shifting}. By quantifying uncertainty, researchers can better understand the model's limitations, particularly in scenarios requiring high precision and complexity \cite{ye2024benchmarking}. This is especially crucial in scientific domains \cite{chen2024question} where incorrect outputs can have significant consequences. UQ also supports iterative refinement by identifying areas where models are uncertain, guiding targeted improvements that align with the rigorous standards of scientific inquiry. When combined with robust safety evaluations, UQ ensures that LLMs not only perform reliably but also provide transparency and accountability, making them more trustworthy for deployment in critical research applications.



