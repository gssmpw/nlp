Deep generative models (DGMs) are a class of deep learning models that aim to learn a $\mparam$-parameterized distribution $p_{\mparam}(\bm{x})$ from the training set $\data=\{\bm{x}_i\}_{i=1}^N$ as an approximation for the true data distribution $p(\bm{x})$. After learning, these models can generate new data samples from $p_{\mparam}(\bm{x})$. DGMs have gained significant attention due to their ability to generate diverse and high-quality data, such as images~\citep{lugmayr2022repaint,saharia2022palette,kawar2023imagic}, text~\citep{gu2022vector,gong2022diffuseq,yang2023diffsound}, audio~\citep{chung2015recurrent,kong2020diffwave,huang2022fastdiff} and molecules~\citep{li2014generalized,collins2015energy,xu2021geodiff}.

Roughly speaking, DGMs can be categorized into two main types. The first type directly parameterizes the data distribution $p_{\bm{\theta}}(\bm{x})$. Examples include energy-based models~\citep{lecun2006tutorial} and score-based models~\citep{song2020score}. The second type introduces latent variables $\bm{z}$ to aid in the learning process and acquire latent representations. These models represent the data distribution as $p_{\bm{\theta}}(\bm{x})=\int p_{\bm{\theta}}(\bm{x}|\bm{z})p(\bm{z})d\bm{x}$. Examples include variational autoencoders~\citep{kingma2014auto}. 


\subsection{Energy-based Models}
\label{sec:dgm_ebm}
Energy-based models (EBMs)~\citep{lecun2006tutorial} are probabilistic models with minimum restrictions on the functional form. They define an \emph{energy function} $E_{\mparam}(\bm{x})$ in the form of unnormalized log-probabilities. Practically, the energy function can be directly parameterized by any deep neural networks, which makes it flexible towards a variety of model architectures. The density given by EBMs is defined as:
\begin{equation}
    p_{\mparam}(\bm{x}):=\frac{1}{Z_{\mparam}}\exp{\left(-E_{\mparam}(\bm{x})\right)},
\end{equation}
where $Z_{\mparam}=\int_{\bm{x}}\exp{\left(-E_{\mparam}(\bm{x})\right)}d\bm{x}$ is the normalizing constant. This normalizing constant is a function of the model parameter $\mparam$, and thus cannot be ignored when learning $\mparam$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/EBMs.pdf}
    \caption{The training procedure of energy-based models. Generated data is obtained by sampling from the neural network-parameterized distribution $\bm{X}_{\text{fake}}\sim\exp(-E_{\mparam}(\bm{x}))$. The model parameter $\mparam$ is updated by minimizing the contrastive divergence between the real and generated data.}
    \label{fig:ebms}
\end{figure}

The ultimate goal of EBMs is to approximate the real data distribution $p(\bm{x})$ with the $\mparam$-parameterized density $p_{\mparam}(\bm{x})$. One standard way to achieve such an approximation is by the maximum likelihood estimation (MLE), in which the expected log-likelihood is maximized:
\begin{equation}
    \mparam^* := \max_{\mparam}E_{\bm{x}\sim p(\bm{x})} [\log p_{\mparam}(\bm{x})].
\end{equation}
The MLE approximation is equivalent to minimizing the KL-divergence $KL[p(\bm{x}) \| p_{\mparam}(\bm{x})]$. 
% \yl{(I personally think the readers of this book should know MLE is equivalent to minimizing KL?)}:
% \begin{equation}
% \begin{aligned}
%     E_{\bm{x}\sim p(\bm{x})}\log p_{\mparam}(\bm{x})&=\int_{\bm{x}\in\data}p(\bm{x})\log p_{\mparam}(\bm{x})d\bm{x} \\
%     &=\int_{\bm{x}\in\data}p(\bm{x})\left[-\log\frac{p(\bm{x})}{p_{\mparam}(\bm{x})}+\log p(\bm{x})\right]d\bm{x} \\
%     &=-KL(p(\bm{x})\|p_{\mparam}(\bm{x}))+E_{\bm{x}\sim p(\bm{x})}\log p(\bm{x}) \\
%     &\propto -KL(p(\bm{x})\|p_{\mparam}(\bm{x})),
% \end{aligned}
% \end{equation}
% where the second expectation is ignored since it is independent of $\mparam$. 
Unfortunately, gradient-based optimization cannot be directly applied to the MLE objective, since the the gradients of the logarithm of $p_{\mparam}(\bm{x})$ contains an intractable term $\nabla_{\mparam}\log Z_{\mparam}$: 
\begin{equation}
    \nabla_{\mparam}\log p_{\mparam}(\bm{x})=-\nabla_{\mparam}E_{\mparam}(\bm{x})-\nabla_{\mparam}\log Z_{\mparam}.
\label{eq:ebm_mle_gradient}
\end{equation}
A popular solution is based on Monte Carlo estimation with a random sample drawn from the EBM distribution, resulting in an approximate gradient to Eq.~\eqref{eq:ebm_mle_gradient}:
\begin{equation}
    \nabla_{\mparam}\log Z_{\mparam}=-E_{\bm{x}\sim p_{\mparam}(\bm{x})}[\nabla_{\mparam}E_{\mparam}(\bm{x})] \approx-\nabla_{\mparam}E_{\mparam}(\widetilde{\bm{x}}), \quad \widetilde{\bm{x}}\sim p_{\mparam}(\bm{x}),
\label{eq:ebm_mle_approx_gradient}
\end{equation}
\begin{equation}
    \Rightarrow \quad \nabla_{\mparam}\log p_{\mparam}(\bm{x}) \approx \nabla_{\mparam}E_{\mparam}(\widetilde{\bm{x}}) - \nabla_{\mparam}E_{\mparam}(\bm{x}), \quad \widetilde{\bm{x}}\sim p_{\mparam}(\bm{x}).
    \label{eq:grad_ebm}
\end{equation}
The training procedure of EBMs is illustrated in Fig.~\ref{fig:ebms}. As we shall see, sampling from the EBM is typically done via MCMC, often with finite number of transition steps. When using the empirical data distribution as the initial distribution, the corresponding approach is named \emph{contrastive divergence}~\citep{hinton2002training}, which is motivated as approximately minimizing the gap between two KL-divergences. 

\subsubsection{Langevin Sampling}
During both training and generation, we need to obtain samples $\widetilde{\bm{x}}$ from the EBM distribution $p_{\mparam}(\bm{x})$. However, $p_{\mparam}(\bm{x})$  lacks a closed form due to the intractable normalizing constant. Therefore, MCMC methods, particularly gradient-based MCMC, are commonly applied to acquire samples from $p_{\mparam}(\bm{x})$. For example, Langevin dynamics~\citep{parisi1981correlation,grenander1994representations,welling2011bayesian} utilizes the gradient of $\log p_{\mparam}(\bm{x})$:
\begin{equation}
    \nabla_{\bm{x}}\log p_{\mparam}(\bm{x})=-\nabla_{\bm{x}}E_{\mparam}(\bm{x})-\nabla_{\bm{x}}\log Z_{\mparam}=-\nabla_{\bm{x}}E_{\mparam}(\bm{x})
    \label{eq:gradient_log_p}
\end{equation}
The second equation holds because the normalizing constant $Z_{\mparam}$ is independent of $x$. Consequently,  the update rule for $x$ using Langevin dynamics is:
\begin{equation}\label{eq:langevin}
    \bm{x}_{k+1} \gets \bm{x}_k-\alpha_k\nabla_{\bm{x}_k}E_{\mparam}(\bm{x}_k)+\sqrt{2\alpha_k}\cdot\bm{\epsilon}_k,
\end{equation}
where $\bm{\epsilon}_k\sim\mathcal{N}(\bm{0},\bm{I})$ is a standard Gaussian noise and $\alpha_k$ is the stepsize. With an annealed step size schedule, $\bm{x}_k$ is guaranteed to converge to the EBM distribution $p_{\mparam}(\bm{x})$ as $k\rightarrow\infty$. 
During the training process, we can use the empirical data distribution as the initial distribution \citep{hinton2002training}, and run a short Markov chain to generate samples $\widetilde{\bm{x}}$ for computing the gradient in Eq.~\eqref{eq:grad_ebm}. The initialization of the MCMC can also be either random noise~\citep{nijkamp2019learning} or persistent states obtained from previous iterations~\citep{tieleman2008training}. A summary of the full training procedure for EBMs with MCMC sampling assistance is provided in Algorithm \ref{alg:ebm_training}. 

% \yl{(Maybe mention contrastive divergence here again in terms of using truncated MCMC?)}

\begin{algorithm}[t]
\caption{Training Energy-based methods}\label{alg:ebm_training}
\begin{algorithmic}[1]
\Require Dataset $\data = \{ \x_n \}_{n=1}^N$, sampling step sizes $\{\alpha_k \}$, total optimization steps $T$, total sampling steps $K$.
\For{$t = 1, ..., T$}

\vspace{1.5mm}
\State $\triangleright$ Generate a sample from the current EBM via Langevin Sampling
\State Initialize $\bm{x}_0$ (see discussions in the main text)
\For{$t = 1, ..., K$}
\State $\bm{x}_{k+1} \gets \bm{x}_k-\alpha_k\nabla_{\bm{x}_k}E_{\mparam}(\bm{x}_k)+\sqrt{2\alpha_k}\cdot\bm{\epsilon}_k, ~~~\bm{\epsilon}_k\sim\mathcal{N}(\bm{0},\bm{I})$
\EndFor
\State Set the negative sample to be $\bm{x}^{-}=\bm{x}_K$
\vspace{1.5mm}
\State $\triangleright$ Maximize the log-likelihood of the EBM
\State Sample a datapoint from the dataset: $\bm{x}^{+}\sim \data$
\State Obtain the gradient of the log-likelihood: $\nabla_{\bm{x}}\log p_{\mparam}(\bm{x}) 
    = \nabla_{\mparam}  E_{\mparam} (\bm{x}^{-}) - \nabla_{\mparam}E_{\mparam} (\bm{x}^{+})$
\State Update $\mparam$ by MLE using gradient-based optimizers
\EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Score Matching}
In Eq.~\eqref{eq:langevin}, generating new data only needs the gradient of the energy $\nabla_{\bm{x}_k}E_{\mparam}(\bm{x}_k)$ rather than the energy itself $E_{\mparam}(\bm{x}_k)$. Therefore, another way to learn EBMs is by directly parameterizing the gradient of $\log p_{\mparam}(\bm{x})$, referred to as the \emph{score function}: $s_{\mparam}(\bm{x})=\nabla_{\bm{x}}\log p_{\mparam}(\bm{x})\approx\nabla_{\bm{x}}\log p(\bm{x})$. Then the sampling rule during generation becomes:
\begin{equation}
    \bm{x}_{k+1} \gets \bm{x}_k+\alpha_ks_{\mparam}(\bm{x}_k)+\sqrt{2\alpha_k}\cdot\bm{\epsilon}_k.
\label{eq:langevin_score_matching}
\end{equation}
Generative models that learn the data distribution through the score function are called \emph{score-based models}~\citep{song2019generative}. One benefit of learning the score function is that it is free from the normalizing constant. From Eq.~\eqref{eq:gradient_log_p}, we have:
\begin{equation}
    s_{\mparam}(\bm{x})=\nabla_{\bm{x}}\log p_{\mparam}(\bm{x})=-\nabla_{\bm{x}}E_{\mparam}(\bm{x}).
\end{equation}
The training of score-based models involves \emph{score matching}~\citep{hyvarinen2005estimation}, in which the Fisher divergence between the learned score and true score is minimized:
\begin{equation}
    D_F(p(x) \| p_{\theta}(x)) = \frac{1}{2}E_{\bm{x}\sim p(\bm{x})}\left[\|s_{\mparam}(\bm{x})-\nabla_{\bm{x}}\log p(\bm{x})\|^2\right]\propto\mathbbm{E}_{\bm{x}\sim p(\bm{x})}\left[tr(\nabla_{\bm{x}}s_{\mparam}(\bm{x}))+\frac{1}{2}\|s_{\mparam}(\bm{x})\|^2\right].
\end{equation}
However, computing the trace of the Jacobian $tr(\nabla_{\bm{x}}s_{\mparam}(\bm{x}))$ is prohibitively costly for deep neural networks. To address this issue, \emph{denoising score matching} is proposed~\citep{vincent2011connection}, which introduces a small Gaussian perturbation to the original data $q(\tilde{\bm{x}}|\bm{x}) = N(\tilde{\bm{x}}|\bm{x}, \sigma^2I)$, and returns an accurate estimate of the score function when $\sigma \rightarrow 0$. Consequently,
\begin{align}
D_F(q(\tilde{\bm{x}}|\bm{x}) \| p_{\theta}(\tilde{\bm{x}})) 
&= \frac{1}{2} E_{p(\bm{x})}E_{q(\tilde{\bm{x}} | \bm{x})}[\| s_{\theta}(\tilde{\bm{x}}) - \triangledown_{\tilde{\bm{x}}} \log q(\tilde{\bm{x}} | \bm{x})\|^2]\nonumber \\
&= \frac{1}{2} E_{p(\bm{x})}E_{q(\tilde{\bm{x}} | \bm{x})}\left[\left\| s_{\theta}(\tilde{\bm{x}}) + \frac{\tilde{\bm{x}} - \bm{x}}{\sigma^2}\right\|^2\right]\nonumber\\
&=\frac{1}{2} E_{p(\bm{x})}E_{\epsilon\sim\mathcal{N}(0,I)}\left[\left\| s_{\theta}(\bm{x}+\sigma\epsilon) + \frac{\epsilon}{\sigma}\right\|^2\right]\label{eq:fisher-divergence}
\end{align}
In practice with $\sigma > 0$, the learned score approximates the score of the perturbed data distribution, not the original data distribution. Even if an accurate data score function estimate is obtained with $\sigma \approx 0$, sampling from the data distribution with Langevin dynamics (Eq.~\eqref{eq:langevin_score_matching}) remains challenging in e.g., fast mixing. To resolve these issues, \citet{song2019generative} proposes learning the score function across different noise scales via \emph{Noise Conditional Score Networks}, which parameterize the score function as $s_{\theta}(\bm{x},\sigma)$ with the noise scale $\sigma$ as an additional input. During generation, 
\emph{Annealed Langevin dynamics}~\citep{song2019generative} is used which simulates Eq.~\eqref{eq:langevin_score_matching} using $s_{\theta}(\bm{x},\sigma)$ with different $\sigma$, starting from the most noise-perturbed distribution (with $\sigma >> 0$) and then progressively reducing the noise scales until $\sigma \approx 0$. Consequently, the samples become close to the original data distribution.


\subsection{Diffusion Models}
\label{sec:dgm_ddpm}
Diffusion models are probabilistic generative methods that generate new data by reversing the process of injecting noise into data~\citep{sohl2015deep,ho2020denoising}.
The intuition of diffusion models is illustrated in Fig.~\ref{fig:diffusion}.
Diffusion models are closely related to score-based models, essentially sharing the same training objective outlined in Eq.~\eqref{eq:fisher-divergence}, differing only by a constant. Subsequently, a continuous-time generalization of diffusion models through Stochastic Differential Equations (SDE) has been developed~\citep{song2020score}. The SDE perspective sheds light on the connection between diffusion models and MCMC methods, which we will discuss in detail below.   
% There are three main formulations of diffusion models, namely Denoising Diffusion Probabilistic Models (DDPM), Score-Based Generative Models (SGM) and Stochastic Differential Equations (SDE). This section primarily focuses on the SDE formulation and its connections to MCMC.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/diffusion.pdf}
    \caption{In diffusion models, the forward process progressively introduces noise, whereas the reverse process aims to denoise the perturbed data. The denoising step typically involves the estimation of the score function. Adapted from
\citet{yang2022diffusion}. Used with kind permission of Ling Yang.}
    \label{fig:diffusion}
\end{figure}

\subsubsection{Stochastic Differential Equations}
The diffusion process that injects noise to data is governed by the following stochastic differential equation:
\begin{equation}
    d\bm{x}=\bm{f}(\bm{x},t)dt+g(t)d\bm{w},
\end{equation}
where $\bm{f}(\bm{x},t)$ and $g(t)$ are the diffusion and drift functions respectively, and $\bm{w}$ is a standard Wiener process (Brownian motion). \citet{anderson1982reverse} shows that any diffusion process can be reversed by solving the following reverse-time SDE:
\begin{equation}
    d\bm{x}=[\bm{f}(\bm{x},t)-g(t)^2\nabla_{\bm{x}}\log q_t(\bm{x})]dt+g(t)d\bar{\bm{w}},
    \label{eq:reverse_time_sde}
\end{equation}
where $\bar{\bm{w}}$ is a standard Wiener process when time flows backwards, and $dt$ denotes an infinitesimal negative time step. 

The reverse-time SDE can be simulated using various general-purpose numerical SDE solvers. To address the approximation errors arising from these numerical methods, gradient-based MCMC methods, specifically Langevin dynamics, have been employed.
The reason is that we
have the score function $s_{\theta}(\bm{x},t) \approx \nabla\log p_t(\bm{x})$ which can be used with gradient-based MCMC to sample from $p_t(\bm{x})$. This forms a hybrid sampler, denoted as \emph{Predictor-Corrector} (PC) sampler~\citep{song2020score}. Here, at each time step $t$, the numerical SDE solver first provides an estimate of the sample at the next time step, $\bm{x}_{t+1}$, serving as a ``predictor". Subsequently, the gradient-based MCMC method adjusts the marginal distribution of the estimated sample by running Monte Carlo sampling, using $\bm{x}_{t+1}$ as the initial value, serving as a ``corrector". 


Recent developments of diffusion model's SDE design have been inspired by the continuous-time MCMC literature. Notably, continuous-time diffusion models have been found to align with overdamped Langevin dynamics with high friction coefficients~\citep{dockhorn2021score}. To enhance the convergence towards equilibrium, a critically-damped Langevin diffusion has been introduced, leveraging concepts from Hamiltonian dynamics~\citep{dockhorn2021score}.



% \citet{song2020score} futher proves that the following ordinary differential equation has the same trajectory marginals as Eq.~\eqref{eq:reverse_time_sde}:
% \begin{equation}
%     d\bm{x}=\left[\bm{f}(\bm{x},t)-\frac{1}{2}g(t)^2\nabla_{\bm{x}}\log q_t(\bm{x})\right]dt.
% \end{equation}
