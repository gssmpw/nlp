\begin{figure}[t]
    \centering
\includegraphics[width=0.3\textwidth]{figures/dlvm.pdf}
    \vspace{-1.5em}
    \caption{Graphical model of a deep latent variable model.}
    \label{fig: dlvm}
\end{figure}

Apart from generating realistic data and accurate density estimation where EBMs/SBMs and diffusion models are well suited, another important application of deep generative models is representation learning, possibly with dimensionality reduction. In such scenario we are interested in obtaining a (possibly lower dimensional) representation $\latent$ for a given datapoint $\bm{x}$ sampled from the underlying data distribution. A starting example is Probabilistic PCA \citep{tipping1999probabilistic} which assumes the following model:
\begin{equation}
\latent \sim p_{\mparam}(\z) := \mathcal{N}(\bm{0}, \mathbf{I}), \quad \bm{x} \sim p_{\mparam}(\bm{x} | \latent) := \mathcal{N}(\weight \latent + \mathbf{b}, \sigma^2 \mathbf{I}).
\label{eq:probabilistic_pca}
\end{equation}
%
The model parameters $\mparam = \{ \weight, \mathbf{b}, \sigma \}$ is then fitted via maximum likelihood estimation (MLE) given a dataset $\data=\{\bm{x}_n\}_{n=1}^N$:
\begin{equation}
    \mparam^* = \arg\max_{\mparam} \sum_{n=1}^N \log p_{\mparam}(\bm{x}_n), \quad p_{\mparam}(\bm{x}_n) = \int p_{\mparam}(\bm{x} | \latent) p_{\mparam}(\latent) d\latent.
    \label{eq:dlvm_mle}
\end{equation}
Since both the prior and the likelihood are Gaussians, the log marginal likelihood $\log p_{\mparam}(\bm{x})$ is tractable, and there exist analytic solutions for the model parameters $\mparam$.

(Probabilistic) PCA returns meaningful representations when the underlying data generation process follows a linear transform of some latent features. Therefore it is less suited for typical deep learning applications with inputs as e.g., images, where useful features must be obtained in a non-linear way.
%
Instead, Deep Latent Variable Models (DLVMs) \citep{welling2014auto,rezende:vae2014} are typically used to extract non-linear features from such high-dimensional and complex datapoints, since they are generative models that assume a data-generating process as transforming latent variables through a neural network. A typical DLVM assumes the observation $\bm{x}$ is generated from the conditional distribution $p_{\mparam}(\bm{x}|\latent)$ parameterized by a DNN with parameters $\mparam$. Compared with Probabilistic PCA, the conditional distribution in Eq.~\eqref{eq:probabilistic_pca} is now replaced by $p_{\mparam}(\bm{x} | \latent) = \mathcal{N}(\bm{x}; f_{\mparam}(\latent), \sigma^2 \mathbf{I})$ with a neural network $f_{\mparam}(\latent)$. It can then be viewed as a non-linear extension of Probabilistic PCA, since this formulation recovers Probabilistic PCA if $f_{\mparam}(\latent)$ is defined as a linear function.
%

%
The model parameters $\mparam$ in a DLVM is also estimated via maximum likelihood estimation, however, here the MLE objective (Eq.~\eqref{eq:dlvm_mle}) becomes intractable, due to the complex integral w.r.t.~the latent variables $\latent_n$ when a non-linear function $f_{\mparam}(\z)$ is in use. In practice, we again resort to approximation, where popular approaches are often VI-based \citep{welling2014auto,rezende:vae2014}. Different from VI in BNNs, amortized inference \citep{gershman2014amortized} plays a key role in VI for DLVMs to further reduce computational costs. As we shall see, instead of constructing individual approximations to the posterior $p_{\mparam}(\latent_n|\bm{x}_n)$ for each $\bm{x}_n$, another 
DNN is employed to directly map $\bm{x}_i$ to the variational parameters of its corresponding variational distribution.

\subsubsection{Variational Autoencoders}
Variational Autoencoders (VAEs) \citep{welling2014auto,rezende:vae2014} are a class of DLVMs paired with \emph{inference networks} (also called encoder networks) for amortized VI. Typically the Gaussian variational distribution family is used: $q_{\vparam}(\latent|\bm{x})=\mathcal{N}(\bm{\mu}_{\vparam}(\bm{x}), \bm{\sigma}^2_{\vparam}(\bm{x}))$, where the posterior mean and variance for an observation $\bm{x}$ are obtained by an inference network $g_{\vparam}(\bm{x}) := [\bm{\mu}_{\vparam}(\bm{x}), \log \bm{\sigma}_{\vparam}(\bm{x})]$ with variational parameters $\vparam$.
%
The model parameters $\mparam$ and the variational parameters $\vparam$ can be jointly trained by maximizing the ELBO: 
\begin{equation}
\begin{aligned}
\mparam^*, \vparam^* &= \arg\max_{\mparam, \vparam} \sum_{n=1}^N \mathcal{L}_{ELBO}(\bm{x}_n, \mparam, q_{\vparam}), \\
\log p_{\mparam}(\bm{x}) \geq \mathcal{L}_{ELBO}(\bm{x}, \mparam, q_{\vparam}) &= E_{q_{\vparam}(\latent|\bm{x})}[\log p_{\mparam}(\bm{x}|\latent)] - KL[q_{\vparam}(\latent|\bm{x})\|p_{\mparam}(\latent)].
\end{aligned}
\label{eq:vae_objective}
\end{equation}
%
Again the intractable expectation can be approximated with Monte Carlo estimation (often with only 1 sample), and the reparameterization trick \citep{welling2014auto,rezende:vae2014} is also applied to enable back-propagation:
%
\begin{equation}
    \mathcal{L}_{ELBO}(\bm{x}, \mparam, q_{\vparam}) \approx \log p_{\mparam}(\bm{x}|\bm{\mu}_{\vparam}(\bm{x})+ \bm{\sigma}_{\vparam}(\bm{x}) \odot \bm{\epsilon}) - KL[q_{\vparam}\|p_{\mparam}], \quad \bm{\epsilon} \sim \mathcal{N}(\bm{0}, \mathbf{I}).
\label{eq:vae_objective_mc_reparam}
\end{equation}
%
This ELBO optimization can be viewed as training a stochastic autoencoder (a stochastic encoder $q_{\vparam}(\latent | \bm{x})$ plus a stochastic decoder $p_{\mparam}(\bm{x} | \latent)$), hence the name ``autoencoder''. The full algorithm for training VAEs is provided as Algorithm \ref{alg:vae} with stochastic gradient descent.

\begin{algorithm}[t]
\caption{SGD Training for a variational auto-encoder with Gaussian $q$ distribution}\label{alg:vae}
\begin{algorithmic}[1]
\Require Dataset $\data = \{ \x_n \}_{n=1}^N$, initialized parameters $\mparam, \vparam$,  batch-size $B$, SGD step sizes $\{\alpha_t \}$, SGD total steps $T$.
\For{$t = 1, ..., T$}
\State Sample a mini-batch of datapoints $\bm{\Xi} = \{ \x \} \sim \data^B$ with $| \bm{\Xi} | = B$
\For{$\x \in \bm{\Xi}$}
    \State Encode the input data: $g_{\vparam}(\bm{x}) \gets [\bm{\mu}_{\vparam}(\bm{x}), \log \bm{\sigma}_{\vparam}(\bm{x})]$
    \State Compute the KL term in the ELBO:
    $\mathcal{L}_{ELBO}(\x, \mparam, q_{\vparam}) \gets - KL[q_{\vparam}(\z | \x)\|p(\z)]$
    \State Sample the latent variable: $\z \gets \bm{\mu}_{\vparam}(\bm{x}) + \bm{\sigma}_{\vparam}(\bm{x}) \odot \bm{\epsilon}$, $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \mathbf{I})$
    \State Decode the latent variable to $f_{\mparam}(\z)$
    \State Add-in data likelihood: $\mathcal{L}_{ELBO}(\x, \mparam, q_{\vparam}) \gets \mathcal{L}_{ELBO}(\x, \mparam, q_{\vparam}) + \log p(\x| \z)$
\EndFor
\State Compute SGD updates:
$(\mparam, \vparam) \gets (\mparam, \vparam) + \alpha_t \nabla_{(\mparam, \vparam)} \frac{1}{|\bm{\Xi}|} \sum_{\x \in \bm{\Xi}} \mathcal{L}_{ELBO}(\x, \mparam, q_{\vparam})$
\EndFor
\end{algorithmic}
\end{algorithm}

The class of VAE models can be extended by considering VI with alternative divergences. For example, we can again control the coverage behavior of $q_{\vparam}(\latent|\bm{x})$ (from mode-seeking to mass covering) by using $\alpha$-divergences with different $\alpha$ hyper-parameters \citep{miguel2015alpha,li2016renyi}. The standard VAE can be viewed as a special case within this framework where the exclusive KL-divergence ($\mathrm{KL}[q\|p]$) is chosen within the $\alpha$-divergence family. Another special case is the use of the inclusive KL-divergence ($\mathrm{KL}[p\|q]$), the resulting model becomes an Importance-Weighed Autoencoder (IWAE) \citep{burda2016iwae}, which was originally proposed to tighten the lower bound of $\log p_{\mparam}(\bm{x})$ for optimization:
%
\begin{equation}
\hspace{-0.7em}
\log p_{\mparam}(\bm{x}) \geq \mathcal{L}_{IWAE}(\bm{x}, \mparam, q_{\vparam})
:= E_{\prod_{m=1}^M q_{\vparam}(\latent^{(m)}|\bm{x})}\left[\log \frac{1}{M} \sum_{m=1}^M \left[\frac{ p_{\mparam}(\bm{x}|\latent^{(m)})p_{\mparam}(\latent^{(m)})}{q_{\vparam}(\latent^{(m)}|\bm{x})} \right] \right]. 
\end{equation}
%
Again Monte Carlo estimation and the reparameterization trick apply to IWAE. The lower bound can be made tighter by using more MC samples, and as $M \rightarrow \infty$, $\mathcal{L}_{IWAE}$ converges to $\log p_{\mparam}(\bm{x})$. While benefiting learning of model parameters $\mparam$, in such case the fitting of $q_{\vparam}(\latent | \bm{x})$ may be sub-optimal, as any reasonable importance sampling proposal can return the exact marginal likelihood with infinite amount of samples \citep{rainforth2018tighter}. 



\subsubsection{Combining VI and MCMC in DLVM}
The ELBO (Eq.~\eqref{eq:vae_objective}) is a biased approximation to the MLE objective (Eq.~\eqref{eq:dlvm_mle}) unless the variational posterior $q_{\vparam}(\latent_i | \bm{x}_i)$ matches the exact posterior $p_{\mparam}(\latent_i | \bm{x}_i)$. This motivates the use of MCMC to generate more accurate approximate posterior samples whose distribution asymptotically converges to the exact posterior \citep{doucet2023differentiable,hoffman2017learning}. Starting from an initial distribution $q_0(\latent | \bm{x})$, one runs $T$ steps of MCMC transitions to obtain an improved approximate posterior $q_T(\latent | \bm{x})$, and optimizes the model parameters $\mparam$ by maximizing $\mathcal{L}_{ELBO}(\bm{x}, \mparam, q_T)$ with the MCMC sample distribution $q_T(\latent | \bm{x})$ as the approximate posterior.
%
In DLVM context, gradient-based samplers such as Hamiltonian Monte-Carlo (HMC) \citep{neal2010mcmc,duane1987hybrid} are preferred, since they adapt to the decoder more efficiently than e.g., Metropolis-Hastings \citep{metropolis1953equation} with random walk proposals \citep{metropolis1953equation,sherlock2010random}. Still MCMC is computationally more expensive than VI and in practice may take prohibitively many transitions to converge, especially when the posterior is high-dimensional.

To speed-up MCMC, a popular approach is to draw its initial samples from a distribution $q_0$ that is already close to the exact posterior, with the hope of reducing the number of MCMC transition steps to obtain high-quality posterior samples \citep{hoffman2017learning,geffner2021mcmc}. A natural choice for such initial distributions is the variational posterior: $q_0^{(\vparam)}(\latent|\bm{x})=q_{\vparam}(\latent|\bm{x})$, where one can fit the variational parameters $\vparam$ by maximizing $\mathcal{L}_{ELBO}(\bm{x}, \mparam, q_{\vparam})$. 
%
Another approach involves distilling the initial distribution using the feedback from the final samples of a MCMC: one can improve $q_0^{(\vparam)}(\latent|\bm{x})$ by minimizing  $D(q_0^{(\vparam)}(\latent|\bm{x}), q_T(\latent|\bm{x}))$ for a chosen distance/divergence measure between distributions. Note that $q_T(\latent|\bm{x})$ implicitly depends on $\vparam$ but in practical implementations we apply a ``stop gradient'' operation to it and treat it as a constant w.r.t.~$\vparam$. Also the density of $q_T(\latent|\bm{x})$ is unknown as it is implicitly defined based on MCMC samples. These suggest the use of the inclusive KL-divergence $KL[q_T(\latent|\bm{x})\|q_0^{(\vparam)}(\latent|\bm{x})] = E_{q_T}[-\log q_0^{(\vparam)}(\latent|\bm{x})] + C$ for such divergence measure, as minimizing it w.r.t.~to $\vparam$ doesn't require the density of $q_T(\latent|\bm{x})$ \citep{li2017mcmc}. 
\citet{pmlr-v97-ruiz19a} further proposed a new divergence tailored to this task.


In addition to improving the initial distributions, variational inference can also be applied to tuning hyper-parameters of MCMC samplers \citep{salimans2015markov,caterini2018hamiltonian,gong2019meta,campbell2021gradient}, such as the step size and momentum variance in HMC, which are known to be critical for fast convergence of the samplers \citep{neal2010mcmc}. A line of research, as illustrated in Figure \ref{fig: mcmc_as_vi}, treats the MCMC hyper-parameters $\psi$ as additional variational parameters, meaning the improved approximate posterior $q_T^{(\psi)}(\latent_T|\bm{x})$ can be further optimized w.r.t.~$\psi$ based on certain divergences between the exact posterior and the implicit distribution.
%
Since $q_T^{(\psi)}(\latent_T|\bm{x})$ is now implicit, its density is intractable, therefore the ELBO $\mathcal{L}_{ELBO}(\bm{x}, \mparam, q_T^{(\psi)})$ cannot be computed. In particular, the ELBO $\mathcal{L}_{ELBO}(\bm{x}, \mparam, q_T^{(\psi)})$ in Eq.~\eqref{eq:vae_objective} can also be reformulated as follows:
\begin{equation}
    \mathcal{L}_{ELBO}(\bm{x}, \mparam, q_T^{(\psi)}) = E_{q_T^{(\psi)}(\latent_T|\bm{x})}[\log p_{\mparam}(\bm{x}, \latent_T)] + E_{q_T^{(\psi)}(\latent|\bm{x})}[-\log q_T^{(\psi)}(\latent_T|\bm{x})].
\label{eq:vae_elbo_entropy}
\end{equation}

Maximizing the differential entropy $H[q_T^{(\psi)}]= E_{q_T^{(\psi)}(\latent|\bm{x})}[- \log q_T^{(\psi)}(\latent_T|\bm{x})]$ prevents $q_T^{(\psi)}$ from collapsing to a point mass located at a mode of the true posterior $p_{\theta}(\latent|\bm{x})$. To circumvent the intractability of $q_T^{(\psi)}(\latent|\bm{x})$ density, \citet{salimans2015markov} proposed a variational inference approach in the expanded space of all intermediate states along the Markov chain (i.e. $\latent_{0:T}$), and derived a lower bound of $\mathcal{L}_{ELBO}$ as the optimization objective without computing $H[q_T^{(\psi)}]$. 
%Specifically, the objective is obtained by subtracting from the $L_{ELBO}$ the expectation of KL between the joint distributions $q_T^{(\psi)}(\latent_{0:T-1}|\latent_T, \bm{x})$ and an auxiliary model approximating the reverse dynamics of the MCMC transitions, $r(\latent_{0:T-1}|\latent_T, \bm{x})$, with respect to $q^{(\psi)}(\latent_T|\bm{x})$.
Noticing that gradient-based optimization is in use, \citet{li2018gradient} and \citet{gong2019meta} proposed to directly approximate the gradient of $H[q_T^{(\psi)}]$ with respect to $\psi$ in the optimization procedure.
\citet{campbell2021gradient} proposed to simply drop $H[q_T^{(\psi)}]$ from the ELBO (Eq.~\eqref{eq:vae_elbo_entropy}) and prevent $q_T^{(\psi)}$ from collapsing by using a mass-covering initial distribution $q_0^{(\vparam)}$. 
%
Alternatively, one can also bypass the intractability of the density of $q_T^{(\psi)}$ by using divergences which only require samples from $q_T^{(\psi)}$ as objective, for example Kernelized Stein Discrepancy \citep{Liu2016kernel,chwialkowski2016kernel}. 

\begin{figure}[t]
    \centering
\includegraphics[width=0.7\textwidth]{figures/mcmc_as_vi.pdf}
\vspace{-2em}
    \caption{Implicit variational distribution $q_T^{(\psi)}(\latent_T|\bm{x})$ defined via a T-step MCMC. The hyper-parameters of MCMC $\psi$ are treated as learnable variational parameters. Adapted from Slide 4 of \citet{campbell2021gradientSlide}. Used with kind permission of Wenlong Chen.}
    \label{fig: mcmc_as_vi}
\end{figure}