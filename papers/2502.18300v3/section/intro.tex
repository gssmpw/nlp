Bayesian computation has achieved profound success in many modeling tasks with statistics tools such as generalized linear models \citep{nelder1972generalized,dobson2018introduction}. Yet these traditional tools fail to produce satisfactory predictions for high-dimensional and highly complex data such as images, speech and videos. Deep Learning \citep{lecun2015deep} provides an attractive solution. At the time of late 2023, deep neural networks achieve accurate predictions for image classification \citep{dehghani2023scaling}, segmentation \citep{kirillov2023segment} and speech recognition tasks \citep{zhang2023google}. Meanwhile they have also demonstrated an astonishing capability for generating photo-realistic and/or artistic images \citep{rombach2022high}, music \citep{agostinelli2023musiclm} and videos \citep{liang2022nuwa}. Nowadays deep neural networks have become a standard modeling tool for many of the applications in AI and related fields, and the success of deep learning so far are based on training deterministic deep neural networks on big data. So one might ask: is there a place for Bayesian computation in modern deep learning? 

The answer is affirmative. Regarding prediction tasks that deep neural networks are primarily used for, Bayesian model averaging is capable for providing further improvements regarding accuracy \citep{wilson2020bayesian}. More importantly, quantifying uncertainty in predictions is crucial for increasing adoptions of neural networks in safety-critical domains such as healthcare \citep{gal2016thesis}. Decision-making tasks based on neural network predictions also require reliable uncertainty estimates \citep{savage1954bayes,jaynes2003probability}. All of these motivate the developments of Bayesian neural networks \citep{mackay:practical1992,neal1996bayesian,blundell2015bbp}, which applies Bayesian modeling to addressing the challenge of uncertainty quantification in deep learning.  
%
On the other hand, many popular neural network-based generative models for image/speech/video generation are based on deep latent variable models \citep{welling2014auto,rezende:vae2014}, whose training requires inferring the unobserved latent variables. In this case posterior inference is a required step in generative model training, sharing many advantages and challenges as Bayesian computation in statistical modeling.

There are three main challenges for Bayesian computation in deep learning. First, the random variables to be inferred can be ultra high-dimensional, e.g., Bayesian neural networks require posterior inference over network weights which can be millions (if not billions) of variables. Second, the likelihood function is highly non-linear as a function of the random variable. In deep latent variable models the likelihood function is defined using a neural network, so that multiple distinct values of a latent variable can be mapped to e.g., the same mean of a Gaussian likelihood, which results in a highly multi-modal posterior. Third, the training data sizes for deep learning tasks are massive, e.g., in millions \citep{deng2009imagenet} to billions \citep{schuhmann2022laion}, so evaluating the likelihood functions on the entire dataset becomes challenging.
%
Traditional MCMC tools such as Metropolis-Hastings \citep{metropolis1953equation} and Gibbs sampling \citep{gelfand2000gibbs} fail to address these challenges in both fast and memory-efficient ways. These three challenges motivate the developments of \emph{approximate (Bayesian) inference} tools such as variational inference \citep{jordan1999vi,beal:vi2003,li2018approx,zhang2018advances}, stochastic-gradient MCMC \citep{welling2011bayesian,ma2015complete,zhangcyclical} and their advanced extensions.

This chapter provides an introduction to approximate inference techniques as Bayesian computation methods applied to deep learning models. We organize the chapter by presenting popular computational methods for (1) Bayesian neural networks and (2) deep generative models, explaining their unique challenges in posterior inference as well as the solutions. More specifically, the sections within this chapter are organised as follows.
\begin{itemize}
    \item Section \ref{sec:bnn} introduces Bayesian neural networks (BNNs), with a particular focus on the Bayesian computation tools applied to them. These computational tools are categorised into (1) Markov Chain Monte Carlo (MCMC, Section \ref{sec:bnn_sgmcmc}) and (2) Variational Inference (VI, Section \ref{sec:bnn_vi}). As we shall see, a common technique shared across the two themes is the use of stochastic gradient-based optimization \citep{bottou:ol1998}, which enables scaling of BNN computations to millions of datapoints.

    \item Section \ref{sec:dgm} discusses a variety of deep generative models which are used for density estimation and/or generating new samples from the data distribution. Specifically, MCMC serves as the key computation tool for sampling and training energy-based, score-based and diffusion models (Sections \ref{sec:dgm_ebm} \& \ref{sec:dgm_ddpm}), while VI is often employed in training deep latent variable models (Section \ref{sec:dgm_lvm}).
\end{itemize}