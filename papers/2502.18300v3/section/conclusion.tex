We have discussed scalable MCMC and approximate inference techniques for Bayesian computation in deep learning, with applications to quantifying uncertainty in DNNs and training deep generative models. 
Advances in fast posterior inference have contributed significantly to the rapid development of Bayesian neural networks as well as the profound success of deep generative models. Bayesian computation has been, and will continue to be, instrumental for embedding probabilistic principles into deep learning models.

%
The emphases on the need of posterior inference exhibit differences in the discussed two categories of modeling applications. For BNNs, posterior inference is the main computation tool for predictions and uncertainty estimates, although approximate inference for BNNs may require fitting parametric approximate posteriors, thereby introducing the concept of ``BNN training'' similar to training deterministic DNNs \citep{blundell2015bbp,gal2016thesis,wilson2020bayesian}. For deep generative models, posterior inference serves as part of the training routine, and it may not be needed for data generation tasks in test time. Interestingly, when considering DLVMs for unsupervised or semi-supervised learning, posterior inference becomes handy again in obtaining useful representations of the observed data, which will then be used for further downstream tasks in test time \citep{kingma2014semi,higgins2016beta,tschannen2018recent}. An interesting recipe in this case is to use fast inference (e.g., mean-field VI) in DLVM training, and high-fidelity approaches (e.g., MCMC with rejection steps) in test time to obtain more accurate and robust inference results \citep{kuzina2022alleviating}.

%
We conclude this chapter by discussing a few important research challenges to be addressed for more accurate and scalable Bayesian computation in deep learning context. 

\vspace{-1em}
\begin{itemize}
    \item For many popular DNNs the network weights are non-identifiable due to weight symmetries \citep{chen1993geometry,phuong2020functional}. This leads to many symmetric modes in the exact posterior of a BNN and increased difficulties of MCMC sampling \citep{papamarkou2022challenges,izmailov2021bayesian}. But ultimately one cares more about the \emph{posterior predictive distribution} (Eq.~\ref{eq:bayespred}) of the neural network outputs, where averaging over symmetric modes in the exact posterior provides no additional advantage in this regard. So future research efforts should focus on accurate computation of the predictive posterior, which may provide exciting opportunities for fast and memory-efficient weight-space posterior approximations \citep{sun2019functional,ma2019variational,ritter2021sparse}. Optimization aspects are also under-explored for both SG-MCMC inference and parametric approximate posterior fitting of BNN posteriors. Apart from incorporating popular adaptive stochastic gradient methods \citep{li2016preconditioned,chen2016bridging}, the study of neural network loss landscape \citep{li2018visualizing,maddox2019simple} may also inspire novel algorithms and analyses of posterior inference methods. 

    \item For training deep generative models with posterior inference as a sub-routine, the bias in the training procedure (as compared with e.g., MLE), resulted from errors in inference, is much less well understood. For example, contrastive divergence \citep{hinton2002training} as an approximation to MLE for energy-based models has been shown to perform adversarial optimization \citep{yair2021contrastive}, but it is still unclear theoretically about the impact of MCMC convergence to the quality of learned energy-based model. For variational auto-encoders, although research has discussed the impact of amortization gap \citep{cremer2018inference,marino2018iterative} in variational inference and the aforementioned research in combining MCMC and VI, the combined impact of amortization and restricted variational family on latent variable model training is still an open question.

    %\item 
\end{itemize}


\subsubsection*{A note on author contributions}
Wenlong Chen and Yingzhen Li wrote the sections regarding variational inference training for both Bayesian neural networks and deep latent variable models. Bolian Li and Ruqi Zhang wrote the sections on MCMC methods for Bayesian neural networks, energy-based models and diffusion models. Ruqi Zhang and Yingzhen Li edited the chapter in its final form for consistent presentations.