%
Deep neural networks (DNNs), thanks to their expressiveness in modeling complex functions at scale \citep{lecun2015dl}, have gained significant attention in a wide range of applications, e.g., recognistion tasks in vision and speech domains \citep{dehghani2023scaling,kirillov2023segment,zhang2023google}.
%
In such supervised learning settings, a training dataset of $N$ input-output pairs $\data=\{(\bm{x}_n,\bm{y}_n)\}_{n=1}^N$ with $\bm{x} \in \mathbb{R}^{d_{in}}$ is provided.
%
We consider fitting an $L$-layer, width $d_h$, feed-forward DNN $f_{\mparam}(\bm{x}): \mathbb{R}^{d_{in}} \rightarrow \mathbb{R}^{d_{out}}$ with weight and bias parameters $\mparam=\{\weight^l, \mathbf{b}^l\}_{l=1}^L$, whose functional form is
\begin{equation}
f_{\mparam}(\bm{x})=\weight^L g(\weight^{L-1}g(\cdot\cdot\cdot g(\weight^1 \bm{x}+\mathbf{b}^1))+\mathbf{b}^{L-1})+\mathbf{b}^L,
\label{eq:feed_forward_dnn}
\end{equation}
%
\[\weight^1 \in \mathbf{R}^{d_h \times d_{in}}, \mathbf{b}^1 \in \mathbf{R}^{d_{in}}, \weight^l \in \mathbf{R}^{d_h \times d_h}, \mathbf{b}^l \in \mathbf{R}^{d_h}, l = 2, ..., L-1, \weight^L \in \mathbf{R}^{d_{out} \times d_h}, \mathbf{b}^L \in \mathbf{R}^{d_{out}}. \]
%
Here a non-linear activation $g(\cdot)$ (e.g., ReLU \citep{hinton2010relu}) is used to make $f_{\mparam}(\bm{x})$ a non-linear function w.r.t.~$\bm{x}$.
%
For regression problems, we use the neural network to produce the prediction $\hat{\bm{y}} = f_{\mparam}(\bm{x})$. In such case $d_{out}$ equals to the dimensionality of $\bm{y}$, and the likelihood of the model is typically defined as a Gaussian distribution with a hyper-parameter $\sigma$:
\begin{equation}
    p(\bm{y} | \bm{x}, \mparam) = \mathcal{N}(\bm{y}; f_{\mparam}(\bm{x}), \sigma^2 \mathbf{I}).
\end{equation}
%
On the other hand, for classification problems, categorical likelihood is used, and the neural network returns the logit of the probability vector of the categorical distribution: assuming $\bm{y} \in \{1, ..., C \}$, then $d_{out} = C$ and
\begin{equation}
    [p(\bm{y} = 1| \bm{x}, \mparam), ..., p(\bm{y} = C| \bm{x}, \mparam)] = \text{softmax}(f_{\mparam}(\bm{x})), \quad \text{softmax}(\bm{l}) := \frac{\exp(\bm{l})}{ \sum_{c=1}^C \exp(\bm{l}_c)}.
\end{equation}
%
The power of DNNs lies in their capability in learning expressive feature representations for the input data. To see this, we rewrite the neural network feed-forward pass up to the $L-1^{\text{th}}$ layer in Eq.~\eqref{eq:feed_forward_dnn} as
$
\Phi_{\mparam}(\bm{x})=\weight^{L-1} g(\weight^{L-2}g(\cdot\cdot\cdot g(\weight^1 \bm{x}+\mathbf{b}^1))+\mathbf{b}^{L-2})+\mathbf{b}^{L-1}.
$
Then the neural network $f_{\mparam}(\bm{x}) = \weight^L \Phi_{\mparam}(\bm{x}) + \mathbf{b}^L$ and the associated likelihood model can be viewed as a (generalized) linear regression model with non-linear features $\Phi_{\mparam}(\bm{x})$. Different from generalized linear regression models, however, these non-linear features are also learned jointly with the last-layer parameters $\{ \weight^L, \mathbf{b}^L \}$. While this joint learning procedure returns expressive features and improved predictive accuracy, the weight and bias parameters $\{ \weight^l, \mathbf{b}^l \}_{l=1}^L$ are often less interpretable in any statistical sense, although sparsity methods may provide explanations akin to feature selection \citep{lemhadri2021lassonet,ghosh2019model}. See later paragraphs for a further discussion in Bayesian computation context.

A typical estimation procedure for the parameters $\mparam$ is maximum likelihood estimation (MLE), which finds the optimal neural network parameters as
\[\mparam^* = \arg\max_{\mparam} \ \sum_{n=1}^N \log p(\bm{y}_n | \bm{x}_n, \mparam). \]
However, when the dataset size is substantially smaller than the network parameter size, MLE estimates may overfit. Maximum a Posteriori (MAP) solutions can help alleviate the overfitting issue, however, both MLE and MAP provide point estimates only for the network parameters $\mparam$, leaving the underlying uncertainty uncaptured.

%
Bayesian inference is a principled framework to obtain reliable uncertainty in DNNs \citep{gal2016thesis}. Instead of using a point estimate of $\mparam$, Bayesian inference computes a posterior distribution over $\mparam$ to explain the observations with uncertainty estimates (See Figure \ref{fig: bnn}): 
%
\begin{equation}
    p(\mparam|\data)=\frac{p(\mparam)p(\data|\mparam)}{p(\data)}=\frac{p(\mparam)p(\data|\mparam)}{\int p(\mparam)p(\data|\mparam)d\mparam}.
\label{eq:bayespost}
\end{equation}
%
Here $p(\mparam)$ is the prior distribution representing our belief about the model parameters without observing any data points, and $p(\data|\mparam)$ is the likelihood. Often we assume i.i.d.~likelihood, which means
$p(\data|\mparam) := \prod_{n=1}^N p(\bm{y}_n | \bm{x}_n, \mparam)$. 
%
In the rest of the chapter we also write $p(\bm{y} | f_{\mparam}(\bm{x})) := p(\bm{y} | \bm{x}, \mparam)$ since the distributional parameters of the likelihood mainly depends on the output of the neural network.
%
The resulting neural networks are called Bayesian neural networks (BNNs) \citep{neal1996bayesian,mackay:practical1992,hinton:mdl1993}. For predictions, the uncertainty in $\mparam$ translates to the predictive uncertainty for $(\bm{x}^\ast, \bm{y}^\ast)$, via the following \emph{posterior predictive distribution}:
%
\begin{equation}
    p(\bm{y}^\ast|\bm{x}^\ast,\data) = \int p(\bm{y}^\ast|\bm{x}^\ast,\mparam) p(\mparam|\data) d\mparam.
\label{eq:bayespred}
\end{equation}

\begin{figure}[t]
    \centering
\includegraphics[width=0.47\textwidth]{figures/bnn.pdf}
    \caption{Difference between standard deep neural network and Bayesian neural network.}
    \label{fig: bnn}
    \vspace{-1em}
\end{figure}

With the posterior predictive distribution (Eq.~\eqref{eq:bayespred}) one can compute uncertainty measures from it. In regression problems a typical uncertainty measure is the variance of the prediction $V[\bm{y}^* | \bm{x}^*] = E_{p(\bm{y}^\ast|\bm{x}^\ast,\data)}[(\bm{y}^* - \bm{\mu}^*)(\bm{y}^* - \bm{\mu}^*)^{\top}]$ with $\bm{\mu}^* = E_{p(\bm{y}^\ast|\bm{x}^\ast,\data)}[\bm{y}^*]$ as the predictive posterior mean. In classification where the likelihood $p(\bm{y} | \bm{x}, \mparam)$ is typically categorical, one can also use the entropy $H[p(\bm{y}^\ast|\bm{x}^\ast,\data)] = -\sum_{c=1}^C p(\bm{y}^\ast = c |\bm{x}^\ast,\data) \log p(\bm{y}^\ast = c |\bm{x}^\ast,\data)$ to express uncertainty. These two quantities represent \emph{total} uncertainty, which can be further decomposed into \emph{epistemic} and \emph{aleatoric} uncertainty measures, for example \citep{houlsby2011bayesian,gal2016thesis}:
\begin{equation}
    \underset{(\text{total uncertainty})}{H[p(\bm{y}^\ast|\bm{x}^\ast,\data)]} = \underset{(\text{aleatoric uncertainty})}{E_{p(\mparam | \data)}[H[p(\bm{y}^\ast|\bm{x}^\ast,\mparam)]]} + \underset{(\text{epistemic uncertainty})}{I[\bm{y}^*; \mparam | \bm{x}^*, \data]}.
\label{eq:total_uncertainty}
\end{equation}
The conditional entropy $E_{p(\mparam | \data)}[H[p(\bm{y}^\ast|\bm{x}^\ast,\mparam)]] = E_{p(\mparam | \data)} E_{p(\bm{y}^\ast|\bm{x}^\ast,\mparam)}[-\log p(\bm{y}^\ast|\bm{x}^\ast,\mparam)]$ reveals the average ``variability'' (under posterior $p(\mparam | \data)$) of each neural network's predictive distribution $p(\bm{y}^* | \bm{x}^*, \mparam)$, and it aims to quantify the intrinsic randomness in data due to e.g.,~noisy measurements.
%
On the other hand, there are two explanations for epistemic uncertainty, originated from two equivalent definitions of mutual information. 
%
The first definition reads
\[I[\bm{y}^*; \mparam | \bm{x}^*, \data] = \mathbb{E}_{p(\bm{y}^* | \bm{x}^*, \data)} [\mathrm{KL}[ p(\mparam | \bm{y}^*, \bm{x}^*, \data) || p(\mparam | \data)]], \]
%
which quantifies the expected information gain for $\mparam$ (i.e., reduction of uncertainty in $p(\mparam | \data)$) when adding the prediction $\bm{y}^*$ on $\bm{x}^*$ to the observations. This concept of ``epistemic uncertainty reduction'' is particularly useful in data acquisition tasks following Bayesian principles \citep{houlsby2011bayesian}. The second definition of mutual information comes from re-arranging terms in the first definition:
\[I[\bm{y}^*; \mparam | \bm{x}^*, \data] = \mathbb{E}_{p(\mparam | \data)} [\mathrm{KL}[ p(\bm{y}^* | \bm{x}^*, \mparam) || p(\bm{y}^* | \bm{x}^*, \data)]]. \]
%
It measures the average deviation (under the posterior $p(\mparam | \data)$) of a single neural network's prediction $p(\bm{y}^* | \bm{x}^*, \mparam)$ from the posterior predictive $p(\bm{y}^* | \bm{x}^*, \data)$. Therefore a high value of mutual information means the neural networks parameterized by different $\mparam$ from high density regions of $p(\mparam | \data)$ have large disagreement in their prediction results, and the BNN is said to be epistemically uncertain about its predictions.

%
Unfortunately, Eq.~\eqref{eq:bayespost} is intractable since the marginal likelihood requires a complex integral $p(\data)=\int p(\mparam)p(\data|\mparam)d\mparam$, which has no analytic solution due to non-linearities in DNNs. Furthermore, numerical integration is infeasible due to extremely high-dimensional $\mparam$, usually over million or even billions in modern DNNs \citep{krizhevsky2012imagenet,dehghani2023scaling}. Therefore, we have to resort to approximate inference techniques \citep{li2018approx}, which uses an approximate posterior distribution $q(\mparam) \approx p(\mparam | \data)$ to replace the exact posterior in Bayesian inference tasks. In such case posterior predictive distribution (Eq.~\eqref{eq:bayespred}) is approximated by (typically also with Monte Carlo estimate):
\begin{equation}
    p(\bm{y}^* | \bm{x}^*, \data) \approx \int p(\bm{y}^\ast|\bm{x}^\ast,\mparam) q(\mparam) d\mparam \approx \frac{1}{M} \sum_{m=1}^M p(\bm{y}^\ast|\bm{x}^\ast,\mparam_m), \quad \mparam_m \sim q(\mparam).
\label{eq:approximate_predictive_distribution}
\end{equation}
This approximate predictive posterior can then be used subsequently for computing the uncertainty measures such as predictive variance or entropy in Eq.~\eqref{eq:total_uncertainty}.
%
We will discuss two classes of approximations: (1) stochastic gradient MCMC \citep{welling2011bayesian,chen2014stochastic}, which constructs $q(\mparam) = \frac{1}{M} \sum_{m=1}^M \delta(\mparam = \mparam_m)$ using samples $\{ \mparam_{m} \}_{m=1}^N$ obtained via an MCMC algorithm, and (2) variational inference \citep{beal:vi2003,jordan1999vi} where $q(\mparam)$ often has a parametric form and is obtained via an optimization procedure. 

Before proceeding to the technical details, an interesting note is that for BNNs, the quality of posterior approximation $q(\mparam) \approx p(\theta | \data)$ is mostly evaluated via the corresponding approximate posterior predictive  (Eq.~\eqref{eq:approximate_predictive_distribution}) rather than by direct inspection. First, symmetry exists in neural network parameters $\mparam$ (i.e., there exists $\mparam_1 \neq \mparam_2$ such that $f_{\mparam_1} = f_{\mparam_2}$, e.g., by swaping two hidden units of a hidden layer in the neural network of $f_{\mparam_1}$ and denoting the corresponding neural network's weight by $\mparam_2$ \citep{phuong2020functional,rolnick2020reverse}), meaning that the exact posterior $p(\mparam | \data)$ is highly multi-modal. Second and most distinctively from traditional statistical models, due to the emphasis of prediction accuracy for deep learning models, the network weights $\mparam$ are not designed to have an associated physical meaning, and in most cases even the exact posterior $p(\mparam | \data)$ is not interpretable by humans. Instead, the posterior predictive distribution (Eq.~\eqref{eq:approximate_predictive_distribution}) directly expresses the uncertainty in prediction, which can then be analyzed and used in further decision-making tasks based on the predictions.

%%%%%%%%

\begin{algorithm}[t]
\caption{SG-MCMC methods for sampling from BNN posteriors}\label{alg:sgmcmc_bnn}
\begin{algorithmic}[1]
\Require Dataset $\data = \{ (\x_n, \y_n) \}_{n=1}^N$, initialized parameter $\mparam_1$,  batch-size $B$, SG-MCMC step sizes $\{\alpha_t \}$, total steps $T$.
\For{$t = 1, ..., T$}
\State Sample a mini-batch of datapoints $\bm{\Xi} = \{ (\x, \y) \} \sim \data^B$ with $| \bm{\Xi} | = B$
\State Get the stochastic gradient $\nabla_{\mparam_t}\widetilde{U}(\mparam_t)$ via automatic differentiation
\State Obtain $\mparam_{t+1}$ through an SG-MCMC update, using the stochastic gradient $\nabla_{\mparam_t}\widetilde{U}(\mparam_t)$ and the step size $\alpha_t$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Stochastic Gradient MCMC}
\label{sec:bnn_sgmcmc}
Classical MCMC methods require computation over the entire dataset to obtain the true likelihood or its gradient, making them prohibitively expensive for large datasets. For example, the standard Metropolis-Hastings (MH) algorithm is unsuitable because the accept/reject step necessitates computing the likelihood, which involves summing across the entire dataset. To mitigate this computational burden, minibatched MH algorithms have been developed~\cite{korattikara2014austerity,maclaurin2014firefly,quiroz2018speeding,zhang2020asymptotically}. These algorithms utilize a subset of the data in the MH step to reduce costs. However, they often rely on strong assumptions, such as bounded log-likelihood or bounded gradients, which render them impractical for deep neural networks. 
% This limitation prevents the adoption of MCMC algorithms to deep neural networks, which typically require training on large-scale datasets. 
Despite MCMC methods, notably Hamiltonian Monte Carlo~\citep{duane1987hybrid,neal2010mcmc}, being regarded as the gold standard for Bayesian computations, they are rarely applied in modern BNNs due to their large computational cost. To address this challenge, Stochastic Gradient MCMC (SG-MCMC) methods~\citep{welling2011bayesian,chen2014stochastic,ma2015complete,zhangcyclical} have been proposed. These methods introduce stochastic gradients into MCMC sampling steps, relying only on a subset of data at each iteration. Consequently, they demand significantly less memory compared to standard MCMC approaches. SG-MCMC enables the use of sampling methods in large-scale Bayesian Neural Networks, such as ResNets~\citep{he:resnet2016}. In this section, we introduce three popular variants of SG-MCMC algorithms in deep learning. A summary of the procedure of SG-MCMC algorithms is provided in Algorithm~\ref{alg:sgmcmc_bnn}. 


\vspace{-1em}
\subsubsection{Stochastic Gradient Langevin Dynamics}
Stochastic Gradient Langevin Dynamics (SGLD)~\citep{welling2011bayesian} stands as the first SG-MCMC algorithm. In contrast to the full energy function $U(\mparam)=-\sum_{(\bm{x}, \bm{y}) \in\data}\log{p(\bm{x}|\mparam)}-\log{p(\mparam)}$ for the exact posterior distribution $p(\mparam|\data)\propto\exp(-U(\mparam))$, the SGLD algorithm considers a mini-batch energy function with a mini-batch of datapoints $\bm{\Xi}\subseteq\data$: 
\begin{equation}
    \widetilde{U}(\mparam)=-\frac{|\data|}{|\bm{\Xi}|}\sum_{(\bm{x}, \bm{y})\in\bm{\Xi}}\log{p(\bm{y} | \bm{x}, \mparam)}-\log{p(\mparam)},
\end{equation}
By resampling the minibatch $\bm{\Xi}$ at each iteration, the update rule of SGLD follows the \emph{underdamped Langevin dynamics}, replacing the true gradient with the stochastic gradient:
\begin{equation}
    \mparam_{t+1} \gets \mparam_t-\alpha_t\nabla_{\mparam_t}\widetilde{U}(\mparam_t)+\sqrt{2\alpha_t}\cdot\bm{\epsilon}_t,
\end{equation}
where $\alpha_t$ is the step size at iteration $t$ and $\bm{\epsilon}_t\sim \mathcal{N}(\bm{0},\bm{I})$ is the standard Gaussian noise. Compared with the update rule of stochastic gradient descent (SGD), the only difference is the addition of the appropriate Gaussian noise.
% At each iteration, a new sample is collected and finally a sample set $\mathcal{S}=\{\mparam_1,\mparam_2,...\}$ is obtained as an approximation of the true posterior distribution:
% \begin{equation}
%     p(\mparam|\data)\approx\frac{1}{|\mathcal{S}|}\sum_{\mparam_t\in\mathcal{S}}\delta(\mparam-\mparam_t),
% \end{equation}
% where $\delta(\cdot)$ is the Dirac delta function. 
\citet{welling2011bayesian} shows that when the step size $\alpha_t$
is sufficiently small, SGLD becomes standard unadjusted Langevin dynamics~\citep{roberts1996exponential}, as the Gaussian noise term dominates over the stochastic noise. To ensure a low asymptotic estimation error, a decaying step size schedule is typically applied \citep{bottou:ol1998,welling2011bayesian,ma2015complete}, satisfying (i) $\sum_{t=1}^\infty\alpha_t=\infty$ and (ii) $\sum_{t=1}^\infty\alpha_t^2<\infty$. The first requirement guarantees that the sampler will reach the high probability areas regardless of initialization, and the second requirement ensures a small asymptotic error~\citep{welling2011bayesian}.

\begin{figure}[t]
    \centering
\includegraphics[width=0.5\textwidth]{figures/lr-exp.pdf}
    % \vspace{-1.5em}
    \caption{Comparison between the cyclical stepsize schedule (red) and the traditional decreasing stepsize schedule (blue) for SG-MCMC algorithms. Adapted from
\citet{zhangcyclical}. Used with the kind permission of Ruqi Zhang.}
    \label{fig: csgmcmc}
\end{figure}
\subsubsection{Cyclical Stochastic Gradient MCMC}
As mentioned in the last section, decreasing step sizes for SG-MCMC is typically required to ensure asymptotic accuracy of posterior estimation. However, small step sizes will significantly reduce the sampler's ability to explore the entire domain and to exploit each mode in detail efficiently, resulting in large approximation errors in finite time~\citep{vollmer2016exploration,chen2015convergence}. This issue is exacerbated when estimating Bayesian neural network posteriors, which are complicated and highly multi-modal~\citep{neal1996bayesian,izmailov2021bayesian}. 

To solve this problem, \cite{zhangcyclical} proposes a cyclical step size schedule for SG-MCMC, depicted in Figure~\ref{fig: csgmcmc}. This schedule defines the step size at iteration $t$ as
\begin{equation}
\alpha_t = \frac{\alpha_0}{2}
\left[\cos\left(\frac{\pi~\text{mod}(t-1,\lceil T/M\rceil)}{\lceil T/M\rceil}\right)+1
\right], \label{eq:cyclic_lr}
\end{equation}
where $\alpha_0$ is the initial step size, $M$ is the number of cycles and $T$ is the number of total iterations. The cyclical step size schedule enables the sampler to efficiently explore various modes and accurately characterize local modes at a fine scale. 

To efficiently sample from the multi-modal posteriors in Bayesian neural networks, other strategies include SG-MCMC methods combined with parallel tempering~\citep{deng2020non} and flat histograms~\citep{deng2020contour}.



 


\subsubsection{Stochastic Gradient Hamiltonian Monte Carlo}
Hamiltonian Monte Carlo (HMC) has achieved the gold standard of performance with small neural networks [Here, refer to the Chapter of HMC]. It incorporates a kinetic energy term, characterized by a set of ``momentum" auxiliary variables. The naive stochastic gradient HMC adopts a mini-batch Hamiltonian function:
\begin{equation}
    \widetilde{H}(\mparam,\bm{r})=\widetilde{U}(\mparam)+\frac{1}{2}\bm{r}^T\bm{M}^{-1}\bm{r}=-\frac{|\data|}{|\bm{\Xi}|}\sum_{(\bm{x}, \bm{y}) \in\bm{\Xi}}\log{p(\bm{y}|\bm{x}, \mparam)}-\log{p(\mparam)}+\frac{1}{2}\bm{r}^T\bm{M}^{-1}\bm{r},
\end{equation}
where $\bm{r}$ is the momentum vector and $\bm{M}$ is a positive definite mass matrix. 
% The sampling process of naive SGHMC follows the gradients of $\widetilde{H}(\mparam,\bm{r})$ and is computed by the following Hamilton's equations~\citep{hamilton1833general}:
% \begin{equation}
% \frac{d\mparam}{dt}=\frac{\partial\widetilde{H}}{\partial\bm{r}}=\bm{M}^{-1}\bm{r}, \quad
% \frac{d\bm{r}}{dt}=-\frac{\partial\widetilde{H}}{\partial\mparam}=-\nabla_{\mparam}\widetilde{U}(\mparam)\approx-\nabla_{\mparam}U(\mparam)- \bm{\epsilon}, \ \bm{\epsilon} \sim \mathcal{N}(\bm{0},\bm{\Sigma}_{\mparam}),
% \end{equation}
% where the stochastic gradient $\nabla_{\mparam}\widetilde{U}(\mparam)$ can be approximated as the true gradient plus some Gaussian noise. The covariance of the Gaussian noise is  influenced by the stochastic noise and can vary based on the value of the current model parameter $\mparam$. 
Despite its intuitive appeal, \citet{chen2014stochastic} indicates that the naive adaption of HMC algorithm to the stochastic gradient version can diverge from the target distribution. This is because the joint distribution of parameters and momentum (i.e., $\pi(\mparam,\bm{r})\propto\exp(-\widetilde{H}(\mparam,\bm{r}))$) are not invariant for this Markov chain due to the introduction of stochastic noise. For corrections, \citet{chen2014stochastic} introduces a ``friction" term $C\bm{M}^{-1}\bm{r}$ where $C$ is the friction weight, to the momentum updating:
\begin{equation}
    \frac{d\mparam}{dt}=\bm{M}^{-1}\bm{r}~~~~\text{and}~~~~\frac{d\bm{r}}{dt}=-\nabla_{\mparam}U(\mparam)-C\bm{M}^{-1}\bm{r}+\sqrt{2C}\bm{\epsilon}, \ \bm{\epsilon} \sim \mathcal{N}(\bm{0},\bm{I}).
\end{equation}
This friction term helps decrease the Hamiltonian $\widetilde{H}(\mparam,\bm{r})$, which counters the effects of random noise. The modified dynamics is commonly denoted as \emph{second-order Langevin dynamics}~\citep{wang1945theory}.


%%%%%%%

\vspace{-3em}
\subsection{Variational Inference}
\label{sec:bnn_vi}
Variational inference (VI) \citep{jordan1999vi,beal:vi2003} is a class of approximate inference techniques, which approximates the exact posterior using a tractable distribution, typically from a parametric distribution family $\mathcal{Q} := \{q_{\vparam}(\mparam) \}$ such as factorized Gaussians. The best approximate posterior $q_{\vparam^*}(\mparam)$, which is used as $q(\mparam)$ in Eq.~\eqref{eq:approximate_predictive_distribution} and also named as the optimal variational distribution, is selected to be the one that is the closest to the posterior $p(\mparam|\data)$ within the parametric family as measured by a divergence measure between distributions. 
%
The Kullback-Leibler (KL) divergence \citep{kullback:divergence1951} is a popular choice for VI: here the approximate posterior is obtained by minimizing the KL-divergence with respect to the parameters $\vparam$ of the chosen parametric distributions family:
%
\begin{equation}
    \vparam^* = \arg\min_{q_{\vparam} \in \mathcal{Q}} KL[q_{\vparam}(\mparam)\|p(\mparam|\data)], \quad KL[q_{\vparam}(\mparam)\|p(\mparam|\data)] = \int q_{\vparam}(\mparam) \log \frac{q_{\vparam}(\mparam)}{p(\mparam|\data)} d\mparam.
\label{eq:kl}
\end{equation}
%
Again $p(\mparam|\data)$ is intractable due to the intractable marginal likelihood $p(\data)$, meaning that direct minimization of the KL-divergence is intractable. Fortunately, this KL-divergence minimization task is equivalent to maximizing the following Evidence Lower Bound (ELBO), which is obtained by subtracting the KL-divergence from the log marginal-likelihood which is constant w.r.t.~the variational parameters $\vparam$:
%
\begin{equation}
    \begin{split}
        \mathcal{L}_{ELBO}(\vparam) &:=\log p(\data) -KL[q_{\vparam}(\mparam)\|p(\mparam|\data)]\\
        %\mathcal{L}_{ELBO} &=\log p(\data) - \int q_{\vparam}(\mparam) \log \frac{q_{\vparam}(\mparam)}{p(\mparam|\data)} d\mparam\\
        %&=\log p(\data) - \int q_{\vparam}(\mparam) \log \frac{q_{\vparam}(\mparam)p(\data)}{p(\mparam)p(\data|\mparam)} d\mparam\\
        %&=\int q_{\vparam}(\mparam) \log \frac{p(\mparam)p(\data|\mparam)}{q_{\vparam}(\mparam)} d\mparam\\
        &=\underset{(i)}{E_{q_{\vparam}(\mparam)}[\log p(\data|\mparam)]} - \underset{(ii)}{KL[q_{\vparam}(\mparam)\|p(\mparam)]}.
    \end{split}
\label{eq:elbo}
\end{equation}
%
In Eq. \eqref{eq:elbo}, the ELBO is decomposed into (i) the expectation of log-likelihood under $q_{\vparam}$, optimization of which encourages better data fitting, and (ii) a KL regularizer, optimization of which encourages $q_{\vparam}$ to be close to the prior. Combining both terms, the ELBO optimization encourages $q_{\vparam}$ to find small but meaningful deviations from the prior that can explain the observed data.
%
In practice, the expected log-likelihood term (i) in the ELBO can not be analytically evaluated. Therefore an unbiased Monte Carlo estimate is applied to the ELBO with $M$ samples from the variational posterior \citep{graves2011practical,blundell2015bbp}:
%
\begin{equation}
    \hat{\mathcal{L}}_{ELBO}(\vparam) = \frac{1}{M} \sum_{m=1}^M \log p(\data|\mparam_m) - KL[q_{\vparam}(\mparam)\|p(\mparam)], \qquad \mparam_m \stackrel{\text{iid}}{\sim} q_{\vparam}(\mparam) .
\label{eq:elbo_mc_estimate}
\end{equation}
%
The KL regularizer term in ELBO may be evaluated analytically for some choices of prior and variational family (e.g., when they are both Gaussians). When analytic solution is not available, this KL term can also be estimated with Monte Carlo.

VI will return the exact posterior if $p(\mparam |\data) \in \mathcal{Q}$, which is not the case in practice. Instead, due to limited computational budget one would prefer simple $q$ distributions that enjoy fast computations and low memory costs. In BNN context, one of the most popular choices is mean-field Gaussian approximation (i.e., factorized Gaussian) \citep{blundell2015bbp}:
\begin{equation}
    q_{\vparam}(\mparam) = \prod_{l=1}^L q_{\vparam}(\weight^l)q_{\vparam}(\mathbf{b}^l), \ q_{\vparam}(\weight^l) = \prod_{ij} \mathcal{N}(\weight^l_{ij}; \mathbf{M}^l_{ij}, (\mathbf{S}^l_{ij})^2), \ q_{\vparam}(\mathbf{b}_l) = \prod_j \mathcal{N}(\mathbf{b}^l_{j}; \mathbf{m}^l_{j}, (\s^l_{j})^2).
\label{eq:mean_field_bnn_q_distribution}
\end{equation}
Here the variational parameters are $\vparam = \{\mathbf{M}^l, \mathbf{m}^l, \mathbf{S}^l, \mathbf{s}^l \}_{l=1}^L$ which are optimized by maximizing the ELBO, typically with Monte Carlo estimates (Eq.~\eqref{eq:elbo_mc_estimate}). Therefore, compared with deterministic DNNs which directly optimizes the weight parameters $\mparam=\{\weight^l, \mathbf{b}^l\}_{l=1}^L$, mean-field VI for BNNs doubles the amount of parameters to be optimized (hence twice amount of memory consumption), due to the additional variance parameters $\{\mathbf{S}^l, \mathbf{s}^l \}_{l=1}^L$. 

The optimization process of ELBO is conducted via gradient ascent, and in order to apply back-propagation with e.g., automatic differentiation techniques \citep{auto2018baydin}, the \emph{reparameterization trick} \citep{welling2014auto} is introduced for a range of variational distribution families including Gaussians. This approach assumes the sampling operation $\mparam \sim q_{\vparam}(\mparam)$ is defined as transforming an auxiliary noise variable $\bm{\epsilon} \sim p_{base}(\bm{\epsilon})$ through a function $T_{\vparam}(\bm{\epsilon})$ that is differentiable w.r.t.~$\vparam$:
%
\begin{equation}
   \mparam \sim q_{\vparam}(\mparam) \quad \Leftrightarrow \quad \mparam = T_{\vparam}(\bm{\epsilon}), \ \bm{\epsilon} \sim p_{base}(\bm{\epsilon}).
\end{equation}
%
For mean-field Gaussians, e.g., $q_{\vparam}(\weight^l)$ in Eq.~\eqref{eq:mean_field_bnn_q_distribution}, this transformation is defined as $\weight^l = T_{\vparam}(\bm{E}^l) := \mathbf{M}^l + \sqrt{\mathbf{S}^l} \odot \bm{E}^l$, with $\bm{E}^l_{ij} \sim \mathcal{N}(0, 1)$, and $\sqrt{\cdot}$ and $\odot$ denote element-wise square-root and multiplication, respectively. This allows us to rewrite the Monte Carlo estimate of the expected log-likelihood term in Eq.~\eqref{eq:elbo} using change-of-variable rules, where we collect the mean and variance parameters into $\bm{\mu} := \{\mathbf{M}^l, \mathbf{m}^l \}_{l=1}^L$ and $\bm{\sigma} := \{\sqrt{\mathbf{S}^l}, \sqrt{\mathbf{s}^l} \}_{l=1}^L$:
\begin{equation}
    E_{q_{\vparam}(\mparam)}[\log p(\data|\mparam)] \approx \frac{1}{M} \sum_{m=1}^M \log p(\data|\bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}_m), \quad \bm{\epsilon}_m \stackrel{\text{iid}}{\sim} \mathcal{N}(\bm{0}, \mathbf{I}).
\end{equation}
The corresponding gradient w.r.t.~$\mathbf{M}^l$ for example can then be computed via back-propagation. Recall that $p(\data|\mparam) := \prod_{n=1}^N p(\bm{y}_n | \bm{x}_n, \mparam)$ with $p(\bm{y} | \bm{x}, \mparam) := p(\bm{y} | f_{\mparam}(\bm{x}))$, then
$$\nabla_{\mathbf{M}^l} E_{q_{\vparam}(\mparam)}[\log p(\data|\mparam)] \approx \frac{1}{M} \sum_{m=1}^M \sum_{n=1}^N \nabla_{\mathbf{M}^l} f \nabla_f \log p(\bm{y}_n| f_{\bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}_m}(\bm{x}_n)), \quad \bm{\epsilon}_m \stackrel{\text{iid}}{\sim} \mathcal{N}(\bm{0}, \mathbf{I}).$$
In practice, using $M=1$ sample, together with proper initialization of the $\vparam$ parameters, is sufficient for the gradient estimation.
Similar operations can be applied to computing the gradient of the KL term in the ELBO when Monte Carlo estimate is also required.

Lastly, evaluating $\log p(\data|\mparam)$ requires processing the entire dataset $\data$ which can be costly in big data setting. Similar to training deterministic DNNs and running SG-MCMC for BNNs, stochastic optimization methods such as stochastic gradient descent (SGD) \citep{bottou:ol1998} are also applicable to variational inference. With $\bm{\Xi}\subseteq\data$ a mini-batch sampled from the entire dataset, the ELBO can be estimated as (with $M=1$ Monte Carlo sample and the reparameterization trick):
%
\begin{equation}
    \tilde{\mathcal{L}}_{ELBO}(\vparam) = \frac{|\data|}{|\bm{\Xi}|} \sum_{(\bm{x}, \bm{y}) \in \bm{\Xi}} \log p(\bm{y}| f_{\bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}}(\bm{x})) - KL[q_{\vparam}(\mparam)\|p(\mparam)], \quad \bm{\epsilon} \sim \mathcal{N}(\bm{0}, \mathbf{I}).
\label{eq:elbo_mc_estimate_minibatch}
\end{equation}
Automatic differentiation can be directly applied to $\tilde{\mathcal{L}}_{ELBO}(\vparam)$, and the optimization of this ELBO estimate has the same time complexity as running stochastic gradient descent training of deterministic DNNs. All the above techniques combined enable BNNs with mean-field Gaussian VI to scale up to modern DNN architectures such as ResNets \citep{he:resnet2016}, and a summary of the training procedure is provided in Algorithm \ref{alg:mfvi_bnn}.


\begin{algorithm}[t]
\caption{SGD Training for a BNN with mean-field Gaussian VI}\label{alg:mfvi_bnn}
\begin{algorithmic}[1]
\Require Dataset $\data = \{ (\x_n, \y_n) \}_{n=1}^N$, initialised variational parameters $\vparam = \{ \bm{\mu}, \bm{\sigma
} \}$,  batch-size $B$, SGD step sizes $\{\alpha_t \}$, SGD total steps $T$.
\For{$t = 1, ..., T$}
\State Compute the KL term in the ELBO:
$\tilde{\mathcal{L}}_{ELBO}(\vparam) \gets - KL[q_{\vparam}(\mparam)\|p(\mparam)]$ 
\State Sample BNN weights with the reparameterization trick: $\mparam \gets \bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}$, $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \mathbf{I})$
\State Sample a mini-batch of datapoints $\bm{\Xi} = \{ (\x, \y) \} \sim \data^B$ with $| \bm{\Xi} | = B$
\For{$(\x, \y) \in \bm{\Xi}$}
    \State Add-in data likelihood: $\tilde{\mathcal{L}}_{ELBO}(\vparam) \gets \tilde{\mathcal{L}}_{ELBO}(\vparam) + \frac{|\data|}{|\bm{\Xi}|} \log p(\bm{y}| f_{\mparam}(\bm{x}))$
\EndFor
\State Compute SGD updates via automatic differentiation:
$\vparam \gets \vparam + \alpha_t \nabla_{\mparam} \tilde{\mathcal{L}}_{ELBO}(\vparam)$
\EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Alternative Divergences}

Standard VI based on minimizing $KL[q_{\vparam}(\mparam)\|p(\mparam|\data)]$, while enjoying many computational advantages,  tends to underestimate posterior uncertainty \citep{turner:two_problems2011}. The Gaussian approximation from standard VI often captures only one mode of the exact posterior, which has been referred to as the mode-seeking phenomenon \citep{miguel2015alpha, li2016renyi}. This issue can be mitigated by extending the VI framework to other divergences. For example, $\alpha$-divergence is a subset of $f$-divergence family \citep{csiszar:divergence1963} with a hyper-parameter $\alpha$ determining the degree of the posterior mass coverage of the variational distribution \citep{renyi:divergence1961,van_erven:renyi2014}:

%
%\begin{equation}
%    D_{\alpha}[ p(\mparam|\data) \| q_{\vparam}(\mparam) ] = \frac{1}{\alpha (1-\alpha)} (1-\int p(\mparam|\data)^{\alpha} q_{\vparam}(\mparam)^{1-\alpha} d\mparam).
%    \label{eq:alpha_divergence}
%\end{equation}
%
\begin{equation}
    D_{\alpha}[ p(\mparam|\data) \| q_{\vparam}(\mparam) ] = \frac{1}{(\alpha-1)} \log \int p(\mparam|\data)^{\alpha} q_{\vparam}(\mparam)^{1-\alpha} d\mparam.
    \label{eq:alpha_divergence}
\end{equation}
%
When $\alpha$ is set to approach $1$ and $0$, we recover the two special cases: $KL[q_{\vparam}(\mparam)\|p(\mparam|\data)]$ (exclusive KL as used in standard VI) and  $KL[p(\mparam|\data)\|q_{\vparam}(\mparam)]$ (inclusive KL) respectively. There exist other equivalent $\alpha$-divergence definitions, e.g., Amari's \citep{amari1985differential}, which also include the two KL divergences as special cases.

Direct minimization of the $\alpha$-divergence in Eq.~\eqref{eq:alpha_divergence} is intractable again due to the intractable $p(\data)$. This is mitigated by again an equivalent maximization task on another lower bound of the log marginal likelihood, which can also be estimated with Monte Carlo, potentially with the reparameterization trick as well \citep{li2016renyi}:
%
\begin{equation}
     \begin{split}
        \mathcal{L}_{\alpha}(\vparam) &:=\log p(\data) - D_{\alpha}[ p(\mparam|\data) \| q_{\vparam}(\mparam) ]\\
        &= \frac{1}{1-\alpha} \log E_{q_{\vparam}(\mparam)}[(\frac{p(\data|\mparam)p(\mparam)}{q_{\vparam}(\mparam)})^{1-\alpha}]\\
        &\approx \frac{1}{1-\alpha} \log \frac{1}{M} \sum_{m=1}^M  (\frac{p(\data|\mparam_m)p(\mparam_m)}{q_{\vparam}(\mparam_m)})^{1-\alpha}, \qquad \mparam_m \stackrel{\text{iid}}{\sim} q_{\vparam}(\mparam)
    \end{split}
\end{equation}
%
\begin{comment}
\wc{In practice, we first identify minimizing $D_{\alpha}[ p(\mparam|\data) \| q_{\vparam}(\mparam)]$ is equivalent to minimizing $-E_{q_{\vparam}(\mparam)} [\frac{p(\mparam, \data)}{q_{\vparam}(\mparam)}]^{\alpha}$: 
\begin{equation}
     \begin{split}
      D_{\alpha}[ p(\mparam|\data) \| q_{\vparam}(\mparam) ]&\propto 1-\int p(\mparam|\data)^{\alpha} q_{\vparam}(\mparam)^{1-\alpha} d\mparam\\
        & = -E_{q_{\vparam}(\mparam)} [\frac{p(\mparam|\data)}{q_{\vparam}(\mparam)}]^{\alpha} + C_1\\
        &= -(\frac{1}{p(\data)})^{\alpha}E_{q_{\vparam}(\mparam)} [\frac{p(\mparam, \data)}{q_{\vparam}(\mparam)}]^{\alpha} + C_1\\
        &= -C_2 E_{q_{\vparam}(\mparam)} [\frac{p(\mparam, \data)}{q_{\vparam}(\mparam)}]^{\alpha} + C_1.  \qquad (C_1, C_2 \text{ are constants)}
    \end{split}
\end{equation}
Then minimize the logarithm of $-E_{q_{\vparam}(\mparam)} [\frac{p(\mparam, \data)}{q_{\vparam}(\mparam)}]^{\alpha}$. Again, we take advantage of Monte-Carlo estimation to obtain the final objective:
\begin{equation}
     \mathcal{L}_{\alpha}(\vparam)= -\log \frac{1}{M} \sum_{m=1}^M  (\frac{p(\data|\mparam_m)p(\mparam_m)}{q_{\vparam}(\mparam_m)})^{\alpha}, \qquad \mparam_m \sim q_{\vparam}(\mparam)
\end{equation}
}
\end{comment}
Figure \ref{fig: alphadiv} illustrates the coverage of mean-field approximations fitted with $\alpha$-divergences, where the target distribution is a correlated Gaussian. Indeed by decreasing $\alpha$, one can interpolate between mode-seeking and mass-covering behaviors for the approximate posterior. 

\begin{center}
%\vspace{-3em}
\begin{minipage}{.4\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/alpha.pdf}
\end{minipage}
\hfill
\begin{minipage}{.5\textwidth}
\captionof{figure}{Factorized Gaussians fitted by minimizing $\alpha$-divergences with different $\alpha$'s for a correlated 2D Gasussian target:}
\label{fig: alphadiv}
\begin{equation*}
\mathcal{N} \left(
\begin{pmatrix}
0\\
0
\end{pmatrix},
\begin{pmatrix}
2.0 & 1.5 \\
1.5 & 1.6 
\end{pmatrix} \right).
\end{equation*}
\end{minipage}
\end{center}
%

\subsubsection{Distribution Family}

One drawback of using mean-field Gaussian approximations is the negligence of covariance structures in the exact posterior. In this regard a Gaussian distribution with a full covariance matrix is favorable. However, storing and decomposing a covariance matrix require $O(d^2)$ memory and $O(d^3)$ time, respectively, which quickly become computationally intractable even if the number of BNN weights $d$ is around tens of thousands. Recall that modern neural networks for image recognition has millions, if not billions, of parameters \citep{krizhevsky2012imagenet,dehghani2023scaling}. Therefore Gaussians with freely parameterized full covariance matrices are not viable solutions, and further approximations are needed.

Recent attention on developing correlated Gaussian approximate posteriors focuses on so called ``low-rank $+$ diagonal'' structure for the covariance matrices. In other words, instead of searching over the entire space of positive semi-definite matrices for the covariance matrix, the Gaussian variational distribution $q_{\vparam}(\mparam)$ has the following structure \citep{tomczak2020efficient}:
\begin{equation}
    q_{\vparam}(\mparam) = \mathcal{N}(\vparam; \bm{\mu}, \mathbf{\Sigma}), \quad \mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top + \mathbf{D},
\end{equation}
where $\mathbf{L} \in \mathbb{R}^{d \times r}$ is a low-rank matrix (with $r << d$) and $\mathbf{D}$ is a diagonal matrix with positive diagonal entries. Here the variational parameters are $\vparam = \{\bm{\mu}, \mathbf{L}, \mathbf{D} \}$ which can be freely optimized using ELBO. In practice correlated Gaussian approximations are only applied layer-wise, i.e., the $q_{\vparam}$ distribution still factorizes over layers, but within each layer a low-rank Gaussian approximate posterior is fitted.

Another approach to introduce structured Gaussian approximations is to use matrix normal distributions. Notice that the popular choice of isotropic Gaussian prior $p(\weight^l) = \mathcal{N}(vec(\weight^l); \bm{0}, \sigma^2 \mathbf{I}_{d_o d_i})$ for the $l^{th}$ layer weight $\weight^l \in \mathbb{R}^{d_o \times d_i}$ has an equivalent matrix normal form $\mathcal{MN}(\weight^L; \bm{0}, \sigma_r^2\mathbf{I}_{d_o}, \sigma_c^2 \mathbf{I}_{d_i})$ with $\sigma_r > 0$ and $\sigma_c > 0$ the row and column standard deviations satisfying $\sigma = \sigma_r \sigma_c$. This inspires the design of matrix normal approximate posteriors $q_{\vparam}(\weight^l) = \mathcal{MN}(\weight^l; \mathbf{M}^l, \Sigma_r, \Sigma_c)$ with $\Sigma_r \in \mathbb{R}^{d_o \times d_o}$ and $\Sigma_c \in \mathbb{R}^{d_i \times d_i}$, reducing the costs to $\mathcal{O}(d_i^2 + d_o^2)$ memory and $\mathcal{O}(d_i^3 + d_o^3)$ time instead of $\mathcal{O}(d_i^2 d_o^2)$ and $\mathcal{O}(d_i^3 d_o^3)$ respectively for using full covariance Gaussian approximations. Further low-rank approximation techniques can be applied to parameterize $\Sigma_r$ and $\Sigma_c$ to further reduce the complexity figures \citep{louizos2017multiplicative,ritter2021sparse}.

Non-Gaussian posterior approximations have also been explored. Hierarchical conjugate priors \citep{kessler2021ibpbnn} and sparsity-inducing priors \citep{ghosh2019model,bai2020efficient} can be used to induce desirable structural properties of the weights, and in such cases posterior approximations typically share similar conjugacy or sparsity structure assumptions. 
%
Another line of work considers DNNs as flexible transformations to obtain expressive posterior approximations \citep{louizos2017multiplicative,tran:implicit2017,mescheder2017adversarial,li2018gradient}. In detail, with a learnable neural network $F_{\vparam}(\bm{\omega
})$ parameterized by $\vparam$ and operated on a potentially lower-dimensional variable $\bm{\omega}$, the approximate posterior $q_{\vparam} := (F_{\vparam})_{\#}q_0$ is defined as the push-forward distribution of a based distribution $q_0(\bm{\omega})$ that is often a standard Gaussian. 
%
When $F_{\vparam}(\bm{\omega
})$ represents an invertible function (e.g., normalizing flows \citep{rezende2015variational,louizos2017multiplicative, papamakarios2021normalizing}), the resulting approximate posterior density can be obtained via change-of-variable rule: $q_{\vparam}(\mparam) = q_0(F_{\vparam}^{-1}(\mparam)) | \frac{d F^{-1}_{\vparam}(\mparam)}{d \mparam}|$,
which can then be plugged into the ELBO (Eq.~\eqref{eq:elbo}), and techniques such as Monte Carlo estimate and the reparameterization trick are applicable.
%
On the other hand, for non-invertible $F_{\vparam}(\bm{\omega
})$ transforms, the resulting approximate posterior $q_{\vparam}(\mparam)$, while still supporting posterior predictive inference via Monte Carlo estimation and the reparameterization trick, no longer has a tractable density. Therefore further approximations to the ELBO objective and its gradients have been proposed \citep{mescheder2017adversarial,li2018gradient}. 
%
When $\bm{\omega}$ is lower-dimensional, these neural network transformed posterior may have mismatched support as compared with the prior. A solution to this issue is to add in e.g., Gaussian noise after neural network transform, resulting in an approximate posterior $q_{\vparam}(\mparam) = \int \mathcal{N}(\mparam | F_{\vparam}(\bm{\omega}), \sigma^2 \mathbf{I})q_0(\bm{\omega}) d\bm{\omega}$ which is then fitted using a lower-bound approximation to the ELBO \citep{salimans2015markov,yin2018semi}.