@article{AMSurvey,
    author = {Lawrence, John and Reed, Chris},
    title = "{Argument Mining: A Survey}",
    journal = {Computational Linguistics},
    volume = {45},
    number = {4},
    pages = {765-818},
    year = {2020},
    month = {01},
    abstract = "{Argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. Understanding argumentative structure makes it possible to determine not only what positions people are adopting, but also why they hold the opinions they do, providing valuable insights in domains as diverse as financial market prediction and public relations. This survey explores the techniques that establish the foundations for argument mining, provides a review of recent advances in argument mining techniques, and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00364},
    url = {https://doi.org/10.1162/coli\_a\_00364},
    eprint = {https://direct.mit.edu/coli/article-pdf/45/4/765/1847520/coli\_a\_00364.pdf},
}

@inproceedings{TransformerHealthcareAM,
    title = "Extraction d{'}arguments bas{\'e}e sur les transformateurs pour des applications dans le domaine de la sant{\'e} (Transformer-based Argument Mining for Healthcare Applications)",
    author = "Mayer, Tobias  and
      Cabrio, Elena  and
      Villata, Serena",
    editor = "Denis, Pascal  and
      Grabar, Natalia  and
      Fraisse, Amel  and
      Cardon, R{\'e}mi  and
      Jacquemin, Bernard  and
      Kergosien, Eric  and
      Balvet, Antonio",
    booktitle = "Actes de la 28e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\'e}rence principale",
    month = "6",
    year = "2021",
    address = "Lille, France",
    publisher = "ATALA",
    url = "https://aclanthology.org/2021.jeptalnrecital-taln.26",
    pages = "265--267",
    abstract = "Nous pr{\'e}sentons des r{\'e}sum{\'e}s en fran{\c{c}}ais et en anglais de l{'}article (Mayer et al., 2020) pr{\'e}sent{\'e} {\`a} la conf{\'e}rence 24th European Conference on Artificial Intelligence (ECAI-2020) en 2020.",
    language = "French",
}

@unpublished{SpeechAndProcessing,
    author = {Jurafsky, Daniel and Martin, James H.},
    title = {Speech and Language Processing (3nd Edition draft)},
    url = "https://web.stanford.edu/~jurafsky/slp3/",
    year = {2023}
}

@inproceedings{stab-gurevych-2014-identifying,
    title = "Identifying Argumentative Discourse Structures in Persuasive Essays",
    author = "Stab, Christian  and
      Gurevych, Iryna",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1006",
    doi = "10.3115/v1/D14-1006",
    pages = "46--56",
}

@article{EndToEndAM,
    author = {Morio, Gaku and Ozaki, Hiroaki and Morishita, Terufumi and Yanai, Kohsuke},
    title = "{End-to-end Argument Mining with Cross-corpora Multi-task Learning}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {639-658},
    year = {2022},
    month = {05},
    abstract = "{Mining an argument structure from text is an important step for tasks such as argument search and summarization. While studies on argument(ation) mining have proposed promising neural network models, they usually suffer from a shortage of training data. To address this issue, we expand the training data with various auxiliary argument mining corpora and propose an end-to-end cross-corpus training method called Multi-Task Argument Mining (MT-AM). To evaluate our approach, we conducted experiments for the main argument mining tasks on several well-established argument mining corpora. The results demonstrate that MT-AM generally outperformed the models trained on a single corpus. Also, the smaller the target corpus was, the better the MT-AM performed. Our extensive analyses suggest that the improvement of MT-AM depends on several factors of transferability among auxiliary and target corpora.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00481},
    url = {https://doi.org/10.1162/tacl\_a\_00481},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00481/2022965/tacl\_a\_00481.pdf},
}

@article{ParsingArgumentationStructures,
    author = {Stab, Christian and Gurevych, Iryna},
    title = "{Parsing Argumentation Structures in Persuasive Essays}",
    journal = {Computational Linguistics},
    volume = {43},
    number = {3},
    pages = {619-659},
    year = {2017},
    month = {09},
    abstract = "{In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using Integer Linear Programming. We show that our model significantly outperforms challenging heuristic baselines on two different types of discourse. Moreover, we introduce a novel corpus of persuasive essays annotated with argumentation structures. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.}",
    issn = {0891-2017},
    doi = {10.1162/COLI_a_00295},
    url = {https://doi.org/10.1162/COLI\_a\_00295},
    eprint = {https://direct.mit.edu/coli/article-pdf/43/3/619/1808352/coli\_a\_00295.pdf},
}





@article{RCI,
author = {Smith, Elliot and Hancox, Peter},
year = {2001},
month = {06},
pages = {295-323},
title = {Representation, Coherence and Inference},
volume = {15},
journal = {Artif. Intell. Rev.},
doi = {10.1023/A:1011092219561}
}

@article{DBLP,
  author       = {Baiyun Cui and
                  Yingming Li and
                  Yaqing Zhang and
                  Zhongfei Zhang},
  title        = {Text Coherence Analysis Based on Deep Neural Network},
  journal      = {CoRR},
  volume       = {abs/1710.07770},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.07770},
  eprinttype    = {arXiv},
  eprint       = {1710.07770},
  timestamp    = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-07770.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PARROT,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@online{NDG,
  author = {Bundesamt für Justiz},
  title = {Gesetz zur Verbesserung der Rechtsdurchsetzung in sozialen Netzwerken (Netzwerkdurchsetzungsgesetz - NetzDG)},
  year = 2017,
  url = {https://www.gesetze-im-internet.de/netzdg/BJNR335210017.html},
  urldate = {2023-11-29}
}

@online{AMR,
  author = {Shrawanty Y; Rohit B; Kanhaiya K; Vineet K},
  title = {Content Moderation Services Market by Component (Solution, Services), by Deployment Mode (On-Premise, Cloud), by Content Type (Image, Text, Video), by Organization Size (Large Enterprises, Small and Medium-sized Enterprises), by Industry Vertical (IT and Telecommunication, Government, Retail and E-Commerce, Healthcare, Media and Entertainment, Education, Others): Global Opportunity Analysis and Industry Forecast, 2021-2031},
  year = 2022,
  url = {https://www.alliedmarketresearch.com/content-moderation-services-market-A31650},
  urldate = {2023-11-29}
}
@article{Ampersand,
  author       = {Tuhin Chakrabarty and
                  Christopher Hidey and
                  Smaranda Muresan and
                  Kathy McKeown and
                  Alyssa Hwang},
  title        = {{AMPERSAND:} Argument Mining for PERSuAsive oNline Discussions},
  journal      = {CoRR},
  volume       = {abs/2004.14677},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.14677},
  eprinttype    = {arXiv},
  eprint       = {2004.14677},
  timestamp    = {Sun, 03 May 2020 17:39:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-14677.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{CDCP,
    title = "A Corpus of e{R}ulemaking User Comments for Measuring Evaluability of Arguments",
    author = "Park, Joonsuk  and
      Cardie, Claire",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1257",
}
@inproceedings{TowardsNonTree,
    title = "Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization",
    author = "Morio, Gaku  and
      Ozaki, Hiroaki  and
      Morishita, Terufumi  and
      Koreeda, Yuta  and
      Yanai, Kohsuke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.298",
    doi = "10.18653/v1/2020.acl-main.298",
    pages = "3259--3266",
    abstract = "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.",
}
@inproceedings{AIFdb,
author = {Lawrence, John and Bex, Floris and Snaith, Mark and Reed, Chris},
year = {2012},
month = {01},
pages = {},
title = {AIFdb: Infrastructure for the Argument Web},
volume = {245},
journal = {Frontiers in Artificial Intelligence and Applications},
doi = {10.3233/978-1-61499-111-3-515}
}

@article{MiningLegal,
	title = {Mining legal arguments in court decisions},
	issn = {1572-8382},
	url = {https://doi.org/10.1007/s10506-023-09361-y},
	doi = {10.1007/s10506-023-09361-y},
	abstract = {Identifying, classifying, and analyzing arguments in legal discourse has been a prominent area of research since the inception of the argument mining field. However, there has been a major discrepancy between the way natural language processing (NLP) researchers model and annotate arguments in court decisions and the way legal experts understand and analyze legal argumentation. While computational approaches typically simplify arguments into generic premises and claims, arguments in legal research usually exhibit a rich typology that is important for gaining insights into the particular case and applications of law in general. We address this problem and make several substantial contributions to move the field forward. First, we design a new annotation scheme for legal arguments in proceedings of the European Court of Human Rights (ECHR) that is deeply rooted in the theory and practice of legal argumentation research. Second, we compile and annotate a large corpus of 373 court decisions (2.3M tokens and 15k annotated argument spans). Finally, we train an argument mining model that outperforms state-of-the-art models in the legal NLP domain and provide a thorough expert-based evaluation. All datasets and source codes are available under open lincenses at https://github.com/trusthlt/mining-legal-arguments.},
	journal = {Artificial Intelligence and Law},
	author = {Habernal, Ivan and Faber, Daniel and Recchia, Nicola and Bretthauer, Sebastian and Gurevych, Iryna and Spiecker genannt Döhmann, Indra and Burchard, Christoph},
	month = jun,
	year = {2023},
}
@inproceedings{TowardsRelationBasedAM,
    title = "Towards relation based Argumentation Mining",
    author = "Carstens, Lucas  and
      Toni, Francesca",
    editor = "Cardie, Claire",
    booktitle = "Proceedings of the 2nd Workshop on Argumentation Mining",
    month = jun,
    year = "2015",
    address = "Denver, CO",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-0504",
    doi = "10.3115/v1/W15-0504",
    pages = "29--34",
}
@inproceedings{stab-gurevych-2014-identifying,
    title = "Identifying Argumentative Discourse Structures in Persuasive Essays",
    author = "Stab, Christian  and
      Gurevych, Iryna",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1006",
    doi = "10.3115/v1/D14-1006",
    pages = "46--56",
}
@misc{text2text,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{t2t-AM,
    title = "Argument Mining as a Text-to-Text Generation Task",
    author = "Kawarada, Masayuki  and
      Hirao, Tsutomu  and
      Uchida, Wataru  and
      Nagata, Masaaki",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.121",
    pages = "2002--2014",
    abstract = "Argument Mining (AM) aims to uncover the argumentative structures within a text. Previous methods require several subtasks, such as span identification, component classification, and relation classification. Consequently, these methods need rule-based postprocessing to derive argumentative structures from the output of each subtask. This approach adds to the complexity of the model and expands the search space of the hyperparameters. To address this difficulty, we propose a simple yet strong method based on a text-to-text generation approach using a pretrained encoder-decoder language model. Our method simultaneously generates argumentatively annotated text for spans, components, and relations, eliminating the need for task-specific postprocessing and hyperparameter tuning. Furthermore, because it is a straightforward text-to-text generation method, we can easily adapt our approach to various types of argumentative structures.Experimental results demonstrate the effectiveness of our method, as it achieves state-of-the-art performance on three different types of benchmark datasets: the Argument-annotated Essays Corpus (AAEC), AbstRCT, and the Cornell eRulemaking Corpus (CDCP).",
}
@inproceedings{glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}
@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.04606},
  year={2016}
}
@inproceedings{bpemb,
    title = "{BPE}mb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages",
    author = "Heinzerling, Benjamin  and
      Strube, Michael",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1473",
}
@misc{ELMo,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{flair,
  title={Contextual String Embeddings for Sequence Labeling},
  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},
  pages     = {1638--1649},
  year      = {2018}
}
@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{biobert,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    volume = {36},
    number = {4},
    pages = {1234-1240},
    year = {2019},
    month = {09},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics\_36\_4\_1234.pdf},
}
@inproceedings{scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at \url{https://github.com/allenai/scibert/}.",
}
@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@book{nn_foundation,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon},
  year={1994},
  publisher={Prentice Hall PTR}
}
@misc{mtc,
    title = {An annotated corpus of argumentative microtexts},
    author = {Andreas Peldszus and Manfred Stede},
    year = {2016},
    booktitle = {Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation},
    volume = {2},
    pages = {801--815},
    address = {London},
    publisher = {College Publications},
}
@inproceedings{mtc,
    title = "An annotated corpus of argumentative microtexts",
    author = "Andreas Peldszus and Manfred Stede",
    booktitle = "Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation",
    year = "2016",
    address = "London, UK",
    publisher = "College Publications",
    pages = "801--815",
}
@article{compu-lingu,
author = {Benzon, William},
year = {1979},
month = {01},
pages = {},
title = {Computational Linguistics and Discourse Analysis},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2508667}
}