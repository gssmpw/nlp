\section{Related work}
Our work builds upon several areas in RAG methods and post-training.

\subsection{Training RAG-enabled LLMs}
As retrieval-augmented generation (RAG) has garnered interest as an effective way to improve LLM performance**Vinyals et al., " Retrieval-Augmented Generation of Conversational Dialogue"**
We focus on the setting where we prepend retrievals to the prompt, which is a common, simple method for incorporating retrievals into the context, 
though alternative RAG architectures exist**Guu et al., "REALM: Towards Generalizability in Multi-Turn Question Answering"**. 
Our work builds upon previous studies which investigate how to train LLMs to better handle retrieved context.
Some of these propose pre-training methods that boost RAG capabilities
**Liu et al., "Pegasus: Pre-sentence Selection for Efficient Neural Dialogue Generation"**
,
while others fine-tune a LLM for better RAG capabilities**Wang et al., "Improving Multi-Turn Dialog Systems with Reinforced Sequence Generation"**.
Our method targets the latter fine-tuning setting, where we hypothesize that out-of-distribution training examples cause performance degradation. 
Our key insight is to self-generate demonstrations, 
rather than using a teacher LLM**Kang et al., "Unfamiliar Data Can Hurt: Out-of-Distribution Training Examples Cause Post-Training Degradation"**
or human-written responses**Chen et al., "Improving Conversational Dialogue Systems with Human-in-the-Loop Supervision"**.

\subsection{Distilling prompted behavior}
Our methodology has similarities with recent work on distilling the behavior of prompted language models back into the unprompted model.
Note that this is different from distilling a larger model into a smaller one; instead, the prompted model and the target model for fine-tuning are the same.
**Liu et al., "PEGASUS: Pre-Sentence Selection for Efficient Neural Dialogue Generation"**
 use prompting frameworks
to generate training examples using a language model, 
then do supervised fine-tuning (SFT) to distill the behavior under these prompts back into that same model.
The resulting model exhibits behavior like the prompted models without requiring prompts. 
Our work takes a similar approach for a similar goal,
though we use automatically optimized prompts instead of hand-crafted prompting frameworks to generate training data,
and we aim to specifically distill RAG capabilities rather than reasoning abilities.
Our method, similar to the one in **Chen et al., "Improving Conversational Dialogue Systems with Human-in-the-Loop Supervision"**,
also requires filtering training examples for correctness.

\subsection{Self-demo training \& learning to abstain}
Our work is partly motivated by the hypothesis **Kang et al., "Unfamiliar Data Can Hurt: Out-of-Distribution Training Examples Cause Post-Training Degradation"**
and  that out-of-distribution training data degrade LLM during post-training and encourage hallucinations.
Similar to our work, **Vinyals et al., " Retrieval-Augmented Generation of Conversational Dialogue"** train LLMs on self-generated demonstrations, but with the goal of improving factuality.
They also investigate using RAG as part of their training pipeline but not during inference.
We also build on their work by introducing a filtering stage to ensure the quality and correctness of the self-generated instances before training on them. 

A large body of existing literature investigates learning when to abstain from answering questions**Liu et al., "PEGASUS: Pre-Sentence Selection for Efficient Neural Dialogue Generation"**.
Our method for selecting self-demos resembles the simple method proposed by **
Guu et al., "REALM: Towards Generalizability in Multi-Turn Question Answering"**
where training examples that the model answers incorrectly are replaced with refusals. 
We incorporate and improve on their template-based refusal strategy by selecting self-generated refusals when none of the self-generated demonstrations are correct.
By using self-generated refusals instead of template-based ones, we better align the training distribution with the LLM's own distribution.

\begin{figure*}
  \centering
  \small
  \input{fig/method}
  \caption[Our method]{
    Our method uses the reference model to generate response candidates for the retrieval-instruction pairs.
    We then use the model along with the gold responses to filter the candidate responses for correct ones.
    Finally, we replace the \sethlcolor{pink}\hl{OOD} responses in the training set 
    with these \sethlcolor{green}\hl{in-distribution} self-generated responses (self-demos) and train on these. 
  }
  \label{fig:method}
\end{figure*}