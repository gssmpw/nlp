\section{Related work}
Our work builds upon several areas in RAG methods and post-training.

\subsection{Training RAG-enabled LLMs}
As retrieval-augmented generation (RAG) has garnered interest as an effective way to improve LLM performance____.
We focus on the setting where we prepend retrievals to the prompt, which is a common, simple method for incorporating retrievals into the context, 
though alternative RAG architectures exist____. 
Our work builds upon previous studies which investigate how to train LLMs to better handle retrieved context.
Some of these propose pre-training methods that boost RAG capabilities
____,
while others fine-tune a LLM for better RAG capabilities____.
Our method targets the latter fine-tuning setting, where we hypothesize that out-of-distribution training examples cause performance degradation. 
Our key insight is to self-generate demonstrations, 
rather than using a teacher LLM____
or human-written responses____.

\subsection{Distilling prompted behavior}
Our methodology has similarities with recent work on distilling the behavior of prompted language models back into the unprompted model.
Note that this is different from distilling a larger model into a smaller one; instead, the prompted model and the target model for fine-tuning are the same.
____ use prompting frameworks____
to generate training examples using a language model, 
then do supervised fine-tuning (SFT) to distill the behavior under these prompts back into that same model.
The resulting model exhibits behavior like the prompted models without requiring prompts. 
Our work takes a similar approach for a similar goal,
though we use automatically optimized prompts instead of hand-crafted prompting frameworks to generate training data,
and we aim to specifically distill RAG capabilities rather than reasoning abilities.
Our method, similar to the one in ____,
also requires filtering training examples for correctness.

\subsection{Self-demo training \& learning to abstain}
Our work is partly motivated by the hypothesis ____ and \citeposs{kang2024unfamiliar} hypothesis that out-of-distribution training data degrade LLM during post-training and encourage hallucinations.
Similar to our work, ____ train LLMs on self-generated demonstrations, but with the goal of improving factuality.
They also investigate using RAG as part of their training pipeline but not during inference.
We also build on their work by introducing a filtering stage to ensure the quality and correctness of the self-generated instances before training on them. 

A large body of existing literature investigates learning when to abstain from answering questions____.
Our method for selecting self-demos resembles the simple method proposed by ____
where training examples that the model answers incorrectly are replaced with refusals. 
We incorporate and improve on their template-based refusal strategy by selecting self-generated refusals when none of the self-generated demonstrations are correct.
By using self-generated refusals instead of template-based ones, we better align the training distribution with the LLM's own distribution.

\begin{figure*}
  \centering
  \small
  \input{fig/method}
  \caption[Our method]{
    Our method uses the reference model to generate response candidates for the retrieval-instruction pairs.
    We then use the model along with the gold responses to filter the candidate responses for correct ones.
    Finally, we replace the \sethlcolor{pink}\hl{OOD} responses in the training set 
    with these \sethlcolor{green}\hl{in-distribution} self-generated responses (self-demos) and train on these. 
  }
  \label{fig:method}
\end{figure*}