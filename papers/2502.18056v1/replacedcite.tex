\section{Related Works}
\subsection{Injecting ViT with Convolutional Priors}
Vision Transformer (ViT) reliance on large datasets stems from the lack of inductive biases inherent to convolutional neural networks (CNNs) ____. CNNs, inspired by the hierarchical processing of the mammalian visual cortex ____, provide important priors for learning spatial relationships in visual data. Recognizing this limitation, numerous studies have previously explored incorporating convolutional priors into ViT architectures ____. Early attempts, such as the “hybrid ViT” ____, fed a ResNet ____ feature map into a transformer encoder, showing a slight performance advantage over ViT at smaller computational budgets. However, later studies ____ revealed that excessive convolutional layers could diminish the generalization power of ViTs, suggesting that a shallow convolutional stem might strike the right balance between CNN-like inductive biases and the representational power of transformers.

The Compact Convolutional Transformer (CCT) ____ followed this principle, introducing a convolutional tokenizer tailored for supervised learning on datasets significantly smaller than ImageNet. While CCT demonstrates strong performance on supervised contexts, its reliance on standard convolutions makes it fundamentally incompatible with masked image modeling self-supervised tasks. 

Building upon this foundation, our work pushes the idea further by leveraging sparse convolutions ____ –a design choice introduced by the Spark framework ____, which enabled BERT pre-training on CNN architectures- to enhance tokenization specifically for self-supervised learning. Unlike Spark, which employs a fully convolutional encoder-decoder architecture reminiscent of UNet and adopts a generative BERT-style training framework, our proposed sparse convolutional tokenizer architecture is tailored to overcome critical limitations such as information leakage and mask vanishing that have previously hampered the application of traditional convolution-based tokenizers for vision transformers in MIM tasks.

\subsection{Masked Predictive Representation Learning}
Masked Image Modeling (MIM), first introduced in BEiT ____, draws inspiration from the success of BERT in NLP ____. In this approach, an image is divided in non-overlapping patches and a subset of these patches is masked out. The model is tasked with reconstructing the masked regions, which encourages learning meaningful representations of visual features, akin to how BERT learns semantic dependencies in text. Since the introduction of MIM, various methods have explored different reconstruction targets, such as raw  pixels ____,  or patch-level tokens via a learned tokenizer ____. While these approaches have been effective in scaling self-supervised learning to larger datasets, they often lead to feature representations at a low-level of semantic abstraction. This is particularly problematic in fine-grained tasks, which require deeper more abstract representations for distinguishing between visually similar classes. Invariance-based pretraining methods that enforce similar embeddings for two or more views of the same image ____ have been combined with MIM objectives, to produce representations of a high semantic level. However, image views are typically constructed using a set of complex hand-crafted data augmentations that enforce strong biases that may be detrimental to certain downstream tasks ____ and also may not generalize to other scientific domains ____. 

Our work is directly inspired by I-JEPA ____, which takes this concept further by predicting masked abstract targets in the representation space produced by a momentum-based target-encoder ViT network. However, as previous MIM-based approaches, I-JEPA is evaluated exclusively on large-scale pretraining to achieve comptetitive results with other foundational vision models. Building on these ideas, we integrate our Sparse Convolutional Tokenizer for Transformers (SCOTT) within the ViT architecture of a JEPA framework based on MIM, which we refer to as MIM-JEPA. While both I-JEPA and MIM-JEPA (ours) instantiate a Joint-Embedding Predictive Architecture (JEPA), the implementations differ. In contrast to I-JEPA, which follows MAE ____ by processing only visible patches in the context-encoder, MIM-JEPA treats the context-encoder and predictor as a unified architecture which processes both visible and masked tokens from the SCOTT tokenizer. 

Overall, our work differs from the aforementioned works in several ways, in that it focuses on proposing a learning framework and models that can be trained from scratch on small datasets that are orders of magnitude smaller than ImageNet. Thus, offering a solution to efficiently train models, with fewer parameters, on small datasets and smaller platforms while still maintaining state-of-the-art results.