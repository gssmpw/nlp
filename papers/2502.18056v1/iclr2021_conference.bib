
@article{balestriero_cookbook_2023,
  title={A Cookbook of Self-Supervised Learning},
  author={Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  journal={arXiv preprint arXiv:2304.12210},
  year={2023}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@article{ozbulak_know_2023,
  title={Know your self-supervised learning: A survey on image-based generative and discriminative training},
  author={Ozbulak, Utku and Lee, Hyun Jung and Boga, Beril and Anzaku, Esla Timothy and Park, Homin and Van Messem, Arnout and De Neve, Wesley and Vankerschaver, Joris},
  journal={arXiv preprint arXiv:2305.13689},
  year={2023}
}

@inproceedings{he_momentum_2020,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{bengio_representation_2014,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{assran2023self,
  title={Self-supervised learning from images with a joint-embedding predictive architecture},
  author={Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15619--15629},
  year={2023}
}

@article{hassani_escaping_2022,
  title={Escaping the big data paradigm with compact transformers},
  author={Hassani, Ali and Walton, Steven and Shah, Nikhil and Abuduweili, Abulikemu and Li, Jiachen and Shi, Humphrey},
  journal={arXiv preprint arXiv:2104.05704},
  year={2021}
}

@article{tian_designing_2023,
  title={Designing bert for convolutional networks: Sparse and hierarchical masked modeling},
  author={Tian, Keyu and Jiang, Yi and Diao, Qishuai and Lin, Chen and Wang, Liwei and Yuan, Zehuan},
  journal={arXiv preprint arXiv:2301.03580},
  year={2023}
}

@article{huang_self-supervised_2023,
	title = {Self-supervised learning for medical image classification: a systematic review and implementation guidelines},
	volume = {6},
	issn = {2398-6352},
	shorttitle = {Self-supervised learning for medical image classification},
	url = {https://www.nature.com/articles/s41746-023-00811-0},
	doi = {10.1038/s41746-023-00811-0},
	abstract = {Abstract
            Advancements in deep learning and computer vision provide promising solutions for medical image analysis, potentially improving healthcare and patient outcomes. However, the prevailing paradigm of training deep learning models requires large quantities of labeled training data, which is both time-consuming and cost-prohibitive to curate for medical images. Self-supervised learning has the potential to make significant contributions to the development of robust medical imaging models through its ability to learn useful insights from copious medical datasets without labels. In this review, we provide consistent descriptions of different self-supervised learning strategies and compose a systematic review of papers published between 2012 and 2022 on PubMed, Scopus, and ArXiv that applied self-supervised learning to medical imaging classification. We screened a total of 412 relevant studies and included 79 papers for data extraction and analysis. With this comprehensive effort, we synthesize the collective knowledge of prior work and provide implementation guidelines for future researchers interested in applying self-supervised learning to their development of medical imaging classification models.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {npj Digit. Med.},
	author = {Huang, Shih-Cheng and Pareek, Anuj and Jensen, Malte and Lungren, Matthew P. and Yeung, Serena and Chaudhari, Akshay S.},
	month = apr,
	year = {2023},
	pages = {74},
	file = {Huang et al. - 2023 - Self-supervised learning for medical image classif.pdf:files/2814/Huang et al. - 2023 - Self-supervised learning for medical image classif.pdf:application/pdf},
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@article{dosovitskiy_image_2021,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{esser_taming_2021,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

@article{steiner_how_2022,
  title={How to train your vit? data, augmentation, and regularization in vision transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	url = {https://ieeexplore.ieee.org/document/6795724},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	urldate = {2024-09-06},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	note = {Conference Name: Neural Computation},
	pages = {541--551},
	file = {IEEE Xplore Abstract Record:files/2840/6795724.html:text/html},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2024-09-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:files/2847/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	abstract = {A neural network modelfor visual pattern recognition, called the "neocognitron, "' was previously proposed by the author In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism o f the neocognitron.},
	language = {en},
	number = {2},
	urldate = {2024-09-06},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
	file = {Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf:files/2850/Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf:application/pdf},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	doi = {10.1113/jphysiol.1959.sp006308},
	language = {eng},
	number = {3},
	journal = {J Physiol},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pmid = {14403679},
	pmcid = {PMC1363130},
	keywords = {Animals, Cats, Cerebral Cortex, CEREBRAL CORTEX/physiology, Neurons, NEURONS/physiology, Visual Cortex},
	pages = {574--591},
	file = {Texto completo:files/2855/Hubel y Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={30392--30400},
  year={2021}
}

@inproceedings{chen_visformer_2021,
	title = {Visformer: {The} {Vision}-friendly {Transformer}},
	shorttitle = {Visformer},
	url = {https://ieeexplore.ieee.org/document/9711046},
	doi = {10.1109/ICCV48922.2021.00063},
	abstract = {The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the ‘Vision-friendly Transformer’. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Fitting, Training, Computer vision, Visualization, Computational modeling, Convolutional codes, Protocols, Recognition and classification, Representation learning},
	pages = {569--578},
	file = {IEEE Xplore Abstract Record:files/2863/9711046.html:text/html;Versión enviada:files/2862/Chen et al. - 2021 - Visformer The Vision-friendly Transformer.pdf:application/pdf},
}

@inproceedings{graham_levit_2021,
  title={Levit: a vision transformer in convnet's clothing for faster inference},
  author={Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={12259--12269},
  year={2021}
}
@inproceedings{wu_cvt_2021,
	title = {{CvT}: {Introducing} {Convolutions} to {Vision} {Transformers}},
	shorttitle = {{CvT}},
	url = {https://ieeexplore.ieee.org/document/9710031},
	doi = {10.1109/ICCV48922.2021.00009},
	abstract = {We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Transformers, Convolutional codes, Recognition and classification, Computer architecture, Distortion, Image recognition, Image resolution, Performance gain},
	pages = {22--31},
	file = {IEEE Xplore Abstract Record:files/2868/9710031.html:text/html;Versión enviada:files/2867/Wu et al. - 2021 - CvT Introducing Convolutions to Vision Transforme.pdf:application/pdf},
}

@inproceedings{yuan_incorporating_2021,
	title = {Incorporating {Convolution} {Designs} into {Visual} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/9711272},
	doi = {10.1109/ICCV48922.2021.00062},
	abstract = {Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state- of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly 1.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Training, Convolution, Feature extraction, Transformers, Visualization, Recognition and classification, Costs, Efficient training and inference methods, Training data},
	pages = {559--568},
	file = {IEEE Xplore Abstract Record:files/2871/9711272.html:text/html;Versión enviada:files/2870/Yuan et al. - 2021 - Incorporating Convolution Designs into Visual Tran.pdf:application/pdf},
}

@inproceedings{Devlin_BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{peng_unified_2022,
	title = {A {Unified} {View} of {Masked} {Image} {Modeling}},
	url = {http://arxiv.org/abs/2210.10615},
	abstract = {Masked image modeling has demonstrated great potential to eliminate the label-hungry problem of training large-scale vision Transformers, achieving impressive performance on various downstream tasks. In this work, we propose a uniﬁed view of masked image modeling after revisiting existing methods. Under the uniﬁed view, we introduce a simple yet eﬀective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images. Experimental results on image classiﬁcation and semantic segmentation show that MaskDistill achieves comparable or superior performance than state-of-the-art methods. When using the huge vision Transformer and pretraining 300 epochs, MaskDistill obtains 88.3\% ﬁne-tuning top-1 accuracy on ImageNet-1k (224 size) and 58.8\% semantic segmentation mIoU metric on ADE20k (512 size). The code and pretrained models will be available at https://aka.ms/unimim.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10615 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Peng et al. - 2022 - A Unified View of Masked Image Modeling.pdf:files/2879/Peng et al. - 2022 - A Unified View of Masked Image Modeling.pdf:application/pdf},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:files/2884/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf:application/pdf;arXiv.org Snapshot:files/2885/2304.html:text/html},
}

@INPROCEEDINGS{deng_imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  abstract={The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848},
  ISSN={1063-6919},
  month={June},}

@inproceedings{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  booktitle={International conference on machine learning},
  pages={1298--1312},
  year={2022},
  organization={PMLR}
}

@inproceedings{tian2020_imagenet100,
  title={Contrastive multiview coding},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XI 16},
  pages={776--794},
  year={2020},
  organization={Springer}
}

@inproceedings{jain_data-based_2023,
	address = {Vancouver, BC, Canada},
	title = {A {Data}-{Based} {Perspective} on {Transfer} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203061/},
	doi = {10.1109/CVPR52729.2023.00352},
	language = {en},
	urldate = {2024-09-10},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jain, Saachi and Salman, Hadi and Khaddaj, Alaa and Wong, Eric and Park, Sung Min and Madry, Aleksander},
	month = jun,
	year = {2023},
	pages = {3613--3622},
	file = {Jain et al. - 2023 - A Data-Based Perspective on Transfer Learning.pdf:files/2888/Jain et al. - 2023 - A Data-Based Perspective on Transfer Learning.pdf:application/pdf},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{liu_sparse_2015,
	title = {Sparse {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/7298681},
	doi = {10.1109/CVPR.2015.7298681},
	abstract = {Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90\% of parameters, with a drop of accuracy that is less than 1\% on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.},
	urldate = {2024-09-10},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Accuracy, Convolutional codes, Kernel, Matrix decomposition, Neural networks, Redundancy, Sparse matrices},
	pages = {806--814},
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6256--6268},
  year={2020}
}

@inproceedings{xie_simmim_2022,
	title = {{SimMIM}: a {Simple} {Framework} for {Masked} {Image} {Modeling}},
	shorttitle = {{SimMIM}},
	url = {https://ieeexplore.ieee.org/document/9880205},
	doi = {10.1109/CVPR52688.2022.00943},
	abstract = {This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8\% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6\%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1\% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.},
	urldate = {2024-09-10},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {Training, Computer vision, Representation learning, Data models, Head, Predictive models, Self-\& semi-\& meta- Representation learning, Self-supervised learning},
	pages = {9643--9653},
	file = {IEEE Xplore Abstract Record:files/2902/9880205.html:text/html;Versión enviada:files/2901/Xie et al. - 2022 - SimMIM a Simple Framework for Masked Image Modeli.pdf:application/pdf},
}

@article{peng_beitv2_2022,
  title={Beit v2: Masked image modeling with vector-quantized visual tokenizers},
  author={Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  journal={arXiv preprint arXiv:2208.06366},
  year={2022}
}

@inproceedings{assran_masked_2022,
	address = {Cham},
	title = {Masked {Siamese} {Networks} for {Label}-{Efficient} {Learning}},
	isbn = {978-3-031-19821-2},
	doi = {10.1007/978-3-031-19821-2_26},
	abstract = {We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4\% top-1 accuracy, and with 1\% of ImageNet-1K labels, we achieve 75.7\% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available at https://github.com/facebookresearch/msn.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Assran, Mahmoud and Caron, Mathilde and Misra, Ishan and Bojanowski, Piotr and Bordes, Florian and Vincent, Pascal and Joulin, Armand and Rabbat, Mike and Ballas, Nicolas},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {456--473},
	file = {Versión enviada:files/2908/Assran et al. - 2022 - Masked Siamese Networks for Label-Efficient Learni.pdf:application/pdf},
}

@article{zhou2021ibot,
  title={ibot: Image bert pre-training with online tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal={arXiv preprint arXiv:2111.07832},
  year={2021}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  pages={1--62},
  year={2022}
}

@article{zhai_large-scale_2020,
  title={A large-scale study of representation learning with the visual task adaptation benchmark},
  author={Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others},
  journal={arXiv preprint arXiv:1910.04867},
  year={2019}
}

@inproceedings{nilsback_automated_2008,
	address = {Bhubaneswar, India},
	title = {Automated {Flower} {Classification} over a {Large} {Number} of {Classes}},
	url = {http://ieeexplore.ieee.org/document/4756141/},
	doi = {10.1109/ICVGIP.2008.47},
	abstract = {We investigate to what extent combinations of features can improve classiﬁcation performance on a large dataset of similar classes. To this end we introduce a 103 class ﬂower dataset. We compute four different features for the ﬂowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classiﬁer. The weights for each class are learnt using the method of Varma and Ray [16], which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difﬁculty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1\% for the best single feature to 72.8\% for the combination of all features.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {2008 {Sixth} {Indian} {Conference} on {Computer} {Vision}, {Graphics} \& {Image} {Processing}},
	publisher = {IEEE},
	author = {Nilsback, Maria-Elena and Zisserman, Andrew},
	month = dec,
	year = {2008},
	pages = {722--729},
	file = {Nilsback y Zisserman - 2008 - Automated Flower Classification over a Large Numbe.pdf:files/2923/Nilsback y Zisserman - 2008 - Automated Flower Classification over a Large Numbe.pdf:application/pdf},
}

@article{parkhi_cats_2012,
	title = {Cats {And} {Dogs}},
	issn = {1063-6919},
	url = {https://ora.ox.ac.uk/objects/uuid:4f79662d-2e2d-4cc4-92e1-90419eea623b},
	abstract = {We investigate the fine grained object categorization problem of determining the breed of animal from an image. To this end we introduce a new annotated dataset of pets covering 37 different breeds of cats and dogs. The visual problem is very challenging as these animals, particularly cats, are very deformable and there can be quite subtle differences between the breeds. We make a number of contributions: first, we introduce a model to classify a pet breed automatically from an image. The model combines shape, captured by a deformable part model detecting the pet face, and appearance, captured by a bag-of-words model that describes the pet fur. Fitting the model involves automatically segmenting the animal in the image. Second, we compare two classification approaches: a hierarchical one, in which a pet is first assigned to the cat or dog family and then to a breed, and a flat one, in which the breed is obtained directly. We also investigate a number of animal and image orientated spatial layouts. These models are very good: they beat all previously published results on the challenging ASIRRA test (cat vs dog discrimination). When applied to the task of discriminating the 37 different breeds of pets, the models obtain an average accuracy of about 59\%, a very encouraging result considering the difficulty of the problem. © 2012 IEEE.},
	language = {English},
	urldate = {2024-09-11},
	journal = {2012 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)},
	author = {Parkhi, O. and Vedaldi, A. and Zisserman, A. and Jawahar, C. and IEEE},
	year = {2012},
	file = {Snapshot:files/2926/uuid4f79662d-2e2d-4cc4-92e1-90419eea623b.html:text/html},
}

@article{sun_revisiting_2017,
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	url = {http://ieeexplore.ieee.org/document/8237359/},
	doi = {10.1109/ICCV.2017.97},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	urldate = {2024-09-11},
	journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = oct,
	year = {2017},
	note = {Conference Name: 2017 IEEE International Conference on Computer Vision (ICCV)
ISBN: 9781538610329
Place: Venice
Publisher: IEEE},
	pages = {843--852},
	annote = {[TLDR] It is found that the performance on vision tasks increases logarithmically based on volume of training data size, and it is shown that representation learning (or pre-training) still holds a lot of promise.},
	file = {Versión enviada:files/2933/Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in D.pdf:application/pdf},
}

@article{asano_pass_2021,
  title={Pass: An imagenet replacement for self-supervised pretraining without humans},
  author={Asano, Yuki M and Rupprecht, Christian and Zisserman, Andrew and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:2109.13228},
  year={2021}
}


@inproceedings{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  booktitle={International conference on machine learning},
  pages={7324--7334},
  year={2019},
  organization={PMLR}
}

@article{graham_submanifold_2017,
  title={Submanifold sparse convolutional networks},
  author={Graham, Benjamin and Van der Maaten, Laurens},
  journal={arXiv preprint arXiv:1706.01307},
  year={2017}
}

@article{bardes2024revisiting,
  title={Revisiting feature prediction for learning visual representations from video},
  author={Bardes, Adrien and Garrido, Quentin and Ponce, Jean and Chen, Xinlei and Rabbat, Michael and LeCun, Yann and Assran, Mahmoud and Ballas, Nicolas},
  journal={arXiv preprint arXiv:2404.08471},
  year={2024}
}

@article{ba_layer_2016,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@article{naseer2021intriguing,
  title={Intriguing properties of vision transformers},
  author={Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23296--23308},
  year={2021}
}

@article{iyer2023explore,
  title={Wide-minima density hypothesis and the explore-exploit learning rate schedule},
  author={Iyer, Nikhil and Thejas, V and Kwatra, Nipun and Ramjee, Ramachandran and Sivathanu, Muthian},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={65},
  pages={1--37},
  year={2023}
}

@article{pinasthika_sparseswin_2024,
	title = {{SparseSwin}: {Swin} transformer with sparse transformer block},
	volume = {580},
	issn = {0925-2312},
	shorttitle = {{SparseSwin}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231224002042},
	doi = {10.1016/j.neucom.2024.127433},
	abstract = {Advancements in computer vision research have put transformer architecture as the state-of-the-art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the dimension of high-level features to the number of latent tokens. We implemented the SparTa Block within the Swin-T architecture (SparseSwin) to leverage Swin's proficiency in extracting low-level features and enhance its capability to extract information from high-level features while reducing the number of parameters. The proposed SparseSwin model outperforms other state-of-the-art models in image classification with an accuracy of 87.26\%, 97.43\%, and 85.35\% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance. The code is available at https://github.com/KrisnaPinasthika/SparseSwin.},
	urldate = {2024-09-18},
	journal = {Neurocomputing},
	author = {Pinasthika, Krisna and Laksono, Blessius Sheldo Putra and Irsal, Riyandi Banovbi Putera and Shabiyya, Syifa’ Hukma and Yudistira, Novanto},
	month = may,
	year = {2024},
	keywords = {Computer vision, CIFAR10, CIFAR100, Image classification, ImageNet100, Transformer},
	pages = {127433},
	file = {Versión enviada:files/3023/Pinasthika et al. - 2024 - SparseSwin Swin transformer with sparse transform.pdf:application/pdf},
}