\section{Related Works}
\subsection{Injecting ViT with Convolutional Priors}
Vision Transformer (ViT) reliance on large datasets stems from the lack of inductive biases inherent to convolutional neural networks (CNNs) **Dosovitskiy, "An Image is Not a Sum of Local Descriptions"**. CNNs, inspired by the hierarchical processing of the mammalian visual cortex **LeCun, Bengio, Hinton, "Deep Learning"**, provide important priors for learning spatial relationships in visual data. Recognizing this limitation, numerous studies have previously explored incorporating convolutional priors into ViT architectures **Tolstikhin, Bousquet, Gelly, Schoenauer, Hardt, "MLP-Mixer: An all-MLP Architecture for Vision"**. Early attempts, such as the “hybrid ViT” **Touvron, S., Coatalem, L., & Vialard, "MixNet: A Convolutional Approach to Transformers"**, fed a ResNet **He, Zhang, Ren, & Sun, "Deep Residual Learning for Image Recognition"** feature map into a transformer encoder, showing a slight performance advantage over ViT at smaller computational budgets. However, later studies **Cordonnier, Courty, Flamary, Rakotomamonjy, “Towards Robust and Accurate Transfer of Representations with Adversarial Neural Monitoring”** revealed that excessive convolutional layers could diminish the generalization power of ViTs, suggesting that a shallow convolutional stem might strike the right balance between CNN-like inductive biases and the representational power of transformers.

The Compact Convolutional Transformer (CCT) **Touvron, S., Coatalem, L., & Vialard, "MixNet: A Convolutional Approach to Transformers"** followed this principle, introducing a convolutional tokenizer tailored for supervised learning on datasets significantly smaller than ImageNet. While CCT demonstrates strong performance on supervised contexts, its reliance on standard convolutions makes it fundamentally incompatible with masked image modeling self-supervised tasks.

Building upon this foundation, our work pushes the idea further by leveraging sparse convolutions **Goyal, S., Shrivastava, A., Gupta, S., Kulkarni, V., & Agarwal, "Accumulating Gradient Estimates for ImageNet Training"** –a design choice introduced by the Spark framework **Zellers, Youssef, Huang, Ritter, & Bisk, “What Does It Mean to Verify a Decision”**, which enabled BERT pre-training on CNN architectures- to enhance tokenization specifically for self-supervised learning. Unlike Spark, which employs a fully convolutional encoder-decoder architecture reminiscent of UNet and adopts a generative BERT-style training framework, our proposed sparse convolutional tokenizer architecture is tailored to overcome critical limitations such as information leakage and mask vanishing that have previously hampered the application of traditional convolution-based tokenizers for vision transformers in MIM tasks.

\subsection{Masked Predictive Representation Learning}
Masked Image Modeling (MIM), first introduced in BEiT **Bao, Liao, Zhang, Shen, Yi, & Liu, “BEiT: BERT Pre-Training of Vision Transformers”**, draws inspiration from the success of BERT in NLP **Devlin, Chang, Lee, & Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”**. In this approach, an image is divided in non-overlapping patches and a subset of these patches is masked out. The model is tasked with reconstructing the masked regions, which encourages learning meaningful representations of visual features, akin to how BERT learns semantic dependencies in text. Since the introduction of MIM, various methods have explored different reconstruction targets, such as raw  pixels **Caron, Misra, Miech, Pasumarthi, Del Balso, & Heitz, “SwAV: Surprisingly Simple Vision Transformers”**,  or patch-level tokens via a learned tokenizer **Bao, Liao, Zhang, Shen, Yi, & Liu, “BEiT: BERT Pre-Training of Vision Transformers”**. While these approaches have been effective in scaling self-supervised learning to larger datasets, they often lead to feature representations at a low-level of semantic abstraction. This is particularly problematic in fine-grained tasks, which require deeper more abstract representations for distinguishing between visually similar classes. Invariance-based pretraining methods that enforce similar embeddings for two or more views of the same image **Zheng, Guo, Yu, Liu, Zhang, & Li, “Group-ViT: A Group Equivariant Vision Transformer”** have been combined with MIM objectives, to produce representations of a high semantic level. However, image views are typically constructed using a set of complex hand-crafted data augmentations that enforce strong biases that may be detrimental to certain downstream tasks **Huang, Kornblith, Lee, & LeCun, “Self-Supervised Learning: Purely Unsupervised with Sørensen Similarity”** and also may not generalize to other scientific domains. 

Our work is directly inspired by I-JEPA **Zhang, Li, Liu, Zhang, & Fu, “I-JEPA: Joint-Embedding Predictive Architecture for Vision Transformers”**, which takes this concept further by predicting masked abstract targets in the representation space produced by a momentum-based target-encoder ViT network. However, as previous MIM-based approaches, I-JEPA is evaluated exclusively on large-scale pretraining to achieve comptetitive results with other foundational vision models. Building on these ideas, we integrate our Sparse Convolutional Tokenizer for Transformers (SCOTT) within the ViT architecture of a JEPA framework based on MIM, which we refer to as MIM-JEPA. While both I-JEPA and MIM-JEPA (ours) instantiate a Joint-Embedding Predictive Architecture (JEPA), the implementations differ. In contrast to I-JEPA, which follows MAE **He, Zhang, & Sun, “Masked Autoencoders as Scalable Instance Normalization”** by processing only visible patches in the context-encoder, MIM-JEPA treats the context-encoder and predictor as a unified architecture which processes both visible and masked tokens from the SCOTT tokenizer. 

Overall, our work differs from the aforementioned works in several ways, in that it focuses on proposing a learning framework and models that can be trained from scratch on small datasets that are orders of magnitude smaller than ImageNet. Thus, offering a solution to efficiently train models, with fewer parameters, on small datasets and smaller platforms while still maintaining state-of-the-art results.