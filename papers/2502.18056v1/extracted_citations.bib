@inproceedings{Devlin_BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}

@inproceedings{assran2023self,
  title={Self-supervised learning from images with a joint-embedding predictive architecture},
  author={Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15619--15629},
  year={2023}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@inproceedings{chen_visformer_2021,
	title = {Visformer: {The} {Vision}-friendly {Transformer}},
	shorttitle = {Visformer},
	url = {https://ieeexplore.ieee.org/document/9711046},
	doi = {10.1109/ICCV48922.2021.00063},
	abstract = {The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the ‘Vision-friendly Transformer’. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Fitting, Training, Computer vision, Visualization, Computational modeling, Convolutional codes, Protocols, Recognition and classification, Representation learning},
	pages = {569--578},
	file = {IEEE Xplore Abstract Record:files/2863/9711046.html:text/html;Versión enviada:files/2862/Chen et al. - 2021 - Visformer The Vision-friendly Transformer.pdf:application/pdf},
}

@article{dosovitskiy_image_2021,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{fukushima_neocognitron_1988,
	title = {Neocognitron: {A} hierarchical neural network capable of visual pattern recognition},
	volume = {1},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	shorttitle = {Neocognitron},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900147},
	doi = {10.1016/0893-6080(88)90014-7},
	abstract = {A neural network modelfor visual pattern recognition, called the "neocognitron, "' was previously proposed by the author In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism o f the neocognitron.},
	language = {en},
	number = {2},
	urldate = {2024-09-06},
	journal = {Neural Networks},
	author = {Fukushima, Kunihiko},
	month = jan,
	year = {1988},
	pages = {119--130},
	file = {Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf:files/2850/Fukushima - 1988 - Neocognitron A hierarchical neural network capabl.pdf:application/pdf},
}

@inproceedings{graham_levit_2021,
  title={Levit: a vision transformer in convnet's clothing for faster inference},
  author={Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={12259--12269},
  year={2021}
}

@article{hassani_escaping_2022,
  title={Escaping the big data paradigm with compact transformers},
  author={Hassani, Ali and Walton, Steven and Shah, Nikhil and Abuduweili, Abulikemu and Li, Jiachen and Shi, Humphrey},
  journal={arXiv preprint arXiv:2104.05704},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@article{huang_self-supervised_2023,
	title = {Self-supervised learning for medical image classification: a systematic review and implementation guidelines},
	volume = {6},
	issn = {2398-6352},
	shorttitle = {Self-supervised learning for medical image classification},
	url = {https://www.nature.com/articles/s41746-023-00811-0},
	doi = {10.1038/s41746-023-00811-0},
	abstract = {Abstract
            Advancements in deep learning and computer vision provide promising solutions for medical image analysis, potentially improving healthcare and patient outcomes. However, the prevailing paradigm of training deep learning models requires large quantities of labeled training data, which is both time-consuming and cost-prohibitive to curate for medical images. Self-supervised learning has the potential to make significant contributions to the development of robust medical imaging models through its ability to learn useful insights from copious medical datasets without labels. In this review, we provide consistent descriptions of different self-supervised learning strategies and compose a systematic review of papers published between 2012 and 2022 on PubMed, Scopus, and ArXiv that applied self-supervised learning to medical imaging classification. We screened a total of 412 relevant studies and included 79 papers for data extraction and analysis. With this comprehensive effort, we synthesize the collective knowledge of prior work and provide implementation guidelines for future researchers interested in applying self-supervised learning to their development of medical imaging classification models.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {npj Digit. Med.},
	author = {Huang, Shih-Cheng and Pareek, Anuj and Jensen, Malte and Lungren, Matthew P. and Yeung, Serena and Chaudhari, Akshay S.},
	month = apr,
	year = {2023},
	pages = {74},
	file = {Huang et al. - 2023 - Self-supervised learning for medical image classif.pdf:files/2814/Huang et al. - 2023 - Self-supervised learning for medical image classif.pdf:application/pdf},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	doi = {10.1113/jphysiol.1959.sp006308},
	language = {eng},
	number = {3},
	journal = {J Physiol},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pmid = {14403679},
	pmcid = {PMC1363130},
	keywords = {Animals, Cats, Cerebral Cortex, CEREBRAL CORTEX/physiology, Neurons, NEURONS/physiology, Visual Cortex},
	pages = {574--591},
	file = {Texto completo:files/2855/Hubel y Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@inproceedings{liu_sparse_2015,
	title = {Sparse {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/7298681},
	doi = {10.1109/CVPR.2015.7298681},
	abstract = {Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90\% of parameters, with a drop of accuracy that is less than 1\% on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.},
	urldate = {2024-09-10},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Accuracy, Convolutional codes, Kernel, Matrix decomposition, Neural networks, Redundancy, Sparse matrices},
	pages = {806--814},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:files/2884/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf:application/pdf;arXiv.org Snapshot:files/2885/2304.html:text/html},
}

@article{peng_beitv2_2022,
  title={Beit v2: Masked image modeling with vector-quantized visual tokenizers},
  author={Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  journal={arXiv preprint arXiv:2208.06366},
  year={2022}
}

@article{tian_designing_2023,
  title={Designing bert for convolutional networks: Sparse and hierarchical masked modeling},
  author={Tian, Keyu and Jiang, Yi and Diao, Qishuai and Lin, Chen and Wang, Liwei and Yuan, Zehuan},
  journal={arXiv preprint arXiv:2301.03580},
  year={2023}
}

@inproceedings{wu_cvt_2021,
	title = {{CvT}: {Introducing} {Convolutions} to {Vision} {Transformers}},
	shorttitle = {{CvT}},
	url = {https://ieeexplore.ieee.org/document/9710031},
	doi = {10.1109/ICCV48922.2021.00009},
	abstract = {We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Transformers, Convolutional codes, Recognition and classification, Computer architecture, Distortion, Image recognition, Image resolution, Performance gain},
	pages = {22--31},
	file = {IEEE Xplore Abstract Record:files/2868/9710031.html:text/html;Versión enviada:files/2867/Wu et al. - 2021 - CvT Introducing Convolutions to Vision Transforme.pdf:application/pdf},
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={30392--30400},
  year={2021}
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6256--6268},
  year={2020}
}

@inproceedings{xie_simmim_2022,
	title = {{SimMIM}: a {Simple} {Framework} for {Masked} {Image} {Modeling}},
	shorttitle = {{SimMIM}},
	url = {https://ieeexplore.ieee.org/document/9880205},
	doi = {10.1109/CVPR52688.2022.00943},
	abstract = {This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8\% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6\%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1\% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM.},
	urldate = {2024-09-10},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {Training, Computer vision, Representation learning, Data models, Head, Predictive models, Self-\& semi-\& meta- Representation learning, Self-supervised learning},
	pages = {9643--9653},
	file = {IEEE Xplore Abstract Record:files/2902/9880205.html:text/html;Versión enviada:files/2901/Xie et al. - 2022 - SimMIM a Simple Framework for Masked Image Modeli.pdf:application/pdf},
}

@inproceedings{yuan_incorporating_2021,
	title = {Incorporating {Convolution} {Designs} into {Visual} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/9711272},
	doi = {10.1109/ICCV48922.2021.00062},
	abstract = {Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state- of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly 1.},
	urldate = {2024-09-06},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Training, Convolution, Feature extraction, Transformers, Visualization, Recognition and classification, Costs, Efficient training and inference methods, Training data},
	pages = {559--568},
	file = {IEEE Xplore Abstract Record:files/2871/9711272.html:text/html;Versión enviada:files/2870/Yuan et al. - 2021 - Incorporating Convolution Designs into Visual Tran.pdf:application/pdf},
}

@article{zhou2021ibot,
  title={ibot: Image bert pre-training with online tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal={arXiv preprint arXiv:2111.07832},
  year={2021}
}

