\section{Related work}
\subsection{Related Datasets}
% Speech Emotion Recognition (SER) datasets play a pivotal role in developing models capable of understanding and classifying emotions in speech. Existing datasets can be broadly categorized into three main groups based on their modalities and conversational structure.

% The first category consists of datasets focused exclusively on a single modality, such as text. These datasets are valuable for tasks that require emotional inference from written dialogue but are inherently limited in capturing the richness of vocal or visual emotional cues. Notable examples include EmoryNLP \cite{zahiri2018emotion}, EmotionLines \cite{chen1802emotionlines}, and DailyDialog \cite{li2017dailydialog}. While these datasets are useful for text-based emotion recognition tasks, their lack of multi-modal information restricts their applicability to tasks requiring comprehensive emotion analysis across speech and visual modalities.

% The second category includes datasets that combine multiple modalities, such as text and audio or audio and video. These datasets, including CMU-MOSEI \cite{zadeh2018multimodal}, AFEW \cite{fromcollecting}, MEC \cite{li2018mec}, and CH-SIMS \cite{yu2020ch}, provide a richer set of features for emotion recognition. However, they are not conversational in nature and therefore fail to capture the dynamic, context-dependent emotional expressions found in natural dialogue scenarios. This limitation makes them less suitable for building systems aimed at understanding emotions in conversational contexts.

% The final category encompasses datasets that combine conversational dialogues with multi-modal information, making them particularly relevant for dialogue-based SER. Examples include IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed}. These datasets effectively integrate multiple modalities, including text, audio, and video, alongside conversational context. Despite their contributions, they primarily focus on languages such as English and Mandarin, leaving a significant gap for low-resource languages.

% While these datasets have advanced the field of SER, none address the specific needs of low-resource languages, such as Akan, nor do they include prosodic annotations—a critical aspect for emotion recognition in tonal languages. The Akan Cinematic Emotions (ACE) dataset introduced in this work bridges this gap by offering a multi-modal resource that incorporates prosodic labels and conversational data, thereby enabling more robust SER for Akan and contributing to broader cross-cultural SER research. Table \ref{tab:datasets} demonstrates a comparison between ACE and the other SER datasets mentioned within this section.

ERC and Speech Emotion Recognition (SER) datasets are essential for developing models to understand and classify emotions in speech. These datasets can be grouped into three main categories. Table~\ref{tab:datasets} compares ACE with other discussed datasets.

\textbf{Single-modality datasets} focus exclusively on a single modality, such as text, including EmoryNLP \cite{zahiri2018emotion}, EmotionLines \cite{chen1802emotionlines}, and DailyDialog \cite{li2017dailydialog}. These are useful for text-based emotion recognition but lack the multi-modal richness needed for comprehensive analysis involving vocal or visual cues.

\textbf{Multi-modal datasets} combine text, audio, or video, such as CMU-MOSEI \cite{zadeh2018multimodal}, AFEW \cite{fromcollecting}, MEC \cite{li2018mec}, and CH-SIMS \cite{yu2020ch}. While offering richer features, they are not conversational and miss the dynamic, context-dependent expressions seen in natural dialogues.

\textbf{Conversational multi-modal datasets} integrate text, audio, and video with conversational context, such as IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed}. But these datasets mainly focus on high-resource languages, leaving gaps for low-resource languages.

Existing datasets lack resources for low-resource languages, such as Akan, and prosodic annotations critical for tonal languages. ACE fills this gap by offering a multi-modal resource with prosodic labels and conversational data, enabling robust SER for Akan and advancing cross-cultural SER research. 

%Existing datasets lack resources for low-resource languages, such as Akan. ACE introduced here fills this gap by offering a multi-modal resource and conversational data, enabling robust SER for Akan and advancing cross-cultural SER research. Table~\ref{tab:datasets} compares ACE with other discussed datasets.


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\textbf{Dataset} & \textbf{Dialogue} & \textbf{Modalities} & \textbf{Prosodic Annotations} & \textbf{Sources} & \textbf{Mul-label} & \textbf{Emos} & \textbf{Spks} & \textbf{Language} & \textbf{Utts} \\ \hline
EmoryNLP \cite{zahiri2018emotion} & Yes & $t$ & No & Friends TV & Yes & 9 & -- & English & 12,606 \\
EmotionLines \cite{chen1802emotionlines} & Yes & $t$ &  No & Friends TV & No & 7 & -- & English & 29,245 \\
DailyDialog \cite{li2017dailydialog} & Yes & $t$ &  No & Daily & No & 7 & -- & English & 102,979 \\\hline
CMU-MOSEI \cite{zadeh2018multimodal} & No & $a, v, t$ &  No & YouTube & No & 7 & 1000 & English & 23,453 \\
AFEW \cite{fromcollecting} & No & $a, v$ &  No & Movies & No & 7 & 330 & English & 1,645 \\
MEC \cite{li2018mec}& No & $a, v$ &  No & Movies, TVs & No & 8 & -- & Mandarin & 7,030 \\
CH-SIMS \cite{yu2020ch} & No & $a, v, t$ &  No & Movies, TVs & No & 5 & 474 & Mandarin & 2,281 \\\hline
IEMOCAP \cite{busso2008iemocap} & Yes & $a, v, t$ &  No & Act & No & 5 & 10 & English & 7,433 \\
MSP-IMPROV \cite{busso2016msp} & Yes & $a, v, t$ &  No & Act & No & 5 & 12 & English & 8,438 \\
MELD \cite{poria2018meld} & Yes & $a, v, t$ &  No & Friends TV & No & 7 & 407 & English & 13,708 \\
M³ED \cite{zhao2022m3ed} & Yes & $a, v, t$ &  No & 56 TVs & Yes & 7 & 626 & Mandarin & 24,449 \\
\textbf{ACE (Ours)} & Yes & $a, v, t$ & Yes & 21 Movies & No & 7 & 308 & Akan & 6,162 \\\hline
\end{tabular}%
}
\caption{Comparison of existing benchmark datasets. $a, v, t$ refer to audio, visual, and text modalities respectively.}
\label{tab:datasets}
\end{table*}




\begin{comment}
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\textbf{Dataset} & \textbf{Dialogue} & \textbf{Modalities} & \textbf{Sources} & \textbf{Mul-label} & \textbf{Emos} & \textbf{Spks} & \textbf{Language} & \textbf{Utts} \\ \hline
EmoryNLP \cite{zahiri2018emotion} & Yes & $t$ & Friends TV & Yes & 9 & -- & English & 12,606 \\
EmotionLines \cite{chen1802emotionlines} & Yes & $t$ & Friends TV & No & 7 & -- & English & 29,245 \\
DailyDialog \cite{li2017dailydialog} & Yes & $t$ & Daily & No & 7 & -- & English & 102,979 \\\hline
CMU-MOSEI \cite{zadeh2018multimodal} & No & $a, v, t$ & YouTube & No & 7 & 1000 & English & 23,453 \\
AFEW \cite{fromcollecting} & No & $a, v$ & Movies & No & 7 & 330 & English & 1,645 \\
MEC \cite{li2018mec}& No & $a, v$ & Movies, TVs & No & 8 & -- & Mandarin & 7,030 \\
CH-SIMS \cite{yu2020ch} & No & $a, v, t$ & Movies, TVs & No & 5 & 474 & Mandarin & 2,281 \\\hline
IEMOCAP \cite{busso2008iemocap} & Yes & $a, v, t$ & Act & No & 5 & 10 & English & 7,433 \\
MSP-IMPROV \cite{busso2016msp} & Yes & $a, v, t$ & Act & No & 5 & 12 & English & 8,438 \\
MELD \cite{poria2018meld} & Yes & $a, v, t$ & Friends TV & No & 7 & 407 & English & 13,708 \\
M³ED \cite{zhao2022m3ed} & Yes & $a, v, t$ & 56 TVs & Yes & 7 & 626 & Mandarin & 24,449 \\
\textbf{ACE (Ours)} & Yes & $a, v, t$ & 21 Movies & No & 7 & 308 & Akan & 6,162 \\\hline
\end{tabular}%
}
\caption{Comparison with existing benchmark datasets. $a, v, t$ refer to audio, visual, and text modalities respectively.}
\label{tab:datasets}
\end{table*}
\end{comment}


\subsection{Related Methods}
% \cite{poria2017context} addresses the gap in multimodal sentiment analysis by capturing the contextual dependencies between utterances within videos, a factor overlooked by prior research. Utilizing datasets like MOSI, MOUD, and IEMOCAP, the authors develop a Long Short-Term Memory (LSTM)-based framework that incorporates textual, visual, and audio features for utterance-level sentiment classification. The experiments demonstrate that their hierarchical LSTM-based fusion approach achieves a 5-10\% improvement in accuracy over state-of-the-art methods, underscoring the importance of modeling contextual information. While the model generalizes well across datasets, limitations include challenges with cross-lingual generalization, acted datasets like IEMOCAP suffering from bias, and difficulty handling cases with weak sentiment or noisy inputs. The study leaves unexplored the potential of attention mechanisms to further refine the contributions of individual modalities and utterances.

% \cite{hazarika2018conversational} addresses the need to improve emotion recognition in dyadic conversations by incorporating contextual dependencies and inter-speaker emotional influences, a gap in prior research. Using the IEMOCAP dataset, which includes multimodal dyadic interactions with audio, visual, and textual annotations, the authors propose a Conversational Memory Network (CMN). The CMN employs multimodal feature extraction and gated recurrent units (GRUs) to model utterance histories as memory cells, leveraging attention mechanisms to capture relevant self- and inter-speaker influences. Experimental results demonstrate that CMN achieves a 3-4\% improvement in accuracy over state-of-the-art methods, with strong performance in capturing inter-speaker emotional dynamics, particularly in active emotions like anger and happiness. However, the model’s reliance on context history limits its effectiveness in cases with sparse or irrelevant historical utterances, and its generalizability to multi-party dialogues remains unexplored. This paper highlights the importance of memory networks and attention mechanisms for modeling emotional dynamics while paving the way for more sophisticated extensions.


% \cite{majumder2019attentive} addresses the challenge of emotion detection in conversational settings by modeling speaker-specific emotional dynamics, a gap in prior research that often treats all speakers uniformly. The authors experiment on the IEMOCAP and AVEC datasets, which consist of dyadic conversations with multimodal annotations. Their proposed DialogueRNN employs gated recurrent units (GRUs) to separately track global conversation context, individual party states, and speaker emotions, leveraging attention mechanisms to capture inter-utterance and inter-speaker dependencies. Experiments demonstrate that DialogueRNN outperforms state-of-the-art methods like CMN, achieving a 2.77\% improvement in accuracy and 3.76\% in F1 score for IEMOCAP. The model excels in capturing long-term dependencies and nuanced emotional shifts but struggles with ambiguous emotional transitions and classes with subtle distinctions, such as neutral versus excited. The paper highlights the scalability of DialogueRNN to multi-party settings.

% \cite{jiao2019higru} addresses key challenges in emotion recognition within dialogue systems, such as handling contextual variations in word usage, addressing imbalanced emotional classes, and effectively capturing long-range contextual dependencies. Using textual dialogue datasets like IEMOCAP, Friends, and EmotionPush, the authors propose a hierarchical framework (HiGRU) with lower-level GRUs for word-level modeling and upper-level GRUs for utterance-level contextual embeddings. They extend this framework with two variants, HiGRU-f and HiGRU-sf, incorporating individual feature fusion and self-attention mechanisms to enhance long-range context modeling. Experiments demonstrate that HiGRU variants outperform state-of-the-art methods by significant margins (e.g., 8.7\% improvement in weighted accuracy on IEMOCAP), with particularly robust handling of minority emotional classes. However, the reliance on textual features limits the exploration of multimodal dynamics, and addressing data scarcity with semi-supervised methods remains an open challenge, especially for underrepresented emotions and low-resource datasets.

% \cite{zhong2019knowledge} addresses the critical challenge of detecting emotions in conversations by leveraging both contextual information and commonsense knowledge, a gap not thoroughly explored in prior models. It utilizes five datasets—EC, DailyDialog, MELD, EmoryNLP, and IEMOCAP—spanning domains like tweets, TV show scripts, and emotional dialogues. The authors propose the Knowledge-Enriched Transformer (KET), which employs hierarchical self-attention for structured contextual modeling and a context-aware affective graph attention mechanism to dynamically integrate knowledge from ConceptNet and NRC VAD. Experimental results show that KET outperforms state-of-the-art models, achieving superior F1 scores across most datasets, particularly excelling in contexts requiring both context and knowledge integration. However, the model struggles with intrinsically similar emotions (e.g., fear and disgust) and data scarcity for underrepresented emotions, highlighting areas for improvement in emotion-specific and low-resource settings.

% \cite{zhang2019modeling} addresses the challenge of accurately detecting emotions in conversations involving multiple speakers by simultaneously modeling context-sensitive and speaker-sensitive dependencies, a gap often overlooked in previous studies focusing on two-speaker dialogues. Using the Multi-modal EmotionLines Dataset (MELD), the authors propose a novel graph-based convolutional neural network, ConGCN, which represents utterances and speakers as nodes in a graph, with context-sensitive and speaker-sensitive dependencies encoded as edges. The model integrates textual and acoustic features for utterance-level emotion classification. Experiments demonstrate that ConGCN significantly outperforms state-of-the-art baselines, achieving improvements of up to 2.4\% in weighted F1 score, particularly excelling in scenarios requiring speaker-specific contextual modeling. However, the model struggles with underrepresented emotional classes and depends heavily on the availability of annotated data for training.

% \cite{tsai2019multimodal} addresses the challenge of integrating multimodal time-series data, such as text, visual, and acoustic modalities, without requiring explicit alignment, a key limitation of prior approaches. Using datasets like CMU-MOSI, CMU-MOSEI, and IEMOCAP, the authors propose the Multimodal Transformer (MulT), which employs a cross-modal attention mechanism to adaptively fuse features across modalities. This method avoids forced alignment and captures long-range dependencies, outperforming state-of-the-art baselines by 5-15\% in sentiment and emotion classification tasks under both aligned and unaligned scenarios. Despite its superior performance, MulT’s reliance on large datasets and computational resources limits its applicability in low-resource settings.

% \cite{ghosal2019dialoguegcn} addresses the challenges of modeling both inter-speaker dependencies and self-dependencies in conversational emotion recognition, which were insufficiently captured in previous recurrent neural network-based methods. Using the IEMOCAP, AVEC, and MELD datasets, the authors propose DialogueGCN, a graph neural network framework that represents utterances as graph nodes and models speaker interactions and sequential dependencies through labeled edges. DialogueGCN outperforms state-of-the-art models like DialogueRNN, achieving a 2\% improvement in F1 score on IEMOCAP and excelling in scenarios requiring long-range contextual understanding. The approach is particularly effective in multi-party conversations, as demonstrated on the MELD dataset. However, the model’s reliance on textual features alone limits its capability to capture multimodal emotional nuances, and its performance for subtle emotional distinctions remains a challenge. 

% \cite{delbrouck2020transformer} addresses the need for computationally efficient and accurate models for multimodal emotion and sentiment analysis by leveraging transformer architectures, a gap previously underexplored. Using the CMU-MOSEI dataset, which includes textual, acoustic, and visual modalities, the authors propose a joint-encoding transformer-based framework enhanced with modular co-attention and glimpse layers. This model processes linguistic, acoustic, and visual features simultaneously, enabling richer cross-modal interactions. The results demonstrate that the linguistic and acoustic modalities combined yield state-of-the-art performance for sentiment classification, surpassing prior models. However, integrating visual features did not improve performance, highlighting challenges in incorporating visual data effectively. Additionally, the lack of contextual modeling for sequential dependencies limits the model’s performance for complex emotional dynamics, leaving room for improvement in context-aware multimodal emotion recognition.

% \cite{wang2020contextualized} addresses the challenge of accurately modeling emotional dynamics in conversations by incorporating emotional consistency into emotion recognition in conversation (ERC), a gap neglected by prior approaches. Using the IEMOCAP, MELD, and DailyDialogue datasets, the authors propose a model called CESTa that frames ERC as a sequence tagging problem. CESTa employs a combination of Transformer-enhanced LSTM-based global context encoders and individual speaker-specific context encoders to capture long-range inter-utterance and self-dependencies. Additionally, a Conditional Random Field (CRF) layer enforces emotional consistency by leveraging dependencies between adjacent emotion tags. Experiments demonstrate that CESTa achieves state-of-the-art performance on the three datasets, significantly improving on challenging emotions like “excited” and “frustrated.” However, the model’s reliance on entire conversation sequences makes it less suitable for online systems that lack future context. The suggested approaches in this work do not however leverage multimodal data inputs. 

% \cite{hu2021mmgcn} addresses the limitations of previous methods in modeling multimodal and long-distance contextual dependencies in emotion recognition in conversation (ERC), especially in multi-party settings. Using the IEMOCAP and MELD datasets, the authors propose the Multimodal Fused Graph Convolutional Network (MMGCN), which constructs a graph where nodes represent utterances across acoustic, visual, and textual modalities, and edges capture both intra-modal and inter-modal relationships. Speaker embeddings are integrated to enhance the representation of speaker-specific context. Experiments demonstrate that MMGCN achieves state-of-the-art performance, with significant improvements in multimodal settings, surpassing models like DialogueGCN by 1.18\% and 0.42\% weighted F1-score on IEMOCAP and MELD, respectively. However, the model’s reliance on aligned multimodal datasets and computationally intensive graph construction limits its scalability and applicability to real-world noisy or misaligned data. 


% \cite{shen2021dialogxl}  addresses the challenge of adapting pre-trained language models for emotion recognition in multi-party conversations, particularly focusing on modeling long-range dependencies and speaker-specific interactions. The authors evaluate DialogXL on four benchmark datasets—MELD, IEMOCAP, DailyDialog, and EmoryNLP—utilizing textual data exclusively. DialogXL introduces two novel mechanisms: utterance recurrence, which extends memory to store up to 1,000 historical tokens for improved context modeling, and dialog-aware self-attention, incorporating global, local, speaker, and listener attentions to capture nuanced inter- and intra-speaker dependencies. Experimental results demonstrate that DialogXL achieves state-of-the-art performance across all datasets, with significant improvements for long dialogues in IEMOCAP due to its ability to encode historical context effectively. However, its reliance on text-based features limits its multimodal applicability, and the attention-based mechanism can occasionally misattribute emotional relevance, especially in cases of emotional shifts. The approach proposed in this work does not also leverage multimodal information. 

% \cite{kim2021emoberta} addresses the challenge of emotion recognition in conversations (ERC) by introducing a simple yet effective speaker-aware approach that integrates speaker information into RoBERTa’s sequence modeling. Utilizing the MELD and IEMOCAP datasets, which include textual, multimodal, and multi-party conversational data, the authors prepend speaker names to utterances and use separation tokens for context encoding. This setup allows EmoBERTa to capture intra- and inter-speaker emotional dynamics while maintaining computational simplicity. Experiments demonstrate that EmoBERTa achieves state-of-the-art performance, with weighted F1 scores of 65.61\% on MELD and 68.57\% on IEMOCAP, surpassing prior methods without altering RoBERTa’s architecture. However, the model’s reliance on textual modality limits its applicability to multimodal settings, and its performance drops with less expressive speaker annotations, as seen with IEMOCAP’s randomly assigned names. 

% \cite{chudasama2022m2fnet} addresses the challenge of effectively integrating visual, acoustic, and textual modalities for emotion recognition in conversations (ERC), overcoming the limitations of text-only or simplistic multimodal fusion approaches. Using the IEMOCAP and MELD datasets, the authors propose M2FNet, a hierarchical multimodal fusion framework that employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations. The model introduces a novel adaptive margin-based triplet loss for learning deeper emotion-relevant features and incorporates a dual network to jointly leverage scene context and facial features. Experiments demonstrate that M2FNet achieves state-of-the-art performance, surpassing prior methods by up to 2.71\% in weighted F1 score on MELD and 1.83\% on IEMOCAP. Despite its strong performance, the model struggles with highly imbalanced datasets and similar emotional classes, such as “happy” and “excited,” often misclassifying them. 

Conversational emotion recognition (ERC) has evolved through various approaches addressing contextual modeling, multimodal integration, and speaker dependencies. Early works used hierarchical LSTMs \cite{poria2017context} and Conversational Memory Networks (CMN) \cite{hazarika2018conversational} to capture context and inter-speaker influences, improving sentiment classification but struggling with generalization and sparse contexts.

DialogueRNN \cite{majumder2019attentive} and HiGRU \cite{jiao2019higru} refined speaker-specific emotion tracking and attention-based modeling but faced challenges with subtle distinctions and multimodal integration. Knowledge-enriched models \cite{zhong2019knowledge} leveraged commonsense knowledge for emotion detection but struggled with closely related emotions and low-resource settings.

Graph-based methods such as ConGCN \cite{zhang2019modeling} and DialogueGCN \cite{ghosal2019dialoguegcn} modeled multi-speaker dependencies effectively but relied heavily on textual features. Multimodal transformers like MulT \cite{tsai2019multimodal} and MMGCN \cite{hu2021mmgcn} advanced cross-modal fusion but faced scalability issues due to dataset alignment and computational demands.

Recent transformer-based models like DialogXL \cite{shen2021dialogxl} and EmoBERTa \cite{kim2021emoberta} improved ERC with dialog-aware attention and speaker-aware features but lacked multimodal capabilities. M2FNet \cite{chudasama2022m2fnet} addressed multimodal fusion, effectively integrating text, audio, and visual data, though it struggled with imbalanced datasets. Recent methods leverage LLMs to enhance performance, reformulating emotion recognition as a generative task \cite{lei2023instructerc}, incorporating acoustic features \cite{wu2024beyond} and contextual information \cite{xue2024bioserc, fu2024ckerc, zhang2023dialoguellm}.

Despite these advancements, existing methods often lack robust solutions for underrepresented languages and datasets. Our work bridges these gaps by introducing a multimodal dataset and a focus on low-resource settings, enabling more comprehensive and inclusive ERC research.