% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{10.1145/3394171.3413909,
author = {Shen, Guangyao and Wang, Xin and Duan, Xuguang and Li, Hongzhi and Zhu, Wenwu},
title = {MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413909},
doi = {10.1145/3394171.3413909},
abstract = {Humans can perceive subtle emotions from various cues and contexts, even without hearing or seeing others. However, existing video datasets mainly focus on recognizing the emotions of the speakers from complete modalities. In this work, we present the task of multimodal emotion reasoning in videos. Beyond directly recognizing emotions from multimodal signals of target persons, this task requires a machine capable of reasoning about human emotions from the contexts and surrounding world. To facilitate the study towards this task, we introduce a new dataset, MEmoR, that provides fine-grained emotion annotations for both speakers and non-speakers. The videos in MEmoR are collected from TV shows closely in real-life scenarios. In these videos, while speakers may be non-visually described, non-speakers always deliver no audio-textual signals and are often visually inconspicuous. This modality-missing characteristic makes MEmoR a more practical yet challenging testbed for multimodal emotion reasoning. In support of various reasoning behaviors, the proposed MEmoR dataset provides both short-term contexts and external knowledge. We further propose an attention-based reasoning approach to model the intra-personal emotion contexts, inter-personal emotion propagation, and the personalities of different individuals. Experimental results demonstrate that our proposed approach outperforms related baselines significantly. We isolate and analyze the validity of different reasoning modules across various emotions of speakers and non-speakers. Finally, we draw forth several future research directions for multimodal emotion reasoning with MEmoR, aiming to empower high Emotional Quotient (EQ) in modern artificial intelligence systems. The code and dataset released on https://github.com/sunlightsgy/MEmoR.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {493–502},
numpages = {10},
keywords = {reasoning, multimodal, emotion recognition, dataset},
location = {Seattle, WA, USA},
series = {MM '20}
}
@misc{dossou2024advancingafricanaccentedspeechrecognition,
      title={Advancing African-Accented Speech Recognition: Epistemic Uncertainty-Driven Data Selection for Generalizable ASR Models}, 
      author={Bonaventure F. P. Dossou},
      year={2024},
      eprint={2306.02105},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.02105}, 
}
@article{ekman1992there,
  title={Are there basic emotions?},
  author={Ekman, Paul},
  year={1992},
  journal = {Psychological review},
  volume  = {99 (3)},
  publisher={American Psychological Association}
}
@article{fleiss1971,
  author = {Fleiss, Joseph L.},
  title = {Measuring Nominal Scale Agreement Among Many Raters},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {5},
  pages = {378--382},
  year = {1971},
  doi = {10.1037/h0031619}
}

@inproceedings{zahiri2018emotion,
  title={Emotion detection on tv show transcripts with sequence-based convolutional neural networks},
  author={Zahiri, Sayyed M and Choi, Jinho D},
  booktitle={Workshops at the thirty-second aaai conference on artificial intelligence},
  year={2018}
}

@article{chen1802emotionlines,
  title={Emotionlines: An emotion corpus of multi-party conversations. arXiv 2018},
  author={Chen, SY and Hsu, CC and Kuo, CC and Ku, LW},
  journal={arXiv preprint arXiv:1802.08379},
  year={2018}
}

@article{li2017dailydialog,
  title={Dailydialog: A manually labelled multi-turn dialogue dataset},
  author={Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},
  journal={arXiv preprint arXiv:1710.03957},
  year={2017}
}

@inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}

@article{fromcollecting,
  title={Collecting Large, Richly Annotated Facial-Expression Databases from Movies},
  author={Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedoen, Tom},
  publisher={Citeseer},
  journal={IEEE Multimedia},
  pages={34--41},
  year={2012}
}

@inproceedings{li2018mec,
  title={Mec 2017: Multimodal emotion recognition challenge},
  author={Li, Ya and Tao, Jianhua and Schuller, Bj{\"o}rn and Shan, Shiguang and Jiang, Dongmei and Jia, Jia},
  booktitle={2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

@inproceedings{yu2020ch,
  title={Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality},
  author={Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={3718--3727},
  year={2020}
}

@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

@article{busso2016msp,
  title={MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception},
  author={Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and AbdelWahab, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower},
  journal={IEEE Transactions on Affective Computing},
  volume={8},
  number={1},
  pages={67--80},
  year={2016},
  publisher={IEEE}
}

@article{poria2018meld,
  title={Meld: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  journal={arXiv preprint arXiv:1810.02508},
  year={2018}
}

@article{zhao2022m3ed,
  title={M3ED: Multi-modal multi-scene multi-label emotional dialogue database},
  author={Zhao, Jinming and Zhang, Tenggan and Hu, Jingwen and Liu, Yuchen and Jin, Qin and Wang, Xinchao and Li, Haizhou},
  journal={arXiv preprint arXiv:2205.10237},
  year={2022}
}

@inproceedings{poria2017context,
  title={Context-dependent sentiment analysis in user-generated videos},
  author={Poria, Soujanya and Cambria, Erik and Hazarika, Devamanyu and Majumder, Navonil and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={873--883},
  year={2017}
}

@inproceedings{hazarika2018conversational,
  title={Conversational memory network for emotion recognition in dyadic dialogue videos},
  author={Hazarika, Devamanyu and Poria, Soujanya and Zadeh, Amir and Cambria, Erik and Morency, Louis-Philippe and Zimmermann, Roger},
  booktitle={Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  volume={2018},
  pages={2122},
  year={2018},
  organization={NIH Public Access}
}

@article{majumder2019attentive,
  title={An attentive RNN for emotion detection in conversations},
  author={Majumder, N and Poria, S and Hazarika, D and Mihalcea, R and Gelbukh, A and DialogueRNN, E Cambria},
  journal={Association for the Advancement of Artificial Intelligence},
  pages={6818--6825},
  year={2019}
}

@article{ghosal2019dialoguegcn,
  title={Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation},
  author={Ghosal, Deepanway and Majumder, Navonil and Poria, Soujanya and Chhaya, Niyati and Gelbukh, Alexander},
  journal={arXiv preprint arXiv:1908.11540},
  year={2019}
}

@article{hu2021mmgcn,
  title={MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation},
  author={Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin},
  journal={arXiv preprint arXiv:2107.06779},
  year={2021}
}

@inproceedings{shen2021dialogxl,
  title={Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition},
  author={Shen, Weizhou and Chen, Junqing and Quan, Xiaojun and Xie, Zhixian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13789--13797},
  year={2021}
}

@article{jiao2019higru,
  title={Higru: Hierarchical gated recurrent units for utterance-level emotion recognition},
  author={Jiao, Wenxiang and Yang, Haiqin and King, Irwin and Lyu, Michael R},
  journal={arXiv preprint arXiv:1904.04446},
  year={2019}
}

@article{zhong2019knowledge,
  title={Knowledge-enriched transformer for emotion detection in textual conversations},
  author={Zhong, Peixiang and Wang, Di and Miao, Chunyan},
  journal={arXiv preprint arXiv:1909.10681},
  year={2019}
}

@inproceedings{chudasama2022m2fnet,
  title={M2fnet: Multi-modal fusion network for emotion recognition in conversation},
  author={Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4652--4661},
  year={2022}
}

@inproceedings{zhang2019modeling,
  title={Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations.},
  author={Zhang, Dong and Wu, Liangqing and Sun, Changlong and Li, Shoushan and Zhu, Qiaoming and Zhou, Guodong},
  booktitle={IJCAI},
  pages={5415--5421},
  year={2019},
  organization={Macao}
}

@article{delbrouck2020transformer,
  title={A transformer-based joint-encoding for emotion recognition and sentiment analysis},
  author={Delbrouck, Jean-Benoit and Tits, No{\'e} and Brousmiche, Mathilde and Dupont, St{\'e}phane},
  journal={arXiv preprint arXiv:2006.15955},
  year={2020}
}

@article{ekman1992argument,
  title={An argument for basic emotions},
  author={Ekman, Paul},
  journal={Cognition \& emotion},
  volume={6},
  number={3-4},
  pages={169--200},
  year={1992},
  publisher={Taylor \& Francis}
}

@inproceedings{tsai2019multimodal,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the conference. Association for computational linguistics. Meeting},
  volume={2019},
  pages={6558},
  year={2019},
  organization={NIH Public Access}
}

@article{kim2021emoberta,
  title={Emoberta: Speaker-aware emotion recognition in conversation with roberta},
  author={Kim, Taewoon and Vossen, Piek},
  journal={arXiv preprint arXiv:2108.12009},
  year={2021}
}

@inproceedings{wang2020contextualized,
  title={Contextualized emotion recognition in conversation as sequence tagging},
  author={Wang, Yan and Zhang, Jiayu and Ma, Jun and Wang, Shaojun and Xiao, Jing},
  booktitle={Proceedings of the 21th annual meeting of the special interest group on discourse and dialogue},
  pages={186--195},
  year={2020}
}

@article{lei2023instructerc,
  title={Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework},
  author={Lei, Shanglin and Dong, Guanting and Wang, Xiaoping and Wang, Keheng and Wang, Sirui},
  journal={arXiv preprint arXiv:2309.11911},
  year={2023}
}

@article{wu2024beyond,
  title={Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances},
  author={Wu, Zehui and Gong, Ziwei and Ai, Lin and Shi, Pengyuan and Donbekci, Kaan and Hirschberg, Julia},
  journal={arXiv preprint arXiv:2407.21315},
  year={2024}
}

@inproceedings{xue2024bioserc,
  title={BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks},
  author={Xue, Jieying and Nguyen, Minh-Phuong and Matheny, Blake and Nguyen, Le-Minh},
  booktitle={International Conference on Artificial Neural Networks},
  pages={277--292},
  year={2024}
}

@article{fu2024ckerc,
  title={CKERC: Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation},
  author={Fu, Yumeng},
  journal={arXiv preprint arXiv:2403.07260},
  year={2024}
}

@article{zhang2023dialoguellm,
  title={Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations},
  author={Zhang, Yazhou and Wang, Mengyao and Tiwari, Prayag and Li, Qiuchi and Wang, Benyou and Qin, Jing},
  journal={arXiv preprint arXiv:2310.11374},
  year={2023}
}

@misc{radford2022robustspeechrecognitionlargescale,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2212.04356}, 
}

@article{Zhang_2016,
   title={Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
   volume={23},
   ISSN={1558-2361},
   url={http://dx.doi.org/10.1109/LSP.2016.2603342},
   DOI={10.1109/lsp.2016.2603342},
   number={10},
   journal={IEEE Signal Processing Letters},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
   year={2016},
   month=oct, pages={1499–1503} }

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{cao2018vggface2datasetrecognisingfaces,
      title={VGGFace2: A dataset for recognising faces across pose and age}, 
      author={Qiong Cao and Li Shen and Weidi Xie and Omkar M. Parkhi and Andrew Zisserman},
      year={2018},
      eprint={1710.08092},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1710.08092}, 
}

@article{cole2017crowd,
  title={Crowd-sourcing prosodic annotation},
  author={Cole, Jennifer and Mahrt, Timothy and Roy, Joseph},
  journal={Computer Speech \& Language},
  volume={45},
  pages={300--325},
  year={2017},
  publisher={Elsevier}
}

@misc{zhou2021graphneuralnetworksreview,
      title={Graph Neural Networks: A Review of Methods and Applications}, 
      author={Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
      year={2021},
      eprint={1812.08434},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.08434}, 
}

@article{poria2019emotion,
  title={Emotion recognition in conversation: Research challenges, datasets, and recent advances},
  author={Poria, Soujanya and Majumder, Navonil and Mihalcea, Rada and Hovy, Eduard},
  journal={IEEE access},
  volume={7},
  pages={100943--100953},
  year={2019},
  publisher={IEEE}
}

@article{fragopanagos2005emotion,
  title={Emotion recognition in human--computer interaction},
  author={Fragopanagos, Nickolaos and Taylor, John G},
  journal={Neural Networks},
  volume={18},
  number={4},
  pages={389--405},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{danieli2015emotion,
  title={Emotion unfolding and affective scenes: A case study in spoken conversations},
  author={Danieli, Morena and Riccardi, Giuseppe and Alam, Firoj},
  booktitle={Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies},
  pages={5--11},
  year={2015}
}

@inproceedings{ringeval2018avec,
  title={AVEC 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition},
  author={Ringeval, Fabien and Schuller, Bj{\"o}rn and Valstar, Michel and Cowie, Roddy and Kaya, Heysem and Schmitt, Maximilian and Amiriparian, Shahin and Cummins, Nicholas and Lalanne, Denis and Michaud, Adrien and others},
  booktitle={Proceedings of the 2018 on audio/visual emotion challenge and workshop},
  pages={3--13},
  year={2018}
}

@incollection{leben2018languages,
  title={Languages of the World},
  author={Leben, William R},
  booktitle={Oxford Research Encyclopedia of Linguistics},
  year={2018}
}

@misc{moibrahim2023facts,
  author    = {{Mo Ibrahim Foundation}},
  title     = {{Africa in the World and the World in Africa: Facts \& Figures, April 2023}},
  year      = {2023},
  url       = {https://mo.ibrahim.foundation/sites/default/files/2023-04/2023-facts-figures-global-africa.pdf},
  note      = {Accessed: 15-Feb-2025}
}

@book{lctlresources,
  author    = {Angeline Peterson and Danya Al-Saleh and Sam Allen and Alex Fochios and Olivia Mulford and Kaden Paulson-Smith and Lauren Parnell Marino},
  editor    = {Katrina Daly Thompson, Adeola Agoke},
  title     = {Resources for Self-Instructional Learners of Less Commonly Taught Languages},
  year      = {n.d.},
  url       = {https://wisc.pb.unizin.org/lctlresources/front-matter/introduction/},
  note      = {Accessed: 15-Feb-2025}
}


@inproceedings{gong21b_interspeech,
  title     = {AST: Audio Spectrogram Transformer},
  author    = {Yuan Gong and Yu-An Chung and James Glass},
  year      = {2021},
  booktitle = {Interspeech 2021},
  pages     = {571--575},
  doi       = {10.21437/Interspeech.2021-698},
  issn      = {2958-1796},
}