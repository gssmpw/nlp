% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{float} 
\usepackage{verbatim}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}

\def\authornote#1#2#3{{\textcolor{#2}{\textsl{\small#1:[#3]}}}}
\newcommand{\lin}[1]{\authornote{Lin}{cyan}{#1}}
\newcommand\todo[1]{\textcolor{red}{\textbf{TODO: #1}}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset\\for Emotion Recognition in Movie Dialogues}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


% \author{David Sasu \and Zehui Wu \and Ziwei Gong \and Lin Ai \and Run Chen \and Pengyuan Shi \and Julia Hirschberg \and Natalie Schluter\\
%         IT University of Copenhagen \\ Columbia University \\ Apple Machine Learning Research}




\author{
 \textbf{David Sasu\textsuperscript{1}},
 \textbf{Zehui Wu\textsuperscript{2}},
 \textbf{Ziwei Gong\textsuperscript{2}},
 \textbf{Run Chen\textsuperscript{2}},
 \textbf{Pengyuan Shi\textsuperscript{2}},
 \\
 \textbf{Lin Ai\textsuperscript{2}},
 \textbf{Julia Hirschberg\textsuperscript{2}},
 \textbf{Natalie Schluter\textsuperscript{1}}
\\
 \textsuperscript{1}IT University of Copenhagen,
 \textsuperscript{2}Columbia University
 %\textsuperscript{3}Apple Machine Learning Research
}
% \\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
% }

\begin{document}
\maketitle
\begin{abstract}
% In this paper, we introduce a multimodal dataset for speech emotion detection that contains audio, visual, and textual modalities, as well as prosodic labels, which can be used in the development of robust speech emotion recognition systems. This data set was developed for the Akan language, making it, to the best of our knowledge, the first multimodal emotion dialogue dataset for an African language. The presence of prosodic labels in this dataset also makes it to the best of our knowledge the first prosodically annotated African language dataset.

In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset.
We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. 
% This dataset enables investigations into the interplay between text, audio, and prosody and facilitates cross-cultural emotion recognition studies. 
We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.
\end{abstract}


%In this paper, we introduce a multimodal dataset for speech emotion detection that contains audio, visual, and textual modalities, which can be used in the development of robust speech emotion recognition systems. This data set was developed for the Akan language, making it, to the best of our knowledge, the first multimodal emotion dialogue dataset for an African language.

\section{Introduction}
Emotion Recognition in Conversation (ERC) is a rapidly evolving subfield of Natural Language Processing (NLP) that focuses on detecting or classifying the emotional states expressed by speakers in multi-turn conversations \cite{poria2019emotion}. Unlike traditional emotion recognition tasks that aim to identify emotions from isolated text or speech snippets or speech utterances such as \cite{zahiri2018emotion}, ERC seeks to leverage contextual cues from prior dialogue, speaker relationships, and conversational flow to infer emotional states more accurately \cite{poria2019emotion}.

In recent years, ERC has garnered significant attention within the NLP community, driven by its growing relevance to a range of real-world applications. Notable examples include empathetic chatbot systems \cite{fragopanagos2005emotion}, call-center dialogue systems \cite{danieli2015emotion}, and mental health support tools \cite{ringeval2018avec}. These systems rely on ERC to capture the evolving emotional dynamics of conversations, enabling more contextually appropriate and emotionally aware responses. Developing robust ERC systems often requires multimodal data integration \cite{poria2018meld}, which is challenging due to the need to jointly model diverse inputs like scene context, discussion topics, conversational history, and speaker personalities \cite{10.1145/3394171.3413909, hazarika-etal-2018-icon, wu-etal-2024-multimodal}. However, comprehensive multimodal ERC dialogue datasets remain scarce, with benchmark resources like IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed} being notable exceptions.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{sample_image.png}
    \caption{An example of a dialogue showing conversational context and emotion labels.}
    \label{fig:example}
    % \vspace{-0.5cm}
\end{figure}
% shortened and moved up: Developing robust ERC systems often necessitates multimodal data integration, as demonstrated by \cite{poria2018meld}. However, multimodal ERC is inherently challenging, as it requires jointly modeling and reasoning over diverse information sources before an accurate emotional assessment of an utterance can be made. This process typically involves incorporating scene context, the topical focus of the discussion, historical conversational exchanges, and sometimes even the personalities of the speakers involved \cite{10.1145/3394171.3413909, hazarika-etal-2018-icon, wu-etal-2024-multimodal}. Despite the importance of multimodal approaches, the availability of comprehensive multimodal ERC dialogue datasets remains limited. Existing benchmark datasets such as IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed} are among the few resources that provide synchronized information across audio, text, and video modalities. 

A major limitation of existing ERC datasets is their focus on high-resource languages, particularly English (IEMOCAP, MSP-IMPROV, MELD) and Chinese (M³ED). This lack of linguistic diversity hinders the development of ERC systems for low-resource languages, especially in Africa. To our knowledge, no multimodal ERC dataset exists for any African language, despite the continent being home to approximately 3,000 of the world’s 7,000 languages \cite{leben2018languages} and 18.3\% of the global population \cite{moibrahim2023facts}.

To address this gap, we introduce the Akan Conversation Emotion (ACE) dataset, a multimodal emotion dialogue dataset for Akan, a major West African language spoken by about 20 million people \cite{lctlresources}. Akan is the most widely spoken language in Ghana, with around 80\% of the population using it as a first or second language, and approximately 44\% identifying as native Akan speakers. It is also natively spoken in parts of Ivory Coast and Togo. The language primarily comprises three main dialects: Asante, Akuapem, and Fante.


ACE contains 385 emotion-labeled dialogues from 21 Akan movies, covering diverse scenes and topics. It includes 6,162 utterances from 308 speakers (155 male, 153 female), ensuring a gender-balanced dataset. As a tonal language, Akan’s prosodic features are crucial for emotion recognition, so ACE includes word-level prosodic prominence annotations to support research on prosody in ERC. Our baseline experiments validate the dataset’s quality and utility for low-resource and cross-cultural emotion recognition research. Our main contributions are:

\begin{itemize}[nosep,topsep=0pt]
    \item[1.] We introduce ACE, the first multimodal emotion dialogue dataset for an African language, enabling cross-cultural emotion research\footnote{Our data and code are available at \href{https://anonymous.4open.science/r/Akan-Cinematic-Emotion-A328}{this anonymous repo}.}.
    \item[2.] We validate ACE through experiments with state-of-the-art ERC methods, establishing a strong baseline and detailed analysis.
    \item[3.] We provide word-level prosodic prominence annotations, making ACE the first prosodically annotated dataset for an African language, facilitating research on prosody’s role in ERC and tonal language processing.
\end{itemize}



% \begin{itemize}
%     \item We introduce the first multi-modal emotion dialogue dataset for an African language, which is in Akan, called ACE, which can help support more cross-cultural emotion recognition explorations in the field of affective computing.
%     \item We perform a rigorous sanity check of the quality of our proposed dataset by running it through several state-of-the-art methods and the produced experimental results prove its validity, providing a solid baseline.
%     \item We provide word-level prosodic prominence annotations for the utterances within this dataset, making this dataset the first prosodically annotated dataset for an African language. These provided prosodic prominence annotations would not only prove useful for further investigations regarding the explicit role of speech prosody in ERC but will also prove to be useful in the other scientific explorations that involve the investigation of the usefulness of speech prosody in tonal languages.
% \end{itemize}

\section{Related work}

\subsection{Related Datasets}
% Speech Emotion Recognition (SER) datasets play a pivotal role in developing models capable of understanding and classifying emotions in speech. Existing datasets can be broadly categorized into three main groups based on their modalities and conversational structure.

% The first category consists of datasets focused exclusively on a single modality, such as text. These datasets are valuable for tasks that require emotional inference from written dialogue but are inherently limited in capturing the richness of vocal or visual emotional cues. Notable examples include EmoryNLP \cite{zahiri2018emotion}, EmotionLines \cite{chen1802emotionlines}, and DailyDialog \cite{li2017dailydialog}. While these datasets are useful for text-based emotion recognition tasks, their lack of multi-modal information restricts their applicability to tasks requiring comprehensive emotion analysis across speech and visual modalities.

% The second category includes datasets that combine multiple modalities, such as text and audio or audio and video. These datasets, including CMU-MOSEI \cite{zadeh2018multimodal}, AFEW \cite{fromcollecting}, MEC \cite{li2018mec}, and CH-SIMS \cite{yu2020ch}, provide a richer set of features for emotion recognition. However, they are not conversational in nature and therefore fail to capture the dynamic, context-dependent emotional expressions found in natural dialogue scenarios. This limitation makes them less suitable for building systems aimed at understanding emotions in conversational contexts.

% The final category encompasses datasets that combine conversational dialogues with multi-modal information, making them particularly relevant for dialogue-based SER. Examples include IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed}. These datasets effectively integrate multiple modalities, including text, audio, and video, alongside conversational context. Despite their contributions, they primarily focus on languages such as English and Mandarin, leaving a significant gap for low-resource languages.

% While these datasets have advanced the field of SER, none address the specific needs of low-resource languages, such as Akan, nor do they include prosodic annotations—a critical aspect for emotion recognition in tonal languages. The Akan Cinematic Emotions (ACE) dataset introduced in this work bridges this gap by offering a multi-modal resource that incorporates prosodic labels and conversational data, thereby enabling more robust SER for Akan and contributing to broader cross-cultural SER research. Table \ref{tab:datasets} demonstrates a comparison between ACE and the other SER datasets mentioned within this section.

ERC and Speech Emotion Recognition (SER) datasets are essential for developing models to understand and classify emotions in speech. These datasets can be grouped into three main categories. Table~\ref{tab:datasets} compares ACE with other discussed datasets.

\textbf{Single-modality datasets} focus exclusively on a single modality, such as text, including EmoryNLP \cite{zahiri2018emotion}, EmotionLines \cite{chen1802emotionlines}, and DailyDialog \cite{li2017dailydialog}. These are useful for text-based emotion recognition but lack the multi-modal richness needed for comprehensive analysis involving vocal or visual cues.

\textbf{Multi-modal datasets} combine text, audio, or video, such as CMU-MOSEI \cite{zadeh2018multimodal}, AFEW \cite{fromcollecting}, MEC \cite{li2018mec}, and CH-SIMS \cite{yu2020ch}. While offering richer features, they are not conversational and miss the dynamic, context-dependent expressions seen in natural dialogues.

\textbf{Conversational multi-modal datasets} integrate text, audio, and video with conversational context, such as IEMOCAP \cite{busso2008iemocap}, MSP-IMPROV \cite{busso2016msp}, MELD \cite{poria2018meld}, and M³ED \cite{zhao2022m3ed}. But these datasets mainly focus on high-resource languages, leaving gaps for low-resource languages.

Existing datasets lack resources for low-resource languages, such as Akan, and prosodic annotations critical for tonal languages. ACE fills this gap by offering a multi-modal resource with prosodic labels and conversational data, enabling robust SER for Akan and advancing cross-cultural SER research. 

%Existing datasets lack resources for low-resource languages, such as Akan. ACE introduced here fills this gap by offering a multi-modal resource and conversational data, enabling robust SER for Akan and advancing cross-cultural SER research. Table~\ref{tab:datasets} compares ACE with other discussed datasets.


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\textbf{Dataset} & \textbf{Dialogue} & \textbf{Modalities} & \textbf{Prosodic Annotations} & \textbf{Sources} & \textbf{Mul-label} & \textbf{Emos} & \textbf{Spks} & \textbf{Language} & \textbf{Utts} \\ \hline
EmoryNLP \cite{zahiri2018emotion} & Yes & $t$ & No & Friends TV & Yes & 9 & -- & English & 12,606 \\
EmotionLines \cite{chen1802emotionlines} & Yes & $t$ &  No & Friends TV & No & 7 & -- & English & 29,245 \\
DailyDialog \cite{li2017dailydialog} & Yes & $t$ &  No & Daily & No & 7 & -- & English & 102,979 \\\hline
CMU-MOSEI \cite{zadeh2018multimodal} & No & $a, v, t$ &  No & YouTube & No & 7 & 1000 & English & 23,453 \\
AFEW \cite{fromcollecting} & No & $a, v$ &  No & Movies & No & 7 & 330 & English & 1,645 \\
MEC \cite{li2018mec}& No & $a, v$ &  No & Movies, TVs & No & 8 & -- & Mandarin & 7,030 \\
CH-SIMS \cite{yu2020ch} & No & $a, v, t$ &  No & Movies, TVs & No & 5 & 474 & Mandarin & 2,281 \\\hline
IEMOCAP \cite{busso2008iemocap} & Yes & $a, v, t$ &  No & Act & No & 5 & 10 & English & 7,433 \\
MSP-IMPROV \cite{busso2016msp} & Yes & $a, v, t$ &  No & Act & No & 5 & 12 & English & 8,438 \\
MELD \cite{poria2018meld} & Yes & $a, v, t$ &  No & Friends TV & No & 7 & 407 & English & 13,708 \\
M³ED \cite{zhao2022m3ed} & Yes & $a, v, t$ &  No & 56 TVs & Yes & 7 & 626 & Mandarin & 24,449 \\
\textbf{ACE (Ours)} & Yes & $a, v, t$ & Yes & 21 Movies & No & 7 & 308 & Akan & 6,162 \\\hline
\end{tabular}%
}
\caption{Comparison of existing benchmark datasets. $a, v, t$ refer to audio, visual, and text modalities respectively.}
\label{tab:datasets}
\end{table*}




\begin{comment}
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\textbf{Dataset} & \textbf{Dialogue} & \textbf{Modalities} & \textbf{Sources} & \textbf{Mul-label} & \textbf{Emos} & \textbf{Spks} & \textbf{Language} & \textbf{Utts} \\ \hline
EmoryNLP \cite{zahiri2018emotion} & Yes & $t$ & Friends TV & Yes & 9 & -- & English & 12,606 \\
EmotionLines \cite{chen1802emotionlines} & Yes & $t$ & Friends TV & No & 7 & -- & English & 29,245 \\
DailyDialog \cite{li2017dailydialog} & Yes & $t$ & Daily & No & 7 & -- & English & 102,979 \\\hline
CMU-MOSEI \cite{zadeh2018multimodal} & No & $a, v, t$ & YouTube & No & 7 & 1000 & English & 23,453 \\
AFEW \cite{fromcollecting} & No & $a, v$ & Movies & No & 7 & 330 & English & 1,645 \\
MEC \cite{li2018mec}& No & $a, v$ & Movies, TVs & No & 8 & -- & Mandarin & 7,030 \\
CH-SIMS \cite{yu2020ch} & No & $a, v, t$ & Movies, TVs & No & 5 & 474 & Mandarin & 2,281 \\\hline
IEMOCAP \cite{busso2008iemocap} & Yes & $a, v, t$ & Act & No & 5 & 10 & English & 7,433 \\
MSP-IMPROV \cite{busso2016msp} & Yes & $a, v, t$ & Act & No & 5 & 12 & English & 8,438 \\
MELD \cite{poria2018meld} & Yes & $a, v, t$ & Friends TV & No & 7 & 407 & English & 13,708 \\
M³ED \cite{zhao2022m3ed} & Yes & $a, v, t$ & 56 TVs & Yes & 7 & 626 & Mandarin & 24,449 \\
\textbf{ACE (Ours)} & Yes & $a, v, t$ & 21 Movies & No & 7 & 308 & Akan & 6,162 \\\hline
\end{tabular}%
}
\caption{Comparison with existing benchmark datasets. $a, v, t$ refer to audio, visual, and text modalities respectively.}
\label{tab:datasets}
\end{table*}
\end{comment}


\subsection{Related Methods}
% \cite{poria2017context} addresses the gap in multimodal sentiment analysis by capturing the contextual dependencies between utterances within videos, a factor overlooked by prior research. Utilizing datasets like MOSI, MOUD, and IEMOCAP, the authors develop a Long Short-Term Memory (LSTM)-based framework that incorporates textual, visual, and audio features for utterance-level sentiment classification. The experiments demonstrate that their hierarchical LSTM-based fusion approach achieves a 5-10\% improvement in accuracy over state-of-the-art methods, underscoring the importance of modeling contextual information. While the model generalizes well across datasets, limitations include challenges with cross-lingual generalization, acted datasets like IEMOCAP suffering from bias, and difficulty handling cases with weak sentiment or noisy inputs. The study leaves unexplored the potential of attention mechanisms to further refine the contributions of individual modalities and utterances.

% \cite{hazarika2018conversational} addresses the need to improve emotion recognition in dyadic conversations by incorporating contextual dependencies and inter-speaker emotional influences, a gap in prior research. Using the IEMOCAP dataset, which includes multimodal dyadic interactions with audio, visual, and textual annotations, the authors propose a Conversational Memory Network (CMN). The CMN employs multimodal feature extraction and gated recurrent units (GRUs) to model utterance histories as memory cells, leveraging attention mechanisms to capture relevant self- and inter-speaker influences. Experimental results demonstrate that CMN achieves a 3-4\% improvement in accuracy over state-of-the-art methods, with strong performance in capturing inter-speaker emotional dynamics, particularly in active emotions like anger and happiness. However, the model’s reliance on context history limits its effectiveness in cases with sparse or irrelevant historical utterances, and its generalizability to multi-party dialogues remains unexplored. This paper highlights the importance of memory networks and attention mechanisms for modeling emotional dynamics while paving the way for more sophisticated extensions.


% \cite{majumder2019attentive} addresses the challenge of emotion detection in conversational settings by modeling speaker-specific emotional dynamics, a gap in prior research that often treats all speakers uniformly. The authors experiment on the IEMOCAP and AVEC datasets, which consist of dyadic conversations with multimodal annotations. Their proposed DialogueRNN employs gated recurrent units (GRUs) to separately track global conversation context, individual party states, and speaker emotions, leveraging attention mechanisms to capture inter-utterance and inter-speaker dependencies. Experiments demonstrate that DialogueRNN outperforms state-of-the-art methods like CMN, achieving a 2.77\% improvement in accuracy and 3.76\% in F1 score for IEMOCAP. The model excels in capturing long-term dependencies and nuanced emotional shifts but struggles with ambiguous emotional transitions and classes with subtle distinctions, such as neutral versus excited. The paper highlights the scalability of DialogueRNN to multi-party settings.

% \cite{jiao2019higru} addresses key challenges in emotion recognition within dialogue systems, such as handling contextual variations in word usage, addressing imbalanced emotional classes, and effectively capturing long-range contextual dependencies. Using textual dialogue datasets like IEMOCAP, Friends, and EmotionPush, the authors propose a hierarchical framework (HiGRU) with lower-level GRUs for word-level modeling and upper-level GRUs for utterance-level contextual embeddings. They extend this framework with two variants, HiGRU-f and HiGRU-sf, incorporating individual feature fusion and self-attention mechanisms to enhance long-range context modeling. Experiments demonstrate that HiGRU variants outperform state-of-the-art methods by significant margins (e.g., 8.7\% improvement in weighted accuracy on IEMOCAP), with particularly robust handling of minority emotional classes. However, the reliance on textual features limits the exploration of multimodal dynamics, and addressing data scarcity with semi-supervised methods remains an open challenge, especially for underrepresented emotions and low-resource datasets.

% \cite{zhong2019knowledge} addresses the critical challenge of detecting emotions in conversations by leveraging both contextual information and commonsense knowledge, a gap not thoroughly explored in prior models. It utilizes five datasets—EC, DailyDialog, MELD, EmoryNLP, and IEMOCAP—spanning domains like tweets, TV show scripts, and emotional dialogues. The authors propose the Knowledge-Enriched Transformer (KET), which employs hierarchical self-attention for structured contextual modeling and a context-aware affective graph attention mechanism to dynamically integrate knowledge from ConceptNet and NRC VAD. Experimental results show that KET outperforms state-of-the-art models, achieving superior F1 scores across most datasets, particularly excelling in contexts requiring both context and knowledge integration. However, the model struggles with intrinsically similar emotions (e.g., fear and disgust) and data scarcity for underrepresented emotions, highlighting areas for improvement in emotion-specific and low-resource settings.

% \cite{zhang2019modeling} addresses the challenge of accurately detecting emotions in conversations involving multiple speakers by simultaneously modeling context-sensitive and speaker-sensitive dependencies, a gap often overlooked in previous studies focusing on two-speaker dialogues. Using the Multi-modal EmotionLines Dataset (MELD), the authors propose a novel graph-based convolutional neural network, ConGCN, which represents utterances and speakers as nodes in a graph, with context-sensitive and speaker-sensitive dependencies encoded as edges. The model integrates textual and acoustic features for utterance-level emotion classification. Experiments demonstrate that ConGCN significantly outperforms state-of-the-art baselines, achieving improvements of up to 2.4\% in weighted F1 score, particularly excelling in scenarios requiring speaker-specific contextual modeling. However, the model struggles with underrepresented emotional classes and depends heavily on the availability of annotated data for training.

% \cite{tsai2019multimodal} addresses the challenge of integrating multimodal time-series data, such as text, visual, and acoustic modalities, without requiring explicit alignment, a key limitation of prior approaches. Using datasets like CMU-MOSI, CMU-MOSEI, and IEMOCAP, the authors propose the Multimodal Transformer (MulT), which employs a cross-modal attention mechanism to adaptively fuse features across modalities. This method avoids forced alignment and captures long-range dependencies, outperforming state-of-the-art baselines by 5-15\% in sentiment and emotion classification tasks under both aligned and unaligned scenarios. Despite its superior performance, MulT’s reliance on large datasets and computational resources limits its applicability in low-resource settings.

% \cite{ghosal2019dialoguegcn} addresses the challenges of modeling both inter-speaker dependencies and self-dependencies in conversational emotion recognition, which were insufficiently captured in previous recurrent neural network-based methods. Using the IEMOCAP, AVEC, and MELD datasets, the authors propose DialogueGCN, a graph neural network framework that represents utterances as graph nodes and models speaker interactions and sequential dependencies through labeled edges. DialogueGCN outperforms state-of-the-art models like DialogueRNN, achieving a 2\% improvement in F1 score on IEMOCAP and excelling in scenarios requiring long-range contextual understanding. The approach is particularly effective in multi-party conversations, as demonstrated on the MELD dataset. However, the model’s reliance on textual features alone limits its capability to capture multimodal emotional nuances, and its performance for subtle emotional distinctions remains a challenge. 

% \cite{delbrouck2020transformer} addresses the need for computationally efficient and accurate models for multimodal emotion and sentiment analysis by leveraging transformer architectures, a gap previously underexplored. Using the CMU-MOSEI dataset, which includes textual, acoustic, and visual modalities, the authors propose a joint-encoding transformer-based framework enhanced with modular co-attention and glimpse layers. This model processes linguistic, acoustic, and visual features simultaneously, enabling richer cross-modal interactions. The results demonstrate that the linguistic and acoustic modalities combined yield state-of-the-art performance for sentiment classification, surpassing prior models. However, integrating visual features did not improve performance, highlighting challenges in incorporating visual data effectively. Additionally, the lack of contextual modeling for sequential dependencies limits the model’s performance for complex emotional dynamics, leaving room for improvement in context-aware multimodal emotion recognition.

% \cite{wang2020contextualized} addresses the challenge of accurately modeling emotional dynamics in conversations by incorporating emotional consistency into emotion recognition in conversation (ERC), a gap neglected by prior approaches. Using the IEMOCAP, MELD, and DailyDialogue datasets, the authors propose a model called CESTa that frames ERC as a sequence tagging problem. CESTa employs a combination of Transformer-enhanced LSTM-based global context encoders and individual speaker-specific context encoders to capture long-range inter-utterance and self-dependencies. Additionally, a Conditional Random Field (CRF) layer enforces emotional consistency by leveraging dependencies between adjacent emotion tags. Experiments demonstrate that CESTa achieves state-of-the-art performance on the three datasets, significantly improving on challenging emotions like “excited” and “frustrated.” However, the model’s reliance on entire conversation sequences makes it less suitable for online systems that lack future context. The suggested approaches in this work do not however leverage multimodal data inputs. 

% \cite{hu2021mmgcn} addresses the limitations of previous methods in modeling multimodal and long-distance contextual dependencies in emotion recognition in conversation (ERC), especially in multi-party settings. Using the IEMOCAP and MELD datasets, the authors propose the Multimodal Fused Graph Convolutional Network (MMGCN), which constructs a graph where nodes represent utterances across acoustic, visual, and textual modalities, and edges capture both intra-modal and inter-modal relationships. Speaker embeddings are integrated to enhance the representation of speaker-specific context. Experiments demonstrate that MMGCN achieves state-of-the-art performance, with significant improvements in multimodal settings, surpassing models like DialogueGCN by 1.18\% and 0.42\% weighted F1-score on IEMOCAP and MELD, respectively. However, the model’s reliance on aligned multimodal datasets and computationally intensive graph construction limits its scalability and applicability to real-world noisy or misaligned data. 


% \cite{shen2021dialogxl}  addresses the challenge of adapting pre-trained language models for emotion recognition in multi-party conversations, particularly focusing on modeling long-range dependencies and speaker-specific interactions. The authors evaluate DialogXL on four benchmark datasets—MELD, IEMOCAP, DailyDialog, and EmoryNLP—utilizing textual data exclusively. DialogXL introduces two novel mechanisms: utterance recurrence, which extends memory to store up to 1,000 historical tokens for improved context modeling, and dialog-aware self-attention, incorporating global, local, speaker, and listener attentions to capture nuanced inter- and intra-speaker dependencies. Experimental results demonstrate that DialogXL achieves state-of-the-art performance across all datasets, with significant improvements for long dialogues in IEMOCAP due to its ability to encode historical context effectively. However, its reliance on text-based features limits its multimodal applicability, and the attention-based mechanism can occasionally misattribute emotional relevance, especially in cases of emotional shifts. The approach proposed in this work does not also leverage multimodal information. 

% \cite{kim2021emoberta} addresses the challenge of emotion recognition in conversations (ERC) by introducing a simple yet effective speaker-aware approach that integrates speaker information into RoBERTa’s sequence modeling. Utilizing the MELD and IEMOCAP datasets, which include textual, multimodal, and multi-party conversational data, the authors prepend speaker names to utterances and use separation tokens for context encoding. This setup allows EmoBERTa to capture intra- and inter-speaker emotional dynamics while maintaining computational simplicity. Experiments demonstrate that EmoBERTa achieves state-of-the-art performance, with weighted F1 scores of 65.61\% on MELD and 68.57\% on IEMOCAP, surpassing prior methods without altering RoBERTa’s architecture. However, the model’s reliance on textual modality limits its applicability to multimodal settings, and its performance drops with less expressive speaker annotations, as seen with IEMOCAP’s randomly assigned names. 

% \cite{chudasama2022m2fnet} addresses the challenge of effectively integrating visual, acoustic, and textual modalities for emotion recognition in conversations (ERC), overcoming the limitations of text-only or simplistic multimodal fusion approaches. Using the IEMOCAP and MELD datasets, the authors propose M2FNet, a hierarchical multimodal fusion framework that employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations. The model introduces a novel adaptive margin-based triplet loss for learning deeper emotion-relevant features and incorporates a dual network to jointly leverage scene context and facial features. Experiments demonstrate that M2FNet achieves state-of-the-art performance, surpassing prior methods by up to 2.71\% in weighted F1 score on MELD and 1.83\% on IEMOCAP. Despite its strong performance, the model struggles with highly imbalanced datasets and similar emotional classes, such as “happy” and “excited,” often misclassifying them. 

Conversational emotion recognition (ERC) has evolved through various approaches addressing contextual modeling, multimodal integration, and speaker dependencies. Early works used hierarchical LSTMs \cite{poria2017context} and Conversational Memory Networks (CMN) \cite{hazarika2018conversational} to capture context and inter-speaker influences, improving sentiment classification but struggling with generalization and sparse contexts.

DialogueRNN \cite{majumder2019attentive} and HiGRU \cite{jiao2019higru} refined speaker-specific emotion tracking and attention-based modeling but faced challenges with subtle distinctions and multimodal integration. Knowledge-enriched models \cite{zhong2019knowledge} leveraged commonsense knowledge for emotion detection but struggled with closely related emotions and low-resource settings.

Graph-based methods such as ConGCN \cite{zhang2019modeling} and DialogueGCN \cite{ghosal2019dialoguegcn} modeled multi-speaker dependencies effectively but relied heavily on textual features. Multimodal transformers like MulT \cite{tsai2019multimodal} and MMGCN \cite{hu2021mmgcn} advanced cross-modal fusion but faced scalability issues due to dataset alignment and computational demands.

Recent transformer-based models like DialogXL \cite{shen2021dialogxl} and EmoBERTa \cite{kim2021emoberta} improved ERC with dialog-aware attention and speaker-aware features but lacked multimodal capabilities. M2FNet \cite{chudasama2022m2fnet} addressed multimodal fusion, effectively integrating text, audio, and visual data, though it struggled with imbalanced datasets. Recent methods leverage LLMs to enhance performance, reformulating emotion recognition as a generative task \cite{lei2023instructerc}, incorporating acoustic features \cite{wu2024beyond} and contextual information \cite{xue2024bioserc, fu2024ckerc, zhang2023dialoguellm}.

Despite these advancements, existing methods often lack robust solutions for underrepresented languages and datasets. Our work bridges these gaps by introducing a multimodal dataset and a focus on low-resource settings, enabling more comprehensive and inclusive ERC research.

\section{ACE Dataset}
We construct the ACE dataset by collecting and annotating dialogues from Akan-language movies, with examples illustrated in Figure \ref{fig:example}. The dataset includes transcriptions with speaker identifications, emotion labels, and word-level prosodic prominence annotations. Table \ref{tab:datasets} compares ACE with other discussed datasets.

\subsection{Data Selection}
The dataset consists of 21 Akan movies that were downloaded from the Internet Archive. To ensure that the movies included within this dataset were of high quality we ensured that each of the movies selected to be a part of the dataset fulfilled the following criteria: (1) the movie must be complete and not truncated in any section, (2) the speech of the actors within the movie should be intelligible, (3) the facial expressions of the actors within movie should be clear. 

\subsection{Annotators and Annotation Process}
% paraphrased for double-blindness: The annotators that were selected for the annotation task were Akan data annotation professionals that were contracted through the Ghana Institute of Linguistics and Bible Translation. 

The annotation task was carried out by Akan data annotation professionals contracted through an institute of linguistics and bible translation in Ghana. The annotators consisted of five men and two women, all native Akan speakers. Of these seven annotators, three were employed to work full-time while the rest worked part-time. One of the full-time annotators opted to annotate seven movies, whereas the other two full-time annotators each chose to annotate five movies. The remaining four part-time annotators annotated one movie each. The movies were randomly assigned to their respective annotators. 

The data annotators recorded the desired data by watching the movies and simultaneously recording the necessary information into Microsoft Excel sheets. All resulting sheets were then collated into one Excel sheet, where all redundant entries were eliminated and annotation errors were corrected. 


\subsection{Text and Speaker Annotation}
Even though there have been recent advances in Akan Automatic Speech Recognition (ASR), most modern Akan ASR systems still generate many recognition errors as a result of the dearth of training data available \cite{dossou2024advancingafricanaccentedspeechrecognition}. As a result of this, all the speech utterances for each movie were manually transcribed by the annotators before any emotion labelling was performed. 

% Also, due to the lack of acoustic models for Akan that could be applied to perform audio alignment to help identify the time stamps associated with each utterance within a movie, the time stamps of each utterance were also tracked and recorded in a manual fashion by the annotators. 

Additionally, due to the lack of acoustic models for Akan that could facilitate audio alignment and automatically generate timestamps for each utterance in a movie, annotators manually tracked and recorded the timestamps for all utterances.

For the speaker annotations, the speaker for each utterance was identified by a unique identifier which consists of a combination of the order in which the speaker first appeared in the current dialogue and their gender. For instance, a possible label that would be assigned to a man who is the first speaker in the current dialogue of a scene within a movie is \texttt{`speaker one man'}.   

To ensure the high quality of the utterance transcriptions, a professional Akan linguist from the same institute was employed to peruse all of the transcriptions provided by the annotators and correct any identified errors.

\subsection{Emotion Annotation}
The emotion demonstrated for each utterance within a dialogue was annotated using one of seven possible emotion labels: \texttt{Sadness, Fear, Anger, Surprise, Disgust, Happy} and \texttt{Neutral}. Six out of these emotions (i.e Sadness, Fear, Anger, Surprise, Happy and Disgust) were proposed by Paul Ekman (\citeyear{ekman1992there}) as the six universal human emotions. Following previous works \cite{poria2018meld,busso2008iemocap,gong-etal-2024-mapping}, a neutral emotion label was added to identify utterances within the dataset that did not carry any pronounced emotional undertone. 

The annotators were instructed to assign emotion labels to each utterance while simultaneously viewing their assigned movies. To ensure the accuracy and reliability of annotations, a preliminary information session was held by a research coordinator at the aforementioned institute in Ghana. This session provided a comprehensive overview of the annotation task, clarified expectations, and included illustrative examples of how each target emotion might manifest in various scenarios. In cases of uncertainty, annotators were guided to select the emotion label they deemed most appropriate for the utterance. The emotion annotation tutorial was designed with inspiration and reference to established emotion annotation guidelines, such as \citet{gong-etal-2024-mapping}.

\subsection{Emotion Annotation Finalization}
Following the preliminary emotion annotation process, two annotators who demonstrated the highest quality in utterance transcriptions were selected to provide second-opinion emotion labels for utterances they had not annotated during the initial labelling phase. After this second round of labelling, the final emotion label for each utterance in the dataset was determined through a majority voting procedure. In cases of inter-annotator disagreement regarding the appropriate emotion label, the final decision was made by an external Akan-speaking consultant, recognized as an expert in Akan Emotion Analysis. Notably, an analysis of inter-annotator agreement yielded an overall Fleiss’ Kappa statistic \cite{fleiss1971} of $k = \pmb{0.488}$ which is comparable to the inter-annotator agreement scores of several other popular high-quality speech emotion datasets such as MELD \cite{poria2018meld} which has a score of 0.43 \cite{poria2018meld}, IEMOCAP which has a score of 0.48 \cite{busso2008iemocap}, MSP-IMPROV which has a score of 0.49 \cite{busso2016msp} and M³ED which has a score of 0.59 \cite{zhao-etal-2022-m3ed}.


\begin{comment}
    \subsection{Prosodic Prominence Annotation}
    The annotation strategy used for prosodic prominence closely mirrored the approach employed for emotion labelling. The same 2 annotators who were responsible for assigning emotion labels to the utterances were selected for this task. Prior to beginning the prosodic prominence annotations, they were provided with detailed instructions outlining the concept of prosodic prominence and the steps involved in performing the task. Additionally, they were presented with examples of prosodic prominence annotations deemed accurate by consulted linguists to ensure a clear understanding of the expectations.
    
    For the annotation task, the annotators were instructed to listen to the audio of each utterance in the dataset and assign a value of 1 to words they deemed prosodically prominent and 0 to words they considered non-prominent. All of the annotations were done in Excel sheets. This approach to prosodic prominence annotation was inspired by a similar approach leveraged in \cite{cole2017crowd}.
    
    \subsection{Prosodic Prominence Annotation Finalization}
    To obtain the final prosodic prominence labels for each word in each utterance, a majority voting strategy was also applied. In the cases where there was inter-annotator disagreement, an expert was consulted to determine the final label to be used. For the prosodic prominence annotation task, an analysis as inter-annotator agreement yielded an overall Fleiss' Kappa statistic \cite{fleiss1971} of k = (calculate value).  
    
\end{comment}

\begin{table}[t]
  \centering
  \resizebox{0.48\textwidth}{!}{%
  \begin{tabular}{lc}
    \hline
    \textbf{General Statistics} & \textbf{Values}\\
    \hline
    Total number of seconds & 87441\\
    Avg. number of seconds per movie & 4163.4\\
    Total number of movies & 21    \\
    Total number of dialogs     & 385         \\
    Total number of words     &    117305      \\
    Total number of utterances     & 6162          \\
    Total number of turns      & 4477            \\
    Number of prominence words     & 37314           \\
    Number of non-prominence words     & 79991           \\
    Average number of turns per dialog     & 11.62           \\
    Average number of utterances per dialog    & 16           \\
    Average number of words per dialog    &     305       \\
    Average utterance length in seconds     & 6.701           \\
    Average number of words per utterance     &    19        \\
    Average duration per dialog in seconds    & 227.1          \\\hline
  \end{tabular}
  }
  \caption{General statistics of the ACE Dataset}
  \label{tab:accents}
\end{table}

\subsection{Prosodic Prominence Annotation}
The annotation strategy used for prosodic prominence closely mirrored the approach employed for emotion labelling. The same two annotators responsible for assigning emotion labels to the utterances were selected for this task. Before starting the prosodic prominence annotations, they received detailed instructions outlining the concept of prosodic prominence and the steps involved in performing the task. Additionally, they were presented with examples of prosodic prominence annotations deemed accurate by consulted linguists to ensure a clear understanding of the expectations.

For the annotation task, the annotators were instructed to listen to the audio of each utterance in the dataset and assign a value of 1 to words they deemed prosodically prominent and 0 to words they considered non-prominent. All annotations were conducted using Excel sheets. This approach to prosodic prominence annotation was inspired by a similar approach leveraged in \citet{cole2017crowd}.

At the time of writing, prosodic prominence labels from one annotator are complete and included in this paper. The second round of annotation is ongoing and will be incorporated into future versions of the dataset to enhance reliability through inter-annotator agreement analysis.


\subsection{Dataset Statistics}
\paragraph{General Dataset Statistics}
Table \ref{tab:accents} presents basic statistics of the Akan Cinematic Emotions (ACE) Dataset. It contains 385 dialogues, 4477 turns and 6162 utterances, which contain an average of 19 words. With respect to prosodic prominence, 37314 words were annotated to be prosodically prominent whereas 79991 words were annotated to be non-prominent. 

\begin{comment}
    With respect to prosodic prominence, \textbf{x} words were annotated to be prosodically prominent whereas \textbf{y} words were annotated to be non-prominent.
\end{comment}


\paragraph{Emotion Distribution}
Table \ref{tab:emotions_dist} illustrates the distribution of emotions in the ACE Dataset. Neutral emotion had the highest frequency, appearing in 2,941 instances, while Fear had the lowest frequency, occurring only 134 times.
\begin{table}[t]
  \centering
  \resizebox{0.27\textwidth}{!}{%
  \begin{tabular}{lc}
    \hline
    \textbf{Emotion Labels} & \textbf{Values} \\
    \hline
    Neutral    & 2941           \\
    Sadness     & 806           \\
    Anger   &  1107          \\
    Fear   & 134           \\
    Surprise   & 364           \\
    Disgust   & 162          \\
    Happy   & 568           \\\hline
  \end{tabular}
  }
  \caption{Distribution of emotions in the ACE dataset}
  \label{tab:emotions_dist}
\end{table}


\paragraph{Speaker Gender Distribution}
The number of speakers in the ACE dataset is 308, of which 155 are men and 153 are women,  as shown in Table \ref{tab:speakers_dist}. 
\begin{table}[t]
\centering
\resizebox{0.32\textwidth}{!}{%
  \begin{tabular}{lc}
    \hline
    \textbf{Speaker statistics} & \textbf{Values} \\
    \hline
    Number of speakers   & 308         \\
    Number of male speakers     & 155  \\
    Number of female speakers     & 153  \\\hline
  \end{tabular}
  }
  \caption{Distribution of speakers in the ACE Dataset}
  \label{tab:speakers_dist}
\end{table}

\section{Experiments and Analysis}
We conduct a series of experiments to establish baseline performance for emotion recognition on ACE using unimodal and multimodal approaches. We first evaluate text, audio, and vision separately with state-of-the-art models, then explore modality combinations through feature concatenation and transformer-based fusion. These results serve as a foundation for future research on multimodal emotion recognition in Akan.

\subsection{Experiment Setup}
% \lin{Would be interesting to do
% \begin{itemize}
%     \item[1.] Separate in-domain zeroshot/training for each single dataset (e.g. Akan, RAVEDESS, EMODB). Use multilingual pre-trained models to showcase the limitations of current models on Akan
%     \item[2.] Mixed training (e.g. train the models with all datasets combined), and test it on each single dataset. Show that adding the Akan dataset is not only beneficial on its own, but could also help SER on other languages.
% \end{itemize}
% }
Each movie in our dataset is segmented into training, testing, and validation sets using a 7:1.5:1.5 ratio. Following a comprehensive data cleaning process that removed invalid utterances -- specifically those with erroneous timestamps or annotations -- the final dataset comprised 3,888 utterances for training, 816 for validation, and 834 for testing.

Segmentation for both audio and video modalities is based on the timestamps associated with each utterance. The audio recordings, originally sampled at 44 kHz, are resampled to 16 kHz to meet the input requirements of the Whisper \cite{radford2022robustspeechrecognitionlargescale} model. Video frames are extracted at two distinct rates -- 1 frame per second and 5 frames per second -- to evaluate the impact of temporal resolution on emotion detection. Additionally, MTCNN \cite{Zhang_2016} is employed to extract faces from each frame, capturing crucial facial cues essential for effective emotion recognition.

We conduct our experiments on an RTX A6000 GPU. To ensure a reliable assessment of model performance, we use weighted F1 and macro F1 scores instead of accuracy, as the latter can be misleading in imbalanced scenarios.

\begin{table}[b]
\centering
\resizebox{0.37\textwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
    \textbf{Setting} & \textbf{Weighted F1} & \textbf{Macro F1} \\
    \hline
    No Context       & 43.12               & 18.85            \\
    Context          & \textbf{44.58}               & \textbf{22.29}            \\
    \hline
    \end{tabular}
}
\caption{Text-based emotion detection results using the Ghana-NLP/abena-base-asante-twi-uncased model.}
\label{tab:text_results}
\end{table}

\subsection{Text Experiments}

For our text experiments, we employ the Ghana-NLP/abena-base-asante-twi-uncased \cite{alabi-etal-2020-massive} model from Hugging Face, a variant of multilingual BERT (mBERT) fine-tuned specifically for the Akan language. The model is initially trained on the Twi subset of the JW300 \cite{agic-vulic-2019-jw300} dataset, which primarily consists of the Akuapem dialect of Twi, and is later fine-tuned on Asante Twi Bible data to specialize in Asante Twi. To our knowledge, this remains the only available language model trained on this language.

We investigate the impact of context by comparing two settings: one incorporating the previous utterance as context and another without contextual information. Following the context modeling approach from MMML \cite{wu-etal-2024-multimodal}, we process the context and current utterance separately before concatenating their feature representations, rather than simply merging them at the input level. The concatenated features are then passed to the classifier layer. We use a learning rate of 1e-5 and a batch size of 16 in both settings.

As shown in Table \ref{tab:text_results}, our results indicate that incorporating context improves performance. Specifically, the model without context achieves a weighted F1 score of 43.12 and a macro F1 score of 18.85, while the context-aware model yields a weighted F1 score of 44.58 and a macro F1 score of 22.29. These findings highlight the benefits of incorporating contextual information for emotion detection in Akan text.



\subsection{Audio Experiments}
We conduct audio experiments using three different encoding methods: Whisper\footnote{https://huggingface.co/openai/whisper-small}, spectrogram-based features, and openSMILE\footnote{https://audeering.github.io/opensmile-python/}, where Whisper achieves the highest performance (Table \ref{tab:audio_results}). We set the learning rate to 1e-5 and use a batch size of 16 for all three methods.
% Consequently, we adopt the Whisper encoder for the fusion experiments in Section \ref{sec:fusion}.

OpenSMILE features are extracted using the ComParE 2016 feature set, incorporating Low-Level Descriptors (LLDs) and Functionals, resulting in a 130-dimensional feature vector with a maximum sequence length of 3000. These features are then used to train an audio transformer encoder, following the approach of \citet{wu-etal-2024-multimodal}. 
The model consists of three transformer encoder layers, each with two attention heads, and positional encoding, followed by a fully connected linear classifier for prediction.
The model reaches a weighted F1 score of 13.80 and a macro F1 score 6.58. This low performance can be attributed to the absence of pretraining.

Spectrogram features are computed with 128 Mel-frequency bins, normalized, and truncated or padded to a maximum length of 1024 frames. These features are then used to fine-tune a pretrained Audio Spectrogram Transformer (AST) \cite{gong21b_interspeech} with an additional linear classifier layer. The model achieves a  weighted F1 score of 47.89 and a macro F1 score of 23.36. 
We select AST due to its pretraining on a diverse auditory data, encompassing both human speech and non-human sounds, such as music and environmental noises. This broad training enables AST to effectively capture complex acoustic patterns, making it particularly well-suited for our ACE dataset, which consists of movie scenes containing a mix of dialogue, background music, and ambient sounds.

Finally, we fine-tune a Whisper-Small encoder without freezing its parameters, achieving the best performance with a weighted F1 score of 52.38 and a macro F1 score of 29.51. These results indicate that pretraining audio models on multiple languages benefits speech emotion recognition in low-resource target languages. However, due to the imbalance in training samples for Whisper, the improvement remains relatively small. This further underscores the necessity of our dataset collection, as it represents the first multimodal emotion dialogue dataset for an African language, addressing the significant resource gap in emotion recognition research for low-resource languages.

\begin{table}[t]
\centering
\resizebox{0.4\textwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
    \textbf{Model} & \textbf{Weighted F1} & \textbf{Macro F1} \\
    \hline
    openSMILE          & 13.80               & 6.58 \\
    Spectrogram     & 47.89             & 23.36 \\
    Whisper-small       & \textbf{52.38}    & \textbf{29.51} \\
    \hline
    \end{tabular}
}
\caption{Audio-based emotion detection results.}
\label{tab:audio_results}
\end{table}


\subsection{Vision Experiments}

For the vision modality, we explore two main approaches for encoding visual information. First, we use ResNet18 and ResNet50 \cite{he2015deepresiduallearningimage} to extract feature representations from entire video frames. To evaluate the impact of temporal resolution on emotion detection, we experiment with frame sampling rates of 1 frame per second (1 fps) and 5 frames per second (5 fps). In addition, we investigate a face-based approach where faces are extracted from each frame using MTCNN and then encoded with InceptionResnetV1, a model pre-trained on VGGFace2 \cite{cao2018vggface2datasetrecognisingfaces}. All vision experiments are conducted using a learning rate of 1e-4 and a batch size of 16.

\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
    \textbf{Model} & \textbf{Weighted F1} & \textbf{Macro F1} \\
    \hline
    ResNet18-1fps            & 40.57	     & \textbf{20.02} \\
    ResNet50-1fps            & 38.19	          & 15.1 \\
    ResNet18-5fps            & \textbf{42.04}	     & 17.92 \\
    ResNet50-5fps            & 41.76	       & 19 \\
    Inception-Face-5fps  & 39.96	   & 16.53\\
    \hline
    \end{tabular}
}
\caption{Vision-based emotion detection results.}
\label{tab:vision_results}
\end{table}

As shown in Table \ref{tab:vision_results}, our results indicate that ResNet18 with a 5 fps sampling rate achieves the highest weighted F1 score (42.04), suggesting that increasing temporal resolution enhances emotion recognition. However, the highest macro F1 score (20.02) is observed with ResNet18 at 1 fps, indicating that this setting may better capture underrepresented emotion classes. Interestingly, ResNet50, despite being a larger model, does not consistently outperform ResNet18, possibly due to overfitting. Its best weighted F1 score (41.76 at 5 fps) slightly trails that of ResNet18-5fps.

The face-based approach using InceptionResNetV1 underperforms compared to whole-frame models, achieving only 39.96 weighted F1 and 16.53 macro F1, suggesting that facial expressions alone may not provide sufficient information for robust emotion detection in our dataset. Unlike datasets such as CMU-MOSEI \cite{zadeh2018multimodal} that enforce a single visible face in close-up shots, our dataset does not impose such constraints. As a result, videos may contain multiple faces, and the primary speaker’s face may be distant from the camera, adding challenges for models relying solely on facial features.  These findings highlight the importance of frame selection strategies and suggest that balancing temporal resolution with model capacity is crucial for optimal vision-based emotion recognition.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{model.png}
    \caption{Illustration of the transformer fusion model.}
    \label{fig:label}
\end{figure*}

\subsection{Multimodal Experiments}
We evaluate modality combinations using the best-performing unimodal models. Starting with simple feature concatenation as a baseline, we then apply transformer-based fusion to enhance cross-modal interactions. These experiments assess the impact of multimodal integration on emotion recognition.

\label{sec:fusion}
\subsubsection{Modality Features Concatenation}
For the fusion experiments, we evaluate all possible combinations of the three modalities to understand how multimodal integration impacts ERC. We use the best-performing unimodal models: the contextual text model (Ghana-NLP/abena-base-asante-twi-uncased) for text, Whisper-small for audio, and ResNet18-5fps for vision. Feature representations from each modality are concatenated and passed through a classifier layer to compute logits for emotion prediction. To consider distinct characteristics of each modality, we experiment with different learning rates but find that using a single learning rate of 1e-5 yields the most stable results.

\begin{table}[t]
\centering
\resizebox{0.47\textwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
    \textbf{Modality} & \textbf{Weighted F1} & \textbf{Macro F1} \\
    \hline
    Text                        & 44.58 & 22.29 \\
    Audio                       & 52.38	 & 29.51 \\
    Vision                      & 40.57 & 20.02 \\
    Text + Audio                & 55.51	& 30.15 \\
    Text + Vision               & 43.33	 & 21.15 \\
    Audio + Vision              & 53.84 & 30.42 \\
    Text + Audio + Vision       & \textbf{55.81}	 & \textbf{30.97} \\
    \hline
    \end{tabular}
}
\caption{Results of modality concatenation experiments using the best unimodal models.}
\label{tab:modality_results}
\end{table}

Our results in Table \ref{tab:modality_results} show that combining modalities improves emotion recognition performance, with the best results achieved when integrating all three modalities. The multimodal model using text, audio, and vision achieves the highest weighted F1 (55.81) and macro F1 (30.97), outperforming all unimodal and bimodal models. 

Among the unimodal models, audio performs best (52.38 weighted F1, 29.51 macro F1), indicating that speech features carry the most discriminative information for emotion recognition in our dataset. Interestingly, text alone (44.58 weighted F1, 22.29 macro F1) underperforms compared to audio, contrasting with trends in high-resource languages where text embeddings often yield the best results \cite{zadeh2018multimodal,yu2020ch}. This gap is likely due to the limited availability of large and diverse pretraining corpora for Akan, restricting the effectiveness of text embeddings. Vision alone performs worst (40.57 weighted F1), suggesting that visual cues are less reliable, possibly due to variations in facial visibility and camera angles.

In bimodal settings, text + audio (55.51 weighted F1) and audio + vision (53.84 weighted F1) show substantial improvements over their unimodal counterparts, reinforcing the importance of speech information in multimodal emotion recognition. However, text + vision (43.33 weighted F1) provides only a marginal improvement over vision alone, suggesting that textual and visual features may not be as complementary as text and audio. Overall, these results highlight the advantages of multimodal fusion, particularly the strong synergy between textual and auditory features, while also emphasizing the challenges posed by the limited availability of high-quality pretraining data for Akan.

\subsubsection{Transformer Fusion}
To further enhance multimodal fusion, we employ a transformer-based cross-attention encoder to capture interdependencies between different modalities. This approach enables a more nuanced integration of modality-specific features by projecting information from one modality into the representational space of another. Given our bimodal results indicating that text and vision do not complement each other effectively, we structure our fusion process around audio-centric interactions. Specifically, we use a cross-attention encoder to fuse text and audio (audio-text fusion) as well as audio and vision (audio-vision fusion).
% Specifically, we fuse text and audio (audio-text fusion) and audio and vision (audio-vision fusion) using the cross-attention encoder.

In this framework, we first extract features from each modality using the best-performing unimodal models. The cross-attention encoder is designed such that the query comes from one modality while the keys and values are derived from another. This mechanism allows each modality to selectively attend to the most relevant aspects of the other, facilitating effective multimodal alignment. The encoder projects the hidden representations of one modality into the representational space of another, enhancing cross-modal interactions.

To structure the fusion process, we prepend a CLS token to the hidden states of each modality before applying the cross-attention mechanism. After audio-text fusion, we obtain two new hidden representations: $T_{a}$, where text features are projected into the audio space, and $A_{t}$, where audio features are projected into the text space. Similarly, for audio-vision fusion, we obtain $A_{v}$ and $V_{a}$, corresponding to audio projected into the vision space and vice versa. For classification, we use the CLS token from each fused representation as features. Additionally, we incorporate the context feature from the text encoder to enrich the final representation. These features are concatenated and passed through a classifier layer to predict emotion labels.

\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{%
    \begin{tabular}{lcc}
    \hline
    \textbf{Model} & \textbf{Weighted F1} & \textbf{Macro F1} \\
    \hline
    Concatenation           & 55.81     & 30.97\\
    Transformer Fusion            & \textbf{56.13}         & \textbf{31.68} \\
    
    \hline
    \end{tabular}
}
\caption{Results of multimodal fusion experiments.}
\label{tab:fusion_results}
\end{table}

As shown in Table \ref{tab:fusion_results}, Transformer fusion outperforms simple concatenation in both weighted and macro F1 scores, achieving 56.13 and 31.68, respectively. This improvement highlights the effectiveness of advanced fusion mechanisms in integrating multimodal features for emotion recognition. The higher macro F1 score suggests that Transformer Fusion provides better balance across emotion classes, likely due to its ability to capture cross-modal dependencies more effectively. These findings underscore the potential of attention-based fusion techniques for enhancing multimodal ERC, particularly in low-resource settings like Akan.


% \section{Future Directions}
% While our study demonstrates the effectiveness of multimodal fusion for emotion recognition in Akan, several avenues remain open for further exploration. One promising direction is improving audio modeling by applying denoising techniques to mitigate the impact of background noise and music, which could enhance the clarity of speech features. Additionally, using larger Whisper models or audio models specifically pre-trained on African languages could further boost performance by capturing linguistic and prosodic nuances unique to Akan.

% For vision-based modeling, future research could explore vision-language models capable of generating scene descriptions, providing richer contextual information beyond facial expressions. This approach could be particularly beneficial given the variability in facial visibility and the presence of multiple speakers in our dataset. 

% Another crucial direction is the adoption of more advanced fusion techniques to better integrate information from different modalities. Graph neural networks (GNNs) \cite{zhou2021graphneuralnetworksreview} and hypergraphs could be investigated for capturing complex interdependencies between modalities, potentially leading to richer feature representations.

% Future research can enhance multimodal emotion recognition in Akan by improving audio modeling through denoising techniques to reduce background noise and leveraging larger Whisper models or audio models pre-trained on African languages for better linguistic representation. For vision-based modeling, incorporating vision-language models to generate scene descriptions could provide richer contextual cues beyond facial expressions. Additionally, exploring advanced fusion techniques, such as graph neural networks (GNNs) and hypergraphs, may further improve cross-modal integration by capturing complex interdependencies between modalities.

\section{Conclusion and Future Directions}
We introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the resource gap in ERC research for low-resource languages. ACE comprises 385 emotion-labeled dialogues and word-level prosodic prominence annotations, making it a valuable resource for cross-cultural emotion recognition and tonal language prosody research. Our experiments with state-of-the-art ERC methods validate ACE’s quality and establish a strong baseline for future research.

% These results highlight ACE as a valuable resource for investigating the interplay between text, audio, and prosody, and for enabling ERC research in culturally and linguistically diverse settings. Future work includes expanding to additional African languages, and leveraging ACE to develop culturally adaptive ERC systems. We hope ACE inspires further research on inclusive and linguistically diverse NLP resources.

Looking ahead, we aim to expand to additional African languages and develop culturally adaptive ERC systems. Multimodal emotion recognition can be improved by speech enhancement techniques and pretraining models on African languages. Integrating vision-language models for scene descriptions can also provide richer context. Advanced fusion techniques like graph neural networks (GNNs) and hypergraphs may further refine cross-modal integration. We hope ACE inspires further research toward culturally adaptive, linguistically diverse NLP resources.

\section*{Limitations}

While the Akan Cinematic Emotions (ACE) dataset represents a significant advancement in multimodal emotion recognition research, particularly for African languages, there are several limitations to acknowledge.

One limitation of this work is that the dataset focuses exclusively on the Akan language. While this contributes to the representation of low-resource languages in emotion recognition research, the findings may not generalize to other African languages or cultural contexts without further adaptation and testing. The emotional expressions and prosodic characteristics in Akan may differ substantially from those in other languages, limiting cross-linguistic applicability.

Another limitation lies in the domain of the dataset, which is derived from movie dialogues. While this ensures the presence of diverse emotions and rich multimodal interactions, it is likely that a portion of the data contains acted emotions rather than naturally occurring ones. Acted emotions may differ in intensity, expression, and prosodic features from emotions encountered in real-world scenarios, potentially introducing a bias in models trained on this dataset. This could affect the generalizability of such models to real-life applications, where emotional expressions might be less exaggerated or contextually different.

Additionally, while the inclusion of prosodic annotations is a novel feature, the labelling process may be subject to subjective interpretations, particularly for ambiguous emotional expressions. The quality and consistency of these annotations could impact the performance of models relying on prosodic features. Further efforts to standardize prosodic annotation practices would benefit future iterations of this dataset.

Another challenge is related to visual data. Although the dataset incorporates visual modalities, the quality and consistency of visual features in movie dialogues may vary due to differences in lighting, camera angles, and actor positioning. These variations could impact the reliability of visual emotion recognition models trained on this dataset. Moreover, further exploration of vision features, including fine-tuned embeddings and advanced visual annotations, may reveal additional insights but was not the focus of this study.

Despite these limitations, we believe that ACE provides an essential foundation for advancing speech emotion recognition in low-resource languages and encourages further exploration in this area.

\section*{Ethical Considerations}
% similarly for ethics sections
The potential for misuse of the ACE dataset must be carefully acknowledged. While the dataset is intended for research purposes, deploying models trained on ACE in real-world applications without proper domain adaptation and validation could result in inaccurate emotion predictions, particularly in scenarios that deviate from cinematic dialogues. As such, researchers and practitioners should exercise caution when extending the use of this dataset to other applications.

% \section*{Acknowledgments}
% not to include in review version


% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{custom}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
