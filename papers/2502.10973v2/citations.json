[
  {
    "index": 0,
    "papers": [
      {
        "key": "zahiri2018emotion",
        "author": "Zahiri, Sayyed M and Choi, Jinho D",
        "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen1802emotionlines",
        "author": "Chen, SY and Hsu, CC and Kuo, CC and Ku, LW",
        "title": "Emotionlines: An emotion corpus of multi-party conversations. arXiv 2018"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2017dailydialog",
        "author": "Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi",
        "title": "Dailydialog: A manually labelled multi-turn dialogue dataset"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zadeh2018multimodal",
        "author": "Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe",
        "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "fromcollecting",
        "author": "Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedoen, Tom",
        "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2018mec",
        "author": "Li, Ya and Tao, Jianhua and Schuller, Bj{\\\"o}rn and Shan, Shiguang and Jiang, Dongmei and Jia, Jia",
        "title": "Mec 2017: Multimodal emotion recognition challenge"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yu2020ch",
        "author": "Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng",
        "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "busso2008iemocap",
        "author": "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",
        "title": "IEMOCAP: Interactive emotional dyadic motion capture database"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "busso2016msp",
        "author": "Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and AbdelWahab, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower",
        "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "poria2018meld",
        "author": "Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada",
        "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhao2022m3ed",
        "author": "Zhao, Jinming and Zhang, Tenggan and Hu, Jingwen and Liu, Yuchen and Jin, Qin and Wang, Xinchao and Li, Haizhou",
        "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zahiri2018emotion",
        "author": "Zahiri, Sayyed M and Choi, Jinho D",
        "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen1802emotionlines",
        "author": "Chen, SY and Hsu, CC and Kuo, CC and Ku, LW",
        "title": "Emotionlines: An emotion corpus of multi-party conversations. arXiv 2018"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2017dailydialog",
        "author": "Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi",
        "title": "Dailydialog: A manually labelled multi-turn dialogue dataset"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zadeh2018multimodal",
        "author": "Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe",
        "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "fromcollecting",
        "author": "Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedoen, Tom",
        "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2018mec",
        "author": "Li, Ya and Tao, Jianhua and Schuller, Bj{\\\"o}rn and Shan, Shiguang and Jiang, Dongmei and Jia, Jia",
        "title": "Mec 2017: Multimodal emotion recognition challenge"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yu2020ch",
        "author": "Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng",
        "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "busso2008iemocap",
        "author": "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",
        "title": "IEMOCAP: Interactive emotional dyadic motion capture database"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "busso2016msp",
        "author": "Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and AbdelWahab, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower",
        "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "poria2018meld",
        "author": "Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada",
        "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhao2022m3ed",
        "author": "Zhao, Jinming and Zhang, Tenggan and Hu, Jingwen and Liu, Yuchen and Jin, Qin and Wang, Xinchao and Li, Haizhou",
        "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "zahiri2018emotion",
        "author": "Zahiri, Sayyed M and Choi, Jinho D",
        "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "chen1802emotionlines",
        "author": "Chen, SY and Hsu, CC and Kuo, CC and Ku, LW",
        "title": "Emotionlines: An emotion corpus of multi-party conversations. arXiv 2018"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "li2017dailydialog",
        "author": "Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi",
        "title": "Dailydialog: A manually labelled multi-turn dialogue dataset"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "zadeh2018multimodal",
        "author": "Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe",
        "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "fromcollecting",
        "author": "Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedoen, Tom",
        "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "li2018mec",
        "author": "Li, Ya and Tao, Jianhua and Schuller, Bj{\\\"o}rn and Shan, Shiguang and Jiang, Dongmei and Jia, Jia",
        "title": "Mec 2017: Multimodal emotion recognition challenge"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "yu2020ch",
        "author": "Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng",
        "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "busso2008iemocap",
        "author": "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",
        "title": "IEMOCAP: Interactive emotional dyadic motion capture database"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "busso2016msp",
        "author": "Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and AbdelWahab, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower",
        "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "poria2018meld",
        "author": "Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada",
        "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "zhao2022m3ed",
        "author": "Zhao, Jinming and Zhang, Tenggan and Hu, Jingwen and Liu, Yuchen and Jin, Qin and Wang, Xinchao and Li, Haizhou",
        "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "zahiri2018emotion",
        "author": "Zahiri, Sayyed M and Choi, Jinho D",
        "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "chen1802emotionlines",
        "author": "Chen, SY and Hsu, CC and Kuo, CC and Ku, LW",
        "title": "Emotionlines: An emotion corpus of multi-party conversations. arXiv 2018"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "li2017dailydialog",
        "author": "Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi",
        "title": "Dailydialog: A manually labelled multi-turn dialogue dataset"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "zadeh2018multimodal",
        "author": "Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe",
        "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "fromcollecting",
        "author": "Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedoen, Tom",
        "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "li2018mec",
        "author": "Li, Ya and Tao, Jianhua and Schuller, Bj{\\\"o}rn and Shan, Shiguang and Jiang, Dongmei and Jia, Jia",
        "title": "Mec 2017: Multimodal emotion recognition challenge"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "yu2020ch",
        "author": "Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng",
        "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "busso2008iemocap",
        "author": "Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",
        "title": "IEMOCAP: Interactive emotional dyadic motion capture database"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "busso2016msp",
        "author": "Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and AbdelWahab, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower",
        "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "poria2018meld",
        "author": "Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada",
        "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "zhao2022m3ed",
        "author": "Zhao, Jinming and Zhang, Tenggan and Hu, Jingwen and Liu, Yuchen and Jin, Qin and Wang, Xinchao and Li, Haizhou",
        "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "poria2017context",
        "author": "Poria, Soujanya and Cambria, Erik and Hazarika, Devamanyu and Majumder, Navonil and Zadeh, Amir and Morency, Louis-Philippe",
        "title": "Context-dependent sentiment analysis in user-generated videos"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "hazarika2018conversational",
        "author": "Hazarika, Devamanyu and Poria, Soujanya and Zadeh, Amir and Cambria, Erik and Morency, Louis-Philippe and Zimmermann, Roger",
        "title": "Conversational memory network for emotion recognition in dyadic dialogue videos"
      }
    ]
  },
  {
    "index": 46,
    "papers": [
      {
        "key": "majumder2019attentive",
        "author": "Majumder, N and Poria, S and Hazarika, D and Mihalcea, R and Gelbukh, A and DialogueRNN, E Cambria",
        "title": "An attentive RNN for emotion detection in conversations"
      }
    ]
  },
  {
    "index": 47,
    "papers": [
      {
        "key": "jiao2019higru",
        "author": "Jiao, Wenxiang and Yang, Haiqin and King, Irwin and Lyu, Michael R",
        "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition"
      }
    ]
  },
  {
    "index": 48,
    "papers": [
      {
        "key": "zhong2019knowledge",
        "author": "Zhong, Peixiang and Wang, Di and Miao, Chunyan",
        "title": "Knowledge-enriched transformer for emotion detection in textual conversations"
      }
    ]
  },
  {
    "index": 49,
    "papers": [
      {
        "key": "zhang2019modeling",
        "author": "Zhang, Dong and Wu, Liangqing and Sun, Changlong and Li, Shoushan and Zhu, Qiaoming and Zhou, Guodong",
        "title": "Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations."
      }
    ]
  },
  {
    "index": 50,
    "papers": [
      {
        "key": "tsai2019multimodal",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Multimodal transformer for unaligned multimodal language sequences"
      }
    ]
  },
  {
    "index": 51,
    "papers": [
      {
        "key": "ghosal2019dialoguegcn",
        "author": "Ghosal, Deepanway and Majumder, Navonil and Poria, Soujanya and Chhaya, Niyati and Gelbukh, Alexander",
        "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 52,
    "papers": [
      {
        "key": "delbrouck2020transformer",
        "author": "Delbrouck, Jean-Benoit and Tits, No{\\'e} and Brousmiche, Mathilde and Dupont, St{\\'e}phane",
        "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis"
      }
    ]
  },
  {
    "index": 53,
    "papers": [
      {
        "key": "wang2020contextualized",
        "author": "Wang, Yan and Zhang, Jiayu and Ma, Jun and Wang, Shaojun and Xiao, Jing",
        "title": "Contextualized emotion recognition in conversation as sequence tagging"
      }
    ]
  },
  {
    "index": 54,
    "papers": [
      {
        "key": "hu2021mmgcn",
        "author": "Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin",
        "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 55,
    "papers": [
      {
        "key": "shen2021dialogxl",
        "author": "Shen, Weizhou and Chen, Junqing and Quan, Xiaojun and Xie, Zhixian",
        "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition"
      }
    ]
  },
  {
    "index": 56,
    "papers": [
      {
        "key": "kim2021emoberta",
        "author": "Kim, Taewoon and Vossen, Piek",
        "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta"
      }
    ]
  },
  {
    "index": 57,
    "papers": [
      {
        "key": "chudasama2022m2fnet",
        "author": "Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki",
        "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 58,
    "papers": [
      {
        "key": "poria2017context",
        "author": "Poria, Soujanya and Cambria, Erik and Hazarika, Devamanyu and Majumder, Navonil and Zadeh, Amir and Morency, Louis-Philippe",
        "title": "Context-dependent sentiment analysis in user-generated videos"
      }
    ]
  },
  {
    "index": 59,
    "papers": [
      {
        "key": "hazarika2018conversational",
        "author": "Hazarika, Devamanyu and Poria, Soujanya and Zadeh, Amir and Cambria, Erik and Morency, Louis-Philippe and Zimmermann, Roger",
        "title": "Conversational memory network for emotion recognition in dyadic dialogue videos"
      }
    ]
  },
  {
    "index": 60,
    "papers": [
      {
        "key": "majumder2019attentive",
        "author": "Majumder, N and Poria, S and Hazarika, D and Mihalcea, R and Gelbukh, A and DialogueRNN, E Cambria",
        "title": "An attentive RNN for emotion detection in conversations"
      }
    ]
  },
  {
    "index": 61,
    "papers": [
      {
        "key": "jiao2019higru",
        "author": "Jiao, Wenxiang and Yang, Haiqin and King, Irwin and Lyu, Michael R",
        "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition"
      }
    ]
  },
  {
    "index": 62,
    "papers": [
      {
        "key": "zhong2019knowledge",
        "author": "Zhong, Peixiang and Wang, Di and Miao, Chunyan",
        "title": "Knowledge-enriched transformer for emotion detection in textual conversations"
      }
    ]
  },
  {
    "index": 63,
    "papers": [
      {
        "key": "zhang2019modeling",
        "author": "Zhang, Dong and Wu, Liangqing and Sun, Changlong and Li, Shoushan and Zhu, Qiaoming and Zhou, Guodong",
        "title": "Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations."
      }
    ]
  },
  {
    "index": 64,
    "papers": [
      {
        "key": "ghosal2019dialoguegcn",
        "author": "Ghosal, Deepanway and Majumder, Navonil and Poria, Soujanya and Chhaya, Niyati and Gelbukh, Alexander",
        "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 65,
    "papers": [
      {
        "key": "tsai2019multimodal",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Multimodal transformer for unaligned multimodal language sequences"
      }
    ]
  },
  {
    "index": 66,
    "papers": [
      {
        "key": "hu2021mmgcn",
        "author": "Hu, Jingwen and Liu, Yuchen and Zhao, Jinming and Jin, Qin",
        "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 67,
    "papers": [
      {
        "key": "shen2021dialogxl",
        "author": "Shen, Weizhou and Chen, Junqing and Quan, Xiaojun and Xie, Zhixian",
        "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition"
      }
    ]
  },
  {
    "index": 68,
    "papers": [
      {
        "key": "kim2021emoberta",
        "author": "Kim, Taewoon and Vossen, Piek",
        "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta"
      }
    ]
  },
  {
    "index": 69,
    "papers": [
      {
        "key": "chudasama2022m2fnet",
        "author": "Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki",
        "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation"
      }
    ]
  },
  {
    "index": 70,
    "papers": [
      {
        "key": "lei2023instructerc",
        "author": "Lei, Shanglin and Dong, Guanting and Wang, Xiaoping and Wang, Keheng and Wang, Sirui",
        "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework"
      }
    ]
  },
  {
    "index": 71,
    "papers": [
      {
        "key": "wu2024beyond",
        "author": "Wu, Zehui and Gong, Ziwei and Ai, Lin and Shi, Pengyuan and Donbekci, Kaan and Hirschberg, Julia",
        "title": "Beyond silent letters: Amplifying llms in emotion recognition with vocal nuances"
      }
    ]
  },
  {
    "index": 72,
    "papers": [
      {
        "key": "xue2024bioserc",
        "author": "Xue, Jieying and Nguyen, Minh-Phuong and Matheny, Blake and Nguyen, Le-Minh",
        "title": "BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks"
      },
      {
        "key": "fu2024ckerc",
        "author": "Fu, Yumeng",
        "title": "CKERC: Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation"
      },
      {
        "key": "zhang2023dialoguellm",
        "author": "Zhang, Yazhou and Wang, Mengyao and Tiwari, Prayag and Li, Qiuchi and Wang, Benyou and Qin, Jing",
        "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations"
      }
    ]
  }
]