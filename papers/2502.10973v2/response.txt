Early works used hierarchical LSTMs [[1]](https://www.example.com/[1]) and Conversational Memory Networks (CMN) [[2]](https://www.example.com/[2]) to capture context and inter-speaker influences, improving sentiment classification but struggling with generalization and sparse contexts.

DialogueRNN [[3]](https://www.example.com/[3]) and HiGRU [[4]](https://www.example.com/[4]) refined speaker-specific emotion tracking and attention-based modeling but faced challenges with subtle distinctions and multimodal integration. Knowledge-enriched models [[5]](https://www.example.com/[5]) leveraged commonsense knowledge for emotion detection but struggled with closely related emotions and low-resource settings.

Graph-based methods such as ConGCN [[6]](https://www.example.com/[6]) and DialogueGCN [[7]](https://www.example.com/[7]) modeled multi-speaker dependencies effectively but relied heavily on textual features. Multimodal transformers like MulT [[8]](https://www.example.com/[8]) and MMGCN [[9]](https://www.example.com/[9]) advanced cross-modal fusion but faced scalability issues due to dataset alignment and computational demands.

Recent transformer-based models like DialogXL [[10]](https://www.example.com/[10]) and EmoBERTa [[11]](https://www.example.com/[11]) improved ERC with dialog-aware attention and speaker-aware features but lacked multimodal capabilities. M2FNet [[12]](https://www.example.com/[12]) addressed multimodal fusion, effectively integrating text, audio, and visual data, though it struggled with imbalanced datasets. Recent methods leverage LLMs to enhance performance, reformulating emotion recognition as a generative task [[13]](https://www.example.com/[13]), incorporating acoustic features [[14]](https://www.example.com/[14]) and contextual information [[15]](https://www.example.com/[15]).

Despite these advancements, existing methods often lack robust solutions for underrepresented languages and datasets. Our work bridges these gaps by introducing a multimodal dataset and a focus on low-resource settings, enabling more comprehensive and inclusive ERC research.