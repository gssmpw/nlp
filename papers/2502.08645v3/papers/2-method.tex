\section{Background}
\label{sec:background_3d_recon}
\subsection{3D Reconstruction}
The goal of 3D reconstruction is to recover the geometry of 3D scenes from various signals, such as images, point clouds, and more. To this end, the first common step is taking structure-from-motion approaches (e.g., Colmap) to estimate camera poses and sparse point clouds~\citep{schoenberger2016sfm}. 
Once the camera poses are determined, dense point clouds can be obtained through multi-view stereo (MVS) techniques. These include traditional methods~\citep{schoenberger2016mvs}, as well as network-based approaches~\citep{yao2018mvsnet}. This process effectively enables the reconstruction of geometry from images.
Besides, commercial software solutions, such as ARCode and PolyCam, leverage RGB-D SLAM methods to simultaneously estimate camera poses and depth information, facilitating the recovery of 3D models. These software solutions often include some post-processing techniques, such as smoothing and inpainting, together with user-friendly GUIs, enhancing the usability and quality of the reconstructed 3D models.

% In our method, we primarily utilize image sequences to reconstruct real-world environments. 
% Specifically, given a set of images with unknown poses, our goal is to apply algorithms that recover their corresponding image poses 3D structures.

% The most commonly used approach is structure-from-motion (SfM) techniques, such as COLMAP, which leverage correspondences within image sequences to recover image poses and generate sparse point clouds. 
% While these methods are robust for pose recovery, the resulting sparse point clouds are challenging to directly integrate into traditional simulation pipelines.
% To obtain dense geometric structures, multi-view stereo (MVS) techniques are more suitable. 
% These methods utilize stereo correspondences and image poses to reconstruct detailed meshes. 
% However, the accuracy of purely image-based reconstruction methods is often constrained by noisy poses and correspondence inaccuracies.
% Commercial software solutions, such as ARCode and PolyCam, address these limitations by employing RGB-D SLAM methods, which estimate camera poses and depth information simultaneously. 
% These methods provide more reliable reconstructions by combining depth data with pose estimation.

\subsection{Rendering}
Rendering involves generating photorealistic images from specified camera views based on either explicit or implicit 3D representations. 
Implicit methods, like Neural Radiance Fields (NeRF)~\citep{mildenhall2021nerf}, represent the 3D space using a density field encoded in a neural network and render images through volumetric rendering techniques. 
Explicit representations, such as meshes and point clouds, use texture mapping~\cite{Blinn1976Texture} for rendering and synthesizing novel views. Another recent progress in explicit representation is Gaussian primitives, which adopt Gaussian rasterization for high-fidelity rendering, also known as 3D Gaussian splatting (3DGS)~\citep{kerbl3Dgaussians}.
% Explicit representations, such as meshes and point clouds, use texture mapping for rendering, while Gaussian primitives employ Gaussian rasterization to produce final images~\citep{kerbl3Dgaussians}. 
Compared with other methods, 3DGS achieves more realistic rendering results and offers higher rendering efficiency.
In detail, each Gaussian primitive is modeled as an ellipse with a full 3D covariance matrix $\Sigma$ defined in world space, centered at point $\mu$:
\begin{equation*}
    G(x) = e^{-\frac{1}{2} x^T \Sigma^{-1} x}~,
\end{equation*}
where the covariance matrix $\Sigma$ of a Gaussian primitive is derived from its rotation $R$ and scaling factor $S$ as:
\begin{equation*}
    \Sigma = R S S^T R^T~.
\end{equation*}
Additionally, each Gaussian is assigned a color \( c_i \) and depth-ordered using the Z-buffer. Using the alpha-blending formula, the rendered color \( C \) for a view is calculated as:
\begin{equation*}
    C = \sum_{i=1}^N \prod_{j<i} \alpha_j (1 - \alpha_i) c_i~.
\end{equation*}

% When obtaining geometry, traditional pipelines often rely on texture mapping and rasterization to render images. However, achieving photorealistic rendering results with these methods requires significant effort, making them less practical for robotic simulation scenarios.

% To address this challenge, methods like Neural Radiance Fields (NeRF) use implicit representations, such as neural radiance fields, to model 3D scenes. 
% These methods can be optimized from sparse posed images and generate rendered views from arbitrary perspectives. 
% However, NeRF-based approaches require sampling hundreds of points along each ray and leveraging volumetric rendering to reconstruct the color of a single ray, which can be computationally intensive.
% For higher rendering quality and efficiency, 3D Gaussian Splatting (3DGS) provides a more effective primitive for representing 3D scenes. 
% 3DGS employs differentiable rasterization techniques to optimize Gaussian primitives, enabling high-quality and efficient photorealistic rendering results.
% In our approach, we recover meshes using ARCode and render images using 3DGS, achieving photorealistic rendering results suitable for simulation scenarios.


%     \subsection{Geometry Reconstruction}
% Image-based 3D geometry reconstruction~\citep{barnes2009patchmatch,galliani2016gipuma,broadhurst2001probabilistic,de1999poxels,kazhdan2013screened,openmvs2020,arcode2022} is the problem to reconstruct the 3D model. 
% Using a set of images with unknown poses, structure-from-motion (SfM) techniques such as COLMAP accurately estimate image poses when significant color variation (features) is present.
% Traditional multi-view stereo(MVS) methods~\citep{barnes2009patchmatch,galliani2016gipuma,broadhurst2001probabilistic,de1999poxels,kazhdan2013screened} can reconstruct the meshes from the images together with the estimated poses. 
% Recently, commercial software such as ARCode~\citep{arcode2022} and Polycam~\citep{polycam2020} enable high-quality textured meshes reconstruction with an iPhone, while Universal Scene Description (USD)~\citep{usd2021} enables robust, scalable interchange, assembly, and editing of 3D scenes across multiple applications. 
% Specifically, USD is a versatile format for describing 3D assets, capable of including meshes of various objects, cameras, and lights, along with metadata in different layers. 
% Polycam can reconstruct the geometry of the scene with a depth sensor on the iPhone and the texture with the iPhone camera. ARCode can reconstruct objects in the USD format, cutting them from the background automatically and inpainting the hidden faces. 

% \subsection{Novel-View Synthesis}
% The other side of 3D reconstruction is novel view synthesis~\citep{mildenhall2021nerf,kerbl3Dgaussians,yu2024gsdf,guedon2024sugar}, which 
% Recent methods leverage neural implicit representations~\citep{mildenhall2021nerf,kerbl3Dgaussians} to generate images from novel views. NeRF~\citep{mildenhall2021nerf} can render high-fidelity images from novel viewpoints given multi-view images, though its rendering speed is limited. 

% \noindent\textbf{3D Gaussian splatting}
% In contrast, methods based on 3D Gaussian\citep{kerbl3Dgaussians,yu2024gsdf,guedon2024sugar} representations enhance rendering efficiency and improve the rendering quality. At the same time, methods like ~\citep{yu2024gsdf,guedon2024sugar} can get the mesh together with 3D Gaussian representations. 


\section{\our}
\label{sec: sim2real}
% background
Physic-based robot simulators, such as Isaac Sim~\cite{isaacsim} (PhysX~\cite{physx}), PyBullet (Bullet)~\cite{pybullet}, Mujoco~\cite{todorov2012mujoco}, and Gazebo~\cite{gazebo} (Bullet, ODE~\cite{ode}, DART~\cite{lee2018dart}, and Simbody~\cite{sherman2011simbody}), offer high-accurate simulation results based on underlying physics engines. However, it's hard to directly obtain an accurate 3D model of the real-world scene, and the simulators are not good at rendering high-fidelity images, leaving a large gap from real-world images. Recent advancements in 3D reconstruction~\citep{mildenhall2021nerf,kerbl3Dgaussians,openmvs2020,yu2024gsdf,guedon2024sugar} provide high-quality rendering results and detailed 3D geometric models, promising to bridge these gaps.
We introduce \our, a 3D reconstruction-based system, to bridge the visual gap and recover their realistic 3D meshes for sim-to-real transfer.

A straightforward approach is to reconstruct objects and the background together. However, using Multi-View Stereo (MVS) methods for background reconstruction often leads to suboptimal visual quality, while techniques like SuGaR ~\citep{guedon2024sugar} struggle with accurately reconstructing flat planes, causing protrusions and indentations that reduce realism in object movement. Additionally, segmenting foreground objects for manipulation is labor-intensive. 
To address these issues, we propose separately reconstructing background meshes for collision estimation and leveraging 3DGS to improve rendering realism, aligning the two. Since object rendering constitutes a small portion of the robot's visual observations, we opt not to use 3DGS to render objects.

\subsection{System Overview}
\our is a real-to-sim-real pipeline that reconstructs meshes for collision estimation, utilizes 3D Gaussian scene representations for rendering, and generates simulation data within a physics-based simulator\footnote{Isaac Sim~\citep{isaacsim} is used by default, but other ray tracing options are also feasible.}. 
It is worth noting that we only consider reconstructing the background and foreground objects, robots are not involved in our system since their accurate 3D assets are usually easily accessible. 
Further, we take this work as an initial study of \our, validated on a table-top robot arm for rigid body tasks.
% Subsequently, the PhysX engine within NVIDIA Isaac Sim accurately estimates the future states of robots and objects. 
% \noindent\textbf{3D reconstruction based simulation.}
As shown in Fig.~\ref{fig:main framework}, \our are composed of four components: 
mesh recovery for object collision computation, hybrid visual rendering combining 3DGS for the background and mesh-based rendering for the foreground, real-world alignment of the reconstructed scene and robot positions, and large-scale data generation for manipulation tasks.


% \begin{enumerate}[label=(\alph*)]
%     \item \textbf{Mesh recovery} (Sec.~\ref{sec:method:mesh_recovery}): reconstructing the mesh geometry of all objects (both foreground and background) included in the scene into the physics-based simulation for collision computation and simulation.
%     \item \textbf{Hybrid visual rendering} (Sec.~\ref{sec:method:rendering}): improving the visual fidelity of the scene via 3DGS. 
%     \item \textbf{Real-world alignment} (Sec.~\ref{sec:method:alignment}): aligning 1) the robot in the real world and the reconstructed scene, 2) the mesh and the Gaussian scene representation in simulation. 
%     \item \textbf{Data generation} (Sec.~\ref{sec:method:data-generation}): large-scale collection on simulation demonstrations of various manipulation skills at ease.
%     % \item \textbf{Deploy} (Sec.~\ref{sec:method:deployment}) ensures successful sim-to-real transfer from . 
% \end{enumerate}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=.9\linewidth]{images/pipeline.pdf}
    \vspace{-5pt}
    \caption{\textbf{Illustration of the proposed real-to-sim-to-real system, \our}. It leverages 3D reconstruction and a physics-based simulator, providing small 3D gaps that enable large-scale simulation data generation for learning manipulation skills via sim-to-real transfer.}
    \label{fig:main framework}
\end{figure*}


\subsection{Mesh Recovery}
\label{sec:method:mesh_recovery}
This step reconstructs realistic objects and background geometries for accurate collision detection and simulation.
% The mesh recovery step is meant for obtaining the realistic geometry information of objects in a scene, by reconstructing the mesh of the background and the objects from photos or a video, and sequentially post-processing the mesh for more accurate physics simulation. 

\noindent\textbf{Mesh reconstruction.} 
The meshes of backgrounds and objects are reconstructed separately for flexibility and are represented in USD for extensibility.
% To integrate backgrounds and objects into a physics simulator, we reconstruct their collision bodies separately. We represent them in USD to ensure high extensibility.
For the background, we first employ SfM techniques (COLMAP) to estimate the pose for each image and get the sparse point cloud based on a set of images, and then utilize OpenMVS~\citep{openmvs2020} for detailed mesh reconstruction. 
PolyCam~\citep{polycam2020} is also able to reconstrct the mesh of the background, but empirically, the geometric reconstruction results of OpenMVS exhibit fewer minor protrusions and depressions compared to those of PolyCam.
Subsequently, we simplify this initial mesh to create collision bodies. This approach effectively captures intricate details while maintaining accurate planar reconstruction. For foreground objects, we employ ARCode~\citep{arcode2022}, which can automatically segment the object, balancing usability and quality.
% providing an optimal balance between usability and reconstruction quality, ensuring efficient and precise representations.

\noindent\textbf{Post-processing.}
Reconstructed collision bodies often have voids or sharp edges due to imperfect image quality, causing erratic behavior in simulation. For example, objects may fail to rest stably on surfaces.
% or exhibit excessive bouncing when dropped from a minimal height.
Void filling and surface smoothing help improve stability and realism in simulation. 
As acquiring physical parameters like mass and friction is challenging with only initial static visual data, we use default values to simplify the process without sacrificing performance and find it works well for many manipulation tasks.
% To mitigate such an issue, we sequentially perform void filling and surface smoothing. 
% These post-processing steps improve stability and realism in simulations. 
% Furthermore, estimating physical parameters, such as mass and friction coefficients, remains challenging when relying solely on visual data. 
% Although approaches like robot-assisted parameter acquisition~\citep{memmel2024asid} provide more accurate parameters, we found that default parameter values suffice for many manipulation tasks, simplifying the process without deteriorating the performance.

\subsection{Hybrid Visual Rendering}
\label{sec:method:rendering}
Color images, as a key type of perception signal, often yield a large visual gap between simulation and the real world.
To close such a gap, we render the foreground objects and backgrounds with hybrid real-to-sim rendering techniques.
For the foreground objects, object renderings are applied from the mesh with ray tracing supports. Background reconstruction contains most parts of the scenes and applies more photorealistic rendering using 3DGS. Z-buffer rendering is applied to mix objects according to ground-truth depth. Notably, hybrid visual rendering supports multi-view rendering according to the perspective views. The rendering time consumption of each component is shown in Tab.~\ref{tab:time_consumed}.


\subsection{Real-World Alignment}
\label{sec:method:alignment}
Despite the prior alignments between rendering and meshes, another key step is to align with real-world scenarios, specifically, the positions of the background/foreground relative to the robots. For foreground objects, we define a range of possible placements in the simulation. Regarding the background, we leverage the ArUco marker on the table to align the 3DGS coordinates with the marker coordinates. Then the ICP ~\citep{schenker1992sensor,censi2008icp,low2004linear} post-processing is applied to optimize the coordinate alignment gap. Specifically, ICP optimizes the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.

% \subsubsection{Position Alignment}
% Aligning reconstructed backgrounds and objects with real-world environments involves determining the positions of the background and foreground objects relative to the robots within the simulation. 

% \noindent\textbf{Background alignment.}
% For the background, an ArUco marker ~\citep{aruco_marker_garrido2014automatic} is placed in the physical scene to establish an initial relative position and orientation, and the ICP algorithm~\citep{schenker1992sensor,censi2008icp,low2004linear} is applied to optimize the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.
% To refine this alignment, we apply the ICP algorithm~\citep{schenker1992sensor,censi2008icp,low2004linear} to optimize the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.

% \noindent\textbf{Foreground alignment.}
% For foreground objects, we aim to define a range of possible placements rather than fix their positions, as in most real-world tasks, object positions are not strictly fixed. This can be achieved by constraining the bounding box or center point to a specified activity region relative to a reference object in the simulation.
% \subsubsection{Collision-Rendering Alignment}
% It is also required to align the collision and rendering for the background, in order to get consistent results between simulation and generated visual observations.
% Since an ArUco marker is placed on the table, the relative spatial position between the reconstructed results and the marker can be calculated. This computation then facilitates determining the relative positional relationship between the two reconstructed coordinate systems. Such alignment ensures consistency between the visual and physical components of the simulation.

% \subsection{Real-World Alignment}
% \label{sec:method:alignment}
% Since we choose to reconstruct background and foreground objects with different techniques, in addition to reconstructing the geometries and visual appearances (colors) of the background separately, it is required to align them together to integrate them into one scene. In detail, this corresponds to 1) the positions of the background/foreground relative to the robots and 2) consistency between the rendering (colors) and the collision simulation (geometries/shapes).

% \subsubsection{Position Alignment}
% % Aligning reconstructed backgrounds and objects with real-world environments involves determining the positions of the background and foreground objects relative to the robots within the simulation. 

% \noindent\textbf{Background alignment.}
% For the background, an ArUco marker ~\citep{aruco_marker_garrido2014automatic} is placed in the physical scene to establish an initial relative position and orientation, and the ICP algorithm~\citep{schenker1992sensor,censi2008icp,low2004linear} is applied to optimize the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.
% % To refine this alignment, we apply the ICP algorithm~\citep{schenker1992sensor,censi2008icp,low2004linear} to optimize the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.

% \noindent\textbf{Foreground alignment.}
% For the foreground objects, we aim to define a range of possible object placements rather than fixing their positions. This flexibility allows the robot to interact effectively with objects, regardless of their random placements within this predefined area. 
% This can be achieved by constraining the bounding box or center point to a specified activity region relative to a reference object in the simulation.
% % To achieve this, we compute the bounding box and the center of the reconstructed object’s point cloud. By constraining the bounding box or center point to a specified activity region relative to a reference object in the simulation, we ensure proper object alignment.

% \subsubsection{Collision-Rendering Alignment}
% It is also required to align the collision and rendering for the background, in order to get consistent results between simulation and generated visual observations.
% Since an ArUco marker is placed on the table, the relative spatial position between the reconstructed results and the marker can be calculated. This computation then facilitates determining the relative positional relationship between the two reconstructed coordinate systems. Such alignment ensures consistency between the visual and physical components of the simulation.

\subsection{Expert Data Collection}
\label{sec:method:data-generation}
To verify the effectiveness of the real-to-sim-to-real system, following the designed task scenarios such as pick-and-place, we collect large-scale demonstrations using the script policy. 
During each rollout, we introduced domain randomizations including source/target object randomization, and robot arm's base position randomization.
% After reconstructing the whole scene in the simulator, we can use \our to generate high-quality simulation data automatically. 
% Following the above steps, objects and the robot are positioned and aligned under different task-specific requirements in the simulator, and both are randomly initialized in different poses for diversity. 
% A long-horizon task can be further divided into multiple stages based on the sequence of object interactions.  
Define an expert policy $\pi_\text{priv}(a_t | o_{\text{priv}, t})$ based on privileged information $o_{\text{priv}, t}$, such as the exact pose of target objects. We can then make use of $\pi_\text{priv}$ to interact with the simulator and generate observation-action pairs $(o_t, a_t)$.
Here, $o_t$ represents observations that are accessible from the real world, including images and proprioception data.
Specifically, we compute the 6D pose of the gripper at some key steps and use the motion planner based on the RRTConnect~\cite{rrtconnect} algorithm to plan the path between the key steps in the joint space.
During data generation, reject sampling is applied to filter out failed rollouts, improving the dataset quality and ensuring reliability.
We can continue to generate such data with the amount according to our needs, obtaining the simulated datasets $\mathcal{D}$. 

% For more complex tasks, we can exponentially expand our dataset using privileged information. For instance, in multi-item picking scenarios, we can initially collect data for cases where only one object is added to a full set of objects. By leveraging the simulator's privileged information to record background object mask information, we can post-process the data to selectively display objects. This generates trajectories equivalent to picking objects from smaller sets, significantly enriching our dataset in a time-efficient manner and consequently improving model performance.
% grasp pose


% ----------------------old method---------------------------
% \subsection{Aligning Hybrid Gaussian Rendering with Physical Simulation}
% \label{sec:real2sim}
% % 单独点一下我们用的是isaacsim?
% Various simulators and physics engines offer accurate physics-based results, such as Isaac Sim (using PhysX), PyBullet (using Bullet), Mujoco, and Gazebo (using Bullet, ODE, DART, and Simbody). While these tools excel at simulating physics across a wide range of parameters, their rendered outputs often differ a lot from real-world photos. 
% Gaussian splatting enables reconstructed representations from 2D images or videos, producing new-view RGB rendering results. To integrate the strengths of both approaches, we propose a Hybrid Visual Rendering framework that integrates Gaussian splatting for vision rendering with the PhysX physics engine in NVIDIA Isaac Sim. We enhance the realism of visual outputs generated by physics simulators, effectively narrowing the sim-to-real gap. Specifically, our design emphasizes the following key features:

% \begin{itemize}
%     \item High-fidelity reconstruction quality. Compared to RialTo~\citep{ritotorne2024rialto}, a prior Real2Sim2Real system, our approach achieves superior rendering quality. We validate this through quantitative results in Tab.~\ref{tab:quantitative comparison} and sim-to-real experiments in Tab.~\ref{table:main_experiment}.
%     \item Low sim-to-real gap: Experiments shown in Tab.~\ref{table:main_experiment} demonstrate this vision improvement.
%     % 这里的数值待精确测试
%     \item Computational efficiency: The hybrid system can render at 20 FPS using two cameras capturing 480p images, details are in Tab.~\ref{tab:time_consumed}.
%     \item Reconstruction time-effectiveness: Our system requiring only about five minutes of human work to reconstruct a novel scene.
%     % \item Standardized interfaces for extensibility.
% \end{itemize}

% The physical reconstruction process is described in Section~\ref{sec:phy_reconstruction},while the vision reconstruction method is detailed in Section~\ref{sec:vis_reconstruction}. 

% \subsubsection{Physical Reconstruction}
% \label{sec:phy_reconstruction}
% \noindent\textbf{Mesh Reconstruction.} 
% To integrate backgrounds and objects into a physics simulator, we reconstruct their collision bodies separately, represented in USD format to ensure high extensibility. For the background's collision body, we first utilize the OpenMVS\citep{openmvs2020} method to reconstruct detailed mesh. Subsequently, we simplify this mesh to create collision bodies. This approach effectively captures intricate details while maintaining accurate planar reconstruction. For object collision bodies, we employ ARCode~\citep{arcode2022} which provides an optimal balance between usability and reconstruction quality, ensuring efficient and precise representations.

% \noindent\textbf{Post-Processing Mesh.} 
% Reconstructed collision bodies often exhibit voids or sharp edges due to limitations in image and video quality, which can lead to erratic behaviors during simulation. For example, objects may fail to rest stably on surfaces or exhibit excessive bouncing when dropped from minimal heights. To mitigate these issues, we sequentially perform void filling and surface smoothing. These post-processing steps improve stability and realism in simulations. Furthermore, estimating physical parameters, such as mass and friction coefficients, remains challenging when relying solely on visual data. Although approaches like robot-assisted parameter acquisition, as demonstrated in ASID~\citep{memmel2024asid}, provide greater accuracy, we have found that default parameter values suffice for many manipulation tasks, simplifying the process without sacrificing performance.

% \noindent\textbf{Real-world Alignment.}
% % 感觉写的有点太细了？
% Aligning reconstructed backgrounds and objects with real-world environments involves determining the positions of the robotic arm (scene alignment) and objects (object alignment) within the simulation. 
% For scene alignment, an ArUco marker ~\citep{aruco_marker_garrido2014automatic} is placed in the physical scene to establish an initial relative position and orientation. To refine this alignment, we apply the ICP algorithm to optimize the relative transformation between the partial point cloud obtained from the depth camera and the complete point cloud sampled from the reconstructed mesh.

% For object alignment, we aim to define a range of possible object placements rather than fixing their positions. This flexibility allows the robot to interact effectively with objects, regardless of their random placements within this predefined area. To achieve this, we compute the bounding box and the center of the reconstructed object’s point cloud. By constraining the bounding box or center point to a specified activity region relative to a reference object in the simulation, we ensure proper object alignment.


% \subsubsection{Visual Reconstruction.}
% \label{sec:vis_reconstruction}
%% rgb选取：more feature, deblur
% \noindent\textbf{Scene Reconstruction.}
% We employ 3DGS to generate rendering results that efficiently represent the entire 3D scene in a photorealistic manner. 
% Specifically, given a set of images captured using a calibrated Realsense camera, we first apply structure-from-motion techniques(COLMAP), to estimate the pose for each image.
% Next, we represent each Gaussian primitive using surfels and adopt the same training pipeline as 3DGS to optimize the 3D scene. 
% % To enhance the robustness of the optimization process, we incorporate depth and pseudo-normal constraints to regularize the Gaussian optimization.
% This approach provides a photorealistic 3D representation of the scene.

% \noindent\textbf{Object Reconstruction.}
% The direct application of Gaussian splatting for object reconstruction often results in incomplete separation of objects from the background, reducing the rendering quality. Conversely, the USD files generated by ARCode include high-quality textures and provide sufficient detail for object rendering, which constitutes only a small portion of the robot’s visual observations. Therefore, for most tasks, we rely on USD reconstructions for object rendering, avoiding additional processing steps.
%% 点云清理

% \noindent\textbf{Visual Alignment.}
% % \ylchen{this part belongs to sim-to-real?}
% To separately reconstruct visual and physical components, alignment between them is necessary. Since an ArUco marker is placed on the table, the relative spatial position between the reconstructed results and the marker can be calculated. This computation then facilitates determining the relative positional relationship between the two reconstructed coordinate systems. Such alignment ensures consistency between the visual and physical components of the simulation.

% \noindent\textbf{Why Choose a Hybrid Visual Rendering System?} 
% Firstly, there is currently no perfect reconstruction method that simultaneously offers high-fidelity visual rendering, accurate collision body estimation, and low reconstruction cost. A Hybrid Visual Rendering system combines the advantages of multiple methods and provides high flexibility, enabling a balance between reconstruction cost and quality as required. Secondly, supporting direct USD rendering of objects facilitates augmenting object datasets with existing simulation data, thereby enhancing model generalization at the object level. Additionally, the multiple reconstruction methods employed share the same input images or videos, imposing virtually no additional human workload on the reconstruction process.

% However, due to our modular design approach, different representations of the real world obtained from various reconstruction algorithms are not inherently aligned. 
% Furthermore, even when we simultaneously reconstruct both visual and collision information of the background, we still need to establish the relative spatial relationship between the robot's approximate workspace and the scene. 
% Essentially, the coordinate system of the reconstructed scene is arbitrary, and we cannot automatically determine the robot's position within this coordinate system during the reconstruction process.

% Moreover, for algorithms like COLMAP that estimate camera poses, the scale of the camera coordinate system remains inherently ambiguous. To align all coordinate systems from different reconstruction modules with the real-world coordinates, we employ AprilTags as reference markers. We uniformly represent an object's position, rotation, and scale information in the coordinate system as: 
% $$ T_A^B = \begin{bmatrix}
%     S_A^B \times R_A^B & t_A^B \\
%     0 & 1
% \end{bmatrix},
% $$
% where $S_A^B$, $R_A^B$, and $t_A^B$ represent the scale, rotation matrix, and position of coordinate system A with respect to coordinate system B, respectively. Through hand-eye calibration, we can obtain the spatial relationship between the camera and the robotic arm, specifically the transformation matrix from the wrist-mounted camera to the robot base coordinate system: $T_\text{wcam}^\text{ee}$, and the transformation matrix from the third-person view cameras to the robot base: $T_\text{tcam}^\text{base}$. For the former, we can further compute the transformation matrix from the wrist camera to the robot base: $T_\text{wcam}^\text{base}$ using forward kinematics.

% At this point, we have obtained at least one transformation matrix from a camera to the robot base. Subsequently, we calculate the transformation matrix of the camera in the marker coordinate system using AprilTags in the real world. Through straightforward algebraic operations, we can derive the relative spatial relationship between the robot base and the marker coordinate system. We then leverage the subset of images containing visible markers from the reconstruction process to establish the spatial relationship between the reconstructed scene and the marker coordinate frame. 

% For manipulatable objects, since they only need to be placed within the workspace, and the relative spatial relationship between the workspace and the robotic arm is highly task-dependent, precise positioning is not critical. A rough estimation of the object's position relative to the robot base is sufficient to ensure reasonable object placement.

% Therefore, by estimating the approximate distance from the object's center to the robot base, we can appropriately position the object in the simulator using its point cloud data, thus completing the coordinate system alignment process.

% Noting that our visual rendering results originate from multiple sources, we utilize the privileged information available in Isaac Sim to extract both object identity information and their corresponding segmentation masks in camera views. This allows us to seamlessly composite images from different sources, thereby achieving optimal visual rendering quality.

% \subsection{Sim-to-real: Automated Scaling Up Simulated Demonstrations}
% After reconstructing the scene in the simulator, high-quality data is collected to train deployable models using imitation learning. The following subsections detail the data collection process, training strategies, and sim-to-real techniques to ensure successful real-world deployment.