\section{Introduction}
\label{sec:intro}

We have witnessed impressive generalization capabilities of robotic models~\cite{rt2, gr2,openvla,li2023vision,li2023generalist,tian2024predictive} in certain tabletop or kitchen scenarios, achieved through high-quality teleoperation data. However, collecting real-world expert data~\cite{pi0, brohan2022rt} remains a time-consuming and costly process. Despite advancements in teleoperation systems~\cite{cheng2024tv,aldaco2024aloha,yang2024ace,mobilealoha}, the labor involved is still intensive, making this a key challenge in robotics research. Recent efforts~\cite{oxe, droid} have explored inter-institutional collaboration to address this issue. In contrast, simulation data offers the advantage of exponential scalability with computational resources, making it an appealing and renewable alternative for training robotic policies. Research on sim-to-real transfer~\cite{mandlekar2023mimicgen,jiang2024dexmimicen,robocasa2024} has increasingly focused on generating high-quality simulated or synthetic data to train real-world robotic policies. Unfortunately, simulation data often suffers from large sim-to-real gaps, necessitating the collection of target domain data for policy fine-tuning.

% \begin{figure}
%     \centering
%     \includegraphics[width=.45\textwidth]{images/teaser2.png}
%     \caption{\textbf{}}
%     \label{fig:teaser}
% \end{figure}

Among these gaps, two notable ones are the geometric gap and the visual gap. The geometric gap arises from shape mismatches between objects, often due to the reliance on a limited set of pre-defined CAD models in simulation, which assume perfect structures.
The visual gap, on the other hand, comes with noiseless and imperfect rendering, posing challenges for tasks that rely heavily on appearance information, such as picking fresh fruits, pressing colorful buttons, sorting products based on packaging, and so on. 

To bridge these gaps between real and simulated environments, we propose a 3D-photorealistic real-to-sim-to-real system, \textbf{\our}, namely, \textbf{re}construction-\textbf{re}ndering-based \textbf{re}al-to-\textbf{sim}. \our faithfully replicates real-world scenarios by combining realistic 3D geometry reconstruction and visual RGB rendering. The 3D geometry is reconstructed using multi-view stereo (MVS) techniques~\cite{schoenberger2016sfm, openmvs2020}, while the visual rendering is achieved through Gaussian rasterization~\cite{kerbl3Dgaussians}. This dual approach enables efficient and high-fidelity reconstruction, with simulated dynamics computed through physics engine backends~\citep{makoviychuk2021isaac,Xiang_2020_SAPIEN,todorov2012mujoco} and real-time rendering handled by a dedicated hybrid rendering engine.

Specifically, \our adopts a sequential real-to-sim pipeline comprising three key steps: (a) \textit{mesh recovery} for reconstructing scene and object geometry; (b) \textit{hybrid visual rendering} for compositing foreground and background elements, and (c) \textit{real-world alignment} to synchronize world coordinates between the real and simulated environments. Human involvement is minimal in the real-to-sim process and limited to:
(a) Placing ArUco markers in the target scenario for real-world alignment.
(b) Capturing photos or videos of the scene and objects, including an additional capture step to align the robot base.

Once all assets and robots are imported into the simulator, \our utilizes a privileged policy to generate high-fidelity expert data. It incorporates the following key features to enhance sim-to-real transfer:
\vspace{-0.1in}
\begin{itemize}[leftmargin=0.in]
    \item \textbf{High-fidelity geometry and vision}: Achieves superior reconstruction and rendering quality, reducing the sim-to-real gap in both geometry and vision.
    \item \textbf{Rapid scene reconstruction}: Enables novel scene reconstruction in under three minutes of manual setup.
    \item \textbf{Efficient rendering}: Provides 24 FPS rendering for 480p images across two independent camera views.
\end{itemize}
% \noindent\textbf{High fidelity}: Achieves superior reconstruction and rendering quality, significantly reducing the sim-to-real gap both in collision and in vision.\\
% \noindent\textbf{Time effectiveness}: Requires less than three minutes of manual setup to reconstruct a novel scene.\\
% % for automated simulation data generation in novel scenes.
% \noindent\textbf{Computational efficiency}: Delivers 24 FPS rendering for 480p images across two independent camera views.
% \begin{itemize}
% \item \textbf{High fidelity}: Achieves superior reconstruction and rendering quality, significantly reducing the sim-to-real gap in collision and vision.
% \item \textbf{Time effectiveness}: Requires less than three minutes of manual setup to reconstruct a novel scene.
% % for automated simulation data generation in novel scenes.
% \item 
% \textbf{Computational efficiency}: Delivers 24 FPS rendering for 480p images across two independent camera views.
% \end{itemize}

To validate the effectiveness of the pipeline, we design several tabletop tasks demonstrating the generality and applicability of the real-to-sim-to-real process, as shown in Fig.~\ref{fig:teaser} The zero-shot sim-to-real policies achieve an average success rate of 58\% with only about 10 minutes of data collection in simulation, showcasing the effectiveness of our approach. We expect these experiments to highlight the potential of the real-to-sim-to-real pipeline in reducing sim-to-real gaps, paving the way for future advancements in scalable, efficient, and generalizable robotic policy.
