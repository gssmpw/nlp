% High-quality, extensive data is the cornerstone of building powerful, robust, and generalizable models for robotics. 
% However, collecting such data in the real world normally requires designing efficient teleoperation systems, skilled operators, and costly robots, making the process resource-intensive. 
% On the contrary, simulation offers a scalable alternative for generating vast amounts of data in a short period but often suffers from sim-to-real gaps due to limitations in realistic geometric modeling and visual rendering.
\begin{abstract}
Real-world data collection for robotics is costly and resource-intensive, requiring skilled operators and expensive hardware. Simulations offer a scalable alternative but often fail to achieve sim-to-real generalization due to geometric and visual gaps. To address these challenges, we propose a 3D-photorealistic real-to-sim system, namely, \our, addressing geometric and visual sim-to-real gaps. 
\our employs advanced 3D reconstruction and neural rendering techniques to faithfully recreate real-world scenarios, enabling real-time rendering of simulated cross-view cameras within a physics-based simulator.
By utilizing privileged information to collect expert demonstrations efficiently in simulation, and train robot policies with imitation learning, we validate the effectiveness of the real-to-sim-to-real pipeline across various manipulation task scenarios. 
Notably, with only simulated data, we can achieve zero-shot sim-to-real transfer with an average success rate exceeding 58\%.
To push the limit of real-to-sim, we further generate a large-scale simulation dataset, demonstrating how a robust policy can be built from simulation data that generalizes across various objects. Codes and demos are available at: \url{http://xshenhan.github.io/Re3Sim/}.
\end{abstract}

