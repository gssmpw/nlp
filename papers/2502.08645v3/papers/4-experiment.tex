\section{Experiment}
\label{sec:experiment}
We mainly investigate the following four research questions based on the proposed \our system:
\begin{enumerate}[label=\textbf{Q\arabic*}]
    \item Does \our produce high-quality and well-aligned reconstruction results?
    \label{ques:1}
    \item Can \our generate high-fidelity simulation data with small 3D sim-to-real gaps to benefit real-world manipulation problems?
    \label{ques:2}
    \item How does large-scale simulation data help in more challenging real-world tasks?
    \label{ques:3}
    \item Can \our build the whole scene in simulation in a time-effective way and synthesize data with low cost?
    \label{ques:4}
\end{enumerate}
\subsection{Experimental Setups}
\noindent\textbf{Hardware platform.}
In this paper, we evaluate \our with a Franka Research 3 robot equipped with a parallel gripper. 
Two Realsense D435i depth cameras capture visual observations—one mounted on the end-effector and another positioned beside the robot for a third-person view.


\noindent\textbf{Policy model and training details.}
% We train a policy model $\pi(\tau_{a, t} | o_t)$ on the generated simulation demonstration dataset $\mathcal{D}$.
During our sim-to-real experiments, we adopt an ACT-structured policy~\citep{zhao2023learning} with DINOV2~\citep{oquab2023dinov2}.
The images in the dataset are compressed using JPEG format.
During training, we apply visual augmentations and for evaluation, we use the temporal ensemble~\citep{zhao2023learning} technique.
Appendix~\ref{sub:experiment_details} provides further details. 

% \noindent\textbf{Designed tasks.}
% We design four tasks as follows:
% \begin{enumerate}
%     \item \texttt{Placing a bottle into the basket.} The robot should pick an orange bottle that is randomly placed within a $25\text{cm} \times 35\text{cm}$ area on a table into the basket.
%     % approximately 700 trajectories in the simulator.
%     \item \texttt{Stacking blocks.} The robot is required to stack three black cubes on the table to finish this task. Each cube is randomly placed within a $25\text{cm} \times 10\text{cm}$ area separately, requiring the robot to grasp and stack them successfully. 
%     % We collect about 1300 trajectories in the simulator.
%     \item \texttt{Placing a vegetable on the board.}
%     A vegetable model is placed on the table in an area of $35\text{cm} \times 50\text{cm}$, and the position of the cutting board is varied in an area of $35\text{cm} \times 80\text{cm}$. The robot needs to accurately grasp the vegetable and determine the appropriate placement location based on the cutting board's position to complete the task. 
%     % We collected about 4000 trajectories for this task.
%     \item \texttt{Cleaning objects on the table}. The robot has to clean up the table by picking all items that are randomly placed on the table in a $40\text{cm} \times 50\text{cm}$ area, and put them into the basket. This is a long-horizon task that requires the policy to sequentially pick both seen and unseen objects one by one.
%     % We collected about 10,000 trajectories for the task.
% \end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Rendering in Robot Task}
\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=.9\linewidth]{images/rvs.pdf}
    \vspace{-8pt}  % Adjust this value as needed
    \caption{\textbf{Visual comparison between real and simulation.} Rendering results from our hybrid rendering method compared with photos captured by real-world cameras, highlighting the high fidelity and realism achieved by our approach.}
    \label{fig:visual qualitative comparison}
\end{figure*}
\label{sec:experiment_reconstruction_results}
\begin{table}[ht]
    \caption{\textbf{Background rendering methods comparison.} Quantitative results of the reconstruction result in terms of PSNR and SSIM. Results are computed from 1029 robot arm positions in the wrist view.}
    % Our approach (\our) using 3DGS outperforms Polycam (mesh-based method) as used in \citet{ritotorne2024rialto}.

    % during a single grasping task
    
    \label{tab:quantitative comparison}
    \centering\small
    \begin{tabular}{ccccc}
    \toprule
        Background Rendering & PSNR & SSIM  \\ \midrule
        Polycam & \makecell{$11.52 $ $\pm 1.40$} & \makecell{$0.34$ $\pm 0.04$}  \\
        OpenMVS & \makecell{$\boldsymbol{13.40}$$ \boldsymbol{\pm 0.96}$} & \makecell{$0.27$ $\pm 0.03$} \\
        3DGS & \makecell{$\boldsymbol{13.29}$ $\pm \boldsymbol{1.11}$} & \makecell{$\boldsymbol{0.37}$ $\boldsymbol{\pm 0.04}$} \\
        \bottomrule
    \end{tabular}
\end{table}

High-quality visual results not only require high-quality reconstruction but also precise alignment. To address \ref{ques:1}, we evaluate the quality and alignment of reconstruction results by selecting a trajectory from the simulation dataset of tasks in Sec.~\ref{sec:experment_sim2real_results}.
This trajectory is then replayed in the real world to obtain ground truth (GT) images captured by calibrated cameras. We compare the visual similarity from both the wrist view and a third-person perspective, showcasing qualitative rendering results in Fig.~\ref{fig:visual qualitative comparison}. More rendering results in the simulation are in Sub. ~\ref{sec: visualization}. The high similarity to real-world images further demonstrates the accuracy of our alignment. 
Furthermore, Tab.~\ref{tab:quantitative comparison} shows the PSNR (peak signal-to-noise ratio) and SSIM (structural similarity index measure) evaluating the quantitative vision gap. 
We compare \our with the reconstruction methods OpenMVS and PolyCam (the method used in ~\citet{ritotorne2024rialto}). Both methods reconstruct textured meshes, which can be rendered to generate visual observations.
The objects are manually aligned with those in the simulation, and some pixel-level discrepancies remain. The background alignment also has some pixel-level deviations. These factors collectively lead to the relatively low PSNR and SSIM values, especially in the texture-rich scene.
3DGS outperforms Polycam in both RSNR and SSIM. Its PSNR is comparable to OpenMVS, but SSIM is notably higher. Qualitative visual comparisons and more analysis are shown in Appendix~\ref{sub:experiment_details}. OpenMVS's reconstruction has cracks, causing an obvious sim-to-real gap. 
These results demonstrate the advantage of the design choice of the rendering component in \our, which benefits small vision gaps. 
% The results indicate that Gaussian splatting enhances background rendering quality
% , whereas foreground rendering methods have minimal impact on overall image quality. Specifically, rendering the foreground using mesh-based methods offers greater flexibility without degrading the rendering outcomes.
% Minor discrepancies between object positions in the simulated visual results and the real-world GT images, arising from challenges in precise real-world replication, contribute to lower absolute PSNR and SSIM values.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Zero-Shot Sim-to-Real}
\label{sec:experment_sim2real_results}

\begin{table*}[!t]
    \caption{\textbf{Quantitative results of real-to-sim-to-real evaluation for real-world manipulation tasks.} Each task is tested 20 times without allowing retries in the event of failure. The policies are trained separately with 100 episodes of simulation data (RialTo+IL and \our+IL) and 50 episodes of real data (Real+IL). AnyGrasp combined with script primitives (AnyGrasp+Prim) is also tested for comparison in the picking-only scenario. \our reduces the sim-to-real gap, enabling successful real-world deployment, achieving the best zero-shot sim-to-real performance for an average success rate of 58\%.}
    \label{table:main_experiment}
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        Real-World Task  & \makecell{RialTo+IL} & \makecell{AnyGrasp+Prim.}  & \makecell{Real+IL} & \makecell{\our+IL} 
        % & \makecell{\{Real+\our\}+IL}  
        \\
        \midrule
        \multirow{1}{*}{Pick and drop a bottle to basket} & 0.4 & 0.9 & \textbf{0.8} & 0.75 & \\
         % & Diffusion &  \\ \midrule
        % \multirow{1}{*}{Pick up two objects to basket} & - & 0.7 & 0.6 & 0.7 & \\
        % \multirow{1}{*}{Pick up two objects to basket} & - & 0.2 &  & 0.4 & \\
        % & Diffusion &\\ \midrule
        \multirow{1}{*}{Stack cubes} & 0 & - & 0.15 & \textbf{0.25} & \\
        % & Diffusion \\ 
        \multirow{1}{*}{Place a vegetable on board} & 0.45 & - & 0.6 & \textbf{0.75}\\
        \bottomrule
    \end{tabular}
\end{table*}
\ref{ques:2} is the main question that motivates us to build \our, thus we design three table-top manipulation tasks to investigate the usage of \our:

1.~\texttt{Pick and drop a bottle into the basket.} The robot must pick an orange bottle that is randomly placed within a $25\text{cm} \times 35\text{cm}$ area on a table and place it into a basket. \\
% approximately 700 trajectories in the simulator.
% We collect about 1300 trajectories in the simulator.
2.~\texttt{Place a vegetable on the board.}
A cucumber model is placed on the table in an area of $35\text{cm} \times 50\text{cm}$, and the cutting board's position is varied within a $35\text{cm} \times 80\text{cm}$ area. The robot must grasp the vegetable and accurately place it on the cutting board. \\
% We collected about 4000 trajectories for this task.
3.~\texttt{Stack blocks.} The robot is tasked with stacking three black cubes on the table. Each cube is placed randomly within a $30\text{cm} \times 10\text{cm}$ area separately, and the robot must grasp and stack them successfully.

We rebuild these tasks in simulation via \our, collect 100 trajectories, and train independent policies.
More details of the implementation of the rule-based policy and the evaluation methods in simulators are in Sub. ~\ref{sub:task_details}. 
We evaluate the policy performances in terms of success rate evaluated in the real world, comparing the one trained on simulation data collected by \our, by a real-to-sim baseline method called RialTo~\citep{ritotorne2024rialto} and the policy trained on 50 real-world trajectories. Using double simulation data leads to similar performance compared with using real data, probably both because of the small simulation-to-real gap and because the different data collection methods for simulation and real-world data lead to varying data quality, as detailed in Appendix ~\ref{sub:diff_sim_and_real}.
We also realized a state-of-the-art grasping method, AnyGrasp~\citep{fang2023anygrasp}, combined with script primitives on the \texttt{pick and drop a bottle into the basket} task for comparison. This method only predicts the grasp pose instead of the whole grasping trajectory and thus cannot be applied to other tasks. 

The numerical results listed in Tab.~\ref{table:main_experiment} demonstrate that the data generated by \our help the policy achieve zero-shot sim-to-real transfer, showing strong performance even compared with the existing real-to-sim and state-of-the-art grasping method.
Furthermore, policies trained on our extensive synthetic dataset can achieve comparable or even slightly better performance than those only trained on real-world data, indicating the effectiveness and huge potential of high-fidelity simulation data. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Large-Scale Sim-to-Real}
\label{sec:large-scale-experiment}
To push the limit of utilizing synthetic data for real-world manipulation problems by answering \ref{ques:3}, we choose a \texttt{clear objects on the table} task: the robot has to clean up the table by picking all items that are randomly placed on the table in a $40\text{cm} \times 50\text{cm}$ area, and put them into the basket. This is a long-horizon task that requires the policy to sequentially pick both seen and unseen objects one by one. To provide a sufficient evaluation, we design four different setups:\\
\noindent\textit{Seen}: four seen objects (the bottle, cucumber, corn, and eggplant) included in the data for training. \\
\noindent\textit{Unseen}: four objects (the green pepper, banana, red bowl, and momordica charantia) that are unseen during training. \\
\noindent\textit{Cluttered}: all eight objects from \textit{seen} and \textit{unseen}. \\
 \noindent\textit{Darkness}: the testing brightness is significantly lower than that of the simulation data. The model is evaluated on four \textit{seen} objects.\\
The results are presented in Fig.~\ref{fig:scale_results}. Each setup was tested 10 times, with two additional automatic grasp attempts per object in case of failure.

\begin{figure}[tb]
    \centering
    \includegraphics[width=1.\linewidth]{images/scale_results.pdf}
    \vspace{-8pt}
    \caption{\textbf{Real-world evaluation and robustness test for large-scale sim-to-real.} The \textit{success rate} reflects the proportion of trials in which all objects were successfully grasped, while the \textit{grasp rate} indicates the proportion of objects grasped relative to the total number on the table. See qualitative results in the \href{http://xshenhan.github.io/Re3Sim/}{ website}.}
    \label{fig:scale_results}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=1.\linewidth]{images/dataset_size_ablation.pdf}
    \caption{\textbf{Data scaling effects}, tested on seen objects in the real world. }
    \label{fig:dataset-size}
\end{figure}

\noindent\textbf{Result analysis.}
Though trained on only five items, the policy generalizes well to grasping new objects of similar size despite differences in shape and color. We hypothesize that the robot leverages the fixed scene and identifies objects based on color differences in the background. Additionally, objects with varying shapes often have similar grasping positions, enabling the robot to execute successful grasps. A larger dataset with multiple objects allows the policy to maintain a relatively higher success rate, while a smaller dataset limits the policy’s ability to exhibit this capability. Furthermore, the policy demonstrates strong robustness to changes in lighting conditions.

We further explored how the success rate in simulation and real-world settings changes with varying amounts of data. As shown in Fig.~\ref{fig:dataset-size}, increasing the size of the synthetic dataset generated by \our leads to a significant improvement in imitation learning performance. When the amount of simulation data is limited, the real-world performance is almost negligible. Doubling the data size often results in a significant improvement in success rate until convergence at a high performance. 

\subsection{Sim-Real Consistency}
% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{images/consistency.pdf}
%     \caption{\textbf{Analysis of real and simulated evaluation consistency.} Models that perform better in simulation tend to perform better in real-world settings as well.}
%     \label{fig:sim policy consistency}
% \end{figure}

We evaluated the consistency of policy performance between simulated and real-world environments using policy ACT. In this experiment, we evaluate policies trained at different stages (from 0 to 120000 training steps) and with 3 different seeds on the \texttt{pick and drop a bottle into a basket} task. 

As illustrated in Fig.~\ref{fig:teaser} (d), the Pearson correlation coefficient is 0.924, indicating the clear correlation between performance in simulation and the real world. Specifically, models that achieve higher success rates in simulation tend to exhibit similar success rates in real-world testing.
When the success rate in simulation is low, it is often difficult for the model to perform well in real-world scenarios. This could be due to alignment errors, leading to a small sim-to-real gap, or the fact that the number of real-world test instances is much lower than in simulation, potentially introducing bias.
This consistency suggests that policies trained with \our are well-suited for real-world deployment, reflecting a minimal sim-to-real gap.

\subsection{Pipeline Efficiency}

To validate \ref{ques:4}, we record and compute the time of \our to reconstruct both the background and objects, along with the time of generating demonstrations.

% ARcode 55s 66s
\begin{table}[tb]
    \caption{\textbf{Human effort in reconstruction.} The table presents estimated reconstruction times at the table level. Additionally, we show the human effort for reconstructing an object with ARCode.}
    \label{tab:human-effort}
    \centering
    \begin{tabular}{cccc}
    \toprule
    Input Types & Video & Images & ARCode \\
    \midrule
    Human Efforts (s) & 51.5 & 84.5 & 60.5 \\
    \bottomrule
    \end{tabular}
\end{table}

\noindent\textbf{Human effort during reconstruction.} In \our, background reconstruction requires only a set of images or a short video of the background, along with an additional image for alignment, which contains depth information. Given the substantial human involvement in this process, we provide estimates for the time required for both foreground and background reconstruction. Tab.~\ref{tab:human-effort} summarizes the time required for different input types: video and images. Using video as input may result in slight quality degradation due to motion blur while using images provides better results but requires slightly more time. Additionally, we present the time required to reconstruct an object using ARCode. The total reconstruction time for the scene is the sum of the background and object reconstruction times.

\begin{table}[tb]
    \caption{\textbf{Time cost for simulation data collection.} Time needed to collect 100 episodes of simulation data for each task, using a machine equipped with 8 RTX 4090 GPUs. Each GPU can run two Isaac Sim processes.}
    \label{tab:data-collection-time}
    \centering
    \begin{tabular}{lccc}
    \toprule
        Tasks & Time Cost (minutes) \\
    \midrule 
    Pick and drop a bottle to basket  & 12.35 \\
    Place a vegetable on the board & 13.78\\
    Stack blocks & 6.45 \\
    \bottomrule 
    \end{tabular}
\vskip -0.1in
\end{table}

\begin{table}[bt]
    \caption{\textbf{Average time consumption per step.} Breakdown of time consumed by each process. 
    % Gaussian splatting rendering introduces low additional computational overhead, with the entire process taking an average of 41.46 ms per step.
    }
    \label{tab:time_consumed}
    \centering
    \begin{tabular}{lc}
    \toprule
         Process & Time (ms) \\
    \midrule
         Physics Simulating & 26.64\\
         Gaussian Splatting Rendering & 12.93\\
         Motion Planning& 0.36\\
         Others & 1.53\\
         Total Time & 41.46 \\
    \bottomrule
    \end{tabular}
\end{table}

\noindent\textbf{Data collection cost.} Tab.~\ref{tab:data-collection-time} shows the time required to collect 100 simulation episodes for each task in Sec.~\ref{sec:experment_sim2real_results}, using 8 RTX 4090 GPUs.
The time required for data collection is much lower than the time needed for teleoperation in real-world scenarios—especially considering \our only involves machine runtime, and the latter mostly requires the time of data collection experts. This demonstrates our capability to efficiently generate large-scale simulation data at minimal cost.

\noindent\textbf{Time consumption breakdown.} We measured the average time spent on each phase during the data collection of the \textit{pick and drop a bottle} task in the simulator over 60 steps, as shown in Tab.~\ref{tab:time_consumed}. 
The reconstructed mesh, with numerous vertices and faces, demands more time for physics simulation. 
Rendering 480p images from two camera views using 3DGS adds 12.93 milliseconds, representing one-third of the total time per step. \our operates at approximately 24 frames per second (FPS) with two cameras, ensuring real-time performance and visual fidelity.