\newpage
\appendix
\onecolumn
\setcounter{page}{1}

The appendix is organized as follows:


\begin{itemize}[leftmargin=*]
    \item Sec.~\ref{sub:task_details} illustrates the detailed design of the rule-based policy and the evaluation method of the tasks in Sec.~\ref{sec:experment_sim2real_results} and Sec.~\ref{sec:large-scale-experiment}. 
    \item Sec.~\ref{sub:experiment_details} illustrates the details of the policy training and image augmentation.
    \item Sec.~\ref{sub:diff_sim_and_real} explains the differences in properties between simulated and real data, providing insights into why policies trained on these datasets exhibit different performance characteristics.
    % \item Sec.~\ref{sec: more ablation studies} shows \textbf{More Ablation Studies}
    \item Sec.~\ref{sec: visualization} presents additional rendering results of various tasks from different camera angles.
\end{itemize}

\section{Task Details}
\label{sub:task_details}

This section describes the design of the rule-based policy and the evaluation method for each task.

\noindent\textbf{\texttt{Pick and drop a bottle into the basket.}}
This task consists of two steps: 1) determining the 6D pose of the gripper before picking, and 2) identifying the 6D pose of the placement position. The pre-pick pose is derived from the object's point cloud, while the placement position is fixed. Motion planning in joint space guides the robotic arm from the starting position to the two key positions for picking or releasing the object. Picking success depends on the positional relationship between the robotic arm, the gripper, and the object's height relative to the table. Upon release, the object is considered successfully placed if its horizontal position lies within the basket's area.

\noindent\textbf{\texttt{Placing a vegetable on the board}}
The board's center serves as the placement position. The rule-based policy follows a similar approach to the previous tasks. Success is achieved only if the object's horizontal position during placement lies within a small region near the board's center.

\noindent\textbf{\texttt{Stack blocks.}}
This task involves sequentially picking and placing two objects. Using the objects' 3D models, the pre-pick pose and placement position are calculated. The implementation resembles the \texttt{pick and drop a bottle into the basket} task. The second block is picked only after the first is successfully placed; otherwise, the trajectory is terminated early and discarded. Stacking is considered successful if the horizontal positions of the two objects align and their height difference equals the height of one block.

\noindent\textbf{\texttt{Clear objects on the table.}}
To minimize action variability in identical states and simplify model training, the rule-based policy prioritizes picking objects closer to the robot base. Due to the large number of objects, randomizing their initial positions often results in overlapping point clouds, leading to unpredictable physics simulation outcomes. Therefore, during domain randomization, configurations with intersecting objects are filtered out before simulation. The task is considered complete only when all objects are successfully picked and placed in their designated positions.

\section{More Experiment Details}
\label{sub:experiment_details}

\begin{table*}[b]
    \caption{Hyper-parameters for Data Augmentation.}
    \label{sub:tab:hyperparam}
    \centering
    \begin{tabular}{ccccc}
    \toprule
        Params & Value \\ \midrule
        Batch size & 8 \\
        Epochs & 100 \\
        Learning rate for warmup steps & 1000 \\ 
        Kl weight & 10 \\
        Hidden dim & 512 \\
        Dim feedforward & 3200 \\

        % Random Robot Base & \\
        % Color Jittering & \\
        % Gaussian Blur & \\
        \bottomrule
    \end{tabular}
\end{table*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/compare.pdf}
    \caption{\textbf{Visual comparison of rendering approaches.} Rendering results of reconstruction outputs from PolyCam, OpenMVS, and 3DGS, compared with real-world photos. }
    \label{fig:compare_reconstrcution_methods}
\end{figure*}
% Sec.~\ref{sub:augmentation}.
\noindent\textbf{Qualtative comparison of background rendering methods.}
We collected a trajectory using teleoperation in the simulator and replayed it in the real world. Visual comparisons of reconstruction results from various methods are shown in Figure ~\ref{fig:compare_reconstrcution_methods}. PolyCam tends to blur details in certain areas, OpenMVS's reconstructed mesh texture shows cracks, while 3DGS produces high-quality renderings. Table ~\ref{tab:quantitative comparison} shows that OpenMVS's PSNR is slightly higher than 3DGS's, likely because the background and crack colors are similar, making PSNR less sensitive. However, the cracks cause increased variance within patches, leading to a lower SSIM compared to 3DGS.


\noindent\textbf{Policy Training.} The policy predicts a sequence of future $k$-step actions from single-step observations, including two-view images and proprioception data. To better capture information from the images, we replace the original ResNet architecture with DINOv2 ~\citep{oquab2023dinov2} small as the model's encoder. 
During training, we apply visual augmentations, including \textit{Gaussian Blur}, \textit{Defocus}, \textit{Color Jittering}, and \textit{Gaussian Noise}, to enhance robustness. Additionally, we apply cosine decay to the learning rate (lr) to facilitate easier convergence during model training. The hyperparameters related to model training for tasks in Sec.~\ref{sec:experment_sim2real_results} are listed in Table \ref{sub:tab:hyperparam}, and because we use a much larger dataset for the \texttt{Clear objects on the table} task, we train the policy only for 8 epochs. During the evaluation, the temporal ensemble ~\citep{zhao2023learning} technique is used to smooth the final action for rollouts.


\begin{table}[t]
    \caption{\textbf{Ablation on data augmentation methods.} Success rates (SR) of the single-item picking task in both simulated and real-world environments for different image augmentation techniques.}
    \label{tab:augmentation_comparison}
    \centering
    \begin{tabular}{lcccc}
    \toprule
        Augmentation & Sim SR & Real SR \\ \midrule
        -  & 0.77 & 0.25 \\
        % + random camera position  & & \\
        % + lightning & & \\
        + Gaussian Blur & 0.45 & 0.4 \\
        + Defocus & 0.97 & 0.6 \\
        + ColorJitter & 0.37 & 0.6 \\
        + Gaussian Noise & 0.94 & 0.8\* \\
        \bottomrule
    \end{tabular}
\end{table}


\noindent\textbf{Image augmentation.} Recognizing that Gaussian splatting inherently cannot replicate certain visual effects such as motion blur and focus-dependent blur, however, due to camera motion, photos taken during the actual deployment of the device may appear blurred. Additionally, the brightness of the room may change over time. To simulate this phenomenon during model training,
we apply random transformations to the images using the implementation from \cite{albumentations}, including \textit{Gaussian Blur}, \textit{Decocus}, \textit{Color Jitter} and \textit{Gaussian Noise}.
% \begin{itemize}
%     \item Gaussian Blur: A Gaussian filter with a random kernel size and sigma value is applied to blur the image.
%     \item Defocus: A combination of disc kernels and Gaussian blur is used to create a realistic defocus effect, simulating the impact of an out-of-focus camera.
%     \item Color Jitter: Random adjustments are made to the brightness, contrast, saturation, and hue of the image.
%     \item Gaussian Noise: Gaussian noise is added to the input image.
% \end{itemize}

As presented in Tab.~\ref{tab:augmentation_comparison}, applying appropriate image augmentation techniques enhances model performance in both simulated and real-world environments. Specifically, augmentations such as blur and defocus simulate the blurring caused by camera motion. Adding noise and blur together may help the model learn key information for completing tasks, reducing sensitivity to changes in irrelevant information and enabling more robust performance in real-world scenarios. Using these image augmentation methods can achieve the highest success rate in real-world environments, indicating their effectiveness in bridging the sim-to-real gap between 3D Gaussian splatting rendering results and real-world camera outputs.

\noindent\textbf{Evaluation in simulation.} To employ rejection sampling during data collection and evaluate the training policy, a mechanism is required to determine whether the task has been successfully completed. The relationship between the gripper and the object’s point cloud, as well as the distance between the object and the tabletop, can help verify if the object has been successfully grasped. Furthermore, the relative position between the object to be grasped and the target can be used to confirm whether the item has been placed in the correct position.  

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=1.\textwidth]{images/compression_quality.pdf}
    \caption{\textbf{Compression analysis.} Qualitative and quantitative analysis using third-person view images to demonstrate the impact of different JPEG quality parameters on image quality, task success rate, and compression ratio in the \texttt{place a vegetable on the board} task. The policies are trained on compressed data, but uncompressed data is used during deployment.}
    \label{fig:img-compression-quality}
\end{figure*}
\noindent\textbf{Data saving format.} 
For complex tasks, we require a large amount of simulation data to achieve satisfactory results. Storing image data without any compression is costly in terms of space. We compress the images using the JPEG format. During deployment, uncompressed data is used to retain more information. Fig.~\ref{fig:img-compression-quality} shows the image quality at different quality settings, the success rate of models trained on the \texttt{place a vegetable on the board} task, and the corresponding compression rates. In our experiments, we chose a quality setting of 40, as it results in almost no perceptible loss in quality while saving a significant amount of storage space.


\section{Comparison over Simulated and Real Data}
\label{sub:diff_sim_and_real}
We train the policy with 100 episodes of simulation data and 50 episodes of real-world data, achieving comparable performance in the real-world setting. In this section, we analyze the differences and similarities between simulated and real data, exploring the underlying causes. Real-world and simulation data often exhibit variations in both distribution and quality, because of differences in scene initialization methods and trajectory preferences between human operators and the rule-based policy.

\subsection{Object Location}
\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/data_distribution.png}
    \caption{\textbf{Data distribution.} We show the initial locations of objects in each task described in Sec.~\ref{sec:experment_sim2real_results}. There are 100 episodes of simulation data(Sim) and 50 episodes of real-world data(Real). We show each cube's location in \texttt{stack blocks} tasks separately.}
    \label{fig:data-distribution}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\textwidth]{images/data_quality.jpg}
    \caption{\textbf{Data quality.}  Analysis of differences between real-world and simulation data properties. The first row presents statistical data on the distribution of gripper angles per time step across episodes. The second row displays the Kernel Density Estimate (KDE) of episode lengths. }
    \label{fig:data-quality}
\end{figure*}
In simulation, we define a rectangular region and use the standard library's random implementation to achieve an almost uniform distribution. 
In the real world, random initialization can only rely on operator estimation.
In Fig.~\ref{fig:data-distribution}, we visualize the initial object position distributions from simulation data (100 episodes) and real-world data (50 episodes) used for training. 
The visualization reveals that despite operators consciously attempting to randomize object positions within the corresponding area, the data distributions show slight differences. 
Specifically, we find that in the \texttt{pick and drop a bottle into the basket} task, the real-world data shows a wider distribution, while in the other two tasks, the simulation data has a slightly broader spread. As seen in Table\ref{table:main_experiment}, tasks with a broader distribution tend to yield better performance.

\subsection{Data Quality}
In Fig.~\ref{fig:data-quality}, we visualize the distribution of episode lengths and gripper angles relative to the gravity direction. Due to different data collection methods, there are significant differences in trajectory lengths. In simulation, the motion planner tends to take the shortest path, resulting in shorter trajectories but with larger angular variations. Additionally, we empirically observe that the motion planner simultaneously adjusts both the gripper's rotation and position, which rarely occurs in real-world teleoperation using a space mouse. When reconstructing the scene, we did not capture images from positions with large angles, which led to reduced rendering realism in high-angle simulations. This could lead to the need for more simulated data to compensate for these discrepancies. On the other hand, longer trajectories may include more pauses, which can negatively impact model training due to reduced action continuity. 


% \section{More Ablation Studies}~\label{sec: more ablation studies}

% Table~A-\ref{tab:hyperparam}

\section{Co-training and Fine-tuning}

\begin{figure}[h]
    \centering
    \subfigure[]{
        \includegraphics[width=0.45\textwidth]{images/delta_distances_distribution.png} 
        \label{fig:delta_distance}
    }
    \subfigure[]{
        \includegraphics[width=0.45\textwidth]{images/close_gripper_index.png} 
        \label{fig:close-step}
    }
    \caption{\textbf{Features of co-training and fine-tuning models.} (a) KDE of the Euclidean distance traveled by the robotic arm's end effector between adjacent time steps.  (b) The number of time steps taken by the robotic arm from the start of movement to the first closure of the gripper. "Sim" and "Real" indicate models trained on simulated and real data, respectively, while "Co-train" and "Fine-tune" refer to models trained on a mix of data and pre-trained with real data, respectively.}
    \label{fig:joint-funetune}
\end{figure}

We tested the effectiveness of co-training and pretrain-finetune on the \texttt{Stack blocks} task. Specifically, we report the speed characteristics during model execution. As shown in Figure~\ref{fig:delta_distance}, the policy trained on simulation data exhibits relatively fast speeds for many time steps, while the model trained on real data mostly demonstrates slower speeds. The co-training model lies between these two extremes. After fine-tuning with real data, the model’s speed decreases a lot compared to the simulation model but remains slightly faster than the real data-trained model. Since only real data was used for fine-tuning, the final model is closer to the real data-trained model than the co-training model.
Figure~\ref{fig:close-step} illustrates the number of time steps required for the robotic arm to move from the start to the first closure of the gripper, reflecting the average speed of the arm's motion. The models trained with real data or fine-tuning with real data are relatively slow, while models trained with simulation data or jointly trained with simulation data exhibit a higher speed. 

These suggest that the distribution of simulation and real data is generally similar and that the data generated by our method can be integrated into real data through pretraining or co-training, introducing new features without causing the training process to collapse.


\section{Result Visualization}
\label{sec: visualization}

OpenMVS is also capable of providing a textured mesh for acquiring visual observations. A qualitative comparison of the rendering quality between \our and OpenMVS is illustrated in Figure~\ref{fig:compare_openmvs}. It is evident that, powered by 3DGS, \our achieves superior visual rendering outcomes. This rationale explains why we do not directly utilize the results from OpenMVS for obtaining visual rendering results.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.\linewidth]{images/compare_openmvs.pdf}
    \caption{Visual comparison between \our and OpenMVS.}
    \label{fig:compare_openmvs}
\end{figure}

We already show the rendering results of \texttt{pick and drop a bottle into the basket} task in Fig.~\ref{fig:visual qualitative comparison}. Here we present multi-view renderings of the data collection process across the other three sim-to-real tasks: \texttt{place a vegetable on the board}, \texttt{stack blocks}, and \texttt{clear objects on the table} in Fig.~\ref{fig:more-vis-place}, Fig.~\ref{fig:more-vis-stack} and Fig.~\ref{fig:more-vis-clear} separately. These visualizations provide a detailed demonstration of our data collection procedure.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.\linewidth]{images/place.jpg}
    \caption{Rendering results of \texttt{place a vegetable on the board} task.}
    \label{fig:more-vis-place}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.\linewidth]{images/stack.jpg}
    \caption{Rendering results of \texttt{stack blocks} task.}
    \label{fig:more-vis-stack}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.\linewidth]{images/multi-items.jpg}
    \caption{Rendering results of \texttt{clear objects on the table} task.}
    \label{fig:more-vis-clear}
\end{figure}
