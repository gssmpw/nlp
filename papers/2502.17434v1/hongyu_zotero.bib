
@article{kassuba_vision_2013,
	title = {Vision holds a greater share in visuo-haptic object recognition than touch},
	volume = {65},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811912009640},
	doi = {10.1016/j.neuroimage.2012.09.054},
	abstract = {The integration of visual and haptic input can facilitate object recognition. Yet, vision might dominate visuo-haptic interactions as it is more effective than haptics in processing several object features in parallel and recognizing objects outside of reaching space. The maximum likelihood approach of multisensory integration would predict that haptics as the less efficient sense for object recognition gains more from integrating additional visual information than vice versa. To test for asymmetries between vision and touch in visuo-haptic interactions, we measured regional changes in brain activity using functional magnetic resonance imaging while healthy individuals performed a delayed-match-to-sample task. We manipulated identity matching of sample and target objects: We hypothesized that only coherent visual and haptic object features would activate unified object representations. The bilateral object-specific lateral occipital cortex, fusiform gyrus, and intraparietal sulcus showed increased activation to crossmodal compared to unimodal matching but only for congruent object pairs. Critically, the visuo-haptic interaction effects in these regions depended on the sensory modality which processed the target object, being more pronounced for haptic than visual targets. This preferential response of visuo-haptic regions indicates a modality-specific asymmetry in crossmodal matching of visual and haptic object features, suggesting a functional primacy of vision over touch in visuo-haptic object recognition.},
	urldate = {2025-01-17},
	journal = {NeuroImage},
	author = {Kassuba, Tanja and Klinge, Corinna and Hölig, Cordula and Röder, Brigitte and Siebner, Hartwig R.},
	month = jan,
	year = {2013},
	keywords = {Multisensory interactions, Object recognition, Touch perception, Visual dominance, Visual perception, fMRI},
	pages = {59--68},
}

@misc{qi_simple_2025,
	title = {From {Simple} to {Complex} {Skills}: {The} {Case} of {In}-{Hand} {Object} {Reorientation}},
	shorttitle = {From {Simple} to {Complex} {Skills}},
	url = {http://arxiv.org/abs/2501.05439},
	doi = {10.48550/arXiv.2501.05439},
	abstract = {Learning policies in simulation and transferring them to the real world has become a promising approach in dexterous manipulation. However, bridging the sim-to-real gap for each new task requires substantial human effort, such as careful reward engineering, hyperparameter tuning, and system identification. In this work, we present a system that leverages low-level skills to address these challenges for more complex tasks. Specifically, we introduce a hierarchical policy for in-hand object reorientation based on previously acquired rotation skills. This hierarchical policy learns to select which low-level skill to execute based on feedback from both the environment and the low-level skill policies themselves. Compared to learning from scratch, the hierarchical policy is more robust to out-of-distribution changes and transfers easily from simulation to real-world environments. Additionally, we propose a generalizable object pose estimator that uses proprioceptive information, low-level skill predictions, and control errors as inputs to estimate the object pose over time. We demonstrate that our system can reorient objects, including symmetrical and textureless ones, to a desired pose.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Qi, Haozhi and Yi, Brent and Lambeta, Mike and Ma, Yi and Calandra, Roberto and Malik, Jitendra},
	month = jan,
	year = {2025},
	note = {arXiv:2501.05439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{suresh_neuralfeels_2024,
	title = {{NeuralFeels} with neural fields: {Visuotactile} perception for in-hand manipulation},
	copyright = {Copyright © 2024 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
	shorttitle = {{NeuralFeels} with neural fields},
	url = {https://www.science.org/doi/10.1126/scirobotics.adl0628},
	doi = {10.1126/scirobotics.adl0628},
	abstract = {Neural perception with vision and touch yields robust tracking and reconstruction of novel objects for in-hand manipulation.},
	language = {EN},
	urldate = {2025-01-17},
	journal = {Science Robotics},
	author = {Suresh, Sudharshan and Qi, Haozhi and Wu, Tingfan and Fan, Taosha and Pineda, Luis and Lambeta, Mike and Malik, Jitendra and Kalakrishnan, Mrinal and Calandra, Roberto and Kaess, Michael and Ortiz, Joseph and Mukadam, Mustafa},
	month = nov,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
}

@misc{hong_easyhec_2024,
	title = {{EasyHeC}++: {Fully} {Automatic} {Hand}-{Eye} {Calibration} with {Pretrained} {Image} {Models}},
	shorttitle = {{EasyHeC}++},
	url = {http://arxiv.org/abs/2410.09293},
	doi = {10.48550/arXiv.2410.09293},
	abstract = {Hand-eye calibration plays a fundamental role in robotics by directly influencing the efficiency of critical operations such as manipulation and grasping. In this work, we present a novel framework, EasyHeC++, designed for fully automatic hand-eye calibration. In contrast to previous methods that necessitate manual calibration, specialized markers, or the training of arm-specific neural networks, our approach is the first system that enables accurate calibration of any robot arm in a marker-free, training-free, and fully automatic manner. Our approach employs a two-step process. First, we initialize the camera pose using a sampling or feature-matching-based method with the aid of pretrained image models. Subsequently, we perform pose optimization through differentiable rendering. Extensive experiments demonstrate the system's superior accuracy in both synthetic and real-world datasets across various robot arms and camera settings. Project page: https://ootts.github.io/easyhec\_plus.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Hong, Zhengdong and Zheng, Kangfu and Chen, Linghao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09293 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{chen_easyhec_2023,
	title = {{EasyHeC}: {Accurate} and {Automatic} {Hand}-{Eye} {Calibration} {Via} {Differentiable} {Rendering} and {Space} {Exploration}},
	volume = {8},
	issn = {2377-3766},
	shorttitle = {{EasyHeC}},
	url = {https://ieeexplore.ieee.org/abstract/document/10251600},
	doi = {10.1109/LRA.2023.3315551},
	abstract = {Hand-eye calibration is a critical task in robotics, as it directly affects the efficacy of critical operations such as manipulation and grasping. Traditional methods for achieving this objective necessitate the careful design of joint poses and the use of specialized calibration markers, while most recent learning-based approaches using solely pose regression are limited in their abilities to diagnose inaccuracies. In this work, we introduce a new approach to hand-eye calibration called EasyHeC, which is markerless, white-box, and delivers superior accuracy and robustness. We propose to use two key technologies: differentiable rendering-based camera pose optimization and consistency-based joint space exploration, which enables accurate end-to-end optimization of the calibration process and eliminates the need for the laborious manual design of robot joint poses. Our evaluation demonstrates superior performance in synthetic and real-world datasets, enhancing downstream manipulation tasks by providing precise camera poses for locating and interacting with objects.},
	number = {11},
	urldate = {2025-01-15},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Linghao and Qin, Yuzhe and Zhou, Xiaowei and Su, Hao},
	month = nov,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Calibration, Calibration and identification, Cameras, Manipulators, Optimization, Rendering (computer graphics), Robot vision systems, Robots, computer vision for automation, recognition},
	pages = {7234--7241},
}

@misc{tang_kalib_2024,
	title = {Kalib: {Markerless} {Hand}-{Eye} {Calibration} with {Keypoint} {Tracking}},
	shorttitle = {Kalib},
	url = {http://arxiv.org/abs/2408.10562},
	doi = {10.48550/arXiv.2408.10562},
	abstract = {Hand-eye calibration involves estimating the transformation between the camera and the robot. Traditional methods rely on fiducial markers, involving much manual labor and careful setup. Recent advancements in deep learning offer markerless techniques, but they present challenges, including the need for retraining networks for each robot, the requirement of accurate mesh models for data generation, and the need to address the sim-to-real gap. In this letter, we propose Kalib, an automatic and universal markerless hand-eye calibration pipeline that leverages the generalizability of visual foundation models to eliminate these barriers. In each calibration process, Kalib uses keypoint tracking and proprioceptive sensors to estimate the transformation between a robot's coordinate space and its corresponding points in camera space. Our method does not require training new networks or access to mesh models. Through evaluations in simulation environments and the real-world dataset DROID, Kalib demonstrates superior accuracy compared to recent baseline methods. This approach provides an effective and flexible calibration process for various robot systems by simplifying setup and removing dependency on precise physical markers.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Tang, Tutian and Liu, Minghao and Xu, Wenqiang and Lu, Cewu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10562 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{rashidinejad_bridging_2021,
	title = {Bridging {Offline} {Reinforcement} {Learning} and {Imitation} {Learning}: {A} {Tale} of {Pessimism}},
	volume = {34},
	shorttitle = {Bridging {Offline} {Reinforcement} {Learning} and {Imitation} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/60ce36723c17bbac504f2ef4c8a46995-Abstract.html},
	urldate = {2025-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
	year = {2021},
	pages = {11702--11716},
}

@inproceedings{kendall_multi-task_2018,
	title = {Multi-{Task} {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.html},
	urldate = {2025-01-15},
	author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
	year = {2018},
	pages = {7482--7491},
}

@inproceedings{kumar_should_2021,
	title = {Should {I} {Run} {Offline} {Reinforcement} {Learning} or {Behavioral} {Cloning}?},
	url = {https://openreview.net/forum?id=AP1MKT37rJ},
	abstract = {Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing only previously collected experience, without any online interaction. While it is widely understood that offline RL is able to extract good policies even from highly suboptimal data, in practice offline RL is often used with data that resembles demonstrations. In this case, one can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning. It seems natural to ask: When should we prefer offline RL over BC? In this paper, our goal is to characterize environments and dataset compositions where offline RL leads to better performance than BC. In particular, we characterize the properties of environments that allow offline RL methods to perform better than BC methods even when only provided with expert data. Additionally, we show that policies trained on suboptimal data that is sufficiently noisy can attain better performance than even BC algorithms with expert data, especially on long-horizon problems. We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robot manipulation, maze navigation and Atari games, when learning from a variety of data sources. We observe that modern offline RL methods trained on suboptimal, noisy data in sparse reward domains outperform cloning the expert data in several practical problems.},
	language = {en},
	urldate = {2025-01-15},
	author = {Kumar, Aviral and Hong, Joey and Singh, Anikait and Levine, Sergey},
	month = oct,
	year = {2021},
}

@inproceedings{feng_finetuning_2023,
	title = {Finetuning {Offline} {World} {Models} in the {Real} {World}},
	url = {https://openreview.net/forum?id=JkFeyEC6VXV},
	abstract = {Reinforcement Learning (RL) is notoriously data-inefficient, which makes training on a real robot difficult. While model-based RL algorithms (world models) improve data-efficiency to some extent, they still require hours or days of interaction to learn skills. Recently, offline RL has been proposed as a framework for training RL policies on pre-existing datasets without any online interaction. However, constraining an algorithm to a fixed dataset induces a state-action distribution shift between training and inference, and limits its applicability to new tasks. In this work, we seek to get the best of both worlds: we consider the problem of pretraining a world model with offline data collected on a real robot, and then finetuning the model on online data collected by planning with the learned model. To mitigate extrapolation errors during online interaction, we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty. We evaluate our method on a variety of visuo-motor control tasks in simulation and on a real robot, and find that our method enables few-shot finetuning to seen and unseen tasks even when offline data is limited. Videos are available at https://yunhaifeng.com/FOWM},
	language = {en},
	urldate = {2025-01-15},
	author = {Feng, Yunhai and Hansen, Nicklas and Xiong, Ziyan and Rajagopalan, Chandramouli and Wang, Xiaolong},
	month = aug,
	year = {2023},
}

@inproceedings{kidambi_morel_2020,
	title = {{MOReL}: {Model}-{Based} {Offline} {Reinforcement} {Learning}},
	volume = {33},
	shorttitle = {{MOReL}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f7efa4f864ae9b88d43527f4b14f750f-Abstract.html},
	abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. This serves as an extreme test for an agent's ability to effectively use historical data which is known to be critical for efficient RL. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP using the offline dataset; (b) learning a near-optimal policy in this pessimistic MDP. The design of the pessimistic MDP is such that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the pessimistic MDP. This enables the pessimistic MDP to serve as a good surrogate for purposes of policy evaluation and learning. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Empirically, MOReL matches or exceeds state-of-the-art results on widely used offline RL benchmarks. Overall, the modular design of MOReL enables translating advances in its components (for e.g., in model learning, planning etc.) to improvements in offline RL.},
	urldate = {2025-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	year = {2020},
	pages = {21810--21823},
}

@inproceedings{xiao_spatialtracker_2024,
	title = {{SpatialTracker}: {Tracking} {Any} {2D} {Pixels} in {3D} {Space}},
	shorttitle = {{SpatialTracker}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_SpatialTracker_Tracking_Any_2D_Pixels_in_3D_Space_CVPR_2024_paper.html},
	language = {en},
	urldate = {2025-01-14},
	author = {Xiao, Yuxi and Wang, Qianqian and Zhang, Shangzhan and Xue, Nan and Peng, Sida and Shen, Yujun and Zhou, Xiaowei},
	year = {2024},
	pages = {20406--20417},
}

@misc{koppula_tapvid-3d_2024,
	title = {{TAPVid}-{3D}: {A} {Benchmark} for {Tracking} {Any} {Point} in {3D}},
	shorttitle = {{TAPVid}-{3D}},
	url = {http://arxiv.org/abs/2407.05921},
	doi = {10.48550/arXiv.2407.05921},
	abstract = {We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video. Code for dataset download, generation, and model evaluation is available at https://tapvid3d.github.io},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Koppula, Skanda and Rocco, Ignacio and Yang, Yi and Heyward, Joe and Carreira, João and Zisserman, Andrew and Brostow, Gabriel and Doersch, Carl},
	month = aug,
	year = {2024},
	note = {arXiv:2407.05921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yuan_general_2024,
	title = {General {Flow} as {Foundation} {Affordance} for {Scalable} {Robot} {Learning}},
	url = {http://arxiv.org/abs/2401.11439},
	doi = {10.48550/arXiv.2401.11439},
	abstract = {We address the challenge of acquiring real-world manipulation skills with a scalable framework. We hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize 3D flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target. To exploit scalable data resources, we turn our attention to human videos. We develop, for the first time, a language-conditioned 3D flow prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable guidance, thus facilitating zero-shot skill transfer in real-world scenarios. We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any in-domain finetuning, our method achieves an impressive 81{\textbackslash}\% success rate in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) wide application: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. Code, data, and supplementary materials are available https://general-flow.github.io},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Yuan, Chengbo and Wen, Chuan and Zhang, Tong and Gao, Yang},
	month = sep,
	year = {2024},
	note = {arXiv:2401.11439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{vecerik_robotap_2024,
	title = {{RoboTAP}: {Tracking} {Arbitrary} {Points} for {Few}-{Shot} {Visual} {Imitation}},
	shorttitle = {{RoboTAP}},
	url = {https://ieeexplore.ieee.org/abstract/document/10611409?casa_token=bh8jqg_V-4UAAAAA:c7R35tBNLFEuWGB23AoB5bH6lt5fuIwaumRooJddhsHQvWmC0Z6gSzMQO2rDWo90sIoM3hu5Ed0},
	doi = {10.1109/ICRA57147.2024.10611409},
	abstract = {For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.},
	urldate = {2025-01-14},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Vecerik, Mel and Doersch, Carl and Yang, Yi and Davchev, Todor and Aytar, Yusuf and Zhou, Guangyao and Hadsell, Raia and Agapito, Lourdes and Scholz, Jon},
	month = may,
	year = {2024},
	keywords = {Planning, Production facilities, Stacking, Task analysis, Tracking, Training, Visualization},
	pages = {5397--5403},
}

@inproceedings{seita_toolflownet_2023,
	title = {{ToolFlowNet}: {Robotic} {Manipulation} with {Tools} via {Predicting} {Tool} {Flow} from {Point} {Clouds}},
	shorttitle = {{ToolFlowNet}},
	url = {https://proceedings.mlr.press/v205/seita23a.html},
	abstract = {Point clouds are a widely available and canonical data modality which convey the 3D geometry of a scene. Despite significant progress in classification and segmentation from point clouds, policy learning from such a modality remains challenging, and most prior works in imitation learning focus on learning policies from images or state information. In this paper, we propose a novel framework for learning policies from point clouds for robotic manipulation with tools. We use a novel neural network, ToolFlowNet, which predicts dense per-point flow on the tool that the robot controls, and then uses the flow to derive the transformation that the robot should execute. We apply this framework to imitation learning of challenging deformable object manipulation tasks with continuous movement of tools, including scooping and pouring, and demonstrate significantly improved performance over baselines which do not use flow. We perform physical scooping experiments with ToolFlowNet and find that we can attain 82\% scooping success. See https://sites.google.com/view/point-cloud-policy/home for supplementary material.},
	language = {en},
	urldate = {2025-01-14},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Seita, Daniel and Wang, Yufei and Shetty, Sarthak J. and Li, Edward Yao and Erickson, Zackory and Held, David},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {1038--1049},
}

@misc{zhang_flowbot_2024,
	title = {{FlowBot}++: {Learning} {Generalized} {Articulated} {Objects} {Manipulation} via {Articulation} {Projection}},
	shorttitle = {{FlowBot}++},
	url = {http://arxiv.org/abs/2306.12893},
	doi = {10.48550/arXiv.2306.12893},
	abstract = {Understanding and manipulating articulated objects, such as doors and drawers, is crucial for robots operating in human environments. We wish to develop a system that can learn to articulate novel objects with no prior interaction, after training on other articulated objects. Previous approaches for articulated object manipulation rely on either modular methods which are brittle or end-to-end methods, which lack generalizability. This paper presents FlowBot++, a deep 3D vision-based robotic system that predicts dense per-point motion and dense articulation parameters of articulated objects to assist in downstream manipulation tasks. FlowBot++ introduces a novel per-point representation of the articulated motion and articulation parameters that are combined to produce a more accurate estimate than either method on their own. Simulated experiments on the PartNet-Mobility dataset validate the performance of our system in articulating a wide range of objects, while real-world experiments on real objects' point clouds and a Sawyer robot demonstrate the generalizability and feasibility of our system in real-world scenarios.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Zhang, Harry and Eisner, Ben and Held, David},
	month = may,
	year = {2024},
	note = {arXiv:2306.12893 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{eisner_flowbot3d_2024,
	title = {{FlowBot3D}: {Learning} {3D} {Articulation} {Flow} to {Manipulate} {Articulated} {Objects}},
	shorttitle = {{FlowBot3D}},
	url = {http://arxiv.org/abs/2205.04382},
	doi = {10.48550/arXiv.2205.04382},
	abstract = {We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable a robot to articulate unseen classes of objects. We propose a vision-based system that learns to predict the potential motions of the parts of a variety of articulated objects to guide downstream motion planning of the system to articulate the objects. To predict the object motions, we train a neural network to output a dense vector field representing the point-wise motion direction of the points in the point cloud under articulation. We then deploy an analytical motion planner based on this vector field to achieve a policy that yields maximum articulation. We train the vision system entirely in simulation, and we demonstrate the capability of our system to generalize to unseen object instances and novel categories in both simulation and the real world, deploying our policy on a Sawyer robot with no finetuning. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Eisner, Ben and Zhang, Harry and Held, David},
	month = may,
	year = {2024},
	note = {arXiv:2205.04382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wen_any-point_2024,
	title = {Any-point {Trajectory} {Modeling} for {Policy} {Learning}},
	url = {http://arxiv.org/abs/2401.00025},
	doi = {10.48550/arXiv.2401.00025},
	abstract = {Learning from demonstration is a powerful method for teaching robots new skills, and having more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Across over 130 language-conditioned tasks we evaluated in both simulation and the real world, ATM outperforms strong video pre-training baselines by 80\% on average. Furthermore, we show effective transfer learning of manipulation skills from human videos and videos from a different robot morphology. Visualizations and code are available at: {\textbackslash}url\{https://xingyu-lin.github.io/atm\}.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Wen, Chuan and Lin, Xingyu and So, John and Chen, Kai and Dou, Qi and Gao, Yang and Abbeel, Pieter},
	month = jul,
	year = {2024},
	note = {arXiv:2401.00025 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{sand_particle_2008,
	title = {Particle {Video}: {Long}-{Range} {Motion} {Estimation} {Using} {Point} {Trajectories}},
	volume = {80},
	issn = {1573-1405},
	shorttitle = {Particle {Video}},
	url = {https://doi.org/10.1007/s11263-008-0136-6},
	doi = {10.1007/s11263-008-0136-6},
	abstract = {This paper describes a new approach to motion estimation in video. We represent video motion using a set of particles. Each particle is an image point sample with a long-duration trajectory and other properties. To optimize particle trajectories we measure appearance consistency along the particle trajectories and distortion between the particles. The resulting motion representation is useful for a variety of applications and cannot be directly obtained using existing methods such as optical flow or feature tracking. We demonstrate the algorithm on challenging real-world videos that include complex scene geometry, multiple types of occlusion, regions with low texture, and non-rigid deformations.},
	language = {en},
	number = {1},
	urldate = {2025-01-14},
	journal = {International Journal of Computer Vision},
	author = {Sand, Peter and Teller, Seth},
	month = oct,
	year = {2008},
	keywords = {Artificial Intelligence, Feature tracking, Optical flow, Video motion estimation},
	pages = {72--91},
}

@misc{liu_joint_2025,
	title = {Joint {Optimization} for {4D} {Human}-{Scene} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2501.02158},
	doi = {10.48550/arXiv.2501.02158},
	abstract = {Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Liu, Zhizheng and Lin, Joe and Wu, Wayne and Zhou, Bolei},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02158 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bian_gs-dit_2025,
	title = {{GS}-{DiT}: {Advancing} {Video} {Generation} with {Pseudo} {4D} {Gaussian} {Fields} through {Efficient} {Dense} {3D} {Point} {Tracking}},
	shorttitle = {{GS}-{DiT}},
	url = {http://arxiv.org/abs/2501.02690},
	doi = {10.48550/arXiv.2501.02690},
	abstract = {4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Bian, Weikang and Huang, Zhaoyang and Shi, Xiaoyu and Li, Yijin and Wang, Fu-Yun and Li, Hongsheng},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02690 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_hawor_2025,
	title = {{HaWoR}: {World}-{Space} {Hand} {Motion} {Reconstruction} from {Egocentric} {Videos}},
	shorttitle = {{HaWoR}},
	url = {http://arxiv.org/abs/2501.02973},
	doi = {10.48550/arXiv.2501.02973},
	abstract = {Despite the advent in 3D hand pose estimation, current methods predominantly focus on single-image 3D hand reconstruction in the camera frame, overlooking the world-space motion of the hands. Such limitation prohibits their direct use in egocentric video settings, where hands and camera are continuously in motion. In this work, we propose HaWoR, a high-fidelity method for hand motion reconstruction in world coordinates from egocentric videos. We propose to decouple the task by reconstructing the hand motion in the camera space and estimating the camera trajectory in the world coordinate system. To achieve precise camera trajectory estimation, we propose an adaptive egocentric SLAM framework that addresses the shortcomings of traditional SLAM methods, providing robust performance under challenging camera dynamics. To ensure robust hand motion trajectories, even when the hands move out of view frustum, we devise a novel motion infiller network that effectively completes the missing frames of the sequence. Through extensive quantitative and qualitative evaluations, we demonstrate that HaWoR achieves state-of-the-art performance on both hand motion reconstruction and world-frame camera trajectory estimation under different egocentric benchmark datasets. Code and models are available on https://hawor-project.github.io/ .},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Zhang, Jinglei and Deng, Jiankang and Ma, Chao and Potamias, Rolandos Alexandros},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02973 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cong_videolifter_2025,
	title = {{VideoLifter}: {Lifting} {Videos} to {3D} with {Fast} {Hierarchical} {Stereo} {Alignment}},
	shorttitle = {{VideoLifter}},
	url = {http://arxiv.org/abs/2501.01949},
	doi = {10.48550/arXiv.2501.01949},
	abstract = {Efficiently reconstructing accurate 3D models from monocular video is a key challenge in computer vision, critical for advancing applications in virtual reality, robotics, and scene understanding. Existing approaches typically require pre-computed camera parameters and frame-by-frame reconstruction pipelines, which are prone to error accumulation and entail significant computational overhead. To address these limitations, we introduce VideoLifter, a novel framework that leverages geometric priors from a learnable model to incrementally optimize a globally sparse to dense 3D representation directly from video sequences. VideoLifter segments the video sequence into local windows, where it matches and registers frames, constructs consistent fragments, and aligns them hierarchically to produce a unified 3D model. By tracking and propagating sparse point correspondences across frames and fragments, VideoLifter incrementally refines camera poses and 3D structure, minimizing reprojection error for improved accuracy and robustness. This approach significantly accelerates the reconstruction process, reducing training time by over 82\% while surpassing current state-of-the-art methods in visual fidelity and computational efficiency.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Cong, Wenyan and Wang, Kevin and Lei, Jiahui and Stearns, Colton and Cai, Yuanhao and Wang, Dilin and Ranjan, Rakesh and Feiszli, Matt and Guibas, Leonidas and Wang, Zhangyang and Wang, Weiyao and Fan, Zhiwen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.01949 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{blanco-mulero_t-dom_2024,
	title = {T-{DOM}: {A} {Taxonomy} for {Robotic} {Manipulation} of {Deformable} {Objects}},
	shorttitle = {T-{DOM}},
	url = {http://arxiv.org/abs/2412.20998},
	doi = {10.48550/arXiv.2412.20998},
	abstract = {Robotic grasp and manipulation taxonomies, inspired by observing human manipulation strategies, can provide key guidance for tasks ranging from robotic gripper design to the development of manipulation algorithms. The existing grasp and manipulation taxonomies, however, often assume object rigidity, which limits their ability to reason about the complex interactions in the robotic manipulation of deformable objects. Hence, to assist in tasks involving deformable objects, taxonomies need to capture more comprehensively the interactions inherent in deformable object manipulation. To this end, we introduce T-DOM, a taxonomy that analyses key aspects involved in the manipulation of deformable objects, such as robot motion, forces, prehensile and non-prehensile interactions and, for the first time, a detailed classification of object deformations. To evaluate T-DOM, we curate a dataset of ten tasks involving a variety of deformable objects, such as garments, ropes, and surgical gloves, as well as diverse types of deformations. We analyse the proposed tasks comparing the T-DOM taxonomy with previous well established manipulation taxonomies. Our analysis demonstrates that T-DOM can effectively distinguish between manipulation skills that were not identified in other taxonomies, across different deformable objects and manipulation actions, offering new categories to characterize a skill. The proposed taxonomy significantly extends past work, providing a more fine-grained classification that can be used to describe the robotic manipulation of deformable objects. This work establishes a foundation for advancing deformable object manipulation, bridging theoretical understanding and practical implementation in robotic systems.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Blanco-Mulero, David and Dong, Yifei and Borras, Julia and Pokorny, Florian T. and Torras, Carme},
	month = dec,
	year = {2024},
	note = {arXiv:2412.20998 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{gupta_reflow6d_2024,
	title = {{ReFlow6D}: {Refraction}-{Guided} {Transparent} {Object} {6D} {Pose} {Estimation} via {Intermediate} {Representation} {Learning}},
	volume = {9},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{ReFlow6D}},
	url = {http://arxiv.org/abs/2412.20830},
	doi = {10.1109/LRA.2024.3455897},
	abstract = {Transparent objects are ubiquitous in daily life, making their perception and robotics manipulation important. However, they present a major challenge due to their distinct refractive and reflective properties when it comes to accurately estimating the 6D pose. To solve this, we present ReFlow6D, a novel method for transparent object 6D pose estimation that harnesses the refractive-intermediate representation. Unlike conventional approaches, our method leverages a feature space impervious to changes in RGB image space and independent of depth information. Drawing inspiration from image matting, we model the deformation of the light path through transparent objects, yielding a unique object-specific intermediate representation guided by light refraction that is independent of the environment in which objects are observed. By integrating these intermediate features into the pose estimation network, we show that ReFlow6D achieves precise 6D pose estimation of transparent objects, using only RGB images as input. Our method further introduces a novel transparent object compositing loss, fostering the generation of superior refractive-intermediate features. Empirical evaluations show that our approach significantly outperforms state-of-the-art methods on TOD and Trans32K-6D datasets. Robot grasping experiments further demonstrate that ReFlow6D's pose estimation accuracy effectively translates to real-world robotics task. The source code is available at: https://github.com/StoicGilgamesh/ReFlow6D and https://github.com/StoicGilgamesh/matting\_rendering.},
	number = {11},
	urldate = {2025-01-13},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gupta, Hrishikesh and Thalhammer, Stefan and Weibel, Jean-Baptiste and Haberl, Alexander and Vincze, Markus},
	month = nov,
	year = {2024},
	note = {arXiv:2412.20830 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {9438--9445},
}

@misc{pang_manivideo_2024,
	title = {{ManiVideo}: {Generating} {Hand}-{Object} {Manipulation} {Video} with {Dexterous} and {Generalizable} {Grasping}},
	shorttitle = {{ManiVideo}},
	url = {http://arxiv.org/abs/2412.16212},
	doi = {10.48550/arXiv.2412.16212},
	abstract = {In this paper, we introduce ManiVideo, a novel method for generating consistent and temporally coherent bimanual hand-object manipulation videos from given motion sequences of hands and objects. The core idea of ManiVideo is the construction of a multi-layer occlusion (MLO) representation that learns 3D occlusion relationships from occlusion-free normal maps and occlusion confidence maps. By embedding the MLO structure into the UNet in two forms, the model enhances the 3D consistency of dexterous hand-object manipulation. To further achieve the generalizable grasping of objects, we integrate Objaverse, a large-scale 3D object dataset, to address the scarcity of video data, thereby facilitating the learning of extensive object consistency. Additionally, we propose an innovative training strategy that effectively integrates multiple datasets, supporting downstream tasks such as human-centric hand-object manipulation video generation. Through extensive experiments, we demonstrate that our approach not only achieves video generation with plausible hand-object interaction and generalizable objects, but also outperforms existing SOTA methods.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Pang, Youxin and Shao, Ruizhi and Zhang, Jiajun and Tu, Hanzhang and Liu, Yun and Zhou, Boyao and Zhang, Hongwen and Liu, Yebin},
	month = dec,
	year = {2024},
	note = {arXiv:2412.16212 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fang_dimsam_2024,
	title = {{DiMSam}: {Diffusion} {Models} as {Samplers} for {Task} and {Motion} {Planning} under {Partial} {Observability}},
	shorttitle = {{DiMSam}},
	url = {https://ieeexplore.ieee.org/document/10802746?denied=},
	doi = {10.1109/IROS58592.2024.10802746},
	abstract = {Generative models such as diffusion models, excel at capturing high-dimensional distributions with diverse input modalities, e.g. robot trajectories, but are less effective at multistep constraint reasoning. Task and Motion Planning (TAMP) approaches are suited for planning multi-step autonomous robot manipulation. However, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by composing diffusion models using a TAMP system. We use the learned components for constraints and samplers that are difficult to engineer in the planning model, and use a TAMP solver to search for the task plan with constraint-satisfying action parameter values. To tractably make predictions for unseen objects in the environment, we define the learned samplers and TAMP operators on learned latent embedding of changing object states. We evaluate our approach in a simulated articulated object manipulation domain and show how the combination of classical TAMP, generative modeling, and latent embedding enables multi-step constraint-based reasoning. We also apply the learned sampler in the real world. Website: https://sites.google.com/view/dimsam-tamp.},
	urldate = {2025-01-12},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Fang, Xiaolin and Garrett, Caelan Reed and Eppner, Clemens and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack and Fox, Dieter},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Autonomous robots, Cognition, Diffusion models, Dynamics, Intelligent robots, Observability, Planning, Trajectory},
	pages = {1412--1419},
}

@inproceedings{fang_embodied_2024,
	title = {Embodied {Uncertainty}-{Aware} {Object} {Segmentation}},
	url = {https://ieeexplore.ieee.org/abstract/document/10801562},
	doi = {10.1109/IROS58592.2024.10801562},
	abstract = {We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg.},
	urldate = {2025-01-12},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Fang, Xiaolin and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Instance segmentation, Intelligent robots, Object segmentation, Uncertainty},
	pages = {2639--2646},
}

@inproceedings{falco_cross-modal_2017,
	title = {Cross-modal visuo-tactile object recognition using robotic active exploration},
	url = {https://ieeexplore.ieee.org/abstract/document/7989619},
	doi = {10.1109/ICRA.2017.7989619},
	abstract = {In this work, we propose a framework to deal with cross-modal visuo-tactile object recognition. By cross-modal visuo-tactile object recognition, we mean that the object recognition algorithm is trained only with visual data and is able to recognize objects leveraging only tactile perception. The proposed cross-modal framework is constituted by three main elements. The first is a unified representation of visual and tactile data, which is suitable for cross-modal perception. The second is a set of features able to encode the chosen representation for classification applications. The third is a supervised learning algorithm, which takes advantage of the chosen descriptor. In order to show the results of our approach, we performed experiments with 15 objects common in domestic and industrial environments. Moreover, we compare the performance of the proposed framework with the performance of 10 humans in a simple cross-modal recognition task.},
	urldate = {2025-01-10},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Falco, Pietro and Lu, Shuang and Cirillo, Andrea and Natale, Ciro and Pirozzi, Salvatore and Lee, Dongheui},
	month = may,
	year = {2017},
	keywords = {Histograms, Object recognition, Robot sensing systems, Three-dimensional displays, Training, Visualization},
	pages = {5273--5280},
}

@inproceedings{zhang_navid_2024,
	title = {{NaVid}: {Video}-based {VLM} {Plans} the {Next} {Step} for {Vision}-and-{Language} {Navigation}},
	volume = {20},
	isbn = {9798990284807},
	shorttitle = {{NaVid}},
	url = {https://www.roboticsproceedings.org/rss20/p079.html},
	urldate = {2025-01-10},
	author = {Zhang, Jiazhao and Wang, Kunyu and Xu, Rongtao and Zhou, Gengze and Hong, Yicong and Fang, Xiaomeng and Wu, Qi and Zhang, Zhizheng and Wang, He},
	month = jul,
	year = {2024},
}

@inproceedings{oller_tactile-driven_2024,
	title = {Tactile-{Driven} {Non}-{Prehensile} {Object} {Manipulation} via {Extrinsic} {Contact} {Mode} {Control}},
	volume = {20},
	isbn = {9798990284807},
	url = {https://www.roboticsproceedings.org/rss20/p135.html},
	urldate = {2025-01-10},
	author = {Oller, Miquel and Berenson, Dmitry and Fazeli, Nima},
	month = jul,
	year = {2024},
}

@article{liu_embodied_2020,
	title = {Embodied tactile perception and learning},
	volume = {6},
	issn = {2096-5958},
	url = {https://journals.sagepub.com/doi/abs/10.26599/BSA.2020.9050012},
	doi = {10.26599/BSA.2020.9050012},
	abstract = {Various living creatures exhibit embodiment intelligence, which is reflected by a collaborative interaction of the brain, body, and environment. The actual behavior of embodiment intelligence is generated by a continuous and dynamic interaction between a subject and the environment through information perception and physical manipulation. The physical interaction between a robot and the environment is the basis for realizing embodied perception and learning. Tactile information plays a critical role in this physical interaction process. It can be used to ensure safety, stability, and compliance, and can provide unique information that is difficult to capture using other perception modalities. However, due to the limitations of existing sensors and perception and learning methods, the development of robotic tactile research lags significantly behind other sensing modalities, such as vision and hearing, thereby seriously restricting the development of robotic embodiment intelligence. This paper presents the current challenges related to robotic tactile embodiment intelligence and reviews the theory and methods of robotic embodied tactile intelligence. Tactile perception and learning methods for embodiment intelligence can be designed based on the development of new large‐scale tactile array sensing devices, with the aim to make breakthroughs in the neuromorphic computing technology of tactile intelligence.},
	language = {en},
	number = {2},
	urldate = {2025-01-10},
	journal = {Brain Science Advances},
	author = {Liu, Huaping and Guo, Di and Sun, Fuchun and Yang, Wuqiang and Furber, Steve and Sun, Tengchen},
	month = jun,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {132--158},
}

@inproceedings{xiao_dexterous_2012,
	title = {Dexterous robotic hand grasp modeling using piecewise linear dynamic model},
	url = {https://ieeexplore.ieee.org/document/6343076},
	doi = {10.1109/MFI.2012.6343076},
	abstract = {Learning from sensor data is important in many robotic research areas, such as dexterous robotic hand grasping. In this paper, a piecewise linear dynamic model is proposed for analyzing robotic hand grasp. The combination of linear dynamic model and the switched systems can achieve better results in grasp learning due to its advantage of modeling multi-phase grasping process. To the best knowledge of the authors, this is the first time for piecewise linear dynamic model to be incorporated into the framework of modeling robotic hand grasp process. The performance of the proposed model is evaluated on our experimental system and shows promising results.},
	urldate = {2025-01-10},
	booktitle = {2012 {IEEE} {International} {Conference} on {Multisensor} {Fusion} and {Integration} for {Intelligent} {Systems} ({MFI})},
	author = {Xiao, Wei and Sun, Fuchun and Liu, Huaping and Liu, Heyu and He, Chao},
	month = sep,
	year = {2012},
	keywords = {Data models, Force, Grasping, Hidden Markov models, Tactile sensors},
	pages = {52--57},
}

@misc{zhang_scenic_2024,
	title = {{SCENIC}: {Scene}-aware {Semantic} {Navigation} with {Instruction}-guided {Control}},
	shorttitle = {{SCENIC}},
	url = {http://arxiv.org/abs/2412.15664},
	doi = {10.48550/arXiv.2412.15664},
	abstract = {Synthesizing natural human motion that adapts to complex environments while allowing creative control remains a fundamental challenge in motion synthesis. Existing models often fall short, either by assuming flat terrain or lacking the ability to control motion semantics through text. To address these limitations, we introduce SCENIC, a diffusion model designed to generate human motion that adapts to dynamic terrains within virtual scenes while enabling semantic control through natural language. The key technical challenge lies in simultaneously reasoning about complex scene geometry while maintaining text control. This requires understanding both high-level navigation goals and fine-grained environmental constraints. The model must ensure physical plausibility and precise navigation across varied terrain, while also preserving user-specified text control, such as ``carefully stepping over obstacles" or ``walking upstairs like a zombie." Our solution introduces a hierarchical scene reasoning approach. At its core is a novel scene-dependent, goal-centric canonicalization that handles high-level goal constraint, and is complemented by an ego-centric distance field that captures local geometric details. This dual representation enables our model to generate physically plausible motion across diverse 3D scenes. By implementing frame-wise text alignment, our system achieves seamless transitions between different motion styles while maintaining scene constraints. Experiments demonstrate our novel diffusion model generates arbitrarily long human motions that both adapt to complex scenes with varying terrain surfaces and respond to textual prompts. Additionally, we show SCENIC can generalize to four real-scene datasets. Our code, dataset, and models will be released at {\textbackslash}url\{https://virtualhumans.mpi-inf.mpg.de/scenic/\}.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Zhang, Xiaohan and Starke, Sebastian and Guzov, Vladimir and Zhang, Zhensong and Pellitero, Eduardo Pérez and Pons-Moll, Gerard},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15664 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-08},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	pages = {34892--34916},
}

@article{haddadin_franka_2022,
	title = {The {Franka} {Emika} {Robot}: {A} {Reference} {Platform} for {Robotics} {Research} and {Education}},
	volume = {29},
	issn = {1558-223X},
	shorttitle = {The {Franka} {Emika} {Robot}},
	url = {https://ieeexplore.ieee.org/abstract/document/9721535?casa_token=LFKuF_G02WcAAAAA:zM_p5rUza_MXk9hgFb5EFjqHLx_lWaARQKi_wTnrYTMG1eYiG6iz34UjlToFB6dhEZaboQUjUkg},
	doi = {10.1109/MRA.2021.3138382},
	abstract = {The importance of robots for industry, research, education, and society as a whole is steadily increasing as reflected by the number of available systems and installed robots, not only in industry but also in the public sector and households. Software-only robotics researchers usually rely on commercially available robots which, in the case of manipulators, are primarily designed for industrial purposes and are often far from their needs. This article is a hands-on tutorial on the Franka Emika robot, the first series of industrial artificial intelligence (AI)-ready tactile robot platforms. Beyond industrial use, the systems can be seamlessly expanded to fulfill the demands of research and education across all robotics and AI disciplines. To satisfy the needs of such a wide variety of fields, it provides three different interfaces: Desk, a high-level app-based user interface for easy and fast task programming; Robot Integrated Development Environment (RIDE), a command-based programming environment used to create high-performance robot skills that enables programming custom apps and integrating external sensors; and the Franka control interface (FCI), a 1-kHz low-level torque and position control interface that exploits the also-available Langrangian dynamics robot model. We take a close look at implementations with all interfaces, ranging from simple solutions, apps, and controllers to robot-learning examples illustrating how to exploit all the advantages of this platform in ongoing robotics research and education.},
	number = {2},
	urldate = {2025-01-04},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Haddadin, Sami and Parusel, Sven and Johannsmeier, Lars and Golz, Saskia and Gabl, Simon and Walch, Florian and Sabaghian, Mohamadreza and Jähne, Christoph and Hausperger, Lukas and Haddadin, Simon},
	month = jun,
	year = {2022},
	note = {Conference Name: IEEE Robotics \& Automation Magazine},
	keywords = {Artificial intelligence, Education, Research and development, Robot kinematics, Robot sensing systems, Robots, Sensors, Service robots},
	pages = {46--64},
}

@inproceedings{wen_robust_2020,
	title = {Robust, {Occlusion}-aware {Pose} {Estimation} for {Objects} {Grasped} by {Adaptive} {Hands}},
	url = {https://ieeexplore.ieee.org/abstract/document/9197350},
	doi = {10.1109/ICRA40945.2020.9197350},
	abstract = {Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose.},
	urldate = {2024-09-22},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wen, Bowen and Mitash, Chaitanya and Soorian, Sruthi and Kimmel, Andrew and Sintov, Avishai and Bekris, Kostas E.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Computational modeling, Pose estimation, Robot sensing systems, Robustness, Solid modeling, Three-dimensional displays},
	pages = {6210--6217},
}

@inproceedings{li_hypertaxel_2024,
	title = {{HyperTaxel}: {Hyper}-{Resolution} for {Taxel}-{Based} {Tactile} {Signals} {Through} {Contrastive} {Learning}},
	shorttitle = {{HyperTaxel}},
	url = {https://ieeexplore.ieee.org/abstract/document/10802001},
	doi = {10.1109/IROS58592.2024.10802001},
	abstract = {To achieve dexterity comparable to that of humans, robots must intelligently process tactile sensor data. Taxel-based tactile signals often have low spatial-resolution, with non-standardized representations. In this paper, we propose a novel framework, HyperTaxel, for learning a geometrically-informed representation of taxel-based tactile signals to address challenges associated with their spatial resolution. We use this representation and a contrastive learning objective to encode and map sparse low-resolution taxel signals to high-resolution contact surfaces. To address the uncertainty inherent in these signals, we leverage joint probability distributions across multiple simultaneous contacts to improve taxel hyper-resolution. We evaluate our representation by comparing it with two baselines and present results that suggest our representation outperforms the baselines. Furthermore, we present qualitative results that demonstrate the learned representation captures the geometric features of the contact surface, such as flatness, curvature, and edges, and generalizes across different objects and sensor configurations. Moreover, we present results that suggest our representation improves the performance of various downstream tasks, such as surface classification, 6D in-hand pose estimation, and sim-to-real transfer.},
	urldate = {2025-01-03},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Li, Hongyu and Dikhale, Snehal and Cui, Jinda and Iba, Soshi and Jamali, Nawid},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Contrastive learning, Databases, Geometry, Intelligent robots, Location awareness, Pose estimation, Probability distribution, Spatial resolution, Tactile sensors, Uncertainty},
	pages = {7499--7506},
}

@misc{zhang_energymogen_2024,
	title = {{EnergyMoGen}: {Compositional} {Human} {Motion} {Generation} with {Energy}-{Based} {Diffusion} {Model} in {Latent} {Space}},
	shorttitle = {{EnergyMoGen}},
	url = {http://arxiv.org/abs/2412.14706},
	doi = {10.48550/arXiv.2412.14706},
	abstract = {Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Zhang, Jianrong and Fan, Hehe and Yang, Yi},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14706 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_gurecon_2024,
	title = {{GURecon}: {Learning} {Detailed} {3D} {Geometric} {Uncertainties} for {Neural} {Surface} {Reconstruction}},
	shorttitle = {{GURecon}},
	url = {http://arxiv.org/abs/2412.14939},
	doi = {10.48550/arXiv.2412.14939},
	abstract = {Neural surface representation has demonstrated remarkable success in the areas of novel view synthesis and 3D reconstruction. However, assessing the geometric quality of 3D reconstructions in the absence of ground truth mesh remains a significant challenge, due to its rendering-based optimization process and entangled learning of appearance and geometry with photometric losses. In this paper, we present a novel framework, i.e, GURecon, which establishes a geometric uncertainty field for the neural surface based on geometric consistency. Different from existing methods that rely on rendering-based measurement, GURecon models a continuous 3D uncertainty field for the reconstructed surface, and is learned by an online distillation approach without introducing real geometric information for supervision. Moreover, in order to mitigate the interference of illumination on geometric consistency, a decoupled field is learned and exploited to finetune the uncertainty field. Experiments on various datasets demonstrate the superiority of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play extension to various neural surface representations and improvement on downstream tasks such as incremental reconstruction. The code and supplementary material are available on the project website: https://zju3dv.github.io/GURecon/.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Yang, Zesong and Zhang, Ru and Shi, Jiale and Ai, Zixiang and Zhao, Boming and Bao, Hujun and Yang, Luwei and Cui, Zhaopeng},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14939 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_embedding_2024,
	title = {Embedding high-resolution touch across robotic hands enables adaptive human-like grasping},
	url = {http://arxiv.org/abs/2412.14482},
	doi = {10.48550/arXiv.2412.14482},
	abstract = {Developing robotic hands that adapt to real-world dynamics remains a fundamental challenge in robotics and machine intelligence. Despite significant advances in replicating human hand kinematics and control algorithms, robotic systems still struggle to match human capabilities in dynamic environments, primarily due to inadequate tactile feedback. To bridge this gap, we present F-TAC Hand, a biomimetic hand featuring high-resolution tactile sensing (0.1mm spatial resolution) across 70\% of its surface area. Through optimized hand design, we overcome traditional challenges in integrating high-resolution tactile sensors while preserving the full range of motion. The hand, powered by our generative algorithm that synthesizes human-like hand configurations, demonstrates robust grasping capabilities in dynamic real-world conditions. Extensive evaluation across 600 real-world trials demonstrates that this tactile-embodied system significantly outperforms non-tactile alternatives in complex manipulation tasks (p{\textless}0.0001). These results provide empirical evidence for the critical role of rich tactile embodiment in developing advanced robotic intelligence, offering new perspectives on the relationship between physical sensing capabilities and intelligent behavior.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Zhao, Zihang and Li, Wanlin and Li, Yuyang and Liu, Tengyu and Li, Boren and Wang, Meng and Du, Kai and Liu, Hangxin and Zhu, Yixin and Wang, Qining and Althoefer, Kaspar and Zhu, Song-Chun},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14482 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{eftekhar_one_2024,
	title = {The {One} {RING}: a {Robotic} {Indoor} {Navigation} {Generalist}},
	shorttitle = {The {One} {RING}},
	url = {http://arxiv.org/abs/2412.14401},
	doi = {10.48550/arXiv.2412.14401},
	abstract = {Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific; a policy learned using one robot's configuration does not typically gracefully generalize to another. Even small changes in the body size or camera viewpoint may cause failures. With the recent surge in custom hardware developments, it is necessary to learn a single policy that can be transferred to other embodiments, eliminating the need to (re)train for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy, trained solely in simulation with diverse randomly initialized embodiments at scale. Specifically, we augment the AI2-THOR simulator with the ability to instantiate robot embodiments with controllable configurations, varying across body size, rotation pivot point, and camera configurations. In the visual object-goal navigation task, RING achieves robust performance on real unseen robot platforms (Stretch RE-1, LoCoBot, Unitree's Go1), achieving an average of 72.1\% and 78.9\% success rate across 5 embodiments in simulation and 4 robot platforms in the real world. (project website: https://one-ring-policy.allen.ai/)},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Eftekhar, Ainaz and Weihs, Luca and Hendrix, Rose and Caglar, Ege and Salvador, Jordi and Herrasti, Alvaro and Han, Winson and VanderBil, Eli and Kembhavi, Aniruddha and Farhadi, Ali and Krishna, Ranjay and Ehsani, Kiana and Zeng, Kuo-Hao},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14401 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{tian_predictive_2024,
	title = {Predictive {Inverse} {Dynamics} {Models} are {Scalable} {Learners} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2412.15109},
	doi = {10.48550/arXiv.2412.15109},
	abstract = {Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to realworld scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the synergy between vision and action, Seer significantly outperforms previous methods across both simulation and real-world experiments. It achieves improvements of 13\% on the LIBERO-LONG benchmark, 21\% on CALVIN ABC-D, and 43\% in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D benchmark, achieving an average length of 4.28, and exhibits superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances on real-world scenarios. Code and models are publicly available at https://github.com/OpenRobotLab/Seer/.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Tian, Yang and Yang, Sizhe and Zeng, Jia and Wang, Ping and Lin, Dahua and Dong, Hao and Pang, Jiangmiao},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15109 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{barcellona_dream_2024,
	title = {Dream to {Manipulate}: {Compositional} {World} {Models} {Empowering} {Robot} {Imitation} {Learning} with {Imagination}},
	shorttitle = {Dream to {Manipulate}},
	url = {http://arxiv.org/abs/2412.14957},
	doi = {10.48550/arXiv.2412.14957},
	abstract = {A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world applications. In this paper, we introduce a new paradigm for constructing world models that are explicit representations of the real world and its dynamics. By integrating cutting-edge advances in real-time photorealism with Gaussian Splatting and physics simulators, we propose the first compositional manipulation world model, which we call DreMa. DreMa replicates the observed world and its dynamics, allowing it to imagine novel configurations of objects and predict the future consequences of robot actions. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in both accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page and source code can be found in https://leobarcellona.github.io/DreamToManipulate/},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Barcellona, Leonardo and Zadaianchuk, Andrii and Allegro, Davide and Papa, Samuele and Ghidoni, Stefano and Gavves, Efstratios},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14957 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{hu_video_2024,
	title = {Video {Prediction} {Policy}: {A} {Generalist} {Robot} {Policy} with {Predictive} {Visual} {Representations}},
	shorttitle = {Video {Prediction} {Policy}},
	url = {http://arxiv.org/abs/2412.14803},
	doi = {10.48550/arXiv.2412.14803},
	abstract = {Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1{\textbackslash}\% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8{\textbackslash}\% increase in success rates for complex real-world dexterous manipulation tasks.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Hu, Yucheng and Guo, Yanjiang and Wang, Pengchao and Chen, Xiaoyu and Wang, Yen-Jen and Zhang, Jianke and Sreenath, Koushil and Lu, Chaochao and Chen, Jianyu},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14803 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wu_robomind_2024,
	title = {{RoboMIND}: {Benchmark} on {Multi}-embodiment {Intelligence} {Normative} {Data} for {Robot} {Manipulation}},
	shorttitle = {{RoboMIND}},
	url = {http://arxiv.org/abs/2412.13877},
	doi = {10.48550/arXiv.2412.13877},
	abstract = {Developing robust and general-purpose robotic manipulation policies is a key goal in the field of robotics. To achieve effective generalization, it is essential to construct comprehensive datasets that encompass a large number of demonstration trajectories and diverse tasks. Unlike vision or language data that can be collected from the Internet, robotic datasets require detailed observations and manipulation actions, necessitating significant investment in hardware-software infrastructure and human labor. While existing works have focused on assembling various individual robot datasets, there remains a lack of a unified data collection standard and insufficient diversity in tasks, scenarios, and robot types. In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot manipulation), featuring 55k real-world demonstration trajectories across 279 diverse tasks involving 61 different object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view RGB-D images, proprioceptive robot state information, end effector details, and linguistic task descriptions. To ensure dataset consistency and reliability during policy learning, RoboMIND is built on a unified data collection platform and standardized protocol, covering four distinct robotic embodiments. We provide a thorough quantitative and qualitative analysis of RoboMIND across multiple dimensions, offering detailed insights into the diversity of our datasets. In our experiments, we conduct extensive real-world testing with four state-of-the-art imitation learning methods, demonstrating that training with RoboMIND data results in a high manipulation success rate and strong generalization. Our project is at https://x-humanoid-robomind.github.io/.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Wu, Kun and Hou, Chengkai and Liu, Jiaming and Che, Zhengping and Ju, Xiaozhu and Yang, Zhuqin and Li, Meng and Zhao, Yinuo and Xu, Zhiyuan and Yang, Guang and Zhao, Zhen and Li, Guangyu and Jin, Zhao and Wang, Lecheng and Mao, Jilei and Wang, Xinhua and Fan, Shichao and Liu, Ning and Ren, Pei and Zhang, Qiang and Lyu, Yaoxu and Liu, Mengzhen and He, Jingyang and Luo, Yulin and Gao, Zeyu and Li, Chenxuan and Gu, Chenyang and Fu, Yankai and Wu, Di and Wang, Xingyu and Chen, Sixiang and Wang, Zhenyu and An, Pengju and Qian, Siyuan and Zhang, Shanghang and Tang, Jian},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13877 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{roh_catsplat_2024,
	title = {{CATSplat}: {Context}-{Aware} {Transformer} with {Spatial} {Guidance} for {Generalizable} {3D} {Gaussian} {Splatting} from {A} {Single}-{View} {Image}},
	shorttitle = {{CATSplat}},
	url = {http://arxiv.org/abs/2412.12906},
	doi = {10.48550/arXiv.2412.12906},
	abstract = {Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Roh, Wonseok and Jung, Hwanhee and Kim, Jong Wook and Lee, Seunggwan and Yoo, Innfarn and Lugmayr, Andreas and Chi, Seunggeun and Ramani, Karthik and Kim, Sangpil},
	month = dec,
	year = {2024},
	note = {arXiv:2412.12906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yu_dyn-hamr_2024,
	title = {Dyn-{HaMR}: {Recovering} {4D} {Interacting} {Hand} {Motion} from a {Dynamic} {Camera}},
	shorttitle = {Dyn-{HaMR}},
	url = {http://arxiv.org/abs/2412.12861},
	doi = {10.48550/arXiv.2412.12861},
	abstract = {We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods. Through extensive evaluations on both in-the-wild and indoor datasets, we show that our approach significantly outperforms state-of-the-art methods in terms of 4D global mesh recovery. This establishes a new benchmark for hand motion reconstruction from monocular video with moving cameras. Our project page is at https://dyn-hamr.github.io/.},
	urldate = {2025-01-01},
	publisher = {arXiv},
	author = {Yu, Zhengdi and Zafeiriou, Stefanos and Birdal, Tolga},
	month = dec,
	year = {2024},
	note = {arXiv:2412.12861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{helbig_optimal_2007,
	title = {Optimal integration of shape information from vision and touch},
	volume = {179},
	issn = {1432-1106},
	url = {https://doi.org/10.1007/s00221-006-0814-y},
	doi = {10.1007/s00221-006-0814-y},
	abstract = {Many tasks can be carried out by using several sources of information. For example, an object’s size and shape can be judged based on visual as well as haptic cues. It has been shown recently that human observers integrate visual and haptic size information in a statistically optimal fashion, in the sense that the integrated estimate is most reliable (Ernst and Banks in Nature 415:429–433, 2002). In the present study, we tested whether this holds also for visual and haptic shape information. In previous studies virtual stimuli were used to test for optimality in integration. Virtual displays may, however, contain additional inappropriate cues that provide conflicting information and thus affect cue integration. Therefore, we studied optimal integration using real objects. Furthermore, we presented visual information via mirrors to create a spatial separation between visual and haptic cues while observers saw their hand touching the object and thus, knew that they were seeing and feeling the same object. Does this knowledge promote integration even though signals are spatially discrepant which has been shown to lead to a breakdown of integration (Gepshtein et al. in J Vis 5:1013–1023, 2005)? Consistent with the model predictions, observers weighted visual and haptic cues to shape according to their reliability: progressively more weight was given to haptics when visual information became less reliable. Moreover, the integrated visual–haptic estimate was more reliable than either unimodal estimate. These findings suggest that observers integrate visual and haptic shape information of real 3D objects. Thereby, knowledge that multisensory signals arise from the same object seems to promote integration.},
	language = {en},
	number = {4},
	urldate = {2024-12-29},
	journal = {Experimental Brain Research},
	author = {Helbig, Hannah B. and Ernst, Marc O.},
	month = jun,
	year = {2007},
	keywords = {Multisensory integration, Optimal perception, Shape information, Touch, Vision},
	pages = {595--606},
}

@article{helbig_neural_2012,
	title = {The neural mechanisms of reliability weighted integration of shape information from vision and touch},
	volume = {60},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811911011475},
	doi = {10.1016/j.neuroimage.2011.09.072},
	abstract = {Behaviourally, humans have been shown to integrate multisensory information in a statistically-optimal fashion by averaging the individual unisensory estimates according to their relative reliabilities. This form of integration is optimal in that it yields the most reliable (i.e. least variable) multisensory percept. The present study investigates the neural mechanisms underlying integration of visual and tactile shape information at the macroscopic scale of the regional BOLD response. Observers discriminated the shapes of ellipses that were presented bimodally (visual–tactile) or visually alone. A 2×5 factorial design manipulated (i) the presence vs. absence of tactile shape information and (ii) the reliability of the visual shape information (five levels). We then investigated whether regional activations underlying tactile shape discrimination depended on the reliability of visual shape. Indeed, in primary somatosensory cortices (bilateral BA2) and the superior parietal lobe the responses to tactile shape input were increased when the reliability of visual shape information was reduced. Conversely, tactile inputs suppressed visual activations in the right posterior fusiform gyrus, when the visual signal was blurred and unreliable. Somatosensory and visual cortices may sustain integration of visual and tactile shape information either via direct connections from visual areas or top-down effects from higher order parietal areas.},
	number = {2},
	urldate = {2024-12-29},
	journal = {NeuroImage},
	author = {Helbig, Hannah B. and Ernst, Marc O. and Ricciardi, Emiliano and Pietrini, Pietro and Thielscher, Axel and Mayer, Katja M. and Schultz, Johannes and Noppeney, Uta},
	month = apr,
	year = {2012},
	keywords = {Maximum Likelihood Estimation, Multisensory integration, Postcentral sulcus, Shape, Touch, Vision, fMRI},
	pages = {1063--1072},
}

@article{takahashi_visual-haptic_2014,
	title = {Visual-haptic integration with pliers and tongs: signal “weights” take account of changes in haptic sensitivity caused by different tools},
	volume = {5},
	issn = {1664-1078},
	shorttitle = {Visual-haptic integration with pliers and tongs},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2014.00109/full},
	doi = {10.3389/fpsyg.2014.00109},
	abstract = {{\textless}p{\textgreater}When we hold an object while looking at it, estimates from visual and haptic cues to size are combined in a statistically optimal fashion, whereby the “weight” given to each signal reflects their relative reliabilities. This allows object properties to be estimated more precisely than would otherwise be possible. Tools such as pliers and tongs systematically perturb the mapping between object size and the hand opening. This could complicate visual-haptic integration because it may alter the reliability of the haptic signal, thereby disrupting the determination of appropriate signal weights. To investigate this we first measured the reliability of haptic size estimates made with virtual pliers-like tools (created using a stereoscopic display and force-feedback robots) with different “gains” between hand opening and object size. Haptic reliability in tool use was straightforwardly determined by a combination of sensitivity to changes in hand opening and the effects of tool geometry. The precise pattern of sensitivity to hand opening, which violated Weber's law, meant that haptic reliability changed with tool gain. We then examined whether the visuo-motor system accounts for these reliability changes. We measured the weight given to visual and haptic stimuli when both were available, again with different tool gains, by measuring the perceived size of stimuli in which visual and haptic sizes were varied independently. The weight given to each sensory cue changed with tool gain in a manner that closely resembled the predictions of optimal sensory integration. The results are consistent with the idea that different tool geometries are modeled by the brain, allowing it to calculate not only the distal properties of objects felt with tools, but also the certainty with which those properties are known. These findings highlight the flexibility of human sensory integration and tool-use, and potentially provide an approach for optimizing the design of visual-haptic devices.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-12-29},
	journal = {Frontiers in Psychology},
	author = {Takahashi, Chie and Watt, Simon Justin},
	month = feb,
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Tool Use, cue weighting, haptic perception, multisensory integration, optimality, vision},
}

@incollection{lacey_chapter_2020,
	title = {Chapter 7 - {Visuo}-haptic object perception},
	isbn = {978-0-12-812492-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128124925000073},
	abstract = {Vision and touch have many similarities in their processing of information, manifested in multiple behavioral similarities in terms of categorization, recognition, and individual differences. This chapter reviews how these similarities contribute to multisensory object processing. For example, similar unisensory visual and haptic representations are integrated into a multisensory representation that supports both visuo-haptic crossmodal object recognition and view-independence. These behavioral similarities between vision and touch, and the evidence for integration of visual and haptic information into a multisensory representation, imply a shared neural basis for visuo-haptic object processing. We review the evidence that several brain regions, previously thought to be specialized for aspects of visual processing, are additionally active during analogous haptic tasks. Finally, we describe a model of visuo-haptic multisensory object recognition in which the object-selective lateral occipital complex is served by both top-down and bottom-up pathways depending on object familiarity and variable involvement of object and spatial imagery processes.},
	urldate = {2024-12-29},
	booktitle = {Multisensory {Perception}},
	publisher = {Academic Press},
	author = {Lacey, Simon and Sathian, K.},
	editor = {Sathian, K. and Ramachandran, V. S.},
	month = jan,
	year = {2020},
	doi = {10.1016/B978-0-12-812492-5.00007-3},
	keywords = {Categorization, Crossmodal, Effective connectivity, Metamodal, Multisensory, Object imagery, Recognition, Spatial imagery, View-dependence, fMRI},
	pages = {157--178},
}

@inproceedings{tremblay_deep_2018,
	title = {Deep {Object} {Pose} {Estimation} for {Semantic} {Robotic} {Grasping} of {Household} {Objects}},
	url = {https://proceedings.mlr.press/v87/tremblay18a.html},
	abstract = {Using synthetic data for training deep neural networks for robotic manipulation holds the promise of an almost unlimited amount of pre-labeled training data, generated safely out of harm’s way. One of the key challenges of synthetic data, to date, has been to bridge the so-called reality gap, so that networks trained on synthetic data operate correctly when exposed to real-world data. We explore the reality gap in the context of 6-DoF pose estimation of known objects from a single RGB image. We show that for this problem the reality gap can be successfully spanned by a simple combination of domain randomized and photorealistic data. Using synthetic data generated in this manner, we introduce a one-shot deep neural network that is able to perform competitively against a state-of-the-art network trained on a combination of real and synthetic data. To our knowledge, this is the first deep network trained only on synthetic data that is able to achieve state-of-the-art performance on 6-DoF object pose estimation. Our network also generalizes better to novel environments including extreme lighting conditions, for which we show qualitative results. Using this network we demonstrate a real-time system estimating object poses with sufficient accuracy for real-world semantic grasping of known household objects in clutter by a real robot.},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Tremblay, Jonathan and To, Thang and Sundaralingam, Balakumar and Xiang, Yu and Fox, Dieter and Birchfield, Stan},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {306--316},
}

@misc{ding_bunny-visionpro_2024,
	title = {Bunny-{VisionPro}: {Real}-{Time} {Bimanual} {Dexterous} {Teleoperation} for {Imitation} {Learning}},
	shorttitle = {Bunny-{VisionPro}},
	url = {http://arxiv.org/abs/2407.03162},
	doi = {10.48550/arXiv.2407.03162},
	abstract = {Teleoperation is a crucial tool for collecting human demonstrations, but controlling robots with bimanual dexterous hands remains a challenge. Existing teleoperation systems struggle to handle the complexity of coordinating two hands for intricate manipulations. We introduce Bunny-VisionPro, a real-time bimanual dexterous teleoperation system that leverages a VR headset. Unlike previous vision-based teleoperation systems, we design novel low-cost devices to provide haptic feedback to the operator, enhancing immersion. Our system prioritizes safety by incorporating collision and singularity avoidance while maintaining real-time performance through innovative designs. Bunny-VisionPro outperforms prior systems on a standard task suite, achieving higher success rates and reduced task completion times. Moreover, the high-quality teleoperation demonstrations improve downstream imitation learning performance, leading to better generalizability. Notably, Bunny-VisionPro enables imitation learning with challenging multi-stage, long-horizon dexterous manipulation tasks, which have rarely been addressed in previous work. Our system's ability to handle bimanual manipulations while prioritizing safety and real-time performance makes it a powerful tool for advancing dexterous manipulation and imitation learning.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Ding, Runyu and Qin, Yuzhe and Zhu, Jiyue and Jia, Chengzhe and Yang, Shiqi and Yang, Ruihan and Qi, Xiaojuan and Wang, Xiaolong},
	month = jul,
	year = {2024},
	note = {arXiv:2407.03162 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{taylor_gelslim_2022,
	title = {{GelSlim} 3.0: {High}-{Resolution} {Measurement} of {Shape}, {Force} and {Slip} in a {Compact} {Tactile}-{Sensing} {Finger}},
	shorttitle = {{GelSlim} 3.0},
	url = {https://ieeexplore.ieee.org/abstract/document/9811832},
	doi = {10.1109/ICRA46639.2022.9811832},
	abstract = {This work presents a new version of tactile-sensing finger, GelSlim 3.0, which integrates the ability to sense high-resolution shape, force, and slip in a more compact form factor than previous implementations, designed for cluttered bin-picking scenarios. The novel design integrates real-time model-based algorithms to measure shape, estimate the 3-D contact force distribution, and detect incipient slip. The constraints imposed by the photometric stereo algorithm used for depth reconstruction and the implementation of a planar sensing surface make the miniaturization of previous designs nontrivial. To achieve a compact integration, we optimize the optical path from illumination source to camera. Using an optical simulation environment, we develop an illumination shaping lens and position the source LEDs and camera. The optimized optical configuration is integrated into a finger design composed of a robust and easily replaceable snap-to-fit fingertip module that facilitates manufacture, assembly, use, and repair. To stimulate future research in tactile-sensing and provide the robotics community access to a reliable and easily reproducible tactile finger with a diversity of sensing modalities, we open-source the design, fabrication methods, and software at https://github.com/mcubelab/gelslim.},
	urldate = {2024-08-10},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Taylor, Ian H. and Dong, Siyuan and Rodriguez, Alberto},
	month = may,
	year = {2022},
	keywords = {Fingers, Force, Force measurement, Integrated optics, Lighting, Optical device fabrication, Shape},
	pages = {10781--10787},
}

@inproceedings{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	volume = {30},
	shorttitle = {{PointNet}++},
	url = {https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html},
	abstract = {Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2024-12-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
	year = {2017},
}

@misc{xu_gaussianproperty_2024,
	title = {{GaussianProperty}: {Integrating} {Physical} {Properties} to {3D} {Gaussians} with {LMMs}},
	shorttitle = {{GaussianProperty}},
	url = {http://arxiv.org/abs/2412.11258},
	doi = {10.48550/arXiv.2412.11258},
	abstract = {Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on {\textbackslash}href\{https://Gaussian-Property.github.io\}\{this https URL\}.},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Xu, Xinli and Ge, Wenhang and Qiu, Dicong and Chen, ZhiFei and Yan, Dongyu and Liu, Zhuoyun and Zhao, Haoyu and Zhao, Hanfeng and Zhang, Shunsi and Liang, Junwei and Chen, Ying-Cong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.11258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_rldg_2024,
	title = {{RLDG}: {Robotic} {Generalist} {Policy} {Distillation} via {Reinforcement} {Learning}},
	shorttitle = {{RLDG}},
	url = {http://arxiv.org/abs/2412.09858},
	doi = {10.48550/arXiv.2412.09858},
	abstract = {Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), a method that leverages reinforcement learning to generate high-quality training data for finetuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40\% higher success rates while generalizing better to new tasks. We also provide a detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers a promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers. Videos and code can be found on our project website https://generalist-distillation.github.io},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Xu, Charles and Li, Qiyang and Luo, Jianlan and Levine, Sergey},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09858 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jin_stereo4d_2024,
	title = {{Stereo4D}: {Learning} {How} {Things} {Move} in {3D} from {Internet} {Stereo} {Videos}},
	shorttitle = {{Stereo4D}},
	url = {http://arxiv.org/abs/2412.09621},
	doi = {10.48550/arXiv.2412.09621},
	abstract = {Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io},
	urldate = {2024-12-22},
	publisher = {arXiv},
	author = {Jin, Linyi and Tucker, Richard and Li, Zhengqi and Fouhey, David and Snavely, Noah and Holynski, Aleksander},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09621 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huang_normalflow_2025,
	title = {{NormalFlow}: {Fast}, {Robust}, and {Accurate} {Contact}-based {Object} {6DoF} {Pose} {Tracking} with {Vision}-based {Tactile} {Sensors}},
	volume = {10},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{NormalFlow}},
	url = {http://arxiv.org/abs/2412.09617},
	doi = {10.1109/LRA.2024.3505815},
	abstract = {Tactile sensing is crucial for robots aiming to achieve human-level dexterity. Among tactile-dependent skills, tactile-based object tracking serves as the cornerstone for many tasks, including manipulation, in-hand manipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a fast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging the precise surface normal estimation of vision-based tactile sensors, NormalFlow determines object movements by minimizing discrepancies between the tactile-derived surface normals. Our results show that NormalFlow consistently outperforms competitive baselines and can track low-texture objects like table surfaces. For long-horizon tracking, we demonstrate when rolling the sensor around a bead for 360 degrees, NormalFlow maintains a rotational tracking error of 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D reconstruction results, showcasing the high accuracy of NormalFlow. We believe NormalFlow unlocks new possibilities for high-precision perception and manipulation tasks that involve interacting with objects using hands. The video demo, code, and dataset are available on our website: https://joehjhuang.github.io/normalflow.},
	number = {1},
	urldate = {2024-12-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Huang, Hung-Jui and Kaess, Michael and Yuan, Wenzhen},
	month = jan,
	year = {2025},
	note = {arXiv:2412.09617 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {452--459},
}

@misc{tian_maximizing_2024,
	title = {Maximizing {Alignment} with {Minimal} {Feedback}: {Efficiently} {Learning} {Rewards} for {Visuomotor} {Robot} {Policy} {Alignment}},
	shorttitle = {Maximizing {Alignment} with {Minimal} {Feedback}},
	url = {http://arxiv.org/abs/2412.04835},
	doi = {10.48550/arXiv.2412.04835},
	abstract = {Visuomotor robot policies, increasingly pre-trained on large-scale datasets, promise significant advancements across robotics domains. However, aligning these policies with end-user preferences remains a challenge, particularly when the preferences are hard to specify. While reinforcement learning from human feedback (RLHF) has become the predominant mechanism for alignment in non-embodied domains like large language models, it has not seen the same success in aligning visuomotor policies due to the prohibitive amount of human feedback required to learn visual reward functions. To address this limitation, we propose Representation-Aligned Preference-based Learning (RAPL), an observation-only method for learning visual rewards from significantly less human preference feedback. Unlike traditional RLHF, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user's visual representation and then constructs a dense visual reward via feature matching in this aligned representation space. We first validate RAPL through simulation experiments in the X-Magical benchmark and Franka Panda robotic manipulation, demonstrating that it can learn rewards aligned with human preferences, more efficiently uses preference data, and generalizes across robot embodiments. Finally, our hardware experiments align pre-trained Diffusion Policies for three object manipulation tasks. We find that RAPL can fine-tune these policies with 5x less real human preference data, taking the first step towards minimizing human feedback while maximizing visuomotor robot policy alignment.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Tian, Ran and Wu, Yilin and Xu, Chenfeng and Tomizuka, Masayoshi and Malik, Jitendra and Bajcsy, Andrea},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04835 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_densematcher_2024,
	title = {{DenseMatcher}: {Learning} {3D} {Semantic} {Correspondence} for {Category}-{Level} {Manipulation} from a {Single} {Demo}},
	shorttitle = {{DenseMatcher}},
	url = {http://arxiv.org/abs/2412.05268},
	doi = {10.48550/arXiv.2412.05268},
	abstract = {Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. In our experiments, we show that DenseMatcher significantly outperforms prior 3D matching baselines by 43.5\%. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Zhu, Junzhe and Ju, Yuanchen and Zhang, Junyi and Wang, Muhan and Yuan, Zhecheng and Hu, Kaizhe and Xu, Huazhe},
	month = dec,
	year = {2024},
	note = {arXiv:2412.05268 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_bimart_2024,
	title = {{BimArt}: {A} {Unified} {Approach} for the {Synthesis} of {3D} {Bimanual} {Interaction} with {Articulated} {Objects}},
	shorttitle = {{BimArt}},
	url = {http://arxiv.org/abs/2412.05066},
	doi = {10.48550/arXiv.2412.05066},
	abstract = {We present BimArt, a novel generative approach for synthesizing 3D bimanual hand interactions with articulated objects. Unlike prior works, we do not rely on a reference grasp, a coarse hand trajectory, or separate modes for grasping and articulating. To achieve this, we first generate distance-based contact maps conditioned on the object trajectory with an articulation-aware feature representation, revealing rich bimanual patterns for manipulation. The learned contact prior is then used to guide our hand motion generator, producing diverse and realistic bimanual motions for object movement and articulation. Our work offers key insights into feature representation and contact prior for articulated objects, demonstrating their effectiveness in taming the complex, high-dimensional space of bimanual hand-object interactions. Through comprehensive quantitative experiments, we demonstrate a clear step towards simplified and high-quality hand-object animations that excel over the state-of-the-art in motion quality and diversity.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Zhang, Wanyue and Dabral, Rishabh and Golyanik, Vladislav and Choutas, Vasileios and Alvarado, Eduardo and Beeler, Thabo and Habermann, Marc and Theobalt, Christian},
	month = dec,
	year = {2024},
	note = {arXiv:2412.05066 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{chane-sane_reinforcement_2024,
	title = {Reinforcement {Learning} from {Wild} {Animal} {Videos}},
	url = {http://arxiv.org/abs/2412.04273},
	doi = {10.48550/arXiv.2412.04273},
	abstract = {We propose to learn legged robot locomotion skills by watching thousands of wild animal videos from the internet, such as those featured in nature documentaries. Indeed, such videos offer a rich and diverse collection of plausible motion examples, which could inform how robots should move. To achieve this, we introduce Reinforcement Learning from Wild Animal Videos (RLWAV), a method to ground these motions into physical robots. We first train a video classifier on a large-scale animal video dataset to recognize actions from RGB clips of animals in their natural habitats. We then train a multi-skill policy to control a robot in a physics simulator, using the classification score of a third-person camera capturing videos of the robot's movements as a reward for reinforcement learning. Finally, we directly transfer the learned policy to a real quadruped Solo. Remarkably, despite the extreme gap in both domain and embodiment between animals in the wild and robots, our approach enables the policy to learn diverse skills such as walking, jumping, and keeping still, without relying on reference trajectories nor skill-specific rewards.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Chane-Sane, Elliot and Roux, Constant and Stasse, Olivier and Mansard, Nicolas},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04273 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_moto_2024,
	title = {Moto: {Latent} {Motion} {Token} as the {Bridging} {Language} for {Robot} {Manipulation}},
	shorttitle = {Moto},
	url = {http://arxiv.org/abs/2412.04445},
	doi = {10.48550/arXiv.2412.04445},
	abstract = {Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich "corpus", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging "language" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Chen, Yi and Ge, Yuying and Li, Yizhuo and Ge, Yixiao and Ding, Mingyu and Shan, Ying and Liu, Xihui},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04445 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{chen_vision_2024,
	title = {Vision {Foundation} {Model} {Enables} {Generalizable} {Object} {Pose} {Estimation}},
	url = {https://openreview.net/forum?id=FTpKGuxEfy},
	abstract = {Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.},
	language = {en},
	urldate = {2024-12-17},
	author = {Chen, Kai and Ma, Yiyao and Lin, Xingyu and James, Stephen and Zhou, Jianshu and Liu, Yun-Hui and Abbeel, Pieter and Dou, Qi},
	month = nov,
	year = {2024},
}

@misc{bar_navigation_2024,
	title = {Navigation {World} {Models}},
	url = {http://arxiv.org/abs/2412.03572},
	doi = {10.48550/arXiv.2412.03572},
	abstract = {Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Bar, Amir and Zhou, Gaoyue and Tran, Danny and Darrell, Trevor and LeCun, Yann},
	month = dec,
	year = {2024},
	note = {arXiv:2412.03572 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_gsgtrack_2024,
	title = {{GSGTrack}: {Gaussian} {Splatting}-{Guided} {Object} {Pose} {Tracking} from {RGB} {Videos}},
	shorttitle = {{GSGTrack}},
	url = {http://arxiv.org/abs/2412.02267},
	doi = {10.48550/arXiv.2412.02267},
	abstract = {Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is crucial for robotic manipulation. However, existing approaches typically rely on accurate depth information, which is non-trivial to obtain in real-world scenarios. Although depth estimation algorithms can be employed, geometric inaccuracy can lead to failures in RGBD-based pose tracking methods. To address this challenge, we introduce GSGTrack, a novel RGB-based pose tracking framework that jointly optimizes geometry and pose. Specifically, we adopt 3D Gaussian Splatting to create an optimizable 3D representation, which is learned simultaneously with a graph-based geometry optimization to capture the object's appearance features and refine its geometry. However, the joint optimization process is susceptible to perturbations from noisy pose and geometry data. Thus, we propose an object silhouette loss to address the issue of pixel-wise loss being overly sensitive to pose noise during tracking. To mitigate the geometric ambiguities caused by inaccurate depth information, we propose a geometry-consistent image pair selection strategy, which filters out low-confidence pairs and ensures robust geometric optimization. Extensive experiments on the OnePose and HO3D datasets demonstrate the effectiveness of GSGTrack in both 6DoF pose tracking and object reconstruction.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Chen, Zhiyuan and Lu, Fan and Yu, Guo and Li, Bin and Qu, Sanqing and Huang, Yuan and Fu, Changhong and Chen, Guang},
	month = dec,
	year = {2024},
	note = {arXiv:2412.02267 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{geng_motion_2024,
	title = {Motion {Prompting}: {Controlling} {Video} {Generation} with {Motion} {Trajectories}},
	shorttitle = {Motion {Prompting}},
	url = {http://arxiv.org/abs/2412.02700},
	doi = {10.48550/arXiv.2412.02700},
	abstract = {Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, "interacting" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Geng, Daniel and Herrmann, Charles and Hur, Junhwa and Cole, Forrester and Zhang, Serena and Pfaff, Tobias and Lopez-Guevara, Tatiana and Doersch, Carl and Aytar, Yusuf and Rubinstein, Michael and Sun, Chen and Wang, Oliver and Owens, Andrew and Sun, Deqing},
	month = dec,
	year = {2024},
	note = {arXiv:2412.02700 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xu_representing_2024,
	title = {Representing {Long} {Volumetric} {Video} with {Temporal} {Gaussian} {Hierarchy}},
	volume = {43},
	issn = {0730-0301, 1557-7368},
	url = {http://arxiv.org/abs/2412.09608},
	doi = {10.1145/3687919},
	abstract = {This paper aims to address the challenge of reconstructing long volumetric videos from multi-view RGB videos. Recent dynamic view synthesis methods leverage powerful 4D representations, like feature grids or point cloud sequences, to achieve high-quality rendering results. However, they are typically limited to short (1{\textasciitilde}2s) video clips and often suffer from large memory footprints when dealing with longer videos. To solve this issue, we propose a novel 4D representation, named Temporal Gaussian Hierarchy, to compactly model long volumetric videos. Our key observation is that there are generally various degrees of temporal redundancy in dynamic scenes, which consist of areas changing at different speeds. Motivated by this, our approach builds a multi-level hierarchy of 4D Gaussian primitives, where each level separately describes scene regions with different degrees of content change, and adaptively shares Gaussian primitives to represent unchanged scene content over different temporal segments, thus effectively reducing the number of Gaussian primitives. In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length. Extensive experimental results demonstrate the superiority of our method over alternative methods in terms of training cost, rendering speed, and storage usage. To our knowledge, this work is the first approach capable of efficiently handling minutes of volumetric video data while maintaining state-of-the-art rendering quality. Our project page is available at: https://zju3dv.github.io/longvolcap.},
	number = {6},
	urldate = {2024-12-14},
	journal = {ACM Transactions on Graphics},
	author = {Xu, Zhen and Xu, Yinghao and Yu, Zhiyuan and Peng, Sida and Sun, Jiaming and Bao, Hujun and Zhou, Xiaowei},
	month = dec,
	year = {2024},
	note = {arXiv:2412.09608 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Multimedia},
	pages = {1--18},
}

@inproceedings{laan_self-calibrating_2024,
	title = {Self-{Calibrating} {Conformal} {Prediction}},
	url = {https://openreview.net/forum?id=BJ6HkT7qIk},
	abstract = {In machine learning, model calibration and predictive inference are essential for producing reliable predictions and quantifying uncertainty to support decision-making. Recognizing the complementary roles of point and interval predictions, we introduce Self-Calibrating Conformal Prediction, a method that combines Venn-Abers calibration and conformal prediction to deliver calibrated point predictions alongside prediction intervals with finite-sample validity conditional on these predictions. To achieve this, we extend the original Venn-Abers procedure from binary classification to regression. Our theoretical framework supports analyzing conformal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions. Real-data experiments show that our method improves interval efficiency through model calibration and offers a practical alternative to feature-conditional validity.},
	language = {en},
	urldate = {2024-12-14},
	author = {Laan, Lars van der and Alaa, Ahmed},
	month = nov,
	year = {2024},
}

@article{huang_normalflow_2025-1,
	title = {{NormalFlow}: {Fast}, {Robust}, and {Accurate} {Contact}-{Based} {Object} {6DoF} {Pose} {Tracking} {With} {Vision}-{Based} {Tactile} {Sensors}},
	volume = {10},
	issn = {2377-3766},
	shorttitle = {{NormalFlow}},
	url = {https://ieeexplore.ieee.org/document/10766628},
	doi = {10.1109/LRA.2024.3505815},
	abstract = {Tactile sensing is crucial for robots aiming to achieve human-level dexterity. Among tactile-dependent skills, tactile-based object tracking serves as the cornerstone for many tasks, including manipulation, in-hand manipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a fast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging the precise surface normal estimation of vision-based tactile sensors, NormalFlow determines object movements by minimizing discrepancies between the tactile-derived surface normals. Our results show that NormalFlow consistently outperforms competitive baselines and can track low-texture objects like table surfaces. For long-horizon tracking, we demonstrate when rolling the sensor around a bead for 360 degrees, NormalFlow maintains a rotational tracking error of 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D reconstruction results, showcasing the high accuracy of NormalFlow. We believe NormalFlow unlocks new possibilities for high-precision perception and manipulation tasks that involve interacting with objects using hands.},
	number = {1},
	urldate = {2024-12-13},
	journal = {IEEE Robotics and Automation Letters},
	author = {Huang, Hung-Jui and Kaess, Michael and Yuan, Wenzhen},
	month = jan,
	year = {2025},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Accuracy, Force and tactile sensing, Noise, Object tracking, Optimization, Point cloud compression, Sensors, Surface reconstruction, Surface treatment, Tactile sensors, Three-dimensional displays, perception for grasping and manipulation},
	pages = {452--459},
}

@misc{lipman_flow_2024,
	title = {Flow {Matching} {Guide} and {Code}},
	url = {http://arxiv.org/abs/2412.06264},
	doi = {10.48550/arXiv.2412.06264},
	abstract = {Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Lipman, Yaron and Havasi, Marton and Holderrieth, Peter and Shaul, Neta and Le, Matt and Karrer, Brian and Chen, Ricky T. Q. and Lopez-Paz, David and Ben-Hamu, Heli and Gat, Itai},
	month = dec,
	year = {2024},
	note = {arXiv:2412.06264 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{kassuba_vision_2013-1,
	title = {Vision holds a greater share in visuo-haptic object recognition than touch},
	volume = {65},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811912009640},
	doi = {10.1016/j.neuroimage.2012.09.054},
	abstract = {The integration of visual and haptic input can facilitate object recognition. Yet, vision might dominate visuo-haptic interactions as it is more effective than haptics in processing several object features in parallel and recognizing objects outside of reaching space. The maximum likelihood approach of multisensory integration would predict that haptics as the less efficient sense for object recognition gains more from integrating additional visual information than vice versa. To test for asymmetries between vision and touch in visuo-haptic interactions, we measured regional changes in brain activity using functional magnetic resonance imaging while healthy individuals performed a delayed-match-to-sample task. We manipulated identity matching of sample and target objects: We hypothesized that only coherent visual and haptic object features would activate unified object representations. The bilateral object-specific lateral occipital cortex, fusiform gyrus, and intraparietal sulcus showed increased activation to crossmodal compared to unimodal matching but only for congruent object pairs. Critically, the visuo-haptic interaction effects in these regions depended on the sensory modality which processed the target object, being more pronounced for haptic than visual targets. This preferential response of visuo-haptic regions indicates a modality-specific asymmetry in crossmodal matching of visual and haptic object features, suggesting a functional primacy of vision over touch in visuo-haptic object recognition.},
	urldate = {2024-12-12},
	journal = {NeuroImage},
	author = {Kassuba, Tanja and Klinge, Corinna and Hölig, Cordula and Röder, Brigitte and Siebner, Hartwig R.},
	month = jan,
	year = {2013},
	keywords = {Multisensory interactions, Object recognition, Touch perception, Visual dominance, Visual perception, fMRI},
	pages = {59--68},
}

@inproceedings{morgan_vision-driven_2021,
	title = {Vision-driven {Compliant} {Manipulation} for {Reliable}, {High}-{Precision} {Assembly} {Tasks}},
	url = {http://arxiv.org/abs/2106.14070},
	doi = {10.15607/RSS.2021.XVII.070},
	abstract = {Highly constrained manipulation tasks continue to be challenging for autonomous robots as they require high levels of precision, typically less than 1mm, which is often incompatible with what can be achieved by traditional perception systems. This paper demonstrates that the combination of state-of-the-art object tracking with passively adaptive mechanical hardware can be leveraged to complete precision manipulation tasks with tight, industrially-relevant tolerances (0.25mm). The proposed control method closes the loop through vision by tracking the relative 6D pose of objects in the relevant workspace. It adjusts the control reference of both the compliant manipulator and the hand to complete object insertion tasks via within-hand manipulation. Contrary to previous efforts for insertion, our method does not require expensive force sensors, precision manipulators, or time-consuming, online learning, which is data hungry. Instead, this effort leverages mechanical compliance and utilizes an object agnostic manipulation model of the hand learned offline, off-the-shelf motion planning, and an RGBD-based object tracker trained solely with synthetic data. These features allow the proposed system to easily generalize and transfer to new tasks and environments. This paper describes in detail the system components and showcases its efficacy with extensive experiments involving tight tolerance peg-in-hole insertion tasks of various geometries as well as open-world constrained placement tasks.},
	urldate = {2024-06-01},
	booktitle = {Robotics: {Science} and {Systems} {XVII}},
	author = {Morgan, Andrew S. and Wen, Bowen and Liang, Junchi and Boularias, Abdeslam and Dollar, Aaron M. and Bekris, Kostas},
	month = jul,
	year = {2021},
	note = {arXiv:2106.14070 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{jiang_se3_2023,
	title = {{SE}(3) {Diffusion} {Model}-based {Point} {Cloud} {Registration} for {Robust} {6D} {Object} {Pose} {Estimation}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/43069caa6776eac8bca4bfd74d4a476d-Abstract-Conference.html},
	language = {en},
	urldate = {2024-08-12},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jiang, Haobo and Salzmann, Mathieu and Dang, Zheng and Xie, Jin and Yang, Jian},
	month = dec,
	year = {2023},
	pages = {21285--21297},
}

@inproceedings{zhou_3d_2023,
	title = {{3D} {Neural} {Embedding} {Likelihood}: {Probabilistic} {Inverse} {Graphics} for {Robust} {6D} {Pose} {Estimation}},
	shorttitle = {{3D} {Neural} {Embedding} {Likelihood}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_3D_Neural_Embedding_Likelihood_Probabilistic_Inverse_Graphics_for_Robust_6D_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-09-23},
	author = {Zhou, Guangyao and Gothoskar, Nishad and Wang, Lirui and Tenenbaum, Joshua B. and Gutfreund, Dan and Lázaro-Gredilla, Miguel and George, Dileep and Mansinghka, Vikash K.},
	year = {2023},
	pages = {21625--21636},
}

@article{gordon_integration_1991,
	title = {The integration of haptically acquired size information in the programming of precision grip},
	volume = {83},
	issn = {1432-1106},
	url = {https://doi.org/10.1007/BF00229825},
	doi = {10.1007/BF00229825},
	abstract = {Recent evidence for the use of visual cues in the programming of the precision grip has been given by Gordon et al. (1991). Visually invoked size-related information influenced the physical forces used to produce a lift, even when it was not consistent with other sensory information. In the present study, blind-folded subjects were required to feel the size of an object by haptic exploration prior to lifting it. Two boxes of equal weight and unequal size were used for the lift objects and were attached to an instrumented (grip) handle. Grip force and load force, their rates, and the vertical move ment of the object were measured. Most subjects report that the small box was heavier, which is consistent with size-weight illusion predictions. However, peak grip force, grip force rate, peak load force, and load force rate were greater for the large box when the boxes were randomly presented, but not when the same boxes were lifted consecutively. If subjects did not feel the box prior to a lift, these parameters were scaled in between those normally employed for the large and small box. Most subjects apparently programmed the parallel increase of the grip and load force during the loading phase as one force rate pulse. This represented a “target strategy” in which an internal neural representation of the objects weight determined the actual target parameter (i.e. just enough force required to overcome gravity). The other subjects exhibited a slower stepwise increase in grip and load force rate. The subjects choosing this “probing strategy” did not scale the force parameters differently for the two boxes. Furthermore, they did not perceive any difference between the objects' weight. Together, these results suggest that haptic exploration may be used to convey size information and further support the hypoth esis that size-related information may be combined with other sensory information in the programming of the precision grip.},
	language = {en},
	number = {3},
	urldate = {2024-10-25},
	journal = {Experimental Brain Research},
	author = {Gordon, A. M. and Forssberg, H. and Johansson, R. S. and Westling, G.},
	month = feb,
	year = {1991},
	keywords = {Force production, Haptic manual exploration, Human, Motor programming, Precision grip, Size-weight illusion},
	pages = {483--488},
}

@inproceedings{chen_learning_2020,
	title = {Learning {Canonical} {Shape} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.html},
	urldate = {2024-08-02},
	author = {Chen, Dengsheng and Li, Jun and Wang, Zheng and Xu, Kai},
	year = {2020},
	pages = {11973--11982},
}

@inproceedings{zhao_point_2021,
	title = {Point {Transformer}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Point_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com},
	language = {en},
	urldate = {2023-01-05},
	author = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip H. S. and Koltun, Vladlen},
	year = {2021},
	pages = {16259--16268},
}

@article{ernst_humans_2002,
	title = {Humans integrate visual and haptic information in a statistically optimal fashion},
	volume = {415},
	copyright = {2002 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/415429a},
	doi = {10.1038/415429a},
	abstract = {When a person looks at an object while exploring it with their hand, vision and touch both provide information for estimating the properties of the object. Vision frequently dominates the integrated visual–haptic percept, for example when judging size, shape or position1,2,3, but in some circumstances the percept is clearly affected by haptics4,5,6,7. Here we propose that a general principle, which minimizes variance in the final estimate, determines the degree to which vision or haptics dominates. This principle is realized by using maximum-likelihood estimation8,9,10,11,12,13,14,15 to combine the inputs. To investigate cue combination quantitatively, we first measured the variances associated with visual and haptic estimation of height. We then used these measurements to construct a maximum-likelihood integrator. This model behaved very similarly to humans in a visual–haptic task. Thus, the nervous system seems to combine visual and haptic information in a fashion that is similar to a maximum-likelihood integrator. Visual dominance occurs when the variance associated with visual estimation is lower than that associated with haptic estimation.},
	language = {en},
	number = {6870},
	urldate = {2024-12-12},
	journal = {Nature},
	author = {Ernst, Marc O. and Banks, Martin S.},
	month = jan,
	year = {2002},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, Science, multidisciplinary},
	pages = {429--433},
}

@article{lederman_haptic_2009,
	title = {Haptic perception: {A} tutorial},
	volume = {71},
	issn = {1943-393X},
	shorttitle = {Haptic perception},
	url = {https://doi.org/10.3758/APP.71.7.1439},
	doi = {10.3758/APP.71.7.1439},
	abstract = {This tutorial focuses on the sense of touch within the context of a fully active human observer. It is intended for graduate students and researchers outside the discipline who seek an introduction to the rapidly evolving field of human haptics. The tutorial begins with a review of peripheral sensory receptors in skin, muscles, tendons, and joints. We then describe an extensive body of research on “what” and “where” channels, the former dealing with haptic perception of objects, surfaces, and their properties, and the latter with perception of spatial layout on the skin and in external space relative to the perceiver. We conclude with a brief discussion of other significant issues in the field, including vision-touch interactions, affective touch, neural plasticity, and applications.},
	language = {en},
	number = {7},
	urldate = {2024-12-12},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Lederman, S. J. and Klatzky, R. L.},
	month = oct,
	year = {2009},
	keywords = {Glabrous Skin, Haptic Perception, Sighted Subject, Tactile Perception, Visual Imagery},
	pages = {1439--1459},
}

@article{lederman_hand_1987,
	title = {Hand movements: {A} window into haptic object recognition},
	volume = {19},
	issn = {0010-0285},
	shorttitle = {Hand movements},
	url = {https://www.sciencedirect.com/science/article/pii/0010028587900089},
	doi = {10.1016/0010-0285(87)90008-9},
	abstract = {Two experiments establish links between desired knowledge about objects and hand movements during haptic object exploration. Experiment 1 used a match-to-sample task, in which blindfolded subjects were directed to match objects on a particular dimension (e.g., texture). Hand movements during object exploration were reliably classified as “exploratory procedures,” each procedure defined by its invariant and typical properties. The movement profile, i.e., the distribution of exploratory procedures, was directly related to the desired object knowledge that was required for the match. Experiment 2 addressed the reasons for the specific links between exploratory procedures and knowledge goals. Hand movements were constrained, and performance on various matching tasks was assessed. The procedures were considered in terms of their necessity, sufficiency, and optimality of performance for each task. The results establish that in free exploration, a procedure is generally used to acquire information about an object property, not because it is merely sufficient, but because it is optimal or even necessary. Hand movements can serve as “windows,” through which it is possible to learn about the underlying representation of objects in memory and the processes by which such representations are derived and utilized.},
	number = {3},
	urldate = {2024-12-12},
	journal = {Cognitive Psychology},
	author = {Lederman, Susan J and Klatzky, Roberta L},
	month = jul,
	year = {1987},
	pages = {342--368},
}

@article{allen_integrating_1988,
	title = {Integrating {Vision} and {Touch} for {Object} {Recognition} {Tasks}},
	volume = {7},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/027836498800700603},
	doi = {10.1177/027836498800700603},
	abstract = {A robotic system for object recognition is described that uses passive stereo vision and active exploratory tactile sensing. The complementary nature of these sensing modalities allows the system to discover the underlying 3-D structure of the objects to be recognized. This structure is embodied in rich, hierarchical, viewpoint-independent 3-D models of the objects which include curved surfaces, concavities and holes. The vision processing provides sparse 3-D data about regions of interest that are then actively explored by the tactile sensor mounted on the end of a six-degree-of-freedom manipulator. A robust, hierarchical procedure has been developed to inte grate the visual and tactile data into accurate 3-D surface and feature primitives. This integration of vision and touch provides geometric measures of the surfaces and features that are used in a matching phase to find model objects that are consistent with the sensory data. Methods for verification of the hypothesis are presented, including the sensing of visually occluded areas with the tactile sensor. A number of experi ments have been performed using real sensors and real, noisy data to demonstrate the utility of these methods and the ability of such a system to recognize objects that would be difficult for a system using vision alone.},
	language = {en},
	number = {6},
	urldate = {2024-12-12},
	journal = {The International Journal of Robotics Research},
	author = {Allen, Peter K.},
	month = dec,
	year = {1988},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {15--33},
}

@article{trojan_rubber_2018,
	title = {The rubber hand illusion induced by visual-thermal stimulation},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-29860-2},
	doi = {10.1038/s41598-018-29860-2},
	abstract = {In the rubber hand illusion (RHI), synchronous touch of a real hand and an artificial hand leads to the feeling of the artificial hand belonging to one’s own body. This study examined whether the RHI can be induced using visual–thermal instead of visual–tactile stimulus patterns and to which extent the congruency between temperature and colour of the visual stimulus influences the RHI. In a within-subject design, we presented cold vs. warm thermal stimuli to the participants’ hidden hand combined with red vs. blue visual stimuli presented synchronously vs. asynchronously at a fake hand. The RHI could be induced using visual–thermal stimuli, yielding RHI vividness ratings comparable to the visual-tactile variant. Congruent (warm–red, cold–blue) synchronous stimulus patterns led to higher RHI vividness than incongruent (warm–blue, cold–red) synchronous combinations; in the asynchronous conditions, an inverse effect was present. Temperature ratings mainly depended on the actual stimulus temperature and were higher with synchronous vs. asynchronous patterns; they were also slightly higher with red vs. blue light, but there were no interactions with temperature or synchrony. In conclusion, we demonstrated that the RHI can be induced via visual-thermal stimuli, opening new perspectives in research on multi-sensory integration and body representations.},
	language = {en},
	number = {1},
	urldate = {2024-12-12},
	journal = {Scientific Reports},
	author = {Trojan, Jörg and Fuchs, Xaver and Speth, Sophie-Louise and Diers, Martin},
	month = aug,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Perception, Psychology},
	pages = {12417},
}

@misc{pham_flashslam_2024,
	title = {{FlashSLAM}: {Accelerated} {RGB}-{D} {SLAM} for {Real}-{Time} {3D} {Scene} {Reconstruction} with {Gaussian} {Splatting}},
	shorttitle = {{FlashSLAM}},
	url = {http://arxiv.org/abs/2412.00682},
	doi = {10.48550/arXiv.2412.00682},
	abstract = {We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based SLAM methods often fall short in sparse view settings and during large camera movements due to their reliance on gradient descent-based optimization, which is both slow and inaccurate. FlashSLAM addresses these limitations by combining 3DGS with a fast vision-based camera tracking technique, utilizing a pretrained feature matching model and point cloud registration for precise pose estimation in under 80 ms - a 90\% reduction in tracking time compared to SplaTAM - without costly iterative rendering. In sparse settings, our method achieves up to a 92\% improvement in average tracking accuracy over previous methods. Additionally, it accounts for noise in depth sensors, enhancing robustness when using unspecialized devices such as smartphones. Extensive experiments show that FlashSLAM performs reliably across both sparse and dense settings, in synthetic and real-world environments. Evaluations on benchmark datasets highlight its superior accuracy and efficiency, establishing FlashSLAM as a versatile and high-performance solution for SLAM, advancing the state-of-the-art in 3D reconstruction across diverse applications.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Pham, Phu and Conover, Damon and Bera, Aniket},
	month = dec,
	year = {2024},
	note = {arXiv:2412.00682 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xiang_structured_2024,
	title = {Structured {3D} {Latents} for {Scalable} and {Versatile} {3D} {Generation}},
	url = {http://arxiv.org/abs/2412.01506},
	doi = {10.48550/arXiv.2412.01506},
	abstract = {We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01506 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{moller_particle-based_2024,
	title = {Particle-based {6D} {Object} {Pose} {Estimation} from {Point} {Clouds} using {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2412.00835},
	doi = {10.48550/arXiv.2412.00835},
	abstract = {Object pose estimation from a single view remains a challenging problem. In particular, partial observability, occlusions, and object symmetries eventually result in pose ambiguity. To account for this multimodality, this work proposes training a diffusion-based generative model for 6D object pose estimation. During inference, the trained generative model allows for sampling multiple particles, i.e., pose hypotheses. To distill this information into a single pose estimate, we propose two novel and effective pose selection strategies that do not require any additional training or computationally intensive operations. Moreover, while many existing methods for pose estimation primarily focus on the image domain and only incorporate depth information for final pose refinement, our model solely operates on point cloud data. The model thereby leverages recent advancements in point cloud processing and operates upon an SE(3)-equivariant latent space that forms the basis for the particle selection strategies and allows for improved inference times. Our thorough experimental results demonstrate the competitive performance of our approach on the Linemod dataset and showcase the effectiveness of our design choices. Code is available at https://github.com/zitronian/6DPoseDiffusion .},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Möller, Christian and Funk, Niklas and Peters, Jan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.00835 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ashutosh_fiction_2024,
	title = {{FIction}: {4D} {Future} {Interaction} {Prediction} from {Video}},
	shorttitle = {{FIction}},
	url = {http://arxiv.org/abs/2412.00932},
	doi = {10.48550/arXiv.2412.00932},
	abstract = {Anticipating how a person will interact with objects in an environment is essential for activity understanding, but existing methods are limited to the 2D space of video frames-capturing physically ungrounded predictions of 'what' and ignoring the 'where' and 'how'. We introduce 4D future interaction prediction from videos. Given an input video of a human activity, the goal is to predict what objects at what 3D locations the person will interact with in the next time period (e.g., cabinet, fridge), and how they will execute that interaction (e.g., poses for bending, reaching, pulling). We propose a novel model FIction that fuses the past video observation of the person's actions and their environment to predict both the 'where' and 'how' of future interactions. Through comprehensive experiments on a variety of activities and real-world environments in Ego-Exo4D, we show that our proposed approach outperforms prior autoregressive and (lifted) 2D video models substantially, with more than 30\% relative gains.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Ashutosh, Kumar and Pavlakos, Georgios and Grauman, Kristen},
	month = dec,
	year = {2024},
	note = {arXiv:2412.00932 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jin_6dope-gs_2024,
	title = {{6DOPE}-{GS}: {Online} {6D} {Object} {Pose} {Estimation} using {Gaussian} {Splatting}},
	shorttitle = {{6DOPE}-{GS}},
	url = {http://arxiv.org/abs/2412.01543},
	doi = {10.48550/arXiv.2412.01543},
	abstract = {Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation {\textbackslash}\& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5\${\textbackslash}times\$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Jin, Yufeng and Prasad, Vignesh and Jauhri, Snehal and Franzius, Mathias and Chalvatzaki, Georgia},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{han_mvimgnet20_2024,
	title = {{MVImgNet2}.0: {A} {Larger}-scale {Dataset} of {Multi}-view {Images}},
	volume = {43},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{MVImgNet2}.0},
	url = {http://arxiv.org/abs/2412.01430},
	doi = {10.1145/3687973},
	abstract = {MVImgNet is a large-scale dataset that contains multi-view images of {\textasciitilde}220k real-world objects in 238 classes. As a counterpart of ImageNet, it introduces 3D visual signals via multi-view shooting, making a soft bridge between 2D and 3D vision. This paper constructs the MVImgNet2.0 dataset that expands MVImgNet into a total of {\textasciitilde}520k objects and 515 categories, which derives a 3D dataset with a larger scale that is more comparable to ones in the 2D domain. In addition to the expanded dataset scale and category range, MVImgNet2.0 is of a higher quality than MVImgNet owing to four new features: (i) most shoots capture 360-degree views of the objects, which can support the learning of object reconstruction with completeness; (ii) the segmentation manner is advanced to produce foreground object masks of higher accuracy; (iii) a more powerful structure-from-motion method is adopted to derive the camera pose for each frame of a lower estimation error; (iv) higher-quality dense point clouds are reconstructed via advanced methods for objects captured in 360-degree views, which can serve for downstream applications. Extensive experiments confirm the value of the proposed MVImgNet2.0 in boosting the performance of large 3D reconstruction models. MVImgNet2.0 will be public at luyues.github.io/mvimgnet2, including multi-view images of all 520k objects, the reconstructed high-quality point clouds, and data annotation codes, hoping to inspire the broader vision community.},
	number = {6},
	urldate = {2024-12-11},
	journal = {ACM Transactions on Graphics},
	author = {Han, Xiaoguang and Wu, Yushuang and Shi, Luyue and Liu, Haolin and Liao, Hongjie and Qiu, Lingteng and Yuan, Weihao and Gu, Xiaodong and Dong, Zilong and Cui, Shuguang},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01430 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	pages = {1--16},
}

@misc{jawale_dynamic_2024,
	title = {Dynamic {Non}-{Prehensile} {Object} {Transport} via {Model}-{Predictive} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2412.00086},
	doi = {10.48550/arXiv.2412.00086},
	abstract = {We investigate the problem of teaching a robot manipulator to perform dynamic non-prehensile object transport, also known as the `robot waiter' task, from a limited set of real-world demonstrations. We propose an approach that combines batch reinforcement learning (RL) with model-predictive control (MPC) by pretraining an ensemble of value functions from demonstration data, and utilizing them online within an uncertainty-aware MPC scheme to ensure robustness to limited data coverage. Our approach is straightforward to integrate with off-the-shelf MPC frameworks and enables learning solely from task space demonstrations with sparsely labeled transitions, while leveraging MPC to ensure smooth joint space motions and constraint satisfaction. We validate the proposed approach through extensive simulated and real-world experiments on a Franka Panda robot performing the robot waiter task and demonstrate robust deployment of value functions learned from 50-100 demonstrations. Furthermore, our approach enables generalization to novel objects not seen during training and can improve upon suboptimal demonstrations. We believe that such a framework can reduce the burden of providing extensive demonstrations and facilitate rapid training of robot manipulators to perform non-prehensile manipulation tasks. Project videos and supplementary material can be found at: https://sites.google.com/view/cvmpc.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Jawale, Neel and Boots, Byron and Sundaralingam, Balakumar and Bhardwaj, Mohak},
	month = nov,
	year = {2024},
	note = {arXiv:2412.00086 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_one-shot_2024,
	title = {One-{Shot} {Real}-to-{Sim} via {End}-to-{End} {Differentiable} {Simulation} and {Rendering}},
	url = {http://arxiv.org/abs/2412.00259},
	doi = {10.48550/arXiv.2412.00259},
	abstract = {Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable simulators to identify world models are incapable of jointly optimizing the shape, appearance, and physical properties of the scene. In this work, we introduce a novel object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based object representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of system identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Zhu, Yifan and Xiang, Tianyi and Dollar, Aaron and Pan, Zherong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.00259 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{torne_robot_2024,
	title = {Robot {Learning} with {Super}-{Linear} {Scaling}},
	url = {http://arxiv.org/abs/2412.01770},
	doi = {10.48550/arXiv.2412.01770},
	abstract = {Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose Crowdsourcing and Amortizing Human Effort for Real-to-Sim-to-Real(CASHER), a pipeline for scaling up data collection and learning in simulation where the performance scales superlinearly with human effort. The key idea is to crowdsource digital twins of real-world scenes using 3D reconstruction and collect large-scale data in simulation, rather than the real-world. Data collection in simulation is initially driven by RL, bootstrapped with human demonstrations. As the training of a generalist policy progresses across environments, its generalization capabilities can be used to replace human effort with model generated demonstrations. This results in a pipeline where behavioral data is collected in simulation with continually reducing human effort. We show that CASHER demonstrates zero-shot and few-shot scaling laws on three real-world tasks across diverse scenarios. We show that CASHER enables fine-tuning of pre-trained policies to a target scenario using a video scan without any additional human effort. See our project website: https://casher-robot-learning.github.io/CASHER/},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Torne, Marcel and Jain, Arhan and Yuan, Jiayi and Macha, Vidaaranya and Ankile, Lars and Simeonov, Anthony and Agrawal, Pulkit and Gupta, Abhishek},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01770 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{singh_dextrah-rgb_2024,
	title = {{DextrAH}-{RGB}: {Visuomotor} {Policies} to {Grasp} {Anything} with {Dexterous} {Hands}},
	shorttitle = {{DextrAH}-{RGB}},
	url = {http://arxiv.org/abs/2412.01791},
	doi = {10.48550/arXiv.2412.01791},
	abstract = {One of the most important yet challenging skills for a robot is the task of dexterous grasping of a diverse range of objects. Much of the prior work is limited by the speed, dexterity, or reliance on depth maps. In this paper, we introduce DextrAH-RGB, a system that can perform dexterous arm-hand grasping end2end from stereo RGB input. We train a teacher fabric-guided policy (FGP) in simulation through reinforcement learning that acts on a geometric fabric action space to ensure reactivity and safety. We then distill this teacher FGP into a stereo RGB-based student FGP in simulation. To our knowledge, this is the first work that is able to demonstrate robust sim2real transfer of an end2end RGB-based policy for complex, dynamic, contact-rich tasks such as dexterous grasping. Our policies are able to generalize grasping to novel objects with unseen geometry, texture, or lighting conditions during training. Videos of our system grasping a diverse range of unseen objects are available at {\textbackslash}url\{https://dextrah-rgb.github.io/\}},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Singh, Ritvik and Allshire, Arthur and Handa, Ankur and Ratliff, Nathan and Wyk, Karl Van},
	month = nov,
	year = {2024},
	note = {arXiv:2412.01791 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{wu_diorama_2024,
	title = {Diorama: {Unleashing} {Zero}-shot {Single}-view {3D} {Scene} {Modeling}},
	shorttitle = {Diorama},
	url = {http://arxiv.org/abs/2411.19492},
	doi = {10.48550/arXiv.2411.19492},
	abstract = {Reconstructing structured 3D scenes from RGB images using CAD objects unlocks efficient and compact scene representations that maintain compositionality and interactability. Existing works propose training-heavy methods relying on either expensive yet inaccurate real-world annotations or controllable yet monotonous synthetic data that do not generalize well to unseen objects or domains. We present Diorama, the first zero-shot open-world system that holistically models 3D scenes from single-view RGB observations without requiring end-to-end training or human annotations. We show the feasibility of our approach by decomposing the problem into subtasks and introduce robust, generalizable solutions to each: architecture reconstruction, 3D shape retrieval, object pose estimation, and scene layout optimization. We evaluate our system on both synthetic and real-world data to show we significantly outperform baselines from prior work. We also demonstrate generalization to internet images and the text-to-scene task.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Wu, Qirui and Iliash, Denys and Ritchie, Daniel and Savva, Manolis and Chang, Angel X.},
	month = nov,
	year = {2024},
	note = {arXiv:2411.19492 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_sadg_2024,
	title = {{SADG}: {Segment} {Any} {Dynamic} {Gaussian} {Without} {Object} {Trackers}},
	shorttitle = {{SADG}},
	url = {http://arxiv.org/abs/2411.19290},
	doi = {10.48550/arXiv.2411.19290},
	abstract = {Understanding dynamic 3D scenes is fundamental for various applications, including extended reality (XR) and autonomous driving. Effectively integrating semantic information into 3D reconstruction enables holistic representation that opens opportunities for immersive and interactive applications. We introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines dynamic Gaussian Splatting representation and semantic information without reliance on object IDs. In contrast to existing works, we do not rely on supervision based on object identities to enable consistent segmentation of dynamic 3D objects. To this end, we propose to learn semantically-aware features by leveraging masks generated from the Segment Anything Model (SAM) and utilizing our novel contrastive learning objective based on hard pixel mining. The learned Gaussian features can be effectively clustered without further post-processing. This enables fast computation for further object-level editing, such as object removal, composition, and style transfer by manipulating the Gaussians in the scene. We further extend several dynamic novel-view datasets with segmentation benchmarks to enable testing of learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks and demonstrate the superior performance of our approach in segmenting objects within dynamic scenes along with its effectiveness for further downstream editing tasks.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Li, Yun-Jin and Gladkova, Mariia and Xia, Yan and Cremers, Daniel},
	month = nov,
	year = {2024},
	note = {arXiv:2411.19290 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fink_proprioceptive_2020,
	title = {Proprioceptive {Sensor} {Fusion} for {Quadruped} {Robot} {State} {Estimation}},
	url = {https://ieeexplore.ieee.org/document/9341521/authors#authors},
	doi = {10.1109/IROS45743.2020.9341521},
	abstract = {Estimation of a quadruped's state is fundamentally important to its operation. In this paper we develop a low-level state estimator for quadrupedal robots that includes attitude, odometry, ground reaction forces, and contact detection. The state estimator is divided into three parts. First, a nonlinear observer estimates attitude by fusing inertial measurements. The attitude estimator is globally exponentially stable and is able to initialize with large errors in the initial state estimates whereas a state-of-the-art EKF would diverge. This is practical for situations when the robot has fallen over and needs to start from its side. Second, leg odometry is calculated with encoders, force sensors, and torque sensors in the robot's joints. Lastly, the leg odometry and inertial measurements are fused to obtain linear position and velocity. We experimentally validate the state estimator using a novel dataset from the HyQ robot. For the entirety of the experiment the estimated attitude matched the ground truth data and had a root mean square error (RMSE) of [2 1 5] deg, the velocity estimates has a RMSE of [0.11 0.15 0.04] m/s, and the position estimates, which are unobservable, drifted on average [2 1 8] mm/s.},
	urldate = {2024-12-11},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Fink, Geoff and Semini, Claudio},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Legged locomotion, Observers, Robot sensing systems, Robots, Root mean square, Sensor fusion, Velocity measurement},
	pages = {10914--10920},
}

@misc{wu_cat4d_2024,
	title = {{CAT4D}: {Create} {Anything} in {4D} with {Multi}-{View} {Video} {Diffusion} {Models}},
	shorttitle = {{CAT4D}},
	url = {http://arxiv.org/abs/2411.18613},
	doi = {10.48550/arXiv.2411.18613},
	abstract = {We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: {\textbackslash}url\{cat-4d.github.io\}.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Wu, Rundi and Gao, Ruiqi and Poole, Ben and Trevithick, Alex and Zheng, Changxi and Barron, Jonathan T. and Holynski, Aleksander},
	month = nov,
	year = {2024},
	note = {arXiv:2411.18613 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{guo_prediction_2024,
	title = {Prediction with {Action}: {Visual} {Policy} {Learning} via {Joint} {Denoising} {Process}},
	shorttitle = {Prediction with {Action}},
	url = {http://arxiv.org/abs/2411.18179},
	doi = {10.48550/arXiv.2411.18179},
	abstract = {Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilities--image prediction and robotic action, respectively--they technically follow a similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce PAD, a novel visual policy learning framework that unifies image Prediction and robot Action within a joint Denoising process. Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. PAD outperforms previous methods, achieving a significant 26.3\% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0\% success rate increase compared to the strongest baseline. Project page at https://sites.google.com/view/pad-paper},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Guo, Yanjiang and Hu, Yucheng and Zhang, Jianke and Wang, Yen-Jen and Chen, Xiaoyu and Lu, Chaochao and Chen, Jianyu},
	month = nov,
	year = {2024},
	note = {arXiv:2411.18179 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{liu_intragen_2024,
	title = {{InTraGen}: {Trajectory}-controlled {Video} {Generation} for {Object} {Interactions}},
	shorttitle = {{InTraGen}},
	url = {http://arxiv.org/abs/2411.16804},
	doi = {10.48550/arXiv.2411.16804},
	abstract = {Advances in video generation have significantly improved the realism and quality of created scenes. This has fueled interest in developing intuitive tools that let users leverage video generation as world simulators. Text-to-video (T2V) generation is one such approach, enabling video creation from text descriptions only. Yet, due to the inherent ambiguity in texts and the limited temporal information offered by text prompts, researchers have explored additional control signals like trajectory-guided systems, for more accurate T2V generation. Nonetheless, methods to evaluate whether T2V models can generate realistic interactions between multiple objects are lacking. We introduce InTraGen, a pipeline for improved trajectory-based generation of object interaction scenarios. We propose 4 new datasets and a novel trajectory quality metric to evaluate the performance of the proposed InTraGen. To achieve object interaction, we introduce a multi-modal interaction encoding pipeline with an object ID injection mechanism that enriches object-environment interactions. Our results demonstrate improvements in both visual fidelity and quantitative performance. Code and datasets are available at https://github.com/insait-institute/InTraGen},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Liu, Zuhao and Yanev, Aleksandar and Mahmood, Ahmad and Nikolov, Ivan and Motamed, Saman and Zheng, Wei-Shi and Wang, Xi and Gool, Luc Van and Paudel, Danda Pani},
	month = nov,
	year = {2024},
	note = {arXiv:2411.16804 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_gmflow_2024,
	title = {{GMFlow}: {Global} {Motion}-{Guided} {Recurrent} {Flow} for {6D} {Object} {Pose} {Estimation}},
	shorttitle = {{GMFlow}},
	url = {http://arxiv.org/abs/2411.17174},
	doi = {10.48550/arXiv.2411.17174},
	abstract = {6D object pose estimation is crucial for robotic perception and precise manipulation. Occlusion and incomplete object visibility are common challenges in this task, but existing pose refinement methods often struggle to handle these issues effectively. To tackle this problem, we propose a global motion-guided recurrent flow estimation method called GMFlow for pose estimation. GMFlow overcomes local ambiguities caused by occlusion or missing parts by seeking global explanations. We leverage the object's structural information to extend the motion of visible parts of the rigid body to its invisible regions. Specifically, we capture global contextual information through a linear attention mechanism and guide local motion information to generate global motion estimates. Furthermore, we introduce object shape constraints in the flow iteration process, making flow estimation suitable for pose estimation scenarios. Experiments on the LM-O and YCB-V datasets demonstrate that our method outperforms existing techniques in accuracy while maintaining competitive computational efficiency.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Liu, Xin and Xue, Shibei and Zhao, Dezong and Ma, Shan and Jiang, Min},
	month = nov,
	year = {2024},
	note = {arXiv:2411.17174 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_fungrasp_2024,
	title = {{FunGrasp}: {Functional} {Grasping} for {Diverse} {Dexterous} {Hands}},
	shorttitle = {{FunGrasp}},
	url = {http://arxiv.org/abs/2411.16755},
	doi = {10.48550/arXiv.2411.16755},
	abstract = {Functional grasping is essential for humans to perform specific tasks, such as grasping scissors by the finger holes to cut materials or by the blade to safely hand them over. Enabling dexterous robot hands with functional grasping capabilities is crucial for their deployment to accomplish diverse real-world tasks. Recent research in dexterous grasping, however, often focuses on power grasps while overlooking task- and object-specific functional grasping poses. In this paper, we introduce FunGrasp, a system that enables functional dexterous grasping across various robot hands and performs one-shot transfer to unseen objects. Given a single RGBD image of functional human grasping, our system estimates the hand pose and transfers it to different robotic hands via a human-to-robot (H2R) grasp retargeting module. Guided by the retargeted grasping poses, a policy is trained through reinforcement learning in simulation for dynamic grasping control. To achieve robust sim-to-real transfer, we employ several techniques including privileged learning, system identification, domain randomization, and gravity compensation. In our experiments, we demonstrate that our system enables diverse functional grasping of unseen objects using single RGBD images, and can be successfully deployed across various dexterous robot hands. The significance of the components is validated through comprehensive ablation studies. Project page: https://hly-123.github.io/FunGrasp/ .},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Huang, Linyi and Zhang, Hui and Wu, Zijian and Christen, Sammy and Song, Jie},
	month = nov,
	year = {2024},
	note = {arXiv:2411.16755 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{mittendorfer_humanoid_2011,
	title = {Humanoid {Multimodal} {Tactile}-{Sensing} {Modules}},
	volume = {27},
	issn = {1941-0468},
	url = {https://ieeexplore.ieee.org/document/5711674},
	doi = {10.1109/TRO.2011.2106330},
	abstract = {In this paper, we present a new generation of active tactile modules (i.e., HEX-O-SKIN), which are developed in order to approach multimodal whole-body-touch sensation for humanoid robots. To better perform like humans, humanoid robots need the variety of different sensory modalities in order to interact with their environment. This calls for certain robustness and fault tolerance as well as an intelligent solution to connect the different sensory modalities to the robot. Each HEX-O-SKIN is a small hexagonal printed circuit board equipped with multiple discrete sensors for temperature, acceleration, and proximity. With these sensors, we emulate the human sense of temperature, vibration, and light touch. Off-the-shelf sensors were utilized to speed up our development cycle; however, in general, we can easily extend our design with new discrete sensors, thereby making it flexible for further exploration. A local controller on each HEX-O-SKIN preprocesses the sensor signals and actively routes data through a network of modules toward the closest PC connection. Local processing decreases the necessary network and high-level processing bandwidth, while a local analog-to-digital conversion and digital-data transfers are less sensitive to electromagnetic interference. With an active data-routing scheme, it is also possible to reroute the data around broken connections—yielding robustness throughout the global structure while minimizing wirings. To support our approach, multiple HEX-O-SKIN are embedded into a rapid-prototyped elastomer skin material and redundantly connected to neighboring modules by just four ports. The wiring complexity is shifted to each HEX-O-SKIN such that a power and data connection between two modules is reduced to four noncrossing wires. Thus, only a very simple robot-specific base frame is needed to support and wire the HEX-O-SKIN to a robot. The potential of our multimodal sensor modules is demonstrated experimentally on a robot platform.},
	number = {3},
	urldate = {2024-12-11},
	journal = {IEEE Transactions on Robotics},
	author = {Mittendorfer, Philipp and Cheng, Gordon},
	month = jun,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Artificial sensor skin, Materials, Robot sensing systems, Skin, Temperature sensors, Vibrations, humanoid skin, multimodal skin, sensor network, tactile-sensor module, touch controller},
	pages = {401--410},
}

@misc{liu_dynamics-aware_2024,
	title = {Dynamics-{Aware} {Gaussian} {Splatting} {Streaming} {Towards} {Fast} {On}-the-{Fly} {Training} for {4D} {Reconstruction}},
	url = {http://arxiv.org/abs/2411.14847},
	doi = {10.48550/arXiv.2411.14847},
	abstract = {The recent development of 3D Gaussian Splatting (3DGS) has led to great interest in 4D dynamic spatial reconstruction from multi-view visual inputs. While existing approaches mainly rely on processing full-length multi-view videos for 4D reconstruction, there has been limited exploration of iterative online reconstruction methods that enable on-the-fly training and per-frame streaming. Current 3DGS-based streaming methods treat the Gaussian primitives uniformly and constantly renew the densified Gaussians, thereby overlooking the difference between dynamic and static features and also neglecting the temporal continuity in the scene. To address these limitations, we propose a novel three-stage pipeline for iterative streamable 4D dynamic spatial reconstruction. Our pipeline comprises a selective inheritance stage to preserve temporal continuity, a dynamics-aware shift stage for distinguishing dynamic and static primitives and optimizing their movements, and an error-guided densification stage to accommodate emerging objects. Our method achieves state-of-the-art performance in online 4D reconstruction, demonstrating a 20\% improvement in on-the-fly training speed, superior representation quality, and real-time rendering capability. Project page: https://www.liuzhening.top/DASS},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Liu, Zhening and Hu, Yingdong and Zhang, Xinjie and Shao, Jiawei and Lin, Zehong and Zhang, Jun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14847 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{meng_aim_2024,
	title = {Aim {My} {Robot}: {Precision} {Local} {Navigation} to {Any} {Object}},
	shorttitle = {Aim {My} {Robot}},
	url = {http://arxiv.org/abs/2411.14770},
	doi = {10.48550/arXiv.2411.14770},
	abstract = {Existing navigation systems mostly consider "success" when the robot reaches within 1m radius to a goal. This precision is insufficient for emerging applications where the robot needs to be positioned precisely relative to an object for downstream tasks, such as docking, inspection, and manipulation. To this end, we design and implement Aim-My-Robot (AMR), a local navigation system that enables a robot to reach any object in its vicinity at the desired relative pose, with centimeter-level precision. AMR achieves high precision and robustness by leveraging multi-modal perception, precise action prediction, and is trained on large-scale photorealistic data generated in simulation. AMR shows strong sim2real transfer and can adapt to different robot kinematics and unseen objects with little to no fine-tuning.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Meng, Xiangyun and Yang, Xuning and Jung, Sanghun and Ramos, Fabio and Jujjavarapu, Srid Sadhan and Paul, Sanjoy and Fox, Dieter},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14770 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{ren_dino-x_2024,
	title = {{DINO}-{X}: {A} {Unified} {Vision} {Model} for {Open}-{World} {Object} {Detection} and {Understanding}},
	shorttitle = {{DINO}-{X}},
	url = {http://arxiv.org/abs/2411.14347},
	doi = {10.48550/arXiv.2411.14347},
	abstract = {In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Ren, Tianhe and Chen, Yihao and Jiang, Qing and Zeng, Zhaoyang and Xiong, Yuda and Liu, Wenlong and Ma, Zhengyu and Shen, Junyi and Gao, Yuan and Jiang, Xiaoke and Chen, Xingyu and Song, Zhuheng and Zhang, Yuhong and Huang, Hongjie and Gao, Han and Liu, Shilong and Zhang, Hao and Li, Feng and Yu, Kent and Zhang, Lei},
	month = dec,
	year = {2024},
	note = {arXiv:2411.14347 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shaw_bimanual_2024,
	title = {Bimanual {Dexterity} for {Complex} {Tasks}},
	url = {http://arxiv.org/abs/2411.13677},
	doi = {10.48550/arXiv.2411.13677},
	abstract = {To train generalist robot policies, machine learning methods often require a substantial amount of expert human teleoperation data. An ideal robot for humans collecting data is one that closely mimics them: bimanual arms and dexterous hands. However, creating such a bimanual teleoperation system with over 50 DoF is a significant challenge. To address this, we introduce Bidex, an extremely dexterous, low-cost, low-latency and portable bimanual dexterous teleoperation system which relies on motion capture gloves and teacher arms. We compare Bidex to a Vision Pro teleoperation system and a SteamVR system and find Bidex to produce better quality data for more complex tasks at a faster rate. Additionally, we show Bidex operating a mobile bimanual robot for in the wild tasks. The robot hands (5k USD) and teleoperation system (7k USD) is readily reproducible and can be used on many robot arms including two xArms (16k USD). Website at https://bidex-teleop.github.io/},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Shaw, Kenneth and Li, Yulong and Yang, Jiahui and Srirama, Mohan Kumar and Liu, Ray and Xiong, Haoyu and Mendonca, Russell and Pathak, Deepak},
	month = nov,
	year = {2024},
	note = {arXiv:2411.13677 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{cai_baking_2024,
	title = {Baking {Gaussian} {Splatting} into {Diffusion} {Denoiser} for {Fast} and {Scalable} {Single}-stage {Image}-to-{3D} {Generation}},
	url = {http://arxiv.org/abs/2411.14384},
	doi = {10.48550/arXiv.2411.14384},
	abstract = {Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric prompt images. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB higher in PSNR and 23.25 lower in FID) and over 5x faster speed ({\textasciitilde}6s on an A100 GPU) than SOTA methods. The user study and text-to-3D applications also reveals the practical values of our method. Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive generation results.},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Cai, Yuanhao and Zhang, He and Zhang, Kai and Liang, Yixun and Ren, Mengwei and Luan, Fujun and Liu, Qing and Kim, Soo Ye and Zhang, Jianming and Zhang, Zhifei and Zhou, Yuqian and Lin, Zhe and Yuille, Alan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14384 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{liu_easyhoi_2024,
	title = {{EasyHOI}: {Unleashing} the {Power} of {Large} {Models} for {Reconstructing} {Hand}-{Object} {Interactions} in the {Wild}},
	shorttitle = {{EasyHOI}},
	url = {http://arxiv.org/abs/2411.14280},
	doi = {10.48550/arXiv.2411.14280},
	abstract = {Our work aims to reconstruct hand-object interactions from a single-view image, which is a fundamental but ill-posed task. Unlike methods that reconstruct from videos, multi-view images, or predefined 3D templates, single-view reconstruction faces significant challenges due to inherent ambiguities and occlusions. These challenges are further amplified by the diverse nature of hand poses and the vast variety of object shapes and sizes. Our key insight is that current foundational models for segmentation, inpainting, and 3D reconstruction robustly generalize to in-the-wild images, which could provide strong visual and geometric priors for reconstructing hand-object interactions. Specifically, given a single image, we first design a novel pipeline to estimate the underlying hand pose and object shape using off-the-shelf large models. Furthermore, with the initial reconstruction, we employ a prior-guided optimization scheme, which optimizes hand pose to comply with 3D physical constraints and the 2D input image content. We perform experiments across several datasets and show that our method consistently outperforms baselines and faithfully reconstructs a diverse set of hand-object interactions. Here is the link of our project page: https://lym29.github.io/EasyHOI-page/},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Liu, Yumeng and Long, Xiaoxiao and Yang, Zemin and Liu, Yuan and Habermann, Marc and Theobalt, Christian and Ma, Yuexin and Wang, Wenping},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14280 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ma_find_2024,
	title = {Find {Any} {Part} in {3D}},
	url = {http://arxiv.org/abs/2411.13550},
	doi = {10.48550/arXiv.2411.13550},
	abstract = {We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/},
	urldate = {2024-12-09},
	publisher = {arXiv},
	author = {Ma, Ziqi and Yue, Yisong and Gkioxari, Georgia},
	month = nov,
	year = {2024},
	note = {arXiv:2411.13550 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vosylius_instant_2024,
	title = {Instant {Policy}: {In}-{Context} {Imitation} {Learning} via {Graph} {Diffusion}},
	shorttitle = {Instant {Policy}},
	url = {http://arxiv.org/abs/2411.12633},
	doi = {10.48550/arXiv.2411.12633},
	abstract = {Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations - arbitrary trajectories generated in simulation - as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Vosylius, Vitalis and Johns, Edward},
	month = nov,
	year = {2024},
	note = {arXiv:2411.12633 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kong_dgs-slam_2024,
	title = {{DGS}-{SLAM}: {Gaussian} {Splatting} {SLAM} in {Dynamic} {Environment}},
	shorttitle = {{DGS}-{SLAM}},
	url = {http://arxiv.org/abs/2411.10722},
	doi = {10.48550/arXiv.2411.10722},
	abstract = {We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic SLAM framework built on the foundation of Gaussian Splatting. While recent advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene representation, most approaches assume a static environment, making them vulnerable to photometric and geometric inconsistencies caused by dynamic objects. To address these challenges, we integrate Gaussian Splatting SLAM with a robust filtering process to handle dynamic objects throughout the entire pipeline, including Gaussian insertion and keyframe selection. Within this framework, to further improve the accuracy of dynamic object removal, we introduce a robust mask generation method that enforces photometric consistency across keyframes, reducing noise from inaccurate segmentation and artifacts such as shadows. Additionally, we propose the loop-aware window selection mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops between the current and past frames, facilitating joint optimization of the current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art performance in both camera tracking and novel view synthesis on various dynamic SLAM benchmarks, proving its effectiveness in handling real-world dynamic scenes.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Kong, Mangyu and Lee, Jaewon and Lee, Seongwon and Kim, Euntai},
	month = nov,
	year = {2024},
	note = {arXiv:2411.10722 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{li_robogsim_2024,
	title = {{RoboGSim}: {A} {Real2Sim2Real} {Robotic} {Gaussian} {Splatting} {Simulator}},
	shorttitle = {{RoboGSim}},
	url = {http://arxiv.org/abs/2411.11839},
	doi = {10.48550/arXiv.2411.11839},
	abstract = {Efficient acquisition of real-world embodied data has been increasingly critical. However, large-scale demonstrations captured by remote operation tend to take extremely high costs and fail to scale up the data size in an efficient manner. Sampling the episodes under a simulated environment is a promising way for large-scale collection while existing simulators fail to high-fidelity modeling on texture and physics. To address these limitations, we introduce the RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting and the physics engine. RoboGSim mainly includes four parts: Gaussian Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine. It can synthesize the simulated data with novel views, objects, trajectories, and scenes. RoboGSim also provides an online, reproducible, and safe evaluation for different manipulation policies. The real2sim and sim2real transfer experiments show a high consistency in the texture and physics. Moreover, the effectiveness of synthetic data is validated under the real-world manipulated tasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison on policy learning. More information can be found on our project page https://robogsim.github.io/ .},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Li, Xinhai and Li, Jialin and Zhang, Ziheng and Zhang, Rui and Jia, Fan and Wang, Tiancai and Fan, Haoqiang and Tseng, Kuo-Kun and Wang, Ruiping},
	month = nov,
	year = {2024},
	note = {arXiv:2411.11839 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_architect_2024,
	title = {Architect: {Generating} {Vivid} and {Interactive} {3D} {Scenes} with {Hierarchical} {2D} {Inpainting}},
	shorttitle = {Architect},
	url = {http://arxiv.org/abs/2411.09823},
	doi = {10.48550/arXiv.2411.09823},
	abstract = {Creating large-scale interactive 3D environments is essential for the development of Robotics and Embodied AI research. Current methods, including manual design, procedural generation, diffusion-based scene generation, and large language model (LLM) guided scene design, are hindered by limitations such as excessive human effort, reliance on predefined rules or training datasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image generative models better capture scene and object configuration than LLMs, we address these challenges by introducing Architect, a generative framework that creates complex and realistic 3D embodied environments leveraging diffusion-based 2D image inpainting. In detail, we utilize foundation visual perception models to obtain each generated object from the image and leverage pre-trained depth estimation models to lift the generated 2D image to 3D space. Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate placement of large furniture and small objects to enrich the scene. This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Wang, Yian and Qiu, Xiaowen and Liu, Jiageng and Chen, Zhehuan and Cai, Jiting and Wang, Yufei and Wang, Tsun-Hsuan and Xian, Zhou and Gan, Chuang},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09823 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wen_vidman_2024,
	title = {{VidMan}: {Exploiting} {Implicit} {Dynamics} from {Video} {Diffusion} {Model} for {Effective} {Robot} {Manipulation}},
	shorttitle = {{VidMan}},
	url = {http://arxiv.org/abs/2411.09153},
	doi = {10.48550/arXiv.2411.09153},
	abstract = {Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose VidMan (Video Diffusion for Robot Manipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7\% relative improvement, and demonstrates over 9\% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Wen, Youpeng and Lin, Junfan and Zhu, Yi and Han, Jianhua and Xu, Hang and Zhao, Shen and Liang, Xiaodan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09153 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{gode_flownav_2024,
	title = {{FlowNav}: {Learning} {Efficient} {Navigation} {Policies} via {Conditional} {Flow} {Matching}},
	shorttitle = {{FlowNav}},
	url = {http://arxiv.org/abs/2411.09524},
	doi = {10.48550/arXiv.2411.09524},
	abstract = {Effective robot navigation in dynamic environments is a challenging task that depends on generating precise control actions at high frequencies. Recent advancements have framed navigation as a goal-conditioned control problem. Current state-of-the-art methods for goal-based navigation, such as diffusion policies, either generate sub-goal images or robot control actions to guide robots. However, despite their high accuracy, these methods incur substantial computational costs, which limits their practicality for real-time applications. Recently, Conditional Flow Matching(CFM) has emerged as a more efficient and robust generalization of diffusion. In this work we explore the use of CFM to learn action policies that help the robot navigate its environment. Our results demonstrate that CFM is able to generate highly accurate robot actions. CFM not only matches the accuracy of diffusion policies but also significantly improves runtime performance. This makes it particularly advantageous for real-time robot navigation, where swift, reliable action generation is vital for collision avoidance and smooth operation. By leveraging CFM, we provide a pathway to more scalable, responsive robot navigation systems capable of handling the demands of dynamic and unpredictable environments.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Gode, Samiran and Nayak, Abhijeet and Burgard, Wolfram},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09524 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liu_one-shot_2024,
	title = {One-{Shot} {Manipulation} {Strategy} {Learning} by {Making} {Contact} {Analogies}},
	url = {http://arxiv.org/abs/2411.09627},
	doi = {10.48550/arXiv.2411.09627},
	abstract = {We present a novel approach, MAGIC (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, MAGIC effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. MAGIC demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories. Website: https://magic-2024.github.io/ .},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Liu, Yuyao and Mao, Jiayuan and Tenenbaum, Joshua and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09627 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yu_dynamic_2024,
	title = {Dynamic {Reconstruction} of {Hand}-{Object} {Interaction} with {Distributed} {Force}-aware {Contact} {Representation}},
	url = {http://arxiv.org/abs/2411.09572},
	doi = {10.48550/arXiv.2411.09572},
	abstract = {We present ViTaM-D, a novel visual-tactile framework for dynamic hand-object interaction reconstruction, integrating distributed tactile sensing for more accurate contact modeling. While existing methods focus primarily on visual inputs, they struggle with capturing detailed contact interactions such as object deformation. Our approach leverages distributed tactile sensors to address this limitation by introducing DF-Field. This distributed force-aware contact representation models both kinetic and potential energy in hand-object interaction. ViTaM-D first reconstructs hand-object interactions using a visual-only network, VDT-Net, and then refines contact details through a force-aware optimization (FO) process, enhancing object deformation modeling. To benchmark our approach, we introduce the HOT dataset, which features 600 sequences of hand-object interactions, including deformable objects, built in a high-precision simulation environment. Extensive experiments on both the DexYCB and HOT datasets demonstrate significant improvements in accuracy over previous state-of-the-art methods such as gSDF and HOTrack. Our results highlight the superior performance of ViTaM-D in both rigid and deformable object reconstruction, as well as the effectiveness of DF-Field in refining hand poses. This work offers a comprehensive solution to dynamic hand-object interaction reconstruction by seamlessly integrating visual and tactile data. Codes, models, and datasets will be available.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Yu, Zhenjun and Xu, Wenqiang and Xie, Pengfei and Li, Yutong and Lu, Cewu},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09572 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yuan_unihoi_2024,
	title = {{UniHOI}: {Learning} {Fast}, {Dense} and {Generalizable} {4D} {Reconstruction} for {Egocentric} {Hand} {Object} {Interaction} {Videos}},
	shorttitle = {{UniHOI}},
	url = {http://arxiv.org/abs/2411.09145},
	doi = {10.48550/arXiv.2411.09145},
	abstract = {Egocentric Hand Object Interaction (HOI) videos provide valuable insights into human interactions with the physical world, attracting growing interest from the computer vision and robotics communities. A key task in fully understanding the geometry and dynamics of HOI scenes is dense pointclouds sequence reconstruction. However, the inherent motion of both hands and the camera makes this challenging. Current methods often rely on time-consuming test-time optimization, making them impractical for reconstructing internet-scale videos. To address this, we introduce UniHOI, a model that unifies the estimation of all variables necessary for dense 4D reconstruction, including camera intrinsic, camera poses, and video depth, for egocentric HOI scene in a fast feed-forward manner. We end-to-end optimize all these variables to improve their consistency in 3D space. Furthermore, our model could be trained solely on large-scale monocular video dataset, overcoming the limitation of scarce labeled HOI data. We evaluate UniHOI with both in-domain and zero-shot generalization setting, surpassing all baselines in pointclouds sequence reconstruction and long-term 3D scene flow recovery. UniHOI is the first approach to offer fast, dense, and generalizable monocular egocentric HOI scene reconstruction in the presence of motion. Code and trained model will be released in the future.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Yuan, Chengbo and Chen, Geng and Yi, Li and Gao, Yang},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09145 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{su_motion_2024,
	title = {Motion {Before} {Action}: {Diffusing} {Object} {Motion} as {Manipulation} {Condition}},
	shorttitle = {Motion {Before} {Action}},
	url = {http://arxiv.org/abs/2411.09658},
	doi = {10.48550/arXiv.2411.09658},
	abstract = {Inferring object motion representations from observations enhances the performance of robotic manipulation tasks. This paper introduces a new paradigm for robot imitation learning that generates action sequences by reasoning about object motion from visual observations. We propose MBA (Motion Before Action), a novel module that employs two cascaded diffusion processes for object motion generation and robot action generation under object motion guidance. MBA first predicts the future pose sequence of the object based on observations, then uses this sequence as a condition to guide robot action generation. Designed as a plug-and-play component, MBA can be flexibly integrated into existing robotic manipulation policies with diffusion action heads. Extensive experiments in both simulated and real-world environments demonstrate that our approach substantially improves the performance of existing policies across a wide range of manipulation tasks. Project page: https://selen-suyue.github.io/MBApage/},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Su, Yue and Zhan, Xinyu and Fang, Hongjie and Li, Yong-Lu and Lu, Cewu and Yang, Lixin},
	month = nov,
	year = {2024},
	note = {arXiv:2411.09658 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{dalal_local_2024,
	title = {Local {Policies} {Enable} {Zero}-shot {Long}-horizon {Manipulation}},
	url = {http://arxiv.org/abs/2410.22332},
	doi = {10.48550/arXiv.2410.22332},
	abstract = {Sim2real for robotic manipulation is difficult due to the challenges of simulating complex contacts and generating realistic task distributions. To tackle the latter problem, we introduce ManipGen, which leverages a new class of policies for sim2real transfer: local policies. Locality enables a variety of appealing properties including invariances to absolute robot and object pose, skill ordering, and global scene configuration. We combine these policies with foundation models for vision, language and motion planning and demonstrate SOTA zero-shot performance of our method to Robosuite benchmark tasks in simulation (97\%). We transfer our local policies from simulation to reality and observe they can solve unseen long-horizon manipulation tasks with up to 8 stages with significant pose, object and scene configuration variation. ManipGen outperforms SOTA approaches such as SayCan, OpenVLA, LLMTrajGen and VoxPoser across 50 real-world manipulation tasks by 36\%, 76\%, 62\% and 60\% respectively. Video results at https://mihdalal.github.io/manipgen/},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Dalal, Murtaza and Liu, Min and Talbott, Walter and Chen, Chen and Pathak, Deepak and Zhang, Jian and Salakhutdinov, Ruslan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22332},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kasolowsky_fine_2024,
	title = {Fine {Manipulation} {Using} a {Tactile} {Skin}: {Learning} in {Simulation} and {Sim}-to-{Real} {Transfer}},
	shorttitle = {Fine {Manipulation} {Using} a {Tactile} {Skin}},
	url = {http://arxiv.org/abs/2409.12735},
	doi = {10.48550/arXiv.2409.12735},
	abstract = {We want to enable fine manipulation with a multi-fingered robotic hand by using modern deep reinforcement learning methods. Key for fine manipulation is a spatially resolved tactile sensor. Here, we present a novel model of a tactile skin that can be used together with rigid-body (hence fast) physics simulators. The model considers the softness of the real fingertips such that a contact can spread across multiple taxels of the sensor depending on the contact geometry. We calibrate the model parameters to allow for an accurate simulation of the real-world sensor. For this, we present a self-contained calibration method without external tools or sensors. To demonstrate the validity of our approach, we learn two challenging fine manipulation tasks: Rolling a marble and a bolt between two fingers. We show in simulation experiments that tactile feedback is crucial for precise manipulation and reaching sub-taxel resolution of {\textless} 1 mm (despite a taxel spacing of 4 mm). Moreover, we demonstrate that all policies successfully transfer from the simulation to the real robotic hand.},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Kasolowsky, Ulf and Bäuml, Berthold},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12735 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{winkelbauer_learning-based_2024,
	title = {A {Learning}-based {Controller} for {Multi}-{Contact} {Grasps} on {Unknown} {Objects} with a {Dexterous} {Hand}},
	url = {http://arxiv.org/abs/2409.12339},
	doi = {10.48550/arXiv.2409.12339},
	abstract = {Existing grasp controllers usually either only support finger-tip grasps or need explicit configuration of the inner forces. We propose a novel grasp controller that supports arbitrary grasp types, including power grasps with multi-contacts, while operating self-contained on before unseen objects. No detailed contact information is needed, but only a rough 3D model, e.g., reconstructed from a single depth image. First, the external wrench being applied to the object is estimated by using the measured torques at the joints. Then, the torques necessary to counteract the estimated wrench while keeping the object at its initial pose are predicted. The torques are commanded via desired joint angles to an underlying joint-level impedance controller. To reach real-time performance, we propose a learning-based approach that is based on a wrench estimator- and a torque predictor neural network. Both networks are trained in a supervised fashion using data generated via the analytical formulation of the controller. In an extensive simulation-based evaluation, we show that our controller is able to keep 83.1\% of the tested grasps stable when applying external wrenches with up to 10N. At the same time, we outperform the two tested baselines by being more efficient and inducing less involuntary object movement. Finally, we show that the controller also works on the real DLR-Hand II, reaching a cycle time of 6ms.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Winkelbauer, Dominik and Triebel, Rudolph and Bäuml, Berthold},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12339},
	keywords = {Computer Science - Robotics},
}

@misc{khargonkar_robotfingerprint_2024,
	title = {{RobotFingerPrint}: {Unified} {Gripper} {Coordinate} {Space} for {Multi}-{Gripper} {Grasp} {Synthesis}},
	shorttitle = {{RobotFingerPrint}},
	url = {http://arxiv.org/abs/2409.14519},
	doi = {10.48550/arXiv.2409.14519},
	abstract = {We introduce a novel representation named as the unified gripper coordinate space for grasp synthesis of multiple grippers. The space is a 2D surface of a sphere in 3D using longitude and latitude as its coordinates, and it is shared for all robotic grippers. We propose a new algorithm to map the palm surface of a gripper into the unified gripper coordinate space, and design a conditional variational autoencoder to predict the unified gripper coordinates given an input object. The predicted unified gripper coordinates establish correspondences between the gripper and the object, which can be used in an optimization problem to solve the grasp pose and the finger joints for grasp synthesis. We demonstrate that using the unified gripper coordinate space improves the success rate and diversity in the grasp synthesis of multiple grippers.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Khargonkar, Ninad and Casas, Luis Felipe and Prabhakaran, Balakrishnan and Xiang, Yu},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14519 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_monst3r_2024,
	title = {{MonST3R}: {A} {Simple} {Approach} for {Estimating} {Geometry} in the {Presence} of {Motion}},
	shorttitle = {{MonST3R}},
	url = {http://arxiv.org/abs/2410.03825},
	doi = {10.48550/arXiv.2410.03825},
	abstract = {Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Zhang, Junyi and Herrmann, Charles and Hur, Junhwa and Jampani, Varun and Darrell, Trevor and Cole, Forrester and Sun, Deqing and Yang, Ming-Hsuan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{pattabiraman_learning_2024,
	title = {Learning {Precise}, {Contact}-{Rich} {Manipulation} through {Uncalibrated} {Tactile} {Skins}},
	url = {http://arxiv.org/abs/2410.17246},
	doi = {10.48550/arXiv.2410.17246},
	abstract = {While visuomotor policy learning has advanced robotic manipulation, precisely executing contact-rich tasks remains challenging due to the limitations of vision in reasoning about physical interactions. To address this, recent work has sought to integrate tactile sensing into policy learning. However, many existing approaches rely on optical tactile sensors that are either restricted to recognition tasks or require complex dimensionality reduction steps for policy learning. In this work, we explore learning policies with magnetic skin sensors, which are inherently low-dimensional, highly sensitive, and inexpensive to integrate with robotic platforms. To leverage these sensors effectively, we present the Visuo-Skin (ViSk) framework, a simple approach that uses a transformer-based policy and treats skin sensor data as additional tokens alongside visual information. Evaluated on four complex real-world tasks involving credit card swiping, plug insertion, USB insertion, and bookshelf retrieval, ViSk significantly outperforms both vision-only and optical tactile sensing based policies. Further analysis reveals that combining tactile and visual modalities enhances policy performance and spatial generalization, achieving an average improvement of 27.5\% across tasks. https://visuoskin.github.io/},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Pattabiraman, Venkatesh and Cao, Yifeng and Haldar, Siddhant and Pinto, Lerrel and Bhirangi, Raunaq},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17246},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{kwon_graspdiffusion_2024,
	title = {{GraspDiffusion}: {Synthesizing} {Realistic} {Whole}-body {Hand}-{Object} {Interaction}},
	shorttitle = {{GraspDiffusion}},
	url = {http://arxiv.org/abs/2410.13911},
	doi = {10.48550/arXiv.2410.13911},
	abstract = {Recent generative models can synthesize high-quality images but often fail to generate humans interacting with objects using their hands. This arises mostly from the model's misunderstanding of such interactions, and the hardships of synthesizing intricate regions of the body. In this paper, we propose GraspDiffusion, a novel generative method that creates realistic scenes of human-object interaction. Given a 3D object mesh, GraspDiffusion first constructs life-like whole-body poses with control over the object's location relative to the human body. This is achieved by separately leveraging the generative priors for 3D body and hand poses, optimizing them into a joint grasping pose. The resulting pose guides the image synthesis to correctly reflect the intended interaction, allowing the creation of realistic and diverse human-object interaction scenes. We demonstrate that GraspDiffusion can successfully tackle the relatively uninvestigated problem of generating full-bodied human-object interactions while outperforming previous methods. Code and models will be available at https://webtoon.github.io/GraspDiffusion},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Kwon, Patrick and Joo, Hanbyul},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13911},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lu_manigaussian_2024,
	title = {{ManiGaussian}: {Dynamic} {Gaussian} {Splatting} for {Multi}-task {Robotic} {Manipulation}},
	shorttitle = {{ManiGaussian}},
	url = {http://arxiv.org/abs/2403.08321},
	doi = {10.48550/arXiv.2403.08321},
	abstract = {Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1{\textbackslash}\% in average success rate. Project page: https://guanxinglu.github.io/ManiGaussian/.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Lu, Guanxing and Zhang, Shiyi and Wang, Ziwei and Liu, Changliu and Lu, Jiwen and Tang, Yansong},
	month = jul,
	year = {2024},
	note = {arXiv:2403.08321 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xie_intertrack_2024,
	title = {{InterTrack}: {Tracking} {Human} {Object} {Interaction} without {Object} {Templates}},
	shorttitle = {{InterTrack}},
	url = {http://arxiv.org/abs/2408.13953},
	doi = {10.48550/arXiv.2408.13953},
	abstract = {Tracking human object interaction from videos is important to understand human behavior from the rapidly growing stream of video data. Previous video-based methods require predefined object templates while single-image-based methods are template-free but lack temporal consistency. In this paper, we present a method to track human object interaction without any object shape templates. We decompose the 4D tracking problem into per-frame pose tracking and canonical shape optimization. We first apply a single-view reconstruction method to obtain temporally-inconsistent per-frame interaction reconstructions. Then, for the human, we propose an efficient autoencoder to predict SMPL vertices directly from the per-frame reconstructions, introducing temporally consistent correspondence. For the object, we introduce a pose estimator that leverages temporal information to predict smooth object rotations under occlusions. To train our model, we propose a method to generate synthetic interaction videos and synthesize in total 10 hour videos of 8.5k sequences with full 3D ground truth. Experiments on BEHAVE and InterCap show that our method significantly outperforms previous template-based video tracking and single-frame reconstruction methods. Our proposed synthetic video dataset also allows training video-based methods that generalize to real-world videos. Our code and dataset will be publicly released.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Xie, Xianghui and Lenssen, Jan Eric and Pons-Moll, Gerard},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13953},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_dg-slam_2024,
	title = {{DG}-{SLAM}: {Robust} {Dynamic} {Gaussian} {Splatting} {SLAM} with {Hybrid} {Pose} {Optimization}},
	shorttitle = {{DG}-{SLAM}},
	url = {http://arxiv.org/abs/2411.08373},
	doi = {10.48550/arXiv.2411.08373},
	abstract = {Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Xu, Yueming and Jiang, Haochen and Xiao, Zhongyang and Feng, Jianfeng and Zhang, Li},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08373 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{kim_4d_2024,
	title = {{4D} {Gaussian} {Splatting} in the {Wild} with {Uncertainty}-{Aware} {Regularization}},
	url = {http://arxiv.org/abs/2411.08879},
	doi = {10.48550/arXiv.2411.08879},
	abstract = {Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Kim, Mijeong and Lim, Jongwoo and Han, Bohyung},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08879 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kumar_open-world_2024,
	title = {Open-{World} {Task} and {Motion} {Planning} via {Vision}-{Language} {Model} {Inferred} {Constraints}},
	url = {http://arxiv.org/abs/2411.08253},
	doi = {10.48550/arXiv.2411.08253},
	abstract = {Foundation models trained on internet-scale data, such as Vision-Language Models (VLMs), excel at performing tasks involving common sense, such as visual question answering. Despite their impressive capabilities, these models cannot currently be directly applied to challenging robot manipulation problems that require complex and precise continuous reasoning. Task and Motion Planning (TAMP) systems can control high-dimensional continuous systems over long horizons through combining traditional primitive robot operations. However, these systems require detailed model of how the robot can impact its environment, preventing them from directly interpreting and addressing novel human objectives, for example, an arbitrary natural language goal. We propose deploying VLMs within TAMP systems by having them generate discrete and continuous language-parameterized constraints that enable TAMP to reason about open-world concepts. Specifically, we propose algorithms for VLM partial planning that constrain a TAMP system's discrete temporal search and VLM continuous constraints interpretation to augment the traditional manipulation constraints that TAMP systems seek to satisfy. We demonstrate our approach on two robot embodiments, including a real world robot, across several manipulation tasks, where the desired objectives are conveyed solely through language.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Kumar, Nishanth and Ramos, Fabio and Fox, Dieter and Garrett, Caelan Reed},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08253 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{jaegle_perceiver_2022,
	title = {Perceiver {IO}: {A} {General} {Architecture} for {Structured} {Inputs} \& {Outputs}},
	shorttitle = {Perceiver {IO}},
	url = {http://arxiv.org/abs/2107.14795},
	doi = {10.48550/arXiv.2107.14795},
	abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Hénaff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joāo},
	month = mar,
	year = {2022},
	note = {arXiv:2107.14795 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{gao_hicom_2024,
	title = {{HiCoM}: {Hierarchical} {Coherent} {Motion} for {Streamable} {Dynamic} {Scene} with {3D} {Gaussian} {Splatting}},
	shorttitle = {{HiCoM}},
	url = {http://arxiv.org/abs/2411.07541},
	doi = {10.48550/arXiv.2411.07541},
	abstract = {The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed HiCoM, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about \$20{\textbackslash}\%\$ and reduces the data storage by \$85{\textbackslash}\%\$, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to \${\textless}2\$ seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Gao, Qiankun and Meng, Jiarui and Wen, Chengxiang and Chen, Jie and Zhang, Jian},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07541 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cherian_llmphy_2024,
	title = {{LLMPhy}: {Complex} {Physical} {Reasoning} {Using} {Large} {Language} {Models} and {World} {Models}},
	shorttitle = {{LLMPhy}},
	url = {http://arxiv.org/abs/2411.08027},
	doi = {10.48550/arXiv.2411.08027},
	abstract = {Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact -- the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present LLMPhy, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, LLMPhy uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of LLMPhy, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Cherian, Anoop and Corcodel, Radu and Jain, Siddarth and Romeres, Diego},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08027 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_splatformer_2024,
	title = {{SplatFormer}: {Point} {Transformer} for {Robust} {3D} {Gaussian} {Splatting}},
	shorttitle = {{SplatFormer}},
	url = {http://arxiv.org/abs/2411.06390},
	doi = {10.48550/arXiv.2411.06390},
	abstract = {3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Chen, Yutong and Mihajlovic, Marko and Chen, Xiyi and Wang, Yiming and Prokudin, Sergey and Tang, Siyu},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06390 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_adaptive_2024,
	title = {Adaptive and {Temporally} {Consistent} {Gaussian} {Surfels} for {Multi}-view {Dynamic} {Reconstruction}},
	url = {http://arxiv.org/abs/2411.06602},
	doi = {10.48550/arXiv.2411.06602},
	abstract = {3D Gaussian Splatting has recently achieved notable success in novel view synthesis for dynamic scenes and geometry reconstruction in static scenes. Building on these advancements, early methods have been developed for dynamic surface reconstruction by globally optimizing entire sequences. However, reconstructing dynamic scenes with significant topology changes, emerging or disappearing objects, and rapid movements remains a substantial challenge, particularly for long sequences. To address these issues, we propose AT-GS, a novel method for reconstructing high-quality dynamic surfaces from multi-view videos through per-frame incremental optimization. To avoid local minima across frames, we introduce a unified and adaptive gradient-aware densification strategy that integrates the strengths of conventional cloning and splitting techniques. Additionally, we reduce temporal jittering in dynamic surfaces by ensuring consistency in curvature maps across consecutive frames. Our method achieves superior accuracy and temporal coherence in dynamic surface reconstruction, delivering high-fidelity space-time novel view synthesis, even in complex and challenging scenes. Extensive experiments on diverse multi-view video datasets demonstrate the effectiveness of our approach, showing clear advantages over baseline methods. Project page: {\textbackslash}url\{https://fraunhoferhhi.github.io/AT-GS\}},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Chen, Decai and Oberson, Brianne and Feldmann, Ingo and Schreer, Oliver and Hilsmann, Anna and Eisert, Peter},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06602 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lei_spartan_2024,
	title = {{SPARTAN}: {A} {Sparse} {Transformer} {Learning} {Local} {Causation}},
	shorttitle = {{SPARTAN}},
	url = {http://arxiv.org/abs/2411.06890},
	doi = {10.48550/arXiv.2411.06890},
	abstract = {Causal structures play a central role in world models that flexibly adapt to changes in the environment. While recent works motivate the benefits of discovering local causal graphs for dynamics modelling, in this work we demonstrate that accurately capturing these relationships in complex settings remains challenging for the current state-of-the-art. To remedy this shortcoming, we postulate that sparsity is a critical ingredient for the discovery of such local causal structures. To this end we present the SPARse TrANsformer World model (SPARTAN), a Transformer-based world model that learns local causal structures between entities in a scene. By applying sparsity regularisation on the attention pattern between object-factored tokens, SPARTAN identifies sparse local causal models that accurately predict future object states. Furthermore, we extend our model to capture sparse interventions with unknown targets on the dynamics of the environment. This results in a highly interpretable world model that can efficiently adapt to changes. Empirically, we evaluate SPARTAN against the current state-of-the-art in object-centric world models on observation-based environments and demonstrate that our model can learn accurate local causal graphs and achieve significantly improved few-shot adaptation to changes in the dynamics of the environment as well as robustness against removing irrelevant distractors.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Lei, Anson and Schölkopf, Bernhard and Posner, Ingmar},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06890 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hess_sampling-based_2024,
	title = {Sampling-{Based} {Model} {Predictive} {Control} for {Dexterous} {Manipulation} on a {Biomimetic} {Tendon}-{Driven} {Hand}},
	url = {http://arxiv.org/abs/2411.06183},
	doi = {10.48550/arXiv.2411.06183},
	abstract = {Biomimetic and compliant robotic hands offer the potential for human-like dexterity, but controlling them is challenging due to high dimensionality, complex contact interactions, and uncertainties in state estimation. Sampling-based model predictive control (MPC), using a physics simulator as the dynamics model, is a promising approach for generating contact-rich behavior. However, sampling-based MPC has yet to be evaluated on physical (non-simulated) robotic hands, particularly on compliant hands with state uncertainties. We present the first successful demonstration of in-hand manipulation on a physical biomimetic tendon-driven robot hand using sampling-based MPC. While sampling-based MPC does not require lengthy training cycles like reinforcement learning approaches, it still necessitates adapting the task-specific objective function to ensure robust behavior execution on physical hardware. To adapt the objective function, we integrate a visual language model (VLM) with a real-time optimizer (MuJoCo MPC). We provide the VLM with a high-level human language description of the task, and a video of the hand's current behavior. The VLM iteratively adapts the objective function, enabling effective behavior generation. In our experiments, the hand achieves an average ball rolling speed of 0.35 rad/s, successful ball flips, and catching with a 67{\textbackslash}\% success rate. Our results demonstrate that sampling-based MPC is a promising approach for generating dexterous manipulation skills on biomimetic hands without extensive training cycles.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Hess, Adrian and Kübler, Alexander M. and Forrai, Benedek and Dogar, Mehmet and Katzschmann, Robert K.},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06183 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yang_sampart3d_2024,
	title = {{SAMPart3D}: {Segment} {Any} {Part} in {3D} {Objects}},
	shorttitle = {{SAMPart3D}},
	url = {http://arxiv.org/abs/2411.07184},
	doi = {10.48550/arXiv.2411.07184},
	abstract = {3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Yang, Yunhan and Huang, Yukun and Guo, Yuan-Chen and Lu, Liangjun and Wu, Xiaoyang and Lam, Edmund Y. and Cao, Yan-Pei and Liu, Xihui},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07184 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{azulay_visuotactile-based_2024,
	title = {Visuotactile-{Based} {Learning} for {Insertion} with {Compliant} {Hands}},
	url = {http://arxiv.org/abs/2411.06408},
	doi = {10.48550/arXiv.2411.06408},
	abstract = {Compared to rigid hands, underactuated compliant hands offer greater adaptability to object shapes, provide stable grasps, and are often more cost-effective. However, they introduce uncertainties in hand-object interactions due to their inherent compliance and lack of precise finger proprioception as in rigid hands. These limitations become particularly significant when performing contact-rich tasks like insertion. To address these challenges, additional sensing modalities are required to enable robust insertion capabilities. This letter explores the essential sensing requirements for successful insertion tasks with compliant hands, focusing on the role of visuotactile perception. We propose a simulation-based multimodal policy learning framework that leverages all-around tactile sensing and an extrinsic depth camera. A transformer-based policy, trained through a teacher-student distillation process, is successfully transferred to a real-world robotic system without further training. Our results emphasize the crucial role of tactile sensing in conjunction with visual perception for accurate object-socket pose estimation, successful sim-to-real transfer and robust task execution.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Azulay, Osher and Ramesh, Dhruv Metha and Curtis, Nimrod and Sintov, Avishai},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06408 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{wang_quadwbg_2024,
	title = {{QuadWBG}: {Generalizable} {Quadrupedal} {Whole}-{Body} {Grasping}},
	shorttitle = {{QuadWBG}},
	url = {http://arxiv.org/abs/2411.06782},
	doi = {10.48550/arXiv.2411.06782},
	abstract = {Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89\% in the real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Wang, Jilong and Rajabov, Javokhirbek and Xu, Chaoyi and Zheng, Yiming and Wang, He},
	month = nov,
	year = {2024},
	note = {arXiv:2411.06782 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{kiemel_safe_2024,
	title = {Safe {Reinforcement} {Learning} of {Robot} {Trajectories} in the {Presence} of {Moving} {Obstacles}},
	url = {http://arxiv.org/abs/2411.05784},
	doi = {10.48550/arXiv.2411.05784},
	abstract = {In this paper, we present an approach for learning collision-free robot trajectories in the presence of moving obstacles. As a first step, we train a backup policy to generate evasive movements from arbitrary initial robot states using model-free reinforcement learning. When learning policies for other tasks, the backup policy can be used to estimate the potential risk of a collision and to offer an alternative action if the estimated risk is considered too high. No matter which action is selected, our action space ensures that the kinematic limits of the robot joints are not violated. We analyze and evaluate two different methods for estimating the risk of a collision. A physics simulation performed in the background is computationally expensive but provides the best results in deterministic environments. If a data-based risk estimator is used instead, the computational effort is significantly reduced, but an additional source of error is introduced. For evaluation, we successfully learn a reaching task and a basketball task while keeping the risk of collisions low. The results demonstrate the effectiveness of our approach for deterministic and stochastic environments, including a human-robot scenario and a ball environment, where no state can be considered permanently safe. By conducting experiments with a real robot, we show that our approach can generate safe trajectories in real time.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Kiemel, Jonas and Righetti, Ludovic and Kröger, Torsten and Asfour, Tamim},
	month = nov,
	year = {2024},
	note = {arXiv:2411.05784 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{venkataraman_real-world_2024,
	title = {Real-{World} {Offline} {Reinforcement} {Learning} from {Vision} {Language} {Model} {Feedback}},
	url = {http://arxiv.org/abs/2411.05273},
	doi = {10.48550/arXiv.2411.05273},
	abstract = {Offline reinforcement learning can enable policy learning from pre-collected, sub-optimal datasets without online interactions. This makes it ideal for real-world robots and safety-critical scenarios, where collecting online data or expert demonstrations is slow, costly, and risky. However, most existing offline RL works assume the dataset is already labeled with the task rewards, a process that often requires significant human effort, especially when ground-truth states are hard to ascertain (e.g., in the real-world). In this paper, we build on prior work, specifically RL-VLM-F, and propose a novel system that automatically generates reward labels for offline datasets using preference feedback from a vision-language model and a text description of the task. Our method then learns a policy using offline RL with the reward-labeled dataset. We demonstrate the system's applicability to a complex real-world robot-assisted dressing task, where we first learn a reward function using a vision-language model on a sub-optimal offline dataset, and then we use the learned reward to employ Implicit Q learning to develop an effective dressing policy. Our method also performs well in simulation tasks involving the manipulation of rigid and deformable objects, and significantly outperform baselines such as behavior cloning and inverse RL. In summary, we propose a new system that enables automatic reward labeling and policy learning from unlabeled, sub-optimal offline datasets.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Venkataraman, Sreyas and Wang, Yufei and Wang, Ziyu and Erickson, Zackory and Held, David},
	month = nov,
	year = {2024},
	note = {arXiv:2411.05273 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{micheli_transformers_2022,
	title = {Transformers are {Sample}-{Efficient} {World} {Models}},
	url = {https://openreview.net/forum?id=vhFu1Acb0xb},
	abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.},
	language = {en},
	urldate = {2024-12-08},
	author = {Micheli, Vincent and Alonso, Eloi and Fleuret, François},
	month = sep,
	year = {2022},
}

@misc{ma_vision_2024,
	title = {Vision {Language} {Models} are {In}-{Context} {Value} {Learners}},
	url = {http://arxiv.org/abs/2411.04549},
	doi = {10.48550/arXiv.2411.04549},
	abstract = {Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning ({\textbackslash}GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and advantage-weighted regression -- all without any model training or finetuning.},
	urldate = {2024-12-08},
	publisher = {arXiv},
	author = {Ma, Yecheng Jason and Hejna, Joey and Wahid, Ayzaan and Fu, Chuyuan and Shah, Dhruv and Liang, Jacky and Xu, Zhuo and Kirmani, Sean and Xu, Peng and Driess, Danny and Xiao, Ted and Tompson, Jonathan and Bastani, Osbert and Jayaraman, Dinesh and Yu, Wenhao and Zhang, Tingnan and Sadigh, Dorsa and Xia, Fei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04549 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_dynamem_2024,
	title = {{DynaMem}: {Online} {Dynamic} {Spatio}-{Semantic} {Memory} for {Open} {World} {Mobile} {Manipulation}},
	shorttitle = {{DynaMem}},
	url = {http://arxiv.org/abs/2411.04999},
	doi = {10.48550/arXiv.2411.04999},
	abstract = {Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70\% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Liu, Peiqi and Guo, Zhanqiu and Warke, Mohit and Chintala, Soumith and Paxton, Chris and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04999 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_dino-wm_2024,
	title = {{DINO}-{WM}: {World} {Models} on {Pre}-trained {Visual} {Features} enable {Zero}-shot {Planning}},
	shorttitle = {{DINO}-{WM}},
	url = {http://arxiv.org/abs/2411.04983},
	doi = {10.48550/arXiv.2411.04983},
	abstract = {The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Zhou, Gaoyue and Pan, Hengkai and LeCun, Yann and Pinto, Lerrel},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04983 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{seo_legato_2024,
	title = {{LEGATO}: {Cross}-{Embodiment} {Imitation} {Using} a {Grasping} {Tool}},
	shorttitle = {{LEGATO}},
	url = {http://arxiv.org/abs/2411.03682},
	doi = {10.48550/arXiv.2411.03682},
	abstract = {Cross-embodiment imitation learning enables policies trained on specific embodiments to transfer across different robots, unlocking the potential for large-scale imitation learning that is both cost-effective and highly reusable. This paper presents LEGATO, a cross-embodiment imitation learning framework for visuomotor skill transfer across varied kinematic morphologies. We introduce a handheld gripper that unifies action and observation spaces, allowing tasks to be defined consistently across robots. Using this gripper, we train visuomotor policies via imitation learning, applying a motion-invariant transformation to compute the training loss. Gripper motions are then retargeted into high-degree-of-freedom whole-body motions using inverse kinematics for deployment across diverse embodiments. Our evaluations in simulation and real-robot experiments highlight the framework's effectiveness in learning and transferring visuomotor skills across various robots. More information can be found at the project page: https://ut-hcrl.github.io/LEGATO.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Seo, Mingyo and Park, H. Andy and Yuan, Shenli and Zhu, Yuke and Sentis, Luis},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03682 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{smith_steer_2024,
	title = {{STEER}: {Flexible} {Robotic} {Manipulation} via {Dense} {Language} {Grounding}},
	shorttitle = {{STEER}},
	url = {http://arxiv.org/abs/2411.03409},
	doi = {10.48550/arXiv.2411.03409},
	abstract = {The complexity of the real world demands robotic systems that can intelligently adapt to unseen situations. We present STEER, a robot learning framework that bridges high-level, commonsense reasoning with precise, flexible low-level control. Our approach translates complex situational awareness into actionable low-level behavior through training language-grounded policies with dense annotation. By structuring policy training around fundamental, modular manipulation skills expressed in natural language, STEER exposes an expressive interface for humans or Vision-Language Models (VLMs) to intelligently orchestrate the robot's behavior by reasoning about the task and context. Our experiments demonstrate the skills learned via STEER can be combined to synthesize novel behaviors to adapt to new situations or perform completely new tasks without additional data collection or training.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Smith, Laura and Irpan, Alex and Arenas, Montserrat Gonzalez and Kirmani, Sean and Kalashnikov, Dmitry and Shah, Dhruv and Xiao, Ted},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03409 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{yang_vq-ace_2024,
	title = {{VQ}-{ACE}: {Efficient} {Policy} {Search} for {Dexterous} {Robotic} {Manipulation} via {Action} {Chunking} {Embedding}},
	shorttitle = {{VQ}-{ACE}},
	url = {http://arxiv.org/abs/2411.03556},
	doi = {10.48550/arXiv.2411.03556},
	abstract = {Dexterous robotic manipulation remains a significant challenge due to the high dimensionality and complexity of hand movements required for tasks like in-hand manipulation and object grasping. This paper addresses this issue by introducing Vector Quantized Action Chunking Embedding (VQ-ACE), a novel framework that compresses human hand motion into a quantized latent space, significantly reducing the action space's dimensionality while preserving key motion characteristics. By integrating VQ-ACE with both Model Predictive Control (MPC) and Reinforcement Learning (RL), we enable more efficient exploration and policy learning in dexterous manipulation tasks using a biomimetic robotic hand. Our results show that latent space sampling with MPC produces more human-like behavior in tasks such as Ball Rolling and Object Picking, leading to higher task success rates and reduced control costs. For RL, action chunking accelerates learning and improves exploration, demonstrated through faster convergence in tasks like cube stacking and in-hand cube reorientation. These findings suggest that VQ-ACE offers a scalable and effective solution for robotic manipulation tasks involving complex, high-dimensional state spaces, contributing to more natural and adaptable robotic systems.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Yang, Chenyu and Liconti, Davide and Katzschmann, Robert K.},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03556 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_object-centric_2024,
	title = {Object-{Centric} {Dexterous} {Manipulation} from {Human} {Motion} {Data}},
	url = {http://arxiv.org/abs/2411.04005},
	doi = {10.48550/arXiv.2411.04005},
	abstract = {Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Chen, Yuanpei and Wang, Chen and Yang, Yaodong and Liu, C. Karen},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04005 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{gao_out--distribution_2024,
	title = {Out-of-{Distribution} {Recovery} with {Object}-{Centric} {Keypoint} {Inverse} {Policy} {For} {Visuomotor} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2411.03294},
	doi = {10.48550/arXiv.2411.03294},
	abstract = {We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7\% over the base policy in OOD. Project Website: https://sites.google.com/view/ocr-penn},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Gao, George Jiayuan and Li, Tianyu and Figueroa, Nadia},
	month = nov,
	year = {2024},
	note = {arXiv:2411.03294 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{reichlin_back_2022,
	title = {Back to the {Manifold}: {Recovering} from {Out}-of-{Distribution} {States}},
	shorttitle = {Back to the {Manifold}},
	url = {https://ieeexplore.ieee.org/abstract/document/9981315},
	doi = {10.1109/IROS47612.2022.9981315},
	abstract = {Learning from previously collected datasets of expert data offers the promise of acquiring robotic policies without unsafe and costly online explorations. However, a major challenge is a distributional shift between the states in the training dataset and the ones visited by the learned policy at the test time. While prior works mainly studied the distribution shift caused by the policy during the offline training, the problem of recovering from out-of-distribution states at the deployment time is not very well studied yet. We alleviate the distributional shift at the deployment time by introducing a recovery policy that brings the agent back to the training manifold whenever it steps out of the in-distribution states, e.g., due to an external perturbation. The recovery policy relies on an approximation of the training data density and a learned equivariant mapping that maps visual observations into a latent space in which translations correspond to the robot actions. We demonstrate the effectiveness of the proposed method through several manipulation experiments on a real robotic platform. Our results show that the recovery policy enables the agent to complete tasks while the behavioral cloning alone fails because of the distributional shift problem.},
	urldate = {2024-12-07},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Reichlin, Alfredo and Marchetti, Giovanni Luca and Yin, Hang and Ghadirzadeh, Ali and Kragic, Danica},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Behavioral sciences, Cloning, Manifolds, Perturbation methods, Training, Training data, Visualization},
	pages = {8660--8666},
}

@misc{nasiriany_rt-affordance_2024,
	title = {{RT}-{Affordance}: {Affordances} are {Versatile} {Intermediate} {Representations} for {Robot} {Manipulation}},
	shorttitle = {{RT}-{Affordance}},
	url = {http://arxiv.org/abs/2411.02704},
	doi = {10.48550/arXiv.2411.02704},
	abstract = {We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50\%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance},
	urldate = {2024-12-07},
	publisher = {arXiv},
	author = {Nasiriany, Soroush and Kirmani, Sean and Ding, Tianli and Smith, Laura and Zhu, Yuke and Driess, Danny and Sadigh, Dorsa and Xiao, Ted},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kaidanov_role_2024,
	title = {The {Role} of {Domain} {Randomization} in {Training} {Diffusion} {Policies} for {Whole}-{Body} {Humanoid} {Control}},
	url = {http://arxiv.org/abs/2411.01349},
	doi = {10.48550/arXiv.2411.01349},
	abstract = {Humanoids have the potential to be the ideal embodiment in environments designed for humans. Thanks to the structural similarity to the human body, they benefit from rich sources of demonstration data, e.g., collected via teleoperation, motion capture, or even using videos of humans performing tasks. However, distilling a policy from demonstrations is still a challenging problem. While Diffusion Policies (DPs) have shown impressive results in robotic manipulation, their applicability to locomotion and humanoid control remains underexplored. In this paper, we investigate how dataset diversity and size affect the performance of DPs for humanoid whole-body control. In a simulated IsaacGym environment, we generate synthetic demonstrations by training Adversarial Motion Prior (AMP) agents under various Domain Randomization (DR) conditions, and we compare DPs fitted to datasets of different size and diversity. Our findings show that, although DPs can achieve stable walking behavior, successful training of locomotion policies requires significantly larger and more diverse datasets compared to manipulation tasks, even in simple scenarios.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Kaidanov, Oleg and Al-Hafez, Firas and Suvari, Yusuf and Belousov, Boris and Peters, Jan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01349},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mirchandani_so_2024,
	title = {So {You} {Think} {You} {Can} {Scale} {Up} {Autonomous} {Robot} {Data} {Collection}?},
	url = {http://arxiv.org/abs/2411.01813},
	doi = {10.48550/arXiv.2411.01813},
	abstract = {A long-standing goal in robot learning is to develop methods for robots to acquire new skills autonomously. While reinforcement learning (RL) comes with the promise of enabling autonomous data collection, it remains challenging to scale in the real-world partly due to the significant effort required for environment design and instrumentation, including the need for designing reset functions or accurate success detectors. On the other hand, imitation learning (IL) methods require little to no environment design effort, but instead require significant human supervision in the form of collected demonstrations. To address these shortcomings, recent works in autonomous IL start with an initial seed dataset of human demonstrations that an autonomous policy can bootstrap from. While autonomous IL approaches come with the promise of addressing the challenges of autonomous RL as well as pure IL strategies, in this work, we posit that such techniques do not deliver on this promise and are still unable to scale up autonomous data collection in the real world. Through a series of real-world experiments, we demonstrate that these approaches, when scaled up to realistic settings, face much of the same scaling challenges as prior attempts in RL in terms of environment design. Further, we perform a rigorous study of autonomous IL methods across different data scales and 7 simulation and real-world tasks, and demonstrate that while autonomous data collection can modestly improve performance, simply collecting more human data often provides significantly more improvement. Our work suggests a negative result: that scaling up autonomous data collection for learning robot policies for real-world tasks is more challenging and impractical than what is suggested in prior work. We hope these insights about the core challenges of scaling up data collection help inform future efforts in autonomous learning.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Mirchandani, Suvir and Belkhale, Suneel and Hejna, Joey and Choi, Evelyn and Islam, Md Sazzad and Sadigh, Dorsa},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01813},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{park_dexhub_2024,
	title = {{DexHub} and {DART}: {Towards} {Internet} {Scale} {Robot} {Data} {Collection}},
	shorttitle = {{DexHub} and {DART}},
	url = {http://arxiv.org/abs/2411.02214},
	doi = {10.48550/arXiv.2411.02214},
	abstract = {The quest to build a generalist robotic system is impeded by the scarcity of diverse and high-quality data. While real-world data collection effort exist, requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. We introduce DART, a teleoperation platform designed for crowdsourcing that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR) to address many limitations of prior data collection efforts. Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation. We also demonstrate that policies trained using DART-collected datasets successfully transfer to reality and are robust to unseen visual disturbances. All data collected through DART is automatically stored in our cloud-hosted database, DexHub, which will be made publicly available upon curation, paving the path for DexHub to become an ever-growing data hub for robot learning. Videos are available at: https://dexhub.ai/project},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Park, Younghyo and Bhatia, Jagdeep Singh and Ankile, Lars and Agrawal, Pulkit},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02214},
	keywords = {Computer Science - Robotics},
}

@misc{lu_garmentlab_2024,
	title = {{GarmentLab}: {A} {Unified} {Simulation} and {Benchmark} for {Garment} {Manipulation}},
	shorttitle = {{GarmentLab}},
	url = {http://arxiv.org/abs/2411.01200},
	doi = {10.48550/arXiv.2411.01200},
	abstract = {Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Lu, Haoran and Wu, Ruihai and Li, Yitong and Li, Sijie and Zhu, Ziyu and Ning, Chuanruo and Shen, Yan and Luo, Longzan and Chen, Yuanpei and Dong, Hao},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01200},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
}

@misc{tan_manibox_2024,
	title = {{ManiBox}: {Enhancing} {Spatial} {Grasping} {Generalization} via {Scalable} {Simulation} {Data} {Generation}},
	shorttitle = {{ManiBox}},
	url = {http://arxiv.org/abs/2411.01850},
	doi = {10.48550/arXiv.2411.01850},
	abstract = {Learning a precise robotic grasping policy is crucial for embodied agents operating in complex real-world manipulation tasks. Despite significant advancements, most models still struggle with accurate spatial positioning of objects to be grasped. We first show that this spatial generalization challenge stems primarily from the extensive data requirements for adequate spatial understanding. However, collecting such data with real robots is prohibitively expensive, and relying on simulation data often leads to visual generalization gaps upon deployment. To overcome these challenges, we then focus on state-based policy generalization and present {\textbackslash}textbf\{ManiBox\}, a novel bounding-box-guided manipulation method built on a simulation-based teacher-student framework. The teacher policy efficiently generates scalable simulation data using bounding boxes, which are proven to uniquely determine the objects' spatial positions. The student policy then utilizes these low-dimensional spatial states to enable zero-shot transfer to real robots. Through comprehensive evaluations in simulated and real-world environments, ManiBox demonstrates a marked improvement in spatial grasping generalization and adaptability to diverse objects and backgrounds. Further, our empirical study into scaling laws for policy performance indicates that spatial volume generalization scales positively with data volume. For a certain level of spatial volume, the success rate of grasping empirically follows Michaelis-Menten kinetics relative to data volume, showing a saturation effect as data increases. Our videos and code are available in https://thkkk.github.io/manibox.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Tan, Hengkai and Xu, Xuezhou and Ying, Chengyang and Mao, Xinyi and Liu, Songming and Zhang, Xingxing and Su, Hang and Zhu, Jun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01850},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{qian_task-oriented_2024,
	title = {Task-{Oriented} {Hierarchical} {Object} {Decomposition} for {Visuomotor} {Control}},
	url = {http://arxiv.org/abs/2411.01284},
	doi = {10.48550/arXiv.2411.01284},
	abstract = {Good pre-trained visual representations could enable robots to learn visuomotor policy efficiently. Still, existing representations take a one-size-fits-all-tasks approach that comes with two important drawbacks: (1) Being completely task-agnostic, these representations cannot effectively ignore any task-irrelevant information in the scene, and (2) They often lack the representational capacity to handle unconstrained/complex real-world scenes. Instead, we propose to train a large combinatorial family of representations organized by scene entities: objects and object parts. This hierarchical object decomposition for task-oriented representations (HODOR) permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task. In our experiments, we find that HODOR outperforms prior pre-trained representations, both scene vector representations and object-centric representations, for sample-efficient imitation learning across 5 simulated and 5 real-world manipulation tasks. We further find that the invariances captured in HODOR are inherited into downstream policies, which can robustly generalize to out-of-distribution test conditions, permitting zero-shot skill chaining. Appendix, code, and videos: https://sites.google.com/view/hodor-corl24.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Qian, Jianing and Li, Yunshuang and Bucher, Bernadette and Jayaraman, Dinesh},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01284},
	keywords = {Computer Science - Robotics},
}

@misc{yang_differentiable_2024,
	title = {Differentiable {Physics}-based {System} {Identification} for {Robotic} {Manipulation} of {Elastoplastic} {Materials}},
	url = {http://arxiv.org/abs/2411.00554},
	doi = {10.48550/arXiv.2411.00554},
	abstract = {Robotic manipulation of volumetric elastoplastic deformable materials, from foods such as dough to construction materials like clay, is in its infancy, largely due to the difficulty of modelling and perception in a high-dimensional space. Simulating the dynamics of such materials is computationally expensive. It tends to suffer from inaccurately estimated physics parameters of the materials and the environment, impeding high-precision manipulation. Estimating such parameters from raw point clouds captured by optical cameras suffers further from heavy occlusions. To address this challenge, this work introduces a novel Differentiable Physics-based System Identification (DPSI) framework that enables a robot arm to infer the physics parameters of elastoplastic materials and the environment using simple manipulation motions and incomplete 3D point clouds, aligning the simulation with the real world. Extensive experiments show that with only a single real-world interaction, the estimated parameters, Young's modulus, Poisson's ratio, yield stress and friction coefficients, can accurately simulate visually and physically realistic deformation behaviours induced by unseen and long-horizon manipulation motions. Additionally, the DPSI framework inherently provides physically intuitive interpretations for the parameters in contrast to black-box approaches such as deep neural networks.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Yang, Xintong and Ji, Ze and Lai, Yu-Kun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00554},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Robotics},
}

@misc{seo_posture-informed_2024,
	title = {Posture-{Informed} {Muscular} {Force} {Learning} for {Robust} {Hand} {Pressure} {Estimation}},
	url = {http://arxiv.org/abs/2410.23629},
	doi = {10.48550/arXiv.2410.23629},
	abstract = {We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Seo, Kyungjin and Seo, Junghoon and Jeong, Hanseok and Kim, Sangpil and Yoon, Sang Ho},
	month = nov,
	year = {2024},
	note = {arXiv:2410.23629},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
}

@misc{winter_state-_2024,
	title = {State- and context-dependent robotic manipulation and grasping via uncertainty-aware imitation learning},
	url = {http://arxiv.org/abs/2410.24035},
	doi = {10.48550/arXiv.2410.24035},
	abstract = {Generating context-adaptive manipulation and grasping actions is a challenging problem in robotics. Classical planning and control algorithms tend to be inflexible with regard to parameterization by external variables such as object shapes. In contrast, Learning from Demonstration (LfD) approaches, due to their nature as function approximators, allow for introducing external variables to modulate policies in response to the environment. In this paper, we utilize this property by introducing an LfD approach to acquire context-dependent grasping and manipulation strategies. We treat the problem as a kernel-based function approximation, where the kernel inputs include generic context variables describing task-dependent parameters such as the object shape. We build on existing work on policy fusion with uncertainty quantification to propose a state-dependent approach that automatically returns to demonstrations, avoiding unpredictable behavior while smoothly adapting to context changes. The approach is evaluated against the LASA handwriting dataset and on a real 7-DoF robot in two scenarios: adaptation to slippage while grasping and manipulating a deformable food item.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Winter, Tim R. and Sundaram, Ashok M. and Friedl, Werner and Roa, Maximo A. and Stulp, Freek and Silvério, João},
	month = oct,
	year = {2024},
	note = {arXiv:2410.24035},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{higuera_sparsh_2024,
	title = {Sparsh: {Self}-supervised touch representations for vision-based tactile sensing},
	shorttitle = {Sparsh},
	url = {http://arxiv.org/abs/2410.24090},
	doi = {10.48550/arXiv.2410.24090},
	abstract = {In this work, we introduce general purpose touch representations for the increasingly accessible class of vision-based tactile sensors. Such sensors have led to many recent advances in robot manipulation as they markedly complement vision, yet solutions today often rely on task and sensor specific handcrafted perception models. Collecting real data at scale with task centric ground truth labels, like contact forces and slip, is a challenge further compounded by sensors of various form factor differing in aspects like lighting and gel markings. To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable performance in computer vision. We present Sparsh, a family of SSL models that can support various vision-based tactile sensors, alleviating the need for custom labels through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. We also build TacBench, to facilitate standardized benchmarking across sensors and models, comprising of six tasks ranging from comprehending tactile properties to enabling physical perception and manipulation planning. In evaluations, we find that SSL pre-training for touch representation outperforms task and sensor-specific end-to-end training by 95.1\% on average over TacBench, and Sparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the merits of learning in latent space for tactile images. Project page: https://sparsh-ssl.github.io/},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Higuera, Carolina and Sharma, Akash and Bodduluri, Chaithanya Krishna and Fan, Taosha and Lancaster, Patrick and Kalakrishnan, Mrinal and Kaess, Michael and Boots, Byron and Lambeta, Mike and Wu, Tingfan and Mukadam, Mustafa},
	month = oct,
	year = {2024},
	note = {arXiv:2410.24090},
	keywords = {Computer Science - Robotics},
}

@misc{huang_3d-vitac_2024,
	title = {{3D}-{ViTac}: {Learning} {Fine}-{Grained} {Manipulation} with {Visuo}-{Tactile} {Sensing}},
	shorttitle = {{3D}-{ViTac}},
	url = {http://arxiv.org/abs/2410.24091},
	doi = {10.48550/arXiv.2410.24091},
	abstract = {Tactile and visual perception are both crucial for humans to perform fine-grained interactions with their environment. Developing similar multi-modal sensing capabilities for robots can significantly enhance and expand their manipulation skills. This paper introduces {\textbackslash}textbf\{3D-ViTac\}, a multi-modal sensing and learning system designed for dexterous bimanual manipulation. Our system features tactile sensors equipped with dense sensing units, each covering an area of 3\$mm{\textasciicircum}2\$. These sensors are low-cost and flexible, providing detailed and extensive coverage of physical contacts, effectively complementing visual information. To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. Through concrete hardware experiments, we demonstrate that even low-cost robots can perform precise manipulations and significantly outperform vision-only policies, particularly in safe interactions with fragile items and executing long-horizon tasks involving in-hand manipulation. Our project page is available at {\textbackslash}url\{https://binghao-huang.github.io/3D-ViTac/\}.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Huang, Binghao and Wang, Yixuan and Yang, Xinyi and Luo, Yiyue and Li, Yunzhu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.24091},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jiang_dexmimicgen_2024,
	title = {{DexMimicGen}: {Automated} {Data} {Generation} for {Bimanual} {Dexterous} {Manipulation} via {Imitation} {Learning}},
	shorttitle = {{DexMimicGen}},
	url = {http://arxiv.org/abs/2410.24185},
	doi = {10.48550/arXiv.2410.24185},
	abstract = {Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Videos and more are at https://dexmimicgen.github.io/},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Jiang, Zhenyu and Xie, Yuqi and Lin, Kevin and Xu, Zhenjia and Wan, Weikang and Mandlekar, Ajay and Fan, Linxi and Zhu, Yuke},
	month = oct,
	year = {2024},
	note = {arXiv:2410.24185},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kareer_egomimic_2024,
	title = {{EgoMimic}: {Scaling} {Imitation} {Learning} via {Egocentric} {Video}},
	shorttitle = {{EgoMimic}},
	url = {http://arxiv.org/abs/2410.24221},
	doi = {10.48550/arXiv.2410.24221},
	abstract = {The scale and diversity of demonstration data required for imitation learning is a significant challenge. We present EgoMimic, a full-stack framework which scales manipulation via human embodiment data, specifically egocentric human videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a system to capture human embodiment data using the ergonomic Project Aria glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap to human data, (3) cross-domain data alignment techniques, and (4) an imitation learning architecture that co-trains on human and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. EgoMimic achieves significant improvement on a diverse set of long-horizon, single-arm and bimanual manipulation tasks over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a favorable scaling trend for EgoMimic, where adding 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. Videos and additional information can be found at https://egomimic.github.io/},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Kareer, Simar and Patel, Dhruv and Punamiya, Ryan and Mathur, Pranay and Cheng, Shuo and Wang, Chen and Hoffman, Judy and Xu, Danfei},
	month = oct,
	year = {2024},
	note = {arXiv:2410.24221},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{agarwal_scenecomplete_2024,
	title = {{SceneComplete}: {Open}-{World} {3D} {Scene} {Completion} in {Complex} {Real} {World} {Environments} for {Robot} {Manipulation}},
	shorttitle = {{SceneComplete}},
	url = {http://arxiv.org/abs/2410.23643},
	doi = {10.48550/arXiv.2410.23643},
	abstract = {Careful robot manipulation in every-day cluttered environments requires an accurate understanding of the 3D scene, in order to grasp and place objects stably and reliably and to avoid mistakenly colliding with other objects. In general, we must construct such a 3D interpretation of a complex scene based on limited input, such as a single RGB-D image. We describe SceneComplete, a system for constructing a complete, segmented, 3D model of a scene from a single view. It provides a novel pipeline for composing general-purpose pretrained perception modules (vision-language, segmentation, image-inpainting, image-to-3D, and pose-estimation) to obtain high-accuracy results. We demonstrate its accuracy and effectiveness with respect to ground-truth models in a large benchmark dataset and show that its accurate whole-object reconstruction enables robust grasp proposal generation, including for a dexterous hand. Project website - https://scenecomplete.github.io/},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Agarwal, Aditya and Singh, Gaurav and Sen, Bipasha and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack},
	month = nov,
	year = {2024},
	note = {arXiv:2410.23643},
	keywords = {Computer Science - Robotics},
}

@misc{wang_neural_2024,
	title = {Neural {Attention} {Field}: {Emerging} {Point} {Relevance} in {3D} {Scenes} for {One}-{Shot} {Dexterous} {Grasping}},
	shorttitle = {Neural {Attention} {Field}},
	url = {http://arxiv.org/abs/2410.23039},
	doi = {10.48550/arXiv.2410.23039},
	abstract = {One-shot transfer of dexterous grasps to novel scenes with object and context variations has been a challenging problem. While distilled feature fields from large vision models have enabled semantic correspondences across 3D scenes, their features are point-based and restricted to object surfaces, limiting their capability of modeling complex semantic feature distributions for hand-object interactions. In this work, we propose the {\textbackslash}textit\{neural attention field\} for representing semantic-aware dense feature fields in the 3D space by modeling inter-point relevance instead of individual point features. Core to it is a transformer decoder that computes the cross-attention between any 3D query point with all the scene points, and provides the query point feature with an attention-based aggregation. We further propose a self-supervised framework for training the transformer decoder from only a few 3D pointclouds without hand demonstrations. Post-training, the attention field can be applied to novel scenes for semantics-aware dexterous grasping from one-shot demonstration. Experiments show that our method provides better optimization landscapes by encouraging the end-effector to focus on task-relevant scene regions, resulting in significant improvements in success rates on real robots compared with the feature-field-based methods.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Wang, Qianxu and Deng, Congyue and Lum, Tyler Ga Wei and Chen, Yuanpei and Yang, Yaodong and Bohg, Jeannette and Zhu, Yixin and Guibas, Leonidas},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23039},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_dexgraspnet_2024,
	title = {{DexGraspNet} 2.0: {Learning} {Generative} {Dexterous} {Grasping} in {Large}-scale {Synthetic} {Cluttered} {Scenes}},
	shorttitle = {{DexGraspNet} 2.0},
	url = {http://arxiv.org/abs/2410.23004},
	doi = {10.48550/arXiv.2410.23004},
	abstract = {Grasping in cluttered scenes remains highly challenging for dexterous hands due to the scarcity of data. To address this problem, we present a large-scale synthetic benchmark, encompassing 1319 objects, 8270 scenes, and 427 million grasps. Beyond benchmarking, we also propose a novel two-stage grasping method that learns efficiently from data by using a diffusion model that conditions on local geometry. Our proposed generative method outperforms all baselines in simulation experiments. Furthermore, with the aid of test-time-depth restoration, our method demonstrates zero-shot sim-to-real transfer, attaining 90.7\% real-world dexterous grasping success rate in cluttered scenes.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Zhang, Jialiang and Liu, Haoran and Li, Danshi and Yu, Xinqiang and Geng, Haoran and Ding, Yufei and Chen, Jiayi and Wang, He},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_multi-task_2024,
	title = {Multi-{Task} {Interactive} {Robot} {Fleet} {Learning} with {Visual} {World} {Models}},
	url = {http://arxiv.org/abs/2410.22689},
	doi = {10.48550/arXiv.2410.22689},
	abstract = {Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a multi-task interactive robot fleet learning framework to address these challenges. Sirius-Fleet monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate Sirius-Fleet's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website: https://ut-austin-rpl.github.io/sirius-fleet},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Liu, Huihan and Zhang, Yu and Betala, Vaarij and Zhang, Evan and Liu, James and Ding, Crystal and Zhu, Yuke},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22689},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{du_efficient_2024,
	title = {An {Efficient} {Representation} of {Whole}-body {Model} {Predictive} {Control} for {Online} {Compliant} {Dual}-arm {Mobile} {Manipulation}},
	url = {http://arxiv.org/abs/2410.22910},
	doi = {10.48550/arXiv.2410.22910},
	abstract = {Dual-arm mobile manipulators can transport and manipulate large-size objects with simple end-effectors. To interact with dynamic environments with strict safety and compliance requirements, achieving whole-body motion planning online while meeting various hard constraints for such highly redundant mobile manipulators poses a significant challenge. We tackle this challenge by presenting an efficient representation of whole-body motion trajectories within our bilevel model-based predictive control (MPC) framework. We utilize B{\textbackslash}'ezier-curve parameterization to represent the optimized collision-free trajectories of two collaborating end-effectors in the first MPC, facilitating fast long-horizon object-oriented motion planning in SE(3) while considering approximated feasibility constraints. This approach is further applied to parameterize whole-body trajectories in the second MPC for whole-body motion generation with predictive admittance control in a relatively short horizon while satisfying whole-body hard constraints. This representation enables two MPCs with continuous properties, thereby avoiding inaccurate model-state transition and dense decision-variable settings in existing MPCs using the discretization method. It strengthens the online execution of the bilevel MPC framework in high-dimensional space and facilitates the generation of consistent commands for our hybrid position/velocity-controlled robot. The simulation comparisons and real-world experiments demonstrate the efficiency and robustness of this approach in various scenarios for static and dynamic obstacle avoidance, and compliant interaction control with the manipulated object and external disturbances.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Du, Wenqian and Long, Ran and Moura, João and Wang, Jiayi and Samadi, Saeid and Vijayakumar, Sethu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22910},
	keywords = {Computer Science - Robotics},
}

@misc{liu_learning_2024,
	title = {Learning {Goal}-oriented {Bimanual} {Dough} {Rolling} {Using} {Dynamic} {Heterogeneous} {Graph} {Based} on {Human} {Demonstration}},
	url = {http://arxiv.org/abs/2410.22355},
	doi = {10.48550/arXiv.2410.22355},
	abstract = {Soft object manipulation poses significant challenges for robots, requiring effective techniques for state representation and manipulation policy learning. State representation involves capturing the dynamic changes in the environment, while manipulation policy learning focuses on establishing the relationship between robot actions and state transformations to achieve specific goals. To address these challenges, this research paper introduces a novel approach: a dynamic heterogeneous graph-based model for learning goal-oriented soft object manipulation policies. The proposed model utilizes graphs as a unified representation for both states and policy learning. By leveraging the dynamic graph, we can extract crucial information regarding object dynamics and manipulation policies. Furthermore, the model facilitates the integration of demonstrations, enabling guided policy learning. To evaluate the efficacy of our approach, we designed a dough rolling task and conducted experiments using both a differentiable simulator and a real-world humanoid robot. Additionally, several ablation studies were performed to analyze the effect of our method, demonstrating its superiority in achieving human-like behavior.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Liu, Junjia and Li, Chenzui and Wang, Shixiong and Dong, Zhipeng and Calinon, Sylvain and Li, Miao and Chen, Fei},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22355},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{guzey_bridging_2024,
	title = {Bridging the {Human} to {Robot} {Dexterity} {Gap} through {Object}-{Oriented} {Rewards}},
	url = {http://arxiv.org/abs/2410.23289},
	doi = {10.48550/arXiv.2410.23289},
	abstract = {Training robots directly from human videos is an emerging area in robotics and computer vision. While there has been notable progress with two-fingered grippers, learning autonomous tasks for multi-fingered robot hands in this way remains challenging. A key reason for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand due to morphology differences. In this work, we present HuDOR, a technique that enables online fine-tuning of policies by directly computing rewards from human videos. Importantly, this reward function is built using object-oriented trajectories derived from off-the-shelf point trackers, providing meaningful learning signals despite the morphology gap and visual differences between human and robot hands. Given a single video of a human solving a task, such as gently opening a music box, HuDOR enables our four-fingered Allegro hand to learn the task with just an hour of online interaction. Our experiments across four tasks show that HuDOR achieves a 4x improvement over baselines. Code and videos are available on our website, https://object-rewards.github.io.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Guzey, Irmak and Dai, Yinlong and Savva, Georgy and Bhirangi, Raunaq and Pinto, Lerrel},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23289},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hong_pf3plat_2024,
	title = {{PF3plat}: {Pose}-{Free} {Feed}-{Forward} {3D} {Gaussian} {Splatting}},
	shorttitle = {{PF3plat}},
	url = {http://arxiv.org/abs/2410.22128},
	doi = {10.48550/arXiv.2410.22128},
	abstract = {We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Hong, Sunghwan and Jung, Jaewoo and Shin, Heeseong and Han, Jisang and Yang, Jiaolong and Luo, Chong and Kim, Seungryong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22128},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_freegaussian_2024,
	title = {{FreeGaussian}: {Guidance}-free {Controllable} {3D} {Gaussian} {Splats} with {Flow} {Derivatives}},
	shorttitle = {{FreeGaussian}},
	url = {http://arxiv.org/abs/2410.22070},
	doi = {10.48550/arXiv.2410.22070},
	abstract = {Reconstructing controllable Gaussian splats from monocular video is a challenging task due to its inherently insufficient constraints. Widely adopted approaches supervise complex interactions with additional masks and control signal annotations, limiting their real-world applications. In this paper, we propose an annotation guidance-free method, dubbed FreeGaussian, that mathematically derives dynamic Gaussian motion from optical flow and camera motion using novel dynamic Gaussian constraints. By establishing a connection between 2D flows and 3D Gaussian dynamic control, our method enables self-supervised optimization and continuity of dynamic Gaussian motions from flow priors. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state with a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Quantitative and qualitative evaluations on extensive experiments demonstrate the state-of-the-art visual performance and control capability of our method. Project page: https://freegaussian.github.io.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Chen, Qizhi and Qu, Delin and Tang, Yiwen and Song, Haoming and Zhang, Yiting and Wang, Dong and Zhao, Bin and Li, Xuelong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22070},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_dofs_2024,
	title = {{DOFS}: {A} {Real}-world {3D} {Deformable} {Object} {Dataset} with {Full} {Spatial} {Information} for {Dynamics} {Model} {Learning}},
	shorttitle = {{DOFS}},
	url = {http://arxiv.org/abs/2410.21758},
	doi = {10.48550/arXiv.2410.21758},
	abstract = {This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane. The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper. In addition, we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object. Our dataset and all CADs of the data collection system will be released soon on our website.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Zhang, Zhen and Chu, Xiangyu and Tang, Yunxi and Au, K. W. Samuel},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21758},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{luo_precise_2024,
	title = {Precise and {Dexterous} {Robotic} {Manipulation} via {Human}-in-the-{Loop} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2410.21845},
	doi = {10.48550/arXiv.2410.21845},
	abstract = {Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. Videos and code are available at our project website https://hil-serl.github.io/.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Luo, Jianlan and Xu, Charles and Wu, Jeffrey and Levine, Sergey},
	month = nov,
	year = {2024},
	note = {arXiv:2410.21845},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{jiang_robots_2024,
	title = {Robots {Pre}-train {Robots}: {Manipulation}-{Centric} {Robotic} {Representation} from {Large}-{Scale} {Robot} {Datasets}},
	shorttitle = {Robots {Pre}-train {Robots}},
	url = {http://arxiv.org/abs/2410.22325},
	doi = {10.48550/arXiv.2410.22325},
	abstract = {The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the "manipulation centricity" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8\%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9\%. Project website: https://robots-pretrain-robots.github.io/.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Jiang, Guangqi and Sun, Yifei and Huang, Tao and Li, Huanyu and Liang, Yongyuan and Xu, Huazhe},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22325},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_blinkvision_2024,
	title = {{BlinkVision}: {A} {Benchmark} for {Optical} {Flow}, {Scene} {Flow} and {Point} {Tracking} {Estimation} using {RGB} {Frames} and {Events}},
	shorttitle = {{BlinkVision}},
	url = {http://arxiv.org/abs/2410.20451},
	doi = {10.48550/arXiv.2410.20451},
	abstract = {Recent advances in event-based vision suggest that these systems complement traditional cameras by providing continuous observation without frame rate limitations and a high dynamic range, making them well-suited for correspondence tasks such as optical flow and point tracking. However, there is still a lack of comprehensive benchmarks for correspondence tasks that include both event data and images. To address this gap, we propose BlinkVision, a large-scale and diverse benchmark with multiple modalities and dense correspondence annotations. BlinkVision offers several valuable features: 1) Rich modalities: It includes both event data and RGB images. 2) Extensive annotations: It provides dense per-pixel annotations covering optical flow, scene flow, and point tracking. 3) Large vocabulary: It contains 410 everyday categories, sharing common classes with popular 2D and 3D datasets like LVIS and ShapeNet. 4) Naturalistic: It delivers photorealistic data and covers various naturalistic factors, such as camera shake and deformation. BlinkVision enables extensive benchmarks on three types of correspondence tasks (optical flow, point tracking, and scene flow estimation) for both image-based and event-based methods, offering new observations, practices, and insights for future research. The benchmark website is https://www.blinkvision.net/.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Li, Yijin and Shen, Yichen and Huang, Zhaoyang and Chen, Shuo and Bian, Weikang and Shi, Xiaoyu and Wang, Fu-Yun and Sun, Keqiang and Bao, Hujun and Cui, Zhaopeng and Zhang, Guofeng and Li, Hongsheng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.20451},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_one-step_2024,
	title = {One-{Step} {Diffusion} {Policy}: {Fast} {Visuomotor} {Policies} via {Diffusion} {Distillation}},
	shorttitle = {One-{Step} {Diffusion} {Policy}},
	url = {http://arxiv.org/abs/2410.21257},
	doi = {10.48550/arXiv.2410.21257},
	abstract = {Diffusion models, praised for their success in generative tasks, are increasingly being applied to robotics, demonstrating exceptional performance in behavior cloning. However, their slow generation process stemming from iterative denoising steps poses a challenge for real-time applications in resource-constrained robotics setups and dynamically changing environments. In this paper, we introduce the One-Step Diffusion Policy (OneDP), a novel approach that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating response times for robotic control tasks. We ensure the distilled generator closely aligns with the original policy distribution by minimizing the Kullback-Leibler (KL) divergence along the diffusion chain, requiring only \$2{\textbackslash}\%\$-\$10{\textbackslash}\%\$ additional pre-training cost for convergence. We evaluated OneDP on 6 challenging simulation tasks as well as 4 self-designed real-world tasks using the Franka robot. The results demonstrate that OneDP not only achieves state-of-the-art success rates but also delivers an order-of-magnitude improvement in inference speed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing its potential for dynamic and computationally constrained robotic applications. We share the project page at https://research.nvidia.com/labs/dir/onedp/.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Wang, Zhendong and Li, Zhaoshuo and Mandlekar, Ajay and Xu, Zhenjia and Fan, Jiaojiao and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Balaji, Yogesh and Zhou, Mingyuan and Liu, Ming-Yu and Zeng, Yu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21257},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{khirodkar_harmony4d_2024,
	title = {{Harmony4D}: {A} {Video} {Dataset} for {In}-{The}-{Wild} {Close} {Human} {Interactions}},
	shorttitle = {{Harmony4D}},
	url = {http://arxiv.org/abs/2410.20294},
	doi = {10.48550/arXiv.2410.20294},
	abstract = {Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing state-of-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8\% PVE in scenes with severe occlusion and contact. Code and data are available at https://jyuntins.github.io/harmony4d/.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Khirodkar, Rawal and Song, Jyun-Ting and Cao, Jinkun and Luo, Zhengyi and Kitani, Kris},
	month = oct,
	year = {2024},
	note = {arXiv:2410.20294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{singh_synthetica_2024,
	title = {Synthetica: {Large} {Scale} {Synthetic} {Data} for {Robot} {Perception}},
	shorttitle = {Synthetica},
	url = {http://arxiv.org/abs/2410.21153},
	doi = {10.48550/arXiv.2410.21153},
	abstract = {Vision-based object detectors are a crucial basis for robotics applications as they provide valuable information about object localisation in the environment. These need to ensure high reliability in different lighting conditions, occlusions, and visual artifacts, all while running in real-time. Collecting and annotating real-world data for these networks is prohibitively time consuming and costly, especially for custom assets, such as industrial objects, making it untenable for generalization to in-the-wild scenarios. To this end, we present Synthetica, a method for large-scale synthetic data generation for training robust state estimators. This paper focuses on the task of object detection, an important problem which can serve as the front-end for most state estimation problems, such as pose estimation. Leveraging data from a photorealistic ray-tracing renderer, we scale up data generation, generating 2.7 million images, to train highly accurate real-time detection transformers. We present a collection of rendering randomization and training-time data augmentation techniques conducive to robust sim-to-real performance for vision tasks. We demonstrate state-of-the-art performance on the task of object detection while having detectors that run at 50-100Hz which is 9 times faster than the prior SOTA. We further demonstrate the usefulness of our training methodology for robotics applications by showcasing a pipeline for use in the real world with custom objects for which there do not exist prior datasets. Our work highlights the importance of scaling synthetic data generation for robust sim-to-real transfer while achieving the fastest real-time inference speeds. Videos and supplementary information can be found at this URL: https://sites.google.com/view/synthetica-vision.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Singh, Ritvik and Liu, Jingzhou and Wyk, Karl Van and Chao, Yu-Wei and Lafleche, Jean-Francois and Shkurti, Florian and Ratliff, Nathan and Handa, Ankur},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21153},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{wada_reorientbot_2022,
	title = {{ReorientBot}: {Learning} {Object} {Reorientation} for {Specific}-{Posed} {Placement}},
	shorttitle = {{ReorientBot}},
	url = {https://ieeexplore.ieee.org/abstract/document/9811881},
	doi = {10.1109/ICRA46639.2022.9811881},
	abstract = {Robots need the capability of placing objects in arbitrary, specific poses to rearrange the world and achieve various valuable tasks. Object reorientation plays a crucial role in this as objects may not initially be oriented such that the robot can grasp and then immediately place them in a specific goal pose. In this work, we present a vision-based manipulation system, ReorientBot, which consists of 1) visual scene understanding with pose estimation and volumetric reconstruction using an onboard RGB-D camera; 2) learned waypoint selection for successful and efficient motion generation for reorientation; 3) traditional motion planning to generate a collision-free trajectory from the selected waypoints. We evaluate our method using the YCB objects in both simulation and the real world, achieving 93\% overall success, 81\% improvement in success rate, and 22\% improvement in execution time compared to a heuristic approach. We demonstrate extended multi-object rearrangement showing the general capability of the system.},
	urldate = {2024-11-28},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wada, Kentaro and James, Stephen and Davison, Andrew J.},
	month = may,
	year = {2022},
	keywords = {Pipelines, Planning, Pose estimation, Robot vision systems, Time factors, Trajectory, Visualization},
	pages = {8252--8258},
}

@misc{he_hover_2024,
	title = {{HOVER}: {Versatile} {Neural} {Whole}-{Body} {Controller} for {Humanoid} {Robots}},
	shorttitle = {{HOVER}},
	url = {http://arxiv.org/abs/2410.21229},
	doi = {10.48550/arXiv.2410.21229},
	abstract = {Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {He, Tairan and Xiao, Wenli and Lin, Toru and Luo, Zhengyi and Xu, Zhenjia and Jiang, Zhenyu and Kautz, Jan and Liu, Changliu and Shi, Guanya and Wang, Xiaolong and Fan, Linxi and Zhu, Yuke},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21229},
	keywords = {Computer Science - Robotics},
}

@misc{biza_-robot_2024,
	title = {On-{Robot} {Reinforcement} {Learning} with {Goal}-{Contrastive} {Rewards}},
	url = {http://arxiv.org/abs/2410.19989},
	doi = {10.48550/arXiv.2410.19989},
	abstract = {Reinforcement Learning (RL) has the potential to enable robots to learn from their own actions in the real world. Unfortunately, RL can be prohibitively expensive, in terms of on-robot runtime, due to inefficient exploration when learning from a sparse reward signal. Designing dense reward functions is labour-intensive and requires domain expertise. In our work, we propose GCR (Goal-Contrastive Rewards), a dense reward function learning method that can be trained on passive video demonstrations. By using videos without actions, our method is easier to scale, as we can use arbitrary videos. GCR combines two loss functions, an implicit value loss function that models how the reward increases when traversing a successful trajectory, and a goal-contrastive loss that discriminates between successful and failed trajectories. We perform experiments in simulated manipulation environments across RoboMimic and MimicGen tasks, as well as in the real world using a Franka arm and a Spot quadruped. We find that GCR leads to a more-sample efficient RL, enabling model-free RL to solve about twice as many tasks as our baseline reward learning methods. We also demonstrate positive cross-embodiment transfer from videos of people and of other robots performing a task. Appendix: {\textbackslash}url\{https://tinyurl.com/gcr-appendix-2\}.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Biza, Ondrej and Weng, Thomas and Sun, Lingfeng and Schmeckpeper, Karl and Kelestemur, Tarik and Ma, Yecheng Jason and Platt, Robert and Meent, Jan-Willem van de and Wong, Lawson L. S.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.19989},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mustafa_visual_2024,
	title = {Visual {Imitation} {Learning} of {Non}-{Prehensile} {Manipulation} {Tasks} with {Dynamics}-{Supervised} {Models}},
	url = {http://arxiv.org/abs/2410.19379},
	doi = {10.48550/arXiv.2410.19379},
	abstract = {Unlike quasi-static robotic manipulation tasks like pick-and-place, dynamic tasks such as non-prehensile manipulation pose greater challenges, especially for vision-based control. Successful control requires the extraction of features relevant to the target task. In visual imitation learning settings, these features can be learnt by backpropagating the policy loss through the vision backbone. Yet, this approach tends to learn task-specific features with limited generalizability. Alternatively, learning world models can realize more generalizable vision backbones. Utilizing the learnt features, task-specific policies are subsequently trained. Commonly, these models are trained solely to predict the next RGB state from the current state and action taken. But only-RGB prediction might not fully-capture the task-relevant dynamics. In this work, we hypothesize that direct supervision of target dynamic states (Dynamics Mapping) can learn better dynamics-informed world models. Beside the next RGB reconstruction, the world model is also trained to directly predict position, velocity, and acceleration of environment rigid bodies. To verify our hypothesis, we designed a non-prehensile 2D environment tailored to two tasks: "Balance-Reaching" and "Bin-Dropping". When trained on the first task, dynamics mapping enhanced the task performance under different training configurations (Decoupled, Joint, End-to-End) and policy architectures (Feedforward, Recurrent). Notably, its most significant impact was for world model pretraining boosting the success rate from 21\% to 85\%. Although frozen dynamics-informed world models could generalize well to a task with in-domain dynamics, but poorly to a one with out-of-domain dynamics.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Mustafa, Abdullah and Hanai, Ryo and Ramirez, Ixchel and Erich, Floris and Nakajo, Ryoichi and Domae, Yukiyasu and Ogata, Tetsuya},
	month = oct,
	year = {2024},
	note = {arXiv:2410.19379},
	keywords = {Computer Science - Robotics},
}

@misc{papagiannis_miles_2024,
	title = {{MILES}: {Making} {Imitation} {Learning} {Easy} with {Self}-{Supervision}},
	shorttitle = {{MILES}},
	url = {http://arxiv.org/abs/2410.19693},
	doi = {10.48550/arXiv.2410.19693},
	abstract = {Data collection in imitation learning often requires significant, laborious human supervision, such as numerous demonstrations, and/or frequent environment resets for methods that incorporate reinforcement learning. In this work, we propose an alternative approach, MILES: a fully autonomous, self-supervised data collection paradigm, and we show that this enables efficient policy learning from just a single demonstration and a single environment reset. MILES autonomously learns a policy for returning to and then following the single demonstration, whilst being self-guided during data collection, eliminating the need for additional human interventions. We evaluated MILES across several real-world tasks, including tasks that require precise contact-rich manipulation such as locking a lock with a key. We found that, under the constraints of a single demonstration and no repeated environment resetting, MILES significantly outperforms state-of-the-art alternatives like imitation learning methods that leverage reinforcement learning. Videos of our experiments and code can be found on our webpage: www.robot-learning.uk/miles.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Papagiannis, Georgios and Johns, Edward},
	month = oct,
	year = {2024},
	note = {arXiv:2410.19693},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{cai_non-rigid_2024,
	title = {Non-rigid {Relative} {Placement} through {3D} {Dense} {Diffusion}},
	url = {http://arxiv.org/abs/2410.19247},
	doi = {10.48550/arXiv.2410.19247},
	abstract = {The task of "relative placement" is to predict the placement of one object in relation to another, e.g. placing a mug onto a mug rack. Through explicit object-centric geometric reasoning, recent methods for relative placement have made tremendous progress towards data-efficient learning for robot manipulation while generalizing to unseen task variations. However, they have yet to represent deformable transformations, despite the ubiquity of non-rigid bodies in real world settings. As a first step towards bridging this gap, we propose ``cross-displacement" - an extension of the principles of relative placement to geometric relationships between deformable objects - and present a novel vision-based method to learn cross-displacement through dense diffusion. To this end, we demonstrate our method's ability to generalize to unseen object instances, out-of-distribution scene configurations, and multimodal goals on multiple highly deformable tasks (both in simulation and in the real world) beyond the scope of prior works. Supplementary information and videos can be found at https://sites.google.com/view/tax3d-corl-2024 .},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Cai, Eric and Donca, Octavian and Eisner, Ben and Held, David},
	month = oct,
	year = {2024},
	note = {arXiv:2410.19247},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{yu_point-bert_2022,
	title = {Point-{BERT}: {Pre}-{Training} {3D} {Point} {Cloud} {Transformers} {With} {Masked} {Point} {Modeling}},
	shorttitle = {Point-{BERT}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-11-27},
	author = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
	year = {2022},
	pages = {19313--19322},
}

@inproceedings{pang_masked_2022,
	address = {Cham},
	title = {Masked {Autoencoders} for {Point} {Cloud} {Self}-supervised {Learning}},
	isbn = {978-3-031-20086-1},
	doi = {10.1007/978-3-031-20086-1_35},
	abstract = {As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud’s properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. The pre-trained models achieve 85.18\% accuracy on ScanObjectNN and 94.04\% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5\%–2.3\% in the few-shot classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud. Codes are available at https://github.com/Pang-Yatian/Point-MAE.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Pang, Yatian and Wang, Wenxiao and Tay, Francis E. H. and Liu, Wei and Tian, Yonghong and Yuan, Li},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {604--621},
}

@article{chen_pointgpt_2023,
	title = {{PointGPT}: {Auto}-regressively {Generative} {Pre}-training from {Point} {Clouds}},
	volume = {36},
	shorttitle = {{PointGPT}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5ed5c3c846f684a54975ad7a2525199f-Abstract-Conference.html},
	language = {en},
	urldate = {2024-11-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Guangyan and Wang, Meiling and Yang, Yi and Yu, Kai and Yuan, Li and Yue, Yufeng},
	month = dec,
	year = {2023},
	pages = {29667--29679},
}

@misc{gyenes_pointpatchrl_2024,
	title = {{PointPatchRL} -- {Masked} {Reconstruction} {Improves} {Reinforcement} {Learning} on {Point} {Clouds}},
	url = {http://arxiv.org/abs/2410.18800},
	doi = {10.48550/arXiv.2410.18800},
	abstract = {Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics. While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects. In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views. However, while deep learning on point clouds has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature. We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers. PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL. We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry. Videos and code are available at https://alrhub.github.io/pprl-website},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Gyenes, Balázs and Franke, Nikolai and Becker, Philipp and Neumann, Gerhard},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18800},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lin_data_2024,
	title = {Data {Scaling} {Laws} in {Imitation} {Learning} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2410.18647},
	doi = {10.48550/arXiv.2410.18647},
	abstract = {Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\% success rates in novel environments with unseen objects.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Lin, Fanqi and Hu, Yingdong and Sheng, Pingyue and Wen, Chuan and You, Jiacheng and Gao, Yang},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18647},
	keywords = {Computer Science - Robotics},
}

@misc{garrett_skillmimicgen_2024,
	title = {{SkillMimicGen}: {Automated} {Demonstration} {Generation} for {Efficient} {Skill} {Learning} and {Deployment}},
	shorttitle = {{SkillMimicGen}},
	url = {http://arxiv.org/abs/2410.18907},
	doi = {10.48550/arXiv.2410.18907},
	abstract = {Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks. To address this issue, we propose SkillMimicGen (SkillGen), an automated system for generating demonstration datasets from a few human demos. SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion. We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time. We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability to produce data for large scene variations, including clutter, and agents that are on average 24\% more successful. We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task. Videos, and more at https://skillgen.github.io.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Garrett, Caelan and Mandlekar, Ajay and Wen, Bowen and Fox, Dieter},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18907},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{zorina_temporally_2024,
	title = {Temporally {Consistent} {Object} {6D} {Pose} {Estimation} for {Robot} {Control}},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/10758211?casa_token=9AqgrC9R4ngAAAAA:9pUVD71DZDxaDPeTQEx9B_Z6jq1IYBWJgQ-Owp7vr5LIW20pbFZz6a4K2x8ALKCRaxhNdCxvOLo},
	doi = {10.1109/LRA.2024.3502052},
	abstract = {Single-view RGB object pose estimators have reached a level of precision and efficiency that makes them good candidates for vision-based robot control. However, off-the-shelf methods lack temporal consistency and robustness that are mandatory for a stable feedback control. In this work, we develop a factor graph approach to enforce temporal consistency of the object pose estimates. In particular, the proposed approach: (i) incorporates object motion models, (ii) explicitly estimates the object pose measurement uncertainty, and (iii) integrates the above two components in an online optimization-based estimator. We demonstrate that with appropriate outlier rejection and smoothing using the proposed factor graph approach, we can significantly improve the results on standardized pose estimation benchmarks. We experimentally validate the stability of the proposed approach for a feedback-based robot control task in which the object is tracked by the camera attached to a torque controlled manipulator.},
	urldate = {2024-11-27},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zorina, Kateryna and Priban, Vojtech and Fourmy, Mederic and Sivic, Josef and Petrik, Vladimir},
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Computational modeling, Computer vision for automation, Pose estimation, Predictive models, Probabilistic logic, Robot control, Robot vision systems, Robots, Smoothing methods, Uncertainty, visual tracking},
	pages = {1--8},
}

@inproceedings{chen_object-centric_2024-1,
	title = {Object-{Centric} {Dexterous} {Manipulation} from {Human} {Motion} {Data}},
	url = {https://openreview.net/forum?id=KAzku0Uyh1},
	abstract = {Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.},
	language = {en},
	urldate = {2024-11-24},
	author = {Chen, Yuanpei and Wang, Chen and Yang, Yaodong and Liu, Karen},
	month = sep,
	year = {2024},
}

@misc{hsu_spot_2024,
	title = {{SPOT}: {SE}(3) {Pose} {Trajectory} {Diffusion} for {Object}-{Centric} {Manipulation}},
	shorttitle = {{SPOT}},
	url = {http://arxiv.org/abs/2411.00965},
	doi = {10.48550/arXiv.2411.00965},
	abstract = {We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the SE(3) object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as cross-embodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually crafted rules. To guide the robot in executing the task, the object trajectory is used to condition a diffusion policy. We show improvement compared to prior work on RLBench simulated tasks. In real-world evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/object\_centric\_diffusion},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Hsu, Cheng-Chun and Wen, Bowen and Xu, Jie and Narang, Yashraj and Wang, Xiaolong and Zhu, Yuke and Biswas, Joydeep and Birchfield, Stan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00965},
	keywords = {Computer Science - Robotics},
}

@article{zheng_differentiable_2024,
	title = {Differentiable {Cloth} {Parameter} {Identification} and {State} {Estimation} in {Manipulation}},
	volume = {9},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/10411033},
	doi = {10.1109/LRA.2024.3357039},
	abstract = {In the realm of robotic cloth manipulation, accurately estimating the cloth state during or post-execution is imperative. However, the inherent complexities in a cloth's dynamic behavior and its near-infinite degrees of freedom (DoF) pose significant challenges. Traditional methods have been restricted to using keypoints or boundaries as cues for cloth state, which do not holistically capture the cloth's structure, especially during intricate tasks like folding. Additionally, the critical influence of cloth physics has often been overlooked in past research. Addressing these concerns, we introduce DiffCP, a novel differentiable pipeline that leverages the Anisotropic Elasto-Plastic (A-EP) constitutive model, tailored for differentiable computation and robotic tasks. DiffCP adopts a “real-to-sim-to-real” methodology. By observing real-world cloth states through an RGB-D camera and projecting this data into a differentiable simulator, the system identifies physics parameters by minimizing the geometric variance between observed and target states. Extensive experiments demonstrate DiffCP's ability and stability to determine physics parameters under varying manipulations, grasping points, and speeds. Additionally, its applications extend to cloth material identification, manipulation trajectory generation, and more notably, enhancing cloth pose estimation accuracy.},
	number = {3},
	urldate = {2024-11-20},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zheng, Dongzhe and Yao, Siqiong and Xu, Wenqiang and Lu, Cewu},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Clothing, Parameter estimation, Physics, Pipelines, Pose estimation, Simulation and animation, Task analysis, Trajectory, calibration and identification, perception for grasping and manipulation},
	pages = {2519--2526},
}

@article{li_diffcloth_2022,
	title = {{DiffCloth}: {Differentiable} {Cloth} {Simulation} with {Dry} {Frictional} {Contact}},
	volume = {42},
	issn = {0730-0301},
	shorttitle = {{DiffCloth}},
	url = {https://dl.acm.org/doi/10.1145/3527660},
	doi = {10.1145/3527660},
	abstract = {Cloth simulation has wide applications in computer animation, garment design, and robot-assisted dressing. This work presents a differentiable cloth simulator whose additional gradient information facilitates cloth-related applications. Our differentiable simulator extends a state-of-the-art cloth simulator based on Projective Dynamics (PD) and with dry frictional contact\&nbsp;[Ly et\&nbsp;al. 2020]. We draw inspiration from previous work\&nbsp;[Du et\&nbsp;al. 2021] to propose a fast and novel method for deriving gradients in PD-based cloth simulation with dry frictional contact. Furthermore, we conduct a comprehensive analysis and evaluation of the usefulness of gradients in contact-rich cloth simulation. Finally, we demonstrate the efficacy of our simulator in a number of downstream applications, including system identification, trajectory optimization for assisted dressing, closed-loop control, inverse design, and real-to-sim transfer. We observe a substantial speedup obtained from using our gradient information in solving most of these applications.},
	number = {1},
	urldate = {2024-11-20},
	journal = {ACM Trans. Graph.},
	author = {Li, Yifei and Du, Tao and Wu, Kui and Xu, Jie and Matusik, Wojciech},
	month = oct,
	year = {2022},
	pages = {2:1--2:20},
}

@misc{wang_gendp_2024,
	title = {{GenDP}: {3D} {Semantic} {Fields} for {Category}-{Level} {Generalizable} {Diffusion} {Policy}},
	shorttitle = {{GenDP}},
	url = {http://arxiv.org/abs/2410.17488},
	doi = {10.48550/arXiv.2410.17488},
	abstract = {Diffusion-based policies have shown remarkable capability in executing complex robotic manipulation tasks but lack explicit characterization of geometry and semantics, which often limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of Diffusion Policy, we introduce a novel framework that incorporates explicit spatial and semantic information via 3D semantic fields. We generate 3D descriptor fields from multi-view RGBD observations with large foundational vision models, then compare these descriptor fields against reference descriptors to obtain semantic fields. The proposed method explicitly considers geometry and semantics, enabling strong generalization capabilities in tasks requiring category-level generalization, resolving geometric ambiguities, and attention to subtle geometric details. We evaluate our method across eight tasks involving articulated objects and instances with varying shapes and textures from multiple object categories. Our method demonstrates its effectiveness by increasing Diffusion Policy's average success rate on unseen instances from 20\% to 93\%. Additionally, we provide a detailed analysis and visualization to interpret the sources of performance gain and explain how our method can generalize to novel instances.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wang, Yixuan and Yin, Guang and Huang, Binghao and Kelestemur, Tarik and Wang, Jiuguang and Li, Yunzhu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17488},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_x-mobility_2024,
	title = {X-{MOBILITY}: {End}-{To}-{End} {Generalizable} {Navigation} via {World} {Modeling}},
	shorttitle = {X-{MOBILITY}},
	url = {http://arxiv.org/abs/2410.17491},
	doi = {10.48550/arXiv.2410.17491},
	abstract = {General-purpose navigation in challenging environments remains a significant problem in robotics, with current state-of-the-art approaches facing myriad limitations. Classical approaches struggle with cluttered settings and require extensive tuning, while learning-based methods face difficulties generalizing to out-of-distribution environments. This paper introduces X-Mobility, an end-to-end generalizable navigation model that overcomes existing challenges by leveraging three key ideas. First, X-Mobility employs an auto-regressive world modeling architecture with a latent state space to capture world dynamics. Second, a diverse set of multi-head decoders enables the model to learn a rich state representation that correlates strongly with effective navigation skills. Third, by decoupling world modeling from action policy, our architecture can train effectively on a variety of data sources, both with and without expert policies: off-policy data allows the model to learn world dynamics, while on-policy data with supervisory control enables optimal action policy learning. Through extensive experiments, we demonstrate that X-Mobility not only generalizes effectively but also surpasses current state-of-the-art navigation approaches. Additionally, X-Mobility also achieves zero-shot Sim2Real transferability and shows strong potential for cross-embodiment generalization.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Wei and Zhao, Huihua and Li, Chenran and Biswas, Joydeep and Okal, Billy and Goyal, Pulkit and Chang, Yan and Pouya, Soha},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17491},
	keywords = {Computer Science - Robotics},
}

@misc{zhou_spire_2024,
	title = {{SPIRE}: {Synergistic} {Planning}, {Imitation}, and {Reinforcement} {Learning} for {Long}-{Horizon} {Manipulation}},
	shorttitle = {{SPIRE}},
	url = {http://arxiv.org/abs/2410.18065},
	doi = {10.48550/arXiv.2410.18065},
	abstract = {Robot learning has proven to be a general and effective technique for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations. Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch. And for both techniques, the learning difficulty increases proportional to the length of the manipulation task. Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35\% to 50\% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Zhou, Zihan and Garg, Animesh and Fox, Dieter and Garrett, Caelan and Mandlekar, Ajay},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18065},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{blank_scaling_2024,
	title = {Scaling {Robot} {Policy} {Learning} via {Zero}-{Shot} {Labeling} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2410.17772},
	doi = {10.48550/arXiv.2410.17772},
	abstract = {A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Blank, Nils and Reuss, Moritz and Rühle, Marcel and Yağmurlu, Ömer Erdinç and Wenzel, Fabian and Mees, Oier and Lioutikov, Rudolf},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17772},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_caging_2024,
	title = {Caging in {Time}: {A} {Framework} for {Robust} {Object} {Manipulation} under {Uncertainties} and {Limited} {Robot} {Perception}},
	shorttitle = {Caging in {Time}},
	url = {http://arxiv.org/abs/2410.16481},
	doi = {10.48550/arXiv.2410.16481},
	abstract = {Real-world object manipulation has been commonly challenged by physical uncertainties and perception limitations. Being an effective strategy, while caging configuration-based manipulation frameworks have successfully provided robust solutions, they are not broadly applicable due to their strict requirements on the availability of multiple robots, widely distributed contacts, or specific geometries of the robots or the objects. To this end, this work proposes a novel concept, termed Caging in Time, to allow caging configurations to be formed even if there is just one robot engaged in a task. This novel concept can be explained by an insight that even if a caging configuration is needed to constrain the motion of an object, only a small portion of the cage is actively manipulating at a time. As such, we can switch the configuration of the robot strategically so that by collapsing its configuration in time, we will see a cage formed and its necessary portion active whenever needed. We instantiate our Caging in Time theory on challenging quasistatic and dynamic manipulation tasks, showing that Caging in Time can be achieved in general state spaces including geometry-based and energy-based spaces. With extensive experiments, we show robust and accurate manipulation, in an open-loop manner, without requiring detailed knowledge of the object geometry or physical properties, nor realtime accurate feedback on the manipulation states. In addition to being an effective and robust open-loop manipulation solution, the proposed theory can be a supplementary strategy to other manipulation systems affected by uncertain or limited robot perception.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wang, Gaotian and Ren, Kejia and Morgan, Andrew S. and Hang, Kaiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2410.16481},
	keywords = {Computer Science - Robotics},
}

@misc{huang_implicit_2024,
	title = {Implicit {Contact} {Diffuser}: {Sequential} {Contact} {Reasoning} with {Latent} {Point} {Cloud} {Diffusion}},
	shorttitle = {Implicit {Contact} {Diffuser}},
	url = {http://arxiv.org/abs/2410.16571},
	doi = {10.48550/arXiv.2410.16571},
	abstract = {Long-horizon contact-rich manipulation has long been a challenging problem, as it requires reasoning over both discrete contact modes and continuous object motion. We introduce Implicit Contact Diffuser (ICD), a diffusion-based model that generates a sequence of neural descriptors that specify a series of contact relationships between the object and the environment. This sequence is then used as guidance for an MPC method to accomplish a given task. The key advantage of this approach is that the latent descriptors provide more task-relevant guidance to MPC, helping to avoid local minima for contact-rich manipulation tasks. Our experiments demonstrate that ICD outperforms baselines on complex, long-horizon, contact-rich manipulation tasks, such as cable routing and notebook folding. Additionally, our experiments also indicate that {\textbackslash}methodshort can generalize a target contact relationship to a different environment. More visualizations can be found on our website \${\textbackslash}href\{https://implicit-contact-diffuser.github.io/\}\{https://implicit-contact-diffuser.github.io\}\$},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Huang, Zixuan and He, Yinong and Lin, Yating and Berenson, Dmitry},
	month = oct,
	year = {2024},
	note = {arXiv:2410.16571},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_singapo_2024,
	title = {{SINGAPO}: {Single} {Image} {Controlled} {Generation} of {Articulated} {Parts} in {Objects}},
	shorttitle = {{SINGAPO}},
	url = {http://arxiv.org/abs/2410.16499},
	doi = {10.48550/arXiv.2410.16499},
	abstract = {We address the challenge of creating 3D assets for household articulated objects from a single image. Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process. These limitations hinder the scalability and practicality for articulated object modeling. In this work, we propose a method to generate articulated objects from a single image. Observing the object in resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image. To capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics. To tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies. Our experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Jiayi and Iliash, Denys and Chang, Angel X. and Savva, Manolis and Mahdavi-Amiri, Ali},
	month = oct,
	year = {2024},
	note = {arXiv:2410.16499},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tremblay_diff-dope_2023,
	title = {Diff-{DOPE}: {Differentiable} {Deep} {Object} {Pose} {Estimation}},
	shorttitle = {Diff-{DOPE}},
	url = {http://arxiv.org/abs/2310.00463},
	doi = {10.48550/arXiv.2310.00463},
	abstract = {We introduce Diff-DOPE, a 6-DoF pose refiner that takes as input an image, a 3D textured model of an object, and an initial pose of the object. The method uses differentiable rendering to update the object pose to minimize the visual error between the image and the projection of the model. We show that this simple, yet effective, idea is able to achieve state-of-the-art results on pose estimation datasets. Our approach is a departure from recent methods in which the pose refiner is a deep neural network trained on a large synthetic dataset to map inputs to refinement steps. Rather, our use of differentiable rendering allows us to avoid training altogether. Our approach performs multiple gradient descent optimizations in parallel with different random learning rates to avoid local minima from symmetric objects, similar appearances, or wrong step size. Various modalities can be used, e.g., RGB, depth, intensity edges, and object segmentation masks. We present experiments examining the effect of various choices, showing that the best results are found when the RGB image is accompanied by an object mask and depth image to guide the optimization process.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Tremblay, Jonathan and Wen, Bowen and Blukis, Valts and Sundaralingam, Balakumar and Tyree, Stephen and Birchfield, Stan},
	month = sep,
	year = {2023},
	note = {arXiv:2310.00463},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_learning_2023,
	title = {Learning {Tool} {Morphology} for {Contact}-{Rich} {Manipulation} {Tasks} with {Differentiable} {Simulation}},
	url = {http://arxiv.org/abs/2211.02201},
	doi = {10.48550/arXiv.2211.02201},
	abstract = {When humans perform contact-rich manipulation tasks, customized tools are often necessary to simplify the task. For instance, we use various utensils for handling food, such as knives, forks and spoons. Similarly, robots may benefit from specialized tools that enable them to more easily complete a variety of tasks. We present an end-to-end framework to automatically learn tool morphology for contact-rich manipulation tasks by leveraging differentiable physics simulators. Previous work relied on manually constructed priors requiring detailed specification of a 3D object model, grasp pose and task description to facilitate the search or optimization process. Our approach only requires defining the objective with respect to task performance and enables learning a robust morphology through randomizing variations of the task. We make this optimization tractable by casting it as a continual learning problem. We demonstrate the effectiveness of our method for designing new tools in several scenarios, such as winding ropes, flipping a box and pushing peas onto a scoop in simulation. Additionally, experiments with real robots show that the tool shapes discovered by our method help them succeed in these scenarios.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Li, Mengxi and Antonova, Rika and Sadigh, Dorsa and Bohg, Jeannette},
	month = feb,
	year = {2023},
	note = {arXiv:2211.02201},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_end--end_2021,
	title = {An {End}-to-{End} {Differentiable} {Framework} for {Contact}-{Aware} {Robot} {Design}},
	url = {http://arxiv.org/abs/2107.07501},
	doi = {10.48550/arXiv.2107.07501},
	abstract = {The current dominant paradigm for robotic manipulation involves two separate stages: manipulator design and control. Because the robot's morphology and how it can be controlled are intimately linked, joint optimization of design and control can significantly improve performance. Existing methods for co-optimization are limited and fail to explore a rich space of designs. The primary reason is the trade-off between the complexity of designs that is necessary for contact-rich tasks against the practical constraints of manufacturing, optimization, contact handling, etc. We overcome several of these challenges by building an end-to-end differentiable framework for contact-aware robot design. The two key components of this framework are: a novel deformation-based parameterization that allows for the design of articulated rigid robots with arbitrary, complex geometry, and a differentiable rigid body simulator that can handle contact-rich scenarios and computes analytical gradients for a full spectrum of kinematic and dynamic parameters. On multiple manipulation tasks, our framework outperforms existing methods that either only optimize for control or for design using alternate representations or co-optimize using gradient-free methods.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Xu, Jie and Chen, Tao and Zlokapa, Lara and Foshey, Michael and Matusik, Wojciech and Sueda, Shinjiro and Agrawal, Pulkit},
	month = aug,
	year = {2021},
	note = {arXiv:2107.07501},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Robotics},
}

@inproceedings{xian_fluidlab_2022,
	title = {{FluidLab}: {A} {Differentiable} {Environment} for {Benchmarking} {Complex} {Fluid} {Manipulation}},
	shorttitle = {{FluidLab}},
	url = {https://openreview.net/forum?id=Cp-io_BoFaE},
	abstract = {Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. Previous research in robotic fluid manipulation mostly consider fluids governed by an ideal, Newtonian model in simple task settings (e.g., pouring water into a container). However, the vast majority of real-world fluid systems manifest their complexities in terms of the fluid’s complex material behaviors (e.g., elastoplastic deformation) and multi-component interactions (e.g. coffee and frothed milk when making latte art), both of which were well beyond the scope of the current literature. To evaluate robot learning algorithms on understanding and interacting with such complex fluid systems, a comprehensive virtual platform with versatile simulation capabilities and well-established tasks is needed. In this work, we introduce FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids. At the heart of our platform is a fully differentiable physics simulator, FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings, extending the scope of the existing differentiable simulation engines. We identify several challenges for fluid manipulation learning by evaluating a set of reinforcement learning and trajectory optimization methods on our platform. To address these challenges, we propose several domain-specific optimization schemes coupled with differentiable physics, which are empirically shown to be effective in tackling optimization problems featured by fluid system’s non-convex and non-smooth properties. Furthermore, we demonstrate reasonable sim-to-real transfer by deploying optimized trajectories in real-world settings. FluidLab is publicly available at: https://fluidlab2023.github.io.},
	language = {en},
	urldate = {2024-11-20},
	author = {Xian, Zhou and Zhu, Bo and Xu, Zhenjia and Tung, Hsiao-Yu and Torralba, Antonio and Fragkiadaki, Katerina and Gan, Chuang},
	month = sep,
	year = {2022},
}

@article{newbury_review_2024,
	title = {A {Review} of {Differentiable} {Simulators}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10589638},
	doi = {10.1109/ACCESS.2024.3425448},
	abstract = {Differentiable simulators continue to push the state of the art across a range of domains including computational physics, robotics, and machine learning. Their main value is the ability to compute gradients of physical processes, which allows differentiable simulators to be readily integrated into commonly employed gradient-based optimization schemes. To achieve this, a number of design decisions need to be considered representing trade-offs in versatility, computational speed, and accuracy of the gradients obtained. This paper presents an in-depth review of the evolving landscape of differentiable physics simulators. We introduce the foundations and core components of differentiable simulators alongside common design choices. This is followed by a practical guide and overview of open-source differentiable simulators that have been used across past research. Finally, we review and contextualize prominent applications of differentiable simulation. By offering a comprehensive review of the current state-of-the-art in differentiable simulation, this work aims to serve as a resource for researchers and practitioners looking to understand and integrate differentiable physics within their research. We conclude by highlighting current limitations as well as providing insights into future directions for the field.},
	urldate = {2024-11-20},
	journal = {IEEE Access},
	author = {Newbury, Rhys and Collins, Jack and He, Kerry and Pan, Jiahe and Posner, Ingmar and Howard, David and Cosgun, Akansel},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Computational modeling, Data visualization, Differentiable simulator, Differential equations, Machine learning, Numerical models, Optimization methods, Physics, Reviews, System identification, Trajectory planning, differentiable physics, morphology optimization, policy optimization, review, robotics, soft body simulation, system identification, trajectory optimization},
	pages = {97581--97604},
}

@misc{lee_diff-dagger_2024,
	title = {Diff-{DAgger}: {Uncertainty} {Estimation} with {Diffusion} {Policy} for {Robotic} {Manipulation}},
	shorttitle = {Diff-{DAgger}},
	url = {http://arxiv.org/abs/2410.14868},
	doi = {10.48550/arXiv.2410.14868},
	abstract = {Recently, diffusion policy has shown impressive results in handling multi-modal tasks in robotic manipulation. However, it has fundamental limitations in out-of-distribution failures that persist due to compounding errors and its limited capability to extrapolate. One way to address these limitations is robot-gated DAgger, an interactive imitation learning with a robot query system to actively seek expert help during policy rollout. While robot-gated DAgger has high potential for learning at scale, existing methods like Ensemble-DAgger struggle with highly expressive policies: They often misinterpret policy disagreements as uncertainty at multi-modal decision points. To address this problem, we introduce Diff-DAgger, an efficient robot-gated DAgger algorithm that leverages the training objective of diffusion policy. We evaluate Diff-DAgger across different robot tasks including stacking, pushing, and plugging, and show that Diff-DAgger improves the task failure prediction by 37\%, the task completion rate by 14\%, and reduces the wall-clock time by up to 540\%. We hope that this work opens up a path for efficiently incorporating expressive yet data-hungry policies into interactive robot learning settings. Project website: diffdagger.github.io},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Lee, Sung-Wook and Kuo, Yen-Ling},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14868},
	keywords = {Computer Science - Robotics},
}

@misc{heeg_learning_2024,
	title = {Learning {Quadrotor} {Control} {From} {Visual} {Features} {Using} {Differentiable} {Simulation}},
	url = {http://arxiv.org/abs/2410.15979},
	doi = {10.48550/arXiv.2410.15979},
	abstract = {The sample inefficiency of reinforcement learning (RL) remains a significant challenge in robotics. RL requires large-scale simulation and, still, can cause long training times, slowing down research and innovation. This issue is particularly pronounced in vision-based control tasks where reliable state estimates are not accessible. Differentiable simulation offers an alternative by enabling gradient back-propagation through the dynamics model, providing low-variance analytical policy gradients and, hence, higher sample efficiency. However, its usage for real-world robotic tasks has yet been limited. This work demonstrates the great potential of differentiable simulation for learning quadrotor control. We show that training in differentiable simulation significantly outperforms model-free RL in terms of both sample efficiency and training time, allowing a policy to learn to recover a quadrotor in seconds when providing vehicle state and in minutes when relying solely on visual features. The key to our success is two-fold. First, the use of a simple surrogate model for gradient computation greatly accelerates training without sacrificing control performance. Second, combining state representation learning with policy learning enhances convergence speed in tasks where only visual features are observable. These findings highlight the potential of differentiable simulation for real-world robotics and offer a compelling alternative to conventional RL approaches.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Heeg, Johannes and Song, Yunlong and Scaramuzza, Davide},
	month = oct,
	year = {2024},
	note = {arXiv:2410.15979},
	keywords = {Computer Science - Robotics},
}

@misc{xia_cage_2024,
	title = {{CAGE}: {Causal} {Attention} {Enables} {Data}-{Efficient} {Generalizable} {Robotic} {Manipulation}},
	shorttitle = {{CAGE}},
	url = {http://arxiv.org/abs/2410.14974},
	doi = {10.48550/arXiv.2410.14974},
	abstract = {Generalization in robotic manipulation remains a critical challenge, particularly when scaling to new environments with limited demonstrations. This paper introduces CAGE, a novel robotic manipulation policy designed to overcome these generalization barriers by integrating a causal attention mechanism. CAGE utilizes the powerful feature extraction capabilities of the vision foundation model DINOv2, combined with LoRA fine-tuning for robust environment understanding. The policy further employs a causal Perceiver for effective token compression and a diffusion-based action prediction head with attention mechanisms to enhance task-specific fine-grained conditioning. With as few as 50 demonstrations from a single training environment, CAGE achieves robust generalization across diverse visual changes in objects, backgrounds, and viewpoints. Extensive experiments validate that CAGE significantly outperforms existing state-of-the-art RGB/RGB-D approaches in various manipulation tasks, especially under large distribution shifts. In similar environments, CAGE offers an average of 42\% increase in task completion rate. While all baselines fail to execute the task in unseen environments, CAGE manages to obtain a 43\% completion rate and a 51\% success rate in average, making a huge step towards practical deployment of robots in real-world settings. Project website: cage-policy.github.io.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Xia, Shangning and Fang, Hongjie and Fang, Hao-Shu and Lu, Cewu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14974},
	keywords = {Computer Science - Robotics},
}

@misc{li_triplane_2024,
	title = {Triplane {Grasping}: {Efficient} 6-{DoF} {Grasping} with {Single} {RGB} {Images}},
	shorttitle = {Triplane {Grasping}},
	url = {http://arxiv.org/abs/2410.15879},
	doi = {10.48550/arXiv.2410.15879},
	abstract = {Reliable object grasping is one of the fundamental tasks in robotics. However, determining grasping pose based on single-image input has long been a challenge due to limited visual information and the complexity of real-world objects. In this paper, we propose Triplane Grasping, a fast grasping decision-making method that relies solely on a single RGB-only image as input. Triplane Grasping creates a hybrid Triplane-Gaussian 3D representation through a point decoder and a triplane decoder, which produce an efficient and high-quality reconstruction of the object to be grasped to meet real-time grasping requirements. We propose to use an end-to-end network to generate 6-DoF parallel-jaw grasp distributions directly from 3D points in the point cloud as potential grasp contacts and anchor the grasp pose in the observed data. Experiments demonstrate that our method achieves rapid modeling and grasping pose decision-making for daily objects, and exhibits a high grasping success rate in zero-shot scenarios.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Li, Yiming and Ren, Hanchi and Deng, Jingjing and Xie, Xianghua},
	month = oct,
	year = {2024},
	note = {arXiv:2410.15879},
	keywords = {Computer Science - Robotics},
}

@misc{zook_grs_2024,
	title = {{GRS}: {Generating} {Robotic} {Simulation} {Tasks} from {Real}-{World} {Images}},
	shorttitle = {{GRS}},
	url = {http://arxiv.org/abs/2410.15536},
	doi = {10.48550/arXiv.2410.15536},
	abstract = {We introduce GRS (Generating Robotic Simulation tasks), a novel system to address the challenge of real-to-sim in robotics, computer vision, and AR/VR. GRS enables the creation of digital twin simulations from single real-world RGB-D observations, complete with diverse, solvable tasks for virtual agent training. We use state-of-the-art vision-language models (VLMs) to achieve a comprehensive real-to-sim pipeline. GRS operates in three stages: 1) scene comprehension using SAM2 for object segmentation and VLMs for object description, 2) matching identified objects with simulation-ready assets, and 3) generating contextually appropriate robotic tasks. Our approach ensures simulations align with task specifications by generating test suites designed to verify adherence to the task specification. We introduce a router that iteratively refines the simulation and test code to ensure the simulation is solvable by a robot policy while remaining aligned to the task specification. Our experiments demonstrate the system's efficacy in accurately identifying object correspondence, which allows us to generate task environments that closely match input environments, and enhance automated simulation task generation through our novel router mechanism.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Zook, Alex and Sun, Fan-Yun and Spjut, Josef and Blukis, Valts and Birchfield, Stan and Tremblay, Jonathan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.15536},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{sheng_msgfield_2024,
	title = {{MSGField}: {A} {Unified} {Scene} {Representation} {Integrating} {Motion}, {Semantics}, and {Geometry} for {Robotic} {Manipulation}},
	shorttitle = {{MSGField}},
	url = {http://arxiv.org/abs/2410.15730},
	doi = {10.48550/arXiv.2410.15730},
	abstract = {Combining accurate geometry with rich semantics has been proven to be highly effective for language-guided robotic manipulation. Existing methods for dynamic scenes either fail to update in real-time or rely on additional depth sensors for simple scene editing, limiting their applicability in real-world. In this paper, we introduce MSGField, a representation that uses a collection of 2D Gaussians for high-quality reconstruction, further enhanced with attributes to encode semantic and motion information. Specially, we represent the motion field compactly by decomposing each primitive's motion into a combination of a limited set of motion bases. Leveraging the differentiable real-time rendering of Gaussian splatting, we can quickly optimize object motion, even for complex non-rigid motions, with image supervision from only two camera views. Additionally, we designed a pipeline that utilizes object priors to efficiently obtain well-defined semantics. In our challenging dataset, which includes flexible and extremely small objects, our method achieve a success rate of 79.2\% in static and 63.3\% in dynamic environments for language-guided manipulation. For specified object grasping, we achieve a success rate of 90\%, on par with point cloud-based methods. Code and dataset will be released at:https://shengyu724.github.io/MSGField.github.io.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Sheng, Yu and Lin, Runfeng and Wang, Lidian and Qiu, Quecheng and Zhang, YanYong and Zhang, Yu and Hua, Bei and Ji, Jianmin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.15730},
	keywords = {Computer Science - Robotics},
}

@misc{pan_vision-language-action_2024,
	title = {Vision-{Language}-{Action} {Model} and {Diffusion} {Policy} {Switching} {Enables} {Dexterous} {Control} of an {Anthropomorphic} {Hand}},
	url = {http://arxiv.org/abs/2410.14022},
	doi = {10.48550/arXiv.2410.14022},
	abstract = {To advance autonomous dexterous manipulation, we propose a hybrid control method that combines the relative advantages of a fine-tuned Vision-Language-Action (VLA) model and diffusion models. The VLA model provides language commanded high-level planning, which is highly generalizable, while the diffusion model handles low-level interactions which offers the precision and robustness required for specific objects and environments. By incorporating a switching signal into the training-data, we enable event based transitions between these two models for a pick-and-place task where the target object and placement location is commanded through language. This approach is deployed on our anthropomorphic ADAPT Hand 2, a 13DoF robotic hand, which incorporates compliance through series elastic actuation allowing for resilience for any interactions: showing the first use of a multi-fingered hand controlled with a VLA model. We demonstrate this model switching approach results in a over 80{\textbackslash}\% success rate compared to under 40{\textbackslash}\% when only using a VLA model, enabled by accurate near-object arm motion by the VLA model and a multi-modal grasping motion with error recovery abilities from the diffusion model.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Pan, Cheng and Junge, Kai and Hughes, Josie},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{ta_multi-modal_2024,
	title = {Multi-modal {Pose} {Diffuser}: {A} {Multimodal} {Generative} {Conditional} {Pose} {Prior}},
	shorttitle = {Multi-modal {Pose} {Diffuser}},
	url = {http://arxiv.org/abs/2410.14540},
	doi = {10.48550/arXiv.2410.14540},
	abstract = {The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human pose estimation, providing a streamlined yet effective representation of the human body. However, ensuring the validity of SMPL configurations during tasks such as human mesh regression remains a significant challenge , highlighting the necessity for a robust human pose prior capable of discerning realistic human poses. To address this, we introduce MOPED: {\textbackslash}underline\{M\}ulti-m{\textbackslash}underline\{O\}dal {\textbackslash}underline\{P\}os{\textbackslash}underline\{E\} {\textbackslash}underline\{D\}iffuser. MOPED is the first method to leverage a novel multi-modal conditional diffusion model as a prior for SMPL pose parameters. Our method offers powerful unconditional pose generation with the ability to condition on multi-modal inputs such as images and text. This capability enhances the applicability of our approach by incorporating additional context often overlooked in traditional pose priors. Extensive experiments across three distinct tasks-pose estimation, pose denoising, and pose completion-demonstrate that our multi-modal diffusion model-based prior significantly outperforms existing methods. These results indicate that our model captures a broader spectrum of plausible human poses.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Ta, Calvin-Khang and Dutta, Arindam and Kundu, Rohit and Lal, Rohit and Cruz, Hannah Dela and Raychaudhuri, Dripta S. and Roy-Chowdhury, Amit},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14540},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_whisker-inspired_2024,
	title = {Whisker-{Inspired} {Tactile} {Sensing}: {A} {Sim2Real} {Approach} for {Precise} {Underwater} {Contact} {Tracking}},
	shorttitle = {Whisker-{Inspired} {Tactile} {Sensing}},
	url = {http://arxiv.org/abs/2410.14005},
	doi = {10.48550/arXiv.2410.14005},
	abstract = {Aquatic mammals, such as pinnipeds, utilize their whiskers to detect and discriminate objects and analyze water movements, inspiring the development of robotic whiskers for sensing contacts, surfaces, and water flows. We present the design and application of underwater whisker sensors based on Fiber Bragg Grating (FBG) technology. These passive whiskers are mounted along the robot\$'\$s exterior to sense its surroundings through light, non-intrusive contacts. For contact tracking, we employ a sim-to-real learning framework, which involves extensive data collection in simulation followed by a sim-to-real calibration process to transfer the model trained in simulation to the real world. Experiments with whiskers immersed in water indicate that our approach can track contact points with an accuracy of \${\textless}2\$ mm, without requiring precise robot proprioception. We demonstrate that the approach also generalizes to unseen objects.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Li, Hao and Xing, Chengyi and Khan, Saad and Zhong, Miaoya and Cutkosky, Mark R.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14005},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{oliveira_sliding_2024,
	title = {Sliding {Puzzles} {Gym}: {A} {Scalable} {Benchmark} for {State} {Representation} in {Visual} {Reinforcement} {Learning}},
	shorttitle = {Sliding {Puzzles} {Gym}},
	url = {http://arxiv.org/abs/2410.14038},
	doi = {10.48550/arXiv.2410.14038},
	abstract = {Learning effective visual representations is crucial in open-world environments where agents encounter diverse and unstructured observations. This ability enables agents to extract meaningful information from raw sensory inputs, like pixels, which is essential for generalization across different tasks. However, evaluating representation learning separately from policy learning remains a challenge in most reinforcement learning (RL) benchmarks. To address this, we introduce the Sliding Puzzles Gym (SPGym), a benchmark that extends the classic 15-tile puzzle with variable grid sizes and observation spaces, including large real-world image datasets. SPGym allows scaling the representation learning challenge while keeping the latent environment dynamics and algorithmic problem fixed, providing a targeted assessment of agents' ability to form compositional and generalizable state representations. Our experiments with both model-free and model-based RL algorithms, with and without explicit representation learning components, show that as the representation challenge scales, SPGym effectively distinguishes agents based on their capabilities. Moreover, SPGym reaches difficulty levels where no tested algorithm consistently excels, highlighting key challenges and opportunities for advancing representation learning for decision-making research.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Oliveira, Bryan L. M. de and Luz, Murilo L. da and Brandão, Bruno and Martins, Luana G. B. and Soares, Telma W. de L. and Melo, Luckeciano C.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14038},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_reward-free_2024,
	title = {Reward-free {World} {Models} for {Online} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2410.14081},
	doi = {10.48550/arXiv.2410.14081},
	abstract = {Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Li, Shangzhe and Huang, Zhiao and Su, Hao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14081},
	keywords = {Computer Science - Machine Learning},
}

@misc{le_articulate-anything_2024,
	title = {Articulate-{Anything}: {Automatic} {Modeling} of {Articulated} {Objects} via a {Vision}-{Language} {Foundation} {Model}},
	shorttitle = {Articulate-{Anything}},
	url = {http://arxiv.org/abs/2410.13882},
	doi = {10.48550/arXiv.2410.13882},
	abstract = {Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation. However, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we present Articulate-Anything, a system that automates the articulation of diverse, complex objects from many input modalities, including text, images, and videos. Articulate-Anything leverages vision-language models (VLMs) to generate code that can be compiled into an interactable digital twin for use in standard 3D simulators. Our system exploits existing 3D asset datasets via a mesh retrieval mechanism, along with an actor-critic system that iteratively proposes, evaluates, and refines solutions for articulating the objects, self-correcting errors to achieve a robust outcome. Qualitative evaluations demonstrate Articulate-Anything's capability to articulate complex and even ambiguous object affordances by leveraging rich grounded inputs. In extensive quantitative experiments on the standard PartNet-Mobility dataset, Articulate-Anything substantially outperforms prior work, increasing the success rate from 8.7-11.6\% to 75\% and setting a new bar for state-of-the-art performance. We further showcase the utility of our generated assets by using them to train robotic policies for fine-grained manipulation tasks that go beyond basic pick and place.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Le, Long and Xie, Jason and Liang, William and Wang, Hung-Ju and Yang, Yue and Ma, Yecheng Jason and Vedder, Kyle and Krishna, Arjun and Jayaraman, Dinesh and Eaton, Eric},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_depthsplat_2024,
	title = {{DepthSplat}: {Connecting} {Gaussian} {Splatting} and {Depth}},
	shorttitle = {{DepthSplat}},
	url = {http://arxiv.org/abs/2410.13862},
	doi = {10.48550/arXiv.2410.13862},
	abstract = {Gaussian splatting and single/multi-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabelled datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. Our code, models, and video results are available at https://haofeixu.github.io/depthsplat/.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Xu, Haofei and Peng, Songyou and Wang, Fangjinhua and Blum, Hermann and Barath, Daniel and Geiger, Andreas and Pollefeys, Marc},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13862},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_motionbank_2024,
	title = {{MotionBank}: {A} {Large}-scale {Video} {Motion} {Benchmark} with {Disentangled} {Rule}-based {Annotations}},
	shorttitle = {{MotionBank}},
	url = {http://arxiv.org/abs/2410.13790},
	doi = {10.48550/arXiv.2410.13790},
	abstract = {In this paper, we tackle the problem of how to build and benchmark a large motion model (LMM). The ultimate goal of LMM is to serve as a foundation model for versatile motion-related tasks, e.g., human motion generation, with interpretability and generalizability. Though advanced, recent LMM-related works are still limited by small-scale motion data and costly text descriptions. Besides, previous motion benchmarks primarily focus on pure body movements, neglecting the ubiquitous motions in context, i.e., humans interacting with humans, objects, and scenes. To address these limitations, we consolidate large-scale video action datasets as knowledge banks to build MotionBank, which comprises 13 video action datasets, 1.24M motion sequences, and 132.9M frames of natural and diverse human motions. Different from laboratory-captured motions, in-the-wild human-centric videos contain abundant motions in context. To facilitate better motion text alignment, we also meticulously devise a motion caption generation algorithm to automatically produce rule-based, unbiased, and disentangled text descriptions via the kinematic characteristics for each motion. Extensive experiments show that our MotionBank is beneficial for general motion-related tasks of human motion generation, motion in-context generation, and motion understanding. Video motions together with the rule-based text annotations could serve as an efficient alternative for larger LMMs. Our dataset, codes, and benchmark will be publicly available at https://github.com/liangxuy/MotionBank.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Xu, Liang and Hua, Shaoyang and Lin, Zili and Liu, Yifan and Ma, Feipeng and Yan, Yichao and Jin, Xin and Yang, Xiaokang and Zeng, Wenjun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13790},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{roessle_l3dg_2024,
	title = {{L3DG}: {Latent} {3D} {Gaussian} {Diffusion}},
	shorttitle = {{L3DG}},
	url = {http://arxiv.org/abs/2410.13530},
	doi = {10.48550/arXiv.2410.13530},
	abstract = {We propose L3DG, the first approach for generative 3D modeling of 3D Gaussians through a latent 3D Gaussian diffusion formulation. This enables effective generative 3D modeling, scaling to generation of entire room-scale scenes which can be very efficiently rendered. To enable effective synthesis of 3D Gaussians, we propose a latent diffusion formulation, operating in a compressed latent space of 3D Gaussians. This compressed latent space is learned by a vector-quantized variational autoencoder (VQ-VAE), for which we employ a sparse convolutional architecture to efficiently operate on room-scale scenes. This way, the complexity of the costly generation process via diffusion is substantially reduced, allowing higher detail on object-level generation, as well as scalability to large scenes. By leveraging the 3D Gaussian representation, the generated scenes can be rendered from arbitrary viewpoints in real-time. We demonstrate that our approach significantly improves visual quality over prior work on unconditional object-level radiance field synthesis and showcase its applicability to room-scale scene generation.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Roessle, Barbara and Müller, Norman and Porzi, Lorenzo and Bulò, Samuel Rota and Kontschieder, Peter and Dai, Angela and Nießner, Matthias},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13530},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{nakamoto_steering_2024,
	title = {Steering {Your} {Generalists}: {Improving} {Robotic} {Foundation} {Models} via {Value} {Guidance}},
	shorttitle = {Steering {Your} {Generalists}},
	url = {http://arxiv.org/abs/2410.13816},
	doi = {10.48550/arXiv.2410.13816},
	abstract = {Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Nakamoto, Mitsuhiko and Mees, Oier and Kumar, Aviral and Levine, Sergey},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13816},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yu_state_2024,
	title = {State {Estimation} {Transformers} for {Agile} {Legged} {Locomotion}},
	url = {http://arxiv.org/abs/2410.13496},
	doi = {10.48550/arXiv.2410.13496},
	abstract = {We propose a state estimation method that can accurately predict the robot's privileged states to push the limits of quadruped robots in executing advanced skills such as jumping in the wild. In particular, we present the State Estimation Transformers (SET), an architecture that casts the state estimation problem as conditional sequence modeling. SET outputs the robot states that are hard to obtain directly in the real world, such as the body height and velocities, by leveraging a causally masked Transformer. By conditioning an autoregressive model on the robot's past states, our SET model can predict these privileged observations accurately even in highly dynamic locomotions. We evaluate our methods on three tasks -- running jumping, running backflipping, and running sideslipping -- on a low-cost quadruped robot, Cyberdog2. Results show that SET can outperform other methods in estimation accuracy and transferability in the simulation as well as success rates of jumping and triggering a recovery controller in the real world, suggesting the superiority of such a Transformer-based explicit state estimator in highly dynamic locomotion tasks.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Yu, Chen and Yang, Yichu and Liu, Tianlin and You, Yangwei and Zhou, Mingliang and Xiang, Diyun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13496},
	keywords = {Computer Science - Robotics},
}

@misc{burde_object_2024,
	title = {Object {Pose} {Estimation} {Using} {Implicit} {Representation} {For} {Transparent} {Objects}},
	url = {http://arxiv.org/abs/2410.13465},
	doi = {10.48550/arXiv.2410.13465},
	abstract = {Object pose estimation is a prominent task in computer vision. The object pose gives the orientation and translation of the object in real-world space, which allows various applications such as manipulation, augmented reality, etc. Various objects exhibit different properties with light, such as reflections, absorption, etc. This makes it challenging to understand the object's structure in RGB and depth channels. Recent research has been moving toward learning-based methods, which provide a more flexible and generalizable approach to object pose estimation utilizing deep learning. One such approach is the render-and-compare method, which renders the object from multiple views and compares it against the given 2D image, which often requires an object representation in the form of a CAD model. We reason that the synthetic texture of the CAD model may not be ideal for rendering and comparing operations. We showed that if the object is represented as an implicit (neural) representation in the form of Neural Radiance Field (NeRF), it exhibits a more realistic rendering of the actual scene and retains the crucial spatial features, which makes the comparison more versatile. We evaluated our NeRF implementation of the render-and-compare method on transparent datasets and found that it surpassed the current state-of-the-art results.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Burde, Varun and Moroz, Artem and Zeman, Vit and Burget, Pavel},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13465},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{doughty_locomotion_2024,
	title = {{LocoMotion}: {Learning} {Motion}-{Focused} {Video}-{Language} {Representations}},
	shorttitle = {{LocoMotion}},
	url = {http://arxiv.org/abs/2410.12018},
	doi = {10.48550/arXiv.2410.12018},
	abstract = {This paper strives for motion-focused video-language representations. Existing methods to learn video-language representations use spatial-focused data, where identifying the objects and scene is often enough to distinguish the relevant caption. We instead propose LocoMotion to learn from motion-focused captions that describe the movement and temporal progression of local object motions. We achieve this by adding synthetic motions to videos and using the parameters of these motions to generate corresponding captions. Furthermore, we propose verb-variation paraphrasing to increase the caption variety and learn the link between primitive motions and high-level verbs. With this, we are able to learn a motion-focused video-language representation. Experiments demonstrate our approach is effective for a variety of downstream tasks, particularly when limited data is available for fine-tuning. Code is available: https://hazeldoughty.github.io/Papers/LocoMotion/},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Doughty, Hazel and Thoker, Fida Mohammad and Snoek, Cees G. M.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12018},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{rana_affordance-centric_2024,
	title = {Affordance-{Centric} {Policy} {Learning}: {Sample} {Efficient} and {Generalisable} {Robot} {Policy} {Learning} using {Affordance}-{Centric} {Task} {Frames}},
	shorttitle = {Affordance-{Centric} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2410.12124},
	doi = {10.48550/arXiv.2410.12124},
	abstract = {Affordances are central to robotic manipulation, where most tasks can be simplified to interactions with task-specific regions on objects. By focusing on these key regions, we can abstract away task-irrelevant information, simplifying the learning process, and enhancing generalisation. In this paper, we propose an affordance-centric policy-learning approach that centres and appropriately {\textbackslash}textit\{orients\} a {\textbackslash}textit\{task frame\} on these affordance regions allowing us to achieve both {\textbackslash}textbf\{intra-category invariance\} -- where policies can generalise across different instances within the same object category -- and {\textbackslash}textbf\{spatial invariance\} -- which enables consistent performance regardless of object placement in the environment. We propose a method to leverage existing generalist large vision models to extract and track these affordance frames, and demonstrate that our approach can learn manipulation tasks using behaviour cloning from as little as 10 demonstrations, with equivalent generalisation to an image-based policy trained on 305 demonstrations. We provide video demonstrations on our project site: https://affordance-policy.github.io.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Rana, Krishan and Abou-Chakra, Jad and Garg, Sourav and Lee, Robert and Reid, Ian and Suenderhauf, Niko},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12124},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{yan_dynamic_2024,
	title = {Dynamic {Open}-{Vocabulary} {3D} {Scene} {Graphs} for {Long}-term {Language}-{Guided} {Mobile} {Manipulation}},
	url = {http://arxiv.org/abs/2410.11989},
	doi = {10.48550/arXiv.2410.11989},
	abstract = {Enabling mobile robots to perform long-term tasks in dynamic real-world environments is a formidable challenge, especially when the environment changes frequently due to human-robot interactions or the robot's own actions. Traditional methods typically assume static scenes, which limits their applicability in the continuously changing real world. To overcome these limitations, we present DovSG, a novel mobile manipulation framework that leverages dynamic open-vocabulary 3D scene graphs and a language-guided task planning module for long-term task execution. DovSG takes RGB-D sequences as input and utilizes vision-language models (VLMs) for object detection to obtain high-level object semantic features. Based on the segmented objects, a structured 3D scene graph is generated for low-level spatial relationships. Furthermore, an efficient mechanism for locally updating the scene graph, allows the robot to adjust parts of the graph dynamically during interactions without the need for full scene reconstruction. This mechanism is particularly valuable in dynamic environments, enabling the robot to continually adapt to scene changes and effectively support the execution of long-term tasks. We validated our system in real-world environments with varying degrees of manual modifications, demonstrating its effectiveness and superior performance in long-term tasks. Our project page is available at: https://bjhyzj.github.io/dovsg-web.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Yan, Zhijie and Li, Shufei and Wang, Zuoxu and Wu, Lixiu and Wang, Han and Zhu, Jun and Chen, Lijiang and Liu, Jihong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11989},
	keywords = {Computer Science - Robotics},
}

@misc{he_visual_2024,
	title = {Visual {Manipulation} with {Legs}},
	url = {http://arxiv.org/abs/2410.11345},
	doi = {10.48550/arXiv.2410.11345},
	abstract = {Animals use limbs for both locomotion and manipulation. We aim to equip quadruped robots with similar versatility. This work introduces a system that enables quadruped robots to interact with objects using their legs, inspired by non-prehensile manipulation. The system has two main components: a visual manipulation policy module and a loco-manipulator module. The visual manipulation policy, trained with reinforcement learning (RL) using point cloud observations and object-centric actions, decides how the leg should interact with the object. The loco-manipulator controller manages leg movements and body pose adjustments, based on impedance control and Model Predictive Control (MPC). Besides manipulating objects with a single leg, the system can select from the left or right leg based on critic maps and move objects to distant goals through base adjustment. Experiments evaluate the system on object pose alignment tasks in both simulation and the real world, demonstrating more versatile object manipulation skills with legs than previous work. Videos can be found at https://legged-manipulation.github.io/},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {He, Xialin and Yuan, Chengjing and Zhou, Wenxuan and Yang, Ruihan and Held, David and Wang, Xiaolong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11345},
	keywords = {Computer Science - Robotics},
}

@misc{li_okami_2024,
	title = {{OKAMI}: {Teaching} {Humanoid} {Robots} {Manipulation} {Skills} through {Single} {Video} {Imitation}},
	shorttitle = {{OKAMI}},
	url = {http://arxiv.org/abs/2410.11792},
	doi = {10.48550/arXiv.2410.11792},
	abstract = {We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, outperforming the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of 79.2\% without the need for labor-intensive teleoperation. More videos can be found on our website https://ut-austin-rpl.github.io/OKAMI/.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Li, Jinhan and Zhu, Yifeng and Xie, Yuqi and Jiang, Zhenyu and Seo, Mingyo and Pavlakos, Georgios and Zhu, Yuke},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11792},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_deformpam_2024,
	title = {{DeformPAM}: {Data}-{Efficient} {Learning} for {Long}-horizon {Deformable} {Object} {Manipulation} via {Preference}-based {Action} {Alignment}},
	shorttitle = {{DeformPAM}},
	url = {http://arxiv.org/abs/2410.11584},
	doi = {10.48550/arXiv.2410.11584},
	abstract = {In recent years, imitation learning has made progress in the field of robotic manipulation. However, it still faces challenges when dealing with complex long-horizon deformable object tasks, such as high-dimensional state spaces, complex dynamics, and multimodal action distributions. Traditional imitation learning methods often require a large amount of data and encounter distributional shifts and accumulative errors in these tasks. To address these issues, we propose a data-efficient general learning framework (DeformPAM) based on preference learning and reward-guided action selection. DeformPAM decomposes long-horizon tasks into multiple action primitives, utilizes 3D point cloud inputs and diffusion models to model action distributions, and trains an implicit reward model using human preference data. During the inference phase, the reward model scores multiple candidate actions, selecting the optimal action for execution, thereby reducing the occurrence of anomalous actions and improving task completion quality. Experiments conducted on three challenging real-world long-horizon deformable object manipulation tasks demonstrate the effectiveness of this method. Results show that DeformPAM improves both task completion quality and efficiency compared to baseline methods even with limited data. Code and data will be available at https://deform-pam.robotflow.ai.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Chen, Wendi and Xue, Han and Zhou, Fangyuan and Fang, Yuan and Lu, Cewu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11584},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kang_incorporating_2024,
	title = {Incorporating {Task} {Progress} {Knowledge} for {Subgoal} {Generation} in {Robotic} {Manipulation} through {Image} {Edits}},
	url = {http://arxiv.org/abs/2410.11013},
	doi = {10.48550/arXiv.2410.11013},
	abstract = {Understanding the progress of a task allows humans to not only track what has been done but also to better plan for future goals. We demonstrate TaKSIE, a novel framework that incorporates task progress knowledge into visual subgoal generation for robotic manipulation tasks. We jointly train a recurrent network with a latent diffusion model to generate the next visual subgoal based on the robot's current observation and the input language command. At execution time, the robot leverages a visual progress representation to monitor the task progress and adaptively samples the next visual subgoal from the model to guide the manipulation policy. We train and validate our model in simulated and real-world robotic tasks, achieving state-of-the-art performance on the CALVIN manipulation benchmark. We find that the inclusion of task progress knowledge can improve the robustness of trained policy for different initial robot poses or various movement speeds during demonstrations. The project website can be found at https://live-robotics-uva.github.io/TaKSIE/ .},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Kang, Xuhui and Kuo, Yen-Ling},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11013},
	keywords = {Computer Science - Robotics},
}

@misc{ye_latent_2024,
	title = {Latent {Action} {Pretraining} from {Videos}},
	url = {http://arxiv.org/abs/2410.11758},
	doi = {10.48550/arXiv.2410.11758},
	abstract = {We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Ye, Seonghyeon and Jang, Joel and Jeon, Byeongguk and Joo, Sejune and Yang, Jianwei and Peng, Baolin and Mandlekar, Ajay and Tan, Reuben and Chao, Yu-Wei and Lin, Bill Yuchen and Liden, Lars and Lee, Kimin and Gao, Jianfeng and Zettlemoyer, Luke and Fox, Dieter and Seo, Minjoon},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11758},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_pavlm_2024,
	title = {{PAVLM}: {Advancing} {Point} {Cloud} based {Affordance} {Understanding} {Via} {Vision}-{Language} {Model}},
	shorttitle = {{PAVLM}},
	url = {http://arxiv.org/abs/2410.11564},
	doi = {10.48550/arXiv.2410.11564},
	abstract = {Affordance understanding, the task of identifying actionable regions on 3D objects, plays a vital role in allowing robotic systems to engage with and operate within the physical world. Although Visual Language Models (VLMs) have excelled in high-level reasoning and long-horizon planning for robotic manipulation, they still fall short in grasping the nuanced physical properties required for effective human-robot interaction. In this paper, we introduce PAVLM (Point cloud Affordance Vision-Language Model), an innovative framework that utilizes the extensive multimodal knowledge embedded in pre-trained language models to enhance 3D affordance understanding of point cloud. PAVLM integrates a geometric-guided propagation module with hidden embeddings from large language models (LLMs) to enrich visual semantics. On the language side, we prompt Llama-3.1 models to generate refined context-aware text, augmenting the instructional input with deeper semantic cues. Experimental results on the 3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods for both full and partial point clouds, particularly excelling in its generalization to novel open-world affordance tasks of 3D objects. For more information, visit our project site: pavlm-source.github.io.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Liu, Shang-Ching and Tran, Van Nhiem and Chen, Wenkai and Cheng, Wei-Lun and Huang, Yen-Lin and Liao, I.-Bin and Li, Yung-Hui and Zhang, Jianwei},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11564},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{hu_disentangled_2024,
	title = {Disentangled {Unsupervised} {Skill} {Discovery} for {Efficient} {Hierarchical} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2410.11251},
	doi = {10.48550/arXiv.2410.11251},
	abstract = {A hallmark of intelligent agents is the ability to learn reusable skills purely from unsupervised interaction with the environment. However, existing unsupervised skill discovery methods often learn entangled skills where one skill variable simultaneously influences many entities in the environment, making downstream skill chaining extremely challenging. We propose Disentangled Unsupervised Skill Discovery (DUSDi), a method for learning disentangled skills that can be efficiently reused to solve downstream tasks. DUSDi decomposes skills into disentangled components, where each skill component only affects one factor of the state space. Importantly, these skill components can be concurrently composed to generate low-level actions, and efficiently chained to tackle downstream tasks through hierarchical Reinforcement Learning. DUSDi defines a novel mutual-information-based objective to enforce disentanglement between the influences of different skill components, and utilizes value factorization to optimize this objective efficiently. Evaluated in a set of challenging environments, DUSDi successfully learns disentangled skills, and significantly outperforms previous skill discovery methods when it comes to applying the learned skills to solve downstream tasks. Code and skills visualization at jiahenghu.github.io/DUSDi-site/.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Hu, Jiaheng and Wang, Zizhao and Stone, Peter and Martín-Martín, Roberto},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11251},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yan_m2diffuser_2024,
	title = {{M2Diffuser}: {Diffusion}-based {Trajectory} {Optimization} for {Mobile} {Manipulation} in {3D} {Scenes}},
	shorttitle = {{M2Diffuser}},
	url = {http://arxiv.org/abs/2410.11402},
	doi = {10.48550/arXiv.2410.11402},
	abstract = {Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Yan, Sixu and Zhang, Zeyu and Han, Muzhi and Wang, Zaijin and Xie, Qi and Li, Zhitian and Li, Zhehan and Liu, Hangxin and Wang, Xinggang and Zhu, Song-Chun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11402},
	keywords = {Computer Science - Robotics},
}

@misc{rodriguez_contrastive_2024,
	title = {Contrastive {Touch}-to-{Touch} {Pretraining}},
	url = {http://arxiv.org/abs/2410.11834},
	doi = {10.48550/arXiv.2410.11834},
	abstract = {Today's tactile sensors have a variety of different designs, making it challenging to develop general-purpose methods for processing touch signals. In this paper, we learn a unified representation that captures the shared information between different tactile sensors. Unlike current approaches that focus on reconstruction or task-specific supervision, we leverage contrastive learning to integrate tactile signals from two different sensors into a shared embedding space, using a dataset in which the same objects are probed with multiple sensors. We apply this approach to paired touch signals from GelSlim and Soft Bubble sensors. We show that our learned features provide strong pretraining for downstream pose estimation and classification tasks. We also show that our embedding enables models trained using one touch sensor to be deployed using another without additional training. Project details can be found at https://www.mmintlab.com/research/cttp/.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Rodriguez, Samanta and Dou, Yiming and Bogert, William van den and Oller, Miquel and So, Kevin and Owens, Andrew and Fazeli, Nima},
	month = oct,
	year = {2024},
	note = {arXiv:2410.11834},
	keywords = {Computer Science - Robotics},
}

@inproceedings{tang_dreamgaussian_2023,
	title = {{DreamGaussian}: {Generative} {Gaussian} {Splatting} for {Efficient} {3D} {Content} {Creation}},
	shorttitle = {{DreamGaussian}},
	url = {https://openreview.net/forum?id=UyNXMqnN3c},
	abstract = {Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.},
	language = {en},
	urldate = {2024-11-18},
	author = {Tang, Jiaxiang and Ren, Jiawei and Zhou, Hang and Liu, Ziwei and Zeng, Gang},
	month = oct,
	year = {2023},
}

@misc{li_learning_2019,
	title = {Learning {Particle} {Dynamics} for {Manipulating} {Rigid} {Bodies}, {Deformable} {Objects}, and {Fluids}},
	url = {http://arxiv.org/abs/1810.01566},
	doi = {10.48550/arXiv.1810.01566},
	abstract = {Real-life control tasks involve matters of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real-world physics, especially in the long term. In this paper, we propose to learn a particle-based simulator for complex control tasks. Combining learning with particle-based systems brings in two major benefits: first, the learned simulator, just like other particle-based systems, acts widely on objects of different materials; second, the particle-based representation poses strong inductive bias for learning: particles of the same type have the same dynamics within. This enables the model to quickly adapt to new environments of unknown dynamics within a few observations. We demonstrate robots achieving complex manipulation tasks using the learned simulator, such as manipulating fluids and deformable foam, with experiments both in simulation and in the real world. Our study helps lay the foundation for robot learning of dynamic scenes with particle-based representations.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Li, Yunzhu and Wu, Jiajun and Tedrake, Russ and Tenenbaum, Joshua B. and Torralba, Antonio},
	month = apr,
	year = {2019},
	note = {arXiv:1810.01566},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Physics - Computational Physics, Statistics - Machine Learning},
}

@misc{hou_adaptive_2024,
	title = {Adaptive {Compliance} {Policy}: {Learning} {Approximate} {Compliance} for {Diffusion} {Guided} {Control}},
	shorttitle = {Adaptive {Compliance} {Policy}},
	url = {http://arxiv.org/abs/2410.09309},
	doi = {10.48550/arXiv.2410.09309},
	abstract = {Compliance plays a crucial role in manipulation, as it balances between the concurrent control of position and force under uncertainties. Yet compliance is often overlooked by today's visuomotor policies that solely focus on position control. This paper introduces Adaptive Compliance Policy (ACP), a novel framework that learns to dynamically adjust system compliance both spatially and temporally for given manipulation tasks from human demonstrations, improving upon previous approaches that rely on pre-selected compliance parameters or assume uniform constant stiffness. However, computing full compliance parameters from human demonstrations is an ill-defined problem. Instead, we estimate an approximate compliance profile with two useful properties: avoiding large contact forces and encouraging accurate tracking. Our approach enables robots to handle complex contact-rich manipulation tasks and achieves over 50{\textbackslash}\% performance improvement compared to state-of-the-art visuomotor policy methods. For result videos, see https://adaptive-compliance.github.io/},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Hou, Yifan and Liu, Zeyi and Chi, Cheng and Cousineau, Eric and Kuppuswamy, Naveen and Feng, Siyuan and Burchfiel, Benjamin and Song, Shuran},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09309},
	keywords = {Computer Science - Robotics},
}

@misc{xu_cavia_2024,
	title = {Cavia: {Camera}-controllable {Multi}-view {Video} {Diffusion} with {View}-{Integrated} {Attention}},
	shorttitle = {Cavia},
	url = {http://arxiv.org/abs/2410.10774},
	doi = {10.48550/arXiv.2410.10774},
	abstract = {In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Xu, Dejia and Jiang, Yifan and Huang, Chen and Song, Liangchen and Gernoth, Thorsten and Cao, Liangliang and Wang, Zhangyang and Tang, Hao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.10774},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tseng_gaussian_2024,
	title = {Gaussian {Splatting} {Visual} {MPC} for {Granular} {Media} {Manipulation}},
	url = {http://arxiv.org/abs/2410.09740},
	doi = {10.48550/arXiv.2410.09740},
	abstract = {Recent advancements in learned 3D representations have enabled significant progress in solving complex robotic manipulation tasks, particularly for rigid-body objects. However, manipulating granular materials such as beans, nuts, and rice, remains challenging due to the intricate physics of particle interactions, high-dimensional and partially observable state, inability to visually track individual particles in a pile, and the computational demands of accurate dynamics prediction. Current deep latent dynamics models often struggle to generalize in granular material manipulation due to a lack of inductive biases. In this work, we propose a novel approach that learns a visual dynamics model over Gaussian splatting representations of scenes and leverages this model for manipulating granular media via Model-Predictive Control. Our method enables efficient optimization for complex manipulation tasks on piles of granular media. We evaluate our approach in both simulated and real-world settings, demonstrating its ability to solve unseen planning tasks and generalize to new environments in a zero-shot transfer. We also show significant prediction and manipulation performance improvements compared to existing granular media manipulation methods.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Tseng, Wei-Cheng and Zhang, Ellina and Jatavallabhula, Krishna Murthy and Shkurti, Florian},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09740},
	keywords = {Computer Science - Robotics},
}

@misc{ze_generalizable_2024,
	title = {Generalizable {Humanoid} {Manipulation} with {Improved} {3D} {Diffusion} {Policies}},
	url = {http://arxiv.org/abs/2410.10803},
	doi = {10.48550/arXiv.2410.10803},
	abstract = {Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Ze, Yanjie and Chen, Zixuan and Wang, Wenhao and Chen, Tianyi and He, Xialin and Yuan, Ying and Peng, Xue Bin and Wu, Jiajun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.10803},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{jeon_prima6d_2020,
	title = {{PrimA6D}: {Rotational} {Primitive} {Reconstruction} for {Enhanced} and {Robust} {6D} {Pose} {Estimation}},
	volume = {5},
	issn = {2377-3766},
	shorttitle = {{PrimA6D}},
	url = {https://ieeexplore.ieee.org/abstract/document/9123683?casa_token=jkjPbq95NkkAAAAA:imsNOjTFxFDxi2eym02BF6OkNU5zeenUFXS9jfcOBtigPqf7ZbW0aC0-2QO94HmvCmZY2Cx8zd8},
	doi = {10.1109/LRA.2020.3004322},
	abstract = {In this letter, we introduce a rotational primitive prediction based 6D object pose estimation using a single image as an input. We solve for the 6D object pose of a known object relative to the camera using a single image with occlusion. Many recent state-of-the-art (SOTA) two-step approaches have exploited image keypoints extraction followed by PnP regression for pose estimation. Instead of relying on bounding box or keypoints on the object, we propose to learn orientation-induced primitive so as to achieve the pose estimation accuracy regardless of the object size. We leverage a Variational AutoEncoder (VAE) to learn this underlying primitive and its associated keypoints. The keypoints inferred from the reconstructed primitive image are then used to regress the rotation using PnP. Lastly, we compute the translation in a separate localization module to complete the entire 6D pose estimation. When evaluated over public datasets, the proposed method yields a notable improvement over the LINEMOD, the Occlusion LINEMOD, and the YCB-Video dataset. We further provide a synthetic-only trained case presenting comparable performance to the existing methods which require real images in the training phase.},
	number = {3},
	urldate = {2024-11-15},
	journal = {IEEE Robotics and Automation Letters},
	author = {Jeon, Myung-Hwan and Kim, Ayoung},
	month = jul,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Decoding, Generative adversarial networks, Image reconstruction, Perception for grasping and manipulation, Pose estimation, Three-dimensional displays, Training, deep learning for visual perception},
	pages = {4955--4962},
}

@misc{zhang_adaptigraph_2024,
	title = {{AdaptiGraph}: {Material}-{Adaptive} {Graph}-{Based} {Neural} {Dynamics} for {Robotic} {Manipulation}},
	shorttitle = {{AdaptiGraph}},
	url = {http://arxiv.org/abs/2407.07889},
	doi = {10.48550/arXiv.2407.07889},
	abstract = {Predictive models are a crucial component of many robotic systems. Yet, constructing accurate predictive models for a variety of deformable objects, especially those with unknown physical properties, remains a significant challenge. This paper introduces AdaptiGraph, a learning-based dynamics modeling approach that enables robots to predict, adapt to, and control a wide array of challenging deformable materials with unknown physical properties. AdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND) framework, which represents material bits as particles and employs a graph neural network (GNN) to predict particle motion. Its key innovation is a unified physical property-conditioned GBND model capable of predicting the motions of diverse materials with varying physical properties without retraining. Upon encountering new materials during online deployment, AdaptiGraph utilizes a physical property optimization process for a few-shot adaptation of the model, enhancing its fit to the observed interaction data. The adapted models can precisely simulate the dynamics and predict the motion of various deformable materials, such as ropes, granular media, rigid boxes, and cloth, while adapting to different physical properties, including stiffness, granular size, and center of pressure. On prediction and manipulation tasks involving a diverse set of real-world deformable objects, our method exhibits superior prediction accuracy and task proficiency over non-material-conditioned and non-adaptive models. The project page is available at https://robopil.github.io/adaptigraph/ .},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Zhang, Kaifeng and Li, Baoyu and Hauser, Kris and Li, Yunzhu},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07889},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{zhang_dynamic_2024,
	title = {Dynamic {3D} {Gaussian} {Tracking} for {Graph}-{Based} {Neural} {Dynamics} {Modeling}},
	url = {https://openreview.net/forum?id=itKJ5uu1gW},
	abstract = {Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at {\textbackslash}url\{https://gaussian-gbnd.github.io/\}.},
	language = {en},
	urldate = {2024-11-15},
	author = {Zhang, Mingtong and Zhang, Kaifeng and Li, Yunzhu},
	month = sep,
	year = {2024},
}

@inproceedings{zhang_generative_2023,
	title = {Generative {Category}-level {Object} {Pose} {Estimation} via {Diffusion} {Models}},
	url = {https://openreview.net/forum?id=l6ypbj6Nv5&noteId=l5yHAYOM4w},
	abstract = {Object pose estimation plays a vital role in embodied AI and computer vision, enabling intelligent agents to comprehend and interact with their surroundings. Despite the practicality of category-level pose estimation, current approaches encounter challenges with partially observed point clouds, known as the multihypothesis issue. In this study, we propose a novel solution by reframing categorylevel object pose estimation as conditional generative modeling, departing from traditional point-to-point regression. Leveraging score-based diffusion models, we estimate object poses by sampling candidates from the diffusion model and aggregating them through a two-step process: filtering out outliers via likelihood estimation and subsequently mean-pooling the remaining candidates. To avoid the costly integration process when estimating the likelihood, we introduce an alternative method that distils an energy-based model from the original score-based model, enabling end-to-end likelihood estimation. Our approach achieves state-of-the-art performance on the REAL275 dataset, surpassing 50\% and 60\% on strict 5 ◦ 2cm and 5 ◦ 5cm metrics, respectively. Furthermore, our method demonstrates strong generalization to novel categories without the need for fine-tuning and can readily adapt to object pose tracking tasks, yielding comparable results to the current state-of-the-art baselines. Our checkpoints and demonstrations can be found at https://sites.google.com/view/genpose.},
	language = {en},
	urldate = {2024-11-15},
	author = {Zhang, Jiyao and Wu, Mingdong and Dong, Hao},
	month = nov,
	year = {2023},
}

@misc{nguyen_tacex_2024,
	title = {{TacEx}: {GelSight} {Tactile} {Simulation} in {Isaac} {Sim} -- {Combining} {Soft}-{Body} and {Visuotactile} {Simulators}},
	shorttitle = {{TacEx}},
	url = {http://arxiv.org/abs/2411.04776},
	doi = {10.48550/arXiv.2411.04776},
	abstract = {Training robot policies in simulation is becoming increasingly popular; nevertheless, a precise, reliable, and easy-to-use tactile simulator for contact-rich manipulation tasks is still missing. To close this gap, we develop TacEx -- a modular tactile simulation framework. We embed a state-of-the-art soft-body simulator for contacts named GIPC and vision-based tactile simulators Taxim and FOTS into Isaac Sim to achieve robust and plausible simulation of the visuotactile sensor GelSight Mini. We implement several Isaac Lab environments for Reinforcement Learning (RL) leveraging our TacEx simulation, including object pushing, lifting, and pole balancing. We validate that the simulation is stable and that the high-dimensional observations, such as the gel deformation and the RGB images from the GelSight camera, can be used for training. The code, videos, and additional results will be released online https://sites.google.com/view/tacex.},
	urldate = {2024-11-13},
	publisher = {arXiv},
	author = {Nguyen, Duc Huy and Schneider, Tim and Duret, Guillaume and Kshirsagar, Alap and Belousov, Boris and Peters, Jan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04776},
	keywords = {Computer Science - Robotics},
}

@inproceedings{jiang_hand-object_2021,
	title = {Hand-{Object} {Contact} {Consistency} {Reasoning} for {Human} {Grasps} {Generation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Jiang_Hand-Object_Contact_Consistency_Reasoning_for_Human_Grasps_Generation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-11-13},
	author = {Jiang, Hanwen and Liu, Shaowei and Wang, Jiashun and Wang, Xiaolong},
	year = {2021},
	pages = {11107--11116},
}

@misc{rupf_zero-shot_2024,
	title = {Zero-{Shot} {Offline} {Imitation} {Learning} via {Optimal} {Transport}},
	url = {http://arxiv.org/abs/2410.08751},
	doi = {10.48550/arXiv.2410.08751},
	abstract = {Zero-shot imitation learning algorithms hold the promise of reproducing unseen behavior from as little as a single demonstration at test time. Existing practical approaches view the expert demonstration as a sequence of goals, enabling imitation with a high-level goal selector, and a low-level goal-conditioned policy. However, this framework can suffer from myopic behavior: the agent's immediate actions towards achieving individual goals may undermine long-term objectives. We introduce a novel method that mitigates this issue by directly optimizing the occupancy matching objective that is intrinsic to imitation learning. We propose to lift a goal-conditioned value function to a distance between occupancies, which are in turn approximated via a learned world model. The resulting method can learn from offline, suboptimal data, and is capable of non-myopic, zero-shot imitation, as we demonstrate in complex, continuous benchmarks.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Rupf, Thomas and Bagatella, Marco and Gürtler, Nico and Frey, Jonas and Martius, Georg},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08751},
	keywords = {Computer Science - Machine Learning},
}

@misc{schmidt_look_2024,
	title = {Look {Gauss}, {No} {Pose}: {Novel} {View} {Synthesis} using {Gaussian} {Splatting} without {Accurate} {Pose} {Initialization}},
	shorttitle = {Look {Gauss}, {No} {Pose}},
	url = {http://arxiv.org/abs/2410.08743},
	doi = {10.48550/arXiv.2410.08743},
	abstract = {3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs .},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Schmidt, Christian and Piekenbrinck, Jens and Leibe, Bastian},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08743},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hao_ego3dt_2024,
	title = {{Ego3DT}: {Tracking} {Every} {3D} {Object} in {Ego}-centric {Videos}},
	shorttitle = {{Ego3DT}},
	url = {http://arxiv.org/abs/2410.08530},
	doi = {10.48550/arXiv.2410.08530},
	abstract = {The growing interest in embodied intelligence has brought ego-centric perspectives to contemporary research. One significant challenge within this realm is the accurate localization and tracking of objects in ego-centric videos, primarily due to the substantial variability in viewing angles. Addressing this issue, this paper introduces a novel zero-shot approach for the 3D reconstruction and tracking of all objects from the ego-centric video. We present Ego3DT, a novel framework that initially identifies and extracts detection and segmentation information of objects within the ego environment. Utilizing information from adjacent video frames, Ego3DT dynamically constructs a 3D scene of the ego view using a pre-trained 3D scene reconstruction model. Additionally, we have innovated a dynamic hierarchical association mechanism for creating stable 3D tracking trajectories of objects in ego-centric videos. Moreover, the efficacy of our approach is corroborated by extensive experiments on two newly compiled datasets, with 1.04x - 2.90x in HOTA, showcasing the robustness and accuracy of our method in diverse ego-centric scenarios.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Hao, Shengyu and Chai, Wenhao and Zhao, Zhonghan and Sun, Meiqi and Hu, Wendi and Zhou, Jieyang and Zhao, Yixian and Li, Qi and Wang, Yizhou and Li, Xi and Wang, Gaoang},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08530},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{mosbach_sold_2024,
	title = {{SOLD}: {Reinforcement} {Learning} with {Slot} {Object}-{Centric} {Latent} {Dynamics}},
	shorttitle = {{SOLD}},
	url = {http://arxiv.org/abs/2410.08822},
	doi = {10.48550/arXiv.2410.08822},
	abstract = {Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning holds the potential to improve sample efficiency over model-free methods by learning inside imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment's state. In contrast, humans reason about objects and their interactions, forecasting how actions will affect specific parts of their surroundings. Inspired by this, we propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a novel algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3, a state-of-the-art model-based RL algorithm, across a range of benchmark robotic environments that evaluate for both relational reasoning and low-level manipulation capabilities. Videos are available at https://slot-latent-dynamics.github.io/.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Mosbach, Malte and Ewertz, Jan Niklas and Villar-Corrales, Angel and Behnke, Sven},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08822},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_vlm_2024,
	title = {{VLM} {See}, {Robot} {Do}: {Human} {Demo} {Video} to {Robot} {Action} {Plan} via {Vision} {Language} {Model}},
	shorttitle = {{VLM} {See}, {Robot} {Do}},
	url = {http://arxiv.org/abs/2410.08792},
	doi = {10.48550/arXiv.2410.08792},
	abstract = {Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to ''see'' human demonstrations and explain the corresponding plans to the robot for it to ''do''. To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Wang, Beichen and Zhang, Juexiao and Dong, Shuwen and Fang, Irving and Feng, Chen},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08792},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_dtactive_2024,
	title = {{DTactive}: {A} {Vision}-{Based} {Tactile} {Sensor} with {Active} {Surface}},
	shorttitle = {{DTactive}},
	url = {http://arxiv.org/abs/2410.08337},
	doi = {10.48550/arXiv.2410.08337},
	abstract = {The development of vision-based tactile sensors has significantly enhanced robots' perception and manipulation capabilities, especially for tasks requiring contact-rich interactions with objects. In this work, we present DTactive, a novel vision-based tactile sensor with active surfaces. DTactive inherits and modifies the tactile 3D shape reconstruction method of DTact while integrating a mechanical transmission mechanism that facilitates the mobility of its surface. Thanks to this design, the sensor is capable of simultaneously performing tactile perception and in-hand manipulation with surface movement. Leveraging the high-resolution tactile images from the sensor and the magnetic encoder data from the transmission mechanism, we propose a learning-based method to enable precise angular trajectory control during in-hand manipulation. In our experiments, we successfully achieved accurate rolling manipulation within the range of [ -180\{{\textbackslash}deg\},180\{{\textbackslash}deg\} ] on various objects, with the root mean square error between the desired and actual angular trajectories being less than 12\{{\textbackslash}deg\} on nine trained objects and less than 19\{{\textbackslash}deg\} on three novel objects. The results demonstrate the potential of DTactive for in-hand object manipulation in terms of effectiveness, robustness and precision.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Xu, Jikai and Wu, Lei and Lin, Changyi and Zhao, Ding and Xu, Huazhe},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08337},
	keywords = {Computer Science - Robotics},
}

@misc{fang_fusionsense_2024,
	title = {{FusionSense}: {Bridging} {Common} {Sense}, {Vision}, and {Touch} for {Robust} {Sparse}-{View} {Reconstruction}},
	shorttitle = {{FusionSense}},
	url = {http://arxiv.org/abs/2410.08282},
	doi = {10.48550/arXiv.2410.08282},
	abstract = {Humans effortlessly integrate common-sense knowledge with sensory input from vision and touch to understand their surroundings. Emulating this capability, we introduce FusionSense, a novel 3D reconstruction framework that enables robots to fuse priors from foundation models with highly sparse observations from vision and tactile sensors. FusionSense addresses three key challenges: (i) How can robots efficiently acquire robust global shape information about the surrounding scene and objects? (ii) How can robots strategically select touch points on the object using geometric and common-sense priors? (iii) How can partial observations such as tactile signals improve the overall representation of the object? Our framework employs 3D Gaussian Splatting as a core representation and incorporates a hierarchical optimization strategy involving global structure construction, object visual hull pruning and local geometric constraints. This advancement results in fast and robust perception in environments with traditionally challenging objects that are transparent, reflective, or dark, enabling more downstream manipulation or navigation tasks. Experiments on real-world data suggest that our framework outperforms previously state-of-the-art sparse-view methods. All code and data are open-sourced on the project website.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Fang, Irving and Shi, Kairui and He, Xujin and Tan, Siqi and Wang, Yifan and Zhao, Hanwen and Huang, Hung-Jui and Yuan, Wenzhen and Feng, Chen and Zhang, Jing},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08282},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{peng_3d_2024,
	title = {{3D} {Vision}-{Language} {Gaussian} {Splatting}},
	url = {http://arxiv.org/abs/2410.07577},
	doi = {10.48550/arXiv.2410.07577},
	abstract = {Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Peng, Qucheng and Planche, Benjamin and Gao, Zhongpai and Zheng, Meng and Choudhuri, Anwesa and Chen, Terrence and Chen, Chen and Wu, Ziyan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07577},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_spa_2024,
	title = {{SPA}: {3D} {Spatial}-{Awareness} {Enables} {Effective} {Embodied} {Representation}},
	shorttitle = {{SPA}},
	url = {http://arxiv.org/abs/2410.08208},
	doi = {10.48550/arXiv.2410.08208},
	abstract = {In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning. Project Page: https://haoyizhu.github.io/spa/.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Zhu, Haoyi and Yang, Honghui and Wang, Yating and Yang, Jiange and Wang, Limin and He, Tong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08208},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_regiongrasp_2024,
	title = {{RegionGrasp}: {A} {Novel} {Task} for {Contact} {Region} {Controllable} {Hand} {Grasp} {Generation}},
	shorttitle = {{RegionGrasp}},
	url = {http://arxiv.org/abs/2410.07995},
	doi = {10.48550/arXiv.2410.07995},
	abstract = {Can machine automatically generate multiple distinct and natural hand grasps, given specific contact region of an object in 3D? This motivates us to consider a novel task of {\textbackslash}textit\{Region Controllable Hand Grasp Generation (RegionGrasp)\}, as follows: given as input a 3D object, together with its specific surface area selected as the intended contact region, to generate a diverse set of plausible hand grasps of the object, where the thumb finger tip touches the object surface on the contact region. To address this task, RegionGrasp-CVAE is proposed, which consists of two main parts. First, to enable contact region-awareness, we propose ConditionNet as the condition encoder that includes in it a transformer-backboned object encoder, O-Enc; a pretraining strategy is adopted by O-Enc, where the point patches of object surface are randomly masked off and subsequently restored, to further capture surface geometric information of the object. Second, to realize interaction awareness, HOINet is introduced to encode hand-object interaction features by entangling high-level hand features with embedded object features through geometric-aware multi-head cross attention. Empirical evaluations demonstrate the effectiveness of our approach qualitatively and quantitatively where it is shown to compare favorably with respect to the state of the art methods.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Wang, Yilin and Guo, Chuan and Cheng, Li and Jiang, Hai},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07995},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_motiongs_2024,
	title = {{MotionGS}: {Exploring} {Explicit} {Motion} {Guidance} for {Deformable} {3D} {Gaussian} {Splatting}},
	shorttitle = {{MotionGS}},
	url = {http://arxiv.org/abs/2410.07707},
	doi = {10.48550/arXiv.2410.07707},
	abstract = {Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS\_page},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Zhu, Ruijie and Liang, Yanzhe and Chang, Hanzhi and Deng, Jiacheng and Lu, Jiahao and Yang, Wenfei and Zhang, Tianzhu and Zhang, Yongdong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07707},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{liu_rdt-1b_2024,
	title = {{RDT}-{1B}: a {Diffusion} {Foundation} {Model} for {Bimanual} {Manipulation}},
	shorttitle = {{RDT}-{1B}},
	url = {http://arxiv.org/abs/2410.07864},
	doi = {10.48550/arXiv.2410.07864},
	abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{\textasciitilde}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07864},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{dai_automated_2024,
	title = {Automated {Creation} of {Digital} {Cousins} for {Robust} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2410.07408},
	doi = {10.48550/arXiv.2410.07408},
	abstract = {Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity between simulated and real-world environments. These discrepancies can be minimized by training in digital twins, which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin, does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for their automated creation, and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that digital cousin scenes that preserve geometric and semantic affordances can be produced automatically, and can be used to train policies that outperform policies trained on digital twins, achieving 90\% vs. 25\% success rates under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Dai, Tianyuan and Wong, Josiah and Jiang, Yunfan and Wang, Chen and Gokmen, Cem and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07408},
	keywords = {Computer Science - Robotics},
}

@misc{obrist_pokeflex_2024,
	title = {{PokeFlex}: {A} {Real}-{World} {Dataset} of {Deformable} {Objects} for {Robotics}},
	shorttitle = {{PokeFlex}},
	url = {http://arxiv.org/abs/2410.07688},
	doi = {10.48550/arXiv.2410.07688},
	abstract = {Data-driven methods have shown great potential in solving challenging manipulation tasks, however, their application in the domain of deformable objects has been constrained, in part, by the lack of data. To address this, we propose PokeFlex, a dataset featuring real-world paired and annotated multimodal data that includes 3D textured meshes, point clouds, RGB images, and depth maps. Such data can be leveraged for several downstream tasks such as online 3D mesh reconstruction, and it can potentially enable underexplored applications such as the real-world deployment of traditional control methods based on mesh simulations. To deal with the challenges posed by real-world 3D mesh reconstruction, we leverage a professional volumetric capture system that allows complete 360\{{\textbackslash}deg\} reconstruction. PokeFlex consists of 18 deformable objects with varying stiffness and shapes. Deformations are generated by dropping objects onto a flat surface or by poking the objects with a robot arm. Interaction forces and torques are also reported for the latter case. Using different data modalities, we demonstrated a use case for the PokeFlex dataset in online 3D mesh reconstruction. We refer the reader to our website ( https://pokeflex-dataset.github.io/ ) for demos and examples of our dataset.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Obrist, Jan and Zamora, Miguel and Zheng, Hehui and Hinchet, Ronan and Ozdemir, Firat and Zarate, Juan and Katzschmann, Robert K. and Coros, Stelian},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07688},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_forcemimic_2024,
	title = {{ForceMimic}: {Force}-{Centric} {Imitation} {Learning} with {Force}-{Motion} {Capture} {System} for {Contact}-{Rich} {Manipulation}},
	shorttitle = {{ForceMimic}},
	url = {http://arxiv.org/abs/2410.07554},
	doi = {10.48550/arXiv.2410.07554},
	abstract = {In most contact-rich manipulation tasks, humans apply time-varying forces to the target object, compensating for inaccuracies in the vision-guided hand trajectory. However, current robot learning algorithms primarily focus on trajectory-based policy, with limited attention given to learning force-related skills. To address this limitation, we introduce ForceMimic, a force-centric robot learning system, providing a natural, force-aware and robot-free robotic demonstration collection system, along with a hybrid force-motion imitation learning algorithm for robust contact-rich manipulation. Using the proposed ForceCapture system, an operator can peel a zucchini in 5 minutes, while force-feedback teleoperation takes over 13 minutes and struggles with task completion. With the collected data, we propose HybridIL to train a force-centric imitation learning model, equipped with hybrid force-position control primitive to fit the predicted wrench-position parameters during robot execution. Experiments demonstrate that our approach enables the model to learn a more robust policy under the contact-rich task of vegetable peeling, increasing the success rates by 54.5\% relatively compared to state-of-the-art pure-vision-based imitation learning. Hardware, code, data and more results would be open-sourced on the project website at https://forcemimic.github.io.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Liu, Wenhai and Wang, Junbo and Wang, Yiming and Wang, Weiming and Lu, Cewu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07554},
	keywords = {Computer Science - Robotics},
}

@inproceedings{hampali_honnotate_2020,
	title = {{HOnnotate}: {A} {Method} for {3D} {Annotation} of {Hand} and {Object} {Poses}},
	shorttitle = {{HOnnotate}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.html},
	urldate = {2024-11-10},
	author = {Hampali, Shreyas and Rad, Mahdi and Oberweger, Markus and Lepetit, Vincent},
	year = {2020},
	pages = {3196--3206},
}

@misc{zhang_hirt_2024,
	title = {{HiRT}: {Enhancing} {Robotic} {Control} with {Hierarchical} {Robot} {Transformers}},
	shorttitle = {{HiRT}},
	url = {http://arxiv.org/abs/2410.05273},
	doi = {10.48550/arXiv.2410.05273},
	abstract = {Large Vision-Language-Action (VLA) models, leveraging powerful pre trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes HiRT, a Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, in static tasks, we double the control frequency and achieve comparable success rates. Additionally, on novel real-world dynamic ma nipulation tasks which are challenging for previous VLA models, HiRT improves the success rate from 48\% to 75\%.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhang, Jianke and Guo, Yanjiang and Chen, Xiaoyu and Wang, Yen-Jen and Hu, Yucheng and Shi, Chengming and Chen, Jianyu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05273},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{lin_omnipose6d_2024,
	title = {{OmniPose6D}: {Towards} {Short}-{Term} {Object} {Pose} {Tracking} in {Dynamic} {Scenes} from {Monocular} {RGB}},
	shorttitle = {{OmniPose6D}},
	url = {http://arxiv.org/abs/2410.06694},
	doi = {10.48550/arXiv.2410.06694},
	abstract = {To address the challenge of short-term object pose tracking in dynamic environments with monocular RGB input, we introduce a large-scale synthetic dataset OmniPose6D, crafted to mirror the diversity of real-world conditions. We additionally present a benchmarking framework for a comprehensive comparison of pose tracking algorithms. We propose a pipeline featuring an uncertainty-aware keypoint refinement network, employing probabilistic modeling to refine pose estimation. Comparative evaluations demonstrate that our approach achieves performance superior to existing baselines on real datasets, underscoring the effectiveness of our synthetic dataset and refinement technique in enhancing tracking precision in dynamic contexts. Our contributions set a new precedent for the development and assessment of object pose tracking methodologies in complex scenes.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Lin, Yunzhi and Zhao, Yipu and Chu, Fu-Jen and Chen, Xingyu and Wang, Weiyao and Tang, Hao and Vela, Patricio A. and Feiszli, Matt and Liang, Kevin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06694},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{bucker_grounding_2024,
	title = {Grounding {Robot} {Policies} with {Visuomotor} {Language} {Guidance}},
	url = {http://arxiv.org/abs/2410.06473},
	doi = {10.48550/arXiv.2410.06473},
	abstract = {Recent advances in the fields of natural language processing and computer vision have shown great potential in understanding the underlying dynamics of the world from large-scale internet data. However, translating this knowledge into robotic systems remains an open challenge, given the scarcity of human-robot interactions and the lack of large-scale datasets of real-world robotic data. Previous robot learning approaches such as behavior cloning and reinforcement learning have shown great capabilities in learning robotic skills from human demonstrations or from scratch in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for new settings. Aiming to address these limitations, we propose an agent-based framework for grounding robot policies to the current context, considering the constraints of a current robot and its environment using visuomotor-grounded language guidance. The proposed framework is composed of a set of conversational agents designed for specific roles -- namely, high-level advisor, visual grounding, monitoring, and robotic agents. Given a base policy, the agents collectively generate guidance at run time to shift the action distribution of the base policy towards more desirable future states. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates both in simulation and in real-world experiments without the need for additional human demonstrations or extensive exploration. Project videos at https://sites.google.com/view/motorcortex/home.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Bucker, Arthur and Ortega-Kral, Pablo and Francis, Jonathan and Oh, Jean},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06473},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{ravan_combining_2024,
	title = {Combining {Planning} and {Diffusion} for {Mobility} with {Unknown} {Dynamics}},
	url = {http://arxiv.org/abs/2410.06911},
	doi = {10.48550/arXiv.2410.06911},
	abstract = {Manipulation of large objects over long horizons (such as carts in a warehouse) is an essential skill for deployable robotic systems. Large objects require mobile manipulation which involves simultaneous manipulation, navigation, and movement with the object in tow. In many real-world situations, object dynamics are incredibly complex, such as the interaction of an office chair (with a rotating base and five caster wheels) and the ground. We present a hierarchical algorithm for long-horizon robot manipulation problems in which the dynamics are partially unknown. We observe that diffusion-based behavior cloning is highly effective for short-horizon problems with unknown dynamics, so we decompose the problem into an abstract high-level, obstacle-aware motion-planning problem that produces a waypoint sequence. We use a short-horizon, relative-motion diffusion policy to achieve the waypoints in sequence. We train mobile manipulation policies on a Spot robot that has to push and pull an office chair. Our hierarchical manipulation policy performs consistently better, especially when the horizon increases, compared to a diffusion policy trained on long-horizon demonstrations or motion planning assuming a rigidly-attached object (success rate of 8 (versus 0 and 5 respectively) out of 10 runs). Importantly, our learned policy generalizes to new layouts, grasps, chairs, and flooring that induces more friction, without any further training, showing promise for other complex mobile manipulation problems. Project Page: https://yravan.github.io/plannerorderedpolicy/},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Ravan, Yajvan and Yang, Zhutian and Chen, Tao and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06911},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{saanum_next_2024,
	title = {Next state prediction gives rise to entangled, yet compositional representations of objects},
	url = {http://arxiv.org/abs/2410.04940},
	doi = {10.48550/arXiv.2410.04940},
	abstract = {Compositional representations are thought to enable humans to generalize across combinatorially vast state spaces. Models with learnable object slots, which encode information about objects in separate latent codes, have shown promise for this type of generalization but rely on strong architectural priors. Models with distributed representations, on the other hand, use overlapping, potentially entangled neural codes, and their ability to support compositional generalization remains underexplored. In this paper we examine whether distributed models can develop linearly separable representations of objects, like slotted models, through unsupervised training on videos of object interactions. We show that, surprisingly, models with distributed representations often match or outperform models with object slots in downstream prediction tasks. Furthermore, we find that linearly separable object representations can emerge without object-centric priors, with auxiliary objectives like next-state prediction playing a key role. Finally, we observe that distributed models' object representations are never fully disentangled, even if they are linearly separable: Multiple objects can be encoded through partially overlapping neural populations while still being highly separable with a linear classifier. We hypothesize that maintaining partially shared codes enables distributed models to better compress object dynamics, potentially enhancing generalization.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Saanum, Tankred and Buschoff, Luca M. Schulze and Dayan, Peter and Schulz, Eric},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04940},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{strong_next_2024,
	title = {Next {Best} {Sense}: {Guiding} {Vision} and {Touch} with {FisherRF} for {3D} {Gaussian} {Splatting}},
	shorttitle = {Next {Best} {Sense}},
	url = {http://arxiv.org/abs/2410.04680},
	doi = {10.48550/arXiv.2410.04680},
	abstract = {We propose a framework for active next best view and touch selection for robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to represent scenes in a both photorealistic and geometrically accurate manner. However, in real-world, online robotic scenes where the number of views is limited given efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping and redundant. We address this issue by proposing an end-to-end online training and active view selection pipeline, which enhances the performance of 3DGS in few-view robotics settings. We first elevate the performance of few-shot 3DGS with a novel semantic depth alignment method using Segment Anything Model 2 (SAM2) that we supplement with Pearson depth and surface normal loss to improve color and depth reconstruction of real-world scenes. We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based on depth uncertainty. We perform online view selection on a real robot system during live 3DGS training. We motivate our improvements to few-shot GS scenes, and extend depth-based FisherRF to them, where we demonstrate both qualitative and quantitative improvements on challenging robot scenes. For more information, please see our project page at https://arm.stanford.edu/next-best-sense.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Strong, Matthew and Lei, Boshu and Swann, Aiden and Jiang, Wen and Daniilidis, Kostas and III, Monroe Kennedy},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04680},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_unsupervised_2024,
	title = {Unsupervised {Prior} {Learning}: {Discovering} {Categorical} {Pose} {Priors} from {Videos}},
	shorttitle = {Unsupervised {Prior} {Learning}},
	url = {http://arxiv.org/abs/2410.03858},
	doi = {10.48550/arXiv.2410.03858},
	abstract = {A prior represents a set of beliefs or assumptions about a system, aiding inference and decision-making. In this work, we introduce the challenge of unsupervised prior learning in pose estimation, where AI models learn pose priors of animate objects from videos in a self-supervised manner. These videos present objects performing various actions, providing crucial information about their keypoints and connectivity. While priors are effective in pose estimation, acquiring them can be difficult. We propose a novel method, named Pose Prior Learner (PPL), to learn general pose priors applicable to any object category. PPL uses a hierarchical memory to store compositional parts of prototypical poses, from which we distill a general pose prior. This prior enhances pose estimation accuracy through template transformation and image reconstruction. PPL learns meaningful pose priors without any additional human annotations or interventions, outperforming competitive baselines on both human and animal pose estimation datasets. Notably, our experimental results reveal the effectiveness of PPL using learnt priors for pose estimation on occluded images. Through iterative inference, PPL leverages priors to refine estimated poses, regressing them to any prototypical poses stored in memory. Our code, model, and data will be publicly available.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Wang, Ziyu and Han, Shuangpeng and Shou, Mike Zheng and Zhang, Mengmi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03858},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jansonnie_unsupervised_2024,
	title = {Unsupervised {Skill} {Discovery} for {Robotic} {Manipulation} through {Automatic} {Task} {Generation}},
	url = {http://arxiv.org/abs/2410.04855},
	doi = {10.48550/arXiv.2410.04855},
	abstract = {Learning skills that interact with objects is of major importance for robotic manipulation. These skills can indeed serve as an efficient prior for solving various manipulation tasks. We propose a novel Skill Learning approach that discovers composable behaviors by solving a large and diverse number of autonomously generated tasks. Our method learns skills allowing the robot to consistently and robustly interact with objects in its environment. The discovered behaviors are embedded in primitives which can be composed with Hierarchical Reinforcement Learning to solve unseen manipulation tasks. In particular, we leverage Asymmetric Self-Play to discover behaviors and Multiplicative Compositional Policies to embed them. We compare our method to Skill Learning baselines and find that our skills are more interactive. Furthermore, the learned skills can be used to solve a set of unseen manipulation tasks, in simulation as well as on a real robotic platform.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Jansonnie, Paul and Wu, Bingbing and Perez, Julien and Peters, Jan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.04855},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	volume = {34},
	shorttitle = {Decision {Transformer}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/7f489f642a0ddb10272b5c31057f0663-Abstract.html},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	urldate = {2024-11-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	year = {2021},
	pages = {15084--15097},
}

@misc{zhang_autoregressive_2024,
	title = {Autoregressive {Action} {Sequence} {Learning} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2410.03132},
	doi = {10.48550/arXiv.2410.03132},
	abstract = {Autoregressive models have demonstrated remarkable success in natural language processing. In this work, we design a simple yet effective autoregressive architecture for robotic manipulation tasks. We propose the Chunking Causal Transformer (CCT), which extends the next-single-token prediction of causal transformers to support multi-token prediction in a single pass. Further, we design a novel attention interleaving strategy that allows CCT to be trained efficiently with teacher-forcing. Based on CCT, we propose the Autoregressive Policy (ARP) model, which learns to generate action sequences autoregressively. We find that action sequence learning enables better leverage of the underlying causal relationships in robotic tasks. We evaluate ARP across diverse robotic manipulation environments, including Push-T, ALOHA, and RLBench, and show that it outperforms the state-of-the-art methods in all tested environments, while being more efficient in computation and parameter sizes. Video demonstrations, our source code, and the models of ARP can be found at http://github.com/mlzxy/arp.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhang, Xinyu and Liu, Yuhan and Chang, Haonan and Schramm, Liam and Boularias, Abdeslam},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03132},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yi_estimating_2024,
	title = {Estimating {Body} and {Hand} {Motion} in an {Ego}-sensed {World}},
	url = {http://arxiv.org/abs/2410.03665},
	doi = {10.48550/arXiv.2410.03665},
	abstract = {We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture the wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18\%. We also show how the bodies estimated by our system can improve the hands: the resulting kinematic and temporal constraints result in over 40\% lower hand estimation errors compared to noisy monocular estimates. Project page: https://egoallo.github.io/},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Yi, Brent and Ye, Vickie and Zheng, Maya and Müller, Lea and Pavlakos, Georgios and Ma, Yi and Malik, Jitendra and Kanazawa, Angjoo},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03665},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hirose_lelan_2024,
	title = {{LeLaN}: {Learning} {A} {Language}-{Conditioned} {Navigation} {Policy} from {In}-the-{Wild} {Videos}},
	shorttitle = {{LeLaN}},
	url = {http://arxiv.org/abs/2410.03603},
	doi = {10.48550/arXiv.2410.03603},
	abstract = {The world is filled with a wide variety of objects. For robots to be useful, they need the ability to find arbitrary objects described by people. In this paper, we present LeLaN(Learning Language-conditioned Navigation policy), a novel approach that consumes unlabeled, action-free egocentric data to learn scalable, language-conditioned object navigation. Our framework, LeLaN leverages the semantic knowledge of large vision-language models, as well as robotic foundation models, to label in-the-wild data from a variety of indoor and outdoor environments. We label over 130 hours of data collected in real-world indoor and outdoor environments, including robot observations, YouTube video tours, and human walking data. Extensive experiments with over 1000 real-world trials show that our approach enables training a policy from unlabeled action-free videos that outperforms state-of-the-art robot navigation methods, while being capable of inference at 4 times their speed on edge compute. We open-source our models, datasets and provide supplementary videos on our project page (https://learning-language-navigation.github.io/).},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Hirose, Noriaki and Glossop, Catherine and Sridhar, Ajay and Shah, Dhruv and Mees, Oier and Levine, Sergey},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03603},
	keywords = {Computer Science - Robotics},
}

@misc{xie_gap-rl_2024,
	title = {{GAP}-{RL}: {Grasps} {As} {Points} for {RL} {Towards} {Dynamic} {Object} {Grasping}},
	shorttitle = {{GAP}-{RL}},
	url = {http://arxiv.org/abs/2410.03509},
	doi = {10.48550/arXiv.2410.03509},
	abstract = {Dynamic grasping of moving objects in complex, continuous motion scenarios remains challenging. Reinforcement Learning (RL) has been applied in various robotic manipulation tasks, benefiting from its closed-loop property. However, existing RL-based methods do not fully explore the potential for enhancing visual representations. In this letter, we propose a novel framework called Grasps As Points for RL (GAP-RL) to effectively and reliably grasp moving objects. By implementing a fast region-based grasp detector, we build a Grasp Encoder by transforming 6D grasp poses into Gaussian points and extracting grasp features as a higher-level abstraction than the original object point features. Additionally, we develop a Graspable Region Explorer for real-world deployment, which searches for consistent graspable regions, enabling smoother grasp generation and stable policy execution. To assess the performance fairly, we construct a simulated dynamic grasping benchmark involving objects with various complex motions. Experiment results demonstrate that our method effectively generalizes to novel objects and unseen dynamic motions compared to other baselines. Real-world experiments further validate the framework's sim-to-real transferability.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Xie, Pengwei and Chen, Siang and Chen, Qianrun and Tang, Wei and Hu, Dingchang and Dai, Yixiang and Chen, Rui and Wang, Guijin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03509},
	keywords = {Computer Science - Robotics},
}

@misc{wagener_online_2019,
	title = {An {Online} {Learning} {Approach} to {Model} {Predictive} {Control}},
	url = {http://arxiv.org/abs/1902.08967},
	doi = {10.48550/arXiv.1902.08967},
	abstract = {Model predictive control (MPC) is a powerful technique for solving dynamic control tasks. In this paper, we show that there exists a close connection between MPC and online learning, an abstract theoretical framework for analyzing online decision making in the optimization literature. This new perspective provides a foundation for leveraging powerful online learning algorithms to design MPC algorithms. Specifically, we propose a new algorithm based on dynamic mirror descent (DMD), an online learning algorithm that is designed for non-stationary setups. Our algorithm, Dynamic Mirror Descent Model Predictive Control (DMD-MPC), represents a general family of MPC algorithms that includes many existing techniques as special instances. DMD-MPC also provides a fresh perspective on previous heuristics used in MPC and suggests a principled way to design new MPC algorithms. In the experimental section of this paper, we demonstrate the flexibility of DMD-MPC, presenting a set of new MPC algorithms on a simple simulated cartpole and a simulated and real-world aggressive driving task. Videos of the real-world experiments can be found at https://youtu.be/vZST3v0\_S9w and https://youtu.be/MhuqiHo2t98.},
	urldate = {2024-11-09},
	publisher = {arXiv},
	author = {Wagener, Nolan and Cheng, Ching-An and Sacks, Jacob and Boots, Byron},
	month = oct,
	year = {2019},
	note = {arXiv:1902.08967},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{jiang_capturing_2024,
	title = {Capturing forceful interaction with deformable objects using a deep learning-powered stretchable tactile array},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-53654-y},
	doi = {10.1038/s41467-024-53654-y},
	abstract = {Capturing forceful interaction with deformable objects during manipulation benefits applications like virtual reality, telemedicine, and robotics. Replicating full hand-object states with complete geometry is challenging because of the occluded object deformations. Here, we report a visual-tactile recording and tracking system for manipulation featuring a stretchable tactile glove with 1152 force-sensing channels and a visual-tactile joint learning framework to estimate dynamic hand-object states during manipulation. To overcome the strain interference caused by contact with deformable objects, an active suppression method based on symmetric response detection and adaptive calibration is proposed and achieves 97.6\% accuracy in force measurement, contributing to an improvement of 45.3\%. The learning framework processes the visual-tactile sequence and reconstructs hand-object states. We experiment on 24 objects from 6 categories including both deformable and rigid ones with an average reconstruction error of 1.8 cm for all sequences, demonstrating a universal ability to replicate human knowledge in manipulating objects with varying degrees of deformability.},
	language = {en},
	number = {1},
	urldate = {2024-11-08},
	journal = {Nature Communications},
	author = {Jiang, Chunpeng and Xu, Wenqiang and Li, Yutong and Yu, Zhenjun and Wang, Longchun and Hu, Xiaotong and Xie, Zhengyi and Liu, Qingkun and Yang, Bin and Wang, Xiaolin and Du, Wenxin and Tang, Tutian and Zheng, Dongzhe and Yao, Siqiong and Lu, Cewu and Liu, Jingquan},
	month = nov,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Electrical and electronic engineering},
	pages = {9513},
}

@misc{elawady_relic_2024,
	title = {{ReLIC}: {A} {Recipe} for 64k {Steps} of {In}-{Context} {Reinforcement} {Learning} for {Embodied} {AI}},
	shorttitle = {{ReLIC}},
	url = {http://arxiv.org/abs/2410.02751},
	doi = {10.48550/arXiv.2410.02751},
	abstract = {Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called "partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Elawady, Ahmad and Chhablani, Gunjan and Ramrakhya, Ram and Yadav, Karmesh and Batra, Dhruv and Kira, Zsolt and Szot, Andrew},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02751},
	keywords = {Computer Science - Machine Learning},
}

@misc{shahidzadeh_feelanyforce_2024,
	title = {{FeelAnyForce}: {Estimating} {Contact} {Force} {Feedback} from {Tactile} {Sensation} for {Vision}-{Based} {Tactile} {Sensors}},
	shorttitle = {{FeelAnyForce}},
	url = {http://arxiv.org/abs/2410.02048},
	doi = {10.48550/arXiv.2410.02048},
	abstract = {In this paper, we tackle the problem of estimating 3D contact forces using vision-based tactile sensors. In particular, our goal is to estimate contact forces over a large range (up to 15 N) on any objects while generalizing across different vision-based tactile sensors. Thus, we collected a dataset of over 200K indentations using a robotic arm that pressed various indenters onto a GelSight Mini sensor mounted on a force sensor and then used the data to train a multi-head transformer for force regression. Strong generalization is achieved via accurate data collection and multi-objective optimization that leverages depth contact images. Despite being trained only on primitive shapes and textures, the regressor achieves a mean absolute error of 4{\textbackslash}\% on a dataset of unseen real-world objects. We further evaluate our approach's generalization capability to other GelSight mini and DIGIT sensors, and propose a reproducible calibration procedure for adapting the pre-trained model to other vision-based sensors. Furthermore, the method was evaluated on real-world tasks, including weighing objects and controlling the deformation of delicate objects, which relies on accurate force feedback. Project webpage: http://prg.cs.umd.edu/FeelAnyForce},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Shahidzadeh, Amir-Hossein and Caddeo, Gabriele and Alapati, Koushik and Natale, Lorenzo and Fermüller, Cornelia and Aloimonos, Yiannis},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02048},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yang_guiding_2024,
	title = {Guiding {Long}-{Horizon} {Task} and {Motion} {Planning} with {Vision} {Language} {Models}},
	url = {http://arxiv.org/abs/2410.02193},
	doi = {10.48550/arXiv.2410.02193},
	abstract = {Vision-Language Models (VLM) can generate plausible high-level plans when prompted with a goal, the context, an image of the scene, and any planning constraints. However, there is no guarantee that the predicted actions are geometrically and kinematically feasible for a particular robot embodiment. As a result, many prerequisite steps such as opening drawers to access objects are often omitted in their plans. Robot task and motion planners can generate motion trajectories that respect the geometric feasibility of actions and insert physically necessary actions, but do not scale to everyday problems that require common-sense knowledge and involve large state spaces comprised of many variables. We propose VLM-TAMP, a hierarchical planning algorithm that leverages a VLM to generate goth semantically-meaningful and horizon-reducing intermediate subgoals that guide a task and motion planner. When a subgoal or action cannot be refined, the VLM is queried again for replanning. We evaluate VLM- TAMP on kitchen tasks where a robot must accomplish cooking goals that require performing 30-50 actions in sequence and interacting with up to 21 objects. VLM-TAMP substantially outperforms baselines that rigidly and independently execute VLM-generated action sequences, both in terms of success rates (50 to 100\% versus 0\%) and average task completion percentage (72 to 100\% versus 15 to 45\%). See project site https://zt-yang.github.io/vlm-tamp-robot/ for more information.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Yang, Zhutian and Garrett, Caelan and Fox, Dieter and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02193},
	keywords = {Computer Science - Robotics},
}

@misc{huang_efficient_2024,
	title = {Efficient {Residual} {Learning} with {Mixture}-of-{Experts} for {Universal} {Dexterous} {Grasping}},
	url = {http://arxiv.org/abs/2410.02475},
	doi = {10.48550/arXiv.2410.02475},
	abstract = {Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. To overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-unaware base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping. ResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8\% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Huang, Ziye and Yuan, Haoqi and Fu, Yuhui and Lu, Zongqing},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02475},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_learning_2024,
	title = {Learning {Diverse} {Bimanual} {Dexterous} {Manipulation} {Skills} from {Human} {Demonstrations}},
	url = {http://arxiv.org/abs/2410.02477},
	doi = {10.48550/arXiv.2410.02477},
	abstract = {Bimanual dexterous manipulation is a critical yet underexplored area in robotics. Its high-dimensional action space and inherent task complexity present significant challenges for policy learning, and the limited task diversity in existing benchmarks hinders general-purpose skill development. Existing approaches largely depend on reinforcement learning, often constrained by intricately designed reward functions tailored to a narrow set of tasks. In this work, we present a novel approach for efficiently learning diverse bimanual dexterous skills from abundant human demonstrations. Specifically, we introduce BiDexHD, a framework that unifies task construction from existing bimanual datasets and employs teacher-student policy learning to address all tasks. The teacher learns state-based policies using a general two-stage reward function across tasks with shared behaviors, while the student distills the learned multi-task policies into a vision-based policy. With BiDexHD, scalable learning of numerous bimanual dexterous skills from auto-constructed tasks becomes feasible, offering promising advances toward universal bimanual dexterous manipulation. Our empirical evaluation on the TACO dataset, spanning 141 tasks across six categories, demonstrates a task fulfillment rate of 74.59\% on trained tasks and 51.07\% on unseen tasks, showcasing the effectiveness and competitive zero-shot generalization capabilities of BiDexHD. For videos and more information, visit our project page https://sites.google.com/view/bidexhd.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Zhou, Bohan and Yuan, Haoqi and Fu, Yuhui and Lu, Zongqing},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02477},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{st-aubin_single-shot_2024,
	title = {Single-{Shot} {Learning} of {Stable} {Dynamical} {Systems} for {Long}-{Horizon} {Manipulation} {Tasks}},
	url = {http://arxiv.org/abs/2410.01033},
	doi = {10.48550/arXiv.2410.01033},
	abstract = {Mastering complex sequential tasks continues to pose a significant challenge in robotics. While there has been progress in learning long-horizon manipulation tasks, most existing approaches lack rigorous mathematical guarantees for ensuring reliable and successful execution. In this paper, we extend previous work on learning long-horizon tasks and stable policies, focusing on improving task success rates while reducing the amount of training data needed. Our approach introduces a novel method that (1) segments long-horizon demonstrations into discrete steps defined by waypoints and subgoals, and (2) learns globally stable dynamical system policies to guide the robot to each subgoal, even in the face of sensory noise and random disturbances. We validate our approach through both simulation and real-world experiments, demonstrating effective transfer from simulation to physical robotic platforms. Code is available at https://github.com/Alestaubin/stable-imitation-policy-with-waypoints},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {St-Aubin, Alexandre and Abyaneh, Amin and Lin, Hsiu-Chin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01033},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{garcia_towards_2024,
	title = {Towards {Generalizable} {Vision}-{Language} {Robotic} {Manipulation}: {A} {Benchmark} and {LLM}-guided {3D} {Policy}},
	shorttitle = {Towards {Generalizable} {Vision}-{Language} {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2410.01345},
	doi = {10.48550/arXiv.2410.01345},
	abstract = {Generalizing language-conditioned robotic policies to new tasks remains a significant challenge, hampered by the lack of suitable simulation benchmarks. In this paper, we address this gap by introducing GemBench, a novel benchmark to assess generalization capabilities of vision-language robotic manipulation policies. GemBench incorporates seven general action primitives and four levels of generalization, spanning novel placements, rigid and articulated objects, and complex long-horizon tasks. We evaluate state-of-the-art approaches on GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich 3D information for action prediction conditioned on language. While 3D-LOTUS excels in both efficiency and performance on seen tasks, it struggles with novel tasks. To address this, we present 3D-LOTUS++, a framework that integrates 3D-LOTUS's motion planning capabilities with the task planning capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++ achieves state-of-the-art performance on novel tasks of GemBench, setting a new standard for generalization in robotic manipulation. The benchmark, codes and trained models are available at {\textbackslash}url\{https://www.di.ens.fr/willow/research/gembench/\}.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Garcia, Ricardo and Chen, Shizhe and Schmid, Cordelia},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01345},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wu_one-shot_2024,
	title = {One-{Shot} {Robust} {Imitation} {Learning} for {Long}-{Horizon} {Visuomotor} {Tasks} from {Unsegmented} {Demonstrations}},
	url = {http://arxiv.org/abs/2410.01630},
	doi = {10.48550/arXiv.2410.01630},
	abstract = {In contrast to single-skill tasks, long-horizon tasks play a crucial role in our daily life, e.g., a pouring task requires a proper concatenation of reaching, grasping and pouring subtasks. As an efficient solution for transferring human skills to robots, imitation learning has achieved great progress over the last two decades. However, when learning long-horizon visuomotor skills, imitation learning often demands a large amount of semantically segmented demonstrations. Moreover, the performance of imitation learning could be susceptible to external perturbation and visual occlusion. In this paper, we exploit dynamical movement primitives and meta-learning to provide a new framework for imitation learning, called Meta-Imitation Learning with Adaptive Dynamical Primitives (MiLa). MiLa allows for learning unsegmented long-horizon demonstrations and adapting to unseen tasks with a single demonstration. MiLa can also resist external disturbances and visual occlusion during task execution. Real-world robotic experiments demonstrate the superiority of MiLa, irrespective of visual occlusion and random perturbations on robots.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Wu, Shaokang and Wang, Yijin and Huang, Yanlong},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01630},
	keywords = {Computer Science - Robotics},
}

@misc{wei_dr_2024,
	title = {D({R}, {O}) {Grasp}: {A} {Unified} {Representation} of {Robot} and {Object} {Interaction} for {Cross}-{Embodiment} {Dexterous} {Grasping}},
	shorttitle = {D({R}, {O}) {Grasp}},
	url = {http://arxiv.org/abs/2410.01702},
	doi = {10.48550/arXiv.2410.01702},
	abstract = {Dexterous grasping is a fundamental yet challenging skill in robotic manipulation, requiring precise interaction between robotic hands and objects. In this paper, we present D(R,O) Grasp, a novel framework that models the interaction between the robotic hand in its grasping pose and the object, enabling broad generalization across various robot hands and object geometries. Our model takes the robot hand's description and object point cloud as inputs and efficiently predicts kinematically valid and stable grasps, demonstrating strong adaptability to diverse robot embodiments and object geometries. Extensive experiments conducted in both simulated and real-world environments validate the effectiveness of our approach, with significant improvements in success rate, grasp diversity, and inference speed across multiple robotic hands. Our method achieves an average success rate of 87.53\% in simulation in less than one second, tested across three different dexterous robotic hands. In real-world experiments using the LeapHand, the method also demonstrates an average success rate of 89\%. D(R,O) Grasp provides a robust solution for dexterous grasping in complex and varied environments. The code, appendix, and videos are available on our project website at https://nus-lins-lab.github.io/drograspweb/.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Wei, Zhenyu and Xu, Zhixuan and Guo, Jingxiang and Hou, Yiwen and Gao, Chongkai and Cai, Zhehao and Luo, Jiayu and Shao, Lin},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01702},
	keywords = {Computer Science - Robotics},
}

@misc{cheng_scaling_2024,
	title = {Scaling {Offline} {Model}-{Based} {RL} via {Jointly}-{Optimized} {World}-{Action} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/2410.00564},
	doi = {10.48550/arXiv.2410.00564},
	abstract = {A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9\% human-level performance on pretrained games using only 10\% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6\% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization. We will release codes and model weights at https://github.com/CJReinforce/JOWA.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Cheng, Jie and Qiao, Ruixi and Xiong, Gang and Miao, Qinghai and Ma, Yingwei and Li, Binhua and Li, Yongbin and Lv, Yisheng},
	month = oct,
	year = {2024},
	note = {arXiv:2410.00564},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{roy_m2distill_2024,
	title = {{M2Distill}: {Multi}-{Modal} {Distillation} for {Lifelong} {Imitation} {Learning}},
	shorttitle = {{M2Distill}},
	url = {http://arxiv.org/abs/2410.00064},
	doi = {10.48550/arXiv.2410.00064},
	abstract = {Lifelong imitation learning for manipulation tasks poses significant challenges due to distribution shifts that occur in incremental learning steps. Existing methods often focus on unsupervised skill discovery to construct an ever-growing skill library or distillation from multiple policies, which can lead to scalability issues as diverse manipulation tasks are continually introduced and may fail to ensure a consistent latent space throughout the learning process, leading to catastrophic forgetting of previously learned skills. In this paper, we introduce M2Distill, a multi-modal distillation-based method for lifelong imitation learning focusing on preserving consistent latent space across vision, language, and action distributions throughout the learning process. By regulating the shifts in latent representations across different modalities from previous to current steps, and reducing discrepancies in Gaussian Mixture Model (GMM) policies between consecutive learning steps, we ensure that the learned policy retains its ability to perform previously learned tasks while seamlessly integrating new skills. Extensive evaluations on the LIBERO lifelong imitation learning benchmark suites, including LIBERO-OBJECT, LIBERO-GOAL, and LIBERO-SPATIAL, demonstrate that our method consistently outperforms prior state-of-the-art methods across all evaluated metrics.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Roy, Kaushik and Dissanayake, Akila and Tidd, Brendan and Moghadam, Peyman},
	month = oct,
	year = {2024},
	note = {arXiv:2410.00064},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{tao_maniskill3_2024,
	title = {{ManiSkill3}: {GPU} {Parallelized} {Robotics} {Simulation} and {Rendering} for {Generalizable} {Embodied} {AI}},
	shorttitle = {{ManiSkill3}},
	url = {http://arxiv.org/abs/2410.00425},
	doi = {10.48550/arXiv.2410.00425},
	abstract = {Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Tao, Stone and Xiang, Fanbo and Shukla, Arth and Qin, Yuzhe and Hinrichsen, Xander and Yuan, Xiaodi and Bao, Chen and Lin, Xinsong and Liu, Yulin and Chan, Tse-kai and Gao, Yuan and Li, Xuanlin and Mu, Tongzhou and Xiao, Nan and Gurha, Arnav and Huang, Zhiao and Calandra, Roberto and Chen, Rui and Luo, Shan and Su, Hao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.00425},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{kumar_constraining_2024,
	title = {Constraining {Gaussian} {Process} {Implicit} {Surfaces} for {Robot} {Manipulation} via {Dataset} {Refinement}},
	url = {http://arxiv.org/abs/2410.00157},
	doi = {10.48550/arXiv.2410.00157},
	abstract = {Model-based control faces fundamental challenges in partially-observable environments due to unmodeled obstacles. We propose an online learning and optimization method to identify and avoid unobserved obstacles online. Our method, Constraint Obeying Gaussian Implicit Surfaces (COGIS), infers contact data using a combination of visual input and state tracking, informed by predictions from a nominal dynamics model. We then fit a Gaussian process implicit surface (GPIS) to these data and refine the dataset through a novel method of enforcing constraints on the estimated surface. This allows us to design a Model Predictive Control (MPC) method that leverages the obstacle estimate to complete multiple manipulation tasks. By modeling the environment instead of attempting to directly adapt the dynamics, our method succeeds at both low-dimensional peg-in-hole tasks and high-dimensional deformable object manipulation tasks. Our method succeeds in 10/10 trials vs 1/10 for a baseline on a real-world cable manipulation task under partial observability of the environment.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Kumar, Abhinav and Mitrano, Peter and Berenson, Dmitry},
	month = sep,
	year = {2024},
	note = {arXiv:2410.00157},
	keywords = {Computer Science - Robotics},
}

@inproceedings{nagabandi_deep_2020,
	title = {Deep {Dynamics} {Models} for {Learning} {Dexterous} {Manipulation}},
	url = {https://proceedings.mlr.press/v100/nagabandi20a.html},
	abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in on-line model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills – and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/.},
	language = {en},
	urldate = {2024-11-08},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1101--1112},
}

@inproceedings{xu_dexterous_2023,
	title = {Dexterous {Manipulation} from {Images}: {Autonomous} {Real}-{World} {RL} via {Substep} {Guidance}},
	shorttitle = {Dexterous {Manipulation} from {Images}},
	url = {https://ieeexplore.ieee.org/abstract/document/10161493},
	doi = {10.1109/ICRA48891.2023.10161493},
	abstract = {Complex and contact-rich robotic manipulation tasks, particularly those that involve multi-fingered hands and underactuated object manipulation, present a significant challenge to any control method. Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions. However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering. This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide. In this paper, we describe a system for vision-based dexterous manipulation that provides a “programming-free” approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction. The core principle under-lying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allows a robot to not only learn a task efficiently but also to autonomously practice. Our system includes a framework for users to define a final task and intermediate sub-tasks with image examples, a reinforcement learning procedure that learns the task autonomously without interventions, and experimental results with a four-finger robotic hand learning multi-stage object manipulation tasks directly in the real world, without simulation, manual modeling, or reward engineering.},
	urldate = {2024-11-08},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Xu, Kelvin and Hu, Zheyuan and Doshi, Ria and Rovinsky, Aaron and Kumar, Vikash and Gupta, Abhishek and Levine, Sergey},
	month = may,
	year = {2023},
	keywords = {Automation, Data collection, Education, Manuals, Reinforcement learning, Robots, Task analysis},
	pages = {5938--5945},
}

@misc{yu_uniaff_2024,
	title = {{UniAff}: {A} {Unified} {Representation} of {Affordances} for {Tool} {Usage} and {Articulation} with {Vision}-{Language} {Models}},
	shorttitle = {{UniAff}},
	url = {http://arxiv.org/abs/2409.20551},
	doi = {10.48550/arXiv.2409.20551},
	abstract = {Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in a unified formulation. Specifically, we constructed a dataset labeled with manipulation-related key attributes, comprising 900 articulated objects from 19 categories and 600 tools from 12 categories. Furthermore, we leverage MLLMs to infer object-centric representations for manipulation tasks, including affordance recognition and reasoning about 3D motion constraints. Comprehensive experiments in both simulation and real-world settings indicate that UniAff significantly improves the generalization of robotic manipulation for tools and articulated objects. We hope that UniAff will serve as a general baseline for unified robotic manipulation tasks in the future. Images, videos, dataset, and code are published on the project website at:https://sites.google.com/view/uni-aff/home},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Yu, Qiaojun and Huang, Siyuan and Yuan, Xibin and Jiang, Zhengkai and Hao, Ce and Li, Xin and Chang, Haonan and Wang, Junbo and Liu, Liu and Li, Hongsheng and Gao, Peng and Lu, Cewu},
	month = sep,
	year = {2024},
	note = {arXiv:2409.20551},
	keywords = {Computer Science - Robotics},
}

@misc{liu_wildfusion_2024,
	title = {{WildFusion}: {Multimodal} {Implicit} {3D} {Reconstructions} in the {Wild}},
	shorttitle = {{WildFusion}},
	url = {http://arxiv.org/abs/2409.19904},
	doi = {10.48550/arXiv.2409.19904},
	abstract = {We propose WildFusion, a novel approach for 3D scene reconstruction in unstructured, in-the-wild environments using multimodal implicit neural representations. WildFusion integrates signals from LiDAR, RGB camera, contact microphones, tactile sensors, and IMU. This multimodal fusion generates comprehensive, continuous environmental representations, including pixel-level geometry, color, semantics, and traversability. Through real-world experiments on legged robot navigation in challenging forest environments, WildFusion demonstrates improved route selection by accurately predicting traversability. Our results highlight its potential to advance robotic navigation and 3D mapping in complex outdoor terrains.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Liu, Yanbaihui and Chen, Boyuan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.19904},
	keywords = {Computer Science - Multimedia, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{mendonca_continuously_2024,
	title = {Continuously {Improving} {Mobile} {Manipulation} with {Autonomous} {Real}-{World} {RL}},
	url = {http://arxiv.org/abs/2409.20568},
	doi = {10.48550/arXiv.2409.20568},
	abstract = {We present a fully autonomous real-world RL framework for mobile manipulation that can learn policies without extensive instrumentation or human supervision. This is enabled by 1) task-relevant autonomy, which guides exploration towards object interactions and prevents stagnation near goal states, 2) efficient policy learning by leveraging basic task knowledge in behavior priors, and 3) formulating generic rewards that combine human-interpretable semantic information with low-level, fine-grained observations. We demonstrate that our approach allows Spot robots to continually improve their performance on a set of four challenging mobile manipulation tasks, obtaining an average success rate of 80\% across tasks, a 3-4 improvement over existing approaches. Videos can be found at https://continual-mobile-manip.github.io/},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Mendonca, Russell and Panov, Emmanuel and Bucher, Bernadette and Wang, Jiuguang and Pathak, Deepak},
	month = sep,
	year = {2024},
	note = {arXiv:2409.20568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{wang_scaling_2024,
	title = {Scaling {Proprioceptive}-{Visual} {Learning} with {Heterogeneous} {Pre}-trained {Transformers}},
	url = {http://arxiv.org/abs/2409.20537},
	doi = {10.48550/arXiv.2409.20537},
	abstract = {One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20\% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Wang, Lirui and Chen, Xinlei and Zhao, Jialiang and He, Kaiming},
	month = sep,
	year = {2024},
	note = {arXiv:2409.20537},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kang_how_2024,
	title = {How {Far} is {Video} {Generation} from {World} {Model}: {A} {Physical} {Law} {Perspective}},
	shorttitle = {How {Far} is {Video} {Generation} from {World} {Model}},
	url = {http://arxiv.org/abs/2411.02385},
	doi = {10.48550/arXiv.2411.02385},
	abstract = {OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color {\textgreater} size {\textgreater} velocity {\textgreater} shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Kang, Bingyi and Yue, Yang and Lu, Rui and Lin, Zhijie and Zhao, Yang and Wang, Kaixin and Huang, Gao and Feng, Jiashi},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02385},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_does_2024,
	title = {Does {End}-to-{End} {Autonomous} {Driving} {Really} {Need} {Perception} {Tasks}?},
	url = {http://arxiv.org/abs/2409.18341},
	doi = {10.48550/arXiv.2409.18341},
	abstract = {End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module that employs a Bird's-Eye View (BEV) world model, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves state-of-the-art planning performance on the nuScenes dataset, demonstrating a 27.2{\textbackslash}\% relative reduction in L2 error and a 51.6{\textbackslash}\% decrease in collision rate to the leading E2EAD method, UniAD. Moreover, SSR offers a 10.9\${\textbackslash}times\$ faster inference speed and 13\${\textbackslash}times\$ faster training time. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment. Code will be released at {\textbackslash}url\{https://github.com/PeidongLi/SSR\}.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Li, Peidong and Cui, Dixiao},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18341},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{saleem_pomdp-based_2024,
	title = {A {POMDP}-based hierarchical planning framework for manipulation under pose uncertainty},
	url = {http://arxiv.org/abs/2409.18775},
	doi = {10.48550/arXiv.2409.18775},
	abstract = {Robots often face challenges in domestic environments where visual feedback is ineffective, such as retrieving objects obstructed by occlusions or finding a light switch in the dark. In these cases, utilizing contacts to localize the target object can be effective. We propose an online planning framework using binary contact signals for manipulation tasks with pose uncertainty, formulated as a Partially Observable Markov Decision Process (POMDP). Naively representing the belief as a particle set makes planning infeasible due to the large uncertainties in domestic settings, as identifying the best sequence of actions requires rolling out thousands of actions across millions of particles, taking significant compute time. To address this, we propose a hierarchical belief representation. Initially, we represent the uncertainty coarsely in a 3D volumetric space. Policies that refine uncertainty in this space are computed and executed, and once uncertainty is sufficiently reduced, the problem is translated back into the particle space for further refinement before task completion. We utilize a closed-loop planning and execution framework with a heuristic-search-based anytime solver that computes partial policies within a limited time budget. The performance of the framework is demonstrated both in real world and in simulation on the high-precision task of inserting a plug into a port using a UR10e manipulator, resolving positional uncertainties up to 50 centimeters and angular uncertainties close to \$2{\textbackslash}pi\$. Experimental results highlight the framework's effectiveness, achieving a 93{\textbackslash}\% success rate in the real world and over 50{\textbackslash}\% improvement in solution quality compared to greedy baselines, significantly accelerating planning and enabling real-time solutions for complex problems.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Saleem, Muhammad Suhail and Veerapaneni, Rishi and Likhachev, Maxim},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18775},
	keywords = {Computer Science - Robotics},
}

@misc{ortiz_dmc-vb_2024,
	title = {{DMC}-{VB}: {A} {Benchmark} for {Representation} {Learning} for {Control} with {Visual} {Distractors}},
	shorttitle = {{DMC}-{VB}},
	url = {http://arxiv.org/abs/2409.18330},
	doi = {10.48550/arXiv.2409.18330},
	abstract = {Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint. In this paper, we present theDeepMind Control Visual Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at: https://github.com/google-deepmind/dmc\_vision\_benchmark.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Ortiz, Joseph and Dedieu, Antoine and Lehrach, Wolfgang and Guntupalli, Swaroop and Wendelken, Carter and Humayun, Ahmad and Zhou, Guangyao and Swaminathan, Sivaramakrishnan and Lázaro-Gredilla, Miguel and Murphy, Kevin},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18330},
	keywords = {Computer Science - Machine Learning},
}

@misc{liang_visarl_2024,
	title = {{ViSaRL}: {Visual} {Reinforcement} {Learning} {Guided} by {Human} {Saliency}},
	shorttitle = {{ViSaRL}},
	url = {http://arxiv.org/abs/2403.10940},
	doi = {10.48550/arXiv.2403.10940},
	abstract = {Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-robot tasks compared to the baseline which does not use saliency.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Liang, Anthony and Thomason, Jesse and Bıyık, Erdem},
	month = oct,
	year = {2024},
	note = {arXiv:2403.10940},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhuang_flatnfold_2024,
	title = {Flat'n'{Fold}: {A} {Diverse} {Multi}-{Modal} {Dataset} for {Garment} {Perception} and {Manipulation}},
	shorttitle = {Flat'n'{Fold}},
	url = {http://arxiv.org/abs/2409.18297},
	doi = {10.48550/arXiv.2409.18297},
	abstract = {We present Flat'n'Fold, a novel large-scale dataset for garment manipulation that addresses critical gaps in existing datasets. Comprising 1,212 human and 887 robot demonstrations of flattening and folding 44 unique garments across 8 categories, Flat'n'Fold surpasses prior datasets in size, scope, and diversity. Our dataset uniquely captures the entire manipulation process from crumpled to folded states, providing synchronized multi-view RGB-D images, point clouds, and action data, including hand or gripper positions and rotations. We quantify the dataset's diversity and complexity compared to existing benchmarks and show that our dataset features natural and diverse manipulations of real-world demonstrations of human and robot demonstrations in terms of visual and action information. To showcase Flat'n'Fold's utility, we establish new benchmarks for grasping point prediction and subtask decomposition. Our evaluation of state-of-the-art models on these tasks reveals significant room for improvement. This underscores Flat'n'Fold's potential to drive advances in robotic perception and manipulation of deformable objects. Our dataset can be downloaded at https://cvas-ug.github.io/flat-n-fold},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Zhuang, Lipeng and Fan, Shiyu and Ru, Yingdong and Audonnet, Florent and Henderson, Paul and Aragon-Camarasa, Gerardo},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18297},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yao_safe_2024,
	title = {Safe {Leaf} {Manipulation} for {Accurate} {Shape} and {Pose} {Estimation} of {Occluded} {Fruits}},
	url = {http://arxiv.org/abs/2409.17389},
	doi = {10.48550/arXiv.2409.17389},
	abstract = {Fruit monitoring plays an important role in crop management, and rising global fruit consumption combined with labor shortages necessitates automated monitoring with robots. However, occlusions from plant foliage often hinder accurate shape and pose estimation. Therefore, we propose an active fruit shape and pose estimation method that physically manipulates occluding leaves to reveal hidden fruits. This paper introduces a framework that plans robot actions to maximize visibility and minimize leaf damage. We developed a novel scene-consistent shape completion technique to improve fruit estimation under heavy occlusion and utilize a perception-driven deformation graph model to predict leaf deformation during planning. Experiments on artificial and real sweet pepper plants demonstrate that our method enables robots to safely move leaves aside, exposing fruits for accurate shape and pose estimation, outperforming baseline methods. Project page: https://shaoxiongyao.github.io/lmap-ssc/.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Yao, Shaoxiong and Pan, Sicong and Bennewitz, Maren and Hauser, Kris},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17389},
	keywords = {Computer Science - Robotics},
}

@misc{li_stable_2024,
	title = {Stable {Object} {Placement} {Under} {Geometric} {Uncertainty} via {Differentiable} {Contact} {Dynamics}},
	url = {http://arxiv.org/abs/2409.17725},
	doi = {10.48550/arXiv.2409.17725},
	abstract = {From serving a cup of coffee to carefully rearranging delicate items, stable object placement is a crucial skill for future robots. This skill is challenging due to the required accuracy, which is difficult to achieve under geometric uncertainty. We leverage differentiable contact dynamics to develop a principled method for stable object placement under geometric uncertainty. We estimate the geometric uncertainty by minimizing the discrepancy between the force-torque sensor readings and the model predictions through gradient descent. We further keep track of a belief over multiple possible geometric parameters to mitigate the gradient-based method's sensitivity to the initialization. We verify our approach in the real world on various geometric uncertainties, including the in-hand pose uncertainty of the grasped object, the object's shape uncertainty, and the environment's shape uncertainty.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Li, Linfeng and Yang, Gang and Shao, Lin and Hsu, David},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17725},
	keywords = {Computer Science - Robotics},
}

@misc{liu_leveraging_2024,
	title = {Leveraging {Semantic} and {Geometric} {Information} for {Zero}-{Shot} {Robot}-to-{Human} {Handover}},
	url = {http://arxiv.org/abs/2409.17621},
	doi = {10.48550/arXiv.2409.17621},
	abstract = {Human-robot interaction (HRI) encompasses a wide range of collaborative tasks, with handover being one of the most fundamental. As robots become more integrated into human environments, the potential for service robots to assist in handing objects to humans is increasingly promising. In robot-to-human (R2H) handover, selecting the optimal grasp is crucial for success, as it requires avoiding interference with the humans preferred grasp region and minimizing intrusion into their workspace. Existing methods either inadequately consider geometric information or rely on data-driven approaches, which often struggle to generalize across diverse objects. To address these limitations, we propose a novel zero-shot system that combines semantic and geometric information to generate optimal handover grasps. Our method first identifies grasp regions using semantic knowledge from vision-language models (VLMs) and, by incorporating customized visual prompts, achieves finer granularity in region grounding. A grasp is then selected based on grasp distance and approach angle to maximize human ease and avoid interference. We validate our approach through ablation studies and real-world comparison experiments. Results demonstrate that our system improves handover success rates and provides a more user-preferred interaction experience. Videos, appendixes and more are available at https://sites.google.com/view/vlm-handover/.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Liu, Jiangshan and Dong, Wenlong and Wang, Jiankun and Meng, Max Q.-H.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17621},
	keywords = {Computer Science - Robotics},
}

@misc{chuang_active_2024,
	title = {Active {Vision} {Might} {Be} {All} {You} {Need}: {Exploring} {Active} {Vision} in {Bimanual} {Robotic} {Manipulation}},
	shorttitle = {Active {Vision} {Might} {Be} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2409.17435},
	doi = {10.48550/arXiv.2409.17435},
	abstract = {Imitation learning has demonstrated significant potential in performing high-precision manipulation tasks using visual feedback from cameras. However, it is common practice in imitation learning for cameras to be fixed in place, resulting in issues like occlusion and limited field of view. Furthermore, cameras are often placed in broad, general locations, without an effective viewpoint specific to the robot's task. In this work, we investigate the utility of active vision (AV) for imitation learning and manipulation, in which, in addition to the manipulation policy, the robot learns an AV policy from human demonstrations to dynamically change the robot's camera viewpoint to obtain better information about its environment and the given task. We introduce AV-ALOHA, a new bimanual teleoperation robot system with AV, an extension of the ALOHA 2 robot system, incorporating an additional 7-DoF robot arm that only carries a stereo camera and is solely tasked with finding the best viewpoint. This camera streams stereo video to an operator wearing a virtual reality (VR) headset, allowing the operator to control the camera pose using head and body movements. The system provides an immersive teleoperation experience, with bimanual first-person control, enabling the operator to dynamically explore and search the scene and simultaneously interact with the environment. We conduct imitation learning experiments of our system both in real-world and in simulation, across a variety of tasks that emphasize viewpoint planning. Our results demonstrate the effectiveness of human-guided AV for imitation learning, showing significant improvements over fixed cameras in tasks with limited visibility. Project website: https://soltanilara.github.io/av-aloha/},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Chuang, Ian and Lee, Andrew and Gao, Dechen and Soltani, Iman},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17435},
	keywords = {Computer Science - Robotics},
}

@misc{wu_canonical_2024,
	title = {Canonical {Representation} and {Force}-{Based} {Pretraining} of {3D} {Tactile} for {Dexterous} {Visuo}-{Tactile} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2409.17549},
	doi = {10.48550/arXiv.2409.17549},
	abstract = {Tactile sensing plays a vital role in enabling robots to perform fine-grained, contact-rich tasks. However, the high dimensionality of tactile data, due to the large coverage on dexterous hands, poses significant challenges for effective tactile feature learning, especially for 3D tactile data, as there are no large standardized datasets and no strong pretrained backbones. To address these challenges, we propose a novel canonical representation that reduces the difficulty of 3D tactile feature learning and further introduces a force-based self-supervised pretraining task to capture both local and net force features, which are crucial for dexterous manipulation. Our method achieves an average success rate of 78\% across four fine-grained, contact-rich dexterous manipulation tasks in real-world experiments, demonstrating effectiveness and robustness compared to other methods. Further analysis shows that our method fully utilizes both spatial and force information from 3D tactile data to accomplish the tasks. The videos can be viewed at https://3dtacdex.github.io.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Wu, Tianhao and Li, Jinzhou and Zhang, Jiyao and Wu, Mingdong and Dong, Hao},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17549},
	keywords = {Computer Science - Robotics},
}

@misc{kerr_robot_2024,
	title = {Robot {See} {Robot} {Do}: {Imitating} {Articulated} {Object} {Manipulation} with {Monocular} {4D} {Reconstruction}},
	shorttitle = {Robot {See} {Robot} {Do}},
	url = {http://arxiv.org/abs/2409.18121},
	doi = {10.48550/arXiv.2409.18121},
	abstract = {Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feature fields in an iterative optimization which enables the use of geometric regularizers to recover 3D motions from only a single video. Given this 4D reconstruction, the robot replicates object trajectories by planning bimanual arm motions that induce the demonstrated object part motion. By representing demonstrations as part-centric trajectories, RSRD focuses on replicating the demonstration's intended behavior while considering the robot's own morphological limits, rather than attempting to reproduce the hand's motion. We evaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part trajectories and RSRD's physical execution performance on 9 objects across 10 trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of 87\% success rate, for a total end-to-end success rate of 60\% across 90 trials. Notably, this is accomplished using only feature fields distilled from large pretrained vision models -- without any task-specific training, fine-tuning, dataset collection, or annotation. Project page: https://robot-see-robot-do.github.io},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Kerr, Justin and Kim, Chung Min and Wu, Mingxuan and Yi, Brent and Wang, Qianqian and Goldberg, Ken and Kanazawa, Angjoo},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18121},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{obrist_pokeflex_2024,
	title = {{PokeFlex}: {Towards} a {Real}-{World} {Dataset} of {Deformable} {Objects} for {Robotic} {Manipulation}},
	shorttitle = {{PokeFlex}},
	url = {http://arxiv.org/abs/2409.17124},
	doi = {10.48550/arXiv.2409.17124},
	abstract = {Advancing robotic manipulation of deformable objects can enable automation of repetitive tasks across multiple industries, from food processing to textiles and healthcare. Yet robots struggle with the high dimensionality of deformable objects and their complex dynamics. While data-driven methods have shown potential for solving manipulation tasks, their application in the domain of deformable objects has been constrained by the lack of data. To address this, we propose PokeFlex, a pilot dataset featuring real-world 3D mesh data of actively deformed objects, together with the corresponding forces and torques applied by a robotic arm, using a simple poking strategy. Deformations are captured with a professional volumetric capture system that allows for complete 360-degree reconstruction. The PokeFlex dataset consists of five deformable objects with varying stiffness and shapes. Additionally, we leverage the PokeFlex dataset to train a vision model for online 3D mesh reconstruction from a single image and a template mesh. We refer readers to the supplementary material and to our website ( https://pokeflex-dataset.github.io/ ) for demos and examples of our dataset.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Obrist, Jan and Zamora, Miguel and Zheng, Hehui and Zarate, Juan and Katzschmann, Robert K. and Coros, Stelian},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17124},
	keywords = {Computer Science - Robotics},
}

@misc{morales_versatile_2024,
	title = {A {Versatile} and {Differentiable} {Hand}-{Object} {Interaction} {Representation}},
	url = {http://arxiv.org/abs/2409.16855},
	doi = {10.48550/arXiv.2409.16855},
	abstract = {Synthesizing accurate hands-object interactions (HOI) is critical for applications in Computer Vision, Augmented Reality (AR), and Mixed Reality (MR). Despite recent advances, the accuracy of reconstructed or generated HOI leaves room for refinement. Some techniques have improved the accuracy of dense correspondences by shifting focus from generating explicit contacts to using rich HOI fields. Still, they lack full differentiability or continuity and are tailored to specific tasks. In contrast, we present a Coarse Hand-Object Interaction Representation (CHOIR), a novel, versatile and fully differentiable field for HOI modelling. CHOIR leverages discrete unsigned distances for continuous shape and pose encoding, alongside multivariate Gaussian distributions to represent dense contact maps with few parameters. To demonstrate the versatility of CHOIR we design JointDiffusion, a diffusion model to learn a grasp distribution conditioned on noisy hand-object interactions or only object geometries, for both refinement and synthesis applications. We demonstrate JointDiffusion's improvements over the SOTA in both applications: it increases the contact F1 score by \$5{\textbackslash}\%\$ for refinement and decreases the sim. displacement by \$46{\textbackslash}\%\$ for synthesis. Our experiments show that JointDiffusion with CHOIR yield superior contact accuracy and physical realism compared to SOTA methods designed for specific tasks. Our models and code will be publicly available to the research community.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Morales, Théo and Taheri, Omid and Lacey, Gerard},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16855},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_synchronize_2024,
	title = {Synchronize {Dual} {Hands} for {Physics}-{Based} {Dexterous} {Guitar} {Playing}},
	url = {http://arxiv.org/abs/2409.16629},
	doi = {10.48550/arXiv.2409.16629},
	abstract = {We present a novel approach to synthesize dexterous motions for physically simulated hands in tasks that require coordination between the control of two hands with high temporal precision. Instead of directly learning a joint policy to control two hands, our approach performs bimanual control through cooperative learning where each hand is treated as an individual agent. The individual policies for each hand are first trained separately, and then synchronized through latent space manipulation in a centralized environment to serve as a joint policy for two-hand control. By doing so, we avoid directly performing policy learning in the joint state-action space of two hands with higher dimensions, greatly improving the overall training efficiency. We demonstrate the effectiveness of our proposed approach in the challenging guitar-playing task. The virtual guitarist trained by our approach can synthesize motions from unstructured reference data of general guitar-playing practice motions, and accurately play diverse rhythms with complex chord pressing and string picking patterns based on the input guitar tabs that do not exist in the references. Along with this paper, we provide the motion capture data that we collected as the reference for policy training. Code is available at: https://pei-xu.github.io/guitar.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Xu, Pei and Wang, Ruocheng},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16629},
	keywords = {Computer Science - Graphics},
}

@misc{lai_world_2024,
	title = {World {Model}-based {Perception} for {Visual} {Legged} {Locomotion}},
	url = {http://arxiv.org/abs/2409.16784},
	doi = {10.48550/arXiv.2409.16784},
	abstract = {Legged locomotion over various terrains is challenging and requires precise perception of the robot and its surroundings from both proprioception and vision. However, learning directly from high-dimensional visual input is often data-inefficient and intricate. To address this issue, traditional methods attempt to learn a teacher policy with access to privileged information first and then learn a student policy to imitate the teacher's behavior with visual input. Despite some progress, this imitation framework prevents the student policy from achieving optimal performance due to the information gap between inputs. Furthermore, the learning process is unnatural since animals intuitively learn to traverse different terrains based on their understanding of the world without privileged knowledge. Inspired by this natural ability, we propose a simple yet effective method, World Model-based Perception (WMP), which builds a world model of the environment and learns a policy based on the world model. We illustrate that though completely trained in simulation, the world model can make accurate predictions of real-world trajectories, thus providing informative signals for the policy controller. Extensive simulated and real-world experiments demonstrate that WMP outperforms state-of-the-art baselines in traversability and robustness. Videos and Code are available at: https://wmp-loco.github.io/.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Lai, Hang and Cao, Jiahang and Xu, Jiafeng and Wu, Hongtao and Lin, Yunfeng and Kong, Tao and Yu, Yong and Zhang, Weinan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16784},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{sun_hierarchical_2024,
	title = {Hierarchical {Hybrid} {Learning} for {Long}-{Horizon} {Contact}-{Rich} {Robotic} {Assembly}},
	url = {http://arxiv.org/abs/2409.16451},
	doi = {10.48550/arXiv.2409.16451},
	abstract = {Generalizable long-horizon robotic assembly requires reasoning at multiple levels of abstraction. End-to-end imitation learning (IL) has been proven a promising approach, but it requires a large amount of demonstration data for training and often fails to meet the high-precision requirement of assembly tasks. Reinforcement Learning (RL) approaches have succeeded in high-precision assembly tasks, but suffer from sample inefficiency and hence, are less competent at long-horizon tasks. To address these challenges, we propose a hierarchical modular approach, named ARCH (Adaptive Robotic Composition Hierarchy), which enables long-horizon high-precision assembly in contact-rich settings. ARCH employs a hierarchical planning framework, including a low-level primitive library of continuously parameterized skills and a high-level policy. The low-level primitive library includes essential skills for assembly tasks, such as grasping and inserting. These primitives consist of both RL and model-based controllers. The high-level policy, learned via imitation learning from a handful of demonstrations, selects the appropriate primitive skills and instantiates them with continuous input parameters. We extensively evaluate our approach on a real robot manipulation platform. We show that while trained on a single task, ARCH generalizes well to unseen tasks and outperforms baseline methods in terms of success rate and data efficiency. Videos can be found at https://long-horizon-assembly.github.io.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Sun, Jiankai and Curtis, Aidan and You, Yang and Xu, Yan and Koehle, Michael and Guibas, Leonidas and Chitta, Sachin and Schwager, Mac and Li, Hui},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16451},
	keywords = {Computer Science - Robotics},
}

@misc{hsu_kinscene_2024,
	title = {{KinScene}: {Model}-{Based} {Mobile} {Manipulation} of {Articulated} {Scenes}},
	shorttitle = {{KinScene}},
	url = {http://arxiv.org/abs/2409.16473},
	doi = {10.48550/arXiv.2409.16473},
	abstract = {Sequentially interacting with articulated objects is crucial for a mobile manipulator to operate effectively in everyday environments. To enable long-horizon tasks involving articulated objects, this study explores building scene-level articulation models for indoor scenes through autonomous exploration. While previous research has studied mobile manipulation with articulated objects by considering object kinematic constraints, it primarily focuses on individual-object scenarios and lacks extension to a scene-level context for task-level planning. To manipulate multiple object parts sequentially, the robot needs to reason about the resultant motion of each part and anticipate its impact on future actions. We introduce KinScene, a full-stack approach for long-horizon manipulation tasks with articulated objects. The robot maps the scene, detects and physically interacts with articulated objects, collects observations, and infers the articulation properties. For sequential tasks, the robot plans a feasible series of object interactions based on the inferred articulation model. We demonstrate that our approach repeatably constructs accurate scene-level kinematic and geometric models, enabling long-horizon mobile manipulation in a real-world scene. Code and additional results are available at https://chengchunhsu.github.io/KinScene/},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Hsu, Cheng-Chun and Abbatematteo, Ben and Jiang, Zhenyu and Zhu, Yuke and Martín-Martín, Roberto and Biswas, Joydeep},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16473},
	keywords = {Computer Science - Robotics},
}

@misc{rath_xmop_2024,
	title = {{XMoP}: {Whole}-{Body} {Control} {Policy} for {Zero}-shot {Cross}-{Embodiment} {Neural} {Motion} {Planning}},
	shorttitle = {{XMoP}},
	url = {http://arxiv.org/abs/2409.15585},
	doi = {10.48550/arXiv.2409.15585},
	abstract = {Classical manipulator motion planners work across different robot embodiments. However they plan on a pre-specified static environment representation, and are not scalable to unseen dynamic environments. Neural Motion Planners (NMPs) are an appealing alternative to conventional planners as they incorporate different environmental constraints to learn motion policies directly from raw sensor observations. Contemporary state-of-the-art NMPs can successfully plan across different environments. However none of the existing NMPs generalize across robot embodiments. In this paper we propose Cross-Embodiment Motion Policy (XMoP), a neural policy for learning to plan over a distribution of manipulators. XMoP implicitly learns to satisfy kinematic constraints for a distribution of robots and \${\textbackslash}textit\{zero-shot\}\$ transfers the planning behavior to unseen robotic manipulators within this distribution. We achieve this generalization by formulating a whole-body control policy that is trained on planning demonstrations from over three million procedurally sampled robotic manipulators in different simulated environments. Despite being completely trained on synthetic embodiments and environments, our policy exhibits strong sim-to-real generalization across manipulators with different kinematic variations and degrees of freedom with a single set of frozen policy parameters. We evaluate XMoP on \$7\$ commercial manipulators and show successful cross-embodiment motion planning, achieving an average \$70{\textbackslash}\%\$ success rate on baseline benchmarks. Furthermore, we demonstrate our policy sim-to-real on two unseen manipulators solving novel planning problems across three real-world domains even with dynamic obstacles.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Rath, Prabin Kumar and Gopalan, Nakul},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15585},
	keywords = {Computer Science - Robotics},
}

@misc{huang_match_2024,
	title = {{MATCH} {POLICY}: {A} {Simple} {Pipeline} from {Point} {Cloud} {Registration} to {Manipulation} {Policies}},
	shorttitle = {{MATCH} {POLICY}},
	url = {http://arxiv.org/abs/2409.15517},
	doi = {10.48550/arXiv.2409.15517},
	abstract = {Many manipulation tasks require the robot to rearrange objects relative to one another. Such tasks can be described as a sequence of relative poses between parts of a set of rigid bodies. In this work, we propose MATCH POLICY, a simple but novel pipeline for solving high-precision pick and place tasks. Instead of predicting actions directly, our method registers the pick and place targets to the stored demonstrations. This transfers action inference into a point cloud registration task and enables us to realize nontrivial manipulation policies without any training. MATCH POLICY is designed to solve high-precision tasks with a key-frame setting. By leveraging the geometric interaction and the symmetries of the task, it achieves extremely high sample efficiency and generalizability to unseen configurations. We demonstrate its state-of-the-art performance across various tasks on RLBench benchmark compared with several strong baselines and test it on a real robot with six tasks.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Huang, Haojie and Liu, Haotian and Wang, Dian and Walters, Robin and Platt, Robert},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15517},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_articulated_2024,
	title = {Articulated {Object} {Manipulation} using {Online} {Axis} {Estimation} with {SAM2}-{Based} {Tracking}},
	url = {http://arxiv.org/abs/2409.16287},
	doi = {10.48550/arXiv.2409.16287},
	abstract = {Articulated object manipulation requires precise object interaction, where the object's axis must be carefully considered. Previous research employed interactive perception for manipulating articulated objects, but typically, open-loop approaches often suffer from overlooking the interaction dynamics. To address this limitation, we present a closed-loop pipeline integrating interactive perception with online axis estimation from segmented 3D point clouds. Our method leverages any interactive perception technique as a foundation for interactive perception, inducing slight object movement to generate point cloud frames of the evolving dynamic scene. These point clouds are then segmented using Segment Anything Model 2 (SAM2), after which the moving part of the object is masked for accurate motion online axis estimation, guiding subsequent robotic actions. Our approach significantly enhances the precision and efficiency of manipulation tasks involving articulated objects. Experiments in simulated environments demonstrate that our method outperforms baseline approaches, especially in tasks that demand precise axis-based control. Project Page: https://hytidel.github.io/video-tracking-for-axis-estimation/.},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Wang, Xi and Chen, Tianxing and Yu, Qiaojun and Xu, Tianling and Chen, Zanxin and Fu, Yiting and Lu, Cewu and Mu, Yao and Luo, Ping},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16287},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{bharadhwaj_gen2act_2024,
	title = {{Gen2Act}: {Human} {Video} {Generation} in {Novel} {Scenarios} enables {Generalizable} {Robot} {Manipulation}},
	shorttitle = {{Gen2Act}},
	url = {http://arxiv.org/abs/2409.16283},
	doi = {10.48550/arXiv.2409.16283},
	abstract = {How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. Videos are at https://homangab.github.io/gen2act/},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Bharadhwaj, Homanga and Dwibedi, Debidatta and Gupta, Abhinav and Tulsiani, Shubham and Doersch, Carl and Xiao, Ted and Shah, Dhruv and Xia, Fei and Sadigh, Dorsa and Kirmani, Sean},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16283},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{brehmer_does_2024,
	title = {Does equivariance matter at scale?},
	url = {http://arxiv.org/abs/2410.23179},
	doi = {10.48550/arXiv.2410.23179},
	abstract = {Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Brehmer, Johann and Behrends, Sönke and Haan, Pim de and Cohen, Taco},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23179},
	keywords = {Computer Science - Machine Learning},
}

@misc{eyzaguirre_tactile_2024,
	title = {Tactile {Neural} {De}-rendering},
	url = {http://arxiv.org/abs/2409.13923},
	doi = {10.48550/arXiv.2409.13923},
	abstract = {Tactile sensing has proven to be an invaluable tool for enhancing robotic perception, particularly in scenarios where visual data is limited or unavailable. However, traditional methods for pose estimation using tactile data often rely on intricate modeling of sensor mechanics or estimation of contact patches, which can be cumbersome and inherently deterministic. In this work, we introduce Tactile Neural De-rendering, a novel approach that leverages a generative model to reconstruct a local 3D representation of an object based solely on its tactile signature. By rendering the object as though perceived by a virtual camera embedded at the fingertip, our method provides a more intuitive and flexible representation of the tactile data. This 3D reconstruction not only facilitates precise pose estimation but also allows for the quantification of uncertainty, providing a robust framework for tactile-based perception in robotics.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Eyzaguirre, Jose A. and Oller, Miquel and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13923},
	keywords = {Computer Science - Robotics},
}

@misc{li_tactile_2024,
	title = {Tactile {Functasets}: {Neural} {Implicit} {Representations} of {Tactile} {Datasets}},
	shorttitle = {Tactile {Functasets}},
	url = {http://arxiv.org/abs/2409.14592},
	doi = {10.48550/arXiv.2409.14592},
	abstract = {Modern incarnations of tactile sensors produce high-dimensional raw sensory feedback such as images, making it challenging to efficiently store, process, and generalize across sensors. To address these concerns, we introduce a novel implicit function representation for tactile sensor feedback. Rather than directly using raw tactile images, we propose neural implicit functions trained to reconstruct the tactile dataset, producing compact representations that capture the underlying structure of the sensory inputs. These representations offer several advantages over their raw counterparts: they are compact, enable probabilistically interpretable inference, and facilitate generalization across different sensors. We demonstrate the efficacy of this representation on the downstream task of in-hand object pose estimation, achieving improved performance over image-based methods while simplifying downstream models. We release code, demos and datasets at https://www.mmintlab.com/tactile-functasets.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Li, Sikai and Rodriguez, Samanta and Dou, Yiming and Owens, Andrew and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14592},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_dynamic_2024,
	title = {Dynamic {2D} {Gaussians}: {Geometrically} accurate radiance fields for dynamic objects},
	shorttitle = {Dynamic {2D} {Gaussians}},
	url = {http://arxiv.org/abs/2409.14072},
	doi = {10.48550/arXiv.2409.14072},
	abstract = {Reconstructing objects and extracting high-quality surfaces play a vital role in the real world. Current 4D representations show the ability to render high-quality novel views for dynamic objects but cannot reconstruct high-quality meshes due to their implicit or geometrically inaccurate representations. In this paper, we propose a novel representation that can reconstruct accurate meshes from sparse image input, named Dynamic 2D Gaussians (D-2DGS). We adopt 2D Gaussians for basic geometry representation and use sparse-controlled points to capture 2D Gaussian's deformation. By extracting the object mask from the rendered high-quality image and masking the rendered depth map, a high-quality dynamic mesh sequence of the object can be extracted. Experiments demonstrate that our D-2DGS is outstanding in reconstructing high-quality meshes from sparse input. More demos and code are available at https://github.com/hustvl/Dynamic-2DGS.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Zhang, Shuai and Wu, Guanjun and Wang, Xinggang and Feng, Bin and Liu, Wenyu},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14072},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{honerkamp_zero-cost_2024,
	title = {Zero-{Cost} {Whole}-{Body} {Teleoperation} for {Mobile} {Manipulation}},
	url = {http://arxiv.org/abs/2409.15095},
	doi = {10.48550/arXiv.2409.15095},
	abstract = {Demonstration data plays a key role in learning complex behaviors and training robotic foundation models. While effective control interfaces exist for static manipulators, data collection remains cumbersome and time intensive for mobile manipulators due to their large number of degrees of freedom. While specialized hardware, avatars, or motion tracking can enable whole-body control, these approaches are either expensive, robot-specific, or suffer from the embodiment mismatch between robot and human demonstrator. In this work, we present MoMa-Teleop, a novel teleoperation method that delegates the base motions to a reinforcement learning agent, leaving the operator to focus fully on the task-relevant end-effector motions. This enables whole-body teleoperation of mobile manipulators with zero additional hardware or setup costs via standard interfaces such as joysticks or hand guidance. Moreover, the operator is not bound to a tracked workspace and can move freely with the robot over spatially extended tasks. We demonstrate that our approach results in a significant reduction in task completion time across a variety of robots and tasks. As the generated data covers diverse whole-body motions without embodiment mismatch, it enables efficient imitation learning. By focusing on task-specific end-effector motions, our approach learns skills that transfer to unseen settings, such as new obstacles or changed object positions, from as little as five demonstrations. We make code and videos available at http://moma-teleop.cs.uni-freiburg.de.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Honerkamp, Daniel and Mahesheka, Harsh and Hartz, Jan Ole von and Welschehold, Tim and Valada, Abhinav},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15095},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{tang_kalie_2024,
	title = {{KALIE}: {Fine}-{Tuning} {Vision}-{Language} {Models} for {Open}-{World} {Manipulation} without {Robot} {Data}},
	shorttitle = {{KALIE}},
	url = {http://arxiv.org/abs/2409.14066},
	doi = {10.48550/arXiv.2409.14066},
	abstract = {Building generalist robotic systems involves effectively endowing robots with the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Tang, Grace and Rajkumar, Swetha and Zhou, Yifei and Walke, Homer Rich and Levine, Sergey and Fang, Kuan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14066},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_contact_2024,
	title = {Contact {Compliance} {Visuo}-{Proprioceptive} {Policy} for {Contact}-{Rich} {Manipulation} with {Cost}-{Efficient} {Haptic} {Hand}-{Arm} {Teleoperation} {System}},
	url = {http://arxiv.org/abs/2409.14440},
	doi = {10.48550/arXiv.2409.14440},
	abstract = {Learning robot manipulation skills in real-world environments is extremely challenging. Robots learning manipulation skills in real-world environments is extremely challenging. Recent research on imitation learning and visuomotor policies has significantly enhanced the ability of robots to perform manipulation tasks. In this paper, we propose Admit Policy, a visuo-proprioceptive imitation learning framework with force compliance, designed to reduce contact force fluctuations during robot execution of contact-rich manipulation tasks. This framework also includes a hand-arm teleoperation system with vibrotactile feedback for efficient data collection. Our framework utilizes RGB images, robot joint positions, and contact forces as observations and leverages a consistency-constrained teacher-student probabilistic diffusion model to generate future trajectories for end-effector positions and contact forces. An admittance model is then employed to track these trajectories, enabling effective force-position control across various tasks.We validated our framework on five challenging contact-rich manipulation tasks. Among these tasks, while improving success rates, our approach most significantly reduced the mean contact force required to complete the tasks by up to 53.92\% and decreased the standard deviation of contact force fluctuations by 76.51\% compared to imitation learning algorithms without dynamic contact force prediction and tracking.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Zhou, Bo and Jiao, Ruixuan and Li, Yi and Fang, Fang and Chen, Fu},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14440},
	keywords = {Computer Science - Robotics},
}

@misc{yi_visual-auditory_2024,
	title = {Visual-auditory {Extrinsic} {Contact} {Estimation}},
	url = {http://arxiv.org/abs/2409.14608},
	doi = {10.48550/arXiv.2409.14608},
	abstract = {Estimating contact locations between a grasped object and the environment is important for robust manipulation. In this paper, we present a visual-auditory method for extrinsic contact estimation, featuring a real-to-sim approach for auditory signals. Our method equips a robotic manipulator with contact microphones and speakers on its fingers, along with an externally mounted static camera providing a visual feed of the scene. As the robot manipulates objects, it detects contact events with surrounding surfaces using auditory feedback from the fingertips and visual feedback from the camera. A key feature of our approach is the transfer of auditory feedback into a simulated environment, where we learn a multimodal representation that is then applied to real world scenes without additional training. This zero-shot transfer is accurate and robust in estimating contact location and size, as demonstrated in our simulated and real world experiments in various cluttered environments.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Yi, Xili and Lee, Jayjun and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14608},
	keywords = {Computer Science - Robotics},
}

@misc{bogert_built_2024,
	title = {Built {Different}: {Tactile} {Perception} to {Overcome} {Cross}-{Embodiment} {Capability} {Differences} in {Collaborative} {Manipulation}},
	shorttitle = {Built {Different}},
	url = {http://arxiv.org/abs/2409.14896},
	doi = {10.48550/arXiv.2409.14896},
	abstract = {Tactile sensing is a powerful means of implicit communication between a human and a robot assistant. In this paper, we investigate how tactile sensing can transcend cross-embodiment differences across robotic systems in the context of collaborative manipulation. Consider tasks such as collaborative object carrying where the human-robot interaction is force rich. Learning and executing such skills requires the robot to comply to the human and to learn behaviors at the joint-torque level. However, most robots do not offer this compliance or provide access to their joint torques. To address this challenge, we present an approach that uses tactile sensors to transfer policies from robots with these capabilities to those without. We show how our method can enable a cooperative task where a robot and human must work together to maneuver objects through space. We first demonstrate the skill on an impedance control-capable robot equipped with tactile sensing, then show the positive transfer of the tactile policy to a planar prismatic robot that is only capable of position control and does not come equipped with any sort of force/torque feedback, yet is able to comply to the human motions only using tactile feedback. Further details and videos can be found on our project website at https://www.mmintlab.com/research/tactile-collaborative/.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Bogert, William van den and Iyengar, Madhavan and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14896},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{dang_bimanual_2024,
	title = {Bimanual {In}-hand {Manipulation} using {Dual} {Limit} {Surfaces}},
	url = {http://arxiv.org/abs/2409.14698},
	doi = {10.48550/arXiv.2409.14698},
	abstract = {In-hand object manipulation is an important capability for dexterous manipulation. In this paper, we introduce a modeling and planning framework for in-hand object reconfiguration, focusing on frictional patch contacts between the robot's palms (or fingers) and the object. Our approach leverages two cooperative patch contacts on either side of the object to iteratively reposition it within the robot's grasp by alternating between sliding and sticking motions. Unlike previous methods that rely on single-point contacts or restrictive assumptions on contact dynamics, our framework models the complex interaction of dual frictional patches, allowing for greater control over object motion. We develop a planning algorithm that computes feasible motions to reorient and re-grasp objects without causing unintended slippage. We demonstrate the effectiveness of our approach in simulation and real-world experiments, showing significant improvements in object stability and pose accuracy across various object geometries.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Dang, An and Lorenz, James and Yi, Xili and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14698},
	keywords = {Computer Science - Robotics},
}

@misc{chane-sane_soloparkour_2024,
	title = {{SoloParkour}: {Constrained} {Reinforcement} {Learning} for {Visual} {Locomotion} from {Privileged} {Experience}},
	shorttitle = {{SoloParkour}},
	url = {http://arxiv.org/abs/2409.13678},
	doi = {10.48550/arXiv.2409.13678},
	abstract = {Parkour poses a significant challenge for legged robots, requiring navigation through complex environments with agility and precision based on limited sensory inputs. In this work, we introduce a novel method for training end-to-end visual policies, from depth pixels to robot control commands, to achieve agile and safe quadruped locomotion. We formulate robot parkour as a constrained reinforcement learning (RL) problem designed to maximize the emergence of agile skills within the robot's physical limits while ensuring safety. We first train a policy without vision using privileged information about the robot's surroundings. We then generate experience from this privileged policy to warm-start a sample efficient off-policy RL algorithm from depth images. This allows the robot to adapt behaviors from this privileged experience to visual locomotion while circumventing the high computational costs of RL directly from pixels. We demonstrate the effectiveness of our method on a real Solo-12 robot, showcasing its capability to perform a variety of parkour skills such as walking, climbing, leaping, and crawling.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Chane-Sane, Elliot and Amigo, Joseph and Flayols, Thomas and Righetti, Ludovic and Mansard, Nicolas},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13678},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yamada_formula-supervised_2024,
	title = {Formula-{Supervised} {Visual}-{Geometric} {Pre}-training},
	url = {http://arxiv.org/abs/2409.13535},
	doi = {10.48550/arXiv.2409.13535},
	abstract = {Throughout the history of computer vision, while research has explored the integration of images (visual) and point clouds (geometric), many advancements in image and 3D object recognition have tended to process these modalities separately. We aim to bridge this divide by integrating images and point clouds on a unified transformer model. This approach integrates the modality-specific properties of images and point clouds and achieves fundamental downstream tasks in image and 3D object recognition on a unified transformer model by learning visual-geometric representations. In this work, we introduce Formula-Supervised Visual-Geometric Pre-training (FSVGP), a novel synthetic pre-training method that automatically generates aligned synthetic images and point clouds from mathematical formulas. Through cross-modality supervision, we enable supervised pre-training between visual and geometric modalities. FSVGP also reduces reliance on real data collection, cross-modality alignment, and human annotation. Our experimental results show that FSVGP pre-trains more effectively than VisualAtom and PC-FractalDB across six tasks: image and 3D object classification, detection, and segmentation. These achievements demonstrate FSVGP's superior generalization in image and 3D object recognition and underscore the potential of synthetic pre-training in visual-geometric representation learning. Our project website is available at https://ryosuke-yamada.github.io/fdsl-fsvgp/.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Yamada, Ryosuke and Hara, Kensho and Kataoka, Hirokatsu and Makihara, Koshi and Inoue, Nakamasa and Yokota, Rio and Satoh, Yutaka},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13535},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{paulius_bootstrapping_2024,
	title = {Bootstrapping {Object}-level {Planning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2409.12262},
	doi = {10.48550/arXiv.2409.12262},
	abstract = {We introduce a new method that extracts knowledge from a large language model (LLM) to produce object-level plans, which describe high-level changes to object state, and uses them to bootstrap task and motion planning (TAMP) in a hierarchical manner. Existing works use LLMs to either directly output task plans or to generate goals in representations like PDDL. However, these methods fall short because they either rely on the LLM to do the actual planning or output a hard-to-satisfy goal. Our approach instead extracts knowledge from a LLM in the form of plan schemas as an object level representation called functional object-oriented networks (FOON), from which we automatically generate PDDL subgoals. Our experiments demonstrate how our method's performance markedly exceeds alternative planning strategies across several tasks in simulation.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Paulius, David and Agostini, Alejandro and Quartey, Benedict and Konidaris, George},
	month = oct,
	year = {2024},
	note = {arXiv:2409.12262},
	keywords = {Computer Science - Robotics},
}

@misc{ngui_extended_2024,
	title = {Extended {Reality} {System} for {Robotic} {Learning} from {Human} {Demonstration}},
	url = {http://arxiv.org/abs/2409.12862},
	doi = {10.48550/arXiv.2409.12862},
	abstract = {Many real-world tasks are intuitive for a human to perform, but difficult to encode algorithmically when utilizing a robot to perform the tasks. In these scenarios, robotic systems can benefit from expert demonstrations to learn how to perform each task. In many settings, it may be difficult or unsafe to use a physical robot to provide these demonstrations, for example, considering cooking tasks such as slicing with a knife. Extended reality provides a natural setting for demonstrating robotic trajectories while bypassing safety concerns and providing a broader range of interaction modalities. We propose the Robot Action Demonstration in Extended Reality (RADER) system, a generic extended reality interface for learning from demonstration. We additionally present its application to an existing state-of-the-art learning from demonstration approach and show comparable results between demonstrations given on a physical robot and those given using our extended reality system.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Ngui, Isaac and McBeth, Courtney and He, Grace and Santos, André Corrêa and Soares, Luciano and Morales, Marco and Amato, Nancy M.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12862},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Robotics},
}

@misc{wen_tinyvla_2024,
	title = {{TinyVLA}: {Towards} {Fast}, {Data}-{Efficient} {Vision}-{Language}-{Action} {Models} for {Robotic} {Manipulation}},
	shorttitle = {{TinyVLA}},
	url = {http://arxiv.org/abs/2409.12514},
	doi = {10.48550/arXiv.2409.12514},
	abstract = {Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that {\textbackslash}methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and Tang, Jian},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12514},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{collins_shape-space_2024,
	title = {Shape-{Space} {Deformer}: {Unified} {Visuo}-{Tactile} {Representations} for {Robotic} {Manipulation} of {Deformable} {Objects}},
	shorttitle = {Shape-{Space} {Deformer}},
	url = {http://arxiv.org/abs/2409.12419},
	doi = {10.48550/arXiv.2409.12419},
	abstract = {Accurate modelling of object deformations is crucial for a wide range of robotic manipulation tasks, where interacting with soft or deformable objects is essential. Current methods struggle to generalise to unseen forces or adapt to new objects, limiting their utility in real-world applications. We propose Shape-Space Deformer, a unified representation for encoding a diverse range of object deformations using template augmentation to achieve robust, fine-grained reconstructions that are resilient to outliers and unwanted artefacts. Our method improves generalization to unseen forces and can rapidly adapt to novel objects, significantly outperforming existing approaches. We perform extensive experiments to test a range of force generalisation settings and evaluate our method's ability to reconstruct unseen deformations, demonstrating significant improvements in reconstruction accuracy and robustness. Our approach is suitable for real-time performance, making it ready for downstream manipulation applications.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Collins, Sean M. V. and Tidd, Brendan and Baktashmotlagh, Mahsa and Moghadam, Peyman},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12419},
	keywords = {Computer Science - Robotics},
}

@misc{wu_modex_2024,
	title = {{MoDex}: {Planning} {High}-{Dimensional} {Dexterous} {Control} via {Learning} {Neural} {Hand} {Models}},
	shorttitle = {{MoDex}},
	url = {http://arxiv.org/abs/2409.10983},
	doi = {10.48550/arXiv.2409.10983},
	abstract = {Controlling hands in the high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the human embodied cognition and reconsider dexterous hands as learnable systems. Specifically, we introduce MoDex, a framework which employs a neural hand model to capture the dynamical characteristics of hand movements. Based on the model, a bidirectional planning method is developed, which demonstrates efficiency in both training and inference. The method is further integrated with a large language model to generate various gestures such as ``Scissorshand" and ``Rock{\textbackslash}\&Roll." Moreover, we show that decomposing the system dynamics into a pretrained hand model and an external model improves data efficiency, as supported by both theoretical analysis and empirical experiments. Additional visualization results are available at https://tongwu19.github.io/MoDex.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Wu, Tong and Li, Shoujie and Lyu, Chuqiao and Sou, Kit-Wa and Chan, Wang-Sing and Ding, Wenbo},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10983},
	keywords = {Computer Science - Robotics},
}

@misc{hwang_motif_2024,
	title = {{MotIF}: {Motion} {Instruction} {Fine}-tuning},
	shorttitle = {{MotIF}},
	url = {http://arxiv.org/abs/2409.10683},
	doi = {10.48550/arXiv.2409.10683},
	abstract = {While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state - e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1\% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions. Project page: https://motif-1k.github.io},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Hwang, Minyoung and Hejna, Joey and Sadigh, Dorsa and Bisk, Yonatan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10683},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{alvarez-padilla_real-time_2024,
	title = {Real-{Time} {Whole}-{Body} {Control} of {Legged} {Robots} with {Model}-{Predictive} {Path} {Integral} {Control}},
	url = {http://arxiv.org/abs/2409.10469},
	doi = {10.48550/arXiv.2409.10469},
	abstract = {This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots. Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator to achieve fast sampling over the robot state and action trajectories. Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy. We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position. To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware. Experiment videos and code can be found at: https://whole-body-mppi.github.io/},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Alvarez-Padilla, Juan and Zhang, John Z. and Kwok, Sofia and Dolan, John M. and Manchester, Zachary},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10469},
	keywords = {Computer Science - Robotics},
}

@misc{newman_pre-trained_2024,
	title = {Do {Pre}-trained {Vision}-{Language} {Models} {Encode} {Object} {States}?},
	url = {http://arxiv.org/abs/2409.10488},
	doi = {10.48550/arXiv.2409.10488},
	abstract = {For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Newman, Kaleb and Wang, Shijie and Zang, Yuan and Heffren, David and Sun, Chen},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10488},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{farkhondeh_childplay-hand_2024,
	title = {{ChildPlay}-{Hand}: {A} {Dataset} of {Hand} {Manipulations} in the {Wild}},
	shorttitle = {{ChildPlay}-{Hand}},
	url = {http://arxiv.org/abs/2409.09319},
	doi = {10.48550/arXiv.2409.09319},
	abstract = {Hand-Object Interaction (HOI) is gaining significant attention, particularly with the creation of numerous egocentric datasets driven by AR/VR applications. However, third-person view HOI has received less attention, especially in terms of datasets. Most third-person view datasets are curated for action recognition tasks and feature pre-segmented clips of high-level daily activities, leaving a gap for in-the-wild datasets. To address this gap, we propose ChildPlay-Hand, a novel dataset that includes person and object bounding boxes, as well as manipulation actions. ChildPlay-Hand is unique in: (1) providing per-hand annotations; (2) featuring videos in uncontrolled settings with natural interactions, involving both adults and children; (3) including gaze labels from the ChildPlay-Gaze dataset for joint modeling of manipulations and gaze. The manipulation actions cover the main stages of an HOI cycle, such as grasping, holding or operating, and different types of releasing. To illustrate the interest of the dataset, we study two tasks: object in hand detection (OiH), i.e. if a person has an object in their hand, and manipulation stages (ManiS), which is more fine-grained and targets the main stages of manipulation. We benchmark various spatio-temporal and segmentation networks, exploring body vs. hand-region information and comparing pose and RGB modalities. Our findings suggest that ChildPlay-Hand is a challenging new benchmark for modeling HOI in the wild.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Farkhondeh, Arya and Tafasca, Samy and Odobez, Jean-Marc},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09319},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lu_ctrnet-x_2024,
	title = {{CtRNet}-{X}: {Camera}-to-{Robot} {Pose} {Estimation} in {Real}-world {Conditions} {Using} a {Single} {Camera}},
	shorttitle = {{CtRNet}-{X}},
	url = {http://arxiv.org/abs/2409.10441},
	doi = {10.48550/arXiv.2409.10441},
	abstract = {Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Lu, Jingpei and Liang, Zekai and Xie, Tristin and Ritcher, Florian and Lin, Shan and Liu, Sainan and Yip, Michael C.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10441},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_catch_2024,
	title = {Catch {It}! {Learning} to {Catch} in {Flight} with {Mobile} {Dexterous} {Hands}},
	url = {http://arxiv.org/abs/2409.10319},
	doi = {10.48550/arXiv.2409.10319},
	abstract = {Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots. This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects. In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task. We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation. The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight. The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80{\textbackslash}\% in simulation, with a significant improvement over the baselines. The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans. Our project page is available at https://mobile-dex-catch.github.io/.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Zhang, Yuanhang and Liang, Tianhai and Chen, Zhenyang and Ze, Yanjie and Xu, Huazhe},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10319},
	keywords = {Computer Science - Robotics},
}

@misc{ocal_realdiff_2024,
	title = {{RealDiff}: {Real}-world {3D} {Shape} {Completion} using {Self}-{Supervised} {Diffusion} {Models}},
	shorttitle = {{RealDiff}},
	url = {http://arxiv.org/abs/2409.10180},
	doi = {10.48550/arXiv.2409.10180},
	abstract = {Point cloud completion aims to recover the complete 3D shape of an object from partial observations. While approaches relying on synthetic shape priors achieved promising results in this domain, their applicability and generalizability to real-world data are still limited. To tackle this problem, we propose a self-supervised framework, namely RealDiff, that formulates point cloud completion as a conditional generation problem directly on real-world measurements. To better deal with noisy observations without resorting to training on synthetic data, we leverage additional geometric cues. Specifically, RealDiff simulates a diffusion process at the missing object parts while conditioning the generation on the partial input to address the multimodal nature of the task. We further regularize the training by matching object silhouettes and depth maps, predicted by our method, with the externally estimated ones. Experimental results show that our method consistently outperforms state-of-the-art methods in real-world point cloud completion.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Öcal, Başak Melis and Tatarchenko, Maxim and Karaoglu, Sezer and Gevers, Theo},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10180},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tang_embodiment-agnostic_2024,
	title = {Embodiment-{Agnostic} {Action} {Planning} via {Object}-{Part} {Scene} {Flow}},
	url = {http://arxiv.org/abs/2409.10032},
	doi = {10.48550/arXiv.2409.10032},
	abstract = {Observing that the key for robotic action planning is to understand the target-object motion when its associated part is manipulated by the end effector, we propose to generate the 3D object-part scene flow and extract its transformations to solve the action trajectories for diverse embodiments. The advantage of our approach is that it derives the robot action explicitly from object motion prediction, yielding a more robust policy by understanding the object motions. Also, beyond policies trained on embodiment-centric data, our method is embodiment-agnostic, generalizable across diverse embodiments, and being able to learn from human demonstrations. Our method comprises three components: an object-part predictor to locate the part for the end effector to manipulate, an RGBD video generator to predict future RGBD videos, and a trajectory planner to extract embodiment-agnostic transformation sequences and solve the trajectory for diverse embodiments. Trained on videos even without trajectory data, our method still outperforms existing works significantly by 27.7\% and 26.2\% on the prevailing virtual environments MetaWorld and Franka-Kitchen, respectively. Furthermore, we conducted real-world experiments, showing that our policy, trained only with human demonstration, can be deployed to various embodiments.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Tang, Weiliang and Pan, Jia-Hui and Zhan, Wei and Zhou, Jianshu and Yao, Huaxiu and Liu, Yun-Hui and Tomizuka, Masayoshi and Ding, Mingyu and Fu, Chi-Wing},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10032},
	keywords = {Computer Science - Robotics},
}

@misc{bu_closed-loop_2024,
	title = {Closed-{Loop} {Visuomotor} {Control} with {Generative} {Expectation} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2409.09016},
	doi = {10.48550/arXiv.2409.09016},
	abstract = {Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8\% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Bu, Qingwen and Zeng, Jia and Chen, Li and Yang, Yanchao and Zhou, Guyue and Yan, Junchi and Luo, Ping and Cui, Heming and Ma, Yi and Li, Hongyang},
	month = oct,
	year = {2024},
	note = {arXiv:2409.09016},
	keywords = {Computer Science - Robotics},
}

@misc{bai_cleardepth_2024,
	title = {{ClearDepth}: {Enhanced} {Stereo} {Perception} of {Transparent} {Objects} for {Robotic} {Manipulation}},
	shorttitle = {{ClearDepth}},
	url = {http://arxiv.org/abs/2409.08926},
	doi = {10.48550/arXiv.2409.08926},
	abstract = {Transparent object depth perception poses a challenge in everyday life and logistics, primarily due to the inability of standard 3D sensors to accurately capture depth on transparent or reflective surfaces. This limitation significantly affects depth map and point cloud-reliant applications, especially in robotic manipulation. We developed a vision transformer-based algorithm for stereo depth recovery of transparent objects. This approach is complemented by an innovative feature post-fusion module, which enhances the accuracy of depth recovery by structural features in images. To address the high costs associated with dataset collection for stereo camera-based perception of transparent objects, our method incorporates a parameter-aligned, domain-adaptive, and physically realistic Sim2Real simulation for efficient data generation, accelerated by AI algorithm. Our experimental results demonstrate the model's exceptional Sim2Real generalizability in real-world scenarios, enabling precise depth mapping of transparent objects to assist in robotic manipulation. Project details are available at https://sites.google.com/view/cleardepth/ .},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Bai, Kaixin and Zeng, Huajian and Zhang, Lei and Liu, Yiwen and Xu, Hongli and Chen, Zhaopeng and Zhang, Jianwei},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08926},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{jiang_dexsim2real2_2024,
	title = {{DexSim2Real}\${\textasciicircum}\{2\}\$: {Building} {Explicit} {World} {Model} for {Precise} {Articulated} {Object} {Dexterous} {Manipulation}},
	shorttitle = {{DexSim2Real}\${\textasciicircum}\{2\}\$},
	url = {http://arxiv.org/abs/2409.08750},
	doi = {10.48550/arXiv.2409.08750},
	abstract = {Articulated object manipulation is ubiquitous in daily life. In this paper, we present DexSim2Real\${\textasciicircum}\{2\}\$, a novel robot learning framework for goal-conditioned articulated object manipulation using both two-finger grippers and multi-finger dexterous hands. The key of our framework is constructing an explicit world model of unseen articulated objects through active one-step interactions. This explicit world model enables sampling-based model predictive control to plan trajectories achieving different manipulation goals without needing human demonstrations or reinforcement learning. It first predicts an interaction motion using an affordance estimation network trained on self-supervised interaction data or videos of human manipulation from the internet. After executing this interaction on the real robot, the framework constructs a digital twin of the articulated object in simulation based on the two point clouds before and after the interaction. For dexterous multi-finger manipulation, we propose to utilize eigengrasp to reduce the high-dimensional action space, enabling more efficient trajectory searching. Extensive experiments validate the framework's effectiveness for precise articulated object manipulation in both simulation and the real world using a two-finger gripper and a 16-DoF dexterous hand. The robust generalizability of the explicit world model also enables advanced manipulation strategies, such as manipulating with different tools.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Jiang, Taoran and Ma, Liqian and Guan, Yixuan and Meng, Jiaojiao and Chen, Weihang and Zeng, Zecui and Li, Lusong and Wu, Dan and Xu, Jing and Chen, Rui},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08750},
	keywords = {Computer Science - Robotics},
}

@inproceedings{yamaguchi_moving_2006,
	title = {Moving {Obstacle} {Detection} using {Monocular} {Vision}},
	url = {https://ieeexplore.ieee.org/abstract/document/1689643?casa_token=oDvu1WVD8k4AAAAA:fUFnX5tHleUwY7-U49gRYl1ob6sTplXnpC2vZsTklUb0Bru_6sPLUsEZF5QH3-ApvL9dni_0QRM},
	doi = {10.1109/IVS.2006.1689643},
	abstract = {This paper proposes a method for detecting moving obstacles on roads, by using a vehicle mounted monocular camera. To detect various moving obstacles, such as vehicles and pedestrians, the ego-motion of the vehicle is initially estimated from images captured by the camera. There are two problems in ego-motion estimation. Firstly, a typical road scene contains moving obstacles. This causes false estimation of the ego-motion. Secondly, roads possess fewer features, when compared to the number associated with background structures. This reduces the accuracy of ego-motion estimation. In our approach, the ego-motion is estimated from the correspondences of dispersed feature points extracted from various regions other than those that contain moving obstacles. After estimating the ego-motion, any moving obstacles are detected by tracking the feature points over consecutive frames. In our experiments, it has been shown that the proposed method is able to detect moving obstacles},
	urldate = {2024-11-05},
	booktitle = {2006 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	author = {Yamaguchi, K. and Kato, T. and Ninomiya, Y.},
	month = jun,
	year = {2006},
	note = {ISSN: 1931-0587},
	keywords = {Cameras, Image motion analysis, Laser radar, Layout, Nonlinear optics, Optical sensors, Radar detection, Road vehicles, Robustness, Vehicle detection},
	pages = {288--293},
}

@article{recht_tour_2019,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	volume = {2},
	issn = {2573-5144},
	shorttitle = {A {Tour} of {Reinforcement} {Learning}},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-control-053018-023825},
	doi = {10.1146/annurev-control-053018-023825},
	abstract = {This article surveys reinforcement learning from the perspective of optimization and control, with a focus on continuous control applications. It reviews the general formulation, terminology, and typical experimental implementations of reinforcement learning as well as competing solution paradigms. In order to compare the relative merits of various techniques, it presents a case study of the linear quadratic regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. It also describes how merging techniques from learning theory and control can provide nonasymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. The article concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
	language = {en},
	number = {Volume 2, 2019},
	urldate = {2024-11-05},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Recht, Benjamin},
	month = may,
	year = {2019},
	note = {Publisher: Annual Reviews},
	pages = {253--279},
}

@misc{meng_lt3sd_2024,
	title = {{LT3SD}: {Latent} {Trees} for {3D} {Scene} {Diffusion}},
	shorttitle = {{LT3SD}},
	url = {http://arxiv.org/abs/2409.08215},
	doi = {10.48550/arXiv.2409.08215},
	abstract = {We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Meng, Quan and Li, Lei and Nießner, Matthias and Dai, Angela},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08215},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_interact_2024,
	title = {{InterACT}: {Inter}-dependency {Aware} {Action} {Chunking} with {Hierarchical} {Attention} {Transformers} for {Bimanual} {Manipulation}},
	shorttitle = {{InterACT}},
	url = {http://arxiv.org/abs/2409.07914},
	doi = {10.48550/arXiv.2409.07914},
	abstract = {Bimanual manipulation presents unique challenges compared to unimanual tasks due to the complexity of coordinating two robotic arms. In this paper, we introduce InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework designed specifically for bimanual manipulation. InterACT leverages hierarchical attention mechanisms to effectively capture inter-dependencies between dual-arm joint states and visual inputs. The framework comprises a Hierarchical Attention Encoder, which processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, and a Multi-arm Decoder that generates each arm's action predictions in parallel, while sharing information between the arms through synchronization blocks by providing the other arm's intermediate output as context. Our experiments, conducted on various simulated and real-world bimanual manipulation tasks, demonstrate that InterACT outperforms existing methods. Detailed ablation studies further validate the significance of key components, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks on task performance. We provide supplementary materials and videos on our project page.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Lee, Andrew and Chuang, Ian and Chen, Ling-Yuan and Soltani, Iman},
	month = oct,
	year = {2024},
	note = {arXiv:2409.07914},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{luo_residual_2024,
	title = {Residual {Policy} {Learning} for {Perceptive} {Quadruped} {Control} {Using} {Differentiable} {Simulation}},
	url = {http://arxiv.org/abs/2410.03076},
	doi = {10.48550/arXiv.2410.03076},
	abstract = {First-order Policy Gradient (FoPG) algorithms such as Backpropagation through Time and Analytical Policy Gradients leverage local simulation physics to accelerate policy search, significantly improving sample efficiency in robot control compared to standard model-free reinforcement learning. However, FoPG algorithms can exhibit poor learning dynamics in contact-rich tasks like locomotion. Previous approaches address this issue by alleviating contact dynamics via algorithmic or simulation innovations. In contrast, we propose guiding the policy search by learning a residual over a simple baseline policy. For quadruped locomotion, we find that the role of residual policy learning in FoPG-based training (FoPG RPL) is primarily to improve asymptotic rewards, compared to improving sample efficiency for model-free RL. Additionally, we provide insights on applying FoPG's to pixel-based local navigation, training a point-mass robot to convergence within seconds. Finally, we showcase the versatility of FoPG RPL by using it to train locomotion and perceptive navigation end-to-end on a quadruped in minutes.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Luo, Jing Yuan and Song, Yunlong and Klemm, Victor and Shi, Fan and Scaramuzza, Davide and Hutter, Marco},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03076},
	keywords = {Computer Science - Robotics},
}

@misc{song_learning_2024,
	title = {Learning {Quadruped} {Locomotion} {Using} {Differentiable} {Simulation}},
	url = {http://arxiv.org/abs/2403.14864},
	doi = {10.48550/arXiv.2403.14864},
	abstract = {This work explores the potential of using differentiable simulation for learning quadruped locomotion. Differentiable simulation promises fast convergence and stable training by computing low-variance first-order gradients using robot dynamics. However, its usage for legged robots is still limited to simulation. The main challenge lies in the complex optimization landscape of robotic tasks due to discontinuous dynamics. This work proposes a new differentiable simulation framework to overcome these challenges. Our approach combines a high-fidelity, non-differentiable simulator for forward dynamics with a simplified surrogate model for gradient backpropagation. This approach maintains simulation accuracy by aligning the robot states from the surrogate model with those of the precise, non-differentiable simulator. Our framework enables learning quadruped walking in simulation in minutes without parallelization. When augmented with GPU parallelization, our approach allows the quadruped robot to master diverse locomotion skills on challenging terrains in minutes. We demonstrate that differentiable simulation outperforms a reinforcement learning algorithm (PPO) by achieving significantly better sample efficiency while maintaining its effectiveness in handling large-scale environments. Our method represents one of the first successful applications of differentiable simulation to real-world quadruped locomotion, offering a compelling alternative to traditional RL methods.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Song, Yunlong and Kim, Sangbae and Scaramuzza, Davide},
	month = oct,
	year = {2024},
	note = {arXiv:2403.14864},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@article{prescott_understanding_2024,
	title = {Understanding the sense of self through robotics},
	volume = {9},
	url = {https://www.science.org/doi/10.1126/scirobotics.adn2733},
	doi = {10.1126/scirobotics.adn2733},
	abstract = {Robotics can play a useful role in the scientific understanding of the sense of self, both through the construction of embodied models of the self and through the use of robots as experimental probes to explore the human self. In both cases, the embodiment of the robot allows us to devise and test hypotheses about the nature of the self, with regard to its development, its manifestation in behavior, and the diversity of selves in humans, animals, and, potentially, machines. This paper reviews robotics research that addresses the topic of the self—the minimal self, the extended self, and disorders of the self—and highlights future directions and open challenges in understanding the self through constructing its components in artificial systems. An emerging view is that key phenomena of the self can be generated in robots with suitably configured sensor and actuator systems and a layered cognitive architecture involving networks of predictive models.},
	number = {95},
	urldate = {2024-11-01},
	journal = {Science Robotics},
	author = {Prescott, Tony J. and Vogeley, Kai and Wykowska, Agnieszka},
	month = oct,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadn2733},
}

@inproceedings{wang_deep_2019,
	title = {Deep {Closest} {Point}: {Learning} {Representations} for {Point} {Cloud} {Registration}},
	shorttitle = {Deep {Closest} {Point}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.html},
	urldate = {2024-11-01},
	author = {Wang, Yue and Solomon, Justin M.},
	year = {2019},
	pages = {3523--3532},
}

@misc{sarode_pcrnet_2019,
	title = {{PCRNet}: {Point} {Cloud} {Registration} {Network} using {PointNet} {Encoding}},
	shorttitle = {{PCRNet}},
	url = {http://arxiv.org/abs/1908.07906},
	doi = {10.48550/arXiv.1908.07906},
	abstract = {PointNet has recently emerged as a popular representation for unstructured point cloud data, allowing application of deep learning to tasks such as object detection, segmentation and shape completion. However, recent works in literature have shown the sensitivity of the PointNet representation to pose misalignment. This paper presents a novel framework that uses the PointNet representation to align point clouds and perform registration for applications such as tracking, 3D reconstruction and pose estimation. We develop a framework that compares PointNet features of template and source point clouds to find the transformation that aligns them accurately. Depending on the prior information about the shape of the object formed by the point clouds, our framework can produce approaches that are shape specific or general to unseen shapes. The shape specific approach uses a Siamese architecture with fully connected (FC) layers and is robust to noise and initial misalignment in data. We perform extensive simulation and real-world experiments to validate the efficacy of our approach and compare the performance with state-of-art approaches.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Sarode, Vinit and Li, Xueqian and Goforth, Hunter and Aoki, Yasuhiro and Srivatsan, Rangaprasad Arun and Lucey, Simon and Choset, Howie},
	month = nov,
	year = {2019},
	note = {arXiv:1908.07906},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{qin_geometric_2022,
	title = {Geometric {Transformer} for {Fast} and {Robust} {Point} {Cloud} {Registration}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-10-31},
	author = {Qin, Zheng and Yu, Hao and Wang, Changjian and Guo, Yulan and Peng, Yuxing and Xu, Kai},
	year = {2022},
	pages = {11143--11152},
}

@inproceedings{abou-chakra_physically_2024,
	title = {Physically {Embodied} {Gaussian} {Splatting}: {A} {Visually} {Learnt} and {Physically} {Grounded} {3D} {Representation} for {Robotics}},
	shorttitle = {Physically {Embodied} {Gaussian} {Splatting}},
	url = {https://openreview.net/forum?id=AEq0onGrN2},
	abstract = {For robots to robustly understand and interact with the physical world, it is highly beneficial to have a comprehensive representation -- modelling geometry, physics, and visual observations -- that informs perception, planning, and control algorithms. We propose a novel dual "Gaussian-Particle" representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates "visual forces" that correct the particle positions while respecting known physical constraints. By integrating predictive physical modeling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality. We validate our approach on 2D and 3D tracking tasks as well as photometric reconstruction quality. Videos are found at https://embodied-gaussians.github.io/},
	language = {en},
	urldate = {2024-10-29},
	author = {Abou-Chakra, Jad and Rana, Krishan and Dayoub, Feras and Suenderhauf, Niko},
	month = sep,
	year = {2024},
}

@article{valsecchi_quadrupedal_2020,
	title = {Quadrupedal {Locomotion} on {Uneven} {Terrain} {With} {Sensorized} {Feet}},
	volume = {5},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/8968320},
	doi = {10.1109/LRA.2020.2969160},
	abstract = {Sensing of the terrain shape is crucial for legged robots deployed in the real world since the knowledge of the local terrain inclination at the contact points allows for an optimized force distribution that minimizes the risk of slipping. In this letter, we present a reactive locomotion strategy for torque controllable quadruped robots based on sensorized feet. Since the present approach works without exteroceptive sensing, it is robust against degraded vision. Inertial and force/torque sensors implemented in specially designed feet with articulated passive ankle joints measure the local terrain inclination and interaction forces. The proposed controller exploits the contact null-space in order to minimize the tangential forces to prevent slippage even in case of extreme contact conditions. We experimentally tested the proposed method in laboratory experiments and validated the approach with the quadrupedal robot ANYmal.},
	number = {2},
	urldate = {2024-10-28},
	journal = {IEEE Robotics and Automation Letters},
	author = {Valsecchi, Giorgio and Grandia, Ruben and Hutter, Marco},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Foot, Force, Legged locomotion, Mechanism design, Pose estimation, Robot sensing systems, legged robots, motion control},
	pages = {1548--1555},
}

@article{jenmalm_lighter_2006,
	title = {Lighter or {Heavier} {Than} {Predicted}: {Neural} {Correlates} of {Corrective} {Mechanisms} during {Erroneously} {Programmed} {Lifts}},
	volume = {26},
	copyright = {Copyright © 2006 Society for Neuroscience 0270-6474/06/269015-07\$15.00/0},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Lighter or {Heavier} {Than} {Predicted}},
	url = {https://www.jneurosci.org/content/26/35/9015},
	doi = {10.1523/JNEUROSCI.5045-05.2006},
	abstract = {A central concept in neuroscience is that the CNS signals the sensory discrepancy between the predicted and actual sensory consequences of action. It has been proposed that the cerebellum and parietal cortex are involved in this process. A discrepancy will trigger preprogrammed corrective responses and update the engaged sensorimotor memories. Here we use functional magnetic resonance imaging with an event-related design to investigate the neuronal correlates of such discrepancies. Healthy adults repeatedly lifted an object between their right index fingers and thumbs, and on some lifting trials, the weight of the object was unpredictably changed between light (230 g) and heavy (830 g). Regardless of whether the weight was heavier or lighter than predicted, activity was found in the right inferior parietal cortex (supramarginal gyrus). This suggests that this region is involved in the comparison of the predicted and actual sensory input and the updating of the sensorimotor memories. When the object was lighter or heavier than predicted, two different types of preprogrammed force corrections occurred. There was a slow force increase when the weight of the object was heavier than predicted. This corrective response was associated with activity in the left primary motor and somatosensory cortices. The fast termination of the excessive force when the object was lighter than predicted activated the right cerebellum. These findings show how the parietal cortex, cerebellum, and motor cortex are involved in the signaling of the discrepancy between predicated and actual sensory feedback and the associated corrective mechanisms.},
	language = {en},
	number = {35},
	urldate = {2024-10-28},
	journal = {Journal of Neuroscience},
	author = {Jenmalm, Per and Schmitz, Christina and Forssberg, Hans and Ehrsson, H. Henrik},
	month = aug,
	year = {2006},
	pmid = {16943559},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {cerebellum, dexterous manipulation, fingertip force control, motor cortex, parietal cortex, precision grip},
	pages = {9015--9021},
}

@article{trampenau_probabilistic_2015,
	title = {Probabilistic information on object weight shapes force dynamics in a grip-lift task},
	volume = {233},
	issn = {1432-1106},
	url = {https://doi.org/10.1007/s00221-015-4244-6},
	doi = {10.1007/s00221-015-4244-6},
	abstract = {Advance information, such as object weight, size and texture, modifies predictive scaling of grip forces in a grip-lift task. Here, we examined the influence of probabilistic advance information about object weight. Fifteen healthy volunteers repeatedly grasped and lifted an object equipped with a force transducer between their thumb and index finger. Three clearly distinguishable object weights were used. Prior to each lift, the probabilities for the three object weights were given by a visual cue. We examined the effect of probabilistic pre-cues on grip and lift force dynamics. We expected predictive scaling of grip force parameters to follow predicted values calculated according to probabilistic contingencies of the cues. We observed that probabilistic cues systematically influenced peak grip and load force rates, as an index of predictive motor scaling. However, the effects of probabilistic cues on force rates were nonlinear, and anticipatory adaptations of the motor output generally seemed to overestimate high probabilities and underestimate low probabilities. These findings support the suggestion that anticipatory adaptations and force scaling of the motor system can integrate probabilistic information. However, probabilistic information seems to influence motor programs in a nonlinear fashion.},
	language = {en},
	number = {6},
	urldate = {2024-10-28},
	journal = {Experimental Brain Research},
	author = {Trampenau, Leif and Kuhtz-Buschbeck, Johann P. and van Eimeren, Thilo},
	month = jun,
	year = {2015},
	keywords = {Adaptation, Anticipation, Force rate, Motor load, Motor system, Predictive},
	pages = {1711--1720},
}

@article{takamuku_better_2019,
	title = {Better grip force control by attending to the controlled object: {Evidence} for direct force estimation from visual motion},
	volume = {9},
	issn = {2045-2322},
	shorttitle = {Better grip force control by attending to the controlled object},
	url = {https://doi.org/10.1038/s41598-019-49359-8},
	doi = {10.1038/s41598-019-49359-8},
	abstract = {Estimating forces acting between our hand and objects is essential for dexterous motor control. An earlier study suggested that vision contributes to the estimation by demonstrating changes in grip force pattern caused by delayed visual feedback. However, two possible vision-based force estimation processes, one based on hand position and another based on object motion, were both able to explain the effect. Here, to test each process, we examined how visual feedback of hand and object each contribute to grip force control during moving an object (mass) connected to the grip by a damped-spring. Although force applied to the hand could be estimated from its displacement, we did not find any improvements by the hand feedback. In contrast, we found that visual feedback of object motion significantly improved the synchrony between grip and load forces. Furthermore, when both feedback sources were provided, the improvement was observed only when participants were instructed to direct their attention to the object. Our results suggest that visual feedback of object motion contributes to estimation of dynamic forces involved in our actions by means of inverse dynamics computation, i.e., the estimation of force from motion, and that visual attention directed towards the object facilitates this effect.},
	language = {en},
	number = {1},
	urldate = {2024-10-28},
	journal = {Scientific Reports},
	author = {Takamuku, Shinya and Gomi, Hiroaki},
	month = sep,
	year = {2019},
	pages = {13114},
}

@inproceedings{achlioptas_referit3d_2020,
	address = {Cham},
	title = {{ReferIt3D}: {Neural} {Listeners} for {Fine}-{Grained} {3D} {Object} {Identification} in {Real}-{World} {Scenes}},
	isbn = {978-3-030-58452-8},
	shorttitle = {{ReferIt3D}},
	doi = {10.1007/978-3-030-58452-8_25},
	abstract = {In this work we study the problem of using referential language to identify common objects in real-world 3D scenes. We focus on a challenging setup where the referred object belongs to a fine-grained object class and the underlying scene contains multiple object instances of that class. Due to the scarcity and unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5 K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a 2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners can recognize the referred object with high ({\textgreater}86\%, 92\% resp.) accuracy. By tapping on this data, we develop novel neural listeners that can comprehend object-centric natural language and identify the referred object directly in a 3D scene. Our key technical contribution is designing an approach for combining linguistic and geometric information (in the form of 3D point clouds) and creating multi-modal (3D) neural listeners . We also show that architectures which promote object-to-object communication via graph neural networks outperform less context-aware alternatives, and that fine-grained object classification is a bottleneck for language-assisted 3D object identification.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Achlioptas, Panos and Abdelreheem, Ahmed and Xia, Fei and Elhoseiny, Mohamed and Guibas, Leonidas},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {422--440},
}

@misc{chisari_learning_2024,
	title = {Learning {Robotic} {Manipulation} {Policies} from {Point} {Clouds} with {Conditional} {Flow} {Matching}},
	url = {http://arxiv.org/abs/2409.07343},
	doi = {10.48550/arXiv.2409.07343},
	abstract = {Learning from expert demonstrations is a promising approach for training robotic manipulation policies from limited data. However, imitation learning algorithms require a number of design choices ranging from the input modality, training objective, and 6-DoF end-effector pose representation. Diffusion-based methods have gained popularity as they enable predicting long-horizon trajectories and handle multimodal action distributions. Recently, Conditional Flow Matching (CFM) (or Rectified Flow) has been proposed as a more flexible generalization of diffusion models. In this paper, we investigate the application of CFM in the context of robotic policy learning and specifically study the interplay with the other design choices required to build an imitation learning algorithm. We show that CFM gives the best performance when combined with point cloud input observations. Additionally, we study the feasibility of a CFM formulation on the SO(3) manifold and evaluate its suitability with a simplified example. We perform extensive experiments on RLBench which demonstrate that our proposed PointFlowMatch approach achieves a state-of-the-art average success rate of 67.8\% over eight tasks, double the performance of the next best method.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Chisari, Eugenio and Heppert, Nick and Argus, Max and Welschehold, Tim and Brox, Thomas and Valada, Abhinav},
	month = sep,
	year = {2024},
	note = {arXiv:2409.07343},
	keywords = {Computer Science - Robotics},
}

@misc{bohlinger_one_2024,
	title = {One {Policy} to {Run} {Them} {All}: an {End}-to-end {Learning} {Approach} to {Multi}-{Embodiment} {Locomotion}},
	shorttitle = {One {Policy} to {Run} {Them} {All}},
	url = {http://arxiv.org/abs/2409.06366},
	doi = {10.48550/arXiv.2409.06366},
	abstract = {Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Bohlinger, Nico and Czechmanowski, Grzegorz and Krupka, Maciej and Kicki, Piotr and Walas, Krzysztof and Peters, Jan and Tateo, Davide},
	month = oct,
	year = {2024},
	note = {arXiv:2409.06366},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shen_world-grounded_2024,
	title = {World-{Grounded} {Human} {Motion} {Recovery} via {Gravity}-{View} {Coordinates}},
	url = {http://arxiv.org/abs/2409.06662},
	doi = {10.48550/arXiv.2409.06662},
	abstract = {We present a novel method for recovering world-grounded human motion from monocular video. The main challenge lies in the ambiguity of defining the world coordinate system, which varies between sequences. Previous approaches attempt to alleviate this issue by predicting relative motion in an autoregressive manner, but are prone to accumulating errors. Instead, we propose estimating human poses in a novel Gravity-View (GV) coordinate system, which is defined by the world gravity and the camera view direction. The proposed GV system is naturally gravity-aligned and uniquely defined for each video frame, largely reducing the ambiguity of learning image-pose mapping. The estimated poses can be transformed back to the world coordinate system using camera rotations, forming a global motion sequence. Additionally, the per-frame estimation avoids error accumulation in the autoregressive methods. Experiments on in-the-wild benchmarks demonstrate that our method recovers more realistic motion in both the camera space and world-grounded settings, outperforming state-of-the-art methods in both accuracy and speed. The code is available at https://zju3dv.github.io/gvhmr/.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Shen, Zehong and Pi, Huaijin and Xia, Yan and Cen, Zhi and Peng, Sida and Hu, Zechen and Bao, Hujun and Hu, Ruizhen and Zhou, Xiaowei},
	month = sep,
	year = {2024},
	note = {arXiv:2409.06662},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bauza_demostart_2024,
	title = {{DemoStart}: {Demonstration}-led auto-curriculum applied to sim-to-real with multi-fingered robots},
	shorttitle = {{DemoStart}},
	url = {http://arxiv.org/abs/2409.06613},
	doi = {10.48550/arXiv.2409.06613},
	abstract = {We present DemoStart, a novel auto-curriculum reinforcement learning method capable of learning complex manipulation behaviors on an arm equipped with a three-fingered robotic hand, from only a sparse reward and a handful of demonstrations in simulation. Learning from simulation drastically reduces the development cycle of behavior generation, and domain randomization techniques are leveraged to achieve successful zero-shot sim-to-real transfer. Transferred policies are learned directly from raw pixels from multiple cameras and robot proprioception. Our approach outperforms policies learned from demonstrations on the real robot and requires 100 times fewer demonstrations, collected in simulation. More details and videos in https://sites.google.com/view/demostart.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Bauza, Maria and Chen, Jose Enrique and Dalibard, Valentin and Gileadi, Nimrod and Hafner, Roland and Martins, Murilo F. and Moore, Joss and Pevceviciute, Rugile and Laurens, Antoine and Rao, Dushyant and Zambelli, Martina and Riedmiller, Martin and Scholz, Jon and Bousmalis, Konstantinos and Nori, Francesco and Heess, Nicolas},
	month = sep,
	year = {2024},
	note = {arXiv:2409.06613},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yuan_robot_2024,
	title = {Robot {Synesthesia}: {In}-{Hand} {Manipulation} with {Visuotactile} {Sensing}},
	shorttitle = {Robot {Synesthesia}},
	url = {http://arxiv.org/abs/2312.01853},
	doi = {10.48550/arXiv.2312.01853},
	abstract = {Executing contact-rich manipulation tasks necessitates the fusion of tactile and visual feedback. However, the distinct nature of these modalities poses significant challenges. In this paper, we introduce a system that leverages visual and tactile sensory inputs to enable dexterous in-hand manipulation. Specifically, we propose Robot Synesthesia, a novel point cloud-based tactile representation inspired by human tactile-visual synesthesia. This approach allows for the simultaneous and seamless integration of both sensory inputs, offering richer spatial information and facilitating better reasoning about robot actions. The method, trained in a simulated environment and then deployed to a real robot, is applicable to various in-hand object rotation tasks. Comprehensive ablations are performed on how the integration of vision and touch can improve reinforcement learning and Sim2Real performance. Our project page is available at https://yingyuan0414.github.io/visuotactile/ .},
	urldate = {2024-10-26},
	publisher = {arXiv},
	author = {Yuan, Ying and Che, Haichuan and Qin, Yuzhe and Huang, Binghao and Yin, Zhao-Heng and Lee, Kang-Won and Wu, Yi and Lim, Soo-Chul and Wang, Xiaolong},
	month = jul,
	year = {2024},
	note = {arXiv:2312.01853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{allen_surface_1984,
	title = {Surface descriptions from vision and touch},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/1087191?casa_token=8vZmNlTNU_UAAAAA:Hq1jQUJC3cS4AwegvVyROQrhnD9uH4CFSlXA5vDJnB1VPYq7kLSbY3LBepXTPFy5FECORSsQ6DM},
	doi = {10.1109/ROBOT.1984.1087191},
	abstract = {The goal of vision is object recognition. Recent research has shown that the human visual system creates a surface description of a scene, including depth and orientation information at all points in a scene as a first step before creating an object centered description. This description is referred to as a 2 1/2 D sketch. Machine vision systems presently do not have the capability of creating this 2 1/2 D sketch from visual information alone, especially for curved surface objects. By using tactile data in cooperation with vision, a method is proposed for creating a surfacc description of an object. This surface description uses bicubic surface patches as a primitive. Once a surface sketch is created with bicubic surface patches the next steps in the hierarchy of processing arc feasible, including a transformation to a full 3D object centered description.},
	urldate = {2024-10-26},
	booktitle = {1984 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Allen, P.},
	month = mar,
	year = {1984},
	keywords = {Bandwidth, Humans, Layout, Least squares approximation, Least squares methods, Optimal control, Polynomials, Robot sensing systems, Robot vision systems, Surface fitting},
	pages = {394--397},
}

@article{navarro-guerrero_visuo-haptic_2023,
	title = {Visuo-haptic object perception for robots: an overview},
	volume = {47},
	issn = {1573-7527},
	shorttitle = {Visuo-haptic object perception for robots},
	url = {https://doi.org/10.1007/s10514-023-10091-y},
	doi = {10.1007/s10514-023-10091-y},
	abstract = {The object perception capabilities of humans are impressive, and this becomes even more evident when trying to develop solutions with a similar proficiency in autonomous robots. While there have been notable advancements in the technologies for artificial vision and touch, the effective integration of these two sensory modalities in robotic applications still needs to be improved, and several open challenges exist. Taking inspiration from how humans combine visual and haptic perception to perceive object properties and drive the execution of manual tasks, this article summarises the current state of the art of visuo-haptic object perception in robots. Firstly, the biological basis of human multimodal object perception is outlined. Then, the latest advances in sensing technologies and data collection strategies for robots are discussed. Next, an overview of the main computational techniques is presented, highlighting the main challenges of multimodal machine learning and presenting a few representative articles in the areas of robotic object recognition, peripersonal space representation and manipulation. Finally, informed by the latest advancements and open challenges, this article outlines promising new research directions.},
	language = {en},
	number = {4},
	urldate = {2024-10-26},
	journal = {Autonomous Robots},
	author = {Navarro-Guerrero, Nicolás and Toprak, Sibel and Josifovski, Josip and Jamone, Lorenzo},
	month = apr,
	year = {2023},
	keywords = {Artificial Intelligence, Haptics, Multimodal machine learning, Object manipulation, Robot perception, Sensor fusion, Tactile sensing},
	pages = {377--403},
}

@inproceedings{liang_contact_2021,
	title = {Contact {Localization} for {Robot} {Arms} in {Motion} without {Torque} {Sensing}},
	url = {https://ieeexplore.ieee.org/abstract/document/9562058},
	doi = {10.1109/ICRA48506.2021.9562058},
	abstract = {Detecting and localizing contacts is essential for robot manipulators to perform contact-rich tasks in unstructured environments. While robot skins can localize contacts on the surface of robot arms, these sensors are not yet robust or easily accessible. As such, prior works have explored using proprioceptive observations, such as joint velocities and torques, to perform contact localization. Many past approaches assume the robot is static during contact incident, a single contact is made at a time, or having access to accurate dynamics models and joint torque sensing. In this work, we relax these assumptions and propose using Domain Randomization to train a neural network to localize contacts of robot arms in motion without joint torque observations. Our method uses a novel cylindrical projection encoding of the robot arm surface, which allows the network to use convolution layers to process input features and transposed convolution layers to predict contacts. The trained network achieves a contact detection accuracy of 91.5\% and a mean contact localization error of 3.0cm. We further demonstrate an application of the contact localization model in an obstacle mapping task, evaluated in both simulation and the real world.},
	urldate = {2024-10-26},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liang, Jacky and Kroemer, Oliver},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Convolution, Data models, Location awareness, Manipulators, Neural networks, Robot sensing systems, Torque},
	pages = {6322--6328},
}

@article{ding_-hand_2018,
	title = {In-{Hand} {Grasping} {Pose} {Estimation} {Using} {Particle} {Filters} in {Combination} with {Haptic} {Rendering} {Models}},
	volume = {15},
	issn = {0219-8436},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0219843618500020},
	doi = {10.1142/S0219843618500020},
	abstract = {Specialized grippers used in the industry are often restricted to specific tasks and objects. However, with the development of dexterous grippers, such as humanoid hands, in-hand pose estimation becomes crucial for successful manipulations, since objects will change their pose during and after the grasping process. In this paper, we present a gripping system and describe a new pose estimation algorithm based on tactile sensory information in combination with haptic rendering models (HRMs). We use a 3-finger manipulator equipped with tactile force sensing elements. A particle filter processes the tactile measurements from these sensor elements to estimate the grasp pose of an object. The algorithm evaluates hypotheses of grasp poses by comparing tactile measurements and expected tactile information from CAD-based haptic renderings, where distance values between the sensor and 3D-model are converted to forces. Our approach compares the force distribution instead of absolute forces or distance values of each taxel. The haptic rendering models of the objects allow us to estimate the pose of soft deformable objects. In comparison to mesh-based approaches, our algorithm reduces the calculation complexity and recognizes ambiguous and geometrically impossible solutions.},
	number = {01},
	urldate = {2024-10-26},
	journal = {International Journal of Humanoid Robotics},
	author = {Ding, Yitao and Bonse, Julian and Andre, Robert and Thomas, Ulrike},
	month = feb,
	year = {2018},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Grasping, haptic rendering models, particle filter, tactile sensing},
	pages = {1850002},
}

@inproceedings{liang_-hand_2020,
	title = {In-{Hand} {Object} {Pose} {Tracking} via {Contact} {Feedback} and {GPU}-{Accelerated} {Robotic} {Simulation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9197117?casa_token=PxRAaiBRUJwAAAAA:zQcZMP9GbKUtzDzjhxdeJ1rFMc25qqmxWMWfurP9e2G24cLD1yry_gFYVy5VaKnB2SNsz24REsM},
	doi = {10.1109/ICRA40945.2020.9197117},
	abstract = {Tracking the pose of an object while it is being held and manipulated by a robot hand is difficult for vision-based methods due to significant occlusions. Prior works have explored using contact feedback and particle filters to localize in-hand objects. However, they have mostly focused on the static grasp setting and not when the object is in motion, as doing so requires modeling of complex contact dynamics. In this work, we propose using GPU-accelerated parallel robot simulations and derivative-free, sample-based optimizers to track in-hand object poses with contact feedback during manipulation. We use physics simulation as the forward model for robot-object interactions, and the algorithm jointly optimizes for the state and the parameters of the simulations, so they better match with those of the real world. Our method runs in real-time (30Hz) on a single GPU, and it achieves an average point cloud distance error of 6mm in simulation experiments and 13mm in the real-world ones.},
	urldate = {2024-10-26},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liang, Jacky and Handa, Ankur and Wyk, Karl Van and Makoviychuk, Viktor and Kroemer, Oliver and Fox, Dieter},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Cost function, Heuristic algorithms, Physics, Pose estimation, Robot sensing systems},
	pages = {6203--6209},
}

@inproceedings{chen_mocha-stereo_2024,
	title = {{MoCha}-{Stereo}: {Motif} {Channel} {Attention} {Network} for {Stereo} {Matching}},
	shorttitle = {{MoCha}-{Stereo}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_MoCha-Stereo_Motif_Channel_Attention_Network_for_Stereo_Matching_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-10-25},
	author = {Chen, Ziyang and Long, Wei and Yao, He and Zhang, Yongjun and Wang, Bingshu and Qin, Yongbin and Wu, Jia},
	year = {2024},
	pages = {27768--27777},
}

@misc{taher_fit-ngp_2024,
	title = {Fit-{NGP}: {Fitting} {Object} {Models} to {Neural} {Graphics} {Primitives}},
	shorttitle = {Fit-{NGP}},
	url = {http://arxiv.org/abs/2401.02357},
	doi = {10.48550/arXiv.2401.02357},
	abstract = {Accurate 3D object pose estimation is key to enabling many robotic applications that involve challenging object interactions. In this work, we show that the density field created by a state-of-the-art efficient radiance field reconstruction method is suitable for highly accurate and robust pose estimation for objects with known 3D models, even when they are very small and with challenging reflective surfaces. We present a fully automatic object pose estimation system based on a robot arm with a single wrist-mounted camera, which can scan a scene from scratch, detect and estimate the 6-Degrees of Freedom (DoF) poses of multiple objects within a couple of minutes of operation. Small objects such as bolts and nuts are estimated with accuracy on order of 1mm.},
	urldate = {2024-10-25},
	publisher = {arXiv},
	author = {Taher, Marwan and Alzugaray, Ignacio and Davison, Andrew J.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{loomis_tactual_1986,
	address = {Oxford, England},
	title = {Tactual perception},
	isbn = {978-0-471-82956-0 978-0-471-82957-7},
	abstract = {deals with the sense of touch primarily as a channel of information about objects and events outside the body / this coverage includes both the normal function of the sense of touch as it is used in the perception of space, texture, and form and its use as a sensory channel by prosthetic devices developed for the blind and deaf  tactile pattern perception / kinesthetic perception / haptic perception / texture perception (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	booktitle = {Handbook of perception and human performance, {Vol}. 2: {Cognitive} processes and performance},
	publisher = {John Wiley \& Sons},
	author = {Loomis, Jack M. and Lederman, Susan J.},
	year = {1986},
	keywords = {Cutaneous Sense, Kinesthetic Perception, Pattern Discrimination, Tactual Perception, Texture Perception},
	pages = {1--41},
}

@inproceedings{ferrandis_learning_2024,
	title = {Learning {Visuotactile} {Estimation} and {Control} for {Non}-prehensile {Manipulation} under {Occlusions}},
	url = {https://openreview.net/forum?id=oSU7M7MK6B&noteId=oSU7M7MK6B},
	abstract = {Manipulation without grasping, known as non-prehensile manipulation, is essential for dexterous robots in contact-rich environments, but presents many challenges relating with underactuation, hybrid-dynamics, and frictional uncertainty. Additionally, object occlusions in a scenario of contact uncertainty and where the motion of the object evolves independently from the robot becomes a critical problem, which previous literature fails to address. We present a method for learning visuotactile state estimators and uncertainty-aware control policies for non-prehensile manipulation under occlusions, by leveraging diverse interaction data from privileged policies trained in simulation. We formulate the estimator within a Bayesian deep learning framework, to model its uncertainty, and then train uncertainty-aware control policies by incorporating the pre-learned estimator into the reinforcement learning (RL) loop, both of which lead to significantly improved estimator and policy performance. Therefore, unlike prior non-prehensile research that relies on complex external perception set-ups, our method successfully handles occlusions after sim-to-real transfer to robotic hardware with a simple onboard camera.},
	language = {en},
	urldate = {2024-10-24},
	author = {Ferrandis, Juan Del Aguila and Moura, Joao and Vijayakumar, Sethu},
	month = sep,
	year = {2024},
}

@inproceedings{yu_mimictouch_2024,
	title = {{MimicTouch}: {Leveraging} {Multi}-modal {Human} {Tactile} {Demonstrations} for {Contact}-rich {Manipulation}},
	shorttitle = {{MimicTouch}},
	url = {https://openreview.net/forum?id=7yMZAUkXa4},
	abstract = {Tactile sensing is critical to fine-grained, contact-rich manipulation tasks, such as insertion and assembly. Prior research has shown the possibility of learning tactile-guided policy from teleoperated demonstration data. However, to provide the demonstration, human users often rely on visual feedback to control the robot. This creates a gap between the sensing modality used for controlling the robot (visual) and the modality of interest (tactile). To bridge this gap, we introduce "MimicTouch'', a novel framework for learning policies directly from demonstrations provided by human users with their hands. The key innovations are i) a human tactile data collection system which collects multi-modal tactile dataset for learning human's tactile-guided control strategy, ii) an imitation learning-based framework for learning human's tactile-guided control strategy through such data, and iii) an online residual RL framework to bridge the embodiment gap between the human hand and the robot gripper. Through comprehensive experiments, we highlight the efficacy of utilizing human's tactile-guided control strategy to resolve contact-rich manipulation tasks. The project website is at https://sites.google.com/view/MimicTouch.},
	language = {en},
	urldate = {2024-10-24},
	author = {Yu, Kelin and Han, Yunhai and Wang, Qixian and Saxena, Vaibhav and Xu, Danfei and Zhao, Ye},
	month = sep,
	year = {2024},
}

@inproceedings{yao_structured_2024,
	title = {Structured {Bayesian} {Meta}-{Learning} for {Data}-{Efficient} {Visual}-{Tactile} {Model} {Estimation}},
	url = {https://openreview.net/forum?id=TzqKmIhcwq},
	abstract = {Estimating visual-tactile models of deformable objects is challenging because vision suffers from occlusion, while touch data is sparse and noisy. We propose a novel data-efficient method for dense heterogeneous model estimation by leveraging experience from diverse training objects. The method is based on Bayesian Meta-Learning (BML), which can mitigate overfitting high-capacity visual-tactile models by meta-learning an informed prior and naturally achieves few-shot online estimation via posterior estimation. However, BML requires a shared parametric model across tasks but visual-tactile models for diverse objects have different parameter spaces. To address this issue, we introduce Structured Bayesian Meta-Learning (SBML) that incorporates heterogeneous physics models, enabling learning from training objects with varying appearances and geometries. SBML performs zero-shot vision-only prediction of deformable model parameters and few-shot adaptation after a handful of touches. Experiments show that in two classes of heterogeneous objects, namely plants and shoes, SBML outperforms existing approaches in force and torque prediction accuracy in zero- and few-shot settings.},
	language = {en},
	urldate = {2024-10-24},
	author = {Yao, Shaoxiong and Zhu, Yifan and Hauser, Kris},
	month = sep,
	year = {2024},
}

@inproceedings{huang_3d-vitac_2024,
	title = {{3D}-{ViTac}: {Learning} {Fine}-{Grained} {Manipulation} with {Visuo}-{Tactile} {Sensing}},
	shorttitle = {{3D}-{ViTac}},
	url = {https://openreview.net/forum?id=bk28WlkqZn},
	abstract = {Tactile and visual perception are both crucial for humans to perform fine-grained interactions with their environment. Developing similar multi-modal sensing capabilities for robots can significantly enhance and expand their manipulation skills. This paper introduces **3D-ViTac**, a multi-modal sensing and learning system designed for dexterous bimanual manipulation. Our system features tactile sensors equipped with dense sensing units, each covering an area of 3\$mm{\textasciicircum}2\$. These sensors are low-cost and flexible, providing detailed and extensive coverage of physical contacts, effectively complementing visual information. To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. Through concrete hardware experiments, we demonstrate that even low-cost robots can perform precise manipulations and significantly outperform vision-only policies, particularly in safe interactions with fragile items and executing long-horizon tasks involving in-hand manipulation. Our project page is available at https://binghao-huang.github.io/3D-ViTac/.},
	language = {en},
	urldate = {2024-10-24},
	author = {Huang, Binghao and Wang, Yixuan and Yang, Xinyi and Luo, Yiyue and Li, Yunzhu},
	month = sep,
	year = {2024},
}

@misc{ikeda_diffusionnocs_2024,
	title = {{DiffusionNOCS}: {Managing} {Symmetry} and {Uncertainty} in {Sim2Real} {Multi}-{Modal} {Category}-level {Pose} {Estimation}},
	shorttitle = {{DiffusionNOCS}},
	url = {http://arxiv.org/abs/2402.12647},
	doi = {10.48550/arXiv.2402.12647},
	abstract = {This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Ikeda, Takuya and Zakharov, Sergey and Ko, Tianyi and Irshad, Muhammad Zubair and Lee, Robert and Liu, Katherine and Ambrus, Rares and Nishiwaki, Koichi},
	month = mar,
	year = {2024},
	note = {arXiv:2402.12647},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{pineda_theseus_2023,
	title = {Theseus: {A} {Library} for {Differentiable} {Nonlinear} {Optimization}},
	shorttitle = {Theseus},
	url = {http://arxiv.org/abs/2207.09442},
	doi = {10.48550/arXiv.2207.09442},
	abstract = {We present Theseus, an efficient application-agnostic open source library for differentiable nonlinear least squares (DNLS) optimization built on PyTorch, providing a common framework for end-to-end structured learning in robotics and vision. Existing DNLS implementations are application specific and do not always incorporate many ingredients important for efficiency. Theseus is application-agnostic, as we illustrate with several example applications that are built using the same underlying differentiable components, such as second-order optimizers, standard costs functions, and Lie groups. For efficiency, Theseus incorporates support for sparse solvers, automatic vectorization, batching, GPU acceleration, and gradient computation with implicit differentiation and direct loss minimization. We do extensive performance evaluation in a set of applications, demonstrating significant efficiency gains and better scalability when these features are incorporated. Project page: https://sites.google.com/view/theseus-ai},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Pineda, Luis and Fan, Taosha and Monge, Maurizio and Venkataraman, Shobha and Sodhi, Paloma and Chen, Ricky T. Q. and Ortiz, Joseph and DeTone, Daniel and Wang, Austin and Anderson, Stuart and Dong, Jing and Amos, Brandon and Mukadam, Mustafa},
	month = jan,
	year = {2023},
	note = {arXiv:2207.09442},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Mathematics - Optimization and Control},
}

@inproceedings{amos_optnet_2017,
	title = {{OptNet}: {Differentiable} {Optimization} as a {Layer} in {Neural} {Networks}},
	shorttitle = {{OptNet}},
	url = {https://proceedings.mlr.press/v70/amos17a.html},
	abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
	language = {en},
	urldate = {2024-10-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Amos, Brandon and Kolter, J. Zico},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {136--145},
}

@inproceedings{moon_genflow_2024,
	title = {{GenFlow}: {Generalizable} {Recurrent} {Flow} for {6D} {Pose} {Refinement} of {Novel} {Objects}},
	shorttitle = {{GenFlow}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Moon_GenFlow_Generalizable_Recurrent_Flow_for_6D_Pose_Refinement_of_Novel_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-10-23},
	author = {Moon, Sungphill and Son, Hyeontae and Hur, Dongcheol and Kim, Sangwook},
	year = {2024},
	pages = {10039--10049},
}

@misc{li_toward_2024,
	title = {Toward {An} {Analytic} {Theory} of {Intrinsic} {Robustness} for {Dexterous} {Grasping}},
	url = {http://arxiv.org/abs/2403.07249},
	doi = {10.48550/arXiv.2403.07249},
	abstract = {Conventional approaches to grasp planning require perfect knowledge of an object's pose and geometry. Uncertainties in these quantities induce uncertainties in the quality of planned grasps, which can lead to failure. Classically, grasp robustness refers to the ability to resist external disturbances after grasping an object. In contrast, this work studies robustness to intrinsic sources of uncertainty like object pose or geometry affecting grasp planning before execution. To do so, we develop a novel analytic theory of grasping that reasons about this intrinsic robustness by characterizing the effect of friction cone uncertainty on a grasp's force closure status. We apply this result in two ways. First, we analyze the theoretical guarantees on intrinsic robustness of two grasp metrics in the literature, the classical Ferrari-Canny metric and more recent min-weight metric. We validate these results with hardware trials that compare grasps synthesized with and without robustness guarantees, showing a clear improvement in success rates. Second, we use our theory to develop a novel analytic notion of probabilistic force closure, which we show can generate unique, uncertainty-aware grasps in simulation.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Li, Albert H. and Culbertson, Preston and Ames, Aaron D.},
	month = aug,
	year = {2024},
	note = {arXiv:2403.07249},
	keywords = {Computer Science - Robotics},
}

@misc{yu_-hand_2024,
	title = {In-{Hand} {Following} of {Deformable} {Linear} {Objects} {Using} {Dexterous} {Fingers} with {Tactile} {Sensing}},
	url = {http://arxiv.org/abs/2403.12676},
	doi = {10.48550/arXiv.2403.12676},
	abstract = {Most research on deformable linear object (DLO) manipulation assumes rigid grasping. However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs, which requires continuously changing the grasp point by in-hand sliding while holding the DLO to prevent it from falling. Achieving such a skill is very challenging for robots without using specially designed but not versatile end-effectors. Previous works have attempted using generic parallel grippers, but their robustness is unsatisfactory owing to the conflict between following and holding, which is hard to balance with a one-degree-of-freedom gripper. In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following. To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design. Experimental results demonstrate the significant superiority of our method over using parallel grippers, as well as its great robustness, generalizability, and efficiency.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Yu, Mingrui and Liang, Boyuan and Zhang, Xiang and Zhu, Xinghao and Sun, Lingfeng and Wang, Changhao and Song, Shiji and Li, Xiang and Tomizuka, Masayoshi},
	month = aug,
	year = {2024},
	note = {arXiv:2403.12676},
	keywords = {Computer Science - Robotics},
}

@misc{jiang_contact-implicit_2024,
	title = {Contact-{Implicit} {Model} {Predictive} {Control} for {Dexterous} {In}-hand {Manipulation}: {A} {Long}-{Horizon} and {Robust} {Approach}},
	shorttitle = {Contact-{Implicit} {Model} {Predictive} {Control} for {Dexterous} {In}-hand {Manipulation}},
	url = {http://arxiv.org/abs/2402.18897},
	doi = {10.48550/arXiv.2402.18897},
	abstract = {Dexterous in-hand manipulation is an essential skill of production and life. Nevertheless, the highly stiff and mutable features of contacts cause limitations to real-time contact discovery and inference, which degrades the performance of model-based methods. Inspired by recent advancements in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations. The proposed approach has the attractive feature, which allows the robot to robustly execute long-horizon in-hand manipulation without pre-defined contact sequences or separated planning procedures. Specifically, we design a contact-implicit model predictive controller at high-level to generate real-time contact plans, which are executed by the low-level tracking controller. Compared with other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large-displacement in-hand tasks more efficiently; Compared with existing learning-based methods, the proposed approach achieves the dexterity and also generalizes to different objects without any pre-training. Detailed simulations and ablation studies demonstrate the efficiency and effectiveness of our method. It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand object rotation task.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Jiang, Yongpeng and Yu, Mingrui and Zhu, Xinghao and Tomizuka, Masayoshi and Li, Xiang},
	month = mar,
	year = {2024},
	note = {arXiv:2402.18897},
	keywords = {Computer Science - Robotics},
}

@misc{fan_instantsplat_2024,
	title = {{InstantSplat}: {Sparse}-view {SfM}-free {Gaussian} {Splatting} in {Seconds}},
	shorttitle = {{InstantSplat}},
	url = {http://arxiv.org/abs/2403.20309},
	doi = {10.48550/arXiv.2403.20309},
	abstract = {While novel view synthesis (NVS) from a sparse set of images has advanced significantly in 3D computer vision, it relies on precise initial estimation of camera parameters using Structure-from-Motion (SfM). For instance, the recently developed Gaussian Splatting depends heavily on the accuracy of SfM-derived points and poses. However, SfM processes are time-consuming and often prove unreliable in sparse-view scenarios, where matched features are scarce, leading to accumulated errors and limited generalization capability across datasets. In this study, we introduce a novel and efficient framework to enhance robust NVS from sparse-view images. Our framework, InstantSplat, integrates multi-view stereo(MVS) predictions with point-based representations to construct 3D Gaussians of large-scale scenes from sparse-view data within seconds, addressing the aforementioned performance and efficiency issues by SfM. Specifically, InstantSplat generates densely populated surface points across all training views and determines the initial camera parameters using pixel-alignment. Nonetheless, the MVS points are not globally accurate, and the pixel-wise prediction from all views results in an excessive Gaussian number, yielding a overparameterized scene representation that compromises both training speed and accuracy. To address this issue, we employ a grid-based, confidence-aware Farthest Point Sampling to strategically position point primitives at representative locations in parallel. Next, we enhance pose accuracy and tune scene parameters through a gradient-based joint optimization framework from self-supervision. By employing this simplified framework, InstantSplat achieves a substantial reduction in training time, from hours to mere seconds, and demonstrates robust performance across various numbers of views in diverse datasets.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Fan, Zhiwen and Cong, Wenyan and Wen, Kairun and Wang, Kevin and Zhang, Jian and Ding, Xinghao and Xu, Danfei and Ivanovic, Boris and Pavone, Marco and Pavlakos, Georgios and Wang, Zhangyang and Wang, Yue},
	month = aug,
	year = {2024},
	note = {arXiv:2403.20309},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sundaram_when_2024,
	title = {When {Does} {Perceptual} {Alignment} {Benefit} {Vision} {Representations}?},
	url = {http://arxiv.org/abs/2410.10817},
	doi = {10.48550/arXiv.2410.10817},
	abstract = {Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Sundaram, Shobhita and Fu, Stephanie and Muttenthaler, Lukas and Tamir, Netanel Y. and Chai, Lucy and Kornblith, Simon and Darrell, Trevor and Isola, Phillip},
	month = oct,
	year = {2024},
	note = {arXiv:2410.10817},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_drop_2024,
	title = {{DROP}: {Dexterous} {Reorientation} via {Online} {Planning}},
	shorttitle = {{DROP}},
	url = {http://arxiv.org/abs/2409.14562},
	doi = {10.48550/arXiv.2409.14562},
	abstract = {Achieving human-like dexterity is a longstanding challenge in robotics, in part due to the complexity of planning and control for contact-rich systems. In reinforcement learning (RL), one popular approach has been to use massively-parallelized, domain-randomized simulations to learn a policy offline over a vast array of contact conditions, allowing robust sim-to-real transfer. Inspired by recent advances in real-time parallel simulation, this work considers instead the viability of online planning methods for contact-rich manipulation by studying the well-known in-hand cube reorientation task. We propose a simple architecture that employs a sampling-based predictive controller and vision-based pose estimator to search for contact-rich control actions online. We conduct thorough experiments to assess the real-world performance of our method, architectural design choices, and key factors for robustness, demonstrating that our simple sampling-based approach achieves performance comparable to prior RL-based works. Supplemental material: https://caltech-amber.github.io/drop.},
	urldate = {2024-10-12},
	publisher = {arXiv},
	author = {Li, Albert H. and Culbertson, Preston and Kurtz, Vince and Ames, Aaron D.},
	month = oct,
	year = {2024},
	note = {arXiv:2409.14562},
	keywords = {Computer Science - Robotics},
}

@misc{funk_actionflow_2024,
	title = {{ActionFlow}: {Equivariant}, {Accurate}, and {Efficient} {Policies} with {Spatially} {Symmetric} {Flow} {Matching}},
	shorttitle = {{ActionFlow}},
	url = {http://arxiv.org/abs/2409.04576},
	doi = {10.48550/arXiv.2409.04576},
	abstract = {Spatial understanding is a critical aspect of most robotic tasks, particularly when generalization is important. Despite the impressive results of deep generative models in complex manipulation tasks, the absence of a representation that encodes intricate spatial relationships between observations and actions often limits spatial generalization, necessitating large amounts of demonstrations. To tackle this problem, we introduce a novel policy class, ActionFlow. ActionFlow integrates spatial symmetry inductive biases while generating expressive action sequences. On the representation level, ActionFlow introduces an SE(3) Invariant Transformer architecture, which enables informed spatial reasoning based on the relative SE(3) poses between observations and actions. For action generation, ActionFlow leverages Flow Matching, a state-of-the-art deep generative model known for generating high-quality samples with fast inference - an essential property for feedback control. In combination, ActionFlow policies exhibit strong spatial and locality biases and SE(3)-equivariant action generation. Our experiments demonstrate the effectiveness of ActionFlow and its two main components on several simulated and real-world robotic manipulation tasks and confirm that we can obtain equivariant, accurate, and efficient policies with spatially symmetric flow matching. Project website: https://flowbasedpolicies.github.io/},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Funk, Niklas and Urain, Julen and Carvalho, Joao and Prasad, Vignesh and Chalvatzaki, Georgia and Peters, Jan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.04576},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{etukuru_robot_2024,
	title = {Robot {Utility} {Models}: {General} {Policies} for {Zero}-{Shot} {Deployment} in {New} {Environments}},
	shorttitle = {Robot {Utility} {Models}},
	url = {http://arxiv.org/abs/2409.05865},
	doi = {10.48550/arXiv.2409.05865},
	abstract = {Robot models, particularly those trained with large amounts of data, have recently shown a plethora of real-world manipulation and navigation capabilities. Several independent efforts have shown that given sufficient training data in an environment, robot policies can generalize to demonstrated variations in that environment. However, needing to finetune robot models to every new environment stands in stark contrast to models in language or vision that can be deployed zero-shot for open-world problems. In this work, we present Robot Utility Models (RUMs), a framework for training and deploying zero-shot robot policies that can directly generalize to new environments without any finetuning. To create RUMs efficiently, we develop new tools to quickly collect data for mobile manipulation tasks, integrate such data into a policy with multi-modal imitation learning, and deploy policies on-device on Hello Robot Stretch, a cheap commodity robot, with an external mLLM verifier for retrying. We train five such utility models for opening cabinet doors, opening drawers, picking up napkins, picking up paper bags, and reorienting fallen objects. Our system, on average, achieves 90\% success rate in unseen, novel environments interacting with unseen objects. Moreover, the utility models can also succeed in different robot and camera set-ups with no further data, training, or fine-tuning. Primary among our lessons are the importance of training data over training algorithm and policy class, guidance about data scaling, necessity for diverse yet high-quality demonstrations, and a recipe for robot introspection and retrying to improve performance on individual environments. Our code, data, models, hardware designs, as well as our experiment and deployment videos are open sourced and can be found on our project website: https://robotutilitymodels.com},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Etukuru, Haritheja and Naka, Norihito and Hu, Zijin and Lee, Seungjae and Mehu, Julian and Edsinger, Aaron and Paxton, Chris and Chintala, Soumith and Pinto, Lerrel and Shafiullah, Nur Muhammad Mahi},
	month = sep,
	year = {2024},
	note = {arXiv:2409.05865},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{dai_hisc4d_2024,
	title = {{HiSC4D}: {Human}-centered interaction and {4D} {Scene} {Capture} in {Large}-scale {Space} {Using} {Wearable} {IMUs} and {LiDAR}},
	shorttitle = {{HiSC4D}},
	url = {http://arxiv.org/abs/2409.04398},
	doi = {10.48550/arXiv.2409.04398},
	abstract = {We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture method, aimed at accurately and efficiently creating a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, rich human-human interactions, and human-environment interactions. By utilizing body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human motions in unconstrained space without the need for external devices and pre-built maps. This affords great flexibility and accessibility for human-centered interaction and 4D scene capturing in various environments. Taking into account that IMUs can capture human spatially unrestricted poses but are prone to drifting for long-period using, and while LiDAR is stable for global localization but rough for local positions and orientations, HiSC4D employs a joint optimization method, harmonizing all sensors and utilizing environment cues, yielding promising results for long-term capture in large scenes. To promote research of egocentric human interaction in large scenes and facilitate downstream tasks, we also present a dataset, containing 8 sequences in 4 large scenes (200 to 5,000 \$m{\textasciicircum}2\$), providing 36k frames of accurate 4D human motions with SMPL annotations and dynamic scenes, 31k frames of cropped human point clouds, and scene mesh of the environment. A variety of scenarios, such as the basketball gym and commercial street, alongside challenging human motions, such as daily greeting, one-on-one basketball playing, and tour guiding, demonstrate the effectiveness and the generalization ability of HiSC4D. The dataset and code will be publicated on www.lidarhumanmotion.net/hisc4d available for research purposes.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Dai, Yudi and Wang, Zhiyong and Lin, Xiping and Wen, Chenglu and Xu, Lan and Shen, Siqi and Ma, Yuexin and Wang, Cheng},
	month = sep,
	year = {2024},
	note = {arXiv:2409.04398},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Multimedia},
}

@misc{chen_rovi-aug_2024,
	title = {{RoVi}-{Aug}: {Robot} and {Viewpoint} {Augmentation} for {Cross}-{Embodiment} {Robot} {Learning}},
	shorttitle = {{RoVi}-{Aug}},
	url = {http://arxiv.org/abs/2409.03403},
	doi = {10.48550/arXiv.2409.03403},
	abstract = {Scaling up robot learning requires large and diverse datasets, and how to efficiently reuse collected data and transfer policies to new embodiments remains an open question. Emerging research such as the Open-X Embodiment (OXE) project has shown promise in leveraging skills by combining datasets including different robots. However, imbalances in the distribution of robot types and camera angles in many datasets make policies prone to overfit. To mitigate this issue, we propose RoVi-Aug, which leverages state-of-the-art image-to-image generative models to augment robot data by synthesizing demonstrations with different robots and camera views. Through extensive physical experiments, we show that, by training on robot- and viewpoint-augmented data, RoVi-Aug can zero-shot deploy on an unseen robot with significantly different camera angles. Compared to test-time adaptation algorithms such as Mirage, RoVi-Aug requires no extra processing at test time, does not assume known camera angles, and allows policy fine-tuning. Moreover, by co-training on both the original and augmented robot datasets, RoVi-Aug can learn multi-robot and multi-task policies, enabling more efficient transfer between robots and skills and improving success rates by up to 30\%. Project website: https://rovi-aug.github.io.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Chen, Lawrence Yunliang and Xu, Chenfeng and Dharmarajan, Karthik and Irshad, Muhammad Zubair and Cheng, Richard and Keutzer, Kurt and Tomizuka, Masayoshi and Vuong, Quan and Goldberg, Ken},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03403},
	keywords = {Computer Science - Robotics},
}

@misc{wei_occllama_2024,
	title = {{OccLLaMA}: {An} {Occupancy}-{Language}-{Action} {Generative} {World} {Model} for {Autonomous} {Driving}},
	shorttitle = {{OccLLaMA}},
	url = {http://arxiv.org/abs/2409.03272},
	doi = {10.48550/arXiv.2409.03272},
	abstract = {The rise of multi-modal large language models(MLLMs) has spurred their applications in autonomous driving. Recent MLLM-based methods perform action by learning a direct mapping from perception to action, neglecting the dynamics of the world and the relations between action and world dynamics. In contrast, human beings possess world model that enables them to simulate the future states based on 3D internal visual representation and plan actions accordingly. To this end, we propose OccLLaMA, an occupancy-language-action generative world model, which uses semantic occupancy as a general visual representation and unifies vision-language-action(VLA) modalities through an autoregressive model. Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently discretize and reconstruct semantic occupancy scenes, considering its sparsity and classes imbalance. Then, we build a unified multi-modal vocabulary for vision, language and action. Furthermore, we enhance LLM, specifically LLaMA, to perform the next token/scene prediction on the unified vocabulary to complete multiple tasks in autonomous driving. Extensive experiments demonstrate that OccLLaMA achieves competitive performance across multiple tasks, including 4D occupancy forecasting, motion planning, and visual question answering, showcasing its potential as a foundation model in autonomous driving.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Wei, Julong and Yuan, Shanshuai and Li, Pengfei and Hu, Qingda and Gan, Zhongxue and Ding, Wenchao},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03272},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{tian_view-invariant_2024,
	title = {View-{Invariant} {Policy} {Learning} via {Zero}-{Shot} {Novel} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2409.03685},
	doi = {10.48550/arXiv.2409.03685},
	abstract = {Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Tian, Stephen and Wulfe, Blake and Sargent, Kyle and Liu, Katherine and Zakharov, Sergey and Guizilini, Vitor and Wu, Jiajun},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ji_graspsplats_2024,
	title = {{GraspSplats}: {Efficient} {Manipulation} with {3D} {Feature} {Splatting}},
	shorttitle = {{GraspSplats}},
	url = {http://arxiv.org/abs/2409.02084},
	doi = {10.48550/arXiv.2409.02084},
	abstract = {The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Ji, Mazeyu and Qiu, Ri-Zhao and Zou, Xueyan and Wang, Xiaolong},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02084},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhao_egopressure_2024,
	title = {{EgoPressure}: {A} {Dataset} for {Hand} {Pressure} and {Pose} {Estimation} in {Egocentric} {Vision}},
	shorttitle = {{EgoPressure}},
	url = {http://arxiv.org/abs/2409.02224},
	doi = {10.48550/arXiv.2409.02224},
	abstract = {Estimating touch contact and pressure in egocentric vision is a central task for downstream applications in Augmented Reality, Virtual Reality, as well as many robotic applications, because it provides precise physical insights into hand-object interaction and object manipulation. However, existing contact pressure datasets lack egocentric views and hand poses, which are essential for accurate estimation during in-situ operation, both for AR/VR interaction and robotic manipulation. In this paper, we introduce EgoPressure,a novel dataset of touch contact and pressure interaction from an egocentric perspective, complemented with hand pose meshes and fine-grained pressure intensities for each contact. The hand poses in our dataset are optimized using our proposed multi-view sequence-based method that processes footage from our capture rig of 8 accurately calibrated RGBD cameras. EgoPressure comprises 5.0 hours of touch contact and pressure interaction from 21 participants captured by a moving egocentric camera and 7 stationary Kinect cameras, which provided RGB images and depth maps at 30 Hz. In addition, we provide baselines for estimating pressure with different modalities, which will enable future developments and benchmarking on the dataset. Overall, we demonstrate that pressure and hand poses are complementary, which supports our intention to better facilitate the physical understanding of hand-object interactions in AR/VR and robotics research.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Zhao, Yiming and Kwon, Taein and Streli, Paul and Pollefeys, Marc and Holz, Christian},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02224},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
}

@misc{hu_depthcrafter_2024,
	title = {{DepthCrafter}: {Generating} {Consistent} {Long} {Depth} {Sequences} for {Open}-world {Videos}},
	shorttitle = {{DepthCrafter}},
	url = {http://arxiv.org/abs/2409.02095},
	doi = {10.48550/arXiv.2409.02095},
	abstract = {Despite significant advancements in monocular depth estimation for static images, estimating video depth in the open world remains challenging, since open-world videos are extremely diverse in content, motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. DepthCrafter achieves generalization ability to open-world videos by training a video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy with the compiled paired video-depth datasets. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that processes extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Hu, Wenbo and Gao, Xiangjun and Li, Xiaoyu and Zhao, Sijie and Cun, Xiaodong and Zhang, Yong and Quan, Long and Shan, Ying},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02095},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{luo_object_2024,
	title = {Object {Gaussian} for {Monocular} {6D} {Pose} {Estimation} from {Sparse} {Views}},
	url = {http://arxiv.org/abs/2409.02581},
	doi = {10.48550/arXiv.2409.02581},
	abstract = {Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Luo, Luqing and Sun, Shichu and Yang, Jiangang and Zheng, Linfang and Du, Jinwei and Liu, Jian},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02581},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ren_diffusion_2024,
	title = {Diffusion {Policy} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/2409.00588},
	doi = {10.48550/arXiv.2409.00588},
	abstract = {We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Ren, Allen Z. and Lidard, Justin and Ankile, Lars L. and Simeonov, Anthony and Agrawal, Pulkit and Majumdar, Anirudha and Burchfiel, Benjamin and Dai, Hongkai and Simchowitz, Max},
	month = sep,
	year = {2024},
	note = {arXiv:2409.00588},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{huang_rekep_2024,
	title = {{ReKep}: {Spatio}-{Temporal} {Reasoning} of {Relational} {Keypoint} {Constraints} for {Robotic} {Manipulation}},
	shorttitle = {{ReKep}},
	url = {http://arxiv.org/abs/2409.01652},
	doi = {10.48550/arXiv.2409.01652},
	abstract = {Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models. Website at https://rekep-robot.github.io.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and Fei-Fei, Li},
	month = sep,
	year = {2024},
	note = {arXiv:2409.01652},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{jiang_learning_2024,
	title = {Learning to {Singulate} {Objects} in {Packed} {Environments} using a {Dexterous} {Hand}},
	url = {http://arxiv.org/abs/2409.00643},
	doi = {10.48550/arXiv.2409.00643},
	abstract = {Robotic object singulation, where a robot must isolate, grasp, and retrieve a target object in a cluttered environment, is a fundamental challenge in robotic manipulation. This task is difficult due to occlusions and how other objects act as obstacles for manipulation. A robot must also reason about the effect of object-object interactions as it tries to singulate the target. Prior work has explored object singulation in scenarios where there is enough free space to perform relatively long pushes to separate objects, in contrast to when space is tight and objects have little separation from each other. In this paper, we propose the Singulating Objects in Packed Environments (SOPE) framework. We propose a novel method that involves a displacement-based state representation and a multi-phase reinforcement learning procedure that enables singulation using the 16-DOF Allegro Hand. We demonstrate extensive experiments in Isaac Gym simulation, showing the ability of our system to singulate a target object in clutter. We directly transfer the policy trained in simulation to the real world. Over 250 physical robot manipulation trials, our method obtains success rates of 79.2\%, outperforming alternative learning and non-learning methods.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Jiang, Hao and Wang, Yuhai and Zhou, Hanyang and Seita, Daniel},
	month = sep,
	year = {2024},
	note = {arXiv:2409.00643},
	keywords = {Computer Science - Robotics},
}

@misc{lin_flowretrieval_2024,
	title = {{FlowRetrieval}: {Flow}-{Guided} {Data} {Retrieval} for {Few}-{Shot} {Imitation} {Learning}},
	shorttitle = {{FlowRetrieval}},
	url = {http://arxiv.org/abs/2408.16944},
	doi = {10.48550/arXiv.2408.16944},
	abstract = {Few-shot imitation learning relies on only a small amount of task-specific demonstrations to efficiently adapt a policy for a given downstream tasks. Retrieval-based methods come with a promise of retrieving relevant past experiences to augment this target data when learning policies. However, existing data retrieval methods fall under two extremes: they either rely on the existence of exact behaviors with visually similar scenes in the prior data, which is impractical to assume; or they retrieve based on semantic similarity of high-level language descriptions of the task, which might not be that informative about the shared low-level behaviors or motions across tasks that is often a more important factor for retrieving relevant data for policy learning. In this work, we investigate how we can leverage motion similarity in the vast amount of cross-task data to improve few-shot imitation learning of the target task. Our key insight is that motion-similar data carries rich information about the effects of actions and object interactions that can be leveraged during few-shot adaptation. We propose FlowRetrieval, an approach that leverages optical flow representations for both extracting similar motions to target tasks from prior data, and for guiding learning of a policy that can maximally benefit from such data. Our results show FlowRetrieval significantly outperforms prior methods across simulated and real-world domains, achieving on average 27\% higher success rate than the best retrieval-based prior method. In the Pen-in-Cup task with a real Franka Emika robot, FlowRetrieval achieves 3.7x the performance of the baseline imitation learning technique that learns from all prior and target data. Website: https://flow-retrieval.github.io},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Lin, Li-Heng and Cui, Yuchen and Xie, Amber and Hua, Tianyu and Sadigh, Dorsa},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16944},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{meden_bop-d_2024,
	title = {{BOP}-{D}: {Revisiting} {6D} {Pose} {Estimation} {Benchmark} for {Better} {Evaluation} under {Visual} {Ambiguities}},
	shorttitle = {{BOP}-{D}},
	url = {http://arxiv.org/abs/2408.17297},
	doi = {10.48550/arXiv.2408.17297},
	abstract = {Currently, 6D pose estimation methods are benchmarked on datasets that consider, for their ground truth annotations, visual ambiguities as only related to global object symmetries. However, as previously observed [26], visual ambiguities can also happen depending on the viewpoint or the presence of occluding objects, when disambiguating parts become hidden. The visual ambiguities are therefore actually different across images. We thus first propose an automatic method to re-annotate those datasets with a 6D pose distribution specific to each image, taking into account the visibility of the object surface in the image to correctly determine the visual ambiguities. Given this improved ground truth, we re-evaluate the state-of-the-art methods and show this greatly modify the ranking of these methods. Our annotations also allow us to benchmark recent methods able to estimate a pose distribution on real images for the first time. We will make our annotations for the T-LESS dataset and our code publicly available.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Meden, Boris and Brazi, Asma and Bourgeois, Steve and Chamisso, Fabrice Mayran de and Lepetit, Vincent},
	month = aug,
	year = {2024},
	note = {arXiv:2408.17297},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{marticorena_rmmi_2024,
	title = {{RMMI}: {Enhanced} {Obstacle} {Avoidance} for {Reactive} {Mobile} {Manipulation} using an {Implicit} {Neural} {Map}},
	shorttitle = {{RMMI}},
	url = {http://arxiv.org/abs/2408.16206},
	doi = {10.48550/arXiv.2408.16206},
	abstract = {We introduce RMMI, a novel reactive control framework for mobile manipulators operating in complex, static environments. Our approach leverages a neural Signed Distance Field (SDF) to model intricate environment details and incorporates this representation as inequality constraints within a Quadratic Program (QP) to coordinate robot joint and base motion. A key contribution is the introduction of an active collision avoidance cost term that maximises the total robot distance to obstacles during the motion. We first evaluate our approach in a simulated reaching task, outperforming previous methods that rely on representing both the robot and the scene as a set of primitive geometries. Compared with the baseline, we improved the task success rate by 25\% in total, which includes increases of 10\% by using the active collision cost. We also demonstrate our approach on a real-world platform, showing its effectiveness in reaching target poses in cluttered and confined spaces using environment models built directly from sensor data. For additional details and experiment videos, visit https://rmmi.github.io/.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Marticorena, Nicolas and Fischer, Tobias and Haviland, Jesse and Suenderhauf, Niko},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16206},
	keywords = {Computer Science - Robotics},
}

@misc{che_op-align_2024,
	title = {{OP}-{Align}: {Object}-level and {Part}-level {Alignment} for {Self}-supervised {Category}-level {Articulated} {Object} {Pose} {Estimation}},
	shorttitle = {{OP}-{Align}},
	url = {http://arxiv.org/abs/2408.16547},
	doi = {10.48550/arXiv.2408.16547},
	abstract = {Category-level articulated object pose estimation focuses on the pose estimation of unknown articulated objects within known categories. Despite its significance, this task remains challenging due to the varying shapes and poses of objects, expensive dataset annotation costs, and complex real-world environments. In this paper, we propose a novel self-supervised approach that leverages a single-frame point cloud to solve this task. Our model consistently generates reconstruction with a canonical pose and joint state for the entire input object, and it estimates object-level poses that reduce overall pose variance and part-level poses that align each part of the input with its corresponding part of the reconstruction. Experimental results demonstrate that our approach significantly outperforms previous self-supervised methods and is comparable to the state-of-the-art supervised methods. To assess the performance of our model in real-world scenarios, we also introduce a new real-world articulated object benchmark dataset.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Che, Yuchen and Furukawa, Ryo and Kanezaki, Asako},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16547},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fu_-context_2024,
	title = {In-{Context} {Imitation} {Learning} via {Next}-{Token} {Prediction}},
	url = {http://arxiv.org/abs/2408.15980},
	doi = {10.48550/arXiv.2408.15980},
	abstract = {We explore how to enhance next-token prediction models to perform in-context imitation learning on a real robot, where the robot executes new tasks by interpreting contextual information provided during the input phase, without updating its underlying policy parameters. We propose In-Context Robot Transformer (ICRT), a causal transformer that performs autoregressive prediction on sensorimotor trajectories without relying on any linguistic data or reward function. This formulation enables flexible and training-free execution of new tasks at test time, achieved by prompting the model with sensorimotor trajectories of the new task composing of image observations, actions and states tuples, collected through human teleoperation. Experiments with a Franka Emika robot demonstrate that the ICRT can adapt to new tasks specified by prompts, even in environment configurations that differ from both the prompt and the training data. In a multitask environment setup, ICRT significantly outperforms current state-of-the-art next-token prediction models in robotics on generalizing to unseen tasks. Code, checkpoints and data are available on https://icrt.dev/},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Fu, Letian and Huang, Huang and Datta, Gaurav and Chen, Lawrence Yunliang and Panitch, William Chung-Ho and Liu, Fangchen and Li, Hui and Goldberg, Ken},
	month = sep,
	year = {2024},
	note = {arXiv:2408.15980},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhang_demobot_2024,
	title = {{DeMoBot}: {Deformable} {Mobile} {Manipulation} with {Vision}-based {Sub}-goal {Retrieval}},
	shorttitle = {{DeMoBot}},
	url = {http://arxiv.org/abs/2408.15919},
	doi = {10.48550/arXiv.2408.15919},
	abstract = {Imitation learning (IL) algorithms typically distill experience into parametric behavior policies to mimic expert demonstrations. Despite their effectiveness, previous methods often struggle with data efficiency and accurately aligning the current state with expert demonstrations, especially in deformable mobile manipulation tasks characterized by partial observations and dynamic object deformations. In this paper, we introduce {\textbackslash}textbf\{DeMoBot\}, a novel IL approach that directly retrieves observations from demonstrations to guide robots in {\textbackslash}textbf\{De\}formable {\textbackslash}textbf\{Mo\}bile manipulation tasks. DeMoBot utilizes vision foundation models to identify relevant expert data based on visual similarity and matches the current trajectory with demonstrated trajectories using trajectory similarity and forward reachability constraints to select suitable sub-goals. Once a goal is determined, a motion generation policy will guide the robot to the next state until the task is completed. We evaluated DeMoBot using a Spot robot in several simulated and real-world settings, demonstrating its effectiveness and generalizability. With only 20 demonstrations, DeMoBot significantly outperforms the baselines, reaching a 50{\textbackslash}\% success rate in curtain opening and 85{\textbackslash}\% in gap covering in simulation.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Zhang, Yuying and Yang, Wenyan and Pajarinen, Joni},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15919},
	keywords = {Computer Science - Robotics},
}

@misc{wang_skillmimic_2024,
	title = {{SkillMimic}: {Learning} {Reusable} {Basketball} {Skills} from {Demonstrations}},
	shorttitle = {{SkillMimic}},
	url = {http://arxiv.org/abs/2408.15270},
	doi = {10.48550/arXiv.2408.15270},
	abstract = {Mastering basketball skills such as diverse layups and dribbling involves complex interactions with the ball and requires real-time adjustments. Traditional reinforcement learning methods for interaction skills rely on labor-intensive, manually designed rewards that do not generalize well across different skills. Inspired by how humans learn from demonstrations, we propose SkillMimic, a data-driven approach that mimics both human and ball motions to learn a wide variety of basketball skills. SkillMimic employs a unified configuration to learn diverse skills from human-ball motion datasets, with skill diversity and generalization improving as the dataset grows. This approach allows training a single policy to learn multiple skills, enabling smooth skill switching even if these switches are not present in the reference dataset. The skills acquired by SkillMimic can be easily reused by a high-level controller to accomplish complex basketball tasks. To evaluate our approach, we introduce two basketball datasets: one estimated through monocular RGB videos and the other using advanced motion capture equipment, collectively containing about 35 minutes of diverse basketball skills. Experiments show that our method can effectively learn various basketball skills included in the dataset with a unified configuration, including various styles of dribbling, layups, and shooting. Furthermore, by training a high-level controller to reuse the acquired skills, we can achieve complex basketball tasks such as layup scoring, which involves dribbling toward the basket, timing the dribble and layup to score, retrieving the rebound, and repeating the process. The project page and video demonstrations are available at https://ingrid789.github.io/SkillMimic/},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Wang, Yinhuai and Zhao, Qihan and Yu, Runyi and Zeng, Ailing and Lin, Jing and Luo, Zhengyi and Tsui, Hok Wai and Yu, Jiwen and Li, Xiu and Chen, Qifeng and Zhang, Jian and Zhang, Lei and Tan, Ping},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15270},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lou_robo-gs_2024,
	title = {Robo-{GS}: {A} {Physics} {Consistent} {Spatial}-{Temporal} {Model} for {Robotic} {Arm} with {Hybrid} {Representation}},
	shorttitle = {Robo-{GS}},
	url = {http://arxiv.org/abs/2408.14873},
	doi = {10.48550/arXiv.2408.14873},
	abstract = {Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes. We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms. This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods. The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Lou, Haozhe and Liu, Yurong and Pan, Yike and Geng, Yiran and Chen, Jianteng and Ma, Wenlong and Li, Chenglong and Wang, Lin and Feng, Hengzhen and Shi, Lu and Luo, Liyi and Shi, Yongliang},
	month = sep,
	year = {2024},
	note = {arXiv:2408.14873},
	keywords = {Computer Science - Numerical Analysis, Computer Science - Robotics, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
}

@misc{yang_driving_2024,
	title = {Driving in the {Occupancy} {World}: {Vision}-{Centric} {4D} {Occupancy} {Forecasting} and {Planning} via {World} {Models} for {Autonomous} {Driving}},
	shorttitle = {Driving in the {Occupancy} {World}},
	url = {http://arxiv.org/abs/2408.14197},
	doi = {10.48550/arXiv.2408.14197},
	abstract = {World models envision potential future states based on various ego actions. They embed extensive knowledge about the driving environment, facilitating safe and scalable autonomous driving. Most existing methods primarily focus on either data generation or the pretraining paradigms of world models. Unlike the aforementioned prior works, we propose Drive-OccWorld, which adapts a vision-centric 4D forecasting world model to end-to-end planning for autonomous driving. Specifically, we first introduce a semantic and motion-conditional normalization in the memory module, which accumulates semantic and dynamic information from historical BEV embeddings. These BEV features are then conveyed to the world decoder for future occupancy and flow forecasting, considering both geometry and spatiotemporal modeling. Additionally, we propose injecting flexible action conditions, such as velocity, steering angle, trajectory, and commands, into the world model to enable controllable generation and facilitate a broader range of downstream applications. Furthermore, we explore integrating the generative capabilities of the 4D world model with end-to-end planning, enabling continuous forecasting of future states and the selection of optimal trajectories using an occupancy-based cost function. Extensive experiments on the nuScenes dataset demonstrate that our method can generate plausible and controllable 4D occupancy, opening new avenues for driving world generation and end-to-end planning.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Yang, Yu and Mei, Jianbiao and Ma, Yukai and Du, Siliang and Chen, Wenqing and Qian, Yijie and Feng, Yuxiang and Liu, Yong},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14197},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gu_advancing_2024,
	title = {Advancing {Humanoid} {Locomotion}: {Mastering} {Challenging} {Terrains} with {Denoising} {World} {Model} {Learning}},
	shorttitle = {Advancing {Humanoid} {Locomotion}},
	url = {http://arxiv.org/abs/2408.14472},
	doi = {10.48550/arXiv.2408.14472},
	abstract = {Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Gu, Xinyang and Wang, Yen-Jen and Zhu, Xiang and Shi, Chengming and Guo, Yanjiang and Liu, Yichen and Chen, Jianyu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14472},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{smart_splatt3r_2024,
	title = {{Splatt3R}: {Zero}-shot {Gaussian} {Splatting} from {Uncalibrated} {Image} {Pairs}},
	shorttitle = {{Splatt3R}},
	url = {http://arxiv.org/abs/2408.13912},
	doi = {10.48550/arXiv.2408.13912},
	abstract = {In this paper, we introduce Splatt3R, a pose-free, feed-forward method for in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without requiring any camera parameters or depth information. For generalizability, we build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R, by extending it to deal with both 3D structure and appearance. Specifically, unlike the original MASt3R which reconstructs only 3D point clouds, we predict the additional Gaussian attributes required to construct a Gaussian primitive for each point. Hence, unlike other novel view synthesis methods, Splatt3R is first trained by optimizing the 3D point cloud's geometry loss, and then a novel view synthesis objective. By doing this, we avoid the local minima present in training 3D Gaussian Splats from stereo views. We also propose a novel loss masking strategy that we empirically find is critical for strong performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++ dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and the resultant splats can be rendered in real-time.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Smart, Brandon and Zheng, Chuanxia and Laina, Iro and Prisacariu, Victor Adrian},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13912},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{luo_physpart_2024,
	title = {{PhysPart}: {Physically} {Plausible} {Part} {Completion} for {Interactable} {Objects}},
	shorttitle = {{PhysPart}},
	url = {http://arxiv.org/abs/2408.13724},
	doi = {10.48550/arXiv.2408.13724},
	abstract = {Interactable objects are ubiquitous in our daily lives. Recent advances in 3D generative models make it possible to automate the modeling of these objects, benefiting a range of applications from 3D printing to the creation of robot simulation environments. However, while significant progress has been made in modeling 3D shapes and appearances, modeling object physics, particularly for interactable objects, remains challenging due to the physical constraints imposed by inter-part motions. In this paper, we tackle the problem of physically plausible part completion for interactable objects, aiming to generate 3D parts that not only fit precisely into the object but also allow smooth part motions. To this end, we propose a diffusion-based part generation model that utilizes geometric conditioning through classifier-free guidance and formulates physical constraints as a set of stability and mobility losses to guide the sampling process. Additionally, we demonstrate the generation of dependent parts, paving the way toward sequential part generation for objects with complex part-whole hierarchies. Experimentally, we introduce a new metric for measuring physical plausibility based on motion success rates. Our model outperforms existing baselines over shape and physical metrics, especially those that do not adequately model physical constraints. We also demonstrate our applications in 3D printing, robot manipulation, and sequential part generation, showing our strength in realistic tasks with the demand for high physical plausibility.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Luo, Rundong and Geng, Haoran and Deng, Congyue and Li, Puhao and Wang, Zan and Jia, Baoxiong and Guibas, Leonidas and Huang, Siyuan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13724},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{igl_deep_2018,
	title = {Deep {Variational} {Reinforcement} {Learning} for {POMDPs}},
	url = {https://proceedings.mlr.press/v80/igl18a.html},
	abstract = {Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of rewards and incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.},
	language = {en},
	urldate = {2024-10-09},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Igl, Maximilian and Zintgraf, Luisa and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2117--2126},
}

@inproceedings{singh_structured_2021,
	title = {Structured {World} {Belief} for {Reinforcement} {Learning} in {POMDP}},
	url = {https://proceedings.mlr.press/v139/singh21a.html},
	abstract = {Object-centric world models provide structured representation of the scene and can be an important backbone in reinforcement learning and planning. However, existing approaches suffer in partially-observable environments due to the lack of belief states. In this paper, we propose Structured World Belief, a model for learning and inference of object-centric belief states. Inferred by Sequential Monte Carlo (SMC), our belief states provide multiple object-centric scene hypotheses. To synergize the benefits of SMC particles with object representations, we also propose a new object-centric dynamics model that considers the inductive bias of object permanence. This enables tracking of object states even when they are invisible for a long time. To further facilitate object tracking in this regime, we allow our model to attend flexibly to any spatial location in the image which was restricted in previous models. In experiments, we show that object-centric belief provides a more accurate and robust performance for filtering and generation. Furthermore, we show the efficacy of structured world belief in improving the performance of reinforcement learning, planning and supervised reasoning.},
	language = {en},
	urldate = {2024-10-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Singh, Gautam and Peri, Skand and Kim, Junghyun and Kim, Hyunseok and Ahn, Sungjin},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9744--9755},
}

@inproceedings{yarats_mastering_2021,
	title = {Mastering {Visual} {Continuous} {Control}: {Improved} {Data}-{Augmented} {Reinforcement} {Learning}},
	shorttitle = {Mastering {Visual} {Continuous} {Control}},
	url = {https://openreview.net/forum?id=_SJ-_yyes8},
	abstract = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2 's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
	language = {en},
	urldate = {2024-10-09},
	author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
	month = oct,
	year = {2021},
}

@inproceedings{lee_stochastic_2020,
	title = {Stochastic {Latent} {Actor}-{Critic}: {Deep} {Reinforcement} {Learning} with a {Latent} {Variable} {Model}},
	volume = {33},
	shorttitle = {Stochastic {Latent} {Actor}-{Critic}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/08058bf500242562c0d031ff830ad094-Abstract.html},
	abstract = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of  challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.},
	urldate = {2024-10-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Alex X. and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
	year = {2020},
	pages = {741--752},
}

@inproceedings{hogewind_safe_2022,
	title = {Safe {Reinforcement} {Learning} {From} {Pixels} {Using} a {Stochastic} {Latent} {Representation}},
	url = {https://openreview.net/forum?id=b39dQt_uffW},
	abstract = {We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints.},
	language = {en},
	urldate = {2024-10-09},
	author = {Hogewind, Yannick and Simão, Thiago D. and Kachman, Tal and Jansen, Nils},
	month = sep,
	year = {2022},
}

@misc{rafailov_direct_2024,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2024-10-08},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = jul,
	year = {2024},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{folgoc_is_2021,
	title = {Is {MC} {Dropout} {Bayesian}?},
	url = {https://arxiv.org/abs/2110.04286v1},
	abstract = {MC Dropout is a mainstream "free lunch" method in medical imaging for approximate Bayesian computations (ABC). Its appeal is to solve out-of-the-box the daunting task of ABC and uncertainty quantification in Neural Networks (NNs); to fall within the variational inference (VI) framework; and to propose a highly multimodal, faithful predictive posterior. We question the properties of MC Dropout for approximate inference, as in fact MC Dropout changes the Bayesian model; its predictive posterior assigns \$0\$ probability to the true model on closed-form benchmarks; the multimodality of its predictive posterior is not a property of the true predictive posterior but a design artefact. To address the need for VI on arbitrary models, we share a generic VI engine within the pytorch framework. The code includes a carefully designed implementation of structured (diagonal plus low-rank) multivariate normal variational families, and mixtures thereof. It is intended as a go-to no-free-lunch approach, addressing shortcomings of mean-field VI with an adjustable trade-off between expressivity and computational complexity.},
	language = {en},
	urldate = {2024-10-08},
	author = {Folgoc, Loic Le and Baltatzis, Vasileios and Desai, Sujal and Devaraj, Anand and Ellis, Sam and Manzanera, Octavio E. Martinez and Nair, Arjun and Qiu, Huaqi and Schnabel, Julia and Glocker, Ben},
	month = oct,
	year = {2021},
}

@inproceedings{curtis_partially_2024,
	title = {Partially {Observable} {Task} and {Motion} {Planning} with {Uncertainty} and {Risk} {Awareness}},
	volume = {20},
	isbn = {9798990284807},
	url = {https://www.roboticsproceedings.org/rss20/p118.html},
	urldate = {2024-10-04},
	author = {Curtis, Aidan and Matheos, George and Gothoskar, Nishad and Mansinghka, Vikash and Tenenbaum, Joshua B. and Lozano-Pérez, Tomás and Kaelbling, Leslie Pack},
	month = jul,
	year = {2024},
}

@article{iskandar_intrinsic_2024,
	title = {Intrinsic sense of touch for intuitive physical human-robot interaction},
	copyright = {Copyright © 2024 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
	url = {https://www.science.org/doi/10.1126/scirobotics.adn4008},
	doi = {10.1126/scirobotics.adn4008},
	abstract = {Force-torque sensors enable an intrinsic sense of touch without tactile skins for physical human-robot interaction.},
	language = {EN},
	urldate = {2024-10-02},
	journal = {Science Robotics},
	author = {Iskandar, Maged and Albu-Schäffer, Alin and Dietrich, Alexander},
	month = aug,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
}

@inproceedings{racaniere_imagination-augmented_2017,
	title = {Imagination-{Augmented} {Agents} for {Deep} {Reinforcement} {Learning}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html},
	abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects.   In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a trained environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several strong baselines.},
	urldate = {2024-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Racanière, Sébastien and Weber, Theophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdomènech Badia, Adrià and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
	year = {2017},
}

@inproceedings{ha_recurrent_2018,
	title = {Recurrent {World} {Models} {Facilitate} {Policy} {Evolution}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html},
	abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
	urldate = {2024-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ha, David and Schmidhuber, Jürgen},
	year = {2018},
}

@misc{nagabandi_neural_2017,
	title = {Neural {Network} {Dynamics} for {Model}-{Based} {Deep} {Reinforcement} {Learning} with {Model}-{Free} {Fine}-{Tuning}},
	url = {https://arxiv.org/abs/1708.02596v2},
	abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
	language = {en},
	urldate = {2024-10-02},
	author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
	month = aug,
	year = {2017},
}

@misc{schulman_equivalence_2017,
	title = {Equivalence {Between} {Policy} {Gradients} and {Soft} {Q}-{Learning}},
	url = {https://arxiv.org/abs/1704.06440v4},
	abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and \$Q\$-learning methods. \$Q\$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the \$Q\$-values they estimate are very inaccurate. A partial explanation may be that \$Q\$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between \$Q\$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) \$Q\$-learning is exactly equivalent to a policy gradient method. We also point out a connection between \$Q\$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of \$Q\$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a \$Q\$-learning method that closely matches the learning dynamics of A3C without using a target network or \${\textbackslash}epsilon\$-greedy exploration schedule.},
	language = {en},
	urldate = {2024-10-02},
	author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
	month = apr,
	year = {2017},
}

@misc{zhang_shapeicp_2024,
	title = {{ShapeICP}: {Iterative} {Category}-level {Object} {Pose} and {Shape} {Estimation} from {Depth}},
	shorttitle = {{ShapeICP}},
	url = {http://arxiv.org/abs/2408.13147},
	doi = {10.48550/arXiv.2408.13147},
	abstract = {Category-level object pose and shape estimation from a single depth image has recently drawn research attention due to its wide applications in robotics and self-driving. The task is particularly challenging because the three unknowns, object pose, object shape, and model-to-measurement correspondences, are compounded together but only a single view of depth measurements is provided. The vast majority of the prior work heavily relies on data-driven approaches to obtain solutions to at least one of the unknowns and typically two, running with the risk of failing to generalize to unseen domains. The shape representations used in the prior work also mainly focus on point cloud and signed distance field (SDF). In stark contrast to the prior work, we approach the problem using an iterative estimation method that does not require learning from any pose-annotated data. In addition, we adopt a novel mesh-based object active shape model that has not been explored by the previous literature. Our algorithm, named ShapeICP, has its foundation in the iterative closest point (ICP) algorithm but is equipped with additional features for the category-level pose and shape estimation task. The results show that even without using any pose-annotated data, ShapeICP surpasses many data-driven approaches that rely on the pose data for training, opening up new solution space for researchers to consider.},
	urldate = {2024-10-02},
	publisher = {arXiv},
	author = {Zhang, Yihao and Leonard, John J.},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13147 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/2005.01643v3},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	language = {en},
	urldate = {2024-10-02},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = may,
	year = {2020},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{yang_multi-finger_2024,
	title = {Multi-finger {Manipulation} via {Trajectory} {Optimization} with {Differentiable} {Rolling} and {Geometric} {Constraints}},
	url = {http://arxiv.org/abs/2408.13229},
	doi = {10.48550/arXiv.2408.13229},
	abstract = {Parameterizing finger rolling and finger-object contacts in a differentiable manner is important for formulating dexterous manipulation as a trajectory optimization problem. In contrast to previous methods which often assume simplified geometries of the robot and object or do not explicitly model finger rolling, we propose a method to further extend the capabilities of dexterous manipulation by accounting for non-trivial geometries of both the robot and the object. By integrating the object's Signed Distance Field (SDF) with a sampling method, our method estimates contact and rolling-related variables and includes those in a trajectory optimization framework. This formulation naturally allows for the emergence of finger-rolling behaviors, enabling the robot to locally adjust the contact points. Our method is tested in a peg alignment task and a screwdriver turning task, where it outperforms the baselines in terms of achieving desired object configurations and avoiding dropping the object. We also successfully apply our method to a real-world screwdriver turning task, demonstrating its robustness to the sim2real gap.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Yang, Fan and Power, Thomas and Marinovic, Sergio Aguilera and Iba, Soshi and Zarrin, Rana Soltani and Berenson, Dmitry},
	month = aug,
	year = {2024},
	note = {arXiv:2408.13229 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{he_s4d_2024,
	title = {{S4D}: {Streaming} {4D} {Real}-{World} {Reconstruction} with {Gaussians} and {3D} {Control} {Points}},
	shorttitle = {{S4D}},
	url = {https://arxiv.org/abs/2408.13036v1},
	abstract = {Recently, the dynamic scene reconstruction using Gaussians has garnered increased interest. Mainstream approaches typically employ a global deformation field to warp a 3D scene in the canonical space. However, the inherently low-frequency nature of implicit neural fields often leads to ineffective representations of complex motions. Moreover, their structural rigidity can hinder adaptation to scenes with varying resolutions and durations. To overcome these challenges, we introduce a novel approach utilizing discrete 3D control points. This method models local rays physically and establishes a motion-decoupling coordinate system, which effectively merges traditional graphics with learnable pipelines for a robust and efficient local 6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have developed a generalized framework that incorporates our control points with Gaussians. Starting from an initial 3D reconstruction, our workflow decomposes the streaming 4D real-world reconstruction into four independent submodules: 3D segmentation, 3D control points generation, object-wise motion manipulation, and residual compensation. Our experiments demonstrate that this method outperforms existing state-of-the-art 4D Gaussian Splatting techniques on both the Neu3DV and CMU-Panoptic datasets. Our approach also significantly accelerates training, with the optimization of our 3D control points achievable within just 2 seconds per frame on a single NVIDIA 4070 GPU.},
	language = {en},
	urldate = {2024-10-01},
	author = {He, Bing and Chen, Yunuo and Lu, Guo and Song, Li and Zhang, Wenjun},
	month = aug,
	year = {2024},
}

@misc{zhang_visual_2024,
	title = {Visual {Localization} in {3D} {Maps}: {Comparing} {Point} {Cloud}, {Mesh}, and {NeRF} {Representations}},
	shorttitle = {Visual {Localization} in {3D} {Maps}},
	url = {http://arxiv.org/abs/2408.11966},
	doi = {10.48550/arXiv.2408.11966},
	abstract = {This paper introduces and assesses a cross-modal global visual localization system that can localize camera images within a color 3D map representation built using both visual and lidar sensing. We present three different state-of-the-art methods for creating the color 3D maps: point clouds, meshes, and neural radiance fields (NeRF). Our system constructs a database of synthetic RGB and depth image pairs from these representations. This database serves as the basis for global localization. We present an automatic approach that builds this database by synthesizing novel images of the scene and exploiting the 3D structure encoded in the different representations. Next, we present a global localization system that relies on the synthetic image database to accurately estimate the 6 DoF camera poses of monocular query images. Our localization approach relies on different learning-based global descriptors and feature detectors which enable robust image retrieval and matching despite the domain gap between (real) query camera images and the synthetic database images. We assess the system's performance through extensive real-world experiments in both indoor and outdoor settings, in order to evaluate the effectiveness of each map representation and the benefits against traditional structure-from-motion localization approaches. Our results show that all three map representations can achieve consistent localization success rates of 55\% and higher across various environments. NeRF synthesized images show superior performance, localizing query images at an average success rate of 72\%. Furthermore, we demonstrate that our synthesized database enables global localization even when the map creation data and the localization sequence are captured when travelling in opposite directions. Our system, operating in real-time on a mobile laptop equipped with a GPU, achieves a processing rate of 1Hz.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zhang, Lintong and Tao, Yifu and Lin, Jiarong and Zhang, Fu and Fallon, Maurice},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11966 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_tactile-morph_2024,
	title = {Tactile-{Morph} {Skills}: {Energy}-{Based} {Control} {Meets} {Data}-{Driven} {Learning}},
	shorttitle = {Tactile-{Morph} {Skills}},
	url = {http://arxiv.org/abs/2408.12285},
	doi = {10.48550/arXiv.2408.12285},
	abstract = {Robotic manipulation is essential for modernizing factories and automating industrial tasks like polishing, which require advanced tactile abilities. These robots must be easily set up, safely work with humans, learn tasks autonomously, and transfer skills to similar tasks. Addressing these needs, we introduce the tactile-morph skill framework, which integrates unified force-impedance control with data-driven learning. Our system adjusts robot movements and force application based on estimated energy levels for the desired trajectory and force profile, ensuring safety by stopping if energy allocated for the control runs out. Using a Temporal Convolutional Network, we estimate the energy distribution for a given motion and force profile, enabling skill transfer across different tasks and surfaces. Our approach maintains stability and performance even on unfamiliar geometries with similar friction characteristics, demonstrating improved accuracy, zero-shot transferable performance, and enhanced safety in real-world scenarios. This framework promises to enhance robotic capabilities in industrial settings, making intelligent robots more accessible and valuable.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zhang, Anran and Karacan, Kübra and Sadeghian, Hamid and Wu, Yansong and Wu, Fan and Haddadin, Sami},
	month = aug,
	year = {2024},
	note = {arXiv:2408.12285 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{xu_embodiedsam_2024,
	title = {{EmbodiedSAM}: {Online} {Segment} {Any} {3D} {Thing} in {Real} {Time}},
	shorttitle = {{EmbodiedSAM}},
	url = {http://arxiv.org/abs/2408.11811},
	doi = {10.48550/arXiv.2408.11811},
	abstract = {Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is almost infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves leading performance even compared with offline methods. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in open-vocabulary and data-efficient setting. Code and demo are available at https://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for training and evaluation.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Xu, Xiuwei and Chen, Huangxing and Zhao, Linqing and Wang, Ziwei and Zhou, Jie and Lu, Jiwen},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{pu_pano2room_2024,
	title = {{Pano2Room}: {Novel} {View} {Synthesis} from a {Single} {Indoor} {Panorama}},
	shorttitle = {{Pano2Room}},
	url = {http://arxiv.org/abs/2408.11413},
	doi = {10.1145/3680528.3687616},
	abstract = {Recent single-view 3D generative methods have made significant advancements by leveraging knowledge distilled from extensive 3D object datasets. However, challenges persist in the synthesis of 3D scenes from a single view, primarily due to the complexity of real-world environments and the limited availability of high-quality prior resources. In this paper, we introduce a novel approach called Pano2Room, designed to automatically reconstruct high-quality 3D indoor scenes from a single panoramic image. These panoramic images can be easily generated using a panoramic RGBD inpainter from captures at a single location with any camera. The key idea is to initially construct a preliminary mesh from the input panorama, and iteratively refine this mesh using a panoramic RGBD inpainter while collecting photo-realistic 3D-consistent pseudo novel views. Finally, the refined mesh is converted into a 3D Gaussian Splatting field and trained with the collected pseudo novel views. This pipeline enables the reconstruction of real-world 3D scenes, even in the presence of large occlusions, and facilitates the synthesis of photo-realistic novel views with detailed geometry. Extensive qualitative and quantitative experiments have been conducted to validate the superiority of our method in single-panorama indoor novel synthesis compared to the state-of-the-art. Our code and data are available at {\textbackslash}url\{https://github.com/TrickyGo/Pano2Room\}.},
	urldate = {2024-10-01},
	author = {Pu, Guo and Zhao, Yiming and Lian, Zhouhui},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zheng_survey_2024,
	title = {A {Survey} of {Embodied} {Learning} for {Object}-{Centric} {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2408.11537},
	doi = {10.48550/arXiv.2408.11537},
	abstract = {Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI. It is crucial for advancing next-generation intelligent robots and has garnered significant interest recently. Unlike data-driven machine learning methods, embodied learning focuses on robot learning through physical interaction with the environment and perceptual feedback, making it especially suitable for robotic manipulation. In this paper, we provide a comprehensive survey of the latest advancements in this field and categorize the existing work into three main branches: 1) Embodied perceptual learning, which aims to predict object pose and affordance through various data representations; 2) Embodied policy learning, which focuses on generating optimal robotic decisions using methods such as reinforcement learning and imitation learning; 3) Embodied task-oriented learning, designed to optimize the robot's performance based on the characteristics of different tasks in object grasping and manipulation. In addition, we offer an overview and discussion of public datasets, evaluation metrics, representative applications, current challenges, and potential future research directions. A project associated with this survey has been established at https://github.com/RayYoh/OCRM\_survey.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zheng, Ying and Yao, Lei and Su, Yuejiao and Zhang, Yi and Wang, Yi and Zhao, Sicheng and Zhang, Yiyi and Chau, Lap-Pui},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{doshi_scaling_2024,
	title = {Scaling {Cross}-{Embodied} {Learning}: {One} {Policy} for {Manipulation}, {Navigation}, {Locomotion} and {Aviation}},
	shorttitle = {Scaling {Cross}-{Embodied} {Learning}},
	url = {http://arxiv.org/abs/2408.11812},
	doi = {10.48550/arXiv.2408.11812},
	abstract = {Modern machine learning systems rely on large datasets to attain broad generalization, and this often poses a challenge in robot learning, where each robotic platform and task might have only a small dataset. By training a single policy across many different kinds of robots, a robot learning method can leverage much broader and more diverse datasets, which in turn can lead to better generalization and robustness. However, training a single policy on multi-robot data is challenging because robots can have widely varying sensors, actuators, and control frequencies. We propose CrossFormer, a scalable and flexible transformer-based policy that can consume data from any embodiment. We train CrossFormer on the largest and most diverse dataset to date, 900K trajectories across 20 different robot embodiments. We demonstrate that the same network weights can control vastly different robots, including single and dual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds. Unlike prior work, our model does not require manual alignment of the observation or action spaces. Extensive experiments in the real world show that our method matches the performance of specialist policies tailored for each embodiment, while also significantly outperforming the prior state of the art in cross-embodiment learning.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Doshi, Ria and Walke, Homer and Mees, Oier and Dasari, Sudeep and Levine, Sergey},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11812 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zheng_3d_2021,
	title = {{3D} {Human} {Pose} {Estimation} with {Spatial} and {Temporal} {Transformers}},
	url = {http://arxiv.org/abs/2103.10455},
	doi = {10.48550/arXiv.2103.10455},
	abstract = {Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at {\textbackslash}url\{https://github.com/zczcwh/PoseFormer\}},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zheng, Ce and Zhu, Sijie and Mendieta, Matias and Yang, Taojiannan and Chen, Chen and Ding, Zhengming},
	month = aug,
	year = {2021},
	note = {arXiv:2103.10455 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
}

@misc{zhong_rumi_2024,
	title = {{RUMI}: {Rummaging} {Using} {Mutual} {Information}},
	shorttitle = {{RUMI}},
	url = {http://arxiv.org/abs/2408.10450},
	doi = {10.48550/arXiv.2408.10450},
	abstract = {This paper presents Rummaging Using Mutual Information (RUMI), a method for online generation of robot action sequences to gather information about the pose of a known movable object in visually-occluded environments. Focusing on contact-rich rummaging, our approach leverages mutual information between the object pose distribution and robot trajectory for action planning. From an observed partial point cloud, RUMI deduces the compatible object pose distribution and approximates the mutual information of it with workspace occupancy in real time. Based on this, we develop an information gain cost function and a reachability cost function to keep the object within the robot's reach. These are integrated into a model predictive control (MPC) framework with a stochastic dynamics model, updating the pose distribution in a closed loop. Key contributions include a new belief framework for object pose estimation, an efficient information gain computation strategy, and a robust MPC-based control scheme. RUMI demonstrates superior performance in both simulated and real tasks compared to baseline methods.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zhong, Sheng and Fazeli, Nima and Berenson, Dmitry},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10450 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, I.2.9},
}

@misc{zhao_rp1m_2024,
	title = {{RP1M}: {A} {Large}-{Scale} {Motion} {Dataset} for {Piano} {Playing} with {Bi}-{Manual} {Dexterous} {Robot} {Hands}},
	shorttitle = {{RP1M}},
	url = {http://arxiv.org/abs/2408.11048},
	doi = {10.48550/arXiv.2408.11048},
	abstract = {It has been a long-standing research goal to endow robot hands with human-level dexterity. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Zhao, Yi and Chen, Le and Schneider, Jan and Gao, Quankai and Kannala, Juho and Schölkopf, Bernhard and Pajarinen, Joni and Büchler, Dieter},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11048 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{newbury_deep_2023,
	title = {Deep {Learning} {Approaches} to {Grasp} {Synthesis}: {A} {Review}},
	volume = {39},
	issn = {1941-0468},
	shorttitle = {Deep {Learning} {Approaches} to {Grasp} {Synthesis}},
	url = {https://ieeexplore.ieee.org/document/10149823},
	doi = {10.1109/TRO.2023.3280597},
	abstract = {Grasping is the process of picking up an object by applying forces and torques at a set of contacts. Recent advances in deep learning methods have allowed rapid progress in robotic object grasping. In this systematic review, we surveyed the publications over the last decade, with a particular interest in grasping an object using all six degrees of freedom of the end-effector pose. Our review found four common methodologies for robotic grasping: sampling-based approaches, direct regression, reinforcement learning, and exemplar approaches In addition, we found two “supporting methods” around grasping that use deep learning to support the grasping process, shape approximation, and affordances. We have distilled the publications found in this systematic review (85 papers) into ten key takeaways we consider crucial for future robotic grasping and manipulation research.},
	number = {5},
	urldate = {2024-10-01},
	journal = {IEEE Transactions on Robotics},
	author = {Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, Jürgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and Fox, Dieter and Cosgun, Akansel},
	month = oct,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Deep learning, Dexterous manipulation, Force, Grasping, Grippers, Shape, Systematics, Task analysis, deep learning in robotics and automation, grasping, perception for grasping and manipulation},
	pages = {3994--4015},
}

@misc{liu_meshformer_2024,
	title = {{MeshFormer}: {High}-{Quality} {Mesh} {Generation} with {3D}-{Guided} {Reconstruction} {Model}},
	shorttitle = {{MeshFormer}},
	url = {https://arxiv.org/abs/2408.10198v1},
	abstract = {Open-world 3D reconstruction models have recently garnered significant attention. However, without sufficient 3D inductive bias, existing methods typically entail expensive training costs and struggle to extract high-quality 3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. Specifically, instead of using a triplane representation, we store features in 3D sparse voxels and combine transformers with 3D convolutions to leverage an explicit 3D structure and projective bias. In addition to sparse-view RGB input, we require the network to take input and generate corresponding normal maps. The input normal maps can be predicted by 2D diffusion models, significantly aiding in the guidance and refinement of the geometry's learning. Moreover, by combining Signed Distance Function (SDF) supervision with surface rendering, we directly learn to generate high-quality meshes without the need for complex multi-stage training processes. By incorporating these explicit 3D biases, MeshFormer can be trained efficiently and deliver high-quality textured meshes with fine-grained geometric details. It can also be integrated with 2D diffusion models to enable fast single-image-to-3D and text-to-3D tasks. Project page: https://meshformer3d.github.io},
	language = {en},
	urldate = {2024-09-27},
	author = {Liu, Minghua and Zeng, Chong and Wei, Xinyue and Shi, Ruoxi and Chen, Linghao and Xu, Chao and Zhang, Mengqi and Wang, Zhaoning and Zhang, Xiaoshuai and Liu, Isabella and Wu, Hongzhi and Su, Hao},
	month = aug,
	year = {2024},
}

@misc{didolkar_zero-shot_2024,
	title = {Zero-{Shot} {Object}-{Centric} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2408.09162},
	doi = {10.48550/arXiv.2408.09162},
	abstract = {The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing pre-trained self-supervised features. However, so far, object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the wider trend in machine learning towards general-purpose models directly applicable to unseen data and tasks. Thus, in this work, we study current object-centric methods through the lens of zero-shot generalization by introducing a benchmark comprising eight different synthetic and real-world datasets. We analyze the factors influencing zero-shot performance and find that training on diverse real-world images improves transferability to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Didolkar, Aniket and Zadaianchuk, Andrii and Goyal, Anirudh and Mozer, Mike and Bengio, Yoshua and Martius, Georg and Seitzer, Maximilian},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09162 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xu_sparp_2024,
	title = {{SpaRP}: {Fast} {3D} {Object} {Reconstruction} and {Pose} {Estimation} from {Sparse} {Views}},
	shorttitle = {{SpaRP}},
	url = {http://arxiv.org/abs/2408.10195},
	doi = {10.48550/arXiv.2408.10195},
	abstract = {Open-world 3D generation has recently attracted considerable attention. While many single-image-to-3D methods have yielded visually appealing outcomes, they often lack sufficient controllability and tend to produce hallucinated regions that may not align with users' expectations. In this paper, we explore an important scenario in which the input consists of one or a few unposed 2D images of a single object, with little or no overlap. We propose a novel method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative camera poses for these sparse-view images. SpaRP distills knowledge from 2D diffusion models and finetunes them to implicitly deduce the 3D spatial relationships between the sparse views. The diffusion model is trained to jointly predict surrogate representations for camera poses and multi-view images of the object under known poses, integrating all information from the input sparse views. These predictions are then leveraged to accomplish 3D reconstruction and pose estimation, and the reconstructed 3D model can be used to further refine the camera poses of input views. Through extensive experiments on three datasets, we demonstrate that our method not only significantly outperforms baseline methods in terms of 3D reconstruction quality and pose prediction accuracy but also exhibits strong efficiency. It requires only about 20 seconds to produce a textured mesh and camera poses for the input views. Project page: https://chaoxu.xyz/sparp.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Xu, Chao and Li, Ang and Chen, Linghao and Liu, Yulin and Shi, Ruoxi and Su, Hao and Liu, Minghua},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10195 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{li_learning_2024,
	title = {Learning {Precise} {Affordances} from {Egocentric} {Videos} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2408.10123},
	doi = {10.48550/arXiv.2408.10123},
	abstract = {Affordance, defined as the potential actions that an object offers, is crucial for robotic manipulation tasks. A deep understanding of affordance can lead to more intelligent AI systems. For example, such knowledge directs an agent to grasp a knife by the handle for cutting and by the blade when passing it to someone. In this paper, we present a streamlined affordance learning system that encompasses data collection, effective model training, and robot deployment. First, we collect training data from egocentric videos in an automatic manner. Different from previous methods that focus only on the object graspable affordance and represent it as coarse heatmaps, we cover both graspable (e.g., object handles) and functional affordances (e.g., knife blades, hammer heads) and extract data with precise segmentation masks. We then propose an effective model, termed Geometry-guided Affordance Transformer (GKT), to train on the collected data. GKT integrates an innovative Depth Feature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing the model's understanding of affordances. To enable affordance-oriented manipulation, we further introduce Aff-Grasp, a framework that combines GKT with a grasp generation model. For comprehensive evaluation, we create an affordance evaluation dataset with pixel-wise annotations, and design real-world tasks for robot experiments. The results show that GKT surpasses the state-of-the-art by 15.9\% in mIoU, and Aff-Grasp achieves high success rates of 95.5\% in affordance prediction and 77.1\% in successful grasping among 179 trials, including evaluations with seen, unseen objects, and cluttered scenes.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Li, Gen and Tsagkas, Nikolaos and Song, Jifei and Mon-Williams, Ruaridh and Vijayakumar, Sethu and Shao, Kun and Sevilla-Lara, Laura},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10123 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{cao_mvinpainter_2024,
	title = {{MVInpainter}: {Learning} {Multi}-{View} {Consistent} {Inpainting} to {Bridge} {2D} and {3D} {Editing}},
	shorttitle = {{MVInpainter}},
	url = {http://arxiv.org/abs/2408.08000},
	doi = {10.48550/arXiv.2408.08000},
	abstract = {Novel View Synthesis (NVS) and 3D generation have recently achieved prominent improvements. However, these works mainly focus on confined categories or synthetic 3D assets, which are discouraged from generalizing to challenging in-the-wild scenes and fail to be employed with 2D synthesis directly. Moreover, these methods heavily depended on camera poses, limiting their real-world applications. To overcome these issues, we propose MVInpainter, re-formulating the 3D editing as a multi-view 2D inpainting task. Specifically, MVInpainter partially inpaints multi-view images with the reference guidance rather than intractably generating an entirely novel view from scratch, which largely simplifies the difficulty of in-the-wild NVS and leverages unmasked clues instead of explicit pose conditions. To ensure cross-view consistency, MVInpainter is enhanced by video priors from motion components and appearance guidance from concatenated reference key\&value attention. Furthermore, MVInpainter incorporates slot attention to aggregate high-level optical flow features from unmasked regions to control the camera movement with pose-free training and inference. Sufficient scene-level experiments on both object-centric and forward-facing datasets verify the effectiveness of MVInpainter, including diverse tasks, such as multi-view object removal, synthesis, insertion, and replacement. The project page is https://ewrfcas.github.io/MVInpainter/.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Cao, Chenjie and Yu, Chaohui and Fu, Yanwei and Wang, Fan and Xue, Xiangyang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.08000 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sudhakar_controlling_2024,
	title = {Controlling the {World} by {Sleight} of {Hand}},
	url = {http://arxiv.org/abs/2408.07147},
	doi = {10.48550/arXiv.2408.07147},
	abstract = {Humans naturally build mental models of object interactions and dynamics, allowing them to imagine how their surroundings will change if they take a certain action. While generative models today have shown impressive results on generating/editing images unconditionally or conditioned on text, current methods do not provide the ability to perform object manipulation conditioned on actions, an important tool for world modeling and action planning. Therefore, we propose to learn an action-conditional generative models by learning from unlabeled videos of human hands interacting with objects. The vast quantity of such data on the internet allows for efficient scaling which can enable high-performing action-conditional models. Given an image, and the shape/location of a desired hand interaction, CosHand, synthesizes an image of a future after the interaction has occurred. Experiments show that the resulting model can predict the effects of hand-object interactions well, with strong generalization particularly to translation, stretching, and squeezing interactions of unseen objects in unseen environments. Further, CosHand can be sampled many times to predict multiple possible effects, modeling the uncertainty of forces in the interaction/environment. Finally, method generalizes to different embodiments, including non-human hands, i.e. robot hands, suggesting that generative video models can be powerful models for robotics.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Sudhakar, Sruthi and Liu, Ruoshi and Van Hoorick, Basile and Vondrick, Carl and Zemel, Richard},
	month = aug,
	year = {2024},
	note = {arXiv:2408.07147 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_slotlifter_2024,
	title = {{SlotLifter}: {Slot}-guided {Feature} {Lifting} for {Learning} {Object}-centric {Radiance} {Fields}},
	shorttitle = {{SlotLifter}},
	url = {http://arxiv.org/abs/2408.06697},
	doi = {10.48550/arXiv.2408.06697},
	abstract = {The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose SlotLifter, a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering state-of-the-art performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in SlotLifter, revealing key insights for potential future directions.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Liu, Yu and Jia, Baoxiong and Chen, Yixin and Huang, Siyuan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06697 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{akinola_tacsl_2024,
	title = {{TacSL}: {A} {Library} for {Visuotactile} {Sensor} {Simulation} and {Learning}},
	shorttitle = {{TacSL}},
	url = {https://arxiv.org/abs/2408.06506v1},
	abstract = {For both humans and robots, the sense of touch, known as tactile sensing, is critical for performing contact-rich manipulation tasks. Three key challenges in robotic tactile sensing are 1) interpreting sensor signals, 2) generating sensor signals in novel scenarios, and 3) learning sensor-based policies. For visuotactile sensors, interpretation has been facilitated by their close relationship with vision sensors (e.g., RGB cameras). However, generation is still difficult, as visuotactile sensors typically involve contact, deformation, illumination, and imaging, all of which are expensive to simulate; in turn, policy learning has been challenging, as simulation cannot be leveraged for large-scale data collection. We present {\textbackslash}textbf\{TacSL\} ({\textbackslash}textit\{taxel\}), a library for GPU-based visuotactile sensor simulation and learning. {\textbackslash}textbf\{TacSL\} can be used to simulate visuotactile images and extract contact-force distributions over \$200{\textbackslash}times\$ faster than the prior state-of-the-art, all within the widely-used Isaac Gym simulator. Furthermore, {\textbackslash}textbf\{TacSL\} provides a learning toolkit containing multiple sensor models, contact-intensive training environments, and online/offline algorithms that can facilitate policy learning for sim-to-real applications. On the algorithmic side, we introduce a novel online reinforcement-learning algorithm called asymmetric actor-critic distillation ({\textbackslash}sysName), designed to effectively and efficiently learn tactile-based policies in simulation that can transfer to the real world. Finally, we demonstrate the utility of our library and algorithms by evaluating the benefits of distillation and multimodal sensing for contact-rich manip ulation tasks, and most critically, performing sim-to-real transfer. Supplementary videos and results are at {\textbackslash}url\{https://iakinola23.github.io/tacsl/\}.},
	language = {en},
	urldate = {2024-09-27},
	author = {Akinola, Iretiayo and Xu, Jie and Carius, Jan and Fox, Dieter and Narang, Yashraj},
	month = aug,
	year = {2024},
}

@misc{xu_unit_2024,
	title = {{UniT}: {Unified} {Tactile} {Representation} for {Robot} {Learning}},
	shorttitle = {{UniT}},
	url = {http://arxiv.org/abs/2408.06481},
	doi = {10.48550/arXiv.2408.06481},
	abstract = {UniT is a novel approach to tactile representation learning, using VQVAE to learn a compact latent space and serve as the tactile representation. It uses tactile images obtained from a single simple object to train the representation with transferability and generalizability. This tactile representation can be zero-shot transferred to various downstream tasks, including perception tasks and manipulation policy learning. Our benchmarking on an in-hand 3D pose estimation task shows that UniT outperforms existing visual and tactile representation learning methods. Additionally, UniT's effectiveness in policy learning is demonstrated across three real-world tasks involving diverse manipulated objects and complex robot-object-environment interactions. Through extensive experimentation, UniT is shown to be a simple-to-train, plug-and-play, yet widely effective method for tactile representation learning. For more details, please refer to our open-source repository https://github.com/ZhengtongXu/UniT and the project website https://zhengtongxu.github.io/unifiedtactile.github.io/.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Xu, Zhengtong and Uppuluri, Raghava and Zhang, Xinwei and Fitch, Cael and Crandall, Philip Glen and Shou, Wan and Wang, Dongyi and She, Yu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06481 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{romero_eyesight_2024,
	title = {{EyeSight} {Hand}: {Design} of a {Fully}-{Actuated} {Dexterous} {Robot} {Hand} with {Integrated} {Vision}-{Based} {Tactile} {Sensors} and {Compliant} {Actuation}},
	shorttitle = {{EyeSight} {Hand}},
	url = {http://arxiv.org/abs/2408.06265},
	doi = {10.48550/arXiv.2408.06265},
	abstract = {In this work, we introduce the EyeSight Hand, a novel 7 degrees of freedom (DoF) humanoid hand featuring integrated vision-based tactile sensors tailored for enhanced whole-hand manipulation. Additionally, we introduce an actuation scheme centered around quasi-direct drive actuation to achieve human-like strength and speed while ensuring robustness for large-scale data collection. We evaluate the EyeSight Hand on three challenging tasks: bottle opening, plasticine cutting, and plate pick and place, which require a blend of complex manipulation, tool use, and precise force application. Imitation learning models trained on these tasks, with a novel vision dropout strategy, showcase the benefits of tactile feedback in enhancing task success rates. Our results reveal that the integration of tactile sensing dramatically improves task performance, underscoring the critical role of tactile information in dexterous manipulation.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Romero, Branden and Fang, Hao-Shu and Agrawal, Pulkit and Adelson, Edward},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06265 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lu_adapting_2024,
	title = {Adapting {Pre}-{Trained} {Vision} {Models} for {Novel} {Instance} {Detection} and {Segmentation}},
	url = {http://arxiv.org/abs/2405.17859},
	doi = {10.48550/arXiv.2405.17859},
	abstract = {Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance. We propose a unified framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment. Leveraging recent advancements in large vision methods, we utilize the Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks. Central to our approach is the generation of high-quality instance embeddings. We utilize foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce. We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting. This methodology enables a straightforward matching strategy, resulting in significant performance gains. Our framework surpasses current state-of-the-art methods, demonstrating notable improvements of 22.3, 46.2, 10.3, and 24.0 in average precision (AP) across four detection datasets. In instance segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the top RGB methods by 3.6 AP and remains competitive with the best RGB-D method. Code is available at: https://github.com/YoungSean/NIDS-Net},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Lu, Yangxiao and P, Jishnu Jaykumar and Guo, Yunhui and Ruozzi, Nicholas and Xiang, Yu},
	month = may,
	year = {2024},
	note = {arXiv:2405.17859 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{cui_dynamo_2024,
	title = {{DynaMo}: {In}-{Domain} {Dynamics} {Pretraining} for {Visuo}-{Motor} {Control}},
	shorttitle = {{DynaMo}},
	url = {http://arxiv.org/abs/2409.12192},
	doi = {10.48550/arXiv.2409.12192},
	abstract = {Imitation learning has proven to be a powerful tool for training complex visuomotor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Cui, Zichen Jeff and Pan, Hengkai and Iyer, Aadhithya and Haldar, Siddhant and Pinto, Lerrel},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{sola_micro_2021,
	title = {A micro {Lie} theory for state estimation in robotics},
	url = {http://arxiv.org/abs/1812.01537},
	doi = {10.48550/arXiv.1812.01537},
	abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Solà, Joan and Deray, Jeremie and Atchuthan, Dinesh},
	month = dec,
	year = {2021},
	note = {arXiv:1812.01537 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{cao_uncertainty-aware_2024,
	title = {Uncertainty-{Aware} {Suction} {Grasping} for {Cluttered} {Scenes}},
	volume = {9},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/10493125},
	doi = {10.1109/LRA.2024.3385609},
	abstract = {In this work, we present a multi-stage pipeline that aims to accurately predict suction grasps for objects with varying properties in cluttered and complex scenes. Existing methods face difficulties in generalizing to unseen objects and effectively handling noisy depth/point cloud data, which often leads to inaccurate grasp predictions. To address these challenges, we utilize the Unseen Object Instance Segmentation (UOIS) technique to segment all objects and extract their instance-level point clouds. Additionally, we introduce the Uncertainty-aware Instance-level Scoring Network (UISN) to generate point-wise suction scores with uncertainties for each object. By calibrating the predicted scores using the estimated uncertainties, we further enhance their reliability for unseen objects and noisy data. Finally, the grasping candidates are ranked based on the calibrated scores, then the most promising grasps can be executed. Our approach achieves state-of-the-art performance on the SuctionNet-1Billion benchmark and demonstrates real robotic grasping, showcasing its accuracy and robustness in cluttered scenes.},
	number = {6},
	urldate = {2024-09-24},
	journal = {IEEE Robotics and Automation Letters},
	author = {Cao, Rui and Yang, Biqi and Li, Yichuan and Fu, Chi-Wing and Heng, Pheng-Ann and Liu, Yun-Hui},
	month = jun,
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Deep learning in grasping and manipulation, Grasping, Noise measurement, Point cloud compression, Predictive models, Robots, Three-dimensional displays, Uncertainty, computer vision for automation, perception for grasping and manipulation},
	pages = {4934--4941},
}

@misc{xiong_adaptive_2024,
	title = {Adaptive {Mobile} {Manipulation} for {Articulated} {Objects} {In} the {Open} {World}},
	url = {http://arxiv.org/abs/2401.14403},
	abstract = {Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50\% of BC pre-training to 95\% using online adaptation. Video results at https://open-world-mobilemanip.github.io/},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Xiong, Haoyu and Mendonca, Russell and Shaw, Kenneth and Pathak, Deepak},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14403 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{abou-chakra_physically_2024,
	title = {Physically {Embodied} {Gaussian} {Splatting}: {A} {Realtime} {Correctable} {World} {Model} for {Robotics}},
	shorttitle = {Physically {Embodied} {Gaussian} {Splatting}},
	url = {http://arxiv.org/abs/2406.10788},
	doi = {10.48550/arXiv.2406.10788},
	abstract = {For robots to robustly understand and interact with the physical world, it is highly beneficial to have a comprehensive representation - modelling geometry, physics, and visual observations - that informs perception, planning, and control algorithms. We propose a novel dual Gaussian-Particle representation that models the physical world while (i) enabling predictive simulation of future states and (ii) allowing online correction from visual observations in a dynamic world. Our representation comprises particles that capture the geometrical aspect of objects in the world and can be used alongside a particle-based physics system to anticipate physically plausible future states. Attached to these particles are 3D Gaussians that render images from any viewpoint through a splatting process thus capturing the visual state. By comparing the predicted and observed images, our approach generates visual forces that correct the particle positions while respecting known physical constraints. By integrating predictive physical modelling with continuous visually-derived corrections, our unified representation reasons about the present and future while synchronizing with reality. Our system runs in realtime at 30Hz using only 3 cameras. We validate our approach on 2D and 3D tracking tasks as well as photometric reconstruction quality. Videos are found at https://embodied-gaussians.github.io/.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Abou-Chakra, Jad and Rana, Krishan and Dayoub, Feras and Sünderhauf, Niko},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10788 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{merrill_symmetry_2022,
	title = {Symmetry and {Uncertainty}-{Aware} {Object} {SLAM} for {6DoF} {Object} {Pose} {Estimation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Merrill_Symmetry_and_Uncertainty-Aware_Object_SLAM_for_6DoF_Object_Pose_Estimation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-09-09},
	author = {Merrill, Nathaniel and Guo, Yuliang and Zuo, Xingxing and Huang, Xinyu and Leutenegger, Stefan and Peng, Xi and Ren, Liu and Huang, Guoquan},
	year = {2022},
	pages = {14901--14910},
}

@inproceedings{brachmann_uncertainty-driven_2016,
	title = {Uncertainty-{Driven} {6D} {Pose} {Estimation} of {Objects} and {Scenes} {From} a {Single} {RGB} {Image}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.html},
	urldate = {2024-09-09},
	author = {Brachmann, Eric and Michel, Frank and Krull, Alexander and Yang, Michael Ying and Gumhold, Stefan and Rother, Carsten},
	year = {2016},
	pages = {3364--3372},
}

@inproceedings{lee_guided_2020,
	title = {Guided {Uncertainty}-{Aware} {Policy} {Optimization}: {Combining} {Learning} and {Model}-{Based} {Strategies} for {Sample}-{Efficient} {Policy} {Learning}},
	shorttitle = {Guided {Uncertainty}-{Aware} {Policy} {Optimization}},
	url = {https://ieeexplore.ieee.org/abstract/document/9197125?casa_token=R2JWibassBQAAAAA:HGf9wTn5thWypcTpMLrxMektDOToA6uoh6Ity3I1WHPzdq6tKFEpkGSOH77svdV9_p7Ggv0uC78},
	doi = {10.1109/ICRA40945.2020.9197125},
	abstract = {Traditional robotic approaches rely on an accurate model of the environment, a detailed description of how to perform the task, and a robust perception system to keep track of the current state. On the other hand, reinforcement learning approaches can operate directly from raw sensory inputs with only a reward signal to describe the task, but are extremely sampleinefficient and brittle. In this work, we combine the strengths of model-based methods with the flexibility of learning-based methods to obtain a general method that is able to overcome inaccuracies in the robotics perception/actuation pipeline, while requiring minimal interactions with the environment. This is achieved by leveraging uncertainty estimates to divide the space in regions where the given model-based policy is reliable, and regions where it may have flaws or not be well defined. In these uncertain regions, we show that a locally learned-policy can be used directly with raw sensory inputs. We test our algorithm, Guided Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing peg insertion. Videos are available at: https://sites.google.com/view/guapo-rl.},
	urldate = {2024-09-23},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Lee, Michelle A. and Florensa, Carlos and Tremblay, Jonathan and Ratliff, Nathan and Garg, Animesh and Ramos, Fabio and Fox, Dieter},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Learning (artificial intelligence), Robot sensing systems, Switches, Task analysis, Uncertainty},
	pages = {7505--7512},
}

@article{wu_plan_2022,
	title = {Plan {To} {Predict}: {Learning} an {Uncertainty}-{Foreseeing} {Model} {For} {Model}-{Based} {Reinforcement} {Learning}},
	volume = {35},
	shorttitle = {Plan {To} {Predict}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/65beb73449888fabcf601b3a3ef4b3a7-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wu, Zifan and Yu, Chao and Chen, Chen and Hao, Jianye and Zhuo, Hankz Hankui},
	month = dec,
	year = {2022},
	pages = {15849--15861},
}

@inproceedings{lambert_objective_2020,
	title = {Objective {Mismatch} in {Model}-based {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v120/lambert20a.html},
	abstract = {Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework.In this paper, we identify a fundamental issue of the standard MBRL framework – what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {Proceedings of the 2nd {Conference} on {Learning} for {Dynamics} and {Control}},
	publisher = {PMLR},
	author = {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {761--770},
}

@article{wei_unified_2023,
	title = {A {Unified} {View} on {Solving} {Objective} {Mismatch} in {Model}-{Based} {Reinforcement} {Learning}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=tQVZgvXhZb},
	abstract = {Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.},
	language = {en},
	urldate = {2024-09-23},
	journal = {Transactions on Machine Learning Research},
	author = {Wei, Ran and Lambert, Nathan and McDonald, Anthony D. and Garcia, Alfredo and Calandra, Roberto},
	month = oct,
	year = {2023},
}

@inproceedings{fu_demonstrating_2023,
	title = {Demonstrating {RFUniverse}: {A} {Multiphysics} {Simulation} {Platform} for {Embodied} {AI}},
	volume = {19},
	isbn = {978-0-9923747-9-2},
	shorttitle = {Demonstrating {RFUniverse}},
	url = {https://www.roboticsproceedings.org/rss19/p087.html},
	urldate = {2024-09-22},
	author = {Fu, Haoyuan and Xu, Wenqiang and Ye, Ruolin and Xue, Han and Yu, Zhenjun and Tang, Tutian and Li, Yutong and Du, Wenxin and Zhang, Jieyi and Lu, Cewu},
	month = jul,
	year = {2023},
}

@inproceedings{gao_closure_2024,
	title = {{CLOSURE}: {Fast} {Quantification} of {Pose} {Uncertainty} {Sets}},
	volume = {20},
	isbn = {9798990284807},
	shorttitle = {{CLOSURE}},
	url = {https://www.roboticsproceedings.org/rss20/p072.html},
	urldate = {2024-09-22},
	author = {Gao, Yihuai and Tang, Yukai and Qi, Han and Yang, Heng},
	month = jul,
	year = {2024},
}

@inproceedings{chen_mirage_2024,
	title = {{MIRAGE}: {Cross}-{Embodiment} {Zero}-{Shot} {Policy} {Transfer} with {Cross}-{Painting}},
	volume = {20},
	isbn = {9798990284807},
	shorttitle = {{MIRAGE}},
	url = {https://www.roboticsproceedings.org/rss20/p069.html},
	urldate = {2024-09-22},
	author = {Chen, Lawrence Yunliang and Dharmarajan, Karthik and Hari, Kush and Xu, Chenfeng and Vuong, Quan and Goldberg, Ken},
	month = jul,
	year = {2024},
}

@article{noauthor_onboard_nodate,
	title = {Onboard {Dynamic}-{Object} {Detection} and {Tracking} for {Autonomous} {Robot} {Navigation} {With} {RGB}-{D} {Camera}},
	url = {https://ieeexplore.ieee.org/abstract/document/10323166},
	abstract = {Deploying autonomous robots in crowded indoor environments usually requires them to have accurate dynamic obstacle perception. Although plenty of previous works in the autonomous driving field have investigated the 3D object detection problem, the usage of dense point clouds from a heavy Light Detection and Ranging (LiDAR) sensor and their high computation cost for learning-based data processing make those methods not applicable to small robots, such as vision-based UAVs with small onboard computers. To address this issue, we propose a lightweight 3D dynamic obstacle detection and tracking (DODT) method based on an RGB-D camera, which is designed for low-power robots with limited computing power. Our method adopts a novel ensemble detection strategy, combining multiple computationally efficient but low-accuracy detectors to achieve real-time high-accuracy obstacle detection. Besides, we introduce a new feature-based data association and tracking method to prevent mismatches utilizing point clouds' statistical features. In addition, our system includes an optional and auxiliary learning-based module to enhance the obstacle detection range and dynamic obstacle identification. The proposed method is implemented in a small quadcopter, and the results show that our method can achieve the lowest position error (0.11 m) and a comparable velocity error (0.23 m/s) across the benchmarking algorithms running on the robot's onboard computer. The flight experiments prove that the tracking results from the proposed method can make the robot efficiently alter its trajectory for navigating dynamic environments. Our software is available on GitHub
{\textless}sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater}1{\textless}/sup{\textgreater}
 as an open-source ROS package.},
	language = {en-US},
	urldate = {2024-09-22},
}

@inproceedings{pitz_dextrous_2023,
	title = {Dextrous {Tactile} {In}-{Hand} {Manipulation} {Using} a {Modular} {Reinforcement} {Learning} {Architecture}},
	url = {https://ieeexplore.ieee.org/abstract/document/10160756?casa_token=S3Mx2BPxiwMAAAAA:IrAcmwKcu96cZGyIo-W9fDPSvQrjacg3FmXc2UKVsBgF7ax8tKaWUaPN-sqYGfRXggX5hsnNgxc},
	doi = {10.1109/ICRA48891.2023.10160756},
	abstract = {Dextrous in-hand manipulation with a multi-fingered robotic hand is a challenging task, esp. when performed with the hand oriented upside down, demanding permanent force-closure, and when no external sensors are used. For the task of reorienting an object to a given goal orientation (vs. infinitely spinning it around an axis), the lack of external sensors is an additional fundamental challenge as the state of the object has to be estimated all the time, e.g., to detect when the goal is reached. In this paper, we show that the task of reorienting a cube to any of the 24 possible goal orientations in a π/2-raster using the torque-controlled DLR-Hand II is possible. The task is learned in simulation using a modular deep reinforcement learning architecture: the actual policy has only a small observation time window of 0.5 s but gets the cube state as an explicit input which is estimated via a deep differentiable particle filter trained on data generated by running the policy. In simulation, we reach a success rate of 92\% while applying significant domain randomization. Via zero-shot Sim2Real-transfer on the real robotic system, all 24 goal orientations can be reached with a high success rate. (Web: dlr-alr.github.io/dlr-tactile-manipulation)},
	urldate = {2024-09-20},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Pitz, Johannes and Röstel, Lennart and Sievers, Leon and Bäuml, Berthold},
	month = may,
	year = {2023},
	keywords = {Deep learning, Particle filters, Reinforcement learning, Sensors, Service-oriented architecture, Training, Uncertainty},
	pages = {1852--1858},
}

@inproceedings{rostel_learning_2022,
	title = {Learning a {State} {Estimator} for {Tactile} {In}-{Hand} {Manipulation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9981730?casa_token=3FKcma7Wz1AAAAAA:Sk7KJFkyFi3ML8p4kKOeoA-6c-KM9UD8Uc1D7-I-g0Nd3OKADxNPo1I-fXDEhxL7HvnuOSb3zpk},
	doi = {10.1109/IROS47612.2022.9981730},
	abstract = {We study the problem of estimating the pose of an object which is being manipulated by a multi-fingered robotic hand by only using proprioceptive feedback. To address this challenging problem, we propose a novel variant of differentiable particle filters, which combines two key extensions. First, our learned proposal distribution incorporates recent measurements in a way that mitigates weight degeneracy. Second, the particle update works on non-euclidean manifolds like Lie-groups, enabling learning-based pose estimation in 3D on SE(3). We show that the method can represent the rich and often multi-modal distributions over poses that arise in tactile state estimation. The models are trained in simulation, but by using domain randomization, we obtain state estimators that can be employed for pose estimation on a real robotic hand (equipped with joint torque sensors). Moreover, the estimator runs fast, allowing for online usage with update rates of more than 100 Hz on a single CPU core. We quantitatively evaluate our method and benchmark it against other approaches in simulation. We also show qualitative experiments on the real torque-controlled DLR-Hand II.},
	urldate = {2024-09-20},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Röstel, Lennart and Sievers, Leon and Pitz, Johannes and Bäuml, Berthold},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Atmospheric measurements, Information filters, Particle measurements, Pose estimation, Three-dimensional displays, Torque, Weight measurement},
	pages = {4749--4756},
}

@inproceedings{williams_information_2017,
	title = {Information theoretic {MPC} for model-based reinforcement learning},
	url = {https://ieeexplore.ieee.org/document/7989202},
	doi = {10.1109/ICRA.2017.7989202},
	abstract = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
	urldate = {2024-09-20},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M. and Boots, Byron and Theodorou, Evangelos A.},
	month = may,
	year = {2017},
	keywords = {Cost function, Heuristic algorithms, Learning (artificial intelligence), Optimal control, Robots, Trajectory},
	pages = {1714--1721},
}

@misc{rodriguez_touch2touch_2024,
	title = {{Touch2Touch}: {Cross}-{Modal} {Tactile} {Generation} for {Object} {Manipulation}},
	shorttitle = {{Touch2Touch}},
	url = {http://arxiv.org/abs/2409.08269},
	doi = {10.48550/arXiv.2409.08269},
	abstract = {Today's touch sensors come in many shapes and sizes. This has made it challenging to develop general-purpose touch processing methods since models are generally tied to one specific sensor design. We address this problem by performing cross-modal prediction between touch sensors: given the tactile signal from one sensor, we use a generative model to estimate how the same physical contact would be perceived by another sensor. This allows us to apply sensor-specific methods to the generated signal. We implement this idea by training a diffusion model to translate between the popular GelSlim and Soft Bubble sensors. As a downstream task, we perform in-hand object pose estimation using GelSlim sensors while using an algorithm that operates only on Soft Bubble signals. The dataset, the code, and additional details can be found at https://www.mmintlab.com/research/touch2touch/.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Rodriguez, Samanta and Dou, Yiming and Oller, Miquel and Owens, Andrew and Fazeli, Nima},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08269 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{singh_hand-object_2024,
	title = {Hand-{Object} {Interaction} {Pretraining} from {Videos}},
	url = {http://arxiv.org/abs/2409.08273},
	doi = {10.48550/arXiv.2409.08273},
	abstract = {We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: {\textbackslash}url\{https://hgaurav2k.github.io/hop/\}.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Singh, Himanshu Gaurav and Loquercio, Antonio and Sferrazza, Carmelo and Wu, Jane and Qi, Haozhi and Abbeel, Pieter and Malik, Jitendra},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08273 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{deng_deep_2022,
	title = {Deep {Bingham} {Networks}: {Dealing} with {Uncertainty} and {Ambiguity} in {Pose} {Estimation}},
	volume = {130},
	issn = {1573-1405},
	shorttitle = {Deep {Bingham} {Networks}},
	url = {https://doi.org/10.1007/s11263-022-01612-w},
	doi = {10.1007/s11263-022-01612-w},
	abstract = {In this work, we introduce Deep Bingham Networks (DBN), a generic framework that can naturally handle pose-related uncertainties and ambiguities arising in almost all real life applications concerning 3D data. While existing works strive to find a single solution to the pose estimation problem, we make peace with the ambiguities causing high uncertainty around which solutions to identify as the best. Instead, we report a family of poses which capture the nature of the solution space. DBN extends the state of the art direct pose regression networks by (i) a multi-hypotheses prediction head which can yield different distribution modes; and (ii) novel loss functions that benefit from Bingham distributions on rotations. This way, DBN can work both in unambiguous cases providing uncertainty information, and in ambiguous scenes where an uncertainty per mode is desired. On a technical front, our network regresses continuous Bingham mixture models and is applicable to both 2D data such as images and to 3D data such as point clouds. We proposed new training strategies so as to avoid mode or posterior collapse during training and to improve numerical stability. Our methods are thoroughly tested on two different applications exploiting two different modalities: (i) 6D camera relocalization from images; and (ii) object pose estimation from 3D point clouds, demonstrating decent advantages over the state of the art. For the former we contributed our own dataset composed of five indoor scenes where it is unavoidable to capture images corresponding to views that are hard to uniquely identify. For the latter we achieve the top results especially for symmetric objects of ModelNet dataset (Wu et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1912–1920, 2015). The code and dataset accompanying this paper is provided under https://multimodal3dvision.github.io.},
	language = {en},
	number = {7},
	urldate = {2024-09-17},
	journal = {International Journal of Computer Vision},
	author = {Deng, Haowen and Bui, Mai and Navab, Nassir and Guibas, Leonidas and Ilic, Slobodan and Birdal, Tolga},
	month = jul,
	year = {2022},
	keywords = {3D computer vision, 6D, Ambiguity, Artificial Intelligence, Bingham distribution, Camera pose, Camera relocalization, Object pose, Point clouds, Posterior distribution, Rotation, Uncertainty, Uncertainty estimation},
	pages = {1627--1654},
}

@article{lang_gaussian_2018,
	title = {Gaussian process for 6-{DoF} rigid motions},
	volume = {42},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-017-9683-4},
	doi = {10.1007/s10514-017-9683-4},
	abstract = {Data-driven modeling approaches receive significant attention in robotics as they are capable of representing system behavior to which first-order principles cannot be employed. Modeling of human motions, based on observations is one of the many application areas. So far, however, the available probabilistic approaches cannot handle dynamics evolving in the space of rigid motions, as rotations are not appropriately considered. In this article, we present a mathematical framework for Gaussian process modeling, where the valid input domain is generalized to full rigid motions, namely the special Euclidean group SE(3). The kernel functions inside the Gaussian process are modified to exploit properties of the input data representation by dual quaternions. We further prove that the presented covariance functions maintain the Gaussian process properties. The correctness and accuracy of our approach is validated on simulated and real human motion data. We analyze the estimation performance of the novel Gaussian process framework in comparison to state of the art techniques, and show significantly improved model behavior of rigid motions.},
	language = {en},
	number = {6},
	urldate = {2024-09-17},
	journal = {Autonomous Robots},
	author = {Lang, Muriel and Kleinsteuber, Martin and Hirche, Sandra},
	month = aug,
	year = {2018},
	keywords = {Artificial Intelligence, Covariance function, Data-driven modelling, Dual quaternions, Gaussian process, Manifold modelling, Nonlinear dynamics, Rigid motion dynamics, Special Euclidean group},
	pages = {1151--1167},
}

@article{deng_poserbpf_2021,
	title = {{PoseRBPF}: {A} {Rao}–{Blackwellized} {Particle} {Filter} for 6-{D} {Object} {Pose} {Tracking}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {{PoseRBPF}},
	url = {https://ieeexplore.ieee.org/abstract/document/9363455},
	doi = {10.1109/TRO.2021.3056043},
	abstract = {Tracking 6-D poses of objects from videos provides rich information to a robot in performing different tasks such as manipulation and navigation. In this article, we formulate the 6-D object pose tracking problem in the Rao–Blackwellized particle filtering framework, where the 3-D rotation and the 3-D translation of an object are decoupled. This factorization allows our approach, called PoseRBPF, to efficiently estimate the 3-D translation of an object along with the full distribution over the 3-D rotation. This is achieved by discretizing the rotation space in a fine-grained manner and training an autoencoder network to construct a codebook of feature embeddings for the discretized rotations. As a result, PoseRBPF can track objects with arbitrary symmetries while still maintaining adequate posterior distributions. Our approach achieves state-of-the-art results on two 6-D pose estimation benchmarks. We open-source our implementation at https://github.com/NVlabs/PoseRBPF.},
	number = {5},
	urldate = {2024-09-17},
	journal = {IEEE Transactions on Robotics},
	author = {Deng, Xinke and Mousavian, Arsalan and Xiang, Yu and Xia, Fei and Bretl, Timothy and Fox, Dieter},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {6-D object pose tracking, Computer vision, Pose estimation, Solid modeling, Target tracking, Task analysis, Tracking, Training, Uncertainty, state estimation},
	pages = {1328--1342},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {1566-2535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
	doi = {10.1016/j.inffus.2021.05.008},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
	urldate = {2024-09-17},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	keywords = {Artificial intelligence, Bayesian statistics, Deep learning, Ensemble learning, Machine learning, Uncertainty quantification},
	pages = {243--297},
}

@article{gawlikowski_survey_2023,
	title = {A survey of uncertainty in deep neural networks},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-023-10562-9},
	doi = {10.1007/s10462-023-10562-9},
	abstract = {Over the last decade, neural networks have reached almost every field of science and become a crucial part of various real world applications. Due to the increasing spread, confidence in neural network predictions has become more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over- or under-confidence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network’s prediction. As a result, different types and sources of uncertainty have been identified and various approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the field, highlights current challenges, and identifies potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this field. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and irreducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks (BNNs), ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these fields as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for calibrating neural networks, and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the fields of medical image analysis, robotics, and earth observation give an idea of the needs and challenges regarding uncertainties in the practical applications of neural networks. Additionally, the practical limitations of uncertainty quantification methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
	language = {en},
	number = {1},
	urldate = {2024-09-17},
	journal = {Artificial Intelligence Review},
	author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Bayesian deep neural networks, Calibration, Ensembles, Test-time augmentation, Uncertainty},
	pages = {1513--1589},
}

@article{shafer_tutorial_2008,
	title = {A {Tutorial} on {Conformal} {Prediction}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/shafer08a.html},
	abstract = {Conformal prediction uses past experience to determine precise levels
of confidence in new predictions.  Given an error probability
ε, together with a method that makes a prediction ŷ
of a label y, it produces a set of labels, typically containing
ŷ, that also contains y with probability 1 – ε.
Conformal prediction can be applied to any method for producing
ŷ: a nearest-neighbor method, a support-vector machine, ridge
regression, etc.

      
Conformal prediction is designed for an on-line setting in which
labels are predicted successively, each one being revealed before the
next is predicted.  The most novel and valuable feature of conformal
prediction is that if the successive examples are sampled
independently from the same distribution, then the successive
predictions will be right 1 – ε of the time, even though they
are based on an accumulating data set rather than on independent data
sets.


In addition to the model under which successive examples are sampled
independently, other on-line compression models can also use conformal
prediction.  The widely used Gaussian linear model is one of these.


This tutorial presents a self-contained account of the theory of
conformal prediction and works through several numerical examples.  A
more comprehensive treatment of the topic is provided in
Algorithmic Learning in a Random World, by Vladimir Vovk,
Alex Gammerman, and Glenn Shafer (Springer, 2005).},
	number = {12},
	urldate = {2024-09-16},
	journal = {Journal of Machine Learning Research},
	author = {Shafer, Glenn and Vovk, Vladimir},
	year = {2008},
	pages = {371--421},
}

@misc{he_survey_2024,
	title = {A {Survey} on {Uncertainty} {Quantification} {Methods} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2302.13425},
	doi = {10.48550/arXiv.2302.13425},
	abstract = {Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incorporate, making it difficult to select an appropriate UQ method in practice. To fill the gap, this paper presents a systematic taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data uncertainty versus model uncertainty). We summarize the advantages and disadvantages of methods in each category. We show how our taxonomy of UQ methodologies can potentially help guide the choice of UQ method in different machine learning problems (e.g., active learning, robustness, and reinforcement learning). We also identify current research gaps and propose several future research directions.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {He, Wenchong and Jiang, Zhe and Xiao, Tingsong and Xu, Zelin and Li, Yukun},
	month = jul,
	year = {2024},
	note = {arXiv:2302.13425 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kabir_neural_2018,
	title = {Neural {Network}-{Based} {Uncertainty} {Quantification}: {A} {Survey} of {Methodologies} and {Applications}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Neural {Network}-{Based} {Uncertainty} {Quantification}},
	url = {https://ieeexplore.ieee.org/document/8371683/?arnumber=8371683},
	doi = {10.1109/ACCESS.2018.2836917},
	abstract = {Uncertainty quantification plays a critical role in the process of decision making and optimization in many fields of science and engineering. The field has gained an overwhelming attention among researchers in recent years resulting in an arsenal of different methods. Probabilistic forecasting and in particular prediction intervals (PIs) are one of the techniques most widely used in the literature for uncertainty quantification. Researchers have reported studies of uncertainty quantification in critical applications such as medical diagnostics, bioinformatics, renewable energies, and power grids. The purpose of this survey paper is to comprehensively study neural network-based methods for construction of prediction intervals. It will cover how PIs are constructed, optimized, and applied for decision-making in presence of uncertainties. Also, different criteria for unbiased PI evaluation are investigated. The paper also provides some guidelines for further research in the field of neural network-based uncertainty quantification.},
	urldate = {2024-09-16},
	journal = {IEEE Access},
	author = {Kabir, H. M. Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and Nahavandi, Saeid},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial neural networks, Forecasting, Prediction interval, Probabilistic logic, Probability density function, Uncertainty, Upper bound, forecast, heteroscedastic uncertainty, neural network, probability, regression, time series data, uncertainty quantification},
	pages = {36218--36234},
}

@misc{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	doi = {10.48550/arXiv.1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = oct,
	year = {2016},
	note = {arXiv:1506.02142 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_mixed_2023,
	title = {Mixed {Neural} {Voxels} for {Fast} {Multi}-view {Video} {Synthesis}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Mixed_Neural_Voxels_for_Fast_Multi-view_Video_Synthesis_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-09-16},
	author = {Wang, Feng and Tan, Sinan and Li, Xinghang and Tian, Zeyue and Song, Yafei and Liu, Huaping},
	year = {2023},
	pages = {19706--19716},
}

@inproceedings{li_neural_2022,
	title = {Neural {3D} {Video} {Synthesis} {From} {Multi}-{View} {Video}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Neural_3D_Video_Synthesis_From_Multi-View_Video_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-09-16},
	author = {Li, Tianye and Slavcheva, Mira and Zollhöfer, Michael and Green, Simon and Lassner, Christoph and Kim, Changil and Schmidt, Tanner and Lovegrove, Steven and Goesele, Michael and Newcombe, Richard and Lv, Zhaoyang},
	year = {2022},
	pages = {5521--5531},
}

@inproceedings{nguyen_cnos_2023,
	title = {{CNOS}: {A} {Strong} {Baseline} for {CAD}-{Based} {Novel} {Object} {Segmentation}},
	shorttitle = {{CNOS}},
	url = {https://openaccess.thecvf.com/content/ICCV2023W/R6D/html/Nguyen_CNOS_A_Strong_Baseline_for_CAD-Based_Novel_Object_Segmentation_ICCVW_2023_paper.html},
	language = {en},
	urldate = {2024-09-15},
	author = {Nguyen, Van Nguyen and Groueix, Thibault and Ponimatkin, Georgy and Lepetit, Vincent and Hodan, Tomas},
	year = {2023},
	pages = {2134--2140},
}

@misc{ornek_foundpose_2024,
	title = {{FoundPose}: {Unseen} {Object} {Pose} {Estimation} with {Foundation} {Features}},
	shorttitle = {{FoundPose}},
	url = {http://arxiv.org/abs/2311.18809},
	doi = {10.48550/arXiv.2311.18809},
	abstract = {We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training. In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap. We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.},
	urldate = {2024-09-15},
	publisher = {arXiv},
	author = {Örnek, Evin Pınar and Labbé, Yann and Tekin, Bugra and Ma, Lingni and Keskin, Cem and Forster, Christian and Hodan, Tomas},
	month = jul,
	year = {2024},
	note = {arXiv:2311.18809 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{wang_slot-vae_2023,
	title = {Slot-{VAE}: {Object}-{Centric} {Scene} {Generation} with {Slot} {Attention}},
	shorttitle = {Slot-{VAE}},
	url = {https://proceedings.mlr.press/v202/wang23r.html},
	abstract = {Slot attention has shown remarkable object-centric representation learning performance in computer vision tasks without requiring any supervision. Despite its object-centric binding ability brought by compositional modelling, as a deterministic module, slot attention lacks the ability to generate novel scenes. In this paper, we propose the Slot-VAE, a generative model that integrates slot attention with the hierarchical VAE framework for object-centric structured scene generation. For each image, the model simultaneously infers a global scene representation to capture high-level scene structure and object-centric slot representations to embed individual object components. During generation, slot representations are generated from the global scene representation to ensure coherent scene structures. Our extensive evaluation of the scene generation ability indicates that Slot-VAE outperforms slot representation-based generative baselines in terms of sample quality and scene structure accuracy.},
	language = {en},
	urldate = {2024-09-14},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Yanbo and Liu, Letao and Dauwels, Justin},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {36020--36035},
}

@inproceedings{locatello_object-centric_2020,
	title = {Object-{Centric} {Learning} with {Slot} {Attention}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html},
	abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
	urldate = {2024-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
	year = {2020},
	pages = {11525--11538},
}

@article{fontana_conformal_2023,
	title = {Conformal prediction: {A} unified review of theory and new challenges},
	volume = {29},
	issn = {1350-7265},
	shorttitle = {Conformal prediction},
	url = {https://projecteuclid.org/journals/bernoulli/volume-29/issue-1/Conformal-prediction--A-unified-review-of-theory-and-new/10.3150/21-BEJ1447.full},
	doi = {10.3150/21-BEJ1447},
	abstract = {In this work we provide a review of basic ideas and novel developments about Conformal Prediction — an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions — that is able to yield in a very straightforward way prediction sets that are valid in a statistical sense also in the finite sample case. The discussion provided in the paper covers the theoretical underpinnings of Conformal Prediction, and then proceeds to list the more advanced developments and adaptations of the original idea.},
	number = {1},
	urldate = {2024-09-13},
	journal = {Bernoulli},
	author = {Fontana, Matteo and Zeni, Gianluca and Vantini, Simone},
	month = feb,
	year = {2023},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {conformal prediction, nonparametric statistics, prediction intervals, review},
	pages = {1--23},
}

@inproceedings{suresh_shapemap_2022,
	title = {{ShapeMap} 3-{D}: {Efficient} shape mapping through dense touch and vision},
	shorttitle = {{ShapeMap} 3-{D}},
	doi = {10.1109/ICRA46639.2022.9812040},
	abstract = {Knowledge of 3-D object shape is of great importance to robot manipulation tasks, but may not be readily available in unstructured environments. While vision is often occluded during robot-object interaction, high-resolution tactile sensors can give a dense local perspective of the object. However, tactile sensors have limited sensing area and the shape representation must faithfully approximate non-contact areas. In addition, a key challenge is efficiently incorporating these dense tactile measurements into a 3-D mapping framework. In this work, we propose an incremental shape mapping method using a GelSight tactile sensor and a depth camera. Local shape is recovered from tactile images via a learned model trained in simulation. Through efficient inference on a spatial factor graph informed by a Gaussian process, we build an implicit surface representation of the object. We demonstrate visuo-tactile mapping in both simulated and real-world experiments, to incrementally build 3-D reconstructions of household objects.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Suresh, Sudharshan and Si, Zilin and Mangelson, Joshua G. and Yuan, Wenzhen and Kaess, Michael},
	month = may,
	year = {2022},
	keywords = {Gaussian processes, Sensors, Shape, Surface reconstruction, Tactile sensors, Three-dimensional displays, Uncertainty},
	pages = {7073--7080},
}

@inproceedings{ambrus_monocular_2021,
	title = {Monocular {Depth} {Estimation} for {Soft} {Visuotactile} {Sensors}},
	url = {https://ieeexplore.ieee.org/abstract/document/9479234},
	doi = {10.1109/RoboSoft51838.2021.9479234},
	abstract = {Fluid-filled soft visuotactile sensors such as the Soft-bubbles alleviate key challenges for robust manipulation, as they enable reliable grasps along with the ability to obtain high-resolution sensory feedback on contact geometry and forces. Although they are simple in construction, their utility has been limited due to size constraints introduced by enclosed custom IR/depth imaging sensors to directly measure surface deformations. Towards mitigating this limitation, we investigate the application of state-of-the-art monocular depth estimation to infer dense internal (tactile) depth maps directly from the internal single small IR imaging sensor. Through real-world experiments, we show that deep networks typically used for long-range depth estimation (1-100m) can be effectively trained for precise predictions at a much shorter range (1-100mm) inside a mostly textureless deformable fluid-filled sensor. We propose a simple supervised learning process to train an object-agnostic network requiring less than 10 random poses in contact for less than 10 seconds for a small set of diverse objects (mug, wine glass, box, and fingers in our experiments). We show that our approach is sample-efficient, accurate, and generalizes across different objects and sensor configurations unseen at training time. Finally, we discuss the implications of our approach for the design of soft visuotactile sensors and grippers1.},
	urldate = {2024-09-12},
	booktitle = {2021 {IEEE} 4th {International} {Conference} on {Soft} {Robotics} ({RoboSoft})},
	author = {Ambrus, Rareş and Guizilini, Vitor and Kuppuswamy, Naveen and Beaulieu, Andrew and Gaidon, Adrien and Alspach, Alex},
	month = apr,
	year = {2021},
	keywords = {Cameras, Estimation, Geometry, Image sensors, Soft robotics, Supervised learning, Training},
	pages = {643--649},
}

@misc{zhou_transfusion_2024,
	title = {Transfusion: {Predict} the {Next} {Token} and {Diffuse} {Images} with {One} {Multi}-{Modal} {Model}},
	shorttitle = {Transfusion},
	url = {https://arxiv.org/abs/2408.11039v1},
	abstract = {We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.},
	language = {en},
	urldate = {2024-09-12},
	author = {Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
	month = aug,
	year = {2024},
}

@misc{zhang_champ_2024,
	title = {{CHAMP}: {Conformalized} {3D} {Human} {Multi}-{Hypothesis} {Pose} {Estimators}},
	shorttitle = {{CHAMP}},
	url = {https://arxiv.org/abs/2407.06141v1},
	abstract = {We introduce CHAMP, a novel method for learning sequence-to-sequence, multi-hypothesis 3D human poses from 2D keypoints by leveraging a conditional distribution with a diffusion model. To predict a single output 3D pose sequence, we generate and aggregate multiple 3D pose hypotheses. For better aggregation results, we develop a method to score these hypotheses during training, effectively integrating conformal prediction into the learning process. This process results in a differentiable conformal predictor that is trained end2end with the 3D pose estimator. Post-training, the learned scoring model is used as the conformity score, and the 3D pose estimator is combined with a conformal predictor to select the most accurate hypotheses for downstream aggregation. Our results indicate that using a simple mean aggregation on the conformal prediction-filtered hypotheses set yields competitive results. When integrated with more sophisticated aggregation techniques, our method achieves state-of-the-art performance across various metrics and datasets while inheriting the probabilistic guarantees of conformal prediction.},
	language = {en},
	urldate = {2024-09-11},
	author = {Zhang, Harry and Carlone, Luca},
	month = may,
	year = {2024},
}

@inproceedings{doula_conformal_2024,
	title = {Conformal {Prediction} for {Semantically}-{Aware} {Autonomous} {Perception} in {Urban} {Environments}},
	url = {https://openreview.net/forum?id=aaY5fVFMVf},
	abstract = {We introduce Knowledge-Refined Prediction Sets (KRPS), a novel approach that performs semantically-aware uncertainty quantification for multitask-based autonomous perception in urban environments. KRPS extends conformal prediction (CP) to ensure 2 properties not typically addressed by CP frameworks: semantic label consistency and true label coverage, across multiple perception tasks. We elucidate the capability of KRPS through high-level classification tasks crucial for semantically-aware autonomous perception in urban environments, including agent classification, agent location classification, and agent action classification. In a theoretical analysis, we introduce the concept of semantic label consistency among tasks and prove the semantic consistency and marginal coverage properties of the produced sets by KRPS. The results of our evaluation on the ROAD dataset and the Waymo/ROAD++ dataset show that KRPS outperforms state-of-the-art CP methods in reducing uncertainty by up to 80{\textbackslash}\% and increasing the semantic consistency by up to 30{\textbackslash}\%, while maintaining the coverage guarantees.},
	language = {en},
	urldate = {2024-09-11},
	author = {Doula, Achref and Güdelhöfer, Tobias and Mühlhäuser, Max and Guinea, Alejandro Sanchez},
	month = sep,
	year = {2024},
}

@inproceedings{chee_uncertainty_2024,
	title = {Uncertainty quantification and robustification of model-based controllers using conformal prediction},
	url = {https://proceedings.mlr.press/v242/chee24a.html},
	abstract = {In modern model-based control frameworks such as model predictive control or model-based reinforcement learning, machine learning has become a ubiquitous class of techniques deployed to improve the accuracy of the dynamics models. By leveraging expressive architectures such as neural networks, these frameworks aim to improve both the model accuracy and the control performance of the system, through the construction of accurate data-driven representations of the system dynamics. Despite achieving significant performance improvements over their non-learning counterparts, there are often little or no guarantees on how these model-based controllers with learned models would perform in the presence of uncertainty. In particular, under the influence of modeling errors, noise and exogenous disturbances, it is challenging to ascertain the accuracy of these learned models. In some cases, constraints may even be violated, rendering the controllers unsafe. In this work, we propose a novel framework that can be applied to a large class of model-based controllers and alleviates the above mentioned issues by robustifying the model-based controllers in an online and modular manner, with provable guarantees on the model accuracy and constraint satisfaction. The framework first deploys conformal prediction to generate finite-sample, provably valid uncertainty regions for the dynamics model in a distribution-free manner. These uncertainty regions are incorporated into the constraints through a dynamic constraint tightening procedure. Together with the formulation of a predictive reference generator, a set of robustified reference trajectories are generated and incorporated into the model-based controller. Using two practical case studies, we demonstrate that our proposed methodology not only produces well-calibrated uncertainty regions that establish the accuracy of the models, but also enables the closed-loop system to satisfy constraints in a robust yet non-conservative manner.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the 6th {Annual} {Learning} for {Dynamics} \& {Control} {Conference}},
	publisher = {PMLR},
	author = {Chee, Kong Yao and Silva, Thales C. and Hsieh, M. Ani and Pappas, George J.},
	month = jun,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {528--540},
}

@article{sun_conformal_2023,
	title = {Conformal {Prediction} for {Uncertainty}-{Aware} {Planning} with {Diffusion} {Dynamics} {Model}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/fe318a2b6c699808019a456b706cd845-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-11},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sun, Jiankai and Jiang, Yiqi and Qiu, Jianing and Nobel, Parth and Kochenderfer, Mykel J. and Schwager, Mac},
	month = dec,
	year = {2023},
	pages = {80324--80337},
}

@inproceedings{nagabandi_deep_2020,
	title = {Deep {Dynamics} {Models} for {Learning} {Dexterous} {Manipulation}},
	url = {https://proceedings.mlr.press/v100/nagabandi20a.html},
	abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in on-line model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills – and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/.},
	language = {en},
	urldate = {2024-09-11},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1101--1112},
}

@inproceedings{chua_deep_2018,
	title = {Deep {Reinforcement} {Learning} in a {Handful} of {Trials} using {Probabilistic} {Dynamics} {Models}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html},
	abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
	urldate = {2024-09-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	year = {2018},
}

@inproceedings{mai_sample_2021,
	title = {Sample {Efficient} {Deep} {Reinforcement} {Learning} via {Uncertainty} {Estimation}},
	url = {https://openreview.net/forum?id=vrW3tvDfOJQ},
	abstract = {In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.},
	language = {en},
	urldate = {2024-09-11},
	author = {Mai, Vincent and Mani, Kaustubh and Paull, Liam},
	month = oct,
	year = {2021},
}

@inproceedings{fan_robust_2017,
	title = {Robust dexterous manipulation under object dynamics uncertainties},
	url = {https://ieeexplore.ieee.org/abstract/document/8014085},
	doi = {10.1109/AIM.2017.8014085},
	abstract = {Dexterous manipulation has broad applications in assembly lines, warehouses and agriculture. To perform broad-scale manipulation tasks, it is desired that a multi-fingered robotic hand can robustly manipulate objects without knowing the exact objects dynamics (i.e. mass and inertia) in advance. However, realizing robust manipulation is challenging due to the complex contact dynamics, the nonlinearities of the system, and the potential sliding during manipulation. In this paper, a dual-stage grasp controller is proposed to handle these challenges. In the first stage, feedback linearization is utilized to linearize the nonlinear uncertain system. Considering the structures of uncertainties, a robust controller is designed for such a linearized system to obtain the desired Cartesian force on the object. In the second stage, a manipulation controller regulates the contact force based on the Cartesian force from the first stage. The dual-stage grasp controller is able to realize robust manipulation without contact modeling, prevent the slippage, and withstand 40\% mass and 50\% inertia uncertainties. Moreover, it does not require velocity measurement or 3D/6D tactile sensor. Simulation results on Mujoco verify the efficacy of the proposed method. The simulation video is available at [1].},
	urldate = {2024-09-10},
	booktitle = {2017 {IEEE} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics} ({AIM})},
	author = {Fan, Yongxiang and Sun, Liting and Zheng, Minghui and Gao, Wei and Tomizuka, Masayoshi},
	month = jul,
	year = {2017},
	note = {ISSN: 2159-6255},
	keywords = {Dynamics, Force, Manipulator dynamics, Nonlinear dynamical systems, Robustness, Uncertainty},
	pages = {613--619},
}

@misc{ravi_accelerating_2020,
	title = {Accelerating {3D} {Deep} {Learning} with {PyTorch3D}},
	url = {http://arxiv.org/abs/2007.08501},
	doi = {10.48550/arXiv.2007.08501},
	abstract = {Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Ravi, Nikhila and Reizenstein, Jeremy and Novotny, David and Gordon, Taylor and Lo, Wan-Yen and Johnson, Justin and Gkioxari, Georgia},
	month = jul,
	year = {2020},
	note = {arXiv:2007.08501 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{mialon_self-supervised_2023,
	title = {Self-{Supervised} {Learning} with {Lie} {Symmetries} for {Partial} {Differential} {Equations}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5c46ae130105fa012da0446126c01d1d-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-10},
	journal = {Advances in Neural Information Processing Systems},
	author = {Mialon, Grégoire and Garrido, Quentin and Lawrence, Hannah and Rehman, Danyal and LeCun, Yann and Kiani, Bobak},
	month = dec,
	year = {2023},
	pages = {28973--29004},
}

@article{howell_equivariant_2023,
	title = {Equivariant {Single} {View} {Pose} {Prediction} {Via} {Induced} and {Restriction} {Representations}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/93b3d975f9a2448964a906199db98a9d-Abstract-Conference.html},
	language = {en},
	urldate = {2024-09-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Howell, Owen and Klee, David and Biza, Ondrej and Zhao, Linfeng and Walters, Robin},
	month = dec,
	year = {2023},
	pages = {47251--47263},
}

@inproceedings{klee_image_2022,
	title = {Image to {Sphere}: {Learning} {Equivariant} {Features} for {Efficient} {Pose} {Prediction}},
	shorttitle = {Image to {Sphere}},
	url = {https://openreview.net/forum?id=_2bDpAtr7PI},
	abstract = {Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in \${\textbackslash}mathrm\{SO\}(3)\$. However, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages \${\textbackslash}mathrm\{SO\}(3)\$ equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at {\textbackslash}url\{https://dmklee.github.io/image2sphere\}.},
	language = {en},
	urldate = {2024-09-09},
	author = {Klee, David and Biza, Ondrej and Platt, Robert and Walters, Robin},
	month = sep,
	year = {2022},
}

@misc{xu_dipgrasp_2024,
	title = {{DiPGrasp}: {Parallel} {Local} {Searching} for {Efficient} {Differentiable} {Grasp} {Planning}},
	shorttitle = {{DiPGrasp}},
	url = {http://arxiv.org/abs/2408.04738},
	doi = {10.48550/arXiv.2408.04738},
	abstract = {Grasp planning is an important task for robotic manipulation. Though it is a richly studied area, a standalone, fast, and differentiable grasp planner that can work with robot grippers of different DOFs has not been reported. In this work, we present DiPGrasp, a grasp planner that satisfies all these goals. DiPGrasp takes a force-closure geometric surface matching grasp quality metric. It adopts a gradient-based optimization scheme on the metric, which also considers parallel sampling and collision handling. This not only drastically accelerates the grasp search process over the object surface but also makes it differentiable. We apply DiPGrasp to three applications, namely grasp dataset construction, mask-conditioned planning, and pose refinement. For dataset generation, as a standalone planner, DiPGrasp has clear advantages over speed and quality compared with several classic planners. For mask-conditioned planning, it can turn a 3D perception model into a 3D grasp detection model instantly. As a pose refiner, it can optimize the coarse grasp prediction from the neural network, as well as the neural network parameters. Finally, we conduct real-world experiments with the Barrett hand and Schunk SVH 5-finger hand. Video and supplementary materials can be viewed on our website: {\textbackslash}url\{https://dipgrasp.robotflow.ai\}.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Xu, Wenqiang and Zhang, Jieyi and Tang, Tutian and Yu, Zhenjun and Li, Yutong and Lu, Cewu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04738 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{pang_depth_2024,
	title = {Depth {Helps}: {Improving} {Pre}-trained {RGB}-based {Policy} with {Depth} {Information} {Injection}},
	shorttitle = {Depth {Helps}},
	url = {http://arxiv.org/abs/2408.05107},
	doi = {10.48550/arXiv.2408.05107},
	abstract = {3D perception ability is crucial for generalizable robotic manipulation. While recent foundation models have made significant strides in perception and decision-making with RGB-based input, their lack of 3D perception limits their effectiveness in fine-grained robotic manipulation tasks. To address these limitations, we propose a Depth Information Injection (\${\textbackslash}bold\{DI\}{\textasciicircum}\{{\textbackslash}bold\{2\}\}\$) framework that leverages the RGB-Depth modality for policy fine-tuning, while relying solely on RGB images for robust and efficient deployment. Concretely, we introduce the Depth Completion Module (DCM) to extract the spatial prior knowledge related to depth information and generate virtual depth information from RGB inputs to aid policy deployment. Further, we propose the Depth-Aware Codebook (DAC) to eliminate noise and reduce the cumulative error from the depth prediction. In the inference phase, this framework employs RGB inputs and accurately predicted depth data to generate the manipulation action. We conduct experiments on simulated LIBERO environments and real-world scenarios, and the experiment results prove that our method could effectively enhance the pre-trained RGB-based policy with 3D perception ability for robotic manipulation. The website is released at https://gewu-lab.github.io/DepthHelps-IROS2024.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Pang, Xincheng and Xia, Wenke and Wang, Zhigang and Zhao, Bin and Hu, Di and Wang, Dong and Li, Xuelong},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05107 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{fang_embodied_2024,
	title = {Embodied {Uncertainty}-{Aware} {Object} {Segmentation}},
	url = {http://arxiv.org/abs/2408.04760},
	doi = {10.48550/arXiv.2408.04760},
	abstract = {We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Fang, Xiaolin and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04760 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{niecksch_mesh-based_2024,
	title = {Mesh-based {Object} {Tracking} for {Dynamic} {Semantic} {3D} {Scene} {Graphs} via {Ray} {Tracing}},
	url = {http://arxiv.org/abs/2408.04979},
	doi = {10.48550/arXiv.2408.04979},
	abstract = {In this paper, we present a novel method for 3D geometric scene graph generation using range sensors and RGB cameras. We initially detect instance-wise keypoints with a YOLOv8s model to compute 6D pose estimates of known objects by solving PnP. We use a ray tracing approach to track a geometric scene graph consisting of mesh models of object instances. In contrast to classical point-to-point matching, this leads to more robust results, especially under occlusions between objects instances. We show that using this hybrid strategy leads to robust self-localization, pre-segmentation of the range sensor data and accurate pose tracking of objects using the same environmental representation. All detected objects are integrated into a semantic scene graph. This scene graph then serves as a front end to a semantic mapping framework to allow spatial reasoning.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Niecksch, Lennart and Mock, Alexander and Igelbrink, Felix and Wiemann, Thomas and Hertzberg, Joachim},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04979 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{kang_defense_2024,
	title = {In {Defense} of {Lazy} {Visual} {Grounding} for {Open}-{Vocabulary} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2408.04961},
	doi = {10.48550/arXiv.2408.04961},
	abstract = {We present lazy visual grounding, a two-stage approach of unsupervised object mask discovery followed by object grounding, for open-vocabulary semantic segmentation. Plenty of the previous art casts this task as pixel-to-text classification without object-level comprehension, leveraging the image-to-text classification capability of pretrained vision-and-language models. We argue that visual objects are distinguishable without the prior text information as segmentation is essentially a vision task. Lazy visual grounding first discovers object masks covering an image with iterative Normalized cuts and then later assigns text on the discovered objects in a late interaction manner. Our model requires no additional training yet shows great performance on five public datasets: Pascal VOC, Pascal Context, COCO-object, COCO-stuff, and ADE 20K. Especially, the visually appealing segmentation results demonstrate the model capability to localize objects precisely. Paper homepage: https://cvlab.postech.ac.kr/research/lazygrounding},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Kang, Dahyun and Cho, Minsu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04961 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{misra_self-supervised_2020,
	title = {Self-{Supervised} {Learning} of {Pretext}-{Invariant} {Representations}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.html},
	urldate = {2024-09-09},
	author = {Misra, Ishan and Maaten, Laurens van der},
	year = {2020},
	pages = {6707--6717},
}

@inproceedings{papadopoulos_inductive_2002,
	address = {Berlin, Heidelberg},
	title = {Inductive {Confidence} {Machines} for {Regression}},
	isbn = {978-3-540-36755-0},
	doi = {10.1007/3-540-36755-1_29},
	abstract = {The existing methods of predicting with confidence give good accuracy and confidence values, but quite often are computationally inefficient. Some partial solutions have been suggested in the past. Both the original method and these solutions were based on transductive inference. In this paper we make a radical step of replacing transductive inference with inductive inference and define what we call the Inductive Confidence Machine (ICM); our main concern in this paper is the use of ICM in regression problems. The algorithm proposed in this paper is based on the Ridge Regression procedure (which is usually used for outputting bare predictions) and is much faster than the existing transductive techniques. The inductive approach described in this paper may be the only option available when dealing with large data sets.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML} 2002},
	publisher = {Springer},
	author = {Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
	editor = {Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
	year = {2002},
	keywords = {Inductive Inference, Kolmogorov Complexity, Ridge Regression, Tolerance Region, True Label},
	pages = {345--356},
}

@incollection{papadopoulos_inductive_2008,
	title = {Inductive {Conformal} {Prediction}: {Theory} and {Application} to {Neural} {Networks}},
	isbn = {978-953-7619-03-9},
	shorttitle = {Inductive {Conformal} {Prediction}},
	url = {https://www.intechopen.com/chapters/5294},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2024-09-09},
	booktitle = {Tools in {Artificial} {Intelligence}},
	publisher = {IntechOpen},
	author = {Papadopoulos, Harris},
	month = aug,
	year = {2008},
	doi = {10.5772/6078},
}

@inproceedings{prokudin_deep_2018,
	title = {Deep {Directional} {Statistics}: {Pose} {Estimation} with {Uncertainty} {Quantification}},
	shorttitle = {Deep {Directional} {Statistics}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Sergey_Prokudin_Deep_Directional_Statistics_ECCV_2018_paper.html},
	urldate = {2024-09-09},
	author = {Prokudin, Sergey and Gehler, Peter and Nowozin, Sebastian},
	year = {2018},
	pages = {534--551},
}

@inproceedings{romano_conformalized_2019,
	title = {Conformalized {Quantile} {Regression}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html},
	abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
	urldate = {2024-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
	year = {2019},
}

@article{dohare_loss_2024,
	title = {Loss of plasticity in deep continual learning},
	volume = {632},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07711-7},
	doi = {10.1038/s41586-024-07711-7},
	abstract = {Artificial neural networks, deep-learning methods and the backpropagation algorithm1 form the foundation of modern machine learning and artificial intelligence. These methods are almost always used in two phases, one in which the weights of the network are updated and one in which the weights are held constant while the network is used or evaluated. This contrasts with natural learning and many applications, which require continual learning. It has been unclear whether or not deep learning methods work in continual learning settings. Here we show that they do not—that standard deep-learning methods gradually lose plasticity in continual-learning settings until they learn no better than a shallow network. We show such loss of plasticity using the classic ImageNet dataset and reinforcement-learning problems across a wide range of variations in the network and the learning algorithm. Plasticity is maintained indefinitely only by algorithms that continually inject diversity into the network, such as our continual backpropagation algorithm, a variation of backpropagation in which a small fraction of less-used units are continually and randomly reinitialized. Our results indicate that methods based on gradient descent are not enough—that sustained deep learning requires a random, non-gradient component to maintain variability and plasticity.},
	language = {en},
	number = {8026},
	urldate = {2024-09-09},
	journal = {Nature},
	author = {Dohare, Shibhansh and Hernandez-Garcia, J. Fernando and Lan, Qingfeng and Rahman, Parash and Mahmood, A. Rupam and Sutton, Richard S.},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Human behaviour},
	pages = {768--774},
}

@inproceedings{ren_robots_2023,
	title = {Robots {That} {Ask} {For} {Help}: {Uncertainty} {Alignment} for {Large} {Language} {Model} {Planners}},
	shorttitle = {Robots {That} {Ask} {For} {Help}},
	url = {https://openreview.net/forum?id=4ZK8ODNyFXx},
	abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities --- from step-by-step planning to commonsense reasoning --- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, a framework for measuring and aligning the uncertainty of LLM-based planners, such that they know when they don't know, and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (for example, from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out-of-the-box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models.},
	language = {en},
	urldate = {2024-09-09},
	author = {Ren, Allen Z. and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and Xu, Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar, Anirudha},
	month = aug,
	year = {2023},
}

@article{guigui_introduction_2023,
	title = {Introduction to {Riemannian} {Geometry} and {Geometric} {Statistics}: {From} {Basic} {Theory} to {Implementation} with {Geomstats}},
	volume = {16},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Introduction to {Riemannian} {Geometry} and {Geometric} {Statistics}},
	url = {https://www.nowpublishers.com/article/Details/MAL-098},
	doi = {10.1561/2200000098},
	abstract = {Introduction to Riemannian Geometry and Geometric Statistics: From Basic Theory to Implementation with Geomstats},
	language = {English},
	number = {3},
	urldate = {2024-09-09},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Guigui, Nicolas and Miolane, Nina and Pennec, Xavier},
	month = feb,
	year = {2023},
	note = {Publisher: Now Publishers, Inc.},
	pages = {329--493},
}

@misc{angelopoulos_gentle_2021,
	title = {A {Gentle} {Introduction} to {Conformal} {Prediction} and {Distribution}-{Free} {Uncertainty} {Quantification}},
	url = {https://arxiv.org/abs/2107.07511v6},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
	language = {en},
	urldate = {2024-09-09},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	month = jul,
	year = {2021},
}

@inproceedings{stutz_learning_2021,
	title = {Learning {Optimal} {Conformal} {Classifiers}},
	url = {https://openreview.net/forum?id=t8O-4LKFVx},
	abstract = {Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the classifier's predictions, e.g., its probability estimates, to predict confidence sets containing the true class with a user-specified probability. However, using CP as a separate processing step after training prevents the underlying model from adapting to the prediction of confidence sets. Thus, this paper explores strategies to differentiate through CP during training with the goal of training model with the conformal wrapper end-to-end. In our approach, conformal training (ConfTr), we specifically "simulate" conformalization on mini-batches during training. Compared to standard training, ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art CP methods applied after training. Moreover, it allows to "shape" the confidence sets predicted at test time, which is difficult for standard CP. On experiments with several datasets, we show ConfTr can influence how inefficiency is distributed across classes, or guide the composition of confidence sets in terms of the included classes, while retaining the guarantees offered by CP.},
	language = {en},
	urldate = {2024-09-09},
	author = {Stutz, David and Dvijotham, Krishnamurthy Dj and Cemgil, Ali Taylan and Doucet, Arnaud},
	month = oct,
	year = {2021},
}

@inproceedings{wang_discovering_2024,
	title = {Discovering {Robotic} {Interaction} {Modes} with {Discrete} {Representation} {Learning}},
	url = {https://openreview.net/forum?id=xcBH8Jhmbi},
	abstract = {Abstract: Human actions manipulating articulated objects, such as opening and closing a drawer, can be categorized into multiple modalities we define as interaction modes. Traditional robot learning approaches lack discrete representations of these modes, which are crucial for empirical sampling and grounding. In this paper, we present ActAIM2, which learns a discrete representation of robot manipulation interaction modes in a purely unsupervised fashion, without the use of expert labels or simulator-based privileged information. Utilizing novel data collection methods involving simulator rollouts, ActAIM2 consists of an interaction mode selector and a low-level action predictor. The selector generates discrete representations of potential interaction modes with self-supervision, while the predictor outputs corresponding action trajectories. Our method is validated through its success rate in manipulating articulated objects and its robustness in sampling meaningful actions from the discrete representation. Extensive experiments demonstrate ActAIM2’s effectiveness in enhancing manipulability and generalizability over baselines and ablation studies. For videos and additional results, see our website: https://actaim2.github.io/.},
	language = {en},
	urldate = {2024-09-09},
	author = {Wang, Liquan and Goyal, Ankit and Xu, Haoping and Garg, Animesh},
	month = sep,
	year = {2024},
}

@inproceedings{li_flowbothd_2024,
	title = {{FlowBotHD}: {History}-{Aware} {Diffuser} {Handling} {Ambiguities} in {Articulated} {Objects} {Manipulation}},
	shorttitle = {{FlowBotHD}},
	url = {https://openreview.net/forum?id=3ZAgXBRvla},
	abstract = {We introduce a novel approach to manipulate articulated objects with ambiguities, such as opening a door, in which multi-modality and occlusions create ambiguities about the opening side and direction. Multi-modality occurs when the method to open a fully closed door (push, pull, slide) is uncertain, or the side from which it should be opened is uncertain. Occlusions further obscure the door’s shape from certain angles, creating further ambiguities during the occlusion. To tackle these challenges, we propose a history-aware diffusion network that models the multi-modal distribution of the articulated object and uses history to disambiguate actions and make stable predictions under occlusions. Experiments and analysis demonstrate the state-of-art performance of our method and specifically improvements in ambiguity-caused failure modes. Our anonymized website is available at https://flowbothd.github.io/.},
	language = {en},
	urldate = {2024-09-09},
	author = {Li, Yishu and Leng, Wen Hui and Fang, Yiming and Eisner, Ben and Held, David},
	month = sep,
	year = {2024},
}

@misc{dou_unlocking_2024,
	title = {Unlocking {Exocentric} {Video}-{Language} {Data} for {Egocentric} {Video} {Representation} {Learning}},
	url = {https://arxiv.org/abs/2408.03567v1},
	abstract = {We present EMBED (Egocentric Models Built with Exocentric Data), a method designed to transform exocentric video-language data for egocentric video representation learning. Large-scale exocentric data covers diverse activities with significant potential for egocentric learning, but inherent disparities between egocentric and exocentric data pose challenges in utilizing one view for the other seamlessly. Egocentric videos predominantly feature close-up hand-object interactions, whereas exocentric videos offer a broader perspective on human activities. Additionally, narratives in egocentric datasets are typically more action-centric and closely linked with the visual content, in contrast to the narrative styles found in exocentric datasets. To address these challenges, we employ a data transformation framework to adapt exocentric data for egocentric training, focusing on identifying specific video clips that emphasize hand-object interactions and transforming narration styles to align with egocentric perspectives. By applying both vision and language style transfer, our framework creates a new egocentric dataset derived from exocentric video-language data. Through extensive evaluations, we demonstrate the effectiveness of EMBED, achieving state-of-the-art results across various egocentric downstream tasks, including an absolute improvement of 4.7\% on the Epic-Kitchens-100 multi-instance retrieval and 6.2\% on the EGTEA classification benchmarks in zero-shot settings. Furthermore, EMBED enables egocentric video-language models to perform competitively in exocentric tasks. Finally, we showcase EMBED's application across various exocentric datasets, exhibiting strong generalization capabilities when applied to different exocentric datasets.},
	language = {en},
	urldate = {2024-09-05},
	author = {Dou, Zi-Yi and Yang, Xitong and Nagarajan, Tushar and Wang, Huiyu and Huang, Jing and Peng, Nanyun and Kitani, Kris and Chu, Fu-Jen},
	month = aug,
	year = {2024},
}

@misc{dambrosio_achieving_2024,
	title = {Achieving {Human} {Level} {Competitive} {Robot} {Table} {Tennis}},
	url = {https://arxiv.org/abs/2408.03906v2},
	abstract = {Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45\% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100\% matches vs. beginners and 55\% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis},
	language = {en},
	urldate = {2024-09-05},
	author = {D'Ambrosio, David B. and Abeyruwan, Saminda and Graesser, Laura and Iscen, Atil and Amor, Heni Ben and Bewley, Alex and Reed, Barney J. and Reymann, Krista and Takayama, Leila and Tassa, Yuval and Choromanski, Krzysztof and Coumans, Erwin and Jain, Deepali and Jaitly, Navdeep and Jaques, Natasha and Kataoka, Satoshi and Kuang, Yuheng and Lazic, Nevena and Mahjourian, Reza and Moore, Sherry and Oslund, Kenneth and Shankar, Anish and Sindhwani, Vikas and Vanhoucke, Vincent and Vesom, Grace and Xu, Peng and Sanketi, Pannag R.},
	month = aug,
	year = {2024},
}

@misc{chen_compositional_2024,
	title = {Compositional {Physical} {Reasoning} of {Objects} and {Events} from {Videos}},
	url = {https://arxiv.org/abs/2408.02687v1},
	abstract = {Understanding and reasoning about objects' physical properties in the natural world is a fundamental challenge in artificial intelligence. While some properties like colors and shapes can be directly observed, others, such as mass and electric charge, are hidden from the objects' visual appearance. This paper addresses the unique challenge of inferring these hidden physical properties from objects' motion and interactions and predicting corresponding dynamics based on the inferred physical properties. We first introduce the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes limited videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions. Besides the synthetic videos from simulators, we also collect a real-world dataset to show further test physical reasoning abilities of different models. We evaluate state-of-the-art video reasoning models on ComPhy and reveal their limited ability to capture these hidden properties, which leads to inferior performance. We also propose a novel neuro-symbolic framework, Physical Concept Reasoner (PCR), that learns and reasons about both visible and hidden physical properties from question answering. After training, PCR demonstrates remarkable capabilities. It can detect and associate objects across frames, ground visible and hidden physical properties, make future and counterfactual predictions, and utilize these extracted representations to answer challenging questions.},
	language = {en},
	urldate = {2024-09-05},
	author = {Chen, Zhenfang and Dong, Shilong and Yi, Kexin and Li, Yunzhu and Ding, Mingyu and Torralba, Antonio and Tenenbaum, Joshua B. and Gan, Chuang},
	month = aug,
	year = {2024},
}

@misc{liu_line-based_2024,
	title = {Line-based 6-{DoF} {Object} {Pose} {Estimation} and {Tracking} {With} an {Event} {Camera}},
	url = {https://arxiv.org/abs/2408.03225v1},
	abstract = {Pose estimation and tracking of objects is a fundamental application in 3D vision. Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion. These features make event cameras an ideal complement over standard cameras for object pose estimation. In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera. Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance. Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models. Furthermore, object poses are refined and continuously tracked by minimizing event-line distances. Events are assigned different weights based on these distances, employing robust estimation algorithms. To evaluate the precision of the proposed methods in object pose estimation and tracking, we have devised and established an event-based moving object dataset. Compared against state-of-the-art methods, the robustness and accuracy of our methods have been validated both on synthetic experiments and the proposed dataset. The source code is available at https://github.com/Zibin6/LOPET.},
	language = {en},
	urldate = {2024-09-05},
	author = {Liu, Zibin and Guan, Banglei and Shang, Yang and Yu, Qifeng and Kneip, Laurent},
	month = aug,
	year = {2024},
}

@misc{yan_object_2024,
	title = {An {Object} is {Worth} 64x64 {Pixels}: {Generating} {3D} {Object} via {Image} {Diffusion}},
	shorttitle = {An {Object} is {Worth} 64x64 {Pixels}},
	url = {https://arxiv.org/abs/2408.03178v1},
	abstract = {We introduce a new approach for generating realistic 3D models with UV maps through a representation termed "Object Images." This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.},
	language = {en},
	urldate = {2024-09-05},
	author = {Yan, Xingguang and Lee, Han-Hung and Wan, Ziyu and Chang, Angel X.},
	month = aug,
	year = {2024},
}

@misc{lu_avatarpose_2024,
	title = {{AvatarPose}: {Avatar}-guided {3D} {Pose} {Estimation} of {Close} {Human} {Interaction} from {Sparse} {Multi}-view {Videos}},
	shorttitle = {{AvatarPose}},
	url = {https://arxiv.org/abs/2408.02110v2},
	abstract = {Despite progress in human motion capture, existing multi-view methods often face challenges in estimating the 3D pose and shape of multiple closely interacting people. This difficulty arises from reliance on accurate 2D joint estimations, which are hard to obtain due to occlusions and body contact when people are in close interaction. To address this, we propose a novel method leveraging the personalized implicit neural avatar of each individual as a prior, which significantly improves the robustness and precision of this challenging pose estimation task. Concretely, the avatars are efficiently reconstructed via layered volume rendering from sparse multi-view videos. The reconstructed avatar prior allows for the direct optimization of 3D poses based on color and silhouette rendering loss, bypassing the issues associated with noisy 2D detections. To handle interpenetration, we propose a collision loss on the overlapping shape regions of avatars to add penetration constraints. Moreover, both 3D poses and avatars are optimized in an alternating manner. Our experimental results demonstrate state-of-the-art performance on several public datasets.},
	language = {en},
	urldate = {2024-09-05},
	author = {Lu, Feichi and Dong, Zijian and Song, Jie and Hilliges, Otmar},
	month = aug,
	year = {2024},
}

@inproceedings{naughton_respilot_2024,
	title = {{ResPilot}: {Teleoperated} {Finger} {Gaiting} via {Gaussian} {Process} {Residual} {Learning}},
	shorttitle = {{ResPilot}},
	url = {https://openreview.net/forum?id=B45HRM4Wb4},
	abstract = {Dexterous robot hand teleoperation allows for long-range transfer of human manipulation expertise, and could simultaneously provide a way for humans to teach these skills to robots. However, current methods struggle to reproduce the functional workspace of the human hand, often limiting them to simple grasping tasks. We present a novel method for finger-gaited manipulation with multi-fingered robot hands. Our method provides the operator enhanced flexibility in making contacts by expanding the reachable workspace of the robot hand through residual Gaussian Process learning. We also assist the operator in maintaining stable contacts with the object by allowing them to constrain fingertips of the hand to move in concert. Extensive quantitative evaluations show that our method significantly increases the reachable workspace of the robot hand and enables the completion of novel dexterous finger gaiting tasks.},
	language = {en},
	urldate = {2024-09-06},
	author = {Naughton, Patrick and Cui, Jinda and Patel, Karankumar and Iba, Soshi},
	month = sep,
	year = {2024},
}

@misc{noseworthy_forge_2024,
	title = {{FORGE}: {Force}-{Guided} {Exploration} for {Robust} {Contact}-{Rich} {Manipulation} under {Uncertainty}},
	shorttitle = {{FORGE}},
	url = {http://arxiv.org/abs/2408.04587},
	doi = {10.48550/arXiv.2408.04587},
	abstract = {We present FORGE, a method that enables sim-to-real transfer of contact-rich manipulation policies in the presence of significant pose uncertainty. FORGE combines a force threshold mechanism with a dynamics randomization scheme during policy learning in simulation, to enable the robust transfer of the learned policies to the real robot. At deployment, FORGE policies, conditioned on a maximum allowable force, adaptively perform contact-rich tasks while respecting the specified force threshold, regardless of the controller gains. Additionally, FORGE autonomously predicts a termination action once the task has succeeded. We demonstrate that FORGE can be used to learn a variety of robust contact-rich policies, enabling multi-stage assembly of a planetary gear system, which requires success across three assembly tasks: nut-threading, insertion, and gear meshing. Project website can be accessed at https://noseworm.github.io/forge/.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Noseworthy, Michael and Tang, Bingjie and Wen, Bowen and Handa, Ankur and Roy, Nicholas and Fox, Dieter and Ramos, Fabio and Narang, Yashraj and Akinola, Iretiayo},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04587 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{bhaskar_navinact_2024,
	title = {{NAVINACT}: {Combining} {Navigation} and {Imitation} {Learning} for {Bootstrapping} {Reinforcement} {Learning}},
	shorttitle = {{NAVINACT}},
	url = {http://arxiv.org/abs/2408.04054},
	doi = {10.48550/arXiv.2408.04054},
	abstract = {Reinforcement Learning (RL) has shown remarkable progress in simulation environments, yet its application to real-world robotic tasks remains limited due to challenges in exploration and generalisation. To address these issues, we introduce NAVINACT, a framework that chooses when the robot should use classical motion planning-based navigation and when it should learn a policy. To further improve the efficiency in exploration, we use imitation data to bootstrap the exploration. NAVINACT dynamically switches between two modes of operation: navigating to a waypoint using classical techniques when away from the objects and reinforcement learning for fine-grained manipulation control when about to interact with objects. NAVINACT consists of a multi-head architecture composed of ModeNet for mode classification, NavNet for waypoint prediction, and InteractNet for precise manipulation. By combining the strengths of RL and Imitation Learning (IL), NAVINACT improves sample efficiency and mitigates distribution shift, ensuring robust task execution. We evaluate our approach across multiple challenging simulation environments and real-world tasks, demonstrating superior performance in terms of adaptability, efficiency, and generalization compared to existing methods. In both simulated and real-world settings, NAVINACT demonstrates robust performance. In simulations, NAVINACT surpasses baseline methods by 10-15{\textbackslash}\% in training success rates at 30k samples and by 30-40{\textbackslash}\% during evaluation phases. In real-world scenarios, it demonstrates a 30-40{\textbackslash}\% higher success rate on simpler tasks compared to baselines and uniquely succeeds in complex, two-stage manipulation tasks. Datasets and supplementary materials can be found on our website: \{https://raaslab.org/projects/NAVINACT/\}.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Bhaskar, Amisha and Mahammad, Zahiruddin and Jadhav, Sachin R. and Tokekar, Pratap},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04054 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{lu_koi_2024,
	title = {{KOI}: {Accelerating} {Online} {Imitation} {Learning} via {Hybrid} {Key}-state {Guidance}},
	shorttitle = {{KOI}},
	url = {http://arxiv.org/abs/2408.02912},
	doi = {10.48550/arXiv.2408.02912},
	abstract = {Online Imitation Learning methods struggle with the gap between extensive online exploration space and limited expert trajectories, which hinder efficient exploration due to inaccurate task-aware reward estimation. Inspired by the findings from cognitive neuroscience that task decomposition could facilitate cognitive processing for efficient learning, we hypothesize that an agent could estimate precise task-aware imitation rewards for efficient online exploration by decomposing the target task into the objectives of "what to do" and the mechanisms of "how to do". In this work, we introduce the hybrid Key-state guided Online Imitation (KOI) learning approach, which leverages the integration of semantic and motion key states as guidance for task-aware reward estimation. Initially, we utilize the visual-language models to segment the expert trajectory into semantic key states, indicating the objectives of "what to do". Within the intervals between semantic key states, optical flow is employed to capture motion key states to understand the process of "how to do". By integrating a thorough grasp of both semantic and motion key states, we refine the trajectory-matching reward computation, encouraging task-aware exploration for efficient online imitation learning. Our experiment results prove that our method is more sample efficient in the Meta-World and LIBERO environments. We also conduct real-world robotic manipulation experiments to validate the efficacy of our method, demonstrating the practical applicability of our KOI method.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Lu, Jingxian and Xia, Wenke and Wang, Dong and Wang, Zhigang and Zhao, Bin and Hu, Di and Li, Xuelong},
	month = aug,
	year = {2024},
	note = {arXiv:2408.02912 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhang_lac-net_2024,
	title = {{LAC}-{Net}: {Linear}-{Fusion} {Attention}-{Guided} {Convolutional} {Network} for {Accurate} {Robotic} {Grasping} {Under} the {Occlusion}},
	shorttitle = {{LAC}-{Net}},
	url = {http://arxiv.org/abs/2408.03238},
	doi = {10.48550/arXiv.2408.03238},
	abstract = {This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Zhang, Jinyu and Gu, Yongchong and Gao, Jianxiong and Lin, Haitao and Sun, Qiang and Sun, Xinwei and Xue, Xiangyang and Fu, Yanwei},
	month = aug,
	year = {2024},
	note = {arXiv:2408.03238 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wang_large-scale_2024,
	title = {Large-scale {Deployment} of {Vision}-based {Tactile} {Sensors} on {Multi}-fingered {Grippers}},
	url = {https://arxiv.org/abs/2408.02206v1},
	abstract = {Vision-based Tactile Sensors (VBTSs) show significant promise in that they can leverage image measurements to provide high-spatial-resolution human-like performance. However, current VBTS designs, typically confined to the fingertips of robotic grippers, prove somewhat inadequate, as many grasping and manipulation tasks require multiple contact points with the object. With an end goal of enabling large-scale, multi-surface tactile sensing via VBTSs, our research (i) develops a synchronized image acquisition system with minimal latency,(ii) proposes a modularized VBTS design for easy integration into finger phalanges, and (iii) devises a zero-shot calibration approach to improve data efficiency in the simultaneous calibration of multiple VBTSs. In validating the system within a miniature 3-fingered robotic gripper equipped with 7 VBTSs we demonstrate improved tactile perception performance by covering the contact surfaces of both gripper fingers and palm. Additionally, we show that our VBTS design can be seamlessly integrated into various end-effector morphologies significantly reducing the data requirements for calibration.},
	language = {en},
	urldate = {2024-09-05},
	author = {Wang, Meng and Li, Wanlin and Liang, Hao and Li, Boren and Althoefer, Kaspar and Su, Yao and Liu, Hangxin},
	month = aug,
	year = {2024},
}

@misc{lu_voxeltrack_2024,
	title = {{VoxelTrack}: {Exploring} {Voxel} {Representation} for {3D} {Point} {Cloud} {Object} {Tracking}},
	shorttitle = {{VoxelTrack}},
	url = {http://arxiv.org/abs/2408.02263},
	doi = {10.48550/arXiv.2408.02263},
	abstract = {Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cross-iterative feature fusion module to further explore fine-grained 3D spatial information for tracking. Benefiting from accurate 3D spatial information being modeled, our VoxelTrack simplifies tracking pipeline with a single regression loss. Extensive experiments are conducted on three widely-adopted datasets including KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that VoxelTrack achieves state-of-the-art performance (88.3\%, 71.4\% and 63.6\% mean precision on the three datasets, respectively), and outperforms the existing trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source code and model will be released.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Lu, Yuxuan and Nie, Jiahao and He, Zhiwei and Gu, Hongjie and Lv, Xudong},
	month = aug,
	year = {2024},
	note = {arXiv:2408.02263 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_joint-motion_2024,
	title = {Joint-{Motion} {Mutual} {Learning} for {Pose} {Estimation} in {Videos}},
	url = {http://arxiv.org/abs/2408.02285},
	doi = {10.48550/arXiv.2408.02285},
	abstract = {Human pose estimation in videos has long been a compelling yet challenging task within the realm of computer vision. Nevertheless, this task remains difficult because of the complex video scenes, such as video defocus and self-occlusion. Recent methods strive to integrate multi-frame visual features generated by a backbone network for pose estimation. However, they often ignore the useful joint information encoded in the initial heatmap, which is a by-product of the backbone generation. Comparatively, methods that attempt to refine the initial heatmap fail to consider any spatio-temporal motion features. As a result, the performance of existing methods for pose estimation falls short due to the lack of ability to leverage both local joint (heatmap) information and global motion (feature) dynamics. To address this problem, we propose a novel joint-motion mutual learning framework for pose estimation, which effectively concentrates on both local joint dependency and global pixel-level motion dynamics. Specifically, we introduce a context-aware joint learner that adaptively leverages initial heatmaps and motion flow to retrieve robust local joint feature. Given that local joint feature and global motion flow are complementary, we further propose a progressive joint-motion mutual learning that synergistically exchanges information and interactively learns between joint feature and motion flow to improve the capability of the model. More importantly, to capture more diverse joint and motion cues, we theoretically analyze and propose an information orthogonality objective to avoid learning redundant information from multi-cues. Empirical experiments show our method outperforms prior arts on three challenging benchmarks.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Wu, Sifan and Chen, Haipeng and Yin, Yifang and Hu, Sihao and Feng, Runyang and Jiao, Yingying and Yang, Ziqi and Liu, Zhenguang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.02285 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yoo_ropotter_2024,
	title = {{RoPotter}: {Toward} {Robotic} {Pottery} and {Deformable} {Object} {Manipulation} with {Structural} {Priors}},
	shorttitle = {{RoPotter}},
	url = {https://arxiv.org/abs/2408.02184v1},
	abstract = {Humans are capable of continuously manipulating a wide variety of deformable objects into complex shapes. This is made possible by our intuitive understanding of material properties and mechanics of the object, for reasoning about object states even when visual perception is occluded. These capabilities allow us to perform diverse tasks ranging from cooking with dough to expressing ourselves with pottery-making. However, developing robotic systems to robustly perform similar tasks remains challenging, as current methods struggle to effectively model volumetric deformable objects and reason about the complex behavior they typically exhibit. To study the robotic systems and algorithms capable of deforming volumetric objects, we introduce a novel robotics task of continuously deforming clay on a pottery wheel. We propose a pipeline for perception and pottery skill-learning, called RoPotter, wherein we demonstrate that structural priors specific to the task of pottery-making can be exploited to simplify the pottery skill-learning process. Namely, we can project the cross-section of the clay to a plane to represent the state of the clay, reducing dimensionality. We also demonstrate a mesh-based method of occluded clay state recovery, toward robotic agents capable of continuously deforming clay. Our experiments show that by using the reduced representation with structural priors based on the deformation behaviors of the clay, RoPotter can perform the long-horizon pottery task with 44.4\% lower final shape error compared to the state-of-the-art baselines.},
	language = {en},
	urldate = {2024-09-05},
	author = {Yoo, Uksang and Hung, Adam and Francis, Jonathan and Oh, Jean and Ichnowski, Jeffrey},
	month = aug,
	year = {2024},
}

@misc{yan_surprisingly_2024,
	title = {A {Surprisingly} {Efficient} {Representation} for {Multi}-{Finger} {Grasping}},
	url = {http://arxiv.org/abs/2408.02455},
	doi = {10.48550/arXiv.2408.02455},
	abstract = {The problem of grasping objects using a multi-finger hand has received significant attention in recent years. However, it remains challenging to handle a large number of unfamiliar objects in real and cluttered environments. In this work, we propose a representation that can be effectively mapped to the multi-finger grasp space. Based on this representation, we develop a simple decision model that generates accurate grasp quality scores for different multi-finger grasp poses using only hundreds to thousands of training samples. We demonstrate that our representation performs well on a real robot and achieves a success rate of 78.64\% after training with only 500 real-world grasp attempts and 87\% with 4500 grasp attempts. Additionally, we achieve a success rate of 84.51\% in a dynamic human-robot handover scenario using a multi-finger hand.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Yan, Hengxu and Fang, Hao-Shu and Lu, Cewu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.02455 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_eqvafford_2024,
	title = {{EqvAfford}: {SE}(3) {Equivariance} for {Point}-{Level} {Affordance} {Learning}},
	shorttitle = {{EqvAfford}},
	url = {http://arxiv.org/abs/2408.01953},
	doi = {10.48550/arXiv.2408.01953},
	abstract = {Humans perceive and interact with the world with the awareness of equivariance, facilitating us in manipulating different objects in diverse poses. For robotic manipulation, such equivariance also exists in many scenarios. For example, no matter what the pose of a drawer is (translation, rotation and tilt), the manipulation strategy is consistent (grasp the handle and pull in a line). While traditional models usually do not have the awareness of equivariance for robotic manipulation, which might result in more data for training and poor performance in novel object poses, we propose our EqvAfford framework, with novel designs to guarantee the equivariance in point-level affordance learning for downstream robotic manipulation, with great performance and generalization ability on representative tasks on objects in diverse poses.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Chen, Yue and Tie, Chenrui and Wu, Ruihai and Dong, Hao},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01953 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-09-03},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
	year = {2023},
	pages = {4015--4026},
}

@misc{liu_grounding_2024,
	title = {Grounding {DINO}: {Marrying} {DINO} with {Grounded} {Pre}-{Training} for {Open}-{Set} {Object} {Detection}},
	shorttitle = {Grounding {DINO}},
	url = {http://arxiv.org/abs/2303.05499},
	doi = {10.48550/arXiv.2303.05499},
	abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
	month = jul,
	year = {2024},
	note = {arXiv:2303.05499 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hodan_bop_2024,
	title = {{BOP} {Challenge} 2023 on {Detection} {Segmentation} and {Pose} {Estimation} of {Seen} and {Unseen} {Rigid} {Objects}},
	url = {https://openaccess.thecvf.com/content/CVPR2024W/CV4MR/html/Hodan_BOP_Challenge_2023_on_Detection_Segmentation_and_Pose_Estimation_of_CVPRW_2024_paper.html},
	language = {en},
	urldate = {2024-09-03},
	author = {Hodan, Tomas and Sundermeyer, Martin and Labbe, Yann and Nguyen, Van Nguyen and Wang, Gu and Brachmann, Eric and Drost, Bertram and Lepetit, Vincent and Rother, Carsten and Matas, Jiri},
	year = {2024},
	pages = {5610--5619},
}

@inproceedings{bregier_deep_2021,
	title = {Deep {Regression} on {Manifolds}: {A} {3D} {Rotation} {Case} {Study}},
	shorttitle = {Deep {Regression} on {Manifolds}},
	url = {https://ieeexplore.ieee.org/abstract/document/9665892?casa_token=Xy6rkv039MAAAAAA:flA1-MNHUHVdtNKb2aM6_Bsm7Kaz7ME464cpakVMsMaToPH_47Smwnox1gqtQFROEivR0J40DRo},
	doi = {10.1109/3DV53792.2021.00027},
	abstract = {Many machine learning problems involve regressing variables on a non-Euclidean manifold–e.g. a discrete probability distribution, or the 6D pose of an object. One way to tackle these problems through gradient-based learning is to use a differentiable function that maps arbitrary inputs of a Euclidean space onto the manifold. In this paper, we establish a set of desirable properties for such mapping, and in particular highlight the importance of pre-images connectivity/convexity. We illustrate these properties with a case study regarding 3D rotations. Through theoretical considerations and methodological experiments on a variety of tasks, we review various differentiable mappings on the 3D rotation space, and conjecture about the importance of their local linearity. We show that a mapping based on Procrustes orthonormalization generally performs best among the mappings considered, but that a rotation vector representation might also be suitable when restricted to small angles.},
	urldate = {2024-09-02},
	booktitle = {2021 {International} {Conference} on {3D} {Vision} ({3DV})},
	author = {Brégier, Romain},
	month = dec,
	year = {2021},
	note = {ISSN: 2475-7888},
	keywords = {Linearity, Machine learning, Manifolds, Neural networks, Probability distribution, SE(3), SO(3), Three-dimensional displays, Training, deep learning, manifold, pose, regression, rotation},
	pages = {166--174},
}

@inproceedings{li_representing_2024,
	title = {Representing {Robot} {Geometry} as {Distance} {Fields}: {Applications} to {Whole}-body {Manipulation}},
	shorttitle = {Representing {Robot} {Geometry} as {Distance} {Fields}},
	url = {https://ieeexplore.ieee.org/document/10611674},
	doi = {10.1109/ICRA57147.2024.10611674},
	abstract = {In this work, we propose a novel approach to represent robot geometry as distance fields (RDF) that extends the principle of signed distance fields (SDFs) to articulated kinematic chains. Our method employs a combination of Bernstein polynomials to encode the signed distance for each robot link with high accuracy and efficiency while ensuring the mathematical continuity and differentiability of SDFs. We further leverage the kinematics chain of the robot to produce the SDF representation in joint space, allowing robust distance queries in arbitrary joint configurations. The proposed RDF representation is differentiable and smooth in both task and joint spaces, enabling its direct integration to optimization problems. Additionally, the 0-level set of the robot corresponds to the robot surface, which can be seamlessly integrated into whole-body manipulation tasks. We conduct various experiments in both simulations and with 7-axis Franka Emika robots, comparing against baseline methods, and demonstrating its effectiveness in collision avoidance and whole-body manipulation tasks. Project page: https://sites.google.com/view/lrdf/home},
	urldate = {2024-09-02},
	booktitle = {2024 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Yiming and Zhang, Yan and Razmjoo, Amirreza and Calinon, Sylvain},
	month = may,
	year = {2024},
	keywords = {Accuracy, Geometry, Kinematics, Planning, Polynomials, Resource description framework, Shape},
	pages = {15351--15357},
}

@inproceedings{toussaint_sequence--constraints_2022,
	title = {Sequence-of-{Constraints} {MPC}: {Reactive} {Timing}-{Optimal} {Control} of {Sequential} {Manipulation}},
	shorttitle = {Sequence-of-{Constraints} {MPC}},
	url = {https://ieeexplore.ieee.org/abstract/document/9982236?casa_token=18ocddqwfYkAAAAA:zpVk4IKTXfT_9Obidufzk013eSUiZbdmnDZ3Y2YoDNssOY8qUknYceoMe_7pCLWLep2nC93O2yI},
	doi = {10.1109/IROS47612.2022.9982236},
	abstract = {Task and Motion Planning has made great progress in solving hard sequential manipulation problems. However, a gap between such planning formulations and control methods for reactive execution remains. In this paper we pro-pose a model predictive control approach dedicated to robustly execute a single sequence of constraints, which corresponds to a discrete decision sequence of a TAMP plan. We decompose the overall control problem into three sub-problems (solving for sequential waypoints, their timing, and a short receding horizon path) that each is a non-linear program solved online in each MPC cycle. The resulting control strategy can account for long-term interdependencies of constraints and reactively plan for a timing-optimal transition through all constraints. We additionally propose phase backtracking when running constraints of the current phase cannot be fulfilled, leading to a fluent re-initiation behavior that is robust to perturbations and interferences by an experimenter.},
	urldate = {2024-09-01},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Toussaint, Marc and Harris, Jason and Ha, Jung-Su and Driess, Danny and Hönig, Wolfgang},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Backtracking, Behavioral sciences, Perturbation methods, Planning, Standards, Task analysis, Timing},
	pages = {13753--13760},
}

@article{marchand_visp_2005,
	title = {{ViSP} for visual servoing: a generic software platform with a wide class of robot control skills},
	volume = {12},
	issn = {1558-223X},
	shorttitle = {{ViSP} for visual servoing},
	url = {https://ieeexplore.ieee.org/abstract/document/1577023},
	doi = {10.1109/MRA.2005.1577023},
	abstract = {ViSP (Visual Servoing Platform), a fully functional modular architecture that allows fast development of visual servoing applications, is described. The platform takes the form of a library which can be divided in three main modules: control processes, canonical vision-based tasks that contain the most classical linkages, and real-time tracking. ViSP software environment features independence with respect to the hardware, simplicity, extendibility, and portability. ViSP also features a large library of elementary tasks with various visual features that can be combined together, an image processing library that allows the tracking of visual cues at video rate, a simulator, an interface with various classical framegrabbers, a virtual 6-DOF robot that allows the simulation of visual servoing experiments, etc. The platform is implemented in C++ under Linux.},
	number = {4},
	urldate = {2024-08-30},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Marchand, E. and Spindler, F. and Chaumette, F.},
	month = dec,
	year = {2005},
	note = {Conference Name: IEEE Robotics \& Automation Magazine},
	keywords = {Application software, Computer architecture, Couplings, Hardware, Image processing, Linux, Process control, Robot control, Software libraries, Visual servoing},
	pages = {40--52},
}

@misc{li_unifying_2024,
	title = {Unifying {3D} {Representation} and {Control} of {Diverse} {Robots} with a {Single} {Camera}},
	url = {http://arxiv.org/abs/2407.08722},
	doi = {10.48550/arXiv.2407.08722},
	abstract = {Mirroring the complex structures and diverse functions of natural organisms is a long-standing challenge in robotics. Modern fabrication techniques have dramatically expanded feasible hardware, yet deploying these systems requires control software to translate desired motions into actuator commands. While conventional robots can easily be modeled as rigid links connected via joints, it remains an open challenge to model and control bio-inspired robots that are often multi-material or soft, lack sensing capabilities, and may change their material properties with use. Here, we introduce Neural Jacobian Fields, an architecture that autonomously learns to model and control robots from vision alone. Our approach makes no assumptions about the robot's materials, actuation, or sensing, requires only a single camera for control, and learns to control the robot without expert intervention by observing the execution of random commands. We demonstrate our method on a diverse set of robot manipulators, varying in actuation, materials, fabrication, and cost. Our approach achieves accurate closed-loop control and recovers the causal dynamic structure of each robot. By enabling robot control with a generic camera as the only sensor, we anticipate our work will dramatically broaden the design space of robotic systems and serve as a starting point for lowering the barrier to robotic automation.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Li, Sizhe Lester and Zhang, Annan and Chen, Boyuan and Matusik, Hanna and Liu, Chao and Rus, Daniela and Sitzmann, Vincent},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{chen_fully_2022,
	title = {Fully body visual self-modeling of robot morphologies},
	volume = {7},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abn1944},
	doi = {10.1126/scirobotics.abn1944},
	abstract = {Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These “self-models” allow robots to consider outcomes of multiple possible future actions without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward kinematic models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot’s state. Such query-driven self-models are continuous in the spatial domain, memory efficient, fully differentiable, and kinematic aware and can be used across a broader range of tasks. In physical experiments, we demonstrate how a visual self-model is accurate to about 1\% of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize, and recover from real-world damage, leading to improved machine resiliency.},
	number = {68},
	urldate = {2024-08-29},
	journal = {Science Robotics},
	author = {Chen, Boyuan and Kwiatkowski, Robert and Vondrick, Carl and Lipson, Hod},
	month = jul,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabn1944},
}

@inproceedings{qi_-hand_2022,
	title = {In-{Hand} {Object} {Rotation} via {Rapid} {Motor} {Adaptation}},
	url = {https://openreview.net/forum?id=Xux9gSS7WE0},
	abstract = {Generalized in-hand manipulation has long been an unsolved challenge of robotics. As a small step towards this grand goal, we demonstrate how to design and learn a simple adaptive controller to achieve in-hand object rotation using only fingertips. The controller is trained entirely in simulation on only cylindrical objects, which then – without any fine-tuning – can be directly deployed to a real robot hand to rotate dozens of objects with diverse sizes, shapes, and weights over the z-axis. This is achieved via rapid online adaptation of the robot’s controller to the object properties using only proprioception history. Furthermore, natural and stable finger gaits automatically emerge from training the control policy via reinforcement learning. Code and more videos are available at https://github.com/HaozhiQi/Hora .},
	language = {en},
	urldate = {2024-08-29},
	author = {Qi, Haozhi and Kumar, Ashish and Calandra, Roberto and Ma, Yi and Malik, Jitendra},
	month = aug,
	year = {2022},
}

@article{kroemer_review_2021,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {A {Review} of {Robot} {Learning} for {Manipulation}},
	url = {http://jmlr.org/papers/v22/19-804.html},
	abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment,  the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
	number = {30},
	urldate = {2024-08-29},
	journal = {Journal of Machine Learning Research},
	author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
	year = {2021},
	pages = {1--82},
}

@inproceedings{li_mrc-net_2024,
	title = {{MRC}-{Net}: 6-{DoF} {Pose} {Estimation} with {MultiScale} {Residual} {Correlation}},
	shorttitle = {{MRC}-{Net}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Li_MRC-Net_6-DoF_Pose_Estimation_with_MultiScale_Residual_Correlation_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-08-28},
	author = {Li, Yuelong and Mao, Yafei and Bala, Raja and Hadap, Sunil},
	year = {2024},
	pages = {10476--10486},
}

@inproceedings{wen_se3-tracknet_2020,
	title = {se(3)-{TrackNet}: {Data}-driven {6D} {Pose} {Tracking} by {Calibrating} {Image} {Residuals} in {Synthetic} {Domains}},
	shorttitle = {se(3)-{TrackNet}},
	url = {https://ieeexplore.ieee.org/document/9341314},
	doi = {10.1109/IROS45743.2020.9341314},
	abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.},
	urldate = {2024-08-23},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wen, Bowen and Mitash, Chaitanya and Ren, Baozhang and Bekris, Kostas E.},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Algebra, Benchmark testing, Neural networks, Task analysis, Three-dimensional displays, Training, Video sequences},
	pages = {10367--10373},
}

@misc{urain_deep_2024,
	title = {Deep {Generative} {Models} in {Robotics}: {A} {Survey} on {Learning} from {Multimodal} {Demonstrations}},
	shorttitle = {Deep {Generative} {Models} in {Robotics}},
	url = {http://arxiv.org/abs/2408.04380},
	doi = {10.48550/arXiv.2408.04380},
	abstract = {Learning from Demonstrations, the field that proposes to learn robot behavior models from data, is gaining popularity with the emergence of deep generative models. Although the problem has been studied for years under names such as Imitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning, classical methods have relied on models that don't capture complex data distributions well or don't scale well to large numbers of demonstrations. In recent years, the robot learning community has shown increasing interest in using deep generative models to capture the complexity of large datasets. In this survey, we aim to provide a unified and comprehensive review of the last year's progress in the use of deep generative models in robotics. We present the different types of models that the community has explored, such as energy-based models, diffusion models, action value maps, or generative adversarial networks. We also present the different types of applications in which deep generative models have been used, from grasp generation to trajectory generation or cost learning. One of the most important elements of generative models is the generalization out of distributions. In our survey, we review the different decisions the community has made to improve the generalization of the learned models. Finally, we highlight the research challenges and propose a number of future directions for learning deep generative models in robotics.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Urain, Julen and Mandlekar, Ajay and Du, Yilun and Shafiullah, Mahi and Xu, Danfei and Fragkiadaki, Katerina and Chalvatzaki, Georgia and Peters, Jan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04380 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{wan_vint-6d_2024,
	title = {{VinT}-{6D}: {A} {Large}-{Scale} {Object}-in-hand {Dataset} from {Vision}, {Touch} and {Proprioception}},
	shorttitle = {{VinT}-{6D}},
	url = {https://proceedings.mlr.press/v235/wan24d.html},
	abstract = {This paper addresses the scarcity of large-scale datasets for accurate object-in-hand pose estimation, which is crucial for robotic in-hand manipulation within the "Perception-Planning-Control" paradigm. Specifically, we introduce VinT-6D, the first extensive multi-modal dataset integrating vision, touch, and proprioception, to enhance robotic manipulation. VinT-6D comprises 2 million VinT-Sim and 0.1 million VinT-Real entries, collected via simulations in Mujoco and Blender and a custom-designed real-world platform. This dataset is tailored for robotic hands, offering models with whole-hand tactile perception and high-quality, well-aligned data. To the best of our knowledge, the VinT-Real is the largest considering the collection difficulties in the real-world environment so it can bridge the gap of simulation to real compared to the previous works. Built upon VinT-6D, we present a benchmark method that shows significant improvements in performance by fusing multi-modal information. The project is available at https://VinT-6D.github.io/.},
	language = {en},
	urldate = {2024-08-23},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wan, Zhaoliang and Ling, Yonggen and Yi, Senlin and Qi, Lu and Lee, Wang Wei and Lu, Minglei and Yang, Sicheng and Teng, Xiao and Lu, Peng and Yang, Xu and Yang, Ming-Hsuan and Cheng, Hui},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {49921--49940},
}

@inproceedings{downs_google_2022,
	title = {Google {Scanned} {Objects}: {A} {High}-{Quality} {Dataset} of {3D} {Scanned} {Household} {Items}},
	shorttitle = {Google {Scanned} {Objects}},
	url = {https://ieeexplore.ieee.org/abstract/document/9811809},
	doi = {10.1109/ICRA46639.2022.9811809},
	abstract = {Interactive 3D simulations have enabled break-throughs in robotics and computer vision, but simulating the broad diversity of environments needed for deep learning requires large corpora of photo-realistic 3D object models. To address this need, we present Google Scanned Objects, an open-source collection of over one thousand 3D-scanned household items released under a Creative Commons license; these models are preprocessed for use in Ignition Gazebo and the Bullet simulation platforms, but are easily adaptable to other simulators. We describe our object scanning and curation pipeline, then provide statistics about the contents of the dataset and its usage. We hope that the diversity, quality, and flexibility of Google Scanned Objects will lead to advances in interactive simulation, synthetic perception, and robotic learning.},
	urldate = {2024-08-22},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Downs, Laura and Francis, Anthony and Koenig, Nate and Kinman, Brandon and Hickman, Ryan and Reymann, Krista and McHugh, Thomas B. and Vanhoucke, Vincent},
	month = may,
	year = {2022},
	keywords = {Adaptation models, Computational modeling, Data Sets for Robot Learning, Data Sets for Robotic Vision, Data models, Ignition, Pipelines, Simulation and Animation, Solid modeling, Three-dimensional displays},
	pages = {2553--2560},
}

@inproceedings{deitke_objaverse_2023,
	title = {Objaverse: {A} {Universe} of {Annotated} {3D} {Objects}},
	shorttitle = {Objaverse},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-08-22},
	author = {Deitke, Matt and Schwenk, Dustin and Salvador, Jordi and Weihs, Luca and Michel, Oscar and VanderBilt, Eli and Schmidt, Ludwig and Ehsani, Kiana and Kembhavi, Aniruddha and Farhadi, Ali},
	year = {2023},
	pages = {13142--13153},
}

@inproceedings{wan_unidexgrasp_2023,
	title = {{UniDexGrasp}++: {Improving} {Dexterous} {Grasping} {Policy} {Learning} via {Geometry}-{Aware} {Curriculum} and {Iterative} {Generalist}-{Specialist} {Learning}},
	shorttitle = {{UniDexGrasp}++},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-08-17},
	author = {Wan, Weikang and Geng, Haoran and Liu, Yun and Shan, Zikang and Yang, Yaodong and Yi, Li and Wang, He},
	year = {2023},
	pages = {3891--3902},
}

@inproceedings{xu_unidexgrasp_2023,
	title = {{UniDexGrasp}: {Universal} {Robotic} {Dexterous} {Grasping} via {Learning} {Diverse} {Proposal} {Generation} and {Goal}-{Conditioned} {Policy}},
	shorttitle = {{UniDexGrasp}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xu_UniDexGrasp_Universal_Robotic_Dexterous_Grasping_via_Learning_Diverse_Proposal_Generation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-08-17},
	author = {Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and Liu, Tengyu and Yi, Li and Wang, He},
	year = {2023},
	pages = {4737--4746},
}

@article{odhner_stable_2015,
	title = {Stable, open-loop precision manipulation with underactuated hands},
	volume = {34},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364914558494},
	doi = {10.1177/0278364914558494},
	abstract = {This paper discusses dexterous, within-hand manipulation with differential-type underactuated hands. We discuss the fact that not only can this class of hands, which to date have been considered almost exclusively for adaptive grasping, be utilized for precision manipulation, but also that the reduction of the number of actuators and constraints can make within-hand manipulation easier to implement and control. Next, we introduce an analytical framework for evaluating the dexterous workspace of objects held within the fingertips in a precision grasp. A set of design principles for underactuated fingers are developed that enable fingertip grasping and manipulation. Finally, we apply this framework to analyze the workspace of stable object configurations for an object held within a pinch grasp of a two-fingered underactuated planar hand, demonstrating a large and useful workspace despite only one actuator per finger. The in-hand manipulation workspace for the iRobot–Harvard–Yale Hand is experimentally measured and presented.},
	language = {en},
	number = {11},
	urldate = {2024-08-16},
	journal = {The International Journal of Robotics Research},
	author = {Odhner, Lael U. and Dollar, Aaron M.},
	month = sep,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {1347--1360},
}

@article{bauza_simple_2024,
	title = {{SimPLE}, a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects},
	copyright = {Copyright © 2024 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
	url = {https://www.science.org/doi/10.1126/scirobotics.adi8808},
	doi = {10.1126/scirobotics.adi8808},
	abstract = {A robot precisely picks and places objects with hand-to-hand regrasps, visuotactile perception, and task-aware grasping.},
	language = {EN},
	urldate = {2024-08-13},
	journal = {Science Robotics},
	author = {Bauza, Maria and Bronars, Antonia and Hou, Yifan and Taylor, Ian and Chavan-Dafle, Nikhil and Rodriguez, Alberto},
	month = jun,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
}

@misc{brudigam_jacta_2024,
	title = {Jacta: {A} {Versatile} {Planner} for {Learning} {Dexterous} and {Whole}-body {Manipulation}},
	shorttitle = {Jacta},
	url = {http://arxiv.org/abs/2408.01258},
	doi = {10.48550/arXiv.2408.01258},
	abstract = {Robotic manipulation is challenging due to discontinuous dynamics, as well as high-dimensional state and action spaces. Data-driven approaches that succeed in manipulation tasks require large amounts of data and expert demonstrations, typically from humans. Existing manipulation planners are restricted to specific systems and often depend on specialized algorithms for using demonstration. Therefore, we introduce a flexible motion planner tailored to dexterous and whole-body manipulation tasks. Our planner creates readily usable demonstrations for reinforcement learning algorithms, eliminating the need for additional training pipeline complexities. With this approach, we can efficiently learn policies for complex manipulation tasks, where traditional reinforcement learning alone only makes little progress. Furthermore, we demonstrate that learned policies are transferable to real robotic systems for solving complex dexterous manipulation tasks.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Brüdigam, Jan and Abbas, Ali-Adeeb and Sorokin, Maks and Fang, Kuan and Hung, Brandon and Guru, Maya and Sosnowski, Stefan and Wang, Jiuguang and Hirche, Sandra and Cleac'h, Simon Le},
	month = aug,
	year = {2024},
	note = {arXiv:2408.01258 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhou_-hand_2024,
	title = {In-{Hand} {Singulation} and {Scooping} {Manipulation} with a 5 {DOF} {Tactile} {Gripper}},
	url = {http://arxiv.org/abs/2408.00610},
	doi = {10.48550/arXiv.2408.00610},
	abstract = {Manipulation tasks often require a high degree of dexterity, typically necessitating grippers with multiple degrees of freedom (DoF). While a robotic hand equipped with multiple fingers can execute precise and intricate manipulation tasks, the inherent redundancy stemming from its extensive DoF often adds unnecessary complexity. In this paper, we introduce the design of a tactile sensor-equipped gripper with two fingers and five DoF. We present a novel design integrating a GelSight tactile sensor, enhancing sensing capabilities and enabling finer control during specific manipulation tasks. To evaluate the gripper's performance, we conduct experiments involving two challenging tasks: 1) retrieving, singularizing, and classification of various objects embedded in granular media, and 2) executing scooping manipulations of credit cards in confined environments to achieve precise insertion. Our results demonstrate the efficiency of the proposed approach, with a high success rate for singulation and classification tasks, particularly for spherical objects at high as 94.3\%, and a 100\% success rate for scooping and inserting credit cards.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Zhou, Yuhao and Zhou, Pokuang and Wang, Shaoxiong and She, Yu},
	month = aug,
	year = {2024},
	note = {arXiv:2408.00610 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yarram_forecasting_2024,
	title = {Forecasting {Future} {Videos} from {Novel} {Views} via {Disentangled} {3D} {Scene} {Representation}},
	url = {http://arxiv.org/abs/2407.21450},
	doi = {10.48550/arXiv.2407.21450},
	abstract = {Video extrapolation in space and time (VEST) enables viewers to forecast a 3D scene into the future and view it from novel viewpoints. Recent methods propose to learn an entangled representation, aiming to model layered scene geometry, motion forecasting and novel view synthesis together, while assuming simplified affine motion and homography-based warping at each scene layer, leading to inaccurate video extrapolation. Instead of entangled scene representation and rendering, our approach chooses to disentangle scene geometry from scene motion, via lifting the 2D scene to 3D point clouds, which enables high quality rendering of future videos from novel views. To model future 3D scene motion, we propose a disentangled two-stage approach that initially forecasts ego-motion and subsequently the residual motion of dynamic objects (e.g., cars, people). This approach ensures more precise motion predictions by reducing inaccuracies from entanglement of ego-motion with dynamic object motion, where better ego-motion forecasting could significantly enhance the visual outcomes. Extensive experimental analysis on two urban scene datasets demonstrate superior performance of our proposed method in comparison to strong baselines.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Yarram, Sudhir and Yuan, Junsong},
	month = aug,
	year = {2024},
	note = {arXiv:2407.21450 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fukaya_four-axis_2024,
	title = {Four-{Axis} {Adaptive} {Fingers} {Hand} for {Object} {Insertion}: {FAAF} {Hand}},
	shorttitle = {Four-{Axis} {Adaptive} {Fingers} {Hand} for {Object} {Insertion}},
	url = {http://arxiv.org/abs/2407.21245},
	doi = {10.48550/arXiv.2407.21245},
	abstract = {Robots operating in the real world face significant but unavoidable issues in object localization that must be dealt with. A typical approach to address this is the addition of compliance mechanisms to hardware to absorb and compensate for some of these errors. However, for fine-grained manipulation tasks, the location and choice of appropriate compliance mechanisms are critical for success. For objects to be inserted in a target site on a flat surface, the object must first be successfully aligned with the opening of the slot, as well as correctly oriented along its central axis, before it can be inserted. We developed the Four-Axis Adaptive Finger Hand (FAAF hand) that is equipped with fingers that can passively adapt in four axes (x, y, z, yaw) enabling it to perform insertion tasks including lid fitting in the presence of significant localization errors. Furthermore, this adaptivity allows the use of simple control methods without requiring contact sensors or other devices. Our results confirm the ability of the FAAF hand on challenging insertion tasks of square and triangle-shaped pegs (or prisms) and placing of container lids in the presence of position errors in all directions and rotational error along the object's central axis, using a simple control scheme.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Fukaya, Naoki and Yamane, Koki and Masuda, Shimpei and Ummadisingu, Avinash and Maeda, Shin-ichi and Takahashi, Kuniyuki},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21245 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{puang_learning_2024,
	title = {Learning {Stable} {Robot} {Grasping} with {Transformer}-based {Tactile} {Control} {Policies}},
	url = {http://arxiv.org/abs/2407.21172},
	doi = {10.48550/arXiv.2407.21172},
	abstract = {Measuring grasp stability is an important skill for dexterous robot manipulation tasks, which can be inferred from haptic information with a tactile sensor. Control policies have to detect rotational displacement and slippage from tactile feedback, and determine a re-grasp strategy in term of location and force. Classic stable grasp task only trains control policies to solve for re-grasp location with objects of fixed center of gravity. In this work, we propose a revamped version of stable grasp task that optimises both re-grasp location and gripping force for objects with unknown and moving center of gravity. We tackle this task with a model-free, end-to-end Transformer-based reinforcement learning framework. We show that our approach is able to solve both objectives after training in both simulation and in a real-world setup with zero-shot transfer. We also provide performance analysis of different models to understand the dynamics of optimizing two opposing objectives.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Puang, En Yen and Li, Zechen and Chew, Chee Meng and Luo, Shan and Wu, Yan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21172 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhao_dynamic_2024,
	title = {Dynamic {Scene} {Understanding} through {Object}-{Centric} {Voxelization} and {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2407.20908},
	doi = {10.48550/arXiv.2407.20908},
	abstract = {Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Zhao, Yanpeng and Hao, Yiwei and Gao, Siyu and Wang, Yunbo and Yang, Xiaokang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20908 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huo_monocular_2024,
	title = {Monocular {Human}-{Object} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2407.20566},
	doi = {10.1145/3664647.3681452},
	abstract = {Learning the prior knowledge of the 3D human-object spatial relation is crucial for reconstructing human-object interaction from images and understanding how humans interact with objects in 3D space. Previous works learn this prior from datasets collected in controlled environments, but due to the diversity of domains, they struggle to generalize to real-world scenarios. To overcome this limitation, we present a 2D-supervised method that learns the 3D human-object spatial relation prior purely from 2D images in the wild. Our method utilizes a flow-based neural network to learn the prior distribution of the 2D human-object keypoint layout and viewports for each image in the dataset. The effectiveness of the prior learned from 2D images is demonstrated on the human-object reconstruction task by applying the prior to tune the relative pose between the human and the object during the post-optimization stage. To validate and benchmark our method on in-the-wild images, we collect the WildHOI dataset from the YouTube website, which consists of various interactions with 8 objects in real-world scenarios. We conduct the experiments on the indoor BEHAVE dataset and the outdoor WildHOI dataset. The results show that our method achieves almost comparable performance with fully 3D supervised methods on the BEHAVE dataset, even if we have only utilized the 2D layout information, and outperforms previous methods in terms of generality and interaction diversity on in-the-wild images.},
	urldate = {2024-08-13},
	author = {Huo, Chaofan and Shi, Ye and Wang, Jingya},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wang_solving_2024,
	title = {Solving {Robotics} {Problems} in {Zero}-{Shot} with {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2407.19094},
	doi = {10.48550/arXiv.2407.19094},
	abstract = {We introduce Wonderful Team, a multi-agent visual LLM (VLLM) framework for solving robotics problems in the zero-shot regime. By zero-shot we mean that, for a novel environment, we feed a VLLM an image of the robot's environment and a description of the task, and have the VLLM output the sequence of actions necessary for the robot to complete the task. Prior work on VLLMs in robotics has largely focused on settings where some part of the pipeline is fine-tuned, such as tuning an LLM on robot data or training a separate vision encoder for perception and action generation. Surprisingly, due to recent advances in the capabilities of VLLMs, this type of fine-tuning may no longer be necessary for many tasks. In this work, we show that with careful engineering, we can prompt a single off-the-shelf VLLM to handle all aspects of a robotics task, from high-level planning to low-level location-extraction and action-execution. Wonderful Team builds on recent advances in multi-agent LLMs to partition tasks across an agent hierarchy, making it self-corrective and able to effectively partition and solve even long-horizon tasks. Extensive experiments on VIMABench and real-world robotic environments demonstrate the system's capability to handle a variety of robotic tasks, including manipulation, visual goal-reaching, and visual reasoning, all in a zero-shot manner. These results underscore a key point: vision-language models have progressed rapidly in the past year, and should strongly be considered as a backbone for robotics problems going forward.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Wang, Zidan and Shen, Rui and Stadie, Bradly},
	month = jul,
	year = {2024},
	note = {arXiv:2407.19094 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{shang_theia_2024,
	title = {Theia: {Distilling} {Diverse} {Vision} {Foundation} {Models} for {Robot} {Learning}},
	shorttitle = {Theia},
	url = {http://arxiv.org/abs/2407.20179},
	doi = {10.48550/arXiv.2407.20179},
	abstract = {Vision-based robot policy learning, which maps visual inputs to actions, necessitates a holistic understanding of diverse visual tasks beyond single-task needs like classification or segmentation. Inspired by this, we introduce Theia, a vision foundation model for robot learning that distills multiple off-the-shelf vision foundation models trained on varied vision tasks. Theia's rich visual representations encode diverse visual knowledge, enhancing downstream robot learning. Extensive experiments demonstrate that Theia outperforms its teacher models and prior robot learning models using less training data and smaller model sizes. Additionally, we quantify the quality of pre-trained visual representations and hypothesize that higher entropy in feature norm distributions leads to improved robot learning performance. Code and models are available at https://github.com/bdaiinstitute/theia.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Shang, Jinghuan and Schmeckpeper, Karl and May, Brandon B. and Minniti, Maria Vittoria and Kelestemur, Tarik and Watkins, David and Herlant, Laura},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20179 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6\% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
	urldate = {2023-01-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	year = {2020},
}

@misc{wang_lessons_2024,
	title = {Lessons from {Learning} to {Spin} "{Pens}"},
	url = {http://arxiv.org/abs/2407.18902},
	doi = {10.48550/arXiv.2407.18902},
	abstract = {In-hand manipulation of pen-like objects is an important skill in our daily lives, as many tools such as hammers and screwdrivers are similarly shaped. However, current learning-based methods struggle with this task due to a lack of high-quality demonstrations and the significant gap between simulation and the real world. In this work, we push the boundaries of learning-based in-hand manipulation systems by demonstrating the capability to spin pen-like objects. We first use reinforcement learning to train an oracle policy with privileged information and generate a high-fidelity trajectory dataset in simulation. This serves two purposes: 1) pre-training a sensorimotor policy in simulation; 2) conducting open-loop trajectory replay in the real world. We then fine-tune the sensorimotor policy using these real-world trajectories to adapt it to the real world dynamics. With less than 50 trajectories, our policy learns to rotate more than ten pen-like objects with different physical properties for multiple revolutions. We present a comprehensive analysis of our design choices and share the lessons learned during development.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Wang, Jun and Yuan, Ying and Che, Haichuan and Qi, Haozhi and Ma, Yi and Malik, Jitendra and Wang, Xiaolong},
	month = jul,
	year = {2024},
	note = {arXiv:2407.18902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{srirama_hrp_2024,
	title = {{HRP}: {Human} {Affordances} for {Robotic} {Pre}-{Training}},
	shorttitle = {{HRP}},
	url = {http://arxiv.org/abs/2407.18911},
	doi = {10.48550/arXiv.2407.18911},
	abstract = {In order to *generalize* to various tasks in the wild, robotic agents will need a suitable representation (i.e., vision network) that enables the robot to predict optimal actions given high dimensional vision inputs. However, learning such a representation requires an extreme amount of diverse training data, which is prohibitively expensive to collect on a real robot. How can we overcome this problem? Instead of collecting more robot data, this paper proposes using internet-scale, human videos to extract "affordances," both at the environment and agent level, and distill them into a pre-trained representation. We present a simple framework for pre-training representations on hand, object, and contact "affordance labels" that highlight relevant objects in images and how to interact with them. These affordances are automatically extracted from human video data (with the help of off-the-shelf computer vision modules) and used to fine-tune existing representations. Our approach can efficiently fine-tune *any* existing representation, and results in models with stronger downstream robotic performance across the board. We experimentally demonstrate (using 3000+ robot trials) that this affordance pre-training scheme boosts performance by a minimum of 15\% on 5 real-world tasks, which consider three diverse robot morphologies (including a dexterous hand). Unlike prior works in the space, these representations improve performance across 3 different camera views. Quantitatively, we find that our approach leads to higher levels of generalization in out-of-distribution settings. For code, weights, and data check: https://hrp-robot.github.io},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Srirama, Mohan Kumar and Dasari, Sudeep and Bahl, Shikhar and Gupta, Abhinav},
	month = jul,
	year = {2024},
	note = {arXiv:2407.18911 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{pitz_learning_2024,
	title = {Learning a {Shape}-{Conditioned} {Agent} for {Purely} {Tactile} {In}-{Hand} {Manipulation} of {Various} {Objects}},
	url = {http://arxiv.org/abs/2407.18834},
	doi = {10.48550/arXiv.2407.18834},
	abstract = {Reorienting diverse objects with a multi-fingered hand is a challenging task. Current methods in robotic in-hand manipulation are either object-specific or require permanent supervision of the object state from visual sensors. This is far from human capabilities and from what is needed in real-world applications. In this work, we address this gap by training shape-conditioned agents to reorient diverse objects in hand, relying purely on tactile feedback (via torque and position measurements of the fingers' joints). To achieve this, we propose a learning framework that exploits shape information in a reinforcement learning policy and a learned state estimator. We find that representing 3D shapes by vectors from a fixed set of basis points to the shape's surface, transformed by its predicted 3D pose, is especially helpful for learning dexterous in-hand manipulation. In simulation and real-world experiments, we show the reorientation of many objects with high success rates, on par with state-of-the-art results obtained with specialized single-object agents. Moreover, we show generalization to novel objects, achieving success rates of \${\textbackslash}sim\$90\% even for non-convex shapes.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Pitz, Johannes and Röstel, Lennart and Sievers, Leon and Burschka, Darius and Bäuml, Berthold},
	month = jul,
	year = {2024},
	note = {arXiv:2407.18834 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{xie_sv4d_2024,
	title = {{SV4D}: {Dynamic} {3D} {Content} {Generation} with {Multi}-{Frame} and {Multi}-{View} {Consistency}},
	shorttitle = {{SV4D}},
	url = {http://arxiv.org/abs/2407.17470},
	doi = {10.48550/arXiv.2407.17470},
	abstract = {We present Stable Video 4D (SV4D), a latent video diffusion model for multi-frame and multi-view consistent dynamic 3D content generation. Unlike previous methods that rely on separately trained generative models for video generation and novel view synthesis, we design a unified diffusion model to generate novel view videos of dynamic 3D objects. Specifically, given a monocular reference video, SV4D generates novel views for each video frame that are temporally consistent. We then use the generated novel view videos to optimize an implicit 4D representation (dynamic NeRF) efficiently, without the need for cumbersome SDS-based optimization used in most prior works. To train our unified novel view video generation model, we curated a dynamic 3D object dataset from the existing Objaverse dataset. Extensive experimental results on multiple datasets and user studies demonstrate SV4D's state-of-the-art performance on novel-view video synthesis as well as 4D generation compared to prior works.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Xie, Yiming and Yao, Chun-Han and Voleti, Vikram and Jiang, Huaizu and Jampani, Varun},
	month = jul,
	year = {2024},
	note = {arXiv:2407.17470 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_frechet_2024,
	title = {Fr{\textbackslash}'echet {Video} {Motion} {Distance}: {A} {Metric} for {Evaluating} {Motion} {Consistency} in {Videos}},
	shorttitle = {Fr{\textbackslash}'echet {Video} {Motion} {Distance}},
	url = {http://arxiv.org/abs/2407.16124},
	doi = {10.48550/arXiv.2407.16124},
	abstract = {Significant advancements have been made in video generative models recently. Unlike image generation, video generation presents greater challenges, requiring not only generating high-quality frames but also ensuring temporal consistency across these frames. Despite the impressive progress, research on metrics for evaluating the quality of generated videos, especially concerning temporal and motion consistency, remains underexplored. To bridge this research gap, we propose Fr{\textbackslash}'echet Video Motion Distance (FVMD) metric, which focuses on evaluating motion consistency in video generation. Specifically, we design explicit motion features based on key point tracking, and then measure the similarity between these features via the Fr{\textbackslash}'echet distance. We conduct sensitivity analysis by injecting noise into real videos to verify the effectiveness of FVMD. Further, we carry out a large-scale human study, demonstrating that our metric effectively detects temporal noise and aligns better with human perceptions of generated video quality than existing metrics. Additionally, our motion features can consistently improve the performance of Video Quality Assessment (VQA) models, indicating that our approach is also applicable to unary video quality evaluation. Code is available at https://github.com/ljh0v0/FMD-frechet-motion-distance.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Liu, Jiahe and Qu, Youran and Yan, Qi and Zeng, Xiaohui and Wang, Lele and Liao, Renjie},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16124 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chen_motion_2024,
	title = {Motion {Capture} from {Inertial} and {Vision} {Sensors}},
	url = {http://arxiv.org/abs/2407.16341},
	doi = {10.48550/arXiv.2407.16341},
	abstract = {Human motion capture is the foundation for many computer vision and graphics tasks. While industrial motion capture systems with complex camera arrays or expensive wearable sensors have been widely adopted in movie and game production, consumer-affordable and easy-to-use solutions for personal applications are still far from mature. To utilize a mixture of a monocular camera and very few inertial measurement units (IMUs) for accurate multi-modal human motion capture in daily life, we contribute MINIONS in this paper, a large-scale Motion capture dataset collected from INertial and visION Sensors. MINIONS has several featured properties: 1) large scale of over five million frames and 400 minutes duration; 2) multi-modality data of IMUs signals and RGB videos labeled with joint positions, joint rotations, SMPL parameters, etc.; 3) a diverse set of 146 fine-grained single and interactive actions with textual descriptions. With the proposed MINIONS, we conduct experiments on multi-modal motion capture and explore the possibilities of consumer-affordable motion capture using a monocular camera and very few IMUs. The experiment results emphasize the unique advantages of inertial and vision sensors, showcasing the promise of consumer-affordable multi-modal motion capture and providing a valuable resource for further research and development.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Chen, Xiaodong and Liu, Wu and Bao, Qian and Liu, Xinchen and Yang, Quanwei and Dai, Ruoli and Mei, Tao},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16341 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yan_dreamdissector_2024,
	title = {{DreamDissector}: {Learning} {Disentangled} {Text}-to-{3D} {Generation} from {2D} {Diffusion} {Priors}},
	shorttitle = {{DreamDissector}},
	url = {http://arxiv.org/abs/2407.16260},
	doi = {10.48550/arXiv.2407.16260},
	abstract = {Text-to-3D generation has recently seen significant progress. To enhance its practicality in real-world applications, it is crucial to generate multiple independent objects with interactions, similar to layer-compositing in 2D image editing. However, existing text-to-3D methods struggle with this task, as they are designed to generate either non-independent objects or independent objects lacking spatially plausible interactions. Addressing this, we propose DreamDissector, a text-to-3D method capable of generating multiple independent objects with interactions. DreamDissector accepts a multi-object text-to-3D NeRF as input and produces independent textured meshes. To achieve this, we introduce the Neural Category Field (NeCF) for disentangling the input NeRF. Additionally, we present the Category Score Distillation Sampling (CSDS), facilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap issue in diffusion models. By leveraging NeCF and CSDS, we can effectively derive sub-NeRFs from the original scene. Further refinement enhances geometry and texture. Our experimental results validate the effectiveness of DreamDissector, providing users with novel means to control 3D synthesis at the object level and potentially opening avenues for various creative applications in the future.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Yan, Zizheng and Zhou, Jiapeng and Meng, Fanpeng and Wu, Yushuang and Qiu, Lingteng and Ye, Zisheng and Cui, Shuguang and Chen, Guanying and Han, Xiaoguang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16260 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{heo_furniturebench_2023,
	title = {{FurnitureBench}: {Reproducible} {Real}-{World} {Benchmark} for {Long}-{Horizon} {Complex} {Manipulation}},
	shorttitle = {{FurnitureBench}},
	url = {http://arxiv.org/abs/2305.12821},
	doi = {10.48550/arXiv.2305.12821},
	abstract = {Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robot manipulation task that requires addressing many current robotic manipulation challenges to solve. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL and IL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Heo, Minho and Lee, Youngwoon and Lee, Doohyun and Lim, Joseph J.},
	month = may,
	year = {2023},
	note = {arXiv:2305.12821 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ankile_imitation_2024,
	title = {From {Imitation} to {Refinement} -- {Residual} {RL} for {Precise} {Visual} {Assembly}},
	url = {http://arxiv.org/abs/2407.16677},
	doi = {10.48550/arXiv.2407.16677},
	abstract = {Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at {\textbackslash}url\{https://residual-assembly.github.io\}.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Ankile, Lars and Simeonov, Anthony and Shenfeld, Idan and Torne, Marcel and Agrawal, Pulkit},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16677 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lee_rgb2point_2024,
	title = {{RGB2Point}: {3D} {Point} {Cloud} {Generation} from {Single} {RGB} {Images}},
	shorttitle = {{RGB2Point}},
	url = {https://arxiv.org/abs/2407.14979v1},
	abstract = {We introduce RGB2Point, an unposed single-view RGB image to a 3D point cloud generation based on Transformer. RGB2Point takes an input image of an object and generates a dense 3D point cloud. Contrary to prior works based on CNN layers and diffusion denoising approaches, we use pre-trained Transformer layers that are fast and generate high-quality point clouds with consistent quality over available categories. Our generated point clouds demonstrate high quality on a real-world dataset, as evidenced by improved Chamfer distance (51.15\%) and Earth Mover's distance (45.96\%) metrics compared to the current state-of-the-art. Additionally, our approach shows a better quality on a synthetic dataset, achieving better Chamfer distance (39.26\%), Earth Mover's distance (26.95\%), and F-score (47.16\%). Moreover, our method produces 63.1\% more consistent high-quality results across various object categories compared to prior works. Furthermore, RGB2Point is computationally efficient, requiring only 2.3GB of VRAM to reconstruct a 3D point cloud from a single RGB image, and our implementation generates the results 15,133x faster than a SOTA diffusion-based model.},
	language = {en},
	urldate = {2024-08-12},
	author = {Lee, Jae Joong and Benes, Bedrich},
	month = jul,
	year = {2024},
}

@misc{xu_flow_2024,
	title = {Flow as the {Cross}-{Domain} {Manipulation} {Interface}},
	url = {https://arxiv.org/abs/2407.15208v1},
	abstract = {We present Im2Flow2Act, a scalable learning framework that enables robots to acquire manipulation skills from diverse data sources. The key idea behind Im2Flow2Act is to use object flow as the manipulation interface, bridging domain gaps between different embodiments (i.e., human and robot) and training environments (i.e., real-world and simulated). Im2Flow2Act comprises two components: a flow generation network and a flow-conditioned policy. The flow generation network, trained on human demonstration videos, generates object flow from the initial scene image, conditioned on the task description. The flow-conditioned policy, trained on simulated robot play data, maps the generated object flow to robot actions to realize the desired object movements. By using flow as input, this policy can be directly deployed in the real world with a minimal sim-to-real gap. By leveraging real-world human videos and simulated robot play data, we bypass the challenges of teleoperating physical robots in the real world, resulting in a scalable system for diverse tasks. We demonstrate Im2Flow2Act's capabilities in a variety of real-world tasks, including the manipulation of rigid, articulated, and deformable objects.},
	language = {en},
	urldate = {2024-08-12},
	author = {Xu, Mengda and Xu, Zhenjia and Xu, Yinghao and Chi, Cheng and Wetzstein, Gordon and Veloso, Manuela and Song, Shuran},
	month = jul,
	year = {2024},
}

@misc{zhou_fast_2024,
	title = {Fast {Learning} of {Signed} {Distance} {Functions} from {Noisy} {Point} {Clouds} via {Noise} to {Noise} {Mapping}},
	url = {https://arxiv.org/abs/2407.14225v1},
	abstract = {Learning signed distance functions (SDFs) from point clouds is an important task in 3D computer vision. However, without ground truth signed distances, point normals or clean point clouds, current methods still struggle from learning SDFs from noisy point clouds. To overcome this challenge, we propose to learn SDFs via a noise to noise mapping, which does not require any clean point cloud or ground truth supervision. Our novelty lies in the noise to noise mapping which can infer a highly accurate SDF of a single object or scene from its multiple or even single noisy observations. We achieve this by a novel loss which enables statistical reasoning on point clouds and maintains geometric consistency although point clouds are irregular, unordered and have no point correspondence among noisy observations. To accelerate training, we use multi-resolution hash encodings implemented in CUDA in our framework, which reduces our training time by a factor of ten, achieving convergence within one minute. We further introduce a novel schema to improve multi-view reconstruction by estimating SDFs as a prior. Our evaluations under widely-used benchmarks demonstrate our superiority over the state-of-the-art methods in surface reconstruction from point clouds or multi-view images, point cloud denoising and upsampling.},
	language = {en},
	urldate = {2024-08-12},
	author = {Zhou, Junsheng and Ma, Baorui and Liu, Yu-Shen and Han, Zhizhong},
	month = jul,
	year = {2024},
}

@misc{zhao_decomposed_2024,
	title = {Decomposed {Vector}-{Quantized} {Variational} {Autoencoder} for {Human} {Grasp} {Generation}},
	url = {https://arxiv.org/abs/2407.14062v1},
	abstract = {Generating realistic human grasps is a crucial yet challenging task for applications involving object manipulation in computer graphics and robotics. Existing methods often struggle with generating fine-grained realistic human grasps that ensure all fingers effectively interact with objects, as they focus on encoding hand with the whole representation and then estimating both hand posture and position in a single step. In this paper, we propose a novel Decomposed Vector-Quantized Variational Autoencoder (DVQ-VAE) to address this limitation by decomposing hand into several distinct parts and encoding them separately. This part-aware decomposed architecture facilitates more precise management of the interaction between each component of hand and object, enhancing the overall reality of generated human grasps. Furthermore, we design a newly dual-stage decoding strategy, by first determining the type of grasping under skeletal physical constraints, and then identifying the location of the grasp, which can greatly improve the verisimilitude as well as adaptability of the model to unseen hand-object interaction. In experiments, our model achieved about 14.1\% relative improvement in the quality index compared to the state-of-the-art methods in four widely-adopted benchmarks. Our source code is available at https://github.com/florasion/D-VQVAE.},
	language = {en},
	urldate = {2024-08-12},
	author = {Zhao, Zhe and Qi, Mengshi and Ma, Huadong},
	month = jul,
	year = {2024},
}

@article{gao_-hand_2023,
	title = {In-{Hand} {Pose} {Estimation} {Using} {Hand}-{Mounted} {RGB} {Cameras} and {Visuotactile} {Sensors}},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10043666},
	doi = {10.1109/ACCESS.2023.3244552},
	abstract = {This paper proposes a method to estimate the 6D pose of an object grasped by a robot hand using RGB cameras mounted on the palm and visuotactile sensors installed at the fingertips. It can deal with objects made from a wide range of materials thanks to combining the two types of sensors. The method allows a robot to robot to perform in-hand pose estimation while holding the object, eliminating the need for preparatory actions or particular environmental backgrounds. The mechanism at the back of the method includes deep-learning-based background subtraction and denoising auto-encoder-based sensor fusion. With the poses estimated using the proposed method, a robot controller can rectify the grasping uncertainty and adjust the robot motion to move an object toward required goals with satisfying accuracy. We conduct various studies and analyses in the experimental Section to understand the proposed method’s advantages and disadvantages. The results demonstrate the benefits of the proposed combination and mechanism. They also provide essential knowledge to readers considering using a similar configuration for estimating object poses.},
	urldate = {2024-08-12},
	journal = {IEEE Access},
	author = {Gao, Yuan and Matsuoka, Shogo and Wan, Weiwei and Kiyokawa, Takuya and Koyama, Keisuke and Harada, Kensuke},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Cameras, In-hand pose estimation, Manipulators, Pose estimation, Robot vision systems, Robots, Sensors, Tactile sensors, Visualization, robotic manipulation, visuotactile sensors},
	pages = {17218--17232},
}

@inproceedings{nguyen_nope_2024,
	title = {{NOPE}: {Novel} {Object} {Pose} {Estimation} from a {Single} {Image}},
	shorttitle = {{NOPE}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Nguyen_NOPE_Novel_Object_Pose_Estimation_from_a_Single_Image_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-08-12},
	author = {Nguyen, Van Nguyen and Groueix, Thibault and Ponimatkin, Georgy and Hu, Yinlin and Marlet, Renaud and Salzmann, Mathieu and Lepetit, Vincent},
	year = {2024},
	pages = {17923--17932},
}

@inproceedings{lin_sam-6d_2024,
	title = {{SAM}-{6D}: {Segment} {Anything} {Model} {Meets} {Zero}-{Shot} {6D} {Object} {Pose} {Estimation}},
	shorttitle = {{SAM}-{6D}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Lin_SAM-6D_Segment_Anything_Model_Meets_Zero-Shot_6D_Object_Pose_Estimation_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-08-12},
	author = {Lin, Jiehong and Liu, Lihua and Lu, Dekun and Jia, Kui},
	year = {2024},
	pages = {27906--27916},
}

@article{liu_enhancing_2024,
	title = {Enhancing {Generalizable} {6D} {Pose} {Tracking} of an {In}-{Hand} {Object} with {Tactile} {Sensing}},
	volume = {9},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2210.04026},
	doi = {10.1109/LRA.2023.3337690},
	abstract = {When manipulating an object to accomplish complex tasks, humans rely on both vision and touch to keep track of the object's 6D pose. However, most existing object pose tracking systems in robotics rely exclusively on visual signals, which hinder a robot's ability to manipulate objects effectively. To address this limitation, we introduce TEG-Track, a tactile-enhanced 6D pose tracking system that can track previously unseen objects held in hand. From consecutive tactile signals, TEG-Track optimizes object velocities from marker flows when slippage does not occur, or regresses velocities using a slippage estimation network when slippage is detected. The estimated object velocities are integrated into a geometric-kinematic optimization scheme to enhance existing visual pose trackers. To evaluate our method and to facilitate future research, we construct a real-world dataset for visual-tactile in-hand object pose tracking. Experimental results demonstrate that TEG-Track consistently enhances state-of-the-art generalizable 6D pose trackers in synthetic and real-world scenarios. Our code and dataset are available at https://github.com/leolyliu/TEG-Track.},
	number = {2},
	urldate = {2024-07-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Liu, Yun and Xu, Xiaomeng and Chen, Weihang and Yuan, Haocheng and Wang, He and Xu, Jing and Chen, Rui and Yi, Li},
	month = feb,
	year = {2024},
	note = {arXiv:2210.04026 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Force and tactile sensing, Kinematics, Robots, Sensors, Tactile sensors, Three-dimensional displays, Tracking, Visualization, sensor fusion, visual tracking},
	pages = {1106--1113},
}

@inproceedings{finn_deep_2016,
	title = {Deep spatial autoencoders for visuomotor learning},
	url = {https://ieeexplore.ieee.org/abstract/document/7487173?casa_token=WJujXWzR46sAAAAA:IwhInWZtY1kUd_W13KEH4mE2k3NrsVkbsR_bgB2iwQ9irfUfkfx9hRQbya5oPBkBUd-ohl9s-Yw},
	doi = {10.1109/ICRA.2016.7487173},
	abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.},
	urldate = {2024-08-08},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
	month = may,
	year = {2016},
	keywords = {Cameras, Learning (artificial intelligence), Robot kinematics, Robot sensing systems, Unsupervised learning, Visualization},
	pages = {512--519},
}

@misc{pinto_asymmetric_2017,
	title = {Asymmetric {Actor} {Critic} for {Image}-{Based} {Robot} {Learning}},
	url = {http://arxiv.org/abs/1710.06542},
	doi = {10.48550/arXiv.1710.06542},
	abstract = {Deep reinforcement learning (RL) has proven a powerful technique in many sequential decision making domains. However, Robotics poses many challenges for RL, most notably training on a physical system can be expensive and dangerous, which has sparked significant interest in learning control policies using a physics simulator. While several recent works have shown promising results in transferring policies trained in simulation to the real world, they often do not fully utilize the advantage of working with a simulator. In this work, we exploit the full state observability in the simulator to train better policies which take as input only partial observations (RGBD images). We do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor (or policy) gets rendered images as input. We show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance. Finally, we combine this method with domain randomization and show real robot experiments for several tasks like picking, pushing, and moving a block. We achieve this simulation to real world transfer without training on any real world data.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
	month = oct,
	year = {2017},
	note = {arXiv:1710.06542 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{bauza_tactile_2019,
	title = {Tactile {Mapping} and {Localization} from {High}-{Resolution} {Tactile} {Imprints}},
	url = {https://ieeexplore.ieee.org/document/8794298},
	doi = {10.1109/ICRA.2019.8794298},
	abstract = {This work studies the problem of shape reconstruction and object localization using a vision-based tactile sensor, GelSlim. The main contributions are the recovery of local shapes from contact, an approach to reconstruct the tactile shape of objects from tactile imprints, and an accurate method for object localization of previously reconstructed objects. The algorithms can be applied to a large variety of 3D objects and provide accurate tactile feedback for in-hand manipulation. Results show that by exploiting the dense tactile information we can reconstruct the shape of objects with high accuracy and do on-line object identification and localization, opening the door to reactive manipulation guided by tactile sensing. We provide videos and supplemental information in the project's website web.mit.edu/mcube/research/tactile localization.html.},
	urldate = {2024-08-06},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bauza, Maria and Canal, Oleguer and Rodriguez, Alberto},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Estimation, Image reconstruction, Shape, Tactile sensors, Three-dimensional displays},
	pages = {3811--3817},
}

@inproceedings{yin_rotating_2023,
	title = {Rotating without {Seeing}: {Towards} {In}-hand {Dexterity} through {Touch}},
	volume = {19},
	isbn = {978-0-9923747-9-2},
	shorttitle = {Rotating without {Seeing}},
	url = {https://www.roboticsproceedings.org/rss19/p036.html},
	urldate = {2024-08-06},
	author = {Yin, Zhao-Heng and Huang, Binghao and Qin, Yuzhe and Chen, Qifeng and Wang, Xiaolong},
	month = jul,
	year = {2023},
}

@inproceedings{tu_posefusion_2023,
	title = {{PoseFusion}: {Robust} {Object}-in-{Hand} {Pose} {Estimation} with {SelectLSTM}},
	shorttitle = {{PoseFusion}},
	url = {https://ieeexplore.ieee.org/document/10341688},
	doi = {10.1109/IROS55552.2023.10341688},
	abstract = {Accurate estimation of the relative pose between an object and a robot hand is critical for many manipulation tasks. However, most of the existing object-in-hand pose datasets use two-finger grippers and also assume that the object remains fixed in the hand without any relative movements, which is not representative of real-world scenarios. To address this issue, a 6D object-in-hand pose dataset is proposed using a teleoperation method with an anthropomorphic Shadow Dexterous hand. Our dataset comprises RGB-D images, proprioception and tactile data, covering diverse grasping poses, finger contact states, and object occlusions. To overcome the significant hand occlusion and limited tactile sensor contact in real-world scenarios, we propose PoseFusion, a hybrid multi-modal fusion approach that integrates the information from visual and tactile perception channels. PoseFusion generates three candidate object poses from three estimators (tactile only, visual only, and visuo-tactile fusion), which are then filtered by a SelectLSTM network to select the optimal pose, avoiding inferior fusion poses resulting from modality collapse. Extensive experiments demonstrate the robustness and advantages of our framework. All data and codes are available on the project website: https://elevenjiang1.github.io/ObjectlnHand-Dataset/.},
	urldate = {2024-08-06},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tu, Yuyang and Jiang, Junnan and Li, Shuang and Hendrich, Norman and Li, Miao and Zhang, Jianwei},
	month = oct,
	year = {2023},
	note = {ISSN: 2153-0866},
	keywords = {Grasping, Information filters, Pose estimation, Robustness, Tactile sensors, Training, Visualization},
	pages = {6839--6846},
}

@inproceedings{labbe_megapose_2022,
	title = {{MegaPose}: {6D} {Pose} {Estimation} of {Novel} {Objects} via {Render} \& {Compare}},
	shorttitle = {{MegaPose}},
	url = {https://openreview.net/forum?id=1zbWQxFIU-},
	abstract = {We introduce MegaPose, a method to estimate the 6D pose of novel objects, that is, objects unseen during training. At inference time, the method only assumes knowledge of (i) a region of interest displaying the object in the image and (ii) a CAD model of the observed object. The contributions of this work are threefold. First, we present a 6D pose refiner based on a render\&compare strategy which can be applied to novel objects. The shape and coordinate system of the novel object are provided as inputs to the network by rendering multiple synthetic views of the object's CAD model. Second, we introduce a novel approach for coarse pose estimation which leverages a network trained to classify whether the pose error between a synthetic rendering and an observed image of the same object can be corrected by the refiner. Third, we introduce a large-scale synthetic dataset of photorealistic images of thousands of objects with diverse visual and shape properties and show that this diversity is crucial to obtain good generalization performance on novel objects. We train our approach on this large synthetic dataset and apply it without retraining to hundreds of novel objects in real images from several pose estimation benchmarks. Our approach achieves state-of-the-art performance on the ModelNet and YCB-Video datasets. An extensive evaluation on the 7 core datasets of the BOP challenge demonstrates that our approach achieves performance competitive with existing approaches that require access to the target objects during training. Code, dataset and trained models are available on the project page: https://megapose6d.github.io/.},
	language = {en},
	urldate = {2024-08-06},
	author = {Labbé, Yann and Manuelli, Lucas and Mousavian, Arsalan and Tyree, Stephen and Birchfield, Stan and Tremblay, Jonathan and Carpentier, Justin and Aubry, Mathieu and Fox, Dieter and Sivic, Josef},
	month = aug,
	year = {2022},
}

@inproceedings{wen_foundationpose_2024,
	title = {{FoundationPose}: {Unified} {6D} {Pose} {Estimation} and {Tracking} of {Novel} {Objects}},
	shorttitle = {{FoundationPose}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.html},
	language = {en},
	urldate = {2024-08-06},
	author = {Wen, Bowen and Yang, Wei and Kautz, Jan and Birchfield, Stan},
	year = {2024},
	pages = {17868--17879},
}

@inproceedings{xiang_posecnn_2018,
	title = {{PoseCNN}: {A} {Convolutional} {Neural} {Network} for {6D} {Object} {Pose} {Estimation} in {Cluttered} {Scenes}},
	volume = {14},
	isbn = {978-0-9923747-4-7},
	shorttitle = {{PoseCNN}},
	url = {https://www.roboticsproceedings.org/rss14/p19.html},
	urldate = {2024-08-06},
	author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
	month = jun,
	year = {2018},
}

@inproceedings{handa_dextreme_2023,
	title = {{DeXtreme}: {Transfer} of {Agile} {In}-hand {Manipulation} from {Simulation} to {Reality}},
	shorttitle = {{DeXtreme}},
	url = {https://ieeexplore.ieee.org/document/10160216},
	doi = {10.1109/ICRA48891.2023.10160216},
	abstract = {Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website.},
	urldate = {2024-08-06},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Handa, Ankur and Allshire, Arthur and Makoviychuk, Viktor and Petrenko, Aleksei and Singh, Ritvik and Liu, Jingzhou and Makoviichuk, Denys and Van Wyk, Karl and Zhurkevich, Alexander and Sundaralingam, Balakumar and Narang, Yashraj},
	month = may,
	year = {2023},
	keywords = {Heart, Industries, Motion capture, Real-time systems, Reinforcement learning, Robot vision systems, Service robots},
	pages = {5977--5984},
}

@misc{papagiannis_rx_2024,
	title = {R+{X}: {Retrieval} and {Execution} from {Everyday} {Human} {Videos}},
	shorttitle = {R+{X}},
	url = {http://arxiv.org/abs/2407.12957},
	doi = {10.48550/arXiv.2407.12957},
	abstract = {We present R+X, a framework which enables robots to learn skills from long, unlabelled, first-person videos of humans performing everyday tasks. Given a language command from a human, R+X first retrieves short video clips containing relevant behaviour, and then executes the skill by conditioning an in-context imitation learning method on this behaviour. By leveraging a Vision Language Model (VLM) for retrieval, R+X does not require any manual annotation of the videos, and by leveraging in-context learning for execution, robots can perform commanded skills immediately, without requiring a period of training on the retrieved videos. Experiments studying a range of everyday household tasks show that R+X succeeds at translating unlabelled human videos into robust robot skills, and that R+X outperforms several recent alternative methods. Videos are available at https://www.robot-learning.uk/r-plus-x.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Papagiannis, Georgios and Di Palo, Norman and Vitiello, Pietro and Johns, Edward},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12957 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_shape_2024,
	title = {Shape of {Motion}: {4D} {Reconstruction} from a {Single} {Video}},
	shorttitle = {Shape of {Motion}},
	url = {http://arxiv.org/abs/2407.13764},
	doi = {10.48550/arXiv.2407.13764},
	abstract = {Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes. Project Page: https://shape-of-motion.github.io/},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Wang, Qianqian and Ye, Vickie and Gao, Hang and Austin, Jake and Li, Zhengqi and Kanazawa, Angjoo},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13764 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{von_hartz_art_2024,
	title = {The {Art} of {Imitation}: {Learning} {Long}-{Horizon} {Manipulation} {Tasks} from {Few} {Demonstrations}},
	shorttitle = {The {Art} of {Imitation}},
	url = {http://arxiv.org/abs/2407.13432},
	doi = {10.48550/arXiv.2407.13432},
	abstract = {Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient method for learning object-centric robot manipulation tasks. However, there are several open challenges to applying TP-GMMs in the wild. In this work, we tackle three crucial challenges synergistically. First, end-effector velocities are non-Euclidean and thus hard to model using standard GMMs. We thus propose to factorize the robot's end-effector velocity into its direction and magnitude, and model them using Riemannian GMMs. Second, we leverage the factorized velocities to segment and sequence skills from complex demonstration trajectories. Through the segmentation, we further align skill trajectories and hence leverage time as a powerful inductive bias. Third, we present a method to automatically detect relevant task parameters per skill from visual observations. Our approach enables learning complex manipulation tasks from just five demonstrations while using only RGB-D observations. Extensive experimental evaluations on RLBench demonstrate that our approach achieves state-of-the-art performance with 20-fold improved sample efficiency. Our policies generalize across different environments, object instances, and object positions, while the learned skills are reusable.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {von Hartz, Jan Ole and Welschehold, Tim and Valada, Abhinav and Boedecker, Joschka},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13432 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{su_vision_2024,
	title = {From {Vision} to {Audio} and {Beyond}: {A} {Unified} {Model} for {Audio}-{Visual} {Representation} and {Generation}},
	shorttitle = {From {Vision} to {Audio} and {Beyond}},
	url = {https://openreview.net/forum?id=jU6iPouOZ6&referrer=%5Bthe%20profile%20of%20Xiulong%20Liu%5D(%2Fprofile%3Fid%3D~Xiulong_Liu1)},
	abstract = {Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.},
	language = {en},
	urldate = {2024-08-05},
	author = {Su, Kun and Liu, Xiulong and Shlizerman, Eli},
	month = jun,
	year = {2024},
}

@misc{zhang_nl2contact_2024,
	title = {{NL2Contact}: {Natural} {Language} {Guided} {3D} {Hand}-{Object} {Contact} {Modeling} with {Diffusion} {Model}},
	shorttitle = {{NL2Contact}},
	url = {http://arxiv.org/abs/2407.12727},
	doi = {10.48550/arXiv.2407.12727},
	abstract = {Modeling the physical contacts between the hand and object is standard for refining inaccurate hand poses and generating novel human grasp in 3D hand-object reconstruction. However, existing methods rely on geometric constraints that cannot be specified or controlled. This paper introduces a novel task of controllable 3D hand-object contact modeling with natural language descriptions. Challenges include i) the complexity of cross-modal modeling from language to contact, and ii) a lack of descriptive text for contact patterns. To address these issues, we propose NL2Contact, a model that generates controllable contacts by leveraging staged diffusion models. Given a language description of the hand and contact, NL2Contact generates realistic and faithful 3D hand-object contacts. To train the model, we build {\textbackslash}textit\{ContactDescribe\}, the first dataset with hand-centered contact descriptions. It contains multi-level and diverse descriptions generated by large language models based on carefully designed prompts (e.g., grasp action, grasp type, contact location, free finger status). We show applications of our model to grasp pose optimization and novel human grasp generation, both based on a textual contact description.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Zhang, Zhongqun and Wang, Hengfei and Yu, Ziwei and Cheng, Yihua and Yao, Angela and Chang, Hyung Jin},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12727 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lv_himo_2024,
	title = {{HIMO}: {A} {New} {Benchmark} for {Full}-{Body} {Human} {Interacting} with {Multiple} {Objects}},
	shorttitle = {{HIMO}},
	url = {http://arxiv.org/abs/2407.12371},
	doi = {10.48550/arXiv.2407.12371},
	abstract = {Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Lv, Xintao and Xu, Liang and Yan, Yichao and Jin, Xin and Xu, Congsheng and Wu, Shuwen and Liu, Yifan and Li, Lincheng and Bi, Mengxiao and Zeng, Wenjun and Yang, Xiaokang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{saito_apricot_2024,
	title = {{APriCoT}: {Action} {Primitives} based on {Contact}-state {Transition} for {In}-{Hand} {Tool} {Manipulation}},
	shorttitle = {{APriCoT}},
	url = {http://arxiv.org/abs/2407.11436},
	doi = {10.48550/arXiv.2407.11436},
	abstract = {In-hand tool manipulation is an operation that not only manipulates a tool within the hand (i.e., in-hand manipulation) but also achieves a grasp suitable for a task after the manipulation. This study aims to achieve an in-hand tool manipulation skill through deep reinforcement learning. The difficulty of learning the skill arises because this manipulation requires (A) exploring long-term contact-state changes to achieve the desired grasp and (B) highly-varied motions depending on the contact-state transition. (A) leads to a sparsity of a reward on a successful grasp, and (B) requires an RL agent to explore widely within the state-action space to learn highly-varied actions, leading to sample inefficiency. To address these issues, this study proposes Action Primitives based on Contact-state Transition (APriCoT). APriCoT decomposes the manipulation into short-term action primitives by describing the operation as a contact-state transition based on three action representations (detach, crossover, attach). In each action primitive, fingers are required to perform short-term and similar actions. By training a policy for each primitive, we can mitigate the issues from (A) and (B). This study focuses on a fundamental operation as an example of in-hand tool manipulation: rotating an elongated object grasped with a precision grasp by half a turn to achieve the initial grasp. Experimental results demonstrated that ours succeeded in both the rotation and the achievement of the desired grasp, unlike existing studies. Additionally, it was found that the policy was robust to changes in object shape.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Saito, Daichi and Kanehira, Atsushi and Sasabuchi, Kazuhiro and Wake, Naoki and Takamatsu, Jun and Koike, Hideki and Ikeuchi, Katsushi},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11436 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{luo_grasping_2024,
	title = {Grasping {Diverse} {Objects} with {Simulated} {Humanoids}},
	url = {https://arxiv.org/abs/2407.11385v1},
	abstract = {We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number ({\textgreater}1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse object and trajectories. For training, we do not need dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.},
	language = {en},
	urldate = {2024-08-05},
	author = {Luo, Zhengyi and Cao, Jinkun and Christen, Sammy and Winkler, Alexander and Kitani, Kris and Xu, Weipeng},
	month = jul,
	year = {2024},
}

@misc{zhao_vlmpc_2024,
	title = {{VLMPC}: {Vision}-{Language} {Model} {Predictive} {Control} for {Robotic} {Manipulation}},
	shorttitle = {{VLMPC}},
	url = {https://arxiv.org/abs/2407.09829v1},
	abstract = {Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. Code is available at{\textasciitilde}{\textbackslash}url\{https://github.com/PPjmchen/VLMPC\}.},
	language = {en},
	urldate = {2024-08-05},
	author = {Zhao, Wentao and Chen, Jiaming and Meng, Ziyu and Mao, Donghui and Song, Ran and Zhang, Wei},
	month = jul,
	year = {2024},
}

@misc{wu_self-supervised_2024,
	title = {Self-supervised {3D} {Point} {Cloud} {Completion} via {Multi}-view {Adversarial} {Learning}},
	url = {https://arxiv.org/abs/2407.09786v1},
	abstract = {In real-world scenarios, scanned point clouds are often incomplete due to occlusion issues. The task of self-supervised point cloud completion involves reconstructing missing regions of these incomplete objects without the supervision of complete ground truth. Current self-supervised methods either rely on multiple views of partial observations for supervision or overlook the intrinsic geometric similarity that can be identified and utilized from the given partial point clouds. In this paper, we propose MAL-SPC, a framework that effectively leverages both object-level and category-specific geometric similarities to complete missing structures. Our MAL-SPC does not require any 3D complete supervision and only necessitates a single partial point cloud for each object. Specifically, we first introduce a Pattern Retrieval Network to retrieve similar position and curvature patterns between the partial input and the predicted shape, then leverage these similarities to densify and refine the reconstructed results. Additionally, we render the reconstructed complete shape into multi-view depth maps and design an adversarial learning module to learn the geometry of the target shape from category-specific single-view depth images. To achieve anisotropic rendering, we design a density-aware radius estimation algorithm to improve the quality of the rendered images. Our MAL-SPC yields the best results compared to current state-of-the-art methods.We will make the source code publicly available at {\textbackslash}url\{\vphantom{\}}https://github.com/ltwu6/malspc},
	language = {en},
	urldate = {2024-08-05},
	author = {Wu, Lintai and Cheng, Xianjing and Hou, Junhui and Xu, Yong and Zeng, Huanqiang},
	month = jul,
	year = {2024},
}

@misc{ojaghi_curriculum_2024,
	title = {Curriculum {Is} {More} {Influential} {Than} {Haptic} {Information} {During} {Reinforcement} {Learning} of {Object} {Manipulation} {Against} {Gravity}},
	url = {http://arxiv.org/abs/2407.09986},
	doi = {10.48550/arXiv.2407.09986},
	abstract = {Learning to lift and rotate objects with the fingertips is necessary for autonomous in-hand dexterous manipulation. In our study, we explore the impact of various factors on successful learning strategies for this task. Specifically, we investigate the role of curriculum learning and haptic feedback in enabling the learning of dexterous manipulation. Using model-free Reinforcement Learning, we compare different curricula and two haptic information modalities (No-tactile vs. 3D-force sensing) for lifting and rotating a ball against gravity with a three-fingered simulated robotic hand with no visual input. Note that our best results were obtained when we used a novel curriculum-based learning rate scheduler, which adjusts the linearly-decaying learning rate when the reward is changed as it accelerates convergence to higher rewards. Our findings demonstrate that the choice of curriculum greatly biases the acquisition of different features of dexterous manipulation. Surprisingly, successful learning can be achieved even in the absence of tactile feedback, challenging conventional assumptions about the necessity of haptic information for dexterous manipulation tasks. We demonstrate the generalizability of our results to balls of different weights and sizes, underscoring the robustness of our learning approach. This work, therefore, emphasizes the importance of the choice curriculum and challenges long-held notions about the need for tactile information to autonomously learn in-hand dexterous manipulation.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Ojaghi, Pegah and Mir, Romina and Marjaninejad, Ali and Erwin, Andrew and Wehner, Michael and Valero-Cueva, Francisco J.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.09986 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lee_affordance-guided_2024,
	title = {Affordance-{Guided} {Reinforcement} {Learning} via {Visual} {Prompting}},
	url = {http://arxiv.org/abs/2407.10341},
	doi = {10.48550/arXiv.2407.10341},
	abstract = {Robots equipped with reinforcement learning (RL) have the potential to learn a wide range of skills solely from a reward signal. However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge. Existing learning-based approaches require significant data, such as demonstrations or examples of success and failure, to learn task-specific reward functions. Recently, there is also a growing adoption of large multi-modal foundation models for robotics. These models can perform visual reasoning in physical contexts and generate coarse robot motions for various manipulation tasks. Motivated by this range of capability, in this work, we propose and study rewards shaped by vision-language models (VLMs). State-of-the-art VLMs have demonstrated an impressive ability to reason about affordances through keypoints in zero-shot, and we leverage this to define dense rewards for robotic learning. On a real-world manipulation task specified by natural language description, we find that these rewards improve the sample efficiency of autonomous RL and enable successful completion of the task in 20K online finetuning steps. Additionally, we demonstrate the robustness of the approach to reductions in the number of in-domain demonstrations used for pretraining, reaching comparable performance in 35K online finetuning steps.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Lee, Olivia Y. and Xie, Annie and Fang, Kuan and Pertsch, Karl and Finn, Chelsea},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_dexgrasp-diffusion_2024,
	title = {{DexGrasp}-{Diffusion}: {Diffusion}-based {Unified} {Functional} {Grasp} {Synthesis} {Pipeline} for {Multi}-{Dexterous} {Robotic} {Hands}},
	shorttitle = {{DexGrasp}-{Diffusion}},
	url = {http://arxiv.org/abs/2407.09899},
	doi = {10.48550/arXiv.2407.09899},
	abstract = {The versatility and adaptability of human grasping catalyze advancing dexterous robotic manipulation. While significant strides have been made in dexterous grasp generation, current research endeavors pivot towards optimizing object manipulation while ensuring functional integrity, emphasizing the synthesis of functional grasps following desired affordance instructions. This paper addresses the challenge of synthesizing functional grasps tailored to diverse dexterous robotic hands by proposing DexGrasp-Diffusion, an end-to-end modularized diffusion-based pipeline. DexGrasp-Diffusion integrates MultiHandDiffuser, a novel unified data-driven diffusion model for multi-dexterous hands grasp estimation, with DexDiscriminator, which employs a Physics Discriminator and a Functional Discriminator with open-vocabulary setting to filter physically plausible functional grasps based on object affordances. The experimental evaluation conducted on the MultiDex dataset provides substantiating evidence supporting the superior performance of MultiHandDiffuser over the baseline model in terms of success rate, grasp diversity, and collision depth. Moreover, we demonstrate the capacity of DexGrasp-Diffusion to reliably generate functional grasps for household objects aligned with specific affordance instructions.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Zhang, Zhengshen and Zhou, Lei and Liu, Chenchen and Liu, Zhiyang and Yuan, Chengran and Guo, Sheng and Zhao, Ruiteng and Ang Jr., Marcelo H. and Tay, Francis EH},
	month = jul,
	year = {2024},
	note = {arXiv:2407.09899 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhi_3d_2024,
	title = {{3D} {Foundation} {Models} {Enable} {Simultaneous} {Geometry} and {Pose} {Estimation} of {Grasped} {Objects}},
	url = {http://arxiv.org/abs/2407.10331},
	doi = {10.48550/arXiv.2407.10331},
	abstract = {Humans have the remarkable ability to use held objects as tools to interact with their environment. For this to occur, humans internally estimate how hand movements affect the object's movement. We wish to endow robots with this capability. We contribute methodology to jointly estimate the geometry and pose of objects grasped by a robot, from RGB images captured by an external camera. Notably, our method transforms the estimated geometry into the robot's coordinate frame, while not requiring the extrinsic parameters of the external camera to be calibrated. Our approach leverages 3D foundation models, large models pre-trained on huge datasets for 3D vision tasks, to produce initial estimates of the in-hand object. These initial estimations do not have physically correct scales and are in the camera's frame. Then, we formulate, and efficiently solve, a coordinate-alignment problem to recover accurate scales, along with a transformation of the objects to the coordinate frame of the robot. Forward kinematics mappings can subsequently be defined from the manipulator's joint angles to specified points on the object. These mappings enable the estimation of points on the held object at arbitrary configurations, enabling robot motion to be designed with respect to coordinates on the grasped objects. We empirically evaluate our approach on a robot manipulator holding a diverse set of real-world objects.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Zhi, Weiming and Tang, Haozhan and Zhang, Tianyi and Johnson-Roberson, Matthew},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10331 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{ha_umi_2024,
	title = {{UMI} on {Legs}: {Making} {Manipulation} {Policies} {Mobile} with {Manipulation}-{Centric} {Whole}-body {Controllers}},
	shorttitle = {{UMI} on {Legs}},
	url = {http://arxiv.org/abs/2407.10353},
	doi = {10.48550/arXiv.2407.10353},
	abstract = {We introduce UMI-on-Legs, a new framework that combines real-world and simulation data for quadruped manipulation systems. We scale task-centric data collection in the real world using a hand-held gripper (UMI), providing a cheap way to demonstrate task-relevant manipulation skills without a robot. Simultaneously, we scale robot-centric data in simulation by training whole-body controller for task-tracking without task simulation setups. The interface between these two policies is end-effector trajectories in the task frame, inferred by the manipulation policy and passed to the whole-body controller for tracking. We evaluate UMI-on-Legs on prehensile, non-prehensile, and dynamic manipulation tasks, and report over 70\% success rate on all tasks. Lastly, we demonstrate the zero-shot cross-embodiment deployment of a pre-trained manipulation policy checkpoint from prior work, originally intended for a fixed-base robot arm, on our quadruped system. We believe this framework provides a scalable path towards learning expressive manipulation skills on dynamic robot embodiments. Please checkout our website for robot videos, code, and data: https://umi-on-legs.github.io},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Ha, Huy and Gao, Yihuai and Fu, Zipeng and Tan, Jie and Song, Shuran},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10353 [cs]},
	keywords = {Computer Science - Robotics, I.2.9},
}

@article{andrychowicz_learning_2020,
	title = {Learning dexterous in-hand manipulation},
	volume = {39},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364919887447},
	doi = {10.1177/0278364919887447},
	abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
	language = {en},
	number = {1},
	urldate = {2024-08-04},
	journal = {The International Journal of Robotics Research},
	author = {Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	month = jan,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {3--20},
}

@misc{openai_solving_2019,
	title = {Solving {Rubik}'s {Cube} with a {Robot} {Hand}},
	url = {http://arxiv.org/abs/1910.07113},
	doi = {10.48550/arXiv.1910.07113},
	abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
	urldate = {2024-08-04},
	publisher = {arXiv},
	author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
	month = oct,
	year = {2019},
	note = {arXiv:1910.07113 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{lambeta_digit_2020,
	title = {{DIGIT}: {A} {Novel} {Design} for a {Low}-{Cost} {Compact} {High}-{Resolution} {Tactile} {Sensor} {With} {Application} to {In}-{Hand} {Manipulation}},
	volume = {5},
	issn = {2377-3766},
	shorttitle = {{DIGIT}},
	url = {https://ieeexplore.ieee.org/document/9018215},
	doi = {10.1109/LRA.2020.2977257},
	abstract = {Despite decades of research, general purpose in-hand manipulation remains one of the unsolved challenges of robotics. One of the contributing factors that limit current robotic manipulation systems is the difficulty of precisely sensing contact forces - sensing and reasoning about contact forces are crucial to accurately control interactions with the environment. As a step towards enabling better robotic manipulation, we introduce DIGIT, an inexpensive, compact, and high-resolution tactile sensor geared towards in-hand manipulation. DIGIT improves upon past vision-based tactile sensors by miniaturizing the form factor to be mountable on multi-fingered hands, and by providing several design improvements that result in an easier, more repeatable manufacturing process, and enhanced reliability. We demonstrate the capabilities of the DIGIT sensor by training deep neural network model-based controllers to manipulate glass marbles in-hand with a multi-finger robotic hand. To provide the robotic community access to reliable and low-cost tactile sensors, we open-source the DIGIT design at www.digit.ml.},
	number = {3},
	urldate = {2024-08-04},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lambeta, Mike and Chou, Po-Wei and Tian, Stephen and Yang, Brian and Maloon, Benjamin and Most, Victoria Rose and Stroud, Dave and Santos, Raymond and Byagowi, Ahmad and Kammerer, Gregg and Jayaraman, Dinesh and Calandra, Roberto},
	month = jul,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Perception for grasping and manipulation, Reliability, Tactile sensors, Task analysis, deep learning in robotics and automation, force and tactile sensing, learning and adaptive systems},
	pages = {3838--3845},
}

@inproceedings{park_pix2pose_2019,
	title = {{Pix2Pose}: {Pixel}-{Wise} {Coordinate} {Regression} of {Objects} for {6D} {Pose} {Estimation}},
	shorttitle = {{Pix2Pose}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.html},
	urldate = {2022-10-13},
	author = {Park, Kiru and Patten, Timothy and Vincze, Markus},
	year = {2019},
	pages = {7668--7677},
}

@inproceedings{liu_gen6d_2022,
	address = {Cham},
	title = {{Gen6D}: {Generalizable} {Model}-{Free} 6-{DoF} {Object} {Pose} {Estimation} from {RGB} {Images}},
	isbn = {978-3-031-19824-3},
	shorttitle = {{Gen6D}},
	doi = {10.1007/978-3-031-19824-3_18},
	abstract = {In this paper, we present a generalizable model-free 6-DoF object pose estimator called Gen6D. Existing generalizable pose estimators either need the high-quality object models or require additional depth maps or object masks in test time, which significantly limits their application scope. In contrast, our pose estimator only requires some posed images of the unseen object and is able to accurately predict poses of the object in arbitrary environments. Gen6D consists of an object detector, a viewpoint selector and a pose refiner, all of which do not require the 3D object model and can generalize to unseen objects. Experiments show that Gen6D achieves state-of-the-art results on two model-free datasets: the MOPED dataset and a new GenMOP dataset. In addition, on the LINEMOD dataset, Gen6D achieves competitive results compared with instance-specific pose estimators. Project page: https://liuyuan-pal.github.io/Gen6D/.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Liu, Yuan and Wen, Yilin and Peng, Sida and Lin, Cheng and Long, Xiaoxiao and Komura, Taku and Wang, Wenping},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {6-Dof object pose estimation, Camera pose estimation},
	pages = {298--315},
}

@article{he_onepose_2022,
	title = {{OnePose}++: {Keypoint}-{Free} {One}-{Shot} {Object} {Pose} {Estimation} without {CAD} {Models}},
	volume = {35},
	shorttitle = {{OnePose}++},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e43f900f571de6c96a70d5724a0fb565-Abstract-Conference.html},
	language = {en},
	urldate = {2024-08-02},
	journal = {Advances in Neural Information Processing Systems},
	author = {He, Xingyi and Sun, Jiaming and Wang, Yuang and Huang, Di and Bao, Hujun and Zhou, Xiaowei},
	month = dec,
	year = {2022},
	pages = {35103--35115},
}

@inproceedings{lee_tta-cope_2023,
	title = {{TTA}-{COPE}: {Test}-{Time} {Adaptation} for {Category}-{Level} {Object} {Pose} {Estimation}},
	shorttitle = {{TTA}-{COPE}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Lee_TTA-COPE_Test-Time_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-08-02},
	author = {Lee, Taeyeop and Tremblay, Jonathan and Blukis, Valts and Wen, Bowen and Lee, Byeong-Uk and Shin, Inkyu and Birchfield, Stan and Kweon, In So and Yoon, Kuk-Jin},
	year = {2023},
	pages = {21285--21295},
}

@inproceedings{chi_diffusion_2023,
	title = {Diffusion {Policy}: {Visuomotor} {Policy} {Learning} via {Action} {Diffusion}},
	volume = {19},
	isbn = {978-0-9923747-9-2},
	shorttitle = {Diffusion {Policy}},
	url = {https://www.roboticsproceedings.org/rss19/p026.html},
	urldate = {2024-08-02},
	author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin CM and Song, Shuran},
	month = jul,
	year = {2023},
}

@inproceedings{schmidt_depth-based_2015,
	title = {Depth-based tracking with physical constraints for robot manipulation},
	url = {https://ieeexplore.ieee.org/abstract/document/7138989},
	doi = {10.1109/ICRA.2015.7138989},
	abstract = {This work integrates visual and physical constraints to perform real-time depth-only tracking of articulated objects, with a focus on tracking a robot's manipulators and manipulation targets in realistic scenarios. As such, we extend DART, an existing visual articulated object tracker, to additionally avoid interpenetration of multiple interacting objects, and to make use of contact information collected via torque sensors or touch sensors. To achieve greater stability, the tracker uses a switching model to detect when an object is stationary relative to the table or relative to the palm and then uses information from multiple frames to converge to an accurate and stable estimate. Deviation from stable states is detected in order to remain robust to failed grasps and dropped objects. The tracker is integrated into a shared autonomy system in which it provides state estimates used by a grasp planner and the controller of two anthropomorphic hands. We demonstrate the advantages and performance of the tracking system in simulation and on a real robot. Qualitative results are also provided for a number of challenging manipulations that are made possible by the speed, accuracy, and stability of the tracking system.},
	urldate = {2024-07-31},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Schmidt, Tanner and Hertkorn, Katharina and Newcombe, Richard and Marton, Zoltan and Suppa, Michael and Fox, Dieter},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {Calibration, Cameras, Joints, Robot sensing systems, Visualization},
	pages = {119--126},
}

@inproceedings{brahmbhatt_contactgrasp_2019,
	title = {{ContactGrasp}: {Functional} {Multi}-finger {Grasp} {Synthesis} from {Contact}},
	shorttitle = {{ContactGrasp}},
	url = {https://ieeexplore.ieee.org/document/8967960/},
	doi = {10.1109/IROS40897.2019.8967960},
	abstract = {Grasping and manipulating objects is an important human skill. Since most objects are designed to be manipulated by human hands, anthropomorphic hands can enable richer human-robot interaction. Desirable grasps are not only stable, but also functional: they enable post-grasp actions with the object. However, functional grasp synthesis for high degree-of-freedom anthropomorphic hands from object shape alone is challenging. We present ContactGrasp, a framework for functional grasp synthesis from object shape and contact on the object surface. Contact can be manually specified or obtained through demonstrations. Our contact representation is object-centric and allows functional grasp synthesis even for hand models different than the one used for demonstration. Using a dataset of contact demonstrations from humans grasping diverse household objects, we synthesize functional grasps for three hand models and two functional intents. The project webpage is https://contactdb.cc.gatech.edu/contactgrasp.html.},
	urldate = {2024-07-31},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Brahmbhatt, Samarth and Handa, Ankur and Hays, James and Fox, Dieter},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {2386--2393},
}

@misc{byravan_nerf2real_2022,
	title = {{NeRF2Real}: {Sim2real} {Transfer} of {Vision}-guided {Bipedal} {Motion} {Skills} using {Neural} {Radiance} {Fields}},
	shorttitle = {{NeRF2Real}},
	url = {http://arxiv.org/abs/2210.04932},
	doi = {10.48550/arXiv.2210.04932},
	abstract = {We present a system for applying sim2real approaches to "in the wild" scenes with realistic visuals, and to policies which rely on active perception using RGB cameras. Given a short video of a static scene collected using a generic phone, we learn the scene's contact geometry and a function for novel view synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering of the static scene by overlaying the rendering of other dynamic objects (e.g. the robot's own body, a ball). A simulation is then created using the rendering engine in a physics simulator which computes contact dynamics from the static scene geometry (estimated from the NeRF volume density) and the dynamic objects' geometry and physical properties (assumed known). We demonstrate that we can use this simulation to learn vision-based whole body navigation and ball pushing policies for a 20 degrees of freedom humanoid robot with an actuated head-mounted RGB camera, and we successfully transfer these policies to a real robot. Project video is available at https://sites.google.com/view/nerf2real/home},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Byravan, Arunkumar and Humplik, Jan and Hasenclever, Leonard and Brussee, Arthur and Nori, Francesco and Haarnoja, Tuomas and Moran, Ben and Bohez, Steven and Sadeghi, Fereshteh and Vujatovic, Bojan and Heess, Nicolas},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04932 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{shang_learning_2022,
	title = {Learning {Viewpoint}-{Agnostic} {Visual} {Representations} by {Recovering} {Tokens} in {3D} {Space}},
	url = {https://openreview.net/forum?id=YBsLfudKlBu},
	abstract = {Humans are remarkably flexible in understanding viewpoint changes due to visual cortex supporting the perception of 3D structure. In contrast, most of the computer vision models that learn visual representation from a pool of 2D images often fail to generalize over novel camera viewpoints. Recently, the vision architectures have shifted towards convolution-free architectures, visual Transformers, which operate on tokens derived from image patches. However, these Transformers do not perform explicit operations to learn viewpoint-agnostic representation for visual understanding. To this end, we propose a 3D Token Representation Layer (3DTRL) that estimates the 3D positional information of the visual tokens and leverages it for learning viewpoint-agnostic representations. The key elements of 3DTRL include a pseudo-depth estimator and a learned camera matrix to impose geometric transformations on the tokens, trained in an unsupervised fashion. These enable 3DTRL to recover the 3D positional information of the tokens from 2D patches. In practice, 3DTRL is easily plugged-in into a Transformer. Our experiments demonstrate the effectiveness of 3DTRL in many vision tasks including image classification, multi-view video alignment, and action recognition. The models with 3DTRL outperform their backbone Transformers in all the tasks with minimal added computation. Our code is available at https://github.com/elicassion/3DTRL.},
	language = {en},
	urldate = {2024-07-30},
	author = {Shang, Jinghuan and Das, Srijan and Ryoo, Michael S.},
	month = oct,
	year = {2022},
}

@inproceedings{chao_dexycb_2021,
	title = {{DexYCB}: {A} {Benchmark} for {Capturing} {Hand} {Grasping} of {Objects}},
	shorttitle = {{DexYCB}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-02-22},
	author = {Chao, Yu-Wei and Yang, Wei and Xiang, Yu and Molchanov, Pavlo and Handa, Ankur and Tremblay, Jonathan and Narang, Yashraj S. and Van Wyk, Karl and Iqbal, Umar and Birchfield, Stan and Kautz, Jan and Fox, Dieter},
	year = {2021},
	pages = {9044--9053},
}

@inproceedings{wang_cold_2023,
	title = {Cold {Diffusion} on the {Replay} {Buffer}: {Learning} to {Plan} from {Known} {Good} {States}},
	shorttitle = {Cold {Diffusion} on the {Replay} {Buffer}},
	url = {https://openreview.net/forum?id=AyRr_i028w},
	abstract = {Learning from demonstrations (LfD) has successfully trained robots to exhibit remarkable generalization capabilities. However, many powerful imitation techniques do not prioritize the feasibility of the robot behaviors they generate. In this work, we explore the feasibility of plans produced by LfD. As in prior work, we employ a temporal diffusion model with fixed start and goal states to facilitate imitation through in-painting. Unlike previous studies, we apply cold diffusion to ensure the optimization process is directed through the agent's replay buffer of previously visited states. This routing approach increases the likelihood that the final trajectories will predominantly occupy the feasible region of the robot's state space. We test this method in simulated robotic environments with obstacles and observe a significant improvement in the agent's ability to avoid these obstacles during planning.},
	language = {en},
	urldate = {2023-12-28},
	author = {Wang, Zidan and Oba, Takeru and Yoneda, Takuma and Shen, Rui and Walter, Matthew and Stadie, Bradly C.},
	month = aug,
	year = {2023},
}

@article{ravichandar_recent_2020,
	title = {Recent {Advances} in {Robot} {Learning} from {Demonstration}},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-control-100819-063206},
	doi = {10.1146/annurev-control-100819-063206},
	abstract = {In the context of robotics and automation, learning from demonstration (LfD) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of LfD over other robot learning methods is compelling when ideal behavior can be neither easily scripted (as is done in traditional robot programming) nor easily defined as an optimization problem, but can be demonstrated. While there have been multiple surveys of this field in the past, there is a need for a new one given the considerable growth in the number of publications in recent years. This review aims to provide an overview of the collection of machine-learning methods used to enable a robot to learn from and imitate a teacher. We focus on recent advancements in the field and present an updated taxonomy and characterization of existing methods. We also discuss mature and emerging application areas for LfD and highlight the significant challenges that remain to be overcome both in theory and in practice.},
	number = {1},
	urldate = {2023-02-01},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Ravichandar, Harish and Polydoros, Athanasios S. and Chernova, Sonia and Billard, Aude},
	year = {2020},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-100819-063206},
	keywords = {imitation learning, learning from demonstration, programming by demonstration, robot learning},
	pages = {297--330},
}

@misc{wen_you_2022,
	title = {You {Only} {Demonstrate} {Once}: {Category}-{Level} {Manipulation} from {Single} {Visual} {Demonstration}},
	shorttitle = {You {Only} {Demonstrate} {Once}},
	url = {http://arxiv.org/abs/2201.12716},
	doi = {10.48550/arXiv.2201.12716},
	abstract = {Promising results have been achieved recently in category-level manipulation that generalizes across object instances. Nevertheless, it often requires expensive real-world data collection and manual specification of semantic keypoints for each object category and task. Additionally, coarse keypoint predictions and ignoring intermediate action sequences hinder adoption in complex manipulation tasks beyond pick-and-place. This work proposes a novel, category-level manipulation framework that leverages an object-centric, category-level representation and model-free 6 DoF motion tracking. The canonical object representation is learned solely in simulation and then used to parse a category-level, task trajectory from a single demonstration video. The demonstration is reprojected to a target trajectory tailored to a novel object via the canonical representation. During execution, the manipulation horizon is decomposed into longrange, collision-free motion and last-inch manipulation. For the latter part, a category-level behavior cloning (CatBC) method leverages motion tracking to perform closed-loop control. CatBC follows the target trajectory, projected from the demonstration and anchored to a dynamically selected category-level coordinate frame. The frame is automatically selected along the manipulation horizon by a local attention mechanism. This framework allows to teach different manipulation strategies by solely providing a single demonstration, without complicated manual programming. Extensive experiments demonstrate its efficacy in a range of challenging industrial tasks in highprecision assembly, which involve learning complex, long-horizon policies. The process exhibits robustness against uncertainty due to dynamics as well as generalization across object instances and scene configurations. The supplementary video is available at https://www.youtube.com/watch?v=WAr8ZY3mYyw},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Wen, Bowen and Lian, Wenzhao and Bekris, Kostas and Schaal, Stefan},
	month = may,
	year = {2022},
	note = {arXiv:2201.12716 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{qin_dexmv_2022,
	title = {{DexMV}: {Imitation} {Learning} for {Dexterous} {Manipulation} from {Human} {Videos}},
	shorttitle = {{DexMV}},
	url = {http://arxiv.org/abs/2108.05877},
	doi = {10.48550/arXiv.2108.05877},
	abstract = {While significant progress has been made on understanding hand-object interactions in computer vision, it is still very challenging for robots to perform complex dexterous manipulation. In this paper, we propose a new platform and pipeline DexMV (Dexterous Manipulation from Videos) for imitation learning. We design a platform with: (i) a simulation system for complex dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer vision system to record large-scale demonstrations of a human hand conducting the same tasks. In our novel pipeline, we extract 3D hand and object poses from videos, and propose a novel demonstration translation method to convert human motion to robot demonstrations. We then apply and benchmark multiple imitation learning algorithms with the demonstrations. We show that the demonstrations can indeed improve robot learning by a large margin and solve the complex tasks which reinforcement learning alone cannot solve. More details can be found in the project page: https://yzqin.github.io/dexmv},
	urldate = {2022-07-29},
	publisher = {arXiv},
	author = {Qin, Yuzhe and Wu, Yueh-Hua and Liu, Shaowei and Jiang, Hanwen and Yang, Ruihan and Fu, Yang and Wang, Xiaolong},
	month = jul,
	year = {2022},
	note = {arXiv:2108.05877 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_manifoundation_2024,
	title = {{ManiFoundation} {Model} for {General}-{Purpose} {Robotic} {Manipulation} of {Contact} {Synthesis} with {Arbitrary} {Objects} and {Robots}},
	url = {http://arxiv.org/abs/2405.06964},
	doi = {10.48550/arXiv.2405.06964},
	abstract = {To substantially enhance robot intelligence, there is a pressing need to develop a large model that enables general-purpose robots to proficiently undertake a broad spectrum of manipulation tasks, akin to the versatile task-planning ability exhibited by LLMs. The vast diversity in objects, robots, and manipulation tasks presents huge challenges. Our work introduces a comprehensive framework to develop a foundation model for general robotic manipulation that formalizes a manipulation task as contact synthesis. Specifically, our model takes as input object and robot manipulator point clouds, object physical attributes, target motions, and manipulation region masks. It outputs contact points on the object and associated contact forces or post-contact motions for robots to achieve the desired manipulation task. We perform extensive experiments both in the simulation and real-world settings, manipulating articulated rigid objects, rigid objects, and deformable objects that vary in dimensionality, ranging from one-dimensional objects like ropes to two-dimensional objects like cloth and extending to three-dimensional objects such as plasticine. Our model achieves average success rates of around 90{\textbackslash}\%. Supplementary materials and videos are available on our project website at https://manifoundationmodel.github.io/.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Xu, Zhixuan and Gao, Chongkai and Liu, Zixuan and Yang, Gang and Tie, Chenrui and Zheng, Haozhuo and Zhou, Haoyu and Peng, Weikun and Wang, Debang and Chen, Tianyi and Yu, Zhouliang and Shao, Lin},
	month = may,
	year = {2024},
	note = {arXiv:2405.06964 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{madgwick_efficient_2010,
	title = {An efficient orientation filter for inertial and inertial / magnetic sensor arrays},
	url = {https://www.semanticscholar.org/paper/An-efficient-orientation-filter-for-inertial-and-Madgwick/bfb456caf5e71d426bd3e2fd529ee833a6c3b7e7},
	abstract = {,},
	urldate = {2024-06-28},
	author = {Madgwick, Sebastian O. H.},
	year = {2010},
}

@misc{liu_maniwav_2024,
	title = {{ManiWAV}: {Learning} {Robot} {Manipulation} from {In}-the-{Wild} {Audio}-{Visual} {Data}},
	shorttitle = {{ManiWAV}},
	url = {http://arxiv.org/abs/2406.19464},
	abstract = {Audio signals provide rich information for the robot interaction and object properties through contact. These information can surprisingly ease the learning of contact-rich robot manipulation skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio data in robot manipulation has been constrained to teleoperated demonstrations collected by either attaching a microphone to the robot or object, which significantly limits its usage in robot learning pipelines. In this work, we introduce ManiWAV: an 'ear-in-hand' data collection device to collect in-the-wild human demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn robot manipulation policy directly from the demonstrations. We demonstrate the capabilities of our system through four contact-rich manipulation tasks that require either passively sensing the contact events and modes, or actively sensing the object surface materials and states. In addition, we show that our system can generalize to unseen in-the-wild environments, by learning from diverse in-the-wild human demonstrations. Project website: https://mani-wav.github.io/},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Liu, Zeyi and Chi, Cheng and Cousineau, Eric and Kuppuswamy, Naveen and Burchfiel, Benjamin and Song, Shuran},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19464 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{yang_learning_2024,
	title = {Learning a {Contact} {Potential} {Field} for {Modeling} the {Hand}-{Object} {Interaction}},
	volume = {46},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/10478277},
	doi = {10.1109/TPAMI.2024.3372102},
	abstract = {Estimating and synthesizing the hand's manipulation of objects is central to understanding human behaviour. To accurately model the interaction between the hand and object (referred to as the “hand-object”), we must not only focus on the pose of the hand and object, but also consider the contact between them. This contact provides valuable information for generating semantically and physically plausible grasps. In this paper, we propose an explicit contact representation called Contact Potential Field (CPF). In CPF, we model the contact between a pair of hand-object vertices as a spring-mass system. This system encodes the distance of the pair, as well as a likelihood of that contact being stable. Therefore, the system of multiple extended and compressed springs forms an elastic potential field with minimal energy at the optimal grasp position. We apply CPF to two relevant tasks, namely, hand-object pose estimation and grasping pose generation. Extensive experiments on the two challenging tasks and three commonly used datasets have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, allowing us to produce more physically plausible hand-object poses even when the ground-truth exhibits severe interpenetration or disjointedness.},
	number = {8},
	urldate = {2024-07-29},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yang, Lixin and Zhan, Xinyu and Li, Kailin and Xu, Wenqiang and Zhang, Junming and Li, Jiefeng and Lu, Cewu},
	month = aug,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Contact modeling, Contacts, Grasping, Image reconstruction, Pose estimation, Semantics, Task analysis, Three-dimensional displays, grasping pose generation, hand-object pose estimation},
	pages = {5645--5662},
}

@article{thalhammer_challenges_2024,
	title = {Challenges for {Monocular} {6D} {Object} {Pose} {Estimation} in {Robotics}},
	issn = {1941-0468},
	url = {https://ieeexplore.ieee.org/abstract/document/10609560},
	doi = {10.1109/TRO.2024.3433870},
	abstract = {Object pose estimation is a core perception task that enables, for example, object manipulation and scene understanding. The widely available, inexpensive and high-resolution RGB sensors and CNNs that allow for fast inference make monocular approaches especially well suited for robotics applications. We observe that previous surveys on establish the state of the art for varying modalities, single- and multi-view settings, and datasets and metrics that consider a multitude of applications. We argue, however, that those works' broad scope hinders the identification of open challenges that are specific to monocular approaches and the derivation of promising future challenges for their application in robotics. By providing a unified view on recent publications from both robotics and computer vision, we find that occlusion handling, pose representations, and formalizing and improving category-level pose estimation are still fundamental challenges that are highly relevant for robotics. Moreover, to further improve robotic performance, large object sets, novel objects, refractive materials, and uncertainty estimates are central, largely unsolved open challenges. In order to address them, ontological reasoning, deformability handling, scene-level reasoning, realistic datasets, and the ecological footprint of algorithms need to be improved.},
	urldate = {2024-07-29},
	journal = {IEEE Transactions on Robotics},
	author = {Thalhammer, tefan and Bauer, Dominik and Hönig, Peter and Weibel, Jean-Baptiste and García-Rodríguez, José and Vincze, Markus},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {6D object pose estimation, Pose estimation, Reviews, Robots, Sensors, Standards, Surveys, Training, monocular, open challenges, perception for manipulation, scene understanding},
	pages = {1--20},
}

@misc{bortolon_6dgs_2024,
	title = {{6DGS}: {6D} {Pose} {Estimation} from a {Single} {Image} and a {3D} {Gaussian} {Splatting} {Model}},
	shorttitle = {{6DGS}},
	url = {http://arxiv.org/abs/2407.15484},
	doi = {10.48550/arXiv.2407.15484},
	abstract = {We propose 6DGS to estimate the camera pose of a target RGB image given a 3D Gaussian Splatting (3DGS) model representing the scene. 6DGS avoids the iterative process typical of analysis-by-synthesis methods (e.g. iNeRF) that also require an initialization of the camera pose in order to converge. Instead, our method estimates a 6DoF pose by inverting the 3DGS rendering process. Starting from the object surface, we define a radiant Ellicell that uniformly generates rays departing from each ellipsoid that parameterize the 3DGS model. Each Ellicell ray is associated with the rendering parameters of each ellipsoid, which in turn is used to obtain the best bindings between the target image pixels and the cast rays. These pixel-ray bindings are then ranked to select the best scoring bundle of rays, which their intersection provides the camera center and, in turn, the camera rotation. The proposed solution obviates the necessity of an "a priori" pose for initialization, and it solves 6DoF pose estimation in closed form, without the need for iterations. Moreover, compared to the existing Novel View Synthesis (NVS) baselines for pose estimation, 6DGS can improve the overall average rotational accuracy by 12\% and translation accuracy by 22\% on real scenes, despite not requiring any initialization pose. At the same time, our method operates near real-time, reaching 15fps on consumer hardware.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Bortolon, Matteo and Tsesmelis, Theodore and James, Stuart and Poiesi, Fabio and Del Bue, Alessio},
	month = jul,
	year = {2024},
	note = {arXiv:2407.15484 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huang_human--robot_2024,
	title = {Human-to-{Robot} {Handover} {Control} of an {Autonomous} {Mobile} {Robot} {Based} on {Hand}-{Masked} {Object} {Pose} {Estimation}},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/10608385?casa_token=mVq540lGpg0AAAAA:-aZkOrt10FnQaJR7qNgmAKiAog5pKm7Dubv-rsv27mSpfvu1Ei701o4l2GLg7nz460nokPE1R4A},
	doi = {10.1109/LRA.2024.3433205},
	abstract = {This paper presents a human-to-robot handover design for an Autonomous Mobile Robot (AMR). The developed control system enables the AMR to navigate to a specific person and grasp the object that the person wants to handover. This paper proposes a motion planning algorithm for grasping an unseen object held in hand. Through hand detection and segmentation, the hand region is masked and removed from the acquired depth image, which is used to estimate the object pose for grasping. For grasp pose determination, we propose to add the Convolutional Block Attention Module (CBAM) to the Generative Grasping Convolutional Neural Network (GGCNN) model to enhance the recognition rate. For the object-grasp task, the AMR localizes the object in person's hand, and uses the Model Predictive Control (MPC)-based controller to simultaneously control the mobile base and manipulator to grasp the object. A laboratory-developed mobile manipulator, equipped with a 6-DoF TM5M-900 is used for experimental verification. The experimental results show an average handover success rate of 81\% for five different objects.},
	urldate = {2024-07-28},
	journal = {IEEE Robotics and Automation Letters},
	author = {Huang, Yu-Yun and Song, Kai-Tai},
	year = {2024},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Collaborative Robots in Manufacturing, Grasping, Handover, Manipulators, Mobile Manipulation, Pose Estimation, Pose estimation, Robots, Task analysis, Task and Motion Planning},
	pages = {1--8},
}

@misc{krishnan_omninocs_2024,
	title = {{OmniNOCS}: {A} unified {NOCS} dataset and model for {3D} lifting of {2D} objects},
	shorttitle = {{OmniNOCS}},
	url = {http://arxiv.org/abs/2407.08711},
	doi = {10.48550/arXiv.2407.08711},
	abstract = {We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized Object Coordinate Space (NOCS) maps, object masks, and 3D bounding box annotations for indoor and outdoor scenes. OmniNOCS has 20 times more object classes and 200 times more instances than existing NOCS datasets (NOCS-Real275, Wild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS prediction model (NOCSformer) that can predict accurate NOCS, instance masks and poses from 2D object detections across diverse classes. It is the first NOCS model that can generalize to a broad range of classes when prompted with 2D boxes. We evaluate our model on the task of 3D oriented bounding box prediction, where it achieves comparable results to state-of-the-art 3D detection methods such as Cube R-CNN. Unlike other 3D detection methods, our model also provides detailed and accurate 3D object shape and segmentation. We propose a novel benchmark for the task of NOCS prediction based on OmniNOCS, which we hope will serve as a useful baseline for future work in this area. Our dataset and code will be at the project website: https://omninocs.github.io.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Krishnan, Akshay and Kundu, Abhijit and Maninis, Kevis-Kokitsi and Hays, James and Brown, Matthew},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08711 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_densefusion-1m_2024,
	title = {{DenseFusion}-{1M}: {Merging} {Vision} {Experts} for {Comprehensive} {Multimodal} {Perception}},
	shorttitle = {{DenseFusion}-{1M}},
	url = {http://arxiv.org/abs/2407.08303},
	doi = {10.48550/arXiv.2407.08303},
	abstract = {Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The dataset and code are publicly available at https://github.com/baaivision/DenseFusion.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Li, Xiaotong and Zhang, Fan and Diao, Haiwen and Wang, Yueze and Wang, Xinlong and Duan, Ling-Yu},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08303 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kim_flow4d_2024,
	title = {{Flow4D}: {Leveraging} {4D} {Voxel} {Network} for {LiDAR} {Scene} {Flow} {Estimation}},
	shorttitle = {{Flow4D}},
	url = {http://arxiv.org/abs/2407.07995},
	doi = {10.48550/arXiv.2407.07995},
	abstract = {Understanding the motion states of the surrounding environment is critical for safe autonomous driving. These motion states can be accurately derived from scene flow, which captures the three-dimensional motion field of points. Existing LiDAR scene flow methods extract spatial features from each point cloud and then fuse them channel-wise, resulting in the implicit extraction of spatio-temporal features. Furthermore, they utilize 2D Bird's Eye View and process only two frames, missing crucial spatial information along the Z-axis and the broader temporal context, leading to suboptimal performance. To address these limitations, we propose Flow4D, which temporally fuses multiple point clouds after the 3D intra-voxel feature encoder, enabling more explicit extraction of spatio-temporal features through a 4D voxel network. However, while using 4D convolution improves performance, it significantly increases the computational load. For further efficiency, we introduce the Spatio-Temporal Decomposition Block (STDB), which combines 3D and 1D convolutions instead of using heavy 4D convolution. In addition, Flow4D further improves performance by using five frames to take advantage of richer temporal information. As a result, the proposed method achieves a 45.9\% higher performance compared to the state-of-the-art while running in real-time, and won 1st place in the 2024 Argoverse 2 Scene Flow Challenge. The code is available at https://github.com/dgist-cvlab/Flow4D.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Kim, Jaeyeul and Woo, Jungwan and Shin, Ukcheol and Oh, Jean and Im, Sunghoon},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07995 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_infinite_2024,
	title = {Infinite {Motion}: {Extended} {Motion} {Generation} via {Long} {Text} {Instructions}},
	shorttitle = {Infinite {Motion}},
	url = {http://arxiv.org/abs/2407.08443},
	doi = {10.48550/arXiv.2407.08443},
	abstract = {In the realm of motion generation, the creation of long-duration, high-quality motion sequences remains a significant challenge. This paper presents our groundbreaking work on "Infinite Motion", a novel approach that leverages long text to extended motion generation, effectively bridging the gap between short and long-duration motion synthesis. Our core insight is the strategic extension and reassembly of existing high-quality text-motion datasets, which has led to the creation of a novel benchmark dataset to facilitate the training of models for extended motion sequences. A key innovation of our model is its ability to accept arbitrary lengths of text as input, enabling the generation of motion sequences tailored to specific narratives or scenarios. Furthermore, we incorporate the timestamp design for text which allows precise editing of local segments within the generated sequences, offering unparalleled control and flexibility in motion synthesis. We further demonstrate the versatility and practical utility of "Infinite Motion" through three specific applications: natural language interactive editing, motion sequence editing within long sequences and splicing of independent motion sequences. Each application highlights the adaptability of our approach and broadens the spectrum of possibilities for research and development in motion generation. Through extensive experiments, we demonstrate the superior performance of our model in generating long sequence motions compared to existing methods.Project page: https://shuochengzhai.github.io/Infinite-motion.github.io/},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Li, Mengtian and Zhai, Chengshuo and Yao, Shengxiang and Xie, Zhifeng and Chen, Keyu and Jiang, Yu-Gang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08443 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zurn_wayvescenes101_2024,
	title = {{WayveScenes101}: {A} {Dataset} and {Benchmark} for {Novel} {View} {Synthesis} in {Autonomous} {Driving}},
	shorttitle = {{WayveScenes101}},
	url = {http://arxiv.org/abs/2407.08280},
	doi = {10.48550/arXiv.2407.08280},
	abstract = {We present WayveScenes101, a dataset designed to help the community advance the state of the art in novel view synthesis that focuses on challenging driving scenes containing many dynamic and deformable elements with changing geometry and texture. The dataset comprises 101 driving scenes across a wide range of environmental conditions and driving scenarios. The dataset is designed for benchmarking reconstructions on in-the-wild driving scenes, with many inherent challenges for scene reconstruction methods including image glare, rapid exposure changes, and highly dynamic scenes with significant occlusion. Along with the raw images, we include COLMAP-derived camera poses in standard data formats. We propose an evaluation protocol for evaluating models on held-out camera views that are off-axis from the training views, specifically testing the generalisation capabilities of methods. Finally, we provide detailed metadata for all scenes, including weather, time of day, and traffic conditions, to allow for a detailed model performance breakdown across scene characteristics. Dataset and code are available at https://github.com/wayveai/wayve\_scenes.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Zürn, Jannik and Gladkov, Paul and Dudas, Sofía and Cotter, Fergal and Toteva, Sofi and Shotton, Jamie and Simaiaki, Vasiliki and Mohan, Nikhil},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08280 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{zawalski_robotic_2024,
	title = {Robotic {Control} via {Embodied} {Chain}-of-{Thought} {Reasoning}},
	url = {http://arxiv.org/abs/2407.08693},
	doi = {10.48550/arXiv.2407.08693},
	abstract = {A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of "chain-of-thought" (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28\% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Zawalski, Michał and Chen, William and Pertsch, Karl and Mees, Oier and Finn, Chelsea and Levine, Sergey},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08693 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jiang_hacman_2024,
	title = {{HACMan}++: {Spatially}-{Grounded} {Motion} {Primitives} for {Manipulation}},
	shorttitle = {{HACMan}++},
	url = {http://arxiv.org/abs/2407.08585},
	doi = {10.48550/arXiv.2407.08585},
	abstract = {Although end-to-end robot learning has shown some success for robot manipulation, the learned policies are often not sufficiently robust to variations in object pose or geometry. To improve the policy generalization, we introduce spatially-grounded parameterized motion primitives in our method HACMan++. Specifically, we propose an action representation consisting of three components: what primitive type (such as grasp or push) to execute, where the primitive will be grounded (e.g. where the gripper will make contact with the world), and how the primitive motion is executed, such as parameters specifying the push direction or grasp orientation. These three components define a novel discrete-continuous action space for reinforcement learning. Our framework enables robot agents to learn to chain diverse motion primitives together and select appropriate primitive parameters to complete long-horizon manipulation tasks. By grounding the primitives on a spatial location in the environment, our method is able to effectively generalize across object shape and pose variations. Our approach significantly outperforms existing methods, particularly in complex scenarios demanding both high-level sequential reasoning and object generalization. With zero-shot sim-to-real transfer, our policy succeeds in challenging real-world manipulation tasks, with generalization to unseen objects. Videos can be found on the project website: https://sgmp-rss2024.github.io.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Jiang, Bowen and Wu, Yilin and Zhou, Wenxuan and Paxton, Chris and Held, David},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08585 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{you_cppf_2024,
	title = {{CPPF}++: {Uncertainty}-{Aware} {Sim2Real} {Object} {Pose} {Estimation} by {Vote} {Aggregation}},
	issn = {1939-3539},
	shorttitle = {{CPPF}++},
	url = {https://ieeexplore.ieee.org/abstract/document/10571609?casa_token=iFPDyHF3FxMAAAAA:Ya7GkEYfxalYsTyTcjrmqOzWGu7a2VjRWWyDLe6p5sUmRih6Cz_GlOwnt4RiKQVs8quCEbuVEt0},
	doi = {10.1109/TPAMI.2024.3419038},
	abstract = {Object pose estimation constitutes a critical area within the domain of 3D vision. While contemporary state-of-the-art methods that leverage real-world pose annotations have demonstrated commendable performance, the procurement of such real training data incurs substantial costs. This paper focuses on a specific setting wherein only 3D CAD models are utilized as a priori knowledge, devoid of any background or clutter information. We introduce a novel method, CPPF++, designed for sim-to-real category-level pose estimation. This method builds upon the foundational point-pair voting scheme of CPPF, reformulating it through a probabilistic view. To address the challenge posed by vote collision, we propose a novel approach that involves modeling the voting uncertainty by estimating the probabilistic distribution of each point pair within the canonical space. Furthermore, we augment the contextual information provided by each voting unit through the introduction of N-point tuples. To enhance the robustness and accuracy of the model, we incorporate several innovative modules, including noisy pair filtering, online alignment optimization, and a tuple feature ensemble. Alongside these methodological advancements, we introduce a new category-level pose estimation dataset, named DiversePose 300. Empirical evidence demonstrates that our method significantly surpasses previous sim-to-real approaches and achieves comparable or superior performance on novel datasets. Our code is available on https://github.com/qq456cvb/CPPF2},
	urldate = {2024-07-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {You, Yang and He, Wenhao and Liu, Jin and Xiong, Hongkai and Wang, Weiming and Lu, Cewu},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Dataset creation, Filtering, Noise measurement, Pose estimation, Solid modeling, Training, Training data, Uncertainty, object pose estimation, point-pair features, sim-to-real transfer},
	pages = {1--16},
}

@inproceedings{shi_fast_2021,
	title = {Fast {Uncertainty} {Quantification} for {Deep} {Object} {Pose} {Estimation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9561483?casa_token=yG2-6q_-F_oAAAAA:2_uebR9Gc6eRJ4hbP0N3hLLJWzFUYG29t4eEF9dN7m0HNLcaRiT8mLpmX4fvu4RbYomjexEYOww},
	doi = {10.1109/ICRA48506.2021.9561483},
	abstract = {Deep learning-based object pose estimators are often unreliable and overconfident especially when the input image is outside the training domain, for instance, with sim2real transfer. Efficient and robust uncertainty quantification (UQ) in pose estimators is critically needed in many robotic tasks. In this work, we propose a simple, efficient, and plug-and-play UQ method for 6-DoF object pose estimation. We ensemble 2–3 pre-trained models with different neural network architectures and/or training data sources, and compute their average pair-wise disagreement against one another to obtain the uncertainty quantification. We propose four disagreement metrics, including a learned metric, and show that the average distance (ADD) is the best learning-free metric and it is only slightly worse than the learned metric, which requires labeled target data. Our method has several advantages compared to the prior art: 1) our method does not require any modification of the training process or the model inputs; and 2) it needs only one forward pass for each model. We evaluate the proposed UQ method on three tasks where our uncertainty quantification yields much stronger correlations with pose estimation errors than the baselines. Moreover, in a real robot grasping task, our method increases the grasping success rate from 35\% to 90\%. Video and code are available at https://sites.google.com/view/fastuq.},
	urldate = {2024-07-23},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Shi, Guanya and Zhu, Yifeng and Tremblay, Jonathan and Birchfield, Stan and Ramos, Fabio and Anandkumar, Animashree and Zhu, Yuke},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Computational modeling, Measurement, Neural networks, Pose estimation, Training, Training data, Uncertainty},
	pages = {5200--5207},
}

@inproceedings{yang_object_2023,
	title = {Object {Pose} {Estimation} {With} {Statistical} {Guarantees}: {Conformal} {Keypoint} {Detection} and {Geometric} {Uncertainty} {Propagation}},
	shorttitle = {Object {Pose} {Estimation} {With} {Statistical} {Guarantees}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-07-22},
	author = {Yang, Heng and Pavone, Marco},
	year = {2023},
	pages = {8947--8958},
}

@article{kuppuswamy_fast_2020,
	title = {Fast {Model}-{Based} {Contact} {Patch} and {Pose} {Estimation} for {Highly} {Deformable} {Dense}-{Geometry} {Tactile} {Sensors}},
	volume = {5},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/8936859?denied=},
	doi = {10.1109/LRA.2019.2961050},
	abstract = {Modeling deformable contact is a well-known problem in soft robotics and is particularly challenging for compliant interfaces that permit large deformations. We present a model for the behavior of a highly deformable dense geometry sensor in its interaction with objects; the forward model predicts the elastic deformation of a mesh given the pose and geometry of a contacting rigid object. We use this model to develop a fast approximation to solve the inverse problem: estimating the contact patch when the sensor is deformed by arbitrary objects. This inverse model can be easily identified through experiments and is formulated as a sparse Quadratic Program (QP) that can be solved efficiently online. The proposed model serves as the first stage of a pose estimation pipeline for robot manipulation. We demonstrate the proposed inverse model through real-time estimation of contact patches on a contact-rich manipulation problem in which oversized fingers screw a nut onto a bolt, and as part of a complete pipeline for pose-estimation and tracking based on the Iterative Closest Point (ICP) algorithm. Our results demonstrate a path towards realizing soft robots with highly compliant surfaces that perform complex real-world manipulation tasks.},
	number = {2},
	urldate = {2024-07-21},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kuppuswamy, Naveen and Castro, Alejandro and Phillips-Grafflin, Calder and Alspach, Alex and Tedrake, Russ},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Analytical models, Contact modeling, Deformable models, Finite element analysis, Geometry, Pose estimation, Robot sensing systems, Strain, and learning for soft robots, control, modeling, perception for grasping and manipulation},
	pages = {1811--1818},
}

@misc{ouyang_actionvos_2024,
	title = {{ActionVOS}: {Actions} as {Prompts} for {Video} {Object} {Segmentation}},
	shorttitle = {{ActionVOS}},
	url = {https://arxiv.org/abs/2407.07402v1},
	abstract = {Delving into the realm of egocentric vision, the advancement of referring video object segmentation (RVOS) stands as pivotal in understanding human activities. However, existing RVOS task primarily relies on static attributes such as object names to segment target objects, posing challenges in distinguishing target objects from background objects and in identifying objects undergoing state changes. To address these problems, this work proposes a novel action-aware RVOS setting called ActionVOS, aiming at segmenting only active objects in egocentric videos using human actions as a key language prompt. This is because human actions precisely describe the behavior of humans, thereby helping to identify the objects truly involved in the interaction and to understand possible state changes. We also build a method tailored to work under this specific setting. Specifically, we develop an action-aware labeling module with an efficient action-guided focal loss. Such designs enable ActionVOS model to prioritize active objects with existing readily-available annotations. Experimental results on VISOR dataset reveal that ActionVOS significantly reduces the mis-segmentation of inactive objects, confirming that actions help the ActionVOS model understand objects' involvement. Further evaluations on VOST and VSCOS datasets show that the novel ActionVOS setting enhances segmentation performance when encountering challenging circumstances involving object state changes. We will make our implementation available at https://github.com/ut-vision/ActionVOS.},
	language = {en},
	urldate = {2024-07-14},
	author = {Ouyang, Liangyang and Liu, Ruicong and Huang, Yifei and Furuta, Ryosuke and Sato, Yoichi},
	month = jul,
	year = {2024},
}

@misc{chernyadev_bigym_2024,
	title = {{BiGym}: {A} {Demo}-{Driven} {Mobile} {Bi}-{Manual} {Manipulation} {Benchmark}},
	shorttitle = {{BiGym}},
	url = {https://arxiv.org/abs/2407.07788v2},
	abstract = {We introduce BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set in home environments, ranging from simple target reaching to complex kitchen cleaning. To capture the real-world performance accurately, we provide human-collected demonstrations for each task, reflecting the diverse modalities found in real-world robot trajectories. BiGym supports a variety of observations, including proprioceptive data and visual inputs such as RGB, and depth from 3 camera views. To validate the usability of BiGym, we thoroughly benchmark the state-of-the-art imitation learning algorithms and demo-driven reinforcement learning algorithms within the environment and discuss the future opportunities.},
	language = {en},
	urldate = {2024-07-14},
	author = {Chernyadev, Nikita and Backshall, Nicholas and Ma, Xiao and Lu, Yunfan and Seo, Younggyo and James, Stephen},
	month = jul,
	year = {2024},
}

@misc{yin_learning_2024,
	title = {Learning {In}-{Hand} {Translation} {Using} {Tactile} {Skin} {With} {Shear} and {Normal} {Force} {Sensing}},
	url = {http://arxiv.org/abs/2407.07885},
	doi = {10.48550/arXiv.2407.07885},
	abstract = {Recent progress in reinforcement learning (RL) and tactile sensing has significantly advanced dexterous manipulation. However, these methods often utilize simplified tactile signals due to the gap between tactile simulation and the real world. We introduce a sensor model for tactile skin that enables zero-shot sim-to-real transfer of ternary shear and binary normal forces. Using this model, we develop an RL policy that leverages sliding contact for dexterous in-hand translation. We conduct extensive real-world experiments to assess how tactile sensing facilitates policy adaptation to various unseen object properties and robot hand orientations. We demonstrate that our 3-axis tactile policies consistently outperform baselines that use only shear forces, only normal forces, or only proprioception. Website: https://jessicayin.github.io/tactile-skin-rl/},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Yin, Jessica and Qi, Haozhi and Malik, Jitendra and Pikul, James and Yim, Mark and Hellebrekers, Tess},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07885 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{koppula_tapvid-3d_2024,
	title = {{TAPVid}-{3D}: {A} {Benchmark} for {Tracking} {Any} {Point} in {3D}},
	shorttitle = {{TAPVid}-{3D}},
	url = {http://arxiv.org/abs/2407.05921},
	doi = {10.48550/arXiv.2407.05921},
	abstract = {We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video. Code for dataset download, generation, and model evaluation is available at https://tapvid3d.github.io},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Koppula, Skanda and Rocco, Ignacio and Yang, Yi and Heyward, Joe and Carreira, João and Zisserman, Andrew and Brostow, Gabriel and Doersch, Carl},
	month = jul,
	year = {2024},
	note = {arXiv:2407.05921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xia_targo_2024,
	title = {{TARGO}: {Benchmarking} {Target}-driven {Object} {Grasping} under {Occlusions}},
	shorttitle = {{TARGO}},
	url = {http://arxiv.org/abs/2407.06168},
	doi = {10.48550/arXiv.2407.06168},
	abstract = {Recent advances in predicting 6D grasp poses from a single depth image have led to promising performance in robotic grasping. However, previous grasping models face challenges in cluttered environments where nearby objects impact the target object's grasp. In this paper, we first establish a new benchmark dataset for TARget-driven Grasping under Occlusions, named TARGO. We make the following contributions: 1) We are the first to study the occlusion level of grasping. 2) We set up an evaluation benchmark consisting of large-scale synthetic data and part of real-world data, and we evaluated five grasp models and found that even the current SOTA model suffers when the occlusion level increases, leaving grasping under occlusion still a challenge. 3) We also generate a large-scale training dataset via a scalable pipeline, which can be used to boost the performance of grasping under occlusion and generalized to the real world. 4) We further propose a transformer-based grasping model involving a shape completion module, termed TARGO-Net, which performs most robustly as occlusion increases. Our benchmark dataset can be found at https://TARGO-benchmark.github.io/.},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Xia, Yan and Ding, Ran and Qin, Ziyuan and Zhan, Guanqi and Zhou, Kaichen and Yang, Long and Dong, Hao and Cremers, Daniel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.06168 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kuang_ram_2024,
	title = {{RAM}: {Retrieval}-{Based} {Affordance} {Transfer} for {Generalizable} {Zero}-{Shot} {Robotic} {Manipulation}},
	shorttitle = {{RAM}},
	url = {http://arxiv.org/abs/2407.04689},
	doi = {10.48550/arXiv.2407.04689},
	abstract = {This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. First, RAM extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation. For more details, please check our website at https://yxkryptonite.github.io/RAM/.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Kuang, Yuxuan and Ye, Junjie and Geng, Haoran and Mao, Jiageng and Deng, Congyue and Guibas, Leonidas and Wang, He and Wang, Yue},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04689 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_voxact-b_2024,
	title = {{VoxAct}-{B}: {Voxel}-{Based} {Acting} and {Stabilizing} {Policy} for {Bimanual} {Manipulation}},
	shorttitle = {{VoxAct}-{B}},
	url = {http://arxiv.org/abs/2407.04152},
	doi = {10.48550/arXiv.2407.04152},
	abstract = {Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world \${\textbackslash}texttt\{Open Drawer\}\$ and \${\textbackslash}texttt\{Open Jar\}\$ tasks using two UR5s. Code, data, and videos will be available at https://voxact-b.github.io.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Liu, I.-Chun Arthur and He, Sicheng and Seita, Daniel and Sukhatme, Gaurav},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04152 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{peng_tiebot_2024,
	title = {{TieBot}: {Learning} to {Knot} a {Tie} from {Visual} {Demonstration} through a {Real}-to-{Sim}-to-{Real} {Approach}},
	shorttitle = {{TieBot}},
	url = {http://arxiv.org/abs/2407.03245},
	doi = {10.48550/arXiv.2407.03245},
	abstract = {The tie-knotting task is highly challenging due to the tie's high deformation and long-horizon manipulation actions. This work presents TieBot, a Real-to-Sim-to-Real learning from visual demonstration system for the robots to learn to knot a tie. We introduce the Hierarchical Feature Matching approach to estimate a sequence of tie's meshes from the demonstration video. With these estimated meshes used as subgoals, we first learn a teacher policy using privileged information. Then, we learn a student policy with point cloud observation by imitating teacher policy. Lastly, our pipeline learns a residual policy when the learned policy is applied to real-world execution, mitigating the Sim2Real gap. We demonstrate the effectiveness of TieBot in simulation and the real world. In the real-world experiment, a dual-arm robot successfully knots a tie, achieving 50\% success rate among 10 trials. Videos can be found https://tiebots.github.io/.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Peng, Weikun and Lv, Jun and Zeng, Yuwei and Chen, Haonan and Zhao, Siheng and Sun, Jichen and Lu, Cewu and Shao, Lin},
	month = jul,
	year = {2024},
	note = {arXiv:2407.03245 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{li_flowtrack_2024,
	title = {{FlowTrack}: {Point}-level {Flow} {Network} for {3D} {Single} {Object} {Tracking}},
	shorttitle = {{FlowTrack}},
	url = {http://arxiv.org/abs/2407.01959},
	doi = {10.48550/arXiv.2407.01959},
	abstract = {3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called FlowTrack. Specifically, by estimating the flow for each point in the target, our method could capture the local motion details of target, thereby improving the tracking performance. At the same time, to handle scenes with sparse points, we present a learnable target feature as the bridge to efficiently integrate target information from past frames. Moreover, we design a novel Instance Flow Head to transform dense point-level flow into instance-level motion, effectively aggregating local motion information to obtain global target motion. Finally, our method achieves competitive performance with improvements of 5.9\% on the KITTI dataset and 2.9\% on NuScenes. The code will be made publicly available soon.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Li, Shuo and Cui, Yubo and Li, Zhiheng and Fang, Zheng},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01959 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{loo_open_2024,
	title = {Open {Scene} {Graphs} for {Open} {World} {Object}-{Goal} {Navigation}},
	url = {http://arxiv.org/abs/2407.02473},
	doi = {10.48550/arXiv.2407.02473},
	abstract = {How can we build robots for open-world semantic navigation tasks, like searching for target objects in novel scenes? While foundation models have the rich knowledge and generalisation needed for these tasks, a suitable scene representation is needed to connect them into a complete robot system. We address this with Open Scene Graphs (OSGs), a topo-semantic representation that retains and organises open-set scene information for these models, and has a structure that can be configured for different environment types. We integrate foundation models and OSGs into the OpenSearch system for Open World Object-Goal Navigation, which is capable of searching for open-set objects specified in natural language, while generalising zero-shot across diverse environments and embodiments. Our OSGs enhance reasoning with Large Language Models (LLM), enabling robust object-goal navigation outperforming existing LLM approaches. Through simulation and real-world experiments, we validate OpenSearch's generalisation across varied environments, robots and novel instructions.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Loo, Joel and Wu, Zhanxin and Hsu, David},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02473 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lum_dextrah-g_2024,
	title = {{DextrAH}-{G}: {Pixels}-to-{Action} {Dexterous} {Arm}-{Hand} {Grasping} with {Geometric} {Fabrics}},
	shorttitle = {{DextrAH}-{G}},
	url = {http://arxiv.org/abs/2407.02274},
	doi = {10.48550/arXiv.2407.02274},
	abstract = {A pivotal challenge in robotics is achieving fast, safe, and robust dexterous grasping across a diverse range of objects, an important goal within industrial applications. However, existing methods often have very limited speed, dexterity, and generality, along with limited or no hardware safety guarantees. In this work, we introduce DextrAH-G, a depth-based dexterous grasping policy trained entirely in simulation that combines reinforcement learning, geometric fabrics, and teacher-student distillation. We address key challenges in joint arm-hand policy learning, such as high-dimensional observation and action spaces, the sim2real gap, collision avoidance, and hardware constraints. DextrAH-G enables a 23 motor arm-hand robot to safely and continuously grasp and transport a large variety of objects at high speed using multi-modal inputs including depth images, allowing generalization across object geometry. Videos at https://sites.google.com/view/dextrah-g.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Lum, Tyler Ga Wei and Matak, Martin and Makoviychuk, Viktor and Handa, Ankur and Allshire, Arthur and Hermans, Tucker and Ratliff, Nathan D. and Van Wyk, Karl},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02274 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{wang_sparse_2024,
	title = {Sparse {Diffusion} {Policy}: {A} {Sparse}, {Reusable}, and {Flexible} {Policy} for {Robot} {Learning}},
	shorttitle = {Sparse {Diffusion} {Policy}},
	url = {http://arxiv.org/abs/2407.01531},
	doi = {10.48550/arXiv.2407.01531},
	abstract = {The increasing complexity of tasks in robotics demands efficient strategies for multitask and continual learning. Traditional models typically rely on a universal policy for all tasks, facing challenges such as high computational costs and catastrophic forgetting when learning new tasks. To address these issues, we introduce a sparse, reusable, and flexible policy, Sparse Diffusion Policy (SDP). By adopting Mixture of Experts (MoE) within a transformer-based diffusion policy, SDP selectively activates experts and skills, enabling efficient and task-specific learning without retraining the entire model. SDP not only reduces the burden of active parameters but also facilitates the seamless integration and reuse of experts across various tasks. Extensive experiments on diverse tasks in both simulations and real world show that SDP 1) excels in multitask scenarios with negligible increases in active parameters, 2) prevents forgetting in continual learning of new tasks, and 3) enables efficient task transfer, offering a promising solution for advanced robotic applications. Demos and codes can be found in https://forrest-110.github.io/sparse\_diffusion\_policy/.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Wang, Yixiao and Zhang, Yifei and Huo, Mingxiao and Tian, Ran and Zhang, Xiang and Xie, Yichen and Xu, Chenfeng and Ji, Pengliang and Zhan, Wei and Ding, Mingyu and Tomizuka, Masayoshi},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01531 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_residual-mppi_2024,
	title = {Residual-{MPPI}: {Online} {Policy} {Customization} for {Continuous} {Control}},
	shorttitle = {Residual-{MPPI}},
	url = {http://arxiv.org/abs/2407.00898},
	doi = {10.48550/arXiv.2407.00898},
	abstract = {Policies learned through Reinforcement Learning (RL) and Imitation Learning (IL) have demonstrated significant potential in achieving advanced performance in continuous control tasks. However, in real-world environments, it is often necessary to further customize a trained policy when there are additional requirements that were unforeseen during the original training phase. It is possible to fine-tune the policy to meet the new requirements, but this often requires collecting new data with the added requirements and access to the original training metric and policy parameters. In contrast, an online planning algorithm, if capable of meeting the additional requirements, can eliminate the necessity for extensive training phases and customize the policy without knowledge of the original training scheme or task. In this work, we propose a generic online planning algorithm for customizing continuous-control policies at the execution time which we call Residual-MPPI. It is able to customize a given prior policy on new performance metrics in few-shot and even zero-shot online settings. Also, Residual-MPPI only requires access to the action distribution produced by the prior policy, without additional knowledge regarding the original task. Through our experiments, we demonstrate that the proposed Residual-MPPI algorithm can accomplish the few-shot/zero-shot online policy customization task effectively, including customizing the champion-level racing agent, Gran Turismo Sophy (GT Sophy) 1.0, in the challenging car racing scenario, Gran Turismo Sport (GTS) environment. Demo videos are available on our website: https://sites.google.com/view/residual-mppi},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Wang, Pengcheng and Li, Chenran and Weaver, Catherine and Kawamoto, Kenta and Tomizuka, Masayoshi and Tang, Chen and Zhan, Wei},
	month = jul,
	year = {2024},
	note = {arXiv:2407.00898 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{grotz_peract2_2024,
	title = {{PerAct2}: {A} {Perceiver} {Actor} {Framework} for {Bimanual} {Manipulation} {Tasks}},
	shorttitle = {{PerAct2}},
	url = {http://arxiv.org/abs/2407.00278},
	doi = {10.48550/arXiv.2407.00278},
	abstract = {Bimanual manipulation is challenging due to precise spatial and temporal coordination required between two arms. While there exist several real-world bimanual systems, there is a lack of simulated benchmarks with a large task diversity for systematically studying bimanual capabilities across a wide range of tabletop tasks. This paper addresses the gap by extending RLBench to bimanual manipulation. We open-source our code and benchmark comprising 13 new tasks with 23 unique task variations, each requiring a high degree of coordination and adaptability. To kickstart the benchmark, we extended several state-of-the art methods to bimanual manipulation and also present a language-conditioned behavioral cloning agent -- PerAct2, which enables the learning and execution of bimanual 6-DoF manipulation tasks. Our novel network architecture efficiently integrates language processing with action prediction, allowing robots to understand and perform complex bimanual tasks in response to user-specified goals. Project website with code is available at: http://bimanual.github.io},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Grotz, Markus and Shridhar, Mohit and Asfour, Tamim and Fox, Dieter},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00278 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_learning_2024-1,
	title = {Learning {Granularity}-{Aware} {Affordances} from {Human}-{Object} {Interaction} for {Tool}-{Based} {Functional} {Grasping} in {Dexterous} {Robotics}},
	url = {http://arxiv.org/abs/2407.00614},
	doi = {10.48550/arXiv.2407.00614},
	abstract = {To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance areas and predicting dexterous coarse gestures. We study the intrinsic mechanisms of human tool use. On one hand, we use fine-grained affordance features of object-functional finger contact areas to locate functional affordance regions. On the other hand, we use highly activated coarse-grained affordance features in hand-object interaction regions to predict grasp gestures. Additionally, we introduce a model-based post-processing module that includes functional finger coordinate localization, finger-to-end coordinate transformation, and force feedback-based coarse-to-fine grasping. This forms a complete dexterous robotic functional grasping framework GAAF-Dex, which learns Granularity-Aware Affordances from human-object interaction for tool-based Functional grasping in Dexterous Robotics. Unlike fully-supervised methods that require extensive data annotation, we employ a weakly supervised approach to extract relevant cues from exocentric (Exo) images of hand-object interactions to supervise feature extraction in egocentric (Ego) images. We have constructed a small-scale dataset, FAH, which includes near 6K images of functional hand-object interaction Exo- and Ego images of 18 commonly used tools performing 6 tasks. Extensive experiments on the dataset demonstrate our method outperforms state-of-the-art methods. The code will be made publicly available at https://github.com/yangfan293/GAAF-DEX.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Yang, Fan and Chen, Wenrui and Yang, Kailun and Lin, Haoran and Luo, DongSheng and Tang, Conghui and Li, Zhiyong and Wang, Yaonan},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00614 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{yang_equibot_2024,
	title = {{EquiBot}: {SIM}(3)-{Equivariant} {Diffusion} {Policy} for {Generalizable} and {Data} {Efficient} {Learning}},
	shorttitle = {{EquiBot}},
	url = {http://arxiv.org/abs/2407.01479},
	doi = {10.48550/arXiv.2407.01479},
	abstract = {Building effective imitation learning methods that enable robots to learn from limited data and still generalize across diverse real-world environments is a long-standing problem in robot learning. We propose EquiBot, a robust, data-efficient, and generalizable approach for robot manipulation task learning. Our approach combines SIM(3)-equivariant neural network architectures with diffusion models. This ensures that our learned policies are invariant to changes in scale, rotation, and translation, enhancing their applicability to unseen environments while retaining the benefits of diffusion-based policy learning such as multi-modality and robustness. We show in a suite of 6 simulation tasks that our proposed method reduces the data requirements and improves generalization to novel scenarios. In the real world, we show with in total 10 variations of 6 mobile manipulation tasks that our method can easily generalize to novel objects and scenes after learning from just 5 minutes of human demonstrations in each task.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Yang, Jingyun and Cao, Zi-ang and Deng, Congyue and Antonova, Rika and Song, Shuran and Bohg, Jeannette},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01479 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jin_audio_2024,
	title = {Audio {Matters} {Too}! {Enhancing} {Markerless} {Motion} {Capture} with {Audio} {Signals} for {String} {Performance} {Capture}},
	url = {http://arxiv.org/abs/2405.04963},
	doi = {10.1145/3658235},
	abstract = {In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection.},
	urldate = {2024-07-11},
	author = {Jin, Yitong and Qiu, Zhiping and Shi, Yi and Sun, Shuangpeng and Wang, Chongwu and Pan, Donghao and Zhao, Jiachen and Liang, Zhenghao and Wang, Yuan and Li, Xiaobing and Yu, Feng and Yu, Tao and Dai, Qionghai},
	month = may,
	year = {2024},
	note = {arXiv:2405.04963 [cs]},
	keywords = {Computer Science - Multimedia},
}

@misc{vecerik_robotap_2023,
	title = {{RoboTAP}: {Tracking} {Arbitrary} {Points} for {Few}-{Shot} {Visual} {Imitation}},
	shorttitle = {{RoboTAP}},
	url = {http://arxiv.org/abs/2308.15975},
	doi = {10.48550/arXiv.2308.15975},
	abstract = {For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Vecerik, Mel and Doersch, Carl and Yang, Yi and Davchev, Todor and Aytar, Yusuf and Zhou, Guangyao and Hadsell, Raia and Agapito, Lourdes and Scholz, Jon},
	month = aug,
	year = {2023},
	note = {arXiv:2308.15975 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_humanvla_2024,
	title = {{HumanVLA}: {Towards} {Vision}-{Language} {Directed} {Object} {Rearrangement} by {Physical} {Humanoid}},
	shorttitle = {{HumanVLA}},
	url = {http://arxiv.org/abs/2406.19972},
	doi = {10.48550/arXiv.2406.19972},
	abstract = {Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications. However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications. To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language. A teacher-student framework is utilized to develop HumanVLA. A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior. Then, it is distilled into a vision-language-action model via behavior cloning. We propose several key insights to facilitate the large-scale learning process. To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks. Through extensive experiments and analysis, we demonstrate the effectiveness of the proposed approach.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Xu, Xinyu and Zhang, Yizheng and Li, Yong-Lu and Han, Lei and Lu, Cewu},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19972 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{jian_perception_2024,
	title = {Perception {Stitching}: {Zero}-{Shot} {Perception} {Encoder} {Transfer} for {Visuomotor} {Robot} {Policies}},
	shorttitle = {Perception {Stitching}},
	url = {http://arxiv.org/abs/2406.19971},
	doi = {10.48550/arXiv.2406.19971},
	abstract = {Vision-based imitation learning has shown promising capabilities of endowing robots with various motion skills given visual observation. However, current visuomotor policies fail to adapt to drastic changes in their visual observations. We present Perception Stitching that enables strong zero-shot adaptation to large visual changes by directly stitching novel combinations of visual encoders. Our key idea is to enforce modularity of visual encoders by aligning the latent visual features among different visuomotor policies. Our method disentangles the perceptual knowledge with the downstream motion skills and allows the reuse of the visual encoders by directly stitching them to a policy network trained with partially different visual conditions. We evaluate our method in various simulated and real-world manipulation tasks. While baseline methods failed at all attempts, our method could achieve zero-shot success in real-world visuomotor tasks. Our quantitative and qualitative analysis of the learned features of the policy network provides more insights into the high performance of our proposed method.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Jian, Pingcheng and Lee, Easop and Bell, Zachary and Zavlanos, Michael M. and Chen, Boyuan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19971 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_egogaussian_2024,
	title = {{EgoGaussian}: {Dynamic} {Scene} {Understanding} from {Egocentric} {Video} with {3D} {Gaussian} {Splatting}},
	shorttitle = {{EgoGaussian}},
	url = {http://arxiv.org/abs/2406.19811},
	doi = {10.48550/arXiv.2406.19811},
	abstract = {Human activities are inherently complex, and even simple household tasks involve numerous object interactions. To better understand these activities and behaviors, it is crucial to model their dynamic interactions with the environment. The recent availability of affordable head-mounted cameras and egocentric data offers a more accessible and efficient means to understand dynamic human-object interactions in 3D environments. However, most existing methods for human activity modeling either focus on reconstructing 3D models of hand-object or human-scene interactions or on mapping 3D scenes, neglecting dynamic interactions with objects. The few existing solutions often require inputs from multiple sources, including multi-camera setups, depth-sensing cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the first method capable of simultaneously reconstructing 3D scenes and dynamically tracking 3D object motion from RGB egocentric input alone. We leverage the uniquely discrete nature of Gaussian Splatting and segment dynamic interactions from the background. Our approach employs a clip-level online learning pipeline that leverages the dynamic nature of human activities, allowing us to reconstruct the temporal evolution of the scene in chronological order and track rigid object motion. Additionally, our method automatically segments object and background Gaussians, providing 3D representations for both static scenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic Gaussian methods in challenging in-the-wild videos and we also qualitatively demonstrate the high quality of the reconstructed models.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Zhang, Daiwei and Li, Gengyan and Li, Jiajie and Bressieux, Mickaël and Hilliges, Otmar and Pollefeys, Marc and Van Gool, Luc and Wang, Xi},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cheng_open-television_2024,
	title = {Open-{TeleVision}: {Teleoperation} with {Immersive} {Active} {Visual} {Feedback}},
	shorttitle = {Open-{TeleVision}},
	url = {http://arxiv.org/abs/2407.01512},
	doi = {10.48550/arXiv.2407.01512},
	abstract = {Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system Open-TeleVision that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (Can Sorting, Can Insertion, Folding, and Unloading) for 2 different humanoid robots and deploy them in the real world. The system is open-sourced at: https://robot-tv.github.io/},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Cheng, Xuxin and Li, Jialong and Yang, Shiqi and Yang, Ge and Wang, Xiaolong},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01512 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{li_language-guided_2024,
	title = {Language-{Guided} {Object}-{Centric} {Diffusion} {Policy} for {Collision}-{Aware} {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2407.00451},
	doi = {10.48550/arXiv.2407.00451},
	abstract = {Learning from demonstrations faces challenges in generalizing beyond the training data and is fragile even to slight visual variations. To tackle this problem, we introduce Lan-o3dp, a language guided object centric diffusion policy that takes 3d representation of task relevant objects as conditional input and can be guided by cost function for safety constraints at inference time. Lan-o3dp enables strong generalization in various aspects, such as background changes, visual ambiguity and can avoid novel obstacles that are unseen during the demonstration process. Specifically, We first train a diffusion policy conditioned on point clouds of target objects and then harness a large language model to decompose the user instruction into task related units consisting of target objects and obstacles, which can be used as visual observation for the policy network or converted to a cost function, guiding the generation of trajectory towards collision free region at test time. Our proposed method shows training efficiency and higher success rates compared with the baselines in simulation experiments. In real world experiments, our method exhibits strong generalization performance towards unseen instances, cluttered scenes, scenes of multiple similar objects and demonstrates training free capability of obstacle avoidance.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Li, Hang and Feng, Qian and Zheng, Zhi and Feng, Jianxiang and Knoll, Alois},
	month = jul,
	year = {2024},
	note = {arXiv:2407.00451 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{ai_robopack_2024,
	title = {{RoboPack}: {Learning} {Tactile}-{Informed} {Dynamics} {Models} for {Dense} {Packing}},
	shorttitle = {{RoboPack}},
	url = {http://arxiv.org/abs/2407.01418},
	doi = {10.48550/arXiv.2407.01418},
	abstract = {Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Ai, Bo and Tian, Stephen and Shi, Haochen and Wang, Yixuan and Tan, Cheston and Li, Yunzhu and Wu, Jiajun},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01418 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, I.2.10, I.2.6, I.2.9},
}

@misc{chen_korol_2024,
	title = {{KOROL}: {Learning} {Visualizable} {Object} {Feature} with {Koopman} {Operator} {Rollout} for {Manipulation}},
	shorttitle = {{KOROL}},
	url = {http://arxiv.org/abs/2407.00548},
	doi = {10.48550/arXiv.2407.00548},
	abstract = {Learning dexterous manipulation skills presents significant challenges due to complex nonlinear dynamics that underlie the interactions between objects and multi-fingered hands. Koopman operators have emerged as a robust method for modeling such nonlinear dynamics within a linear framework. However, current methods rely on runtime access to ground-truth (GT) object states, making them unsuitable for vision-based practical applications. Unlike image-to-action policies that implicitly learn visual features for control, we use a dynamics model, specifically the Koopman operator, to learn visually interpretable object features critical for robotic manipulation within a scene. We construct a Koopman operator using object features predicted by a feature extractor and utilize it to auto-regressively advance system states. We train the feature extractor to embed scene information into object features, thereby enabling the accurate propagation of robot trajectories. We evaluate our approach on simulated and real-world robot tasks, with results showing that it outperformed the model-based imitation learning NDP by 1.08\${\textbackslash}times\$ and the image-to-action Diffusion Policy by 1.16\${\textbackslash}times\$. The results suggest that our method maintains task success rates with learned features and extends applicability to real-world manipulation without GT object states.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Chen, Hongyi and Abuduweili, Abulikemu and Agrawal, Aviral and Han, Yunhai and Ravichandar, Harish and Liu, Changliu and Ichnowski, Jeffrey},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00548 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_survey_2022,
	title = {A {Survey} on {Graph} {Neural} {Networks} and {Graph} {Transformers} in {Computer} {Vision}: {A} {Task}-{Oriented} {Perspective}},
	shorttitle = {A {Survey} on {Graph} {Neural} {Networks} and {Graph} {Transformers} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2209.13232},
	doi = {10.48550/arXiv.2209.13232},
	abstract = {Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining ({\textbackslash}emph\{e.g.,\} social network analysis and recommender systems), computer vision ({\textbackslash}emph\{e.g.,\} object detection and point cloud learning), and natural language processing ({\textbackslash}emph\{e.g.,\} relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, {\textbackslash}emph\{i.e.,\} 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Chen, Chaoqi and Wu, Yushuang and Dai, Qiyuan and Zhou, Hong-Yu and Xu, Mutian and Yang, Sibei and Han, Xiaoguang and Yu, Yizhou},
	month = oct,
	year = {2022},
	note = {arXiv:2209.13232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cai_connection_2023,
	title = {On the {Connection} {Between} {MPNN} and {Graph} {Transformer}},
	url = {http://arxiv.org/abs/2301.11956},
	doi = {10.48550/arXiv.2301.11956},
	abstract = {Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022) shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT. In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer (Choromanski et al., 2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1) width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with O(n{\textasciicircum}d) width and O(1) depth can approximate the self-attention layer arbitrarily well, where d is the input feature dimension. Lastly, under some assumptions, we provide an explicit construction of MPNN + VN with O(1) width and O(n) depth approximating the self-attention layer in GT arbitrarily well. On the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly strong baseline, outperforming GT on the recently proposed Long Range Graph Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer and MPNN on the climate modeling task.},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Cai, Chen and Hy, Truong Son and Yu, Rose and Wang, Yusu},
	month = jun,
	year = {2023},
	note = {arXiv:2301.11956 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kim_pure_2022,
	title = {Pure {Transformers} are {Powerful} {Graph} {Learners}},
	url = {http://arxiv.org/abs/2207.02505},
	doi = {10.48550/arXiv.2207.02505},
	abstract = {We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
	month = oct,
	year = {2022},
	note = {arXiv:2207.02505 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lee_pixels_2024,
	title = {From {Pixels} to {Torques} with {Linear} {Feedback}},
	url = {http://arxiv.org/abs/2406.18699},
	abstract = {We demonstrate the effectiveness of simple observer-based linear feedback policies for "pixels-to-torques" control of robotic systems using only a robot-facing camera. Specifically, we show that the matrices of an image-based Luenberger observer (linear state estimator) for a "student" output-feedback policy can be learned from demonstration data provided by a "teacher" state-feedback policy via simple linear-least-squares regression. The resulting linear output-feedback controller maps directly from high-dimensional raw images to torques while being amenable to the rich set of analytical tools from linear systems theory, alowing us to enforce closed-loop stability constraints in the learning problem. We also investigate a nonlinear extension of the method via the Koopman embedding. Finally, we demonstrate the surprising effectiveness of linear pixels-to-torques policies on a cartpole system, both in simulation and on real-world hardware. The policy successfully executes both stabilizing and swing-up trajectory tracking tasks using only camera feedback while subject to model mismatch, process and sensor noise, perturbations, and occlusions.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Lee, Jeong Hun and Schoedel, Sam and Bhardwaj, Aditya and Manchester, Zachary},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18699 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhu_geometric_2024,
	title = {Geometric {Features} {Enhanced} {Human}-{Object} {Interaction} {Detection}},
	url = {http://arxiv.org/abs/2406.18691},
	abstract = {Cameras are essential vision instruments to capture images for pattern detection and measurement. Human-object interaction (HOI) detection is one of the most popular pattern detection approaches for captured human-centric visual scenes. Recently, Transformer-based models have become the dominant approach for HOI detection due to their advanced network architectures and thus promising results. However, most of them follow the one-stage design of vanilla Transformer, leaving rich geometric priors under-exploited and leading to compromised performance especially when occlusion occurs. Given that geometric features tend to outperform visual ones in occluded scenarios and offer information that complements visual cues, we propose a novel end-to-end Transformer-style HOI detection model, i.e., geometric features enhanced HOI detector (GeoHOI). One key part of the model is a new unified self-supervised keypoint learning method named UniPointNet that bridges the gap of consistent keypoint representation across diverse object categories, including humans. GeoHOI effectively upgrades a Transformer-based HOI detector benefiting from the keypoints similarities measuring the likelihood of human-object interactions as well as local keypoint patches to enhance interaction query representation, so as to boost HOI predictions. Extensive experiments show that the proposed method outperforms the state-of-the-art models on V-COCO and achieves competitive performance on HICO-DET. Case study results on the post-disaster rescue with vision-based instruments showcase the applicability of the proposed GeoHOI in real-world applications.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Zhu, Manli and Ho, Edmond S. L. and Chen, Shuang and Yang, Longzhi and Shum, Hubert P. H.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18691 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{stearns_dynamic_2024,
	title = {Dynamic {Gaussian} {Marbles} for {Novel} {View} {Synthesis} of {Casual} {Monocular} {Videos}},
	url = {http://arxiv.org/abs/2406.18717},
	abstract = {Gaussian splatting has become a popular representation for novel-view synthesis, exhibiting clear strengths in efficiency, photometric quality, and compositional edibility. Following its success, many works have extended Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while also tracking scene geometry far better than alternative representations. Yet, these methods assume dense multi-view videos as supervision, constraining their use to controlled capture settings. In this work, we extend the capability of Gaussian scene representations to casually captured monocular videos. We show that existing 4D Gaussian methods dramatically fail in this setup because the monocular setting is underconstrained. Building off this finding, we propose Dynamic Gaussian Marbles (DGMarbles), consisting of three core modifications that target the difficulties of the monocular setting. First, DGMarbles uses isotropic Gaussian "marbles", reducing the degrees of freedom of each Gaussian, and constraining the optimization to focus on motion and appearance over local shape. Second, DGMarbles employs a hierarchical divide-and-conquer learning strategy to guide the optimization towards solutions with coherent motion. Finally, DGMarbles adds image-level and geometry-level priors into the optimization, including a tracking loss that takes advantage of recent progress in point tracking. By constraining the optimization in these ways, DGMarbles learns Gaussian trajectories that enable novel-view rendering and accurately capture the 3D motion of the scene elements. We evaluate on the (monocular) Nvidia Dynamic Scenes dataset and the Dycheck iPhone dataset, and show that DGMarbles significantly outperforms other Gaussian baselines in quality, and is on-par with non-Gaussian representations, all while maintaining the efficiency, compositionality, editability, and tracking benefits of Gaussians.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Stearns, Colton and Harley, Adam and Uy, Mikaela and Dubost, Florian and Tombari, Federico and Wetzstein, Gordon and Guibas, Leonidas},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18717 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tziafas_3d_2024,
	title = {{3D} {Feature} {Distillation} with {Object}-{Centric} {Priors}},
	url = {http://arxiv.org/abs/2406.18742},
	abstract = {Grounding natural language to the physical world is a ubiquitous topic with a wide range of applications in computer vision and robotics. Recently, 2D vision-language models such as CLIP have been widely popularized, due to their impressive capabilities for open-vocabulary grounding in 2D images. Recent works aim to elevate 2D CLIP features to 3D via feature distillation, but either learn neural fields that are scene-specific and hence lack generalization, or focus on indoor room scan data that require access to multiple camera views, which is not practical in robot manipulation scenarios. Additionally, related methods typically fuse features at pixel-level and assume that all camera views are equally informative. In this work, we show that this approach leads to sub-optimal 3D features, both in terms of grounding accuracy, as well as segmentation crispness. To alleviate this, we propose a multi-view feature fusion strategy that employs object-centric priors to eliminate uninformative views based on semantic information, and fuse features at object-level via instance segmentation masks. To distill our object-centric 3D features, we generate a large-scale synthetic multi-view dataset of cluttered tabletop scenes, spawning 15k scenes from over 3300 unique object instances, which we make publicly available. We show that our method reconstructs 3D CLIP features with improved grounding capacity and spatial consistency, while doing so from single-view RGB-D, thus departing from the assumption of multiple camera views at test time. Finally, we show that our approach can generalize to novel tabletop domains and be re-purposed for 3D instance segmentation without fine-tuning, and demonstrate its utility for language-guided robotic grasping in clutter},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Tziafas, Georgios and Xu, Yucheng and Li, Zhibin and Kasaei, Hamidreza},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18742 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{tziafas_towards_2024,
	title = {Towards {Open}-{World} {Grasping} with {Large} {Vision}-{Language} {Models}},
	url = {http://arxiv.org/abs/2406.18722},
	abstract = {The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and action models to ground such knowledge into the environment and parameterize actuation. This setup suffers from two major bottlenecks: a) the LLM's reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios. In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. We propose OWG, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor scene datasets to showcase OWG's robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Tziafas, Georgios and Kasaei, Hamidreza},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_core4d_2024,
	title = {{CORE4D}: {A} {4D} {Human}-{Object}-{Human} {Interaction} {Dataset} for {Collaborative} {Object} {REarrangement}},
	shorttitle = {{CORE4D}},
	url = {http://arxiv.org/abs/2406.19353},
	doi = {10.48550/arXiv.2406.19353},
	abstract = {Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies. Our dataset and code are available at https://github.com/leolyliu/CORE4D-Instructions.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Zhang, Chengwen and Liu, Yun and Xing, Ruofan and Tang, Bingda and Yi, Li},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{duan_manipulate-anything_2024,
	title = {Manipulate-{Anything}: {Automating} {Real}-{World} {Robots} using {Vision}-{Language} {Models}},
	shorttitle = {Manipulate-{Anything}},
	url = {http://arxiv.org/abs/2406.18915},
	doi = {10.48550/arXiv.2406.18915},
	abstract = {Large-scale endeavors like RT-1 and widespread community efforts such as Open-X-Embodiment have contributed to growing the scale of robot demonstration data. However, there is still an opportunity to improve the quality, quantity, and diversity of robot demonstration data. Although vision-language models have been shown to automatically generate demonstration data, their utility has been limited to environments with privileged state information, they require hand-designed skills, and are limited to interactions with few object instances. We propose Manipulate-Anything, a scalable automated generation method for real-world robotic manipulation. Unlike prior work, our method can operate in real-world environments without any privileged state information, hand-designed skills, and can manipulate any static object. We evaluate our method using two setups. First, Manipulate-Anything successfully generates trajectories for all 5 real-world and 12 simulation tasks, significantly outperforming existing methods like VoxPoser. Second, Manipulate-Anything's demonstrations can train more robust behavior cloning policies than training with human demonstrations, or from data generated by VoxPoser and Code-As-Policies. We believe {\textbackslash}methodLong{\textbackslash} can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Duan, Jiafei and Yuan, Wentao and Pumacay, Wilbert and Wang, Yi Ru and Ehsani, Kiana and Fox, Dieter and Krishna, Ranjay},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{harrison_tapsense_2011,
	address = {New York, NY, USA},
	series = {{UIST} '11},
	title = {{TapSense}: enhancing finger interaction on touch surfaces},
	isbn = {978-1-4503-0716-1},
	shorttitle = {{TapSense}},
	url = {https://doi.org/10.1145/2047196.2047279},
	doi = {10.1145/2047196.2047279},
	abstract = {We present TapSense, an enhancement to touch interaction that allows conventional surfaces to identify the type of object being used for input. This is achieved by segmenting and classifying sounds resulting from an object's impact. For example, the diverse anatomy of a human finger allows different parts to be recognized including the tip, pad, nail and knuckle - without having to instrument the user. This opens several new and powerful interaction opportunities for touch input, especially in mobile devices, where input is extremely constrained. Our system can also identify different sets of passive tools. We conclude with a comprehensive investigation of classification accuracy and training implications. Results show our proof-of-concept system can support sets with four input types at around 95\% accuracy. Small, but useful input sets of two (e.g., pen and finger discrimination) can operate in excess of 99\% accuracy.},
	urldate = {2024-06-28},
	booktitle = {Proceedings of the 24th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Harrison, Chris and Schwarz, Julia and Hudson, Scott E.},
	month = oct,
	year = {2011},
	pages = {627--636},
}

@inproceedings{zhang_actitouch_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {{ActiTouch}: {Robust} {Touch} {Detection} for {On}-{Skin} {AR}/{VR} {Interfaces}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {{ActiTouch}},
	url = {https://doi.org/10.1145/3332165.3347869},
	doi = {10.1145/3332165.3347869},
	abstract = {Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. In response, we developed ActiTouch, a new electrical method that enables precise on-skin touch segmentation by using the body as an RF waveguide. We combine this method with computer vision, enabling a system with both high tracking precision and robust touch detection. Our system requires no cumbersome instrumentation of the fingers or hands, requiring only a single wristband (e.g., smartwatch) and sensors integrated into an AR/VR headset. We quantify the accuracy of our approach through a user study and demonstrate how it can enable touchscreen-like interactions on the skin.},
	urldate = {2024-06-28},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yang and Kienzle, Wolf and Ma, Yanjun and Ng, Shiu S. and Benko, Hrvoje and Harrison, Chris},
	month = oct,
	year = {2019},
	pages = {1151--1159},
}

@inproceedings{gu_accurate_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Accurate and {Low}-{Latency} {Sensing} of {Touch} {Contact} on {Any} {Surface} with {Finger}-{Worn} {IMU} {Sensor}},
	isbn = {978-1-4503-6816-2},
	url = {https://doi.org/10.1145/3332165.3347947},
	doi = {10.1145/3332165.3347947},
	abstract = {Head-mounted Mixed Reality (MR) systems enable touch in­teraction on any physical surface. However, optical methods (i.e., with cameras on the headset) have difficulty in determin­ing the touch contact accurately. We show that a finger ring with Inertial Measurement Unit (IMU) can substantially im­prove the accuracy of contact sensing from 84.74\% to 98.61\% (f1 score), with a low latency of 10 ms. We tested different ring wearing positions and tapping postures (e.g., with different fingers and parts). Results show that an IMU-based ring worn on the proximal phalanx of the index finger can accurately sense touch contact of most usable tapping postures. Partici­pants preferred wearing a ring for better user experience. Our approach can be used in combination with the optical touch sensing to provide robust and low-latency contact detection.},
	urldate = {2024-06-28},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Gu, Yizheng and Yu, Chun and Li, Zhipeng and Li, Weiqi and Xu, Shuchang and Wei, Xiaoying and Shi, Yuanchun},
	month = oct,
	year = {2019},
	pages = {1059--1070},
}

@misc{tziafas_lifelong_2024,
	title = {Lifelong {Robot} {Library} {Learning}: {Bootstrapping} {Composable} and {Generalizable} {Skills} for {Embodied} {Control} with {Language} {Models}},
	shorttitle = {Lifelong {Robot} {Library} {Learning}},
	url = {http://arxiv.org/abs/2406.18746},
	abstract = {Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully hand-crafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL\_project.},
	urldate = {2024-06-28},
	publisher = {arXiv},
	author = {Tziafas, Georgios and Kasaei, Hamidreza},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18746 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{shen_mousering_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {{MouseRing}: {Always}-available {Touchpad} {Interaction} with {IMU} {Rings}},
	isbn = {9798400703300},
	shorttitle = {{MouseRing}},
	url = {https://doi.org/10.1145/3613904.3642225},
	doi = {10.1145/3613904.3642225},
	abstract = {Tracking fine-grained finger movements with IMUs for continuous 2D-cursor control poses significant challenges due to limited sensing capabilities. Our findings suggest that finger-motion patterns and the inherent structure of joints provide beneficial physical knowledge, which lead us to enhance motion perception accuracy by integrating physical priors into ML models. We propose MouseRing, a novel ring-shaped IMU device that enables continuous finger-sliding on unmodified physical surfaces like a touchpad. A motion dataset was created using infrared cameras, touchpads, and IMUs. We then identified several useful physical constraints, such as joint co-planarity, rigid constraints, and velocity consistency. These principles help refine the finger-tracking predictions from an RNN model. By incorporating touch state detection as a cursor movement switch, we achieved precise cursor control. In a Fitts’ Law study, MouseRing demonstrated input efficiency comparable to touchpads. In real-world applications, MouseRing ensured robust, efficient input and good usability across various surfaces and body postures.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Shen, Xiyuan and Yu, Chun and Wang, Xutong and Liang, Chen and Chen, Haozhan and Shi, Yuanchun},
	month = may,
	year = {2024},
	pages = {1--19},
}

@misc{liu_plan_2022,
	title = {Plan {Your} {Target} and {Learn} {Your} {Skills}: {Transferable} {State}-{Only} {Imitation} {Learning} via {Decoupled} {Policy} {Optimization}},
	shorttitle = {Plan {Your} {Target} and {Learn} {Your} {Skills}},
	url = {http://arxiv.org/abs/2203.02214},
	doi = {10.48550/arXiv.2203.02214},
	abstract = {Recent progress in state-only imitation learning extends the scope of applicability of imitation learning to real-world settings by relieving the need for observing expert actions. However, existing solutions only learn to extract a state-to-action mapping policy from the data, without considering how the expert plans to the target. This hinders the ability to leverage demonstrations and limits the flexibility of the policy. In this paper, we introduce Decoupled Policy Optimization (DePO), which explicitly decouples the policy as a high-level state planner and an inverse dynamics model. With embedded decoupled policy gradient and generative adversarial training, DePO enables knowledge transfer to different action spaces or state transition dynamics, and can generalize the planner to out-of-demonstration state regions. Our in-depth experimental analysis shows the effectiveness of DePO on learning a generalized target state planner while achieving the best imitation performance. We demonstrate the appealing usage of DePO for transferring across different tasks by pre-training, and the potential for co-training agents with various skills.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Liu, Minghuan and Zhu, Zhengbang and Zhuang, Yuzheng and Zhang, Weinan and Hao, Jianye and Yu, Yong and Wang, Jun},
	month = sep,
	year = {2022},
	note = {arXiv:2203.02214 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{qian_3d-mvp_2024,
	title = {{3D}-{MVP}: {3D} {Multiview} {Pretraining} for {Robotic} {Manipulation}},
	shorttitle = {{3D}-{MVP}},
	url = {http://arxiv.org/abs/2406.18158},
	doi = {10.48550/arXiv.2406.18158},
	abstract = {Recent works have shown that visual pretraining on egocentric datasets using masked autoencoders (MAE) can improve generalization for downstream robotics tasks. However, these approaches pretrain only on 2D images, while many robotics applications require 3D scene understanding. In this work, we propose 3D-MVP, a novel approach for 3D multi-view pretraining using masked autoencoders. We leverage Robotic View Transformer (RVT), which uses a multi-view transformer to understand the 3D scene and predict gripper pose actions. We split RVT's multi-view transformer into visual encoder and action decoder, and pretrain its visual encoder using masked autoencoding on large-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of virtual robot manipulation tasks and demonstrate improved performance over baselines. We also show promising results on a real robot platform with minimal finetuning. Our results suggest that 3D-aware pretraining is a promising approach to improve sample efficiency and generalization of vision-based robotic manipulation policies. We will release code and pretrained models for 3D-MVP to facilitate future research. Project site: https://jasonqsy.github.io/3DMVP},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Qian, Shengyi and Mo, Kaichun and Blukis, Valts and Fouhey, David F. and Fox, Dieter and Goyal, Ankit},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18158 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_sonicsense_2024,
	title = {{SonicSense}: {Object} {Perception} from {In}-{Hand} {Acoustic} {Vibration}},
	shorttitle = {{SonicSense}},
	url = {http://arxiv.org/abs/2406.17932},
	abstract = {We introduce SonicSense, a holistic design of hardware and software to enable rich robot object perception through in-hand acoustic vibration sensing. While previous studies have shown promising results with acoustic sensing for object perception, current solutions are constrained to a handful of objects with simple geometries and homogeneous materials, single-finger sensing, and mixing training and testing on the same objects. SonicSense enables container inventory status differentiation, heterogeneous material prediction, 3D shape reconstruction, and object re-identification from a diverse set of 83 real-world objects. Our system employs a simple but effective heuristic exploration policy to interact with the objects as well as end-to-end learning-based algorithms to fuse vibration signals to infer object properties. Our framework underscores the significance of in-hand acoustic vibration sensing in advancing robot tactile perception.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Liu, Jiaxun and Chen, Boyuan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17932 [cs, eess]},
	keywords = {Computer Science - Multimedia, Computer Science - Robotics, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wu_human-object_2024,
	title = {Human-{Object} {Interaction} from {Human}-{Level} {Instructions}},
	url = {http://arxiv.org/abs/2406.17840},
	abstract = {Intelligent agents need to autonomously navigate and interact within contextual environments to perform a wide range of daily tasks based on human-level instructions. These agents require a foundational understanding of the world, incorporating common sense and knowledge, to interpret such instructions. Moreover, they must possess precise low-level skills for movement and interaction to execute the detailed task plans derived from these instructions. In this work, we address the task of synthesizing continuous human-object interactions for manipulating large objects within contextual environments, guided by human-level instructions. Our goal is to generate synchronized object motion, full-body human motion, and detailed finger motion, all essential for realistic interactions. Our framework consists of a large language model (LLM) planning module and a low-level motion generator. We use LLMs to deduce spatial object relationships and devise a method for accurately determining their positions and orientations in target scene layouts. Additionally, the LLM planner outlines a detailed task plan specifying a sequence of sub-tasks. This task plan, along with the target object poses, serves as input for our low-level motion generator, which seamlessly alternates between navigation and interaction modules. We present the first complete system that can synthesize object motion, full-body motion, and finger motion simultaneously from human-level instructions. Our experiments demonstrate the effectiveness of our high-level planner in generating plausible target layouts and our low-level motion generator in synthesizing realistic interactions for diverse objects. Please refer to our project page for more results: https://hoifhli.github.io/.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Wu, Zhen and Li, Jiaman and Liu, C. Karen},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17840 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_extract_2024,
	title = {{EXTRACT}: {Efficient} {Policy} {Learning} by {Extracting} {Transferrable} {Robot} {Skills} from {Offline} {Data}},
	shorttitle = {{EXTRACT}},
	url = {http://arxiv.org/abs/2406.17768},
	doi = {10.48550/arXiv.2406.17768},
	abstract = {Most reinforcement learning (RL) methods focus on learning optimal policies over low-level action spaces. While these methods can perform well in their training environments, they lack the flexibility to transfer to new tasks. Instead, RL agents that can act over useful, temporally extended skills rather than low-level actions can learn new tasks more easily. Prior work in skill-based RL either requires expert supervision to define useful skills, which is hard to scale, or learns a skill-space from offline data with heuristics that limit the adaptability of the skills, making them difficult to transfer during downstream RL. Our approach, EXTRACT, instead utilizes pre-trained vision language models to extract a discrete set of semantically meaningful skills from offline data, each of which is parameterized by continuous arguments, without human supervision. This skill parameterization allows robots to learn new tasks by only needing to learn when to select a specific skill and how to modify its arguments for the specific task. We demonstrate through experiments in sparse-reward, image-based, robot manipulation environments that EXTRACT can more quickly learn new tasks than prior works, with major gains in sample efficiency and performance over prior skill-based RL. Website at https://www.jessezhang.net/projects/extract/.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Zhang, Jesse and Heo, Minho and Liu, Zuxin and Biyik, Erdem and Lim, Joseph J. and Liu, Yao and Fakoor, Rasool},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17768 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{harish_reinforcement_2024,
	title = {Reinforcement {Learning} via {Auxiliary} {Task} {Distillation}},
	url = {http://arxiv.org/abs/2406.17168},
	doi = {10.48550/arXiv.2406.17168},
	abstract = {We present Reinforcement Learning via Auxiliary Task Distillation (AuxDistill), a new method that enables reinforcement learning (RL) to perform long-horizon robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill achieves this by concurrently carrying out multi-task RL with auxiliary tasks, which are easier to learn and relevant to the main task. A weighted distillation loss transfers behaviors from these auxiliary tasks to solve the main task. We demonstrate that AuxDistill can learn a pixels-to-actions policy for a challenging multi-stage embodied object rearrangement task from the environment reward without demonstrations, a learning curriculum, or pre-trained skills. AuxDistill achieves \$2.3 {\textbackslash}times\$ higher success than the previous state-of-the-art baseline in the Habitat Object Rearrangement benchmark and outperforms methods that use pre-trained skills and expert demonstrations.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Harish, Abhinav Narayan and Heck, Larry and Hanna, Josiah P. and Kira, Zsolt and Szot, Andrew},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17168 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jia_open-vocabulary_2024,
	title = {Open-vocabulary {Pick} and {Place} via {Patch}-level {Semantic} {Maps}},
	url = {http://arxiv.org/abs/2406.15677},
	doi = {10.48550/arXiv.2406.15677},
	abstract = {Controlling robots through natural language instructions in open-vocabulary scenarios is pivotal for enhancing human-robot collaboration and complex robot behavior synthesis. However, achieving this capability poses significant challenges due to the need for a system that can generalize from limited data to a wide range of tasks and environments. Existing methods rely on large, costly datasets and struggle with generalization. This paper introduces Grounded Equivariant Manipulation (GEM), a novel approach that leverages the generative capabilities of pre-trained vision-language models and geometric symmetries to facilitate few-shot and zero-shot learning for open-vocabulary robot manipulation tasks. Our experiments demonstrate GEM's high sample efficiency and superior generalization across diverse pick-and-place tasks in both simulation and real-world experiments, showcasing its ability to adapt to novel instructions and unseen objects with minimal data requirements. GEM advances a significant step forward in the domain of language-conditioned robot control, bridging the gap between semantic understanding and action generation in robotic systems.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Jia, Mingxi and Huang, Haojie and Zhang, Zhewen and Wang, Chenghao and Zhao, Linfeng and Wang, Dian and Liu, Jason Xinyu and Walters, Robin and Platt, Robert and Tellex, Stefanie},
	month = jun,
	year = {2024},
	note = {arXiv:2406.15677 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liang_dreamitate_2024,
	title = {Dreamitate: {Real}-{World} {Visuomotor} {Policy} {Learning} via {Video} {Generation}},
	shorttitle = {Dreamitate},
	url = {http://arxiv.org/abs/2406.16862},
	doi = {10.48550/arXiv.2406.16862},
	abstract = {A key challenge in manipulation is learning a policy that can robustly generalize to diverse visual environments. A promising mechanism for learning robust policies is to leverage video generative models, which are pretrained on large-scale datasets of internet videos. In this paper, we propose a visuomotor policy learning framework that fine-tunes a video diffusion model on human demonstrations of a given task. At test time, we generate an example of an execution of the task conditioned on images of a novel scene, and use this synthesized execution directly to control the robot. Our key insight is that using common tools allows us to effortlessly bridge the embodiment gap between the human hand and the robot manipulator. We evaluate our approach on four tasks of increasing complexity and demonstrate that harnessing internet-scale generative models allows the learned policy to achieve a significantly higher degree of generalization than existing behavior cloning approaches.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Liang, Junbang and Liu, Ruoshi and Ozguroglu, Ege and Sudhakar, Sruthi and Dave, Achal and Tokmakov, Pavel and Song, Shuran and Vondrick, Carl},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16862 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{qu_livescene_2024,
	title = {{LiveScene}: {Language} {Embedding} {Interactive} {Radiance} {Fields} for {Physical} {Scene} {Rendering} and {Control}},
	shorttitle = {{LiveScene}},
	url = {http://arxiv.org/abs/2406.16038},
	doi = {10.48550/arXiv.2406.16038},
	abstract = {This paper aims to advance the progress of physical world interactive scene reconstruction by extending the interactive object reconstruction from single object level to complex scene level. To this end, we first construct one simulated and one real scene-level physical interaction dataset containing 28 scenes with multiple interactive objects per scene. Furthermore, to accurately model the interactive motions of multiple objects in complex scenes, we propose LiveScene, the first scene-level language-embedded interactive neural radiance field that efficiently reconstructs and controls multiple interactive objects in complex scenes. LiveScene introduces an efficient factorization that decomposes the interactive scene into multiple local deformable fields to separately reconstruct individual interactive objects, achieving the first accurate and independent control on multiple interactive objects in a complex scene. Moreover, we introduce an interaction-aware language embedding method that generates varying language embeddings to localize individual interactive objects under different interactive states, enabling arbitrary control of interactive objects using natural language. Finally, we evaluate LiveScene on the constructed datasets OminiSim and InterReal with various simulated and real-world complex scenes. Extensive experiment results demonstrate that the proposed approach achieves SOTA novel view synthesis and language grounding performance, surpassing existing methods by +9.89, +1.30, and +1.99 in PSNR on CoNeRF Synthetic, OminiSim \#chanllenging, and InterReal \#chanllenging datasets, and +65.12 of mIOU on OminiSim, respectively. Project page: {\textbackslash}href\{https://livescenes.github.io\}\{https://livescenes.github.io\}.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Qu, Delin and Chen, Qizhi and Zhang, Pingrui and Gao, Xianqiang and Zhao, Bin and Wang, Dong and Li, Xuelong},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_transferable_2024,
	title = {Transferable {Tactile} {Transformers} for {Representation} {Learning} {Across} {Diverse} {Sensors} and {Tasks}},
	url = {http://arxiv.org/abs/2406.13640},
	doi = {10.48550/arXiv.2406.13640},
	abstract = {This paper presents T3: Transferable Tactile Transformers, a framework for tactile representation learning that scales across multi-sensors and multi-tasks. T3 is designed to overcome the contemporary issue that camera-based tactile sensing is extremely heterogeneous, i.e. sensors are built into different form factors, and existing datasets were collected for disparate tasks. T3 captures the shared latent information across different sensor-task pairings by constructing a shared trunk transformer with sensor-specific encoders and task-specific decoders. The pre-training of T3 utilizes a novel Foundation Tactile (FoTa) dataset, which is aggregated from several open-sourced datasets and it contains over 3 million data points gathered from 13 sensors and 11 tasks. FoTa is the largest and most diverse dataset in tactile sensing to date and it is made publicly available in a unified format. Across various sensors and tasks, experiments show that T3 pre-trained with FoTa achieved zero-shot transferability in certain sensor-task pairings, can be further fine-tuned with small amounts of domain-specific data, and its performance scales with bigger network sizes. T3 is also effective as a tactile encoder for long horizon contact-rich manipulation. Results from sub-millimeter multi-pin electronics insertion tasks show that T3 achieved a task success rate 25\% higher than that of policies trained with tactile encoders trained from scratch, or 53\% higher than without tactile sensing. Data, code, and model checkpoints are open-sourced at https://t3.alanz.info.},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Zhao, Jialiang and Ma, Yuxiang and Wang, Lirui and Adelson, Edward H.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.13640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{li_tactile_2024,
	title = {Tactile {SoftHand}-{A}: {3D}-{Printed}, {Tactile}, {Highly}-underactuated, {Anthropomorphic} {Robot} {Hand} with an {Antagonistic} {Tendon} {Mechanism}},
	shorttitle = {Tactile {SoftHand}-{A}},
	url = {http://arxiv.org/abs/2406.12731},
	doi = {10.48550/arXiv.2406.12731},
	abstract = {For tendon-driven multi-fingered robotic hands, ensuring grasp adaptability while minimizing the number of actuators needed to provide human-like functionality is a challenging problem. Inspired by the Pisa/IIT SoftHand, this paper introduces a 3D-printed, highly-underactuated, five-finger robotic hand named the Tactile SoftHand-A, which features only two actuators. The dual-tendon design allows for the active control of specific (distal or proximal interphalangeal) joints to adjust the hand's grasp gesture. We have also developed a new design of fully 3D-printed tactile sensor that requires no hand assembly and is printed directly as part of the robotic finger. This sensor is integrated into the fingertips and combined with the antagonistic tendon mechanism to develop a human-hand-guided tactile feedback grasping system. The system can actively mirror human hand gestures, adaptively stabilize grasp gestures upon contact, and adjust grasp gestures to prevent object movement after detecting slippage. Finally, we designed four different experiments to evaluate the novel fingers coupled with the antagonistic mechanism for controlling the robotic hand's gestures, adaptive grasping ability, and human-hand-guided tactile feedback grasping capability. The experimental results demonstrate that the Tactile SoftHand-A can adaptively grasp objects of a wide range of shapes and automatically adjust its gripping gestures upon detecting contact and slippage. Overall, this study points the way towards a class of low-cost, accessible, 3D-printable, underactuated human-like robotic hands, and we openly release the designs to facilitate others to build upon this work. This work is Open-sourced at github.com/SoutheastWind/Tactile\_SoftHand\_A},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Li, Haoran and Ford, Christopher J. and Lu, Chenghua and Lin, Yijiong and Bianchi, Matteo and Catalano, Manuel G. and Psomopoulou, Efi and Lepora, Nathan F.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.12731 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{kerr_lerf_2023,
	title = {{LERF}: {Language} {Embedded} {Radiance} {Fields}},
	shorttitle = {{LERF}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html?trk=public_post_comment-text},
	language = {en},
	urldate = {2024-06-24},
	author = {Kerr, Justin and Kim, Chung Min and Goldberg, Ken and Kanazawa, Angjoo and Tancik, Matthew},
	year = {2023},
	pages = {19729--19739},
}

@misc{zhang_freemotion_2024,
	title = {{FreeMotion}: {MoCap}-{Free} {Human} {Motion} {Synthesis} with {Multimodal} {Large} {Language} {Models}},
	shorttitle = {{FreeMotion}},
	url = {http://arxiv.org/abs/2406.10740},
	doi = {10.48550/arXiv.2406.10740},
	abstract = {Human motion synthesis is a fundamental task in computer animation. Despite recent progress in this field utilizing deep learning and motion capture data, existing methods are always limited to specific motion categories, environments, and styles. This poor generalizability can be partially attributed to the difficulty and expense of collecting large-scale and high-quality motion data. At the same time, foundation models trained with internet-scale image and text data have demonstrated surprising world knowledge and reasoning ability for various downstream tasks. Utilizing these foundation models may help with human motion synthesis, which some recent works have superficially explored. However, these methods didn't fully unveil the foundation models' potential for this task and only support several simple actions and environments. In this paper, we for the first time, without any motion data, explore open-set human motion synthesis using natural language instructions as user control signals based on MLLMs across any motion task and environment. Our framework can be split into two stages: 1) sequential keyframe generation by utilizing MLLMs as a keyframe designer and animator; 2) motion filling between keyframes through interpolation and motion tracking. Our method can achieve general human motion synthesis for many downstream tasks. The promising results demonstrate the worth of mocap-free human motion synthesis aided by MLLMs and pave the way for future research.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Zhang, Zhikai and Li, Yitang and Huang, Haofeng and Lin, Mingxian and Yi, Li},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10740 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_duoduo_2024,
	title = {Duoduo {CLIP}: {Efficient} {3D} {Understanding} with {Multi}-{View} {Images}},
	shorttitle = {Duoduo {CLIP}},
	url = {http://arxiv.org/abs/2406.11579},
	doi = {10.48550/arXiv.2406.11579},
	abstract = {We introduce Duoduo CLIP, a model for 3D representation learning that learns shape encodings from multi-view images instead of point-clouds. The choice of multi-view images allows us to leverage 2D priors from off-the-shelf CLIP models to facilitate fine-tuning with 3D data. Our approach not only shows better generalization compared to existing point cloud methods, but also reduces GPU requirements and training time. In addition, we modify the model with cross-view attention to leverage information across multiple frames of the object which further boosts performance. Compared to the current SOTA point cloud method that requires 480 A100 hours to train 1 billion model parameters we only require 57 A5000 hours and 87 million parameters. Multi-view images also provide more flexibility in use cases compared to point clouds. This includes being able to encode objects with a variable number of images, with better performance when more views are used. This is in contrast to point cloud based methods, where an entire scan or model of an object is required. We showcase this flexibility with object retrieval from images of real-world objects. Our model also achieves better performance in more fine-grained text to shape retrieval, demonstrating better text-and-shape alignment than point cloud based models.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Lee, Han-Hung and Zhang, Yiming and Chang, Angel X.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11579 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kitouni_fingertip_2024,
	title = {Fingertip {Contact} {Force} {Direction} {Control} using {Tactile} {Feedback}},
	url = {http://arxiv.org/abs/2406.11545},
	doi = {10.48550/arXiv.2406.11545},
	abstract = {The human hand is an immensely sophisticated tool adept at manipulating and grasping objects of unknown characteristics. Its capability lies in perceiving interaction dynamics through touch and adjusting contact force direction and magnitude to ensure successful manipulation. Despite advancements in control algorithms, sensing technologies, compliance integration, and ongoing research, precise finger force control for dexterous manipulation using tactile sensing remains relatively unexplored.In this work, we explore the challenges related to individual finger contact force control and propose a method for directing such forces perceived through tactile sensing. The proposed method is evaluated using an Allegro hand with Xela tactile sensors. Results are presented and discussed, alongside consideration for potential future improvements.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Kitouni, Dounia and Chelly, Elie and Khoramshahi, Mahdi and Perdereau, Veronique},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11545 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{huang_imagination_2024,
	title = {Imagination {Policy}: {Using} {Generative} {Point} {Cloud} {Models} for {Learning} {Manipulation} {Policies}},
	shorttitle = {Imagination {Policy}},
	url = {http://arxiv.org/abs/2406.11740},
	doi = {10.48550/arXiv.2406.11740},
	abstract = {Humans can imagine goal states during planning and perform actions to match those goals. In this work, we propose Imagination Policy, a novel multi-task key-frame policy network for solving high-precision pick and place tasks. Instead of learning actions directly, Imagination Policy generates point clouds to imagine desired states which are then translated to actions using rigid action estimation. This transforms action inference into a local generative task. We leverage pick and place symmetries underlying the tasks in the generation process and achieve extremely high sample efficiency and generalizability to unseen configurations. Finally, we demonstrate state-of-the-art performance across various tasks on the RLbench benchmark compared with several strong baselines.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Huang, Haojie and Schmeckpeper, Karl and Wang, Dian and Biza, Ondrej and Qian, Yaoyao and Liu, Haotian and Jia, Mingxi and Platt, Robert and Walters, Robin},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11740 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ren_l4gm_2024,
	title = {{L4GM}: {Large} {4D} {Gaussian} {Reconstruction} {Model}},
	shorttitle = {{L4GM}},
	url = {http://arxiv.org/abs/2406.10324},
	doi = {10.48550/arXiv.2406.10324},
	abstract = {We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input -- in a single feed-forward pass that takes only a second. Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. We keep our L4GM simple for scalability and build directly on top of LGM, a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input. L4GM outputs a per-frame 3D Gaussian Splatting representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. We showcase that L4GM that is only trained on synthetic data generalizes extremely well on in-the-wild videos, producing high quality animated 3D assets.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Ren, Jiawei and Xie, Kevin and Mirzaei, Ashkan and Liang, Hanxue and Zeng, Xiaohui and Kreis, Karsten and Liu, Ziwei and Torralba, Antonio and Fidler, Sanja and Kim, Seung Wook and Ling, Huan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10324 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_leveraging_2024,
	title = {Leveraging {Locality} to {Boost} {Sample} {Efficiency} in {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2406.10615},
	doi = {10.48550/arXiv.2406.10615},
	abstract = {Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-action locality, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models. Project website: http://sgrv2-robot.github.io},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Zhang, Tong and Hu, Yingdong and You, Jiacheng and Gao, Yang},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yuan_robopoint_2024,
	title = {{RoboPoint}: {A} {Vision}-{Language} {Model} for {Spatial} {Affordance} {Prediction} for {Robotics}},
	shorttitle = {{RoboPoint}},
	url = {http://arxiv.org/abs/2406.10721},
	doi = {10.48550/arXiv.2406.10721},
	abstract = {From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8\% in the accuracy of predicting spatial affordance and by 30.5\% in the success rate of downstream tasks. Project website: https://robo-point.github.io.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Yuan, Wentao and Duan, Jiafei and Blukis, Valts and Pumacay, Wilbert and Krishna, Ranjay and Murali, Adithyavairavan and Mousavian, Arsalan and Fox, Dieter},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{fu_humanplus_2024,
	title = {{HumanPlus}: {Humanoid} {Shadowing} and {Imitation} from {Humans}},
	shorttitle = {{HumanPlus}},
	url = {http://arxiv.org/abs/2406.10454},
	doi = {10.48550/arXiv.2406.10454},
	abstract = {One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100\% success rates using up to 40 demonstrations. Project website: https://humanoid-ai.github.io/},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Fu, Zipeng and Zhao, Qingqing and Wu, Qi and Wetzstein, Gordon and Finn, Chelsea},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10454 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{ma_imagenet3d_2024,
	title = {{ImageNet3D}: {Towards} {General}-{Purpose} {Object}-{Level} {3D} {Understanding}},
	shorttitle = {{ImageNet3D}},
	url = {http://arxiv.org/abs/2406.09613},
	doi = {10.48550/arXiv.2406.09613},
	abstract = {A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning.. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Ma, Wufei and Zeng, Guanning and Zhang, Guofeng and Liu, Qihao and Zhang, Letian and Kortylewski, Adam and Liu, Yaoyao and Yuille, Alan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09613 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ma_nymeria_2024,
	title = {Nymeria: {A} {Massive} {Collection} of {Multimodal} {Egocentric} {Daily} {Motion} in the {Wild}},
	shorttitle = {Nymeria},
	url = {http://arxiv.org/abs/2406.09905},
	doi = {10.48550/arXiv.2406.09905},
	abstract = {We introduce Nymeria - a large-scale, diverse, richly annotated human motion dataset collected in the wild with multiple multimodal egocentric devices. The dataset comes with a) full-body 3D motion ground truth; b) egocentric multimodal recordings from Project Aria devices with RGB, grayscale, eye-tracking cameras, IMUs, magnetometer, barometer, and microphones; and c) an additional "observer" device providing a third-person viewpoint. We compute world-aligned 6DoF transformations for all sensors, across devices and capture sessions. The dataset also provides 3D scene point clouds and calibrated gaze estimation. We derive a protocol to annotate hierarchical language descriptions of in-context human motion, from fine-grain pose narrations, to atomic actions and activity summarization. To the best of our knowledge, the Nymeria dataset is the world largest in-the-wild collection of human motion with natural and diverse activities; first of its kind to provide synchronized and localized multi-device multimodal egocentric data; and the world largest dataset with motion-language descriptions. It contains 1200 recordings of 300 hours of daily activities from 264 participants across 50 locations, travelling a total of 399Km. The motion-language descriptions provide 310.5K sentences in 8.64M words from a vocabulary size of 6545. To demonstrate the potential of the dataset we define key research tasks for egocentric body tracking, motion synthesis, and action recognition and evaluate several state-of-the-art baseline algorithms. Data and code will be open-sourced.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Ma, Lingni and Ye, Yuting and Hong, Fangzhou and Guzov, Vladimir and Jiang, Yifeng and Postyeni, Rowan and Pesqueira, Luis and Gamino, Alexander and Baiyya, Vijay and Kim, Hyo Jin and Bailey, Kevin and Fosas, David Soriano and Liu, C. Karen and Liu, Ziwei and Engel, Jakob and De Nardi, Renzo and Newcombe, Richard},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09905 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{yu_bikc_2024,
	title = {{BiKC}: {Keypose}-{Conditioned} {Consistency} {Policy} for {Bimanual} {Robotic} {Manipulation}},
	shorttitle = {{BiKC}},
	url = {http://arxiv.org/abs/2406.10093},
	doi = {10.48550/arXiv.2406.10093},
	abstract = {Bimanual manipulation tasks typically involve multiple stages which require efficient interactions between two arms, posing step-wise and stage-wise challenges for imitation learning systems. Specifically, failure and delay of one step will broadcast through time, hinder success and efficiency of each sub-stage task, and thereby overall task performance. Although recent works have made strides in addressing certain challenges, few approaches explicitly consider the multi-stage nature of bimanual tasks while simultaneously emphasizing the importance of inference speed. In this paper, we introduce a novel keypose-conditioned consistency policy tailored for bimanual manipulation. It is a hierarchical imitation learning framework that consists of a high-level keypose predictor and a low-level trajectory generator. The predicted keyposes provide guidance for trajectory generation and also mark the completion of one sub-stage task. The trajectory generator is designed as a consistency model trained from scratch without distillation, which generates action sequences conditioning on current observations and predicted keyposes with fast inference speed. Simulated and real-world experimental results demonstrate that the proposed approach surpasses baseline methods in terms of success rate and operational efficiency.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Yu, Dongjie and Xu, Hang and Chen, Yizhou and Ren, Yi and Pan, Jia},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ma_contrastive_2024,
	title = {Contrastive {Imitation} {Learning} for {Language}-guided {Multi}-{Task} {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2406.09738},
	doi = {10.48550/arXiv.2406.09738},
	abstract = {Developing robots capable of executing various manipulation tasks, guided by natural language instructions and visual observations of intricate real-world environments, remains a significant challenge in robotics. Such robot agents need to understand linguistic commands and distinguish between the requirements of different tasks. In this work, we present Sigma-Agent, an end-to-end imitation learning agent for multi-task robotic manipulation. Sigma-Agent incorporates contrastive Imitation Learning (contrastive IL) modules to strengthen vision-language and current-future representations. An effective and efficient multi-view querying Transformer (MVQ-Former) for aggregating representative semantic information is introduced. Sigma-Agent shows substantial improvement over state-of-the-art methods under diverse settings in 18 RLBench tasks, surpassing RVT by an average of 5.2\% and 5.9\% in 10 and 100 demonstration training, respectively. Sigma-Agent also achieves 62\% success rate with a single policy in 5 real-world manipulation tasks. The code will be released upon acceptance.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Ma, Teli and Zhou, Jiaming and Wang, Zifan and Qiu, Ronghe and Liang, Junwei},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09738 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yoo_neural_2024,
	title = {Neural {Pose} {Representation} {Learning} for {Generating} and {Transferring} {Non}-{Rigid} {Object} {Poses}},
	url = {http://arxiv.org/abs/2406.09728},
	doi = {10.48550/arXiv.2406.09728},
	abstract = {We propose a novel method for learning representations of poses for 3D deformable objects, which specializes in 1) disentangling pose information from the object's identity, 2) facilitating the learning of pose variations, and 3) transferring pose information to other object identities. Based on these properties, our method enables the generation of 3D deformable objects with diversity in both identities and poses, using variations of a single object. It does not require explicit shape parameterization such as skeletons or joints, point-level or shape-level correspondence supervision, or variations of the target object for pose transfer. To achieve pose disentanglement, compactness for generative models, and transferability, we first design the pose extractor to represent the pose as a keypoint-based hybrid representation and the pose applier to learn an implicit deformation field. To better distill pose information from the object's geometry, we propose the implicit pose applier to output an intrinsic mesh property, the face Jacobian. Once the extracted pose information is transferred to the target object, the pose applier is fine-tuned in a self-supervised manner to better describe the target object's shapes with pose variations. The extracted poses are also used to train a cascaded diffusion model to enable the generation of novel poses. Our experiments with the DeformThings4D and Human datasets demonstrate state-of-the-art performance in pose transfer and the ability to generate diverse deformed shapes with various objects and poses.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Yoo, Seungwoo and Koo, Juil and Yeo, Kyeongmin and Sung, Minhyuk},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09728 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{banerjee_introducing_2024,
	title = {Introducing {HOT3D}: {An} {Egocentric} {Dataset} for {3D} {Hand} and {Object} {Tracking}},
	shorttitle = {Introducing {HOT3D}},
	url = {http://arxiv.org/abs/2406.09598},
	doi = {10.48550/arXiv.2406.09598},
	abstract = {We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up/observe/put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR/AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. We aim to accelerate research on egocentric hand-object interaction by making the HOT3D dataset publicly available and by co-organizing public challenges on the dataset at ECCV 2024. The dataset can be downloaded from the project website: https://facebookresearch.github.io/hot3d/.},
	urldate = {2024-06-23},
	publisher = {arXiv},
	author = {Banerjee, Prithviraj and Shkodrani, Sindi and Moulon, Pierre and Hampali, Shreyas and Zhang, Fan and Fountain, Jade and Miller, Edward and Basol, Selen and Newcombe, Richard and Wang, Robert and Engel, Jakob Julian and Hodan, Tomas},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09598 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xing_contrastive_2024,
	title = {Contrastive {Learning} for {Enhancing} {Robust} {Scene} {Transfer} in {Vision}-based {Agile} {Flight}},
	url = {http://arxiv.org/abs/2309.09865},
	doi = {10.48550/arXiv.2309.09865},
	abstract = {Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines.},
	urldate = {2024-06-22},
	publisher = {arXiv},
	author = {Xing, Jiaxu and Bauersfeld, Leonard and Song, Yunlong and Xing, Chunwei and Scaramuzza, Davide},
	month = feb,
	year = {2024},
	note = {arXiv:2309.09865 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{lea_temporal_2016,
	title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}},
	shorttitle = {Temporal {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.08242},
	doi = {10.48550/arXiv.1608.08242},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
	urldate = {2024-06-22},
	publisher = {arXiv},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	month = aug,
	year = {2016},
	note = {arXiv:1608.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ajay_is_2023,
	title = {Is {Conditional} {Generative} {Modeling} all you need for {Decision}-{Making}?},
	url = {http://arxiv.org/abs/2211.15657},
	doi = {10.48550/arXiv.2211.15657},
	abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
	month = jul,
	year = {2023},
	note = {arXiv:2211.15657 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{xie_lrm-zero_2024,
	title = {{LRM}-{Zero}: {Training} {Large} {Reconstruction} {Models} with {Synthesized} {Data}},
	shorttitle = {{LRM}-{Zero}},
	url = {http://arxiv.org/abs/2406.09371},
	doi = {10.48550/arXiv.2406.09371},
	abstract = {We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Xie, Desai and Bi, Sai and Shu, Zhixin and Zhang, Kai and Xu, Zexiang and Zhou, Yi and Pirk, Sören and Kaufman, Arie and Sun, Xin and Tan, Hao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09371 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{goyal_rvt-2_2024,
	title = {{RVT}-2: {Learning} {Precise} {Manipulation} from {Few} {Demonstrations}},
	shorttitle = {{RVT}-2},
	url = {http://arxiv.org/abs/2406.08545},
	doi = {10.48550/arXiv.2406.08545},
	abstract = {In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65\% to 82\%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Goyal, Ankit and Blukis, Valts and Xu, Jie and Guo, Yijie and Chao, Yu-Wei and Fox, Dieter},
	month = jun,
	year = {2024},
	note = {arXiv:2406.08545 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kim_openvla_2024,
	title = {{OpenVLA}: {An} {Open}-{Source} {Vision}-{Language}-{Action} {Model}},
	shorttitle = {{OpenVLA}},
	url = {http://arxiv.org/abs/2406.09246},
	doi = {10.48550/arXiv.2406.09246},
	abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09246 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{murali_touch_2023,
	title = {Touch if it's transparent! {ACTOR}: {Active} {Tactile}-based {Category}-{Level} {Transparent} {Object} {Reconstruction}},
	shorttitle = {Touch if it's transparent! {ACTOR}},
	url = {http://arxiv.org/abs/2307.16254},
	doi = {10.48550/arXiv.2307.16254},
	abstract = {Accurate shape reconstruction of transparent objects is a challenging task due to their non-Lambertian surfaces and yet necessary for robots for accurate pose perception and safe manipulation. As vision-based sensing can produce erroneous measurements for transparent objects, the tactile modality is not sensitive to object transparency and can be used for reconstructing the object's shape. We propose ACTOR, a novel framework for ACtive tactile-based category-level Transparent Object Reconstruction. ACTOR leverages large datasets of synthetic object with our proposed self-supervised learning approach for object shape reconstruction as the collection of real-world tactile data is prohibitively expensive. ACTOR can be used during inference with tactile data from category-level unknown transparent objects for reconstruction. Furthermore, we propose an active-tactile object exploration strategy as probing every part of the object surface can be sample inefficient. We also demonstrate tactile-based category-level object pose estimation task using ACTOR. We perform an extensive evaluation of our proposed methodology with real-world robotic experiments with comprehensive comparison studies with state-of-the-art approaches. Our proposed method outperforms these approaches in terms of tactile-based object reconstruction and object pose estimation.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Murali, Prajval Kumar and Porr, Bernd and Kaboli, Mohsen},
	month = jul,
	year = {2023},
	note = {arXiv:2307.16254 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{bhardwaj_storm_2022,
	title = {{STORM}: {An} {Integrated} {Framework} for {Fast} {Joint}-{Space} {Model}-{Predictive} {Control} for {Reactive} {Manipulation}},
	shorttitle = {{STORM}},
	url = {https://proceedings.mlr.press/v164/bhardwaj22a.html},
	abstract = {Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms (125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc},
	language = {en},
	urldate = {2024-06-20},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Bhardwaj, Mohak and Sundaralingam, Balakumar and Mousavian, Arsalan and Ratliff, Nathan D. and Fox, Dieter and Ramos, Fabio and Boots, Byron},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {750--759},
}

@misc{he_omnih2o_2024,
	title = {{OmniH2O}: {Universal} and {Dexterous} {Human}-to-{Humanoid} {Whole}-{Body} {Teleoperation} and {Learning}},
	shorttitle = {{OmniH2O}},
	url = {http://arxiv.org/abs/2406.08858},
	doi = {10.48550/arXiv.2406.08858},
	abstract = {We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {He, Tairan and Luo, Zhengyi and He, Xialin and Xiao, Wenli and Zhang, Chong and Zhang, Weinan and Kitani, Kris and Liu, Changliu and Shi, Guanya},
	month = jun,
	year = {2024},
	note = {arXiv:2406.08858 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{liang_robo360_2023,
	title = {Robo360: {A} {3D} {Omnispective} {Multi}-{Material} {Robotic} {Manipulation} {Dataset}},
	shorttitle = {Robo360},
	url = {http://arxiv.org/abs/2312.06686},
	doi = {10.48550/arXiv.2312.06686},
	abstract = {Building robots that can automate labor-intensive tasks has long been the core motivation behind the advancements in computer vision and the robotics community. Recent interest in leveraging 3D algorithms, particularly neural fields, has led to advancements in robot perception and physical understanding in manipulation scenarios. However, the real world's complexity poses significant challenges. To tackle these challenges, we present Robo360, a dataset that features robotic manipulation with a dense view coverage, which enables high-quality 3D neural representation learning, and a diverse set of objects with various physical and optical properties and facilitates research in various object manipulation and physical world modeling tasks. We confirm the effectiveness of our dataset using existing dynamic NeRF and evaluate its potential in learning multi-view policies. We hope that Robo360 can open new research directions yet to be explored at the intersection of understanding the physical world in 3D and robot control.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Liang, Litian and Bian, Liuyu and Xiao, Caiwei and Zhang, Jialin and Chen, Linghao and Liu, Isabella and Xiang, Fanbo and Huang, Zhiao and Su, Hao},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06686 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{huang_ptt_2024,
	title = {{PTT}: {Point}-{Trajectory} {Transformer} for {Efficient} {Temporal} {3D} {Object} {Detection}},
	shorttitle = {{PTT}},
	url = {http://arxiv.org/abs/2312.08371},
	doi = {10.48550/arXiv.2312.08371},
	abstract = {Recent temporal LiDAR-based 3D object detectors achieve promising performance based on the two-stage proposal-based approach. They generate 3D box candidates from the first-stage dense detector, followed by different temporal aggregation methods. However, these approaches require per-frame objects or whole point clouds, posing challenges related to memory bank utilization. Moreover, point clouds and trajectory features are combined solely based on concatenation, which may neglect effective interactions between them. In this paper, we propose a point-trajectory transformer with long short-term memory for efficient temporal 3D object detection. To this end, we only utilize point clouds of current-frame objects and their historical trajectories as input to minimize the memory bank storage requirement. Furthermore, we introduce modules to encode trajectory features, focusing on long short-term and future-aware perspectives, and then effectively aggregate them with point cloud features. We conduct extensive experiments on the large-scale Waymo dataset to demonstrate that our approach performs well against state-of-the-art methods. Code and models will be made publicly available at https://github.com/kuanchihhuang/PTT.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Huang, Kuan-Chih and Lyu, Weijie and Yang, Ming-Hsuan and Tsai, Yi-Hsuan},
	month = apr,
	year = {2024},
	note = {arXiv:2312.08371 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mandi_real2code_2024,
	title = {{Real2Code}: {Reconstruct} {Articulated} {Objects} via {Code} {Generation}},
	shorttitle = {{Real2Code}},
	url = {http://arxiv.org/abs/2406.08474},
	doi = {10.48550/arXiv.2406.08474},
	abstract = {We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model. We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Mandi, Zhao and Weng, Yijia and Bauer, Dominik and Song, Shuran},
	month = jun,
	year = {2024},
	note = {arXiv:2406.08474 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_scaling_2024,
	title = {Scaling {Manipulation} {Learning} with {Visual} {Kinematic} {Chain} {Prediction}},
	url = {http://arxiv.org/abs/2406.07837},
	doi = {10.48550/arXiv.2406.07837},
	abstract = {Learning general-purpose models from diverse datasets has achieved great success in machine learning. In robotics, however, existing methods in multi-task learning are typically constrained to a single robot and workspace, while recent work such as RT-X requires a non-trivial action normalization procedure to manually bridge the gap between different action spaces in diverse environments. In this paper, we propose the visual kinematics chain as a precise and universal representation of quasi-static actions for robot learning over diverse environments, which requires no manual adjustment since the visual kinematic chains can be automatically obtained from the robot's model and camera parameters. We propose the Visual Kinematics Transformer (VKT), a convolution-free architecture that supports an arbitrary number of camera viewpoints, and that is trained with a single objective of forecasting kinematic structures through optimal point-set matching. We demonstrate the superior performance of VKT over BC transformers as a general agent on Calvin, RLBench, Open-X, and real robot manipulation tasks. Video demonstrations can be found at https://mlzxy.github.io/visual-kinetic-chain.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Zhang, Xinyu and Liu, Yuhan and Chang, Haonan and Boularias, Abdeslam},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhao_im_2023,
	title = {I'{M} {HOI}: {Inertia}-aware {Monocular} {Capture} of {3D} {Human}-{Object} {Interactions}},
	shorttitle = {I'{M} {HOI}},
	url = {https://arxiv.org/abs/2312.08869v2},
	abstract = {We are living in a world surrounded by diverse and "smart" devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.},
	language = {en},
	urldate = {2024-06-17},
	author = {Zhao, Chengfeng and Zhang, Juze and Du, Jiashen and Shan, Ziwei and Wang, Junye and Yu, Jingyi and Wang, Jingya and Xu, Lan},
	month = dec,
	year = {2023},
}

@misc{huang_a3vlm_2024,
	title = {{A3VLM}: {Actionable} {Articulation}-{Aware} {Vision} {Language} {Model}},
	shorttitle = {{A3VLM}},
	url = {http://arxiv.org/abs/2406.07549},
	doi = {10.48550/arXiv.2406.07549},
	abstract = {Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation. However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM have focused on directly learning robot-centric actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM. We release our code and other materials at https://github.com/changhaonan/A3VLM.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Huang, Siyuan and Chang, Haonan and Liu, Yuhan and Zhu, Yimeng and Dong, Hao and Gao, Peng and Boularias, Abdeslam and Li, Hongsheng},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07549 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yang_depth_2024,
	title = {Depth {Anything} {V2}},
	url = {http://arxiv.org/abs/2406.09414},
	doi = {10.48550/arXiv.2406.09414},
	abstract = {This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09414 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_ho-cap_2024,
	title = {{HO}-{Cap}: {A} {Capture} {System} and {Dataset} for {3D} {Reconstruction} and {Pose} {Tracking} of {Hand}-{Object} {Interaction}},
	shorttitle = {{HO}-{Cap}},
	url = {http://arxiv.org/abs/2406.06843},
	doi = {10.48550/arXiv.2406.06843},
	abstract = {We introduce a data capture system and a new dataset named HO-Cap that can be used to study 3D reconstruction and pose tracking of hands and objects in videos. The capture system uses multiple RGB-D cameras and a HoloLens headset for data collection, avoiding the use of expensive 3D scanners or mocap systems. We propose a semi-automatic method to obtain annotations of shape and pose of hands and objects in the collected videos, which significantly reduces the required annotation time compared to manual labeling. With this system, we captured a video dataset of humans using objects to perform different tasks, as well as simple pick-and-place and handover of an object from one hand to the other, which can be used as human demonstrations for embodied AI and robot manipulation research. Our data capture setup and annotation framework can be used by the community to reconstruct 3D shapes of objects and human hands and track their poses in videos.},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Wang, Jikai and Zhang, Qifan and Chao, Yu-Wei and Wen, Bowen and Guo, Xiaohu and Xiang, Yu},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06843 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{goyal_rvt_2023,
	title = {{RVT}: {Robotic} {View} {Transformer} for {3D} {Object} {Manipulation}},
	shorttitle = {{RVT}},
	url = {http://arxiv.org/abs/2306.14896},
	doi = {10.48550/arXiv.2306.14896},
	abstract = {For 3D object manipulation, methods that build an explicit 3D representation perform better than those relying only on camera images. But using explicit 3D representations like voxels comes at large computing cost, adversely affecting scalability. In this work, we propose RVT, a multi-view transformer for 3D manipulation that is both scalable and accurate. Some key features of RVT are an attention mechanism to aggregate information across views and re-rendering of the camera input from virtual views around the robot workspace. In simulations, we find that a single RVT model works well across 18 RLBench tasks with 249 task variations, achieving 26\% higher relative success than the existing state-of-the-art method (PerAct). It also trains 36X faster than PerAct for achieving the same performance and achieves 2.3X the inference speed of PerAct. Further, RVT can perform a variety of manipulation tasks in the real world with just a few (\${\textbackslash}sim\$10) demonstrations per task. Visual results, code, and trained model are provided at https://robotic-view-transformer.github.io/.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Goyal, Ankit and Xu, Jie and Guo, Yijie and Blukis, Valts and Chao, Yu-Wei and Fox, Dieter},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14896 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yu_image_2024,
	title = {An {Image} is {Worth} 32 {Tokens} for {Reconstruction} and {Generation}},
	url = {http://arxiv.org/abs/2406.07550},
	doi = {10.48550/arXiv.2406.07550},
	abstract = {Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 x 256 x 3 image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64x, leading to 410x faster generation process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples 74x faster.},
	urldate = {2024-06-15},
	publisher = {arXiv},
	author = {Yu, Qihang and Weber, Mark and Deng, Xueqing and Shen, Xiaohui and Cremers, Daniel and Chen, Liang-Chieh},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07550 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deshpande_data_2024,
	title = {Data {Efficient} {Behavior} {Cloning} for {Fine} {Manipulation} via {Continuity}-based {Corrective} {Labels}},
	url = {http://arxiv.org/abs/2405.19307},
	doi = {10.48550/arXiv.2405.19307},
	abstract = {We consider imitation learning with access only to expert demonstrations, whose real-world application is often limited by covariate shift due to compounding errors during execution. We investigate the effectiveness of the Continuity-based Corrective Labels for Imitation Learning (CCIL) framework in mitigating this issue for real-world fine manipulation tasks. CCIL generates corrective labels by learning a locally continuous dynamics model from demonstrations to guide the agent back toward expert states. Through extensive experiments on peg insertion and fine grasping, we provide the first empirical validation that CCIL can significantly improve imitation learning performance despite discontinuities present in contact-rich manipulation. We find that: (1) real-world manipulation exhibits sufficient local smoothness to apply CCIL, (2) generated corrective labels are most beneficial in low-data regimes, and (3) label filtering based on estimated dynamics model error enables performance gains. To effectively apply CCIL to robotic domains, we offer a practical instantiation of the framework and insights into design choices and hyperparameter selection. Our work demonstrates CCIL's practicality for alleviating compounding errors in imitation learning on physical robots.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Deshpande, Abhay and Ke, Liyiming and Pfeifer, Quinn and Gupta, Abhishek and Srinivasa, Siddhartha S.},
	month = jun,
	year = {2024},
	note = {arXiv:2405.19307 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yang_3d-grand_2024,
	title = {{3D}-{GRAND}: {Towards} {Better} {Grounding} and {Less} {Hallucination} for {3D}-{LLMs}},
	shorttitle = {{3D}-{GRAND}},
	url = {http://arxiv.org/abs/2406.05132},
	doi = {10.48550/arXiv.2406.05132},
	abstract = {The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research. Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs. Project website: https://3d-grand.github.io},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Yang, Jianing and Chen, Xuweiyi and Madaan, Nikhil and Iyengar, Madhavan and Qian, Shengyi and Fouhey, David F. and Chai, Joyce},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_omni6dpose_2024,
	title = {{Omni6DPose}: {A} {Benchmark} and {Model} for {Universal} {6D} {Object} {Pose} {Estimation} and {Tracking}},
	shorttitle = {{Omni6DPose}},
	url = {http://arxiv.org/abs/2406.04316},
	doi = {10.48550/arXiv.2406.04316},
	abstract = {6D Object Pose Estimation is a crucial yet challenging task in computer vision, suffering from a significant lack of large-scale datasets. This scarcity impedes comprehensive evaluation of model performance, limiting research advancements. Furthermore, the restricted number of available instances or categories curtails its applications. To address these issues, this paper introduces Omni6DPose, a substantial dataset characterized by its diversity in object categories, large scale, and variety in object materials. Omni6DPose is divided into three main components: ROPE (Real 6D Object Pose Estimation Dataset), which includes 332K images annotated with over 1.5M annotations across 581 instances in 149 categories; SOPE(Simulated 6D Object Pose Estimation Dataset), consisting of 475K images created in a mixed reality setting with depth simulation, annotated with over 5M annotations across 4162 instances in the same 149 categories; and the manually aligned real scanned objects used in both ROPE and SOPE. Omni6DPose is inherently challenging due to the substantial variations and ambiguities. To address this challenge, we introduce GenPose++, an enhanced version of the SOTA category-level pose estimation framework, incorporating two pivotal improvements: Semantic-aware feature extraction and Clustering-based aggregation. Moreover, we provide a comprehensive benchmarking analysis to evaluate the performance of previous methods on this large-scale dataset in the realms of 6D object pose estimation and pose tracking.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Zhang, Jiyao and Huang, Weiyao and Peng, Bo and Wu, Mingdong and Hu, Fei and Chen, Zijian and Zhao, Bo and Dong, Hao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04316 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cheng_touch100k_2024,
	title = {Touch100k: {A} {Large}-{Scale} {Touch}-{Language}-{Vision} {Dataset} for {Touch}-{Centric} {Multimodal} {Representation}},
	shorttitle = {Touch100k},
	url = {http://arxiv.org/abs/2406.03813},
	doi = {10.48550/arXiv.2406.03813},
	abstract = {Touch holds a pivotal position in enhancing the perceptual and interactive capabilities of both humans and robots. Despite its significance, current tactile research mainly focuses on visual and tactile modalities, overlooking the language domain. Inspired by this, we construct Touch100k, a paired touch-language-vision dataset at the scale of 100k, featuring tactile sensation descriptions in multiple granularities (i.e., sentence-level natural expressions with rich semantics, including contextual and dynamic relationships, and phrase-level descriptions capturing the key features of tactile sensations). Based on the dataset, we propose a pre-training method, Touch-Language-Vision Representation Learning through Curriculum Linking (TLV-Link, for short), inspired by the concept of curriculum learning. TLV-Link aims to learn a tactile representation for the GelSight sensor and capture the relationship between tactile, language, and visual modalities. We evaluate our representation's performance across two task categories (namely, material property identification and robot grasping prediction), focusing on tactile representation and zero-shot touch understanding. The experimental evaluation showcases the effectiveness of our representation. By enabling TLV-Link to achieve substantial improvements and establish a new state-of-the-art in touch-centric multimodal representation learning, Touch100k demonstrates its value as a valuable resource for research. Project page: https://cocacola-lab.github.io/Touch100k/.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Cheng, Ning and Guan, Changhao and Gao, Jing and Wang, Weihao and Li, You and Meng, Fandong and Zhou, Jie and Fang, Bin and Xu, Jinan and Han, Wenjuan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.03813 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{xiong_event3dgs_2024,
	title = {{Event3DGS}: {Event}-based {3D} {Gaussian} {Splatting} for {Fast} {Egomotion}},
	shorttitle = {{Event3DGS}},
	url = {http://arxiv.org/abs/2406.02972},
	doi = {10.48550/arXiv.2406.02972},
	abstract = {The recent emergence of 3D Gaussian splatting (3DGS) leverages the advantage of explicit point-based representations, which significantly improves the rendering speed and quality of novel-view synthesis. However, 3D radiance field rendering in environments with high-dynamic motion or challenging illumination condition remains problematic in real-world robotic tasks. The reason is that fast egomotion is prevalent real-world robotic tasks, which induces motion blur, leading to inaccuracies and artifacts in the reconstructed structure. To alleviate this problem, we propose Event3DGS, the first method that learns Gaussian Splatting solely from raw event streams. By exploiting the high temporal resolution of event cameras and explicit point-based representation, Event3DGS can reconstruct high-fidelity 3D structures solely from the event streams under fast egomotion. Our sparsity-aware sampling and progressive training approaches allow for better reconstruction quality and consistency. To further enhance the fidelity of appearance, we explicitly incorporate the motion blur formation process into a differentiable rasterizer, which is used with a limited set of blurred RGB images to refine the appearance. Extensive experiments on multiple datasets validate the superior rendering quality of Event3DGS compared with existing approaches, with over 95\% lower training time and faster rendering speed in orders of magnitude.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Xiong, Tianyi and Wu, Jiayi and He, Botao and Fermuller, Cornelia and Aloimonos, Yiannis and Huang, Heng and Metzler, Christopher A.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02972 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{nasiriany_robocasa_2024,
	title = {{RoboCasa}: {Large}-{Scale} {Simulation} of {Everyday} {Tasks} for {Generalist} {Robots}},
	shorttitle = {{RoboCasa}},
	url = {http://arxiv.org/abs/2406.02523},
	doi = {10.48550/arXiv.2406.02523},
	abstract = {Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Nasiriany, Soroush and Maddukuri, Abhiram and Zhang, Lance and Parikh, Adeet and Lo, Aaron and Joshi, Abhishek and Mandlekar, Ajay and Zhu, Yuke},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02523 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_camco_2024,
	title = {{CamCo}: {Camera}-{Controllable} {3D}-{Consistent} {Image}-to-{Video} {Generation}},
	shorttitle = {{CamCo}},
	url = {http://arxiv.org/abs/2406.02509},
	doi = {10.48550/arXiv.2406.02509},
	abstract = {Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl{\textbackslash}"ucker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: https://ir1d.github.io/CamCo/},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Xu, Dejia and Nie, Weili and Liu, Chao and Liu, Sifei and Kautz, Jan and Wang, Zhangyang and Vahdat, Arash},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02509 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zeng_learning_2024,
	title = {Learning {Manipulation} by {Predicting} {Interaction}},
	url = {http://arxiv.org/abs/2406.00439},
	doi = {10.48550/arXiv.2406.00439},
	abstract = {Representation learning approaches for robotic manipulation have boomed in recent years. Due to the scarcity of in-domain robot data, prevailing methodologies tend to leverage large-scale human video datasets to extract generalizable features for visuomotor policy learning. Despite the progress achieved, prior endeavors disregard the interactive dynamics that capture behavior patterns and physical interaction during the manipulation process, resulting in an inadequate understanding of the relationship between objects and the environment. To this end, we propose a general pre-training pipeline that learns Manipulation by Predicting the Interaction (MPI) and enhances the visual representation.Given a pair of keyframes representing the initial and final states, along with language instructions, our algorithm predicts the transition frame and detects the interaction object, respectively. These two learning objectives achieve superior comprehension towards "how-to-interact" and "where-to-interact". We conduct a comprehensive evaluation of several challenging robotic tasks.The experimental results demonstrate that MPI exhibits remarkable improvement by 10\% to 64\% compared with previous state-of-the-art in real-world robot platforms as well as simulation environments. Code and checkpoints are publicly shared at https://github.com/OpenDriveLab/MPI.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Zeng, Jia and Bu, Qingwen and Wang, Bangjun and Xia, Wenke and Chen, Li and Dong, Hao and Song, Haoming and Wang, Dong and Hu, Di and Luo, Ping and Cui, Heming and Zhao, Bin and Li, Xuelong and Qiao, Yu and Li, Hongyang},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00439 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{tang_lgm_2024,
	title = {{LGM}: {Large} {Multi}-{View} {Gaussian} {Model} for {High}-{Resolution} {3D} {Content} {Creation}},
	shorttitle = {{LGM}},
	url = {http://arxiv.org/abs/2402.05054},
	doi = {10.48550/arXiv.2402.05054},
	abstract = {3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05054 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yuan_sornet_2022,
	title = {{SORNet}: {Spatial} {Object}-{Centric} {Representations} for {Sequential} {Manipulation}},
	shorttitle = {{SORNet}},
	url = {https://proceedings.mlr.press/v164/yuan22a.html},
	abstract = {Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or end-to-end learning struggle with novel objects or new tasks. In this work, we propose SORNet (Spatial Object-Centric Representation Network), which extracts object-centric representations from RGB images conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize zero-shot to unseen object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.},
	language = {en},
	urldate = {2024-06-10},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Yuan, Wentao and Paxton, Chris and Desingh, Karthik and Fox, Dieter},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {148--157},
}

@misc{sathe_fingertac_2023,
	title = {{FingerTac} -- {An} {Interchangeable} and {Wearable} {Tactile} {Sensor} for the {Fingertips} of {Human} and {Robot} {Hands}},
	url = {https://arxiv.org/abs/2310.09201v1},
	abstract = {Skill transfer from humans to robots is challenging. Presently, many researchers focus on capturing only position or joint angle data from humans to teach the robots. Even though this approach has yielded impressive results for grasping applications, reconstructing motion for object handling or fine manipulation from a human hand to a robot hand has been sparsely explored. Humans use tactile feedback to adjust their motion to various objects, but capturing and reproducing the applied forces is an open research question. In this paper we introduce a wearable fingertip tactile sensor, which captures the distributed 3-axis force vectors on the fingertip. The fingertip tactile sensor is interchangeable between the human hand and the robot hand, meaning that it can also be assembled to fit on a robot hand such as the Allegro hand. This paper presents the structural aspects of the sensor as well as the methodology and approach used to design, manufacture, and calibrate the sensor. The sensor is able to measure forces accurately with a mean absolute error of 0.21, 0.16, and 0.44 Newtons in X, Y, and Z directions, respectively.},
	language = {en},
	urldate = {2024-06-07},
	author = {Sathe, Prathamesh and Schmitz, Alexander and Funabashi, Satoshi and Tomo, Tito Pradhono and Somlor, Sophon and Shigeki, Sugano},
	month = oct,
	year = {2023},
}

@misc{lin_4dhands_2024,
	title = {{4DHands}: {Reconstructing} {Interactive} {Hands} in {4D} with {Transformers}},
	shorttitle = {{4DHands}},
	url = {http://arxiv.org/abs/2405.20330},
	doi = {10.48550/arXiv.2405.20330},
	abstract = {In this paper, we introduce 4DHands, a robust approach to recovering interactive hand meshes and their relative movement from monocular inputs. Our approach addresses two major limitations of previous methods: lacking a unified solution for handling various hand image inputs and neglecting the positional relationship of two hands within images. To overcome these challenges, we develop a transformer-based architecture with novel tokenization and feature fusion strategies. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT) method to embed positional relation information into the hand tokens. In this way, our network can handle both single-hand and two-hand inputs and explicitly leverage relative hand positions, facilitating the reconstruction of intricate hand interactions in real-world scenarios. As such tokenization indicates the relative relationship of two hands, it also supports more effective feature fusion. To this end, we further develop a Spatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D with attention and decode them into 3D hand meshes and relative temporal movements. The efficacy of our approach is validated on several benchmark datasets. The results on in-the-wild videos and real-world scenarios demonstrate the superior performances of our approach for interactive hand reconstruction. More video results can be found on the project page: https://4dhands.github.io.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Lin, Dixuan and Zhang, Yuxiang and Li, Mengcheng and Liu, Yebin and Jing, Wei and Yan, Qi and Wang, Qianying and Zhang, Hongwen},
	month = may,
	year = {2024},
	note = {arXiv:2405.20330 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{zhang_sam-e_2024,
	title = {{SAM}-{E}: {Leveraging} {Visual} {Foundation} {Model} with {Sequence} {Imitation} for {Embodied} {Manipulation}},
	shorttitle = {{SAM}-{E}},
	url = {http://arxiv.org/abs/2405.19586},
	doi = {10.48550/arXiv.2405.19586},
	abstract = {Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot's end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose SAM-E, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Zhang, Junjie and Bai, Chenjia and He, Haoran and Xia, Wenke and Wang, Zhigang and Zhao, Bin and Li, Xiu and Li, Xuelong},
	month = may,
	year = {2024},
	note = {arXiv:2405.19586 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yuan_real-time_2024,
	title = {Real-{Time} {Dynamic} {Robot}-{Assisted} {Hand}-{Object} {Interaction} via {Motion} {Primitives}},
	url = {http://arxiv.org/abs/2405.19531},
	doi = {10.48550/arXiv.2405.19531},
	abstract = {Advances in artificial intelligence (AI) have been propelling the evolution of human-robot interaction (HRI) technologies. However, significant challenges remain in achieving seamless interactions, particularly in tasks requiring physical contact with humans. These challenges arise from the need for accurate real-time perception of human actions, adaptive control algorithms for robots, and the effective coordination between human and robotic movements. In this paper, we propose an approach to enhancing physical HRI with a focus on dynamic robot-assisted hand-object interaction (HOI). Our methodology integrates hand pose estimation, adaptive robot control, and motion primitives to facilitate human-robot collaboration. Specifically, we employ a transformer-based algorithm to perform real-time 3D modeling of human hands from single RGB images, based on which a motion primitives model (MPM) is designed to translate human hand motions into robotic actions. The robot's action implementation is dynamically fine-tuned using the continuously updated 3D hand models. Experimental validations, including a ring-wearing task, demonstrate the system's effectiveness in adapting to real-time movements and assisting in precise task executions.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Yuan, Mingqi and Wang, Huijiang and Chu, Kai-Fung and Iida, Fumiya and Li, Bo and Zeng, Wenjun},
	month = may,
	year = {2024},
	note = {arXiv:2405.19531 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_vision-based_2024,
	title = {Vision-based {Manipulation} from {Single} {Human} {Video} with {Open}-{World} {Object} {Graphs}},
	url = {http://arxiv.org/abs/2405.20321},
	doi = {10.48550/arXiv.2405.20321},
	abstract = {We present an object-centric approach to empower robots to learn vision-based manipulation skills from human videos. We investigate the problem of imitating robot manipulation from a single human video in the open-world setting, where a robot must learn to manipulate novel objects from one video demonstration. We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB-D video and deriving a policy that conditions on the extracted plan. Our method enables the robot to learn from videos captured by daily mobile devices such as an iPad and generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances. We systematically evaluate our method on both short-horizon and long-horizon tasks, demonstrating the efficacy of ORION in learning from a single human video in the open world. Videos can be found in the project website https://ut-austin-rpl.github.io/ORION-release.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Zhu, Yifeng and Lim, Arisrei and Stone, Peter and Zhu, Yuke},
	month = may,
	year = {2024},
	note = {arXiv:2405.20321 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{labe_dgd_2024,
	title = {{DGD}: {Dynamic} {3D} {Gaussians} {Distillation}},
	shorttitle = {{DGD}},
	url = {http://arxiv.org/abs/2405.19321},
	doi = {10.48550/arXiv.2405.19321},
	abstract = {We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Labe, Isaac and Issachar, Noam and Lang, Itai and Benaim, Sagie},
	month = may,
	year = {2024},
	note = {arXiv:2405.19321 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hansen_hierarchical_2024,
	title = {Hierarchical {World} {Models} as {Visual} {Whole}-{Body} {Humanoid} {Controllers}},
	url = {http://arxiv.org/abs/2405.18418},
	doi = {10.48550/arXiv.2405.18418},
	abstract = {Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans. Code and videos: https://nicklashansen.com/rlpuppeteer},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Hansen, Nicklas and S V, Jyothir and Sobal, Vlad and LeCun, Yann and Wang, Xiaolong and Su, Hao},
	month = may,
	year = {2024},
	note = {arXiv:2405.18418 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_gflow_2024,
	title = {{GFlow}: {Recovering} {4D} {World} from {Monocular} {Video}},
	shorttitle = {{GFlow}},
	url = {http://arxiv.org/abs/2405.18426},
	doi = {10.48550/arXiv.2405.18426},
	abstract = {Reconstructing 4D scenes from video inputs is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios. In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses. To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time. GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames. Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content. Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way. Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose. By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power. Visit our project website at: https://littlepure2333.github.io/GFlow},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Wang, Shizun and Yang, Xingyi and Shen, Qiuhong and Jiang, Zhenxiang and Wang, Xinchao},
	month = may,
	year = {2024},
	note = {arXiv:2405.18426 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ahn_vader_2024,
	title = {{VADER}: {Visual} {Affordance} {Detection} and {Error} {Recovery} for {Multi} {Robot} {Human} {Collaboration}},
	shorttitle = {{VADER}},
	url = {http://arxiv.org/abs/2405.16021},
	doi = {10.48550/arXiv.2405.16021},
	abstract = {Robots today can exploit the rich world knowledge of large language models to chain simple behavioral skills into long-horizon tasks. However, robots often get interrupted during long-horizon tasks due to primitive skill failures and dynamic environments. We propose VADER, a plan, execute, detect framework with seeking help as a new skill that enables robots to recover and complete long-horizon tasks with the help of humans or other robots. VADER leverages visual question answering (VQA) modules to detect visual affordances and recognize execution errors. It then generates prompts for a language model planner (LMP) which decides when to seek help from another robot or human to recover from errors in long-horizon task execution. We show the effectiveness of VADER with two long-horizon robotic tasks. Our pilot study showed that VADER is capable of performing complex long-horizon tasks by asking for help from another robot to clear a table. Our user study showed that VADER is capable of performing complex long-horizon tasks by asking for help from a human to clear a path. We gathered feedback from people (N=19) about the performance of the VADER performance vs. a robot that did not ask for help. https://google-vader.github.io/},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Ahn, Michael and Arenas, Montserrat Gonzalez and Bennice, Matthew and Brown, Noah and Chan, Christine and David, Byron and Francis, Anthony and Gonzalez, Gavin and Hessmer, Rainer and Jackson, Tomas and Joshi, Nikhil J. and Lam, Daniel and Lee, Tsang-Wei Edward and Luong, Alex and Maddineni, Sharath and Patel, Harsh and Peralta, Jodilyn and Quiambao, Jornell and Reyes, Diego and Ruano, Rosario M. Jauregui and Sadigh, Dorsa and Sanketi, Pannag and Takayama, Leila and Vodenski, Pavel and Xia, Fei},
	month = may,
	year = {2024},
	note = {arXiv:2405.16021 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liu_part123_2024,
	title = {Part123: {Part}-aware {3D} {Reconstruction} from a {Single}-view {Image}},
	shorttitle = {Part123},
	url = {http://arxiv.org/abs/2405.16888},
	doi = {10.48550/arXiv.2405.16888},
	abstract = {Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Liu, Anran and Lin, Cheng and Liu, Yuan and Long, Xiaoxiao and Dou, Zhiyang and Guo, Hao-Xiang and Luo, Ping and Wang, Wenping},
	month = may,
	year = {2024},
	note = {arXiv:2405.16888 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{kuang_collaborative_2024,
	title = {Collaborative {Video} {Diffusion}: {Consistent} {Multi}-video {Generation} with {Camera} {Control}},
	shorttitle = {Collaborative {Video} {Diffusion}},
	url = {http://arxiv.org/abs/2405.17414},
	doi = {10.48550/arXiv.2405.17414},
	abstract = {Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories make strides towards it. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments. Project page: https://collaborativevideodiffusion.github.io/.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Kuang, Zhengfei and Cai, Shengqu and He, Hao and Xu, Yinghao and Li, Hongsheng and Guibas, Leonidas and Wetzstein, Gordon},
	month = may,
	year = {2024},
	note = {arXiv:2405.17414 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{lei_mosca_2024,
	title = {{MoSca}: {Dynamic} {Gaussian} {Fusion} from {Casual} {Videos} via {4D} {Motion} {Scaffolds}},
	shorttitle = {{MoSca}},
	url = {http://arxiv.org/abs/2405.17421},
	doi = {10.48550/arXiv.2405.17421},
	abstract = {We introduce 4D Motion Scaffolds (MoSca), a neural information processing system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models, lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions / deformations. The scene geometry and appearance are then disentangled from the deformation field, and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera poses can be seamlessly initialized and refined during the dynamic rendering process, without the need for other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Lei, Jiahui and Weng, Yijia and Harley, Adam and Guibas, Leonidas and Daniilidis, Kostas},
	month = may,
	year = {2024},
	note = {arXiv:2405.17421 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{li_crossway_2024,
	title = {Crossway {Diffusion}: {Improving} {Diffusion}-based {Visuomotor} {Policy} via {Self}-supervised {Learning}},
	shorttitle = {Crossway {Diffusion}},
	url = {http://arxiv.org/abs/2307.01849},
	doi = {10.48550/arXiv.2307.01849},
	abstract = {Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning in a sequence modeling fashion, benefiting from their exceptional capabilities in modeling complex data distributions. The standard diffusion-based policy iteratively generates action sequences from random noise conditioned on the input states. Nonetheless, the model for diffusion policy can be further improved in terms of visual representations. In this work, we propose Crossway Diffusion, a simple yet effective method to enhance diffusion-based visuomotor policy learning via a carefully designed state decoder and an auxiliary self-supervised learning (SSL) objective. The state decoder reconstructs raw image pixels and other state information from the intermediate representations of the reverse diffusion process. The whole model is jointly optimized by the SSL objective and the original diffusion loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its consistent advantages over the standard diffusion-based policy and substantial improvements over the baselines.},
	urldate = {2024-06-01},
	publisher = {arXiv},
	author = {Li, Xiang and Belagali, Varun and Shang, Jinghuan and Ryoo, Michael S.},
	month = jan,
	year = {2024},
	note = {arXiv:2307.01849 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_egochoir_2024,
	title = {{EgoChoir}: {Capturing} {3D} {Human}-{Object} {Interaction} {Regions} from {Egocentric} {Views}},
	shorttitle = {{EgoChoir}},
	url = {http://arxiv.org/abs/2405.13659},
	doi = {10.48550/arXiv.2405.13659},
	abstract = {Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g., ''what'' interaction is occurring, capturing ''where'' the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric view. However, incomplete observations of interacting parties in the egocentric view introduce ambiguity between visual observations and interaction contents, impairing their efficacy. From the egocentric view, humans integrate the visual cortex, cerebellum, and brain to internalize their intentions and interaction concepts of objects, allowing for the pre-formulation of interactions and making behaviors even when interaction regions are out of sight. In light of this, we propose harmonizing the visual appearance, head motion, and 3D object to excavate the object interaction concept and subject intention, jointly inferring 3D human contact and object affordance from egocentric videos. To achieve this, we present EgoChoir, which links object structures with interaction contexts inherent in appearance and head motion to reveal object affordance, further utilizing it to model human contact. Additionally, a gradient modulation is employed to adopt appropriate clues for capturing interaction regions across various egocentric scenarios. Moreover, 3D contact and affordance are annotated for egocentric videos collected from Ego-Exo4D and GIMO to support the task. Extensive experiments on them demonstrate the effectiveness and superiority of EgoChoir. Code and data will be open.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Yang, Yuhang and Zhai, Wei and Wang, Chengfeng and Yu, Chengjun and Cao, Yang and Zha, Zheng-Jun},
	month = may,
	year = {2024},
	note = {arXiv:2405.13659 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{van_hoorick_generative_2024,
	title = {Generative {Camera} {Dolly}: {Extreme} {Monocular} {Dynamic} {Novel} {View} {Synthesis}},
	shorttitle = {Generative {Camera} {Dolly}},
	url = {http://arxiv.org/abs/2405.14868},
	doi = {10.48550/arXiv.2405.14868},
	abstract = {Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose \${\textbackslash}textbf\{GCD\}\$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Van Hoorick, Basile and Wu, Rundi and Ozguroglu, Ege and Sargent, Kyle and Liu, Ruoshi and Tokmakov, Pavel and Dave, Achal and Zheng, Changxi and Vondrick, Carl},
	month = may,
	year = {2024},
	note = {arXiv:2405.14868 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_one-shot_2024,
	title = {One-{Shot} {Imitation} {Learning} with {Invariance} {Matching} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2405.13178},
	doi = {10.48550/arXiv.2405.13178},
	abstract = {Learning a single universal policy that can perform a diverse set of manipulation tasks is a promising new direction in robotics. However, existing techniques are limited to learning policies that can only perform tasks that are encountered during training, and require a large number of demonstrations to learn new tasks. Humans, on the other hand, often can learn a new task from a single unannotated demonstration. In this work, we propose the Invariance-Matching One-shot Policy Learning (IMOP) algorithm. In contrast to the standard practice of learning the end-effector's pose directly, IMOP first learns invariant regions of the state space for a given task, and then computes the end-effector's pose through matching the invariant regions between demonstrations and test scenes. Trained on the 18 RLBench tasks, IMOP achieves a success rate that outperforms the state-of-the-art consistently, by 4.5\% on average over the 18 tasks. More importantly, IMOP can learn a novel task from a single unannotated demonstration, and without any fine-tuning, and achieves an average success rate improvement of \$11.5{\textbackslash}\%\$ over the state-of-the-art on 22 novel tasks selected across nine categories. IMOP can also generalize to new shapes and learn to manipulate objects that are different from those in the demonstration. Further, IMOP can perform one-shot sim-to-real transfer using a single real-robot demonstration.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Zhang, Xinyu and Boularias, Abdeslam},
	month = may,
	year = {2024},
	note = {arXiv:2405.13178 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{xian_chaineddiffuser_2023,
	title = {{ChainedDiffuser}: {Unifying} {Trajectory} {Diffusion} and {Keypose} {Prediction} for {Robotic} {Manipulation}},
	shorttitle = {{ChainedDiffuser}},
	url = {https://openreview.net/forum?id=W0zgY2mBTA8},
	abstract = {We present ChainedDiffuser, a policy architecture that unifies action keypose prediction and trajectory diffusion generation for learning robot manipulation from demonstrations. Our main innovation is to use a global transformer-based action predictor to predict actions at keyframes, a task that requires multi- modal semantic scene understanding, and to use a local trajectory diffuser to predict trajectory segments that connect predicted macro-actions. ChainedDiffuser sets a new record on established manipulation benchmarks, and outperforms both state-of-the-art keypose (macro-action) prediction models that use motion plan- ners for trajectory prediction, and trajectory diffusion policies that do not predict keyframe macro-actions. We conduct experiments in both simulated and real-world environments and demonstrate ChainedDiffuser’s ability to solve a wide range of manipulation tasks involving interactions with diverse objects.},
	language = {en},
	urldate = {2024-05-29},
	author = {Xian, Zhou and Gkanatsios, Nikolaos and Gervet, Theophile and Ke, Tsung-Wei and Fragkiadaki, Katerina},
	month = aug,
	year = {2023},
}

@misc{zhang_s3o_2024,
	title = {{S3O}: {A} {Dual}-{Phase} {Approach} for {Reconstructing} {Dynamic} {Shape} and {Skeleton} of {Articulated} {Objects} from {Single} {Monocular} {Video}},
	shorttitle = {{S3O}},
	url = {http://arxiv.org/abs/2405.12607},
	doi = {10.48550/arXiv.2405.12607},
	abstract = {Reconstructing dynamic articulated objects from a singular monocular video is challenging, requiring joint estimation of shape, motion, and camera parameters from limited views. Current methods typically demand extensive computational resources and training time, and require additional human annotations such as predefined parametric models, camera poses, and key points, limiting their generalizability. We propose Synergistic Shape and Skeleton Optimization (S3O), a novel two-phase method that forgoes these prerequisites and efficiently learns parametric models including visible shapes and underlying skeletons. Conventional strategies typically learn all parameters simultaneously, leading to interdependencies where a single incorrect prediction can result in significant errors. In contrast, S3O adopts a phased approach: it first focuses on learning coarse parametric models, then progresses to motion learning and detail addition. This method substantially lowers computational complexity and enhances robustness in reconstruction from limited viewpoints, all without requiring additional annotations. To address the current inadequacies in 3D reconstruction from monocular video benchmarks, we collected the PlanetZoo dataset. Our experimental evaluations on standard benchmarks and the PlanetZoo dataset affirm that S3O provides more accurate 3D reconstruction, and plausible skeletons, and reduces the training time by approximately 60\% compared to the state-of-the-art, thus advancing the state of the art in dynamic object reconstruction.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Zhang, Hao and Li, Fang and Rawlekar, Samyak and Ahuja, Narendra},
	month = may,
	year = {2024},
	note = {arXiv:2405.12607 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jiang_omniglue_2024,
	title = {{OmniGlue}: {Generalizable} {Feature} {Matching} with {Foundation} {Model} {Guidance}},
	shorttitle = {{OmniGlue}},
	url = {http://arxiv.org/abs/2405.12979},
	doi = {10.48550/arXiv.2405.12979},
	abstract = {The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of \$7\$ datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of \$20.9{\textbackslash}\%\$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by \$9.5{\textbackslash}\%\$ relatively.Code and model can be found at https://hwjiang1510.github.io/OmniGlue},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Jiang, Hanwen and Karpur, Arjun and Cao, Bingyi and Huang, Qixing and Araujo, Andre},
	month = may,
	year = {2024},
	note = {arXiv:2405.12979 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_physics-based_2024,
	title = {Physics-based {Scene} {Layout} {Generation} from {Human} {Motion}},
	url = {http://arxiv.org/abs/2405.12460},
	doi = {10.1145/3641519.3657517},
	abstract = {Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.},
	urldate = {2024-05-29},
	author = {Li, Jianan and Huang, Tao and Zhu, Qingxu and Wong, Tien-Tsin},
	month = may,
	year = {2024},
	note = {arXiv:2405.12460 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{qin_anyteleop_2024,
	title = {{AnyTeleop}: {A} {General} {Vision}-{Based} {Dexterous} {Robot} {Arm}-{Hand} {Teleoperation} {System}},
	shorttitle = {{AnyTeleop}},
	url = {http://arxiv.org/abs/2307.04577},
	doi = {10.48550/arXiv.2307.04577},
	abstract = {Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator. Project page: https://yzqin.github.io/anyteleop/.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Qin, Yuzhe and Yang, Wei and Huang, Binghao and Van Wyk, Karl and Su, Hao and Wang, Xiaolong and Chao, Yu-Wei and Fox, Dieter},
	month = may,
	year = {2024},
	note = {arXiv:2307.04577 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_rl-vlm-f_2024,
	title = {{RL}-{VLM}-{F}: {Reinforcement} {Learning} from {Vision} {Language} {Foundation} {Model} {Feedback}},
	shorttitle = {{RL}-{VLM}-{F}},
	url = {http://arxiv.org/abs/2402.03681},
	abstract = {Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Wang, Yufei and Sun, Zhanyi and Zhang, Jesse and Xian, Zhou and Biyik, Erdem and Held, David and Erickson, Zackory},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03681 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_grounded_2024,
	title = {Grounded {3D}-{LLM} with {Referent} {Tokens}},
	url = {http://arxiv.org/abs/2405.10370},
	doi = {10.48550/arXiv.2405.10370},
	abstract = {Prior studies on 3D scene understanding have primarily developed specialized models for specific tasks or required task-specific fine-tuning. In this study, we propose Grounded 3D-LLM, which explores the potential of 3D large multi-modal models (3D LMMs) to consolidate various 3D vision tasks within a unified generative framework. The model uses scene referent tokens as special noun phrases to reference 3D scenes, enabling the handling of sequences that interleave 3D and textual data. It offers a natural approach for translating 3D vision tasks into language formats using task-specific instruction templates. To facilitate the use of referent tokens in subsequent language modeling, we have curated large-scale grounded language datasets that offer finer scene-text correspondence at the phrase level by bootstrapping existing object labels. Subsequently, we introduced Contrastive LAnguage-Scene Pre-training (CLASP) to effectively leverage this data, thereby integrating 3D vision with language models. Our comprehensive evaluation covers open-ended tasks like dense captioning and 3D QA, alongside close-ended tasks such as object detection and language grounding. Experiments across multiple 3D benchmarks reveal the leading performance and the broad applicability of Grounded 3D-LLM. Code and datasets will be released on the project page: https://groundedscenellm.github.io/grounded\_3d-llm.github.io.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Chen, Yilun and Yang, Shuai and Huang, Haifeng and Wang, Tai and Lyu, Ruiyuan and Xu, Runsen and Lin, Dahua and Pang, Jiangmiao},
	month = may,
	year = {2024},
	note = {arXiv:2405.10370 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{octo_model_team_octo_2024,
	title = {Octo: {An} {Open}-{Source} {Generalist} {Robot} {Policy}},
	shorttitle = {Octo},
	url = {http://arxiv.org/abs/2405.12213},
	doi = {10.48550/arXiv.2405.12213},
	abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Octo Model Team and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
	month = may,
	year = {2024},
	note = {arXiv:2405.12213 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yu_natural_2024,
	title = {Natural {Language} {Can} {Help} {Bridge} the {Sim2Real} {Gap}},
	url = {http://arxiv.org/abs/2405.10020},
	doi = {10.48550/arXiv.2405.10020},
	abstract = {The main challenge in learning image-conditioned robotic policies is acquiring a visual representation conducive to low-level control. Due to the high dimensionality of the image space, learning a good visual representation requires a considerable amount of visual data. However, when learning in the real world, data is expensive. Sim2Real is a promising paradigm for overcoming data scarcity in the real-world target domain by using a simulator to collect large amounts of cheap data closely related to the target task. However, it is difficult to transfer an image-conditioned policy from sim to real when the domains are very visually dissimilar. To bridge the sim2real visual gap, we propose using natural language descriptions of images as a unifying signal across domains that captures the underlying task-relevant semantics. Our key insight is that if two image observations from different domains are labeled with similar language, the policy should predict similar action distributions for both images. We demonstrate that training the image encoder to predict the language description or the distance between descriptions of a sim or real image serves as a useful, data-efficient pretraining step that helps learn a domain-invariant image representation. We can then use this image encoder as the backbone of an IL policy trained simultaneously on a large amount of simulated and a handful of real demonstrations. Our approach outperforms widely used prior sim2real methods and strong vision-language pretraining baselines like CLIP and R3M by 25 to 40\%.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Yu, Albert and Foote, Adeline and Mooney, Raymond and Martín-Martín, Roberto},
	month = may,
	year = {2024},
	note = {arXiv:2405.10020 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, I.2.6, I.2.7, I.2.9},
}

@misc{wang_dance_2024,
	title = {Dance {Any} {Beat}: {Blending} {Beats} with {Visuals} in {Dance} {Video} {Generation}},
	shorttitle = {Dance {Any} {Beat}},
	url = {http://arxiv.org/abs/2405.09266},
	doi = {10.48550/arXiv.2405.09266},
	abstract = {The task of generating dance from music is crucial, yet current methods, which mainly produce joint sequences, lead to outputs that lack intuitiveness and complicate data collection due to the necessity for precise joint annotations. We introduce a Dance Any Beat Diffusion model, namely DabFusion, that employs music as a conditional input to directly create dance videos from still images, utilizing conditional image-to-video generation principles. This approach pioneers the use of music as a conditioning factor in image-to-video synthesis. Our method unfolds in two stages: training an auto-encoder to predict latent optical flow between reference and driving frames, eliminating the need for joint annotation, and training a U-Net-based diffusion model to produce these latent optical flows guided by music rhythm encoded by CLAP. Although capable of producing high-quality dance videos, the baseline model struggles with rhythm alignment. We enhance the model by adding beat information, improving synchronization. We introduce a 2D motion-music alignment score (2D-MM Align) for quantitative assessment. Evaluated on the AIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align score and established metrics. Video results can be found on our project page: https://DabFusion.github.io.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Wang, Xuanchen and Wang, Heng and Liu, Dongnan and Cai, Weidong},
	month = may,
	year = {2024},
	note = {arXiv:2405.09266 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{ge_behavior_2024,
	title = {{BEHAVIOR} {Vision} {Suite}: {Customizable} {Dataset} {Generation} via {Simulation}},
	shorttitle = {{BEHAVIOR} {Vision} {Suite}},
	url = {http://arxiv.org/abs/2405.09546},
	doi = {10.48550/arXiv.2405.09546},
	abstract = {The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as "filled" and "folded"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Ge, Yunhao and Tang, Yihe and Xu, Jiashu and Gokmen, Cem and Li, Chengshu and Ai, Wensi and Martinez, Benjamin Jose and Aydin, Arman and Anvari, Mona and Chakravarthy, Ayush K. and Yu, Hong-Xing and Wong, Josiah and Srivastava, Sanjana and Lee, Sharon and Zha, Shengxin and Itti, Laurent and Li, Yunzhu and Martín-Martín, Roberto and Liu, Miao and Zhang, Pengchuan and Zhang, Ruohan and Fei-Fei, Li and Wu, Jiajun},
	month = may,
	year = {2024},
	note = {arXiv:2405.09546 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_graingrasp_2024,
	title = {{GrainGrasp}: {Dexterous} {Grasp} {Generation} with {Fine}-grained {Contact} {Guidance}},
	shorttitle = {{GrainGrasp}},
	url = {http://arxiv.org/abs/2405.09310},
	doi = {10.48550/arXiv.2405.09310},
	abstract = {One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called GrainGrasp that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Zhao, Fuqiang and Tsetserukou, Dzmitry and Liu, Qian},
	month = may,
	year = {2024},
	note = {arXiv:2405.09310 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{dong_coin3d_2024,
	title = {{Coin3D}: {Controllable} and {Interactive} {3D} {Assets} {Generation} with {Proxy}-{Guided} {Conditioning}},
	shorttitle = {{Coin3D}},
	url = {http://arxiv.org/abs/2405.08054},
	doi = {10.48550/arXiv.2405.08054},
	abstract = {As humans, we aspire to create media content that is both freely willed and readily controlled. Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting. However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation. In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds. To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction. Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Dong, Wenqi and Yang, Bangbang and Ma, Lin and Liu, Xiao and Cui, Liyuan and Bao, Hujun and Ma, Yuewen and Cui, Zhaopeng},
	month = may,
	year = {2024},
	note = {arXiv:2405.08054 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wu_cross-category_2024,
	title = {Cross-{Category} {Functional} {Grasp} {Tansfer}},
	url = {http://arxiv.org/abs/2405.08310},
	doi = {10.48550/arXiv.2405.08310},
	abstract = {The grasp generation of dexterous hand often requires a large number of grasping annotations. Especially for functional grasp-requiring the grasp pose to be convenient for the subsequent use of the object. However, annotating high DoF dexterous hand pose is rather challenging. This prompt us to explore how people achieve manipulations on new objects based on past grasp experiences. We find that people are adept at discovering and leveraging various similarities between objects when grasping new items, including shape, layout, and grasp type. In light of this, we analyze and collect grasp-related similarity relationships among 51 common tool-like object categories and annotate semantic grasp representation for 1768 objects. These data are organized into the form of a knowledge graph, which helps infer our proposed cross-category functional grasp synthesis. Through extensive experiments, we demonstrate that the grasp-related knowledge indeed contributed to achieving functional grasp transfer across unknown or entirely new categories of objects. We will publicly release the dataset and code to facilitate future research.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Wu, Rina and Zhu, Tianqiang and Lin, Xiangbo and Sun, Yi},
	month = may,
	year = {2024},
	note = {arXiv:2405.08310 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{mejia_hearing_2024,
	title = {Hearing {Touch}: {Audio}-{Visual} {Pretraining} for {Contact}-{Rich} {Manipulation}},
	shorttitle = {Hearing {Touch}},
	url = {http://arxiv.org/abs/2405.08576},
	doi = {10.48550/arXiv.2405.08576},
	abstract = {Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation. For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Mejia, Jared and Dean, Victoria and Hellebrekers, Tess and Gupta, Abhinav},
	month = may,
	year = {2024},
	note = {arXiv:2405.08576 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xue_shape_2024,
	title = {Shape {Conditioned} {Human} {Motion} {Generation} with {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2405.06778},
	doi = {10.48550/arXiv.2405.06778},
	abstract = {Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and muscles, considering solely the skeleton for generation may neglect their inherent interdependency, which can limit the variability and precision of the generated results. To address this issue, we propose a Shape-conditioned Motion Diffusion model (SMD), which enables the generation of motion sequences directly in mesh format, conditioned on a specified target mesh. In SMD, the input meshes are transformed into spectral coefficients using graph Laplacian, to efficiently represent meshes. Subsequently, we propose a Spectral-Temporal Autoencoder (STAE) to leverage cross-temporal dependencies within the spectral domain. Extensive experimental evaluations show that SMD not only produces vivid and realistic motions but also achieves competitive performance in text-to-motion and action-to-motion tasks when compared to state-of-the-art methods.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Xue, Kebing and Seo, Hyewon},
	month = may,
	year = {2024},
	note = {arXiv:2405.06778 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{uppal_spin_2024,
	title = {{SPIN}: {Simultaneous} {Perception}, {Interaction} and {Navigation}},
	shorttitle = {{SPIN}},
	url = {http://arxiv.org/abs/2405.07991},
	doi = {10.48550/arXiv.2405.07991},
	abstract = {While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move. This allows it to not only move around and interact with its environment but also, choose "when" to perceive "what" using an active visual system. We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps. Results visualizations and videos at https://spin-robot.github.io/},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Uppal, Shagun and Agarwal, Ananye and Xiong, Haoyu and Shaw, Kenneth and Pathak, Deepak},
	month = may,
	year = {2024},
	note = {arXiv:2405.07991 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{mo_motion_2024,
	title = {Motion {Keyframe} {Interpolation} for {Any} {Human} {Skeleton} via {Temporally} {Consistent} {Point} {Cloud} {Sampling} and {Reconstruction}},
	url = {http://arxiv.org/abs/2405.07444},
	abstract = {In the character animation field, modern supervised keyframe interpolation models have demonstrated exceptional performance in constructing natural human motions from sparse pose definitions. As supervised models, large motion datasets are necessary to facilitate the learning process; however, since motion is represented with fixed hierarchical skeletons, such datasets are incompatible for skeletons outside the datasets’ native configurations. Consequently, the expected availability of a motion dataset for desired skeletons severely hinders the feasibility of learned interpolation in practice. To combat this limitation, we propose Point Cloud-based Motion Representation Learning (PC-MRL), an unsupervised approach to enabling cross-compatibility between skeletons for motion interpolation learning. PC-MRL consists of a skeleton obfuscation strategy using temporal point cloud sampling, and an unsupervised skeleton reconstruction method from point clouds. We devise a temporal point-wise K-nearest neighbors loss for unsupervised learning. Moreover, we propose First-frame Offset Quaternion (FOQ) and Rest Pose Augmentation (RPA) strategies to overcome necessary limitations of our unsupervised point cloud-to-skeletal motion process. Comprehensive experiments demonstrate the effectiveness of PC-MRL in motion interpolation for desired skeletons without supervision from native datasets.},
	language = {en},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Mo, Clinton and Hu, Kun and Long, Chengjiang and Yuan, Dong and Wang, Zhiyong},
	month = may,
	year = {2024},
	note = {arXiv:2405.07444 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cen_generating_2024,
	title = {Generating {Human} {Motion} in {3D} {Scenes} from {Text} {Descriptions}},
	url = {http://arxiv.org/abs/2405.07784},
	doi = {10.48550/arXiv.2405.07784},
	abstract = {Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Cen, Zhi and Pi, Huaijin and Peng, Sida and Shen, Zehong and Yang, Minghui and Zhu, Shuai and Bao, Hujun and Zhou, Xiaowei},
	month = may,
	year = {2024},
	note = {arXiv:2405.07784 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_anyrotate_2024,
	title = {{AnyRotate}: {Gravity}-{Invariant} {In}-{Hand} {Object} {Rotation} with {Sim}-to-{Real} {Touch}},
	shorttitle = {{AnyRotate}},
	url = {http://arxiv.org/abs/2405.07391},
	doi = {10.48550/arXiv.2405.07391},
	abstract = {In-hand manipulation is an integral component of human dexterity. Our hands rely on tactile feedback for stable and reactive motions to ensure objects do not slip away unintentionally during manipulation. For a robot hand, this level of dexterity requires extracting and utilizing rich contact information for precise motor control. In this paper, we present AnyRotate, a system for gravity-invariant multi-axis in-hand object rotation using dense featured sim-to-real touch. We construct a continuous contact feature representation to provide tactile feedback for training a policy in simulation and introduce an approach to perform zero-shot policy transfer by training an observation model to bridge the sim-to-real gap. Our experiments highlight the benefit of detailed contact information when handling objects with varying properties. In the real world, we demonstrate successful sim-to-real transfer of the dense tactile policy, generalizing to a diverse range of objects for various rotation axes and hand directions and outperforming other forms of low-dimensional touch. Interestingly, despite not having explicit slip detection, rich multi-fingered tactile sensing can implicitly detect object movement within grasp and provide a reactive behavior that improves the robustness of the policy, highlighting the importance of information-rich tactile sensing for in-hand manipulation.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Yang, Max and Lu, Chenghua and Church, Alex and Lin, Yijiong and Ford, Chris and Li, Haoran and Psomopoulou, Efi and Barton, David A. W. and Lepora, Nathan F.},
	month = may,
	year = {2024},
	note = {arXiv:2405.07391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{sun_learning_2024,
	title = {Learning {Latent} {Dynamic} {Robust} {Representations} for {World} {Models}},
	url = {http://arxiv.org/abs/2405.06263},
	doi = {10.48550/arXiv.2405.06263},
	abstract = {Visual Model-Based Reinforcement Learning (MBRL) promises to encapsulate agent's knowledge about the underlying dynamics of the environment, enabling learning a world model as a useful planner. However, top MBRL agents such as Dreamer often struggle with visual pixel-based inputs in the presence of exogenous or irrelevant noise in the observation space, due to failure to capture task-specific features while filtering out irrelevant spatio-temporal details. To tackle this problem, we apply a spatio-temporal masking strategy, a bisimulation principle, combined with latent reconstruction, to capture endogenous task-specific aspects of the environment for world models, effectively eliminating non-essential information. Joint training of representations, dynamics, and policy often leads to instabilities. To further address this issue, we develop a Hybrid Recurrent State-Space Model (HRSSM) structure, enhancing state representation robustness for effective policy learning. Our empirical evaluation demonstrates significant performance improvements over existing methods in a range of visually complex control tasks such as Maniskill {\textbackslash}cite\{gu2023maniskill2\} with exogenous distractors from the Matterport environment. Our code is avaliable at https://github.com/bit1029public/HRSSM.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Sun, Ruixiang and Zang, Hongyu and Li, Xin and Islam, Riashat},
	month = may,
	year = {2024},
	note = {arXiv:2405.06263 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{duisterhof_residual-nerf_2024,
	title = {Residual-{NeRF}: {Learning} {Residual} {NeRFs} for {Transparent} {Object} {Manipulation}},
	shorttitle = {Residual-{NeRF}},
	url = {http://arxiv.org/abs/2405.06181},
	doi = {10.48550/arXiv.2405.06181},
	abstract = {Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1\% lower RMSE and a 29.5\% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Duisterhof, Bardienus P. and Mao, Yuemin and Teng, Si Heng and Ichnowski, Jeffrey},
	month = may,
	year = {2024},
	note = {arXiv:2405.06181 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{fang_anygrasp_2023,
	title = {{AnyGrasp}: {Robust} and {Efficient} {Grasp} {Perception} in {Spatial} and {Temporal} {Domains}},
	shorttitle = {{AnyGrasp}},
	url = {http://arxiv.org/abs/2212.08333},
	doi = {10.48550/arXiv.2212.08333},
	abstract = {As the basis for prehensile manipulation, it is vital to enable robots to grasp as robustly as humans. Our innate grasping system is prompt, accurate, flexible, and continuous across spatial and temporal domains. Few existing methods cover all these properties for robot grasping. In this paper, we propose AnyGrasp for grasp perception to enable robots these abilities using a parallel gripper. Specifically, we develop a dense supervision strategy with real perception and analytic labels in the spatial-temporal domain. Additional awareness of objects' center-of-mass is incorporated into the learning process to help improve grasping stability. Utilization of grasp correspondence across observations enables dynamic grasp tracking. Our model can efficiently generate accurate, 7-DoF, dense, and temporally-smooth grasp poses and works robustly against large depth-sensing noise. Using AnyGrasp, we achieve a 93.3\% success rate when clearing bins with over 300 unseen objects, which is on par with human subjects under controlled conditions. Over 900 mean-picks-per-hour is reported on a single-arm system. For dynamic grasping, we demonstrate catching swimming robot fish in the water. Our project page is at https://graspnet.net/anygrasp.html},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Fang, Hao-Shu and Wang, Chenxi and Fang, Hongjie and Gou, Minghao and Liu, Jirong and Yan, Hengxu and Liu, Wenhai and Xie, Yichen and Lu, Cewu},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08333 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{huang_subgoal_2024,
	title = {Subgoal {Diffuser}: {Coarse}-to-fine {Subgoal} {Generation} to {Guide} {Model} {Predictive} {Control} for {Robot} {Manipulation}},
	shorttitle = {Subgoal {Diffuser}},
	url = {http://arxiv.org/abs/2403.13085},
	doi = {10.48550/arXiv.2403.13085},
	abstract = {Manipulation of articulated and deformable objects can be difficult due to their compliant and under-actuated nature. Unexpected disturbances can cause the object to deviate from a predicted state, making it necessary to use Model-Predictive Control (MPC) methods to plan motion. However, these methods need a short planning horizon to be practical. Thus, MPC is ill-suited for long-horizon manipulation tasks due to local minima. In this paper, we present a diffusion-based method that guides an MPC method to accomplish long-horizon manipulation tasks by dynamically specifying sequences of subgoals for the MPC to follow. Our method, called Subgoal Diffuser, generates subgoals in a coarse-to-fine manner, producing sparse subgoals when the task is easily accomplished by MPC and more dense subgoals when the MPC method needs more guidance. The density of subgoals is determined dynamically based on a learned estimate of reachability, and subgoals are distributed to focus on challenging parts of the task. We evaluate our method on two robot manipulation tasks and find it improves the planning performance of an MPC method, and also outperforms prior diffusion-based methods.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Huang, Zixuan and Lin, Yating and Yang, Fan and Berenson, Dmitry},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13085 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{prasad_consistency_2024,
	title = {Consistency {Policy}: {Accelerated} {Visuomotor} {Policies} via {Consistency} {Distillation}},
	shorttitle = {Consistency {Policy}},
	url = {http://arxiv.org/abs/2405.07503},
	doi = {10.48550/arXiv.2405.07503},
	abstract = {Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as two real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps. Code and training details will be released publicly.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Prasad, Aaditya and Lin, Kevin and Wu, Jimmy and Zhou, Linqi and Bohg, Jeannette},
	month = may,
	year = {2024},
	note = {arXiv:2405.07503 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{gupta_pre-trained_2024,
	title = {Pre-trained {Text}-to-{Image} {Diffusion} {Models} {Are} {Versatile} {Representation} {Learners} for {Control}},
	url = {http://arxiv.org/abs/2405.05852},
	doi = {10.48550/arXiv.2405.05852},
	abstract = {Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Gupta, Gunshi and Yadav, Karmesh and Gal, Yarin and Batra, Dhruv and Kira, Zsolt and Lu, Cong and Rudner, Tim G. J.},
	month = may,
	year = {2024},
	note = {arXiv:2405.05852 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{li_evaluating_2024,
	title = {Evaluating {Real}-{World} {Robot} {Manipulation} {Policies} in {Simulation}},
	url = {http://arxiv.org/abs/2405.05941},
	doi = {10.48550/arXiv.2405.05941},
	abstract = {The field of robotics has made significant advances towards generalist robot manipulation policies. However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform. We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments. We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups. Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world. Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts. We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Li, Xuanlin and Hsu, Kyle and Gu, Jiayuan and Pertsch, Karl and Mees, Oier and Walke, Homer Rich and Fu, Chuyuan and Lunawat, Ishikaa and Sieh, Isabel and Kirmani, Sean and Levine, Sergey and Wu, Jiajun and Finn, Chelsea and Su, Hao and Vuong, Quan and Xiao, Ted},
	month = may,
	year = {2024},
	note = {arXiv:2405.05941 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shi_asgrasp_2024,
	title = {{ASGrasp}: {Generalizable} {Transparent} {Object} {Reconstruction} and {Grasping} from {RGB}-{D} {Active} {Stereo} {Camera}},
	shorttitle = {{ASGrasp}},
	url = {http://arxiv.org/abs/2405.05648},
	doi = {10.48550/arXiv.2405.05648},
	abstract = {In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90\% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.Project page: https://pku-epic.github.io/ASGrasp},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Shi, Jun and A, Yong and Jin, Yixiang and Li, Dingzhe and Niu, Haoyu and Jin, Zhezhu and Wang, He},
	month = may,
	year = {2024},
	note = {arXiv:2405.05648 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{pan_tax-pose_2023,
	title = {{TAX}-{Pose}: {Task}-{Specific} {Cross}-{Pose} {Estimation} for {Robot} {Manipulation}},
	shorttitle = {{TAX}-{Pose}},
	url = {https://proceedings.mlr.press/v205/pan23a.html},
	abstract = {How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship “cross-pose” and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired pose relationship (placing a pan into the oven or the mug onto the mug rack). We demonstrate our method’s capability to generalize to unseen objects, in some cases after training on only 10 demonstrations in the real world. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments across a number of tasks. Supplementary information and videos can be found at https://sites.google.com/view/tax-pose/home.},
	language = {en},
	urldate = {2024-05-10},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Pan, Chuer and Okorn, Brian and Zhang, Harry and Eisner, Ben and Held, David},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {1783--1792},
}

@misc{min_driveworld_2024,
	title = {{DriveWorld}: {4D} {Pre}-trained {Scene} {Understanding} via {World} {Models} for {Autonomous} {Driving}},
	shorttitle = {{DriveWorld}},
	url = {http://arxiv.org/abs/2405.04390},
	doi = {10.48550/arXiv.2405.04390},
	abstract = {Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed {\textbackslash}emph\{DriveWorld\}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5\% increase in mAP for 3D object detection, a 3.0\% increase in IoU for online mapping, a 5.0\% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0\% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Min, Chen and Zhao, Dawei and Xiao, Liang and Zhao, Jian and Xu, Xinli and Zhu, Zheng and Jin, Lei and Li, Jianshu and Guo, Yulan and Xing, Junliang and Jing, Liping and Nie, Yiming and Dai, Bin},
	month = may,
	year = {2024},
	note = {arXiv:2405.04390 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shorinwa_textbfsplat-mover_2024,
	title = {\${\textbackslash}textbf\{{Splat}-{MOVER}\}\$: {Multi}-{Stage}, {Open}-{Vocabulary} {Robotic} {Manipulation} via {Editable} {Gaussian} {Splatting}},
	shorttitle = {\${\textbackslash}textbf\{{Splat}-{MOVER}\}\$},
	url = {http://arxiv.org/abs/2405.04378},
	doi = {10.48550/arXiv.2405.04378},
	abstract = {We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) \${\textbackslash}textit\{ASK-Splat\}\$, a GSplat representation that distills latent codes for language semantics and grasp affordance into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical for many robotics tasks; (ii) \${\textbackslash}textit\{SEE-Splat\}\$, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a "digital twin" of the evolving environment throughout the manipulation task; and (iii) \${\textbackslash}textit\{Grasp-Splat\}\$, a grasp generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks, as well as in four multi-stage manipulation tasks using the edited scene to reflect scene changes due to prior manipulation stages, which is not possible with the existing baselines. Code for this project and a link to the project page will be made available soon.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Shorinwa, Ola and Tucker, Johnathan and Smith, Aliyah and Swann, Aiden and Chen, Timothy and Firoozi, Roya and Kennedy III, Monroe and Schwager, Mac},
	month = may,
	year = {2024},
	note = {arXiv:2405.04378 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{dou_tactile-augmented_2024,
	title = {Tactile-{Augmented} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2405.04534},
	doi = {10.48550/arXiv.2405.04534},
	abstract = {We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew},
	month = may,
	year = {2024},
	note = {arXiv:2405.04534 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhong_3d_2024,
	title = {{3D} {LiDAR} {Mapping} in {Dynamic} {Environments} {Using} a {4D} {Implicit} {Neural} {Representation}},
	url = {http://arxiv.org/abs/2405.03388},
	doi = {10.48550/arXiv.2405.03388},
	abstract = {Building accurate maps is a key building block to enable reliable localization, planning, and navigation of autonomous vehicles. We propose a novel approach for building accurate maps of dynamic environments utilizing a sequence of LiDAR scans. To this end, we propose encoding the 4D scene into a novel spatio-temporal implicit neural map representation by fitting a time-dependent truncated signed distance function to each point. Using our representation, we extract the static map by filtering the dynamic parts. Our neural representation is based on sparse feature grids, a globally shared decoder, and time-dependent basis functions, which we jointly optimize in an unsupervised fashion. To learn this representation from a sequence of LiDAR scans, we design a simple yet efficient loss function to supervise the map optimization in a piecewise way. We evaluate our approach on various scenes containing moving objects in terms of the reconstruction quality of static maps and the segmentation of dynamic point clouds. The experimental results demonstrate that our method is capable of removing the dynamic part of the input point clouds while reconstructing accurate and complete 3D maps, outperforming several state-of-the-art methods. Codes are available at: https://github.com/PRBonn/4dNDF},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Zhong, Xingguang and Pan, Yue and Stachniss, Cyrill and Behley, Jens},
	month = may,
	year = {2024},
	note = {arXiv:2405.03388 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{yu_octopi_2024,
	title = {Octopi: {Object} {Property} {Reasoning} with {Large} {Tactile}-{Language} {Models}},
	shorttitle = {Octopi},
	url = {http://arxiv.org/abs/2405.02794},
	doi = {10.48550/arXiv.2405.02794},
	abstract = {Physical reasoning is important for effective robot manipulation. Recent work has investigated both vision and language modalities for physical reasoning; vision can reveal information about objects in the environment and language serves as an abstraction and communication medium for additional context. Although these works have demonstrated success on a variety of physical reasoning tasks, they are limited to physical properties that can be inferred from visual or language inputs. In this work, we investigate combining tactile perception with language, which enables embodied systems to obtain physical properties through interaction and apply common-sense reasoning. We contribute a new dataset PhysiCleAR, which comprises both physical/property reasoning tasks and annotated tactile videos obtained using a GelSight tactile sensor. We then introduce Octopi, a system that leverages both tactile representation learning and large vision-language models to predict and reason about tactile inputs with minimal language fine-tuning. Our evaluations on PhysiCleAR show that Octopi is able to effectively use intermediate physical property predictions to improve physical reasoning in both trained tasks and for zero-shot reasoning. PhysiCleAR and Octopi are available on https://github.com/clear-nus/octopi.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Yu, Samson and Lin, Kelvin and Xiao, Anxing and Duan, Jiafei and Soh, Harold},
	month = may,
	year = {2024},
	note = {arXiv:2405.02794 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{mao_dexskills_2024,
	title = {{DexSkills}: {Skill} {Segmentation} {Using} {Haptic} {Data} for {Learning} {Autonomous} {Long}-{Horizon} {Robotic} {Manipulation} {Tasks}},
	shorttitle = {{DexSkills}},
	url = {http://arxiv.org/abs/2405.03476},
	doi = {10.48550/arXiv.2405.03476},
	abstract = {Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations have shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Mao, Xiaofeng and Giudici, Gabriele and Coppola, Claudio and Althoefer, Kaspar and Farkhatdinov, Ildar and Li, Zhibin and Jamone, Lorenzo},
	month = may,
	year = {2024},
	note = {arXiv:2405.03476 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chu_dreamscene4d_2024,
	title = {{DreamScene4D}: {Dynamic} {Multi}-{Object} {Scene} {Generation} from {Monocular} {Videos}},
	shorttitle = {{DreamScene4D}},
	url = {http://arxiv.org/abs/2405.02280},
	doi = {10.48550/arXiv.2405.02280},
	abstract = {Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a "decompose-then-recompose" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Chu, Wen-Hsuan and Ke, Lei and Fragkiadaki, Katerina},
	month = may,
	year = {2024},
	note = {arXiv:2405.02280 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_frame_2022,
	title = {Frame {Mining}: a {Free} {Lunch} for {Learning} {Robotic} {Manipulation} from {3D} {Point} {Clouds}},
	shorttitle = {Frame {Mining}},
	url = {http://arxiv.org/abs/2210.07442},
	doi = {10.48550/arXiv.2210.07442},
	abstract = {We study how choices of input point cloud coordinate frames impact learning of manipulation skills from 3D point clouds. There exist a variety of coordinate frame choices to normalize captured robot-object-interaction point clouds. We find that different frames have a profound effect on agent learning performance, and the trend is similar across 3D backbone networks. In particular, the end-effector frame and the target-part frame achieve higher training efficiency than the commonly used world frame and robot-base frame in many tasks, intuitively because they provide helpful alignments among point clouds across time steps and thus can simplify visual module learning. Moreover, the well-performing frames vary across tasks, and some tasks may benefit from multiple frame candidates. We thus propose FrameMiners to adaptively select candidate frames and fuse their merits in a task-agnostic manner. Experimentally, FrameMiners achieves on-par or significantly higher performance than the best single-frame version on five fully physical manipulation tasks adapted from ManiSkill and OCRTOC. Without changing existing camera placements or adding extra cameras, point cloud frame mining can serve as a free lunch to improve 3D manipulation learning.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Liu, Minghua and Li, Xuanlin and Ling, Zhan and Li, Yangyan and Su, Hao},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07442 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{chen_vividex_2024,
	title = {{ViViDex}: {Learning} {Vision}-based {Dexterous} {Manipulation} from {Human} {Videos}},
	shorttitle = {{ViViDex}},
	url = {http://arxiv.org/abs/2404.15709},
	doi = {10.48550/arXiv.2404.15709},
	abstract = {In this work, we aim to learn a unified vision-based policy for a multi-fingered robot hand to manipulate different objects in diverse poses. Though prior work has demonstrated that human videos can benefit policy learning, performance improvement has been limited by physically implausible trajectories extracted from videos. Moreover, reliance on privileged object information such as ground-truth object states further limits the applicability in realistic scenarios. To address these limitations, we propose a new framework ViViDex to improve vision-based policy learning from human videos. It first uses reinforcement learning with trajectory guided rewards to train state-based policies for each video, obtaining both visually natural and physically plausible trajectories from the video. We then rollout successful episodes from state-based policies and train a unified visual policy without using any privileged information. A coordinate transformation method is proposed to significantly boost the performance. We evaluate our method on three dexterous manipulation tasks and demonstrate a large improvement over state-of-the-art algorithms.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Chen, Zerui and Chen, Shizhe and Schmid, Cordelia and Laptev, Ivan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15709 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{rana_sayplan_2023,
	title = {{SayPlan}: {Grounding} {Large} {Language} {Models} using {3D} {Scene} {Graphs} for {Scalable} {Robot} {Task} {Planning}},
	shorttitle = {{SayPlan}},
	url = {https://openreview.net/forum?id=wMpOMO0Ss7a},
	abstract = {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.},
	language = {en},
	urldate = {2024-05-09},
	author = {Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko},
	month = aug,
	year = {2023},
}

@article{kapelyukh_dall-e-bot_2023,
	title = {{DALL}-{E}-{Bot}: {Introducing} {Web}-{Scale} {Diffusion} {Models} to {Robotics}},
	volume = {8},
	issn = {2377-3766},
	shorttitle = {{DALL}-{E}-{Bot}},
	url = {https://ieeexplore.ieee.org/abstract/document/10114570?casa_token=QYU2IEec0UMAAAAA:9mBcXWMgeVBYqvjygA9McJpazgHkjgq-xRSBHnURhs3oOx2hl7vGeN_E7dR0swvyke9Ruws04g},
	doi = {10.1109/LRA.2023.3272516},
	abstract = {We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and finally physically arranging the objects according to that goal image. We show that this is possible zero-shot using DALL-E, without needing any further example arrangements, data collection, or training. DALL-E-Bot is fully autonomous and is not restricted to a pre-defined set of objects or scenes, thanks to DALL-E's web-scale pre-training. Encouraging real-world results, with both human studies and objective metrics, show that integrating web-scale diffusion models into robotics pipelines is a promising direction for scalable, unsupervised robot learning.},
	number = {7},
	urldate = {2024-05-09},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kapelyukh, Ivan and Vosylius, Vitalis and Johns, Edward},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {AI-based methods, Big Data in robotics and automation, Image segmentation, Pipelines, Predictive models, Robots, Task analysis, Training, Visualization, deep learning in grasping and manipulation},
	pages = {3956--3963},
}

@misc{portela_learning_2024,
	title = {Learning {Force} {Control} for {Legged} {Manipulation}},
	url = {http://arxiv.org/abs/2405.01402},
	doi = {10.48550/arXiv.2405.01402},
	abstract = {Controlling contact forces during interactions is critical for locomotion and manipulation tasks. While sim-to-real reinforcement learning (RL) has succeeded in many contact-rich problems, current RL methods achieve forceful interactions implicitly without explicitly regulating forces. We propose a method for training RL policies for direct force control without requiring access to force sensing. We showcase our method on a whole-body control platform of a quadruped robot with an arm. Such force control enables us to perform gravity compensation and impedance control, unlocking compliant whole-body manipulation. The learned whole-body controller with variable compliance makes it intuitive for humans to teleoperate the robot by only commanding the manipulator, and the robot's body adjusts automatically to achieve the desired position and force. Consequently, a human teleoperator can easily demonstrate a wide variety of loco-manipulation tasks. To the best of our knowledge, we provide the first deployment of learned whole-body force control in legged manipulators, paving the way for more versatile and adaptable legged robots.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Portela, Tifanny and Margolis, Gabriel B. and Ji, Yandong and Agrawal, Pulkit},
	month = may,
	year = {2024},
	note = {arXiv:2405.01402 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{tang_minigpt-3d_2024,
	title = {{MiniGPT}-{3D}: {Efficiently} {Aligning} {3D} {Point} {Clouds} with {Large} {Language} {Models} using {2D} {Priors}},
	shorttitle = {{MiniGPT}-{3D}},
	url = {https://arxiv.org/abs/2405.01413v1},
	abstract = {Large 2D vision-language models (2D-LLMs) have gained significant attention by bridging Large Language Models (LLMs) with images using a simple projector. Inspired by their success, large 3D point cloud-language models (3D-LLMs) also integrate point clouds into LLMs. However, directly aligning point clouds with LLM requires expensive training costs, typically in hundreds of GPU-hours on A100, which hinders the development of 3D-LLMs. In this paper, we introduce MiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA results while training for only 27 hours on one RTX 3090. Specifically, we propose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which can leverage the similarity between 2D and 3D visual information. We introduce a novel four-stage training strategy for modality alignment in a cascaded way, and a mixture of query experts module to adaptively aggregate features with high efficiency. Moreover, we utilize parameter-efficient fine-tuning methods LoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which is up to 260x fewer than existing methods. Extensive experiments show that MiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with significantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12 increase on GPT-4 evaluation score for the challenging object captioning task compared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800. We are the first to explore the efficient 3D-LLM, offering new insights to the community. Code and weights are available at https://github.com/TangYuan96/MiniGPT-3D.},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {{arXiv}.org},
	author = {Tang, Yuan and Han, Xu and Li, Xianzhi and Yu, Qiao and Hao, Yixue and Hu, Long and Chen, Min},
	month = may,
	year = {2024},
}

@misc{bharadhwaj_track2act_2024,
	title = {{Track2Act}: {Predicting} {Point} {Tracks} from {Internet} {Videos} enables {Diverse} {Zero}-shot {Robot} {Manipulation}},
	shorttitle = {{Track2Act}},
	url = {http://arxiv.org/abs/2405.01527},
	doi = {10.48550/arXiv.2405.01527},
	abstract = {We seek to learn a generalizable goal-conditioned policy that enables zero-shot robot manipulation: interacting with unseen objects in novel scenes without test-time adaptation. While typical approaches rely on a large amount of demonstration data for such generalization, we propose an approach that leverages web videos to predict plausible interaction plans and learns a task-agnostic transformation to obtain robot actions in the real world. Our framework,Track2Act predicts tracks of how points in an image should move in future time-steps based on a goal, and can be trained with diverse videos on the web including those of humans and robots manipulating everyday objects. We use these 2D track predictions to infer a sequence of rigid transforms of the object to be manipulated, and obtain robot end-effector poses that can be executed in an open-loop manner. We then refine this open-loop plan by predicting residual actions through a closed loop policy trained with a few embodiment-specific demonstrations. We show that this approach of combining scalably learned track prediction with a residual policy requiring minimal in-domain robot-specific data enables zero-shot robot manipulation, and present a wide array of real-world robot manipulation results across unseen tasks, objects, and scenes. https://homangab.github.io/track2act/},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Bharadhwaj, Homanga and Mottaghi, Roozbeh and Gupta, Abhinav and Tulsiani, Shubham},
	month = may,
	year = {2024},
	note = {arXiv:2405.01527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{jing_stt_2024,
	title = {{STT}: {Stateful} {Tracking} with {Transformers} for {Autonomous} {Driving}},
	shorttitle = {{STT}},
	url = {http://arxiv.org/abs/2405.00236},
	doi = {10.48550/arXiv.2405.00236},
	abstract = {Tracking objects in three-dimensional space is critical for autonomous driving. To ensure safety while driving, the tracker must be able to reliably track objects across frames and accurately estimate their states such as velocity and acceleration in the present. Existing works frequently focus on the association task while either neglecting the model performance on state estimation or deploying complex heuristics to predict the states. In this paper, we propose STT, a Stateful Tracking model built with Transformers, that can consistently track objects in the scenes while also predicting their states accurately. STT consumes rich appearance, geometry, and motion signals through long term history of detections and is jointly optimized for both data association and state estimation tasks. Since the standard tracking metrics like MOTA and MOTP do not capture the combined performance of the two tasks in the wider spectrum of object states, we extend them with new metrics called S-MOTA and MOTPS that address this limitation. STT achieves competitive real-time performance on the Waymo Open Dataset.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Jing, Longlong and Yu, Ruichi and Chen, Xu and Zhao, Zhengli and Sheng, Shiwei and Graber, Colin and Chen, Qi and Li, Qinru and Wu, Shangxuan and Deng, Han and Lee, Sangjin and Sweeney, Chris and He, Qiurui and Hung, Wei-Chih and He, Tong and Zhou, Xingyi and Moussavi, Farshid and Guo, Zijian and Zhou, Yin and Tan, Mingxing and Yang, Weilong and Li, Congcong},
	month = apr,
	year = {2024},
	note = {arXiv:2405.00236 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{dai_motionlcm_2024,
	title = {{MotionLCM}: {Real}-time {Controllable} {Motion} {Generation} via {Latent} {Consistency} {Model}},
	shorttitle = {{MotionLCM}},
	url = {http://arxiv.org/abs/2404.19759},
	doi = {10.48550/arXiv.2404.19759},
	abstract = {This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD). By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Dai, Wenxun and Chen, Ling-Hao and Wang, Jingbo and Liu, Jinpeng and Dai, Bo and Tang, Yansong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19759 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_gs-lrm_2024,
	title = {{GS}-{LRM}: {Large} {Reconstruction} {Model} for {3D} {Gaussian} {Splatting}},
	shorttitle = {{GS}-{LRM}},
	url = {http://arxiv.org/abs/2404.19702},
	doi = {10.48550/arXiv.2404.19702},
	abstract = {We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ .},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Zhang, Kai and Bi, Sai and Tan, Hao and Xiangli, Yuanbo and Zhao, Nanxuan and Sunkavalli, Kalyan and Xu, Zexiang},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19702 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_diffuseloco_2024,
	title = {{DiffuseLoco}: {Real}-{Time} {Legged} {Locomotion} {Control} with {Diffusion} from {Offline} {Datasets}},
	shorttitle = {{DiffuseLoco}},
	url = {http://arxiv.org/abs/2404.19264},
	doi = {10.48550/arXiv.2404.19264},
	abstract = {This work introduces DiffuseLoco, a framework for training multi-skill diffusion-based policies for dynamic legged locomotion from offline datasets, enabling real-time control of diverse skills on robots in the real world. Offline learning at scale has led to breakthroughs in computer vision, natural language processing, and robotic manipulation domains. However, scaling up learning for legged robot locomotion, especially with multiple skills in a single policy, presents significant challenges for prior online reinforcement learning methods. To address this challenge, we propose a novel, scalable framework that leverages diffusion models to directly learn from offline multimodal datasets with a diverse set of locomotion skills. With design choices tailored for real-time control in dynamical systems, including receding horizon control and delayed inputs, DiffuseLoco is capable of reproducing multimodality in performing various locomotion skills, zero-shot transfer to real quadrupedal robots, and it can be deployed on edge computing devices. Furthermore, DiffuseLoco demonstrates free transitions between skills and robustness against environmental variations. Through extensive benchmarking in real-world experiments, DiffuseLoco exhibits better stability and velocity tracking performance compared to prior reinforcement learning and non-diffusion-based behavior cloning baselines. The design choices are validated via comprehensive ablation studies. This work opens new possibilities for scaling up learning-based legged locomotion controllers through the scaling of large, expressive models and diverse offline datasets.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Huang, Xiaoyu and Chi, Yufeng and Wang, Ruofeng and Li, Zhongyu and Peng, Xue Bin and Shao, Sophia and Nikolic, Borivoje and Sreenath, Koushil},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19264 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{sridhar_nomad_2023,
	title = {{NoMaD}: {Goal} {Masked} {Diffusion} {Policies} for {Navigation} and {Exploration}},
	shorttitle = {{NoMaD}},
	url = {http://arxiv.org/abs/2310.07896},
	doi = {10.48550/arXiv.2310.07896},
	abstract = {Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Sridhar, Ajay and Shah, Dhruv and Glossop, Catherine and Levine, Sergey},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07896 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shi_waypoint-based_2023,
	title = {Waypoint-{Based} {Imitation} {Learning} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2307.14326},
	doi = {10.48550/arXiv.2307.14326},
	abstract = {While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25\% in simulation and by 4-28\% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Shi, Lucy Xiaoyang and Sharma, Archit and Zhao, Tony Z. and Finn, Chelsea},
	month = jul,
	year = {2023},
	note = {arXiv:2307.14326 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_dexterous_2024,
	title = {Dexterous {Grasp} {Transformer}},
	url = {http://arxiv.org/abs/2404.18135},
	doi = {10.48550/arXiv.2404.18135},
	abstract = {In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass. We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it. However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance. To address these issues, we propose progressive strategies for both the training and testing phases. First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase. Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase. Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity. Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing. Codes are available at https://github.com/iSEE-Laboratory/DGTR .},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Xu, Guo-Hao and Wei, Yi-Lin and Zheng, Dian and Wu, Xiao-Ming and Zheng, Wei-Shi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18135 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lin_generalize_2024,
	title = {Generalize by {Touching}: {Tactile} {Ensemble} {Skill} {Transfer} for {Robotic} {Furniture} {Assembly}},
	shorttitle = {Generalize by {Touching}},
	url = {http://arxiv.org/abs/2404.17684},
	doi = {10.48550/arXiv.2404.17684},
	abstract = {Furniture assembly remains an unsolved problem in robotic manipulation due to its long task horizon and nongeneralizable operations plan. This paper presents the Tactile Ensemble Skill Transfer (TEST) framework, a pioneering offline reinforcement learning (RL) approach that incorporates tactile feedback in the control loop. TEST's core design is to learn a skill transition model for high-level planning, along with a set of adaptive intra-skill goal-reaching policies. Such design aims to solve the robotic furniture assembly problem in a more generalizable way, facilitating seamless chaining of skills for this long-horizon task. We first sample demonstration from a set of heuristic policies and trajectories consisting of a set of randomized sub-skill segments, enabling the acquisition of rich robot trajectories that capture skill stages, robot states, visual indicators, and crucially, tactile signals. Leveraging these trajectories, our offline RL method discerns skill termination conditions and coordinates skill transitions. Our evaluations highlight the proficiency of TEST on the in-distribution furniture assemblies, its adaptability to unseen furniture configurations, and its robustness against visual disturbances. Ablation studies further accentuate the pivotal role of two algorithmic components: the skill transition model and tactile ensemble policies. Results indicate that TEST can achieve a success rate of 90{\textbackslash}\% and is over 4 times more efficient than the heuristic policy in both in-distribution and generalization settings, suggesting a scalable skill transfer approach for contact-rich manipulation.},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Lin, Haohong and Corcodel, Radu and Zhao, Ding},
	month = apr,
	year = {2024},
	note = {arXiv:2404.17684 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{peri_point_2024,
	title = {Point {Cloud} {Models} {Improve} {Visual} {Robustness} in {Robotic} {Learners}},
	url = {http://arxiv.org/abs/2404.18926},
	doi = {10.48550/arXiv.2404.18926},
	abstract = {Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Project Webpage: https://pvskand.github.io/projects/PCWM},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Peri, Skand and Lee, Iain and Kim, Chanho and Fuxin, Li and Hermans, Tucker and Lee, Stefan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18926 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jonnavittula_view_2024,
	title = {{VIEW}: {Visual} {Imitation} {Learning} with {Waypoints}},
	shorttitle = {{VIEW}},
	url = {http://arxiv.org/abs/2404.17906},
	doi = {10.48550/arXiv.2404.17906},
	abstract = {Robots can use Visual Imitation Learning (VIL) to learn everyday tasks from video demonstrations. However, translating visual observations into actionable robot policies is challenging due to the high-dimensional nature of video data. This challenge is further exacerbated by the morphological differences between humans and robots, especially when the video demonstrations feature humans performing tasks. To address these problems we introduce Visual Imitation lEarning with Waypoints (VIEW), an algorithm that significantly enhances the sample efficiency of human-to-robot VIL. VIEW achieves this efficiency using a multi-pronged approach: extracting a condensed prior trajectory that captures the demonstrator's intent, employing an agent-agnostic reward function for feedback on the robot's actions, and utilizing an exploration algorithm that efficiently samples around waypoints in the extracted trajectory. VIEW also segments the human trajectory into grasp and task phases to further accelerate learning efficiency. Through comprehensive simulations and real-world experiments, VIEW demonstrates improved performance compared to current state-of-the-art VIL methods. VIEW enables robots to learn a diverse range of manipulation tasks involving multiple objects from arbitrarily long video demonstrations. Additionally, it can learn standard manipulation tasks such as pushing or moving objects from a single video demonstration in under 30 minutes, with fewer than 20 real-world rollouts. Code and videos here: https://collab.me.vt.edu/view/},
	urldate = {2024-05-04},
	publisher = {arXiv},
	author = {Jonnavittula, Ananth and Parekh, Sagar and Losey, Dylan P.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.17906 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{yariv_multiview_2020,
	title = {Multiview {Neural} {Surface} {Reconstruction} by {Disentangling} {Geometry} and {Appearance}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1a77befc3b608d6ed363567685f70e1e-Abstract.html},
	abstract = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. 
The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. 
We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
	urldate = {2024-05-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yariv, Lior and Kasten, Yoni and Moran, Dror and Galun, Meirav and Atzmon, Matan and Ronen, Basri and Lipman, Yaron},
	year = {2020},
	pages = {2492--2502},
}

@misc{he_fs6d_2022,
	title = {{FS6D}: {Few}-{Shot} {6D} {Pose} {Estimation} of {Novel} {Objects}},
	shorttitle = {{FS6D}},
	url = {http://arxiv.org/abs/2203.14628},
	doi = {10.48550/arXiv.2203.14628},
	abstract = {6D object pose estimation networks are limited in their capability to scale to large numbers of object instances due to the close-set assumption and their reliance on high-fidelity object CAD models. In this work, we study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few support views without extra training. To tackle the problem, we point out the importance of fully exploring the appearance and geometric relationship between the given support views and query scene patches and propose a dense prototypes matching framework by extracting and matching dense RGBD prototypes with transformers. Moreover, we show that the priors from diverse appearances and shapes are crucial to the generalization capability under the problem setting and thus propose a large-scale RGBD photorealistic dataset (ShapeNet6D) for network pre-training. A simple and effective online texture blending approach is also introduced to eliminate the domain gap from the synthesis dataset, which enriches appearance diversity at a low cost. Finally, we discuss possible solutions to this problem and establish benchmarks on popular datasets to facilitate future research. The project page is at {\textbackslash}url\{https://fs6d.github.io/\}.},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {He, Yisheng and Wang, Yao and Fan, Haoqiang and Sun, Jian and Chen, Qifeng},
	month = mar,
	year = {2022},
	note = {arXiv:2203.14628 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{potje_xfeat_2024,
	title = {{XFeat}: {Accelerated} {Features} for {Lightweight} {Image} {Matching}},
	shorttitle = {{XFeat}},
	url = {http://arxiv.org/abs/2404.19174},
	abstract = {We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat\_cvpr24.},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Potje, Guilherme and Cadar, Felipe and Araujo, Andre and Martins, Renato and Nascimento, Erickson R.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19174 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lin_se3-equivariant_2023,
	title = {{SE}(3)-{Equivariant} {Point} {Cloud}-{Based} {Place} {Recognition}},
	url = {https://proceedings.mlr.press/v205/lin23a.html},
	abstract = {This paper reports on a new 3D point cloud-based place recognition framework that uses SE(3)-equivariant networks to learn SE(3)-invariant global descriptors. We discover that, unlike existing methods, learned SE(3)-invariant global descriptors are more robust to matching inaccuracy and failure in severe rotation and translation configurations. Mobile robots undergo arbitrary rotational and translational movements. The SE(3)-invariant property ensures that the learned descriptors are robust to the rotation and translation changes in the robot pose and can represent the intrinsic geometric information of the scene. Furthermore, we have discovered that the attention module aids in the enhancement of performance while allowing significant downsampling. We evaluate the performance of the proposed framework on real-world data sets. The experimental results show that the proposed framework outperforms state-of-the-art baselines in various metrics, leading to a reliable point cloud-based place recognition network. We have open-sourced our code at: https://github.com/UMich-CURLY/se3\_equivariant\_place\_recognition.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Lin, Chien Erh and Song, Jingwei and Zhang, Ray and Zhu, Minghan and Ghaffari, Maani},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {1520--1530},
}

@misc{armani_ultra_2024,
	title = {Ultra {Inertial} {Poser}: {Scalable} {Motion} {Capture} and {Tracking} from {Sparse} {Inertial} {Sensors} and {Ultra}-{Wideband} {Ranging}},
	shorttitle = {Ultra {Inertial} {Poser}},
	url = {http://arxiv.org/abs/2404.19541},
	abstract = {While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging\$-\$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from \$13.62\$ to \$10.65cm\$ (\$22{\textbackslash}\%\$ better) and lowering jitter from \$1.56\$ to \$0.055km/s{\textasciicircum}3\$ (a reduction of \$97{\textbackslash}\%\$).},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Armani, Rayan and Qian, Changlin and Jiang, Jiaxi and Holz, Christian},
	month = apr,
	year = {2024},
	note = {arXiv:2404.19541 [cs, eess]},
	keywords = {68T07, 68T45, 68U01, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Electrical Engineering and Systems Science - Signal Processing, I.2, I.3, I.4, I.5},
}

@misc{fan_enhancing_2024,
	title = {Enhancing {mmWave} {Radar} {Point} {Cloud} via {Visual}-inertial {Supervision}},
	url = {http://arxiv.org/abs/2404.17229},
	abstract = {Complementary to prevalent LiDAR and camera systems, millimeter-wave (mmWave) radar is robust to adverse weather conditions like fog, rainstorms, and blizzards but offers sparse point clouds. Current techniques enhance the point cloud by the supervision of LiDAR's data. However, high-performance LiDAR is notably expensive and is not commonly available on vehicles. This paper presents mmEMP, a supervised learning approach that enhances radar point clouds using a low-cost camera and an inertial measurement unit (IMU), enabling crowdsourcing training data from commercial vehicles. Bringing the visual-inertial (VI) supervision is challenging due to the spatial agnostic of dynamic objects. Moreover, spurious radar points from the curse of RF multipath make robots misunderstand the scene. mmEMP first devises a dynamic 3D reconstruction algorithm that restores the 3D positions of dynamic features. Then, we design a neural network that densifies radar data and eliminates spurious radar points. We build a new dataset in the real world. Extensive experiments show that mmEMP achieves competitive performance compared with the SOTA approach training by LiDAR's data. In addition, we use the enhanced point cloud to perform object detection, localization, and mapping to demonstrate mmEMP's effectiveness.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Fan, Cong and Zhang, Shengkai and Liu, Kezhong and Wang, Shuai and Yang, Zheng and Wang, Wei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.17229 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{li_ag2manip_2024,
	title = {{Ag2Manip}: {Learning} {Novel} {Manipulation} {Skills} with {Agent}-{Agnostic} {Visual} and {Action} {Representations}},
	shorttitle = {{Ag2Manip}},
	url = {http://arxiv.org/abs/2404.17521},
	abstract = {Autonomous robotic systems capable of learning novel manipulation tasks are poised to transform industries from manufacturing to service automation. However, modern methods (e.g., VIP and R3M) still face significant hurdles, notably the domain gap among robotic embodiments and the sparsity of successful task executions within specific action spaces, resulting in misaligned and ambiguous task representations. We introduce Ag2Manip (Agent-Agnostic representations for Manipulation), a framework aimed at surmounting these challenges through two key innovations: a novel agent-agnostic visual representation derived from human manipulation videos, with the specifics of embodiments obscured to enhance generalizability; and an agent-agnostic action representation abstracting a robot's kinematics to a universal agent proxy, emphasizing crucial interactions between end-effector and object. Ag2Manip's empirical validation across simulated benchmarks like FrankaKitchen, ManiSkill, and PartManip shows a 325\% increase in performance, achieved without domain-specific demonstrations. Ablation studies underline the essential contributions of the visual and action representations to this success. Extending our evaluations to the real world, Ag2Manip significantly improves imitation learning success rates from 50\% to 77.5\%, demonstrating its effectiveness and generalizability across both simulated and physical environments.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Li, Puhao and Liu, Tengyu and Li, Yuyang and Han, Muzhi and Geng, Haoran and Wang, Shu and Zhu, Yixin and Zhu, Song-Chun and Huang, Siyuan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.17521 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{ma_vip_2023,
	title = {{VIP}: {Towards} {Universal} {Visual} {Reward} and {Representation} via {Value}-{Implicit} {Pre}-{Training}},
	shorttitle = {{VIP}},
	url = {http://arxiv.org/abs/2210.00030},
	doi = {10.48550/arXiv.2210.00030},
	abstract = {Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce \${\textbackslash}textbf\{V\}\$alue-\${\textbackslash}textbf\{I\}\$mplicit \${\textbackslash}textbf\{P\}\$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP's frozen representation can provide dense visual reward for an extensive set of simulated and \${\textbackslash}textbf\{real-robot\}\$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, \${\textbackslash}textbf\{few-shot\}\$ offline RL on a suite of real-world robot tasks with as few as 20 trajectories.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Ma, Yecheng Jason and Sodhani, Shagun and Jayaraman, Dinesh and Bastani, Osbert and Kumar, Vikash and Zhang, Amy},
	month = mar,
	year = {2023},
	note = {arXiv:2210.00030 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhang_large_2024,
	title = {Large {Motion} {Model} for {Unified} {Multi}-{Modal} {Motion} {Generation}},
	url = {http://arxiv.org/abs/2404.01284},
	doi = {10.48550/arXiv.2404.01284},
	abstract = {Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhang, Mingyuan and Jin, Daisheng and Gu, Chenyang and Hong, Fangzhou and Cai, Zhongang and Huang, Jingfang and Zhang, Chongzhi and Guo, Xinying and Yang, Lei and He, Ying and Liu, Ziwei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01284 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_dexcap_2024,
	title = {{DexCap}: {Scalable} and {Portable} {Mocap} {Data} {Collection} {System} for {Dexterous} {Manipulation}},
	shorttitle = {{DexCap}},
	url = {http://arxiv.org/abs/2403.07788},
	doi = {10.48550/arXiv.2403.07788},
	abstract = {Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at https://dex-cap.github.io},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Wang, Chen and Shi, Haochen and Wang, Weizhuo and Zhang, Ruohan and Fei-Fei, Li and Liu, C. Karen},
	month = mar,
	year = {2024},
	note = {arXiv:2403.07788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jiang_motiongpt_2023,
	title = {{MotionGPT}: {Human} {Motion} as a {Foreign} {Language}},
	shorttitle = {{MotionGPT}},
	url = {http://arxiv.org/abs/2306.14795},
	doi = {10.48550/arXiv.2306.14795},
	abstract = {Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multi-modal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this "motion vocabulary", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt learning, we pre-train MotionGPT with a mixture of motion-language data and fine-tune it on prompt-based question-and-answer tasks. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
	month = jul,
	year = {2023},
	note = {arXiv:2306.14795 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{liconti_leveraging_2024,
	title = {Leveraging {Pretrained} {Latent} {Representations} for {Few}-{Shot} {Imitation} {Learning} on a {Dexterous} {Robotic} {Hand}},
	url = {http://arxiv.org/abs/2404.16483},
	doi = {10.48550/arXiv.2404.16483},
	abstract = {In the context of imitation learning applied to dexterous robotic hands, the high complexity of the systems makes learning complex manipulation tasks challenging. However, the numerous datasets depicting human hands in various different tasks could provide us with better knowledge regarding human hand motion. We propose a method to leverage multiple large-scale task-agnostic datasets to obtain latent representations that effectively encode motion subtrajectories that we included in a transformer-based behavior cloning method. Our results demonstrate that employing latent representations yields enhanced performance compared to conventional behavior cloning methods, particularly regarding resilience to errors and noise in perception and proprioception. Furthermore, the proposed approach solely relies on human demonstrations, eliminating the need for teleoperation and, therefore, accelerating the data acquisition process. Accurate inverse kinematics for fingertip retargeting ensures precise transfer from human hand data to the robot, facilitating effective learning and deployment of manipulation policies. Finally, the trained policies have been successfully transferred to a real-world 23Dof robotic system.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Liconti, Davide and Toshimitsu, Yasunori and Katzschmann, Robert},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16483 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{wang_single-view_2024,
	title = {Single-{View} {Scene} {Point} {Cloud} {Human} {Grasp} {Generation}},
	url = {http://arxiv.org/abs/2404.15815},
	doi = {10.48550/arXiv.2404.15815},
	abstract = {In this work, we explore a novel task of generating human grasps based on single-view scene point clouds, which more accurately mirrors the typical real-world situation of observing objects from a single viewpoint. Due to the incompleteness of object point clouds and the presence of numerous scene points, the generated hand is prone to penetrating into the invisible parts of the object and the model is easily affected by scene points. Thus, we introduce S2HGrasp, a framework composed of two key modules: the Global Perception module that globally perceives partial object point clouds, and the DiffuGrasp module designed to generate high-quality human grasps based on complex inputs that include scene points. Additionally, we introduce S2HGD dataset, which comprises approximately 99,000 single-object single-view scene point clouds of 1,668 unique objects, each annotated with one human grasp. Our extensive experiments demonstrate that S2HGrasp can not only generate natural human grasps regardless of scene points, but also effectively prevent penetration between the hand and invisible parts of the object. Moreover, our model showcases strong generalization capability when applied to unseen objects. Our code and dataset are available at https://github.com/iSEE-Laboratory/S2HGrasp.},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Wang, Yan-Kang and Xing, Chengyi and Wei, Yi-Lin and Wu, Xiao-Ming and Zheng, Wei-Shi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15815 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lin_learning_2024,
	title = {Learning {Visuotactile} {Skills} with {Two} {Multifingered} {Hands}},
	url = {http://arxiv.org/abs/2404.16823},
	doi = {10.48550/arXiv.2404.16823},
	abstract = {Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .},
	urldate = {2024-04-27},
	publisher = {arXiv},
	author = {Lin, Toru and Zhang, Yu and Li, Qiyang and Qi, Haozhi and Yi, Brent and Levine, Sergey and Malik, Jitendra},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16823 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{lindenberger_lightglue_2023,
	title = {{LightGlue}: {Local} {Feature} {Matching} at {Light} {Speed}},
	shorttitle = {{LightGlue}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-04-25},
	author = {Lindenberger, Philipp and Sarlin, Paul-Edouard and Pollefeys, Marc},
	year = {2023},
	pages = {17627--17638},
}

@misc{smith_flowmap_2024,
	title = {{FlowMap}: {High}-{Quality} {Camera} {Poses}, {Intrinsics}, and {Depth} via {Gradient} {Descent}},
	shorttitle = {{FlowMap}},
	url = {http://arxiv.org/abs/2404.15259},
	doi = {10.48550/arXiv.2404.15259},
	abstract = {This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Smith, Cameron and Charatan, David and Tewari, Ayush and Sitzmann, Vincent},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15259 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shi_composing_2024,
	title = {Composing {Pre}-{Trained} {Object}-{Centric} {Representations} for {Robotics} {From} "{What}" and "{Where}" {Foundation} {Models}},
	url = {http://arxiv.org/abs/2404.13474},
	doi = {10.48550/arXiv.2404.13474},
	abstract = {There have recently been large advances both in pre-training visual representations for robotic control and segmenting unknown category objects in general images. To leverage these for improved robot learning, we propose \${\textbackslash}textbf\{POCR\}\$, a new framework for building pre-trained object-centric representations for robotic control. Building on theories of "what-where" representations in psychology and computer vision, we use segmentations from a pre-trained model to stably locate across timesteps, various entities in the scene, capturing "where" information. To each such segmented entity, we apply other pre-trained models that build vector descriptions suitable for robotic control tasks, thus capturing "what" the entity is. Thus, our pre-trained object-centric representations for control are constructed by appropriately combining the outputs of off-the-shelf pre-trained models, with no new training. On various simulated and real robotic tasks, we show that imitation policies for robotic manipulators trained on POCR achieve better performance and systematic generalization than state of the art pre-trained representations for robotics, as well as prior object-centric representations that are typically trained from scratch.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Shi, Junyao and Qian, Jianing and Ma, Yecheng Jason and Jayaraman, Dinesh},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13474 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lucas_posegpt_2022,
	title = {{PoseGPT}: {Quantization}-based {3D} {Human} {Motion} {Generation} and {Forecasting}},
	shorttitle = {{PoseGPT}},
	url = {http://arxiv.org/abs/2210.10542},
	doi = {10.48550/arXiv.2210.10542},
	abstract = {We address the problem of action-conditioned generation of human motion sequences. Existing work falls into two categories: forecast models conditioned on observed past motions, or generative models conditioned on action labels and duration only. In contrast, we generate motion conditioned on observations of arbitrary length, including none. To solve this generalized problem, we propose PoseGPT, an auto-regressive transformer-based approach which internally compresses human motion into quantized latent sequences. An auto-encoder first maps human motion to latent index sequences in a discrete space, and vice-versa. Inspired by the Generative Pretrained Transformer (GPT), we propose to train a GPT-like model for next-index prediction in that space; this allows PoseGPT to output distributions on possible futures, with or without conditioning on past motion. The discrete and compressed nature of the latent space allows the GPT-like model to focus on long-range signal, as it removes low-level redundancy in the input signal. Predicting discrete indices also alleviates the common pitfall of predicting averaged poses, a typical failure case when regressing continuous values, as the average of discrete targets is not a target itself. Our experimental results show that our proposed approach achieves state-of-the-art results on HumanAct12, a standard but small scale dataset, as well as on BABEL, a recent large scale MoCap dataset, and on GRAB, a human-object interactions dataset.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Lucas, Thomas and Baradel, Fabien and Weinzaepfel, Philippe and Rogez, Grégory},
	month = oct,
	year = {2022},
	note = {arXiv:2210.10542 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ugrinovic_purposer_2024,
	title = {Purposer: {Putting} {Human} {Motion} {Generation} in {Context}},
	shorttitle = {Purposer},
	url = {http://arxiv.org/abs/2404.12942},
	abstract = {We present a novel method to generate human motion to populate 3D indoor scenes. It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds. State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, Purposer can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Ugrinovic, Nicolas and Lucas, Thomas and Baradel, Fabien and Weinzaepfel, Philippe and Rogez, Gregory and Moreno-Noguer, Francesc},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12942 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_enabling_2024,
	title = {Enabling {Stateful} {Behaviors} for {Diffusion}-based {Policy} {Learning}},
	url = {http://arxiv.org/abs/2404.12539},
	abstract = {While imitation learning provides a simple and effective framework for policy learning, acquiring consistent actions during robot execution remains a challenging task. Existing approaches primarily focus on either modifying the action representation at data curation stage or altering the model itself, both of which do not fully address the scalability of consistent action generation. To overcome this limitation, we introduce the Diff-Control policy, which utilizes a diffusion-based model to learn the action representation from a state-space modeling viewpoint. We demonstrate that we can reduce diffusion-based policies' uncertainty by making it stateful through a Bayesian formulation facilitated by ControlNet, leading to improved robustness and success rates. Our experimental results demonstrate the significance of incorporating action statefulness in policy learning, where Diff-Control shows improved performance across various tasks. Specifically, Diff-Control achieves an average success rate of 72\% and 84\% on stateful and dynamic tasks, respectively. Project page: https://github.com/ir-lab/Diff-Control},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Liu, Xiao and Weigend, Fabian and Zhou, Yifan and Amor, Heni Ben},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12539 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_physdreamer_2024,
	title = {{PhysDreamer}: {Physics}-{Based} {Interaction} with {3D} {Objects} via {Video} {Generation}},
	shorttitle = {{PhysDreamer}},
	url = {http://arxiv.org/abs/2404.13026},
	abstract = {Realistic object interactions are crucial for creating immersive virtual experiences, yet synthesizing realistic 3D object dynamics in response to novel interactions remains a significant challenge. Unlike unconditional or text-conditioned dynamics generation, action-conditioned dynamics requires perceiving the physical material properties of objects and grounding the 3D motion prediction on these properties, such as object stiffness. However, estimating physical material properties is an open problem due to the lack of material ground-truth data, as measuring these properties for real objects is highly difficult. We present PhysDreamer, a physics-based approach that endows static 3D objects with interactive dynamics by leveraging the object dynamics priors learned by video generation models. By distilling these priors, PhysDreamer enables the synthesis of realistic object responses to novel interactions, such as external forces or agent manipulations. We demonstrate our approach on diverse examples of elastic objects and evaluate the realism of the synthesized interactions through a user study. PhysDreamer takes a step towards more engaging and realistic virtual experiences by enabling static 3D objects to dynamically respond to interactive stimuli in a physically plausible manner. See our project page at https://physdreamer.github.io/.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Zhang, Tianyuan and Yu, Hong-Xing and Wu, Rundi and Feng, Brandon Y. and Zheng, Changxi and Snavely, Noah and Wu, Jiajun and Freeman, William T.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13026 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lemke_spot-compose_2024,
	title = {Spot-{Compose}: {A} {Framework} for {Open}-{Vocabulary} {Object} {Retrieval} and {Drawer} {Manipulation} in {Point} {Clouds}},
	shorttitle = {Spot-{Compose}},
	url = {http://arxiv.org/abs/2404.12440},
	abstract = {In recent years, modern techniques in deep learning and large-scale datasets have led to impressive progress in 3D instance segmentation, grasp pose estimation, and robotics. This allows for accurate detection directly in 3D scenes, object- and environment-aware grasp prediction, as well as robust and repeatable robotic manipulation. This work aims to integrate these recent methods into a comprehensive framework for robotic interaction and manipulation in human-centric environments. Specifically, we leverage 3D reconstructions from a commodity 3D scanner for open-vocabulary instance segmentation, alongside grasp pose estimation, to demonstrate dynamic picking of objects, and opening of drawers. We show the performance and robustness of our model in two sets of real-world experiments including dynamic object retrieval and drawer opening, reporting a 51\% and 82\% success rate respectively. Code of our framework as well as videos are available on: https://spot-compose.github.io/.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Lemke, Oliver and Bauer, Zuria and Zurbrügg, René and Pollefeys, Marc and Engelmann, Francis and Blum, Hermann},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12440 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, I.2.10, I.2.9},
}

@misc{xie_moving_2024,
	title = {Moving {Object} {Segmentation}: {All} {You} {Need} {Is} {SAM} (and {Flow})},
	shorttitle = {Moving {Object} {Segmentation}},
	url = {http://arxiv.org/abs/2404.12389},
	doi = {10.48550/arXiv.2404.12389},
	abstract = {The objective of this paper is motion segmentation -- discovering and segmenting the moving objects in a video. This is a much studied area with numerous careful,and sometimes complex, approaches and training schemes including: self-supervised learning, learning from synthetic datasets, object-centric representations, amodal representations, and many more. Our interest in this paper is to determine if the Segment Anything model (SAM) can contribute to this task. We investigate two models for combining SAM with optical flow that harness the segmentation power of SAM with the ability of flow to discover and group moving objects. In the first model, we adapt SAM to take optical flow, rather than RGB, as an input. In the second, SAM takes RGB as an input, and flow is used as a segmentation prompt. These surprisingly simple methods, without any further modifications, outperform all previous approaches by a considerable margin in both single and multi-object benchmarks. We also extend these frame-level segmentations to sequence-level segmentations that maintain object identity. Again, this simple model outperforms previous methods on multiple video object segmentation benchmarks.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Xie, Junyu and Yang, Charig and Xie, Weidi and Zisserman, Andrew},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12389 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{oconnell_neural-fly_2022,
	title = {Neural-{Fly} {Enables} {Rapid} {Learning} for {Agile} {Flight} in {Strong} {Winds}},
	volume = {7},
	issn = {2470-9476},
	url = {http://arxiv.org/abs/2205.06908},
	doi = {10.1126/scirobotics.abm6597},
	abstract = {Executing safe and precise flight maneuvers in dynamic high-speed winds is important for the ongoing commoditization of uninhabited aerial vehicles (UAVs). However, because the relationship between various wind conditions and its effect on aircraft maneuverability is not well understood, it is challenging to design effective robot controllers using traditional control design methods. We present Neural-Fly, a learning-based approach that allows rapid online adaptation by incorporating pretrained representations through deep learning. Neural-Fly builds on two key observations that aerodynamics in different wind conditions share a common representation and that the wind-specific part lies in a low-dimensional space. To that end, Neural-Fly uses a proposed learning algorithm, domain adversarially invariant meta-learning (DAIML), to learn the shared representation, only using 12 minutes of flight data. With the learned representation as a basis, Neural-Fly then uses a composite adaptation law to update a set of linear coefficients for mixing the basis elements. When evaluated under challenging wind conditions generated with the Caltech Real Weather Wind Tunnel, with wind speeds up to 43.6 kilometers/hour (12.1 meters/second), Neural-Fly achieves precise flight control with substantially smaller tracking error than state-of-the-art nonlinear and adaptive controllers. In addition to strong empirical performance, the exponential stability of Neural-Fly results in robustness guarantees. Last, our control design extrapolates to unseen wind conditions, is shown to be effective for outdoor flights with only onboard sensors, and can transfer across drones with minimal performance degradation.},
	number = {66},
	urldate = {2024-04-21},
	journal = {Science Robotics},
	author = {O'Connell, Michael and Shi, Guanya and Shi, Xichen and Azizzadenesheli, Kamyar and Anandkumar, Anima and Yue, Yisong and Chung, Soon-Jo},
	month = may,
	year = {2022},
	note = {arXiv:2205.06908 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	pages = {eabm6597},
}

@misc{tang_sparseocc_2024,
	title = {{SparseOcc}: {Rethinking} {Sparse} {Latent} {Representation} for {Vision}-{Based} {Semantic} {Occupancy} {Prediction}},
	shorttitle = {{SparseOcc}},
	url = {http://arxiv.org/abs/2404.09502},
	doi = {10.48550/arXiv.2404.09502},
	abstract = {Vision-based perception for autonomous driving requires an explicit modeling of a 3D space, where 2D latent representations are mapped and subsequent 3D operators are applied. However, operating on dense latent spaces introduces a cubic time and space complexity, which limits scalability in terms of perception range or spatial resolution. Existing approaches compress the dense representation using projections like Bird's Eye View (BEV) or Tri-Perspective View (TPV). Although efficient, these projections result in information loss, especially for tasks like semantic occupancy prediction. To address this, we propose SparseOcc, an efficient occupancy network inspired by sparse point cloud processing. It utilizes a lossless sparse latent representation with three key innovations. Firstly, a 3D sparse diffuser performs latent completion using spatially decomposed 3D sparse convolutional kernels. Secondly, a feature pyramid and sparse interpolation enhance scales with information from others. Finally, the transformer head is redesigned as a sparse variant. SparseOcc achieves a remarkable 74.9\% reduction on FLOPs over the dense baseline. Interestingly, it also improves accuracy, from 12.8\% to 14.1\% mIOU, which in part can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels.},
	urldate = {2024-04-21},
	publisher = {arXiv},
	author = {Tang, Pin and Wang, Zhongdao and Wang, Guoqing and Zheng, Jilai and Ren, Xiangxuan and Feng, Bailan and Ma, Chao},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{comi_snap-it_2024,
	title = {Snap-it, {Tap}-it, {Splat}-it: {Tactile}-{Informed} {3D} {Gaussian} {Splatting} for {Reconstructing} {Challenging} {Surfaces}},
	shorttitle = {Snap-it, {Tap}-it, {Splat}-it},
	url = {http://arxiv.org/abs/2403.20275},
	abstract = {Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object's geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Comi, Mauro and Tonioni, Alessio and Yang, Max and Tremblay, Jonathan and Blukis, Valts and Lin, Yijiong and Lepora, Nathan F. and Aitchison, Laurence},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20275 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{bahl_affordances_2023,
	title = {Affordances from {Human} {Videos} as a {Versatile} {Representation} for {Robotics}},
	url = {http://arxiv.org/abs/2304.08488},
	doi = {10.48550/arXiv.2304.08488},
	abstract = {Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call VRB, across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild. Results, visualizations and videos at https://robo-affordances.github.io/},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08488 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@misc{xiao_spatialtracker_2024,
	title = {{SpatialTracker}: {Tracking} {Any} {2D} {Pixels} in {3D} {Space}},
	shorttitle = {{SpatialTracker}},
	url = {http://arxiv.org/abs/2404.04319},
	abstract = {Recovering dense and long-range pixel motion in videos is a challenging problem. Part of the difficulty arises from the 3D-to-2D projection process, leading to occlusions and discontinuities in the 2D motion domain. While 2D motion can be intricate, we posit that the underlying 3D motion can often be simple and low-dimensional. In this work, we propose to estimate point trajectories in 3D space to mitigate the issues caused by image projection. Our method, named SpatialTracker, lifts 2D pixels to 3D using monocular depth estimators, represents the 3D content of each frame efficiently using a triplane representation, and performs iterative updates using a transformer to estimate 3D trajectories. Tracking in 3D allows us to leverage as-rigid-as-possible (ARAP) constraints while simultaneously learning a rigidity embedding that clusters pixels into different rigid parts. Extensive evaluation shows that our approach achieves state-of-the-art tracking performance both qualitatively and quantitatively, particularly in challenging scenarios such as out-of-plane rotation.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Xiao, Yuxi and Wang, Qianqian and Zhang, Shangzhan and Xue, Nan and Peng, Sida and Shen, Yujun and Zhou, Xiaowei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04319 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_2d_2024,
	title = {{2D} {Gaussian} {Splatting} for {Geometrically} {Accurate} {Radiance} {Fields}},
	url = {https://arxiv.org/abs/2403.17888v1},
	abstract = {3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Our code will be made publicly available.},
	language = {en},
	urldate = {2024-04-19},
	author = {Huang, Binbin and Yu, Zehao and Chen, Anpei and Geiger, Andreas and Gao, Shenghua},
	month = mar,
	year = {2024},
}

@misc{roth_viplanner_2023,
	title = {{ViPlanner}: {Visual} {Semantic} {Imperative} {Learning} for {Local} {Navigation}},
	shorttitle = {{ViPlanner}},
	url = {https://arxiv.org/abs/2310.00982v1},
	abstract = {Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02\% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner.},
	language = {en},
	urldate = {2024-04-21},
	author = {Roth, Pascal and Nubert, Julian and Yang, Fan and Mittal, Mayank and Hutter, Marco},
	month = oct,
	year = {2023},
}

@misc{geist_learning_2024,
	title = {Learning with {3D} rotations, a hitchhiker's guide to {SO}(3)},
	url = {http://arxiv.org/abs/2404.11735},
	abstract = {Many settings in machine learning require the selection of a rotation representation. However, choosing a suitable representation from the many available options is challenging. This paper acts as a survey and guide through rotation representations. We walk through their properties that harm or benefit deep learning with gradient-based optimization. By consolidating insights from rotation-based learning, we provide a comprehensive overview of learning functions with rotation representations. We provide guidance on selecting representations based on whether rotations are in the model's input or output and whether the data primarily comprises small angles.},
	urldate = {2024-04-20},
	publisher = {arXiv},
	author = {Geist, A. René and Frey, Jonas and Zobro, Mikel and Levina, Anna and Martius, Georg},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11735 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_dynamic_2024,
	title = {Dynamic {Gaussians} {Mesh}: {Consistent} {Mesh} {Reconstruction} from {Monocular} {Videos}},
	shorttitle = {Dynamic {Gaussians} {Mesh}},
	url = {http://arxiv.org/abs/2404.12379},
	doi = {10.48550/arXiv.2404.12379},
	abstract = {Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Liu, Isabella and Su, Hao and Wang, Xiaolong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12379 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_robodreamer_2024,
	title = {{RoboDreamer}: {Learning} {Compositional} {World} {Models} for {Robot} {Imagination}},
	shorttitle = {{RoboDreamer}},
	url = {http://arxiv.org/abs/2404.12377},
	doi = {10.48550/arXiv.2404.12377},
	abstract = {Text-to-video models have demonstrated substantial potential in robotic decision-making, enabling the imagination of realistic plans of future actions as well as accurate environment simulation. However, one major issue in such models is generalization -- models are limited to synthesizing videos subject to language instructions similar to those seen at training time. This is heavily limiting in decision-making, where we seek a powerful world model to synthesize plans of unseen combinations of objects and actions in order to solve previously unseen tasks in new environments. To resolve this issue, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation. We leverage the natural compositionality of language to parse instructions into a set of lower-level primitives, which we condition a set of models on to generate videos. We illustrate how this factorization naturally enables compositional generalization, by allowing us to formulate a new natural language instruction as a combination of previously seen components. We further show how such a factorization enables us to add additional multimodal goals, allowing us to specify a video we wish to generate given both natural language instructions and a goal image. Our approach can successfully synthesize video plans on unseen goals in the RT-X, enables successful robot execution in simulation, and substantially outperforms monolithic baseline approaches to video generation.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Zhou, Siyuan and Du, Yilun and Chen, Jiaben and Li, Yandong and Yeung, Dit-Yan and Gan, Chuang},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12377 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{dong_caging_2024,
	title = {Caging in {Motion}: {Characterizing} {Robustness} in {Manipulation} through {Energy} {Margin} and {Dynamic} {Caging} {Analysis}},
	shorttitle = {Caging in {Motion}},
	url = {http://arxiv.org/abs/2404.12115},
	doi = {10.48550/arXiv.2404.12115},
	abstract = {To develop robust manipulation policies, quantifying robustness is essential. Evaluating robustness in general dexterous manipulation, nonetheless, poses significant challenges due to complex hybrid dynamics, combinatorial explosion of possible contact interactions, global geometry, etc. This paper introduces ``caging in motion'', an approach for analyzing manipulation robustness through energy margins and caging-based analysis. Our method assesses manipulation robustness by measuring the energy margin to failure and extends traditional caging concepts for a global analysis of dynamic manipulation. This global analysis is facilitated by a kinodynamic planning framework that naturally integrates global geometry, contact changes, and robot compliance. We validate the effectiveness of our approach in the simulation and real-world experiments of multiple dynamic manipulation scenarios, highlighting its potential to predict manipulation success and robustness.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Dong, Yifei and Cheng, Xianyi and Pokorny, Florian T.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12115 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{somraj_factorized_2024,
	title = {Factorized {Motion} {Fields} for {Fast} {Sparse} {Input} {Dynamic} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2404.11669},
	abstract = {Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task. While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints. In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints. However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learning. Existing fast dynamic scene models do not explicitly model the motion, making them difficult to be constrained with motion priors. We design an explicit motion model as a factorized 4D representation that is fast and can exploit the spatio-temporal correlation of the motion field. We then introduce reliable flow priors including a combination of sparse flow priors across cameras and dense flow priors within cameras to regularize our motion model. Our model is fast, compact and achieves very good performance on popular multi-view dynamic scene datasets with sparse input viewpoints. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Somraj, Nagabhushan and Choudhary, Kapil and Mupparaju, Sai Harsha and Soundararajan, Rajiv},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11669 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ye_g-hop_2024,
	title = {G-{HOP}: {Generative} {Hand}-{Object} {Prior} for {Interaction} {Reconstruction} and {Grasp} {Synthesis}},
	shorttitle = {G-{HOP}},
	url = {http://arxiv.org/abs/2404.12383},
	doi = {10.48550/arXiv.2404.12383},
	abstract = {We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines. Project website: https://judyye.github.io/ghop-www},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Ye, Yufei and Gupta, Abhinav and Kitani, Kris and Tulsiani, Shubham},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12383 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_rise_2024,
	title = {{RISE}: {3D} {Perception} {Makes} {Real}-{World} {Robot} {Imitation} {Simple} and {Effective}},
	shorttitle = {{RISE}},
	url = {http://arxiv.org/abs/2404.12281},
	doi = {10.48550/arXiv.2404.12281},
	abstract = {Precise robot manipulations require rich spatial information in imitation learning. Image-based policies model object positions from fixed cameras, which are sensitive to camera view changes. Policies utilizing 3D point clouds usually predict keyframes rather than continuous actions, posing difficulty in dynamic and contact-rich scenarios. To utilize 3D perception efficiently, we present RISE, an end-to-end baseline for real-world imitation learning, which predicts continuous actions directly from single-view point clouds. It compresses the point cloud to tokens with a sparse 3D encoder. After adding sparse positional encoding, the tokens are featurized using a transformer. Finally, the features are decoded into robot actions by a diffusion head. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency. Experiments also demonstrate that RISE is more general and robust to environmental change compared with previous baselines. Project website: rise-policy.github.io.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Wang, Chenxi and Fang, Hongjie and Fang, Hao-Shu and Lu, Cewu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12281 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{jing_cross-modal_2021,
	title = {Cross-{Modal} {Center} {Loss} for {3D} {Cross}-{Modal} {Retrieval}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-04-19},
	author = {Jing, Longlong and Vahdani, Elahe and Tan, Jiaxing and Tian, Yingli},
	year = {2021},
	pages = {3142--3151},
}

@misc{vacek_let_2024,
	title = {Let {It} {Flow}: {Simultaneous} {Optimization} of {3D} {Flow} and {Object} {Clustering}},
	shorttitle = {Let {It} {Flow}},
	url = {http://arxiv.org/abs/2404.08363},
	doi = {10.48550/arXiv.2404.08363},
	abstract = {We study the problem of self-supervised 3D scene flow estimation from real large-scale raw point cloud sequences, which is crucial to various tasks like trajectory prediction or instance segmentation. In the absence of ground truth scene flow labels, contemporary approaches concentrate on deducing optimizing flow across sequential pairs of point clouds by incorporating structure based regularization on flow and object rigidity. The rigid objects are estimated by a variety of 3D spatial clustering methods. While state-of-the-art methods successfully capture overall scene motion using the Neural Prior structure, they encounter challenges in discerning multi-object motions. We identified the structural constraints and the use of large and strict rigid clusters as the main pitfall of the current approaches and we propose a novel clustering approach that allows for combination of overlapping soft clusters as well as non-overlapping rigid clusters representation. Flow is then jointly estimated with progressively growing non-overlapping rigid clusters together with fixed size overlapping soft clusters. We evaluate our method on multiple datasets with LiDAR point clouds, demonstrating the superior performance over the self-supervised baselines reaching new state of the art results. Our method especially excels in resolving flow in complicated dynamic scenes with multiple independently moving objects close to each other which includes pedestrians, cyclists and other vulnerable road users. Our codes will be publicly available.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Vacek, Patrik and Hurych, David and Svoboda, Tomáš and Zimmermann, Karel},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08363 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{piccinelli_unidepth_2024,
	title = {{UniDepth}: {Universal} {Monocular} {Metric} {Depth} {Estimation}},
	shorttitle = {{UniDepth}},
	url = {http://arxiv.org/abs/2403.18913},
	doi = {10.48550/arXiv.2403.18913},
	abstract = {Accurate monocular metric depth estimation (MMDE) is crucial to solving downstream tasks in 3D perception and modeling. However, the remarkable accuracy of recent MMDE methods is confined to their training domains. These methods fail to generalize to unseen domains even in the presence of moderate domain gaps, which hinders their practical applicability. We propose a new model, UniDepth, capable of reconstructing metric 3D scenes from solely single images across domains. Departing from the existing MMDE methods, UniDepth directly predicts metric 3D points from the input image at inference time without any additional information, striving for a universal and flexible MMDE solution. In particular, UniDepth implements a self-promptable camera module predicting dense camera representation to condition depth features. Our model exploits a pseudo-spherical output representation, which disentangles camera and depth representations. In addition, we propose a geometric invariance loss that promotes the invariance of camera-prompted depth features. Thorough evaluations on ten datasets in a zero-shot regime consistently demonstrate the superior performance of UniDepth, even when compared with methods directly trained on the testing domains. Code and models are available at: https://github.com/lpiccinelli-eth/unidepth},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Piccinelli, Luigi and Yang, Yung-Hsu and Sakaridis, Christos and Segu, Mattia and Li, Siyuan and Van Gool, Luc and Yu, Fisher},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18913 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_text-controlled_2024,
	title = {Text-controlled {Motion} {Mamba}: {Text}-{Instructed} {Temporal} {Grounding} of {Human} {Motion}},
	shorttitle = {Text-controlled {Motion} {Mamba}},
	url = {http://arxiv.org/abs/2404.11375},
	doi = {10.48550/arXiv.2404.11375},
	abstract = {Human motion understanding is a fundamental task with diverse practical applications, facilitated by the availability of large-scale motion capture datasets. Recent studies focus on text-motion tasks, such as text-based motion generation, editing and question answering. In this study, we introduce the novel task of text-based human motion grounding (THMG), aimed at precisely localizing temporal segments corresponding to given textual descriptions within untrimmed motion sequences. Capturing global temporal information is crucial for the THMG task. However, transformer-based models that rely on global temporal self-attention face challenges when handling long untrimmed sequences due to the quadratic computational cost. We address these challenges by proposing Text-controlled Motion Mamba (TM-Mamba), a unified model that integrates temporal global context, language query control, and spatial graph topology with only linear memory cost. The core of the model is a text-controlled selection mechanism which dynamically incorporates global temporal information based on text query. The model is further enhanced to be topology-aware through the integration of relational embeddings. For evaluation, we introduce BABEL-Grounding, the first text-motion dataset that provides detailed textual descriptions of human actions along with their corresponding temporal segments. Extensive evaluations demonstrate the effectiveness of TM-Mamba on BABEL-Grounding.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Wang, Xinghan and Kang, Zixi and Mu, Yadong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11375 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@misc{yoon_spatio-temporal_2024,
	title = {Spatio-{Temporal} {Motion} {Retargeting} for {Quadruped} {Robots}},
	url = {http://arxiv.org/abs/2404.11557},
	doi = {10.48550/arXiv.2404.11557},
	abstract = {This work introduces a motion retargeting approach for legged robots, which aims to create motion controllers that imitate the fine behavior of animals. Our approach, namely spatio-temporal motion retargeting (STMR), guides imitation learning procedures by transferring motion from source to target, effectively bridging the morphological disparities by ensuring the feasibility of imitation on the target system. Our STMR method comprises two components: spatial motion retargeting (SMR) and temporal motion retargeting (TMR). On the one hand, SMR tackles motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories. On the other hand, TMR aims to retarget motion at the dynamic level by optimizing motion in the temporal domain. We showcase the effectiveness of our method in facilitating Imitation Learning (IL) for complex animal movements through a series of simulation and hardware experiments. In these experiments, our STMR method successfully tailored complex animal motions from various media, including video captured by a hand-held camera, to fit the morphology and physical properties of the target robots. This enabled RL policy training for precise motion tracking, while baseline methods struggled with highly dynamic motion involving flying phases. Moreover, we validated that the control policy can successfully imitate six different motions in two quadruped robots with different dimensions and physical properties in real-world settings.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Yoon, Taerim and Kang, Dongho and Kim, Seungmin and Ahn, Minsung and Coros, Stelian and Choi, Sungjoon},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11557 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{park_learning_2024,
	title = {Learning {SO}(3)-{Invariant} {Semantic} {Correspondence} via {Local} {Shape} {Transform}},
	url = {http://arxiv.org/abs/2404.11156},
	doi = {10.48550/arXiv.2404.11156},
	abstract = {Establishing accurate 3D correspondences between shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised Rotation-Invariant 3D correspondence learner with Local Shape Transform, dubbed RIST, that learns to establish dense correspondences between shapes even under challenging intra-class variations and arbitrary orientations. Specifically, RIST learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shapes to be mapped to similar local shape descriptors, enabling RIST to establish dense point-wise correspondences. RIST demonstrates state-of-the-art performances on 3D part label transfer and semantic keypoint transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Park, Chunghyun and Sim, Seungwook and Park, Jaesik and Cho, Minsu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11156 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deng_vg4d_2024,
	title = {{VG4D}: {Vision}-{Language} {Model} {Goes} {4D} {Video} {Recognition}},
	shorttitle = {{VG4D}},
	url = {http://arxiv.org/abs/2404.11605},
	doi = {10.48550/arXiv.2404.11605},
	abstract = {Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network. Our approach involves aligning the 4D encoder's representation with a VLM to learn a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset. Code is available at {\textbackslash}url\{https://github.com/Shark0-0/VG4D\}.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Deng, Zhichao and Li, Xiangtai and Li, Xia and Tong, Yunhai and Zhao, Shen and Liu, Mengyuan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11605 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhi_closed-loop_2024,
	title = {Closed-{Loop} {Open}-{Vocabulary} {Mobile} {Manipulation} with {GPT}-{4V}},
	url = {http://arxiv.org/abs/2404.10220},
	abstract = {Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate ({\textasciitilde}25\%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Zhi, Peiyuan and Zhang, Zhiyuan and Han, Muzhi and Zhang, Zeyu and Li, Zhitian and Jiao, Ziyuan and Jia, Baoxiong and Huang, Siyuan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10220 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yi_generating_2024,
	title = {Generating {Human} {Interaction} {Motions} in {Scenes} with {Text} {Control}},
	url = {http://arxiv.org/abs/2404.10685},
	doi = {10.48550/arXiv.2404.10685},
	abstract = {We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions. Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Yi, Hongwei and Thies, Justus and Black, Michael J. and Peng, Xue Bin and Rempe, Davis},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10685 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{tang_foundationgrasp_2024,
	title = {{FoundationGrasp}: {Generalizable} {Task}-{Oriented} {Grasping} with {Foundation} {Models}},
	shorttitle = {{FoundationGrasp}},
	url = {http://arxiv.org/abs/2404.10399},
	doi = {10.48550/arXiv.2404.10399},
	abstract = {Task-oriented grasping (TOG), which refers to the problem of synthesizing grasps on an object that are configurationally compatible with the downstream manipulation task, is the first milestone towards tool manipulation. Analogous to the activation of two brain regions responsible for semantic and geometric reasoning during cognitive processes, modeling the complex relationship between objects, tasks, and grasps requires rich prior knowledge about objects and tasks. Existing methods typically limit the prior knowledge to a closed-set scope and cannot support the generalization to novel objects and tasks out of the training set. To address such a limitation, we propose FoundationGrasp, a foundation model-based TOG framework that leverages the open-ended knowledge from foundation models to learn generalizable TOG skills. Comprehensive experiments are conducted on the contributed Language and Vision Augmented TaskGrasp (LaViA-TaskGrasp) dataset, demonstrating the superiority of FoudationGrasp over existing methods when generalizing to novel object instances, object classes, and tasks out of the training set. Furthermore, the effectiveness of FoudationGrasp is validated in real-robot grasping and manipulation experiments on a 7 DoF robotic arm. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/foundationgrasp.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Tang, Chao and Huang, Dehao and Dong, Wenlong and Xu, Ruinian and Zhang, Hong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10399 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{hou_learning_2024,
	title = {Learning {Human} {Motion} from {Monocular} {Videos} via {Cross}-{Modal} {Manifold} {Alignment}},
	url = {http://arxiv.org/abs/2404.09499},
	doi = {10.48550/arXiv.2404.09499},
	abstract = {Learning 3D human motion from 2D inputs is a fundamental task in the realms of computer vision and computer graphics. Many previous methods grapple with this inherently ambiguous task by introducing motion priors into the learning process. However, these approaches face difficulties in defining the complete configurations of such priors or training a robust model. In this paper, we present the Video-to-Motion Generator (VTM), which leverages motion priors through cross-modal latent feature space alignment between 3D human motion and 2D inputs, namely videos and 2D keypoints. To reduce the complexity of modeling motion priors, we model the motion data separately for the upper and lower body parts. Additionally, we align the motion data with a scale-invariant virtual skeleton to mitigate the interference of human skeleton variations to the motion priors. Evaluated on AIST++, the VTM showcases state-of-the-art performance in reconstructing 3D human motion from monocular videos. Notably, our VTM exhibits the capabilities for generalization to unseen view angles and in-the-wild videos.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Hou, Shuaiying and Tao, Hongyu and Fang, Junheng and Zou, Changqing and Bao, Hujun and Xu, Weiwei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09499 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{yang_physcene_2024,
	title = {{PhyScene}: {Physically} {Interactable} {3D} {Scene} {Synthesis} for {Embodied} {AI}},
	shorttitle = {{PhyScene}},
	url = {http://arxiv.org/abs/2404.09465},
	doi = {10.48550/arXiv.2404.09465},
	abstract = {With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: http://physcene.github.io.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Yang, Yandan and Jia, Baoxiong and Zhi, Peiyuan and Huang, Siyuan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09465 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xu_vifu_2024,
	title = {{ViFu}: {Multiple} 360\${\textasciicircum}{\textbackslash}circ\$ {Objects} {Reconstruction} with {Clean} {Background} via {Visible} {Part} {Fusion}},
	shorttitle = {{ViFu}},
	url = {http://arxiv.org/abs/2404.09426},
	doi = {10.48550/arXiv.2404.09426},
	abstract = {In this paper, we propose a method to segment and recover a static, clean background and multiple 360\${\textasciicircum}{\textbackslash}circ\$ objects from observations of scenes at different timestamps. Recent works have used neural radiance fields to model 3D scenes and improved the quality of novel view synthesis, while few studies have focused on modeling the invisible or occluded parts of the training images. These under-reconstruction parts constrain both scene editing and rendering view selection, thereby limiting their utility for synthetic data generation for downstream tasks. Our basic idea is that, by observing the same set of objects in various arrangement, so that parts that are invisible in one scene may become visible in others. By fusing the visible parts from each scene, occlusion-free rendering of both background and foreground objects can be achieved. We decompose the multi-scene fusion task into two main components: (1) objects/background segmentation and alignment, where we leverage point cloud-based methods tailored to our novel problem formulation; (2) radiance fields fusion, where we introduce visibility field to quantify the visible information of radiance fields, and propose visibility-aware rendering for the fusion of series of scenes, ultimately obtaining clean background and 360\${\textasciicircum}{\textbackslash}circ\$ object rendering. Comprehensive experiments were conducted on synthetic and real datasets, and the results demonstrate the effectiveness of our method.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Xu, Tianhan and Ikeda, Takuya and Nishiwaki, Koichi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_physics-aware_2024,
	title = {Physics-{Aware} {Iterative} {Learning} and {Prediction} of {Saliency} {Map} for {Bimanual} {Grasp} {Planning}},
	url = {http://arxiv.org/abs/2404.08944},
	doi = {10.48550/arXiv.2404.08944},
	abstract = {Learning the skill of human bimanual grasping can extend the capabilities of robotic systems when grasping large or heavy objects. However, it requires a much larger search space for grasp points than single-hand grasping and numerous bimanual grasping annotations for network learning, making both data-driven or analytical grasping methods inefficient and insufficient. We propose a framework for bimanual grasp saliency learning that aims to predict the contact points for bimanual grasping based on existing human single-handed grasping data. We learn saliency corresponding vectors through minimal bimanual contact annotations that establishes correspondences between grasp positions of both hands, capable of eliminating the need for training a large-scale bimanual grasp dataset. The existing single-handed grasp saliency value serves as the initial value for bimanual grasp saliency, and we learn a saliency adjusted score that adds the initial value to obtain the final bimanual grasp saliency value, capable of predicting preferred bimanual grasp positions from single-handed grasp saliency. We also introduce a physics-balance loss function and a physics-aware refinement module that enables physical grasp balance, capable of enhancing the generalization of unknown objects. Comprehensive experiments in simulation and comparisons on dexterous grippers have demonstrated that our method can achieve balanced bimanual grasping effectively.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Wang, Shiyao and Liu, Xiuping and Wang, Charlie C. L. and Liu, Jian},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08944 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_multi-fingered_2024,
	title = {Multi-fingered {Robotic} {Hand} {Grasping} in {Cluttered} {Environments} through {Hand}-object {Contact} {Semantic} {Mapping}},
	url = {http://arxiv.org/abs/2404.08844},
	doi = {10.48550/arXiv.2404.08844},
	abstract = {The integration of optimization method and generative models has significantly advanced dexterous manipulation techniques for five-fingered hand grasping. Yet, the application of these techniques in cluttered environments is a relatively unexplored area. To address this research gap, we have developed a novel method for generating five-fingered hand grasp samples in cluttered settings. This method emphasizes simulated grasp quality and the nuanced interaction between the hand and surrounding objects. A key aspect of our approach is our data generation method, capable of estimating contact spatial and semantic representations and affordance grasps based on object affordance information. Furthermore, our Contact Semantic Conditional Variational Autoencoder (CoSe-CVAE) network is adept at creating comprehensive contact maps from point clouds, incorporating both spatial and semantic data. We introduce a unique grasp detection technique that efficiently formulates mechanical hand grasp poses from these maps. Additionally, our evaluation model is designed to assess grasp quality and collision probability, significantly improving the practicality of five-fingered hand grasping in complex scenarios. Our data generation method outperforms previous datasets in grasp diversity, scene diversity, modality diversity. Our grasp generation method has demonstrated remarkable success, outperforming established baselines with 81.0\% average success rate in real-world single-object grasping and 75.3\% success rate in multi-object grasping. The dataset and supplementary materials can be found at https://sites.google.com/view/ffh-clutteredgrasping, and we will release the code upon publication.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Zhang, Lei and Bai, Kaixin and Huang, Guowen and Chen, Zhaopeng and Zhang, Jianwei},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08844 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{she_learning_2024,
	title = {Learning {Cross}-hand {Policies} for {High}-{DOF} {Reaching} and {Grasping}},
	url = {http://arxiv.org/abs/2404.09150},
	doi = {10.48550/arXiv.2404.09150},
	abstract = {Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper without retraining. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of predefined key points on the gripper, and a gripper specific adaptation model that translates these displacements into adjustments for controlling the grippers' joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry. In the experimental part, we evaluate our method on several dexterous grippers and objects of diverse shapes, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across different dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {She, Qijin and Zhang, Shishun and Ye, Yunfan and Liu, Min and Hu, Ruizhen and Xu, Kai},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09150 [cs]},
	keywords = {Computer Science - Graphics, Computer Science - Robotics},
}

@inproceedings{valassakis_learning_2022,
	title = {Learning {Eye}-in-{Hand} {Camera} {Calibration} from a {Single} {Image}},
	url = {https://proceedings.mlr.press/v164/valassakis22a.html},
	abstract = {Eye-in-hand camera calibration is a fundamental and long-studied problem in robotics. We present a study on using learning-based methods for solving this problem online from a single RGB image, whilst training our models with entirely synthetic data. We study three main approaches: one direct regression model that directly predicts the extrinsic matrix from an image, one sparse correspondence model that regresses 2D keypoints and then uses PnP, and one dense correspondence model that uses regressed depth and segmentation maps to enable ICP pose estimation. In our experiments, we benchmark these methods against each other and against well-established classical methods, to find the surprising result that direct regression outperforms other approaches, and we perform noise-sensitivity analysis to gain further insights into these results.},
	language = {en},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Valassakis, Eugene and Dreczkowski, Kamil and Johns, Edward},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1336--1346},
}

@misc{cao_vector-based_2020,
	title = {A {Vector}-based {Representation} to {Enhance} {Head} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2010.07184},
	doi = {10.48550/arXiv.2010.07184},
	abstract = {This paper proposes to use the three vectors in a rotation matrix as the representation in head pose estimation and develops a new neural network based on the characteristic of such representation. We address two potential issues existed in current head pose estimation works: 1. Public datasets for head pose estimation use either Euler angles or quaternions to annotate data samples. However, both of these annotations have the issue of discontinuity and thus could result in some performance issues in neural network training. 2. Most research works report Mean Absolute Error (MAE) of Euler angles as the measurement of performance. We show that MAE may not reflect the actual behavior especially for the cases of profile views. To solve these two problems, we propose a new annotation method which uses three vectors to describe head poses and a new measurement Mean Absolute Error of Vectors (MAEV) to assess the performance. We also train a new neural network to predict the three vectors with the constraints of orthogonality. Our proposed method achieves state-of-the-art results on both AFLW2000 and BIWI datasets. Experiments show our vector-based annotation method can effectively reduce prediction errors for large pose angles.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Cao, Zhiwen and Chu, Zongcheng and Liu, Dongfang and Chen, Yingjie},
	month = dec,
	year = {2020},
	note = {arXiv:2010.07184 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_mode-adaptive_2018,
	title = {Mode-adaptive neural networks for quadruped motion control},
	volume = {37},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201366},
	doi = {10.1145/3197517.3201366},
	abstract = {Quadruped motion includes a wide variation of gaits such as walk, pace, trot and canter, and actions such as jumping, sitting, turning and idling. Applying existing data-driven character control frameworks to such data requires a significant amount of data preprocessing such as motion labeling and alignment. In this paper, we propose a novel neural network architecture called Mode-Adaptive Neural Networks for controlling quadruped characters. The system is composed of the motion prediction network and the gating network. At each frame, the motion prediction network computes the character state in the current frame given the state in the previous frame and the user-provided control signals. The gating network dynamically updates the weights of the motion prediction network by selecting and blending what we call the expert weights, each of which specializes in a particular movement. Due to the increased flexibility, the system can learn consistent expert weights across a wide range of non-periodic/periodic actions, from unstructured motion capture data, in an end-to-end fashion. In addition, the users are released from performing complex labeling of phases in different gaits. We show that this architecture is suitable for encoding the multi-modality of quadruped locomotion and synthesizing responsive motion in real-time.},
	number = {4},
	urldate = {2024-04-13},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, He and Starke, Sebastian and Komura, Taku and Saito, Jun},
	month = jul,
	year = {2018},
	keywords = {character animation, character control, deep learning, human motion, locomotion, neural networks},
	pages = {145:1--145:11},
}

@article{starke_deepphase_2022,
	title = {{DeepPhase}: periodic autoencoders for learning motion phase manifolds},
	volume = {41},
	issn = {0730-0301},
	shorttitle = {{DeepPhase}},
	url = {https://doi.org/10.1145/3528223.3530178},
	doi = {10.1145/3528223.3530178},
	abstract = {Learning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases.},
	number = {4},
	urldate = {2024-04-13},
	journal = {ACM Transactions on Graphics},
	author = {Starke, Sebastian and Mason, Ian and Komura, Taku},
	month = jul,
	year = {2022},
	keywords = {character animation, character control, character interactions, deep learning, human motion, neural networks},
	pages = {136:1--136:13},
}

@misc{mattamala_wild_2024,
	title = {Wild {Visual} {Navigation}: {Fast} {Traversability} {Learning} via {Pre}-{Trained} {Models} and {Online} {Self}-{Supervision}},
	shorttitle = {Wild {Visual} {Navigation}},
	url = {http://arxiv.org/abs/2404.07110},
	doi = {10.48550/arXiv.2404.07110},
	abstract = {Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN), an online self-supervised learning system for visual traversability estimation. The system is able to continuously adapt from a short human demonstration in the field, only using onboard sensing and computing. One of the key ideas to achieve this is the use of high-dimensional features from pre-trained self-supervised models, which implicitly encode semantic information that massively simplifies the learning task. Further, the development of an online scheme for supervision generator enables concurrent training and inference of the learned model in the wild. We demonstrate our approach through diverse real-world deployments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex, previously unseen outdoor terrains. Code: https://bit.ly/498b0CV - Project page:https://bit.ly/3M6nMHH},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Mattamala, Matías and Frey, Jonas and Libera, Piotr and Chebrolu, Nived and Martius, Georg and Cadena, Cesar and Hutter, Marco and Fallon, Maurice},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07110 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{huang_zero-shot_2024,
	title = {Zero-shot {Point} {Cloud} {Completion} {Via} {2D} {Priors}},
	url = {http://arxiv.org/abs/2404.06814},
	doi = {10.48550/arXiv.2404.06814},
	abstract = {3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training \%, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Huang, Tianxin and Yan, Zhiwen and Zhao, Yuyang and Lee, Gim Hee},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06814 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{burns_genchip_2024,
	title = {{GenCHiP}: {Generating} {Robot} {Policy} {Code} for {High}-{Precision} and {Contact}-{Rich} {Manipulation} {Tasks}},
	shorttitle = {{GenCHiP}},
	url = {http://arxiv.org/abs/2404.06645},
	abstract = {Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks. Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Burns, Kaylee and Jain, Ajinkya and Go, Keegan and Xia, Fei and Stark, Michael and Schaal, Stefan and Hausman, Karol},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06645 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, I.2.9},
}

@misc{lu_3d_2024,
	title = {{3D} {Geometry}-aware {Deformable} {Gaussian} {Splatting} for {Dynamic} {View} {Synthesis}},
	url = {http://arxiv.org/abs/2404.06270},
	doi = {10.48550/arXiv.2404.06270},
	abstract = {In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance. The project is available at https://npucvr.github.io/GaGS/},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Lu, Zhicheng and Guo, Xiang and Hui, Le and Chen, Tianrui and Yang, Min and Tang, Xiao and Zhu, Feng and Dai, Yuchao},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06270 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_reconstructing_2024,
	title = {Reconstructing {Hand}-{Held} {Objects} in {3D}},
	url = {http://arxiv.org/abs/2404.06507},
	doi = {10.48550/arXiv.2404.06507},
	abstract = {Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos. Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels. At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects. With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets. Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object geometry given a single RGB image and inferred 3D hand as inputs. Subsequently, we use GPT-4(V) to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred geometry; we call this alignment Retrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Wu, Jane and Pavlakos, Georgios and Gkioxari, Georgia and Malik, Jitendra},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06507 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_omnicolor_2024,
	title = {{OmniColor}: {A} {Global} {Camera} {Pose} {Optimization} {Approach} of {LiDAR}-{360Camera} {Fusion} for {Colorizing} {Point} {Clouds}},
	shorttitle = {{OmniColor}},
	url = {http://arxiv.org/abs/2404.04693},
	doi = {10.48550/arXiv.2404.04693},
	abstract = {A Colored point cloud, as a simple and efficient 3D representation, has many advantages in various fields, including robotic navigation and scene reconstruction. This representation is now commonly used in 3D reconstruction tasks relying on cameras and LiDARs. However, fusing data from these two types of sensors is poorly performed in many existing frameworks, leading to unsatisfactory mapping results, mainly due to inaccurate camera poses. This paper presents OmniColor, a novel and efficient algorithm to colorize point clouds using an independent 360-degree camera. Given a LiDAR-based point cloud and a sequence of panorama images with initial coarse camera poses, our objective is to jointly optimize the poses of all frames for mapping images onto geometric reconstructions. Our pipeline works in an off-the-shelf manner that does not require any feature extraction or matching process. Instead, we find optimal poses by directly maximizing the photometric consistency of LiDAR maps. In experiments, we show that our method can overcome the severe visual distortion of omnidirectional images and greatly benefit from the wide field of view (FOV) of 360-degree cameras to reconstruct various scenarios with accuracy and stability. The code will be released at https://github.com/liubonan123/OmniColor/.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Liu, Bonan and Zhao, Guoyang and Jiao, Jianhao and Cai, Guang and Li, Chengyang and Yin, Handi and Wang, Yuyang and Liu, Ming and Hui, Pan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04693 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{nam_joint_2024,
	title = {Joint {Reconstruction} of {3D} {Human} and {Object} via {Contact}-{Based} {Refinement} {Transformer}},
	url = {http://arxiv.org/abs/2404.04819},
	doi = {10.48550/arXiv.2404.04819},
	abstract = {Human-object contact serves as a strong cue to understand how humans physically interact with objects. Nevertheless, it is not widely explored to utilize human-object contact information for the joint reconstruction of 3D human and object from a single image. In this work, we present a novel joint 3D human-object reconstruction method (CONTHO) that effectively exploits contact information between humans and objects. There are two core designs in our system: 1) 3D-guided contact estimation and 2) contact-based 3D human and object refinement. First, for accurate human-object contact estimation, CONTHO initially reconstructs 3D humans and objects and utilizes them as explicit 3D guidance for contact estimation. Second, to refine the initial reconstructions of 3D human and object, we propose a novel contact-based refinement Transformer that effectively aggregates human features and object features based on the estimated human-object contact. The proposed contact-based refinement prevents the learning of erroneous correlation between human and object, which enables accurate 3D reconstruction. As a result, our CONTHO achieves state-of-the-art performance in both human-object contact estimation and joint reconstruction of 3D human and object. The code is publicly available at https://github.com/dqj5182/CONTHO\_RELEASE.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Nam, Hyeongjin and Jung, Daniel Sungho and Moon, Gyeongsik and Lee, Kyoung Mu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04819 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{han_learning_2024,
	title = {Learning {Prehensile} {Dexterity} by {Imitating} and {Emulating} {State}-only {Observations}},
	url = {http://arxiv.org/abs/2404.05582},
	doi = {10.48550/arXiv.2404.05582},
	abstract = {When humans learn physical skills (e.g., learn to play tennis), we tend to first observe and learn what an expert is doing. But this is often insufficient. Therefore, we subsequently engage in practice, where we try to emulate the expert. Inspired by this observation, we introduce Combining IMitation and Emulation for Motion Refinement (CIMER) -- a two-stage framework to learn dexterous prehensile manipulation skills from state-only observations. CIMER's first stage involves imitation: simultaneously encode the complex interdependent motions of the robot hand and the object in a structured dynamical system. This results in a reactive motion generation policy that provides a reasonable motion prior, but lacks the ability to reason about contact effects due to the lack of action labels. The second stage involves emulation: learn a motion refinement policy to make adjustments to the motion prior of the robot hand such that the desired object motion is reenacted. CIMER is both task-agnostic (no task-specific reward design or shaping) and intervention-free (no need for additional teleoperated or labeled demonstrations). Detailed experiments reveal that i) Imitation alone is insufficient, but adding emulation drastically improves performance, ii) CIMER outperforms existing methods in terms of sample efficiency and the ability to generate realistic and stable motions, iii) CIMER can either zero-shot generalize or learn to adapt to novel objects from the YCB dataset, even outperforming expert policies trained with action labels in most cases.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Han, Yunhai and Chen, Zhenyang and Ravichandar, Harish},
	month = apr,
	year = {2024},
	note = {arXiv:2404.05582 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{chen_towards_2022,
	title = {Towards {Human}-{Level} {Bimanual} {Dexterous} {Manipulation} with {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=D29JbExncTP},
	abstract = {Achieving human-level dexterity is an important open problem in robotics. However, tasks of dexterous hand manipulation even at the baby level are challenging to solve through reinforcement learning (RL). The difficulty lies in the high degrees of freedom and the required cooperation among heterogeneous agents (e.g., joints of fingers). In this study, we propose the Bimanual Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two dexterous hands with tens of bimanual manipulation tasks and thousands of target objects. Tasks in Bi-DexHands are first designed to match human-level motor skills according to literature in cognitive science, and then are built in Issac Gym; this enables highly efficient RL trainings, reaching 30,000+ FPS by only one single NVIDIA RTX 3090. We provide a comprehensive benchmark for popular RL algorithms under different settings; this includes multi-agent RL, offline RL, multi-task RL, and meta RL. Our results show that PPO type on-policy algorithms can learn to solve simple manipulation tasks that are equivalent up to 48-month human baby (e.g., catching a flying object, opening a bottle), while multi-agent RL can further help to learn manipulations that require skilled bimanual cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each individual task, when it comes to mastering multiple manipulation skills, existing RL algorithms fail to work in most of the multi-task and the few-shot learning tasks, which calls for more future development from the RL community. Our project is open-sourced at https://github.com/PKU-MARL/DexterousHands.},
	language = {en},
	urldate = {2024-04-10},
	author = {Chen, Yuanpei and Wu, Tianhao and Wang, Shengjie and Feng, Xidong and Jiang, Jiechuan and Lu, Zongqing and McAleer, Stephen Marcus and Dong, Hao and Zhu, Song-Chun and Yang, Yaodong},
	month = jun,
	year = {2022},
}

@inproceedings{tseng_edge_2023,
	title = {{EDGE}: {Editable} {Dance} {Generation} {From} {Music}},
	shorttitle = {{EDGE}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-04-09},
	author = {Tseng, Jonathan and Castellon, Rodrigo and Liu, Karen},
	year = {2023},
	pages = {448--458},
}

@article{alexanderson_listen_2023,
	title = {Listen, {Denoise}, {Action}! {Audio}-{Driven} {Motion} {Synthesis} with {Diffusion} {Models}},
	volume = {42},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/3592458},
	doi = {10.1145/3592458},
	abstract = {Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.},
	number = {4},
	urldate = {2024-04-09},
	journal = {ACM Transactions on Graphics},
	author = {Alexanderson, Simon and Nagy, Rajmund and Beskow, Jonas and Henter, Gustav Eje},
	month = jul,
	year = {2023},
	keywords = {conformers, dance, diffusion models, ensemble models, generative models, gestures, guided interpolation, locomotion, machine learning, product of experts},
	pages = {44:1--44:20},
}

@inproceedings{li_ai_2021,
	title = {{AI} {Choreographer}: {Music} {Conditioned} {3D} {Dance} {Generation} {With} {AIST}++},
	shorttitle = {{AI} {Choreographer}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-04-09},
	author = {Li, Ruilong and Yang, Shan and Ross, David A. and Kanazawa, Angjoo},
	year = {2021},
	pages = {13401--13412},
}

@misc{han_more_2024,
	title = {The {More} {You} {See} in {2D}, the {More} {You} {Perceive} in {3D}},
	url = {http://arxiv.org/abs/2404.03652},
	doi = {10.48550/arXiv.2404.03652},
	abstract = {Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Han, Xinyang and Gao, Zelin and Kanazawa, Angjoo and Goel, Shubham and Gandelsman, Yossi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.03652 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{minderer_simple_2022,
	title = {Simple {Open}-{Vocabulary} {Object} {Detection} with {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2205.06230},
	doi = {10.48550/arXiv.2205.06230},
	abstract = {Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
	month = jul,
	year = {2022},
	note = {arXiv:2205.06230 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_semgrasp_2024,
	title = {{SemGrasp}: {Semantic} {Grasp} {Generation} via {Language} {Aligned} {Discretization}},
	shorttitle = {{SemGrasp}},
	url = {http://arxiv.org/abs/2404.03590},
	doi = {10.48550/arXiv.2404.03590},
	abstract = {Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-text-aligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Li, Kailin and Wang, Jingbo and Yang, Lixin and Lu, Cewu and Dai, Bo},
	month = apr,
	year = {2024},
	note = {arXiv:2404.03590 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ling_align_2024,
	title = {Align {Your} {Gaussians}: {Text}-to-{4D} with {Dynamic} {3D} {Gaussians} and {Composed} {Diffusion} {Models}},
	shorttitle = {Align {Your} {Gaussians}},
	url = {http://arxiv.org/abs/2312.13763},
	doi = {10.48550/arXiv.2312.13763},
	abstract = {Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Ling, Huan and Kim, Seung Wook and Torralba, Antonio and Fidler, Sanja and Kreis, Karsten},
	month = jan,
	year = {2024},
	note = {arXiv:2312.13763 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{yoshida_text-driven_2024,
	title = {Text-driven {Affordance} {Learning} from {Egocentric} {Vision}},
	url = {http://arxiv.org/abs/2404.02523},
	doi = {10.48550/arXiv.2404.02523},
	abstract = {Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset creation pipeline and build a large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of the contact points, trajectories, images, and text tuples. We extend existing referring expression comprehension models for our task, and experimental results show that our approach robustly handles multiple affordances, serving as a new standard for affordance learning in real-world scenarios.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Yoshida, Tomoya and Kurita, Shuhei and Nishimura, Taichi and Mori, Shinsuke},
	month = apr,
	year = {2024},
	note = {arXiv:2404.02523 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{le_jrdb-panotrack_2024,
	title = {{JRDB}-{PanoTrack}: {An} {Open}-world {Panoptic} {Segmentation} and {Tracking} {Robotic} {Dataset} in {Crowded} {Human} {Environments}},
	shorttitle = {{JRDB}-{PanoTrack}},
	url = {http://arxiv.org/abs/2404.01686},
	doi = {10.48550/arXiv.2404.01686},
	abstract = {Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Le, Duy-Tho and Gou, Chenhui and Datta, Stavya and Shi, Hengcan and Reid, Ian and Cai, Jianfei and Rezatofighi, Hamid},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01686 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_gears_2024,
	title = {{GEARS}: {Local} {Geometry}-aware {Hand}-object {Interaction} {Synthesis}},
	shorttitle = {{GEARS}},
	url = {http://arxiv.org/abs/2404.01758},
	doi = {10.48550/arXiv.2404.01758},
	abstract = {Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless, these methods show limited generalizability across object categories, shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors, and 2) scarcity of available training data. To tackle this challenge, we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity, we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions. Moreover, we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training, in turn enhancing our model's generalization capability. We evaluate on two public datasets, GRAB and InterCap, where our method shows superiority over baselines both quantitatively and perceptually.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Zhou, Keyang and Bhatnagar, Bharat Lal and Lenssen, Jan Eric and Pons-moll, Gerard},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01758 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{qiu_feature_2024,
	title = {Feature {Splatting}: {Language}-{Driven} {Physics}-{Based} {Scene} {Synthesis} and {Editing}},
	shorttitle = {Feature {Splatting}},
	url = {http://arxiv.org/abs/2404.01223},
	doi = {10.48550/arXiv.2404.01223},
	abstract = {Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Qiu, Ri-Zhao and Yang, Ge and Zeng, Weijia and Wang, Xiaolong},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{weng_neural_2024,
	title = {Neural {Implicit} {Representation} for {Building} {Digital} {Twins} of {Unknown} {Articulated} {Objects}},
	url = {http://arxiv.org/abs/2404.01440},
	doi = {10.48550/arXiv.2404.01440},
	abstract = {We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Weng, Yijia and Wen, Bowen and Tremblay, Jonathan and Blukis, Valts and Fox, Dieter and Guibas, Leonidas and Birchfield, Stan},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01440 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{wolf_surface_2024,
	title = {Surface {Reconstruction} from {Gaussian} {Splatting} via {Novel} {Stereo} {Views}},
	url = {http://arxiv.org/abs/2404.01810},
	doi = {10.48550/arXiv.2404.01810},
	abstract = {The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge. We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements' locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods. We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: https://gs2mesh.github.io/.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Wolf, Yaniv and Bracha, Amit and Kimmel, Ron},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01810 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_contacthandover_2024,
	title = {{ContactHandover}: {Contact}-{Guided} {Robot}-to-{Human} {Object} {Handover}},
	shorttitle = {{ContactHandover}},
	url = {http://arxiv.org/abs/2404.01402},
	doi = {10.48550/arXiv.2404.01402},
	abstract = {Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on https://clairezixiwang.github.io/ContactHandover.github.io},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Wang, Zixi and Liu, Zeyi and Ouporov, Nicolas and Song, Shuran},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01402 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{chen_sugar_2024,
	title = {{SUGAR}: {Pre}-training {3D} {Visual} {Representations} for {Robotics}},
	shorttitle = {{SUGAR}},
	url = {http://arxiv.org/abs/2404.01491},
	doi = {10.48550/arXiv.2404.01491},
	abstract = {Learning generalizable visual representations from Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D representation learning has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation. Experimental results show that SUGAR's 3D representation outperforms state-of-the-art 2D and 3D representations.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Chen, Shizhe and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01491 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhai_language-guided_2023,
	title = {Language-guided {Human} {Motion} {Synthesis} with {Atomic} {Actions}},
	url = {http://arxiv.org/abs/2308.09611},
	doi = {10.48550/arXiv.2308.09611},
	abstract = {Language-guided human motion synthesis has been a challenging task due to the inherent complexity and diversity of human behaviors. Previous methods face limitations in generalization to novel actions, often resulting in unrealistic or incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtion Modeling) to mitigate this problem, by decomposing actions into atomic actions, and employing a curriculum learning strategy to learn atomic action composition. First, we disentangle complex human motions into a set of atomic actions during learning, and then assemble novel actions using the learned atomic actions, which offers better adaptability to new actions. Moreover, we introduce a curriculum learning training strategy that leverages masked motion modeling with a gradual increase in the mask ratio, and thus facilitates atomic action assembly. This approach mitigates the overfitting problem commonly encountered in previous methods while enforcing the model to learn better motion representations. We demonstrate the effectiveness of ATOM through extensive experiments, including text-to-motion and action-to-motion synthesis tasks. We further illustrate its superiority in synthesizing plausible and coherent text-guided human motion sequences.},
	urldate = {2024-04-03},
	publisher = {arXiv},
	author = {Zhai, Yuanhao and Huang, Mingzhen and Luan, Tianyu and Dong, Lu and Nwogu, Ifeoma and Lyu, Siwei and Doermann, David and Yuan, Junsong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ko_learning_2023,
	title = {Learning to {Act} from {Actionless} {Videos} through {Dense} {Correspondences}},
	url = {http://arxiv.org/abs/2310.08576},
	doi = {10.48550/arXiv.2310.08576},
	abstract = {In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Ko, Po-Chen and Mao, Jiayuan and Du, Yilun and Sun, Shao-Hua and Tenenbaum, Joshua B.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08576 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{zhang_hoi-m3capture_2024,
	title = {{HOI}-{M3}:{Capture} {Multiple} {Humans} and {Objects} {Interaction} within {Contextual} {Environment}},
	shorttitle = {{HOI}-{M3}},
	url = {http://arxiv.org/abs/2404.00299},
	doi = {10.48550/arXiv.2404.00299},
	abstract = {Humans naturally interact with both others and the surrounding multiple objects, engaging in various social activities. However, recent advances in modeling human-object interactions mostly focus on perceiving isolated individuals and objects, due to fundamental data scarcity. In this paper, we introduce HOI-M3, a novel large-scale dataset for modeling the interactions of Multiple huMans and Multiple objects. Notably, it provides accurate 3D tracking for both humans and objects from dense RGB and object-mounted IMU inputs, covering 199 sequences and 181M frames of diverse humans and objects under rich activities. With the unique HOI-M3 dataset, we introduce two novel data-driven tasks with companion strong baselines: monocular capture and unstructured generation of multiple human-object interactions. Extensive experiments demonstrate that our dataset is challenging and worthy of further research about multiple human-object interactions and behavior analysis. Our HOI-M3 dataset, corresponding codes, and pre-trained models will be disseminated to the community for future research.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Zhang, Juze and Zhang, Jingyan and Song, Zining and Shi, Zhanhe and Zhao, Chengfeng and Shi, Ye and Yu, Jingyi and Xu, Lan and Wang, Jingya},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00299 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cha_text2hoi_2024,
	title = {{Text2HOI}: {Text}-guided {3D} {Motion} {Generation} for {Hand}-{Object} {Interaction}},
	shorttitle = {{Text2HOI}},
	url = {http://arxiv.org/abs/2404.00562},
	doi = {10.48550/arXiv.2404.00562},
	abstract = {This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: https://github.com/JunukCha/Text2HOI.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Cha, Junuk and Kim, Jihyeon and Yoon, Jae Shin and Baek, Seungryul},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00562 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deng_neslam_2024,
	title = {{NeSLAM}: {Neural} {Implicit} {Mapping} and {Self}-{Supervised} {Feature} {Tracking} {With} {Depth} {Completion} and {Denoising}},
	shorttitle = {{NeSLAM}},
	url = {http://arxiv.org/abs/2403.20034},
	doi = {10.48550/arXiv.2403.20034},
	abstract = {In recent years, there have been significant advancements in 3D reconstruction and dense RGB-D SLAM systems. One notable development is the application of Neural Radiance Fields (NeRF) in these systems, which utilizes implicit neural representation to encode 3D scenes. This extension of NeRF to SLAM has shown promising results. However, the depth images obtained from consumer-grade RGB-D sensors are often sparse and noisy, which poses significant challenges for 3D reconstruction and affects the accuracy of the representation of the scene geometry. Moreover, the original hierarchical feature grid with occupancy value is inaccurate for scene geometry representation. Furthermore, the existing methods select random pixels for camera tracking, which leads to inaccurate localization and is not robust in real-world indoor environments. To this end, we present NeSLAM, an advanced framework that achieves accurate and dense depth estimation, robust camera tracking, and realistic synthesis of novel views. First, a depth completion and denoising network is designed to provide dense geometry prior and guide the neural implicit representation optimization. Second, the occupancy scene representation is replaced with Signed Distance Field (SDF) hierarchical scene representation for high-quality reconstruction and view synthesis. Furthermore, we also propose a NeRF-based self-supervised feature tracking algorithm for robust real-time tracking. Experiments on various indoor datasets demonstrate the effectiveness and accuracy of the system in reconstruction, tracking quality, and novel view synthesis.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Deng, Tianchen and Wang, Yanbo and Xie, Hongle and Wang, Hesheng and Wang, Jingchuan and Wang, Danwei and Chen, Weidong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{xu_unified_2024,
	title = {A {Unified} {Framework} for {Human}-centric {Point} {Cloud} {Video} {Understanding}},
	url = {http://arxiv.org/abs/2403.20031},
	doi = {10.48550/arXiv.2403.20031},
	abstract = {Human-centric Point Cloud Video Understanding (PVU) is an emerging field focused on extracting and interpreting human-related features from sequences of human point clouds, further advancing downstream human-centric tasks and applications. Previous works usually focus on tackling one specific task and rely on huge labeled data, which has poor generalization capability. Considering that human has specific characteristics, including the structural semantics of human body and the dynamics of human motions, we propose a unified framework to make full use of the prior knowledge and explore the inherent features in the data itself for generalized human-centric point cloud video understanding. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various human-related tasks, including action recognition and 3D pose estimation. All datasets and code will be released soon.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Xu, Yiteng and Ye, Kecheng and Han, Xiao and Ren, Yiming and Zhu, Xinge and Ma, Yuexin},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20031 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{huang_voxposer_2023,
	title = {{VoxPoser}: {Composable} {3D} {Value} {Maps} for {Robotic} {Manipulation} with {Language} {Models}},
	shorttitle = {{VoxPoser}},
	url = {https://proceedings.mlr.press/v229/huang23b.html},
	abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language.},
	language = {en},
	urldate = {2024-04-01},
	booktitle = {Proceedings of {The} 7th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
	month = dec,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {540--562},
}

@inproceedings{guzey_dexterity_2023,
	title = {Dexterity from {Touch}: {Self}-{Supervised} {Pre}-{Training} of {Tactile} {Representations} with {Robotic} {Play}},
	shorttitle = {Dexterity from {Touch}},
	url = {https://proceedings.mlr.press/v229/guzey23a.html},
	abstract = {Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vision and torque-based models by an average of 1.7X. Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of play data, architectures, and representation learning.},
	language = {en},
	urldate = {2024-04-01},
	booktitle = {Proceedings of {The} 7th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Guzey, Irmak and Evans, Ben and Chintala, Soumith and Pinto, Lerrel},
	month = dec,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {3142--3166},
}

@inproceedings{pari_surprising_2022,
	title = {The {Surprising} {Effectiveness} of {Representation} {Learning} for {Visual} {Imitation}},
	volume = {18},
	isbn = {978-0-9923747-8-5},
	url = {https://www.roboticsproceedings.org/rss18/p010.html},
	urldate = {2024-04-01},
	author = {Pari, Jyothish and Shafiullah, Nur Muhammad (Mahi) and Arunachalam, Sridhar Pandian and Pinto, Lerrel},
	month = jun,
	year = {2022},
}

@misc{yan_reli11d_2024,
	title = {{RELI11D}: {A} {Comprehensive} {Multimodal} {Human} {Motion} {Dataset} and {Method}},
	shorttitle = {{RELI11D}},
	url = {http://arxiv.org/abs/2403.19501},
	abstract = {Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Yan, Ming and Zhang, Yan and Cai, Shuqiang and Fan, Shuqi and Lin, Xincheng and Dai, Yudi and Shen, Siqi and Wen, Chenglu and Xu, Lan and Ma, Yuexin and Wang, Cheng},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19501 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_interdreamer_2024,
	title = {{InterDreamer}: {Zero}-{Shot} {Text} to {3D} {Dynamic} {Human}-{Object} {Interaction}},
	shorttitle = {{InterDreamer}},
	url = {http://arxiv.org/abs/2403.19652},
	abstract = {Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Xu, Sirui and Wang, Ziyin and Wang, Yu-Xiong and Gui, Liang-Yan},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19652 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{di_palo_keypoint_2024,
	title = {Keypoint {Action} {Tokens} {Enable} {In}-{Context} {Imitation} {Learning} in {Robotics}},
	url = {http://arxiv.org/abs/2403.19578},
	abstract = {We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos are available at https://www.robot-learning.uk/keypoint-action-tokens.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Di Palo, Norman and Johns, Edward},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19578 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@misc{zhang_graspxl_2024,
	title = {{GraspXL}: {Generating} {Grasping} {Motions} for {Diverse} {Objects} at {Scale}},
	shorttitle = {{GraspXL}},
	url = {http://arxiv.org/abs/2403.19649},
	abstract = {Human hands possess the dexterity to interact with diverse objects such as grasping specific parts of the objects and/or approaching them from desired directions. More importantly, humans can grasp objects of any shape without object-specific skills. Recent works synthesize grasping motions following single objectives such as a desired approach heading direction or a grasping area. Moreover, they usually rely on expensive 3D hand-object data during training and inference, which limits their capability to synthesize grasping motions for unseen objects at scale. In this paper, we unify the generation of hand-object grasping motions across multiple motion objectives, diverse object shapes and dexterous hand morphologies in a policy learning framework GraspXL. The objectives are composed of the graspable area, heading direction during approach, wrist rotation, and hand position. Without requiring any 3D hand-object interaction data, our policy trained with 58 objects can robustly synthesize diverse grasping motions for more than 500k unseen objects with a success rate of 82.2\%. At the same time, the policy adheres to objectives, which enables the generation of diverse grasps per object. Moreover, we show that our framework can be deployed to different dexterous hands and work with reconstructed or generated objects. We quantitatively and qualitatively evaluate our method to show the efficacy of our approach. Our model and code will be available.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Zhang, Hui and Christen, Sammy and Fan, Zicong and Hilliges, Otmar and Song, Jie},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19649 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhan_oakink2_2024,
	title = {{OAKINK2}: {A} {Dataset} of {Bimanual} {Hands}-{Object} {Manipulation} in {Complex} {Task} {Completion}},
	shorttitle = {{OAKINK2}},
	url = {http://arxiv.org/abs/2403.19417},
	abstract = {We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Zhan, Xinyu and Yang, Lixin and Zhao, Yifei and Mao, Kangrui and Xu, Hanlin and Lin, Zenan and Li, Kailin and Lu, Cewu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19417 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_move_2024,
	title = {Move as {You} {Say}, {Interact} as {You} {Can}: {Language}-guided {Human} {Motion} {Generation} with {Scene} {Affordance}},
	shorttitle = {Move as {You} {Say}, {Interact} as {You} {Can}},
	url = {http://arxiv.org/abs/2403.18036},
	abstract = {Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Wang, Zan and Chen, Yixin and Jia, Baoxiong and Li, Puhao and Zhang, Jinlu and Zhang, Jingze and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Huang, Siyuan},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18036 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lin_locoman_2024,
	title = {{LocoMan}: {Advancing} {Versatile} {Quadrupedal} {Dexterity} with {Lightweight} {Loco}-{Manipulators}},
	shorttitle = {{LocoMan}},
	url = {http://arxiv.org/abs/2403.18197},
	abstract = {Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Lin, Changyi and Liu, Xingyu and Yang, Yuxiang and Niu, Yaru and Yu, Wenhao and Zhang, Tingnan and Tan, Jie and Boots, Byron and Zhao, Ding},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18197 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{yew_regtr_2022,
	title = {{REGTR}: {End}-to-{End} {Point} {Cloud} {Correspondences} {With} {Transformers}},
	shorttitle = {{REGTR}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yew_REGTR_End-to-End_Point_Cloud_Correspondences_With_Transformers_CVPR_2022_paper.html},
	language = {en},
	urldate = {2024-03-29},
	author = {Yew, Zi Jian and Lee, Gim Hee},
	year = {2022},
	pages = {6677--6686},
}

@misc{rajeswaran_learning_2017,
	title = {Learning {Complex} {Dexterous} {Manipulation} with {Deep} {Reinforcement} {Learning} and {Demonstrations}},
	url = {https://arxiv.org/abs/1709.10087v2},
	abstract = {Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.},
	language = {en},
	urldate = {2024-03-28},
	author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
	month = sep,
	year = {2017},
}

@article{qin_one_2022,
	title = {From {One} {Hand} to {Multiple} {Hands}: {Imitation} {Learning} for {Dexterous} {Manipulation} {From} {Single}-{Camera} {Teleoperation}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {From {One} {Hand} to {Multiple} {Hands}},
	url = {https://ieeexplore.ieee.org/abstract/document/9849105?casa_token=d_U8wpVvLcMAAAAA:fT4n-fjSawYyAjWDIMnJrafiqDqP97Dxo7Ol8DTrh7Vv5er25LCGYDFxvLxXTQXdxnfo7DOG16A},
	doi = {10.1109/LRA.2022.3196104},
	abstract = {We propose to perform imitation learning for dexterous manipulation with multi-finger robot hand from human demonstrations, and transfer the policy to the real robot hand. We introduce a novel single-camera teleoperation system to collect the 3D demonstrations efficiently with only an iPad and a computer. One key contribution of our system is that we construct a customized robot hand for each user in the simulator, which is a manipulator resembling the same structure of the operator's hand. It provides an intuitive interface and avoid unstable human-robot hand retargeting for data collection, leading to large-scale and high quality data. Once the data is collected, the customized robot hand trajectories can be converted to different specified robot hands (models that are manufactured) to generate training demonstrations. With imitation learning using our data, we show large improvement over baselines with multiple complex manipulation tasks. Importantly, we show our learned policy is significantly more robust when transferring to the real robot.},
	number = {4},
	urldate = {2024-03-27},
	journal = {IEEE Robotics and Automation Letters},
	author = {Qin, Yuzhe and Su, Hao and Wang, Xiaolong},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Behavioral sciences, Cameras, Dexterous manipulation, Robot vision systems, Robots, Tablet computers, Task analysis, Trajectory, imitation learning},
	pages = {10873--10881},
}

@inproceedings{arunachalam_dexterous_2023,
	title = {Dexterous {Imitation} {Made} {Easy}: {A} {Learning}-{Based} {Framework} for {Efficient} {Dexterous} {Manipulation}},
	shorttitle = {Dexterous {Imitation} {Made} {Easy}},
	url = {https://ieeexplore.ieee.org/abstract/document/10160275},
	doi = {10.1109/ICRA48891.2023.10160275},
	abstract = {Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Such prior work often require extensive trial-and-error training along with task-specific tuning of reward functions, which makes applying dexterous manipulation for general purpose problems quite impractical. A sample-efficient and practical alternate to trial-and-error learning is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging due to the high-dimensional action-space involved with multi-finger control. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera that observes a human operator to teleoperate a robotic hand. Once demonstrations are collected, DIME employs state-of-the-art imitation learning methods to train dexterous manipulation policies. On real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with just 30 demonstrations and no additional robot training. Our code, pre-collected demonstrations, and robot videos are publicly available at: https://nyu-robot-learning.github.io/dime.},
	urldate = {2024-03-27},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Arunachalam, Sridhar Pandian and Silwal, Sneha and Evans, Ben and Pinto, Lerrel},
	month = may,
	year = {2023},
	keywords = {Cameras, Codes, Learning systems, Reinforcement learning, Robot vision systems, Task analysis, Training},
	pages = {5954--5961},
}

@inproceedings{qi_general_2023,
	title = {General {In}-hand {Object} {Rotation} with {Vision} and {Touch}},
	url = {https://openreview.net/forum?id=RN00jfIV-X},
	abstract = {We introduce Rotateit, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and highlight the importance of visual and tactile sensing.},
	language = {en},
	urldate = {2024-03-27},
	author = {Qi, Haozhi and Yi, Brent and Suresh, Sudharshan and Lambeta, Mike and Ma, Yi and Calandra, Roberto and Malik, Jitendra},
	month = aug,
	year = {2023},
}

@misc{zhang_mmvp_2024,
	title = {{MMVP}: {A} {Multimodal} {MoCap} {Dataset} with {Vision} and {Pressure} {Sensors}},
	shorttitle = {{MMVP}},
	url = {http://arxiv.org/abs/2403.17610},
	doi = {10.48550/arXiv.2403.17610},
	abstract = {Foot contact is an important cue not only for human motion capture but also for motion understanding and physically plausible motion generation. However, most of the foot-contact annotations in existing datasets are estimated by purely visual matching and distance thresholding, which results in low accuracy and coarse granularity. Even though existing multimodal datasets synergistically capture plantar pressure (foot contact) and visual signals, they are specifically designed for small-range and slow motion such as Taiji Quan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://haolyuan.github.io/MMVP-Dataset/.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Zhang, He and Ren, Shenghao and Yuan, Haolei and Zhao, Jianhui and Li, Fan and Sun, Shuangpeng and Liang, Zhenghao and Yu, Tao and Shen, Qiu and Cao, Xun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17610 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{christen_diffh2o_2024,
	title = {{DiffH2O}: {Diffusion}-{Based} {Synthesis} of {Hand}-{Object} {Interactions} from {Textual} {Descriptions}},
	shorttitle = {{DiffH2O}},
	url = {http://arxiv.org/abs/2403.17827},
	doi = {10.48550/arXiv.2403.17827},
	abstract = {Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Christen, Sammy and Hampali, Shreyas and Sener, Fadime and Remelli, Edoardo and Hodan, Tomas and Sauser, Eric and Ma, Shugao and Tekin, Bugra},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17827 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{ingelhag_robotic_2024,
	title = {A {Robotic} {Skill} {Learning} {System} {Built} {Upon} {Diffusion} {Policies} and {Foundation} {Models}},
	url = {http://arxiv.org/abs/2403.16730},
	doi = {10.48550/arXiv.2403.16730},
	abstract = {In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. Foundational models are being used to perform skill selection given the user's prompt in natural language. Before executing a skill the foundational model performs a precondition check given an observation of the workspace. We compare the performance of different foundational models to this end as well as give a detailed experimental evaluation of the skills taught by the user in simulation and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Ingelhag, Nils and Munkeby, Jesper and van Haastregt, Jonne and Varava, Anastasia and Welle, Michael C. and Kragic, Danica},
	month = mar,
	year = {2024},
	note = {arXiv:2403.16730 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liu_visual_2024,
	title = {Visual {Whole}-{Body} {Control} for {Legged} {Loco}-{Manipulation}},
	url = {http://arxiv.org/abs/2403.16967},
	doi = {10.48550/arXiv.2403.16967},
	abstract = {We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely {\textbackslash}ourFull{\textasciitilde}({\textbackslash}our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights, locations, orientations) and environments. Project page: https://wholebody-b1.github.io},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Liu, Minghuan and Chen, Zixuan and Cheng, Xuxin and Ji, Yandong and Yang, Ruihan and Wang, Xiaolong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.16967 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{zhu_learning_2023,
	title = {Learning {Generalizable} {Manipulation} {Policies} with {Object}-{Centric} {3D} {Representations}},
	url = {https://openreview.net/forum?id=9SM6l0HyY_},
	abstract = {We introduce GROOT, an imitation learning method for learning robust policies with object-centric and 3D priors. GROOT builds policies that generalize beyond their initial training conditions for vision-based manipulation. It constructs object-centric 3D representations that are robust toward background changes and camera views and reason over these representations using a transformer-based policy. Furthermore, we introduce a segmentation correspondence model that allows policies to generalize to new objects at test time. Through comprehensive experiments, we validate the robustness of GROOT policies against perceptual variations in simulated and real-world environments. GROOT’s performance excels in generalization over background changes, camera viewpoint shifts, and the presence of new object instances, whereas both state-of-the-art end-to-end learning methods and object proposal-based approaches fall short. We also extensively evaluate GROOT policies on real robots, where we demonstrate the efficacy under very wild changes in setup. More videos and model details can be found in the appendix and the project website https://ut-austin-rpl.github.io/GROOT.},
	language = {en},
	urldate = {2024-03-27},
	author = {Zhu, Yifeng and Jiang, Zhenyu and Stone, Peter and Zhu, Yuke},
	month = aug,
	year = {2023},
}

@inproceedings{zhang_efficient_2023,
	title = {Efficient {Sim}-to-real {Transfer} of {Contact}-{Rich} {Manipulation} {Skills} with {Online} {Admittance} {Residual} {Learning}},
	url = {https://openreview.net/forum?id=gFXVysXh48K},
	abstract = {Learning contact-rich manipulation skills is essential. Such skills require the robots to interact with the environment with feasible manipulation trajectories and suitable compliance control parameters to enable safe and stable contact. However, learning these skills is challenging due to data inefficiency in the real world and the sim-to-real gap in simulation. In this paper, we introduce a hybrid offline-online framework to learn robust manipulation skills. We employ model-free reinforcement learning for the offline phase to obtain the robot motion and compliance control parameters in simulation {\textbackslash}RV\{with domain randomization\}. Subsequently, in the online phase, we learn the residual of the compliance control parameters to maximize robot performance-related criteria with force sensor measurements in real-time. To demonstrate the effectiveness and robustness of our approach, we provide comparative results against existing methods for assembly, pivoting, and screwing tasks.},
	language = {en},
	urldate = {2024-03-27},
	author = {Zhang, Xiang and Wang, Changhao and Sun, Lingfeng and Wu, Zheng and Zhu, Xinghao and Tomizuka, Masayoshi},
	month = aug,
	year = {2023},
}

@misc{rofer_pseudotouch_2024,
	title = {{PseudoTouch}: {Efficiently} {Imaging} the {Surface} {Feel} of {Objects} for {Robotic} {Manipulation}},
	shorttitle = {{PseudoTouch}},
	url = {http://arxiv.org/abs/2403.15107},
	abstract = {Humans seemingly incorporate potential touch signals in their perception. Our goal is to equip robots with a similar capability, which we term {\textbackslash}ourmodel. {\textbackslash}ourmodel aims to predict the expected touch signal based on a visual patch representing the touched area. We frame this problem as the task of learning a low-dimensional visual-tactile embedding, wherein we encode a depth patch from which we decode the tactile signal. To accomplish this task, we employ ReSkin, an inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we collect and train PseudoTouch on a dataset comprising aligned tactile and visual data pairs obtained through random touching of eight basic geometric shapes. We demonstrate the efficacy of PseudoTouch through its application to two downstream tasks: object recognition and grasp stability prediction. In the object recognition task, we evaluate the learned embedding's performance on a set of five basic geometric shapes and five household objects. Using PseudoTouch, we achieve an object recognition accuracy 84\% after just ten touches, surpassing a proprioception baseline. For the grasp stability task, we use ACRONYM labels to train and evaluate a grasp success predictor using PseudoTouch's predictions derived from virtual depth information. Our approach yields an impressive 32\% absolute improvement in accuracy compared to the baseline relying on partial point cloud data. We make the data, code, and trained models publicly available at http://pseudotouch.cs.uni-freiburg.de.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Röfer, Adrian and Heppert, Nick and Ayman, Abdallah and Chisari, Eugenio and Valada, Abhinav},
	month = mar,
	year = {2024},
	note = {arXiv:2403.15107 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{lipman_flow_2022,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {https://openreview.net/forum?id=PqvMRDCJT9t},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	language = {en},
	urldate = {2024-03-25},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matthew},
	month = sep,
	year = {2022},
}

@misc{daniel_unsupervised_2022,
	title = {Unsupervised {Image} {Representation} {Learning} with {Deep} {Latent} {Particles}},
	url = {http://arxiv.org/abs/2205.15821},
	doi = {10.48550/arXiv.2205.15821},
	abstract = {We propose a new representation of visual data that disentangles object position from appearance. Our method, termed Deep Latent Particles (DLP), decomposes the visual input into low-dimensional latent ``particles'', where each particle is described by its spatial location and features of its surrounding region. To drive learning of such representations, we follow a VAE-based approach and introduce a prior for particle positions based on a spatial-softmax architecture, and a modification of the evidence lower bound loss inspired by the Chamfer distance between particles. We demonstrate that our DLP representations are useful for downstream tasks such as unsupervised keypoint (KP) detection, image manipulation, and video prediction for scenes composed of multiple dynamic objects. In addition, we show that our probabilistic interpretation of the problem naturally provides uncertainty estimates for particle locations, which can be used for model selection, among other tasks. Videos and code are available: https://taldatech.github.io/deep-latent-particles-web/},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Daniel, Tal and Tamar, Aviv},
	month = jul,
	year = {2022},
	note = {arXiv:2205.15821 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cho_object-centric_2024,
	title = {Object-{Centric} {Domain} {Randomization} for {3D} {Shape} {Reconstruction} in the {Wild}},
	url = {http://arxiv.org/abs/2403.14539},
	doi = {10.48550/arXiv.2403.14539},
	abstract = {One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of {\textless}3D shape, 2D image{\textgreater}-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry prior which is consistent across various domains. We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world benchmark. In a scale-up evaluation, our pre-training achieves 23.6\% superior results compared with the pre-training on high-quality computer graphics renderings.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Cho, Junhyeong and Youwang, Kim and Yang, Hunmin and Oh, Tae-Hyun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14539 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cai_visual_2024,
	title = {Visual {Imitation} {Learning} of {Task}-{Oriented} {Object} {Grasping} and {Rearrangement}},
	url = {http://arxiv.org/abs/2403.14000},
	doi = {10.48550/arXiv.2403.14000},
	abstract = {Task-oriented object grasping and rearrangement are critical skills for robots to accomplish different real-world manipulation tasks. However, they remain challenging due to partial observations of the objects and shape variations in categorical objects. In this paper, we propose the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field. Training such a model on multiple features ensures that it embeds the object shapes consistently in different aspects, thus improving its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, we propose a framework to learn task-oriented object grasping and rearrangement from single or multiple human demonstration videos. The evaluations in simulation show that our approach outperforms the state-of-the-art methods for multi- and single-view observations. Real-world experiments demonstrate the efficacy of our approach in one- and few-shot imitation learning of manipulation tasks.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Cai, Yichen and Gao, Jianfeng and Pohl, Christoph and Asfour, Tamim},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14000 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{goyal_retrieval-augmented_2022,
	title = {Retrieval-{Augmented} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v162/goyal22a.html},
	abstract = {Most deep reinforcement learning (RL) algorithms distill experience into parametric behavior policies or value functions via gradient updates. While effective, this approach has several disadvantages: (1) it is computationally expensive, (2) it can take many updates to integrate experiences into the parametric model, (3) experiences that are not fully integrated do not appropriately influence the agent’s behavior, and (4) behavior is limited by the capacity of the model. In this paper we explore an alternative paradigm in which we train a network to map a dataset of past experiences to optimal behavior. Specifically, we augment an RL agent with a retrieval process (parameterized as a neural network) that has direct access to a dataset of experiences. This dataset can come from the agent’s past experiences, expert demonstrations, or any other relevant source. The retrieval process is trained to retrieve information from the dataset that may be useful in the current context, to help the agent achieve its goal faster and more efficiently. The proposed method facilitates learning agents that at test time can condition their behavior on the entire dataset and not only the current state, or current trajectory. We integrate our method into two different RL agents: an offline DQN agent and an online R2D2 agent. In offline multi-task problems, we show that the retrieval-augmented DQN agent avoids task interference and learns faster than the baseline DQN agent. On Atari, we show that retrieval-augmented R2D2 learns significantly faster than the baseline R2D2 agent and achieves higher scores. We run extensive ablations to measure the contributions of the components of our proposed method.},
	language = {en},
	urldate = {2024-03-23},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Goyal, Anirudh and Friesen, Abram and Banino, Andrea and Weber, Theophane and Ke, Nan Rosemary and Badia, Adrià Puigdomènech and Guez, Arthur and Mirza, Mehdi and Humphreys, Peter C. and Konyushova, Ksenia and Valko, Michal and Osindero, Simon and Lillicrap, Timothy and Heess, Nicolas and Blundell, Charles},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7740--7765},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-03-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	pages = {9459--9474},
}

@inproceedings{jakab_unsupervised_2018,
	title = {Unsupervised {Learning} of {Object} {Landmarks} through {Conditional} {Image} {Generation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html},
	abstract = {We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.},
	urldate = {2024-03-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jakab, Tomas and Gupta, Ankush and Bilen, Hakan and Vedaldi, Andrea},
	year = {2018},
}

@misc{avetisyan_scenescript_2024,
	title = {{SceneScript}: {Reconstructing} {Scenes} {With} {An} {Autoregressive} {Structured} {Language} {Model}},
	shorttitle = {{SceneScript}},
	url = {http://arxiv.org/abs/2403.13064},
	abstract = {We introduce SceneScript, a method that directly produces full scene models as a sequence of structured language commands using an autoregressive, token-based approach. Our proposed scene representation is inspired by recent successes in transformers \& LLMs, and departs from more traditional methods which commonly describe scenes as meshes, voxel grids, point clouds or radiance fields. Our method infers the set of structured language commands directly from encoded visual data using a scene language encoder-decoder architecture. To train SceneScript, we generate and release a large-scale synthetic dataset called Aria Synthetic Environments consisting of 100k high-quality in-door scenes, with photorealistic and ground-truth annotated renders of egocentric scene walkthroughs. Our method gives state-of-the art results in architectural layout estimation, and competitive results in 3D object detection. Lastly, we explore an advantage for SceneScript, which is the ability to readily adapt to new commands via simple additions to the structured language, which we illustrate for tasks such as coarse 3D object part reconstruction.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Avetisyan, Armen and Xie, Christopher and Howard-Jenkins, Henry and Yang, Tsun-Yi and Aroudj, Samir and Patra, Suvam and Zhang, Fuyang and Frost, Duncan and Holland, Luke and Orme, Campbell and Engel, Jakob and Miller, Edward and Newcombe, Richard and Balntas, Vasileios},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13064 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{haramati_entity-centric_2023,
	title = {Entity-{Centric} {Reinforcement} {Learning} for {Object} {Manipulation} from {Pixels}},
	url = {https://openreview.net/forum?id=uDxeSZ1wdI},
	abstract = {Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl},
	language = {en},
	urldate = {2024-03-23},
	author = {Haramati, Dan and Daniel, Tal and Tamar, Aviv},
	month = oct,
	year = {2023},
}

@inproceedings{li_learning_2023,
	title = {Learning to {Jointly} {Understand} {Visual} and {Tactile} {Signals}},
	url = {https://openreview.net/forum?id=NtQqIcSbqv},
	abstract = {Modeling and analyzing object and shape has been well studied in the past. However, manipulation of these complex tools and articulated objects remains difficult for autonomous agents. Our human hands, however, are dexterous and adaptive. We can easily adapt a manipulation skill on one object to all objects in the class and to other similar classes. Our intuition comes from that there is a close connection between manipulations and topology and articulation of objects. The possible articulation of objects indicates the types of manipulation necessary to operate the object. In this work, we aim to take a manipulation perspective to understand everyday objects and tools. We collect a multi-modal visual-tactile dataset that contains paired full-hand force pressure maps and manipulation videos. We also propose a novel method to learn a cross-modal latent manifold that allow for cross-modal prediction and discovery of latent structure in different data modalities. We conduct extensive experiments to demonstrate the effectiveness of our method.},
	language = {en},
	urldate = {2024-03-23},
	author = {Li, Yichen and Du, Yilun and Liu, Chao and Williams, Francis and Foshey, Michael and Eckart, Benjamin and Kautz, Jan and Tenenbaum, Joshua B. and Torralba, Antonio and Matusik, Wojciech},
	month = oct,
	year = {2023},
}

@misc{wang_d3fields_2023,
	title = {D\${\textasciicircum}3\${Fields}: {Dynamic} {3D} {Descriptor} {Fields} for {Zero}-{Shot} {Generalizable} {Robotic} {Manipulation}},
	shorttitle = {D\${\textasciicircum}3\${Fields}},
	url = {http://arxiv.org/abs/2309.16118},
	doi = {10.48550/arXiv.2309.16118},
	abstract = {Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D\${\textasciicircum}3\$Fields - dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonstrate that D\${\textasciicircum}3\$Fields are both generalizable and effective for zero-shot robotic manipulation tasks. In quantitative comparisons with state-of-the-art dense descriptors, such as Dense Object Nets and DINO, D\${\textasciicircum}3\$Fields exhibit significantly better generalization abilities and manipulation accuracy.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Wang, Yixuan and Li, Zhuoran and Zhang, Mingtong and Driggs-Campbell, Katherine and Wu, Jiajun and Fei-Fei, Li and Li, Yunzhu},
	month = oct,
	year = {2023},
	note = {arXiv:2309.16118 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ding_odtformer_2024,
	title = {{ODTFormer}: {Efficient} {Obstacle} {Detection} and {Tracking} with {Stereo} {Cameras} {Based} on {Transformer}},
	shorttitle = {{ODTFormer}},
	url = {http://arxiv.org/abs/2403.14626},
	abstract = {Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a Transformer-based model to address both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI benchmarks, our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. The code and model weights will be publicly released.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Ding, Tianye and Li, Hongyu and Jiang, Huaizu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14626 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wen_any-point_2023,
	title = {Any-point {Trajectory} {Modeling} for {Policy} {Learning}},
	url = {http://arxiv.org/abs/2401.00025},
	doi = {10.48550/arXiv.2401.00025},
	abstract = {Learning from demonstration is a powerful method for teaching robots new skills, and more demonstration data often improves policy learning. However, the high cost of collecting demonstration data is a significant bottleneck. Videos, as a rich data source, contain knowledge of behaviors, physics, and semantics, but extracting control-specific information from them is challenging due to the lack of action labels. In this work, we introduce a novel framework, Any-point Trajectory Modeling (ATM), that utilizes video demonstrations by pre-training a trajectory model to predict future trajectories of arbitrary points within a video frame. Once trained, these trajectories provide detailed control guidance, enabling the learning of robust visuomotor policies with minimal action-labeled data. Our method's effectiveness is demonstrated across 130 simulation tasks, focusing on language-conditioned manipulation tasks. Visualizations and code are available at: {\textbackslash}url\{https://xingyu-lin.github.io/atm\}.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Wen, Chuan and Lin, Xingyu and So, John and Chen, Kai and Dou, Qi and Gao, Yang and Abbeel, Pieter},
	month = dec,
	year = {2023},
	note = {arXiv:2401.00025 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhao_learning_2023,
	title = {Learning {Fine}-{Grained} {Bimanual} {Manipulation} with {Low}-{Cost} {Hardware}},
	url = {http://arxiv.org/abs/2304.13705},
	doi = {10.48550/arXiv.2304.13705},
	abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13705 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_dust3r_2023,
	title = {{DUSt3R}: {Geometric} {3D} {Vision} {Made} {Easy}},
	shorttitle = {{DUSt3R}},
	url = {http://arxiv.org/abs/2312.14132},
	doi = {10.48550/arXiv.2312.14132},
	abstract = {Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.},
	urldate = {2023-12-28},
	publisher = {arXiv},
	author = {Wang, Shuzhe and Leroy, Vincent and Cabon, Yohann and Chidlovskii, Boris and Revaud, Jerome},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14132 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ranftl_towards_2022,
	title = {Towards {Robust} {Monocular} {Depth} {Estimation}: {Mixing} {Datasets} for {Zero}-{Shot} {Cross}-{Dataset} {Transfer}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Towards {Robust} {Monocular} {Depth} {Estimation}},
	doi = {10.1109/TPAMI.2020.3019967},
	abstract = {The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ranftl, René and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Estimation, Measurement, Monocular depth estimation, Motion pictures, Three-dimensional displays, Training, Videos, multi-dataset training, single-image depth prediction, zero-shot cross-dataset transfer},
	pages = {1623--1637},
}

@misc{braun_riemannian_2024,
	title = {Riemannian {Flow} {Matching} {Policy} for {Robot} {Motion} {Learning}},
	url = {http://arxiv.org/abs/2403.10672},
	abstract = {We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with significantly lower inference times.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Braun, Max and Jaquier, Noémie and Rozo, Leonel and Asfour, Tamim},
	month = mar,
	year = {2024},
	note = {arXiv:2403.10672 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{jain_vid2robot_2024,
	title = {{Vid2Robot}: {End}-to-end {Video}-conditioned {Policy} {Learning} with {Cross}-{Attention} {Transformers}},
	shorttitle = {{Vid2Robot}},
	url = {http://arxiv.org/abs/2403.12943},
	doi = {10.48550/arXiv.2403.12943},
	abstract = {While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations. We evaluate Vid2Robot on real-world robots, demonstrating a 20\% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Jain, Vidhi and Attarian, Maria and Joshi, Nikhil J. and Wahid, Ayzaan and Driess, Danny and Vuong, Quan and Sanketi, Pannag R. and Sermanet, Pierre and Welker, Stefan and Chan, Christine and Gilitschenski, Igor and Bisk, Yonatan and Dwibedi, Debidatta},
	month = mar,
	year = {2024},
	note = {arXiv:2403.12943 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@article{hoeller_anymal_2024,
	title = {{ANYmal} parkour: {Learning} agile navigation for quadrupedal robots},
	volume = {9},
	shorttitle = {{ANYmal} parkour},
	url = {https://www.science.org/doi/10.1126/scirobotics.adi7566},
	doi = {10.1126/scirobotics.adi7566},
	abstract = {Performing agile navigation with four-legged robots is a challenging task because of the highly dynamic motions, contacts with various parts of the robot, and the limited field of view of the perception sensors. Here, we propose a fully learned approach to training such robots and conquer scenarios that are reminiscent of parkour challenges. The method involves training advanced locomotion skills for several types of obstacles, such as walking, jumping, climbing, and crouching, and then using a high-level policy to select and control those skills across the terrain. Thanks to our hierarchical formulation, the navigation policy is aware of the capabilities of each skill, and it will adapt its behavior depending on the scenario at hand. In addition, a perception module was trained to reconstruct obstacles from highly occluded and noisy sensory data and endows the pipeline with scene understanding. Compared with previous attempts, our method can plan a path for challenging scenarios without expert demonstration, offline computation, a priori knowledge of the environment, or taking contacts explicitly into account. Although these modules were trained from simulated data only, our real-world experiments demonstrate successful transfer on hardware, where the robot navigated and crossed consecutive challenging obstacles with speeds of up to 2 meters per second.},
	number = {88},
	urldate = {2024-03-22},
	journal = {Science Robotics},
	author = {Hoeller, David and Rudin, Nikita and Sako, Dhionis and Hutter, Marco},
	month = mar,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadi7566},
}

@misc{li_stereonavnet_2024,
	title = {{StereoNavNet}: {Learning} to {Navigate} using {Stereo} {Cameras} with {Auxiliary} {Occupancy} {Voxels}},
	shorttitle = {{StereoNavNet}},
	url = {http://arxiv.org/abs/2403.12039},
	doi = {10.48550/arXiv.2403.12039},
	abstract = {Visual navigation has received significant attention recently. Most of the prior works focus on predicting navigation actions based on semantic features extracted from visual encoders. However, these approaches often rely on large datasets and exhibit limited generalizability. In contrast, our approach draws inspiration from traditional navigation planners that operate on geometric representations, such as occupancy maps. We propose StereoNavNet (SNN), a novel visual navigation approach employing a modular learning framework comprising perception and policy modules. Within the perception module, we estimate an auxiliary 3D voxel occupancy grid from stereo RGB images and extract geometric features from it. These features, along with user-defined goals, are utilized by the policy module to predict navigation actions. Through extensive empirical evaluation, we demonstrate that SNN outperforms baseline approaches in terms of success rates, success weighted by path length, and navigation error. Furthermore, SNN exhibits better generalizability, characterized by maintaining leading performance when navigating across previously unseen environments.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Li, Hongyu and Padir, Taskin and Jiang, Huaizu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.12039 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{khazatsky_droid_2024,
	title = {{DROID}: {A} {Large}-{Scale} {In}-{The}-{Wild} {Robot} {Manipulation} {Dataset}},
	shorttitle = {{DROID}},
	url = {http://arxiv.org/abs/2403.12945},
	doi = {10.48550/arXiv.2403.12945},
	abstract = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and Fagan, Peter David and Hejna, Joey and Itkina, Masha and Lepert, Marion and Ma, Yecheng Jason and Miller, Patrick Tree and Wu, Jimmy and Belkhale, Suneel and Dass, Shivin and Ha, Huy and Jain, Arhan and Lee, Abraham and Lee, Youngwoon and Memmel, Marius and Park, Sungjae and Radosavovic, Ilija and Wang, Kaiyuan and Zhan, Albert and Black, Kevin and Chi, Cheng and Hatch, Kyle Beltran and Lin, Shan and Lu, Jingpei and Mercat, Jean and Rehman, Abdul and Sanketi, Pannag R. and Sharma, Archit and Simpson, Cody and Vuong, Quan and Walke, Homer Rich and Wulfe, Blake and Xiao, Ted and Yang, Jonathan Heewon and Yavary, Arefeh and Zhao, Tony Z. and Agia, Christopher and Baijal, Rohan and Castro, Mateo Guaman and Chen, Daphne and Chen, Qiuyu and Chung, Trinity and Drake, Jaimyn and Foster, Ethan Paul and Gao, Jensen and Herrera, David Antonio and Heo, Minho and Hsu, Kyle and Hu, Jiaheng and Jackson, Donovon and Le, Charlotte and Li, Yunshuang and Lin, Kevin and Lin, Roy and Ma, Zehan and Maddukuri, Abhiram and Mirchandani, Suvir and Morton, Daniel and Nguyen, Tony and O'Neill, Abigail and Scalise, Rosario and Seale, Derick and Son, Victor and Tian, Stephen and Tran, Emi and Wang, Andrew E. and Wu, Yilin and Xie, Annie and Yang, Jingyun and Yin, Patrick and Zhang, Yunchu and Bastani, Osbert and Berseth, Glen and Bohg, Jeannette and Goldberg, Ken and Gupta, Abhinav and Gupta, Abhishek and Jayaraman, Dinesh and Lim, Joseph J. and Malik, Jitendra and Martín-Martín, Roberto and Ramamoorthy, Subramanian and Sadigh, Dorsa and Song, Shuran and Wu, Jiajun and Yip, Michael C. and Zhu, Yuke and Kollar, Thomas and Levine, Sergey and Finn, Chelsea},
	month = mar,
	year = {2024},
	note = {arXiv:2403.12945 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{voleti_sv3d_2024,
	title = {{SV3D}: {Novel} {Multi}-view {Synthesis} and {3D} {Generation} from a {Single} {Image} using {Latent} {Video} {Diffusion}},
	shorttitle = {{SV3D}},
	url = {http://arxiv.org/abs/2403.12008},
	doi = {10.48550/arXiv.2403.12008},
	abstract = {We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Voleti, Vikram and Yao, Chun-Han and Boss, Mark and Letts, Adam and Pankratz, David and Tochilkin, Dmitry and Laforte, Christian and Rombach, Robin and Jampani, Varun},
	month = mar,
	year = {2024},
	note = {arXiv:2403.12008 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huang_manipvqa_2024,
	title = {{ManipVQA}: {Injecting} {Robotic} {Affordance} and {Physically} {Grounded} {Information} into {Multi}-{Modal} {Large} {Language} {Models}},
	shorttitle = {{ManipVQA}},
	url = {http://arxiv.org/abs/2403.11289},
	doi = {10.48550/arXiv.2403.11289},
	abstract = {The integration of Multimodal Large Language Models (MLLMs) with robotic systems has significantly enhanced the ability of robots to interpret and act upon natural language instructions. Despite these advancements, conventional MLLMs are typically trained on generic image-text pairs, lacking essential robotics knowledge such as affordances and physical knowledge, which hampers their efficacy in manipulation tasks. To bridge this gap, we introduce ManipVQA, a novel framework designed to endow MLLMs with Manipulation-centric knowledge through a Visual Question-Answering format. This approach not only encompasses tool detection and affordance recognition but also extends to a comprehensive understanding of physical concepts. Our approach starts with collecting a varied set of images displaying interactive objects, which presents a broad range of challenges in tool object detection, affordance, and physical concept predictions. To seamlessly integrate this robotic-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we adopt a unified VQA format and devise a fine-tuning strategy that preserves the original vision-reasoning abilities while incorporating the new robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. Code and dataset will be made publicly available at https://github.com/SiyuanHuang95/ManipVQA.},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Huang, Siyuan and Ponomarenko, Iaroslav and Jiang, Zhengkai and Li, Xiaoqi and Hu, Xiaobin and Gao, Peng and Li, Hongsheng and Dong, Hao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.11289 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lipman_flow_2023,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	month = feb,
	year = {2023},
	note = {arXiv:2210.02747 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ismail_narrate_2024,
	title = {{NARRATE}: {Versatile} {Language} {Architecture} for {Optimal} {Control} in {Robotics}},
	shorttitle = {{NARRATE}},
	url = {http://arxiv.org/abs/2403.10762},
	abstract = {The impressive capabilities of Large Language Models (LLMs) have led to various efforts to enable robots to be controlled through natural language instructions, opening exciting possibilities for human-robot interaction The goal is for the motor-control task to be performed accurately, efficiently and safely while also enjoying the flexibility imparted by LLMs to specify and adjust the task through natural language. In this work, we demonstrate how a careful layering of an LLM in combination with a Model Predictive Control (MPC) formulation allows for accurate and flexible robotic control via natural language while taking into consideration safety constraints. In particular, we rely on the LLM to effectively frame constraints and objective functions as mathematical expressions, which are later used in the motor-control module via MPC. The transparency of the optimization formulation allows for interpretability of the task and enables adjustments through human feedback. We demonstrate the validity of our method through extensive experiments on long-horizon reasoning, contact-rich, and multi-object interaction tasks. Our evaluations show that NARRATE outperforms current existing methods on these benchmarks and effectively transfers to the real world on two different embodiments. Videos, Code and Prompts at narrate-mpc.github.io},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Ismail, Seif and Arbues, Antonio and Cotterell, Ryan and Zurbrügg, René and Alonso, Carmen Amo},
	month = mar,
	year = {2024},
	note = {arXiv:2403.10762 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_hvdistill_2024,
	title = {{HVDistill}: {Transferring} {Knowledge} from {Images} to {Point} {Clouds} via {Unsupervised} {Hybrid}-{View} {Distillation}},
	shorttitle = {{HVDistill}},
	url = {http://arxiv.org/abs/2403.11817},
	doi = {10.48550/arXiv.2403.11817},
	abstract = {We present a hybrid-view-based knowledge distillation framework, termed HVDistill, to guide the feature learning of a point cloud neural network with a pre-trained image network in an unsupervised man- ner. By exploiting the geometric relationship between RGB cameras and LiDAR sensors, the correspondence between the two modalities based on both image- plane view and bird-eye view can be established, which facilitates representation learning. Specifically, the image-plane correspondences can be simply ob- tained by projecting the point clouds, while the bird- eye-view correspondences can be achieved by lifting pixels to the 3D space with the predicted depths un- der the supervision of projected point clouds. The image teacher networks provide rich semantics from the image-plane view and meanwhile acquire geometric information from the bird-eye view. Indeed, image features from the two views naturally comple- ment each other and together can ameliorate the learned feature representation of the point cloud stu- dent networks. Moreover, with a self-supervised pre- trained 2D network, HVDistill requires neither 2D nor 3D annotations. We pre-train our model on nuScenes dataset and transfer it to several downstream tasks on nuScenes, SemanticKITTI, and KITTI datasets for evaluation. Extensive experimental results show that our method achieves consistent improvements over the baseline trained from scratch and significantly out- performs the existing schemes. Codes are available at git@github.com:zhangsha1024/HVDistill.git.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Zhang, Sha and Deng, Jiajun and Bai, Lei and Li, Houqiang and Ouyang, Wanli and Zhang, Yanyong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.11817 null},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_droid_nodate,
	title = {{DROID}: {A} {Large}-{Scale} {In}-the-{Wild} {Robot} {Manipulation} {Dataset}},
	url = {https://droid-dataset.github.io/},
	urldate = {2024-03-19},
}

@misc{george_visuo-tactile_2024,
	title = {Visuo-{Tactile} {Pretraining} for {Cable} {Plugging}},
	url = {http://arxiv.org/abs/2403.11898},
	doi = {10.48550/arXiv.2403.11898},
	abstract = {Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a first for imitation learning. Additionally, we explore how tactile information can be used to train non-tactile agents through a contrastive-loss pretraining process. Our results show that by pretraining with tactile information, the performance of a non-tactile agent can be significantly improved, reaching a level on par with visuo-tactile agents. For demonstration videos and access to our codebase, see the project website: https://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {George, Abraham and Gano, Selam and Katragadda, Pranav and Farimani, Amir Barati},
	month = mar,
	year = {2024},
	note = {arXiv:2403.11898 null},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{murrilo_multigrippergrasp_2024,
	title = {{MultiGripperGrasp}: {A} {Dataset} for {Robotic} {Grasping} from {Parallel} {Jaw} {Grippers} to {Dexterous} {Hands}},
	shorttitle = {{MultiGripperGrasp}},
	url = {http://arxiv.org/abs/2403.09841},
	abstract = {We introduce a large-scale dataset named MultiGripperGrasp for robotic grasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects. These grippers range from two-finger grippers to five-finger grippers, including a human hand. All grasps in the dataset are verified in Isaac Sim to classify them as successful and unsuccessful grasps. Additionally, the object fall-off time for each grasp is recorded as a grasp quality measurement. Furthermore, the grippers in our dataset are aligned according to the orientation and position of their palms, allowing us to transfer grasps from one gripper to another. The grasp transfer significantly increases the number of successful grasps for each gripper in the dataset. Our dataset is useful to study generalized grasp planning and grasp transfer across different grippers.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Murrilo, Luis Felipe Casas and Khargonkar, Ninad and Prabhakaran, Balakrishnan and Xiang, Yu},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09841 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{swann_touch-gs_2024,
	title = {Touch-{GS}: {Visual}-{Tactile} {Supervised} {3D} {Gaussian} {Splatting}},
	shorttitle = {Touch-{GS}},
	url = {http://arxiv.org/abs/2403.09875},
	abstract = {In this work, we propose a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance weighted depth supervised loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective and transparent objects. Please see our project page at http://armlabstanford.github.io/touch-gs},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Swann, Aiden and Strong, Matthew and Do, Won Kyung and Camps, Gadiel Sznaier and Schwager, Mac and Kennedy III, Monroe},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09875 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{shah_vint_2023,
	title = {{ViNT}: {A} {Foundation} {Model} for {Visual} {Navigation}},
	shorttitle = {{ViNT}},
	url = {http://arxiv.org/abs/2306.14846},
	doi = {10.48550/arXiv.2306.14846},
	abstract = {General-purpose pre-trained models ("foundation models") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Shah, Dhruv and Sridhar, Ajay and Dashora, Nitish and Stachowicz, Kyle and Black, Kevin and Hirose, Noriaki and Levine, Sergey},
	month = oct,
	year = {2023},
	note = {arXiv:2306.14846 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zheng_gaussiangrasper_2024,
	title = {{GaussianGrasper}: {3D} {Language} {Gaussian} {Splatting} for {Open}-vocabulary {Robotic} {Grasping}},
	shorttitle = {{GaussianGrasper}},
	url = {http://arxiv.org/abs/2403.09637},
	doi = {10.48550/arXiv.2403.09637},
	abstract = {Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature Distillation (EFD) module that employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models. With the reconstructed geometry of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at https://github.com/MrSecant/GaussianGrasper.},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Zheng, Yuhang and Chen, Xiangyu and Zheng, Yupeng and Gu, Songen and Yang, Runyi and Jin, Bu and Li, Pengfei and Zhong, Chengliang and Wang, Zengmao and Liu, Lina and Yang, Chao and Wang, Dawei and Chen, Zhen and Long, Xiaoxiao and Wang, Meiqing},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09637 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{huang_copa_2024,
	title = {{CoPa}: {General} {Robotic} {Manipulation} through {Spatial} {Constraints} of {Parts} with {Foundation} {Models}},
	shorttitle = {{CoPa}},
	url = {http://arxiv.org/abs/2403.08248},
	doi = {10.48550/arXiv.2403.08248},
	abstract = {Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/},
	urldate = {2024-03-15},
	publisher = {arXiv},
	author = {Huang, Haoxu and Lin, Fanqi and Hu, Yingdong and Wang, Shengjie and Gao, Yang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08248 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{chen_learning_2020,
	title = {Learning by {Cheating}},
	url = {https://proceedings.mlr.press/v100/chen20a.html},
	abstract = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100\% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art.},
	language = {en},
	urldate = {2024-01-28},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Chen, Dian and Zhou, Brady and Koltun, Vladlen and Krähenbühl, Philipp},
	month = may,
	year = {2020},
	pages = {66--75},
}

@inproceedings{akmandor_deep_2022,
	title = {Deep {Reinforcement} {Learning} based {Robot} {Navigation} in {Dynamic} {Environments} using {Occupancy} {Values} of {Motion} {Primitives}},
	doi = {10.1109/IROS47612.2022.9982133},
	abstract = {This paper presents a Deep Reinforcement Learning based navigation approach in which we define the occu-pancy observations as heuristic evaluations of motion primitives, rather than using raw sensor data. Our method enables fast mapping of the occupancy data, generated by multi-sensor fusion, into trajectory values in 3D workspace. The computationally efficient trajectory evaluation allows dense sampling of the action space. We utilize our occupancy observations in different data structures to analyze their effects on both training process and navigation performance. We train and test our methodology on two different robots within challenging physics-based simulation environments including static and dy-namic obstacles. We benchmark our occupancy representations with other conventional data structures from state-of-the-art methods. The trained navigation policies are also validated successfully with physical robots in dynamic environments. The results show that our method not only decreases the required training time but also improves the navigation performance as compared to other occupancy representations. The open-source implementation of our work and all related info are available at https://github.com/RIVeR-Lab/tentabot.},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Akmandor, Neset Unver and Li, Hongyu and Lvov, Gary and Dusel, Eric and Padir, Taşkin},
	month = oct,
	year = {2022},
	keywords = {Benchmark testing, Deep learning, Navigation, Neural networks, Reinforcement learning, Three-dimensional displays, Training},
	pages = {11687--11694},
}

@inproceedings{rosmann_kinodynamic_2017,
	title = {Kinodynamic trajectory optimization and control for car-like robots},
	doi = {10.1109/IROS.2017.8206458},
	abstract = {This paper presents a novel generic formulation of Timed-Elastic-Bands for efficient online motion planning of car-like robots. The planning problem is defined in terms of a finite-dimensional and sparse optimization problem subject to the robots kinodynamic constraints and obstacle avoidance. Control actions are implicitly included in the optimized trajectory. Reliable navigation in dynamic environments is accomplished by augmenting the inner optimization loop with state feedback. The predictive control scheme is real-time capable and responds to obstacles within the robot's perceptual field. Navigation in large and complex environments is achieved in a pure pursuit fashion by requesting intermediate goals from a global planner. Requirements on the initial global path are fairly mild, compliance with the robot kinematics is not required. A comparative analysis with Reeds and Shepp curves and investigation of prototypical car maneuvers illustrate the advantages of the approach.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Rösmann, Christoph and Hoffmann, Frank and Bertram, Torsten},
	month = sep,
	year = {2017},
	keywords = {Kinematics, Navigation, Optimization, Planning, Robots, Trajectory, Turning},
	pages = {5681--5686},
}

@misc{xu_memory-based_2024,
	title = {Memory-based {Adapters} for {Online} {3D} {Scene} {Perception}},
	url = {http://arxiv.org/abs/2403.06974},
	doi = {10.48550/arXiv.2403.06974},
	abstract = {In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. {\textbackslash}href\{https://xuxw98.github.io/Online3D/\}\{Project page\}.},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Xu, Xiuwei and Xia, Chong and Wang, Ziwei and Zhao, Linqing and Duan, Yueqi and Zhou, Jie and Lu, Jiwen},
	month = mar,
	year = {2024},
	note = {arXiv:2403.06974 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_audio-synchronized_2024,
	title = {Audio-{Synchronized} {Visual} {Animation}},
	url = {http://arxiv.org/abs/2403.05659},
	doi = {10.48550/arXiv.2403.05659},
	abstract = {Current visual generation methods can produce high quality videos guided by texts. However, effectively controlling object dynamics remains a challenge. This work explores audio as a cue to generate temporally synchronized image animations. We introduce Audio Synchronized Visual Animation (ASVA), a task animating a static image to demonstrate motion dynamics, temporally guided by audio clips across multiple classes. To this end, we present AVSync15, a dataset curated from VGGSound with videos featuring synchronized audio visual events across 15 categories. We also present a diffusion model, AVSyncD, capable of generating dynamic animations guided by audios. Extensive evaluations validate AVSync15 as a reliable benchmark for synchronized generation and demonstrate our models superior performance. We further explore AVSyncDs potential in a variety of audio synchronized generation tasks, from generating full videos without a base image to controlling object motions with various sounds. We hope our established benchmark can open new avenues for controllable visual generation. More videos on project webpage https://lzhangbj.github.io/projects/asva/asva.html.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Zhang, Lin and Mo, Shentong and Zhang, Yijing and Morgado, Pedro},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05659 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tao_silvr_2024,
	title = {{SiLVR}: {Scalable} {Lidar}-{Visual} {Reconstruction} with {Neural} {Radiance} {Fields} for {Robotic} {Inspection}},
	shorttitle = {{SiLVR}},
	url = {http://arxiv.org/abs/2403.06877},
	doi = {10.48550/arXiv.2403.06877},
	abstract = {We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Tao, Yifu and Bhalgat, Yash and Fu, Lanke Frank Tarimo and Mattamala, Matias and Chebrolu, Nived and Fallon, Maurice},
	month = mar,
	year = {2024},
	note = {arXiv:2403.06877 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{hu_pose-aware_2023,
	title = {Pose-{Aware} {Attention} {Network} for {Flexible} {Motion} {Retargeting} by {Body} {Part}},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/10129844},
	doi = {10.1109/TVCG.2023.3277918},
	abstract = {Motion retargeting is a fundamental problem in computer graphics and computer vision. Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology. To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation, we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. To enhance the spatial modeling capability of the motion encoder, we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose, and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN. Our code is publicly available.},
	urldate = {2024-03-10},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hu, Lei and Zhang, Zihao and Zhong, Chongyang and Jiang, Boyuan and Xia, Shihong},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Codes, Deep learning, Dynamics, Feature extraction, Quadrupedal robots, Skeleton, Splicing, Task analysis, motion processing, motion retargeting},
	pages = {1--17},
}

@misc{wu_daydreamer_2022,
	title = {{DayDreamer}: {World} {Models} for {Physical} {Robot} {Learning}},
	shorttitle = {{DayDreamer}},
	url = {http://arxiv.org/abs/2206.14176},
	doi = {10.48550/arXiv.2206.14176},
	abstract = {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning.},
	urldate = {2024-03-10},
	publisher = {arXiv},
	author = {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Goldberg, Ken and Abbeel, Pieter},
	month = jun,
	year = {2022},
	note = {arXiv:2206.14176 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{abdul-massih_motion_2017,
	title = {Motion {Style} {Retargeting} to {Characters} {With} {Different} {Morphologies}},
	volume = {36},
	copyright = {© 2016 The Authors Computer Graphics Forum © 2016 The Eurographics Association and John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12860},
	doi = {10.1111/cgf.12860},
	abstract = {We present a novel approach for style retargeting to non-humanoid characters by allowing extracted stylistic features from one character to be added to the motion of another character with a different body morphology. We introduce the concept of groups of body parts (GBPs), for example, the torso, legs and tail, and we argue that they can be used to capture the individual style of a character motion. By separating GBPs from a character, the user can define mappings between characters with different morphologies. We automatically extract the motion of each GBP from the source, map it to the target and then use a constrained optimization to adjust all joints in each GBP in the target to preserve the original motion while expressing the style of the source. We show results on characters that present different morphologies to the source motion from which the style is extracted. The style transfer is intuitive and provides a high level of control. For most of the examples in this paper, the definition of GBP takes around 5 min and the optimization about 7 min on average. For the most complicated examples, the definition of three GBPs and their mapping takes about 10 min and the optimization another 30 min.},
	number = {6},
	urldate = {2024-03-10},
	journal = {Computer Graphics Forum},
	author = {Abdul-Massih, M. and Yoo, I. and Benes, B.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12860},
	keywords = {I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Animation, animation retargeting, animation systems, animation w/constraints, different morphologies, motion control},
	pages = {86--99},
}

@article{choi_online_2000,
	title = {Online motion retargetting},
	volume = {11},
	copyright = {Copyright © 2000 John Wiley \& Sons, Ltd.},
	issn = {1099-1778},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-1778%28200012%2911%3A5%3C223%3A%3AAID-VIS236%3E3.0.CO%3B2-5},
	doi = {10.1002/1099-1778(200012)11:5<223::AID-VIS236>3.0.CO;2-5},
	abstract = {This paper presents a method to retarget the motion of a character to another in real time. The technique is based on inverse rate control, which computes the changes in joint angles corresponding to the changes in end-effector position. While tracking the multiple end-effector trajectories of the original subject or character, our online motion retargetting also minimizes the joint angle differences by exploiting the kinematic redundancies of the animated model. This method can apply a captured motion to another anthropometry so that it can perform slightly different motion, while preserving the original motion characteristics. Because the above is done online, a real-time performance can be mapped to other characters. Moreover, if the method is used interactively during motion capture session, the feedback of retargetted motion on the screen provides more chances to get satisfactory results. As a by-product, our algorithm can be used to reduce measurement errors in restoring captured motion. The data enhancement improves the accuracy in both joint angles and end-effector positions. Experimental results show that our retargetting algorithm preserves high-frequency details of the original motion quite accurately. Copyright © 2000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2024-03-10},
	journal = {The Journal of Visualization and Computer Animation},
	author = {Choi, Kwang-Jin and Ko, Hyeong-Seok},
	year = {2000},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1099-1778\%28200012\%2911\%3A5\%3C223\%3A\%3AAID-VIS236\%3E3.0.CO\%3B2-5},
	keywords = {character animation, inverse kinematics, motion editing, motion retargetting},
	pages = {223--235},
}

@misc{li_crossloco_2023,
	title = {{CrossLoco}: {Human} {Motion} {Driven} {Control} of {Legged} {Robots} via {Guided} {Unsupervised} {Reinforcement} {Learning}},
	shorttitle = {{CrossLoco}},
	url = {https://arxiv.org/abs/2309.17046v1},
	abstract = {Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.},
	language = {en},
	urldate = {2024-03-10},
	author = {Li, Tianyu and Jung, Hyunyoung and Gombolay, Matthew and Cho, Yong Kwon and Ha, Sehoon},
	month = sep,
	year = {2023},
}

@misc{bohez_imitate_2022,
	title = {Imitate and {Repurpose}: {Learning} {Reusable} {Robot} {Movement} {Skills} {From} {Human} and {Animal} {Behaviors}},
	shorttitle = {Imitate and {Repurpose}},
	url = {https://arxiv.org/abs/2203.17138v1},
	abstract = {We investigate the use of prior knowledge of human and animal movement to learn reusable locomotion skills for real legged robots. Our approach builds upon previous work on imitating human or dog Motion Capture (MoCap) data to learn a movement skill module. Once learned, this skill module can be reused for complex downstream tasks. Importantly, due to the prior imposed by the MoCap data, our approach does not require extensive reward engineering to produce sensible and natural looking behavior at the time of reuse. This makes it easy to create well-regularized, task-oriented controllers that are suitable for deployment on real robots. We demonstrate how our skill module can be used for imitation, and train controllable walking and ball dribbling policies for both the ANYmal quadruped and OP3 humanoid. These policies are then deployed on hardware via zero-shot simulation-to-reality transfer. Accompanying videos are available at https://bit.ly/robot-npmp.},
	language = {en},
	urldate = {2024-03-10},
	author = {Bohez, Steven and Tunyasuvunakool, Saran and Brakel, Philemon and Sadeghi, Fereshteh and Hasenclever, Leonard and Tassa, Yuval and Parisotto, Emilio and Humplik, Jan and Haarnoja, Tuomas and Hafner, Roland and Wulfmeier, Markus and Neunert, Michael and Moran, Ben and Siegel, Noah and Huber, Andrea and Romano, Francesco and Batchelor, Nathan and Casarini, Federico and Merel, Josh and Hadsell, Raia and Heess, Nicolas},
	month = mar,
	year = {2022},
}

@inproceedings{wang_generative_2017,
	title = {A generative human-robot motion retargeting approach using a single depth sensor},
	url = {https://ieeexplore.ieee.org/document/7989632?denied=},
	doi = {10.1109/ICRA.2017.7989632},
	abstract = {The goal of human-robot motion retargeting is to let a robot follow the movements performed by a human subject. This is traditionally achieved by applying the estimated poses from a human pose tracking system to a robot via explicit joint mapping strategies. In this paper, we present a novel approach that combine the human pose estimation and the motion retarget procedure in a unified generative framework. A 3D parametric human-robot model is proposed that has the specific joint and stability configurations as a robot while its shape resembles a human subject. Using a single depth camera to monitor human pose, we use its raw depth map as input and drive the human-robot model to fit the input 3D point cloud. The calculated joint angles of the fitted model can be applied onto the robots for retargeting. The robot's joint angles, instead of fitted individually, are fitted globally so that the transformed surface shape is as consistent as possible to the input point cloud. The robot configurations including its skeleton proportion, joint limitation, and DoF are enforced implicitly in the formulation. No explicit and pre-defined joints mapping strategies are needed. This framework is tested with both simulations and real robots that have different skeleton proportion and DoFs compared with human to show its effectiveness for motion retargeting.},
	urldate = {2024-03-10},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wang, Sen and Zuo, Xinxin and Wang, Runxiao and Cheng, Fuhua and Yang, Ruigang},
	month = may,
	year = {2017},
	keywords = {Computational modeling, Joints, Robot sensing systems, Solid modeling, Three-dimensional displays},
	pages = {5369--5376},
}

@inproceedings{li_ace_2023,
	address = {New York, NY, USA},
	series = {{SA} '23},
	title = {{ACE}: {Adversarial} {Correspondence} {Embedding} for {Cross} {Morphology} {Motion} {Retargeting} from {Human} to {Nonhuman} {Characters}},
	isbn = {9798400703157},
	shorttitle = {{ACE}},
	url = {https://dl.acm.org/doi/10.1145/3610548.3618255},
	doi = {10.1145/3610548.3618255},
	abstract = {Motion retargeting is a promising approach for generating natural and compelling animations for nonhuman characters. However, it is challenging to translate human movements into semantically equivalent motions for target characters with different morphologies due to the ambiguous nature of the problem. This work presents a novel learning-based motion retargeting framework, Adversarial Correspondence Embedding (ACE), to retarget human motions onto target characters with different body dimensions and structures. Our framework is designed to produce natural and feasible character motions by leveraging generative-adversarial networks (GANs) while preserving high-level motion semantics by introducing an additional feature loss. In addition, we pretrain a character motion prior that can be controlled in a latent embedding space and seek to establish a compact correspondence. We demonstrate that the proposed framework can produce retargeted motions for three different characters – a quadrupedal robot with a manipulator, a crab character, and a wheeled manipulator. We further validate the design choices of our framework by conducting baseline comparisons and a user study. We also showcase sim-to-real transfer of the retargeted motions by transferring them to a real Spot robot.},
	urldate = {2024-03-09},
	booktitle = {{SIGGRAPH} {Asia} 2023 {Conference} {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Li, Tianyu and Won, Jungdam and Clegg, Alexander and Kim, Jeonghwan and Rai, Akshara and Ha, Sehoon},
	month = dec,
	year = {2023},
	keywords = {adversarial learning, character animation, motion retargeting},
	pages = {1--11},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-01-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@article{vedula_three-dimensional_2005,
	title = {Three-dimensional scene flow},
	volume = {27},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/1388274},
	doi = {10.1109/TPAMI.2005.63},
	abstract = {Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene structure from the inconsistencies in multiple optical flows.},
	number = {3},
	urldate = {2024-02-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Vedula, S. and Rander, P. and Collins, R. and Kanade, T.},
	month = mar,
	year = {2005},
	keywords = {Cameras, Computer vision, Deformable models, Fluid flow measurement, Image motion analysis, Index Terms- Scene flow, Layout, Motion estimation, Optical computing, Reflectivity, Smoothing methods, normal flow, optical flow, the brightness constancy constraint, three-dimensional dense nonrigid motion, three-dimensional normal flow.},
	pages = {475--480},
}

@article{falanga_how_2019,
	title = {How {Fast} {Is} {Too} {Fast}? {The} {Role} of {Perception} {Latency} in {High}-{Speed} {Sense} and {Avoid}},
	volume = {4},
	issn = {2377-3766},
	shorttitle = {How {Fast} {Is} {Too} {Fast}?},
	doi = {10.1109/LRA.2019.2898117},
	abstract = {In this letter, we study the effects that perception latency has on the maximum speed a robot can reach to safely navigate through an unknown cluttered environment. We provide a general analysis that can serve as a baseline for future quantitative reasoning for design tradeoffs in autonomous robot navigation. We consider the case where the robot is modeled as a linear secondorder system with bounded input and navigates through static obstacles. Also, we focus on a scenario where the robot wants to reach a target destination in as little time as possible, and therefore cannot change its longitudinal velocity to avoid obstacles. We show how the maximum latency that the robot can tolerate to guarantee safety is related to the desired speed, the range of its sensing pipeline, and the actuation limitations of the platform (i.e., the maximum acceleration it can produce). As a particular case study, we compare monocular and stereo frame-based cameras against novel, low-latency sensors, such as event cameras, in the case of quadrotor flight. To validate our analysis, we conduct experiments on a quadrotor platform equipped with an event camera to detect and avoid obstacles thrown towards the robot. To the best of our knowledge, this is the first theoretical work in which perception and actuation limitations are jointly considered to study the performance of a robotic platform in high-speed navigation.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Falanga, Davide and Kim, Suseong and Scaramuzza, Davide},
	month = apr,
	year = {2019},
	keywords = {Cameras, Collision avoidance, Navigation, Robot vision systems, aerial systems: perception and autonomy, visual-based navigation},
	pages = {1884--1891},
}

@inproceedings{wang_autonomous_2021,
	title = {Autonomous {Flights} in {Dynamic} {Environments} with {Onboard} {Vision}},
	url = {https://ieeexplore.ieee.org/abstract/document/9636117},
	doi = {10.1109/IROS51168.2021.9636117},
	abstract = {In this paper, we introduce a complete system for autonomous flight of quadrotors in dynamic environments with onboard sensing. Extended from existing work, we develop an occlusion-aware dynamic perception method based on depth images, which classifies obstacles as dynamic and static. For representing generic dynamic environment, we model dynamic objects with moving ellipsoids and fuse static ones into an occupancy grid map. To achieve dynamic avoidance, we design a planning method composed of modified kinodynamic path searching and gradient-based optimization. The method leverages manually constructed gradients without maintaining a signed distance field (SDF), making the planning procedure finished in milliseconds. We integrate the above methods into a customized quadrotor system and thoroughly test it in real-world experiments, verifying its effective collision avoidance in dynamic environments.},
	urldate = {2024-02-16},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wang, Yingjian and Ji, Jialin and Wang, Qianhao and Xu, Chao and Gao, Fei},
	month = sep,
	year = {2021},
	keywords = {Design methodology, Fuses, Navigation, Planning, Robot sensing systems, Sensors, Trajectory},
	pages = {1966--1973},
}

@article{lu_perception_2022,
	title = {Perception and {Avoidance} of {Multiple} {Small} {Fast} {Moving} {Objects} for {Quadrotors} {With} {Only} {Low}-{Cost} {RGBD} {Camera}},
	volume = {7},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/9881875},
	doi = {10.1109/LRA.2022.3205114},
	abstract = {The autonomous navigation of unmanned aerial vehicles in a rapidly changing environment, such as avoiding small fast moving objects with onboard sensing, still remains a challenge. In this letter, we propose a complete system that only relies on a lightweight RGBD camera to achieve fast and accurate perception and avoidance of small dynamic obstacles, whereas navigating in a complex environment. Firstly, we detect the moving objects by Yolo-Fastest in RGB frame, obtain the 3D information with the depth image, and track the multiple detected objects with our proposed 3D-SORT (Simple Online and Real-time Tracking in Three-dimensional Space) algorithm. To achieve fast dynamic avoidance, we design an effective method to generate the optimized smooth trajectory to dodge all the static and dynamic obstacles with the predicted moving objects' trajectories. Finally, we integrate the above methods on our UAV platform, and demonstrate the performance of our system by testing thoroughly in simulation and real-world experiments.},
	number = {4},
	urldate = {2024-02-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lu, Minghao and Chen, Han and Lu, Peng},
	month = oct,
	year = {2022},
	keywords = {Aerial systems, Cameras, Costs, Prediction algorithms, Real-time systems, Three-dimensional displays, Trajectory, Vehicle dynamics, collision avoidance, motion and path planning, perception and autonomy},
	pages = {11657--11664},
}

@inproceedings{chen_socially_2017,
	title = {Socially aware motion planning with deep reinforcement learning},
	doi = {10.1109/IROS.2017.8202312},
	abstract = {For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Chen, Yu Fan and Everett, Michael and Liu, Miao and How, Jonathan P.},
	month = sep,
	year = {2017},
	keywords = {Collision avoidance, Learning (artificial intelligence), Legged locomotion, Machine learning, Navigation, Robot sensing systems},
	pages = {1343--1350},
}

@inproceedings{li_bevformer_2022,
	title = {{BEVFormer}: {Learning} {Bird}’s-{Eye}-{View} {Representation} from {Multi}-camera {Images} via {Spatiotemporal} {Transformers}},
	isbn = {978-3-031-20077-9},
	shorttitle = {{BEVFormer}},
	doi = {10.1007/978-3-031-20077-9_1},
	abstract = {3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\% in terms of NDS metric on the nuScenes test set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. The code is available at https://github.com/zhiqi-li/BEVFormer.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	author = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Qiao, Yu and Dai, Jifeng},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {3D object detection, Autonomous driving, Bird’s-Eye-View, Map segmentation, Transformer},
	pages = {1--18},
}

@misc{gao_objectfolder_2023,
	title = {The {ObjectFolder} {Benchmark}: {Multisensory} {Learning} with {Neural} and {Real} {Objects}},
	shorttitle = {The {ObjectFolder} {Benchmark}},
	url = {http://arxiv.org/abs/2306.00956},
	doi = {10.48550/arXiv.2306.00956},
	abstract = {We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We also introduce the ObjectFolder Real dataset, including the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. We conduct systematic benchmarking on both the 1,000 multisensory neural objects from ObjectFolder, and the real multisensory data from ObjectFolder Real. Our results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for different object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Gao, Ruohan and Dou, Yiming and Li, Hao and Agarwal, Tanmay and Bohg, Jeannette and Li, Yunzhu and Fei-Fei, Li and Wu, Jiajun},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00956 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
}

@inproceedings{garcia-garcia_tactilegcn_2019,
	title = {{TactileGCN}: {A} {Graph} {Convolutional} {Network} for {Predicting} {Grasp} {Stability} with {Tactile} {Sensors}},
	shorttitle = {{TactileGCN}},
	doi = {10.1109/IJCNN.2019.8851984},
	abstract = {Tactile sensors provide useful contact data during the interaction with an object which can be used to accurately learn to determine the stability of a grasp. Most of the works in the literature represented tactile readings as plain feature vectors or matrix-like tactile images, using them to train machine learning models. In this work, we explore an alternative way of exploiting tactile information to predict grasp stability by leveraging graph-like representations of tactile data, which preserve the actual spatial arrangement of the sensor's taxels and their locality. In experimentation, we trained a Graph Neural Network to binary classify grasps as stable or slippery ones. To train such network and prove its predictive capabilities for the problem at hand, we captured a novel dataset of 5000 three-fingered grasps across 41 objects for training and 1000 grasps with 10 unknown objects for testing. Our experiments prove that this novel approach can be effectively used to predict grasp stability.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Garcia-Garcia, Alberto and Zapata-Impata, Brayan S. and Orts-Escolano, Sergio and Gil, Pablo and Garcia-Rodriguez, Jose},
	month = jul,
	year = {2019},
	keywords = {Neural networks, Stability analysis, Tactile sensors, Thermal stability},
	pages = {1--8},
}

@article{yang_tacgnn_2023,
	title = {{TacGNN}: {Learning} {Tactile}-{Based} {In}-{Hand} {Manipulation} {With} a {Blind} {Robot} {Using} {Hierarchical} {Graph} {Neural} {Network}},
	volume = {8},
	issn = {2377-3766},
	shorttitle = {{TacGNN}},
	doi = {10.1109/LRA.2023.3264759},
	abstract = {In this letter, we propose a novel framework for tactile-based dexterous manipulation learning with a blind anthropomorphic robotic hand, i.e. without visual sensing. First, object-related states were extracted from the raw tactile signals by a graph-based perception model - TacGNN. The resulting tactile features were then utilized in the policy learning of an in-hand manipulation task in the second stage. This method was examined by a Baoding ball task - simultaneously manipulating two spheres around each other by 180 degrees in hand. We conducted experiments on object states prediction and in-hand manipulation using a reinforcement learning algorithm (PPO). Results show that TacGNN is effective in predicting object-related states during manipulation by decreasing the RMSE of prediction to 0.096 cm comparing to other methods, such as MLP, CNN, and GCN. Finally, the robot hand could finish an in-hand manipulation task solely relying on the robotic own perception - tactile sensing and proprioception. In addition, our methods are tested on three tasks with different difficulty levels and transferred to the real robot without further training.},
	number = {6},
	journal = {IEEE Robotics and Automation Letters},
	author = {Yang, Linhan and Huang, Bidan and Li, Qingbiao and Tsai, Ya-Yen and Lee, Wang Wei and Song, Chaoyang and Pan, Jia},
	month = jun,
	year = {2023},
	keywords = {Convolutional neural networks, Dexterous manipulation, Graph neural networks, Reinforcement learning, Robot sensing systems, Sensors, Tactile sensors, Task analysis, force and tactile sensing, reinforcement learning, representation learning},
	pages = {3605--3612},
}

@article{li_visualtactile_2023,
	title = {Visual–{Tactile} {Fusion} for {Transparent} {Object} {Grasping} in {Complex} {Backgrounds}},
	issn = {1941-0468},
	doi = {10.1109/TRO.2023.3286071},
	abstract = {The grasping of transparent objects is challenging but of significance to robots. In this article, a visual–tactile fusion framework for transparent object grasping in complex backgrounds is proposed, which synergizes the advantages of vision and touch, and greatly improves the grasping efficiency of transparent objects. First, we propose a multiscene synthetic grasping dataset named SimTrans12 K together with a Gaussian-mask annotation method. Next, based on the TaTa gripper, we propose a grasping network named transparent object-grasping convolutional neural network for grasping position detection, which shows good performance in both synthetic and real scenes. Inspired by human grasping, a tactile calibration method and a visual–tactile fusion classification method are designed, which improve the grasping success rate by 36.7\% compared with direct grasping and the classification accuracy by 39.1\%. Furthermore, the tactile height sensing module and the tactile position exploration module are added to solve the problem of grasping transparent objects in irregular and visually undetectable scenes. The experimental results demonstrate the validity of the framework.},
	journal = {IEEE Transactions on Robotics},
	author = {Li, Shoujie and Yu, Haixin and Ding, Wenbo and Liu, Houde and Ye, Linqi and Xia, Chongkun and Wang, Xueqian and Zhang, Xiao-Ping},
	year = {2023},
	keywords = {Annotations, Cameras, Complex backgrounds, Grasping, Grippers, Robot vision systems, Robots, Synthetic data, synthetic transparent object dataset, tactile calibration, transparent object grasping, visual–tactile fusion},
	pages = {1--19},
}

@inproceedings{ding_sim--real_2021,
	title = {Sim-to-{Real} {Transfer} for {Robotic} {Manipulation} with {Tactile} {Sensory}},
	url = {https://ieeexplore.ieee.org/document/9636259},
	doi = {10.1109/IROS51168.2021.9636259},
	abstract = {Reinforcement Learning (RL) methods have been widely applied for robotic manipulations via sim-to-real transfer, typically with proprioceptive and visual information. However, the incorporation of tactile sensing into RL for contact-rich tasks lacks investigation. In this paper, we model a tactile sensor in simulation and study the effects of its feedback in RL-based robotic control via a zero-shot sim-to-real approach with domain randomization. We demonstrate that learning and controlling with feedback from tactile sensor arrays at the gripper, both in simulation and reality, can enhance grasping stability, which leads to a significant improvement in robotic manipulation performance for a door opening task. In real-world experiments, the door open angle was increased by 45\% on average for transferred policies with tactile sensing over those without it.},
	urldate = {2023-11-19},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Ding, Zihan and Tsai, Ya-Yen and Lee, Wang Wei and Huang, Bidan},
	month = sep,
	year = {2021},
	pages = {6778--6785},
}

@inproceedings{wu_tactile_2022,
	title = {Tactile {Pattern} {Super} {Resolution} with {Taxel}-based {Sensors}},
	doi = {10.1109/IROS47612.2022.9981062},
	abstract = {In contrast to sophisticated means of visual su-per resolution (SR), not much work has been done in the tactile SR field. Existing tactile SR algorithms for taxel-based sensors mainly focus on enhancing the localization accuracy, and generally associate with a specific type of hardware, sometimes not applicable to generic taxel-based tactile sensors. Inspired by image SR, we investigate the tactile pattern SR in this paper, and present how to transform successful image SR schemes, e.g. Convolutional Neural Network (CNN) and Generative Adversarial Network (GAN) to serve the tactile SR. We propose two tactile SR models, i.e. TactileSRCNN and TactileSRGAN, and establish a new tactile pattern SR dataset for model learning. The ground truth of high resolution (HR) tactile patterns in the dataset is obtained via multi-sampling (i.e. overlapping reception) and registration of low resolution (LR) sensor. One key contribution of this research lies in achieving ×100 (from 3×4×4 to 40×40) times tactile pattern SR with a one-time tapping of 3-axis taxel-based sensor. Different from existing tactile SR algorithms which improves the localization accuracy of a single contact point, the proposed scheme can provide multi-point contact detection to robotic applications.},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wu, Bing and Liu, Qian and Zhang, Qiang},
	month = oct,
	year = {2022},
	keywords = {Generative adversarial networks, Image resolution, Location awareness, Sensor systems, Tactile sensors, Transforms, Visualization},
	pages = {3644--3650},
}

@misc{zhao_galore_2024,
	title = {{GaLore}: {Memory}-{Efficient} {LLM} {Training} by {Gradient} {Low}-{Rank} {Projection}},
	shorttitle = {{GaLore}},
	url = {http://arxiv.org/abs/2403.03507},
	doi = {10.48550/arXiv.2403.03507},
	abstract = {Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5\% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5\% and total training memory by 63.3\%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03507 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{xu_dexterous_2024,
	title = {Dexterous {Legged} {Locomotion} in {Confined} {3D} {Spaces} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2403.03848},
	doi = {10.48550/arXiv.2403.03848},
	abstract = {Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In simulation, our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our simulation-trained controller on a real robot.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Xu, Zifan and Raj, Amir Hossain and Xiao, Xuesu and Stone, Peter},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03848 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{gao_bi-kvil_2024,
	title = {Bi-{KVIL}: {Keypoints}-based {Visual} {Imitation} {Learning} of {Bimanual} {Manipulation} {Tasks}},
	shorttitle = {Bi-{KVIL}},
	url = {http://arxiv.org/abs/2403.03270},
	abstract = {Visual imitation learning has achieved impressive progress in learning unimanual manipulation tasks from a small set of visual observations, thanks to the latest advances in computer vision. However, learning bimanual coordination strategies and complex object relations from bimanual visual demonstrations, as well as generalizing them to categorical objects in novel cluttered scenes remain unsolved challenges. In this paper, we extend our previous work on keypoints-based visual imitation learning ({\textbackslash}mbox\{K-VIL\}){\textasciitilde}{\textbackslash}cite\{gao\_kvil\_2023\} to bimanual manipulation tasks. The proposed Bi-KVIL jointly extracts so-called {\textbackslash}emph\{Hybrid Master-Slave Relationships\} (HMSR) among objects and hands, bimanual coordination strategies, and sub-symbolic task representations. Our bimanual task representation is object-centric, embodiment-independent, and viewpoint-invariant, thus generalizing well to categorical objects in novel scenes. We evaluate our approach in various real-world applications, showcasing its ability to learn fine-grained bimanual manipulation tasks from a small number of human demonstration videos. Videos and source code are available at https://sites.google.com/view/bi-kvil.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Gao, Jianfeng and Tao, Zhi and Jaquier, Noémie and Asfour, Tamim},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03270 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{torne_reconciling_2024,
	title = {Reconciling {Reality} through {Simulation}: {A} {Real}-to-{Sim}-to-{Real} {Approach} for {Robust} {Manipulation}},
	shorttitle = {Reconciling {Reality} through {Simulation}},
	url = {http://arxiv.org/abs/2403.03949},
	doi = {10.48550/arXiv.2403.03949},
	abstract = {Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67\%) in policy robustness without requiring extensive human data collection. Project website and videos at https://real-to-sim-to-real.github.io/RialTo/},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Torne, Marcel and Simeonov, Anthony and Li, Zechu and Chan, April and Chen, Tao and Gupta, Abhishek and Agrawal, Pulkit},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03949 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ze_3d_2024,
	title = {{3D} {Diffusion} {Policy}},
	url = {http://arxiv.org/abs/2403.03954},
	doi = {10.48550/arXiv.2403.03954},
	abstract = {Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3\% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85\%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Ze, Yanjie and Zhang, Gu and Zhang, Kangning and Hu, Chenyuan and Wang, Muhan and Xu, Huazhe},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03954 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{miki_learning_2024,
	title = {Learning to walk in confined spaces using {3D} representation},
	url = {http://arxiv.org/abs/2403.00187},
	abstract = {Legged robots have the potential to traverse complex terrain and access confined spaces beyond the reach of traditional platforms thanks to their ability to carefully select footholds and flexibly adapt their body posture while walking. However, robust deployment in real-world applications is still an open challenge. In this paper, we present a method for legged locomotion control using reinforcement learning and 3D volumetric representations to enable robust and versatile locomotion in confined and unstructured environments. By employing a two-layer hierarchical policy structure, we exploit the capabilities of a highly robust low-level policy to follow 6D commands and a high-level policy to enable three-dimensional spatial awareness for navigating under overhanging obstacles. Our study includes the development of a procedural terrain generator to create diverse training environments. We present a series of experimental evaluations in both simulation and real-world settings, demonstrating the effectiveness of our approach in controlling a quadruped robot in confined, rough terrain. By achieving this, our work extends the applicability of legged robots to a broader range of scenarios.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Miki, Takahiro and Lee, Joonho and Wellhausen, Lorenz and Hutter, Marco},
	month = feb,
	year = {2024},
	note = {arXiv:2403.00187 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_splat-nav_2024,
	title = {Splat-{Nav}: {Safe} {Real}-{Time} {Robot} {Navigation} in {Gaussian} {Splatting} {Maps}},
	shorttitle = {Splat-{Nav}},
	url = {http://arxiv.org/abs/2403.02751},
	doi = {10.48550/arXiv.2403.02751},
	abstract = {We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision. We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map. We then optimize a B-spline trajectory through this corridor. We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud. The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization. We also incorporate semantics into the GSplat in order to obtain better images for localization. All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction. We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Chen, Timothy and Shorinwa, Ola and Zeng, Weijia and Bruno, Joseph and Dames, Philip and Schwager, Mac},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02751 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lin_twisting_2024,
	title = {Twisting {Lids} {Off} with {Two} {Hands}},
	url = {http://arxiv.org/abs/2403.02338},
	abstract = {Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Lin, Toru and Yin, Zhao-Heng and Qi, Haozhi and Abbeel, Pieter and Malik, Jitendra},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhao_tac-man_2024,
	title = {Tac-{Man}: {Tactile}-{Informed} {Prior}-{Free} {Manipulation} of {Articulated} {Objects}},
	shorttitle = {Tac-{Man}},
	url = {http://arxiv.org/abs/2403.01694},
	doi = {10.48550/arXiv.2403.01694},
	abstract = {Integrating robotics into human-centric environments such as homes, necessitates advanced manipulation skills as robotic devices will need to engage with articulated objects like doors and drawers. Key challenges in robotic manipulation are the unpredictability and diversity of these objects' internal structures, which render models based on priors, both explicit and implicit, inadequate. Their reliability is significantly diminished by pre-interaction ambiguities, imperfect structural parameters, encounters with unknown objects, and unforeseen disturbances. Here, we present a prior-free strategy, Tac-Man, focusing on maintaining stable robot-object contact during manipulation. Utilizing tactile feedback, but independent of object priors, Tac-Man enables robots to proficiently handle a variety of articulated objects, including those with complex joints, even when influenced by unexpected disturbances. Demonstrated in both real-world experiments and extensive simulations, it consistently achieves near-perfect success in dynamic and varied settings, outperforming existing methods. Our results indicate that tactile sensing alone suffices for managing diverse articulated objects, offering greater robustness and generalization than prior-based approaches. This underscores the importance of detailed contact modeling in complex manipulation tasks, especially with articulated objects. Advancements in tactile sensors significantly expand the scope of robotic applications in human-centric environments, particularly where accurate models are difficult to obtain.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Zhao, Zihang and Li, Yuyang and Li, Wanlin and Qi, Zhenghao and Ruan, Lecheng and Zhu, Yixin and Althoefer, Kaspar},
	month = mar,
	year = {2024},
	note = {arXiv:2403.01694 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{garrido_learning_2024,
	title = {Learning and {Leveraging} {World} {Models} in {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2403.00504},
	abstract = {Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Garrido, Quentin and Assran, Mahmoud and Ballas, Nicolas and Bardes, Adrien and Najman, Laurent and LeCun, Yann},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00504 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kim_texterity_2024,
	title = {{TEXterity} -- {Tactile} {Extrinsic} {deXterity}: {Simultaneous} {Tactile} {Estimation} and {Control} for {Extrinsic} {Dexterity}},
	shorttitle = {{TEXterity} -- {Tactile} {Extrinsic} {deXterity}},
	url = {http://arxiv.org/abs/2403.00049},
	doi = {10.48550/arXiv.2403.00049},
	abstract = {We introduce a novel approach that combines tactile estimation and control for in-hand object manipulation. By integrating measurements from robot kinematics and an image-based tactile sensor, our framework estimates and tracks object pose while simultaneously generating motion plans in a receding horizon fashion to control the pose of a grasped object. This approach consists of a discrete pose estimator that tracks the most likely sequence of object poses in a coarsely discretized grid, and a continuous pose estimator-controller to refine the pose estimate and accurately manipulate the pose of the grasped object. Our method is tested on diverse objects and configurations, achieving desired manipulation objectives and outperforming single-shot methods in estimation accuracy. The proposed approach holds potential for tasks requiring precise manipulation and limited intrinsic in-hand dexterity under visual occlusion, laying the foundation for closed-loop behavior in applications such as regrasping, insertion, and tool use. Please see https://sites.google.com/view/texterity for videos of real-world demonstrations.},
	urldate = {2024-03-06},
	publisher = {arXiv},
	author = {Kim, Sangwoon and Bronars, Antonia and Patre, Parag and Rodriguez, Alberto},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00049 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{zhang_ga-net_2019,
	title = {{GA}-{Net}: {Guided} {Aggregation} {Net} for {End}-{To}-{End} {Stereo} {Matching}},
	shorttitle = {{GA}-{Net}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_GA-Net_Guided_Aggregation_Net_for_End-To-End_Stereo_Matching_CVPR_2019_paper.html},
	urldate = {2024-03-02},
	author = {Zhang, Feihu and Prisacariu, Victor and Yang, Ruigang and Torr, Philip H. S.},
	year = {2019},
	pages = {185--194},
}

@article{lepora_tactile_2015,
	title = {Tactile {Superresolution} and {Biomimetic} {Hyperacuity}},
	volume = {31},
	issn = {1941-0468},
	url = {https://ieeexplore.ieee.org/document/7078841},
	doi = {10.1109/TRO.2015.2414135},
	abstract = {Motivated by the impact of superresolution methods for imaging, we undertake a detailed and systematic analysis of localization acuity for a biomimetic fingertip and a flat region of tactile skin. We identify three key factors underlying superresolution that enable the perceptual acuity to surpass the sensor resolution: 1) the sensor is constructed with multiple overlapping, broad but sensitive receptive fields; 2) the tactile perception method interpolates between receptors (taxels) to attain subtaxel acuity; and 3) active perception ensures robustness to unknown initial contact location. All factors follow from active Bayesian perception applied to biomimetic tactile sensors with an elastomeric covering that spreads the contact over multiple taxels. In consequence, we attain extreme superresolution with a 35-fold improvement of localization acuity (0.12 mm) over sensor resolution (4 mm). We envisage that these principles will enable cheap high-acuity tactile sensors that are highly customizable to suit their robotic use. Practical applications encompass any scenario where an end-effector must be placed accurately via the sense of touch.},
	number = {3},
	urldate = {2023-10-15},
	journal = {IEEE Transactions on Robotics},
	author = {Lepora, Nathan F. and Martinez-Hernandez, Uriel and Evans, Mathew and Natale, Lorenzo and Metta, Giorgio and Prescott, Tony J.},
	month = jun,
	year = {2015},
	pages = {605--618},
}

@article{bimbo_-hand_2016,
	title = {In-{Hand} {Object} {Pose} {Estimation} {Using} {Covariance}-{Based} {Tactile} {To} {Geometry} {Matching}},
	volume = {1},
	issn = {2377-3766},
	doi = {10.1109/LRA.2016.2517244},
	abstract = {This letter presents a strategy to represent data from a tactile array sensor and match it to an object's geometric features. Using that representation, a method is presented to localise a grasped object within a robot hand. The method consists of computing the covariance matrix in the tactile sensors' pressure data and computing the eigenbasis from its principal axes. A global search is carried out to find a pose in which the object's local geometry in the vicinity of the contact is coherent with that basis, i.e., is aligned with the principal axes and has similar variances. This approach, which can be used as a measurement model for tactile sensors, is compared and outperforms methods using the distance between the tactile sensor elements and object surface.},
	number = {1},
	journal = {IEEE Robotics and Automation Letters},
	author = {Bimbo, Joao and Luo, Shan and Althoefer, Kaspar and Liu, Hongbin},
	month = jan,
	year = {2016},
	keywords = {Arrays, Contact Modelling, Covariance matrices, Force and Tactile Sensing, Geometry, Grasping, Tactile sensors},
	pages = {570--577},
}

@article{dikhale_visuotactile_2022,
	title = {{VisuoTactile} {6D} {Pose} {Estimation} of an {In}-{Hand} {Object} {Using} {Vision} and {Tactile} {Sensor} {Data}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3143289},
	abstract = {Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Dikhale, Snehal and Patel, Karankumar and Dhingra, Daksh and Naramura, Itoshi and Hayashi, Akinobu and Iba, Soshi and Jamali, Nawid},
	month = apr,
	year = {2022},
	keywords = {Channel estimation, Color, Image color analysis, Perception for grasping and manipulation, Point cloud compression, Pose estimation, Robot sensing systems, Tactile sensors, deep learning in grasping and manipulation, force and tactile sensing},
	pages = {2148--2155},
}

@article{piacenza_data-driven_2018,
	title = {Data-{Driven} {Super}-{Resolution} on a {Tactile} {Dome}},
	volume = {3},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/8276589},
	doi = {10.1109/LRA.2018.2800081},
	abstract = {While tactile sensor technology has made great strides over the past decades, applications in robotic manipulation are limited by aspects such as blind spots, difficult integration into hands, and low spatial resolution. We present a method for localizing contact with high accuracy over curved, 3-D surfaces, with a low wire count and reduced integration complexity. To achieve this, we build a volume of soft material embedded with individual off-the-shelf pressure sensors. Using data-driven techniques, we map the raw signals from these pressure sensors to known surface locations and indentation depths. Additionally, we show that a finite-element model can be used to improve the placement of the pressure sensors inside the volume and to explore the design space in simulation. We validate our approach on physically implemented tactile domes that achieve high contact localization accuracy (1.1 mm in the best case) over a large, curved sensing area (1300-mm2 hemisphere). We believe this approach can be used to deploy tactile sensing capabilities over 3-D surfaces such as a robotic finger or palm.},
	number = {3},
	urldate = {2023-10-15},
	journal = {IEEE Robotics and Automation Letters},
	author = {Piacenza, Pedro and Sherman, Sydney and Ciocarlie, Matei},
	month = jul,
	year = {2018},
	pages = {1434--1441},
}

@article{luu_simulation_2023,
	title = {Simulation, {Learning}, and {Application} of {Vision}-{Based} {Tactile} {Sensing} at {Large} {Scale}},
	volume = {39},
	issn = {1941-0468},
	doi = {10.1109/TRO.2023.3245983},
	abstract = {Large-scale robotic skin with tactile sensing ability is emerging with the potential for use in close-contact human–robot systems. Although recent developments in vision-based tactile sensing and related learning methods are promising, they have been mostly designed for small-scale use, such as by fingers and hands, in manipulation tasks. Moreover, learning perception for such tactile devices demands a huge tactile dataset, which complicates the data collection process. To address this, this study introduces a multiphysics simulation pipeline, called SimTacLS, which considers not only the mechanical properties of external physical contact but also the realistic rendering of tactile images in a simulation environment. The system utilizes the obtained simulation dataset, including virtual images and skin deformation, to train a tactile deep neural network to extract high-level tactile information. Moreover, we adopt a generative network to minimize sim2real inaccuracy, preserving the simulation-based tactile sensing performance. Last but not least, we showcase this sim2real sensing method for our large-scale tactile sensor (TacLink) by demonstrating its use in two trial cases, namely, whole-arm nonprehensile manipulation and intuitive motion guidance, using a custom-built tactile robot arm integrated with TacLink. This article opens new possibilities in the learning of transferable tactile-driven robotics tasks from virtual worlds to actual scenarios without compromising accuracy.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Luu, Quan Khanh and Nguyen, Nhan Huu and Ho, Van Anh},
	month = jun,
	year = {2023},
	keywords = {Cameras, Deformable model, Image sensors, Robot sensing systems, Robots, Sensors, Skin, Visualization, machine learning, soft robotics, tactile sensors},
	pages = {2003--2019},
}

@article{wettels_biomimetic_2008,
	title = {Biomimetic {Tactile} {Sensor} {Array}},
	volume = {22},
	issn = {0169-1864},
	url = {https://doi.org/10.1163/156855308X314533},
	doi = {10.1163/156855308X314533},
	abstract = {The performance of robotic and prosthetic hands in unstructured environments is severely limited by their having little or no tactile information compared to the rich tactile feedback of the human hand. We are developing a novel, robust tactile sensor array that mimics the mechanical properties and distributed touch receptors of the human fingertip. It consists of a rigid core surrounded by a weakly conductive fluid contained within an elastomeric skin. The sensor uses the deformable properties of the finger pad as part of the transduction process. Multiple electrodes are mounted on the surface of the rigid core and connected to impedance-measuring circuitry safely embedded within the core. External forces deform the fluid path around the electrodes, resulting in a distributed pattern of impedance changes containing information about those forces and the objects that applied them. Here we describe means to optimize the dynamic range of individual electrode sensors by texturing the inner surface of the silicone skin. Forces ranging from 0.1 to 30 N produced impedances ranging from 5 to 1000 kΩ. Spatial resolution (below 2 mm) and frequency response (above 50 Hz) appeared to be limited only by the viscoelastic properties of the silicone elastomeric skin.},
	number = {8},
	urldate = {2024-01-14},
	journal = {Advanced Robotics},
	author = {Wettels, Nicholas and Santos, Veronica J. and Johansson, Roland S. and Loeb, Gerald E.},
	month = jan,
	year = {2008},
	keywords = {BIOMIMETIC, ELECTRODE IMPEDANCE, HAPTICS, PRESSURE SENSOR, TACTILE SENSOR},
	pages = {829--849},
}

@article{tomo_new_2018,
	title = {A {New} {Silicone} {Structure} for {uSkin}—{A} {Soft}, {Distributed}, {Digital} 3-{Axis} {Skin} {Sensor} and {Its} {Integration} on the {Humanoid} {Robot} {iCub}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2812915},
	abstract = {Tactile sensing is one important element that can enable robots to interact with an unstructured world. By having tactile perception, a robot can explore its environment by touching objects. Like human skin, a tactile sensor that can provide rich information such as distributed normal and shear forces with high density can help the robot to recognize objects. In previous work, we introduced uSkin, a soft skin with distributed 3-axis force-sensitive elements and a center-to-center distance between the 3-axis load cells of 4.7 mm for the flat version. This letter presents a new structure for the distributed soft force transducer that reduces the crosstalk between the components of the 3-axis force measurements. Three dimensionally (3-D) printing the silicone structure eased the prototype production. However, the 3-D printed material has a higher hysteresis than the previously used Ecoflex. Microcontroller boards originally developed for the skin of iCub were implemented for uSkin, increasing the readout frequency and reducing the space requirements and number of wires. The sensor was installed on iCub and successfully used for shape exploration.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tomo, Tito Pradhono and Regoli, Massimo and Schmitz, Alexander and Natale, Lorenzo and Kristanto, Harris and Somlor, Sophon and Jamone, Lorenzo and Metta, Giorgio and Sugano, Shigeki},
	month = jul,
	year = {2018},
	keywords = {Crosstalk, Force, Force and tactile sensing, Force measurement, Robot sensing systems, Skin, dexterous manipulation, multifingered hands},
	pages = {2584--2591},
}

@article{yuan_gelsight_2017,
	title = {{GelSight}: {High}-{Resolution} {Robot} {Tactile} {Sensors} for {Estimating} {Geometry} and {Force}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {{GelSight}},
	url = {https://www.mdpi.com/1424-8220/17/12/2762},
	doi = {10.3390/s17122762},
	abstract = {Tactile sensing is an important perception mode for robots, but the existing tactile technologies have multiple limitations. What kind of tactile information robots need, and how to use the information, remain open questions. We believe a soft sensor surface and high-resolution sensing of geometry should be important components of a competent tactile sensor. In this paper, we discuss the development of a vision-based optical tactile sensor, GelSight. Unlike the traditional tactile sensors which measure contact force, GelSight basically measures geometry, with very high spatial resolution. The sensor has a contact surface of soft elastomer, and it directly measures its deformation, both vertical and lateral, which corresponds to the exact object shape and the tension on the contact surface. The contact force, and slip can be inferred from the sensor’s deformation as well. Particularly, we focus on the hardware and software that support GelSight’s application on robot hands. This paper reviews the development of GelSight, with the emphasis in the sensing principle and sensor design. We introduce the design of the sensor’s optical system, the algorithm for shape, force and slip measurement, and the hardware designs and fabrication of different sensor versions. We also show the experimental evaluation on the GelSight’s performance on geometry and force measurement. With the high-resolution measurement of shape and contact force, the sensor has successfully assisted multiple robotic tasks, including material perception or recognition and in-hand localization for robot manipulation.},
	language = {en},
	number = {12},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {Yuan, Wenzhen and Dong, Siyuan and Adelson, Edward H.},
	month = dec,
	year = {2017},
	keywords = {GelSight, robotics, tactile sensing},
	pages = {2762},
}

@article{dahiya_directions_2013,
	title = {Directions {Toward} {Effective} {Utilization} of {Tactile} {Skin}: {A} {Review}},
	volume = {13},
	issn = {1558-1748},
	shorttitle = {Directions {Toward} {Effective} {Utilization} of {Tactile} {Skin}},
	doi = {10.1109/JSEN.2013.2279056},
	abstract = {A wide variety of tactile (touch) sensors exist today for robotics and related applications. They make use of various transduction methods, smart materials and engineered structures, complex electronics, and sophisticated data processing. While highly useful in themselves, effective utilization of tactile sensors in robotics applications has been slow to come and largely remains elusive today. This paper surveys the state of the art and the research issues in this area, with the emphasis on effective utilization of tactile sensors in robotic systems. One specific with the use of tactile sensing in robotics is that the sensors have to be spread along the robot body, the way the human skin is-thus dictating varied 3-D spatio-temporal requirements, decentralized and distributed control, and handling of multiple simultaneous tactile contacts. Satisfying these requirements pose challenges to making tactile sensor modality a reality. Overcoming these challenges requires dealing with issues such as sensors placement, electronic/mechanical hardware, methods to access and acquire signals, automatic calibration techniques, and algorithms to process and interpret sensing data in real time. We survey this field from a system perspective, recognizing the fact that the system performance tends to depend on how its various components are put together. It is hoped that the survey will be of use to practitioners designing tactile sensing hardware (whole-body or large-patch sensor coverage), and to researchers working on cognitive robotics involving tactile sensing.},
	number = {11},
	journal = {IEEE Sensors Journal},
	author = {Dahiya, Ravinder S. and Mittendorfer, Philipp and Valle, Maurizio and Cheng, Gordon and Lumelsky, Vladimir J.},
	month = nov,
	year = {2013},
	keywords = {Skin, Substrates, Tactile sensors, Tactile skin, bendable electronics, embedded systems, interface electronics, modular, sensor fusion},
	pages = {4121--4138},
}

@article{dahiya_tactile_2010,
	title = {Tactile {Sensing}—{From} {Humans} to {Humanoids}},
	volume = {26},
	issn = {1941-0468},
	doi = {10.1109/TRO.2009.2033627},
	abstract = {Starting from human ¿sense of touch,¿ this paper reviews the state of tactile sensing in robotics. The physiology, coding, and transferring tactile data and perceptual importance of the ¿sense of touch¿ in humans are discussed. Following this, a number of design hints derived for robotic tactile sensing are presented. Various technologies and transduction methods used to improve the touch sense capability of robots are presented. Tactile sensing, focused to fingertips and hands until past decade or so, has now been extended to whole body, even though many issues remain open. Trend and methods to develop tactile sensing arrays for various body sites are presented. Finally, various system issues that keep tactile sensing away from widespread utility are discussed.},
	number = {1},
	journal = {IEEE Transactions on Robotics},
	author = {Dahiya, Ravinder S. and Metta, Giorgio and Valle, Maurizio and Sandini, Giulio},
	month = feb,
	year = {2010},
	keywords = {Cognitive robotics, Cutaneous sensing, Educational robots, Humanoid robots, Humans, Intelligent robots, Manufacturing, Physiology, Reactive power, Robot sensing systems, Skin, extrinsic sensing, humanoid robots, robotic skin, tactile sensing, touch sensing system},
	pages = {1--20},
}

@misc{wang_egocentric_2023,
	title = {Egocentric {Whole}-{Body} {Motion} {Capture} with {FisheyeViT} and {Diffusion}-{Based} {Motion} {Refinement}},
	url = {http://arxiv.org/abs/2311.16495},
	doi = {10.48550/arXiv.2311.16495},
	abstract = {In this work, we explore egocentric whole-body motion capture using a single fisheye camera, which simultaneously estimates human body and hand motion. This task presents significant challenges due to three factors: the lack of high-quality datasets, fisheye camera distortion, and human body self-occlusion. To address these challenges, we propose a novel approach that leverages FisheyeViT to extract fisheye image features, which are subsequently converted into pixel-aligned 3D heatmap representations for 3D human body pose prediction. For hand tracking, we incorporate dedicated hand detection and hand pose estimation networks for regressing 3D hand poses. Finally, we develop a diffusion-based whole-body motion prior model to refine the estimated whole-body motion while accounting for joint uncertainties. To train these networks, we collect a large synthetic dataset, EgoWholeBody, comprising 840,000 high-quality egocentric images captured across a diverse range of whole-body motion sequences. Quantitative and qualitative evaluations demonstrate the effectiveness of our method in producing high-quality whole-body motion estimates from a single egocentric camera.},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Wang, Jian and Cao, Zhe and Luvizon, Diogo and Liu, Lingjie and Sarkar, Kripasindhu and Tang, Danhang and Beeler, Thabo and Theobalt, Christian},
	month = dec,
	year = {2023},
	note = {arXiv:2311.16495 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{thoduka_multimodal_2024,
	title = {A {Multimodal} {Handover} {Failure} {Detection} {Dataset} and {Baselines}},
	url = {http://arxiv.org/abs/2402.18319},
	doi = {10.48550/arXiv.2402.18319},
	abstract = {An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the multimodal Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D CNNs and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Thoduka, Santosh and Hochgeschwender, Nico and Gall, Juergen and Plöger, Paul G.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18319 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{epstein_disentangled_2024,
	title = {Disentangled {3D} {Scene} {Generation} with {Layout} {Learning}},
	url = {http://arxiv.org/abs/2402.16936},
	abstract = {We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Epstein, Dave and Poole, Ben and Mildenhall, Ben and Efros, Alexei A. and Holynski, Aleksander},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16936 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cheng_expressive_2024,
	title = {Expressive {Whole}-{Body} {Control} for {Humanoid} {Robots}},
	url = {http://arxiv.org/abs/2402.16796},
	doi = {10.48550/arXiv.2402.16796},
	abstract = {Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Cheng, Xuxin and Ji, Yandong and Chen, Junming and Yang, Ruihan and Yang, Ge and Wang, Xiaolong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16796 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{guo_phygrasp_2024,
	title = {{PhyGrasp}: {Generalizing} {Robotic} {Grasping} with {Physics}-informed {Large} {Multimodal} {Models}},
	shorttitle = {{PhyGrasp}},
	url = {http://arxiv.org/abs/2402.16836},
	doi = {10.48550/arXiv.2402.16836},
	abstract = {Robotic grasping is a fundamental aspect of robot functionality, defining how robots interact with objects. Despite substantial progress, its generalizability to counter-intuitive or long-tailed scenarios, such as objects with uncommon materials or shapes, remains a challenge. In contrast, humans can easily apply their intuitive physics to grasp skillfully and change grasps efficiently, even for objects they have never seen before. This work delves into infusing such physical commonsense reasoning into robotic manipulation. We introduce PhyGrasp, a multimodal large model that leverages inputs from two modalities: natural language and 3D point clouds, seamlessly integrated through a bridge module. The language modality exhibits robust reasoning capabilities concerning the impacts of diverse physical properties on grasping, while the 3D modality comprehends object shapes and parts. With these two capabilities, PhyGrasp is able to accurately assess the physical properties of object parts and determine optimal grasping poses. Additionally, the model's language comprehension enables human instruction interpretation, generating grasping poses that align with human preferences. To train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances with varying physical properties and human preferences, alongside their corresponding language descriptions. Extensive experiments conducted in the simulation and on the real robots demonstrate that PhyGrasp achieves state-of-the-art performance, particularly in long-tailed cases, e.g., about 10\% improvement in success rate over GraspNet. Project page: https://sites.google.com/view/phygrasp},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Guo, Dingkun and Xiang, Yuqi and Zhao, Shuqi and Zhu, Xinghao and Tomizuka, Masayoshi and Ding, Mingyu and Zhan, Wei},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16836 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wu_dreamup3d_2024,
	title = {{DreamUp3D}: {Object}-{Centric} {Generative} {Models} for {Single}-{View} {3D} {Scene} {Understanding} and {Real}-to-{Sim} {Transfer}},
	shorttitle = {{DreamUp3D}},
	url = {http://arxiv.org/abs/2402.16308},
	doi = {10.48550/arXiv.2402.16308},
	abstract = {3D scene understanding for robotic applications exhibits a unique set of requirements including real-time inference, object-centric latent representation learning, accurate 6D pose estimation and 3D reconstruction of objects. Current methods for scene understanding typically rely on a combination of trained models paired with either an explicit or learnt volumetric representation, all of which have their own drawbacks and limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model (OCGM) designed explicitly to perform inference on a 3D scene informed only by a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end, and is capable of segmenting objects, providing 3D object reconstructions, generating object-centric latent representations and accurate per-object 6D pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene reconstruction, object matching and object pose estimation. Our experiments show that our model outperforms all baselines by a significant margin in real-world scenarios displaying its applicability for 3D scene understanding tasks while meeting the strict demands exhibited in robotics applications.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Wu, Yizhe and Borde, Haitz Sáez de Ocáriz and Collins, Jack and Jones, Oiwi Parker and Posner, Ingmar},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16308 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{zhao_e2-equivariant_2024,
	title = {E(2)-{Equivariant} {Graph} {Planning} for {Navigation}},
	volume = {9},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/10416668},
	doi = {10.1109/LRA.2024.3360011},
	abstract = {Learning for robot navigation presents a critical and challenging task. The scarcity and costliness of real-world datasets necessitate efficient learning approaches. In this letter, we exploit Euclidean symmetry in planning for 2D navigation, which originates from Euclidean transformations between reference frames and enables parameter sharing. To address the challenges of unstructured environments, we formulate the navigation problem as planning on a geometric graph and develop an equivariant message passing network to perform value iteration. Furthermore, to handle multi-camera input, we propose a learnable equivariant layer to lift features to a desired space. We conduct comprehensive evaluations across five diverse tasks encompassing structured and unstructured environments, along with maps of known and unknown, given point goals or semantic goals. Our experiments confirm the substantial benefits on training efficiency, stability, and generalization. More details can be found at the project website: https://lhy.xyz/e2-planning/.},
	number = {4},
	urldate = {2024-02-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zhao, Linfeng and Li, Hongyu and Padır, Taşkın and Jiang, Huaizu and Wong, Lawson L.S.},
	year = {2024},
	keywords = {Cameras, Integrated planning and learning, Message passing, Navigation, Planning, Robots, Task analysis, Visualization, deep learning methods, vision-based navigation},
	pages = {3371 -- 3378},
}

@article{loquercio_learning_2021,
	title = {Learning high-speed flight in the wild},
	volume = {6},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.abg5810},
	doi = {10.1126/scirobotics.abg5810},
	abstract = {Deep Learning enables agile flight in challenging environments with onboard sensing and computation. Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with onboard sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. Although this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. The subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here, we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and human-made environments at high speeds with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.},
	number = {59},
	journal = {Science Robotics},
	author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, René and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Obstacle Avoidance},
	pages = {eabg5810},
}

@article{tellex_robots_2020,
	title = {Robots {That} {Use} {Language}},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-control-101119-071628},
	doi = {10.1146/annurev-control-101119-071628},
	abstract = {This article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human–robot interaction. Finally, we describe several application domains for language-using robots.},
	number = {1},
	urldate = {2023-06-09},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Tellex, Stefanie and Gopalan, Nakul and Kress-Gazit, Hadas and Matuszek, Cynthia},
	year = {2020},
	keywords = {dialogue, grounding, language, learning, logic, robots},
	pages = {25--55},
}

@article{sorokin_learning_2022,
	title = {Learning to {Navigate} {Sidewalks} in {Outdoor} {Environments}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3145947},
	abstract = {Outdoor navigation on sidewalks in urban environments is the key technology behind important human assistive applications, such as last-mile delivery or neighborhood patrol. This letter aims to develop a quadruped robot that follows a route plan generated by public map services, while remaining on sidewalks and avoiding collisions with obstacles and pedestrians. We devise a two-staged learning framework, which first trains a teacher agent in an abstract world with privileged ground-truth information, and then applies Behavior Cloning to teach the skills to a student agent who only has access to realistic sensors. The main research effort of this letter focuses on overcoming challenges when deploying the student policy on a quadruped robot in the real world. We propose methodologies for designing sensing modalities, network architectures, and training procedures to enable zero-shot policy transfer to unstructured and dynamic real outdoor environments. We evaluate our learning framework on a quadrupedal robot navigating sidewalks in the city of Atlanta, USA (Fig. 1). Using the learned navigation policy and its onboard sensors, the robot is able to walk 3.2 kilometers with a limited number of human interventions.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sorokin, Maks and Tan, Jie and Liu, C. Karen and Ha, Sehoon},
	month = apr,
	year = {2022},
	keywords = {Legged robots, Navigation, Quadrupedal robots, Robot sensing systems, Robots, Sensors, Urban areas, Visualization, reinforcement learning, vision-based navigation},
	pages = {3906--3913},
}

@article{yasuda_safe_2023,
	title = {Safe and {Efficient} {Dynamic} {Window} {Approach} for {Differential} {Mobile} {Robots} {With} {Stochastic} {Dynamics} {Using} {Deterministic} {Sampling}},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3257681},
	abstract = {We propose an efficient and safe dynamic window approach (DWA) by using deterministic sampling. When the system dynamics have uncertainty, the control input includes errors, so that the DWA objective function becomes a random variable. When a random-choice algorithm with a finite number of samples is used to estimate the objective function, it may miss collisions during prediction. In this work, we approximate the end-state distribution as a one-dimensional distribution for each input candidate in advance and generate sample paths deterministically to eliminate the misses to achieve safe control. Numerical experiments have shown that this method is approximately three times as efficient as the Monte Carlo method in most indoor environments.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Yasuda, Shinya and Kumagai, Taichi and Yoshida, Hiroshi},
	year = {2023},
	keywords = {Collision Avoidance, Collision avoidance, Linear programming, Motion and Path Planning, Path planning, Planning under Uncertainty, Robots, Safety, Stochastic processes, Uncertainty},
	pages = {1--8},
}

@article{fox_dynamic_1997,
	title = {The dynamic window approach to collision avoidance},
	volume = {4},
	issn = {1558-223X},
	doi = {10.1109/100.580977},
	abstract = {This approach, designed for mobile robots equipped with synchro-drives, is derived directly from the motion dynamics of the robot. In experiments, the dynamic window approach safely controlled the mobile robot RHINO at speeds of up to 95 cm/sec, in populated and dynamic environments.},
	number = {1},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Fox, D. and Burgard, W. and Thrun, S.},
	month = mar,
	year = {1997},
	keywords = {Acceleration, Collision avoidance, Humans, Mobile robots, Motion control, Motion planning, Orbital robotics, Robot control, Robot sensing systems, Robotics and automation},
	pages = {23--33},
}

@article{tolani_visual_2021,
	title = {Visual {Navigation} {Among} {Humans} {With} {Optimal} {Control} as a {Supervisor}},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2021.3060638},
	abstract = {Real world visual navigation requires robots to operate in unfamiliar, human-occupied dynamic environments. Navigation around humans is especially difficult because it requires anticipating their future motion, which can be quite challenging. We propose an approach that combines learning-based perception with model-based optimal control to navigate among humans based only on monocular, first-person RGB images. Our approach is enabled by our novel data-generation tool, HumANav, that allows for photorealistic renderings of indoor environment scenes with humans in them, which are then used to train the perception module entirely in simulation. Through simulations and experiments on a mobile robot, we demonstrate that the learned navigation policies can anticipate and react to humans without explicitly predicting future human motion, generalize to previously unseen environments and human behaviors, and transfer directly from simulation to reality. Videos describing our approach and experiments, as well as a demo of HumANav are available on the project website.1},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tolani, Varun and Bansal, Somil and Faust, Aleksandra and Tomlin, Claire},
	month = apr,
	year = {2021},
	keywords = {Machine learning for robot control, Navigation, Planning, Robot kinematics, Robots, Tools, Trajectory, Visualization, transfer learning, vision-based navigation},
	pages = {2288--2295},
}

@article{loquercio_dronet_2018,
	title = {{DroNet}: {Learning} to {Fly} by {Driving}},
	volume = {3},
	issn = {2377-3766},
	shorttitle = {{DroNet}},
	doi = {10.1109/LRA.2018.2795643},
	abstract = {Civilian drones are soon expected to be used in a wide variety of tasks, such as aerial surveillance, delivery, or monitoring of existing architectures. Nevertheless, their deployment in urban environments has so far been limited. Indeed, in unstructured and highly dynamic scenarios, drones face numerous challenges to navigate autonomously in a feasible and safe way. In contrast to traditional “map-localize-plan” methods, this letter explores a data-driven approach to cope with the above challenges. To accomplish this, we propose DroNet: a convolutional neural network that can safely drive a drone through the streets of a city. Designed as a fast eight-layers residual network, DroNet produces two outputs for each single input image: A steering angle to keep the drone navigating while avoiding obstacles, and a collision probability to let the UAV recognize dangerous situations and promptly react to them. The challenge is however to collect enough data in an unstructured outdoor environment such as a city. Clearly, having an expert pilot providing training trajectories is not an option given the large amount of data required and, above all, the risk that it involves for other vehicles or pedestrians moving in the streets. Therefore, we propose to train a UAV from data collected by cars and bicycles, which, already integrated into the urban environment, would not endanger other vehicles and pedestrians. Although trained on city streets from the viewpoint of urban vehicles, the navigation policy learned by DroNet is highly generalizable. Indeed, it allows a UAV to successfully fly at relative high altitudes and even in indoor environments, such as parking lots and corridors. To share our findings with the robotics community, we publicly release all our datasets, code, and trained networks.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Loquercio, Antonio and Maqueda, Ana I. and del-Blanco, Carlos R. and Scaramuzza, Davide},
	month = apr,
	year = {2018},
	keywords = {Automobiles, Drones, Learning from demonstration, Navigation, Robots, Training, Urban areas, aerial systems: perception and autonomy, deep learning in robotics and automation},
	pages = {1088--1095},
}

@article{kahn_land_2021,
	title = {{LaND}: {Learning} to {Navigate} {From} {Disengagements}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {{LaND}},
	doi = {10.1109/LRA.2021.3060404},
	abstract = {Consistently testing autonomous mobile robots in real world scenarios is a necessary aspect of developing autonomous navigation systems. Each time the human safety monitor disengages the robot's autonomy system due to the robot performing an undesirable maneuver, the autonomy developers gain insight into how to improve the autonomy system. However, we believe that these disengagements not only show where the system fails, which is useful for troubleshooting, but also provide a direct learning signal by which the robot can learn to navigate. We present a reinforcement learning approach for learning to navigate from disengagements, or LaND. LaND learns a neural network model that predicts which actions lead to disengagements given the current sensory observation, and then at test time plans and executes actions that avoid disengagements. Our results demonstrate LaND can successfully learn to navigate in diverse, real world sidewalk environments, outperforming both imitation learning and reinforcement learning approaches. Videos, code, and other material are available on our website https://sites.google.com/view/sidewalk-learning.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
	month = apr,
	year = {2021},
	keywords = {Big data in robotics and automation, Mobile robots, Navigation, Predictive models, Reinforcement learning, Robot sensing systems, Robots, Testing, field robots, machine learning for robot control},
	pages = {1872--1879},
}

@article{li_vihope_2023,
	title = {{ViHOPE}: {Visuotactile} {In}-{Hand} {Object} {6D} {Pose} {Estimation} {With} {Shape} {Completion}},
	volume = {8},
	issn = {2377-3766},
	shorttitle = {{ViHOPE}},
	url = {https://ieeexplore.ieee.org/document/10246361},
	doi = {10.1109/LRA.2023.3313941},
	abstract = {In this letter, we introduce ViHOPE, a novel framework for estimating the 6D pose of an in-hand object using visuotactile perception. Our key insight is that the accuracy of the 6D object pose estimate can be improved by explicitly completing the shape of the object. To this end, we introduce a novel visuotactile shape completion module that uses a conditional Generative Adversarial Network to complete the shape of an in-hand object based on volumetric representation. This approach improves over prior works that directly regress visuotactile observations to a 6D pose. By explicitly completing the shape of the in-hand object and jointly optimizing the shape completion and pose estimation tasks, we improve the accuracy of the 6D object pose estimate. We train and test our model on a synthetic dataset and compare it with the state-of-the-art. In the visuotactile shape completion task, we outperform the state-of-the-art by 265\% using the Intersection of Union metric and achieve 88\% lower Chamfer Distance. In the visuotactile pose estimation task, we present results that suggest our framework reduces position and angular errors by 35\% and 64\%, respectively. Furthermore, we ablate our framework to confirm the gain on the 6D object pose estimate from explicitly completing the shape. Ultimately, we show that our framework produces models that are robust to sim-to-real transfer on a real-world robot platform.},
	number = {11},
	urldate = {2023-09-30},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Hongyu and Dikhale, Snehal and Iba, Soshi and Jamali, Nawid},
	month = nov,
	year = {2023},
	pages = {6963--6970},
}

@article{miki_learning_2022,
	title = {Learning robust perceptive locomotion for quadrupedal robots in the wild},
	volume = {7},
	url = {https://www.science.org/doi/10.1126/scirobotics.abk2822},
	doi = {10.1126/scirobotics.abk2822},
	abstract = {Legged robots that can operate autonomously in remote and hazardous environments will greatly increase opportunities for exploration into underexplored areas. Exteroceptive perception is crucial for fast and energy-efficient locomotion: Perceiving the terrain before making contact with it enables planning and adaptation of the gait ahead of time to maintain speed and stability. However, using exteroceptive perception robustly for locomotion has remained a grand challenge in robotics. Snow, vegetation, and water visually appear as obstacles on which the robot cannot step or are missing altogether due to high reflectance. In addition, depth perception can degrade due to difficult lighting, dust, fog, reflective or transparent surfaces, sensor occlusion, and more. For this reason, the most robust and general solutions to legged locomotion to date rely solely on proprioception. This severely limits locomotion speed because the robot has to physically feel out the terrain before adapting its gait accordingly. Here, we present a robust and general solution to integrating exteroceptive and proprioceptive perception for legged locomotion. We leverage an attention-based recurrent encoder that integrates proprioceptive and exteroceptive input. The encoder is trained end to end and learns to seamlessly combine the different perception modalities without resorting to heuristics. The result is a legged locomotion controller with high robustness and speed. The controller was tested in a variety of challenging natural and urban environments over multiple seasons and completed an hour-long hike in the Alps in the time recommended for human hikers.},
	number = {62},
	urldate = {2024-02-27},
	journal = {Science Robotics},
	author = {Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
	month = jan,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabk2822},
}

@misc{hafner_mastering_2023,
	title = {Mastering {Diverse} {Domains} through {World} {Models}},
	url = {http://arxiv.org/abs/2301.04104},
	doi = {10.48550/arXiv.2301.04104},
	abstract = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04104 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{jiang_roboexp_2024,
	title = {{RoboEXP}: {Action}-{Conditioned} {Scene} {Graph} via {Interactive} {Exploration} for {Robotic} {Manipulation}},
	shorttitle = {{RoboEXP}},
	url = {http://arxiv.org/abs/2402.15487},
	abstract = {Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities. The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG. We apply our system across various real-world settings in a zero-shot manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before. Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Jiang, Hanxiao and Huang, Binghao and Wu, Ruihai and Li, Zhuoran and Garg, Shubham and Nayyeri, Hooshang and Wang, Shenlong and Li, Yunzhu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.15487 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_geneoh_2024,
	title = {{GeneOH} {Diffusion}: {Towards} {Generalizable} {Hand}-{Object} {Interaction} {Denoising} via {Denoising} {Diffusion}},
	shorttitle = {{GeneOH} {Diffusion}},
	url = {http://arxiv.org/abs/2402.14810},
	doi = {10.48550/arXiv.2402.14810},
	abstract = {In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a "denoising via diffusion" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Liu, Xueyi and Yi, Li},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14810 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{luiten_dynamic_2023,
	title = {Dynamic {3D} {Gaussians}: {Tracking} by {Persistent} {Dynamic} {View} {Synthesis}},
	shorttitle = {Dynamic {3D} {Gaussians}},
	url = {http://arxiv.org/abs/2308.09713},
	doi = {10.48550/arXiv.2308.09713},
	abstract = {We present a method that simultaneously addresses the tasks of dynamic scene novel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense scene elements. We follow an analysis-by-synthesis framework, inspired by recent work that models scenes as a collection of 3D Gaussians which are optimized to reconstruct input images via differentiable rendering. To model dynamic scenes, we allow Gaussians to move and rotate over time while enforcing that they have persistent color, opacity, and size. By regularizing Gaussians' motion and rotation with local-rigidity constraints, we show that our Dynamic 3D Gaussians correctly model the same area of physical space over time, including the rotation of that space. Dense 6-DOF tracking and dynamic reconstruction emerges naturally from persistent dynamic view synthesis, without requiring any correspondence or flow as input. We demonstrate a large number of downstream applications enabled by our representation, including first-person view synthesis, dynamic compositional scene synthesis, and 4D video editing.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Luiten, Jonathon and Kopanas, Georgios and Leibe, Bastian and Ramanan, Deva},
	month = aug,
	year = {2023},
	note = {arXiv:2308.09713 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{keetha_splatam_2023,
	title = {{SplaTAM}: {Splat}, {Track} \& {Map} {3D} {Gaussians} for {Dense} {RGB}-{D} {SLAM}},
	shorttitle = {{SplaTAM}},
	url = {http://arxiv.org/abs/2312.02126},
	doi = {10.48550/arXiv.2312.02126},
	abstract = {Dense simultaneous localization and mapping (SLAM) is pivotal for embodied scene understanding. Recent work has shown that 3D Gaussians enable high-quality reconstruction and real-time rendering of scenes using multiple posed cameras. In this light, we show for the first time that representing a scene by 3D Gaussians can enable dense SLAM using a single unposed monocular RGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiance field-based representations, including fast rendering and optimization, the ability to determine if areas have been previously mapped, and structured map expansion by adding more Gaussians. We employ an online tracking and mapping pipeline while tailoring it to specifically use an underlying Gaussian representation and silhouette-guided optimization via differentiable rendering. Extensive experiments show that SplaTAM achieves up to 2X state-of-the-art performance in camera pose estimation, map construction, and novel-view synthesis, demonstrating its superiority over existing approaches, while allowing real-time rendering of a high-resolution dense 3D map.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02126 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_cameras_2024,
	title = {Cameras as {Rays}: {Pose} {Estimation} via {Ray} {Diffusion}},
	shorttitle = {Cameras as {Rays}},
	url = {http://arxiv.org/abs/2402.14817},
	doi = {10.48550/arXiv.2402.14817},
	abstract = {Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views ({\textless}10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Zhang, Jason Y. and Lin, Amy and Kumar, Moneish and Yang, Tzu-Hsuan and Ramanan, Deva and Tulsiani, Shubham},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14817 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{agarwal_legged_2022,
	title = {Legged {Locomotion} in {Challenging} {Terrains} using {Egocentric} {Vision}},
	url = {http://arxiv.org/abs/2211.07638},
	doi = {10.48550/arXiv.2211.07638},
	abstract = {Animals are capable of precise and agile locomotion using vision. Replicating this ability has been a long-standing goal in robotics. The traditional approach has been to decompose this problem into elevation mapping and foothold planning phases. The elevation mapping, however, is susceptible to failure and large noise artifacts, requires specialized hardware, and is biologically implausible. In this paper, we present the first end-to-end locomotion system capable of traversing stairs, curbs, stepping stones, and gaps. We show this result on a medium-sized quadruped robot using a single front-facing depth camera. The small size of the robot necessitates discovering specialized gait patterns not seen elsewhere. The egocentric camera requires the policy to remember past information to estimate the terrain under its hind feet. We train our policy in simulation. Training has two phases - first, we train a policy using reinforcement learning with a cheap-to-compute variant of depth image and then in phase 2 distill it into the final policy that uses depth using supervised learning. The resulting policy transfers to the real world and is able to run in real-time on the limited compute of the robot. It can traverse a large variety of terrain while being robust to perturbations like pushes, slippery surfaces, and rocky terrain. Videos are at https://vision-locomotion.github.io},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Agarwal, Ananye and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07638 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{rudin_learning_2022,
	title = {Learning to {Walk} in {Minutes} {Using} {Massively} {Parallel} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2109.11978},
	doi = {10.48550/arXiv.2109.11978},
	abstract = {In this work, we present and study a training set-up that achieves fast policy generation for real-world robotic tasks by using massive parallelism on a single workstation GPU. We analyze and discuss the impact of different training algorithm components in the massively parallel regime on the final policy performance and training times. In addition, we present a novel game-inspired curriculum that is well suited for training with thousands of simulated robots in parallel. We evaluate the approach by training the quadrupedal robot ANYmal to walk on challenging terrain. The parallel approach allows training policies for flat terrain in under four minutes, and in twenty minutes for uneven terrain. This represents a speedup of multiple orders of magnitude compared to previous work. Finally, we transfer the policies to the real robot to validate the approach. We open-source our training code to help accelerate further research in the field of learned legged locomotion.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Rudin, Nikita and Hoeller, David and Reist, Philipp and Hutter, Marco},
	month = aug,
	year = {2022},
	note = {arXiv:2109.11978 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{he_large-scale_2024,
	title = {Large-{Scale} {Actionless} {Video} {Pre}-{Training} via {Discrete} {Diffusion} for {Efficient} {Policy} {Learning}},
	url = {http://arxiv.org/abs/2402.14407},
	doi = {10.48550/arXiv.2402.14407},
	abstract = {Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {He, Haoran and Bai, Chenjia and Pan, Ling and Zhang, Weinan and Zhao, Bin and Li, Xuelong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14407 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_cyberdemo_2024,
	title = {{CyberDemo}: {Augmenting} {Simulated} {Human} {Demonstration} for {Real}-{World} {Dexterous} {Manipulation}},
	shorttitle = {{CyberDemo}},
	url = {http://arxiv.org/abs/2402.14795},
	doi = {10.48550/arXiv.2402.14795},
	abstract = {We introduce CyberDemo, a novel approach to robotic imitation learning that leverages simulated human demonstrations for real-world tasks. By incorporating extensive data augmentation in a simulated environment, CyberDemo outperforms traditional in-domain real-world demonstrations when transferred to the real world, handling diverse physical and visual conditions. Regardless of its affordability and convenience in data collection, CyberDemo outperforms baseline methods in terms of success rates across various tasks and exhibits generalizability with previously unseen objects. For example, it can rotate novel tetra-valve and penta-valve, despite human demonstrations only involving tri-valves. Our research demonstrates the significant potential of simulated human demonstrations for real-world dexterous manipulation tasks. More details can be found at https://cyber-demo.github.io},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Wang, Jun and Qin, Yuzhe and Kuang, Kaiming and Korkmaz, Yigit and Gurumoorthy, Akhilan and Su, Hao and Wang, Xiaolong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14795 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{li_fld_2024,
	title = {{FLD}: {Fourier} {Latent} {Dynamics} for {Structured} {Motion} {Representation} and {Learning}},
	shorttitle = {{FLD}},
	url = {http://arxiv.org/abs/2402.13820},
	doi = {10.48550/arXiv.2402.13820},
	abstract = {Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Li, Chenhao and Stanger-Jones, Elijah and Heim, Steve and Kim, Sangbae},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13820 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{christen_synh2r_2023,
	title = {{SynH2R}: {Synthesizing} {Hand}-{Object} {Motions} for {Learning} {Human}-to-{Robot} {Handovers}},
	shorttitle = {{SynH2R}},
	url = {http://arxiv.org/abs/2311.05599},
	doi = {10.48550/arXiv.2311.05599},
	abstract = {Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Christen, Sammy and Feng, Lan and Yang, Wei and Chao, Yu-Wei and Hilliges, Otmar and Song, Jie},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05599 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{zhao_combined_2024,
	title = {A {Combined} {Learning} and {Optimization} {Framework} to {Transfer} {Human} {Whole}-body {Loco}-manipulation {Skills} to {Mobile} {Manipulators}},
	url = {http://arxiv.org/abs/2402.13915},
	doi = {10.48550/arXiv.2402.13915},
	abstract = {Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Zhao, Jianzhuang and Tassi, Francesco and Huang, Yanlong and De Momi, Elena and Ajoudani, Arash},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13915 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liu_realdex_2024,
	title = {{RealDex}: {Towards} {Human}-like {Grasping} for {Robotic} {Dexterous} {Hand}},
	shorttitle = {{RealDex}},
	url = {http://arxiv.org/abs/2402.13853},
	doi = {10.48550/arXiv.2402.13853},
	abstract = {In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Liu, Yumeng and Yang, Yaxun and Wang, Youzhuo and Wu, Xiaofei and Wang, Jiamin and Yao, Yichen and Schwertfeger, Sören and Yang, Sibei and Wang, Wenping and Yu, Jingyi and He, Xuming and Ma, Yuexin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13853 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{pan_renderme-360_2023,
	title = {{RenderMe}-360: {A} {Large} {Digital} {Asset} {Library} and {Benchmarks} {Towards} {High}-fidelity {Head} {Avatars}},
	shorttitle = {{RenderMe}-360},
	url = {https://openreview.net/forum?id=22RlsVAOTT},
	abstract = {Synthesizing high-fidelity head avatars is a central problem for computer vision and graphics. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world scenarios. One of the vital causes is the inadequate datasets -- 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task directions; 2) these datasets usually contain digital head assets with limited data volume, and narrow distribution over different attributes, such as expressions, ages, and accessories. In this paper, we present RenderMe-360, a comprehensive 4D human head dataset to drive advance in head avatar algorithms across different scenarios. It contains massive data assets, with 243+ million complete head frames and over 800k video sequences from 500 different identities captured by multi-view cameras at 30 FPS. It is a large-scale digital library for head avatars with three key attributes: 1) High Fidelity: all subjects are captured in 360 degrees via 60 synchronized, high-resolution 2K cameras. 2) High Diversity: The collected subjects vary from different ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geometry. Moreover, each subject is asked to perform various dynamic motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annotations: the dataset provides annotations with different granularities: cameras' parameters, background matting, scan, 2D/3D facial landmarks, FLAME fitting, and text description. Based on the dataset, we build a comprehensive benchmark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel view synthesis, novel expression synthesis, hair rendering, hair editing, and talking head generation. Our experiments uncover the strengths and flaws of state-of-the-art methods. RenderMe-360 opens the door for future exploration in modern head avatars. All of the data, code, and models will be publicly available at https://renderme-360.github.io/.},
	language = {en},
	urldate = {2024-02-22},
	author = {Pan, Dongwei and Zhuo, Long and Piao, Jingtan and Luo, Huiwen and Cheng, Wei and Wang, Yuxin and Fan, Siming and Liu, Shengqi and Yang, Lei and Dai, Bo and Liu, Ziwei and Loy, Chen Change and Qian, Chen and Wu, Wayne and Lin, Dahua and Lin, Kwan-Yee},
	month = nov,
	year = {2023},
}

@misc{fu_touch_2024,
	title = {A {Touch}, {Vision}, and {Language} {Dataset} for {Multimodal} {Alignment}},
	url = {http://arxiv.org/abs/2402.13232},
	abstract = {Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10\%) and textual pseudo-labels from GPT-4V (90\%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29\% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12\%) and open-source vision-language models (+32\%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Fu, Letian and Datta, Gaurav and Huang, Huang and Panitch, William Chung-Ho and Drake, Jaimyn and Ortiz, Joseph and Mukadam, Mustafa and Lambeta, Mike and Calandra, Roberto and Goldberg, Ken},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13232 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{komeno_incipient_2024,
	title = {Incipient {Slip} {Detection} by {Vibration} {Injection} into {Soft} {Sensor}},
	url = {http://arxiv.org/abs/2402.11879},
	doi = {10.48550/arXiv.2402.11879},
	abstract = {In robotic manipulation, preventing objects from slipping and establishing a secure grip on them is critical. Successful manipulation requires tactile sensors that detect the microscopic incipient slip phenomenon at the contact surface. Unfortunately, the tiny signals generated by incipient slip are quickly buried by environmental noise, and precise stress-distribution measurement requires an extensive optical system and integrated circuits. In this study, we focus on the macroscopic deformation of the entire fingertip's soft structure instead of directly observing the contact surface and its role as a vibration medium for sensing. The proposed method compresses the stick ratio's information into a one-dimensional pressure signal using the change in the propagation characteristics by vibration injection into the soft structure, which magnifies the microscopic incipient slip phenomena into the entire deformation. This mechanism allows a tactile sensor to use just a single vibration sensor. In the implemented system, a biomimetic tactile sensor is vibrated using a white signal from a PZT motor and utilizes frequency spectrum change of the propagated vibration as features. We investigated the proposed method's effectiveness on stick-ratio estimation and {\textbackslash}red\{stick-ratio stabilization\} control during incipient slip. Our estimation error and the control performance results significantly outperformed the conventional methods.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Komeno, Naoto and Matsubara, Takamitsu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11879 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{cheng_towards_2024,
	title = {Towards {A} {Richer} {2D} {Understanding} of {Hands} at {Scale}},
	url = {https://www.scholar-inbox.com/?date=02-20-2024},
	abstract = {As humans, we learn a lot about how to interact with the world by observing others interacting with their hands. To help AI systems obtain a better understanding of hand interactions, we introduce a new model that produces a rich understanding of hand interaction. Our system produces a richer output than past systems at a larger scale. Our outputs include boxes and segments for hands, in-contact objects, and second objects touched by tools as well as contact and grasp type. Supporting this method are annotations of 257K images, 401K hands, 288K objects, and 19K second objects spanning four datasets. We show that our method provides rich information and performs and generalizes well.},
	language = {en},
	urldate = {2024-02-20},
	author = {Cheng, Tianyi and Shan, Dandan and Hassen, Ayda and Higgins, Richard and Fouhey, David},
	month = feb,
	year = {2024},
}

@inproceedings{cho_visual_2024,
	title = {Visual {Programming} for {Step}-by-{Step} {Text}-to-{Image} {Generation} and {Evaluation}},
	url = {https://www.scholar-inbox.com/?date=02-20-2024},
	abstract = {As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows that VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope that our work encourages future progress on interpretable/explainable generation and evaluation for T2I models.},
	language = {en},
	urldate = {2024-02-20},
	author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
	month = feb,
	year = {2024},
}

@misc{nguyen_real-time_2024,
	title = {Real-time {3D} {Semantic} {Scene} {Perception} for {Egocentric} {Robots} with {Binocular} {Vision}},
	url = {http://arxiv.org/abs/2402.11872},
	doi = {10.48550/arXiv.2402.11872},
	abstract = {Perceiving a three-dimensional (3D) scene with multiple objects while moving indoors is essential for vision-based mobile cobots, especially for enhancing their manipulation tasks. In this work, we present an end-to-end pipeline with instance segmentation, feature matching, and point-set registration for egocentric robots with binocular vision, and demonstrate the robot's grasping capability through the proposed pipeline. First, we design an RGB image-based segmentation approach for single-view 3D semantic scene segmentation, leveraging common object classes in 2D datasets to encapsulate 3D points into point clouds of object instances through corresponding depth maps. Next, 3D correspondences of two consecutive segmented point clouds are extracted based on matched keypoints between objects of interest in RGB images from the prior step. In addition, to be aware of spatial changes in 3D feature distribution, we also weigh each 3D point pair based on the estimated distribution using kernel density estimation (KDE), which subsequently gives robustness with less central correspondences while solving for rigid transformations between point clouds. Finally, we test our proposed pipeline on the 7-DOF dual-arm Baxter robot with a mounted Intel RealSense D435i RGB-D camera. The result shows that our robot can segment objects of interest, register multiple views while moving, and grasp the target object. The source code is available at https://github.com/mkhangg/semantic\_scene\_perception.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Nguyen, K. and Dang, T. and Huber, M.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11872 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{wang_mimicplay_2023,
	title = {{MimicPlay}: {Long}-{Horizon} {Imitation} {Learning} by {Watching} {Human} {Play}},
	shorttitle = {{MimicPlay}},
	url = {https://openreview.net/forum?id=hRZ1YjDZmTo},
	abstract = {Imitation learning from human demonstrations is a promising paradigm for teaching robots manipulation skills in the real world. However, learning complex long-horizon tasks often requires an unattainable amount of demonstrations. To reduce the high data requirement, we resort to human play data - video sequences of people freely interacting with the environment using their hands. Even with different morphologies, we hypothesize that human play data contain rich and salient information about physical interactions that can readily facilitate robot policy learning. Motivated by this, we introduce a hierarchical learning framework named MimicPlay that learns latent plans from human play data to guide low-level visuomotor control trained on a small number of teleoperated demonstrations. With systematic evaluations of 14 long-horizon manipulation tasks in the real world, we show that MimicPlay outperforms state-of-the-art imitation learning methods in task success rate, generalization ability, and robustness to disturbances. Code and videos are available at https://mimic-play.github.io.},
	language = {en},
	urldate = {2024-02-20},
	author = {Wang, Chen and Fan, Linxi and Sun, Jiankai and Zhang, Ruohan and Fei-Fei, Li and Xu, Danfei and Zhu, Yuke and Anandkumar, Anima},
	month = aug,
	year = {2023},
}

@inproceedings{loshchilov_decoupled_2018,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/AdamW-and-SGDW\}},
	language = {en},
	urldate = {2024-02-20},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = sep,
	year = {2018},
}

@article{zbontar_stereo_2016,
	title = {Stereo {Matching} by {Training} a {Convolutional} {Neural} {Network} to {Compare} {Image} {Patches}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-535.html},
	abstract = {We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.},
	number = {65},
	urldate = {2024-02-19},
	journal = {Journal of Machine Learning Research},
	author = {Žbontar, Jure and LeCun, Yann},
	year = {2016},
	pages = {1--32},
}

@misc{chi_universal_2024,
	title = {Universal {Manipulation} {Interface}: {In}-{The}-{Wild} {Robot} {Teaching} {Without} {In}-{The}-{Wild} {Robots}},
	shorttitle = {Universal {Manipulation} {Interface}},
	url = {http://arxiv.org/abs/2402.10329},
	doi = {10.48550/arXiv.2402.10329},
	abstract = {We present Universal Manipulation Interface (UMI) -- a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI's versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI's hardware and software system is open-sourced at https://umi-gripper.github.io.},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Chi, Cheng and Xu, Zhenjia and Pan, Chuer and Cousineau, Eric and Burchfiel, Benjamin and Feng, Siyuan and Tedrake, Russ and Song, Shuran},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10329 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{hewing_learning-based_2020,
	title = {Learning-{Based} {Model} {Predictive} {Control}: {Toward} {Safe} {Learning} in {Control}},
	volume = {3},
	shorttitle = {Learning-{Based} {Model} {Predictive} {Control}},
	url = {https://doi.org/10.1146/annurev-control-090419-075625},
	doi = {10.1146/annurev-control-090419-075625},
	abstract = {Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties.},
	number = {1},
	urldate = {2024-02-19},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
	year = {2020},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-090419-075625},
	keywords = {adaptive control, autonomous systems, learning-based control, model predictive control, safe learning},
	pages = {269--296},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2024-02-18},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
	pages = {211--252},
}

@misc{liso_loopy-slam_2024,
	title = {Loopy-{SLAM}: {Dense} {Neural} {SLAM} with {Loop} {Closures}},
	shorttitle = {Loopy-{SLAM}},
	url = {http://arxiv.org/abs/2402.09944},
	doi = {10.48550/arXiv.2402.09944},
	abstract = {Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Liso, Lorenzo and Sandström, Erik and Yugay, Vladimir and Van Gool, Luc and Oswald, Martin R.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09944 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{teed_raft-3d_2021,
	title = {{RAFT}-{3D}: {Scene} {Flow} using {Rigid}-{Motion} {Embeddings}},
	shorttitle = {{RAFT}-{3D}},
	url = {http://arxiv.org/abs/2012.00726},
	doi = {10.48550/arXiv.2012.00726},
	abstract = {We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d {\textless} 0.05) from 34.3\% to 83.7\%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Teed, Zachary and Deng, Jia},
	month = apr,
	year = {2021},
	note = {arXiv:2012.00726 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{jiang_sense_2019,
	title = {{SENSE}: a {Shared} {Encoder} {Network} for {Scene}-flow {Estimation}},
	shorttitle = {{SENSE}},
	url = {http://arxiv.org/abs/1910.12361},
	doi = {10.48550/arXiv.1910.12361},
	abstract = {We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Jiang, Huaizu and Sun, Deqing and Jampani, Varun and Lv, Zhaoyang and Learned-Miller, Erik and Kautz, Jan},
	month = oct,
	year = {2019},
	note = {arXiv:1910.12361 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shepel_occupancy_2022,
	title = {Occupancy {Grid} {Generation} {With} {Dynamic} {Obstacle} {Segmentation} in {Stereo} {Images}},
	volume = {23},
	issn = {1558-0016},
	url = {https://ieeexplore.ieee.org/abstract/document/9652040},
	doi = {10.1109/TITS.2021.3133799},
	abstract = {The detection of dynamic and static obstacles is a key task for the navigation of autonomous ground vehicles. The article presents a new algorithm for generating an occupancy map of the surrounding space from noisy point clouds obtained from one or several stereo cameras. The camera images are segmented by the proposed deep neural network FCN-ResNet-M-OC, which combines the speed of the FCN-ResNet method and improves the quality of the model using the concept of object context representation. The paper investigates supervised approaches to network training on unbalanced samples with road scenes such as the weighted cross entropy and the Focal Loss. The occupancy map is built from point clouds with semantic labels, in which static environment and potentially dynamic obstacles are highlighted. Our solution is operational in real time and applicable on platforms with limited computing resources. The approach was tested on autonomous vehicle datasets: Semantic KITTI, KITTI-360, Mapillary Vistas and custom OpenTaganrog. The usage of semantically labeled point clouds increased the precision of obstacle detection by an average of 17\%. The performance of the entire approach on various computing platforms with Jetson Xavier, RTX3070, GPUs NVidia Tesla V100 is respectively from 10 to 15 FPS for input image resolution 1920{\textbackslash}times 1080 pixels.},
	number = {9},
	urldate = {2024-02-16},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Shepel, Ilya and Adeshkin, Vasily and Belkin, Ilya and Yudin, Dmitry A.},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Cameras, Heuristic algorithms, Image segmentation, Laser radar, Occupancy grid, Point cloud compression, Semantics, Sensors, embedded systems, neural network, point cloud, real time, semantic segmentation, stereo image, unmanned ground vehicle},
	pages = {14779--14789},
}

@inproceedings{xu_real-time_2023,
	title = {A real-time dynamic obstacle tracking and mapping system for {UAV} navigation and collision avoidance with an {RGB}-{D} camera},
	url = {https://ieeexplore.ieee.org/abstract/document/10161194},
	doi = {10.1109/ICRA48891.2023.10161194},
	abstract = {The real-time dynamic environment perception has become vital for autonomous robots in crowded spaces. Although the popular voxel-based mapping methods can efficiently represent 3D obstacles with arbitrarily complex shapes, they can hardly distinguish between static and dynamic obstacles, leading to the limited performance of obstacle avoidance. While plenty of sophisticated learning-based dynamic obstacle detection algorithms exist in autonomous driving, the quad-copter's limited computation resources cannot achieve real-time performance using those approaches. To address these issues, we propose a real-time dynamic obstacle tracking and mapping system for quadcopter obstacle avoidance using an RGB-D camera. The proposed system first utilizes a depth image with an occupancy voxel map to generate potential dynamic obstacle regions as proposals. With the obstacle region proposals, the Kalman filter and our continuity filter are applied to track each dynamic obstacle. Finally, the environment-aware trajectory prediction method is proposed based on the Markov chain using the states of tracked dynamic obstacles. We implemented the proposed system with our custom quadcopter and navigation planner. The simulation and physical experiments show that our methods can successfully track and represent obstacles in dynamic environments in real-time and safely avoid obstacles.},
	urldate = {2024-02-16},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Xu, Zhefan and Zhan, Xiaoyang and Chen, Baihan and Xiu, Yumeng and Yang, Chenhao and Shimada, Kenji},
	month = may,
	year = {2023},
	keywords = {Cameras, Navigation, Real-time systems, Robot vision systems, Shape, Three-dimensional displays, Trajectory},
	pages = {10645--10651},
}

@article{hoeller_learning_2021,
	title = {Learning a {State} {Representation} and {Navigation} in {Cluttered} and {Dynamic} {Environments}},
	volume = {6},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9385894},
	doi = {10.1109/LRA.2021.3068639},
	abstract = {In this work, we present a learning-based pipeline to realise local navigation with a quadrupedal robot in cluttered environments with static and dynamic obstacles. Given high-level navigation commands, the robot is able to safely locomote to a target location based on frames from a depth camera without any explicit mapping of the environment. First, the sequence of images and the current trajectory of the camera are fused to form a model of the world using state representation learning. The output of this lightweight module is then directly fed into a target-reaching and obstacle-avoiding policy trained with reinforcement learning. We show that decoupling the pipeline into these components results in a sample efficient policy learning stage that can be fully trained in simulation in just a dozen minutes. The key part is the state representation, which is trained to not only estimate the hidden state of the world in an unsupervised fashion, but also helps bridging the reality gap, enabling successful sim-to-real transfer. In our experiments with the quadrupedal robot ANYmal in simulation and in reality, we show that our system can handle noisy depth images, avoid dynamic obstacles unseen during training, and is endowed with local spatial awareness.},
	number = {3},
	urldate = {2024-02-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Hoeller, David and Wellhausen, Lorenz and Farshidian, Farbod and Hutter, Marco},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Collision avoidance, Navigation, Pipelines, Robot kinematics, Robots, Task analysis, Trajectory, representation learning, vision-based navigation},
	pages = {5081--5088},
}

@article{kaelbling_foundation_2020,
	title = {The foundation of efficient robot learning},
	volume = {369},
	url = {https://www.science.org/doi/10.1126/science.aaz7597},
	doi = {10.1126/science.aaz7597},
	number = {6506},
	urldate = {2024-02-15},
	journal = {Science},
	author = {Kaelbling, Leslie Pack},
	month = aug,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {915--916},
}

@misc{michaux_safe_2024,
	title = {Safe {Planning} for {Articulated} {Robots} {Using} {Reachability}-based {Obstacle} {Avoidance} {With} {Spheres}},
	url = {http://arxiv.org/abs/2402.08857},
	doi = {10.48550/arXiv.2402.08857},
	abstract = {Generating safe motion plans in real-time is necessary for the wide-scale deployment of robots in unstructured and human-centric environments. These motion plans must be safe to ensure humans are not harmed and nearby objects are not damaged. However, they must also be generated in real-time to ensure the robot can quickly adapt to changes in the environment. Many trajectory optimization methods introduce heuristics that trade-off safety and real-time performance, which can lead to potentially unsafe plans. This paper addresses this challenge by proposing Safe Planning for Articulated Robots Using Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a receding-horizon trajectory planner that utilizes the combination of a novel reachable set representation and an exact signed distance function to generate provably-safe motion plans. At runtime, SPARROWS uses parameterized trajectories to compute reachable sets composed entirely of spheres that overapproximate the swept volume of the robot's motion. SPARROWS then performs trajectory optimization to select a safe trajectory that is guaranteed to be collision-free. We demonstrate that SPARROWS' novel reachable set is significantly less conservative than previous approaches. We also demonstrate that SPARROWS outperforms a variety of state-of-the-art methods in solving challenging motion planning tasks in cluttered environments. Code, data, and video demonstrations can be found at {\textbackslash}url\{https://roahmlab.github.io/sparrows/\}.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Michaux, Jonathan and Li, Adam and Chen, Qingyi and Chen, Che and Zhang, Bohao and Vasudevan, Ram},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08857 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{mahjourian_occupancy_2022,
	title = {Occupancy {Flow} {Fields} for {Motion} {Forecasting} in {Autonomous} {Driving}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3151613},
	abstract = {We propose Occupancy Flow Fields, a new representation for motion forecasting of multiple agents, an important task in autonomous driving.Our representation is a spatio-temporal grid with each grid cell containing both the probability of the cell being occupied by any agent, and a two-dimensional flow vector representing the direction and magnitude of the motion in that cell. Our method successfully mitigates shortcomings of the two most commonly-used representations for motion forecasting: trajectory sets and occupancy grids. Although occupancy grids efficiently represent the probabilistic location of many agents jointly, they do not capture agent motion and lose the agent identities. To this end, we propose a deep learning architecture that generates Occupancy Flow Fields with the help of a new flow trace loss that establishes consistency between the occupancy and flow predictions. We demonstrate the effectiveness of our approach using three metrics on occupancy prediction, motion estimation, and agent ID recovery. In addition, we introduce the problem of predicting speculative agents, which are currently-occluded agents that may appear in the future through dis-occlusion or by entering the field of view. We report experimental results on a large in-house autonomous driving dataset and the public INTERACTION dataset, and show that our model outperforms state-of-the-art models.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Mahjourian, Reza and Kim, Jinkyu and Chai, Yuning and Tan, Mingxing and Sapp, Ben and Anguelov, Dragomir},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Autonomous vehicle navigation, Autonomous vehicles, Computer Science - Machine Learning, Computer Science - Robotics, Computer architecture, Forecasting, Microprocessors, Predictive models, Roads, Trajectory, agent-based systems, deep learning methods},
	pages = {5639--5646},
}

@inproceedings{xu_attention_2022,
	title = {Attention {Concatenation} {Volume} for {Accurate} and {Efficient} {Stereo} {Matching}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Attention_Concatenation_Volume_for_Accurate_and_Efficient_Stereo_Matching_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-11},
	author = {Xu, Gangwei and Cheng, Junda and Guo, Peng and Yang, Xin},
	year = {2022},
	pages = {12981--12990},
}

@article{liu_local_2022,
	title = {Local {Similarity} {Pattern} and {Cost} {Self}-{Reassembling} for {Deep} {Stereo} {Matching} {Networks}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20056},
	doi = {10.1609/aaai.v36i2.20056},
	abstract = {Although convolutional neural network based stereo matching architectures have made impressive achievements, there are still some limitations: 1) Convolutional Feature (CF) tends to capture appearance information, which is inadequate for accurate matching. 2) Due to the static filters, current convolution based disparity refinement modules often produce over-smooth results. In this paper, we present two schemes to address these issues, where some traditional wisdoms are integrated. Firstly, we introduce a pairwise feature for deep stereo matching networks, named LSP (Local Similarity Pattern). Through explicitly revealing the neighbor relationships, LSP contains rich structural information, which can be leveraged to aid CF for more discriminative feature description. Secondly, we design a dynamic self-reassembling refinement strategy and apply it to the cost distribution and the disparity map respectively. The former could be equipped with the unimodal distribution constraint to alleviate the over-smoothing problem, and the latter is more practical. The effectiveness of the proposed methods is demonstrated via incorporating them into two well-known basic architectures, GwcNet and GANet-deep. Experimental results on the SceneFlow and KITTI benchmarks show that our modules significantly improve the performance of the model. Code is available at https://github.com/SpadeLiu/Lac-GwcNet.},
	language = {en},
	number = {2},
	urldate = {2023-01-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Biyang and Yu, Huimin and Long, Yangqi},
	month = jun,
	year = {2022},
	note = {Number: 2},
	keywords = {Computer Vision (CV)},
	pages = {1647--1655},
}

@inproceedings{shen_cfnet_2021,
	title = {{CFNet}: {Cascade} and {Fused} {Cost} {Volume} for {Robust} {Stereo} {Matching}},
	shorttitle = {{CFNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Shen_CFNet_Cascade_and_Fused_Cost_Volume_for_Robust_Stereo_Matching_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-02-15},
	author = {Shen, Zhelun and Dai, Yuchao and Rao, Zhibo},
	year = {2021},
	pages = {13906--13915},
}

@inproceedings{shamsafar_mobilestereonet_2022,
	title = {{MobileStereoNet}: {Towards} {Lightweight} {Deep} {Networks} for {Stereo} {Matching}},
	shorttitle = {{MobileStereoNet}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Shamsafar_MobileStereoNet_Towards_Lightweight_Deep_Networks_for_Stereo_Matching_WACV_2022_paper.html},
	language = {en},
	urldate = {2024-02-15},
	author = {Shamsafar, Faranak and Woerz, Samuel and Rahim, Rafia and Zell, Andreas},
	year = {2022},
	pages = {2417--2426},
}

@inproceedings{xie_pixel-aligned_2023,
	title = {Pixel-{Aligned} {Recurrent} {Queries} for {Multi}-{View} {3D} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Pixel-Aligned_Recurrent_Queries_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-02-15},
	author = {Xie, Yiming and Jiang, Huaizu and Gkioxari, Georgia and Straub, Julian},
	year = {2023},
	pages = {18370--18380},
}

@misc{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1402.1128},
	doi = {10.48550/arXiv.1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	month = feb,
	year = {2014},
	note = {arXiv:1402.1128 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{pumacay_colosseum_2024,
	title = {{THE} {COLOSSEUM}: {A} {Benchmark} for {Evaluating} {Generalization} for {Robotic} {Manipulation}},
	shorttitle = {{THE} {COLOSSEUM}},
	url = {http://arxiv.org/abs/2402.08191},
	doi = {10.48550/arXiv.2402.08191},
	abstract = {To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50\% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades \${\textbackslash}geq\$75\%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated (\${\textbackslash}bar\{R\}{\textasciicircum}2 = 0.614\$) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation. See https://robot-colosseum.github.io/ for more details.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Pumacay, Wilbert and Singh, Ishika and Duan, Jiafei and Krishna, Ranjay and Thomason, Jesse and Fox, Dieter},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08191 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{huang_tri-perspective_2023,
	title = {Tri-{Perspective} {View} for {Vision}-{Based} {3D} {Semantic} {Occupancy} {Prediction}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-02-13},
	author = {Huang, Yuanhui and Zheng, Wenzhao and Zhang, Yunpeng and Zhou, Jie and Lu, Jiwen},
	year = {2023},
	pages = {9223--9232},
}

@inproceedings{wei_surroundocc_2023,
	title = {{SurroundOcc}: {Multi}-camera {3D} {Occupancy} {Prediction} for {Autonomous} {Driving}},
	shorttitle = {{SurroundOcc}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-02-13},
	author = {Wei, Yi and Zhao, Linqing and Zheng, Wenzhao and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
	year = {2023},
	pages = {21729--21740},
}

@inproceedings{li_voxformer_2023,
	title = {{VoxFormer}: {Sparse} {Voxel} {Transformer} for {Camera}-{Based} {3D} {Semantic} {Scene} {Completion}},
	shorttitle = {{VoxFormer}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-02-13},
	author = {Li, Yiming and Yu, Zhiding and Choy, Christopher and Xiao, Chaowei and Alvarez, Jose M. and Fidler, Sanja and Feng, Chen and Anandkumar, Anima},
	year = {2023},
	pages = {9087--9098},
}

@inproceedings{li_behavior-1k_2023,
	title = {{BEHAVIOR}-{1K}: {A} {Benchmark} for {Embodied} {AI} with 1,000 {Everyday} {Activities} and {Realistic} {Simulation}},
	shorttitle = {{BEHAVIOR}-{1K}},
	url = {https://proceedings.mlr.press/v205/li23a.html},
	abstract = {We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 5,000 objects annotated with rich physical and semantic properties. The second is OmniGibson, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K’s human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.},
	language = {en},
	urldate = {2024-02-13},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and Martín-Martín, Roberto and Wang, Chen and Levine, Gabrael and Lingelbach, Michael and Sun, Jiankai and Anvari, Mona and Hwang, Minjune and Sharma, Manasi and Aydin, Arman and Bansal, Dhruva and Hunter, Samuel and Kim, Kyu-Young and Lou, Alan and Matthews, Caleb R. and Villa-Renteria, Ivan and Tang, Jerry Huayang and Tang, Claire and Xia, Fei and Savarese, Silvio and Gweon, Hyowon and Liu, Karen and Wu, Jiajun and Fei-Fei, Li},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {80--93},
}

@misc{wang_navformer_2024,
	title = {{NavFormer}: {A} {Transformer} {Architecture} for {Robot} {Target}-{Driven} {Navigation} in {Unknown} and {Dynamic} {Environments}},
	shorttitle = {{NavFormer}},
	url = {http://arxiv.org/abs/2402.06838},
	abstract = {In unknown cluttered and dynamic environments such as disaster scenes, mobile robots need to perform target-driven navigation in order to find people or objects of interest, while being solely guided by images of the targets. In this paper, we introduce NavFormer, a novel end-to-end transformer architecture developed for robot target-driven navigation in unknown and dynamic environments. NavFormer leverages the strengths of both 1) transformers for sequential data processing and 2) self-supervised learning (SSL) for visual representation to reason about spatial layouts and to perform collision-avoidance in dynamic settings. The architecture uniquely combines dual-visual encoders consisting of a static encoder for extracting invariant environment features for spatial reasoning, and a general encoder for dynamic obstacle avoidance. The primary robot navigation task is decomposed into two sub-tasks for training: single robot exploration and multi-robot collision avoidance. We perform cross-task training to enable the transfer of learned skills to the complex primary navigation task without the need for task-specific fine-tuning. Simulated experiments demonstrate that NavFormer can effectively navigate a mobile robot in diverse unknown environments, outperforming existing state-of-the-art methods in terms of success rate and success weighted by (normalized inverse) path length. Furthermore, a comprehensive ablation study is performed to evaluate the impact of the main design choices of the structure and training of NavFormer, further validating their effectiveness in the overall system.},
	urldate = {2024-02-13},
	publisher = {arXiv},
	author = {Wang, Haitong and Tan, Aaron Hao and Nejat, Goldie},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06838 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{ma_transformer_2024,
	title = {Do {Transformer} {World} {Models} {Give} {Better} {Policy} {Gradients}?},
	url = {http://arxiv.org/abs/2402.05290},
	doi = {10.48550/arXiv.2402.05290},
	abstract = {A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimization landscapes that are easier to navigate even when compared to those from the simulator itself. This property allows transformer AWMs to produce better policies than competitive baselines in realistic long-horizon tasks.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Ma, Michel and Ni, Tianwei and Gehring, Clement and D'Oro, Pierluca and Bacon, Pierre-Luc},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zhang_ncrf_2024,
	title = {{NCRF}: {Neural} {Contact} {Radiance} {Fields} for {Free}-{Viewpoint} {Rendering} of {Hand}-{Object} {Interaction}},
	shorttitle = {{NCRF}},
	url = {http://arxiv.org/abs/2402.05532},
	doi = {10.48550/arXiv.2402.05532},
	abstract = {Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Zhang, Zhongqun and Song, Jifei and Pérez-Pellitero, Eduardo and Zhou, Yiren and Chang, Hyung Jin and Leonardis, Aleš},
	month = feb,
	year = {2024},
	note = {arXiv:2402.05532 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yang_depth_2024,
	title = {Depth {Anything}: {Unleashing} the {Power} of {Large}-{Scale} {Unlabeled} {Data}},
	shorttitle = {Depth {Anything}},
	url = {http://arxiv.org/abs/2401.10891},
	doi = {10.48550/arXiv.2401.10891},
	abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data ({\textasciitilde}62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10891 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{venuto_code_2024,
	title = {Code as {Reward}: {Empowering} {Reinforcement} {Learning} with {VLMs}},
	shorttitle = {Code as {Reward}},
	url = {http://arxiv.org/abs/2402.04764},
	doi = {10.48550/arXiv.2402.04764},
	abstract = {Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Venuto, David and Islam, Sami Nur and Klissarov, Martin and Precup, Doina and Yang, Sherry and Anand, Ankit},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04764 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{honig_star_2024,
	title = {{STAR}: {Shape}-focused {Texture} {Agnostic} {Representations} for {Improved} {Object} {Detection} and {6D} {Pose} {Estimation}},
	shorttitle = {{STAR}},
	url = {http://arxiv.org/abs/2402.04878},
	doi = {10.48550/arXiv.2402.04878},
	abstract = {Recent advances in machine learning have greatly benefited object detection and 6D pose estimation for robotic grasping. However, textureless and metallic objects still pose a significant challenge due to fewer visual cues and the texture bias of CNNs. To address this issue, we propose a texture-agnostic approach that focuses on learning from CAD models and emphasizes object shape features. To achieve a focus on learning shape features, the textures are randomized during the rendering of the training data. By treating the texture as noise, the need for real-world object instances or their final appearance during training data generation is eliminated. The TLESS and ITODD datasets, specifically created for industrial settings in robotics and featuring textureless and metallic objects, were used for evaluation. Texture agnosticity also increases the robustness against image perturbations such as imaging noise, motion blur, and brightness changes, which are common in robotics applications. Code and datasets are publicly available at github.com/hoenigpeter/randomized\_texturing.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Hönig, Peter and Thalhammer, Stefan and Weibel, Jean-Baptiste and Hirschmanner, Matthias and Vincze, Markus},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04878 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lakshmipathy_kinematic_2024,
	title = {Kinematic {Motion} {Retargeting} for {Contact}-{Rich} {Anthropomorphic} {Manipulations}},
	url = {http://arxiv.org/abs/2402.04820},
	doi = {10.48550/arXiv.2402.04820},
	abstract = {Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers. We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas. We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK). Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects. We additionally compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Lakshmipathy, Arjun S. and Hodgins, Jessica K. and Pollard, Nancy S.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04820 [cs]},
	keywords = {Computer Science - Graphics, Computer Science - Robotics},
}

@misc{xu_tactile-based_2024,
	title = {Tactile-based {Object} {Retrieval} {From} {Granular} {Media}},
	url = {http://arxiv.org/abs/2402.04536},
	abstract = {We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing. Videos and additional information can be found at https://jxu.ai/geotact.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Xu, Jingxi and Jia, Yinsen and Yang, Dongxiao and Meng, Patrick and Zhu, Xinyue and Guo, Zihan and Song, Shuran and Ciocarlie, Matei},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ding_diffusion_2024,
	title = {Diffusion {World} {Model}},
	url = {http://arxiv.org/abs/2402.03570},
	abstract = {We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a \$44{\textbackslash}\%\$ performance gain, and achieves state-of-the-art performance.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Ding, Zihan and Zhang, Amy and Tian, Yuandong and Zheng, Qinqing},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03570 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{luo_perpetual_2023,
	title = {Perpetual {Humanoid} {Control} for {Real}-time {Simulated} {Avatars}},
	shorttitle = {{PHC}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-02-06},
	author = {Luo, Zhengyi and Cao, Jinkun and AlexanderWinkler and Kitani, Kris and Xu, Weipeng},
	year = {2023},
	pages = {10895--10904},
}

@misc{lin_motion-x_2024,
	title = {Motion-{X}: {A} {Large}-scale {3D} {Expressive} {Whole}-body {Human} {Motion} {Dataset}},
	shorttitle = {Motion-{X}},
	url = {http://arxiv.org/abs/2307.00818},
	doi = {10.48550/arXiv.2307.00818},
	abstract = {In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which comprises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose descriptions and 81.1K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Lin, Jing and Zeng, Ailing and Lu, Shunlin and Cai, Yuanhao and Zhang, Ruimao and Wang, Haoqian and Zhang, Lei},
	month = jan,
	year = {2024},
	note = {arXiv:2307.00818 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhu_point_2024,
	title = {Point {Cloud} {Matters}: {Rethinking} the {Impact} of {Different} {Observation} {Spaces} on {Robot} {Learning}},
	shorttitle = {Point {Cloud} {Matters}},
	url = {http://arxiv.org/abs/2402.02500},
	abstract = {In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help design more generalizable and robust robotic models.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Zhu, Haoyi and Wang, Yating and Huang, Di and Ye, Weicai and Ouyang, Wanli and He, Tong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02500 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{weng_dexdiffuser_2024,
	title = {{DexDiffuser}: {Generating} {Dexterous} {Grasps} with {Diffusion} {Models}},
	shorttitle = {{DexDiffuser}},
	url = {http://arxiv.org/abs/2402.02989},
	doi = {10.48550/arXiv.2402.02989},
	abstract = {We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20{\textbackslash}\% higher grasp success rate.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Weng, Zehang and Lu, Haofei and Kragic, Danica and Lundell, Jens},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02989 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_poco_2024,
	title = {{PoCo}: {Policy} {Composition} from and for {Heterogeneous} {Robot} {Learning}},
	shorttitle = {{PoCo}},
	url = {http://arxiv.org/abs/2402.02511},
	abstract = {Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments. See https://liruiw.github.io/policycomp for more details .},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Wang, Lirui and Zhao, Jialiang and Du, Yilun and Adelson, Edward H. and Tedrake, Russ},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02511 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lee_echowrist_2024,
	title = {{EchoWrist}: {Continuous} {Hand} {Pose} {Tracking} and {Hand}-{Object} {Interaction} {Recognition} {Using} {Low}-{Power} {Active} {Acoustic} {Sensing} {On} a {Wristband}},
	shorttitle = {{EchoWrist}},
	url = {http://arxiv.org/abs/2401.17409},
	doi = {10.48550/arXiv.2401.17409},
	abstract = {Our hands serve as a fundamental means of interaction with the world around us. Therefore, understanding hand poses and interaction context is critical for human-computer interaction. We present EchoWrist, a low-power wristband that continuously estimates 3D hand pose and recognizes hand-object interactions using active acoustic sensing. EchoWrist is equipped with two speakers emitting inaudible sound waves toward the hand. These sound waves interact with the hand and its surroundings through reflections and diffractions, carrying rich information about the hand's shape and the objects it interacts with. The information captured by the two microphones goes through a deep learning inference system that recovers hand poses and identifies various everyday hand activities. Results from the two 12-participant user studies show that EchoWrist is effective and efficient at tracking 3D hand poses and recognizing hand-object interactions. Operating at 57.9mW, EchoWrist is able to continuously reconstruct 20 3D hand joints with MJEDE of 4.81mm and recognize 12 naturalistic hand-object interactions with 97.6\% accuracy.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Lee, Chi-Jung and Zhang, Ruidong and Agarwal, Devansh and Yu, Tianhong Catherine and Gunda, Vipin and Lopez, Oliver and Kim, James and Yin, Sicheng and Deng, Boao and Li, Ke and Sakashita, Mose and Guimbretiere, Francois and Zhang, Cheng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.17409 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{wang_coplanner_2023,
	title = {{COPlanner}: {Plan} to {Roll} {Out} {Conservatively} but to {Explore} {Optimistically} for {Model}-{Based} {RL}},
	shorttitle = {{COPlanner}},
	url = {http://arxiv.org/abs/2310.07220},
	doi = {10.48550/arXiv.2310.07220},
	abstract = {Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose \${\textbackslash}texttt\{COPlanner\}\$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. \${\textbackslash}texttt\{COPlanner\}\$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, \${\textbackslash}texttt\{COPlanner\}\$ can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. \${\textbackslash}texttt\{COPlanner\}\$ is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with \${\textbackslash}texttt\{COPlanner\}\$.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Wang, Xiyao and Zheng, Ruijie and Sun, Yanchao and Jia, Ruonan and Wongkamjan, Wichayaporn and Xu, Huazhe and Huang, Furong},
	month = dec,
	year = {2023},
	note = {arXiv:2310.07220 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{yang_binding_2024,
	title = {Binding {Touch} to {Everything}: {Learning} {Unified} {Multimodal} {Tactile} {Representations}},
	shorttitle = {Binding {Touch} to {Everything}},
	url = {http://arxiv.org/abs/2401.18084},
	abstract = {The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Yang, Fengyu and Feng, Chao and Chen, Ziyang and Park, Hyoungseob and Wang, Daniel and Dou, Yiming and Zeng, Ziyao and Chen, Xien and Gangopadhyay, Rit and Owens, Andrew and Wong, Alex},
	month = jan,
	year = {2024},
	note = {arXiv:2401.18084 null},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{gu_contactgen_2024,
	title = {{ContactGen}: {Contact}-{Guided} {Interactive} {3D} {Human} {Generation} for {Partners}},
	shorttitle = {{ContactGen}},
	url = {http://arxiv.org/abs/2401.17212},
	abstract = {Among various interactions between humans, such as eye contact and gestures, physical interactions by contact can act as an essential moment in understanding human behaviors. Inspired by this fact, given a 3D partner human with the desired interaction label, we introduce a new task of 3D human generation in terms of physical contact. Unlike previous works of interacting with static objects or scenes, a given partner human can have diverse poses and different contact regions according to the type of interaction. To handle this challenge, we propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework. Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label. Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model. We demonstrate ContactGen on the CHI3D dataset, where our method generates physically plausible and diverse poses compared to comparison methods.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Gu, Dongjun and Shim, Jaehyeok and Jang, Jaehoon and Kang, Changwoo and Joo, Kyungdon},
	month = jan,
	year = {2024},
	note = {arXiv:2401.17212 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_reinforcement_2024,
	title = {Reinforcement {Learning} for {Versatile}, {Dynamic}, and {Robust} {Bipedal} {Locomotion} {Control}},
	url = {http://arxiv.org/abs/2401.16889},
	abstract = {This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world.The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16889 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{arolovitch_kinesthetic-based_2024,
	title = {Kinesthetic-based {In}-{Hand} {Object} {Recognition} with an {Underactuated} {Robotic} {Hand}},
	url = {http://arxiv.org/abs/2401.16802},
	abstract = {Tendon-based underactuated hands are intended to be simple, compliant and affordable. Often, they are 3D printed and do not include tactile sensors. Hence, performing in-hand object recognition with direct touch sensing is not feasible. Adding tactile sensors can complicate the hardware and introduce extra costs to the robotic hand. Also, the common approach of visual perception may not be available due to occlusions. In this paper, we explore whether kinesthetic haptics can provide in-direct information regarding the geometry of a grasped object during in-hand manipulation with an underactuated hand. By solely sensing actuator positions and torques over a period of time during motion, we show that a classifier can recognize an object from a set of trained ones with a high success rate of almost 95\%. In addition, the implementation of a real-time majority vote during manipulation further improves recognition. Additionally, a trained classifier is also shown to be successful in distinguishing between shape categories rather than just specific objects.},
	urldate = {2024-01-31},
	publisher = {arXiv},
	author = {Arolovitch, Julius and Azulay, Osher and Sintov, Avishai},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16802 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{shuai_reconstructing_2023,
	title = {Reconstructing {Close} {Human} {Interactions} from {Multiple} {Views}},
	volume = {42},
	issn = {0730-0301, 1557-7368},
	url = {http://arxiv.org/abs/2401.16173},
	doi = {10.1145/3618336},
	abstract = {This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.},
	number = {6},
	urldate = {2024-01-30},
	journal = {ACM Transactions on Graphics},
	author = {Shuai, Qing and Yu, Zhiyuan and Zhou, Zhize and Fan, Lixin and Yang, Haijun and Yang, Can and Zhou, Xiaowei},
	month = dec,
	year = {2023},
	note = {arXiv:2401.16173 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--14},
}

@misc{shah_mv2mae_2024,
	title = {{MV2MAE}: {Multi}-{View} {Video} {Masked} {Autoencoders}},
	shorttitle = {{MV2MAE}},
	url = {http://arxiv.org/abs/2401.15900},
	abstract = {Videos captured from multiple viewpoints can help in perceiving the 3D structure of the world and benefit computer vision tasks such as action recognition, tracking, etc. In this paper, we present a method for self-supervised learning from synchronized multi-view videos. We use a cross-view reconstruction task to inject geometry information in the model. Our approach is based on the masked autoencoder (MAE) framework. In addition to the same-view decoder, we introduce a separate cross-view decoder which leverages cross-attention mechanism to reconstruct a target viewpoint video using a video from source viewpoint, to help representations robust to viewpoint changes. For videos, static regions can be reconstructed trivially which hinders learning meaningful representations. To tackle this, we introduce a motion-weighted reconstruction loss which improves temporal modeling. We report state-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well as in the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets, demonstrating the robustness of our approach. Code will be made available.},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Shah, Ketul and Crandall, Robert and Xu, Jie and Zhou, Peng and George, Marian and Bansal, Mayank and Chellappa, Rama},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15900 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{khandate_rtimesr_2024,
	title = {R\${\textbackslash}times\${R}: {Rapid} {eXploration} for {Reinforcement} {Learning} via {Sampling}-based {Reset} {Distributions} and {Imitation} {Pre}-training},
	shorttitle = {R\${\textbackslash}times\${R}},
	url = {http://arxiv.org/abs/2401.15484},
	abstract = {We present a method for enabling Reinforcement Learning of motor control policies for complex skills such as dexterous manipulation. We posit that a key difficulty for training such policies is the difficulty of exploring the problem state space, as the accessible and useful regions of this space form a complex structure along manifolds of the original high-dimensional state space. This work presents a method to enable and support exploration with Sampling-based Planning. We use a generally applicable non-holonomic Rapidly-exploring Random Trees algorithm and present multiple methods to use the resulting structure to bootstrap model-free Reinforcement Learning. Our method is effective at learning various challenging dexterous motor control skills of higher difficulty than previously shown. In particular, we achieve dexterous in-hand manipulation of complex objects while simultaneously securing the object without the use of passive support surfaces. These policies also transfer effectively to real robots. A number of example videos can also be found on the project website: https://sbrl.cs.columbia.edu},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Khandate, Gagan and Saidi, Tristan L. and Shang, Siqi and Chang, Eric T. and Liu, Yang and Dennis, Seth and Adams, Johnson and Ciocarlie, Matei},
	month = jan,
	year = {2024},
	note = {arXiv:2401.15484 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{ceola_resprect_2024,
	title = {{RESPRECT}: {Speeding}-up {Multi}-fingered {Grasping} with {Residual} {Reinforcement} {Learning}},
	shorttitle = {{RESPRECT}},
	url = {http://arxiv.org/abs/2401.14858},
	abstract = {Deep Reinforcement Learning (DRL) has proven effective in learning control policies using robotic grippers, but much less practical for solving the problem of grasping with dexterous hands -- especially on real robotic platforms -- due to the high dimensionality of the problem. In this work, we focus on the multi-fingered grasping task with the anthropomorphic hand of the iCub humanoid. We propose the RESidual learning with PREtrained CriTics (RESPRECT) method that, starting from a policy pre-trained on a large set of objects, can learn a residual policy to grasp a novel object in a fraction (\${\textbackslash}sim 5 {\textbackslash}times\$ faster) of the timesteps required to train a policy from scratch, without requiring any task demonstration. To our knowledge, this is the first Residual Reinforcement Learning (RRL) approach that learns a residual policy on top of another policy pre-trained with DRL. We exploit some components of the pre-trained policy during residual learning that further speed-up the training. We benchmark our results in the iCub simulated environment, and we show that RESPRECT can be effectively used to learn a multi-fingered grasping policy on the real iCub robot. The code to reproduce the experiments is released together with the paper with an open source license.},
	urldate = {2024-01-29},
	publisher = {arXiv},
	author = {Ceola, Federico and Rosasco, Lorenzo and Natale, Lorenzo},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14858 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{feng_finetuning_2023,
	title = {Finetuning {Offline} {World} {Models} in the {Real} {World}},
	url = {http://arxiv.org/abs/2310.16029},
	doi = {10.48550/arXiv.2310.16029},
	abstract = {Reinforcement Learning (RL) is notoriously data-inefficient, which makes training on a real robot difficult. While model-based RL algorithms (world models) improve data-efficiency to some extent, they still require hours or days of interaction to learn skills. Recently, offline RL has been proposed as a framework for training RL policies on pre-existing datasets without any online interaction. However, constraining an algorithm to a fixed dataset induces a state-action distribution shift between training and inference, and limits its applicability to new tasks. In this work, we seek to get the best of both worlds: we consider the problem of pretraining a world model with offline data collected on a real robot, and then finetuning the model on online data collected by planning with the learned model. To mitigate extrapolation errors during online interaction, we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty. We evaluate our method on a variety of visuo-motor control tasks in simulation and on a real robot, and find that our method enables few-shot finetuning to seen and unseen tasks even when offline data is limited. Videos, code, and data are available at https://yunhaifeng.com/FOWM .},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Feng, Yunhai and Hansen, Nicklas and Xiong, Ziyan and Rajagopalan, Chandramouli and Wang, Xiaolong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hansen_td-mpc2_2023,
	title = {{TD}-{MPC2}: {Scalable}, {Robust} {World} {Models} for {Continuous} {Control}},
	shorttitle = {{TD}-{MPC2}},
	url = {http://arxiv.org/abs/2310.16828},
	doi = {10.48550/arXiv.2310.16828},
	abstract = {TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://nicklashansen.github.io/td-mpc2},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16828 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wang_building_2024,
	title = {Building {Minimal} and {Reusable} {Causal} {State} {Abstractions} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2401.12497},
	abstract = {Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on manipulation environments and Deepmind Control Suite reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstractions allow a task learner to achieve near-oracle levels of sample efficiency and outperform baselines on all tasks.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Wang, Zizhao and Wang, Caroline and Xiao, Xuesu and Zhu, Yuke and Stone, Peter},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12497 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, I.2.6, I.2.8, I.2.9},
}

@misc{bar-tal_lumiere_2024,
	title = {Lumiere: {A} {Space}-{Time} {Diffusion} {Model} for {Video} {Generation}},
	shorttitle = {Lumiere},
	url = {http://arxiv.org/abs/2401.12945},
	abstract = {We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Li, Yuanzhen and Michaeli, Tomer and Wang, Oliver and Sun, Deqing and Dekel, Tali and Mosseri, Inbar},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12945 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_dextouch_2024,
	title = {{DexTouch}: {Learning} to {Seek} and {Manipulate} {Objects} with {Tactile} {Dexterity}},
	shorttitle = {{DexTouch}},
	url = {http://arxiv.org/abs/2401.12496},
	abstract = {The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. Extensive research has been conducted over time to apply these human tactile abilities to robots. In this paper, we introduce a multi-finger robot system designed to search for and manipulate objects using the sense of touch without relying on visual information. Randomly located target objects are searched using tactile sensors, and the objects are manipulated for tasks that mimic daily-life. The objective of the study is to endow robots with human-like tactile capabilities. To achieve this, binary tactile sensors are implemented on one side of the robot hand to minimize the Sim2Real gap. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment, we demonstrate that object search and manipulation using tactile sensors is possible even in an environment without vision information. In addition, an ablation study was conducted to analyze the effect of tactile information on manipulative tasks. Our project page is available at https://lee-kangwon.github.io/dextouch/},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Lee, Kang-Won and Qin, Yuzhe and Wang, Xiaolong and Lim, Soo-Chul},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12496 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xia_rgbd_2024,
	title = {{RGBD} {Objects} in the {Wild}: {Scaling} {Real}-{World} {3D} {Object} {Learning} from {RGB}-{D} {Videos}},
	shorttitle = {{RGBD} {Objects} in the {Wild}},
	url = {http://arxiv.org/abs/2401.12592},
	abstract = {We introduce a new RGB-D object dataset captured in the wild called WildRGB-D. Unlike most existing real-world object-centric datasets which only come with RGB capturing, the direct capture of the depth channel allows better 3D annotations and broader downstream applications. WildRGB-D comprises large-scale category-level RGB-D object videos, which are taken using an iPhone to go around the objects in 360 degrees. It contains around 8500 recorded objects and nearly 20000 RGB-D videos across 46 common object categories. These videos are taken with diverse cluttered backgrounds with three setups to cover as many real-world scenarios as possible: (i) a single object in one video; (ii) multiple objects in one video; and (iii) an object with a static hand in one video. The dataset is annotated with object masks, real-world scale camera poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark four tasks with WildRGB-D including novel view synthesis, camera pose estimation, object 6d pose estimation, and object surface reconstruction. Our experiments show that the large-scale capture of RGB-D objects provides a large potential to advance 3D object learning. Our project page is https://wildrgbd.github.io/.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Xia, Hongchi and Fu, Yang and Liu, Sifei and Wang, Xiaolong},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12592 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hui_make--shape_2024,
	title = {Make-{A}-{Shape}: a {Ten}-{Million}-scale {3D} {Shape} {Model}},
	shorttitle = {Make-{A}-{Shape}},
	url = {http://arxiv.org/abs/2401.11067},
	abstract = {Significant progress has been made in training large generative models for natural language and images. Yet, the advancement of 3D generative models is hindered by their substantial resource demands for training, along with inefficient, non-compact, and less expressive representations. This paper introduces Make-A-Shape, a new 3D generative model designed for efficient training on a vast scale, capable of utilizing 10 millions publicly-available shapes. Technical-wise, we first innovate a wavelet-tree representation to compactly encode shapes by formulating the subband coefficient filtering scheme to efficiently exploit coefficient relations. We then make the representation generatable by a diffusion model by devising the subband coefficients packing scheme to layout the representation in a low-resolution grid. Further, we derive the subband adaptive training strategy to train our model to effectively learn to generate coarse and detail wavelet coefficients. Last, we extend our framework to be controlled by additional input conditions to enable it to generate shapes from assorted modalities, e.g., single/multi-view images, point clouds, and low-resolution voxels. In our extensive set of experiments, we demonstrate various applications, such as unconditional generation, shape completion, and conditional generation on a wide range of modalities. Our approach not only surpasses the state of the art in delivering high-quality results but also efficiently generates shapes within a few seconds, often achieving this in just 2 seconds for most conditions.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Hui, Ka-Hei and Sanghi, Aditya and Rampini, Arianna and Malekshan, Kamal Rahimi and Liu, Zhengzhe and Shayani, Hooman and Fu, Chi-Wing},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11067 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{dave_multimodal_2024,
	title = {Multimodal {Visual}-{Tactile} {Representation} {Learning} through {Self}-{Supervised} {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2401.12024},
	abstract = {The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Dave, Vedant and Lygerakis, Fotios and Rueckert, Elmar},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12024 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_bimanual_2024,
	title = {Bimanual {Deformable} {Bag} {Manipulation} {Using} a {Structure}-of-{Interest} {Based} {Latent} {Dynamics} {Model}},
	url = {http://arxiv.org/abs/2401.11432},
	abstract = {The manipulation of deformable objects by robotic systems presents a significant challenge due to their complex and infinite-dimensional configuration spaces. This paper introduces a novel approach to Deformable Object Manipulation (DOM) by emphasizing the identification and manipulation of Structures of Interest (SOIs) in deformable fabric bags. We propose a bimanual manipulation framework that leverages a Graph Neural Network (GNN)-based latent dynamics model to succinctly represent and predict the behavior of these SOIs. Our approach involves constructing a graph representation from partial point cloud data of the object and learning the latent dynamics model that effectively captures the essential deformations of the fabric bag within a reduced computational space. By integrating this latent dynamics model with Model Predictive Control (MPC), we empower robotic manipulators to perform precise and stable manipulation tasks focused on the SOIs. We have validated our framework through various empirical experiments demonstrating its efficacy in bimanual manipulation of fabric bags. Our contributions not only address the complexities inherent in DOM but also provide new perspectives and methodologies for enhancing robotic interactions with deformable objects by concentrating on their critical structural elements. Experimental videos can be obtained from https://sites.google.com/view/bagbot.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Zhou, Peng and Zheng, Pai and Qi, Jiaming and Li, Chenxi and Yang, Chenguang and Navarro-Alarcon, David and Pan, Jia},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11432 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yuan_general_2024,
	title = {General {Flow} as {Foundation} {Affordance} for {Scalable} {Robot} {Learning}},
	url = {http://arxiv.org/abs/2401.11439},
	abstract = {We address the challenge of acquiring real-world manipulation skills with a scalable framework.Inspired by the success of large-scale auto-regressive prediction in Large Language Models (LLMs), we hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target in robot learning. To exploit scalable data resources, we turn our attention to cross-embodiment datasets. We develop, for the first time, a language-conditioned prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable geometric and physics guidance, thus facilitating stable zero-shot skill transfer in real-world scenarios.We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any additional training, our method achieves an impressive 81\% success rate in human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) universality: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. These lead to a new pathway towards scalable general robot learning. Data, code, and model weights will be made publicly available.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Yuan, Chengbo and Wen, Chuan and Zhang, Tong and Gao, Yang},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11439 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{nie_towards_2024,
	title = {Towards {Category} {Unification} of {3D} {Single} {Object} {Tracking} on {Point} {Clouds}},
	url = {http://arxiv.org/abs/2401.11204},
	abstract = {Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We find that the attribute variances of point cloud objects primarily occur from the varying size and shape (e.g., large and square vehicles v.s. small and slender humans). Based on this observation, we design a novel point set representation learning network inheriting transformer architecture, termed AdaFormer, which adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. We further incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models SiamCUT and MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin (e.g., on KITTI dataset, 12\% and 3\% performance gains on the Siamese and motion paradigms). Our code will be available.},
	urldate = {2024-01-23},
	publisher = {arXiv},
	author = {Nie, Jiahao and He, Zhiwei and Lv, Xudong and Zhou, Xueyi and Chae, Dong-Kyu and Xie, Fei},
	month = jan,
	year = {2024},
	note = {arXiv:2401.11204 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{liu_strajnet_2023,
	title = {{STrajNet}: {Multi}-modal {Hierarchical} {Transformer} for {Occupancy} {Flow} {Field} {Prediction} in {Autonomous} {Driving}},
	shorttitle = {{STrajNet}},
	url = {http://arxiv.org/abs/2208.00394},
	doi = {10.1109/ICRA48891.2023.10160855},
	abstract = {Forecasting the future states of surrounding traffic participants is a crucial capability for autonomous vehicles. The recently proposed occupancy flow field prediction introduces a scalable and effective representation to jointly predict surrounding agents' future motions in a scene. However, the challenging part is to model the underlying social interactions among traffic agents and the relations between occupancy and flow. Therefore, this paper proposes a novel Multi-modal Hierarchical Transformer network that fuses the vectorized (agent motion) and visual (scene flow, map, and occupancy) modalities and jointly predicts the flow and occupancy of the scene. Specifically, visual and vector features from sensory data are encoded through a multi-stage Transformer module and then a late-fusion Transformer module with temporal pixel-wise attention. Importantly, a flow-guided multi-head self-attention (FG-MSA) module is designed to better aggregate the information on occupancy and flow and model the mathematical relations between them. The proposed method is comprehensively validated on the Waymo Open Motion Dataset and compared against several state-of-the-art models. The results reveal that our model with much more compact architecture and data inputs than other methods can achieve comparable performance. We also demonstrate the effectiveness of incorporating vectorized agent motion features and the proposed FG-MSA module. Compared to the ablated model without the FG-MSA module, which won 2nd place in the 2022 Waymo Occupancy and Flow Prediction Challenge, the current model shows better separability for flow and occupancy and further performance improvements.},
	urldate = {2024-01-23},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liu, Haochen and Huang, Zhiyu and Lv, Chen},
	month = may,
	year = {2023},
	note = {arXiv:2208.00394 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {1449--1455},
}

@inproceedings{chen_learning_2019,
	title = {Learning to {Plan} in {High} {Dimensions} via {Neural} {Exploration}-{Exploitation} {Trees}},
	url = {https://openreview.net/forum?id=rJgJDAVKvB},
	abstract = {We propose a meta path planning algorithm named {\textbackslash}emph\{Neural Exploration-Exploitation Trees{\textasciitilde}(NEXT)\} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between {\textbackslash}emph\{exploration\} and {\textbackslash}emph\{exploitation\} when solving a new problem. We conduct thorough experiments to show that NEXT accomplishes new planning problems with more compact search trees and significantly outperforms state-of-the-art methods on several benchmarks.},
	language = {en},
	urldate = {2024-01-23},
	author = {Chen, Binghong and Dai, Bo and Lin, Qinjie and Ye, Guo and Liu, Han and Song, Le},
	month = sep,
	year = {2019},
}

@misc{li_synthesizing_2024,
	title = {Synthesizing {Moving} {People} with {3D} {Control}},
	url = {http://arxiv.org/abs/2401.10889},
	abstract = {In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {Li, Boyi and Rajasegaran, Jathushan and Gandelsman, Yossi and Efros, Alexei A. and Malik, Jitendra},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10889 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gu_efficiently_2021,
	title = {Efficiently {Modeling} {Long} {Sequences} with {Structured} {State} {Spaces}},
	url = {https://openreview.net/forum?id=uYLFoz1vlAC},
	abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) {\textbackslash}( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) {\textbackslash}), and showed that for appropriate choices of the state matrix {\textbackslash}( A {\textbackslash}), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning {\textbackslash}( A {\textbackslash}) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91{\textbackslash}\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60{\textbackslash}times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
	language = {en},
	urldate = {2024-01-21},
	author = {Gu, Albert and Goel, Karan and Re, Christopher},
	month = oct,
	year = {2021},
}

@misc{fathi_course_2023,
	title = {Course {Correcting} {Koopman} {Representations}},
	url = {http://arxiv.org/abs/2310.15386},
	doi = {10.48550/arXiv.2310.15386},
	abstract = {Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.},
	urldate = {2024-01-21},
	publisher = {arXiv},
	author = {Fathi, Mahan and Gehring, Clement and Pilault, Jonathan and Kanaa, David and Bacon, Pierre-Luc and Goroshin, Ross},
	month = nov,
	year = {2023},
	note = {arXiv:2310.15386 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{yadav_habitat-matterport_2023,
	title = {Habitat-{Matterport} {3D} {Semantics} {Dataset}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-01-20},
	author = {Yadav, Karmesh and Ramrakhya, Ram and Ramakrishnan, Santhosh Kumar and Gervet, Theo and Turner, John and Gokaslan, Aaron and Maestre, Noah and Chang, Angel Xuan and Batra, Dhruv and Savva, Manolis and Clegg, Alexander William and Chaplot, Devendra Singh},
	year = {2023},
	pages = {4927--4936},
}

@article{dudzik_graph_2022,
	title = {Graph {Neural} {Networks} are {Dynamic} {Programmers}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8248b1ded388fcdbbd121bcdfea3068c-Abstract-Conference.html},
	language = {en},
	urldate = {2024-01-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dudzik, Andrew J. and Veličković, Petar},
	month = dec,
	year = {2022},
	pages = {20635--20647},
}

@inproceedings{cohen_steerable_2016,
	title = {Steerable {CNNs}},
	url = {https://openreview.net/forum?id=rJQKYt5ll},
	abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
	language = {en},
	urldate = {2024-01-20},
	author = {Cohen, Taco S. and Welling, Max},
	month = nov,
	year = {2016},
}

@inproceedings{weiler_general_2019,
	title = {General {E}(2)-{Equivariant} {Steerable} {CNNs}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/45d6637b718d0f24a237069fe41b0db4-Abstract.html},
	abstract = {The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of E(2)-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group E(2) and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. E(2)-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as drop in replacement for non-equivariant convolutions.},
	urldate = {2024-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Weiler, Maurice and Cesa, Gabriele},
	year = {2019},
}

@inproceedings{brandstetter_geometric_2021,
	title = {Geometric and {Physical} {Quantities} improve {E}(3) {Equivariant} {Message} {Passing}},
	url = {https://openreview.net/forum?id=_xwr8gOBeV1},
	abstract = {Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(\$3\$) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. Our model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions. Through the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.},
	language = {en},
	urldate = {2024-01-20},
	author = {Brandstetter, Johannes and Hesselink, Rob and Pol, Elise van der and Bekkers, Erik J. and Welling, Max},
	month = oct,
	year = {2021},
}

@inproceedings{satorras_en_2021,
	title = {E(n) {Equivariant} {Graph} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v139/satorras21a.html},
	abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
	language = {en},
	urldate = {2024-01-20},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Satorras, Vı́ctor Garcia and Hoogeboom, Emiel and Welling, Max},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9323--9332},
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {https://proceedings.mlr.press/v48/cohenc16.html},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	language = {en},
	urldate = {2024-01-20},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {2990--2999},
}

@inproceedings{karkus_differentiable_2019,
	title = {Differentiable {Algorithm} {Networks} for {Composable} {Robot} {Learning}},
	volume = {15},
	isbn = {978-0-9923747-5-4},
	url = {https://www.roboticsproceedings.org/rss15/p39.html},
	urldate = {2024-01-20},
	author = {Karkus, Peter and Ma, Xiao and Hsu, David and Kaelbling, Leslie and Lee, Wee Sun and Lozano-Perez, Tomas},
	month = jun,
	year = {2019},
}

@misc{wang_worlddreamer_2024,
	title = {{WorldDreamer}: {Towards} {General} {World} {Models} for {Video} {Generation} via {Predicting} {Masked} {Tokens}},
	shorttitle = {{WorldDreamer}},
	url = {http://arxiv.org/abs/2401.09985},
	abstract = {World models play a crucial role in understanding and predicting the dynamics of the world, which is essential for video generation. However, existing world models are confined to specific scenarios such as gaming or driving, limiting their ability to capture the complexity of general world dynamic environments. Therefore, we introduce WorldDreamer, a pioneering world model to foster a comprehensive comprehension of general world physics and motions, which significantly enhances the capabilities of video generation. Drawing inspiration from the success of large language models, WorldDreamer frames world modeling as an unsupervised visual sequence modeling challenge. This is achieved by mapping visual inputs to discrete tokens and predicting the masked ones. During this process, we incorporate multi-modal prompts to facilitate interaction within the world model. Our experiments show that WorldDreamer excels in generating videos across different scenarios, including natural scenes and driving environments. WorldDreamer showcases versatility in executing tasks such as text-to-video conversion, image-tovideo synthesis, and video editing. These results underscore WorldDreamer's effectiveness in capturing dynamic elements within diverse general world environments.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Wang, Boyuan and Chen, Xinze and Lu, Jiwen},
	month = jan,
	year = {2024},
	note = {arXiv:2401.09985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kim_parahome_2024,
	title = {{ParaHome}: {Parameterizing} {Everyday} {Home} {Activities} {Towards} {3D} {Generative} {Modeling} of {Human}-{Object} {Interactions}},
	shorttitle = {{ParaHome}},
	url = {http://arxiv.org/abs/2401.10232},
	abstract = {To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Kim, Jeonghwan and Kim, Jisoo and Na, Jeonghyeon and Joo, Hanbyul},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10232 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bronars_simultaneous_2024,
	title = {Simultaneous {Tactile} {Estimation} and {Control} for {Extrinsic} {Dexterity}},
	url = {http://arxiv.org/abs/2401.10230},
	abstract = {We introduce a novel approach that combines tactile estimation and control for in-hand object manipulation. By integrating measurements from robot kinematics and an image-based tactile sensor, our framework estimates and tracks object pose while simultaneously generating motion plans to control the pose of a grasped object. This approach consists of a discrete pose estimator that uses the Viterbi decoding algorithm to find the most likely sequence of object poses in a coarsely discretized grid, and a continuous pose estimator-controller to refine the pose estimate and accurately manipulate the pose of the grasped object. Our method is tested on diverse objects and configurations, achieving desired manipulation objectives and outperforming single-shot methods in estimation accuracy. The proposed approach holds potential for tasks requiring precise manipulation in scenarios where visual perception is limited, laying the foundation for closed-loop behavior applications such as assembly and tool use. Please see supplementary videos for real-world demonstration at https://sites.google.com/view/texterity.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Bronars, Antonia and Kim, Sangwoon and Patre, Parag and Rodriguez, Alberto},
	month = jan,
	year = {2024},
	note = {arXiv:2401.10230 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{petrovich_multi-track_2024,
	title = {Multi-{Track} {Timeline} {Control} for {Text}-{Driven} {3D} {Human} {Motion} {Generation}},
	url = {http://arxiv.org/abs/2401.08559},
	doi = {10.48550/arXiv.2401.08559},
	abstract = {Recent advances in generative modeling have led to promising progress on synthesizing 3D human motion from text, with methods that can generate character animations from short prompts and specified durations. However, using a single text prompt as input lacks the fine-grained control needed by animators, such as composing multiple actions and defining precise durations for parts of the motion. To address this, we introduce the new problem of timeline control for text-driven motion synthesis, which provides an intuitive, yet fine-grained, input interface for users. Instead of a single prompt, users can specify a multi-track timeline of multiple prompts organized in temporal intervals that may overlap. This enables specifying the exact timings of each action and composing multiple actions in sequence or at overlapping intervals. To generate composite animations from a multi-track timeline, we propose a new test-time denoising method. This method can be integrated with any pre-trained motion diffusion model to synthesize realistic motions that accurately reflect the timeline. At every step of denoising, our method processes each timeline interval (text prompt) individually, subsequently aggregating the predictions with consideration for the specific body parts engaged in each action. Experimental comparisons and ablations validate that our method produces realistic motions that respect the semantics and timing of given text prompts. Our code and models are publicly available at https://mathis.petrovich.fr/stmc.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Petrovich, Mathis and Litany, Or and Iqbal, Umar and Black, Michael J. and Varol, Gül and Peng, Xue Bin and Rempe, Davis},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08559 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{bono_learning_2023,
	title = {Learning with a {Mole}: {Transferable} latent spatial representations for navigation without reconstruction},
	shorttitle = {Learning with a {Mole}},
	url = {http://arxiv.org/abs/2306.03857},
	doi = {10.48550/arXiv.2306.03857},
	abstract = {Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Bono, Guillaume and Antsfeld, Leonid and Sadek, Assem and Monaci, Gianluca and Wolf, Christian},
	month = sep,
	year = {2023},
	note = {arXiv:2306.03857 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{vobecky_pop-3d_2024,
	title = {{POP}-{3D}: {Open}-{Vocabulary} {3D} {Occupancy} {Prediction} from {Images}},
	shorttitle = {{POP}-{3D}},
	url = {http://arxiv.org/abs/2401.09413},
	abstract = {We describe an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images with the objective of enabling 3D grounding, segmentation and retrieval of free-form language queries. This is a challenging problem because of the 2D-3D ambiguity and the open-vocabulary nature of the target tasks, where obtaining annotated training data in 3D is difficult. The contributions of this work are three-fold. First, we design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Second, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language and (iii) LiDAR point clouds, and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. Finally, we demonstrate quantitatively the strengths of the proposed model on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes. You can find the project page here https://vobecant.github.io/POP3D.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Vobecky, Antonin and Siméoni, Oriane and Hurych, David and Gidaris, Spyros and Bursuc, Andrei and Pérez, Patrick and Sivic, Josef},
	month = jan,
	year = {2024},
	note = {arXiv:2401.09413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_visual_2024,
	title = {Visual {Robotic} {Manipulation} with {Depth}-{Aware} {Pretraining}},
	url = {http://arxiv.org/abs/2401.09038},
	abstract = {Recent work on visual representation learning has shown to be efficient for robotic manipulation tasks. However, most existing works pretrained the visual backbone solely on 2D images or egocentric videos, ignoring the fact that robots learn to act in 3D space, which is hard to learn from 2D observation. In this paper, we examine the effectiveness of pretraining for vision backbone with public-available large-scale 3D data to improve manipulation policy learning. Our method, namely Depth-aware Pretraining for Robotics (DPR), enables an RGB-only backbone to learn 3D scene representations from self-supervised contrastive learning, where depth information serves as auxiliary knowledge. No 3D information is necessary during manipulation policy learning and inference, making our model enjoy both efficiency and effectiveness in 3D space manipulation. Furthermore, we introduce a new way to inject robots' proprioception into the policy networks that makes the manipulation model robust and generalizable. We demonstrate in experiments that our proposed framework improves performance on unseen objects and visual environments for various robotics tasks on both simulated and real robots.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Wang, Wanying and Li, Jinming and Zhu, Yichen and Xu, Zhiyuan and Che, Zhengping and Peng, Yaxin and Shen, Chaomin and Liu, Dong and Feng, Feifei and Tang, Jian},
	month = jan,
	year = {2024},
	note = {arXiv:2401.09038 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{xu_visual-tactile_2023,
	title = {Visual-{Tactile} {Sensing} for {In}-{Hand} {Object} {Reconstruction}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-01-17},
	author = {Xu, Wenqiang and Yu, Zhenjun and Xue, Han and Ye, Ruolin and Yao, Siqiong and Lu, Cewu},
	year = {2023},
	pages = {8803--8812},
}

@misc{ju_robo-abc_2024,
	title = {Robo-{ABC}: {Affordance} {Generalization} {Beyond} {Categories} via {Semantic} {Correspondence} for {Robot} {Manipulation}},
	shorttitle = {Robo-{ABC}},
	url = {http://arxiv.org/abs/2401.07487},
	abstract = {Enabling robotic manipulation that generalizes to out-of-distribution scenes is a crucial step toward open-world embodied intelligence. For human beings, this ability is rooted in the understanding of semantic correspondence among objects, which naturally transfers the interaction experience of familiar objects to novel ones. Although robots lack such a reservoir of interaction experience, the vast availability of human videos on the Internet may serve as a valuable resource, from which we extract an affordance memory including the contact points. Inspired by the natural way humans think, we propose Robo-ABC: when confronted with unfamiliar objects that require generalization, the robot can acquire affordance by retrieving objects that share visual or semantic similarities from the affordance memory. The next step is to map the contact points of the retrieved objects to the new object. While establishing this correspondence may present formidable challenges at first glance, recent research finds it naturally arises from pre-trained diffusion models, enabling affordance mapping even across disparate object categories. Through the Robo-ABC framework, robots may generalize to manipulate out-of-category objects in a zero-shot manner without any manual annotation, additional training, part segmentation, pre-coded knowledge, or viewpoint restrictions. Quantitatively, Robo-ABC significantly enhances the accuracy of visual affordance retrieval by a large margin of 31.6\% compared to state-of-the-art (SOTA) end-to-end affordance models. We also conduct real-world experiments of cross-category object-grasping tasks. Robo-ABC achieved a success rate of 85.7\%, proving its capacity for real-world tasks.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Ju, Yuanchen and Hu, Kaizhe and Zhang, Guowei and Zhang, Gu and Jiang, Mingrun and Xu, Huazhe},
	month = jan,
	year = {2024},
	note = {arXiv:2401.07487 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{zhang_rohm_2024,
	title = {{RoHM}: {Robust} {Human} {Motion} {Reconstruction} via {Diffusion}},
	shorttitle = {{RoHM}},
	url = {http://arxiv.org/abs/2401.08570},
	abstract = {We propose RoHM, an approach for robust 3D human motion reconstruction from monocular RGB(-D) videos in the presence of noise and occlusions. Most previous approaches either train neural networks to directly regress motion in 3D or learn data-driven motion priors and combine them with optimization at test time. The former do not recover globally coherent motion and fail under occlusions; the latter are time-consuming, prone to local minima, and require manual tuning. To overcome these shortcomings, we exploit the iterative, denoising nature of diffusion models. RoHM is a novel diffusion-based motion model that, conditioned on noisy and occluded input data, reconstructs complete, plausible motions in consistent global coordinates. Given the complexity of the problem -- requiring one to address different tasks (denoising and infilling) in different solution spaces (local and global motion) -- we decompose it into two sub-tasks and learn two models, one for global trajectory and one for local motion. To capture the correlations between the two, we then introduce a novel conditioning module, combining it with an iterative inference scheme. We apply RoHM to a variety of tasks -- from motion reconstruction and denoising to spatial and temporal infilling. Extensive experiments on three popular datasets show that our method outperforms state-of-the-art approaches qualitatively and quantitatively, while being faster at test time. The code will be available at https://sanweiliti.github.io/ROHM/ROHM.html.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Zhang, Siwei and Bhatnagar, Bharat Lal and Xu, Yuanlu and Winkler, Alexander and Kadlecek, Petr and Tang, Siyu and Bogo, Federica},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08570 null},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_taco_2024,
	title = {{TACO}: {Benchmarking} {Generalizable} {Bimanual} {Tool}-{ACtion}-{Object} {Understanding}},
	shorttitle = {{TACO}},
	url = {http://arxiv.org/abs/2401.08399},
	abstract = {Humans commonly work with multiple objects in daily life and can intuitively transfer manipulation skills to novel objects by understanding object functional regularities. However, existing technical approaches for analyzing and synthesizing hand-object manipulation are mostly limited to handling a single hand and object due to the lack of data support. To address this, we construct TACO, an extensive bimanual hand-object-interaction dataset spanning a large variety of tool-action-object compositions for daily human activities. TACO contains 2.5K motion sequences paired with third-person and egocentric views, precise hand-object 3D meshes, and action labels. To rapidly expand the data scale, we present a fully-automatic data acquisition pipeline combining multi-view sensing with an optical motion capture system. With the vast research fields provided by TACO, we benchmark three generalizable hand-object-interaction tasks: compositional action recognition, generalizable hand-object motion forecasting, and cooperative grasp synthesis. Extensive experiments reveal new insights, challenges, and opportunities for advancing the studies of generalizable hand-object motion analysis and synthesis. Our data and code are available at https://taco2024.github.io.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Liu, Yun and Yang, Haolin and Si, Xu and Liu, Ling and Li, Zipeng and Zhang, Yuxiang and Liu, Yebin and Yi, Li},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08399 null},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kim_multi-task_2024,
	title = {Multi-task robot data for dual-arm fine manipulation},
	url = {http://arxiv.org/abs/2401.07603},
	abstract = {In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals as well as dual-action labels, a signal that separates actions into a robust reaching trajectory and precise interaction with objects, and language instructions to achieve robust and precise object manipulation. We applied the dataset to our Dual-Action and Attention (DAA), a model designed for fine-grained dual arm manipulation tasks and robust against covariate shifts. The model was tested with over 7k total trials in real robot manipulation tasks, demonstrating its capability in fine manipulation.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Kim, Heecheol and Ohmura, Yoshiyuki and Kuniyoshi, Yasuo},
	month = jan,
	year = {2024},
	note = {arXiv:2401.07603 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{luo_fmb_2024,
	title = {{FMB}: a {Functional} {Manipulation} {Benchmark} for {Generalizable} {Robotic} {Learning}},
	shorttitle = {{FMB}},
	url = {http://arxiv.org/abs/2401.08553},
	abstract = {In this paper, we propose a real-world benchmark for studying robotic learning in the context of functional manipulation: a robot needs to accomplish complex long-horizon behaviors by composing individual manipulation skills in functionally relevant ways. The core design principles of our Functional Manipulation Benchmark (FMB) emphasize a harmonious balance between complexity and accessibility. Tasks are deliberately scoped to be narrow, ensuring that models and datasets of manageable scale can be utilized effectively to track progress. Simultaneously, they are diverse enough to pose a significant generalization challenge. Furthermore, the benchmark is designed to be easily replicable, encompassing all essential hardware and software components. To achieve this goal, FMB consists of a variety of 3D-printed objects designed for easy and accurate replication by other researchers. The objects are procedurally generated, providing a principled framework to study generalization in a controlled fashion. We focus on fundamental manipulation skills, including grasping, repositioning, and a range of assembly behaviors. The FMB can be used to evaluate methods for acquiring individual skills, as well as methods for combining and ordering such skills to solve complex, multi-stage manipulation tasks. We also offer an imitation learning framework that includes a suite of policies trained to solve the proposed tasks. This enables researchers to utilize our tasks as a versatile toolkit for examining various parts of the pipeline. For example, researchers could propose a better design for a grasping controller and evaluate it in combination with our baseline reorientation and assembly policies as part of a pipeline for solving multi-stage tasks. Our dataset, object CAD files, code, and evaluation videos can be found on our project website: https://functional-manipulation-benchmark.github.io},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Luo, Jianlan and Xu, Charles and Liu, Fangchen and Tan, Liam and Lin, Zipeng and Wu, Jeffrey and Abbeel, Pieter and Levine, Sergey},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08553 null},
	keywords = {Computer Science - Robotics},
}

@misc{xu_drm_2023,
	title = {{DrM}: {Mastering} {Visual} {Reinforcement} {Learning} through {Dormant} {Ratio} {Minimization}},
	shorttitle = {{DrM}},
	url = {http://arxiv.org/abs/2310.19668},
	doi = {10.48550/arXiv.2310.19668},
	abstract = {Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Xu, Guowei and Zheng, Ruijie and Liang, Yongyuan and Wang, Xiyao and Yuan, Zhecheng and Ji, Tianying and Luo, Yu and Liu, Xiaoyu and Yuan, Jiaxin and Hua, Pu and Li, Shuzhen and Ze, Yanjie and Daumé III, Hal and Huang, Furong and Xu, Huazhe},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19668 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gandelsman_interpreting_2023,
	title = {Interpreting {CLIP}'s {Image} {Representation} via {Text}-{Based} {Decomposition}},
	url = {http://arxiv.org/abs/2310.05916},
	doi = {10.48550/arXiv.2310.05916},
	abstract = {We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.},
	urldate = {2024-01-17},
	publisher = {arXiv},
	author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yang_generating_2023,
	title = {Generating {Visual} {Scenes} from {Touch}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Generating_Visual_Scenes_from_Touch_ICCV_2023_paper.html},
	language = {en},
	urldate = {2024-01-16},
	author = {Yang, Fengyu and Zhang, Jiacheng and Owens, Andrew},
	year = {2023},
	pages = {22070--22080},
}

@inproceedings{yang_touch_2022,
	title = {Touch and {Go}: {Learning} from {Human}-{Collected} {Vision} and {Touch}},
	shorttitle = {Touch and {Go}},
	url = {https://openreview.net/forum?id=ZZ3FeSSPPblo},
	abstract = {The ability to associate touch with sight is essential for tasks that require physically interacting with objects in the world. We propose a dataset with paired visual and tactile data called Touch and Go, in which human data collectors probe objects in natural environments using tactile sensors, while simultaneously recording egocentric video. In contrast to previous efforts, which have largely been confined to lab settings or simulated environments, our dataset spans a large number of “in the wild” objects and scenes. We successfully apply our dataset to a variety of multimodal learning tasks: 1) self-supervised visuo-tactile feature learning, 2) tactile-driven image stylization, i.e., making the visual appearance of an object more consistent with a given tactile signal, and 3) predicting future frames of a tactile signal from visuo-tactile inputs.},
	language = {en},
	urldate = {2024-01-16},
	author = {Yang, Fengyu and Ma, Chenyang and Zhang, Jiacheng and Zhu, Jing and Yuan, Wenzhen and Owens, Andrew},
	month = jun,
	year = {2022},
}

@misc{tong_eyes_2024,
	title = {Eyes {Wide} {Shut}? {Exploring} the {Visual} {Shortcomings} of {Multimodal} {LLMs}},
	shorttitle = {Eyes {Wide} {Shut}?},
	url = {http://arxiv.org/abs/2401.06209},
	doi = {10.48550/arXiv.2401.06209},
	abstract = {Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06209 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cao_motion2vecsets_2024,
	title = {{Motion2VecSets}: {4D} {Latent} {Vector} {Set} {Diffusion} for {Non}-rigid {Shape} {Reconstruction} and {Tracking}},
	shorttitle = {{Motion2VecSets}},
	url = {http://arxiv.org/abs/2401.06614},
	abstract = {We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based prior enables more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent vector sets instead of using a global latent. This novel 4D representation allows us to learn local surface shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporal-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid the computational overhead, we design an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against the state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations, notably achieving a 19\% improvement in Intersection over Union (IoU) compared to CaDex for reconstructing unseen individuals from sparse point clouds on the DeformingThings4D-Animals dataset. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Cao, Wei and Luo, Chang and Zhang, Biao and Nießner, Matthias and Tang, Jiapeng},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06614 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{qian_affordancellm_2024,
	title = {{AffordanceLLM}: {Grounding} {Affordance} from {Vision} {Language} {Models}},
	shorttitle = {{AffordanceLLM}},
	url = {http://arxiv.org/abs/2401.06341},
	abstract = {Affordance grounding refers to the task of finding the area of an object with which one can interact. It is a fundamental but challenging task, as a successful solution requires the comprehensive understanding of a scene in multiple aspects including detection, localization, and recognition of objects with their parts, of geo-spatial configuration/layout of the scene, of 3D shapes and physics, as well as of the functionality and potential interaction of the objects and humans. Much of the knowledge is hidden and beyond the image content with the supervised labels from a limited training set. In this paper, we make an attempt to improve the generalization capability of the current affordance grounding by taking the advantage of the rich world, abstract, and human-object-interaction knowledge from pretrained large-scale vision language models. Under the AGD20K benchmark, our proposed model demonstrates a significant performance gain over the competing methods for in-the-wild object affordance grounding. We further demonstrate it can ground affordance for objects from random Internet images, even if both objects and actions are unseen during training. Project site: https://jasonqsy.github.io/AffordanceLLM/},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Qian, Shengyi and Chen, Weifeng and Bai, Min and Zhou, Xiong and Tu, Zhuowen and Li, Li Erran},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06341 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@book{jones_human_2006,
	title = {Human {Hand} {Function}},
	isbn = {978-0-19-517315-4},
	abstract = {Human Hand Function is a multidisciplinary book that reviews the sensory and motor aspects of normal hand function from both neurophysiological and behavioral perspectives. Lynette Jones and Susan Lederman present hand function as a continuum ranging from activities that are essentially sensory in nature to those that have a strong motor component. They delineate four categories of function along this sensorimotor continuum--tactile sensing, active haptic sensing, prehension, and non-prehensile skilled movements--that they use as a framework for analyzing and synthesizing the results from a broad range of studies that have contributed to our understanding of how the normal human hand functions.The book begins with a historical overview of research on the hand and a discussion of the hand's evolutionary development in terms of anatomical structure. The subsequent chapters review the research in each of the four categories along the continuum, covering topics such as the intensive spatial, temporal, and thermal sensitivity of the hand, the role of hand movements in recognizing common objects, the control of reaching and grasping movements, and the organization of keyboard skills. Jones and Lederman also examine how sensory and motor function develops in the hand from birth to old age, and how the nature of the end effector (e.g., a single finger or the whole hand) that is used to interact with the environment influences the types of information obtained and the tasks performed. The book closes with an assessment of how basic research on the hand has contributed to an array of more applied domains, including communication systems for the blind, haptic interfaces used in teleoperation and virtual-environment applications, tests used to assess hand impairments, and haptic exploration in art. Human Hand Function will be a valuable resource for student and professional researchers in neuroscience, cognitive psychology, engineering, human-technology interaction, and physiology.},
	language = {en},
	publisher = {Oxford University Press, USA},
	author = {Jones, Lynette A. and Lederman, Susan J.},
	month = apr,
	year = {2006},
	note = {Google-Books-ID: NRI8n9oeAsUC},
	keywords = {Medical / Anatomy, Psychology / Cognitive Psychology \& Cognition},
}

@article{yao_controlvae_2022,
	title = {{ControlVAE}: {Model}-{Based} {Learning} of {Generative} {Controllers} for {Physics}-{Based} {Characters}},
	volume = {41},
	issn = {0730-0301},
	shorttitle = {{ControlVAE}},
	url = {https://dl.acm.org/doi/10.1145/3550454.3555434},
	doi = {10.1145/3550454.3555434},
	abstract = {In this paper, we introduce ControlVAE, a novel model-based framework for learning generative motion control policies based on variational autoencoders (VAE). Our framework can learn a rich and flexible latent representation of skills and a skill-conditioned generative control policy from a diverse set of unorganized motion sequences, which enables the generation of realistic human behaviors by sampling in the latent space and allows high-level control policies to reuse the learned skills to accomplish a variety of downstream tasks. In the training of ControlVAE, we employ a learnable world model to realize direct supervision of the latent space and the control policy. This world model effectively captures the unknown dynamics of the simulation system, enabling efficient model-based learning of high-level downstream tasks. We also learn a state-conditional prior distribution in the VAE-based generative control policy, which generates a skill embedding that outperforms the non-conditional priors in downstream tasks. We demonstrate the effectiveness of ControlVAE using a diverse set of tasks, which allows realistic and interactive control of the simulated characters.},
	number = {6},
	urldate = {2024-01-12},
	journal = {ACM Transactions on Graphics},
	author = {Yao, Heyuan and Song, Zhenhua and Chen, Baoquan and Liu, Libin},
	month = nov,
	year = {2022},
	keywords = {VAE, deep reinforcement learning, generative model, motion control, physics-based character animation},
	pages = {183:1--183:16},
}

@misc{shahbazi_inserf_2024,
	title = {{InseRF}: {Text}-{Driven} {Generative} {Object} {Insertion} in {Neural} {3D} {Scenes}},
	shorttitle = {{InseRF}},
	url = {http://arxiv.org/abs/2401.05335},
	abstract = {We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf.},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Shahbazi, Mohamad and Claessens, Liesbeth and Niemeyer, Michael and Collins, Edo and Tonioni, Alessio and Van Gool, Luc and Tombari, Federico},
	month = jan,
	year = {2024},
	note = {arXiv:2401.05335 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{skreta_replan_2024,
	title = {{RePLan}: {Robotic} {Replanning} with {Perception} and {Language} {Models}},
	shorttitle = {{RePLan}},
	url = {http://arxiv.org/abs/2401.04157},
	abstract = {Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects. One way to prevent these challenges is to rely on human-provided step-by-step instructions, limiting the autonomy of robotic systems. Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering and image captioning. Leveraging the capabilities of VLMs, we present a novel framework called Robotic Replanning with Perception and Language Models (RePLan) that enables real-time replanning capabilities for long-horizon tasks. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We test our approach within four environments containing seven long-horizion tasks. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, where baseline models cannot. Find more information at https://replan-lm.github.io/replan.github.io/},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Skreta, Marta and Zhou, Zihan and Yuan, Jia Lin and Darvish, Kourosh and Aspuru-Guzik, Alán and Garg, Animesh},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04157 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {http://arxiv.org/abs/1903.02428},
	doi = {10.48550/arXiv.1903.02428},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = apr,
	year = {2019},
	note = {arXiv:1903.02428 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {https://proceedings.mlr.press/v70/gilmer17a.html},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	language = {en},
	urldate = {2024-01-09},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1263--1272},
}

@misc{rafailov_moto_2024,
	title = {{MOTO}: {Offline} {Pre}-training to {Online} {Fine}-tuning for {Model}-based {Robot} {Learning}},
	shorttitle = {{MOTO}},
	url = {http://arxiv.org/abs/2401.03306},
	abstract = {We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty. We find that our approach successfully solves tasks from the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation environment completely from images. To the best of our knowledge, MOTO is the first method to solve this environment from pixels.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Hatch, Kyle and Kolev, Victor and Martin, John D. and Phielipp, Mariano and Finn, Chelsea},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03306 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mishra_generative_2023,
	title = {Generative {Skill} {Chaining}: {Long}-{Horizon} {Skill} {Planning} with {Diffusion} {Models}},
	shorttitle = {Generative {Skill} {Chaining}},
	url = {http://arxiv.org/abs/2401.03360},
	abstract = {Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge in manipulation planning. Skill chaining is a practical approach to solving unseen tasks by combining learned skill priors. However, such methods are myopic if sequenced greedily and face scalability issues with search-based planning strategy. To address these challenges, we introduce Generative Skill Chaining{\textasciitilde}(GSC), a probabilistic framework that learns skill-centric diffusion models and composes their learned distributions to generate long-horizon plans during inference. GSC samples from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We evaluate the method on various long-horizon tasks and demonstrate its capability in reasoning about action dependencies, constraint handling, and generalization, along with its ability to replan in the face of perturbations. We show results in simulation and on real robot to validate the efficiency and scalability of GSC, highlighting its potential for advancing long-horizon task planning. More details are available at: https://generative-skill-chaining.github.io/},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Mishra, Utkarsh A. and Xue, Shangjie and Chen, Yongxin and Xu, Danfei},
	month = oct,
	year = {2023},
	note = {arXiv:2401.03360 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{tarasiou_locally_2024,
	title = {Locally {Adaptive} {Neural} {3D} {Morphable} {Models}},
	url = {http://arxiv.org/abs/2401.02937},
	doi = {10.48550/arXiv.2401.02937},
	abstract = {We present the Locally Adaptive Morphable Model (LAMM), a highly flexible Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes. We train our architecture following a simple self-supervised training scheme in which input displacements over a set of sparse control vertices are used to overwrite the encoded geometry in order to transform one training sample into another. During inference, our model produces a dense output that adheres locally to the specified sparse geometry while maintaining the overall appearance of the encoded object. This approach results in state-of-the-art performance in both disentangling manipulated geometry and 3D mesh reconstruction. To the best of our knowledge LAMM is the first end-to-end framework that enables direct local control of 3D vertex geometry in a single forward pass. A very efficient computational graph allows our network to train with only a fraction of the memory required by previous methods and run faster during inference, generating 12k vertex meshes at \${\textgreater}\$60fps on a single CPU thread. We further leverage local geometry control as a primitive for higher level editing operations and present a set of derivative capabilities such as swapping and sampling object parts. Code and pretrained models can be found at https://github.com/michaeltrs/LAMM.},
	urldate = {2024-01-09},
	publisher = {arXiv},
	author = {Tarasiou, Michail and Potamias, Rolandos Alexandros and O'Sullivan, Eimear and Ploumpis, Stylianos and Zafeiriou, Stefanos},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02937 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fujimoto_addressing_2018,
	title = {Addressing {Function} {Approximation} {Error} in {Actor}-{Critic} {Methods}},
	url = {https://proceedings.mlr.press/v80/fujimoto18a.html},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
	language = {en},
	urldate = {2024-01-08},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1587--1596},
}

@misc{lee_mocap_2024,
	title = {Mocap {Everyone} {Everywhere}: {Lightweight} {Motion} {Capture} {With} {Smartwatches} and a {Head}-{Mounted} {Camera}},
	shorttitle = {Mocap {Everyone} {Everywhere}},
	url = {http://arxiv.org/abs/2401.00847},
	doi = {10.48550/arXiv.2401.00847},
	abstract = {We present a lightweight and affordable motion capture method based on two smartwatches and a head-mounted camera. In contrast to the existing approaches that use six or more expert-level IMU devices, our approach is much more cost-effective and convenient. Our method can make wearable motion capture accessible to everyone everywhere, enabling 3D full-body motion capture in diverse environments. As a key idea to overcome the extreme sparsity and ambiguities of sensor inputs, we integrate 6D head poses obtained from the head-mounted cameras for motion estimation. To enable capture in expansive indoor and outdoor scenes, we propose an algorithm to track and update floor level changes to define head poses, coupled with a multi-stage Transformer-based regression module. We also introduce novel strategies leveraging visual cues of egocentric images to further enhance the motion capture quality while reducing ambiguities. We demonstrate the performance of our method on various challenging scenarios, including complex outdoor environments and everyday motions including object interactions and social interactions among multiple individuals.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Lee, Jiye and Joo, Hanbyul},
	month = jan,
	year = {2024},
	note = {arXiv:2401.00847 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{lu_unified-io_2023,
	title = {Unified-{IO} 2: {Scaling} {Autoregressive} {Multimodal} {Models} with {Vision}, {Language}, {Audio}, and {Action}},
	shorttitle = {Unified-{IO} 2},
	url = {http://arxiv.org/abs/2312.17172},
	abstract = {We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17172 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{li_manipllm_2023,
	title = {{ManipLLM}: {Embodied} {Multimodal} {Large} {Language} {Model} for {Object}-{Centric} {Robotic} {Manipulation}},
	shorttitle = {{ManipLLM}},
	url = {http://arxiv.org/abs/2312.16217},
	abstract = {Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16217 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{jiang_-hand_2023,
	title = {In-{Hand} {3D} {Object} {Reconstruction} from a {Monocular} {RGB} {Video}},
	url = {http://arxiv.org/abs/2312.16425},
	abstract = {Our work aims to reconstruct a 3D object that is held and rotated by a hand in front of a static RGB camera. Previous methods that use implicit neural representations to recover the geometry of a generic hand-held object from multi-view images achieved compelling results in the visible part of the object. However, these methods falter in accurately capturing the shape within the hand-object contact region due to occlusion. In this paper, we propose a novel method that deals with surface reconstruction under occlusion by incorporating priors of 2D occlusion elucidation and physical contact constraints. For the former, we introduce an object amodal completion network to infer the 2D complete mask of objects under occlusion. To ensure the accuracy and view consistency of the predicted 2D amodal masks, we devise a joint optimization method for both amodal mask refinement and 3D reconstruction. For the latter, we impose penetration and attraction constraints on the local geometry in contact regions. We evaluate our approach on HO3D and HOD datasets and demonstrate that it outperforms the state-of-the-art methods in terms of reconstruction surface quality, with an improvement of \$52{\textbackslash}\%\$ on HO3D and \$20{\textbackslash}\%\$ on HOD. Project webpage: https://east-j.github.io/ihor.},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Jiang, Shijian and Ye, Qi and Xie, Rengan and Huo, Yuchi and Li, Xiang and Zhou, Yang and Chen, Jiming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16425 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ren_insactor_2023,
	title = {{InsActor}: {Instruction}-driven {Physics}-based {Characters}},
	shorttitle = {{InsActor}},
	url = {https://arxiv.org/abs/2312.17135v1},
	abstract = {Generating animation of physics-based characters with intuitive control has long been a desirable task with numerous applications. However, generating physically simulated animations that reflect high-level human instructions remains a difficult problem due to the complexity of physical environments and the richness of human language. In this paper, we present InsActor, a principled generative framework that leverages recent advancements in diffusion-based human motion models to produce instruction-driven animations of physics-based characters. Our framework empowers InsActor to capture complex relationships between high-level human instructions and character motions by employing diffusion policies for flexibly conditioned motion planning. To overcome invalid states and infeasible state transitions in planned motions, InsActor discovers low-level skills and maps plans to latent skill sequences in a compact latent space. Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading. Notably, the ability of InsActor to generate physically simulated animations using high-level human instructions makes it a valuable tool, particularly in executing long-horizon tasks with a rich set of instructions.},
	language = {en},
	urldate = {2023-12-29},
	journal = {arXiv.org},
	author = {Ren, Jiawei and Zhang, Mingyuan and Yu, Cunjun and Ma, Xiao and Pan, Liang and Liu, Ziwei},
	month = dec,
	year = {2023},
}

@misc{tevet_human_2022,
	title = {Human {Motion} {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2209.14916},
	doi = {10.48550/arXiv.2209.14916},
	abstract = {Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .},
	urldate = {2023-12-28},
	publisher = {arXiv},
	author = {Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Cohen-Or, Daniel and Bermano, Amit H.},
	month = oct,
	year = {2022},
	note = {arXiv:2209.14916 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{janner_planning_2022,
	title = {Planning with {Diffusion} for {Flexible} {Behavior} {Synthesis}},
	url = {http://arxiv.org/abs/2205.09991},
	doi = {10.48550/arXiv.2205.09991},
	abstract = {Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
	urldate = {2023-12-28},
	publisher = {arXiv},
	author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua B. and Levine, Sergey},
	month = dec,
	year = {2022},
	note = {arXiv:2205.09991 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zhang_ins-hoi_2023,
	title = {Ins-{HOI}: {Instance} {Aware} {Human}-{Object} {Interactions} {Recovery}},
	shorttitle = {Ins-{HOI}},
	url = {http://arxiv.org/abs/2312.09641},
	doi = {10.48550/arXiv.2312.09641},
	abstract = {Recovering detailed interactions between humans/hands and objects is an appealing yet challenging task. Existing methods typically use template-based representations to track human/hand and objects in interactions. Despite the progress, they fail to handle the invisible contact surfaces. In this paper, we propose Ins-HOI, an end-to-end solution to recover human/hand-object reconstruction via instance-level implicit reconstruction. To this end, we introduce an instance-level occupancy field to support simultaneous human/hand and object representation, and a complementary training strategy to handle the lack of instance-level ground truths. Such a representation enables learning a contact prior implicitly from sparse observations. During the complementary training, we augment the real-captured data with synthesized data by randomly composing individual scans of humans/hands and objects and intentionally allowing for penetration. In this way, our network learns to recover individual shapes as completely as possible from the synthesized data, while being aware of the contact constraints and overall reasonability based on real-captured scans. As demonstrated in experiments, our method Ins-HOI can produce reasonable and realistic non-visible contact surfaces even in cases of extremely close interaction. To facilitate the research of this task, we collect a large-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans with real-world human-chair and hand-object interactions. We will release our dataset and source codes. Data examples and the video results of our method can be found on the project page.},
	urldate = {2023-12-25},
	publisher = {arXiv},
	author = {Zhang, Jiajun and Zhang, Yuxiang and Zhang, Hongwen and Zhou, Boyao and Shao, Ruizhi and Hu, Zonghai and Liu, Yebin},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09641 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shimada_macs_2023,
	title = {{MACS}: {Mass} {Conditioned} {3D} {Hand} and {Object} {Motion} {Synthesis}},
	shorttitle = {{MACS}},
	url = {http://arxiv.org/abs/2312.14929},
	abstract = {The physical properties of an object, such as mass, significantly affect how we manipulate it with our hands. Surprisingly, this aspect has so far been neglected in prior work on 3D motion synthesis. To improve the naturalness of the synthesized 3D hand object motions, this work proposes MACS the first MAss Conditioned 3D hand and object motion Synthesis approach. Our approach is based on cascaded diffusion models and generates interactions that plausibly adjust based on the object mass and interaction type. MACS also accepts a manually drawn 3D object trajectory as input and synthesizes the natural 3D hand motions conditioned by the object mass. This flexibility enables MACS to be used for various downstream applications, such as generating synthetic training data for ML tasks, fast animation of hands for graphics workflows, and generating character interactions for computer games. We show experimentally that a small-scale dataset is sufficient for MACS to reasonably generalize across interpolated and extrapolated object masses unseen during the training. Furthermore, MACS shows moderate generalization to unseen objects, thanks to the mass-conditioned contact labels generated by our surface contact synthesis model ConNet. Our comprehensive user study confirms that the synthesized 3D hand-object interactions are highly plausible and realistic.},
	urldate = {2023-12-25},
	publisher = {arXiv},
	author = {Shimada, Soshi and Mueller, Franziska and Bednarik, Jan and Doosti, Bardia and Bickel, Bernd and Tang, Danhang and Golyanik, Vladislav and Taylor, Jonathan and Theobalt, Christian and Beeler, Thabo},
	month = dec,
	year = {2023},
	note = {arXiv:2312.14929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{sharma_dynamics-aware_2020,
	title = {Dynamics-{Aware} {Unsupervised} {Discovery} of {Skills}},
	url = {http://arxiv.org/abs/1907.01657},
	doi = {10.48550/arXiv.1907.01657},
	abstract = {Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.},
	urldate = {2023-12-23},
	publisher = {arXiv},
	author = {Sharma, Archit and Gu, Shixiang and Levine, Sergey and Kumar, Vikash and Hausman, Karol},
	month = feb,
	year = {2020},
	note = {arXiv:1907.01657 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{ji_power_2023,
	title = {The {Power} of {Contrast} for {Feature} {Learning}: {A} {Theoretical} {Analysis}},
	shorttitle = {The {Power} of {Contrast} for {Feature} {Learning}},
	url = {http://arxiv.org/abs/2110.02473},
	doi = {10.48550/arXiv.2110.02473},
	abstract = {Contrastive learning has achieved state-of-the-art performance in various self-supervised learning tasks and even outperforms its supervised counterpart. Despite its empirical success, theoretical understanding of the superiority of contrastive learning is still limited. In this paper, under linear representation settings, (i) we provably show that contrastive learning outperforms the standard autoencoders and generative adversarial networks, two classical generative unsupervised learning methods, for both feature recovery and in-domain downstream tasks; (ii) we also illustrate the impact of labeled data in supervised contrastive learning. This provides theoretical support for recent findings that contrastive learning with labels improves the performance of learned representations in the in-domain downstream task, but it can harm the performance in transfer learning. We verify our theory with numerical experiments.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Ji, Wenlong and Deng, Zhun and Nakada, Ryumei and Zou, James and Zhang, Linjun},
	month = dec,
	year = {2023},
	note = {arXiv:2110.02473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{piacenza_sensorized_2020,
	title = {A {Sensorized} {Multicurved} {Robot} {Finger} with {Data}-driven {Touch} {Sensing} via {Overlapping} {Light} {Signals}},
	volume = {25},
	issn = {1083-4435, 1941-014X},
	url = {http://arxiv.org/abs/2004.00685},
	doi = {10.1109/TMECH.2020.2975578},
	abstract = {Despite significant advances in touch and force transduction, tactile sensing is still far from ubiquitous in robotic manipulation. Existing methods for building touch sensors have proven difficult to integrate into robot fingers due to multiple challenges, including difficulty in covering multicurved surfaces, high wire count, or packaging constrains preventing their use in dexterous hands. In this paper, we present a multicurved robotic finger with accurate touch localization and normal force detection over complex, three-dimensional surfaces. The key to our approach is the novel use of overlapping signals from light emitters and receivers embedded in a transparent waveguide layer that covers the functional areas of the finger. By measuring light transport between every emitter and receiver, we show that we can obtain a very rich signal set that changes in response to deformation of the finger due to touch. We then show that purely data-driven deep learning methods are able to extract useful information from such data, such as contact location and applied normal force, without the need for analytical models. The final result is a fully integrated, sensorized robot finger, with a low wire count and using easily accessible manufacturing methods, designed for easy integration into dexterous manipulators.},
	number = {5},
	urldate = {2023-12-22},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Piacenza, Pedro and Behrman, Keith and Schifferer, Benedikt and Kymissis, Ioannis and Ciocarlie, Matei},
	month = oct,
	year = {2020},
	note = {arXiv:2004.00685 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {2416--2427},
}

@inproceedings{hu_modular_2023,
	title = {Modular {Neural} {Network} {Policies} for {Learning} {In}-{Flight} {Object} {Catching} with a {Robot} {Hand}-{Arm} {System}},
	url = {http://arxiv.org/abs/2312.13987},
	doi = {10.1109/IROS55552.2023.10341463},
	abstract = {We present a modular framework designed to enable a robot hand-arm system to learn how to catch flying objects, a task that requires fast, reactive, and accurately-timed robot motions. Our framework consists of five core modules: (i) an object state estimator that learns object trajectory prediction, (ii) a catching pose quality network that learns to score and rank object poses for catching, (iii) a reaching control policy trained to move the robot hand to pre-catch poses, (iv) a grasping control policy trained to perform soft catching motions for safe and robust grasping, and (v) a gating network trained to synthesize the actions given by the reaching and grasping policy. The former two modules are trained via supervised learning and the latter three use deep reinforcement learning in a simulated environment. We conduct extensive evaluations of our framework in simulation for each module and the integrated system, to demonstrate high success rates of in-flight catching and robustness to perturbations and sensory noise. Whilst only simple cylindrical and spherical objects are used for training, the integrated system shows successful generalization to a variety of household objects that are not used in training.},
	urldate = {2023-12-22},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Hu, Wenbin and Acero, Fernando and Triantafyllidis, Eleftherios and Liu, Zhaocheng and Li, Zhibin},
	month = oct,
	year = {2023},
	note = {arXiv:2312.13987 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	pages = {944--951},
}

@misc{shah_ziplora_2023,
	title = {{ZipLoRA}: {Any} {Subject} in {Any} {Style} by {Effectively} {Merging} {LoRAs}},
	shorttitle = {{ZipLoRA}},
	url = {http://arxiv.org/abs/2311.13600},
	doi = {10.48550/arXiv.2311.13600},
	abstract = {Methods for finetuning generative models for concept-driven personalization generally achieve strong results for subject-driven or style-driven generation. Recently, low-rank adaptations (LoRA) have been proposed as a parameter-efficient way of achieving concept-driven personalization. While recent work explores the combination of separate LoRAs to achieve joint generation of learned styles and subjects, existing techniques do not reliably address the problem; they often compromise either subject fidelity or style fidelity. We propose ZipLoRA, a method to cheaply and effectively merge independently trained style and subject LoRAs in order to achieve generation of any user-provided subject in any user-provided style. Experiments on a wide range of subject and style combinations show that ZipLoRA can generate compelling results with meaningful improvements over baselines in subject and style fidelity while preserving the ability to recontextualize. Project page: https://ziplora.github.io},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Shah, Viraj and Ruiz, Nataniel and Cole, Forrester and Lu, Erika and Lazebnik, Svetlana and Li, Yuanzhen and Jampani, Varun},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13600 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{li_self-conditioned_2023,
	title = {Self-conditioned {Image} {Generation} via {Generating} {Representations}},
	url = {http://arxiv.org/abs/2312.03701},
	doi = {10.48550/arXiv.2312.03701},
	abstract = {This paper presents \${\textbackslash}textbf\{R\}\$epresentation-\${\textbackslash}textbf\{C\}\$onditioned image \${\textbackslash}textbf\{G\}\$eneration (RCG), a simple yet effective image generation framework which sets a new benchmark in class-unconditional image generation. RCG does not condition on any human annotations. Instead, it conditions on a self-supervised representation distribution which is mapped from the image distribution using a pre-trained encoder. During generation, RCG samples from such representation distribution using a representation diffusion model (RDM), and employs a pixel generator to craft image pixels conditioned on the sampled representation. Such a design provides substantial guidance during the generative process, resulting in high-quality image generation. Tested on ImageNet 256\${\textbackslash}times\$256, RCG achieves a Frechet Inception Distance (FID) of 3.31 and an Inception Score (IS) of 253.4. These results not only significantly improve the state-of-the-art of class-unconditional image generation but also rival the current leading methods in class-conditional image generation, bridging the long-standing performance gap between these two tasks. Code is available at https://github.com/LTH14/rcg.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Li, Tianhong and Katabi, Dina and He, Kaiming},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03701 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{burns_what_2023,
	title = {What {Makes} {Pre}-{Trained} {Visual} {Representations} {Successful} for {Robust} {Manipulation}?},
	url = {http://arxiv.org/abs/2312.12444},
	abstract = {Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as a means to improve the learning efficiency and generalization ability of policies learned from pixels. To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing diverse tasks, in pursuit of manipulation-relevant features. Although this approach improves the efficiency of policy learning, it remains unclear how reliable these representations are in the presence of distribution shifts that arise commonly in robotic applications. Surprisingly, we find that visual representations designed for manipulation and control tasks do not necessarily generalize under subtle changes in lighting and scene texture or the introduction of distractor objects. To understand what properties do lead to robust representations, we compare the performance of 15 pre-trained vision models under different visual appearances. We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT models. The rank order induced by this metric is more predictive than metrics that have previously guided generalization research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by cue-conflict performance. We test this finding extensively on a suite of distribution shifts in ten tasks across two simulated manipulation environments. On the ALOHA setup, segmentation score predicts real-world performance after offline training with 50 demonstrations.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Burns, Kaylee and Witzel, Zach and Hamid, Jubayer Ibn and Yu, Tianhe and Finn, Chelsea and Hausman, Karol},
	month = nov,
	year = {2023},
	note = {arXiv:2312.12444 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{szymanowicz_splatter_2023,
	title = {Splatter {Image}: {Ultra}-{Fast} {Single}-{View} {3D} {Reconstruction}},
	shorttitle = {Splatter {Image}},
	url = {http://arxiv.org/abs/2312.13150},
	abstract = {We introduce the Splatter Image, an ultra-fast approach for monocular 3D object reconstruction which operates at 38 FPS. Splatter Image is based on Gaussian Splatting, which has recently brought real-time rendering, fast training, and excellent scaling to multi-view reconstruction. For the first time, we apply Gaussian Splatting in a monocular reconstruction setting. Our approach is learning-based, and, at test time, reconstruction only requires the feed-forward evaluation of a neural network. The main innovation of Splatter Image is the surprisingly straightforward design: it uses a 2D image-to-image network to map the input image to one 3D Gaussian per pixel. The resulting Gaussians thus have the form of an image, the Splatter Image. We further extend the method to incorporate more than one image as input, which we do by adding cross-view attention. Owning to the speed of the renderer (588 FPS), we can use a single GPU for training while generating entire images at each iteration in order to optimize perceptual metrics like LPIPS. On standard benchmarks, we demonstrate not only fast reconstruction but also better results than recent and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Szymanowicz, Stanislaw and Rupprecht, Christian and Vedaldi, Andrea},
	month = dec,
	year = {2023},
	note = {arXiv:2312.13150 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wu_unleashing_2023,
	title = {Unleashing {Large}-{Scale} {Video} {Generative} {Pre}-training for {Visual} {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2312.13139},
	abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9\% to 94.9\%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3\% to 85.4\%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
	month = dec,
	year = {2023},
	note = {arXiv:2312.13139 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{lloyd_pose_2023,
	title = {Pose and shear-based tactile servoing},
	url = {http://arxiv.org/abs/2312.08411},
	doi = {10.48550/arXiv.2312.08411},
	abstract = {Tactile servoing is an important technique because it enables robots to manipulate objects with precision and accuracy while adapting to changes in their environments in real-time. One approach for tactile servo control with high-resolution soft tactile sensors is to estimate the contact pose relative to an object surface using a convolutional neural network (CNN) for use as a feedback signal. In this paper, we investigate how the surface pose estimation model can be extended to include shear, and utilize these combined pose-and-shear models to develop a tactile robotic system that can be programmed for diverse non-prehensile manipulation tasks, such as object tracking, surface following, single-arm object pushing and dual-arm object pushing. In doing this, two technical challenges had to be overcome. Firstly, the use of tactile data that includes shear-induced slippage can lead to error-prone estimates unsuitable for accurate control, and so we modified the CNN into a Gaussian-density neural network and used a discriminative Bayesian filter to improve the predictions with a state dynamics model that utilizes the robot kinematics. Secondly, to achieve smooth robot motion in 3D space while interacting with objects, we used SE(3) velocity-based servo control, which required re-deriving the Bayesian filter update equations using Lie group theory, as many standard assumptions do not hold for state variables defined on non-Euclidean manifolds. In future, we believe that pose and shear-based tactile servoing will enable many object manipulation tasks and the fully-dexterous utilization of multi-fingered tactile robot hands. Video: https://www.youtube.com/watch?v=xVs4hd34ek0},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Lloyd, John and Lepora, Nathan F.},
	month = dec,
	year = {2023},
	note = {arXiv:2312.08411 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{van_hoorick_revealing_2022,
	title = {Revealing {Occlusions} {With} {4D} {Neural} {Fields}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Van_Hoorick_Revealing_Occlusions_With_4D_Neural_Fields_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-12-20},
	author = {Van Hoorick, Basile and Tendulkar, Purva and Surís, Dídac and Park, Dennis and Stent, Simon and Vondrick, Carl},
	year = {2022},
	pages = {3011--3021},
}

@misc{suris_vipergpt_2023,
	title = {{ViperGPT}: {Visual} {Inference} via {Python} {Execution} for {Reasoning}},
	shorttitle = {{ViperGPT}},
	url = {http://arxiv.org/abs/2303.08128},
	doi = {10.48550/arXiv.2303.08128},
	abstract = {Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks.},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Surís, Dídac and Menon, Sachit and Vondrick, Carl},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08128 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{di_palo_effectiveness_2023,
	title = {On the {Effectiveness} of {Retrieval}, {Alignment}, and {Replay} in {Manipulation}},
	url = {http://arxiv.org/abs/2312.12345},
	abstract = {Imitation learning with visual observations is notoriously inefficient when addressed with end-to-end behavioural cloning methods. In this paper, we explore an alternative paradigm which decomposes reasoning into three phases. First, a retrieval phase, which informs the robot what it can do with an object. Second, an alignment phase, which informs the robot where to interact with the object. And third, a replay phase, which informs the robot how to interact with the object. Through a series of real-world experiments on everyday tasks, such as grasping, pouring, and inserting objects, we show that this decomposition brings unprecedented learning efficiency, and effective inter- and intra-class generalisation. Videos are available at https://www.robot-learning.uk/retrieval-alignment-replay.},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Di Palo, Norman and Johns, Edward},
	month = dec,
	year = {2023},
	note = {arXiv:2312.12345 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ceola_lhmanip_2023,
	title = {{LHManip}: {A} {Dataset} for {Long}-{Horizon} {Language}-{Grounded} {Manipulation} {Tasks} in {Cluttered} {Tabletop} {Environments}},
	shorttitle = {{LHManip}},
	url = {http://arxiv.org/abs/2312.12036},
	abstract = {Instructing a robot to complete an everyday task within our homes has been a long-standing challenge for robotics. While recent progress in language-conditioned imitation learning and offline reinforcement learning has demonstrated impressive performance across a wide range of tasks, they are typically limited to short-horizon tasks -- not reflective of those a home robot would be expected to complete. While existing architectures have the potential to learn these desired behaviours, the lack of the necessary long-horizon, multi-step datasets for real robotic systems poses a significant challenge. To this end, we present the Long-Horizon Manipulation (LHManip) dataset comprising 200 episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks, including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset. The full LHManip dataset is made publicly available {\textbackslash}href\{https://github.com/fedeceola/LHManip\}\{here\}.},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Ceola, Federico and Natale, Lorenzo and Sünderhauf, Niko and Rana, Krishan},
	month = dec,
	year = {2023},
	note = {arXiv:2312.12036 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{barad_graspldm_2023,
	title = {{GraspLDM}: {Generative} 6-{DoF} {Grasp} {Synthesis} using {Latent} {Diffusion} {Models}},
	shorttitle = {{GraspLDM}},
	url = {http://arxiv.org/abs/2312.11243},
	doi = {10.48550/arXiv.2312.11243},
	abstract = {Vision-based grasping of unknown objects in unstructured environments is a key challenge for autonomous robotic manipulation. A practical grasp synthesis system is required to generate a diverse set of 6-DoF grasps from which a task-relevant grasp can be executed. Although generative models are suitable for learning such complex data distributions, existing models have limitations in grasp quality, long training times, and a lack of flexibility for task-specific generation. In this work, we present GraspLDM- a modular generative framework for 6-DoF grasp synthesis that uses diffusion models as priors in the latent space of a VAE. GraspLDM learns a generative model of object-centric \$SE(3)\$ grasp poses conditioned on point clouds. GraspLDM's architecture enables us to train task-specific models efficiently by only re-training a small de-noising network in the low-dimensional latent space, as opposed to existing models that need expensive re-training. Our framework provides robust and scalable models on both full and single-view point clouds. GraspLDM models trained with simulation data transfer well to the real world and provide an 80{\textbackslash}\% success rate for 80 grasp attempts of diverse test objects, improving over existing generative models. We make our implementation available at https://github.com/kuldeepbrd1/graspldm .},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Barad, Kuldeep R. and Orsula, Andrej and Richard, Antoine and Dentler, Jan and Olivares-Mendez, Miguel and Martinez, Carol},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11243 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zou_triplane_2023,
	title = {Triplane {Meets} {Gaussian} {Splatting}: {Fast} and {Generalizable} {Single}-{View} {3D} {Reconstruction} with {Transformers}},
	shorttitle = {Triplane {Meets} {Gaussian} {Splatting}},
	url = {http://arxiv.org/abs/2312.09147},
	doi = {10.48550/arXiv.2312.09147},
	abstract = {Recent advancements in 3D reconstruction from single images have been driven by the evolution of generative models. Prominent among these are methods based on Score Distillation Sampling (SDS) and the adaptation of diffusion models in the 3D domain. Despite their progress, these techniques often face limitations due to slow optimization or rendering processes, leading to extensive training and optimization times. In this paper, we introduce a novel approach for single-view reconstruction that efficiently generates a 3D model from a single image via feed-forward inference. Our method utilizes two transformer-based networks, namely a point decoder and a triplane decoder, to reconstruct 3D objects using a hybrid Triplane-Gaussian intermediate representation. This hybrid representation strikes a balance, achieving a faster rendering speed compared to implicit representations while simultaneously delivering superior rendering quality than explicit representations. The point decoder is designed for generating point clouds from single images, offering an explicit representation which is then utilized by the triplane decoder to query Gaussian features for each point. This design choice addresses the challenges associated with directly regressing explicit 3D Gaussian attributes characterized by their non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to enable rapid rendering through splatting. Both decoders are built upon a scalable, transformer-based architecture and have been efficiently trained on large-scale 3D datasets. The evaluations conducted on both synthetic datasets and real-world images demonstrate that our method not only achieves higher quality but also ensures a faster runtime in comparison to previous state-of-the-art techniques. Please see our project page at https://zouzx.github.io/TriplaneGaussian/.},
	urldate = {2023-12-20},
	publisher = {arXiv},
	author = {Zou, Zi-Xin and Yu, Zhipeng and Guo, Yuan-Chen and Li, Yangguang and Liang, Ding and Cao, Yan-Pei and Zhang, Song-Hai},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09147 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shi_zero123_2023,
	title = {Zero123++: a {Single} {Image} to {Consistent} {Multi}-view {Diffusion} {Base} {Model}},
	shorttitle = {Zero123++},
	url = {http://arxiv.org/abs/2310.15110},
	doi = {10.48550/arXiv.2310.15110},
	abstract = {We report Zero123++, an image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view. To take full advantage of pretrained 2D generative priors, we develop various conditioning and training schemes to minimize the effort of finetuning from off-the-shelf image diffusion models such as Stable Diffusion. Zero123++ excels in producing high-quality, consistent multi-view images from a single image, overcoming common issues like texture degradation and geometric misalignment. Furthermore, we showcase the feasibility of training a ControlNet on Zero123++ for enhanced control over the generation process. The code is available at https://github.com/SUDO-AI-3D/zero123plus.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Shi, Ruoxi and Chen, Hansheng and Zhang, Zhuoyang and Liu, Minghua and Xu, Chao and Wei, Xinyue and Chen, Linghao and Zeng, Chong and Su, Hao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15110 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{sargent_zeronvs_2023,
	title = {{ZeroNVS}: {Zero}-{Shot} 360-{Degree} {View} {Synthesis} from a {Single} {Real} {Image}},
	shorttitle = {{ZeroNVS}},
	url = {http://arxiv.org/abs/2310.17994},
	doi = {10.48550/arXiv.2310.17994},
	abstract = {We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view synthesis for in-the-wild scenes. While existing methods are designed for single objects with masked backgrounds, we propose new techniques to address challenges introduced by in-the-wild multi-object scenes with complex backgrounds. Specifically, we train a generative prior on a mixture of data sources that capture object-centric, indoor, and outdoor scenes. To address issues from data mixture such as depth-scale ambiguity, we propose a novel camera conditioning parameterization and normalization scheme. Further, we observe that Score Distillation Sampling (SDS) tends to truncate the distribution of complex backgrounds during distillation of 360-degree scenes, and propose "SDS anchoring" to improve the diversity of synthesized novel views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset in the zero-shot setting, even outperforming methods specifically trained on DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark for single-image novel view synthesis, and demonstrate strong performance in this setting. Our code and data are at http://kylesargent.github.io/zeronvs/},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Sargent, Kyle and Li, Zizhang and Shah, Tanmay and Herrmann, Charles and Yu, Hong-Xing and Zhang, Yunzhi and Chan, Eric Ryan and Lagun, Dmitry and Fei-Fei, Li and Sun, Deqing and Wu, Jiajun},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17994 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{wu_reconfusion_2023,
	title = {{ReconFusion}: {3D} {Reconstruction} with {Diffusion} {Priors}},
	shorttitle = {{ReconFusion}},
	url = {http://arxiv.org/abs/2312.02981},
	doi = {10.48550/arXiv.2312.02981},
	abstract = {3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Wu, Rundi and Mildenhall, Ben and Henzler, Philipp and Park, Keunhong and Gao, Ruiqi and Watson, Daniel and Srinivasan, Pratul P. and Verbin, Dor and Barron, Jonathan T. and Poole, Ben and Holynski, Aleksander},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02981 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_one-2-3-45_2023,
	title = {One-2-3-45++: {Fast} {Single} {Image} to {3D} {Objects} with {Consistent} {Multi}-{View} {Generation} and {3D} {Diffusion}},
	shorttitle = {One-2-3-45++},
	url = {http://arxiv.org/abs/2311.07885},
	doi = {10.48550/arXiv.2311.07885},
	abstract = {Recent advancements in open-world 3D object generation have been remarkable, with image-to-3D methods offering superior fine-grained control over their text-to-3D counterparts. However, most existing models fall short in simultaneously providing rapid generation speeds and high fidelity to input images - two features essential for practical applications. In this paper, we present One-2-3-45++, an innovative method that transforms a single image into a detailed 3D textured mesh in approximately one minute. Our approach aims to fully harness the extensive knowledge embedded in 2D diffusion models and priors from valuable yet limited 3D data. This is achieved by initially finetuning a 2D diffusion model for consistent multi-view image generation, followed by elevating these images to 3D with the aid of multi-view conditioned 3D native diffusion models. Extensive experimental evaluations demonstrate that our method can produce high-quality, diverse 3D assets that closely mirror the original input image. Our project webpage: https://sudo-ai-3d.github.io/One2345plus\_page.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Liu, Minghua and Shi, Ruoxi and Chen, Linghao and Zhang, Zhuoyang and Xu, Chao and Wei, Xinyue and Chen, Hansheng and Zeng, Chong and Gu, Jiayuan and Su, Hao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.07885 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{brooks_large_2023,
	title = {Large {Language} {Models} can {Implement} {Policy} {Iteration}},
	url = {http://arxiv.org/abs/2210.03821},
	doi = {10.48550/arXiv.2210.03821},
	abstract = {This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the "few-shot" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Brooks, Ethan and Walls, Logan and Lewis, Richard L. and Singh, Satinder},
	month = aug,
	year = {2023},
	note = {arXiv:2210.03821 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{scheikl_movement_2023,
	title = {Movement {Primitive} {Diffusion}: {Learning} {Gentle} {Robotic} {Manipulation} of {Deformable} {Objects}},
	shorttitle = {Movement {Primitive} {Diffusion}},
	url = {http://arxiv.org/abs/2312.10008},
	doi = {10.48550/arXiv.2312.10008},
	abstract = {Policy learning in robot-assisted surgery (RAS) lacks data efficient and versatile methods that exhibit the desired motion quality for delicate surgical interventions. To this end, we introduce Movement Primitive Diffusion (MPD), a novel method for imitation learning (IL) in RAS that focuses on gentle manipulation of deformable objects. The approach combines the versatility of diffusion-based imitation learning (DIL) with the high-quality motion generation capabilities of Probabilistic Dynamic Movement Primitives (ProDMPs). This combination enables MPD to achieve gentle manipulation of deformable objects, while maintaining data efficiency critical for RAS applications where demonstration data is scarce. We evaluate MPD across various simulated tasks and a real world robotic setup on both state and image observations. MPD outperforms state-of-the-art DIL methods in success rate, motion quality, and data efficiency.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Scheikl, Paul Maria and Schreiber, Nicolas and Haas, Christoph and Freymuth, Niklas and Neumann, Gerhard and Lioutikov, Rudolf and Mathis-Ullrich, Franziska},
	month = dec,
	year = {2023},
	note = {arXiv:2312.10008 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{soucek_genhowto_2023,
	title = {{GenHowTo}: {Learning} to {Generate} {Actions} and {State} {Transformations} from {Instructional} {Videos}},
	shorttitle = {{GenHowTo}},
	url = {http://arxiv.org/abs/2312.07322},
	doi = {10.48550/arXiv.2312.07322},
	abstract = {We address the task of generating temporally consistent and physically plausible images of actions and object state transformations. Given an input image and a text prompt describing the targeted transformation, our generated images preserve the environment and transform objects in the initial image. Our contributions are threefold. First, we leverage a large body of instructional videos and automatically mine a dataset of triplets of consecutive frames corresponding to initial object states, actions, and resulting object transformations. Second, equipped with this data, we develop and train a conditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a variety of objects and actions and show superior performance compared to existing methods. In particular, we introduce a quantitative evaluation where GenHowTo achieves 88\% and 74\% on seen and unseen interaction categories, respectively, outperforming prior work by a large margin.},
	urldate = {2023-12-16},
	publisher = {arXiv},
	author = {Souček, Tomáš and Damen, Dima and Wray, Michael and Laptev, Ivan and Sivic, Josef},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07322 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{luo_universal_2023,
	title = {Universal {Humanoid} {Motion} {Representations} for {Physics}-{Based} {Control}},
	url = {http://arxiv.org/abs/2310.04582},
	doi = {10.48550/arXiv.2310.04582},
	abstract = {We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high-dimensionality of humanoid control as well as the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a narrow range of movement styles (e.g. locomotion, game characters) from specialized motion datasets. This limited scope hampers its applicability in complex tasks. Our work closes this gap, significantly increasing the coverage of motion representation space. To achieve this, we first learn a motion imitator that can imitate all of human motion from a large, unstructured motion dataset. We then create our motion representation by distilling skills directly from the imitator. This is achieved using an encoder-decoder structure with a variational information bottleneck. Additionally, we jointly learn a prior conditioned on proprioception (humanoid's own pose and velocities) to improve model expressiveness and sampling efficiency for downstream tasks. Sampling from the prior, we can generate long, stable, and diverse human motions. Using this latent space for hierarchical RL, we show that our policies solve tasks using natural and realistic human behavior. We demonstrate the effectiveness of our motion representation by solving generative tasks (e.g. strike, terrain traversal) and motion tracking using VR controllers.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Luo, Zhengyi and Cao, Jinkun and Merel, Josh and Winkler, Alexander and Huang, Jing and Kitani, Kris and Xu, Weipeng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04582 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{li_controllable_2023,
	title = {Controllable {Human}-{Object} {Interaction} {Synthesis}},
	url = {http://arxiv.org/abs/2312.03913},
	doi = {10.48550/arXiv.2312.03913},
	abstract = {Synthesizing semantic-aware, long-horizon, human-object interaction is critical to simulate realistic human behaviors. In this work, we address the challenging problem of generating synchronized object motion and human motion guided by language descriptions in 3D scenes. We propose Controllable Human-Object Interaction Synthesis (CHOIS), an approach that generates object motion and human motion simultaneously using a conditional diffusion model given a language description, initial object and human states, and sparse object waypoints. While language descriptions inform style and intent, waypoints ground the motion in the scene and can be effectively extracted using high-level planning methods. Naively applying a diffusion model fails to predict object motion aligned with the input waypoints and cannot ensure the realism of interactions that require precise hand-object contact and appropriate contact grounded by the floor. To overcome these problems, we introduce an object geometry loss as additional supervision to improve the matching between generated object motion and input object waypoints. In addition, we design guidance terms to enforce contact constraints during the sampling process of the trained diffusion model.},
	urldate = {2023-12-09},
	publisher = {arXiv},
	author = {Li, Jiaman and Clegg, Alexander and Mottaghi, Roozbeh and Wu, Jiajun and Puig, Xavier and Liu, C. Karen},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03913 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ehsani_imitating_2023,
	title = {Imitating {Shortest} {Paths} in {Simulation} {Enables} {Effective} {Navigation} and {Manipulation} in the {Real} {World}},
	url = {http://arxiv.org/abs/2312.02976},
	doi = {10.48550/arXiv.2312.02976},
	abstract = {Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets. Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Ehsani, Kiana and Gupta, Tanmay and Hendrix, Rose and Salvador, Jordi and Weihs, Luca and Zeng, Kuo-Hao and Singh, Kunal Pratap and Kim, Yejin and Han, Winson and Herrasti, Alvaro and Krishna, Ranjay and Schwenk, Dustin and VanderBilt, Eli and Kembhavi, Aniruddha},
	month = dec,
	year = {2023},
	note = {arXiv:2312.02976 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{wan_diffusionphase_2023,
	title = {{DiffusionPhase}: {Motion} {Diffusion} in {Frequency} {Domain}},
	shorttitle = {{DiffusionPhase}},
	url = {http://arxiv.org/abs/2312.04036},
	doi = {10.48550/arXiv.2312.04036},
	abstract = {In this study, we introduce a learning-based method for generating high-quality human motion sequences from text descriptions (e.g., ``A person walks forward"). Existing techniques struggle with motion diversity and smooth transitions in generating arbitrary-length motion sequences, due to limited text-to-motion datasets and the pose representations used that often lack expressiveness or compactness. To address these issues, we propose the first method for text-conditioned human motion generation in the frequency domain of motions. We develop a network encoder that converts the motion space into a compact yet expressive parameterized phase space with high-frequency details encoded, capturing the local periodicity of motions in time and space with high accuracy. We also introduce a conditional diffusion model for predicting periodic motion parameters based on text descriptions and a start pose, efficiently achieving smooth transitions between motion sequences associated with different text descriptions. Experiments demonstrate that our approach outperforms current methods in generating a broader variety of high-quality motions, and synthesizing long sequences with natural transitions.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Wan, Weilin and Huang, Yiming and Wu, Shutong and Komura, Taku and Wang, Wenping and Jayaraman, Dinesh and Liu, Lingjie},
	month = dec,
	year = {2023},
	note = {arXiv:2312.04036 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wang_physhoi_2023,
	title = {{PhysHOI}: {Physics}-{Based} {Imitation} of {Dynamic} {Human}-{Object} {Interaction}},
	shorttitle = {{PhysHOI}},
	url = {http://arxiv.org/abs/2312.04393},
	doi = {10.48550/arXiv.2312.04393},
	abstract = {Humans interact with objects all the time. Enabling a humanoid to learn human-object interaction (HOI) is a key step for future smart animation and intelligent robotics systems. However, recent progress in physics-based HOI requires carefully designed task-specific rewards, making the system unscalable and labor-intensive. This work focuses on dynamic HOI imitation: teaching humanoid dynamic interaction skills through imitating kinematic HOI demonstrations. It is quite challenging because of the complexity of the interaction between body parts and objects and the lack of dynamic HOI data. To handle the above issues, we present PhysHOI, the first physics-based whole-body HOI imitation approach without task-specific reward designs. Except for the kinematic HOI representations of humans and objects, we introduce the contact graph to model the contact relations between body parts and objects explicitly. A contact graph reward is also designed, which proved to be critical for precise HOI imitation. Based on the key designs, PhysHOI can imitate diverse HOI tasks simply yet effectively without prior knowledge. To make up for the lack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset that contains eight whole-body basketball skills. We validate PhysHOI on diverse HOI tasks, including whole-body grasping and basketball skills.},
	urldate = {2023-12-08},
	publisher = {arXiv},
	author = {Wang, Yinhuai and Lin, Jing and Zeng, Ailing and Luo, Zhengyi and Zhang, Jian and Zhang, Lei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.04393 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@misc{li_uni3dl_2023,
	title = {{Uni3DL}: {Unified} {Model} for {3D} and {Language} {Understanding}},
	shorttitle = {{Uni3DL}},
	url = {http://arxiv.org/abs/2312.03026},
	doi = {10.48550/arXiv.2312.03026},
	abstract = {In this work, we present Uni3DL, a unified model for 3D and Language understanding. Distinct from existing unified vision-language models in 3D which are limited in task variety and predominantly dependent on projected multi-view images, Uni3DL operates directly on point clouds. This approach significantly expands the range of supported tasks in 3D, encompassing both vision and vision-language tasks in 3D. At the core of Uni3DL, a query transformer is designed to learn task-agnostic semantic and mask outputs by attending to 3D visual features, and a task router is employed to selectively generate task-specific outputs required for diverse tasks. With a unified architecture, our Uni3DL model enjoys seamless task decomposition and substantial parameter sharing across tasks. Uni3DL has been rigorously evaluated across diverse 3D vision-language understanding tasks, including semantic segmentation, object detection, instance segmentation, visual grounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates performance on par with or surpassing state-of-the-art (SOTA) task-specific models. We hope our benchmark and Uni3DL model will serve as a solid step to ease future research in unified models in the realm of 3D and language understanding. Project page: https://uni3dl.github.io.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Li, Xiang and Ding, Jian and Chen, Zhaoyang and Elhoseiny, Mohamed},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03026 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{yokoyama_vlfm_2023,
	title = {{VLFM}: {Vision}-{Language} {Frontier} {Maps} for {Zero}-{Shot} {Semantic} {Navigation}},
	shorttitle = {{VLFM}},
	url = {http://arxiv.org/abs/2312.03275},
	doi = {10.48550/arXiv.2312.03275},
	abstract = {Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Yokoyama, Naoki and Ha, Sehoon and Batra, Dhruv and Wang, Jiuguang and Bucher, Bernadette},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03275 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@article{ma_combining_2022,
	title = {Combining {Learning}-{Based} {Locomotion} {Policy} {With} {Model}-{Based} {Manipulation} for {Legged} {Mobile} {Manipulators}},
	volume = {7},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/9684679},
	doi = {10.1109/LRA.2022.3143567},
	abstract = {Deep reinforcement learning produces robust locomotion policies for legged robots over challenging terrains. To date, few studies have leveraged model-based methods to combine these locomotion skills with the precise control of manipulators. Here, we incorporate external dynamics plans into learning-based locomotion policies for mobile manipulation. We train the base policy by applying a random wrench sequence on the robot base in simulation and add the noisified wrench sequence prediction to the policy observations. The policy then learns to counteract the partially-known future disturbance. The random wrench sequences are replaced with the wrench prediction generated with the dynamics plans from model predictive control to enable deployment. We show zero-shot adaptation for manipulators unseen during training. On the hardware, we demonstrate stable locomotion of legged robots with the prediction of the external wrench.},
	number = {2},
	urldate = {2023-12-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Ma, Yuntao and Farshidian, Farbod and Miki, Takahiro and Lee, Joonho and Hutter, Marco},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	pages = {2377--2384},
}

@misc{fan_hold_2023,
	title = {{HOLD}: {Category}-agnostic {3D} {Reconstruction} of {Interacting} {Hands} and {Objects} from {Video}},
	shorttitle = {{HOLD}},
	url = {http://arxiv.org/abs/2311.18448},
	doi = {10.48550/arXiv.2311.18448},
	abstract = {Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold},
	urldate = {2023-12-06},
	publisher = {arXiv},
	author = {Fan, Zicong and Parelli, Maria and Kadoglou, Maria Eleni and Kocabas, Muhammed and Chen, Xu and Black, Michael J. and Hilliges, Otmar},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18448 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yonetani_path_2021,
	title = {Path {Planning} using {Neural} {A}* {Search}},
	url = {https://proceedings.mlr.press/v139/yonetani21a.html},
	abstract = {We present Neural A*, a novel data-driven search method for path planning problems. Despite the recent increasing attention to data-driven path planning, machine learning approaches to search-based planning are still challenging due to the discrete nature of search algorithms. In this work, we reformulate a canonical A* search algorithm to be differentiable and couple it with a convolutional encoder to form an end-to-end trainable neural network planner. Neural A* solves a path planning problem by encoding a problem instance to a guidance map and then performing the differentiable A* search with the guidance map. By learning to match the search results with ground-truth paths provided by experts, Neural A* can produce a path consistent with the ground truth accurately and efficiently. Our extensive experiments confirmed that Neural A* outperformed state-of-the-art data-driven planners in terms of the search optimality and efficiency trade-off. Furthermore, Neural A* successfully predicted realistic human trajectories by directly performing search-based planning on natural image inputs.},
	language = {en},
	urldate = {2023-11-13},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yonetani, Ryo and Taniai, Tatsunori and Barekatain, Mohammadamin and Nishimura, Mai and Kanezaki, Asako},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12029--12039},
}

@misc{geng_sage_2023,
	title = {{SAGE}: {Bridging} {Semantic} and {Actionable} {Parts} for {GEneralizable} {Articulated}-{Object} {Manipulation} under {Language} {Instructions}},
	shorttitle = {{SAGE}},
	url = {http://arxiv.org/abs/2312.01307},
	doi = {10.48550/arXiv.2312.01307},
	abstract = {Generalizable manipulation of articulated objects remains a challenging problem in many real-world scenarios, given the diverse object structures, functionalities, and goals. In these tasks, both semantic interpretations and physical plausibilities are crucial for a policy to succeed. To address this problem, we propose SAGE, a novel framework that bridges the understanding of semantic and actionable parts of articulated objects to achieve generalizable manipulation under language instructions. Given a manipulation goal specified by natural language, an instruction interpreter with Large Language Models (LLMs) first translates them into programmatic actions on the object's semantic parts. This process also involves a scene context parser for understanding the visual inputs, which is designed to generate scene descriptions with both rich information and accurate interaction-related facts by joining the forces of generalist Visual-Language Models (VLMs) and domain-specialist part perception models. To further convert the action programs into executable policies, a part grounding module then maps the object semantic parts suggested by the instruction interpreter into so-called Generalizable Actionable Parts (GAParts). Finally, an interactive feedback module is incorporated to respond to failures, which greatly increases the robustness of the overall framework. Experiments both in simulation environments and on real robots show that our framework can handle a large variety of articulated objects with diverse language-instructed goals. We also provide a new benchmark for language-guided articulated-object manipulation in realistic scenarios.},
	urldate = {2023-12-05},
	publisher = {arXiv},
	author = {Geng, Haoran and Wei, Songlin and Deng, Congyue and Shen, Bokui and Wang, He and Guibas, Leonidas},
	month = dec,
	year = {2023},
	note = {arXiv:2312.01307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{chang_investigation_2023,
	title = {An {Investigation} of {Multi}-feature {Extraction} and {Super}-resolution with {Fast} {Microphone} {Arrays}},
	url = {http://arxiv.org/abs/2310.00206},
	doi = {10.48550/arXiv.2310.00206},
	abstract = {In this work, we use MEMS microphones as vibration sensors to simultaneously classify texture and estimate contact position and velocity. Vibration sensors are an important facet of both human and robotic tactile sensing, providing fast detection of contact and onset of slip. Microphones are an attractive option for implementing vibration sensing as they offer a fast response and can be sampled quickly, are affordable, and occupy a very small footprint. Our prototype sensor uses only a sparse array of distributed MEMS microphones (8-9 mm spacing) embedded under an elastomer. We use transformer-based architectures for data analysis, taking advantage of the microphones' high sampling rate to run our models on time-series data as opposed to individual snapshots. This approach allows us to obtain 77.3\% average accuracy on 4-class texture classification (84.2\% when excluding the slowest drag velocity), 1.5 mm median error on contact localization, and 4.5 mm/s median error on contact velocity. We show that the learned texture and localization models are robust to varying velocity and generalize to unseen velocities. We also report that our sensor provides fast contact detection, an important advantage of fast transducers. This investigation illustrates the capabilities one can achieve with a MEMS microphone array alone, leaving valuable sensor real estate available for integration with complementary tactile sensing modalities.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Chang, Eric T. and Wang, Runsheng and Ballentine, Peter and Xu, Jingxi and Smith, Trey and Coltin, Brian and Kymissis, Ioannis and Ciocarlie, Matei},
	month = sep,
	year = {2023},
	note = {arXiv:2310.00206 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{siddiqui_meshgpt_2023,
	title = {{MeshGPT}: {Generating} {Triangle} {Meshes} with {Decoder}-{Only} {Transformers}},
	shorttitle = {{MeshGPT}},
	url = {http://arxiv.org/abs/2311.15475},
	doi = {10.48550/arXiv.2311.15475},
	abstract = {We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9\% increase in shape coverage and a 30-point enhancement in FID scores across various categories.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Siddiqui, Yawar and Alliegro, Antonio and Artemov, Alexey and Tommasi, Tatiana and Sirigatti, Daniele and Rosov, Vladislav and Dai, Angela and Nießner, Matthias},
	month = nov,
	year = {2023},
	note = {arXiv:2311.15475 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{hinterstoisser_model_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Model {Based} {Training}, {Detection} and {Pose} {Estimation} of {Texture}-{Less} {3D} {Objects} in {Heavily} {Cluttered} {Scenes}},
	isbn = {978-3-642-37331-2},
	doi = {10.1007/978-3-642-37331-2_42},
	abstract = {We propose a framework for automatic modeling, detection, and tracking of 3D objects with a Kinect. The detection part is mainly based on the recent template-based LINEMOD approach [1] for object detection. We show how to build the templates automatically from 3D models, and how to estimate the 6 degrees-of-freedom pose accurately and in real-time. The pose estimation and the color information allow us to check the detection hypotheses and improves the correct detection rate by 13\% with respect to the original LINEMOD. These many improvements make our framework suitable for object manipulation in Robotics applications. Moreover we propose a new dataset made of 15 registered, 1100+ frame video sequences of 15 various objects for the evaluation of future competing methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer},
	author = {Hinterstoisser, Stefan and Lepetit, Vincent and Ilic, Slobodan and Holzer, Stefan and Bradski, Gary and Konolige, Kurt and Navab, Nassir},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	keywords = {Cluttered Scene, Color Gradient, Correct Detection Rate, Object Detection, Object Projection},
	pages = {548--562},
}

@misc{bharadhwaj_towards_2023,
	title = {Towards {Generalizable} {Zero}-{Shot} {Manipulation} via {Translating} {Human} {Interaction} {Plans}},
	url = {http://arxiv.org/abs/2312.00775},
	doi = {10.48550/arXiv.2312.00775},
	abstract = {We pursue the goal of developing robots that can interact zero-shot with generic unseen objects via a diverse repertoire of manipulation skills and show how passive human videos can serve as a rich source of data for learning such generalist robots. Unlike typical robot learning approaches which directly learn how a robot should act from interaction data, we adopt a factorized approach that can leverage large-scale human videos to learn how a human would accomplish a desired task (a human plan), followed by translating this plan to the robots embodiment. Specifically, we learn a human plan predictor that, given a current image of a scene and a goal image, predicts the future hand and object configurations. We combine this with a translation module that learns a plan-conditioned robot manipulation policy, and allows following humans plans for generic manipulation tasks in a zero-shot manner with no deployment-time training. Importantly, while the plan predictor can leverage large-scale human videos for learning, the translation module only requires a small amount of in-domain data, and can generalize to tasks not seen during training. We show that our learned system can perform over 16 manipulation skills that generalize to 40 objects, encompassing 100 real-world tasks for table-top manipulation and diverse in-the-wild manipulation. https://homangab.github.io/hopman/},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Bharadhwaj, Homanga and Gupta, Abhinav and Kumar, Vikash and Tulsiani, Shubham},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00775 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{zhu_survey_2022,
	title = {A {Survey} on {Deep} {Graph} {Generation}: {Methods} and {Applications}},
	shorttitle = {A {Survey} on {Deep} {Graph} {Generation}},
	url = {https://proceedings.mlr.press/v198/zhu22a.html},
	abstract = {Graphs are ubiquitous in encoding relational information of real-world objects in many domains. Graph generation, whose purpose is to generate new graphs from a distribution similar to the observed graphs, has received increasing attention thanks to the recent advances of deep learning models. In this paper, we conduct a comprehensive review on the existing literature of deep graph generation from a variety of emerging methods to its wide application areas. Specifically, we first formulate the problem of deep graph generation and discuss its difference with several related graph learning tasks. Secondly, we divide the state-of-the-art methods into three categories based on model architectures and summarize their generation strategies. Thirdly, we introduce three key application areas of deep graph generation. Lastly, we highlight challenges and opportunities in the future study of deep graph generation. We hope that our survey will be useful for researchers and practitioners who are interested in this exciting and rapidly-developing field.},
	language = {en},
	urldate = {2023-12-03},
	booktitle = {Proceedings of the {First} {Learning} on {Graphs} {Conference}},
	publisher = {PMLR},
	author = {Zhu, Yanqiao and Du, Yuanqi and Wang, Yinkai and Xu, Yichen and Zhang, Jieyu and Liu, Qiang and Wu, Shu},
	month = dec,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {47:1--47:21},
}

@inproceedings{allshire_laser_2021,
	title = {{LASER}: {Learning} a {Latent} {Action} {Space} for {Efficient} {Reinforcement} {Learning}},
	shorttitle = {{LASER}},
	url = {https://ieeexplore.ieee.org/document/9561232},
	doi = {10.1109/ICRA48506.2021.9561232},
	abstract = {The process of learning a manipulation task depends strongly on the action space used for exploration: posed in the incorrect action space, solving a task with reinforcement learning can be drastically inefficient. Additionally, similar tasks or instances of the same task family impose latent manifold constraints on the most effective action space: the task family can be best solved with actions in a manifold of the entire action space of the robot. Combining these insights we present LASER, a method to learn latent action spaces for efficient reinforcement learning. LASER factorizes the learning problem into two sub-problems, namely action space learning and policy learning in the new action space. It leverages data from similar manipulation task instances, either from an offline expert or online during policy learning, and learns from these trajectories a mapping from the original to a latent action space. LASER is trained as a variational encoder-decoder model to map raw actions into a disentangled latent action space while maintaining action reconstruction and latent space dynamic consistency. We evaluate LASER on two contact-rich robotic tasks in simulation, and analyze the benefit of policy learning in the generated latent action space. We show improved sample efficiency compared to the original action space from better alignment of the action space to the task space, as we observe with visualizations of the learned action space manifold. Additional details: pair.toronto.edu/laser},
	urldate = {2023-11-30},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Allshire, Arthur and Martín-Martín, Roberto and Lin, Charles and Manuel, Shawn and Savarese, Silvio and Garg, Animesh},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	pages = {6650--6656},
}

@inproceedings{losey_controlling_2020,
	title = {Controlling {Assistive} {Robots} with {Learned} {Latent} {Actions}},
	url = {https://ieeexplore.ieee.org/abstract/document/9197197?casa_token=U0grtgrguAsAAAAA:k5cHJGGXK23IX48_Kx4N75Uw4-K_hgsRn-JoJYw_QujY2jLnKNyDdYlhgGo8n0kTuf2Xz_uZkLY},
	doi = {10.1109/ICRA40945.2020.9197197},
	abstract = {Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful also makes them challenging to teleoperate: the robot has more degrees-of-freedom than the human can directly coordinate with a handheld joystick. Our insight is that we can make assistive robots easier for humans to control by leveraging latent actions. Latent actions provide a lowdimensional embedding of high-dimensional robot behavior: for example, one latent dimension might guide the assistive arm along a pouring motion. In this paper, we design a teleoperation algorithm for assistive robots that learns latent actions from task demonstrations. We formulate the controllability, consistency, and scaling properties that user-friendly latent actions should have, and evaluate how different lowdimensional embeddings capture these properties. Finally, we conduct two user studies on a robotic arm to compare our latent action approach to both state-of-the-art shared autonomy baselines and a teleoperation strategy currently used by assistive arms. Participants completed assistive eating and cooking tasks more efficiently when leveraging our latent actions, and also subjectively reported that latent actions made the task easier to perform. The video accompanying this paper can be found at: https://youtu.be/wjnhrzugBj4.},
	urldate = {2023-11-30},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Losey, Dylan P. and Srinivasan, Krishnan and Mandlekar, Ajay and Garg, Animesh and Sadigh, Dorsa},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	pages = {378--384},
}

@article{zeng_lion_2022,
	title = {{LION}: {Latent} {Point} {Diffusion} {Models} for {3D} {Shape} {Generation}},
	volume = {35},
	shorttitle = {{LION}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/40e56dabe12095a5fc44a6e4c3835948-Abstract-Conference.html},
	language = {en},
	urldate = {2023-11-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zeng, Xiaohui and Vahdat, Arash and Williams, Francis and Gojcic, Zan and Litany, Or and Fidler, Sanja and Kreis, Karsten},
	month = dec,
	year = {2022},
	pages = {10021--10039},
}

@article{chen_visual_2023,
	title = {Visual dexterity: {In}-hand reorientation of novel and complex object shapes},
	volume = {8},
	shorttitle = {Visual dexterity},
	url = {https://www.science.org/doi/10.1126/scirobotics.adc9244},
	doi = {10.1126/scirobotics.adc9244},
	abstract = {In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments, which remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following conditions: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasi-static manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints that make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real time, with the median reorientation time being close to 7 seconds. The controller was trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the most challenging scenario of reorienting objects held in the air by a downward-facing hand that must counteract gravity during reorientation. Our hardware platform only used open-source components that cost less than 5000 dollars. Although we demonstrate the ability to overcome assumptions in prior work, there is ample scope for improving absolute performance. For instance, the challenging duck-shaped object not used for training was dropped in 56\% of the trials. When it was not dropped, our controller reoriented the object within 0.4 radians (23°) 75\% of the time.},
	number = {84},
	urldate = {2023-11-30},
	journal = {Science Robotics},
	author = {Chen, Tao and Tippur, Megha and Wu, Siyang and Kumar, Vikash and Adelson, Edward and Agrawal, Pulkit},
	month = nov,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadc9244},
}

@misc{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{cohen_steerable_2016,
	title = {Steerable {CNNs}},
	url = {http://arxiv.org/abs/1612.08498},
	doi = {10.48550/arXiv.1612.08498},
	abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Cohen, Taco S. and Welling, Max},
	month = dec,
	year = {2016},
	note = {arXiv:1612.08498 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhou_vlprompt_2023,
	title = {{VLPrompt}: {Vision}-{Language} {Prompting} for {Panoptic} {Scene} {Graph} {Generation}},
	shorttitle = {{VLPrompt}},
	url = {http://arxiv.org/abs/2311.16492},
	doi = {10.48550/arXiv.2311.16492},
	abstract = {Panoptic Scene Graph Generation (PSG) aims at achieving a comprehensive image understanding by simultaneously segmenting objects and predicting relations among objects. However, the long-tail problem among relations leads to unsatisfactory results in real-world applications. Prior methods predominantly rely on vision information or utilize limited language information, such as object or relation names, thereby overlooking the utility of language information. Leveraging the recent progress in Large Language Models (LLMs), we propose to use language information to assist relation prediction, particularly for rare relations. To this end, we propose the Vision-Language Prompting (VLPrompt) model, which acquires vision information from images and language information from LLMs. Then, through a prompter network based on attention mechanism, it achieves precise relation prediction. Our extensive experiments show that VLPrompt significantly outperforms previous state-of-the-art methods on the PSG dataset, proving the effectiveness of incorporating language information and alleviating the long-tail problem of relations.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Zhou, Zijian and Shi, Miaojing and Caesar, Holger},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16492 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_handypriors_2023,
	title = {{HandyPriors}: {Physically} {Consistent} {Perception} of {Hand}-{Object} {Interactions} with {Differentiable} {Priors}},
	shorttitle = {{HandyPriors}},
	url = {http://arxiv.org/abs/2311.16552},
	doi = {10.48550/arXiv.2311.16552},
	abstract = {Various heuristic objectives for modeling hand-object interaction have been proposed in past work. However, due to the lack of a cohesive framework, these objectives often possess a narrow scope of applicability and are limited by their efficiency or accuracy. In this paper, we propose HandyPriors, a unified and general pipeline for pose estimation in human-object interaction scenes by leveraging recent advances in differentiable physics and rendering. Our approach employs rendering priors to align with input images and segmentation masks along with physics priors to mitigate penetration and relative-sliding across frames. Furthermore, we present two alternatives for hand and object pose estimation. The optimization-based pose estimation achieves higher accuracy, while the filtering-based tracking, which utilizes the differentiable priors as dynamics and observation models, executes faster. We demonstrate that HandyPriors attains comparable or superior results in the pose estimation task, and that the differentiable physics module can predict contact information for pose refinement. We also show that our approach generalizes to perception tasks, including robotic hand manipulation and human-object pose estimation in the wild.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Zhang, Shutong and Qiao, Yi-Ling and Zhu, Guanglei and Heiden, Eric and Turpin, Dylan and Liu, Jingzhou and Lin, Ming and Macklin, Miles and Garg, Animesh},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16552 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{liu_rgbgrasp_2023,
	title = {{RGBGrasp}: {Image}-based {Object} {Grasping} by {Capturing} {Multiple} {Views} during {Robot} {Arm} {Movement} with {Neural} {Radiance} {Fields}},
	shorttitle = {{RGBGrasp}},
	url = {http://arxiv.org/abs/2311.16592},
	doi = {10.48550/arXiv.2311.16592},
	abstract = {Robotic research encounters a significant hurdle when it comes to the intricate task of grasping objects that come in various shapes, materials, and textures. Unlike many prior investigations that heavily leaned on specialized point-cloud cameras or abundant RGB visual data to gather 3D insights for object-grasping missions, this paper introduces a pioneering approach called RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D surroundings containing transparent and specular objects and achieve accurate grasping. Our method utilizes pre-trained depth prediction models to establish geometry constraints, enabling precise 3D structure estimation, even under limited view conditions. Finally, we integrate hash encoding and a proposal sampler strategy to significantly accelerate the 3D reconstruction process. These innovations significantly enhance the adaptability and effectiveness of our algorithm in real-world scenarios. Through comprehensive experimental validation, we demonstrate that RGBGrasp achieves remarkable success across a wide spectrum of object-grasping scenarios, establishing it as a promising solution for real-world robotic manipulation tasks. The demo of our method can be found on: https://sites.google.com/view/rgbgrasp},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Liu, Chang and Shi, Kejian and Zhou, Kaichen and Wang, Haoxiao and Zhang, Jiyao and Dong, Hao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16592 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{lu_ugg_2023,
	title = {{UGG}: {Unified} {Generative} {Grasping}},
	shorttitle = {{UGG}},
	url = {http://arxiv.org/abs/2311.16917},
	doi = {10.48550/arXiv.2311.16917},
	abstract = {Dexterous grasping aims to produce diverse grasping postures with a high grasping success rate. Regression-based methods that directly predict grasping parameters given the object may achieve a high success rate but often lack diversity. Generation-based methods that generate grasping postures conditioned on the object can often produce diverse grasping, but they are insufficient for high grasping success due to lack of discriminative information. To mitigate, we introduce a unified diffusion-based dexterous grasp generation model, dubbed the name UGG, which operates within the object point cloud and hand parameter spaces. Our all-transformer architecture unifies the information from the object, the hand, and the contacts, introducing a novel representation of contact points for improved contact modeling. The flexibility and quality of our model enable the integration of a lightweight discriminator, benefiting from simulated discriminative data, which pushes for a high success rate while preserving high diversity. Beyond grasp generation, our model can also generate objects based on hand information, offering valuable insights into object design and studying how the generative model perceives objects. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research. Our project page is https://jiaxin-lu.github.io/ugg/ .},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Lu, Jiaxin and Kang, Hao and Li, Haoxiang and Liu, Bo and Yang, Yiding and Huang, Qixing and Hua, Gang},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16917 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{diller_cg-hoi_2023,
	title = {{CG}-{HOI}: {Contact}-{Guided} {3D} {Human}-{Object} {Interaction} {Generation}},
	shorttitle = {{CG}-{HOI}},
	url = {http://arxiv.org/abs/2311.16097},
	doi = {10.48550/arXiv.2311.16097},
	abstract = {We propose CG-HOI, the first method to address the task of generating dynamic 3D human-object interactions (HOIs) from text. We model the motion of both human and object in an interdependent fashion, as semantically rich human motion rarely happens in isolation without any interactions. Our key insight is that explicitly modeling contact between the human body surface and object geometry can be used as strong proxy guidance, both during training and inference. Using this guidance to bridge human and object motion enables generating more realistic and physically plausible interaction sequences, where the human body and corresponding object move in a coherent manner. Our method first learns to model human motion, object motion, and contact in a joint diffusion process, inter-correlated through cross-attention. We then leverage this learned contact for guidance during inference synthesis of realistic, coherent HOIs. Extensive evaluation shows that our joint contact-based human-object interaction approach generates realistic and physically plausible sequences, and we show two applications highlighting the capabilities of our method. Conditioned on a given object trajectory, we can generate the corresponding human motion without re-training, demonstrating strong human-object interdependency learning. Our approach is also flexible, and can be applied to static real-world 3D scene scans.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Diller, Christian and Dai, Angela},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16097 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.4.8, I.5.1, I.5.4},
}

@misc{wang_gs-pose_2023,
	title = {{GS}-{Pose}: {Category}-{Level} {Object} {Pose} {Estimation} via {Geometric} and {Semantic} {Correspondence}},
	shorttitle = {{GS}-{Pose}},
	url = {http://arxiv.org/abs/2311.13777},
	doi = {10.48550/arXiv.2311.13777},
	abstract = {Category-level pose estimation is a challenging task with many potential applications in computer vision and robotics. Recently, deep-learning-based approaches have made great progress, but are typically hindered by the need for large datasets of either pose-labelled real images or carefully tuned photorealistic simulators. This can be avoided by using only geometry inputs such as depth images to reduce the domain-gap but these approaches suffer from a lack of semantic information, which can be vital in the pose estimation problem. To resolve this conflict, we propose to utilize both geometric and semantic features obtained from a pre-trained foundation model.Our approach projects 2D features from this foundation model into 3D for a single object model per category, and then performs matching against this for new single view observations of unseen object instances with a trained matching network. This requires significantly less data to train than prior methods since the semantic features are robust to object texture and appearance. We demonstrate this with a rich evaluation, showing improved performance over prior methods with a fraction of the data required.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Wang, Pengyuan and Ikeda, Takuya and Lee, Robert and Nishiwaki, Koichi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fu_hacd_2023,
	title = {{HACD}: {Hand}-{Aware} {Conditional} {Diffusion} for {Monocular} {Hand}-{Held} {Object} {Reconstruction}},
	shorttitle = {{HACD}},
	url = {http://arxiv.org/abs/2311.14189},
	doi = {10.48550/arXiv.2311.14189},
	abstract = {Reconstructing hand-held objects from a single RGB image without known 3D object templates, category prior, or depth information is a vital yet challenging problem in computer vision. In contrast to prior works that utilize deterministic modeling paradigms, which make it hard to account for the uncertainties introduced by hand- and self-occlusion, we employ a probabilistic point cloud denoising diffusion model to tackle the above challenge. In this work, we present Hand-Aware Conditional Diffusion for monocular hand-held object reconstruction (HACD), modeling the hand-object interaction in two aspects. First, we introduce hand-aware conditioning to model hand-object interaction from both semantic and geometric perspectives. Specifically, a unified hand-object semantic embedding compensates for the 2D local feature deficiency induced by hand occlusion, and a hand articulation embedding further encodes the relationship between object vertices and hand joints. Second, we propose a hand-constrained centroid fixing scheme, which utilizes hand vertices priors to restrict the centroid deviation of partially denoised point cloud during diffusion and reverse process. Removing the centroid bias interference allows the diffusion models to focus on the reconstruction of shape, thus enhancing the stability and precision of local feature projection. Experiments on the synthetic ObMan dataset and two real-world datasets, HO3D and MOW, demonstrate our approach surpasses all existing methods by a large margin.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Fu, Bowen and Di, Yan and Zhang, Chenyangguang and Wang, Gu and Huang, Ziqin and Leng, Zhiying and Manhardt, Fabian and Ji, Xiangyang and Tombari, Federico},
	month = nov,
	year = {2023},
	note = {arXiv:2311.14189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tang_rftrans_2023,
	title = {{RFTrans}: {Leveraging} {Refractive} {Flow} of {Transparent} {Objects} for {Surface} {Normal} {Estimation} and {Manipulation}},
	shorttitle = {{RFTrans}},
	url = {http://arxiv.org/abs/2311.12398},
	doi = {10.48550/arXiv.2311.12398},
	abstract = {Transparent objects are widely used in our daily lives, making it important to teach robots to interact with them. However, it's not easy because the reflective and refractive effects can make RGB-D cameras fail to give accurate geometry measurements. To solve this problem, this paper introduces RFTrans, an RGB-D-based method for surface normal estimation and manipulation of transparent objects. By leveraging refractive flow as an intermediate representation, RFTrans circumvents the drawbacks of directly predicting the geometry (e.g. surface normal) from RGB images and helps bridge the sim-to-real gap. RFTrans integrates the RFNet, which predicts refractive flow, object mask, and boundaries, followed by the F2Net, which estimates surface normal from the refractive flow. To make manipulation possible, a global optimization module will take in the predictions, refine the raw depth, and construct the point cloud with normal. An analytical grasp planning algorithm, ISF, is followed to generate the grasp poses. We build a synthetic dataset with physically plausible ray-tracing rendering techniques to train the networks. Results show that the RFTrans trained on the synthetic dataset can consistently outperform the baseline ClearGrasp in both synthetic and real-world benchmarks by a large margin. Finally, a real-world robot grasping task witnesses an 83\% success rate, proving that refractive flow can help enable direct sim-to-real transfer. The code, data, and supplementary materials are available at https://rftrans.robotflow.ai.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Tang, Tutian and Liu, Jiyu and Zhang, Jieyi and Fu, Haoyuan and Xu, Wenqiang and Lu, Cewu},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12398 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{comi_touchsdf_2023,
	title = {{TouchSDF}: {A} {DeepSDF} {Approach} for {3D} {Shape} {Reconstruction} using {Vision}-{Based} {Tactile} {Sensing}},
	shorttitle = {{TouchSDF}},
	url = {http://arxiv.org/abs/2311.12602},
	doi = {10.48550/arXiv.2311.12602},
	abstract = {Humans rely on their visual and tactile senses to develop a comprehensive 3D understanding of their physical environment. Recently, there has been a growing interest in exploring and manipulating objects using data-driven approaches that utilise high-resolution vision-based tactile sensors. However, 3D shape reconstruction using tactile sensing has lagged behind visual shape reconstruction because of limitations in existing techniques, including the inability to generalise over unseen shapes, the absence of real-world testing, and limited expressive capacity imposed by discrete representations. To address these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D shape reconstruction that leverages the rich information provided by a vision-based tactile sensor and the expressivity of the implicit neural representation DeepSDF. Our technique consists of two components: (1) a Convolutional Neural Network that maps tactile images into local meshes representing the surface at the touch location, and (2) an implicit neural function that predicts a signed distance function to extract the desired 3D shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D shapes from tactile inputs in simulation and real-world settings, opening up research avenues for robust 3D-aware representations and improved multimodal perception in robotics. Code and supplementary material are available at: https://touchsdf.github.io/},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Comi, Mauro and Lin, Yijiong and Church, Alex and Tonioni, Alessio and Aitchison, Laurence and Lepora, Nathan F.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12602 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{chen_point_2023,
	title = {Point {Cloud} {Self}-supervised {Learning} via {3D} to {Multi}-view {Masked} {Autoencoder}},
	url = {http://arxiv.org/abs/2311.10887},
	doi = {10.48550/arXiv.2311.10887},
	abstract = {In recent years, the field of 3D self-supervised learning has witnessed significant progress, resulting in the emergence of Multi-Modality Masked AutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds for pre-training. However, a notable limitation of these approaches is that they do not fully utilize the multi-view attributes inherent in 3D point clouds, which is crucial for a deeper understanding of 3D structures. Building upon this insight, we introduce a novel approach employing a 3D to multi-view masked autoencoder to fully harness the multi-modal attributes of 3D point clouds. To be specific, our method uses the encoded tokens from 3D masked point clouds to generate original point clouds and multi-view depth images across various poses. This approach not only enriches the model's comprehension of geometric structures but also leverages the inherent multi-modal properties of point clouds. Our experiments illustrate the effectiveness of the proposed method for different tasks and under different settings. Remarkably, our method outperforms state-of-the-art counterparts by a large margin in a variety of downstream tasks, including 3D object classification, few-shot learning, part segmentation, and 3D object detection. Code will be available at: https://github.com/Zhimin-C/Multiview-MAE},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Chen, Zhimin and Li, Yingwei and Jing, Longlong and Yang, Liang and Li, Bing},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10887 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wake_gpt-4vision_2023,
	title = {{GPT}-{4V}(ision) for {Robotics}: {Multimodal} {Task} {Planning} from {Human} {Demonstration}},
	shorttitle = {{GPT}-{4V}(ision) for {Robotics}},
	url = {http://arxiv.org/abs/2311.12015},
	doi = {10.48550/arXiv.2311.12015},
	abstract = {We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation. This system analyzes videos of humans performing tasks and creates executable robot programs that incorporate affordance insights. The computation starts by analyzing the videos with GPT-4V to convert environmental and action details into text, followed by a GPT-4-empowered task planner. In the following analyses, vision systems reanalyze the video with the task plan. Object names are grounded using an open-vocabulary object detector, while focus on the hand-object relation helps to detect the moment of grasping and releasing. This spatiotemporal grounding allows the vision systems to further gather affordance data (e.g., grasp type, way points, and body postures). Experiments across various scenarios demonstrate this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Wake, Naoki and Kanehira, Atsushi and Sasabuchi, Kazuhiro and Takamatsu, Jun and Ikeuchi, Katsushi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12015 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{sundaram_learning_2019,
	title = {Learning the signatures of the human grasp using a scalable tactile glove},
	volume = {569},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1234-z},
	doi = {10.1038/s41586-019-1234-z},
	abstract = {Humans can feel, weigh and grasp diverse objects, and simultaneously infer their material properties while applying the right amount of force—a challenging set of tasks for a modern robot1. Mechanoreceptor networks that provide sensory feedback and enable the dexterity of the human grasp2 remain difficult to replicate in robots. Whereas computer-vision-based robot grasping strategies3–5 have progressed substantially with the abundance of visual data and emerging machine-learning tools, there are as yet no equivalent sensing platforms and large-scale datasets with which to probe the use of the tactile information that humans rely on when grasping objects. Studying the mechanics of how humans grasp objects will complement vision-based robotic object handling. Importantly, the inability to record and analyse tactile signals currently limits our understanding of the role of tactile information in the human grasp itself—for example, how tactile maps are used to identify objects and infer their properties is unknown6. Here we use a scalable tactile glove and deep convolutional neural networks to show that sensors uniformly distributed over the hand can be used to identify individual objects, estimate their weight and explore the typical tactile patterns that emerge while grasping objects. The sensor array (548 sensors) is assembled on a knitted glove, and consists of a piezoresistive film connected by a network of conductive thread electrodes that are passively probed. Using a low-cost (about US\$10) scalable tactile glove sensor array, we record a large-scale tactile dataset with 135,000 frames, each covering the full hand, while interacting with 26 different objects. This set of interactions with different objects reveals the key correspondences between different regions of a human hand while it is manipulating objects. Insights from the tactile signatures of the human grasp—through the lens of an artificial analogue of the natural mechanoreceptor network—can thus aid the future design of prosthetics7, robot grasping tools and human–robot interactions1,8–10.},
	language = {en},
	number = {7758},
	urldate = {2023-11-19},
	journal = {Nature},
	author = {Sundaram, Subramanian and Kellnhofer, Petr and Li, Yunzhu and Zhu, Jun-Yan and Torralba, Antonio and Matusik, Wojciech},
	month = may,
	year = {2019},
	note = {Number: 7758
Publisher: Nature Publishing Group},
	keywords = {Computer science, Electrical and electronic engineering},
	pages = {698--702},
}

@inproceedings{ha_scaling_2023,
	title = {Scaling {Up} and {Distilling} {Down}: {Language}-{Guided} {Robot} {Skill} {Acquisition}},
	shorttitle = {Scaling {Up} and {Distilling} {Down}},
	url = {https://openreview.net/forum?id=3uwj8QZROL},
	abstract = {We present a framework for robot skill acquisition, which 1) efficiently scale up data generation of language-labelled robot data and 2) effectively distills this data down into a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large language model (LLM) to guide high-level planning, and sampling-based robot planners (e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories. To robustify this data-collection process, the LLM also infers a code-snippet for the success condition of each task, simultaneously enabling the data-collection process to detect failure and retry as well as the automatic labeling of trajectories with success/failure. For (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task settings with language conditioning. Finally, we propose a new multi-task benchmark with 18 tasks across five domains to test long-horizon behavior, common-sense reasoning, tool-use, and intuitive physics. We find that our distilled policy successfully learned the robust retrying behavior in its data collection procedure, while improving absolute success rates by 33.2\% on average across five domains. Code, data, and additional qualitative results are available on https://www.cs.columbia.edu/{\textasciitilde}huy/scalingup/.},
	language = {en},
	urldate = {2023-11-16},
	author = {Ha, Huy and Florence, Pete and Song, Shuran},
	month = aug,
	year = {2023},
}

@article{wang_neural_2020,
	title = {Neural {RRT}*: {Learning}-{Based} {Optimal} {Path} {Planning}},
	volume = {17},
	issn = {1558-3783},
	shorttitle = {Neural {RRT}*},
	url = {https://ieeexplore.ieee.org/abstract/document/9037111?casa_token=tG9E-IwdHjMAAAAA:GqW-9FdGg2tLmqBraENGQCpzV2QODatwELEshp50aCg8p35d4zN7wyJQo7M58sErckLbOmi5c4A},
	doi = {10.1109/TASE.2020.2976560},
	abstract = {Rapidly random-exploring tree (RRT) and its variants are very popular due to their ability to quickly and efficiently explore the state space. However, they suffer sensitivity to the initial solution and slow convergence to the optimal solution, which means that they consume a lot of memory and time to find the optimal path. It is critical to quickly find a short path in many applications such as the autonomous vehicle with limited power/fuel. To overcome these limitations, we propose a novel optimal path planning algorithm based on the convolutional neural network (CNN), namely the neural RRT* (NRRT*). The NRRT* utilizes a nonuniform sampling distribution generated from a CNN model. The model is trained using quantities of successful path planning cases. In this article, we use the A* algorithm to generate the training data set consisting of the map information and the optimal path. For a given task, the proposed CNN model can predict the probability distribution of the optimal path on the map, which is used to guide the sampling process. The time cost and memory usage of the planned path are selected as the metric to demonstrate the effectiveness and efficiency of the NRRT*. The simulation results reveal that the NRRT* can achieve convincing performance compared with the state-of-the-art path planning algorithms. Note to Practitioners-The motivation of this article stems from the need to develop a fast and efficient path planning algorithm for practical applications such as autonomous driving, warehouse robot, and countless others. Sampling-based algorithms are widely used in these areas due to their good scalability and high efficiency. However, the quality of the initial path is not guaranteed and it takes much time to converge to the optimal path. To quickly obtain a high-quality initial path and accelerate the convergence speed, we propose the NRRT*. It utilizes a nonuniform sampling distribution and achieves better performance. The NRRT* can be also applied to other sampling-based algorithms for improved results in different applications.},
	number = {4},
	urldate = {2023-11-13},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Wang, Jiankun and Chi, Wenzheng and Li, Chenming and Wang, Chaoqun and Meng, Max Q.-H.},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Automation Science and Engineering},
	pages = {1748--1758},
}

@misc{grady_visual_2022,
	title = {Visual {Pressure} {Estimation} and {Control} for {Soft} {Robotic} {Grippers}},
	url = {http://arxiv.org/abs/2204.07268},
	doi = {10.48550/arXiv.2204.07268},
	abstract = {Soft robotic grippers facilitate contact-rich manipulation, including robust grasping of varied objects. Yet the beneficial compliance of a soft gripper also results in significant deformation that can make precision manipulation challenging. We present visual pressure estimation \& control (VPEC), a method that infers pressure applied by a soft gripper using an RGB image from an external camera. We provide results for visual pressure inference when a pneumatic gripper and a tendon-actuated gripper make contact with a flat surface. We also show that VPEC enables precision manipulation via closed-loop control of inferred pressure images. In our evaluation, a mobile manipulator (Stretch RE1 from Hello Robot) uses visual servoing to make contact at a desired pressure; follow a spatial pressure trajectory; and grasp small low-profile objects, including a microSD card, a penny, and a pill. Overall, our results show that visual estimates of applied pressure can enable a soft gripper to perform precision manipulation.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Grady, Patrick and Collins, Jeremy A. and Brahmbhatt, Samarth and Twigg, Christopher D. and Tang, Chengcheng and Hays, James and Kemp, Charles C.},
	month = aug,
	year = {2022},
	note = {arXiv:2204.07268 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{grady_pressurevision_2022,
	title = {{PressureVision}: {Estimating} {Hand} {Pressure} from a {Single} {RGB} {Image}},
	shorttitle = {{PressureVision}},
	url = {http://arxiv.org/abs/2203.10385},
	doi = {10.48550/arXiv.2203.10385},
	abstract = {People often interact with their surroundings by applying pressure with their hands. While hand pressure can be measured by placing pressure sensors between the hand and the environment, doing so can alter contact mechanics, interfere with human tactile perception, require costly sensors, and scale poorly to large environments. We explore the possibility of using a conventional RGB camera to infer hand pressure, enabling machine perception of hand pressure from uninstrumented hands and surfaces. The central insight is that the application of pressure by a hand results in informative appearance changes. Hands share biomechanical properties that result in similar observable phenomena, such as soft-tissue deformation, blood distribution, hand pose, and cast shadows. We collected videos of 36 participants with diverse skin tone applying pressure to an instrumented planar surface. We then trained a deep model (PressureVisionNet) to infer a pressure image from a single RGB image. Our model infers pressure for participants outside of the training data and outperforms baselines. We also show that the output of our model depends on the appearance of the hand and cast shadows near contact regions. Overall, our results suggest the appearance of a previously unobserved human hand can be used to accurately infer applied pressure. Data, code, and models are available online.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Grady, Patrick and Tang, Chengcheng and Brahmbhatt, Samarth and Twigg, Christopher D. and Wan, Chengde and Hays, James and Kemp, Charles C.},
	month = sep,
	year = {2022},
	note = {arXiv:2203.10385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{luo_intelligent_2021,
	title = {Intelligent {Carpet}: {Inferring} {3D} {Human} {Pose} from {Tactile} {Signals}},
	shorttitle = {Intelligent {Carpet}},
	url = {https://ieeexplore.ieee.org/document/9577856},
	doi = {10.1109/CVPR46437.2021.01110},
	abstract = {Daily human activities, e.g., locomotion, exercises, and resting, are heavily guided by the tactile interactions between the human and the ground. In this work, leveraging such tactile interactions, we propose a 3D human pose estimation approach using the pressure maps recorded by a tactile carpet as input. We build a low-cost, high-density, large-scale intelligent carpet, which enables the real-time recordings of human-floor tactile interactions in a seamless manner. We collect a synchronized tactile and visual dataset on various human activities. Employing a state-of-the-art camera-based pose estimation model as supervision, we design and implement a deep neural network model to infer 3D human poses using only the tactile information. Our pipeline can be further scaled up to multi-person pose estimation. We evaluate our system and demonstrate its potential applications in diverse fields.},
	urldate = {2023-11-13},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Luo, Yiyue and Li, Yunzhu and Foshey, Michael and Shou, Wan and Sharma, Pratyusha and Palacios, Tomás and Torralba, Antonio and Matusik, Wojciech},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	pages = {11250--11260},
}

@article{luo_learning_2021,
	title = {Learning human–environment interactions using conformal tactile textiles},
	volume = {4},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-021-00558-0},
	doi = {10.1038/s41928-021-00558-0},
	abstract = {Recording, modelling and understanding tactile interactions is important in the study of human behaviour and in the development of applications in healthcare and robotics. However, such studies remain challenging because existing wearable sensory interfaces are limited in terms of performance, flexibility, scalability and cost. Here, we report a textile-based tactile learning platform that can be used to record, monitor and learn human–environment interactions. The tactile textiles are created via digital machine knitting of inexpensive piezoresistive fibres, and can conform to arbitrary three-dimensional geometries. To ensure that our system is robust against variations in individual sensors, we use machine learning techniques for sensing correction and calibration. Using the platform, we capture diverse human–environment interactions (more than a million tactile frames) and show that the artificial-intelligence-powered sensing textiles can classify humans’ sitting poses, motions and other interactions with the environment. We also show that the platform can recover dynamic whole-body poses, reveal environmental spatial information and discover biomechanical signatures.},
	language = {en},
	number = {3},
	urldate = {2023-11-13},
	journal = {Nature Electronics},
	author = {Luo, Yiyue and Li, Yunzhu and Sharma, Pratyusha and Shou, Wan and Wu, Kui and Foshey, Michael and Li, Beichen and Palacios, Tomás and Torralba, Antonio and Matusik, Wojciech},
	month = mar,
	year = {2021},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Computer science, Electrical and electronic engineering, Mechanical engineering, Sensors and biosensors},
	pages = {193--201},
}

@inproceedings{grady_contactopt_2021,
	title = {{ContactOpt}: {Optimizing} {Contact} to {Improve} {Grasps}},
	shorttitle = {{ContactOpt}},
	url = {https://ieeexplore.ieee.org/abstract/document/9578455},
	doi = {10.1109/CVPR46437.2021.00152},
	abstract = {Physical contact between hands and objects plays a critical role in human grasps. We show that optimizing the pose of a hand to achieve expected contact with an object can improve hand poses inferred via image-based methods. Given a hand mesh and an object mesh, a deep model trained on ground truth contact data infers desirable contact across the surfaces of the meshes. Then, ContactOpt efficiently optimizes the pose of the hand to achieve desirable contact using a differentiable contact model. Notably, our contact model encourages mesh interpenetration to approximate deformable soft tissue in the hand. In our evaluations, our methods result in grasps that better match ground truth contact, have lower kinematic error, and are significantly preferred by human participants. Code and models are available online1.},
	urldate = {2023-10-22},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Grady, Patrick and Tang, Chengcheng and Twigg, Christopher D. and Vo, Minh and Brahmbhatt, Samarth and Kemp, Charles C.},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	pages = {1471--1481},
}

@inproceedings{liu_contactgen_2023,
	title = {{ContactGen}: {Generative} {Contact} {Modeling} for {Grasp} {Generation}},
	shorttitle = {{ContactGen}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-11-12},
	author = {Liu, Shaowei and Zhou, Yang and Yang, Jimei and Gupta, Saurabh and Wang, Shenlong},
	year = {2023},
	pages = {20609--20620},
}

@inproceedings{guzhov_audioclip_2022,
	title = {Audioclip: {Extending} {Clip} to {Image}, {Text} and {Audio}},
	shorttitle = {Audioclip},
	url = {https://ieeexplore.ieee.org/abstract/document/9747631},
	doi = {10.1109/ICASSP43922.2022.9747631},
	abstract = {The rapidly evolving field of sound classification has greatly benefited from the methods of other domains. Today, the trend is to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models.We present AudioCLIP – an extension of the CLIP model that handles audio in addition to text and images. Utilizing the AudioSet dataset, our proposed model incorporates the ESResNeXt audio-model into the CLIP framework, thus enabling it to perform multimodal classification and keeping CLIP’s zero-shot capabilities.AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task and out-performs others by reaching accuracies of 97.15 \% on ESC-50 and 90.07 \% on UrbanSound8K. Further, it sets new baselines in the zero-shot ESC-task on the same datasets (69.40 \% and 68.78 \%, respectively).We also asses the influence of different training setups on the final performance of the proposed model. For the sake of reproducibility, our code is published.},
	urldate = {2023-11-12},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Guzhov, Andrey and Raue, Federico and Hees, Jörn and Dengel, Andreas},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {976--980},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	doi = {10.48550/arXiv.1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{yan_soft_2021,
	title = {Soft magnetic skin for super-resolution tactile sensing with force self-decoupling},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abc8801},
	doi = {10.1126/scirobotics.abc8801},
	abstract = {Human skin can sense subtle changes of both normal and shear forces (i.e., self-decoupled) and perceive stimuli with finer resolution than the average spacing between mechanoreceptors (i.e., super-resolved). By contrast, existing tactile sensors for robotic applications are inferior, lacking accurate force decoupling and proper spatial resolution at the same time. Here, we present a soft tactile sensor with self-decoupling and super-resolution abilities by designing a sinusoidally magnetized flexible film (with the thickness {\textasciitilde}0.5 millimeters), whose deformation can be detected by a Hall sensor according to the change of magnetic flux densities under external forces. The sensor can accurately measure the normal force and the shear force (demonstrated in one dimension) with a single unit and achieve a 60-fold super-resolved accuracy enhanced by deep learning. By mounting our sensor at the fingertip of a robotic gripper, we show that robots can accomplish challenging tasks such as stably grasping fragile objects under external disturbance and threading a needle via teleoperation. This research provides new insight into tactile sensor design and could be beneficial to various applications in robotics field, such as adaptive grasping, dexterous manipulation, and human-robot interaction.},
	number = {51},
	urldate = {2023-11-10},
	journal = {Science Robotics},
	author = {Yan, Youcan and Hu, Zhe and Yang, Zhengbao and Yuan, Wenzhen and Song, Chaoyang and Pan, Jia and Shen, Yajing},
	month = feb,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabc8801},
}

@inproceedings{attarian_geometry_2023,
	title = {Geometry {Matching} for {Multi}-{Embodiment} {Grasping}},
	url = {https://openreview.net/forum?id=oyWkrG-LD5},
	abstract = {While significant progress has been made on the problem of generating grasps, many existing learning-based approaches still concentrate on a single embodiment, provide limited generalization to higher DoF end-effectors and cannot capture a diverse set of grasp modes. In this paper, we tackle the problem of grasping multi-embodiments through the viewpoint of learning rich geometric representations for both objects and end-effectors using Graph Neural Networks (GNN). Our novel method - GeoMatch - applies supervised learning on grasping data from multiple embodiments, learning end-to-end contact point likelihood maps as well as conditional autoregressive prediction of grasps keypoint-by-keypoint. We compare our method against 3 baselines that provide multi-embodiment support. Our approach performs better across 3 end-effectors, while also providing competitive diversity of grasps. Examples can be found at geomatch.github.io.},
	language = {en},
	urldate = {2023-11-10},
	author = {Attarian, Maria and Asif, Muhammad Adil and Liu, Jingzhou and Hari, Ruthrash and Garg, Animesh and Gilitschenski, Igor and Tompson, Jonathan},
	month = aug,
	year = {2023},
}

@misc{li_generative_2023,
	title = {Generative {Category}-{Level} {Shape} and {Pose} {Estimation} with {Semantic} {Primitives}},
	url = {http://arxiv.org/abs/2210.01112},
	doi = {10.48550/arXiv.2210.01112},
	abstract = {Empowering autonomous agents with 3D understanding for daily objects is a grand challenge in robotics applications. When exploring in an unknown environment, existing methods for object pose estimation are still not satisfactory due to the diversity of object shapes. In this paper, we propose a novel framework for category-level object shape and pose estimation from a single RGB-D image. To handle the intra-category variation, we adopt a semantic primitive representation that encodes diverse shapes into a unified latent space, which is the key to establish reliable correspondences between observed point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape descriptor, we gracefully decouple the shape and pose of an object, thus supporting latent shape optimization of target objects in arbitrary poses. Extensive experiments show that the proposed method achieves SOTA pose estimation performance and better generalization in the real-world dataset. Code and video are available at https://zju3dv.github.io/gCasp.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Li, Guanglin and Li, Yifeng and Ye, Zhichao and Zhang, Qihang and Kong, Tao and Cui, Zhaopeng and Zhang, Guofeng},
	month = feb,
	year = {2023},
	note = {arXiv:2210.01112 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{ward-cherrier_neurotac_2020,
	title = {{NeuroTac}: {A} {Neuromorphic} {Optical} {Tactile} {Sensor} applied to {Texture} {Recognition}},
	shorttitle = {{NeuroTac}},
	url = {https://ieeexplore.ieee.org/abstract/document/9197046},
	doi = {10.1109/ICRA40945.2020.9197046},
	abstract = {Developing artificial tactile sensing capabilities that rival human touch is a long-term goal in robotics and prosthetics. Gradually more elaborate biomimetic tactile sensors are being developed and applied to grasping and manipulation tasks to help achieve this goal. Here we present the neuroTac, a novel neuromorphic optical tactile sensor. The neuroTac combines the biomimetic hardware design from the TacTip sensor which mimicks the layered papillae structure of human glabrous skin, with an event-based camera (DAVIS240, iniVation) and algorithms which transduce contact information in the form of spike trains. The performance of the sensor is evaluated on a texture classification task, with four spike coding methods being implemented and compared: Intensive, Spatial, Temporal and Spatiotemporal. We found timing-based coding methods performed with the highest accuracy over both artificial and natural textures. The spike-based output of the neuroTac could enable the development of biomimetic tactile perception algorithms in robotics as well as non-invasive and invasive haptic feedback methods in prosthetics.},
	urldate = {2023-11-03},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Ward-Cherrier, Benjamin and Pestell, Nicholas and Lepora, Nathan F.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	pages = {2654--2660},
}

@inproceedings{althoefer_miniaturised_2023,
	title = {A {Miniaturised} {Camera}-based {Multi}-{Modal} {Tactile} {Sensor}},
	url = {https://ieeexplore.ieee.org/document/10160634},
	doi = {10.1109/ICRA48891.2023.10160634},
	abstract = {In conjunction with huge recent progress in cam-era and computer vision technology, camera-based sensors have increasingly shown considerable promise in relation to tactile sensing. In comparison to competing technologies (be they resistive, capacitive or magnetic based), they offer super-high-resolution, while suffering from fewer wiring problems. The human tactile system is composed of various types of mechanoreceptors, each able to perceive and process distinct information such as force, pressure, texture, etc. Camera-based tactile sensors such as GelSight mainly focus on high-resolution geometric sensing on a flat surface, and their force measurement capabilities are limited by the hysteresis and non-linearity of the silicone material. In this paper, we present a miniaturised dome-shaped camera-based tactile sensor that allows accurate force and tactile sensing in a single coherent system. The key novelty of the sensor design is as follows. First, we demonstrate how to build a smooth silicone hemispheric sensing medium with uniform markers on its curved surface. Second, we enhance the illumination of the rounded silicone with diffused LEDs. Third, we construct a force-sensitive mechanical structure in a compact form factor with usage of springs to accurately perceive forces. Our multi-modal sensor is able to acquire tactile information from multi-axis forces, local force distribution, and contact geometry, all in real-time. We apply an end-to-end deep learning method to process all the information.},
	urldate = {2023-11-03},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Althoefer, Kaspar and Ling, Yonggen and Li, Wanlin and Qian, Xinyuan and Lee, Wang Wei and Qi, Peng},
	month = may,
	year = {2023},
	pages = {12570--12575},
}

@inproceedings{do_densetact_2023,
	title = {{DenseTact} 2.0: {Optical} {Tactile} {Sensor} for {Shape} and {Force} {Reconstruction}},
	shorttitle = {{DenseTact} 2.0},
	url = {https://ieeexplore.ieee.org/document/10161150},
	doi = {10.1109/ICRA48891.2023.10161150},
	abstract = {Collaborative robots stand to have an immense impact on both human welfare in domestic service applications and industrial superiority in advanced manufacturing with dexterous assembly. The outstanding challenge is providing robotic fingertips with a physical design that makes them adept at performing dexterous tasks that require high-resolution, calibrated shape reconstruction and force sensing. In this work, we present DenseTact 2.0, an optical-tactile sensor capable of visualizing the deformed surface of a soft fingertip and using that image in a neural network to perform both calibrated shape reconstruction and 6-axis wrench estimation. We demon-strate the sensor accuracy of 0.3633mm per pixel for shape reconstruction, 0.410N for forces, 0.387N. mm for torques, and the ability to calibrate new fingers through transfer learning, which achieves comparable performance with only 12\% of the non-transfer learning dataset size.},
	urldate = {2023-11-03},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Do, Won Kyung and Jurewicz, Bianca and Kennedy, Monroe},
	month = may,
	year = {2023},
	pages = {12549--12555},
}

@inproceedings{zhao_-situ_2023,
	title = {In-situ {Mechanical} {Calibration} for {Vision}-based {Tactile} {Sensors}},
	url = {https://ieeexplore.ieee.org/document/10161153},
	doi = {10.1109/ICRA48891.2023.10161153},
	abstract = {This paper proposes a novel approach to conduct routine calibration for the changing mechanical parameters over time of a vision-based tactile sensor, without disassembling its overall structure, i.e., in-situ mechanical calibration. Calibration for mechanical parameters, Young's modulus and Poisson's ratio, of a tactile sensor's sensing elastomer, is crucial for its force perception capabilities. However, there are few methods that can retrieve values of these parameters both accurately and conveniently. To address this problem, we propose an in-situ approach to calibrate mechanical parameters other than the verbose traditional evaluation process. This method incorporates the deformation sensing capability of the sensor, the accurate force sensing capability of a force/torque sensor, and most importantly, the deformation-force relation-ship for an indentation with embedded mechanical parameters of the elastomers. We also present the indentation test setup and the complete pipeline to extract Young's modulus and Poisson's ratio from experimental results. We validate the method by comparing the indentation depths simulated through finite element analysis (FEA) using the cali-brated parameters with the indentation depths measured in real experiments. Furthermore, superior contact force distribution can be achieved with the accurate mechanical parameters. The proposed method provides the theoretical basis for accurate, lifelong routine calibration, whether weekly or even daily, which can enhance the applications of tactile sensors in real manipulation scenarios.},
	urldate = {2023-11-03},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhao, Can and Ren, Jieji and Yu, Hexi and Ma, Daolin},
	month = may,
	year = {2023},
	pages = {10387--10393},
}

@misc{wang_robogen_2023,
	title = {{RoboGen}: {Towards} {Unleashing} {Infinite} {Data} for {Automated} {Robot} {Learning} via {Generative} {Simulation}},
	shorttitle = {{RoboGen}},
	url = {http://arxiv.org/abs/2311.01455},
	doi = {10.48550/arXiv.2311.01455},
	abstract = {We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Wang, Yufei and Xian, Zhou and Chen, Feng and Wang, Tsun-Hsuan and Wang, Yian and Fragkiadaki, Katerina and Erickson, Zackory and Held, David and Gan, Chuang},
	month = nov,
	year = {2023},
	note = {arXiv:2311.01455 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ablett_push_2023,
	title = {Push it to the {Demonstrated} {Limit}: {Multimodal} {Visuotactile} {Imitation} {Learning} with {Force} {Matching}},
	shorttitle = {Push it to the {Demonstrated} {Limit}},
	url = {http://arxiv.org/abs/2311.01248},
	doi = {10.48550/arXiv.2311.01248},
	abstract = {Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a real robotic manipulator with door-opening and closing tasks, including over 3,000 real test episodes. Our results highlight the importance of tactile sensing for imitation learning, both for data collection to allow force matching, and for policy execution to allow accurate task feedback.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Ablett, Trevor and Limoyo, Oliver and Sigal, Adam and Jilani, Affan and Kelly, Jonathan and Siddiqi, Kaleem and Hogan, Francois and Dudek, Gregory},
	month = nov,
	year = {2023},
	note = {arXiv:2311.01248 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{sferrazza_power_2023,
	title = {The {Power} of the {Senses}: {Generalizable} {Manipulation} from {Vision} and {Touch} through {Masked} {Multimodal} {Learning}},
	shorttitle = {The {Power} of the {Senses}},
	url = {http://arxiv.org/abs/2311.00924},
	doi = {10.48550/arXiv.2311.00924},
	abstract = {Humans rely on the synergy of their senses for most essential tasks. For tasks requiring object manipulation, we seamlessly and effectively exploit the complementarity of our senses of vision and touch. This paper draws inspiration from such capabilities and aims to find a systematic approach to fuse visual and tactile information in a reinforcement learning setting. We propose Masked Multimodal Learning (M3L), which jointly learns a policy and visual-tactile representations based on masked autoencoding. The representations jointly learned from vision and touch improve sample efficiency, and unlock generalization capabilities beyond those achievable through each of the senses separately. Remarkably, representations learned in a multimodal setting also benefit vision-only policies at test time. We evaluate M3L on three simulated environments with both visual and tactile observations: robotic insertion, door opening, and dexterous in-hand manipulation, demonstrating the benefits of learning a multimodal policy. Code and videos of the experiments are available at https://sferrazza.cc/m3l\_site.},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Sferrazza, Carmelo and Seo, Younggyo and Liu, Hao and Lee, Youngwoon and Abbeel, Pieter},
	month = nov,
	year = {2023},
	note = {arXiv:2311.00924 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{kannan_deft_2023,
	title = {{DEFT}: {Dexterous} {Fine}-{Tuning} for {Real}-{World} {Hand} {Policies}},
	shorttitle = {{DEFT}},
	url = {http://arxiv.org/abs/2310.19797},
	doi = {10.48550/arXiv.2310.19797},
	abstract = {Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. However, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation. Please see our website at https://dexterous-finetuning.github.io for video results.},
	urldate = {2023-10-31},
	publisher = {arXiv},
	author = {Kannan, Aditya and Shaw, Kenneth and Bahl, Shikhar and Mannam, Pragna and Pathak, Deepak},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19797 null},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{mandlekar_mimicgen_2023,
	title = {{MimicGen}: {A} {Data} {Generation} {System} for {Scalable} {Robot} {Learning} using {Human} {Demonstrations}},
	shorttitle = {{MimicGen}},
	url = {http://arxiv.org/abs/2310.17596},
	doi = {10.48550/arXiv.2310.17596},
	abstract = {Imitation learning from a large set of human demonstrations has proved to be an effective paradigm for building capable robot agents. However, the demonstrations can be extremely costly and time-consuming to collect. We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts. We use MimicGen to generate over 50K demonstrations across 18 tasks with diverse scene configurations, object instances, and robot arms from just {\textasciitilde}200 human demonstrations. We show that robot agents can be effectively trained on this generated dataset by imitation learning to achieve strong performance in long-horizon and high-precision tasks, such as multi-part assembly and coffee preparation, across broad initial state distributions. We further demonstrate that the effectiveness and utility of MimicGen data compare favorably to collecting additional human demonstrations, making it a powerful and economical approach towards scaling up robot learning. Datasets, simulation environments, videos, and more at https://mimicgen.github.io .},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Mandlekar, Ajay and Nasiriany, Soroush and Wen, Bowen and Akinola, Iretiayo and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Fox, Dieter},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17596 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hansen_temporal_2022,
	title = {Temporal {Difference} {Learning} for {Model} {Predictive} {Control}},
	url = {http://arxiv.org/abs/2203.04955},
	doi = {10.48550/arXiv.2203.04955},
	abstract = {Data-driven model predictive control has two key advantages over model-free methods: a potential for improved sample efficiency through model learning, and better performance as computational budget for planning increases. However, it is both costly to plan over long horizons and challenging to obtain an accurate model of the environment. In this work, we combine the strengths of model-free and model-based methods. We use a learned task-oriented latent dynamics model for local trajectory optimization over a short horizon, and use a learned terminal value function to estimate long-term return, both of which are learned jointly by temporal difference learning. Our method, TD-MPC, achieves superior sample efficiency and asymptotic performance over prior work on both state and image-based continuous control tasks from DMControl and Meta-World. Code and video results are available at https://nicklashansen.github.io/td-mpc.},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Hansen, Nicklas and Wang, Xiaolong and Su, Hao},
	month = jul,
	year = {2022},
	note = {arXiv:2203.04955 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_learning_2023,
	title = {Learning {Generalizable} {Manipulation} {Policies} with {Object}-{Centric} {3D} {Representations}},
	url = {http://arxiv.org/abs/2310.14386},
	doi = {10.48550/arXiv.2310.14386},
	abstract = {We introduce GROOT, an imitation learning method for learning robust policies with object-centric and 3D priors. GROOT builds policies that generalize beyond their initial training conditions for vision-based manipulation. It constructs object-centric 3D representations that are robust toward background changes and camera views and reason over these representations using a transformer-based policy. Furthermore, we introduce a segmentation correspondence model that allows policies to generalize to new objects at test time. Through comprehensive experiments, we validate the robustness of GROOT policies against perceptual variations in simulated and real-world environments. GROOT's performance excels in generalization over background changes, camera viewpoint shifts, and the presence of new object instances, whereas both state-of-the-art end-to-end learning methods and object proposal-based approaches fall short. We also extensively evaluate GROOT policies on real robots, where we demonstrate the efficacy under very wild changes in setup. More videos and model details can be found in the appendix and the project website: https://ut-austin-rpl.github.io/GROOT .},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Zhu, Yifeng and Jiang, Zhenyu and Stone, Peter and Zhu, Yuke},
	month = oct,
	year = {2023},
	note = {arXiv:2310.14386 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_dreamspace_2023,
	title = {{DreamSpace}: {Dreaming} {Your} {Room} {Space} with {Text}-{Driven} {Panoramic} {Texture} {Propagation}},
	shorttitle = {{DreamSpace}},
	url = {http://arxiv.org/abs/2310.13119},
	doi = {10.48550/arXiv.2310.13119},
	abstract = {Diffusion-based methods have achieved prominent success in generating 2D media. However, accomplishing similar proficiencies for scene-level mesh texturing in 3D spatial applications, e.g., XR/VR, remains constrained, primarily due to the intricate nature of 3D geometry and the necessity for immersive free-viewpoint rendering. In this paper, we propose a novel indoor scene texturing framework, which delivers text-driven texture generation with enchanting details and authentic spatial coherence. The key insight is to first imagine a stylized 360\{{\textbackslash}deg\} panoramic texture from the central viewpoint of the scene, and then propagate it to the rest areas with inpainting and imitating techniques. To ensure meaningful and aligned textures to the scene, we develop a novel coarse-to-fine panoramic texture generation approach with dual texture alignment, which both considers the geometry and texture cues of the captured scenes. To survive from cluttered geometries during texture propagation, we design a separated strategy, which conducts texture inpainting in confidential regions and then learns an implicit imitating network to synthesize textures in occluded and tiny structural areas. Extensive experiments and the immersive VR application on real-world indoor scenes demonstrate the high quality of the generated textures and the engaging experience on VR headsets. Project webpage: https://ybbbbt.com/publication/dreamspace},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Yang, Bangbang and Dong, Wenqi and Ma, Lin and Hu, Wenbo and Liu, Xiao and Cui, Zhaopeng and Ma, Yuewen},
	month = oct,
	year = {2023},
	note = {arXiv:2310.13119 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@article{tekden_grasp_2023,
	title = {Grasp {Transfer} {Based} on {Self}-{Aligning} {Implicit} {Representations} of {Local} {Surfaces}},
	volume = {8},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/abstract/document/10223273?casa_token=O_VYt6LJXvMAAAAA:461-nGpco3eTkEnIgKpiSNaNxNgOjWV4QJi6wzq6CxwV6YxF9C15BLlHoLHNTzIE2ZUWo55borI},
	doi = {10.1109/LRA.2023.3306272},
	abstract = {Objects we interact with and manipulate often share similar parts, such as handles, that allow us to transfer our actions flexibly due to their shared functionality. This work addresses the problem of transferring a grasp experience or a demonstration to a novel object that shares shape similarities with objects the robot has previously encountered. Existing approaches for solving this problem are typically restricted to a specific object category or a parametric shape. Our approach, however, can transfer grasps associated with implicit models of local surfaces shared across object categories. Specifically, we employ a single expert grasp demonstration to learn an implicit local surface representation model from a small dataset of object meshes. At inference time, this model is used to transfer grasps to novel objects by identifying the most geometrically similar surfaces to the one on which the expert grasp is demonstrated. Our model is trained entirely in simulation and is evaluated on simulated and real-world objects that are not seen during training. Evaluations indicate that grasp transfer to unseen object categories using this approach can be successfully performed both in simulation and real-world experiments. The simulation results also show that the proposed approach leads to better spatial precision and grasp accuracy compared to a baseline approach.},
	number = {10},
	urldate = {2023-10-28},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tekden, Ahmet and Deisenroth, Marc Peter and Bekiroglu, Yasemin},
	month = oct,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	pages = {6315--6322},
}

@article{chaudhury_using_2022,
	title = {Using {Collocated} {Vision} and {Tactile} {Sensors} for {Visual} {Servoing} and {Localization}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2204.11686},
	doi = {10.1109/LRA.2022.3146565},
	abstract = {Coordinating proximity and tactile imaging by collocating cameras with tactile sensors can 1) provide useful information before contact such as object pose estimates and visually servo a robot to a target with reduced occlusion and higher resolution compared to head-mounted or external depth cameras, 2) simplify the contact point and pose estimation problems and help tactile sensing avoid erroneous matches when a surface does not have significant texture or has repetitive texture with many possible matches, and 3) use tactile imaging to further refine contact point and object pose estimation. We demonstrate our results with objects that have more surface texture than most objects in standard manipulation datasets. We learn that optic flow needs to be integrated over a substantial amount of camera travel to be useful in predicting movement direction. Most importantly, we also learn that state of the art vision algorithms do not do a good job localizing tactile images on object models, unless a reasonable prior can be provided from collocated cameras.},
	number = {2},
	urldate = {2022-10-06},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chaudhury, Arkadeep Narayan and Man, Timothy and Yuan, Wenzhen and Atkeson, Christopher G.},
	month = apr,
	year = {2022},
	note = {arXiv:2204.11686 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {3427--3434},
}

@inproceedings{yang_cpf_2021,
	title = {{CPF}: {Learning} a {Contact} {Potential} {Field} to {Model} the {Hand}-{Object} {Interaction}},
	shorttitle = {{CPF}},
	url = {https://ieeexplore.ieee.org/abstract/document/9710247},
	doi = {10.1109/ICCV48922.2021.01091},
	abstract = {Modeling the hand-object (HO) interaction not only requires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hybrid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole system forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two commonly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction metrics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe interpenetration or disjointedness. Our code is available at https://github.com/lixiny/CPF.},
	urldate = {2023-10-22},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yang, Lixin and Zhan, Xinyu and Li, Kailin and Xu, Wenqiang and Li, Jiefeng and Lu, Cewu},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	pages = {11077--11086},
}

@misc{black_zero-shot_2023,
	title = {Zero-{Shot} {Robotic} {Manipulation} with {Pretrained} {Image}-{Editing} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.10639},
	doi = {10.48550/arXiv.2310.10639},
	abstract = {If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. Specifically, we finetune InstructPix2Pix on video data, consisting of both human videos and robot rollouts, such that it outputs hypothetical future "subgoal" observations given the robot's current observation and a language command. We also use the robot data to train a low-level goal-conditioned policy to act as the aforementioned low-level controller. We find that the high-level subgoal predictions can utilize Internet-scale pretraining and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization and precision than conventional language-conditioned policies. We achieve state-of-the-art results on the CALVIN benchmark, and also demonstrate robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data. The project website can be found at http://rail-berkeley.github.io/susie .},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Black, Kevin and Nakamoto, Mitsuhiko and Atreya, Pranav and Walke, Homer and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10639 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{peng_3d_2023,
	title = {{3D} {Force} and {Contact} {Estimation} for a {Soft}-{Bubble} {Visuotactile} {Sensor} {Using} {FEM}},
	url = {http://arxiv.org/abs/2310.11372},
	doi = {10.48550/arXiv.2310.11372},
	abstract = {Soft-bubble tactile sensors have the potential to capture dense contact and force information across a large contact surface. However, it is difficult to extract contact forces directly from observing the bubble surface because local contacts change the global surface shape significantly due to membrane mechanics and air pressure. This paper presents a model-based method of reconstructing dense contact forces from the bubble sensor's internal RGBD camera and air pressure sensor. We present a finite element model of the force response of the bubble sensor that uses a linear plane stress approximation that only requires calibrating 3 variables. Our method is shown to reconstruct normal and shear forces significantly more accurately than the state-of-the-art, with comparable accuracy for detecting the contact patch, and with very little calibration data.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Peng, Jing-Chen and Yao, Shaoxiong and Hauser, Kris},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11372 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{lau_tactile_2016,
	title = {Tactile mesh saliency},
	volume = {35},
	issn = {0730-0301},
	url = {https://dl.acm.org/doi/10.1145/2897824.2925927},
	doi = {10.1145/2897824.2925927},
	abstract = {While the concept of visual saliency has been previously explored in the areas of mesh and image processing, saliency detection also applies to other sensory stimuli. In this paper, we explore the problem of tactile mesh saliency, where we define salient points on a virtual mesh as those that a human is more likely to grasp, press, or touch if the mesh were a real-world object. We solve the problem of taking as input a 3D mesh and computing the relative tactile saliency of every mesh vertex. Since it is difficult to manually define a tactile saliency measure, we introduce a crowdsourcing and learning framework. It is typically easy for humans to provide relative rankings of saliency between vertices rather than absolute values. We thereby collect crowdsourced data of such relative rankings and take a learning-to-rank approach. We develop a new formulation to combine deep learning and learning-to-rank methods to compute a tactile saliency measure. We demonstrate our framework with a variety of 3D meshes and various applications including material suggestion for rendering and fabrication.},
	number = {4},
	urldate = {2023-10-17},
	journal = {ACM Transactions on Graphics},
	author = {Lau, Manfred and Dev, Kapil and Shi, Weiqi and Dorsey, Julie and Rushmeier, Holly},
	month = jul,
	year = {2016},
	keywords = {crowdsourcing, deep learning, fabrication material suggestion, perception, saliency},
	pages = {52:1--52:11},
}

@inproceedings{fan_arctic_2023,
	title = {{ARCTIC}: {A} {Dataset} for {Dexterous} {Bimanual} {Hand}-{Object} {Manipulation}},
	shorttitle = {{ARCTIC}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-10-17},
	author = {Fan, Zicong and Taheri, Omid and Tzionas, Dimitrios and Kocabas, Muhammed and Kaufmann, Manuel and Black, Michael J. and Hilliges, Otmar},
	year = {2023},
	pages = {12943--12954},
}

@inproceedings{taheri_grab_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{GRAB}: {A} {Dataset} of {Whole}-{Body} {Human} {Grasping} of {Objects}},
	isbn = {978-3-030-58548-8},
	shorttitle = {{GRAB}},
	doi = {10.1007/978-3-030-58548-8_34},
	abstract = {Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {581--600},
}

@inproceedings{brahmbhatt_contactdb_2019,
	title = {{ContactDB}: {Analyzing} and {Predicting} {Grasp} {Contact} via {Thermal} {Imaging}},
	shorttitle = {{ContactDB}},
	url = {https://ieeexplore.ieee.org/document/8953821},
	doi = {10.1109/CVPR.2019.00891},
	abstract = {Grasping and manipulating objects is an important human skill. Since hand-object contact is fundamental to grasping, capturing it can lead to important insights. However, observing contact through external sensors is challenging because of occlusion and the complexity of the human hand. We present ContactDB, a novel dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. Participants in our study grasped 3D printed objects with a post-grasp functional intent. ContactDB includes 3750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images. To the best of our knowledge, this is the first large-scale dataset that records detailed contact maps for human grasps. Analysis of this data shows the influence of functional intent and object size on grasping, the tendency to touch/avoid `active areas', and the high frequency of palm and proximal finger contact. Finally, we train state-of-the art image translation and 3D convolution algorithms to predict diverse contact patterns from object shape. Data, code and models are available at https://contactdb.cc.gatech.edu.},
	urldate = {2023-10-17},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Brahmbhatt, Samarth and Ham, Cusuh and Kemp, Charles C. and Hays, James},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	pages = {8701--8711},
}

@article{padmanabha_multimodal_2023,
	title = {A multimodal sensing ring for quantification of scratch intensity},
	volume = {3},
	copyright = {2023 Springer Nature Limited},
	issn = {2730-664X},
	url = {https://www.nature.com/articles/s43856-023-00345-2},
	doi = {10.1038/s43856-023-00345-2},
	abstract = {An objective measurement of chronic itch is necessary for improvements in patient care for numerous medical conditions. While wearables have shown promise for scratch detection, they are currently unable to estimate scratch intensity, preventing a comprehensive understanding of the effect of itch on an individual.},
	language = {en},
	number = {1},
	urldate = {2023-10-17},
	journal = {Communications Medicine},
	author = {Padmanabha, Akhil and Choudhary, Sonal and Majidi, Carmel and Erickson, Zackory},
	month = sep,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Signs and symptoms, Skin diseases},
	pages = {1--12},
}

@misc{lan_dexcatch_2023,
	title = {{DexCatch}: {Learning} to {Catch} {Arbitrary} {Objects} with {Dexterous} {Hands}},
	shorttitle = {{DexCatch}},
	url = {https://arxiv.org/abs/2310.08809v1},
	abstract = {Achieving human-like dexterous manipulation remains a crucial area of research in robotics. Current research focuses on improving the success rate of pick-and-place tasks. Compared with pick-and-place, throw-catching behavior has the potential to increase picking speed without transporting objects to their destination. However, dynamic dexterous manipulation poses a major challenge for stable control due to a large number of dynamic contacts. In this paper, we propose a Stability-Constrained Reinforcement Learning (SCRL) algorithm to learn to catch diverse objects with dexterous hands. The SCRL algorithm outperforms baselines by a large margin, and the learned policies show strong zero-shot transfer performance on unseen objects. Remarkably, even though the object in a hand facing sideward is extremely unstable due to the lack of support from the palm, our method can still achieve a high level of success in the most challenging task. Video demonstrations of learned behaviors and the code can be found on the supplementary website.},
	language = {en},
	urldate = {2023-10-17},
	journal = {arXiv.org},
	author = {Lan, Fengbo and Wang, Shengjie and Zhang, Yunzhe and Xu, Haotian and Oseni, Oluwatosin and Gao, Yang and Zhang, Tao},
	month = oct,
	year = {2023},
}

@misc{li_imagemanip_2023,
	title = {{ImageManip}: {Image}-based {Robotic} {Manipulation} with {Affordance}-guided {Next} {View} {Selection}},
	shorttitle = {{ImageManip}},
	url = {https://arxiv.org/abs/2310.09069v1},
	abstract = {In the realm of future home-assistant robots, 3D articulated object manipulation is essential for enabling robots to interact with their environment. Many existing studies make use of 3D point clouds as the primary input for manipulation policies. However, this approach encounters challenges due to data sparsity and the significant cost associated with acquiring point cloud data, which can limit its practicality. In contrast, RGB images offer high-resolution observations using cost effective devices but lack spatial 3D geometric information. To overcome these limitations, we present a novel image-based robotic manipulation framework. This framework is designed to capture multiple perspectives of the target object and infer depth information to complement its geometry. Initially, the system employs an eye-on-hand RGB camera to capture an overall view of the target object. It predicts the initial depth map and a coarse affordance map. The affordance map indicates actionable areas on the object and serves as a constraint for selecting subsequent viewpoints. Based on the global visual prior, we adaptively identify the optimal next viewpoint for a detailed observation of the potential manipulation success area. We leverage geometric consistency to fuse the views, resulting in a refined depth map and a more precise affordance map for robot manipulation decisions. By comparing with prior works that adopt point clouds or RGB images as inputs, we demonstrate the effectiveness and practicality of our method. In the project webpage (https://sites.google.com/view/imagemanip), real world experiments further highlight the potential of our method for practical deployment.},
	language = {en},
	urldate = {2023-10-17},
	journal = {arXiv.org},
	author = {Li, Xiaoqi and Wang, Yanzi and Shen, Yan and Iaroslav, Ponomarenko and Lu, Haoran and Wang, Qianxu and An, Boshi and Liu, Jiaming and Dong, Hao},
	month = oct,
	year = {2023},
}

@article{yuksel_sample_2015,
	title = {Sample {Elimination} for {Generating} {Poisson} {Disk} {Sample} {Sets}},
	volume = {34},
	copyright = {© 2015 The Author(s) Computer Graphics Forum © 2015 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12538},
	doi = {10.1111/cgf.12538},
	abstract = {In this paper we describe sample elimination for generating Poisson disk sample sets with a desired size. We introduce a greedy sample elimination algorithm that assigns a weight to each sample in a given set and eliminates the ones with greater weights in order to pick a subset of a desired size with Poisson disk property without having to specify a Poisson disk radius. This new algorithm is simple, computationally efficient, and it can work in any sampling domain, producing sample sets with more pronounced blue noise characteristics than dart throwing. Most importantly, it allows unbiased progressive (adaptive) sampling and it scales better to high dimensions than previous methods. However, it cannot guarantee maximal coverage. We provide a statistical analysis of our algorithm in 2D and higher dimensions as well as results from our tests with different example applications.},
	language = {en},
	number = {2},
	urldate = {2023-10-16},
	journal = {Computer Graphics Forum},
	author = {Yuksel, Cem},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12538},
	pages = {25--32},
}

@article{wang_tacto_2022,
	title = {{TACTO}: {A} {Fast}, {Flexible}, and {Open}-{Source} {Simulator} for {High}-{Resolution} {Vision}-{Based} {Tactile} {Sensors}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {{TACTO}},
	url = {https://ieeexplore.ieee.org/document/9697425},
	doi = {10.1109/LRA.2022.3146945},
	abstract = {Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.},
	number = {2},
	urldate = {2023-10-15},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, Shaoxiong and Lambeta, Mike and Chou, Po-Wei and Calandra, Roberto},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	pages = {3930--3937},
}

@inproceedings{li_stereovoxelnet_2023,
	title = {{StereoVoxelNet}: {Real}-{Time} {Obstacle} {Detection} {Based} on {Occupancy} {Voxels} from a {Stereo} {Camera} {Using} {Deep} {Neural} {Networks}},
	shorttitle = {{StereoVoxelNet}},
	url = {https://ieeexplore.ieee.org/document/10160924},
	doi = {10.1109/ICRA48891.2023.10160924},
	abstract = {Obstacle detection is a safety-critical problem in robot navigation, where stereo matching is a popular vision-based approach. While deep neural networks have shown impressive results in computer vision, most of the previous obstacle detection works only leverage traditional stereo matching techniques to meet the computational constraints for real-time feedback. This paper proposes a computationally efficient method that employs a deep neural network to detect occupancy from stereo images directly. Instead of learning the point cloud correspondence from the stereo data, our approach extracts the compact obstacle distribution based on volumetric representations. In addition, we prune the computation of safety irrelevant spaces in a coarse-to-fine manner based on octrees generated by the decoder. As a result, we achieve real-time performance on the onboard computer (NVIDIA Jetson TX2). Our approach detects obstacles accurately in the range of 32 meters and achieves better IoU (Intersection over Union) and CD (Chamfer Distance) scores with only 2\% of the computation cost of the state-of-the-art stereo model. Furthermore, we validate our method's robustness and real-world feasibility through autonomous navigation experiments with a real robot. Hence, our work contributes toward closing the gap between the stereo-based system in robot perception and state-of-the-art stereo models in computer vision. To counter the scarcity of high-quality real-world indoor stereo datasets, we collect a 1.36 hours stereo dataset with a mobile robot which is used to fine-tune our model. The dataset, the code, and further details including additional visualizations are available at https://lhy.xyz/stereovoxelnet/.},
	urldate = {2023-10-15},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Hongyu and Li, Zhengang and Akmandor, Neşet Ünver and Jiang, Huaizu and Wang, Yanzhi and Padır, Taşkın},
	month = may,
	year = {2023},
	pages = {4826--4833},
}

@inproceedings{lepora_superresolution_2015,
	title = {Superresolution with an optical tactile sensor},
	url = {https://ieeexplore.ieee.org/document/7353744},
	doi = {10.1109/IROS.2015.7353744},
	abstract = {Although superresolution has been studied to huge impact in visual imaging, it is relatively unexplored in tactile robotics. Here we demonstrate a novel optical sensor design (the TacTip) capable of achieving 40-fold localization superresolution to 0.1mm accuracy compared with a 4mm resolution between tactile elements. This superresolution is reached for localizing a 40mm diameter hemicylinder with a tactile finger pad also of 40mm diameter. Deformations of the sensor surface are measured as displacements of molded internal pins, with pin separation thus defining sensor resolution. Active Bayesian perception for classifying object location was used to ensure robust localization and hence the magnitude of the superresolution. These results are comparable with those for capacitive tactile sensors, which we interpret as originating from a convergence in the taxel-based design of the optical sensor and capacitive tactile sensors. The attained superresolution is comparable to the best perceptual hyperacuity in humans.},
	urldate = {2023-10-15},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Lepora, Nathan F. and Ward-Cherrier, Benjamin},
	month = sep,
	year = {2015},
	pages = {2686--2691},
}

@article{hubert_comparing_1985,
	title = {Comparing partitions},
	volume = {2},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/BF01908075},
	doi = {10.1007/BF01908075},
	abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1.},
	language = {en},
	number = {1},
	urldate = {2023-10-15},
	journal = {Journal of Classification},
	author = {Hubert, Lawrence and Arabie, Phipps},
	month = dec,
	year = {1985},
	keywords = {Consensus indices, Measures of agreement, Measures of association},
	pages = {193--218},
}

@inproceedings{koval_manifold_2017,
	title = {The manifold particle filter for state estimation on high-dimensional implicit manifolds},
	url = {https://ieeexplore.ieee.org/abstract/document/7989543},
	doi = {10.1109/ICRA.2017.7989543},
	abstract = {We estimate the state of a noisy robot arm and underactuated hand using an implicit Manifold Particle Filter (MPF) informed by contact sensors. As the robot touches the world, its state space collapses to a contact manifold that we represent implicitly using a signed distance field. This allows us to extend the MPF to higher (six or more) dimensional state spaces. Earlier work, which explicitly represents the contact manifold, was only capable of scaling to three dimensions. Through a series of experiments, we show that the implicit MPF converges faster and is more accurate than a conventional particle filter during periods of persistent contact. We present three methods of drawing samples from an implicit contact manifold, and compare them in experiments.},
	urldate = {2023-10-14},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Koval, Michael C. and Klingensmith, Matthew and Srinivasa, Siddhartha S. and Pollard, Nancy S. and Kaess, Michael},
	month = may,
	year = {2017},
	pages = {4673--4680},
}

@inproceedings{chalon_online_2013,
	title = {Online in-hand object localization},
	url = {https://ieeexplore.ieee.org/document/6696778},
	doi = {10.1109/IROS.2013.6696778},
	abstract = {Robotic hands are a key component of humanoids. Initially more fragile and larger than their human counterparts, the technology has evolved and the latest generation is close to the human hand in size and robustness. However, it is still disappointing to see how little robotic hands are able to do once the grasp is acquired due to the difficulty to obtain a reliable pose of the object within the palm. This paper presents a novel method based on a particle filter used to estimate online the object pose. It is shown that the method is robust, accurate and handles many realistic scenario without hand crafted rules. It combines an efficient collision checker with a few very simple ideas, that require only a basic knowledge of the geometry of the objects. It is shown, by experiments and simulations, that the algorithm is able to deal with inaccurate finger position measurements and can integrate tactile measurements. The method greatly enhances the performance of common manipulation operations, such as a pick and place tasks, and boosts the sensing capabilities of the robot.},
	urldate = {2023-10-14},
	booktitle = {2013 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Chalon, Maxime and Reinecke, Jens and Pfanne, Martin},
	month = nov,
	year = {2013},
	note = {ISSN: 2153-0866},
	pages = {2977--2984},
}

@article{petrovskaya_global_2011,
	title = {Global {Localization} of {Objects} via {Touch}},
	volume = {27},
	issn = {1941-0468},
	url = {https://ieeexplore.ieee.org/abstract/document/5784199},
	doi = {10.1109/TRO.2011.2138450},
	abstract = {Humans are capable of manipulating objects based solely on the sense of touch. For robots to achieve the same feat in unstructured environments, global localization of objects via touch is required. Bayesian approaches provide the means to cope with uncertainties of the real world, but the estimation of the Bayesian posterior for the full six degrees of freedom (6-DOF) global localization problem is computationally prohibitive. We propose an efficient Bayesian approach termed Scaling Series. It is capable of solving the full problem reliably in real time. This is a Monte Carlo approach that performs a series of successive refinements coupled with annealing. We also propose an analytical measurement model, which can be computed efficiently at run time for any object represented as a polygonal mesh. Extensive empirical evaluation shows that Scaling Series drastically outperforms prior approaches. We demonstrate general applicability of the approach on five common solid objects, which are rigidly fixed during the experiments. We also consider 6-DOF localization and tracking of free-standing objects that can move during tactile exploration.},
	number = {3},
	urldate = {2023-10-14},
	journal = {IEEE Transactions on Robotics},
	author = {Petrovskaya, Anna and Khatib, Oussama},
	month = jun,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Robotics},
	pages = {569--585},
}

@inproceedings{schaeffer_methods_2003,
	title = {Methods for intelligent localization and mapping during haptic exploration},
	volume = {4},
	url = {https://ieeexplore.ieee.org/document/1244421},
	doi = {10.1109/ICSMC.2003.1244421},
	abstract = {This paper presents a set of algorithms for use in simultaneous localization and mapping during haptic exploration. Several solutions are provided for the problem of a single spherical robot finger exploring a known smooth surface, starting with an unknown pose. Using pose estimates, pattern matching is performed between the robot's internal model of the surface and the known model. The robot finger is guided to explore regions of the surface that will maximize the probability of recognition. Simulation results demonstrate the effectiveness of one algorithm. In addition, it is shown that haptic exploration and dexterous manipulation can be achieved concurrently when multiple robot fingers are used.},
	urldate = {2023-10-14},
	booktitle = {{SMC}'03 {Conference} {Proceedings}. 2003 {IEEE} {International} {Conference} on {Systems}, {Man} and {Cybernetics}. {Conference} {Theme} - {System} {Security} and {Assurance} ({Cat}. {No}.{03CH37483})},
	author = {Schaeffer, M.A. and Okamura, A.M.},
	month = oct,
	year = {2003},
	note = {ISSN: 1062-922X},
	pages = {3438--3445 vol.4},
}

@inproceedings{molchanov_contact_2016,
	title = {Contact localization on grasped objects using tactile sensing},
	url = {https://ieeexplore.ieee.org/abstract/document/7759058},
	doi = {10.1109/IROS.2016.7759058},
	abstract = {Manipulation tasks often require robots to make contact between a grasped tool and another object in the robot's environment. The ability to detect and estimate the positions and directions of these contact points is crucial for monitoring the progress of the task, and detecting failures. In this paper, we present a data-driven approach for detecting and localizing contacts between a grasped object and the environment using tactile sensing. We explore framing the contact localization as both a regression and a classification problem and train neural networks accordingly to estimate the contact parameters. We also compare the neural networks with Gaussian process regression and support vector machine classification with spatio-temporal hierarchical matching pursuit feature learning. We evaluate the presented approach using hundreds of contact events on eighteen objects with different shapes, sizes and material properties. The experiments show that the neural network approach can learn to localize contact events for individual objects with a mean absolute error of less than 2.5 cm for the positions and less than 10° for the directions.},
	urldate = {2023-10-14},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Molchanov, Artem and Kroemer, Oliver and Su, Zhe and Sukhatme, Gaurav S.},
	month = oct,
	year = {2016},
	note = {ISSN: 2153-0866},
	pages = {216--222},
}

@inproceedings{corcoran_measurement_2010,
	title = {A measurement model for tracking hand-object state during dexterous manipulation},
	url = {https://ieeexplore.ieee.org/document/5509194},
	doi = {10.1109/ROBOT.2010.5509194},
	abstract = {It is frequently accepted in the manipulation literature that tactile sensing is needed to improve the precision of robot manipulation. However, there is no consensus on how this may be achieved. This paper applies particle filtering to the problem of localizing the pose and shape of an object that the robot touches. We are motivated by the situation where the robot has enclosed its fingers around an object but has not yet grasped it. This might be the case just prior to grasping or when the robot is holding on to something fixtured elsewhere in the environment. In order to solve this problem, we propose a new model for position measurements of points on the robot manipulator that tactile sensing indicates are touching the object. We also propose a model for points on the manipulator that tactile measurements indicate are not touching the object. Finally, we characterize the approach in simulation and use it to localize an object that Robonaut 2 holds in its hand.},
	urldate = {2023-10-14},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Corcoran, Craig and Platt, Robert},
	month = may,
	year = {2010},
	note = {ISSN: 1050-4729},
	pages = {4302--4308},
}

@misc{sodhi_patchgraph_2022,
	title = {{PatchGraph}: {In}-hand tactile tracking with learned surface normals},
	shorttitle = {{PatchGraph}},
	url = {http://arxiv.org/abs/2111.07524},
	doi = {10.48550/arXiv.2111.07524},
	abstract = {We address the problem of tracking 3D object poses from touch during in-hand manipulations. Specifically, we look at tracking small objects using vision-based tactile sensors that provide high-dimensional tactile image measurements at the point of contact. While prior work has relied on a-priori information about the object being localized, we remove this requirement. Our key insight is that an object is composed of several local surface patches, each informative enough to achieve reliable object tracking. Moreover, we can recover the geometry of this local patch online by extracting local surface normal information embedded in each tactile image. We propose a novel two-stage approach. First, we learn a mapping from tactile images to surface normals using an image translation network. Second, we use these surface normals within a factor graph to both reconstruct a local patch map and use it to infer 3D object poses. We demonstrate reliable object tracking for over \$100\$ contact sequences across unique shapes with four objects in simulation and two objects in the real-world. Supplementary video: https://youtu.be/FHks--haOGY},
	urldate = {2023-10-14},
	publisher = {arXiv},
	author = {Sodhi, Paloma and Kaess, Michael and Mukadam, Mustafa and Anderson, Stuart},
	month = apr,
	year = {2022},
	note = {arXiv:2111.07524 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yang_octopus_2023,
	title = {Octopus: {Embodied} {Vision}-{Language} {Programmer} from {Environmental} {Feedback}},
	shorttitle = {Octopus},
	url = {http://arxiv.org/abs/2310.08588},
	doi = {10.48550/arXiv.2310.08588},
	abstract = {Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.},
	urldate = {2023-10-14},
	publisher = {arXiv},
	author = {Yang, Jingkang and Dong, Yuhao and Liu, Shuai and Li, Bo and Wang, Ziyue and Jiang, Chencheng and Tan, Haoran and Kang, Jiamu and Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_learning_2023,
	title = {Learning {Interactive} {Real}-{World} {Simulators}},
	url = {http://arxiv.org/abs/2310.06114},
	doi = {10.48550/arXiv.2310.06114},
	abstract = {Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as "open the drawer" and low-level controls such as "move by x, y" from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.},
	urldate = {2023-10-14},
	publisher = {arXiv},
	author = {Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06114 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{parakh_human-assisted_2023,
	title = {Human-{Assisted} {Continual} {Robot} {Learning} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2309.14321},
	doi = {10.48550/arXiv.2309.14321},
	abstract = {Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Parakh, Meenal and Fong, Alisha and Simeonov, Anthony and Gupta, Abhishek and Chen, Tao and Agrawal, Pulkit},
	month = sep,
	year = {2023},
	note = {arXiv:2309.14321 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kumar_words_2023,
	title = {Words into {Action}: {Learning} {Diverse} {Humanoid} {Robot} {Behaviors} using {Language} {Guided} {Iterative} {Motion} {Refinement}},
	shorttitle = {Words into {Action}},
	url = {http://arxiv.org/abs/2310.06226},
	doi = {10.48550/arXiv.2310.06226},
	abstract = {Humanoid robots are well suited for human habitats due to their morphological similarity, but developing controllers for them is a challenging task that involves multiple sub-problems, such as control, planning and perception. In this paper, we introduce a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands. We first learn a neural network policy that generates behaviors given a natural language command, such as "walk forward", by combining Large Language Models (LLMs), motion retargeting, and motion imitation. Based on the synthesized motion, we iteratively fine-tune by updating the text prompt and querying LLMs to find the best checkpoint associated with the closest motion in history. We validate our approach using a simulated Digit humanoid robot and demonstrate learning of diverse motions, such as walking, hopping, and kicking, without the burden of complex reward engineering. In addition, we show that our iterative refinement enables us to learn 3x times faster than a naive formulation that learns from scratch.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Kumar, K. Niranjan and Essa, Irfan and Ha, Sehoon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06226 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{chen_forgetful_2023,
	title = {Forgetful {Large} {Language} {Models}: {Lessons} {Learned} from {Using} {LLMs} in {Robot} {Programming}},
	shorttitle = {Forgetful {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.06646},
	doi = {10.48550/arXiv.2310.06646},
	abstract = {Large language models offer new ways of empowering people to program robot applications-namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being "forgetful" of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Chen, Juo-Tung and Huang, Chien-Ming},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06646 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{kumar_robohive_2023,
	title = {{RoboHive}: {A} {Unified} {Framework} for {Robot} {Learning}},
	shorttitle = {{RoboHive}},
	url = {http://arxiv.org/abs/2310.06828},
	doi = {10.48550/arXiv.2310.06828},
	abstract = {We present RoboHive, a comprehensive software platform and ecosystem for research in the field of Robot Learning and Embodied Artificial Intelligence. Our platform encompasses a diverse range of pre-existing and novel environments, including dexterous manipulation with the Shadow Hand, whole-arm manipulation tasks with Franka and Fetch robots, quadruped locomotion, among others. Included environments are organized within and cover multiple domains such as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc. In comparison to prior works, RoboHive offers a streamlined and unified task interface taking dependency on only a minimal set of well-maintained packages, features tasks with high physics fidelity and rich visual diversity, and supports common hardware drivers for real-world deployment. The unified interface of RoboHive offers a convenient and accessible abstraction for algorithmic research in imitation, reinforcement, multi-task, and hierarchical learning. Furthermore, RoboHive includes expert demonstrations and baseline results for most environments, providing a standard for benchmarking and comparisons. Details: https://sites.google.com/view/robohive},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Kumar, Vikash and Shah, Rutav and Zhou, Gaoyue and Moens, Vincent and Caggiano, Vittorio and Vakil, Jay and Gupta, Abhishek and Rajeswaran, Aravind},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06828 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{mirjalili_lan-grasp_2023,
	title = {{LAN}-grasp: {Using} {Large} {Language} {Models} for {Semantic} {Object} {Grasping}},
	shorttitle = {{LAN}-grasp},
	url = {http://arxiv.org/abs/2310.05239},
	doi = {10.48550/arXiv.2310.05239},
	abstract = {In this paper, we propose LAN-grasp, a novel approach towards more appropriate semantic grasping. We use foundation models to provide the robot with a deeper understanding of the objects, the right place to grasp an object, or even the parts to avoid. This allows our robot to grasp and utilize objects in a more meaningful and safe manner. We leverage the combination of a Large Language Model, a Vision Language Model, and a traditional grasp planner to generate grasps demonstrating a deeper semantic understanding of the objects. We first prompt the Large Language Model about which object part is appropriate for grasping. Next, the Vision Language Model identifies the corresponding part in the object image. Finally, we generate grasp proposals in the region proposed by the Vision Language Model. Building on foundation models provides us with a zero-shot grasp method that can handle a wide range of objects without the need for further training or fine-tuning. We evaluated our method in real-world experiments on a custom object data set. We present the results of a survey that asks the participants to choose an object part appropriate for grasping. The results show that the grasps generated by our method are consistently ranked higher by the participants than those generated by a conventional grasping planner and a recent semantic grasping approach.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Mirjalili, Reihaneh and Krawez, Michael and Silenzi, Simone and Blei, Yannik and Burgard, Wolfram},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05239 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{drabek_wearing_2010,
	title = {Wearing the wrong size latex surgical gloves impairs manual dexterity},
	volume = {7},
	issn = {1545-9632},
	doi = {10.1080/15459620903481660},
	abstract = {Universal precautions mandate that health care workers wear gloves when dealing with patients, often in situations requiring a high level of technical skill. Although it seems obvious that wearing the wrong size gloves could impair or prolong tasks involving manual dexterity, the issue has not been formally studied. We tested the hypothesis that wearing the wrong size gloves impairs manual dexterity. We administered a grooved pegboard test to 20 healthy, paid, volunteer health care workers. The subjects performed the test with bare hands and while wearing their preferred size of latex surgical gloves, gloves that were a full size smaller, and gloves that were a full size larger. Each subject did three runs with each size glove and three runs with bare hands. The time necessary to insert pegs was measured with a stopwatch. Peg insertion time was not affected by wearing preferred size gloves (vs. bare-handed) but was increased 7-10\% by gloves that were either too small or too large (both effects: P {\textless} 0.05 vs. preferred size; both P {\textless} 0.001 vs. bare-handed). The subjects reported that the too-small gloves limited hand motion or hurt their hands, whereas the too-large gloves were clumsy but comfortable. Health care workers should wear gloves that fit properly when doing tasks that require manual dexterity. If the preferred size is unavailable, wearing gloves that are too large seems the best alternative.},
	language = {eng},
	number = {3},
	journal = {Journal of Occupational and Environmental Hygiene},
	author = {Drabek, Tomas and Boucek, Charles D. and Buffington, Charles W.},
	month = mar,
	year = {2010},
	pmid = {20017056},
	keywords = {Adult, Fingers, Gloves, Surgical, Hand, Health Personnel, Humans, Middle Aged, Motor Skills, Time Factors},
	pages = {152--155},
}

@article{neiburger_latex_1992,
	title = {Latex gloves and manual dexterity. {A} study of 50 {Midwest} dentists},
	volume = {58},
	issn = {0028-7571},
	abstract = {Fifty practicing dentists were each given a series of tests to perform in their own offices with and without gloves on to determine how the use of gloves affected the performance of dental operations. While wearing gloves did not appreciably influence speed, it significantly reduced manual dexterity and light touch perception. And there was a high correlation between wearing gloves and hand dermatitis. Increased age was associated with a slight loss in speed and dexterity but not light touch perception. Women produced better scores than men in all tests.},
	language = {eng},
	number = {1},
	journal = {The New York State Dental Journal},
	author = {Neiburger, E. J.},
	month = jan,
	year = {1992},
	pmid = {1531254},
	keywords = {Adult, Age Factors, Attitude of Health Personnel, Dental Instruments, Dentists, Dermatitis, Contact, Dermatitis, Occupational, Female, Functional Laterality, Gloves, Surgical, Hand, Humans, Male, Middle Aged, Motor Skills, Root Canal Therapy, Rubber, Sex Factors, Time Factors, Touch},
	pages = {24--28},
}

@inproceedings{kim_vision-based_2023,
	title = {Vision-{Based} {Contact} {Localization} {Without} {Touch} or {Force} {Sensing}},
	url = {https://openreview.net/forum?id=h8halpbqB-},
	abstract = {Contacts play a critical role in most manipulation tasks, yet robots today do not have any approach to reliably sense contacts in general settings. Force torque and touch sensing are limited by how little of the world they can sense and by sensor drift, so that today's contact perception approaches built on them require restrictive assumptions or prior knowledge of object geometries and frequent re-calibration. We propose a challenging vision-based extrinsic contact localization task: with only a single RGB-D camera view of a robot workspace, identify when and where an object held by the robot contacts the rest of the environment. We show that careful task-attuned design is critical for a neural network trained in simulation to discover solutions that transfer well to a real robot. Our final approach im2contact demonstrates the promise of versatile general-purpose contact perception from vision alone, performing well for localizing various contact types (point, line, or planar; sticking, sliding, or rolling; single or multiple), and even under occlusions in its camera view.},
	language = {en},
	urldate = {2023-10-11},
	author = {Kim, Leon and Li, Yunshuang and Posa, Michael and Jayaraman, Dinesh},
	month = aug,
	year = {2023},
}

@misc{lu_diva-360_2023,
	title = {{DiVA}-360: {The} {Dynamic} {Visuo}-{Audio} {Dataset} for {Immersive} {Neural} {Fields}},
	shorttitle = {{DiVA}-360},
	url = {https://arxiv.org/abs/2307.16897v1},
	abstract = {Advances in neural fields are enabling high-fidelity capture of the shape and appearance of static and dynamic scenes. However, their capabilities lag behind those offered by representations such as pixels or meshes due to algorithmic challenges and the lack of large-scale real-world datasets. We address the dataset limitation with DiVA-360, a real-world 360 dynamic visual-audio dataset with synchronized multimodal visual, audio, and textual information about table-scale scenes. It contains 46 dynamic scenes, 30 static scenes, and 95 static objects spanning 11 categories captured using a new hardware system using 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M image frames and 1360 s of dynamic data. We provide detailed text descriptions for all scenes, foreground-background segmentation masks, category-specific 3D pose alignment for static objects, as well as metrics for comparison. Our data, hardware and software, and code are available at https://diva360.github.io/.},
	language = {en},
	urldate = {2023-10-10},
	journal = {arXiv.org},
	author = {Lu, Cheng-You and Zhou, Peisen and Xing, Angela and Pokhariya, Chandradeep and Dey, Arnab and Shah, Ishaan and Mavidipalli, Rugved and Hu, Dylan and Comport, Andrew and Chen, Kefan and Sridhar, Srinath},
	month = jul,
	year = {2023},
}

@inproceedings{rupavatharam_sonicfinger_2023,
	title = {{SonicFinger}: {Pre}-touch and {Contact} {Detection} {Tactile} {Sensor} for {Reactive} {Pregrasping}},
	shorttitle = {{SonicFinger}},
	url = {https://ieeexplore.ieee.org/document/10161074},
	doi = {10.1109/ICRA48891.2023.10161074},
	abstract = {Robot end effectors with proximity detection and contact sensing capabilities can reactively position the gripper to align objects and ensure successful grasps. In this paper, we introduce SonicFinger, an acoustic aura based sensing system capable of full-surface pre-touch and contact sensing. A single piezoelectric transducer embedded within a novel 3D printed finger is excited using a monotone to create an acoustic aura encompassing the finger; this enables pre-touch sensing and gripper alignment, while changes in finger-transducer acoustic coupling indicate contact. SonicFinger is low-cost, compact, and easy to manufacture and assemble. Sensing capabilities are evaluated using a set of objects with various physical properties such as optical reflectivity, dielectric constants, mechanical properties, and acoustic absorption. A dataset with over 8,000 proximity and contact events is collected. Our system shows a pre-touch detection true positive rate (TPR) of 92.4\% and a true negative rate (TNR) of 95.3\%. Contact detection experiments show a TPR of 93.7\% and a TNR of 98.7\%. Furthermore, pretouch detection information from Sonic Finger is used to adjust the robot grippers pose to align a target object at the center of both fingers.},
	urldate = {2023-10-09},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Rupavatharam, Siddharth and Escobedo, Caleb and Lee, Daewon and Prepscius, Colin and Jackel, Larry and Howard, Richard and Isler, Volkan},
	month = may,
	year = {2023},
	pages = {12556--12562},
}

@inproceedings{chen_viobject_2021,
	address = {New York, NY, USA},
	series = {{UIST} '21 {Adjunct}},
	title = {{ViObject}: {A} {Smartwatch}-based {Object} {Recognition} {System} via {Vibrations}},
	isbn = {978-1-4503-8655-5},
	shorttitle = {{ViObject}},
	url = {https://dl.acm.org/doi/10.1145/3474349.3480182},
	doi = {10.1145/3474349.3480182},
	abstract = {Today, in order to start an interaction with most digital technology, we must perform some sort of action to indicate our intention, such as shaking a computer’s mouse to wake it or pressing a coffee maker’s start button for your morning cup of coffee. Our system aims to help remove these currently necessary ”trigger actions” and aims to support an array of applications to create borderless and fluid interactions between the technological world and our own. Our system as has the potential for application within the world of accessible technology as well. The system we propose is a method of identifying held objects via a smartwatch’s accelerometer and gyroscope sensors. A preview demo video is available at https://youtu.be/1YCTzvjcJ18.},
	urldate = {2023-10-09},
	booktitle = {Adjunct {Proceedings} of the 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Wenqiang and Bevan, Daniel and Stankovic, John},
	month = oct,
	year = {2021},
	keywords = {Object Recognition, Vibration Intelligence, Wearable Sensing},
	pages = {97--99},
}

@article{chen_vifin_2021,
	title = {{ViFin}: {Harness} {Passive} {Vibration} to {Continuous} {Micro} {Finger} {Writing} with a {Commodity} {Smartwatch}},
	volume = {5},
	shorttitle = {{ViFin}},
	url = {https://dl.acm.org/doi/10.1145/3448119},
	doi = {10.1145/3448119},
	abstract = {Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90\% and 91\% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.},
	number = {1},
	urldate = {2023-10-09},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John},
	month = mar,
	year = {2021},
	keywords = {micro finger writing, text input, vibration intelligence, wearable devices},
	pages = {45:1--45:25},
}

@inproceedings{chen_vitype_2018,
	title = {{ViType}: {A} {Cost} {Efficient} {On}-{Body} {Typing} {System} through {Vibration}},
	shorttitle = {{ViType}},
	url = {https://ieeexplore.ieee.org/document/8397098},
	doi = {10.1109/SAHCN.2018.8397098},
	abstract = {Nowadays, smart wristbands have become one of the most prevailing wearable devices as they are small and portable. However, due to the limited size of the touch screens, smart wristbands typically have poor interactive experience. There are a few works appropriating the human body as a surface to extend the input. Yet by using multiple sensors at high sampling rates, they are not portable and are energy-consuming in practice. To break this stalemate, we proposed a portable, cost efficient text-entry system, termed ViType, which firstly leverages a single small form factor sensor to achieve a practical user input with much lower sampling rates. To enhance the input accuracy with less vibration information introduced by lower sampling rate, ViType designs a set of novel mechanisms, including an artificial neural network to process the vibration signals, and a runtime calibration and adaptation scheme to recover the error due to temporal instability. Extensive experiments have been conducted on 30 human subjects. The results demonstrate that ViType is robust to fight against various confounding factors. The average recognition accuracy is 94.8\% with an initial training sample size of 20 for each key, which is 1.52 times higher than the state-of-the-art on-body typing system. Furthermore, when turning on the runtime calibration and adaptation system to update and enlarge the training sample size, the accuracy can reach around 98\% on average during one month.},
	urldate = {2023-10-09},
	booktitle = {2018 15th {Annual} {IEEE} {International} {Conference} on {Sensing}, {Communication}, and {Networking} ({SECON})},
	author = {Chen, Wenqiang and Guan, Maoning and Huang, Yandao and Wang, Lu and Ruby, Rukhsana and Hu, Wen and Wu, Kaishun},
	month = jun,
	year = {2018},
	note = {ISSN: 2155-5494},
	pages = {1--9},
}

@article{liu_ntu_2020,
	title = {{NTU} {RGB}+{D} 120: {A} {Large}-{Scale} {Benchmark} for {3D} {Human} {Activity} {Understanding}},
	volume = {42},
	issn = {1939-3539},
	shorttitle = {{NTU} {RGB}+{D} 120},
	url = {https://ieeexplore.ieee.org/abstract/document/8713892?casa_token=fbVOc9cRgaMAAAAA:ODiVS1oUvwyqq4P_buRKYhU8jA3rHEh4TLBG7uwEVynut-RmLqVb4p4h4YEfqU2jXKRT2wIxFqo},
	doi = {10.1109/TPAMI.2019.2916873},
	abstract = {Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding.},
	number = {10},
	urldate = {2023-10-08},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C.},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {2684--2701},
}

@inproceedings{kong_mmact_2019,
	title = {{MMAct}: {A} {Large}-{Scale} {Dataset} for {Cross} {Modal} {Human} {Action} {Understanding}},
	shorttitle = {{MMAct}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.html},
	urldate = {2023-10-08},
	author = {Kong, Quan and Wu, Ziming and Deng, Ziwei and Klinkigt, Martin and Tong, Bin and Murakami, Tomokazu},
	year = {2019},
	pages = {8658--8667},
}

@misc{engel_project_2023,
	title = {Project {Aria}: {A} {New} {Tool} for {Egocentric} {Multi}-{Modal} {AI} {Research}},
	shorttitle = {Project {Aria}},
	url = {http://arxiv.org/abs/2308.13561},
	doi = {10.48550/arXiv.2308.13561},
	abstract = {Egocentric, multi-modal data as available on future augmented reality (AR) devices provides unique challenges and opportunities for machine perception. These future devices will need to be all-day wearable in a socially acceptable form-factor to support always available, context-aware and personalized AI applications. Our team at Meta Reality Labs Research built the Aria device, an egocentric, multi-modal data recording and streaming device with the goal to foster and accelerate research in this area. In this paper, we describe the Aria device hardware including its sensor configuration and the corresponding software tools that enable recording and processing of such data.},
	urldate = {2023-10-08},
	publisher = {arXiv},
	author = {Engel, Jakob and Somasundaram, Kiran and Goesele, Michael and Sun, Albert and Gamino, Alexander and Turner, Andrew and Talattof, Arjang and Yuan, Arnie and Souti, Bilal and Meredith, Brighid and Peng, Cheng and Sweeney, Chris and Wilson, Cole and Barnes, Dan and DeTone, Daniel and Caruso, David and Valleroy, Derek and Ginjupalli, Dinesh and Frost, Duncan and Miller, Edward and Mueggler, Elias and Oleinik, Evgeniy and Zhang, Fan and Somasundaram, Guruprasad and Solaira, Gustavo and Lanaras, Harry and Howard-Jenkins, Henry and Tang, Huixuan and Kim, Hyo Jin and Rivera, Jaime and Luo, Ji and Dong, Jing and Straub, Julian and Bailey, Kevin and Eckenhoff, Kevin and Ma, Lingni and Pesqueira, Luis and Schwesinger, Mark and Monge, Maurizio and Yang, Nan and Charron, Nick and Raina, Nikhil and Parkhi, Omkar and Borschowa, Peter and Moulon, Pierre and Gupta, Prince and Mur-Artal, Raul and Pennington, Robbie and Kulkarni, Sachin and Miglani, Sagar and Gondi, Santosh and Solanki, Saransh and Diener, Sean and Cheng, Shangyi and Green, Simon and Saarinen, Steve and Patra, Suvam and Mourikis, Tassos and Whelan, Thomas and Singh, Tripti and Balntas, Vasileios and Baiyya, Vijay and Dreewes, Wilson and Pan, Xiaqing and Lou, Yang and Zhao, Yipu and Mansour, Yusuf and Zou, Yuyang and Lv, Zhaoyang and Wang, Zijian and Yan, Mingfei and Ren, Carl and De Nardi, Renzo and Newcombe, Richard},
	month = oct,
	year = {2023},
	note = {arXiv:2308.13561 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
}

@article{delpreto_actionsense_2022,
	title = {{ActionSense}: {A} {Multimodal} {Dataset} and {Recording} {Framework} for {Human} {Activities} {Using} {Wearable} {Sensors} in a {Kitchen} {Environment}},
	volume = {35},
	shorttitle = {{ActionSense}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5985e81d65605827ac35401999aea22a-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2023-10-07},
	journal = {Advances in Neural Information Processing Systems},
	author = {DelPreto, Joseph and Liu, Chao and Luo, Yiyue and Foshey, Michael and Li, Yunzhu and Torralba, Antonio and Matusik, Wojciech and Rus, Daniela},
	month = dec,
	year = {2022},
	pages = {13800--13813},
}

@article{gibson_adaptation_1933,
	title = {Adaptation, after-effect and contrast in the perception of curved lines},
	volume = {16},
	issn = {0022-1015},
	doi = {10.1037/h0074626},
	abstract = {A series of eight experiments in which the S's wore a pair of prisms shifting the visual field about 15° to the right, and—more important to the problem—the examination of the phenomenal bending of vertical straight lines into curves convex to the left, the horizontal components of the figures remaining undistorted. During an hour's observation "a curved line becomes phenomenally less curved than it was at the beginning of the period, and at the end of the period an objectively straight line will seem curved in the opposite direction. This fact holds whether the curvature is actually in the object, or is induced by the distorting effect of the prisms." Two theoretical considerations are offered and examined, preliminary to further experimentation: (1) The effects noted may be ascribed to conflict between experiences designated as visual and kinesthetic, following the discussions of Stratton, Wooster, Young, and Ewert on variations of the same general problem. (2) The effects may be described in terms of a function of the perceptual process "akin to sensory adaptation." The first argument is rejected in favor of the second, and further observational evidence is adduced in support from experiments 2 to 9 inclusive. The adaptation effect and the negative after-effect are of the same degree of magnitude, both simultaneous and successive contrast may be demonstrated and both these effects occur for kinesthetic as well as for visual perception. The essential condition for adaptation and after-effect seems not to be mere curvature of line, but departure from rectilinearity, since the phenomena appear as well on fixating an obtuse angle. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {1},
	journal = {Journal of Experimental Psychology},
	author = {Gibson, J. J.},
	year = {1933},
	note = {Place: US
Publisher: Psychological Review Company},
	keywords = {Adaptation, Visual Field, Visual Perception},
	pages = {1--31},
}

@article{tastevin_en_1937,
	title = {En partant de l'expérience d'{Aristote} les déplacements artificiels des parties du corps ne sont pas suivis par le sentiment de ces parties ni par les sensations qu'on peut y produire. [{Starting} from {Aristotle}'s experiment the artificial displacements of parts of the body are not followed by feeling in these parts or by the sensations which can be produced there.]},
	volume = {32},
	issn = {0013-7006},
	abstract = {If a small ball is placed between the ends of two crossed fingers, the sense of touch causes two balls to be perceived: this is Aristotle's experiment. The author considers: (1) the inversion of the sensations produced on the crossed fingers, (2) the perceptions of the crossed fingers while in natural position, (3) the errors of tactile relief produced by the crossed fingers, (4) the false localizations obtained by artificial displacements of different parts of the body, (5) the problem of outward perception and the theory of localizing judgments. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {L'Encéphale: Revue de psychiatrie clinique biologique et thérapeutique},
	author = {Tastevin, J.},
	year = {1937},
	note = {Place: France
Publisher: Masson},
	pages = {57--84; 140--158},
}

@article{postema_haptic_2021,
	title = {Haptic exploration improves performance of a laparoscopic training task},
	volume = {35},
	issn = {0930-2794},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8263408/},
	doi = {10.1007/s00464-020-07898-6},
	abstract = {Background
Laparoscopy has reduced tactile and visual feedback compared to open surgery. There is increasing evidence that visual and haptic information converge to form a more robust mental representation of an object. We investigated whether tactile exploration of an object prior to executing a laparoscopic action on it improves performance.

Methods
A prospective cohort study with 20 medical students randomized in two different groups was conducted. A silicone ileocecal model, on which a laparoscopic action had to be performed, was used inside an outside a ForceSense box trainer. During the pre-test, students either did a combined manual and visual exploration or only visual exploration of the caecum model. To track performance during the trials of the study we used force, motion and time parameters as representatives of technical skills development. The final trial data were used for statistical comparison between groups.

Results
All included time and motion parameters did not show any clear differences between groups. However, the force parameters Mean force non-zero (p = 004), Maximal force (p = 0.01) Maximal impulse (p = 0.02), Force volume (p = 0.02) and SD force (p = 0.01) showed significant lower values in favour of the tactile exploration group for the final trials.

Conclusions
By adding haptic sensation to the existing visual information during training of laparoscopic tasks on life-like models, tissue manipulation skills improve during training.

Electronic supplementary material
The online version of this article 10.1007/s00464-020-07898-6 contains supplementary material, which is available to authorized users.},
	number = {8},
	urldate = {2023-10-06},
	journal = {Surgical Endoscopy},
	author = {Postema, Roelf R. and van Gastel, Leonie A. and Hardon, Sem F. and Bonjer, H. Jaap and Horeman, Tim},
	year = {2021},
	pmid = {32875419},
	pmcid = {PMC8263408},
	pages = {4175--4182},
}

@article{zhou_effect_2012,
	title = {Effect of {Haptic} {Feedback} in {Laparoscopic} {Surgery} {Skill} {Acquisition}},
	volume = {26},
	issn = {0930-2794},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3321358/},
	doi = {10.1007/s00464-011-2011-8},
	abstract = {Background
The benefits of haptic feedback in laparoscopic surgery training simulators is a topic of debate in the literature. It is hypothesized that novice surgeons may not benefit from the haptic information, especially during the initial phase of learning a new task. Therefore, providing haptic feedback to novice trainees in the early stage of training may be distracting and detrimental to learning.

Objective
A controlled experiment was conducted to examine the effect of haptic feedback on the learning curve of a complex laparoscopic suturing and knot-tying task.

Method
The ProMIS and the MIST-VR surgical simulators were used to represent conditions with and without haptic feedback, respectively. Twenty novice subjects (10 per simulator) were trained to perform suturing and knot-tying and practiced the tasks over eighteen one-hour sessions.

Results
At the end of the 3-week training period, subjects performed equally fast but more consistently with haptics (ProMIS) than without (MIST-VR). Subjects showed slightly higher learning rate and reached the first plateau of the learning curve earlier with haptic feedback.

Conclusion
In general, learning with haptic feedback was significantly better than without haptic feedback for a laparoscopic suturing and knot-tying task, but only in the first 5 hours of training.

Application
Haptic feedback may not be warranted in laparoscopic surgical trainers. The benefits of a shorter time to the first performance plateau and more consistent initial performance should be balanced with the cost of implementing haptic feedback in surgical simulators.},
	number = {4},
	urldate = {2023-10-06},
	journal = {Surgical Endoscopy},
	author = {Zhou, M. and Tse, S. and Derevianko, A. and Jones, D.B. and Schwaitzberg, S.D. and Cao, C. G. L.},
	month = apr,
	year = {2012},
	pmid = {22044975},
	pmcid = {PMC3321358},
	pages = {1128--1134},
}

@article{wesslein_vision_2014,
	title = {Vision affects tactile target and distractor processing even when space is task-irrelevant},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3915095/},
	doi = {10.3389/fpsyg.2014.00084},
	abstract = {The human brain is adapted to integrate the information from multiple sensory modalities into coherent, robust representations of the objects and events in the external world. A large body of empirical research has demonstrated the ubiquitous nature of the interactions that take place between vision and touch, with the former typically dominating over the latter. Many studies have investigated the influence of visual stimuli on the processing of tactile stimuli (and vice versa). Other studies, meanwhile, have investigated the effect of directing a participant’s gaze either toward or else away from the body-part receiving the target tactile stimulation. Other studies, by contrast, have compared performance in those conditions in which the participant’s eyes have been open versus closed. We start by reviewing the research that has been published to date demonstrating the influence of vision on the processing of tactile targets, that is, on those stimuli that have to be attended or responded to. We outline that many – but not all – of the visuotactile interactions that have been observed to date may be attributable to the direction of spatial attention. We then move on to focus on the crossmodal influence of vision, as well as of the direction of gaze, on the processing of tactile distractors. We highlight the results of those studies demonstrating the influence of vision, rather than gaze direction (i.e., the direction of overt spatial attention), on tactile distractor processing (e.g., tactile variants of the negative-priming or flanker task). The conclusion is that no matter how vision of a tactile distractor is engaged, the result would appear to be the same, namely that tactile distractors are processed more thoroughly.},
	urldate = {2023-10-06},
	journal = {Frontiers in Psychology},
	author = {Wesslein, Ann-Katrin and Spence, Charles and Frings, Christian},
	month = feb,
	year = {2014},
	pmid = {24567727},
	pmcid = {PMC3915095},
	pages = {84},
}

@article{klatzky_identifying_1985,
	title = {Identifying objects by touch: {An} “expert system”},
	volume = {37},
	issn = {1532-5962},
	shorttitle = {Identifying objects by touch},
	url = {https://doi.org/10.3758/BF03211351},
	doi = {10.3758/BF03211351},
	abstract = {How good are we at recognizing objects by touch? Intuition may suggest that the haptic system is a poor recognition device, and previous research with nonsense shapes and tangible-graphics displays supports this opinion. We argue that the recognition capabilities of touch are best assessed with three-dimensional, familiar objects. The present study provides a baseline measure of recognition under those circumstances, and it indicates that haptic object recognition can be both rapid and accurate.},
	language = {en},
	number = {4},
	urldate = {2023-10-06},
	journal = {Perception \& Psychophysics},
	author = {Klatzky, Roberta L. and Lederman, Susan J. and Metzger, Victoria A.},
	month = jul,
	year = {1985},
	keywords = {Haptic Exploration, Haptic Object, Haptic Perception, Haptic System, Odor Identification},
	pages = {299--302},
}

@inproceedings{zhao_learning_2023,
	title = {Learning {Symmetry}-{Aware} {Geometry} {Correspondences} for {6D} {Object} {Pose} {Estimation}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-10-04},
	author = {Zhao, Heng and Wei, Shenxing and Shi, Dahu and Tan, Wenming and Li, Zheyang and Ren, Ye and Wei, Xing and Yang, Yi and Pu, Shiliang},
	year = {2023},
	pages = {14045--14054},
}

@inproceedings{zeng_parameterized_2023,
	title = {Parameterized {Cost} {Volume} for {Stereo} {Matching}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Zeng_Parameterized_Cost_Volume_for_Stereo_Matching_ICCV_2023_paper.html},
	language = {en},
	urldate = {2023-10-04},
	author = {Zeng, Jiaxi and Yao, Chengtang and Yu, Lidong and Wu, Yuwei and Jia, Yunde},
	year = {2023},
	pages = {18347--18357},
}

@misc{yang_generalized_2023,
	title = {Generalized {Animal} {Imitator}: {Agile} {Locomotion} with {Versatile} {Motion} {Prior}},
	shorttitle = {Generalized {Animal} {Imitator}},
	url = {http://arxiv.org/abs/2310.01408},
	doi = {10.48550/arXiv.2310.01408},
	abstract = {The agility of animals, particularly in complex activities such as running, turning, jumping, and backflipping, stands as an exemplar for robotic system design. Transferring this suite of behaviors to legged robotic systems introduces essential inquiries: How can a robot be trained to learn multiple locomotion behaviors simultaneously? How can the robot execute these tasks with a smooth transition? And what strategies allow for the integrated application of these skills? This paper introduces the Versatile Instructable Motion prior (VIM) - a Reinforcement Learning framework designed to incorporate a range of agile locomotion tasks suitable for advanced robotic applications. Our framework enables legged robots to learn diverse agile low-level skills by imitating animal motions and manually designed motions with Functionality reward and Stylization reward. While the Functionality reward guides the robot's ability to adopt varied skills, the Stylization reward ensures performance alignment with reference motions. Our evaluations of the VIM framework span both simulation environments and real-world deployment. To our understanding, this is the first work that allows a robot to concurrently learn diverse agile locomotion tasks using a singular controller. Further details and supportive media can be found at our project site: https://rchalyang.github.io/VIM .},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Yang, Ruihan and Chen, Zhuoqun and Ma, Jianhan and Zheng, Chongyi and Chen, Yiyu and Nguyen, Quan and Wang, Xiaolong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01408 null},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{ledig_photo-realistic_2017,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html},
	urldate = {2023-10-03},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year = {2017},
	pages = {4681--4690},
}

@misc{gu_conceptgraphs_2023,
	title = {{ConceptGraphs}: {Open}-{Vocabulary} {3D} {Scene} {Graphs} for {Perception} and {Planning}},
	shorttitle = {{ConceptGraphs}},
	url = {http://arxiv.org/abs/2309.16650},
	doi = {10.48550/arXiv.2309.16650},
	abstract = {For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and de Melo, Celso Miguel and Tenenbaum, Joshua B. and Torralba, Antonio and Shkurti, Florian and Paull, Liam},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16650 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{luo_localizing_2015,
	title = {Localizing the {Object} {Contact} through {Matching} {Tactile} {Features} with {Visual} {Map}},
	url = {http://arxiv.org/abs/1708.04441},
	doi = {10.1109/ICRA.2015.7139743},
	abstract = {This paper presents a novel framework for integration of vision and tactile sensing by localizing tactile readings in a visual object map. Intuitively, there are some correspondences, e.g., prominent features, between visual and tactile object identification. To apply it in robotics, we propose to localize tactile readings in visual images by sharing same sets of feature descriptors through two sensing modalities. It is then treated as a probabilistic estimation problem solved in a framework of recursive Bayesian filtering. Feature-based measurement model and Gaussian based motion model are thus built. In our tests, a tactile array sensor is utilized to generate tactile images during interaction with objects and the results have proven the feasibility of our proposed framework.},
	urldate = {2023-04-26},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Luo, Shan and Mou, Wenxuan and Althoefer, Kaspar and Liu, Hongbin},
	month = may,
	year = {2015},
	note = {arXiv:1708.04441 [cs]},
	keywords = {Computer Science - Robotics, Feature extraction, Shape, Tactile sensors, Visualization},
	pages = {3903--3908},
}

@inproceedings{shan_understanding_2020,
	title = {Understanding {Human} {Hands} in {Contact} at {Internet} {Scale}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html},
	urldate = {2022-08-06},
	author = {Shan, Dandan and Geng, Jiaqi and Shu, Michelle and Fouhey, David F.},
	year = {2020},
	pages = {9869--9878},
}

@inproceedings{grauman_ego4d_2022,
	title = {{Ego4D}: {Around} the {World} in 3,000 {Hours} of {Egocentric} {Video}},
	shorttitle = {{Ego4D}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-09-30},
	author = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Gebreselasie, Abrham and González, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolář, Jáchym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Ruiz, Paola and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhao, Ziwei and Zhu, Yunyi and Arbeláez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Fuegen, Christian and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
	year = {2022},
	pages = {18995--19012},
}

@inproceedings{goyal_something_2017,
	title = {The "{Something} {Something}" {Video} {Database} for {Learning} and {Evaluating} {Visual} {Common} {Sense}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Goyal_The_Something_Something_ICCV_2017_paper.html},
	urldate = {2023-09-30},
	author = {Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and Hoppe, Florian and Thurau, Christian and Bax, Ingo and Memisevic, Roland},
	year = {2017},
	pages = {5842--5850},
}

@article{damen_rescaling_2022,
	title = {Rescaling {Egocentric} {Vision}: {Collection}, {Pipeline} and {Challenges} for {EPIC}-{KITCHENS}-100},
	volume = {130},
	issn = {1573-1405},
	shorttitle = {Rescaling {Egocentric} {Vision}},
	url = {https://doi.org/10.1007/s11263-021-01531-2},
	doi = {10.1007/s11263-021-01531-2},
	abstract = {This paper introduces the pipeline to extend the largest dataset in egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (Damen in Scaling egocentric vision: ECCV, 2018), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54\% more actions per minute) and more complete annotations of fine-grained actions (+128\% more action segments). This collection enables new challenges such as action detection and evaluating the “test of time”—i.e. whether models trained on data collected in 2018 can generalise to new footage collected two years later. The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.},
	language = {en},
	number = {1},
	urldate = {2023-09-30},
	journal = {International Journal of Computer Vision},
	author = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino and Kazakos, Evangelos and Ma, Jian and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
	month = jan,
	year = {2022},
	keywords = {Action understanding, Annotation quality, Egocentric vision, First-person vision, Multi-benchmark large-scale dataset, Video dataset},
	pages = {33--55},
}

@inproceedings{damen_scaling_2018,
	title = {Scaling {Egocentric} {Vision}: {The} {EPIC}-{KITCHENS} {Dataset}},
	shorttitle = {Scaling {Egocentric} {Vision}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.html},
	urldate = {2023-09-30},
	author = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
	year = {2018},
	pages = {720--736},
}

@inproceedings{radosavovic_real-world_2023,
	title = {Real-{World} {Robot} {Learning} with {Masked} {Visual} {Pre}-training},
	url = {https://proceedings.mlr.press/v205/radosavovic23a.html},
	abstract = {In this work, we explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual representations are pre-trained via a masked autoencoder (MAE), frozen, and then passed into a learnable control module. Unlike prior work, we show that the pre-trained representations are effective across a range of real-world robotic tasks and embodiments. We find that our encoder consistently outperforms CLIP (up to 75\%), supervised ImageNet pre-training (up to 81\%), and training from scratch (up to 81\%). Finally, we train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.},
	language = {en},
	urldate = {2023-09-30},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Radosavovic, Ilija and Xiao, Tete and James, Stephen and Abbeel, Pieter and Malik, Jitendra and Darrell, Trevor},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {416--426},
}

@misc{shah_mutex_2023,
	title = {{MUTEX}: {Learning} {Unified} {Policies} from {Multimodal} {Task} {Specifications}},
	shorttitle = {{MUTEX}},
	url = {http://arxiv.org/abs/2309.14320},
	doi = {10.48550/arXiv.2309.14320},
	abstract = {Humans use different modalities, such as speech, text, images, videos, etc., to communicate their intent and goals with teammates. For robots to become better assistants, we aim to endow them with the ability to follow instructions and understand tasks specified by their human partners. Most robotic policy learning methods have focused on one single modality of task specification while ignoring the rich cross-modal information. We present MUTEX, a unified approach to policy learning from multimodal task specifications. It trains a transformer-based architecture to facilitate cross-modal reasoning, combining masked modeling and cross-modal matching objectives in a two-stage training procedure. After training, MUTEX can follow a task specification in any of the six learned modalities (video demonstrations, goal images, text goal descriptions, text instructions, speech goal descriptions, and speech instructions) or a combination of them. We systematically evaluate the benefits of MUTEX in a newly designed dataset with 100 tasks in simulation and 50 tasks in the real world, annotated with multiple instances of task specifications in different modalities, and observe improved performance over methods trained specifically for any single modality. More information at https://ut-austin-rpl.github.io/MUTEX/},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Shah, Rutav and Martín-Martín, Roberto and Zhu, Yuke},
	month = sep,
	year = {2023},
	note = {arXiv:2309.14320 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{li_contact2grasp_2023,
	title = {{Contact2Grasp}: {3D} {Grasp} {Synthesis} via {Hand}-{Object} {Contact} {Constraint}},
	shorttitle = {{Contact2Grasp}},
	url = {http://arxiv.org/abs/2210.09245},
	doi = {10.48550/arXiv.2210.09245},
	abstract = {3D grasp synthesis generates grasping poses given an input object. Existing works tackle the problem by learning a direct mapping from objects to the distributions of grasping poses. However, because the physical contact is sensitive to small changes in pose, the high-nonlinear mapping between 3D object representation to valid poses is considerably non-smooth, leading to poor generation efficiency and restricted generality. To tackle the challenge, we introduce an intermediate variable for grasp contact areas to constrain the grasp generation; in other words, we factorize the mapping into two sequential stages by assuming that grasping poses are fully constrained given contact maps: 1) we first learn contact map distributions to generate the potential contact maps for grasps; 2) then learn a mapping from the contact maps to the grasping poses. Further, we propose a penetration-aware optimization with the generated contacts as a consistency constraint for grasp refinement. Extensive validations on two public datasets show that our method outperforms state-of-the-art methods regarding grasp generation on various metrics.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Li, Haoming and Lin, Xinzhuo and Zhou, Yang and Li, Xiang and Huo, Yuchi and Chen, Jiming and Ye, Qi},
	month = may,
	year = {2023},
	note = {arXiv:2210.09245 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{wu_embodied_2023,
	title = {Embodied {Task} {Planning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.01848},
	doi = {10.48550/arXiv.2307.01848},
	abstract = {Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{hassan_populating_2021,
	title = {Populating {3D} {Scenes} by {Learning} {Human}-{Scene} {Interaction}},
	url = {http://arxiv.org/abs/2012.11581},
	doi = {10.48550/arXiv.2012.11581},
	abstract = {Humans live within a 3D space and constantly interact with it to perform tasks. Such interactions involve physical contact between surfaces that is semantically meaningful. Our goal is to learn how humans interact with scenes and leverage this to enable virtual characters to do the same. To that end, we introduce a novel Human-Scene Interaction (HSI) model that encodes proximal relationships, called POSA for "Pose with prOximitieS and contActs". The representation of interaction is body-centric, which enables it to generalize to new scenes. Specifically, POSA augments the SMPL-X parametric human body model such that, for every mesh vertex, it encodes (a) the contact probability with the scene surface and (b) the corresponding semantic scene label. We learn POSA with a VAE conditioned on the SMPL-X vertices, and train on the PROX dataset, which contains SMPL-X meshes of people interacting with 3D scenes, and the corresponding scene semantics from the PROX-E dataset. We demonstrate the value of POSA with two applications. First, we automatically place 3D scans of people in scenes. We use a SMPL-X model fit to the scan as a proxy and then find its most likely placement in 3D. POSA provides an effective representation to search for "affordances" in the scene that match the likely contact relationships for that pose. We perform a perceptual study that shows significant improvement over the state of the art on this task. Second, we show that POSA's learned representation of body-scene interaction supports monocular human pose estimation that is consistent with a 3D scene, improving on the state of the art. Our model and code are available for research purposes at https://posa.is.tue.mpg.de.},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Hassan, Mohamed and Ghosh, Partha and Tesch, Joachim and Tzionas, Dimitrios and Black, Michael J.},
	month = apr,
	year = {2021},
	note = {arXiv:2012.11581 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xiao_unified_2023,
	title = {Unified {Human}-{Scene} {Interaction} via {Prompted} {Chain}-of-{Contacts}},
	url = {http://arxiv.org/abs/2309.07918},
	doi = {10.48550/arXiv.2309.07918},
	abstract = {Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. This framework is built upon the definition of interaction as Chain of Contacts (CoC): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. The project page is at https://github.com/OpenRobotLab/UniHSI .},
	urldate = {2023-09-26},
	publisher = {arXiv},
	author = {Xiao, Zeqi and Wang, Tai and Wang, Jingbo and Cao, Jinkun and Zhang, Wenwei and Dai, Bo and Lin, Dahua and Pang, Jiangmiao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07918 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hafner_dream_2020,
	title = {Dream to {Control}: {Learning} {Behaviors} by {Latent} {Imagination}},
	shorttitle = {Dream to {Control}},
	url = {http://arxiv.org/abs/1912.01603},
	abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
	month = mar,
	year = {2020},
	note = {arXiv:1912.01603 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hoeller_learning_2021,
	title = {Learning a {State} {Representation} and {Navigation} in {Cluttered} and {Dynamic} {Environments}},
	url = {http://arxiv.org/abs/2103.04351},
	doi = {10.48550/arXiv.2103.04351},
	abstract = {In this work, we present a learning-based pipeline to realise local navigation with a quadrupedal robot in cluttered environments with static and dynamic obstacles. Given high-level navigation commands, the robot is able to safely locomote to a target location based on frames from a depth camera without any explicit mapping of the environment. First, the sequence of images and the current trajectory of the camera are fused to form a model of the world using state representation learning. The output of this lightweight module is then directly fed into a target-reaching and obstacle-avoiding policy trained with reinforcement learning. We show that decoupling the pipeline into these components results in a sample efficient policy learning stage that can be fully trained in simulation in just a dozen minutes. The key part is the state representation, which is trained to not only estimate the hidden state of the world in an unsupervised fashion, but also helps bridging the reality gap, enabling successful sim-to-real transfer. In our experiments with the quadrupedal robot ANYmal in simulation and in reality, we show that our system can handle noisy depth images, avoid dynamic obstacles unseen during training, and is endowed with local spatial awareness.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Hoeller, David and Wellhausen, Lorenz and Farshidian, Farbod and Hutter, Marco},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04351 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{chang_semantic_2020,
	title = {Semantic {Visual} {Navigation} by {Watching} {YouTube} {Videos}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html},
	abstract = {Semantic cues and statistical regularities in real-world environment layouts can improve efficiency for navigation in novel environments. This paper learns and leverages such semantic cues for navigating to objects of interest in novel environments, by simply watching YouTube videos. This is challenging because YouTube videos don't come with labels for actions or goals, and may not even showcase optimal behavior. Our method tackles these challenges through the use of Q-learning on pseudo-labeled transition quadruples (image, action, next image, reward). We show that such off-policy Q-learning from passive data is able to learn meaningful semantic cues for navigation. These cues, when used in a hierarchical navigation policy, lead to improved efficiency at the ObjectGoal task in visually realistic simulations. We observe a relative improvement of 15-83\% over end-to-end RL, behavior cloning, and classical methods, while using minimal direct interaction.},
	urldate = {2023-06-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Matthew and Gupta, Arjun and Gupta, Saurabh},
	year = {2020},
	pages = {4283--4294},
}

@misc{garrett_integrated_2020,
	title = {Integrated {Task} and {Motion} {Planning}},
	url = {http://arxiv.org/abs/2010.01083},
	doi = {10.48550/arXiv.2010.01083},
	abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	month = oct,
	year = {2020},
	note = {arXiv:2010.01083 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{jamali_material_2010,
	title = {Material classification by tactile sensing using surface textures},
	doi = {10.1109/ROBOT.2010.5509675},
	abstract = {In this paper we describe an application of machine learning to distinguish between seven different materials, based on their surface texture. Applications of such a system includes quality assurance and estimating surface friction during manipulation tasks. A naive Bayes classifier is used to distinguish textures sensed by a bio-inspired artificial finger. The finger has randomly distributed strain gauges and Polyvinylidene Fluoride (PVDF) films embedded in silicone. Different textures induce different intensity of vibrations in the silicone. Textures can be distinguished by the presence of different frequencies in the signal. The data from the finger is pre-processed and the Fourier coefficients of the sensor outputs are used to learn a classifier for different textures. The performance of the classifier is evaluated against a naive time domain based learner. Preliminary results show that our classifier performs better.},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Jamali, Nawid and Sammut, Claude},
	month = may,
	year = {2010},
	note = {ISSN: 1050-4729},
	keywords = {Anisotropic magnetoresistance, Friction, Gravity, Joining processes, Mobile robots, Robot kinematics, Robotics and automation, Shape, Surface texture, Wheels},
	pages = {2336--2341},
}

@inproceedings{li_tata_2022,
	title = {{TaTa}: {A} {Universal} {Jamming} {Gripper} with {High}-{Quality} {Tactile} {Perception} and {Its} {Application} to {Underwater} {Manipulation}},
	shorttitle = {{TaTa}},
	doi = {10.1109/ICRA46639.2022.9811806},
	abstract = {Large-area and high-precision tactile sensing information can not only improve the stability of robot grasping but also compensate for the lack of visual information in specific environments such as turbid underwater, dimness, and smoke. In this paper, we devise a universal jamming gripper with high-quality tactile sensing capability. The gripper adopts the particle jamming mechanism for grasping, and simultaneously uses a built-in camera to detect the deformation of its surface to obtain tactile information. To make the inside of the gripper transparent, glass beads and liquid with the same refractive index are applied as the internal filling. Besides, special treatments are taken to improve the tactile perception resolution of the gripper. The design perfectly merges visual-based tactile sensing into the traditional universal jamming gripper without changing its original gripping performance, making it possible for simultaneous grasping and sensing. To verify the tactile perception and grasping ability of the gripper in specific environments, we design two underwater experiments for grasping and pipe leak detection based on tactile information. Both have achieved a success rate not less than 95\%, which demonstrates the effectiveness of the proposed gripper for manipulation in low visibility environments.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Shoujie and Yin, Xianghui and Xia, Chongkun and Ye, Linqi and Wang, Xueqian and Liang, Bin},
	month = may,
	year = {2022},
	keywords = {Grasping, Grippers, Refractive index, Robot vision systems, Sensors, Stability criteria, Visualization},
	pages = {6151--6157},
}

@inproceedings{buscher_augmenting_2015,
	title = {Augmenting curved robot surfaces with soft tactile skin},
	doi = {10.1109/IROS.2015.7353568},
	abstract = {We present a novel, soft, tactile skin composed of a fabric-based, stretchable sensor technology based on the piezoresistive effect. Softness is achieved by a combination of a soft silicone padding covered by a skin of more durable, tearproof silicone with an imprinted surface pattern mimicking human glabrous skin, found e.g. in fingertips. Its very thin layer structure (starting from 2.5 mm) facilitates integration on existing robot surfaces, particularly on small and highly curved links. For example, we augmented our Shadow Dexterous Hand with 12 palm sensors, and 2 resp. 3 sensors in the middle resp. proximal phalanges of each finger. To demonstrate the usefulness and efficiency of the proposed sensor skin, we performed a challenging classification task distinguishing squeezed objects based on their varying stiffness.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Büscher, Gereon and Meier, Martin and Walck, Guillaume and Haschke, Robert and Ritter, Helge J.},
	month = sep,
	year = {2015},
	keywords = {Fabrics, Force, Piezoresistance, Robot sensing systems, Skin},
	pages = {1514--1519},
}

@article{ward-cherrier_tactip_2018,
	title = {The {TacTip} {Family}: {Soft} {Optical} {Tactile} {Sensors} with {3D}-{Printed} {Biomimetic} {Morphologies}},
	volume = {5},
	issn = {2169-5172},
	shorttitle = {The {TacTip} {Family}},
	url = {https://www.liebertpub.com/doi/full/10.1089/soro.2017.0052},
	doi = {10.1089/soro.2017.0052},
	abstract = {Tactile sensing is an essential component in human–robot interaction and object manipulation. Soft sensors allow for safe interaction and improved gripping performance. Here we present the TacTip family of sensors: a range of soft optical tactile sensors with various morphologies fabricated through dual-material 3D printing. All of these sensors are inspired by the same biomimetic design principle: transducing deformation of the sensing surface via movement of pins analogous to the function of intermediate ridges within the human fingertip. The performance of the TacTip, TacTip-GR2, TacTip-M2, and TacCylinder sensors is here evaluated and shown to attain submillimeter accuracy on a rolling cylinder task, representing greater than 10-fold super-resolved acuity. A version of the TacTip sensor has also been open-sourced, enabling other laboratories to adopt it as a platform for tactile sensing and manipulation research. These sensors are suitable for real-world applications in tactile perception, exploration, and manipulation, and will enable further research and innovation in the field of soft tactile sensing.},
	number = {2},
	urldate = {2023-09-14},
	journal = {Soft Robotics},
	author = {Ward-Cherrier, Benjamin and Pestell, Nicholas and Cramphorn, Luke and Winstone, Benjamin and Giannaccini, Maria Elena and Rossiter, Jonathan and Lepora, Nathan F.},
	month = apr,
	year = {2018},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	keywords = {dexterous manipulation, soft sensors, tactile sensors},
	pages = {216--227},
}

@article{jamali_majority_2011,
	title = {Majority {Voting}: {Material} {Classification} by {Tactile} {Sensing} {Using} {Surface} {Texture}},
	volume = {27},
	issn = {1941-0468},
	shorttitle = {Majority {Voting}},
	doi = {10.1109/TRO.2011.2127110},
	abstract = {In this paper, we present an application of machine learning to distinguish between different materials based on their surface texture. Such a system can be used for the estimation of surface friction during manipulation tasks; quality assurance in the textile, cosmetics, and harvesting industries; and other applications requiring tactile sensing. Several machine learning algorithms, such as naive Bayes, decision trees, and naive Bayes trees, have been trained to distinguish textures sensed by a biologically inspired artificial finger. The finger has randomly distributed strain gauges and polyvinylidene fluoride (PVDF) films embedded in silicone. Different textures induce different intensities of vibrations in the silicone. Consequently, textures can be distinguished by the presence of different frequencies in the signal. The data from the finger are preprocessed, and the Fourier coefficients of the sensor outputs are used to train classifiers. We show that the classifiers generalize well for unseen datasets with performance exceeding previously reported algorithms. Our classifiers can distinguish between different materials, such as carpet, flooring vinyls, tiles, sponge, wood, and polyvinyl-chloride (PVC) woven mesh with an accuracy of on unseen test data.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Jamali, Nawid and Sammut, Claude},
	month = jun,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Frequency-domain analysis, Humans, Materials, Strain, Tactile sensors, machine learning, tactile sensing, texture classification},
	pages = {508--521},
}

@article{gervet_navigating_2023,
	title = {Navigating to objects in the real world},
	volume = {8},
	url = {https://www.science.org/doi/abs/10.1126/scirobotics.adf6991},
	doi = {10.1126/scirobotics.adf6991},
	abstract = {Semantic navigation is necessary to deploy mobile robots in uncontrolled environments such as homes or hospitals. Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation, which builds a geometric map using depth sensors and plans to reach point goals. Broadly, end-to-end learning approaches reactively map sensor inputs to actions with deep neural networks, whereas modular learning approaches enrich the classical pipeline with learning-based semantic sensing and exploration. However, learned visual navigation policies have predominantly been evaluated in sim, with little known about what works on a robot. We present a large-scale empirical study of semantic visual navigation methods comparing representative methods with classical, modular, and end-to-end learning approaches across six homes with no prior experience, maps, or instrumentation. We found that modular learning works well in the real world, attaining a 90\% success rate. In contrast, end-to-end learning does not, dropping from 77\% sim to a 23\% real-world success rate because of a large image domain gap between sim and reality. For practitioners, we show that modular learning is a reliable approach to navigate to objects: Modularity and abstraction in policy design enable sim-to-real transfer. For researchers, we identify two key issues that prevent today’s simulators from being reliable evaluation benchmarks—a large sim-to-real gap in images and a disconnect between sim and real-world error modes—and propose concrete steps forward.},
	number = {79},
	urldate = {2023-09-12},
	journal = {Science Robotics},
	author = {Gervet, Theophile and Chintala, Soumith and Batra, Dhruv and Malik, Jitendra and Chaplot, Devendra Singh},
	month = jun,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadf6991},
}

@inproceedings{zhang_self-supervised_2022,
	title = {Self-{Supervised} {Geometric} {Correspondence} for {Category}-{Level} {6D} {Object} {Pose} {Estimation} in the {Wild}},
	url = {https://openreview.net/forum?id=ZKDUlVMqG_O},
	abstract = {While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other downstream tasks such as keypoint transfer. Surprisingly, our method, without any human annotations or simulators, can achieve on-par or even better performance than previous supervised or semi-supervised methods on in-the-wild images. Code and videos are available at https://kywind.github.io/self-pose.},
	language = {en},
	urldate = {2023-09-12},
	author = {Zhang, Kaifeng and Fu, Yang and Borse, Shubhankar and Cai, Hong and Porikli, Fatih and Wang, Xiaolong},
	month = sep,
	year = {2022},
}

@inproceedings{jiang_synergies_2021,
	title = {Synergies {Between} {Affordance} and {Geometry}: 6-{DoF} {Grasp} {Detection} via {Implicit} {Representations}},
	volume = {17},
	isbn = {978-0-9923747-7-8},
	shorttitle = {Synergies {Between} {Affordance} and {Geometry}},
	url = {https://roboticsproceedings.org/rss17/p024.html},
	urldate = {2023-09-12},
	author = {Jiang, Zhenyu and Zhu, Yifeng and Svetlik, Maxwell and Fang, Kuan and Zhu, Yuke},
	month = jul,
	year = {2021},
}

@misc{huang_dynamic_2023,
	title = {Dynamic {Handover}: {Throw} and {Catch} with {Bimanual} {Hands}},
	shorttitle = {Dynamic {Handover}},
	url = {http://arxiv.org/abs/2309.05655},
	doi = {10.48550/arXiv.2309.05655},
	abstract = {Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at {\textbackslash}url\{https://binghao-huang.github.io/dynamic\_handover/\}.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Huang, Binghao and Chen, Yuanpei and Wang, Tianyu and Qin, Yuzhe and Yang, Yaodong and Atanasov, Nikolay and Wang, Xiaolong},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05655 null},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hong_3d_2022,
	title = {{3D} {Concept} {Grounding} on {Neural} {Fields}},
	url = {http://arxiv.org/abs/2207.06403},
	doi = {10.48550/arXiv.2207.06403},
	abstract = {In this paper, we address the challenging problem of 3D concept grounding (i.e. segmenting and learning visual concepts) by looking at RGBD images and reasoning about paired questions and answers. Existing visual reasoning approaches typically utilize supervised methods to extract 2D segmentation masks on which concepts are grounded. In contrast, humans are capable of grounding concepts on the underlying 3D representation of images. However, traditionally inferred 3D representations (e.g., point clouds, voxelgrids, and meshes) cannot capture continuous 3D features flexibly, thus making it challenging to ground concepts to 3D regions based on the language description of the object being referred to. To address both issues, we propose to leverage the continuous, differentiable nature of neural fields to segment and learn concepts. Specifically, each 3D coordinate in a scene is represented as a high-dimensional descriptor. Concept grounding can then be performed by computing the similarity between the descriptor vector of a 3D coordinate and the vector embedding of a language concept, which enables segmentations and concept learning to be jointly learned on neural fields in a differentiable fashion. As a result, both 3D semantic and instance segmentations can emerge directly from question answering supervision using a set of defined neural operators on top of neural fields (e.g., filtering and counting). Experimental results show that our proposed framework outperforms unsupervised/language-mediated segmentation models on semantic and instance segmentation tasks, as well as outperforms existing models on the challenging 3D aware visual reasoning tasks. Furthermore, our framework can generalize well to unseen shape categories and real scans.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Hong, Yining and Du, Yilun and Lin, Chunru and Tenenbaum, Joshua B. and Gan, Chuang},
	month = jul,
	year = {2022},
	note = {arXiv:2207.06403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{hong_3d_2023,
	title = {{3D} {Concept} {Learning} and {Reasoning} from {Multi}-{View} {Images}},
	url = {http://arxiv.org/abs/2303.11327},
	doi = {10.48550/arXiv.2303.11327},
	abstract = {Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world. Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator. In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step towards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly combines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Hong, Yining and Lin, Chunru and Du, Yilun and Chen, Zhenfang and Tenenbaum, Joshua B. and Gan, Chuang},
	month = mar,
	year = {2023},
	note = {arXiv:2303.11327 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{hong_3d-llm_2023,
	title = {{3D}-{LLM}: {Injecting} the {3D} {World} into {Large} {Language} {Models}},
	shorttitle = {{3D}-{LLM}},
	url = {http://arxiv.org/abs/2307.12981},
	doi = {10.48550/arXiv.2307.12981},
	abstract = {Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.},
	urldate = {2023-09-08},
	publisher = {arXiv},
	author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
	month = jul,
	year = {2023},
	note = {arXiv:2307.12981 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{zhang_visual-tactile_2023,
	title = {Visual-{Tactile} {Learning} of {Garment} {Unfolding} for {Robot}-{Assisted} {Dressing}},
	volume = {8},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3296371},
	abstract = {Assistive robots have the potential to support disabled and elderly people in daily dressing activities. An intermediate stage of dressing is to manipulate the garment from a crumpled initial state to an unfolded configuration that facilitates robust dressing. Applying quasi-static grasping actions with vision feedback on garment unfolding usually suffers from occluded grasping points. In this work, we propose a dynamic manipulation strategy: tracing the garment edge until the hidden corner is revealed. We introduce a model-based approach, where a deep visual-tactile predictive model iteratively learns to perform servoing from raw sensor data. The predictive model is formalized as Conditional Variational Autoencoder with contrastive optimization, which jointly learns underlying visual-tactile latent representations, a latent garment dynamics model, and future predictions of garment states. Two cost functions are explored: the visual cost, defined by garment corner positions, guarantees the gripper to move towards the corner, while the tactile cost, defined by garment edge poses, prevents the garment from falling from the gripper. The experimental results demonstrate the improvement of our contrastive visual-tactile model predictive control over single sensing modality and baseline model learning techniques. The proposed method enables a robot to unfold back-opening hospital gowns and perform upper-body dressing.},
	number = {9},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zhang, Fan and Demiris, Yiannis},
	month = sep,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Clothing, Force and tactile sensing, Grasping, Grippers, Predictive models, Robot sensing systems, Robots, Visualization, model learning for control, physical human-robot interaction},
	pages = {5512--5519},
}

@article{paton_algorithm_1969,
	title = {An algorithm for finding a fundamental set of cycles of a graph},
	volume = {12},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/363219.363232},
	doi = {10.1145/363219.363232},
	abstract = {A fast method is presented for finding a fundamental set of cycles for an undirected finite graph. A spanning tree is grown and the vertices examined in turn, unexamined vertices being stored in a pushdown list to await examination. One stage in the process is to take the top element v of the pushdown list and examine it, i.e. inspect all those edges (v, z) of the graph for which z has not yet been examined. If z is already in the tree, a fundamental cycle is added; if not, the edge (v, z) is placed in the tree. There is exactly one such stage for each of the n vertices of the graph. For large n, the store required increases as n2 and the time as nγ where γ depends on the type of graph involved. γ is bounded below by 2 and above by 3, and it is shown that both bounds are attained. In terms of storage our algorithm is similar to that of Gotlieb and Corneil and superior to that of Welch; in terms of speed it is similar to that of Welch and superior to that of Gotlieb and Corneil. Tests show our algorithm to be remarkably efficient (γ = 2) on random graphs.},
	number = {9},
	urldate = {2023-08-10},
	journal = {Communications of the ACM},
	author = {Paton, Keith},
	month = sep,
	year = {1969},
	keywords = {algorithm, cycle, fundamental cycle set, graph, spanning tree},
	pages = {514--518},
}

@misc{wang_dynamic_2019,
	title = {Dynamic {Graph} {CNN} for {Learning} on {Point} {Clouds}},
	url = {http://arxiv.org/abs/1801.07829},
	doi = {10.48550/arXiv.1801.07829},
	abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.},
	urldate = {2023-08-03},
	publisher = {arXiv},
	author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
	month = jun,
	year = {2019},
	note = {arXiv:1801.07829 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shi_point-gnn_2020,
	title = {Point-{GNN}: {Graph} {Neural} {Network} for {3D} {Object} {Detection} in a {Point} {Cloud}},
	shorttitle = {Point-{GNN}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Shi_Point-GNN_Graph_Neural_Network_for_3D_Object_Detection_in_a_CVPR_2020_paper.html},
	urldate = {2023-07-30},
	author = {Shi, Weijing and Rajkumar, Raj},
	year = {2020},
	pages = {1711--1719},
}

@inproceedings{rezazadeh_hierarchical_2023,
	title = {Hierarchical {Graph} {Neural} {Networks} for {Proprioceptive} {6D} {Pose} {Estimation} of {In}-hand {Objects}},
	doi = {10.1109/ICRA48891.2023.10161264},
	abstract = {Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estimation, (2) we introduce a hierarchical message passing operation that flows the information within and across modalities to learn a graph-based object representation, and (3) we introduce a method that accounts for the proprioceptive information for in-hand object representation. We evaluate our model on a diverse subset of objects from the YCB Object and Model Set, and show that our method substantially outperforms existing state-of-the-art work in accuracy and robustness to occlusion. We also deploy our proposed framework on a real robot and qualitatively demonstrate successful transfer to real settings.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Rezazadeh, Alireza and Dikhale, Snehal and Iba, Soshi and Jamali, Nawid},
	month = may,
	year = {2023},
	keywords = {Fingers, Message passing, Pose estimation, Propioception, Representation learning, Sensor phenomena and characterization, Visualization},
	pages = {2884--2890},
}

@inproceedings{caddeo_collision-aware_2023,
	title = {Collision-aware {In}-hand {6D} {Object} {Pose} {Estimation} using {Multiple} {Vision}-based {Tactile} {Sensors}},
	doi = {10.1109/ICRA48891.2023.10160359},
	abstract = {In this paper, we address the problem of estimating the in-hand 6D pose of an object in contact with multiple vision-based tactile sensors. We reason on the possible spatial configurations of the sensors along the object surface. Specifically, we filter contact hypotheses using geometric reasoning and a Convolutional Neural Network (CNN), trained on simulated object-agnostic images, to promote those that better comply with the actual tactile images from the sensors. We use the selected sensors configurations to optimize over the space of 6D poses using a Gradient Descent-based approach. We finally rank the obtained poses by penalizing those that are in collision with the sensors. We carry out experiments in simulation using the DIGIT vision-based sensor with several objects, from the standard YCB model set. The results demonstrate that our approach estimates object poses that are compatible with actual object-sensor contacts in 87.5\% of cases while reaching an average positional error in the order of 2 centimeters. Our analysis also includes qualitative results of experiments with a real DIGIT sensor.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Caddeo, Gabriele M. and Piga, Nicola A. and Bottarel, Fabrizio and Natale, Lorenzo},
	month = may,
	year = {2023},
	keywords = {Estimation error, Image sensors, Pipelines, Pose estimation, Sensor systems, Streaming media, Tactile sensors},
	pages = {719--725},
}

@misc{ahn_as_2022,
	title = {Do {As} {I} {Can}, {Not} {As} {I} {Say}: {Grounding} {Language} in {Robotic} {Affordances}},
	shorttitle = {Do {As} {I} {Can}, {Not} {As} {I} {Say}},
	url = {http://arxiv.org/abs/2204.01691},
	doi = {10.48550/arXiv.2204.01691},
	abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
	month = aug,
	year = {2022},
	note = {arXiv:2204.01691 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{funabashi_multi-fingered_2022,
	title = {Multi-{Fingered} {In}-{Hand} {Manipulation} {With} {Various} {Object} {Properties} {Using} {Graph} {Convolutional} {Networks} and {Distributed} {Tactile} {Sensors}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3142417},
	abstract = {Multi-fingered hands could be used to achieve many dexterous manipulation tasks, similarly to humans, and tactile sensing could enhance the manipulation stability for a variety of objects. However, tactile sensors on multi-fingered hands have a variety of sizes and shapes. Convolutional neural networks (CNN) can be useful for processing tactile information, but the information from multi-fingered hands needs an arbitrary pre-processing, as CNNs require a rectangularly shaped input, which may lead to unstable results. Therefore, how to process such complex shaped tactile information and utilize it for achieving manipulation skills is still an open issue. This letter presents a control method based on a graph convolutional network (GCN) which extracts geodesical features from the tactile data with complicated sensor alignments. Moreover, object property labels are provided to the GCN to adjust in-hand manipulation motions. Distributed tri-axial tactile sensors are mounted on the fingertips, finger phalanges and palm of an Allegro hand, resulting in 1152 tactile measurements. Training data is collected with a data-glove to transfer human dexterous manipulation directly to the robot hand. The GCN achieved high success rates for in-hand manipulation. We also confirmed that fragile objects were deformed less when correct object labels were provided to the GCN. When visualizing the activation of the GCN with a PCA, we verified that the network acquired geodesical features. Our method achieved stable manipulation even when an experimenter pulled a grasped object and for untrained objects.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Funabashi, Satoshi and Isobe, Tomoki and Hongyi, Fei and Hiramoto, Atsumu and Schmitz, Alexander and Sugano, Shigeki and Ogata, Tetsuya},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Convolutional neural networks, Deep learning in grasping and manipulation, Grasping, Robot sensing systems, Shape, Tactile sensors, Task analysis, Thumb, dexterous manipulation, force and tactile sensing, multifingered hands},
	pages = {2102--2109},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: {Text}-to-{3D} using {2D} {Diffusion}},
	shorttitle = {{DreamFusion}},
	url = {https://arxiv.org/abs/2209.14988v1},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	language = {en},
	urldate = {2023-07-19},
	journal = {arXiv.org},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	month = sep,
	year = {2022},
}

@misc{xue_arraybot_2023,
	title = {{ArrayBot}: {Reinforcement} {Learning} for {Generalizable} {Distributed} {Manipulation} through {Touch}},
	shorttitle = {{ArrayBot}},
	url = {http://arxiv.org/abs/2306.16857},
	doi = {10.48550/arXiv.2306.16857},
	abstract = {We present ArrayBot, a distributed manipulation system consisting of a \$16 {\textbackslash}times 16\$ array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Surprisingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also transfer to the physical robot without any domain randomization. Leveraging the deployed policy, we present abundant real-world manipulation tasks, illustrating the vast potential of RL on ArrayBot for distributed manipulation.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Xue, Zhengrong and Zhang, Han and Cheng, Jingwen and He, Zhengmao and Ju, Yuanchen and Lin, Changyi and Zhang, Gu and Xu, Huazhe},
	month = jun,
	year = {2023},
	note = {arXiv:2306.16857 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{castano-amoros_measuring_2023,
	title = {Measuring {Object} {Rotation} via {Visuo}-{Tactile} {Segmentation} of {Grasping} {Region}},
	volume = {8},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3285471},
	abstract = {When carrying out robotic manipulation tasks, objects occasionally fall as a result of the rotation caused by slippage. This can be prevented by obtaining tactile information that provides better knowledge on the physical properties of the grasping. In this letter, we estimate the rotation angle of a grasped object when slippage occurs. We implement a system made up of a neural network with which to segment the contact region and an algorithm with which to estimate the rotated angle of that region. This method is applied to DIGIT tactile sensors. Our system has additionally been trained and tested with our publicly available dataset which is, to the best of our knowledge, the first dataset related to tactile segmentation from non-synthetic images to appear in the literature, and with which we have attained results of 95\% and 90\% as regards Dice and IoU metrics in the worst scenario. Moreover, we have obtained a maximum error of {\textbackslash}approx 3$^{\textrm{{\textbackslash}circ }}$ when testing with objects not previously seen by our system in 45 different lifts. This, therefore, proved that our approach is able to detect the slippage movement, thus providing a possible reaction that will prevent the object from falling.},
	number = {8},
	journal = {IEEE Robotics and Automation Letters},
	author = {Castaño-Amorós, Julio and Gil, Pablo},
	month = aug,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Estimation, Grasping, Image segmentation, Neural networks, Proposals, Tactile sensors, Task analysis, force and tactile sensing, perception for grasping and manipulation},
	pages = {4537--4544},
}

@article{seminara_hierarchical_2023,
	title = {A hierarchical sensorimotor control framework for human-in-the-loop robotic hands},
	volume = {8},
	url = {https://www.science.org/doi/10.1126/scirobotics.add5434},
	doi = {10.1126/scirobotics.add5434},
	abstract = {Human manual dexterity relies critically on touch. Robotic and prosthetic hands are much less dexterous and make little use of the many tactile sensors available. We propose a framework modeled on the hierarchical sensorimotor controllers of the nervous system to link sensing to action in human-in-the-loop, haptically enabled, artificial hands.},
	number = {78},
	urldate = {2023-07-13},
	journal = {Science Robotics},
	author = {Seminara, Lucia and Dosen, Strahinja and Mastrogiovanni, Fulvio and Bianchi, Matteo and Watt, Simon and Beckerle, Philipp and Nanayakkara, Thrishantha and Drewing, Knut and Moscatelli, Alessandro and Klatzky, Roberta L. and Loeb, Gerald E.},
	month = may,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadd5434},
}

@article{li_review_2020,
	title = {A {Review} of {Tactile} {Information}: {Perception} and {Action} {Through} {Touch}},
	volume = {36},
	issn = {1941-0468},
	shorttitle = {A {Review} of {Tactile} {Information}},
	doi = {10.1109/TRO.2020.3003230},
	abstract = {Tactile sensing is a key sensor modality for robots interacting with their surroundings. These sensors provide a rich and diverse set of data signals that contain detailed information collected from contacts between the robot and its environment. The data are however not limited to individual contacts and can be used to extract a wide range of information about the objects in the environment as well as the actions of the robot during the interactions. In this article, we provide an overview of tactile information and its applications in robotics. We present a hierarchy consisting of raw, contact, object, and action levels to structure the tactile information, with higher-level information often building upon lower-level information. We discuss different types of information that can be extracted at each level of the hierarchy. The article also includes an overview of different types of robot applications and the types of tactile information that they employ. Finally we end the article with a discussion for future tactile applications which are still beyond the current capabilities of robots.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Li, Qiang and Kroemer, Oliver and Su, Zhe and Veiga, Filipe Fernandes and Kaboli, Mohsen and Ritter, Helge Joachim},
	month = dec,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Grasping, Human-robot interaction, Tactile sensors, human–robot interaction (HRI), in-hand manipulation, interactive perception, locomotion, nonprehensile manipulation, tactile exploration, tactile sensing, tool manipulation, whole body manipulation},
	pages = {1619--1634},
}

@article{james_slip_2021,
	title = {Slip {Detection} for {Grasp} {Stabilization} {With} a {Multifingered} {Tactile} {Robot} {Hand}},
	volume = {37},
	issn = {1941-0468},
	doi = {10.1109/TRO.2020.3031245},
	abstract = {Tactile sensing is used by humans when grasping to prevent us dropping objects. One key facet of tactile sensing is slip detection, which allows a gripper to know when a grasp is failing and take action to prevent an object being dropped. This study demonstrates the slip detection capabilities of the recently developed Tactile Model O (T-MO) robotic hand by using support vector machines to detect slip and test multiple slip scenarios including responding to the onset of slip in real time with 11 different objects in various grasps. In this article, we demonstrate the benefits of slip detection in grasping by testing two real-world scenarios: adding weight to destabilize a grasp and using slip detection to lift up objects at the first attempt. The T-MO is able to detect when an object is slipping, react to stabilize the grasp, and be deployed in real-world scenarios. This shows the T-MO is a suitable platform for autonomous grasping by using reliable slip detection to ensure a stable grasp in unstructured environments.},
	number = {2},
	journal = {IEEE Transactions on Robotics},
	author = {James, Jasper Wollaston and Lepora, Nathan F.},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Grasping, Grippers, Manipulation, Pins, Tactile sensors, slip detection, tactile sensors},
	pages = {506--519},
}

@article{sundaralingam_-hand_2021,
	title = {In-{Hand} {Object}-{Dynamics} {Inference} {Using} {Tactile} {Fingertips}},
	volume = {37},
	issn = {1941-0468},
	doi = {10.1109/TRO.2020.3043675},
	abstract = {Having the ability to estimate an object's properties through interaction will enable robots to manipulate novel objects. Object's dynamics, specifically the friction and inertial parameters have only been estimated in a lab environment with precise and often external sensing. Could we infer an object's dynamics in the wild with only the robot's sensors? In this article, we explore the estimation of dynamics of a grasped object in motion, with tactile force sensing at multiple fingertips. Our estimation approach does not rely on torque sensing to estimate the dynamics. To estimate friction, we develop a control scheme to actively interact with the object until slip is detected. To robustly perform the inertial estimation, we setup a factor graph that fuses all our sensor measurements on physically consistent manifolds and perform inference. We show that tactile fingertips enable in-hand dynamics estimation of low mass objects.},
	number = {4},
	journal = {IEEE Transactions on Robotics},
	author = {Sundaralingam, Balakumar and Hermans, Tucker},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Dynamics, Estimation, Force, Friction, Robot sensing systems, Robots, Sensors, System identification, robotics, tactile sensors},
	pages = {1115--1126},
}

@misc{li_gendexgrasp_2023,
	title = {{GenDexGrasp}: {Generalizable} {Dexterous} {Grasping}},
	shorttitle = {{GenDexGrasp}},
	url = {http://arxiv.org/abs/2210.00722},
	doi = {10.48550/arXiv.2210.00722},
	abstract = {Generating dexterous grasping has been a long-standing and challenging robotic task. Despite recent progress, existing methods primarily suffer from two issues. First, most prior arts focus on a specific type of robot hand, lacking the generalizable capability of handling unseen ones. Second, prior arts oftentimes fail to rapidly generate diverse grasps with a high success rate. To jointly tackle these challenges with a unified solution, we propose GenDexGrasp, a novel hand-agnostic grasping algorithm for generalizable grasping. GenDexGrasp is trained on our proposed large-scale multi-hand grasping dataset MultiDex synthesized with force closure optimization. By leveraging the contact map as a hand-agnostic intermediate representation, GenDexGrasp efficiently generates diverse and plausible grasping poses with a high success rate and can transfer among diverse multi-fingered robotic hands. Compared with previous methods, GenDexGrasp achieves a three-way trade-off among success rate, inference speed, and diversity. Code is available at https://github.com/tengyu-liu/GenDexGrasp.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Li, Puhao and Liu, Tengyu and Li, Yuyang and Geng, Yiran and Zhu, Yixin and Yang, Yaodong and Huang, Siyuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.00722 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{zhang_dynamic_2021,
	title = {Dynamic {Modeling} of {Hand}-{Object} {Interactions} via {Tactile} {Sensing}},
	doi = {10.1109/IROS51168.2021.9636361},
	abstract = {Tactile sensing is critical for humans to perform everyday tasks. While significant progress has been made in analyzing object grasping from vision, it remains unclear how we can utilize tactile sensing to reason about and model the dynamics of hand-object interactions. In this work, we employ a high-resolution tactile glove to perform four different interactive activities on a diversified set of objects. We propose a framework aiming at predicting the 3d locations of both the hand and the object purely from the touch data by combining a predictive model and a contrastive learning module. This framework can reason about the interaction patterns from the tactile data, hallucinate the changes in the environment, esti-mate the uncertainty of the prediction, and generalize to unseen objects. We also provide detailed ablation studies regarding different system designs as well as visualizations of the predicted trajectories. This work takes a step on dynamics modeling in hand-object interactions from dense tactile sensing, which opens the door for future applications in activity learning, human-computer interactions, and imitation learning for robotics.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zhang, Qiang and Li, Yunzhu and Luo, Yiyue and Shou, Wan and Foshey, Michael and Yan, Junchi and Tenenbaum, Joshua B. and Matusik, Wojciech and Torralba, Antonio},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Human computer interaction, Predictive models, Robot sensing systems, Sensors, Three-dimensional displays, Trajectory, Uncertainty},
	pages = {2874--2881},
}

@misc{achlioptas_learning_2018,
	title = {Learning {Representations} and {Generative} {Models} for {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/1707.02392},
	doi = {10.48550/arXiv.1707.02392},
	abstract = {Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.},
	urldate = {2023-06-24},
	publisher = {arXiv},
	author = {Achlioptas, Panos and Diamanti, Olga and Mitliagkas, Ioannis and Guibas, Leonidas},
	month = jun,
	year = {2018},
	note = {arXiv:1707.02392 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gao_convmae_2022,
	title = {{ConvMAE}: {Masked} {Convolution} {Meets} {Masked} {Autoencoders}},
	shorttitle = {{ConvMAE}},
	url = {http://arxiv.org/abs/2205.03892},
	doi = {10.48550/arXiv.2205.03892},
	abstract = {Vision Transformers (ViT) become widely-adopted architectures for various vision tasks. Masked auto-encoding for feature pretraining and multi-scale hybrid convolution-transformer architectures can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation. In this paper, our ConvMAE framework demonstrates that multi-scale hybrid convolution-transformer can learn more discriminative representations via the mask auto-encoding scheme. However, directly using the original masking strategy leads to the heavy computational cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the masked convolution to prevent information leakage in the convolution blocks. A simple block-wise masking strategy is proposed to ensure computational efficiency. We also propose to more directly supervise the multi-scale features of the encoder to boost multi-scale features. Based on our pretrained ConvMAE models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4\% compared with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs surpasses MAE-Base fined-tuned for 100 epochs by 2.9\% box AP and 2.2\% mask AP respectively. Code and pretrained models are available at https://github.com/Alpha-VL/ConvMAE.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Gao, Peng and Ma, Teli and Li, Hongsheng and Lin, Ziyi and Dai, Jifeng and Qiao, Yu},
	month = may,
	year = {2022},
	note = {arXiv:2205.03892 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_empirical_2021,
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-06-22},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	year = {2021},
	pages = {9640--9649},
}

@misc{donlon_gelslim_2018,
	title = {{GelSlim}: {A} {High}-{Resolution}, {Compact}, {Robust}, and {Calibrated} {Tactile}-sensing {Finger}},
	shorttitle = {{GelSlim}},
	url = {http://arxiv.org/abs/1803.00628},
	doi = {10.48550/arXiv.1803.00628},
	abstract = {This work describes the development of a high-resolution tactile-sensing finger for robot grasping. This finger, inspired by previous GelSight sensing techniques, features an integration that is slimmer, more robust, and with more homogeneous output than previous vision-based tactile sensors. To achieve a compact integration, we redesign the optical path from illumination source to camera by combining light guides and an arrangement of mirror reflections. We parameterize the optical path with geometric design variables and describe the tradeoffs between the finger thickness, the depth of field of the camera, and the size of the tactile sensing area. The sensor sustains the wear from continuous use -- and abuse -- in grasping tasks by combining tougher materials for the compliant soft gel, a textured fabric skin, a structurally rigid body, and a calibration process that maintains homogeneous illumination and contrast of the tactile images during use. Finally, we evaluate the sensor's durability along four metrics that track the signal quality during more than 3000 grasping experiments.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Donlon, Elliott and Dong, Siyuan and Liu, Melody and Li, Jianhua and Adelson, Edward and Rodriguez, Alberto},
	month = may,
	year = {2018},
	note = {arXiv:1803.00628 [cs]},
	keywords = {70B15, Computer Science - Robotics},
}

@inproceedings{shi_top-down_2023,
	title = {Top-{Down} {Visual} {Attention} {From} {Analysis} by {Synthesis}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Shi_Top-Down_Visual_Attention_From_Analysis_by_Synthesis_CVPR_2023_paper.html},
	language = {en},
	urldate = {2023-06-22},
	author = {Shi, Baifeng and Darrell, Trevor and Wang, Xin},
	year = {2023},
	pages = {2102--2112},
}

@misc{liang_code_2023,
	title = {Code as {Policies}: {Language} {Model} {Programs} for {Embodied} {Control}},
	shorttitle = {Code as {Policies}},
	url = {http://arxiv.org/abs/2209.07753},
	doi = {10.48550/arXiv.2209.07753},
	abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	month = may,
	year = {2023},
	note = {arXiv:2209.07753 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{yu_language_2023,
	title = {Language to {Rewards} for {Robotic} {Skill} {Synthesis}},
	url = {http://arxiv.org/abs/2306.08647},
	doi = {10.48550/arXiv.2306.08647},
	abstract = {Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate interface generated by LLMs, we can effectively bridge the gap between high-level language instructions or corrections to low-level robot actions. Meanwhile, combining this with a real-time optimizer, MuJoCo MPC, empowers an interactive behavior creation experience where users can immediately observe the results and provide feedback to the system. To systematically evaluate the performance of our proposed method, we designed a total of 17 tasks for a simulated quadruped robot and a dexterous manipulator robot. We demonstrate that our proposed method reliably tackles 90\% of the designed tasks, while a baseline using primitive skills as the interface with Code-as-policies achieves 50\% of the tasks. We further validated our method on a real robot arm where complex manipulation skills such as non-prehensile pushing emerge through our interactive system.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Yu, Wenhao and Gileadi, Nimrod and Fu, Chuyuan and Kirmani, Sean and Lee, Kuang-Huei and Arenas, Montse Gonzalez and Chiang, Hao-Tien Lewis and Erez, Tom and Hasenclever, Leonard and Humplik, Jan and Ichter, Brian and Xiao, Ted and Xu, Peng and Zeng, Andy and Zhang, Tingnan and Heess, Nicolas and Sadigh, Dorsa and Tan, Jie and Tassa, Yuval and Xia, Fei},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08647 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{gupta_visual_2022,
	title = {Visual {Programming}: {Compositional} visual reasoning without training},
	shorttitle = {Visual {Programming}},
	url = {http://arxiv.org/abs/2211.11559},
	doi = {10.48550/arXiv.2211.11559},
	abstract = {We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {Gupta, Tanmay and Kembhavi, Aniruddha},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11559 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fishman_motion_2023,
	title = {Motion {Policy} {Networks}},
	url = {https://proceedings.mlr.press/v205/fishman23a.html},
	abstract = {Collision-free motion generation in unknown environments is a core building block for robot manipulation. Generating such motions is challenging due to multiple objectives; not only should the solutions be optimal, the motion generator itself must be fast enough for real-time performance and reliable enough for practical deployment. A wide variety of methods have been proposed ranging from local controllers to global planners, often being combined to offset their shortcomings. We present an end-to-end neural model called Motion Policy Networks (Mππ{\textbackslash}piNets) to generate collision-free, smooth motion from just a single depth camera observation. Mππ{\textbackslash}piNets are trained on over 3 million motion planning problems in more than 500,000 environments. Our experiments show that Mππ{\textbackslash}piNets are significantly faster than global planners while exhibiting the reactivity needed to deal with dynamic scenes. They are 46\% better than prior neural planners and more robust than local control policies. Despite being only trained in simulation, Mππ{\textbackslash}piNets transfer well to the real robot with noisy partial point clouds. Videos and code are available at https://mpinets.github.io},
	language = {en},
	urldate = {2023-06-15},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Fishman, Adam and Murali, Adithyavairavan and Eppner, Clemens and Peele, Bryan and Boots, Byron and Fox, Dieter},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {967--977},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{ze_visual_2023,
	title = {Visual {Reinforcement} {Learning} {With} {Self}-{Supervised} {3D} {Representations}},
	volume = {8},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3259681},
	abstract = {A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency compared to 2D representation learning methods. Additionally, our learned policies transfer zero-shot to a real robot setup with only approximate geometric correspondence, and successfully solve motor control tasks that involve grasping and lifting from a single, uncalibrated RGB camera.},
	number = {5},
	journal = {IEEE Robotics and Automation Letters},
	author = {Ze, Yanjie and Hansen, Nicklas and Chen, Yinbo and Jain, Mohit and Wang, Xiaolong},
	month = may,
	year = {2023},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Reinforcement learning, Representation learning, Robot vision systems, Task analysis, Three-dimensional displays, Training, Visualization, deep learning for visual perception, representation learning},
	pages = {2890--2897},
}

@inproceedings{jeong_self-supervised_2020,
	title = {Self-{Supervised} {Sim}-to-{Real} {Adaptation} for {Visual} {Robotic} {Manipulation}},
	doi = {10.1109/ICRA40945.2020.9197326},
	abstract = {Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training reinforcement learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the accuracy and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep reinforcement learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self- supervised objectives. These use the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We further propose a novel such objective, the Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based reinforcement learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Jeong, Rae and Aytar, Yusuf and Khosid, David and Zhou, Yuxiang and Kay, Jackie and Lampe, Thomas and Bousmalis, Konstantinos and Nori, Francesco},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Adaptation models, Data models, Robots, Stacking, Task analysis, Training, Visualization},
	pages = {2718--2724},
}

@inproceedings{jianu_reducing_2022,
	title = {Reducing {Tactile} {Sim2Real} {Domain} {Gaps} via {Deep} {Texture} {Generation} {Networks}},
	doi = {10.1109/ICRA46639.2022.9811801},
	abstract = {Recently simulation methods have been developed for optical tactile sensors to enable the Sim2Real learning, i.e., first training models in simulation before deploying them on a real robot. However, some artefacts in real objects are unpredictable, such as imperfections caused by fabrication processes, or scratches by natural wear and tear, and thus cannot be represented in the simulation, resulting in a significant gap between the simulated and real tactile images. To address this Sim2Real gap, we propose a novel texture generation network to map the simulated images into photorealistic tactile images that resemble a real sensor contacting a real imperfect object. Each simulated tactile image is first divided into two types of regions: areas that are in contact with the object and areas that are not. The former is applied with generated textures learned from real textures in the real tactile images, whereas the latter maintains its appearance as when the sensor is not in contact with any object. This makes sure that the artefacts are only applied to deformed regions of the sensor. Our extensive experiments show that the proposed texture generation network can generate realistic artefacts on the deformed regions of the sensor, while avoiding leaking the textures into areas of no contact. Quantitative experiments further reveal that when using the adapted images generated by our proposed network for a Sim2Real classification task, the drop in accuracy caused by the Sim2Real gap is reduced from 38.43\% to merely 0.81\%. As such, this work has potential to accelerate the Sim2Real learning for robotic tasks requiring tactile sensing.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Jianu, Tudor and Gomes, Daniel Fernandes and Luo, Shan},
	month = may,
	year = {2022},
	keywords = {Costs, Grasping, Optical device fabrication, Optical imaging, Robot sensing systems, Tactile sensors, Training},
	pages = {8305--8311},
}

@article{chen_bidirectional_2022,
	title = {Bidirectional {Sim}-to-{Real} {Transfer} for {GelSight} {Tactile} {Sensors} {With} {CycleGAN}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3167064},
	abstract = {GelSight optical tactile sensors have high-resolution and low-cost advantages and have witnessed growing adoption in various contact-rich robotic applications. Sim2Real for GelSight sensors can reduce the time cost and sensor damage during data collection and is crucial for learning-based tactile perception and control. However, it remains difficult for existing simulation methods to resemble the complex and non-ideal light transmission of real sensors. In this letter, we propose to narrow the gap between simulation and real world using CycleGAN. Due to the bidirectional generators of CycleGAN, the proposed method can not only generate more realistic simulated tactile images, but also improve the deformation measurement accuracy of real sensors by transferring them to simulation domain. Experiments on a public dataset and our own GelSight sensors have validated the effectiveness of our method. The materials related to this letter are available at https://github.com/RVSATHU/GelSight-Sim2Real.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Weihang and Xu, Yuan and Chen, Zhenyang and Zeng, Peiyu and Dang, Renjun and Chen, Rui and Xu, Jing},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Deep learning methods, Optical imaging, Optical sensors, Robot sensing systems, Sensors, Strain, Tactile sensors, Task analysis, force and tactile sensing, transfer learning},
	pages = {6187--6194},
}

@inproceedings{chen_exploring_2021,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-06-12},
	author = {Chen, Xinlei and He, Kaiming},
	year = {2021},
	pages = {15750--15758},
}

@misc{chen_improved_2020,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	doi = {10.48550/arXiv.2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv:2003.04297 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wang_pixel2mesh_2018,
	title = {{Pixel2Mesh}: {Generating} {3D} {Mesh} {Models} from {Single} {RGB} {Images}},
	shorttitle = {{Pixel2Mesh}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html},
	urldate = {2023-06-12},
	author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
	year = {2018},
	pages = {52--67},
}

@inproceedings{kato_neural_2018,
	title = {Neural {3D} {Mesh} {Renderer}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html},
	urldate = {2023-06-12},
	author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
	year = {2018},
	pages = {3907--3916},
}

@misc{fei_self-supervised_2023,
	title = {Self-supervised {Learning} for {Pre}-{Training} {3D} {Point} {Clouds}: {A} {Survey}},
	shorttitle = {Self-supervised {Learning} for {Pre}-{Training} {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2305.04691},
	doi = {10.48550/arXiv.2305.04691},
	abstract = {Point cloud data has been extensively studied due to its compact form and flexibility in representing complex 3D structures. The ability of point cloud data to accurately capture and represent intricate 3D geometry makes it an ideal choice for a wide range of applications, including computer vision, robotics, and autonomous driving, all of which require an understanding of the underlying spatial structures. Given the challenges associated with annotating large-scale point clouds, self-supervised point cloud representation learning has attracted increasing attention in recent years. This approach aims to learn generic and useful point cloud representations from unlabeled data, circumventing the need for extensive manual annotations. In this paper, we present a comprehensive survey of self-supervised point cloud representation learning using DNNs. We begin by presenting the motivation and general trends in recent research. We then briefly introduce the commonly used datasets and evaluation metrics. Following that, we delve into an extensive exploration of self-supervised point cloud representation learning methods based on these techniques. Finally, we share our thoughts on some of the challenges and potential issues that future research in self-supervised learning for pre-training 3D point clouds may encounter.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Fei, Ben and Yang, Weidong and Liu, Liwen and Luo, Tianyue and Zhang, Rui and Li, Yixuan and He, Ying},
	month = may,
	year = {2023},
	note = {arXiv:2305.04691 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hart_formal_1968,
	title = {A {Formal} {Basis} for the {Heuristic} {Determination} of {Minimum} {Cost} {Paths}},
	volume = {4},
	issn = {2168-2887},
	doi = {10.1109/TSSC.1968.300136},
	abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.},
	number = {2},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	author = {Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},
	month = jul,
	year = {1968},
	note = {Conference Name: IEEE Transactions on Systems Science and Cybernetics},
	keywords = {Automatic control, Automatic programming, Chemical technology, Costs, Functional programming, Gradient methods, Instruction sets, Mathematical programming, Minimax techniques, Minimization methods},
	pages = {100--107},
}

@inproceedings{hess_real-time_2016,
	title = {Real-time loop closure in {2D} {LIDAR} {SLAM}},
	doi = {10.1109/ICRA.2016.7487258},
	abstract = {Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve realtime loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hess, Wolfgang and Kohler, Damon and Rapp, Holger and Andor, Daniel},
	month = may,
	year = {2016},
	keywords = {Buildings, Feature extraction, Laser radar, Optimization, Real-time systems, Simultaneous localization and mapping, Upper bound},
	pages = {1271--1278},
}

@article{grisetti_improved_2007,
	title = {Improved {Techniques} for {Grid} {Mapping} {With} {Rao}-{Blackwellized} {Particle} {Filters}},
	volume = {23},
	issn = {1941-0468},
	doi = {10.1109/TRO.2006.889486},
	abstract = {Recently, Rao-Blackwellized particle filters (RBPF) have been introduced as an effective means to solve the simultaneous localization and mapping problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper, we present adaptive techniques for reducing this number in a RBPF for learning grid maps. We propose an approach to compute an accurate proposal distribution, taking into account not only the movement of the robot, but also the most recent observation. This drastically decreases the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out resampling operations, which seriously reduces the problem of particle depletion. Experimental results carried out with real mobile robots in large-scale indoor, as well as outdoor, environments illustrate the advantages of our methods over previous approaches},
	number = {1},
	journal = {IEEE Transactions on Robotics},
	author = {Grisetti, Giorgio and Stachniss, Cyrill and Burgard, Wolfram},
	month = feb,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Adaptive resampling, Computer science, Contracts, Distributed computing, Mobile robots, Orbital robotics, Particle filters, Proposals, Rao-Blackwellized particle filter (RBPF), Robot sensing systems, Simultaneous localization and mapping, Uncertainty, improved proposal, motion model, simultaneous localization and mapping (SLAM)},
	pages = {34--46},
}

@inproceedings{chaplot_object_2020,
	title = {Object {Goal} {Navigation} using {Goal}-{Oriented} {Semantic} {Exploration}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2c75cf2681788adaca63aa95ae028b22-Abstract.html},
	abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
	urldate = {2023-06-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chaplot, Devendra Singh and Gandhi, Dhiraj Prakashchand and Gupta, Abhinav and Salakhutdinov, Russ R},
	year = {2020},
	pages = {4247--4258},
}

@inproceedings{krantz_beyond_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Beyond the {Nav}-{Graph}: {Vision}-and-{Language} {Navigation} in {Continuous} {Environments}},
	isbn = {978-3-030-58604-1},
	shorttitle = {Beyond the {Nav}-{Graph}},
	doi = {10.1007/978-3-030-58604-1_7},
	abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at jacobkrantz.github.io/vlnce.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Embodied agents, Vision-and-Language Navigation},
	pages = {104--120},
}

@inproceedings{suresh_midastouch_2023,
	title = {{MidasTouch}: {Monte}-{Carlo} inference over distributions across sliding touch},
	shorttitle = {{MidasTouch}},
	url = {https://proceedings.mlr.press/v205/suresh23a.html},
	abstract = {We present MidasTouch, a tactile perception system for online global localization of a vision-based touch sensor sliding on an object surface. This framework takes in posed tactile images over time, and outputs an evolving distribution of sensor pose on the object’s surface, without the need for visual priors. Our key insight is to estimate local surface geometry with tactile sensing, learn a compact representation for it, and disambiguate these signals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo particle filter, with a measurement model based on a tactile code network learned from tactile simulation. This network, inspired by LIDAR place recognition, compactly summarizes local surface geometries. These generated codes are efficiently compared against a precomputed tactile codebook per-object, to update the pose distribution. We further release the YCB-Slide dataset of real-world and simulated forceful sliding interactions between a vision-based tactile sensor and standard YCB objects. While single-touch localization can be inherently ambiguous, we can quickly localize our sensor by traversing salient surface geometries. Project page: https://suddhu.github.io/midastouch-tactile/},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Suresh, Sudharshan and Si, Zilin and Anderson, Stuart and Kaess, Michael and Mukadam, Mustafa},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {319--331},
}

@misc{velickovic_graph_2018,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	doi = {10.48550/arXiv.1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10903 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_gsdf_2023,
	title = {{gSDF}: {Geometry}-{Driven} {Signed} {Distance} {Functions} for {3D} {Hand}-{Object} {Reconstruction}},
	shorttitle = {{gSDF}},
	url = {http://arxiv.org/abs/2304.11970},
	doi = {10.48550/arXiv.2304.11970},
	abstract = {Signed distance functions (SDFs) is an attractive framework that has recently shown promising results for 3D shape reconstruction from images. SDFs seamlessly generalize to different shape resolutions and topologies but lack explicit modelling of the underlying 3D geometry. In this work, we exploit the hand structure and use it as guidance for SDF-based shape reconstruction. In particular, we address reconstruction of hands and manipulated objects from monocular RGB images. To this end, we estimate poses of hands and objects and use them to guide 3D reconstruction. More specifically, we predict kinematic chains of pose transformations and align SDFs with highly-articulated hand poses. We improve the visual features of 3D points with geometry alignment and further leverage temporal information to enhance the robustness to occlusion and motion blurs. We conduct extensive experiments on the challenging ObMan and DexYCB benchmarks and demonstrate significant improvements of the proposed method over the state of the art.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Chen, Zerui and Chen, Shizhe and Schmid, Cordelia and Laptev, Ivan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11970 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{armeni_3d_2019,
	title = {{3D} {Scene} {Graph}: {A} {Structure} for {Unified} {Semantics}, {3D} {Space}, and {Camera}},
	shorttitle = {{3D} {Scene} {Graph}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.html},
	urldate = {2023-05-21},
	author = {Armeni, Iro and He, Zhi-Yang and Gwak, JunYoung and Zamir, Amir R. and Fischer, Martin and Malik, Jitendra and Savarese, Silvio},
	year = {2019},
	pages = {5664--5673},
}

@inproceedings{savva_habitat_2019,
	title = {Habitat: {A} {Platform} for {Embodied} {AI} {Research}},
	shorttitle = {Habitat},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html},
	urldate = {2023-05-20},
	author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
	year = {2019},
	pages = {9339--9347},
}

@inproceedings{xia_gibson_2018,
	title = {Gibson {Env}: {Real}-{World} {Perception} for {Embodied} {Agents}},
	shorttitle = {Gibson {Env}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html},
	urldate = {2023-05-20},
	author = {Xia, Fei and Zamir, Amir R. and He, Zhiyang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
	year = {2018},
	pages = {9068--9079},
}

@inproceedings{zhao_integrating_2023,
	title = {Integrating {Symmetry} into {Differentiable} {Planning} with {Steerable} {Convolutions}},
	url = {https://openreview.net/forum?id=n7CPzMPKQl},
	abstract = {To achieve this, we draw inspiration from equivariant convolution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant operator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navigation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2-DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN.},
	language = {en},
	urldate = {2023-05-20},
	author = {Zhao, Linfeng and Zhu, Xupeng and Kong, Lingzhi and Walters, Robin and Wong, Lawson L. S.},
	month = feb,
	year = {2023},
}

@inproceedings{lee_gated_2018,
	title = {Gated {Path} {Planning} {Networks}},
	url = {https://proceedings.mlr.press/v80/lee18c.html},
	abstract = {Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.},
	language = {en},
	urldate = {2023-05-20},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Lisa and Parisotto, Emilio and Chaplot, Devendra Singh and Xing, Eric and Salakhutdinov, Ruslan},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2947--2955},
}

@misc{he_towards_2022,
	title = {Towards a {Unified} {View} of {Parameter}-{Efficient} {Transfer} {Learning}},
	url = {http://arxiv.org/abs/2110.04366},
	doi = {10.48550/arXiv.2110.04366},
	abstract = {Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
	month = feb,
	year = {2022},
	note = {arXiv:2110.04366 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {http://arxiv.org/abs/2305.05665},
	doi = {10.48550/arXiv.2305.05665},
	abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	month = may,
	year = {2023},
	note = {arXiv:2305.05665 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{wen_bundlesdf_2023,
	title = {{BundleSDF}: {Neural} 6-{DoF} {Tracking} and {3D} {Reconstruction} of {Unknown} {Objects}},
	shorttitle = {{BundleSDF}},
	url = {http://arxiv.org/abs/2303.14158},
	doi = {10.48550/arXiv.2303.14158},
	abstract = {We present a near real-time method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that our method significantly outperforms existing approaches. Project page: https://bundlesdf.github.io},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Wen, Bowen and Tremblay, Jonathan and Blukis, Valts and Tyree, Stephen and Muller, Thomas and Evans, Alex and Fox, Dieter and Kautz, Jan and Birchfield, Stan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@article{shao_unigrasp_2020,
	title = {{UniGrasp}: {Learning} a {Unified} {Model} to {Grasp} with {Multifingered} {Robotic} {Hands}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{UniGrasp}},
	url = {http://arxiv.org/abs/1910.10900},
	doi = {10.1109/LRA.2020.2969946},
	abstract = {To achieve a successful grasp, gripper attributes such as its geometry and kinematics play a role as important as the object geometry. The majority of previous work has focused on developing grasp methods that generalize over novel object geometry but are specific to a certain robot hand. We propose UniGrasp, an efficient data-driven grasp synthesis method that considers both the object geometry and gripper attributes as inputs. UniGrasp is based on a novel deep neural network architecture that selects sets of contact points from the input point cloud of the object. The proposed model is trained on a large dataset to produce contact points that are in force closure and reachable by the robot hand. By using contact points as output, we can transfer between a diverse set of multifingered robotic hands. Our model produces over 90\% valid contact points in Top10 predictions in simulation and more than 90\% successful grasps in real world experiments for various known two-fingered and three-fingered grippers. Our model also achieves 93\%, 83\% and 90\% successful grasps in real world experiments for an unseen two-fingered gripper and two unseen multi-fingered anthropomorphic robotic hands.},
	number = {2},
	urldate = {2023-05-15},
	journal = {IEEE Robotics and Automation Letters},
	author = {Shao, Lin and Ferreira, Fabio and Jorda, Mikael and Nambiar, Varun and Luo, Jianlan and Solowjow, Eugen and Ojea, Juan Aparicio and Khatib, Oussama and Bohg, Jeannette},
	month = apr,
	year = {2020},
	note = {arXiv:1910.10900 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	pages = {2286--2293},
}

@misc{wu_tidybot_2023,
	title = {{TidyBot}: {Personalized} {Robot} {Assistance} with {Large} {Language} {Models}},
	shorttitle = {{TidyBot}},
	url = {http://arxiv.org/abs/2305.05658},
	doi = {10.48550/arXiv.2305.05658},
	abstract = {For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2\% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0\% of objects in real-world test scenarios.},
	urldate = {2023-05-10},
	publisher = {arXiv},
	author = {Wu, Jimmy and Antonova, Rika and Kan, Adam and Lepert, Marion and Zeng, Andy and Song, Shuran and Bohg, Jeannette and Rusinkiewicz, Szymon and Funkhouser, Thomas},
	month = may,
	year = {2023},
	note = {arXiv:2305.05658 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{pal_learning_2021,
	title = {Learning hierarchical relationships for object-goal navigation},
	url = {https://proceedings.mlr.press/v155/pal21a.html},
	abstract = {Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an 82.9\% and 93.5\% gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the 2020 {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Pal, Anwesan and Qiu, Yiding and Christensen, Henrik},
	month = oct,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {517--528},
}

@misc{zhu_soon_2021,
	title = {{SOON}: {Scenario} {Oriented} {Object} {Navigation} with {Graph}-based {Exploration}},
	shorttitle = {{SOON}},
	url = {http://arxiv.org/abs/2103.17138},
	doi = {10.48550/arXiv.2103.17138},
	abstract = {The ability to navigate like a human towards a language-guided target from anywhere in a 3D embodied environment is one of the 'holy grail' goals of intelligent robots. Most visual navigation benchmarks, however, focus on navigating toward a target from a fixed starting point, guided by an elaborate set of instructions that depicts step-by-step. This approach deviates from real-world problems in which human-only describes what the object and its surrounding look like and asks the robot to start navigation from anywhere. Accordingly, in this paper, we introduce a Scenario Oriented Object Navigation (SOON) task. In this task, an agent is required to navigate from an arbitrary position in a 3D embodied environment to localize a target following a scene description. To give a promising direction to solve this task, we propose a novel graph-based exploration (GBE) method, which models the navigation state as a graph and introduces a novel graph-based exploration approach to learn knowledge from the graph and stabilize training by learning sub-optimal trajectories. We also propose a new large-scale benchmark named From Anywhere to Object (FAO) dataset. To avoid target ambiguity, the descriptions in FAO provide rich semantic scene information includes: object attribute, object relationship, region description, and nearby region description. Our experiments reveal that the proposed GBE outperforms various state-of-the-arts on both FAO and R2R datasets. And the ablation studies on FAO validates the quality of the dataset.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Zhu, Fengda and Liang, Xiwen and Zhu, Yi and Chang, Xiaojun and Liang, Xiaodan},
	month = oct,
	year = {2021},
	note = {arXiv:2103.17138 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance} {Discrimination}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html},
	urldate = {2023-01-05},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
	year = {2018},
	pages = {3733--3742},
}

@misc{chen_learning_2021,
	title = {Learning {Generalizable} {Robotic} {Reward} {Functions} from "{In}-{The}-{Wild}" {Human} {Videos}},
	url = {http://arxiv.org/abs/2103.16817},
	doi = {10.48550/arXiv.2103.16817},
	abstract = {We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, "in-the-wild" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Chen, Annie S. and Nair, Suraj and Finn, Chelsea},
	month = mar,
	year = {2021},
	note = {arXiv:2103.16817 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{chaplot_differentiable_2021,
	title = {Differentiable {Spatial} {Planning} using {Transformers}},
	url = {https://proceedings.mlr.press/v139/chaplot21a.html},
	abstract = {We consider the problem of spatial path planning. In contrast to the classical solutions which optimize a new plan from scratch and assume access to the full map with ground truth obstacle locations, we learn a planner from the data in a differentiable manner that allows us to leverage statistical regularities from past data. We propose Spatial Planning Transformers (SPT), which given an obstacle map learns to generate actions by planning over long-range spatial dependencies, unlike prior data-driven planners that propagate information locally via convolutional structure in an iterative manner. In the setting where the ground truth map is not known to the agent, we leverage pre-trained SPTs in an end-to-end framework that has the structure of mapper and planner built into it which allows seamless generalization to out-of-distribution maps and goals. SPTs outperform prior state-of-the-art differentiable planners across all the setups for both manipulation and navigation tasks, leading to an absolute improvement of 7-19\%.},
	language = {en},
	urldate = {2023-05-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chaplot, Devendra Singh and Pathak, Deepak and Malik, Jitendra},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1484--1495},
}

@misc{zhu_contactart_2023,
	title = {{ContactArt}: {Learning} {3D} {Interaction} {Priors} for {Category}-level {Articulated} {Object} and {Hand} {Poses} {Estimation}},
	shorttitle = {{ContactArt}},
	url = {http://arxiv.org/abs/2305.01618},
	doi = {10.48550/arXiv.2305.01618},
	abstract = {We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method significantly improves the performance on joint hand and articulated object poses estimation over the existing state-of-the-art methods. The project is available at https://zehaozhu.github.io/ContactArt/ .},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Zhu, Zehao and Wang, Jiashun and Qin, Yuzhe and Sun, Deqing and Jampani, Varun and Wang, Xiaolong},
	month = may,
	year = {2023},
	note = {arXiv:2305.01618 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{xue_sfd2_2023,
	title = {{SFD2}: {Semantic}-guided {Feature} {Detection} and {Description}},
	shorttitle = {{SFD2}},
	url = {http://arxiv.org/abs/2304.14845},
	doi = {10.48550/arXiv.2304.14845},
	abstract = {Visual localization is a fundamental task for various applications including autonomous driving and robotics. Prior methods focus on extracting large amounts of often redundant locally reliable features, resulting in limited efficiency and accuracy, especially in large-scale environments under challenging conditions. Instead, we propose to extract globally reliable features by implicitly embedding high-level semantics into both the detection and description processes. Specifically, our semantic-aware detector is able to detect keypoints from reliable regions (e.g. building, traffic lane) and suppress unreliable areas (e.g. sky, car) implicitly instead of relying on explicit semantic labels. This boosts the accuracy of keypoint matching by reducing the number of features sensitive to appearance changes and avoiding the need of additional segmentation networks at test time. Moreover, our descriptors are augmented with semantics and have stronger discriminative ability, providing more inliers at test time. Particularly, experiments on long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons datasets demonstrate that our model outperforms previous local features and gives competitive accuracy to advanced matchers but is about 2 and 3 times faster when using 2k and 4k keypoints, respectively.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Xue, Fei and Budvytis, Ignas and Cipolla, Roberto},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14845 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_llama-adapter_2023,
	title = {{LLaMA}-{Adapter}: {Efficient} {Fine}-tuning of {Language} {Models} with {Zero}-init {Attention}},
	shorttitle = {{LLaMA}-{Adapter}},
	url = {http://arxiv.org/abs/2303.16199},
	doi = {10.48550/arXiv.2303.16199},
	abstract = {We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter.},
	urldate = {2023-04-30},
	publisher = {arXiv},
	author = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.16199 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
}

@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2304.12210},
	doi = {10.48550/arXiv.2304.12210},
	abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12210 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zhou_continuity_2019,
	title = {On the {Continuity} of {Rotation} {Representations} in {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.html},
	urldate = {2023-04-27},
	author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
	year = {2019},
	pages = {5745--5753},
}

@inproceedings{izatt_tracking_2017,
	title = {Tracking objects with point clouds from vision and touch},
	doi = {10.1109/ICRA.2017.7989460},
	abstract = {We present an object-tracking framework that fuses point cloud information from an RGB-D camera with tactile information from a GelSight contact sensor. GelSight can be treated as a source of dense local geometric information, which we incorporate directly into a conventional point-cloud-based articulated object tracker based on signed-distance functions. Our implementation runs at 12 Hz using an online depth reconstruction algorithm for GelSight and a modified second-order update for the tracking algorithm. We present data from hardware experiments demonstrating that the addition of contact-based geometric information significantly improves the pose accuracy during contact, and provides robustness to occlusions of small objects by the robot's end effector.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Izatt, Gregory and Mirano, Geronimo and Adelson, Edward and Tedrake, Russ},
	month = may,
	year = {2017},
	keywords = {Cameras, Geometry, Iterative closest point algorithm, Optimization, Robot sensing systems, Three-dimensional displays},
	pages = {4000--4007},
}

@inproceedings{li_localization_2014,
	title = {Localization and manipulation of small parts using {GelSight} tactile sensing},
	doi = {10.1109/IROS.2014.6943123},
	abstract = {Robust manipulation and insertion of small parts can be challenging because of the small tolerances typically involved. The key to robust control of these kinds of manipulation interactions is accurate tracking and control of the parts involved. Typically, this is accomplished using visual servoing or force-based control. However, these approaches have drawbacks. Instead, we propose a new approach that uses tactile sensing to accurately localize the pose of a part grasped in the robot hand. Using a feature-based matching technique in conjunction with a newly developed tactile sensing technology known as GelSight that has much higher resolution than competing methods, we synthesize high-resolution height maps of object surfaces. As a result of these high-resolution tactile maps, we are able to localize small parts held in a robot hand very accurately. We quantify localization accuracy in benchtop experiments and experimentally demonstrate the practicality of the approach in the context of a small parts insertion problem.},
	booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Li, Rui and Platt, Robert and Yuan, Wenzhen and ten Pas, Andreas and Roscup, Nathan and Srinivasan, Mandayam A. and Adelson, Edward},
	month = sep,
	year = {2014},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Connectors, Grippers, Robot sensing systems, Universal Serial Bus},
	pages = {3988--3993},
}

@misc{shah_lm-nav_2022,
	title = {{LM}-{Nav}: {Robotic} {Navigation} with {Large} {Pre}-{Trained} {Models} of {Language}, {Vision}, and {Action}},
	shorttitle = {{LM}-{Nav}},
	url = {http://arxiv.org/abs/2207.04429},
	doi = {10.48550/arXiv.2207.04429},
	abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions. For videos of our experiments, code release, and an interactive Colab notebook that runs in your browser, please check out our project page https://sites.google.com/view/lmnav},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
	month = jul,
	year = {2022},
	note = {arXiv:2207.04429 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shah_gnm_2022,
	title = {{GNM}: {A} {General} {Navigation} {Model} to {Drive} {Any} {Robot}},
	shorttitle = {{GNM}},
	url = {http://arxiv.org/abs/2210.03370},
	doi = {10.48550/arXiv.2210.03370},
	abstract = {Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out http://sites.google.com/view/drive-any-robot.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Shah, Dhruv and Sridhar, Ajay and Bhorkar, Arjun and Hirose, Noriaki and Levine, Sergey},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03370 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{chahine_robust_2023,
	title = {Robust flight navigation out of distribution with liquid neural networks},
	volume = {8},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.adc8892},
	doi = {10.1126/scirobotics.adc8892},
	abstract = {Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations and generalize well to online and unseen scenarios within the same environment they have been trained on. It is challenging for these agents to take a step further and robustly generalize to new environments with drastic scenery changes that they have never encountered. Here, we present a method to create robust flight navigation agents that successfully perform vision-based fly-to-target tasks beyond their training environment under drastic distribution shifts. To this end, we designed an imitation learning framework using liquid neural networks, a brain-inspired class of continuous-time neural models that are causal and adapt to changing conditions. We observed that liquid agents learn to distill the task they are given from visual inputs and drop irrelevant features. Thus, their learned navigation skills transferred to new environments. When compared with several other state-of-the-art deep agents, experiments showed that this level of robustness in decision-making is exclusive to liquid networks, both in their differential equation and closed-form representations.},
	number = {77},
	urldate = {2023-04-22},
	journal = {Science Robotics},
	author = {Chahine, Makram and Hasani, Ramin and Kao, Patrick and Ray, Aaron and Shubert, Ryan and Lechner, Mathias and Amini, Alexander and Rus, Daniela},
	month = apr,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eadc8892},
}

@misc{majumdar_where_2023,
	title = {Where are we in the search for an {Artificial} {Visual} {Cortex} for {Embodied} {Intelligence}?},
	url = {http://arxiv.org/abs/2303.18240},
	doi = {10.48550/arXiv.2303.18240},
	abstract = {We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. These models required over 10,000 GPU-hours to train and can be found on our website for the benefit of the research community.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Majumdar, Arjun and Yadav, Karmesh and Arnaud, Sergio and Ma, Yecheng Jason and Chen, Claire and Silwal, Sneha and Jain, Aryan and Berges, Vincent-Pierre and Abbeel, Pieter and Malik, Jitendra and Batra, Dhruv and Lin, Yixin and Maksymets, Oleksandr and Rajeswaran, Aravind and Meier, Franziska},
	month = mar,
	year = {2023},
	note = {arXiv:2303.18240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yokoyama_adaptive_2023,
	title = {Adaptive {Skill} {Coordination} for {Robotic} {Mobile} {Manipulation}},
	url = {http://arxiv.org/abs/2304.00410},
	doi = {10.48550/arXiv.2304.00410},
	abstract = {We present Adaptive Skill Coordination (ASC) - an approach for accomplishing long-horizon tasks (e.g., mobile pick-and-place, consisting of navigating to an object, picking it, navigating to another location, placing it, repeating). ASC consists of three components - (1) a library of basic visuomotor skills (navigation, pick, place), (2) a skill coordination policy that chooses which skills are appropriate to use when, and (3) a corrective policy that adapts pre-trained skills when out-of-distribution states are perceived. All components of ASC rely only on onboard visual and proprioceptive sensing, without access to privileged information like pre-built maps or precise object locations, easing real-world deployment. We train ASC in simulated indoor environments, and deploy it zero-shot in two novel real-world environments on the Boston Dynamics Spot robot. ASC achieves near-perfect performance at mobile pick-and-place, succeeding in 59/60 (98\%) episodes, while sequentially executing skills succeeds in only 44/60 (73\%) episodes. It is robust to hand-off errors, changes in the environment layout, dynamic obstacles (e.g., people), and unexpected disturbances, making it an ideal framework for complex, long-horizon tasks. Supplementary videos available at adaptiveskillcoordination.github.io.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Yokoyama, Naoki and Clegg, Alexander William and Undersander, Eric and Ha, Sehoon and Batra, Dhruv and Rai, Akshara},
	month = apr,
	year = {2023},
	note = {arXiv:2304.00410 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{zhang_agil_2018,
	title = {{AGIL}: {Learning} {Attention} from {Human} for {Visuomotor} {Tasks}},
	shorttitle = {{AGIL}},
	url = {http://arxiv.org/abs/1806.03960},
	doi = {10.48550/arXiv.1806.03960},
	abstract = {When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agents' performance. With this motivation, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another network to predict human actions (the policy network). Incorporating the learned attention model from the gaze network into the policy network significantly improves the action prediction accuracy and task performance.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Zhang, Ruohan and Liu, Zhuode and Zhang, Luxin and Whritner, Jake A. and Muller, Karl S. and Hayhoe, Mary M. and Ballard, Dana H.},
	month = jun,
	year = {2018},
	note = {arXiv:1806.03960 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhang_atari-head_2019,
	title = {Atari-{HEAD}: {Atari} {Human} {Eye}-{Tracking} and {Demonstration} {Dataset}},
	shorttitle = {Atari-{HEAD}},
	url = {http://arxiv.org/abs/1903.06754},
	doi = {10.48550/arXiv.1903.06754},
	abstract = {Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115{\textbackslash}\% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Zhang, Ruohan and Walshe, Calen and Liu, Zhuode and Guan, Lin and Muller, Karl S. and Whritner, Jake A. and Zhang, Luxin and Hayhoe, Mary M. and Ballard, Dana H.},
	month = sep,
	year = {2019},
	note = {arXiv:1903.06754 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{saran_efficiently_2021,
	title = {Efficiently {Guiding} {Imitation} {Learning} {Agents} with {Human} {Gaze}},
	url = {http://arxiv.org/abs/2002.12500},
	doi = {10.48550/arXiv.2002.12500},
	abstract = {Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95\% for BC, 343\% for BCO, and 390\% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Saran, Akanksha and Zhang, Ruohan and Short, Elaine Schaertl and Niekum, Scott},
	month = apr,
	year = {2021},
	note = {arXiv:2002.12500 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{zhang_human_2020,
	title = {Human {Gaze} {Assisted} {Artificial} {Intelligence}: {A} {Review}},
	volume = {5},
	shorttitle = {Human {Gaze} {Assisted} {Artificial} {Intelligence}},
	url = {https://www.ijcai.org/proceedings/2020/689},
	doi = {10.24963/ijcai.2020/689},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2023-04-20},
	author = {Zhang, Ruohan and Saran, Akanksha and Liu, Bo and Zhu, Yifeng and Guo, Sihang and Niekum, Scott and Ballard, Dana and Hayhoe, Mary},
	month = jul,
	year = {2020},
	note = {ISSN: 1045-0823},
	pages = {4951--4958},
}

@inproceedings{gao_objectfolder_2022,
	title = {{ObjectFolder} 2.0: {A} {Multisensory} {Object} {Dataset} for {Sim2Real} {Transfer}},
	shorttitle = {{ObjectFolder} 2.0},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Gao_ObjectFolder_2.0_A_Multisensory_Object_Dataset_for_Sim2Real_Transfer_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-04-18},
	author = {Gao, Ruohan and Si, Zilin and Chang, Yen-Yu and Clarke, Samuel and Bohg, Jeannette and Fei-Fei, Li and Yuan, Wenzhen and Wu, Jiajun},
	year = {2022},
	pages = {10598--10608},
}

@misc{ze_visual_2023,
	title = {Visual {Reinforcement} {Learning} with {Self}-{Supervised} {3D} {Representations}},
	url = {http://arxiv.org/abs/2210.07241},
	doi = {10.48550/arXiv.2210.07241},
	abstract = {A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency in simulated manipulation tasks compared to 2D representation learning methods. Additionally, our learned policies transfer zero-shot to a real robot setup with only approximate geometric correspondence, and successfully solve motor control tasks that involve grasping and lifting from a single, uncalibrated RGB camera. Code and videos are available at https://yanjieze.com/3d4rl/ .},
	urldate = {2023-04-18},
	publisher = {arXiv},
	author = {Ze, Yanjie and Hansen, Nicklas and Chen, Yinbo and Jain, Mohit and Wang, Xiaolong},
	month = mar,
	year = {2023},
	note = {arXiv:2210.07241 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yang_hierarchical_2019,
	title = {Hierarchical {Deep} {Stereo} {Matching} on {High}-resolution {Images}},
	url = {https://arxiv.org/abs/1912.06704v1},
	abstract = {We explore the problem of real-time stereo matching on high-res imagery. Many state-of-the-art (SOTA) methods struggle to process high-res imagery because of memory constraints or speed limitations. To address this issue, we propose an end-to-end framework that searches for correspondences incrementally over a coarse-to-fine hierarchy. Because high-res stereo datasets are relatively rare, we introduce a dataset with high-res stereo pairs for both training and evaluation. Our approach achieved SOTA performance on Middlebury-v3 and KITTI-15 while running significantly faster than its competitors. The hierarchical design also naturally allows for anytime on-demand reports of disparity by capping intermediate coarse results, allowing us to accurately predict disparity for near-range structures with low latency (30ms). We demonstrate that the performance-vs-speed trade-off afforded by on-demand hierarchies may address sensing needs for time-critical applications such as autonomous driving.},
	language = {en},
	urldate = {2023-04-17},
	journal = {arXiv.org},
	author = {Yang, Gengshan and Manela, Joshua and Happold, Michael and Ramanan, Deva},
	month = dec,
	year = {2019},
}

@misc{tosi_nerf-supervised_2023,
	title = {{NeRF}-{Supervised} {Deep} {Stereo}},
	url = {http://arxiv.org/abs/2303.17603},
	doi = {10.48550/arXiv.2303.17603},
	abstract = {We introduce a novel framework for training deep stereo networks effortlessly and without any ground-truth. By leveraging state-of-the-art neural rendering solutions, we generate stereo training data from image sequences collected with a single handheld camera. On top of them, a NeRF-supervised training procedure is carried out, from which we exploit rendered stereo triplets to compensate for occlusions and depth maps as proxy labels. This results in stereo networks capable of predicting sharp and detailed disparity maps. Experimental results show that models trained under this regime yield a 30-40\% improvement over existing self-supervised methods on the challenging Middlebury dataset, filling the gap to supervised models and, most times, outperforming them at zero-shot generalization.},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Tosi, Fabio and Tonioni, Alessio and De Gregorio, Daniele and Poggi, Matteo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17603 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{radosavovic_real-world_2022,
	title = {Real-{World} {Robot} {Learning} with {Masked} {Visual} {Pre}-training},
	url = {http://arxiv.org/abs/2210.03109},
	doi = {10.48550/arXiv.2210.03109},
	abstract = {In this work, we explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual representations are pre-trained via a masked autoencoder (MAE), frozen, and then passed into a learnable control module. Unlike prior work, we show that the pre-trained representations are effective across a range of real-world robotic tasks and embodiments. We find that our encoder consistently outperforms CLIP (up to 75\%), supervised ImageNet pre-training (up to 81\%), and training from scratch (up to 81\%). Finally, we train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning.},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Radosavovic, Ilija and Xiao, Tete and James, Stephen and Abbeel, Pieter and Malik, Jitendra and Darrell, Trevor},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03109 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{sermanet_time-contrastive_2018,
	title = {Time-{Contrastive} {Networks}: {Self}-{Supervised} {Learning} from {Video}},
	shorttitle = {Time-{Contrastive} {Networks}},
	url = {http://arxiv.org/abs/1704.06888},
	doi = {10.48550/arXiv.1704.06888},
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey},
	month = mar,
	year = {2018},
	note = {arXiv:1704.06888 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{nanayakkara_concept2robot_2021,
	title = {{Concept2Robot}: {Learning} manipulation concepts from instructions and human demonstrations},
	volume = {40},
	issn = {0278-3649},
	shorttitle = {{Concept2Robot}},
	url = {https://doi.org/10.1177/02783649211046285},
	doi = {10.1177/02783649211046285},
	abstract = {We aim to endow a robot with the ability to learn manipulation concepts that link natural language instructions to motor skills. Our goal is to learn a single multi-task policy that takes as input a natural language instruction and an image of the initial scene and outputs a robot motion trajectory to achieve the specified task. This policy has to generalize over different instructions and environments. Our insight is that we can approach this problem through learning from demonstration by leveraging large-scale video datasets of humans performing manipulation actions. Thereby, we avoid more time-consuming processes such as teleoperation or kinesthetic teaching. We also avoid having to manually design task-specific rewards. We propose a two-stage learning process where we first learn single-task policies through reinforcement learning. The reward is provided by scoring how well the robot visually appears to perform the task. This score is given by a video-based action classifier trained on a large-scale human activity dataset. In the second stage, we train a multi-task policy through imitation learning to imitate all the single-task policies. In extensive simulation experiments, we show that the multi-task policy learns to perform a large percentage of the 78 different manipulation tasks on which it was trained. The tasks are of greater variety and complexity than previously considered robot manipulation tasks. We show that the policy generalizes over variations of the environment. We also show examples of successful generalization over novel but similar instructions.},
	number = {12-14},
	urldate = {2023-04-15},
	journal = {International Journal of Robotics Research},
	author = {Nanayakkara, Thrishantha and Barfoot, Tim and Howard, Thomas and Shao, Lin and Migimatsu, Toki and Zhang, Qiang and Yang, Karen and Bohg, Jeannette},
	month = dec,
	year = {2021},
	keywords = {Learning from demonstration, human–robot interaction, manipulation, natural language understanding},
	pages = {1419--1434},
}

@misc{nair_r3m_2022,
	title = {{R3M}: {A} {Universal} {Visual} {Representation} for {Robot} {Manipulation}},
	shorttitle = {{R3M}},
	url = {http://arxiv.org/abs/2203.12601},
	doi = {10.48550/arXiv.2203.12601},
	abstract = {We study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. Concretely, we pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20\% compared to training from scratch and by over 10\% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations. Code and pre-trained models are available at https://tinyurl.com/robotr3m.},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav},
	month = nov,
	year = {2022},
	note = {arXiv:2203.12601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{ye_learning_2023,
	title = {Learning {Continuous} {Grasping} {Function} with a {Dexterous} {Hand} from {Human} {Demonstrations}},
	url = {http://arxiv.org/abs/2207.05053},
	doi = {10.48550/arXiv.2207.05053},
	abstract = {We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project page is available at https://jianglongye.com/cgf .},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Ye, Jianglong and Wang, Jiashun and Huang, Binghao and Qin, Yuzhe and Wang, Xiaolong},
	month = mar,
	year = {2023},
	note = {arXiv:2207.05053 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{lai_eye_2022,
	title = {In the {Eye} of {Transformer}: {Global}-{Local} {Correlation} for {Egocentric} {Gaze} {Estimation}},
	shorttitle = {In the {Eye} of {Transformer}},
	url = {http://arxiv.org/abs/2208.04464},
	doi = {10.48550/arXiv.2208.04464},
	abstract = {In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel Global-Local Correlation (GLC) module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets - EGTEA Gaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds previous state-of-the-arts by a large margin. We also provide additional visualizations to support our claim that global-local correlation serves a key representation for predicting gaze fixation from egocentric videos. More details can be found in our website (https://bolinlai.github.io/GLC-EgoGazeEst).},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04464 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{song_consistency_2023,
	title = {Consistency {Models}},
	url = {http://arxiv.org/abs/2303.01469},
	doi = {10.48550/arXiv.2303.01469},
	abstract = {Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01469 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{lin_mesh_2021,
	title = {Mesh {Graphormer}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lin_Mesh_Graphormer_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Lin, Kevin and Wang, Lijuan and Liu, Zicheng},
	year = {2021},
	pages = {12939--12948},
}

@inproceedings{lin_end--end_2021,
	title = {End-to-{End} {Human} {Pose} and {Mesh} {Reconstruction} with {Transformers}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lin_End-to-End_Human_Pose_and_Mesh_Reconstruction_with_Transformers_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-04-12},
	author = {Lin, Kevin and Wang, Lijuan and Liu, Zicheng},
	year = {2021},
	pages = {1954--1963},
}

@inproceedings{pan_deep_2019,
	title = {Deep {Mesh} {Reconstruction} {From} {Single} {RGB} {Images} via {Topology} {Modification} {Networks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html},
	urldate = {2023-04-12},
	author = {Pan, Junyi and Han, Xiaoguang and Chen, Weikai and Tang, Jiapeng and Jia, Kui},
	year = {2019},
	pages = {9964--9973},
}

@article{martin-martin_jrdb_2021,
	title = {{JRDB}: {A} {Dataset} and {Benchmark} of {Egocentric} {Robot} {Visual} {Perception} of {Humans} in {Built} {Environments}},
	issn = {1939-3539},
	shorttitle = {{JRDB}},
	doi = {10.1109/TPAMI.2021.3070543},
	abstract = {We present JRDB, a novel egocentric dataset collected from our social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 spherical image from a fisheye camera and encoder values from the robot's wheels. Our dataset incorporates data from traditionally underrepresented scenes such as indoor environments and pedestrian areas, all from the ego-perspective of the robot, both stationary and navigating. The dataset has been annotated with over 2.4 million bounding boxes spread over 5 individual cameras and 1.8 million associated 3D cuboids around all people in the scenes totaling over 3500 time consistent trajectories. Together with our dataset and the annotations, we launch a benchmark and metrics for 2D and 3D person detection and tracking. With this dataset, which we plan on extending with further types of annotation in the future, we hope to provide a new source of data and a test-bench for research in the areas of egocentric robot vision, autonomous navigation, and all perceptual tasks around social robotics in human environments.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Martin-Martin, Roberto and Patel, Mihir and Rezatofighi, Hamid and Shenoi, Abhijeet and Gwak, Junyoung and Frankel, Eric and Sadeghian, Amir and Savarese, Silvio},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Annotations, Benchmark testing, Cameras, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Person Detection, Person Tracking, Robot Navigation, Robots, Sensors, Social Robotics, Three-dimensional displays, Two dimensional displays},
	pages = {1--1},
}

@inproceedings{tatarchenko_octree_2017,
	title = {Octree {Generating} {Networks}: {Efficient} {Convolutional} {Architectures} for {High}-{Resolution} {3D} {Outputs}},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
	month = oct,
	year = {2017},
	keywords = {3D Reconstruction, Octree, Sparse},
}

@inproceedings{park_deepsdf_2019,
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	shorttitle = {{DeepSDF}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html},
	urldate = {2022-09-27},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	year = {2019},
	pages = {165--174},
}

@inproceedings{groueix_papier-mache_2018,
	title = {A {Papier}-{Mâché} {Approach} to {Learning} {3D} {Surface} {Generation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html},
	urldate = {2023-04-07},
	author = {Groueix, Thibault and Fisher, Matthew and Kim, Vladimir G. and Russell, Bryan C. and Aubry, Mathieu},
	year = {2018},
	pages = {216--224},
}

@misc{zagoruyko_paying_2017,
	title = {Paying {More} {Attention} to {Attention}: {Improving} the {Performance} of {Convolutional} {Neural} {Networks} via {Attention} {Transfer}},
	shorttitle = {Paying {More} {Attention} to {Attention}},
	url = {http://arxiv.org/abs/1612.03928},
	doi = {10.48550/arXiv.1612.03928},
	abstract = {Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer},
	urldate = {2023-04-05},
	publisher = {arXiv},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = feb,
	year = {2017},
	note = {arXiv:1612.03928 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{liu_gaze_2019,
	address = {New York, NY, USA},
	series = {{ETRA} '19},
	title = {A gaze model improves autonomous driving},
	isbn = {978-1-4503-6709-7},
	url = {https://doi.org/10.1145/3314111.3319846},
	doi = {10.1145/3314111.3319846},
	abstract = {End-to-end behavioral cloning trained by human demonstration is now a popular approach for vision-based autonomous driving. A deep neural network maps drive-view images directly to steering commands. However, the images contain much task-irrelevant data. Humans attend to behaviorally relevant information using saccades that direct gaze towards important areas. We demonstrate that behavioral cloning also benefits from active control of gaze. We trained a conditional generative adversarial network (GAN) that accurately predicts human gaze maps while driving in both familiar and unseen environments. We incorporated the predicted gaze maps into end-to-end networks for two behaviors: following and overtaking. Incorporating gaze information significantly improves generalization to unseen environments. We hypothesize that incorporating gaze information enables the network to focus on task critical objects, which vary little between environments, and ignore irrelevant elements in the background, which vary greatly.},
	urldate = {2023-04-02},
	booktitle = {Proceedings of the 11th {ACM} {Symposium} on {Eye} {Tracking} {Research} \& {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Congcong and Chen, Yuying and Tai, Lei and Ye, Haoyang and Liu, Ming and Shi, Bertram E.},
	month = jun,
	year = {2019},
	keywords = {autonomous driving, eye tracking, imitation learning},
	pages = {1--5},
}

@misc{chen_gaze_2019,
	title = {Gaze {Training} by {Modulated} {Dropout} {Improves} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1904.08377},
	doi = {10.48550/arXiv.1904.08377},
	abstract = {Imitation learning by behavioral cloning is a prevalent method that has achieved some success in vision-based autonomous driving. The basic idea behind behavioral cloning is to have the neural network learn from observing a human expert's behavior. Typically, a convolutional neural network learns to predict the steering commands from raw driver-view images by mimicking the behaviors of human drivers. However, there are other cues, such as gaze behavior, available from human drivers that have yet to be exploited. Previous researches have shown that novice human learners can benefit from observing experts' gaze patterns. We present here that deep neural networks can also profit from this. We propose a method, gaze-modulated dropout, for integrating this gaze information into a deep driving network implicitly rather than as an additional input. Our experimental results demonstrate that gaze-modulated dropout enhances the generalization capability of the network to unseen scenes. Prediction error in steering commands is reduced by 23.5\% compared to uniform dropout. Running closed loop in the simulator, the gaze-modulated dropout net increased the average distance travelled between infractions by 58.5\%. Consistent with these results, the gaze-modulated dropout net shows lower model uncertainty.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Chen, Yuying and Liu, Congcong and Tai, Lei and Liu, Ming and Shi, Bertram E.},
	month = aug,
	year = {2019},
	note = {arXiv:1904.08377 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{chen_robot_2020,
	title = {Robot {Navigation} in {Crowds} by {Graph} {Convolutional} {Networks} {With} {Attention} {Learned} {From} {Human} {Gaze}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2972868},
	abstract = {Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment. We incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.4\% and decreasing navigation time by 16.4\%.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Yuying and Liu, Congcong and Shi, Bertram E. and Liu, Ming},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aggregates, Autonomous vehicle navigation, Collision avoidance, Navigation, Reinforcement learning, Robot sensing systems, Task analysis, deep learning in robotics and automation, social human-robot interaction},
	pages = {2754--2761},
}

@misc{mondal_gazeformer_2023,
	title = {Gazeformer: {Scalable}, {Effective} and {Fast} {Prediction} of {Goal}-{Directed} {Human} {Attention}},
	shorttitle = {Gazeformer},
	url = {http://arxiv.org/abs/2303.15274},
	doi = {10.48550/arXiv.2303.15274},
	abstract = {Predicting human gaze is important in Human-Computer Interaction (HCI). However, to practically serve HCI applications, gaze prediction models must be scalable, fast, and accurate in their spatial and temporal gaze predictions. Recent scanpath prediction models focus on goal-directed attention (search). Such models are limited in their application due to a common approach relying on trained target detectors for all possible objects, and the availability of human gaze data for their training (both not scalable). In response, we pose a new task called ZeroGaze, a new variant of zero-shot learning where gaze is predicted for never-before-searched objects, and we develop a novel model, Gazeformer, to solve the ZeroGaze problem. In contrast to existing methods using object detector modules, Gazeformer encodes the target using a natural language model, thus leveraging semantic similarities in scanpath prediction. We use a transformer-based encoder-decoder architecture because transformers are particularly useful for generating contextual representations. Gazeformer surpasses other models by a large margin on the ZeroGaze setting. It also outperforms existing target-detection models on standard gaze prediction for both target-present and target-absent search tasks. In addition to its improved performance, Gazeformer is more than five times faster than the state-of-the-art target-present visual search model.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Mondal, Sounak and Yang, Zhibo and Ahn, Seoyoung and Samaras, Dimitris and Zelinsky, Gregory and Hoai, Minh},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15274 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{vendrow_jrdb-pose_2023,
	title = {{JRDB}-{Pose}: {A} {Large}-scale {Dataset} for {Multi}-{Person} {Pose} {Estimation} and {Tracking}},
	shorttitle = {{JRDB}-{Pose}},
	url = {http://arxiv.org/abs/2210.11940},
	doi = {10.48550/arXiv.2210.11940},
	abstract = {Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets either do not provide pose annotations or include scene types unrelated to robotic applications. Many datasets also lack the diversity of poses and occlusions found in crowded human scenes. To address this limitation we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking using videos captured from a social navigation robot. The dataset contains challenge scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene. A public evaluation server is made available for fair evaluation on a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .},
	urldate = {2023-03-31},
	publisher = {arXiv},
	author = {Vendrow, Edward and Le, Duy Tho and Cai, Jianfei and Rezatofighi, Hamid},
	month = mar,
	year = {2023},
	note = {arXiv:2210.11940 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{ehsanpour_jrdb-act_2021,
	title = {{JRDB}-{Act}: {A} {Large}-scale {Dataset} for {Spatio}-temporal {Action}, {Social} {Group} and {Activity} {Detection}},
	shorttitle = {{JRDB}-{Act}},
	url = {http://arxiv.org/abs/2106.08827},
	doi = {10.48550/arXiv.2106.08827},
	abstract = {The availability of large-scale video action understanding datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numerous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a significant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple{\textasciitilde}(optional) interaction-based action labels. Moreover JRDB-Act provides social group annotation, conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities{\textasciitilde}(common activities in each social group). Each annotated label in JRDB-Act is tagged with the annotators' confidence level which contributes to the development of reliable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection. The data and the evaluation code is publicly available at https://jrdb.erc.monash.edu/.},
	urldate = {2023-03-31},
	publisher = {arXiv},
	author = {Ehsanpour, Mahsa and Saleh, Fatemeh and Savarese, Silvio and Reid, Ian and Rezatofighi, Hamid},
	month = nov,
	year = {2021},
	note = {arXiv:2106.08827 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{choy_3d-r2n2_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{3D}-{R2N2}: {A} {Unified} {Approach} for {Single} and {Multi}-view {3D} {Object} {Reconstruction}},
	isbn = {978-3-319-46484-8},
	shorttitle = {{3D}-{R2N2}},
	doi = {10.1007/978-3-319-46484-8_38},
	abstract = {Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data [13]. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework (i) outperforms the state-of-the-art methods for single view reconstruction, and (ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Choy, Christopher B. and Xu, Danfei and Gwak, JunYoung and Chen, Kevin and Savarese, Silvio},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Multi-view, Reconstruction, Recurrent neural network},
	pages = {628--644},
}

@misc{wiles_silnet_2017,
	title = {{SilNet} : {Single}- and {Multi}-{View} {Reconstruction} by {Learning} from {Silhouettes}},
	shorttitle = {{SilNet}},
	url = {http://arxiv.org/abs/1711.07888},
	doi = {10.48550/arXiv.1711.07888},
	abstract = {The objective of this paper is 3D shape understanding from single and multiple images. To this end, we introduce a new deep-learning architecture and loss function, SilNet, that can handle multiple views in an order-agnostic manner. The architecture is fully convolutional, and for training we use a proxy task of silhouette prediction, rather than directly learning a mapping from 2D images to 3D shape as has been the target in most recent work. We demonstrate that with the SilNet architecture there is generalisation over the number of views -- for example, SilNet trained on 2 views can be used with 3 or 4 views at test-time; and performance improves with more views. We introduce two new synthetics datasets: a blobby object dataset useful for pre-training, and a challenging and realistic sculpture dataset; and demonstrate on these datasets that SilNet has indeed learnt 3D shape. Finally, we show that SilNet exceeds the state of the art on the ShapeNet benchmark dataset, and use SilNet to generate novel views of the sculpture dataset.},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Wiles, Olivia and Zisserman, Andrew},
	month = nov,
	year = {2017},
	note = {arXiv:1711.07888 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wu_marrnet_2017,
	title = {{MarrNet}: {3D} {Shape} {Reconstruction} via 2.{5D} {Sketches}},
	volume = {30},
	shorttitle = {{MarrNet}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html},
	abstract = {3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenge for learning-based approaches, as 3D object annotations in real images are scarce. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from the domain adaptation issue when tested on real data.  In this work, we propose an end-to-end trainable framework, sequentially estimating 2.5D sketches and 3D object shapes. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image, and to transfer from synthetic to real data. Second, for 3D reconstruction from the 2.5D sketches, we can easily transfer the learned model on synthetic data to real images, as rendered 2.5D sketches are invariant to object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches, making the framework end-to-end trainable on real images, requiring no real-image annotations. Our framework achieves state-of-the-art performance on 3D shape reconstruction.},
	urldate = {2023-03-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Jiajun and Wang, Yifan and Xue, Tianfan and Sun, Xingyuan and Freeman, Bill and Tenenbaum, Josh},
	year = {2017},
}

@inproceedings{yan_perspective_2016,
	title = {Perspective {Transformer} {Nets}: {Learning} {Single}-{View} {3D} {Object} {Reconstruction} without {3D} {Supervision}},
	volume = {29},
	shorttitle = {Perspective {Transformer} {Nets}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html},
	abstract = {Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the projective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.},
	urldate = {2023-03-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yan, Xinchen and Yang, Jimei and Yumer, Ersin and Guo, Yijie and Lee, Honglak},
	year = {2016},
}

@inproceedings{knyaz_image--voxel_2018,
	title = {Image-to-{Voxel} {Model} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content_eccv_2018_workshops/w6/html/Knyaz_Image-to-Voxel_Model_Translation_with_Conditional_Adversarial_Networks_ECCVW_2018_paper.html},
	urldate = {2023-03-29},
	author = {Knyaz, Vladimir A. and Kniaz, Vladimir V. and Remondino, Fabio},
	year = {2018},
	pages = {0--0},
}

@misc{thavamani_learning_2023,
	title = {Learning to {Zoom} and {Unzoom}},
	url = {http://arxiv.org/abs/2303.15390},
	doi = {10.48550/arXiv.2303.15390},
	abstract = {Many perception systems in mobile computing, autonomous navigation, and AR/VR face strict compute constraints that are particularly challenging for high-resolution input images. Previous works propose nonuniform downsamplers that "learn to zoom" on salient image regions, reducing compute while retaining task-relevant image information. However, for tasks with spatial labels (such as 2D/3D object detection and semantic segmentation), such distortions may harm performance. In this work (LZU), we "learn to zoom" in on the input image, compute spatial features, and then "unzoom" to revert any deformations. To enable efficient and differentiable unzooming, we approximate the zooming warp with a piecewise bilinear mapping that is invertible. LZU can be applied to any task with 2D spatial input and any model with 2D spatial features, and we demonstrate this versatility by evaluating on a variety of tasks and datasets: object detection on Argoverse-HD, semantic segmentation on Cityscapes, and monocular 3D object detection on nuScenes. Interestingly, we observe boosts in performance even when high-resolution sensor data is unavailable, implying that LZU can be used to "learn to upsample" as well.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Thavamani, Chittesh and Li, Mengtian and Ferroni, Francesco and Ramanan, Deva},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15390 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hart_using_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Human}-{Inspired} {Signals} to {Disambiguate} {Navigational} {Intentions}},
	isbn = {978-3-030-62056-1},
	doi = {10.1007/978-3-030-62056-1_27},
	abstract = {People are proficient at communicating their intentions in order to avoid conflicts when navigating in narrow, crowded environments. Mobile robots, on the other hand, often lack both the ability to interpret human intentions and the ability to clearly communicate their own intentions to people sharing their space. This work addresses the second of these points, leveraging insights about how people implicitly communicate with each other through gaze to enable mobile robots to more clearly signal their navigational intention. We present a human study measuring the importance of gaze in coordinating people’s navigation. This study is followed by the development of a virtual agent head which is added to a mobile robot platform. Comparing the performance of a robot with a virtual agent head against one with an LED turn signal demonstrates its ability to impact people’s navigational choices, and that people more easily interpret the gaze cue than the LED turn signal.},
	language = {en},
	booktitle = {Social {Robotics}},
	publisher = {Springer International Publishing},
	author = {Hart, Justin and Mirsky, Reuth and Xiao, Xuesu and Tejeda, Stone and Mahajan, Bonny and Goo, Jamin and Baldauf, Kathryn and Owen, Sydney and Stone, Peter},
	editor = {Wagner, Alan R. and Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Williams, Thomas and He, Hongsheng and Sam Ge, Shuzhi},
	year = {2020},
	keywords = {Gaze, Human-robot interaction, Social navigation},
	pages = {320--331},
}

@misc{nguyen_toward_2023,
	title = {Toward {Human}-{Like} {Social} {Robot} {Navigation}: {A} {Large}-{Scale}, {Multi}-{Modal}, {Social} {Human} {Navigation} {Dataset}},
	shorttitle = {Toward {Human}-{Like} {Social} {Robot} {Navigation}},
	url = {http://arxiv.org/abs/2303.14880},
	doi = {10.48550/arXiv.2303.14880},
	abstract = {Humans are well-adept at navigating public spaces shared with others, where current autonomous mobile robots still struggle: while safely and efficiently reaching their goals, humans communicate their intentions and conform to unwritten social norms on a daily basis; conversely, robots become clumsy in those daily social scenarios, getting stuck in dense crowds, surprising nearby pedestrians, or even causing collisions. While recent research on robot learning has shown promises in data-driven social robot navigation, good-quality training data is still difficult to acquire through either trial and error or expert demonstrations. In this work, we propose to utilize the body of rich, widely available, social human navigation data in many natural human-inhabited public spaces for robots to learn similar, human-like, socially compliant navigation behaviors. To be specific, we design an open-source egocentric data collection sensor suite wearable by walking humans to provide multi-modal robot perception data; we collect a large-scale ({\textasciitilde}50 km, 10 hours, 150 trials, 7 humans) dataset in a variety of public spaces which contain numerous natural social navigation interactions; we analyze our dataset, demonstrate its usability, and point out future research directions and use cases.},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Nguyen, Duc M. and Nazeri, Mohammad and Payandeh, Amirreza and Datar, Aniket and Xiao, Xuesu},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14880 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{shen_situational_2019,
	title = {Situational {Fusion} of {Visual} {Representation} for {Visual} {Navigation}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Situational_Fusion_of_Visual_Representation_for_Visual_Navigation_ICCV_2019_paper.html},
	urldate = {2023-03-27},
	author = {Shen, William B. and Xu, Danfei and Zhu, Yuke and Guibas, Leonidas J. and Fei-Fei, Li and Savarese, Silvio},
	year = {2019},
	pages = {2881--2890},
}

@inproceedings{wasserman_last-mile_2023,
	title = {Last-{Mile} {Embodied} {Visual} {Navigation}},
	url = {https://proceedings.mlr.press/v205/wasserman23a.html},
	abstract = {Realistic long-horizon tasks like image-goal navigation involve exploratory and exploitative phases. Assigned with an image of the goal, an embodied agent must explore to discover the goal, i.e., search efficiently using learned priors. Once the goal is discovered, the agent must accurately calibrate the last-mile of navigation to the goal. As with any robust system, switches between exploratory goal discovery and exploitative last-mile navigation enable better recovery from errors. Following these intuitive guide rails, we propose SLING to improve the performance of existing image-goal navigation systems. Entirely complementing prior methods, we focus on last-mile navigation and leverage the underlying geometric structure of the problem with neural descriptors. With simple but effective switches, we can easily connect SLING with heuristic, reinforcement learning, and neural modular policies. On a standardized image-goal navigation benchmark (Hahn et al. 2021), we improve performance across policies, scenes, and episode complexity, raising the state-of-the-art from 45\% to 55\% success rate. Beyond photorealistic simulation, we conduct real-robot experiments in three physical scenes and find these improvements to transfer well to real environments.},
	language = {en},
	urldate = {2023-03-27},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Wasserman, Justin and Yadav, Karmesh and Chowdhary, Girish and Gupta, Abhinav and Jain, Unnat},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {666--678},
}

@inproceedings{shaw_videodex_2023,
	title = {{VideoDex}: {Learning} {Dexterity} from {Internet} {Videos}},
	shorttitle = {{VideoDex}},
	url = {https://proceedings.mlr.press/v205/shaw23a.html},
	abstract = {To build general robotic agents that can operate in many environments, it is often imperative for the robot to collect experience in the real world.  However, this is often not feasible due to safety, time and hardware restrictions.  We thus propose leveraging the next best thing as real world experience: internet videos of humans using their hands.  Visual priors, such as visual features, are often learned from videos, but we believe that more information from videos can be utilized as a stronger prior.  We build a learning algorithm, Videodex, that leverages visual, action and physical priors from human video datasets to guide robot behavior.  These action and physical priors in the neural network dictate the typical human behavior for a particular robot task.   We test our approach on a robot arm and dexterous hand based system and show strong results on many different manipulation tasks, outperforming various state-of-the-art methods. For videos and supplemental material visit our website at https://video-dex.github.io.},
	language = {en},
	urldate = {2023-03-27},
	booktitle = {Proceedings of {The} 6th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Shaw, Kenneth and Bahl, Shikhar and Pathak, Deepak},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {654--665},
}

@misc{schwinn_simulating_2022,
	title = {Simulating {Human} {Gaze} with {Neural} {Visual} {Attention}},
	url = {http://arxiv.org/abs/2211.12100},
	doi = {10.48550/arXiv.2211.12100},
	abstract = {Existing models of human visual attention are generally unable to incorporate direct task guidance and therefore cannot model an intent or goal when exploring a scene. To integrate guidance of any downstream visual task into attention modeling, we propose the Neural Visual Attention (NeVA) algorithm. To this end, we impose to neural networks the biological constraint of foveated vision and train an attention mechanism to generate visual explorations that maximize the performance with respect to the downstream task. We observe that biologically constrained neural networks generate human-like scanpaths without being trained for this objective. Extensive experiments on three common benchmark datasets show that our method outperforms state-of-the-art unsupervised human attention models in generating human-like scanpaths.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Schwinn, Leo and Precup, Doina and Eskofier, Bjoern and Zanca, Dario},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12100 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	url = {https://proceedings.mlr.press/v97/houlsby19a.html},
	abstract = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8\% of the performance of full fine-tuning, adding only 3.63.63.6\% parameters per task. By contrast, fine-tuning trains 100100100\% of the parameters per task.},
	language = {en},
	urldate = {2023-03-24},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2790--2799},
}

@misc{ye_affordance_2023,
	title = {Affordance {Diffusion}: {Synthesizing} {Hand}-{Object} {Interactions}},
	shorttitle = {Affordance {Diffusion}},
	url = {http://arxiv.org/abs/2303.12538},
	doi = {10.48550/arXiv.2303.12538},
	abstract = {Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (ie, an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation. Project page: https://judyye.github.io/affordiffusion-www},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Ye, Yufei and Li, Xueting and Gupta, Abhinav and De Mello, Shalini and Birchfield, Stan and Song, Jiaming and Tulsiani, Shubham and Liu, Sifei},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12538 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{li_tell_2018,
	title = {Tell {Me} {Where} to {Look}: {Guided} {Attention} {Inference} {Network}},
	shorttitle = {Tell {Me} {Where} to {Look}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Tell_Me_Where_CVPR_2018_paper.html},
	urldate = {2023-03-23},
	author = {Li, Kunpeng and Wu, Ziyan and Peng, Kuan-Chuan and Ernst, Jan and Fu, Yun},
	year = {2018},
	pages = {9215--9223},
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent {Models} of {Visual} {Attention}},
	volume = {27},
	url = {https://papers.nips.cc/paper_files/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html},
	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	urldate = {2023-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and kavukcuoglu, koray},
	year = {2014},
}

@article{schwinn_behind_2022,
	title = {Behind the {Machine}’s {Gaze}: {Neural} {Networks} with {Biologically}-inspired {Constraints} {Exhibit} {Human}-like {Visual} {Attention}},
	issn = {2835-8856},
	shorttitle = {Behind the {Machine}’s {Gaze}},
	url = {https://openreview.net/forum?id=7iSYW1FRWA},
	abstract = {By and large, existing computational models of visual attention tacitly assume perfect vision and full access to the stimulus and thereby deviate from foveated biological vision. Moreover, modeling top-down attention is generally reduced to the integration of semantic features without incorporating the signal of a high-level visual tasks that have been shown to partially guide human attention. We propose the Neural Visual Attention (NeVA) algorithm to generate visual scanpaths in a top-down manner. With our method, we explore the ability of neural networks on which we impose a biologically-inspired foveated vision constraint to generate human-like scanpaths without directly training for this objective. The loss of a neural network performing a downstream visual task (i.e., classification or reconstruction) flexibly provides top-down guidance to the scanpath. Extensive experiments show that our method outperforms state-of-the-art unsupervised human attention models in terms of similarity to human scanpaths. Additionally, the flexibility of the framework allows to quantitatively investigate the role of different tasks in the generated visual behaviors. Finally, we demonstrate the superiority of the approach in a novel experiment that investigates the utility of scanpaths in real-world applications, where imperfect viewing conditions are given.},
	language = {en},
	urldate = {2023-03-22},
	journal = {Transactions on Machine Learning Research},
	author = {Schwinn, Leo and Precup, Doina and Eskofier, Bjoern and Zanca, Dario},
	month = oct,
	year = {2022},
}

@article{van_dyck_guiding_2022,
	title = {Guiding {Visual} {Attention} in {Deep} {Convolutional} {Neural} {Networks} {Based} on {Human} {Eye} {Movements}},
	volume = {16},
	issn = {1662-453X},
	url = {http://arxiv.org/abs/2206.10587},
	doi = {10.3389/fnins.2022.975639},
	abstract = {Deep Convolutional Neural Networks (DCNNs) were originally inspired by principles of biological vision, have evolved into best current computational models of object recognition, and consequently indicate strong architectural and functional parallelism with the ventral visual pathway throughout comparisons with neuroimaging and neural time series data. As recent advances in deep learning seem to decrease this similarity, computational neuroscience is challenged to reverse-engineer the biological plausibility to obtain useful models. While previous studies have shown that biologically inspired architectures are able to amplify the human-likeness of the models, in this study, we investigate a purely data-driven approach. We use human eye tracking data to directly modify training examples and thereby guide the models' visual attention during object recognition in natural images either towards or away from the focus of human fixations. We compare and validate different manipulation types (i.e., standard, human-like, and non-human-like attention) through GradCAM saliency maps against human participant eye tracking data. Our results demonstrate that the proposed guided focus manipulation works as intended in the negative direction and non-human-like models focus on significantly dissimilar image parts compared to humans. The observed effects were highly category-specific, enhanced by animacy and face presence, developed only after feedforward processing was completed, and indicated a strong influence on face detection. With this approach, however, no significantly increased human-likeness was found. Possible applications of overt visual attention in DCNNs and further implications for theories of face detection are discussed.},
	urldate = {2023-03-22},
	journal = {Frontiers in Neuroscience},
	author = {van Dyck, Leonard E. and Denzler, Sebastian J. and Gruber, Walter R.},
	month = sep,
	year = {2022},
	note = {arXiv:2206.10587 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {975639},
}

@misc{zhang_couch_2022,
	title = {{COUCH}: {Towards} {Controllable} {Human}-{Chair} {Interactions}},
	shorttitle = {{COUCH}},
	url = {https://arxiv.org/abs/2205.00541v1},
	abstract = {Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts.},
	language = {en},
	urldate = {2023-03-22},
	journal = {arXiv.org},
	author = {Zhang, Xiaohan and Bhatnagar, Bharat Lal and Guzov, Vladimir and Starke, Sebastian and Pons-Moll, Gerard},
	month = may,
	year = {2022},
}

@misc{geng_gapartnet_2022,
	title = {{GAPartNet}: {Cross}-{Category} {Domain}-{Generalizable} {Object} {Perception} and {Manipulation} via {Generalizable} and {Actionable} {Parts}},
	shorttitle = {{GAPartNet}},
	url = {https://arxiv.org/abs/2211.05272v1},
	abstract = {Perceiving and manipulating objects in a generalizable way has been actively studied by the computer vision and robotics communities, where cross-category generalizable manipulation skills are highly desired yet underexplored. In this work, we propose to learn such generalizable perception and manipulation via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (e.g. buttons, handles, etc), we show that our part-centric approach allows our method to learn object perception and manipulation skills from seen object categories and directly generalize to unseen categories. Following the GAPart definition, we construct a large-scale part-centric interactive dataset, GAPartNet, where rich, part-level annotations (semantics, poses) are provided for 1166 objects and 8489 part instances. Based on GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the large domain gaps between seen and unseen object categories, we propose a strong 3D segmentation method from the perspective of domain generalization by integrating adversarial learning techniques. Our method outperforms all existing methods by a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both simulation and real world. The dataset and code will be released.},
	language = {en},
	urldate = {2023-03-22},
	journal = {arXiv.org},
	author = {Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
	month = nov,
	year = {2022},
}

@misc{datta_integrating_2020,
	title = {Integrating {Egocentric} {Localization} for {More} {Realistic} {Point}-{Goal} {Navigation} {Agents}},
	url = {http://arxiv.org/abs/2009.03231},
	doi = {10.48550/arXiv.2009.03231},
	abstract = {Recent work has presented embodied agents that can navigate to point-goal targets in novel indoor environments with near-perfect accuracy. However, these agents are equipped with idealized sensors for localization and take deterministic actions. This setting is practically sterile by comparison to the dirty reality of noisy sensors and actuations in the real world -- wheels can slip, motion sensors have error, actuations can rebound. In this work, we take a step towards this noisy reality, developing point-goal navigation agents that rely on visual estimates of egomotion under noisy action dynamics. We find these agents outperform naive adaptions of current point-goal agents to this setting as well as those incorporating classic localization baselines. Further, our model conceptually divides learning agent dynamics or odometry (where am I?) from task-specific navigation policy (where do I want to go?). This enables a seamless adaption to changing dynamics (a different robot or floor type) by simply re-calibrating the visual odometry model -- circumventing the expense of re-training of the navigation policy. Our agent was the runner-up in the PointNav track of CVPR 2020 Habitat Challenge.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Datta, Samyak and Maksymets, Oleksandr and Hoffman, Judy and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	month = sep,
	year = {2020},
	note = {arXiv:2009.03231 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ye_auxiliary_2020,
	title = {Auxiliary {Tasks} {Speed} {Up} {Learning} {PointGoal} {Navigation}},
	url = {http://arxiv.org/abs/2007.04561},
	doi = {10.48550/arXiv.2007.04561},
	abstract = {PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable but their method is computationally prohibitive, requiring 2.5 billion frames and 180 GPU-days. In this work, we develop a method to significantly increase sample and time efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory,etc.).We find that naively combining multiple auxiliary tasks improves sample efficiency,but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations learnt from individual auxiliary tasks. Our best agent is 5.5x faster to reach the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is publicly available at https://github.com/joel99/habitat-pointnav-aux.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Ye, Joel and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},
	month = nov,
	year = {2020},
	note = {arXiv:2007.04561 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{anderson_evaluation_2018,
	title = {On {Evaluation} of {Embodied} {Navigation} {Agents}},
	url = {http://arxiv.org/abs/1807.06757},
	doi = {10.48550/arXiv.1807.06757},
	abstract = {Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Anderson, Peter and Chang, Angel and Chaplot, Devendra Singh and Dosovitskiy, Alexey and Gupta, Saurabh and Koltun, Vladlen and Kosecka, Jana and Malik, Jitendra and Mottaghi, Roozbeh and Savva, Manolis and Zamir, Amir R.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.06757 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wijmans_how_2020,
	title = {How to {Train} {PointGoal} {Navigation} {Agents} on a ({Sample} and {Compute}) {Budget}},
	url = {http://arxiv.org/abs/2012.06117},
	doi = {10.48550/arXiv.2012.06117},
	abstract = {PointGoal navigation has seen significant recent interest and progress, spurred on by the Habitat platform and associated challenge. In this paper, we study PointGoal navigation under both a sample budget (75 million frames) and a compute budget (1 GPU for 1 day). We conduct an extensive set of experiments, cumulatively totaling over 50,000 GPU-hours, that let us identify and discuss a number of ostensibly minor but significant design choices -- the advantage estimation procedure (a key component in training), visual encoder architecture, and a seemingly minor hyper-parameter change. Overall, these design choices to lead considerable and consistent improvements over the baselines present in Savva et al. Under a sample budget, performance for RGB-D agents improves 8 SPL on Gibson (14\% relative improvement) and 20 SPL on Matterport3D (38\% relative improvement). Under a compute budget, performance for RGB-D agents improves by 19 SPL on Gibson (32\% relative improvement) and 35 SPL on Matterport3D (220\% relative improvement). We hope our findings and recommendations will make serve to make the community's experiments more efficient.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Wijmans, Erik and Essa, Irfan and Batra, Dhruv},
	month = dec,
	year = {2020},
	note = {arXiv:2012.06117 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{partsey_is_2022,
	title = {Is {Mapping} {Necessary} for {Realistic} {PointGoal} {Navigation}?},
	url = {http://arxiv.org/abs/2206.00997},
	doi = {10.48550/arXiv.2206.00997},
	abstract = {Can an autonomous agent navigate in a new environment without building an explicit map? For the task of PointGoal navigation ('Go to \${\textbackslash}Delta x\$, \${\textbackslash}Delta y\$') under idealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the answer is a clear 'yes' - map-less neural models composed of task-agnostic components (CNNs and RNNs) trained with large-scale reinforcement learning achieve 100\% Success on a standard dataset (Gibson). However, for PointNav in a realistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open question; one we tackle in this paper. The strongest published result for this task is 71.7\% Success. First, we identify the main (perhaps, only) cause of the drop in performance: the absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D sensing and actuation noise achieves 99.8\% Success (Gibson-v2 val). This suggests that (to paraphrase a meme) robust visual odometry is all we need for realistic PointNav; if we can achieve that, we can ignore the sensing and actuation noise. With that as our operating hypothesis, we scale the dataset and model size, and develop human-annotation-free data-augmentation techniques to train models for visual odometry. We advance the state of art on the Habitat Realistic PointNav Challenge from 71\% to 94\% Success (+23, 31\% relative) and 53\% to 74\% SPL (+21, 40\% relative). While our approach does not saturate or 'solve' this dataset, this strong improvement combined with promising zero-shot sim2real transfer (to a LoCoBot) provides evidence consistent with the hypothesis that explicit mapping may not be necessary for navigation, even in a realistic setting.},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Partsey, Ruslan and Wijmans, Erik and Yokoyama, Naoki and Dobosevych, Oles and Batra, Dhruv and Maksymets, Oleksandr},
	month = jun,
	year = {2022},
	note = {arXiv:2206.00997 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kareer_vinl_2023,
	title = {{ViNL}: {Visual} {Navigation} and {Locomotion} {Over} {Obstacles}},
	shorttitle = {{ViNL}},
	url = {http://arxiv.org/abs/2210.14791},
	doi = {10.48550/arXiv.2210.14791},
	abstract = {We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual navigation or visual locomotion, to the best of our knowledge, this is the first fully learned approach that leverages vision to accomplish both (1) intelligent navigation in new environments, and (2) intelligent visual locomotion that aims to traverse cluttered environments without disrupting obstacles. On the task of navigation to distant goals in unknown environments, ViNL using just egocentric vision significantly outperforms prior work on robust locomotion using privileged terrain maps (+32.8\% success and -4.42 collisions per meter). Additionally, we ablate our locomotion policy to show that each aspect of our approach helps reduce obstacle collisions. Videos and code at http://www.joannetruong.com/projects/vinl.html},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Kareer, Simar and Yokoyama, Naoki and Batra, Dhruv and Ha, Sehoon and Truong, Joanne},
	month = jan,
	year = {2023},
	note = {arXiv:2210.14791 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wijmans_dd-ppo_2020,
	title = {{DD}-{PPO}: {Learning} {Near}-{Perfect} {PointGoal} {Navigators} from 2.5 {Billion} {Frames}},
	shorttitle = {{DD}-{PPO}},
	url = {http://arxiv.org/abs/1911.00357},
	doi = {10.48550/arXiv.1911.00357},
	abstract = {We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90\% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).},
	urldate = {2023-03-14},
	publisher = {arXiv},
	author = {Wijmans, Erik and Kadian, Abhishek and Morcos, Ari and Lee, Stefan and Essa, Irfan and Parikh, Devi and Savva, Manolis and Batra, Dhruv},
	month = jan,
	year = {2020},
	note = {arXiv:1911.00357 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{golroudbari_recent_2023,
	title = {Recent {Advancements} in {Deep} {Learning} {Applications} and {Methods} for {Autonomous} {Navigation} -- {A} {Comprehensive} {Review}},
	url = {http://arxiv.org/abs/2302.11089},
	doi = {10.48550/arXiv.2302.11089},
	abstract = {This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Golroudbari, Arman Asgharpoor and Sabour, Mohammad Hossein},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11089 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Electrical Engineering and Systems Science - Signal Processing, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{zhu_deep_2021,
	title = {Deep {Learning} for {Embodied} {Vision} {Navigation}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Embodied} {Vision} {Navigation}},
	url = {http://arxiv.org/abs/2108.04097},
	doi = {10.48550/arXiv.2108.04097},
	abstract = {"Embodied visual navigation" problem requires an agent to navigate in a 3D environment mainly rely on its first-person observation. This problem has attracted rising attention in recent years due to its wide application in autonomous driving, vacuum cleaner, and rescue robot. A navigation agent is supposed to have various intelligent skills, such as visual perceiving, mapping, planning, exploring and reasoning, etc. Building such an agent that observes, thinks, and acts is a key to real intelligence. The remarkable learning ability of deep learning methods empowered the agents to accomplish embodied visual navigation tasks. Despite this, embodied visual navigation is still in its infancy since a lot of advanced skills are required, including perceiving partially observed visual input, exploring unseen areas, memorizing and modeling seen scenarios, understanding cross-modal instructions, and adapting to a new environment, etc. Recently, embodied visual navigation has attracted rising attention of the community, and numerous works has been proposed to learn these skills. This paper attempts to establish an outline of the current works in the field of embodied visual navigation by providing a comprehensive literature survey. We summarize the benchmarks and metrics, review different methods, analysis the challenges, and highlight the state-of-the-art methods. Finally, we discuss unresolved challenges in the field of embodied visual navigation and give promising directions in pursuing future research.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Zhu, Fengda and Zhu, Yi and Lee, Vincent CS and Liang, Xiaodan and Chang, Xiaojun},
	month = oct,
	year = {2021},
	note = {arXiv:2108.04097 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{kim_deep_2015,
	title = {Deep {Neural} {Network} for {Real}-{Time} {Autonomous} {Indoor} {Navigation}},
	url = {http://arxiv.org/abs/1511.04668},
	doi = {10.48550/arXiv.1511.04668},
	abstract = {Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses many challenges. One main reason is that GPS has limited precision in indoor environments. The additional fact that MAVs are not able to carry heavy weight or power consuming sensors, such as range finders, makes indoor autonomous navigation a challenging task. In this paper, we propose a practical system in which a quadcopter autonomously navigates indoors and finds a specific target, i.e., a book bag, by using a single camera. A deep learning model, Convolutional Neural Network (ConvNet), is used to learn a controller strategy that mimics an expert pilot's choice of action. We show our system's performance through real-time experiments in diverse indoor locations. To understand more about our trained network, we use several visualization techniques.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Kim, Dong Ki and Chen, Tsuhan},
	month = nov,
	year = {2015},
	note = {arXiv:1511.04668 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{de_queiroz_mendes_deep_2021,
	title = {On deep learning techniques to boost monocular depth estimation for autonomous navigation},
	volume = {136},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889020305418},
	doi = {10.1016/j.robot.2020.103701},
	abstract = {Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.},
	language = {en},
	urldate = {2023-03-13},
	journal = {Robotics and Autonomous Systems},
	author = {de Queiroz Mendes, Raul and Ribeiro, Eduardo Godinho and dos Santos Rosa, Nicolas and Grassi, Valdir},
	month = feb,
	year = {2021},
	keywords = {CNN, Deep learning, SIDE},
	pages = {103701},
}

@article{murray_using_2000,
	title = {Using {Real}-{Time} {Stereo} {Vision} for {Mobile} {Robot} {Navigation}},
	volume = {8},
	issn = {1573-7527},
	url = {https://doi.org/10.1023/A:1008987612352},
	doi = {10.1023/A:1008987612352},
	abstract = {This paper describes a working vision-based mobile robot that navigates and autonomously explores its environment while building occupancy grid maps of the environment. We present a method for reducing stereo vision disparity images to two-dimensional map information. Stereo vision has several attributes that set it apart from other sensors more commonly used for occupancy grid mapping. We discuss these attributes, the errors that some of them create, and how to overcome them. We reduce errors by segmenting disparity images based on continuous disparity surfaces to reject “spikes” caused by stereo mismatches. Stereo vision processing and map updates are done at 5 Hz and the robot moves at speeds of 300 cm/s.},
	language = {en},
	number = {2},
	urldate = {2023-03-13},
	journal = {Autonomous Robots},
	author = {Murray, Don and Little, James J.},
	month = apr,
	year = {2000},
	keywords = {mobile robot navigation, stereo vision},
	pages = {161--171},
}

@misc{seo_learning_2023,
	title = {Learning to {Walk} by {Steering}: {Perceptive} {Quadrupedal} {Locomotion} in {Dynamic} {Environments}},
	shorttitle = {Learning to {Walk} by {Steering}},
	url = {http://arxiv.org/abs/2209.09233},
	doi = {10.48550/arXiv.2209.09233},
	abstract = {We tackle the problem of perceptive locomotion in dynamic environments. In this problem, a quadrupedal robot must exhibit robust and agile walking behaviors in response to environmental clutter and moving obstacles. We present a hierarchical learning framework, named PRELUDE, which decomposes the problem of perceptive locomotion into high-level decision-making to predict navigation commands and low-level gait generation to realize the target commands. In this framework, we train the high-level navigation controller with imitation learning on human demonstrations collected on a steerable cart and the low-level gait controller with reinforcement learning (RL). Therefore, our method can acquire complex navigation behaviors from human supervision and discover versatile gaits from trial and error. We demonstrate the effectiveness of our approach in simulation and with hardware experiments. Videos and code can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.},
	urldate = {2023-03-11},
	publisher = {arXiv},
	author = {Seo, Mingyo and Gupta, Ryan and Zhu, Yifeng and Skoutnev, Alexy and Sentis, Luis and Zhu, Yuke},
	month = feb,
	year = {2023},
	note = {arXiv:2209.09233 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{sandler_mobilenetv2_2018,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html},
	urldate = {2023-03-10},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	year = {2018},
	pages = {4510--4520},
}

@inproceedings{yu_bisenet_2018,
	title = {{BiSeNet}: {Bilateral} {Segmentation} {Network} for {Real}-time {Semantic} {Segmentation}},
	shorttitle = {{BiSeNet}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Changqian_Yu_BiSeNet_Bilateral_Segmentation_ECCV_2018_paper.html},
	urldate = {2023-03-09},
	author = {Yu, Changqian and Wang, Jingbo and Peng, Chao and Gao, Changxin and Yu, Gang and Sang, Nong},
	year = {2018},
	pages = {325--341},
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {https://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6105--6114},
}

@misc{ding_hidden_2023,
	title = {Hidden {Gems}: {4D} {Radar} {Scene} {Flow} {Learning} {Using} {Cross}-{Modal} {Supervision}},
	shorttitle = {Hidden {Gems}},
	url = {http://arxiv.org/abs/2303.00462},
	doi = {10.48550/arXiv.2303.00462},
	abstract = {This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on {\textbackslash}url\{https://github.com/Toytiny/CMFlow.\}},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Ding, Fangqiang and Palffy, Andras and Gavrila, Dariu M. and Lu, Chris Xiaoxuan},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00462 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{akmandor_reactive_2021,
	title = {Reactive navigation framework for mobile robots by heuristically evaluated pre-sampled trajectories},
	journal = {arXiv preprint arXiv:2105.08145},
	author = {Akmandor, Neset Unver and Padır, Taşkın},
	year = {2021},
}

@misc{huang_inner_2022,
	title = {Inner {Monologue}: {Embodied} {Reasoning} through {Planning} with {Language} {Models}},
	shorttitle = {Inner {Monologue}},
	url = {http://arxiv.org/abs/2207.05608},
	doi = {10.48550/arXiv.2207.05608},
	abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{shah_value_2022,
	title = {Value {Function} {Spaces}: {Skill}-{Centric} {State} {Abstractions} for {Long}-{Horizon} {Reasoning}},
	shorttitle = {Value {Function} {Spaces}},
	url = {http://arxiv.org/abs/2111.03189},
	doi = {10.48550/arXiv.2111.03189},
	abstract = {Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Shah, Dhruv and Xu, Peng and Lu, Yao and Xiao, Ted and Toshev, Alexander and Levine, Sergey and Ichter, Brian},
	month = mar,
	year = {2022},
	note = {arXiv:2111.03189 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhou_3d_2021,
	title = {{3D} {Shape} {Generation} and {Completion} through {Point}-{Voxel} {Diffusion}},
	url = {http://arxiv.org/abs/2104.03670},
	doi = {10.48550/arXiv.2104.03670},
	abstract = {We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Zhou, Linqi and Du, Yilun and Wu, Jiajun},
	month = aug,
	year = {2021},
	note = {arXiv:2104.03670 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lee_diffusion_2023,
	title = {Diffusion {Probabilistic} {Models} for {Scene}-{Scale} {3D} {Categorical} {Data}},
	url = {http://arxiv.org/abs/2301.00527},
	doi = {10.48550/arXiv.2301.00527},
	abstract = {In this paper, we learn a diffusion model to generate 3D data on a scene-scale. Specifically, our model crafts a 3D scene consisting of multiple objects, while recent diffusion research has focused on a single object. To realize our goal, we represent a scene with discrete class labels, i.e., categorical distribution, to assign multiple objects into semantic categories. Thus, we extend discrete diffusion models to learn scene-scale categorical distributions. In addition, we validate that a latent diffusion model can reduce computation costs for training and deploying. To the best of our knowledge, our work is the first to apply discrete and latent diffusion for 3D categorical data on a scene-scale. We further propose to perform semantic scene completion (SSC) by learning a conditional distribution using our diffusion model, where the condition is a partial observation in a sparse point cloud. In experiments, we empirically show that our diffusion models not only generate reasonable scenes, but also perform the scene completion task better than a discriminative model. Our code and models are available at https://github.com/zoomin-lee/scene-scale-diffusion},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Lee, Jumin and Im, Woobin and Lee, Sebin and Yoon, Sung-Eui},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kar_learning_2017,
	title = {Learning a {Multi}-{View} {Stereo} {Machine}},
	url = {http://arxiv.org/abs/1708.05375},
	doi = {10.48550/arXiv.1708.05375},
	abstract = {We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Kar, Abhishek and Häne, Christian and Malik, Jitendra},
	month = aug,
	year = {2017},
	note = {arXiv:1708.05375 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{peng_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Occupancy} {Networks}},
	isbn = {978-3-030-58580-8},
	doi = {10.1007/978-3-030-58580-8_31},
	abstract = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {523--540},
}

@misc{you_pseudo-lidar_2019,
	title = {Pseudo-{LiDAR}++: {Accurate} {Depth} for {3D} {Object} {Detection} in {Autonomous} {Driving}},
	shorttitle = {Pseudo-{LiDAR}++},
	url = {https://arxiv.org/abs/1906.06310v3},
	abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for faraway objects by 40\%. Our code is available at https://github.com/mileyan/Pseudo\_Lidar\_V2.},
	language = {en},
	urldate = {2023-02-23},
	journal = {arXiv.org},
	author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	month = jun,
	year = {2019},
	doi = {10.48550/arXiv.1906.06310},
}

@inproceedings{wang_detecting_2021,
	title = {Detecting and {Mapping} {Trees} in {Unstructured} {Environments} with a {Stereo} {Camera} and {Pseudo}-{Lidar}},
	doi = {10.1109/ICRA48506.2021.9562056},
	abstract = {We present a method for detecting and mapping trees in noisy stereo camera point clouds, using a learned 3D object detector. Inspired by recent advancements in 3-D object detection using a pseudo-lidar representation for stereo data, we train a PointRCNN detector to recognize trees in forest-like environments. We generate detector training data with a novel automatic labeling process that clusters a fused global point cloud. This process annotates large stereo point cloud training data sets with minimal user supervision, and unlike previous pseudo-lidar detection pipelines, requires no 3D ground truth from other sensors such as lidar. Our mapping system additionally uses a Kalman filter to associate detections and consistently estimate the positions and sizes of trees. We collect a data set for tree detection consisting of 8680 stereo point clouds, and validate our method on an outdoors test sequence. Our results demonstrate robust tree recognition in noisy stereo data at ranges of up to 7 meters, on 720p resolution images from a Stereolabs ZED 2 camera. Code and data are available at https://github.com/brian-h-wang/pseudolidar-tree-detection.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wang, Brian H. and Diaz-Ruiz, Carlos and Banfi, Jacopo and Campbell, Mark},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Detectors, Meters, Pipelines, Robot sensing systems, Three-dimensional displays, Training data},
	pages = {14120--14126},
}

@inproceedings{wang_pseudo-lidar_2019,
	title = {Pseudo-{LiDAR} {From} {Visual} {Depth} {Estimation}: {Bridging} the {Gap} in {3D} {Object} {Detection} for {Autonomous} {Driving}},
	shorttitle = {Pseudo-{LiDAR} {From} {Visual} {Depth} {Estimation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Pseudo-LiDAR_From_Visual_Depth_Estimation_Bridging_the_Gap_in_3D_CVPR_2019_paper.html},
	urldate = {2023-02-22},
	author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	year = {2019},
	pages = {8445--8453},
}

@misc{zhang_contrastive_2022,
	title = {Contrastive {Learning} of {Medical} {Visual} {Representations} from {Paired} {Images} and {Text}},
	url = {http://arxiv.org/abs/2010.00747},
	doi = {10.48550/arXiv.2010.00747},
	abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose ConVIRT, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test ConVIRT by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10{\textbackslash}\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	month = sep,
	year = {2022},
	note = {arXiv:2010.00747 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {https://proceedings.mlr.press/v162/huang22a.html},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
	language = {en},
	urldate = {2023-02-21},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {9118--9147},
}

@misc{akmandor_3d_2020,
	title = {A {3D} {Reactive} {Navigation} {Algorithm} for {Mobile} {Robots} by {Using} {Tentacle}-{Based} {Sampling}},
	url = {http://arxiv.org/abs/2001.09199},
	doi = {10.48550/arXiv.2001.09199},
	abstract = {This paper introduces a reactive navigation framework for mobile robots in 3-dimensional (3D) space. The proposed approach does not rely on the global map information and achieves fast navigation by employing a tentacle based sampling and their heuristic evaluations on-the-fly. This reactive nature of the approach comes from the prior arrangement of navigation points on tentacles (parametric contours) to sample the navigation space. These tentacles are evaluated at each time-step, based on heuristic features such as closeness to the goal, previous tentacle preferences and nearby obstacles in a robot-centered 3D grid. Then, the navigable sampling point on the selected tentacle is passed to a controller for the motion execution. The proposed framework does not only extend its 2D tentacle-based counterparts into 3D, but also introduces offline and online parameters, whose tuning provides versatility and adaptability of the algorithm to work in unknown environments. To demonstrate the superior performance of the proposed algorithm over a state-of-art method, the statistical results from physics-based simulations on various maps are presented. The video of the work is available at https://youtu.be/rrF7wHCz-0M.},
	urldate = {2022-09-04},
	publisher = {arXiv},
	author = {Akmandor, Neset Unver and Padır, Taşkın},
	month = jan,
	year = {2020},
	note = {arXiv:2001.09199 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{liu_va-depthnet_2023,
	title = {{VA}-{DepthNet}: {A} {Variational} {Approach} to {Single} {Image} {Depth} {Prediction}},
	shorttitle = {{VA}-{DepthNet}},
	url = {http://arxiv.org/abs/2302.06556},
	doi = {10.48550/arXiv.2302.06556},
	abstract = {We introduce VA-DepthNet, a simple, effective, and accurate deep neural network approach for the single-image depth prediction (SIDP) problem. The proposed approach advocates using classical first-order variational constraints for this problem. While state-of-the-art deep neural network methods for SIDP learn the scene depth from images in a supervised setting, they often overlook the invaluable invariances and priors in the rigid scene space, such as the regularity of the scene. The paper's main contribution is to reveal the benefit of classical and well-founded variational constraints in the neural network design for the SIDP task. It is shown that imposing first-order variational constraints in the scene space together with popular encoder-decoder-based network architecture design provides excellent results for the supervised SIDP task. The imposed first-order variational constraint makes the network aware of the depth gradient in the scene space, i.e., regularity. The paper demonstrates the usefulness of the proposed approach via extensive evaluation and ablation analysis over several benchmark datasets, such as KITTI, NYU Depth V2, and SUN RGB-D. The VA-DepthNet at test time shows considerable improvements in depth prediction accuracy compared to the prior art and is accurate also at high-frequency regions in the scene space. At the time of writing this paper, our method -- labeled as VA-DepthNet, when tested on the KITTI depth-prediction evaluation set benchmarks, shows state-of-the-art results, and is the top-performing published approach.},
	urldate = {2023-02-19},
	publisher = {arXiv},
	author = {Liu, Ce and Kumar, Suryansh and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zhou_detecting_2022,
	title = {Detecting {Twenty}-thousand {Classes} using {Image}-level {Supervision}},
	url = {http://arxiv.org/abs/2201.02605},
	doi = {10.48550/arXiv.2201.02605},
	abstract = {Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at {\textbackslash}url\{https://github.com/facebookresearch/Detic\}.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Krähenbühl, Philipp and Misra, Ishan},
	month = jul,
	year = {2022},
	note = {arXiv:2201.02605 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{eysenbach_contrastive_2022,
	title = {Contrastive {Learning} as {Goal}-{Conditioned} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2206.07568},
	doi = {10.48550/arXiv.2206.07568},
	abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},
	month = jun,
	year = {2022},
	note = {arXiv:2206.07568 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{xiao_masked_2022,
	title = {Masked {Visual} {Pre}-training for {Motor} {Control}},
	url = {http://arxiv.org/abs/2203.06173},
	doi = {10.48550/arXiv.2203.06173},
	abstract = {This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80\% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Xiao, Tete and Radosavovic, Ilija and Darrell, Trevor and Malik, Jitendra},
	month = mar,
	year = {2022},
	note = {arXiv:2203.06173 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{hafner_dream_2019,
	title = {Dream to {Control}: {Learning} {Behaviors} by {Latent} {Imagination}},
	shorttitle = {Dream to {Control}},
	url = {https://arxiv.org/abs/1912.01603v3},
	doi = {10.48550/arXiv.1912.01603},
	abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
	language = {en},
	urldate = {2023-02-15},
	author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
	month = dec,
	year = {2019},
}

@article{mendonca_alan_2023,
	title = {{ALAN}: {Autonomously} {Exploring} {Robotic} {Agents} in the {Real} {World}},
	shorttitle = {{ALAN}},
	url = {https://arxiv.org/abs/2302.06604v1},
	doi = {10.48550/arXiv.2302.06604},
	abstract = {Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images. Website at https://robo-explorer.github.io/},
	language = {en},
	urldate = {2023-02-15},
	author = {Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
	month = feb,
	year = {2023},
}

@misc{wi_virdo_2022,
	title = {{VIRDO}++: {Real}-{World}, {Visuo}-tactile {Dynamics} and {Perception} of {Deformable} {Objects}},
	shorttitle = {{VIRDO}++},
	url = {http://arxiv.org/abs/2210.03701},
	doi = {10.48550/arXiv.2210.03701},
	abstract = {Deformable objects manipulation can benefit from representations that seamlessly integrate vision and touch while handling occlusions. In this work, we present a novel approach for, and real-world demonstration of, multimodal visuo-tactile state-estimation and dynamics prediction for deformable objects. Our approach, VIRDO++, builds on recent progress in multimodal neural implicit representations for deformable object state-estimation [1] via a new formulation for deformation dynamics and a complementary state-estimation algorithm that (i) maintains a belief over deformations, and (ii) enables practical real-world application by removing the need for privileged contact information. In the context of two real-world robotic tasks, we show:(i) high-fidelity cross-modal state-estimation and prediction of deformable objects from partial visuo-tactile feedback, and (ii) generalization to unseen objects and contact formations.},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Wi, Youngsun and Zeng, Andy and Florence, Pete and Fazeli, Nima},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03701 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{quinlan_elastic_1993,
	title = {Elastic bands: connecting path planning and control},
	shorttitle = {Elastic bands},
	doi = {10.1109/ROBOT.1993.291936},
	abstract = {Elastic bands are proposed as the basis for a framework to close the gap between global path planning and real-time sensor-based robot control. An elastic band is a deformable collision-free path. The initial shape of the elastic is the free path generated by a planner. Subjected to artificial forces, the elastic band deforms in real time to a short and smooth path that maintains clearance from the obstacles. The elastic continues to deform as changes in the environment are detected by sensors, enabling the robot to accommodate uncertainties and react to unexpected and moving obstacles. While providing a tight connection between the robot and its environment, the elastic band preserves the global nature of the planned path. The framework is outlined, and an efficient implementation based on bubbles is discussed.{\textless}{\textgreater}},
	booktitle = {[1993] {Proceedings} {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Quinlan, S. and Khatib, O.},
	month = may,
	year = {1993},
	keywords = {Computer science, Control systems, Control theory, Joining processes, Laboratories, Path planning, Robot control, Robot sensing systems, Shape, Uncertainty},
	pages = {802--807 vol.2},
}

@inproceedings{gandhi_learning_2017,
	title = {Learning to fly by crashing},
	doi = {10.1109/IROS.2017.8206247},
	abstract = {How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see:.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Gandhi, Dhiraj and Pinto, Lerrel and Gupta, Abhinav},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Computer crashes, Data collection, Drones, Navigation, Trajectory},
	pages = {3948--3955},
}

@article{yamaguchi_recent_2019,
	title = {Recent progress in tactile sensing and sensors for robotic manipulation: can we turn tactile sensing into vision?},
	volume = {33},
	issn = {0169-1864},
	shorttitle = {Recent progress in tactile sensing and sensors for robotic manipulation},
	url = {https://doi.org/10.1080/01691864.2019.1632222},
	doi = {10.1080/01691864.2019.1632222},
	abstract = {This paper surveys recently published literature on tactile sensing in robotic manipulation to understand effective strategies for using tactile sensing and the issues involved in tactile sensing. It consists of a brief review of existing tactile sensors for robotic grippers and hands, review of modalities available from tactile sensing, review of the applications of tactile sensing in robotic manipulations, and discussion of the issues of tactile sensing and an approach to make tactile sensors more useful. We emphasize vision-based tactile sensing because of its potential to be a good tactile sensor for robots.},
	number = {14},
	urldate = {2023-01-31},
	journal = {Advanced Robotics},
	author = {Yamaguchi, Akihiko and Atkeson, Christopher G.},
	month = jul,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01691864.2019.1632222},
	keywords = {Tactile sensor, robotic hand, robotic manipulation, tactile behavior},
	pages = {661--673},
}

@inproceedings{zhang_self-supervised_2021,
	title = {Self-{Supervised} {Pretraining} of {3D} {Features} on {Any} {Point}-{Cloud}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Self-Supervised_Pretraining_of_3D_Features_on_Any_Point-Cloud_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-05},
	author = {Zhang, Zaiwei and Girdhar, Rohit and Joulin, Armand and Misra, Ishan},
	year = {2021},
	pages = {10252--10263},
}

@misc{zhou_open3d_2018,
	title = {{Open3D}: {A} {Modern} {Library} for {3D} {Data} {Processing}},
	shorttitle = {{Open3D}},
	url = {http://arxiv.org/abs/1801.09847},
	doi = {10.48550/arXiv.1801.09847},
	abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
	urldate = {2023-02-11},
	publisher = {arXiv},
	author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
	month = jan,
	year = {2018},
	note = {arXiv:1801.09847 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@inproceedings{nguyen_motion_2022,
	title = {Motion {Primitives}-based {Navigation} {Planning} using {Deep} {Collision} {Prediction}},
	doi = {10.1109/ICRA46639.2022.9812231},
	abstract = {This paper contributes a method to design a novel navigation planner exploiting a learning-based collision prediction network. The neural network is tasked to predict the collision cost of each action sequence in a predefined motion primitives library in the robot's velocity-steering angle space, given only the current depth image and the estimated linear and angular velocities of the robot. Furthermore, we account for the uncertainty of the robot's partial state by utilizing the Unscented Transform and the uncertainty of the neural network model by using Monte Carlo dropout. The uncertainty-aware collision cost is then combined with the goal direction given by a global planner in order to determine the best action sequence to execute in a receding horizon manner. To demonstrate the method, we develop a resilient small flying robot integrating lightweight sensing and computing resources. A set of simulation and experimental studies, including a field deployment, in both cluttered and perceptually-challenging environments is conducted to evaluate the quality of the prediction network and the performance of the proposed planner.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Nguyen, Huan and Fyhn, Sondre Holm and De Petris, Paolo and Alexis, Kostas},
	month = may,
	year = {2022},
	keywords = {Costs, Navigation, Neural networks, Predictive models, Robot sensing systems, Transforms, Uncertainty},
	pages = {9660--9667},
}

@inproceedings{wang_appli_2021,
	title = {{APPLI}: {Adaptive} {Planner} {Parameter} {Learning} {From} {Interventions}},
	shorttitle = {{APPLI}},
	doi = {10.1109/ICRA48506.2021.9561311},
	abstract = {While classical autonomous navigation systems can typically move robots from one point to another safely and in a collision-free manner, these systems may fail or produce suboptimal behavior in certain scenarios. The current practice in such scenarios is to manually re-tune the system’s parameters, e.g. max speed, sampling rate, inflation radius, to optimize performance. This practice requires expert knowledge and may jeopardize performance in the originally good scenarios. Meanwhile, it is relatively easy for a human to identify those failure or suboptimal cases and provide a teleoperated intervention to correct the failure or suboptimal behavior. In this work, we seek to learn from those human interventions to improve navigation performance. In particular, we propose Adaptive Planner Parameter Learning from Interventions (APPLI), in which multiple sets of navigation parameters are learned during training and applied based on a confidence measure to the underlying navigation system during deployment. In our physical experiments, the robot achieves better performance compared to the planner with static default parameters, and even dynamic parameters learned from a full human demonstration. We also show APPLI’s generalizability in another unseen physical test course, and a suite of 300 simulated navigation environments.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wang, Zizhao and Xiao, Xuesu and Liu, Bo and Warnell, Garrett and Stone, Peter},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Atmospheric measurements, Automation, Conferences, Machine learning, Navigation, Particle measurements, Training},
	pages = {6079--6085},
}

@inproceedings{simeonov_neural_2022,
	title = {Neural {Descriptor} {Fields}: {SE}(3)-{Equivariant} {Object} {Representations} for {Manipulation}},
	shorttitle = {Neural {Descriptor} {Fields}},
	doi = {10.1109/ICRA46639.2022.9812146},
	abstract = {We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (∼5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Simeonov, Anthony and Du, Yilun and Tagliasacchi, Andrea and Tenenbaum, Joshua B. and Rodriguez, Alberto and Agrawal, Pulkit and Sitzmann, Vincent},
	month = may,
	year = {2022},
	keywords = {Automation, Grippers, Optimization, Robots, Search problems, Task analysis, Three-dimensional displays},
	pages = {6394--6400},
}

@inproceedings{shah_ving_2021,
	title = {{ViNG}: {Learning} {Open}-{World} {Navigation} with {Visual} {Goals}},
	shorttitle = {{ViNG}},
	doi = {10.1109/ICRA48506.2021.9561936},
	abstract = {We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations 1.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Shah, Dhruv and Eysenbach, Benjamin and Kahn, Gregory and Rhinehart, Nicholas and Levine, Sergey},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Inspection, Navigation, Planning, Reinforcement learning, Training, Urban areas, Visualization},
	pages = {13215--13222},
}

@misc{ilg_uncertainty_2018,
	title = {Uncertainty {Estimates} and {Multi}-{Hypotheses} {Networks} for {Optical} {Flow}},
	url = {http://arxiv.org/abs/1802.07095},
	doi = {10.48550/arXiv.1802.07095},
	abstract = {Optical flow estimation can be formulated as an end-to-end supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoff compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the first time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical flow estimation. Moreover, we introduce a new network architecture utilizing the Winner-Takes-All loss and show that this can provide complementary hypotheses and uncertainty estimates efficiently with a single forward pass and without the need for sampling or ensembles. Finally, we demonstrate the quality of the different uncertainty estimates, which is clearly above previous confidence measures on optical flow and allows for interactive frame rates.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Ilg, Eddy and Çiçek, Özgün and Galesso, Silvio and Klein, Aaron and Makansi, Osama and Hutter, Frank and Brox, Thomas},
	month = dec,
	year = {2018},
	note = {arXiv:1802.07095 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{truong_pdc-net_2021,
	title = {{PDC}-{Net}+: {Enhanced} {Probabilistic} {Dense} {Correspondence} {Network}},
	shorttitle = {{PDC}-{Net}+},
	url = {http://arxiv.org/abs/2109.13912},
	doi = {10.48550/arXiv.2109.13912},
	abstract = {Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches. We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval. Code and models are available at https://github.com/PruneTruong/DenseMatching.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Truong, Prune and Danelljan, Martin and Timofte, Radu and Van Gool, Luc},
	month = sep,
	year = {2021},
	note = {arXiv:2109.13912 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kahn_badgr_2021,
	title = {{BADGR}: {An} {Autonomous} {Self}-{Supervised} {Learning}-{Based} {Navigation} {System}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {{BADGR}},
	doi = {10.1109/LRA.2021.3057023},
	abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our reinforcement learning approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with autonomously-labeled off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Big Data in robotics and automation, Collision avoidance, Data models, Mobile robots, Navigation, Predictive models, Robot sensing systems, Robots, autonomous agents, reinforcement learning},
	pages = {1312--1319},
}

@misc{codevilla_end--end_2018,
	title = {End-to-end {Driving} via {Conditional} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1710.02410},
	doi = {10.48550/arXiv.1710.02410},
	abstract = {Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands. The supplementary video can be viewed at https://youtu.be/cFtnflNe5fM},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Codevilla, Felipe and Müller, Matthias and López, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
	month = mar,
	year = {2018},
	note = {arXiv:1710.02410 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{pan_agile_2019,
	title = {Agile {Autonomous} {Driving} using {End}-to-{End} {Deep} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1709.07174},
	doi = {10.48550/arXiv.1709.07174},
	abstract = {We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
	month = aug,
	year = {2019},
	note = {arXiv:1709.07174 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{ho_generative_2016,
	title = {Generative {Adversarial} {Imitation} {Learning}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html},
	abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.},
	urldate = {2023-02-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Ermon, Stefano},
	year = {2016},
}

@inproceedings{kendall_learning_2019,
	title = {Learning to {Drive} in a {Day}},
	doi = {10.1109/ICRA.2019.8793742},
	abstract = {We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Autonomous vehicles, Global Positioning System, Markov processes, Reinforcement learning, Sensors, Task analysis, Training},
	pages = {8248--8254},
}

@misc{epic_games_unreal_2019,
	title = {Unreal {Engine}},
	url = {https://www.unrealengine.com},
	author = {{Epic Games}},
	month = apr,
	year = {2019},
}

@article{rosmann_integrated_2017,
	title = {Integrated online trajectory planning and optimization in distinctive topologies},
	volume = {88},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889016300495},
	doi = {10.1016/j.robot.2016.11.007},
	abstract = {This paper presents a novel integrated approach for efficient optimization based online trajectory planning of topologically distinctive mobile robot trajectories. Online trajectory optimization deforms an initial coarse path generated by a global planner by minimizing objectives such as path length, transition time or control effort. Kinodynamic motion properties of mobile robots and clearance from obstacles impose additional equality and inequality constraints on the trajectory optimization. Local planners account for efficiency by restricting the search space to locally optimal solutions only. However, the objective function is usually non-convex as the presence of obstacles generates multiple distinctive local optima. The proposed method maintains and simultaneously optimizes a subset of admissible candidate trajectories of distinctive topologies and thus seeking the overall best candidate among the set of alternative local solutions. Time-optimal trajectories for differential-drive and carlike robots are obtained efficiently by adopting the Timed-Elastic-Band approach for the underlying trajectory optimization problem. The investigation of various example scenarios and a comparative analysis with conventional local planners confirm the advantages of integrated exploration, maintenance and optimization of topologically distinctive trajectories.},
	language = {en},
	urldate = {2023-01-26},
	journal = {Robotics and Autonomous Systems},
	author = {Rösmann, Christoph and Hoffmann, Frank and Bertram, Torsten},
	month = feb,
	year = {2017},
	keywords = {Distinctive topologies, Homology classes, Mobile robot motion planning, Model predictive control, Online trajectory optimization, Timed-Elastic-Band},
	pages = {142--153},
}

@inproceedings{gupta_cognitive_2017,
	title = {Cognitive {Mapping} and {Planning} for {Visual} {Navigation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Gupta_Cognitive_Mapping_and_CVPR_2017_paper.html},
	urldate = {2023-01-24},
	author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
	year = {2017},
	pages = {2616--2625},
}

@misc{sen_scarp_2023,
	title = {{SCARP}: {3D} {Shape} {Completion} in {ARbitrary} {Poses} for {Improved} {Grasping}},
	shorttitle = {{SCARP}},
	url = {http://arxiv.org/abs/2301.07213},
	doi = {10.48550/arXiv.2301.07213},
	abstract = {Recovering full 3D shapes from partial observations is a challenging task that has been extensively addressed in the computer vision community. Many deep learning methods tackle this problem by training 3D shape generation networks to learn a prior over the full 3D shapes. In this training regime, the methods expect the inputs to be in a fixed canonical form, without which they fail to learn a valid prior over the 3D shapes. We propose SCARP, a model that performs Shape Completion in ARbitrary Poses. Given a partial pointcloud of an object, SCARP learns a disentangled feature representation of pose and shape by relying on rotationally equivariant pose features and geometric shape features trained using a multi-tasking objective. Unlike existing methods that depend on an external canonicalization, SCARP performs canonicalization, pose estimation, and shape completion in a single network, improving the performance by 45\% over the existing baselines. In this work, we use SCARP for improving grasp proposals on tabletop objects. By completing partial tabletop objects directly in their observed poses, SCARP enables a SOTA grasp proposal network improve their proposals by 71.2\% on partial shapes. Project page: https://bipashasen.github.io/scarp},
	urldate = {2023-01-21},
	publisher = {arXiv},
	author = {Sen, Bipasha and Agarwal, Aditya and Singh, Gaurav and B., Brojeshwar and Sridhar, Srinath and Krishna, Madhava},
	month = jan,
	year = {2023},
	note = {arXiv:2301.07213 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{lu_slam-supported_2022,
	title = {{SLAM}-{Supported} {Self}-{Training} for {6D} {Object} {Pose} {Estimation}},
	doi = {10.1109/IROS47612.2022.9981145},
	abstract = {Recent progress in object pose prediction provides a promising path for robots to build object-level scene representations during navigation. However, as we deploy a robot in novel environments, the out-of-distribution data can degrade the prediction performance. To mitigate the domain gap, we can potentially perform self-training in the target domain, using predictions on robot-captured images as pseudo labels to fine-tune the object pose estimator. Unfortunately, the pose predictions are typically outlier-corrupted, and it is hard to quantify their uncertainties, which can result in low-quality pseudo-labeled data. To address the problem, we propose a SLAM-supported self-training method, leveraging robot understanding of the 3D scene geometry to enhance the object pose inference performance. Combining the pose predictions with robot odometry, we formulate and solve pose graph optimization to refine the object pose estimates and make pseudo labels more consistent across frames. We incorporate the pose prediction covariances as variables into the optimization to automatically model their uncertainties. This automatic covariance tuning (ACT) process can fit 6D pose prediction noise at the component level, leading to higher-quality pseudo training data. We test our method with the deep object pose estimator (DOPE) on the YCB video dataset and in real robot experiments. It achieves respectively 34.3\% and 17.8\% accuracy enhancements in pose prediction on the two tests. Our code is available at https://github.com/520xyxyzq/slam-super-6d.},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Lu, Ziqi and Zhang, Yihao and Doherty, Kevin and Severinsen, Odin and Yang, Ethan and Leonard, John},
	month = oct,
	year = {2022},
	note = {ISSN: 2153-0866},
	keywords = {Pose estimation, Predictive models, Robots, Simultaneous localization and mapping, Three-dimensional displays, Training data, Uncertainty},
	pages = {2833--2840},
}

@article{badrinarayanan_segnet_2017,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	volume = {39},
	issn = {1939-3539},
	shorttitle = {{SegNet}},
	doi = {10.1109/TPAMI.2016.2644615},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computer architecture, Convolutional codes, Decoding, Deep convolutional neural networks, Image segmentation, Neural networks, Semantics, Training, decoder, encoder, indoor scenes, pooling, road scenes, semantic pixel-wise segmentation, upsampling},
	pages = {2481--2495},
}

@inproceedings{wang_3d_2018,
	title = {{3D} {Shape} {Perception} from {Monocular} {Vision}, {Touch}, and {Shape} {Priors}},
	doi = {10.1109/IROS.2018.8593430},
	abstract = {Perceiving accurate 3D object shape is important for robots to interact with the physical world. Current research along this direction has been primarily relying on visual observations. Vision, however useful, has inherent limitations due to occlusions and the 2D-3D ambiguities, especially for perception with a monocular camera. In contrast, touch gets precise local shape information, though its efficiency for reconstructing the entire shape could be low. In this paper, we propose a novel paradigm that efficiently perceives accurate 3D object shape by incorporating visual and tactile observations, as well as prior knowledge of common object shapes learned from large-scale shape repositories. We use vision first, applying neural networks with learned shape priors to predict an object's 3D shape from a single-view color image. We then use tactile sensing to refine the shape; the robot actively touches the object regions where the visual prediction has high uncertainty. Our method efficiently builds the 3D shape of common objects from a color image and a small number of tactile explorations (around 10). Our setup is easy to apply and has potentials to help robots better perform grasping or manipulation tasks on real-world objects.},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wang, Shaoxiong and Wu, Jiajun and Sun, Xingyuan and Yuan, Wenzhen and Freeman, William T. and Tenenbaum, Joshua B. and Adelson, Edward H.},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {Image reconstruction, Robot sensing systems, Shape, Surface reconstruction, Three-dimensional displays},
	pages = {1606--1613},
}

@inproceedings{varley_shape_2017,
	title = {Shape completion enabled robotic grasping},
	doi = {10.1109/IROS.2017.8206060},
	abstract = {This work provides an architecture to enable robotic grasp planning via shape completion. Shape completion is accomplished through the use of a 3D convolutional neural network (CNN). The network is trained on our own new open source dataset of over 440,000 3D exemplars captured from varying viewpoints. At runtime, a 2.5D pointcloud captured from a single point of view is fed into the CNN, which fills in the occluded regions of the scene, allowing grasps to be planned and executed on the completed object. Runtime shape completion is very rapid because most of the computational costs of shape completion are borne during offline training. We explore how the quality of completions vary based on several factors. These include whether or not the object being completed existed in the training data and how many object models were used to train the network. We also look at the ability of the network to generalize to novel objects allowing the system to complete previously unseen objects at runtime. Finally, experimentation is done both in simulation and on actual robotic hardware to explore the relationship between completion quality and the utility of the completed mesh model for grasping.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Varley, Jacob and DeChant, Chad and Richardson, Adam and Ruales, Joaquín and Allen, Peter},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Databases, Planning, Robots, Runtime, Shape, Three-dimensional displays, Training},
	pages = {2442--2447},
}

@inproceedings{wu_multimodal_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multimodal {Shape} {Completion} via {Conditional} {Generative} {Adversarial} {Networks}},
	isbn = {978-3-030-58548-8},
	doi = {10.1007/978-3-030-58548-8_17},
	abstract = {Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wu, Rundi and Chen, Xuelin and Zhuang, Yixin and Chen, Baoquan},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Conditional generative adversarial network, Multimodal mapping, Shape completion},
	pages = {281--296},
}

@inproceedings{wang_gdr-net_2021,
	title = {{GDR}-{Net}: {Geometry}-{Guided} {Direct} {Regression} {Network} for {Monocular} {6D} {Object} {Pose} {Estimation}},
	shorttitle = {{GDR}-{Net}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-17},
	author = {Wang, Gu and Manhardt, Fabian and Tombari, Federico and Ji, Xiangyang},
	year = {2021},
	pages = {16611--16621},
}

@inproceedings{wang_normalized_2019,
	title = {Normalized {Object} {Coordinate} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.html},
	urldate = {2023-01-17},
	author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
	year = {2019},
	pages = {2642--2651},
}

@inproceedings{li_deepim_2018,
	title = {{DeepIM}: {Deep} {Iterative} {Matching} for {6D} {Pose} {Estimation}},
	shorttitle = {{DeepIM}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yi_Li_DeepIM_Deep_Iterative_ECCV_2018_paper.html},
	urldate = {2022-10-13},
	author = {Li, Yi and Wang, Gu and Ji, Xiangyang and Xiang, Yu and Fox, Dieter},
	year = {2018},
	pages = {683--698},
}

@inproceedings{watkins-valls_multi-modal_2019,
	title = {Multi-{Modal} {Geometric} {Learning} for {Grasping} and {Manipulation}},
	doi = {10.1109/ICRA.2019.8794233},
	abstract = {This work provides an architecture that incorporates depth and tactile information to create rich and accurate 3D models useful for robotic manipulation tasks. This is accomplished through the use of a 3D convolutional neural network (CNN). Offline, the network is provided with both depth and tactile information and trained to predict the object's geometry, thus filling in regions of occlusion. At runtime, the network is provided a partial view of an object. Tactile information is acquired to augment the captured depth information. The network can then reason about the object's geometry by utilizing both the collected tactile and depth information. We demonstrate that even small amounts of additional tactile information can be incredibly helpful in reasoning about object geometry. This is particularly true when information from depth alone fails to produce an accurate geometric prediction. Our method is benchmarked against and outperforms other visual-tactile approaches to general geometric reasoning. We also provide experimental results comparing grasping success with our method.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Watkins-Valls, David and Varley, Jacob and Allen, Peter},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Geometry, Grasping, Robot sensing systems, Shape, Three-dimensional displays, Training},
	pages = {7339--7345},
}

@inproceedings{villalonga_tactile_2021,
	title = {Tactile {Object} {Pose} {Estimation} from the {First} {Touch} with {Geometric} {Contact} {Rendering}},
	url = {https://proceedings.mlr.press/v155/villalonga21a.html},
	abstract = {In this paper, we present an approach to tactile pose estimation from the first touch for known objects. First, we create an object-agnostic map from real tactile observations to contact shapes. Next, for a new object with known geometry, we learn a tailored perception model completely in simulation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor output, we match it against the pre-computed set using the object-specific embedding learned purely in simulation using contrastive learning. This results in a perception model that can localize objects from a single tactile observation. It also allows reasoning over pose distributions and including additional pose constraints coming from other perception systems or multiple contacts. We provide quantitative results for four objects. Our approach provides high accuracy pose estimations from distinctive tactile observations while regressing pose distributions to account for those contact shapes that could result from different object poses. We further extend and test our approach in multi-contact scenarios where several tactile sensors are simultaneously in contact with the object.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of the 2020 {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Villalonga, Maria Bauza and Rodriguez, Alberto and Lim, Bryan and Valls, Eric and Sechopoulos, Theo},
	month = oct,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1015--1029},
}

@inproceedings{hu_segmentation-driven_2019,
	title = {Segmentation-{Driven} {6D} {Object} {Pose} {Estimation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Segmentation-Driven_6D_Object_Pose_Estimation_CVPR_2019_paper.html},
	urldate = {2022-10-13},
	author = {Hu, Yinlin and Hugonot, Joachim and Fua, Pascal and Salzmann, Mathieu},
	year = {2019},
	pages = {3385--3394},
}

@inproceedings{chen_epro-pnp_2022,
	title = {{EPro}-{PnP}: {Generalized} {End}-to-{End} {Probabilistic} {Perspective}-{N}-{Points} for {Monocular} {Object} {Pose} {Estimation}},
	shorttitle = {{EPro}-{PnP}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-13},
	author = {Chen, Hansheng and Wang, Pichao and Wang, Fan and Tian, Wei and Xiong, Lu and Li, Hao},
	year = {2022},
	pages = {2781--2790},
}

@inproceedings{wang_densefusion_2019,
	title = {{DenseFusion}: {6D} {Object} {Pose} {Estimation} by {Iterative} {Dense} {Fusion}},
	shorttitle = {{DenseFusion}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.html},
	urldate = {2022-09-29},
	author = {Wang, Chen and Xu, Danfei and Zhu, Yuke and Martin-Martin, Roberto and Lu, Cewu and Fei-Fei, Li and Savarese, Silvio},
	year = {2019},
	pages = {3343--3352},
}

@inproceedings{chen_system_2022,
	title = {A {System} for {General} {In}-{Hand} {Object} {Re}-{Orientation}},
	url = {https://proceedings.mlr.press/v164/chen22a.html},
	abstract = {In-hand object reorientation has been a challenging problem in robotics due to high dimensional actuation space and the frequent change in contact state between the fingers and the objects. We present a simple model-free framework that can learn to reorient objects with both the hand facing upwards and downwards. We demonstrate the capability of reorienting over 200020002000 geometrically different objects in both cases. The learned policies show strong zero-shot transfer performance on new objects. We provide evidence that these policies are amenable to real-world operation by distilling them to use observations easily available in the real world. The videos of the learned policies are available at: https://taochenshh.github.io/projects/in-hand-reorientation.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Chen, Tao and Xu, Jie and Agrawal, Pulkit},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {297--307},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2022-11-14},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@misc{he_continuous_2022,
	title = {Continuous {Neural} {Algorithmic} {Planners}},
	url = {http://arxiv.org/abs/2211.15839},
	doi = {10.48550/arXiv.2211.15839},
	abstract = {Neural algorithmic reasoning studies the problem of learning algorithms with neural networks, especially with graph architectures. A recent proposal, XLVIN, reaps the benefits of using a graph neural network that simulates the value iteration algorithm in deep reinforcement learning agents. It allows model-free planning without access to privileged information about the environment, which is usually unavailable. However, XLVIN only supports discrete action spaces, and is hence nontrivially applicable to most tasks of real-world interest. We expand XLVIN to continuous action spaces by discretization, and evaluate several selective expansion policies to deal with the large planning graphs. Our proposal, CNAP, demonstrates how neural algorithmic reasoning can make a measurable impact in higher-dimensional continuous control settings, such as MuJoCo, bringing gains in low-data settings and outperforming model-free baselines.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {He, Yu and Veličković, Petar and Liò, Pietro and Deac, Andreea},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15839 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{deac_xlvin_2020,
	title = {{XLVIN}: {eXecuted} {Latent} {Value} {Iteration} {Nets}},
	shorttitle = {{XLVIN}},
	url = {http://arxiv.org/abs/2010.13146},
	doi = {10.48550/arXiv.2010.13146},
	abstract = {Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Deac, Andreea and Veličković, Petar and Milinković, Ognjen and Bacon, Pierre-Luc and Tang, Jian and Nikolić, Mladen},
	month = dec,
	year = {2020},
	note = {arXiv:2010.13146 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ross_reduction_2011,
	title = {A {Reduction} of {Imitation} {Learning} and {Structured} {Prediction} to {No}-{Regret} {Online} {Learning}},
	url = {https://proceedings.mlr.press/v15/ross11a.html},
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {627--635},
}

@inproceedings{schleich_value_2019,
	title = {Value {Iteration} {Networks} on {Multiple} {Levels} of {Abstraction}},
	url = {http://arxiv.org/abs/1905.11068},
	doi = {10.15607/RSS.2019.XV.014},
	abstract = {Learning-based methods are promising to plan robot motion without performing extensive search, which is needed by many non-learning approaches. Recently, Value Iteration Networks (VINs) received much interest since---in contrast to standard CNN-based architectures---they learn goal-directed behaviors which generalize well to unseen domains. However, VINs are restricted to small and low-dimensional domains, limiting their applicability to real-world planning problems. To address this issue, we propose to extend VINs to representations with multiple levels of abstraction. While the vicinity of the robot is represented in sufficient detail, the representation gets spatially coarser with increasing distance from the robot. The information loss caused by the decreasing resolution is compensated by increasing the number of features representing a cell. We show that our approach is capable of solving significantly larger 2D grid world planning tasks than the original VIN implementation. In contrast to a multiresolution coarse-to-fine VIN implementation which does not employ additional descriptive features, our approach is capable of solving challenging environments, which demonstrates that the proposed method learns to encode useful information in the additional features. As an application for solving real-world planning tasks, we successfully employ our method to plan omnidirectional driving for a search-and-rescue robot in cluttered terrain.},
	urldate = {2023-01-15},
	booktitle = {Robotics: {Science} and {Systems} {XV}},
	author = {Schleich, Daniel and Klamt, Tobias and Behnke, Sven},
	month = jun,
	year = {2019},
	note = {arXiv:1905.11068 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{niu_generalized_2017,
	title = {Generalized {Value} {Iteration} {Networks}: {Life} {Beyond} {Lattices}},
	shorttitle = {Generalized {Value} {Iteration} {Networks}},
	url = {http://arxiv.org/abs/1706.02416},
	doi = {10.48550/arXiv.1706.02416},
	abstract = {In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value iteration algorithm by using a novel graph convolution operator, which enables GVIN to learn and plan on irregular spatial graphs. We propose three novel differentiable kernels as graph convolution operators and show that the embedding based kernel achieves the best performance. We further propose episodic Q-learning, an improvement upon traditional n-step Q-learning that stabilizes training for networks that contain a planning module. Lastly, we evaluate GVIN on planning problems in 2D mazes, irregular graphs, and real-world street networks, showing that GVIN generalizes well for both arbitrary graphs and unseen graphs of larger scale and outperforms a naive generalization of VIN (discretizing a spatial graph into a 2D image).},
	urldate = {2023-01-15},
	publisher = {arXiv},
	author = {Niu, Sufeng and Chen, Siheng and Guo, Hanyu and Targonski, Colin and Smith, Melissa C. and Kovačević, Jelena},
	month = oct,
	year = {2017},
	note = {arXiv:1706.02416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{ishida_towards_2022,
	title = {Towards {Real}-{World} {Navigation} {With} {Deep} {Differentiable} {Planners}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ishida_Towards_Real-World_Navigation_With_Deep_Differentiable_Planners_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-01-14},
	author = {Ishida, Shu and Henriques, João F.},
	year = {2022},
	pages = {17327--17336},
}

@misc{gao_intention-net_2017,
	title = {Intention-{Net}: {Integrating} {Planning} and {Deep} {Learning} for {Goal}-{Directed} {Autonomous} {Navigation}},
	shorttitle = {Intention-{Net}},
	url = {http://arxiv.org/abs/1710.05627},
	doi = {10.48550/arXiv.1710.05627},
	abstract = {How can a delivery robot navigate reliably to a destination in a new office building, with minimal prior information? To tackle this challenge, this paper introduces a two-level hierarchical approach, which integrates model-free deep learning and model-based path planning. At the low level, a neural-network motion controller, called the intention-net, is trained end-to-end to provide robust local navigation. The intention-net maps images from a single monocular camera and "intentions" directly to robot controls. At the high level, a path planner uses a crude map, e.g., a 2-D floor plan, to compute a path from the robot's current location to the goal. The planned path provides intentions to the intention-net. Preliminary experiments suggest that the learned motion controller is robust against perceptual uncertainty and by integrating with a path planner, it generalizes effectively to new environments and goals.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Gao, Wei and Hsu, David and Lee, Wee Sun and Shen, Shengmei and Subramanian, Karthikk},
	month = oct,
	year = {2017},
	note = {arXiv:1710.05627 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{tai_virtual--real_2017,
	title = {Virtual-to-real {Deep} {Reinforcement} {Learning}: {Continuous} {Control} of {Mobile} {Robots} for {Mapless} {Navigation}},
	shorttitle = {Virtual-to-real {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.00420},
	doi = {10.48550/arXiv.1703.00420},
	abstract = {We present a learning-based mapless motion planner by taking the sparse 10-dimensional range findings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Tai, Lei and Paolo, Giuseppe and Liu, Ming},
	month = jul,
	year = {2017},
	note = {arXiv:1703.00420 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{zhu_target-driven_2016,
	title = {Target-driven {Visual} {Navigation} in {Indoor} {Scenes} using {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1609.05143},
	doi = {10.48550/arXiv.1609.05143},
	abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose AI2-THOR framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment. The supplementary video can be accessed at the following link: https://youtu.be/SmBxMDiOrvs.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
	month = sep,
	year = {2016},
	note = {arXiv:1609.05143 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{amos_differentiable_2018,
	title = {Differentiable {MPC} for {End}-to-end {Planning} and {Control}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/ba6d843eb4251a4526ce65d1807a9309-Abstract.html},
	abstract = {We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.},
	urldate = {2023-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
	year = {2018},
}

@misc{zhou_robust_2019,
	title = {Robust and {Efficient} {Quadrotor} {Trajectory} {Generation} for {Fast} {Autonomous} {Flight}},
	url = {http://arxiv.org/abs/1907.01531},
	doi = {10.48550/arXiv.1907.01531},
	abstract = {In this paper, we propose a robust and efficient quadrotor motion planning system for fast flight in 3-D complex environments. We adopt a kinodynamic path searching method to find a safe, kinodynamic feasible and minimum-time initial trajectory in the discretized control space. We improve the smoothness and clearance of the trajectory by a B-spline optimization, which incorporates gradient information from a Euclidean distance field (EDF) and dynamic constraints efficiently utilizing the convex hull property of B-spline. Finally, by representing the final trajectory as a non-uniform B-spline, an iterative time adjustment method is adopted to guarantee dynamically feasible and non-conservative trajectories. We validate our proposed method in various complex simulational environments. The competence of the method is also validated in challenging real-world tasks. We release our code as an open-source package.},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Zhou, Boyu and Gao, Fei and Wang, Luqi and Liu, Chuhao and Shen, Shaojie},
	month = jul,
	year = {2019},
	note = {arXiv:1907.01531 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{bewley_learning_2018,
	title = {Learning to {Drive} from {Simulation} without {Real} {World} {Labels}},
	url = {http://arxiv.org/abs/1812.03823},
	doi = {10.48550/arXiv.1812.03823},
	abstract = {Simulation can be a powerful tool for understanding machine learning systems and designing methods to solve real-world problems. Training and evaluating methods purely in simulation is often "doomed to succeed" at the desired task in a simulated environment, but the resulting models are incapable of operation in the real world. Here we present and evaluate a method for transferring a vision-based lane following driving policy from simulation to operation on a rural road without any real-world labels. Our approach leverages recent advances in image-to-image translation to achieve domain transfer while jointly learning a single-camera control policy from simulation control labels. We assess the driving performance of this method using both open-loop regression metrics, and closed-loop performance operating an autonomous vehicle on rural and urban roads.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Bewley, Alex and Rigley, Jessica and Liu, Yuxuan and Hawke, Jeffrey and Shen, Richard and Lam, Vinh-Dieu and Kendall, Alex},
	month = dec,
	year = {2018},
	note = {arXiv:1812.03823 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_raptor_2020,
	title = {{RAPTOR}: {Robust} and {Perception}-aware {Trajectory} {Replanning} for {Quadrotor} {Fast} {Flight}},
	shorttitle = {{RAPTOR}},
	url = {http://arxiv.org/abs/2007.03465},
	doi = {10.48550/arXiv.2007.03465},
	abstract = {Recent advances in trajectory replanning have enabled quadrotor to navigate autonomously in unknown environments. However, high-speed navigation still remains a significant challenge. Given very limited time, existing methods have no strong guarantee on the feasibility or quality of the solutions. Moreover, most methods do not consider environment perception, which is the key bottleneck to fast flight. In this paper, we present RAPTOR, a robust and perception-aware replanning framework to support fast and safe flight. A path-guided optimization (PGO) approach that incorporates multiple topological paths is devised, to ensure finding feasible and high-quality trajectories in very limited time. We also introduce a perception-aware planning strategy to actively observe and avoid unknown obstacles. A risk-aware trajectory refinement ensures that unknown obstacles which may endanger the quadrotor can be observed earlier and avoid in time. The motion of yaw angle is planned to actively explore the surrounding space that is relevant for safe navigation. The proposed methods are tested extensively. We will release our implementation as an open-source package for the community.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {Zhou, Boyu and Pan, Jie and Gao, Fei and Shen, Shaojie},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03465 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{sohn_fixmatch_2020,
	title = {{FixMatch}: {Simplifying} {Semi}-{Supervised} {Learning} with {Consistency} and {Confidence}},
	shorttitle = {{FixMatch}},
	url = {http://arxiv.org/abs/2001.07685},
	doi = {10.48550/arXiv.2001.07685},
	abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
	month = nov,
	year = {2020},
	note = {arXiv:2001.07685 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{karnan_socially_2022,
	title = {Socially {Compliant} {Navigation} {Dataset} ({SCAND}): {A} {Large}-{Scale} {Dataset} of {Demonstrations} for {Social} {Navigation}},
	shorttitle = {Socially {Compliant} {Navigation} {Dataset} ({SCAND})},
	url = {http://arxiv.org/abs/2203.15041},
	doi = {10.48550/arXiv.2203.15041},
	abstract = {Social navigation is the capability of an autonomous agent, such as a robot, to navigate in a 'socially compliant' manner in the presence of other intelligent agents such as humans. With the emergence of autonomously navigating mobile robots in human populated environments (e.g., domestic service robots in homes and restaurants and food delivery robots on public sidewalks), incorporating socially compliant navigation behaviors on these robots becomes critical to ensuring safe and comfortable human robot coexistence. To address this challenge, imitation learning is a promising framework, since it is easier for humans to demonstrate the task of social navigation rather than to formulate reward functions that accurately capture the complex multi objective setting of social navigation. The use of imitation learning and inverse reinforcement learning to social navigation for mobile robots, however, is currently hindered by a lack of large scale datasets that capture socially compliant robot navigation demonstrations in the wild. To fill this gap, we introduce Socially CompliAnt Navigation Dataset (SCAND) a large scale, first person view dataset of socially compliant navigation demonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of socially compliant, human teleoperated driving demonstrations that comprises multi modal data streams including 3D lidar, joystick commands, odometry, visual and inertial information, collected on two morphologically different mobile robots a Boston Dynamics Spot and a Clearpath Jackal by four different human demonstrators in both indoor and outdoor environments. We additionally perform preliminary analysis and validation through real world robot experiments and show that navigation policies learned by imitation learning on SCAND generate socially compliant behaviors},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Karnan, Haresh and Nair, Anirudh and Xiao, Xuesu and Warnell, Garrett and Pirk, Soeren and Toshev, Alexander and Hart, Justin and Biswas, Joydeep and Stone, Peter},
	month = jun,
	year = {2022},
	note = {arXiv:2203.15041 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{bansal_combining_2020,
	title = {Combining {Optimal} {Control} and {Learning} for {Visual} {Navigation} in {Novel} {Environments}},
	url = {https://proceedings.mlr.press/v100/bansal20a.html},
	abstract = {Model-based control is a popular paradigm for robot navigation because it can leverage a known dynamics model to efficiently plan robust robot trajectories. However, it is challenging to use model-based methods in settings where the environment is a priori unknown and can only be observed partially through onboard sensors on the robot. In this work, we address this short-coming by coupling model-based control with learning-based perception. The learning-based perception module produces a series of waypoints that guide the robot to the goal via a collision-free path. These waypoints are used by a model-based planner to generate a smooth and dynamically feasible trajectory that is executed on the physical system using feedback control. Our experiments in simulated real-world cluttered environments and on an actual ground vehicle demonstrate that the proposed approach can reach goal locations more reliably and efficiently in novel environments as compared to purely geometric mapping-based or end-to-end learning-based alternatives. Our approach does not rely on detailed explicit 3D maps of the environment, works well with low frame rates, and generalizes well from simulation to the real world. Videos describing our approach and experiments are available on the project website4.},
	language = {en},
	urldate = {2023-01-12},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Bansal, Somil and Tolani, Varun and Gupta, Saurabh and Malik, Jitendra and Tomlin, Claire},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {420--429},
}

@inproceedings{tobin_domain_2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	doi = {10.1109/IROS.2017.8202133},
	abstract = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Adaptation models, Cameras, Data models, Robots, Solid modeling, Three-dimensional displays, Training},
	pages = {23--30},
}

@inproceedings{chen_big_2020,
	title = {Big {Self}-{Supervised} {Models} are {Strong} {Semi}-{Supervised} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html},
	urldate = {2023-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
	year = {2020},
	pages = {22243--22255},
}

@inproceedings{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-09-10},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	year = {2021},
	pages = {9650--9660},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xu_adagrasp_2021,
	title = {{AdaGrasp}: {Learning} an {Adaptive} {Gripper}-{Aware} {Grasping} {Policy}},
	shorttitle = {{AdaGrasp}},
	url = {http://arxiv.org/abs/2011.14206},
	doi = {10.48550/arXiv.2011.14206},
	abstract = {This paper aims to improve robots' versatility and adaptability by allowing them to use a large variety of end-effector tools and quickly adapt to new tools. We propose AdaGrasp, a method to learn a single grasping policy that generalizes to novel grippers. By training on a large collection of grippers, our algorithm is able to acquire generalizable knowledge of how different grippers should be used in various tasks. Given a visual observation of the scene and the gripper, AdaGrasp infers the possible grasp poses and their grasp scores by computing the cross convolution between the shape encodings of the gripper and scene. Intuitively, this cross convolution operation can be considered as an efficient way of exhaustively matching the scene geometry with gripper geometry under different grasp poses (i.e., translations and orientations), where a good "match" of 3D geometry will lead to a successful grasp. We validate our methods in both simulation and real-world environments. Our experiment shows that AdaGrasp significantly outperforms the existing multi-gripper grasping policy method, especially when handling cluttered environments and partial observations. Video is available at https://youtu.be/kknTYTbORfs},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Xu, Zhenjia and Qi, Beichun and Agrawal, Shubham and Song, Shuran},
	month = mar,
	year = {2021},
	note = {arXiv:2011.14206 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{sun_efficient_2022,
	title = {Efficient {Spatial}-{Temporal} {Information} {Fusion} for {LiDAR}-{Based} {3D} {Moving} {Object} {Segmentation}},
	url = {http://arxiv.org/abs/2207.02201},
	doi = {10.48550/arXiv.2207.02201},
	abstract = {Accurate moving object segmentation is an essential task for autonomous driving. It can provide effective information for many downstream tasks, such as collision avoidance, path planning, and static map construction. How to effectively exploit the spatial-temporal information is a critical question for 3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a novel deep neural network exploiting both spatial-temporal information and different representation modalities of LiDAR scans to improve LiDAR-MOS performance. Specifically, we first use a range image-based dual-branch structure to separately deal with spatial and temporal information that can be obtained from sequential LiDAR scans, and later combine them using motion-guided attention modules. We also use a point refinement module via 3D sparse convolution to fuse the information from both LiDAR range image and point cloud representations and reduce the artifacts on the borders of the objects. We verify the effectiveness of our proposed approach on the LiDAR-MOS benchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods significantly in terms of LiDAR-MOS IoU. Benefiting from the devised coarse-to-fine architecture, our method operates online at sensor frame rate. The implementation of our method is available as open source at: https://github.com/haomo-ai/MotionSeg3D.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Sun, Jiadai and Dai, Yuchao and Zhang, Xianjing and Xu, Jintao and Ai, Rui and Gu, Weihao and Chen, Xieyuanli},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02201 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{kim_rvmos_2022,
	title = {{RVMOS}: {Range}-{View} {Moving} {Object} {Segmentation} {Leveraged} by {Semantic} and {Motion} {Features}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {{RVMOS}},
	doi = {10.1109/LRA.2022.3186080},
	abstract = {Detecting traffic participants is an essential and age-old problem in autonomous driving. Recently, the recognition of moving objects has emerged as a major issue in this field for safe driving. In this paper, we present RVMOS, a LiDAR Range-View-based Moving Object Segmentation framework that segments moving objects given a sequence of range-view images. In contrast to the conventional method, our network incorporates both motion and semantic features, each of which encodes the motion of objects and the surrounding circumstance of the objects. In addition, we design a new feature extraction module suitably designed for range-view images. Lastly, we introduce simple yet effective data augmentation methods: time interval modulation and zero residual image synthesis. With these contributions, we achieve a 19\% higher performance (mIoU) with 10\% faster computational time (34 FPS on RTX 3090) than the state-of-the-art method with the SemanticKitti benchmark. Extensive experiments demonstrate the effectiveness of our network design and data augmentation scheme.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Kim, Jaeyeul and Woo, Jungwan and Im, Sunghoon},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Autonomous driving, Image segmentation, Laser radar, LiDAR, Motion segmentation, Object segmentation, Point cloud compression, Semantics, Three-dimensional displays, moving object segmentation, perception, range-view},
	pages = {8044--8051},
}

@inproceedings{peri_forecasting_2022,
	title = {Forecasting {From} {LiDAR} via {Future} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Peri_Forecasting_From_LiDAR_via_Future_Object_Detection_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-01-10},
	author = {Peri, Neehar and Luiten, Jonathon and Li, Mengtian and Ošep, Aljoša and Leal-Taixé, Laura and Ramanan, Deva},
	year = {2022},
	pages = {17202--17211},
}

@misc{matak_planning_2022,
	title = {Planning {Visual}-{Tactile} {Precision} {Grasps} via {Complementary} {Use} of {Vision} and {Touch}},
	url = {http://arxiv.org/abs/2212.08604},
	doi = {10.48550/arXiv.2212.08604},
	abstract = {Reliably planning fingertip grasps for multi-fingered hands lies as a key challenge for many tasks including tool use, insertion, and dexterous in-hand manipulation. This task becomes even more difficult when the robot lacks an accurate model of the object to be grasped. Tactile sensing offers a promising approach to account for uncertainties in object shape. However, current robotic hands tend to lack full tactile coverage. As such, a problem arises of how to plan and execute grasps for multi-fingered hands such that contact is made with the area covered by the tactile sensors. To address this issue, we propose an approach to grasp planning that explicitly reasons about where the fingertips should contact the estimated object surface while maximizing the probability of grasp success. Key to our method's success is the use of visual surface estimation for initial planning to encode the contact constraint. The robot then executes this plan using a tactile-feedback controller that enables the robot to adapt to online estimates of the object's surface to correct for errors in the initial plan. Importantly, the robot never explicitly integrates object pose or surface estimates between visual and tactile sensing, instead it uses the two modalities in complementary ways. Vision guides the robots motion prior to contact; touch updates the plan when contact occurs differently than predicted from vision. We show that our method successfully synthesises and executes precision grasps for previously unseen objects using surface estimates from a single camera view. Further, our approach outperforms a state of the art multi-fingered grasp planner, while also beating several baselines we propose.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Matak, Martin and Hermans, Tucker},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08604 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{liang_sscnav_2021,
	title = {{SSCNav}: {Confidence}-{Aware} {Semantic} {Scene} {Completion} for {Visual} {Semantic} {Navigation}},
	shorttitle = {{SSCNav}},
	doi = {10.1109/ICRA48506.2021.9560925},
	abstract = {This paper focuses on visual semantic navigation, the task of producing actions for an active agent to navigate to a specified target object category in an unknown environment. To complete this task, the algorithm should simultaneously locate and navigate to an instance of the category. In comparison to the traditional point goal navigation, this task requires the agent to have a stronger contextual prior to indoor environments. We introduce SSCNav, an algorithm that explicitly models scene priors using a confidence-aware semantic scene completion module to complete the scene and guide the agent's navigation planning. Given a partial observation of the environment, SSC-Nav first infers a complete scene representation with semantic labels for the unobserved scene together with a confidence map associated with its own prediction. Then, a policy network infers the action from the scene completion result and confidence map. Our experiments demonstrate that the proposed scene completion module improves the efficiency of the downstream navigation policies. Code and data: https://sscnav.cs.columbia.edu/},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liang, Yiqing and Chen, Boyuan and Song, Shuran},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Automation, Codes, Conferences, Navigation, Planning, Semantics, Visualization},
	pages = {13194--13200},
}

@inproceedings{fan_point_2021,
	title = {Point {4D} {Transformer} {Networks} for {Spatio}-{Temporal} {Modeling} in {Point} {Cloud} {Videos}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-01-05},
	author = {Fan, Hehe and Yang, Yi and Kankanhalli, Mohan},
	year = {2021},
	pages = {14204--14213},
}

@article{sodano_robust_2022,
	title = {Robust {Double}-{Encoder} {Network} for {RGB}-{D} {Panoptic} {Segmentation}},
	url = {https://arxiv.org/abs/2210.02834v1},
	doi = {10.48550/arXiv.2210.02834},
	abstract = {Perception is crucial for robots that act in real-world environments, as autonomous systems need to see and understand the world around them to act appropriately. Panoptic segmentation provides an interpretation of the scene by computing a pixel-wise semantic label together with instance IDs. In this paper, we address panoptic segmentation using RGB-D data of indoor scenes. We propose a novel encoder-decoder neural network that processes RGB and depth separately through two encoders. The features of the individual encoders are progressively merged at different resolutions, such that the RGB features are enhanced using complementary depth information. We propose a novel merging approach called ResidualExcite, which reweighs each entry of the feature map according to its importance. With our double-encoder architecture, we are robust to missing cues. In particular, the same model can train and infer on RGB-D, RGB-only, and depth-only input data, without the need to train specialized models. We evaluate our method on publicly available datasets and show that our approach achieves superior results compared to other common approaches for panoptic segmentation.},
	language = {en},
	urldate = {2023-01-05},
	author = {Sodano, Matteo and Magistri, Federico and Guadagnino, Tiziano and Behley, Jens and Stachniss, Cyrill},
	month = oct,
	year = {2022},
}

@inproceedings{huang_spatio-temporal_2021,
	title = {Spatio-{Temporal} {Self}-{Supervised} {Representation} {Learning} for {3D} {Point} {Clouds}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Spatio-Temporal_Self-Supervised_Representation_Learning_for_3D_Point_Clouds_ICCV_2021_paper.html},
	language = {en},
	urldate = {2023-01-05},
	author = {Huang, Siyuan and Xie, Yichen and Zhu, Song-Chun and Zhu, Yixin},
	year = {2021},
	pages = {6535--6545},
}

@inproceedings{mersch_self-supervised_2022,
	title = {Self-supervised {Point} {Cloud} {Prediction} {Using} {3D} {Spatio}-temporal {Convolutional} {Networks}},
	url = {https://proceedings.mlr.press/v164/mersch22a.html},
	abstract = {Exploiting past 3D LiDAR scans to predict future point clouds is a promising method for autonomous mobile systems to realize foresighted state estimation, collision avoidance, and planning. In this paper, we address the problem of predicting future 3D LiDAR point clouds given a sequence of past LiDAR scans. Estimating the future scene on the sensor level does not require any preceding steps as in localization or tracking systems and can be trained self-supervised. We propose an end-to-end approach that exploits a 2D range image representation of each 3D LiDAR scan and concatenates a sequence of range images to obtain a 3D tensor. Based on such tensors, we develop an encoder-decoder architecture using 3D convolutions to jointly aggregate spatial and temporal information of the scene and to predict the future 3D point clouds. We evaluate our method on multiple datasets and the experimental results suggest that our method outperforms existing point cloud prediction architectures and generalizes well to new, unseen environments without additional fine-tuning. Our method operates online and is faster than the common LiDAR frame rate of 10 Hz.},
	language = {en},
	urldate = {2023-01-05},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Mersch, Benedikt and Chen, Xieyuanli and Behley, Jens and Stachniss, Cyrill},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1444--1454},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-01-04},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
}

@inproceedings{barrera_birdnet_2020,
	title = {{BirdNet}+: {End}-to-{End} {3D} {Object} {Detection} in {LiDAR} {Bird}’s {Eye} {View}},
	shorttitle = {{BirdNet}+},
	doi = {10.1109/ITSC45102.2020.9294293},
	abstract = {On-board 3D object detection in autonomous vehicles often relies on geometry information captured by LiDAR devices. Albeit image features are typically preferred for detection, numerous approaches take only spatial data as input. Exploiting this information in inference usually involves the use of compact representations such as the Bird's Eye View (BEV) projection, which entails a loss of information and thus hinders the joint inference of all the parameters of the objects' 3D boxes. In this paper, we present a fully end-toend 3D object detection framework that can infer oriented 3D boxes solely from BEV images by using a two-stage object detector and ad-hoc regression branches, eliminating the need for a post-processing stage. The method outperforms its predecessor (BirdNet) by a large margin and obtains state-of the-art results on the KITTI 3D Object Detection Benchmark for all the categories in evaluation. Source code is available at https://github.com/AlejandroBarrera/BirdNet2.},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Barrera, Alejandro and Guindel, Carlos and Beltrán, Jorge and García, Fernando},
	month = sep,
	year = {2020},
	keywords = {Estimation, Feature extraction, Laser radar, Object detection, Proposals, Task analysis, Three-dimensional displays},
	pages = {1--6},
}

@inproceedings{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
	urldate = {2023-01-04},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year = {2020},
	pages = {9729--9738},
}

@misc{duan_survey_2022,
	title = {A {Survey} of {Embodied} {AI}: {From} {Simulators} to {Research} {Tasks}},
	shorttitle = {A {Survey} of {Embodied} {AI}},
	url = {http://arxiv.org/abs/2103.04918},
	doi = {10.48550/arXiv.2103.04918},
	abstract = {There has been an emerging paradigm shift from the era of "internet AI" to "embodied AI", where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneficial to the greater pursuit of Artificial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this field. This paper aims to provide an encyclopedic survey for the field of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI -- visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the field, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the field.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
	month = jan,
	year = {2022},
	note = {arXiv:2103.04918 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{bai_glpanodepth_2022,
	title = {{GLPanoDepth}: {Global}-to-{Local} {Panoramic} {Depth} {Estimation}},
	shorttitle = {{GLPanoDepth}},
	url = {http://arxiv.org/abs/2202.02796},
	doi = {10.48550/arXiv.2202.02796},
	abstract = {In this paper, we propose a learning-based method for predicting dense depth values of a scene from a monocular omnidirectional image. An omnidirectional image has a full field-of-view, providing much more complete descriptions of the scene than perspective images. However, fully-convolutional networks that most current solutions rely on fail to capture rich global contexts from the panorama. To address this issue and also the distortion of equirectangular projection in the panorama, we propose Cubemap Vision Transformers (CViT), a new transformer-based architecture that can model long-range dependencies and extract distortion-free global features from the panorama. We show that cubemap vision transformers have a global receptive field at every stage and can provide globally coherent predictions for spherical signals. To preserve important local features, we further design a convolution-based branch in our pipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision transformers at multiple scales. This global-to-local strategy allows us to fully exploit useful global and local features in the panorama, achieving state-of-the-art performance in panoramic depth estimation.},
	urldate = {2022-12-31},
	publisher = {arXiv},
	author = {Bai, Jiayang and Lai, Shuichang and Qin, Haoyu and Guo, Jie and Guo, Yanwen},
	month = feb,
	year = {2022},
	note = {arXiv:2202.02796 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jiang_unifuse_2021,
	title = {{UniFuse}: {Unidirectional} {Fusion} for 360\${\textasciicircum}\{{\textbackslash}circ\}\$ {Panorama} {Depth} {Estimation}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{UniFuse}},
	url = {http://arxiv.org/abs/2102.03550},
	doi = {10.1109/LRA.2021.3058957},
	abstract = {Learning depth from spherical panoramas is becoming a popular research topic because a panorama has a full field-of-view of the environment and provides a relatively complete description of a scene. However, applying well-studied CNNs for perspective images to the standard representation of spherical panoramas, i.e., the equirectangular projection, is suboptimal, as it becomes distorted towards the poles. Another representation is the cubemap projection, which is distortion-free but discontinued on edges and limited in the field-of-view. This paper introduces a new framework to fuse features from the two projections, unidirectionally feeding the cubemap features to the equirectangular features only at the decoding stage. Unlike the recent bidirectional fusion approach operating at both the encoding and decoding stages, our fusion scheme is much more efficient. Besides, we also designed a more effective fusion module for our fusion scheme. Experiments verify the effectiveness of our proposed fusion strategy and module, and our model achieves state-of-the-art performance on four popular datasets. Additional experiments show that our model also has the advantages of model complexity and generalization capability.The code is available at https://github.com/alibaba/UniFuse-Unidirectional-Fusion.},
	number = {2},
	urldate = {2022-12-31},
	journal = {IEEE Robotics and Automation Letters},
	author = {Jiang, Hualie and Sheng, Zhe and Zhu, Siyu and Dong, Zilong and Huang, Rui},
	month = apr,
	year = {2021},
	note = {arXiv:2102.03550 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {1519--1526},
}

@inproceedings{hinterstoisser_multimodal_2011,
	title = {Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes},
	doi = {10.1109/ICCV.2011.6126326},
	abstract = {We present a method for detecting 3D objects using multi-modalities. While it is generic, we demonstrate it on the combination of an image and a dense depth map which give complementary object information. It works in real-time, under heavy clutter, does not require a time consuming training stage, and can handle untextured objects. It is based on an efficient representation of templates that capture the different modalities, and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Hinterstoisser, Stefan and Holzer, Stefan and Cagniart, Cedric and Ilic, Slobodan and Konolige, Kurt and Navab, Nassir and Lepetit, Vincent},
	month = nov,
	year = {2011},
	note = {ISSN: 2380-7504},
	keywords = {Robustness},
	pages = {858--865},
}

@article{fischler_random_1981,
	title = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
	volume = {24},
	issn = {0001-0782},
	shorttitle = {Random sample consensus},
	url = {https://doi.org/10.1145/358669.358692},
	doi = {10.1145/358669.358692},
	abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
	number = {6},
	urldate = {2022-12-28},
	journal = {Communications of the ACM},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	month = jun,
	year = {1981},
	keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
	pages = {381--395},
}

@misc{zakharov_keep_2018,
	title = {Keep it {Unreal}: {Bridging} the {Realism} {Gap} for 2.{5D} {Recognition} with {Geometry} {Priors} {Only}},
	shorttitle = {Keep it {Unreal}},
	url = {http://arxiv.org/abs/1804.09113},
	doi = {10.48550/arXiv.1804.09113},
	abstract = {With the increasing availability of large databases of 3D CAD models, depth-based recognition methods can be trained on an uncountable number of synthetically rendered images. However, discrepancies with the real data acquired from various depth sensors still noticeably impede progress. Previous works adopted unsupervised approaches to generate more realistic depth data, but they all require real scans for training, even if unlabeled. This still represents a strong requirement, especially when considering real-life/industrial settings where real training images are hard or impossible to acquire, but texture-less 3D models are available. We thus propose a novel approach leveraging only CAD models to bridge the realism gap. Purely trained on synthetic data, playing against an extensive augmentation pipeline in an unsupervised manner, our generative adversarial network learns to effectively segment depth images and recover the clean synthetic-looking depth information even from partial occlusions. As our solution is not only fully decoupled from the real domains but also from the task-specific analytics, the pre-processed scans can be handed to any kind and number of recognition methods also trained on synthetic data. Through various experiments, we demonstrate how this simplifies their training and consistently enhances their performance, with results on par with the same methods trained on real data, and better than usual approaches doing the reverse mapping.},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Zakharov, Sergey and Planche, Benjamin and Wu, Ziyan and Hutter, Andreas and Kosch, Harald and Ilic, Slobodan},
	month = may,
	year = {2018},
	note = {arXiv:1804.09113 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wen_bundletrack_2021,
	title = {{BundleTrack}: {6D} {Pose} {Tracking} for {Novel} {Objects} without {Instance} or {Category}-{Level} {3D} {Models}},
	shorttitle = {{BundleTrack}},
	url = {http://arxiv.org/abs/2108.00516},
	doi = {10.48550/arXiv.2108.00516},
	abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
	urldate = {2022-12-28},
	publisher = {arXiv},
	author = {Wen, Bowen and Bekris, Kostas},
	month = aug,
	year = {2021},
	note = {arXiv:2108.00516 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
}

@inproceedings{liu_soft_2019,
	title = {Soft {Rasterizer}: {A} {Differentiable} {Renderer} for {Image}-{Based} {3D} {Reasoning}},
	shorttitle = {Soft {Rasterizer}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.html},
	urldate = {2022-12-27},
	author = {Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao},
	year = {2019},
	pages = {7708--7717},
}

@article{du_vision-based_2021,
	title = {Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review},
	volume = {54},
	issn = {1573-7462},
	shorttitle = {Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers},
	doi = {10.1007/s10462-020-09888-5},
	abstract = {This paper presents a comprehensive survey on vision-based robotic grasping. We conclude three key tasks during vision-based robotic grasping, which are object localization, object pose estimation and grasp estimation. In detail, the object localization task contains object localization without classification, object detection and object instance segmentation. This task provides the regions of the target object in the input data. The object pose estimation task mainly refers to estimating the 6D object pose and includes correspondence-based methods, template-based methods and voting-based methods, which affords the generation of grasp poses for known objects. The grasp estimation task includes 2D planar grasp methods and 6DoF grasp methods, where the former is constrained to grasp from one direction. These three tasks could accomplish the robotic grasping with different combinations. Lots of object pose estimation methods need not object localization, and they conduct object localization and object pose estimation jointly. Lots of grasp estimation methods need not object localization and object pose estimation, and they conduct grasp estimation in an end-to-end manner. Both traditional methods and latest deep learning-based methods based on the RGB-D image inputs are reviewed elaborately in this survey. Related datasets and comparisons between state-of-the-art methods are summarized as well. In addition, challenges about vision-based robotic grasping and future directions in addressing these challenges are also pointed out.},
	language = {en},
	number = {3},
	urldate = {2022-12-09},
	journal = {Artificial Intelligence Review},
	author = {Du, Guoguang and Wang, Kai and Lian, Shiguo and Zhao, Kaiyong},
	month = mar,
	year = {2021},
	keywords = {Grasp estimation, Object localization, Object pose estimation, Robotic grasping},
	pages = {1677--1734},
}

@article{nigam_detect_2018,
	title = {Detect {Globally}, {Label} {Locally}: {Learning} {Accurate} 6-{DOF} {Object} {Pose} {Estimation} by {Joint} {Segmentation} and {Coordinate} {Regression}},
	volume = {3},
	issn = {2377-3766},
	shorttitle = {Detect {Globally}, {Label} {Locally}},
	doi = {10.1109/LRA.2018.2858446},
	abstract = {Coordinate regression has established itself as one of the most successful current trends in model-based 6 degree of freedom (6-DOF) object pose estimation from a single image. The underlying idea is to train a system that can regress the three-dimensional coordinates of an object, given an input RGB or RGB-D image and known object geometry, followed by a robust procedure such as RANSAC to optimize the object pose. These coordinate regression based approaches exhibit state-of-the-art performance by using pixel-level cues to model the probability distribution of object parts within the image. However, they fail to capture global information at the object level to learn accurate foreground/background segmentation. In this letter, we show that combining global features for object segmentation and local features for coordinate regression results in pixel-accurate object boundary detections and consequently a substantial reduction in outliers and an increase in overall performance. We propose a deep architecture with an instance-level object segmentation network that exploits global image information for object/background segmentation and a pixel-level classification network for coordinate regression based on local features. We evaluate our approach on the standard ground-truth 6-DOF pose estimation benchmarks and show that our joint approach to accurate object segmentation and coordinate regression results in the state-of-the-art performance on both RGB and RGB-D 6-DOF pose estimation.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Nigam, Apurv and Penate-Sanchez, Adrian and Agapito, Lourdes},
	month = oct,
	year = {2018},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Image segmentation, Labeling, Object detection, Object segmentation, Pose estimation, Robots, Robustness, Three-dimensional displays, deep learning in robotics and automation, segmentation and categorization},
	pages = {3960--3967},
}

@misc{kehl_deep_2016,
	title = {Deep {Learning} of {Local} {RGB}-{D} {Patches} for {3D} {Object} {Detection} and {6D} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1607.06038},
	doi = {10.48550/arXiv.1607.06038},
	abstract = {We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Kehl, Wadim and Milletari, Fausto and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06038 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_cdpn_2019,
	title = {{CDPN}: {Coordinates}-{Based} {Disentangled} {Pose} {Network} for {Real}-{Time} {RGB}-{Based} 6-{DoF} {Object} {Pose} {Estimation}},
	shorttitle = {{CDPN}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html},
	urldate = {2022-12-15},
	author = {Li, Zhigang and Wang, Gu and Ji, Xiangyang},
	year = {2019},
	pages = {7678--7687},
}

@misc{brohan_rt-1_2022,
	title = {{RT}-1: {Robotics} {Transformer} for {Real}-{World} {Control} at {Scale}},
	shorttitle = {{RT}-1},
	url = {http://arxiv.org/abs/2212.06817},
	doi = {10.48550/arXiv.2212.06817},
	abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer.github.io},
	urldate = {2022-12-15},
	publisher = {arXiv},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	month = dec,
	year = {2022},
	note = {arXiv:2212.06817 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{besl_method_1992,
	title = {Method for registration of 3-{D} shapes},
	volume = {1611},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1611/0000/Method-for-registration-of-3-D-shapes/10.1117/12.57955.full},
	doi = {10.1117/12.57955},
	abstract = {This paper describes a general purpose, representation independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six-degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and experience shows that the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. For examples, a given 'model' shape and a sensed 'data' shape that represents a major portion of the model shape can be registered in minutes by testing one initial translation and a relatively small set of rotations to allow for the given level of model complexity. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model prior to shape inspection. The described method is also useful for deciding fundamental issues such as the congruence (shape equivalence) of different geometric representations as well as for estimating the motion between point sets where the correspondences are not known. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.},
	urldate = {2022-12-09},
	booktitle = {Sensor {Fusion} {IV}: {Control} {Paradigms} and {Data} {Structures}},
	publisher = {SPIE},
	author = {Besl, Paul J. and McKay, Neil D.},
	month = apr,
	year = {1992},
	pages = {586--606},
}

@inproceedings{peng_pvnet_2019,
	title = {{PVNet}: {Pixel}-{Wise} {Voting} {Network} for {6DoF} {Pose} {Estimation}},
	shorttitle = {{PVNet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.html},
	urldate = {2022-12-09},
	author = {Peng, Sida and Liu, Yuan and Huang, Qixing and Zhou, Xiaowei and Bao, Hujun},
	year = {2019},
	pages = {4561--4570},
}

@misc{zhou_voxelnet_2017,
	title = {{VoxelNet}: {End}-to-{End} {Learning} for {Point} {Cloud} {Based} {3D} {Object} {Detection}},
	shorttitle = {{VoxelNet}},
	url = {http://arxiv.org/abs/1711.06396},
	doi = {10.48550/arXiv.1711.06396},
	abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
	urldate = {2022-12-09},
	publisher = {arXiv},
	author = {Zhou, Yin and Tuzel, Oncel},
	month = nov,
	year = {2017},
	note = {arXiv:1711.06396 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{iwase_repose_2021,
	title = {{RePOSE}: {Fast} {6D} {Object} {Pose} {Refinement} via {Deep} {Texture} {Rendering}},
	shorttitle = {{RePOSE}},
	url = {https://arxiv.org/abs/2104.00633v2},
	doi = {10.48550/arXiv.2104.00633},
	abstract = {We present RePOSE, a fast iterative refinement method for 6D object pose estimation. Prior methods perform refinement by feeding zoomed-in input and rendered RGB images into a CNN and directly regressing an update of a refined pose. Their runtime is slow due to the computational cost of CNN, which is especially prominent in multiple-object pose refinement. To overcome this problem, RePOSE leverages image rendering for fast feature extraction using a 3D model with a learnable texture. We call this deep texture rendering, which uses a shallow multi-layer perceptron to directly regress a view-invariant image representation of an object. Furthermore, we utilize differentiable Levenberg-Marquardt (LM) optimization to refine a pose fast and accurately by minimizing the feature-metric error between the input and rendered image representations without the need of zooming in. These image representations are trained such that differentiable LM optimization converges within few iterations. Consequently, RePOSE runs at 92 FPS and achieves state-of-the-art accuracy of 51.6\% on the Occlusion LineMOD dataset - a 4.1\% absolute improvement over the prior art, and comparable result on the YCB-Video dataset with a much faster runtime. The code is available at https://github.com/sh8/repose.},
	language = {en},
	urldate = {2022-12-09},
	author = {Iwase, Shun and Liu, Xingyu and Khirodkar, Rawal and Yokota, Rio and Kitani, Kris M.},
	month = apr,
	year = {2021},
}

@article{zakharov_dpod_2019,
	title = {{DPOD}: {6D} {Pose} {Object} {Detector} and {Refiner}},
	shorttitle = {{DPOD}},
	url = {https://arxiv.org/abs/1902.11020v3},
	doi = {10.48550/arXiv.1902.11020},
	abstract = {In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.},
	language = {en},
	urldate = {2022-12-09},
	author = {Zakharov, Sergey and Shugurov, Ivan and Ilic, Slobodan},
	month = feb,
	year = {2019},
}

@misc{yang_teaser_2020,
	title = {{TEASER}: {Fast} and {Certifiable} {Point} {Cloud} {Registration}},
	shorttitle = {{TEASER}},
	url = {http://arxiv.org/abs/2001.07715},
	doi = {10.48550/arXiv.2001.07715},
	abstract = {We propose the first fast and certifiable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. We first reformulate the registration problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via adaptive voting, (ii) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates, and (iii) the graph-theoretic framework allows drastic pruning of outliers by finding the maximum clique. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second fast and certifiable algorithm, named TEASER++, that uses graduated non-convexity to solve the rotation subproblem and leverages Douglas-Rachford Splitting to efficiently certify global optimality. For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard, object detection, and the 3DMatch benchmarks, and show that (i) both algorithms dominate the state of the art and are robust to more than 99\% outliers, (ii) TEASER++ can run in milliseconds, and (iii) TEASER++ is so robust it can also solve problems without correspondences, where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
	month = oct,
	year = {2020},
	note = {arXiv:2001.07715 [cs, math]},
	keywords = {68T40, 74Pxx, 46N10, 65D19, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, G.1.6, I.2.9, I.4.5, Mathematics - Optimization and Control},
}

@inproceedings{calli_ycb_2015,
	title = {The {YCB} object and {Model} set: {Towards} common benchmarks for manipulation research},
	shorttitle = {The {YCB} object and {Model} set},
	doi = {10.1109/ICAR.2015.7251504},
	abstract = {In this paper we present the Yale-CMU-Berkeley (YCB) Object and Model set, intended to be used for benchmarking in robotic grasping and manipulation research. The objects in the set are designed to cover various aspects of the manipulation problem; it includes objects of daily life with different shapes, sizes, textures, weight and rigidity, as well as some widely used manipulation tests. The associated database provides high-resolution RGBD scans, physical properties and geometric models of the objects for easy incorporation into manipulation and planning software platforms. A comprehensive literature survey on existing benchmarks and object datasets is also presented and their scope and limitations are discussed. The set will be freely distributed to research groups worldwide at a series of tutorials at robotics conferences, and will be otherwise available at a reasonable purchase cost.},
	booktitle = {2015 {International} {Conference} on {Advanced} {Robotics} ({ICAR})},
	author = {Calli, Berk and Singh, Arjun and Walsman, Aaron and Srinivasa, Siddhartha and Abbeel, Pieter and Dollar, Aaron M.},
	month = jul,
	year = {2015},
	keywords = {Benchmark testing, Databases, Grasping, Planning, Robots, Shape, Solid modeling, benchmarking, grasping, manipulation, prosthetics, rehabilitation},
	pages = {510--517},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2022-11-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@misc{fu_coupling_2022,
	title = {Coupling {Vision} and {Proprioception} for {Navigation} of {Legged} {Robots}},
	url = {http://arxiv.org/abs/2112.02094},
	doi = {10.48550/arXiv.2112.02094},
	abstract = {We exploit the complementary strengths of vision and proprioception to develop a point-goal navigation system for legged robots, called VP-Nav. Legged systems are capable of traversing more complex terrain than wheeled robots, but to fully utilize this capability, we need a high-level path planner in the navigation system to be aware of the walking capabilities of the low-level locomotion policy in varying environments. We achieve this by using proprioceptive feedback to ensure the safety of the planned path by sensing unexpected obstacles like glass walls, terrain properties like slipperiness or softness of the ground and robot properties like extra payload that are likely missed by vision. The navigation system uses onboard cameras to generate an occupancy map and a corresponding cost map to reach the goal. A fast marching planner then generates a target path. A velocity command generator takes this as input to generate the desired velocity for the walking policy. A safety advisor module adds sensed unexpected obstacles to the occupancy map and environment-determined speed limits to the velocity command generator. We show superior performance compared to wheeled robot baselines, and ablation studies which have disjoint high-level planning and low-level control. We also show the real-world deployment of VP-Nav on a quadruped robot with onboard sensors and computation. Videos at https://navigation-locomotion.github.io},
	urldate = {2022-11-22},
	publisher = {arXiv},
	author = {Fu, Zipeng and Kumar, Ashish and Agarwal, Ananye and Qi, Haozhi and Malik, Jitendra and Pathak, Deepak},
	month = jul,
	year = {2022},
	note = {arXiv:2112.02094 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{falanga_pampc_2018,
	title = {{PAMPC}: {Perception}-{Aware} {Model} {Predictive} {Control} for {Quadrotors}},
	shorttitle = {{PAMPC}},
	url = {http://arxiv.org/abs/1804.04811},
	doi = {10.48550/arXiv.1804.04811},
	abstract = {We present the first perception-aware model predictive control framework for quadrotors that unifies control and planning with respect to action and perception objectives. Our framework leverages numerical optimization to compute trajectories that satisfy the system dynamics and require control inputs within the limits of the platform. Simultaneously, it optimizes perception objectives for robust and reliable sens- ing by maximizing the visibility of a point of interest and minimizing its velocity in the image plane. Considering both perception and action objectives for motion planning and control is challenging due to the possible conflicts arising from their respective requirements. For example, for a quadrotor to track a reference trajectory, it needs to rotate to align its thrust with the direction of the desired acceleration. However, the perception objective might require to minimize such rotation to maximize the visibility of a point of interest. A model-based optimization framework, able to consider both perception and action objectives and couple them through the system dynamics, is therefore necessary. Our perception-aware model predictive control framework works in a receding-horizon fashion by iteratively solving a non-linear optimization problem. It is capable of running in real-time, fully onboard our lightweight, small-scale quadrotor using a low-power ARM computer, to- gether with a visual-inertial odometry pipeline. We validate our approach in experiments demonstrating (I) the contradiction between perception and action objectives, and (II) improved behavior in extremely challenging lighting conditions.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Falanga, Davide and Foehn, Philipp and Lu, Peng and Scaramuzza, Davide},
	month = jul,
	year = {2018},
	note = {arXiv:1804.04811 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{pan_task-driven_2022,
	title = {Task-{Driven} {In}-{Hand} {Manipulation} of {Unknown} {Objects} with {Tactile} {Sensing}},
	url = {http://arxiv.org/abs/2210.13403},
	doi = {10.48550/arXiv.2210.13403},
	abstract = {Manipulation of objects in-hand without an object model is a foundational skill for many tasks in unstructured environments. In many cases, vision-only approaches may not be feasible; for example, due to occlusion in cluttered spaces. In this paper, we introduce a method to reorient unknown objects by incrementally building a probabilistic estimate of the object shape and pose during task-driven manipulation. Our method leverages Bayesian optimization to strategically trade-off exploration of the global object shape with efficient task completion. We demonstrate our approach on a Tactile-Enabled Roller Grasper, a gripper that rolls objects in hand while continuously collecting tactile data. We evaluate our method in simulation on a set of randomly generated objects and find that our method reliably reorients objects while significantly reducing the exploration time needed to do so. On the Roller Grasper hardware, we show successful qualitative reconstruction of the object model. In summary, this work (1) presents a system capable of simultaneously learning unknown 3D object shape and pose using tactile sensing; and (2) demonstrates that task-driven exploration results in more efficient object manipulation than the common paradigm of complete object exploration before task-completion.},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Pan, Chaoyi and Lepert, Marion and Yuan, Shenli and Antonova, Rika and Bohg, Jeannette},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13403 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{wu_6d-vnet_2019,
	title = {{6D}-{VNet}: {End}-{To}-{End} {6DoF} {Vehicle} {Pose} {Estimation} {From} {Monocular} {RGB} {Images}},
	shorttitle = {{6D}-{VNet}},
	doi = {10.1109/CVPRW.2019.00163},
	abstract = {We present a conceptually simple framework for 6DoF object pose estimation, especially for autonomous driving scenario. Our approach efficiently detects traffic participants in a monocular RGB image while simultaneously regressing their 3D translation and rotation vectors. The method, called 6D-VNet, extends Mask R-CNN by adding customised heads for predicting vehicle's finer class, rotation and translation. The proposed 6D-VNet is trained end-to-end compared to previous methods. Furthermore, we show that the inclusion of translational regression in the joint losses is crucial for the 6DoF pose estimation task, where object translation distance along longitudinal axis varies significantly, e.g., in autonomous driving scenarios. Additionally, we incorporate the mutual information between traffic participants via a modified non-local block. As opposed to the original non-local block implementation, the proposed weighting modification takes the spatial neighbouring information into consideration whilst counteracting the effect of extreme gradient values. Our 6D-VNet reaches the 1 st place in ApolloScape challenge 3D Car Instance task. Code has been made available at: https://github.com/stevenwudi/6DVNET .},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Wu, Di and Zhuang, Zhaoyong and Xiang, Canqun and Zou, Wenbin and Li, Xia},
	month = jun,
	year = {2019},
	note = {ISSN: 2160-7516},
	keywords = {Head, Object detection, Pose estimation, Quaternions, Three-dimensional displays, Two dimensional displays},
	pages = {1238--1247},
}

@inproceedings{caesar_nuscenes_2020,
	title = {{nuScenes}: {A} {Multimodal} {Dataset} for {Autonomous} {Driving}},
	shorttitle = {{nuScenes}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.html},
	urldate = {2022-11-20},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	year = {2020},
	pages = {11621--11631},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	shorttitle = {Are we ready for autonomous driving?},
	doi = {10.1109/CVPR.2012.6248074},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6919},
	keywords = {Benchmark testing, Cameras, Measurement, Optical imaging, Optical sensors, Visualization},
	pages = {3354--3361},
}

@misc{qin_one_2022,
	title = {From {One} {Hand} to {Multiple} {Hands}: {Imitation} {Learning} for {Dexterous} {Manipulation} from {Single}-{Camera} {Teleoperation}},
	shorttitle = {From {One} {Hand} to {Multiple} {Hands}},
	url = {http://arxiv.org/abs/2204.12490},
	doi = {10.48550/arXiv.2204.12490},
	abstract = {We propose to perform imitation learning for dexterous manipulation with multi-finger robot hand from human demonstrations, and transfer the policy to the real robot hand. We introduce a novel single-camera teleoperation system to collect the 3D demonstrations efficiently with only an iPad and a computer. One key contribution of our system is that we construct a customized robot hand for each user in the physical simulator, which is a manipulator resembling the same kinematics structure and shape of the operator's hand. This provides an intuitive interface and avoid unstable human-robot hand retargeting for data collection, leading to large-scale and high quality data. Once the data is collected, the customized robot hand trajectories can be converted to different specified robot hands (models that are manufactured) to generate training demonstrations. With imitation learning using our data, we show large improvement over baselines with multiple complex manipulation tasks. Importantly, we show our learned policy is significantly more robust when transferring to the real robot. More videos can be found in the https://yzqin.github.io/dex-teleop-imitation .},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Qin, Yuzhe and Su, Hao and Wang, Xiaolong},
	month = apr,
	year = {2022},
	note = {arXiv:2204.12490 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{wang_6-pack_2020,
	title = {6-{PACK}: {Category}-level {6D} {Pose} {Tracker} with {Anchor}-{Based} {Keypoints}},
	shorttitle = {6-{PACK}},
	doi = {10.1109/ICRA40945.2020.9196679},
	abstract = {We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wang, Chen and Martín-Martín, Roberto and Xu, Danfei and Lv, Jun and Lu, Cewu and Fei-Fei, Li and Savarese, Silvio and Zhu, Yuke},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Pose estimation, Real-time systems, Robots, Robustness, Three-dimensional displays, Tracking, Visualization},
	pages = {10059--10066},
}

@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	language = {en},
	urldate = {2022-11-19},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {214--223},
}

@misc{chen_seeing_2022,
	title = {Seeing {Beyond} the {Brain}: {Conditional} {Diffusion} {Model} with {Sparse} {Masked} {Modeling} for {Vision} {Decoding}},
	shorttitle = {Seeing {Beyond} the {Brain}},
	url = {http://arxiv.org/abs/2211.06956},
	doi = {10.48550/arXiv.2211.06956},
	abstract = {Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66\% and 41\% respectively. An exhaustive ablation study was also conducted to analyze our framework.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Chen, Zijiao and Qing, Jiaxin and Xiang, Tiange and Yue, Wan Lin and Zhou, Juan Helen},
	month = nov,
	year = {2022},
	note = {arXiv:2211.06956 [cs]},
	keywords = {ACM-class: I.4, I.5, J.3, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{van_der_merwe_learning_2020,
	title = {Learning {Continuous} {3D} {Reconstructions} for {Geometrically} {Aware} {Grasping}},
	url = {http://arxiv.org/abs/1910.00983},
	doi = {10.48550/arXiv.1910.00983},
	abstract = {Deep learning has enabled remarkable improvements in grasp synthesis for previously unseen objects from partial object views. However, existing approaches lack the ability to explicitly reason about the full 3D geometry of the object when selecting a grasp, relying on indirect geometric reasoning derived when learning grasp success networks. This abandons explicit geometric reasoning, such as avoiding undesired robot object collisions. We propose to utilize a novel, learned 3D reconstruction to enable geometric awareness in a grasping system. We leverage the structure of the reconstruction network to learn a grasp success classifier which serves as the objective function for a continuous grasp optimization. We additionally explicitly constrain the optimization to avoid undesired contact, directly using the reconstruction. We examine the role of geometry in grasping both in the training of grasp metrics and through 96 robot grasping trials. Our results can be found on https://sites.google.com/view/reconstruction-grasp/.},
	urldate = {2022-11-16},
	publisher = {arXiv},
	author = {Van der Merwe, Mark and Lu, Qingkai and Sundaralingam, Balakumar and Matak, Martin and Hermans, Tucker},
	month = mar,
	year = {2020},
	note = {arXiv:1910.00983 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{he_voxel_2022,
	title = {Voxel {Set} {Transformer}: {A} {Set}-to-{Set} {Approach} to {3D} {Object} {Detection} {From} {Point} {Clouds}},
	shorttitle = {Voxel {Set} {Transformer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-15},
	author = {He, Chenhang and Li, Ruihuang and Li, Shuai and Zhang, Lei},
	year = {2022},
	pages = {8417--8427},
}

@inproceedings{park_fast_2022,
	title = {Fast {Point} {Transformer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Park_Fast_Point_Transformer_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-15},
	author = {Park, Chunghyun and Jeong, Yoonwoo and Cho, Minsu and Park, Jaesik},
	year = {2022},
	pages = {16949--16958},
}

@inproceedings{mao_voxel_2021,
	title = {Voxel {Transformer} for {3D} {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Mao_Voxel_Transformer_for_3D_Object_Detection_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-11-15},
	author = {Mao, Jiageng and Xue, Yujing and Niu, Minzhe and Bai, Haoyue and Feng, Jiashi and Liang, Xiaodan and Xu, Hang and Xu, Chunjing},
	year = {2021},
	pages = {3164--3173},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {http://arxiv.org/abs/1207.0580},
	doi = {10.48550/arXiv.1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2022-11-15},
	publisher = {arXiv},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv:1207.0580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{maturana_voxnet_2015,
	title = {{VoxNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object recognition},
	shorttitle = {{VoxNet}},
	doi = {10.1109/IROS.2015.7353481},
	abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Maturana, Daniel and Scherer, Sebastian},
	month = sep,
	year = {2015},
	keywords = {Feature extraction, Laser radar, Neural networks, Object recognition, Robots, Sensors, Three-dimensional displays},
	pages = {922--928},
}

@misc{mao_least_2017,
	title = {Least {Squares} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.04076},
	doi = {10.48550/arXiv.1611.04076},
	abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \${\textbackslash}chi{\textasciicircum}2\$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
	urldate = {2022-11-14},
	publisher = {arXiv},
	author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
	month = apr,
	year = {2017},
	note = {arXiv:1611.04076 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gupta_cognitive_2019,
	title = {Cognitive {Mapping} and {Planning} for {Visual} {Navigation}},
	url = {http://arxiv.org/abs/1702.03920},
	doi = {10.48550/arXiv.1702.03920},
	abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.},
	urldate = {2022-11-13},
	publisher = {arXiv},
	author = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
	month = feb,
	year = {2019},
	note = {arXiv:1702.03920 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{magistri_contrastive_2022,
	title = {Contrastive {3D} {Shape} {Completion} and {Reconstruction} for {Agricultural} {Robots} {Using} {RGB}-{D} {Frames}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3193239},
	abstract = {Monitoring plants and fruits is important in modern agriculture, with applications ranging from high-throughput phenotyping to autonomous harvesting. Obtaining highly accurate 3D measurements under real agricultural conditions is a challenging task. In this letter, we address the problem of estimating the 3D shape of fruits when only a partial view is available. We propose a pipeline that exploits high-resolution 3D data in the learning phase but only requires a single RGB-D frame to predict the 3D shape of a complete fruit during operation. To achieve this, we first learn a latent space of potential fruit appearances that we can decode into an SDF volume. With the pretrained, frozen decoder, we subsequently learn an encoder that can produce meaningful latent vectors from a single RGB-D frame. The experiments presented in this letter suggest that our approach can predict the 3D shape of whole fruits online, needing only 4 ms for inference. We evaluate our approach in controlled environments and illustrate its deployment in greenhouses without modifications.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Magistri, Federico and Marks, Elias and Nagulavancha, Sumanth and Vizzo, Ignacio and Läebe, Thomas and Behley, Jens and Halstead, Michael and McCool, Chris and Stachniss, Cyrill},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Crops, Deep learning for visual perception, Greenhouses, RGB-D perception, Robot sensing systems, Robots, Shape, Task analysis, Three-dimensional displays, robotics and automation in agriculture and forestry},
	pages = {10120--10127},
}

@misc{xu_show_2016,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	shorttitle = {Show, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/1502.03044},
	doi = {10.48550/arXiv.1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv:1502.03044 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{levine_end--end_2016,
	title = {End-to-{End} {Training} of {Deep} {Visuomotor} {Policies}},
	url = {http://arxiv.org/abs/1504.00702},
	doi = {10.48550/arXiv.1504.00702},
	abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
	month = apr,
	year = {2016},
	note = {arXiv:1504.00702 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{liu_structdiffusion_2022,
	title = {{StructDiffusion}: {Object}-{Centric} {Diffusion} for {Semantic} {Rearrangement} of {Novel} {Objects}},
	shorttitle = {{StructDiffusion}},
	url = {http://arxiv.org/abs/2211.04604},
	doi = {10.48550/arXiv.2211.04604},
	abstract = {Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures out of a single RGB-D image based on high-level language goals, such as "set the table." Our method shows how diffusion models can be used for complex multi-step 3D planning tasks. StructDiffusion improves success rate on assembling physically-valid structures out of unseen objects by on average 16\% over an existing multi-modal transformer model, while allowing us to use one multi-task model to produce a wider range of different structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. For videos and additional results, check out our website: http://weiyuliu.com/StructDiffusion/.},
	urldate = {2022-11-11},
	publisher = {arXiv},
	author = {Liu, Weiyu and Hermans, Tucker and Chernova, Sonia and Paxton, Chris},
	month = nov,
	year = {2022},
	note = {arXiv:2211.04604 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{tahoun_visual-tactile_2021,
	title = {Visual-{Tactile} {Fusion} for {3D} {Objects} {Reconstruction} from a {Single} {Depth} {View} and a {Single} {Gripper} {Touch} for {Robotics} {Tasks}},
	doi = {10.1109/IROS51168.2021.9636150},
	abstract = {The planning of robotic manipulation and grasping tasks depends on the reconstruction of the 3D object’s shape. Most of the existing 3D object reconstruction methods are based on visual sensing that are limited due to the lack of the object’s occluded side information. The goal of this paper is to overcome these limitations and improve the 3D objects’ reconstruction by adding the tactile sensing to the visual data. In this paper, a novel multi-modal (visual and tactile) semi-supervised generative model is presented to reconstruct the complete 3D object’s shape using a single arbitrary depth-view and a single dexterous-hand’s touch. The presented approach takes the strength of the autoencoder and generative networks to provide an end-to-end trainable model with high generalization ability. The 3D voxel grids of the depth and tactile data are the only requirements of the proposed model to predict a high resolution voxel grids of 643 for the incomplete shape. This research generates its tactile dataset based on the kinematic model of the shadow dexterous hand. The developed dataset has aligned depth, tactile and ground truth voxel grids of different resolutions (403, 643 and 1283) from different camera views. Experimental results show that the proposed multi-modal model outperforms other state-of-the-art methods.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tahoun, Mohamed and Tahri, Omar and Corrales Ramón, Juan Antonio and Mezouar, Youcef},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Grasping, Kinematics, Sensors, Shape, Solid modeling, Three-dimensional displays, Visualization},
	pages = {6786--6793},
}

@misc{weng_neural_2022,
	title = {Neural {Grasp} {Distance} {Fields} for {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2211.02647},
	doi = {10.48550/arXiv.2211.02647},
	abstract = {We formulate grasp learning as a neural field and present Neural Grasp Distance Fields (NGDF). Here, the input is a 6D pose of a robot end effector and output is a distance to a continuous manifold of valid grasps for an object. In contrast to current approaches that predict a set of discrete candidate grasps, the distance-based NGDF representation is easily interpreted as a cost, and minimizing this cost produces a successful grasp pose. This grasp distance cost can be incorporated directly into a trajectory optimizer for joint optimization with other costs such as trajectory smoothness and collision avoidance. During optimization, as the various costs are balanced and minimized, the grasp target is allowed to smoothly vary, as the learned grasp field is continuous. In simulation benchmarks with a Franka arm, we find that joint grasping and planning with NGDF outperforms baselines by 63\% execution success while generalizing to unseen query poses and unseen object shapes. Project page: https://sites.google.com/view/neural-grasp-distance-fields.},
	urldate = {2022-11-10},
	publisher = {arXiv},
	author = {Weng, Thomas and Held, David and Meier, Franziska and Mukadam, Mustafa},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02647 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{tamar_value_2017,
	title = {Value {Iteration} {Networks}},
	url = {http://arxiv.org/abs/1602.02867},
	doi = {10.48550/arXiv.1602.02867},
	abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
	urldate = {2022-11-10},
	publisher = {arXiv},
	author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
	month = mar,
	year = {2017},
	note = {arXiv:1602.02867 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{yao_c-mixup_2022,
	title = {C-{Mixup}: {Improving} {Generalization} in {Regression}},
	shorttitle = {C-{Mixup}},
	url = {http://arxiv.org/abs/2210.05775},
	doi = {10.48550/arXiv.2210.05775},
	abstract = {Improving the generalization of deep networks is an important open challenge, particularly in domains without plentiful data. The mixup algorithm improves generalization by linearly interpolating a pair of examples and their corresponding labels. These interpolated examples augment the original training set. Mixup has shown promising results in various classification tasks, but systematic analysis of mixup in regression remains underexplored. Using mixup directly on regression labels can result in arbitrarily incorrect labels. In this paper, we propose a simple yet powerful algorithm, C-Mixup, to improve generalization on regression tasks. In contrast with vanilla mixup, which picks training examples for mixing with uniform probability, C-Mixup adjusts the sampling probability based on the similarity of the labels. Our theoretical analysis confirms that C-Mixup with label similarity obtains a smaller mean square error in supervised regression and meta-regression than vanilla mixup and using feature similarity. Another benefit of C-Mixup is that it can improve out-of-distribution robustness, where the test distribution is different from the training distribution. By selectively interpolating examples with similar labels, it mitigates the effects of domain-associated information and yields domain-invariant representations. We evaluate C-Mixup on eleven datasets, ranging from tabular to video data. Compared to the best prior approach, C-Mixup achieves 6.56\%, 4.76\%, 5.82\% improvements in in-distribution generalization, task generalization, and out-of-distribution robustness, respectively. Code is released at https://github.com/huaxiuyao/C-Mixup.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Yao, Huaxiu and Wang, Yiping and Zhang, Linjun and Zou, James and Finn, Chelsea},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05775 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	doi = {10.48550/arXiv.1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv:1411.1784 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{peng_convolutional_2020,
	title = {Convolutional {Occupancy} {Networks}},
	url = {http://arxiv.org/abs/2003.04618},
	doi = {10.48550/arXiv.2003.04618},
	abstract = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.},
	urldate = {2022-10-31},
	publisher = {arXiv},
	author = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
	month = aug,
	year = {2020},
	note = {arXiv:2003.04618 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{streli_capcontact_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {{CapContact}: {Super}-resolution {Contact} {Areas} from {Capacitive} {Touchscreens}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {{CapContact}},
	url = {https://doi.org/10.1145/3411764.3445621},
	doi = {10.1145/3411764.3445621},
	abstract = {Touch input is dominantly detected using mutual-capacitance sensing, which measures the proximity of close-by objects that change the electric field between the sensor lines. The exponential drop-off in intensities with growing distance enables software to detect touch events, but does not reveal true contact areas. In this paper, we introduce CapContact, a novel method to precisely infer the contact area between the user’s finger and the surface from a single capacitive image. At 8 × super-resolution, our convolutional neural network generates refined touch masks from 16-bit capacitive images as input, which can even discriminate adjacent touches that are not distinguishable with existing methods. We trained and evaluated our method using supervised learning on data from 10 participants who performed touch gestures. Our capture apparatus integrates optical touch sensing to obtain ground-truth contact through high-resolution frustrated total internal reflection. We compare our method with a baseline using bicubic upsampling as well as the ground truth from FTIR images. We separately evaluate our method’s performance in discriminating adjacent touches. CapContact successfully separated closely adjacent touch contacts in 494 of 570 cases (87\%) compared to the baseline’s 43 of 570 cases (8\%). Importantly, we demonstrate that our method accurately performs even at half of the sensing resolution at twice the grid-line pitch across the same surface area, challenging the current industry-wide standard of a ∼ 4 mm sensing pitch. We conclude this paper with implications for capacitive touch sensing in general and for touch-input accuracy in particular.},
	urldate = {2022-10-19},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Streli, Paul and Holz, Christian},
	month = may,
	year = {2021},
	keywords = {Accuracy, Capacitive sensing, Contact area, Generative adversarial networks;, Super-resolution, Touch input},
	pages = {1--14},
}

@misc{lahoud_3d_2022,
	title = {{3D} {Vision} with {Transformers}: {A} {Survey}},
	shorttitle = {{3D} {Vision} with {Transformers}},
	url = {http://arxiv.org/abs/2208.04309},
	doi = {10.48550/arXiv.2208.04309},
	abstract = {The success of the transformer architecture in natural language processing has recently triggered attention in the computer vision field. The transformer has been used as a replacement for the widely used convolution operators, due to its ability to learn long-range dependencies. This replacement was proven to be successful in numerous tasks, in which several state-of-the-art methods rely on transformers for better learning. In computer vision, the 3D field has also witnessed an increase in employing the transformer for 3D convolution neural networks and multi-layer perceptron networks. Although a number of surveys have focused on transformers in vision in general, 3D vision requires special attention due to the difference in data representation and processing when compared to 2D vision. In this work, we present a systematic and thorough review of more than 100 transformers methods for different 3D vision tasks, including classification, segmentation, detection, completion, pose estimation, and others. We discuss transformer design in 3D vision, which allows it to process data with various 3D representations. For each application, we highlight key properties and contributions of proposed transformer-based methods. To assess the competitiveness of these methods, we compare their performance to common non-transformer methods on 12 3D benchmarks. We conclude the survey by discussing different open directions and challenges for transformers in 3D vision. In addition to the presented papers, we aim to frequently update the latest relevant papers along with their corresponding implementations at: https://github.com/lahoud/3d-vision-transformers.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Lahoud, Jean and Cao, Jiale and Khan, Fahad Shahbaz and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Yang, Ming-Hsuan},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04309 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fu_robust_2021,
	title = {Robust {Point} {Cloud} {Registration} {Framework} {Based} on {Deep} {Graph} {Matching}},
	url = {http://arxiv.org/abs/2103.04256},
	doi = {10.48550/arXiv.2103.04256},
	abstract = {3D point cloud registration is a fundamental problem in computer vision and robotics. There has been extensive research in this area, but existing methods meet great challenges in situations with a large proportion of outliers and time constraints, but without good transformation initialization. Recently, a series of learning-based algorithms have been introduced and show advantages in speed. Many of them are based on correspondences between the two point clouds, so they do not rely on transformation initialization. However, these learning-based methods are sensitive to outliers, which lead to more incorrect correspondences. In this paper, we propose a novel deep graph matchingbased framework for point cloud registration. Specifically, we first transform point clouds into graphs and extract deep features for each point. Then, we develop a module based on deep graph matching to calculate a soft correspondence matrix. By using graph matching, not only the local geometry of each point but also its structure and topology in a larger range are considered in establishing correspondences, so that more correct correspondences are found. We train the network with a loss directly defined on the correspondences, and in the test stage the soft correspondences are transformed into hard one-to-one correspondences so that registration can be performed by singular value decomposition. Furthermore, we introduce a transformer-based method to generate edges for graph construction, which further improves the quality of the correspondences. Extensive experiments on registering clean, noisy, partial-to-partial and unseen category point clouds show that the proposed method achieves state-of-the-art performance. The code will be made publicly available at https://github.com/fukexue/RGM.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Fu, Kexue and Liu, Shaolei and Luo, Xiaoyuan and Wang, Manning},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04256 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mittal_autosdf_2022,
	title = {{AutoSDF}: {Shape} {Priors} for {3D} {Completion}, {Reconstruction} and {Generation}},
	shorttitle = {{AutoSDF}},
	url = {http://arxiv.org/abs/2203.09516},
	doi = {10.48550/arXiv.2203.09516},
	abstract = {Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the effectiveness of the proposed method using both quantitative and qualitative evaluation and show that the proposed method outperforms the specialized state-of-the-art methods trained for individual tasks. The project page with code and video visualizations can be found at https://yccyenchicheng.github.io/AutoSDF/.},
	urldate = {2022-10-19},
	publisher = {arXiv},
	author = {Mittal, Paritosh and Cheng, Yen-Chi and Singh, Maneesh and Tulsiani, Shubham},
	month = apr,
	year = {2022},
	note = {arXiv:2203.09516 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{oberweger_making_2018,
	title = {Making {Deep} {Heatmaps} {Robust} to {Partial} {Occlusions} for {3D} {Object} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1804.03959},
	doi = {10.48550/arXiv.1804.03959},
	abstract = {We introduce a novel method for robust and accurate 3D object pose estimation from a single color image under large occlusions. Following recent approaches, we first predict the 2D projections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training subsequently becomes challenging because patches with similar appearances but different positions on the object correspond to different heatmaps. However, we provide a simple yet effective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded LineMOD dataset and the YCB-Video dataset, both exhibiting cluttered scenes with highly occluded objects. Project website: https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/robust-object-pose-estimation/},
	urldate = {2022-10-17},
	publisher = {arXiv},
	author = {Oberweger, Markus and Rad, Mahdi and Lepetit, Vincent},
	month = jul,
	year = {2018},
	note = {arXiv:1804.03959 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huynh_metrics_2009,
	title = {Metrics for {3D} {Rotations}: {Comparison} and {Analysis}},
	volume = {35},
	issn = {1573-7683},
	shorttitle = {Metrics for {3D} {Rotations}},
	url = {https://doi.org/10.1007/s10851-009-0161-2},
	doi = {10.1007/s10851-009-0161-2},
	abstract = {3D rotations arise in many computer vision, computer graphics, and robotics problems and evaluation of the distance between two 3D rotations is often an essential task. This paper presents a detailed analysis of six functions for measuring distance between 3D rotations that have been proposed in the literature. Based on the well-developed theory behind 3D rotations, we demonstrate that five of them are bi-invariant metrics on SO(3) but that only four of them are boundedly equivalent to each other. We conclude that it is both spatially and computationally more efficient to use quaternions for 3D rotations. Lastly, by treating the two rotations as a true and an estimated rotation matrix, we illustrate the geometry associated with iso-error measures.},
	language = {en},
	number = {2},
	urldate = {2022-10-14},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Huynh, Du Q.},
	month = oct,
	year = {2009},
	keywords = {3D rotations, Distance functions, Lie algebra, Matrix Lie group, Quaternions},
	pages = {155--164},
}

@article{hartley_rotation_2013,
	title = {Rotation {Averaging}},
	volume = {103},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-012-0601-0},
	doi = {10.1007/s11263-012-0601-0},
	abstract = {This paper is conceived as a tutorial on rotation averaging, summarizing the research that has been carried out in this area; it discusses methods for single-view and multiple-view rotation averaging, as well as providing proofs of convergence and convexity in many cases. However, at the same time it contains many new results, which were developed to fill gaps in knowledge, answering fundamental questions such as radius of convergence of the algorithms, and existence of local minima. These matters, or even proofs of correctness have in many cases not been considered in the Computer Vision literature. We consider three main problems: single rotation averaging, in which a single rotation is computed starting from several measurements; multiple-rotation averaging, in which absolute orientations are computed from several relative orientation measurements; and conjugate rotation averaging, which relates a pair of coordinate frames. This last is related to the hand-eye coordination problem and to multiple-camera calibration.},
	language = {en},
	number = {3},
	urldate = {2022-10-14},
	journal = {International Journal of Computer Vision},
	author = {Hartley, Richard and Trumpf, Jochen and Dai, Yuchao and Li, Hongdong},
	month = jul,
	year = {2013},
	keywords = {Angular distance, Chordal distance, Geodesic distance, L\_1 mean, L\_2 mean, Quaternion distance, conjugate rotation},
	pages = {267--305},
}

@inproceedings{di_so-pose_2021,
	title = {{SO}-{Pose}: {Exploiting} {Self}-{Occlusion} for {Direct} {6D} {Pose} {Estimation}},
	shorttitle = {{SO}-{Pose}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Di_SO-Pose_Exploiting_Self-Occlusion_for_Direct_6D_Pose_Estimation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-10-13},
	author = {Di, Yan and Manhardt, Fabian and Wang, Gu and Ji, Xiangyang and Navab, Nassir and Tombari, Federico},
	year = {2021},
	pages = {12396--12405},
}

@inproceedings{kehl_ssd-6d_2017,
	title = {{SSD}-{6D}: {Making} {RGB}-{Based} {3D} {Detection} and {6D} {Pose} {Estimation} {Great} {Again}},
	shorttitle = {{SSD}-{6D}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Kehl_SSD-6D_Making_RGB-Based_ICCV_2017_paper.html},
	urldate = {2022-10-13},
	author = {Kehl, Wadim and Manhardt, Fabian and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
	year = {2017},
	pages = {1521--1529},
}

@misc{hoeller_neural_2022,
	title = {Neural {Scene} {Representation} for {Locomotion} on {Structured} {Terrain}},
	url = {http://arxiv.org/abs/2206.08077},
	doi = {10.48550/arXiv.2206.08077},
	abstract = {We propose a learning-based method to reconstruct the local terrain for locomotion with a mobile robot traversing urban environments. Using a stream of depth measurements from the onboard cameras and the robot's trajectory, the algorithm estimates the topography in the robot's vicinity. The raw measurements from these cameras are noisy and only provide partial and occluded observations that in many cases do not show the terrain the robot stands on. Therefore, we propose a 3D reconstruction model that faithfully reconstructs the scene, despite the noisy measurements and large amounts of missing data coming from the blind spots of the camera arrangement. The model consists of a 4D fully convolutional network on point clouds that learns the geometric priors to complete the scene from the context and an auto-regressive feedback to leverage spatio-temporal consistency and use evidence from the past. The network can be solely trained with synthetic data, and due to extensive augmentation, it is robust in the real world, as shown in the validation on a quadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline on the robot's onboard low-power computer using an efficient sparse tensor implementation and show that the proposed method outperforms classical map representations.},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Hoeller, David and Rudin, Nikita and Choy, Christopher and Anandkumar, Animashree and Hutter, Marco},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08077 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{smith_3d_2020,
	title = {{3D} {Shape} {Reconstruction} from {Vision} and {Touch}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/a3842ed7b3d0fe3ac263bcabd2999790-Abstract.html},
	abstract = {When a toddler is presented a new toy, their instinctual behaviour is to pick it up and inspect it with their hand and eyes in tandem, clearly searching over its surface to properly understand what they are playing with. At any instance here, touch provides high fidelity localized information while vision provides complementary global context. However, in 3D shape reconstruction, the complementary fusion of visual and haptic modalities remains largely unexplored. In this paper, we study this problem and present an effective chart-based approach to multi-modal shape understanding which encourages a similar fusion vision and touch information. To do so, we introduce a dataset of simulated touch and vision signals from the interaction between a robotic hand and a large array of 3D objects. Our results show that (1) leveraging both vision and touch signals consistently improves single- modality baselines; (2) our approach outperforms alternative modality fusion methods and strongly benefits from the proposed chart-based structure; (3) the reconstruction quality increases with the number of grasps provided; and (4) the touch information not only enhances the reconstruction at the touch site but also extrapolates to its local neighborhood.},
	urldate = {2022-10-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Smith, Edward and Calandra, Roberto and Romero, Adriana and Gkioxari, Georgia and Meger, David and Malik, Jitendra and Drozdzal, Michal},
	year = {2020},
	pages = {14193--14206},
}

@article{zhang_partial--partial_2022,
	title = {Partial-to-{Partial} {Point} {Generation} {Network} for {Point} {Cloud} {Completion}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3210300},
	abstract = {Point cloud completion aims at predicting dense complete 3D shapes from sparse incomplete point clouds captured from 3D sensors or scanners. It plays an essential role in various applications such as autonomous driving, 3D reconstruction, augmented reality, and robot navigation. Existing point cloud completion methods follow the encoder-decoder paradigm, in which the complete point clouds are recovered in a coarse-to-fine strategy. However, only using the global feature is difficult and will lead to blurring of the global structure and distortion of local details. To address this problem, we propose a novel Partial-to-Partial Point Generation Network (\${\textbackslash}textP{\textasciicircum}2\$GNet), a learning-based approach for point cloud completion. In \${\textbackslash}textP{\textasciicircum}2\$GNet, we use a feature disentangle encoder to obtain the global feature, and missing code and novel view partial point cloud are generated conditioned on the view-related missing code. To better aggregate partial point clouds, an attentive sampling module is proposed to sample multiple partial point clouds into the final complete result. Extensive experiments on several public benchmarks demonstrate that our \${\textbackslash}textP{\textasciicircum}2\$GNet outperforms state-of-the-art point cloud completion methods.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zhang, Ziyu and Yu, Yi and Da, Feipeng},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Codes, Computer Vision for Automation, Decoding, Deep Learning for Visual Perception, Geometry, Point Cloud Completion, Point cloud compression, Shape, Task analysis, Three-dimensional displays},
	pages = {11990--11997},
}

@inproceedings{brachmann_learning_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {6D} {Object} {Pose} {Estimation} {Using} {3D} {Object} {Coordinates}},
	isbn = {978-3-319-10605-2},
	doi = {10.1007/978-3-319-10605-2_35},
	abstract = {This work addresses the problem of estimating the 6D Pose of specific objects from a single RGB-D image. We present a flexible approach that can deal with generic objects, both textured and texture-less. The key new concept is a learned, intermediate representation in form of a dense 3D object coordinate labelling paired with a dense class labelling. We are able to show that for a common dataset with texture-less objects, where template-based techniques are suitable and state of the art, our approach is slightly superior in terms of accuracy. We also demonstrate the benefits of our approach, compared to template-based techniques, in terms of robustness with respect to varying lighting conditions. Towards this end, we contribute a new ground truth dataset with 10k images of 20 objects captured each under three different lighting conditions. We demonstrate that our approach scales well with the number of objects and has capabilities to run fast.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Brachmann, Eric and Krull, Alexander and Michel, Frank and Gumhold, Stefan and Shotton, Jamie and Rother, Carsten},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Background Model, Decision Forest, Object Detection, Training Image, Vary Lighting Condition},
	pages = {536--551},
}

@inproceedings{rios-cabrera_discriminatively_2013,
	title = {Discriminatively {Trained} {Templates} for {3D} {Object} {Detection}: {A} {Real} {Time} {Scalable} {Approach}},
	shorttitle = {Discriminatively {Trained} {Templates} for {3D} {Object} {Detection}},
	doi = {10.1109/ICCV.2013.256},
	abstract = {In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, we propose a challenging new dataset made of 12 objects, for future competing methods on monocular color images.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Rios-Cabrera, Reyes and Tuytelaars, Tinne},
	month = dec,
	year = {2013},
	note = {ISSN: 2380-7504},
	keywords = {3D, Clutter, Object Detection, Object detection, Support vector machines, Testing, Three-dimensional displays, Training, US Department of Transportation},
	pages = {2048--2055},
}

@inproceedings{wada_morefusion_2020,
	title = {{MoreFusion}: {Multi}-object {Reasoning} for {6D} {Pose} {Estimation} from {Volumetric} {Fusion}},
	shorttitle = {{MoreFusion}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wada_MoreFusion_Multi-object_Reasoning_for_6D_Pose_Estimation_from_Volumetric_Fusion_CVPR_2020_paper.html},
	urldate = {2022-10-06},
	author = {Wada, Kentaro and Sucar, Edgar and James, Stephen and Lenton, Daniel and Davison, Andrew J.},
	year = {2020},
	pages = {14540--14549},
}

@inproceedings{honda_real-time_1998,
	title = {Real-time pose estimation of an object manipulated by multi-fingered hand using {3D} stereo vision and tactile sensing},
	volume = {3},
	doi = {10.1109/IROS.1998.724860},
	abstract = {We describe a real-time model-based sensing system to estimate the pose of an object manipulated by a multifingered hand in a realistic environment. The system uses the least-squares method to estimate the pose, on the assumption that the approximate initial pose and geometric shapes of the object and fingertips are known. 3D measurement of features on the surface of the object is achieved by visual tracking with stereo camera using a template matching method. The primary features used for visual tracking are distinguishable texture patterns on the object surface. In complex environments, visual tracking can be failed due to the various occlusion. The system solves the occlusion problem by detecting the occlusion and using contact information of fingertips with the 3D visual information. Actual implementation of the system is explained together with results of experiments about the stability of tracking, the accuracy of estimation, and the processing time of the system.},
	booktitle = {Proceedings. 1998 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}. {Innovations} in {Theory}, {Practice} and {Applications} ({Cat}. {No}.{98CH36190})},
	author = {Honda, K. and Hasegawa, T. and Kiriki, T. and Matsuoka, T.},
	month = oct,
	year = {1998},
	keywords = {Cameras, Fingers, Information science, Layout, Machine vision, Real time systems, Shape measurement, Stability, Stereo vision, Surface texture},
	pages = {1814--1819 vol.3},
}

@inproceedings{haidacher_estimating_2003,
	title = {Estimating finger contact location and object pose from contact measurements in {3D} grasping},
	volume = {2},
	doi = {10.1109/ROBOT.2003.1241856},
	abstract = {Autonomously grasping a predefined object is a topic of recent research in the field of service robotics. On the one hand, there are numerous approaches in the area of image processing concerned with recognition and localization of this object. On the other hand, a lot of work has been done in the development of planning, approaching and grasping of the object with a dextrous manipulator mounted on top of a robot arm. However, in-between locating and grasping, there are significant sources of uncertainty, e.g. estimation errors in image processing, errors in calibration of cameras and robot alone and with respect to each other, and positioning errors in the robot control. During the critical closing phase of grasping however, visual servoing and position correction is almost impossible to achieve due to obstruction of the object by the gripper. This paper presents an algorithm to locally estimate the position and orientation of the object to be grasped from contact information and a geometric description of the object. In this scenario, an object description is usually available to a sufficiently accurate extent from grasp planning.},
	booktitle = {2003 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({Cat}. {No}.{03CH37422})},
	author = {Haidacher, S. and Hirzinger, G.},
	month = sep,
	year = {2003},
	note = {ISSN: 1050-4729},
	keywords = {Calibration, Error correction, Estimation error, Fingers, Grasping, Image processing, Image recognition, Manipulators, Robots, Uncertainty},
	pages = {1805--1810 vol.2},
}

@inproceedings{von_drigalski_precise_2021,
	title = {Precise {Multi}-{Modal} {In}-{Hand} {Pose} {Estimation} using {Low}-{Precision} {Sensors} for {Robotic} {Assembly}},
	doi = {10.1109/ICRA48506.2021.9561222},
	abstract = {In industrial assembly tasks, the in-hand pose of grasped objects needs to be known with high precision for subsequent manipulation tasks such as insertion. This problem (in-hand-pose estimation) has traditionally been addressed using visual recognition or tactile sensing. On the one hand, while visual recognition can provide efficient pose estimates, it tends to suffer from low precision due to noise, occlusions and calibration errors. On the other hand, tactile fingertip sensors can provide precise complementary information, but their low durability significantly limits their use in real-world applications. To get the best of both worlds, we propose an efficient method for in-hand pose estimation using off-the-shelf cameras and robot wrist force sensors, which requires no precise camera calibration. The key idea is to utilize visual and contact information adaptively to maximally reduce the uncertainty about the in-hand object pose in a Bayesian state estimation framework. As most of the uncertainty can be resolved from visual observations, our approach reduces the number of physical environment interactions while keeping a high pose estimation accuracy. Our experimental evaluation demonstrates that our approach can estimate object poses with sub-mm precision with an off-the-shelf camera and force-torque sensor.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {von Drigalski, Felix and Hayashi, Kennosuke and Huang, Yifei and Yonetani, Ryo and Hamaya, Masashi and Tanaka, Kazutoshi and Ijiri, Yoshihisa},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Pose estimation, Robot vision systems, Tactile sensors, Uncertainty, Visualization, Wrist},
	pages = {968--974},
}

@inproceedings{li_leveraging_2021,
	title = {Leveraging {SE}(3) {Equivariance} for {Self}-supervised {Category}-{Level} {Object} {Pose} {Estimation} from {Point} {Clouds}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html},
	abstract = {Category-level object pose estimation aims to find 6D object poses of previously unseen object instances from known categories without access to object CAD models. To reduce the huge amount of pose annotations needed for category-level learning, we propose for the first time a self-supervised learning framework to estimate category-level 6D object pose from single 3D point clouds. During training, our method assumes no ground-truth pose annotations, no CAD models, and no multi-view supervision. The key to our method is to disentangle shape and pose through an invariant shape reconstruction module and an equivariant pose estimation module, empowered by SE(3) equivariant point cloud networks. The invariant shape reconstruction module learns to perform aligned reconstructions, yielding a category-level reference frame without using any annotations. In addition, the equivariant pose estimation module achieves category-level pose estimation accuracy that is comparable to some fully supervised methods. Extensive experiments demonstrate the effectiveness of our approach on both complete and partial depth point clouds from the ModelNet40 benchmark, and on real depth point clouds from the NOCS-REAL 275 dataset. The project page with code and visualizations can be found at: dragonlong.github.io/equi-pose.},
	urldate = {2022-10-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Xiaolong and Weng, Yijia and Yi, Li and Guibas, Leonidas J and Abbott, A. and Song, Shuran and Wang, He},
	year = {2021},
	pages = {15370--15381},
}

@inproceedings{zhu_correspondence-free_2022,
	title = {Correspondence-{Free} {Point} {Cloud} {Registration} with {SO}(3)-{Equivariant} {Implicit} {Shape} {Representations}},
	url = {https://proceedings.mlr.press/v164/zhu22b.html},
	abstract = {This paper proposes a correspondence-free method for point cloud rotational registration. We learn an embedding for each point cloud in a feature space that preserves the SO(3)-equivariance property, enabled by recent developments in equivariant neural networks. The proposed shape registration method achieves three major advantages through combining equivariant feature learning with implicit shape models. First, the necessity of data association is removed because of the permutation-invariant property in network architectures similar to PointNet. Second, the registration in feature space can be solved in closed-form using Horn’s method due to the SO(3)-equivariance property. Third, the registration is robust to noise in the point cloud because of the joint training of registration and implicit shape reconstruction. The experimental results show superior performance compared with existing correspondence-free deep registration methods.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Zhu, Minghan and Ghaffari, Maani and Peng, Huei},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1412--1422},
}

@misc{thomas_tensor_2018,
	title = {Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds},
	shorttitle = {Tensor field networks},
	url = {http://arxiv.org/abs/1802.08219},
	doi = {10.48550/arXiv.1802.08219},
	abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
	month = may,
	year = {2018},
	note = {arXiv:1802.08219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{esteves_learning_2018,
	title = {Learning {SO}(3) {Equivariant} {Representations} with {Spherical} {CNNs}},
	url = {http://arxiv.org/abs/1711.06721},
	doi = {10.48550/arXiv.1711.06721},
	abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
	month = sep,
	year = {2018},
	note = {arXiv:1711.06721 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_equivariant_2021,
	title = {Equivariant {Point} {Network} for {3D} {Point} {Cloud} {Analysis}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Equivariant_Point_Network_for_3D_Point_Cloud_Analysis_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-10-03},
	author = {Chen, Haiwei and Liu, Shichen and Chen, Weikai and Li, Hao and Hill, Randall},
	year = {2021},
	pages = {14514--14523},
}

@misc{deng_vector_2021,
	title = {Vector {Neurons}: {A} {General} {Framework} for {SO}(3)-{Equivariant} {Networks}},
	shorttitle = {Vector {Neurons}},
	url = {http://arxiv.org/abs/2104.12229},
	doi = {10.48550/arXiv.2104.12229},
	abstract = {Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Deng, Congyue and Litany, Or and Duan, Yueqi and Poulenard, Adrien and Tagliasacchi, Andrea and Guibas, Leonidas},
	month = apr,
	year = {2021},
	note = {arXiv:2104.12229 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{yang_foldingnet_2018,
	title = {{FoldingNet}: {Point} {Cloud} {Auto}-{Encoder} via {Deep} {Grid} {Deformation}},
	shorttitle = {{FoldingNet}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.html},
	urldate = {2022-10-02},
	author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
	year = {2018},
	pages = {206--215},
}

@inproceedings{hirschmuller_accurate_2005,
	title = {Accurate and efficient stereo processing by semi-global matching and mutual information},
	volume = {2},
	doi = {10.1109/CVPR.2005.56},
	abstract = {This paper considers the objectives of accurate stereo matching, especially at object boundaries, robustness against recording or illumination changes and efficiency of the calculation. These objectives lead to the proposed semi-global matching method that performs pixelwise matching based on mutual information and the approximation of a global smoothness constraint. Occlusions are detected and disparities determined with sub-pixel accuracy. Additionally, an extension for multi-baseline stereo images is presented. There are two novel contributions. Firstly, a hierarchical calculation of mutual information based matching is shown, which is almost as fast as intensity based matching. Secondly, an approximation of a global cost calculation is proposed that can be performed in a time that is linear to the number of pixels and disparities. The implementation requires just 1 second on typical images.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Hirschmuller, H.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {Belief propagation, Costs, Geometry, Image reconstruction, Lighting, Mutual information, Pixel, Reflection, Robustness, Stereo vision},
	pages = {807--814 vol. 2},
}

@misc{bojarski_end_2016,
	title = {End to {End} {Learning} for {Self}-{Driving} {Cars}},
	url = {http://arxiv.org/abs/1604.07316},
	doi = {10.48550/arXiv.1604.07316},
	abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
	month = apr,
	year = {2016},
	note = {arXiv:1604.07316 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{amodei_deep_2016,
	title = {Deep {Speech} 2 : {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	shorttitle = {Deep {Speech} 2},
	url = {https://proceedings.mlr.press/v48/amodei16.html},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {173--182},
}

@misc{teed_tangent_2021,
	title = {Tangent {Space} {Backpropagation} for {3D} {Transformation} {Groups}},
	url = {http://arxiv.org/abs/2103.12032},
	doi = {10.48550/arXiv.2103.12032},
	abstract = {We address the problem of performing backpropagation for computation graphs involving 3D transformation groups SO(3), SE(3), and Sim(3). 3D transformation groups are widely used in 3D vision and robotics, but they do not form vector spaces and instead lie on smooth manifolds. The standard backpropagation approach, which embeds 3D transformations in Euclidean spaces, suffers from numerical difficulties. We introduce a new library, which exploits the group structure of 3D transformations and performs backpropagation in the tangent spaces of manifolds. We show that our approach is numerically more stable, easier to implement, and beneficial to a diverse set of tasks. Our plug-and-play PyTorch library is available at https://github.com/princeton-vl/lietorch.},
	urldate = {2022-10-01},
	publisher = {arXiv},
	author = {Teed, Zachary and Deng, Jia},
	month = mar,
	year = {2021},
	note = {arXiv:2103.12032 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gao_6d_2020,
	title = {{6D} {Object} {Pose} {Regression} via {Supervised} {Learning} on {Point} {Clouds}},
	doi = {10.1109/ICRA40945.2020.9197461},
	abstract = {This paper addresses the task of estimating the 6 degrees of freedom pose of a known 3D object from depth information represented by a point cloud. Deep features learned by convolutional neural networks from color information have been the dominant features to be used for inferring object poses, while depth information receives much less attention. However, depth information contains rich geometric information of the object shape, which is important for inferring the object pose. We use depth information represented by point clouds as the input to both deep networks and geometry-based pose refinement and use separate networks for rotation and translation regression. We argue that the axis-angle representation is a suitable rotation representation for deep learning, and use a geodesic loss function for rotation regression. Ablation studies show that these design choices outperform alternatives such as the quaternion representation and L2 loss, or regressing translation and rotation with the same network. Our simple yet effective approach clearly outperforms state-of-the-art methods on the YCB-video dataset.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Gao, Ge and Lauri, Mikko and Wang, Yulong and Hu, Xiaolin and Zhang, Jianwei and Frintrop, Simone},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Feature extraction, Image color analysis, Pose estimation, Quaternions, Rotation measurement, Supervised learning, Three-dimensional displays},
	pages = {3643--3649},
}

@inproceedings{yan_shapeformer_2022,
	title = {{ShapeFormer}: {Transformer}-{Based} {Shape} {Completion} via {Sparse} {Representation}},
	shorttitle = {{ShapeFormer}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Yan_ShapeFormer_Transformer-Based_Shape_Completion_via_Sparse_Representation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-09-30},
	author = {Yan, Xingguang and Lin, Liqiang and Mitra, Niloy J. and Lischinski, Dani and Cohen-Or, Daniel and Huang, Hui},
	year = {2022},
	pages = {6239--6249},
}

@misc{chen_unpaired_2020,
	title = {Unpaired {Point} {Cloud} {Completion} on {Real} {Scans} using {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1904.00069},
	doi = {10.48550/arXiv.1904.00069},
	abstract = {As 3D scanning solutions become increasingly popular, several deep learning setups have been developed geared towards that task of scan completion, i.e., plausibly filling in regions there were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, i.e., partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training data. We develop a first approach that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. We evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport, KITTI), quantitatively on 3D-EPN shape completion benchmark dataset, and demonstrate realistic completions under varying levels of incompleteness.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Chen, Xuelin and Chen, Baoquan and Mitra, Niloy J.},
	month = feb,
	year = {2020},
	note = {arXiv:1904.00069 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mittal_autosdf_2022-1,
	title = {{AutoSDF}: {Shape} {Priors} for {3D} {Completion}, {Reconstruction} and {Generation}},
	shorttitle = {{AutoSDF}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Mittal_AutoSDF_Shape_Priors_for_3D_Completion_Reconstruction_and_Generation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-09-30},
	author = {Mittal, Paritosh and Cheng, Yen-Chi and Singh, Maneesh and Tulsiani, Shubham},
	year = {2022},
	pages = {306--315},
}

@article{rustler_active_2022,
	title = {Active {Visuo}-{Haptic} {Object} {Shape} {Completion}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3152975},
	abstract = {Recent advancements in object shape completion have enabled impressive object reconstructions using only visual input. However, due to self-occlusion, the reconstructions have high uncertainty in the occluded object parts, which negatively impacts the performance of downstream robotic tasks such as grasping. In this letter, we propose an active visuo-haptic shape completion method called Act-VH that actively computes where to touch the objects based on the reconstruction uncertainty. Act-VH reconstructs objects from point clouds and calculates the reconstruction uncertainty using IGR, a recent state-of-the-art implicit surface deep neural network. We experimentally evaluate the reconstruction accuracy of Act-VH against five baselines in simulation and in the real world. We also propose a new simulation environment for this purpose. The results show that Act-VH outperforms all baselines and that an uncertainty-driven haptic exploration policy leads to higher reconstruction accuracy than a random policy and a policy driven by Gaussian Process Implicit Surfaces. As a final experiment, we evaluate Act-VH and the best reconstruction baseline on grasping 10 novel objects. The results show that Act-VH reaches a significantly higher grasp success rate than the baseline on all objects. Together, this letter opens up the door for using active visuo-haptic shape completion in more complex cluttered scenes.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rustler, Lukas and Lundell, Jens and Behrens, Jan Kristof and Kyrki, Ville and Hoffmann, Matej},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Deep learning for visual perception, Grasping, Image reconstruction, Point cloud compression, RGB-D perception, Robots, Shape, Surface reconstruction, Uncertainty, perception for grasping and manipulation},
	pages = {5254--5261},
}

@inproceedings{cai_learning_2022,
	title = {Learning a {Structured} {Latent} {Space} for {Unsupervised} {Point} {Cloud} {Completion}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Cai_Learning_a_Structured_Latent_Space_for_Unsupervised_Point_Cloud_Completion_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-09-30},
	author = {Cai, Yingjie and Lin, Kwan-Yee and Zhang, Chao and Wang, Qiang and Wang, Xiaogang and Li, Hongsheng},
	year = {2022},
	pages = {5543--5553},
}

@misc{yuan_pcn_2019,
	title = {{PCN}: {Point} {Completion} {Network}},
	shorttitle = {{PCN}},
	url = {http://arxiv.org/abs/1808.00671},
	doi = {10.48550/arXiv.1808.00671},
	abstract = {Shape completion, the problem of estimating the complete geometry of objects from partial observations, lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. Unlike existing shape completion methods, PCN directly operates on raw point clouds without any structural assumption (e.g. symmetry) or annotation (e.g. semantic class) about the underlying shape. It features a decoder design that enables the generation of fine-grained completions while maintaining a small number of parameters. Our experiments show that PCN produces dense, complete point clouds with realistic structures in the missing regions on inputs with various levels of incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Yuan, Wentao and Khot, Tejas and Held, David and Mertz, Christoph and Hebert, Martial},
	month = sep,
	year = {2019},
	note = {arXiv:1808.00671 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{gu_image_2020,
	title = {Image {Processing} {Using} {Multi}-{Code} {GAN} {Prior}},
	url = {http://arxiv.org/abs/1912.07116},
	doi = {10.48550/arXiv.1912.07116},
	abstract = {Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Gu, Jinjin and Shen, Yujun and Zhou, Bolei},
	month = mar,
	year = {2020},
	note = {arXiv:1912.07116 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_cascaded_2020,
	title = {Cascaded {Refinement} {Network} for {Point} {Cloud} {Completion}},
	url = {http://arxiv.org/abs/2004.03327},
	doi = {10.48550/arXiv.2004.03327},
	abstract = {Point clouds are often sparse and incomplete. Existing shape completion methods are incapable of generating details of objects or learning the complex point distributions. To this end, we propose a cascaded refinement network together with a coarse-to-fine strategy to synthesize the detailed object shapes. Considering the local details of partial input with the global shape information together, we can preserve the existing details in the incomplete point set and generate the missing parts with high fidelity. We also design a patch discriminator that guarantees every local area has the same pattern with the ground truth to learn the complicated point distribution. Quantitative and qualitative experiments on different datasets show that our method achieves superior results compared to existing state-of-the-art approaches on the 3D point cloud completion task. Our source code is available at https://github.com/xiaogangw/cascaded-point-completion.git.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Wang, Xiaogang and Ang Jr, Marcelo H. and Lee, Gim Hee},
	month = jun,
	year = {2020},
	note = {arXiv:2004.03327 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{gao_kpam-sc_2021,
	title = {{kPAM}-{SC}: {Generalizable} {Manipulation} {Planning} using {KeyPoint} {Affordance} and {Shape} {Completion}},
	shorttitle = {{kPAM}-{SC}},
	doi = {10.1109/ICRA48506.2021.9561428},
	abstract = {While traditional approaches to manipulation planning assume known object templates, recent approaches to "category-level manipulation" aim to manipulate a category of objects with potentially unknown instances and large intra-category shape variation. In this paper we explore an object representation to enable precise category-level manipulation, capturing a notion of the object configuration and extent, while being generalizable to novel instances. Building on our previous work, kPAM 1, we combine semantic keypoints with dense geometry (a point cloud or mesh) as the interface between the perception module and motion planner. Leveraging advances in learning-based keypoint detection and shape completion, both dense geometry and keypoints can be perceived from raw sensor input. Using the proposed hybrid object representation, we formulate the manipulation task as a motion planning problem which encodes both the object target configuration and physical feasibility for a category of objects. In this way, many existing manipulation planners can be generalized to categories of objects, and the resulting perception-to-action manipulation pipeline is robust to large intra-category shape variation. Extensive hardware experiments demonstrate our pipeline can produce robot trajectories that accomplish tasks with never-before-seen objects. The video demo is available on this link: https://sites.google.com/view/generalizable-manipulation.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Gao, Wei and Tedrake, Russ},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Geometry, Hardware, Pipelines, Pose estimation, Robot sensing systems, Semantics, Shape},
	pages = {6527--6533},
}

@inproceedings{arora_multimodal_2022,
	title = {Multimodal {Shape} {Completion} via {Implicit} {Maximum} {Likelihood} {Estimation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022W/DLGC/html/Arora_Multimodal_Shape_Completion_via_Implicit_Maximum_Likelihood_Estimation_CVPRW_2022_paper.html},
	language = {en},
	urldate = {2022-09-29},
	author = {Arora, Himanshu and Mishra, Saurabh and Peng, Shichong and Li, Ke and Mahdavi-Amiri, Ali},
	year = {2022},
	pages = {2958--2967},
}

@inproceedings{chi_garmentnets_2021,
	title = {{GarmentNets}: {Category}-{Level} {Pose} {Estimation} for {Garments} via {Canonical} {Space} {Shape} {Completion}},
	shorttitle = {{GarmentNets}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chi_GarmentNets_Category-Level_Pose_Estimation_for_Garments_via_Canonical_Space_Shape_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-09-29},
	author = {Chi, Cheng and Song, Shuran},
	year = {2021},
	pages = {3324--3333},
}

@inproceedings{wang_voxel-based_2021,
	title = {Voxel-{Based} {Network} for {Shape} {Completion} by {Leveraging} {Edge} {Generation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Voxel-Based_Network_for_Shape_Completion_by_Leveraging_Edge_Generation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-09-29},
	author = {Wang, Xiaogang and Ang, Marcelo H. and Lee, Gim Hee},
	year = {2021},
	pages = {13189--13198},
}

@inproceedings{zhang_unsupervised_2021,
	title = {Unsupervised {3D} {Shape} {Completion} {Through} {GAN} {Inversion}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Unsupervised_3D_Shape_Completion_Through_GAN_Inversion_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-09-29},
	author = {Zhang, Junzhe and Chen, Xinyi and Cai, Zhongang and Pan, Liang and Zhao, Haiyu and Yi, Shuai and Yeo, Chai Kiat and Dai, Bo and Loy, Chen Change},
	year = {2021},
	pages = {1768--1777},
}

@inproceedings{pereira_maskedfusion_2020,
	title = {{MaskedFusion}: {Mask}-based {6D} {Object} {Pose} {Estimation}},
	shorttitle = {{MaskedFusion}},
	doi = {10.1109/ICMLA51294.2020.00021},
	abstract = {MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3\% on average using the ADD metric on the LineMOD dataset and 93.3\% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods.},
	booktitle = {2020 19th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Pereira, Nuno and Alexandre, Luís A.},
	month = dec,
	year = {2020},
	keywords = {Artificial neural networks, Measurement, Pipelines, Pose estimation, Shape, Software development management, Training},
	pages = {71--78},
}

@misc{groueix_atlasnet_2018,
	title = {{AtlasNet}: {A} {Papier}-{M}{\textbackslash}{\textasciicircum}ach{\textbackslash}'e {Approach} to {Learning} {3D} {Surface} {Generation}},
	shorttitle = {{AtlasNet}},
	url = {http://arxiv.org/abs/1802.05384},
	doi = {10.48550/arXiv.1802.05384},
	abstract = {We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Groueix, Thibault and Fisher, Matthew and Kim, Vladimir G. and Russell, Bryan C. and Aubry, Mathieu},
	month = jul,
	year = {2018},
	note = {arXiv:1802.05384 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{tatarchenko_what_2019,
	title = {What {Do} {Single}-{View} {3D} {Reconstruction} {Networks} {Learn}?},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Tatarchenko_What_Do_Single-View_3D_Reconstruction_Networks_Learn_CVPR_2019_paper.html},
	urldate = {2022-09-28},
	author = {Tatarchenko, Maxim and Richter, Stephan R. and Ranftl, Rene and Li, Zhuwen and Koltun, Vladlen and Brox, Thomas},
	year = {2019},
	pages = {3405--3414},
}

@misc{wi_virdo_2022,
	title = {{VIRDO}: {Visio}-tactile {Implicit} {Representations} of {Deformable} {Objects}},
	shorttitle = {{VIRDO}},
	url = {http://arxiv.org/abs/2202.00868},
	doi = {10.48550/arXiv.2202.00868},
	abstract = {Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO:an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts.Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations,and iii) state-estimation with partial visio-tactile feedback},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Wi, Youngsun and Florence, Pete and Zeng, Andy and Fazeli, Nima},
	month = feb,
	year = {2022},
	note = {arXiv:2202.00868 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{gao_objectfolder_2021,
	title = {{ObjectFolder}: {A} {Dataset} of {Objects} with {Implicit} {Visual}, {Auditory}, and {Tactile} {Representations}},
	shorttitle = {{ObjectFolder}},
	url = {http://arxiv.org/abs/2109.07991},
	doi = {10.48550/arXiv.2109.07991},
	abstract = {Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Gao, Ruohan and Chang, Yen-Yu and Mall, Shivani and Fei-Fei, Li and Wu, Jiajun},
	month = nov,
	year = {2021},
	note = {arXiv:2109.07991 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{smith_active_2021,
	title = {Active {3D} {Shape} {Reconstruction} from {Vision} and {Touch}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/8635b5fd6bc675033fb72e8a3ccc10a0-Abstract.html},
	abstract = {Humans build 3D understandings of the world through active object exploration, using jointly their senses of vision and touch. However, in 3D shape reconstruction, most recent progress has relied on static datasets of limited sensory data such as RGB images, depth maps or haptic readings, leaving the active exploration of the shape largely unexplored. In active touch sensing for 3D reconstruction, the goal is to actively select the tactile readings that maximize the improvement in shape reconstruction accuracy. However, the development of deep learning-based active touch models is largely limited by the lack of frameworks for shape exploration. In this paper, we focus on this problem and introduce a system composed of: 1) a haptic simulator leveraging high spatial resolution vision-based tactile sensors for active touching of 3D objects; 2) a mesh-based 3D shape reconstruction model that relies on tactile or visuotactile signals; and 3) a set of data-driven solutions with either tactile or visuotactile priors to guide the shape exploration. Our framework enables the development of the first fully data-driven solutions to active touch on top of learned models for object understanding. Our experiments show the benefits of such solutions in the task of 3D shape understanding where our models consistently outperform natural baselines. We provide our framework as a tool to foster future research in this direction.},
	urldate = {2022-09-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Smith, Edward and Meger, David and Pineda, Luis and Calandra, Roberto and Malik, Jitendra and Romero Soriano, Adriana and Drozdzal, Michal},
	year = {2021},
	pages = {16064--16078},
}

@article{lee_making_2020,
	title = {Making {Sense} of {Vision} and {Touch}: {Learning} {Multimodal} {Representations} for {Contact}-{Rich} {Tasks}},
	volume = {36},
	issn = {1941-0468},
	shorttitle = {Making {Sense} of {Vision} and {Touch}},
	doi = {10.1109/TRO.2019.2959445},
	abstract = {Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. It is nontrivial to manually design a robot controller that combines these modalities, which have very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to train directly on real robots due to sample complexity. In this article, we use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. Evaluating our method on a peg insertion task, we show that it generalizes over varying geometries, configurations, and clearances, while being robust to external perturbations. We also systematically study different self-supervised learning objectives and representation learning architectures. Results are presented in simulation and on a physical robot.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Lee, Michelle A. and Zhu, Yuke and Zachares, Peter and Tan, Matthew and Srinivasan, Krishnan and Savarese, Silvio and Fei-Fei, Li and Garg, Animesh and Bohg, Jeannette},
	month = jun,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Deep learning in robotics and automation, Haptic interfaces, Reinforcement learning, Robot sensing systems, Solid modeling, Task analysis, Visualization, perception for grasping and manipulation, sensor fusion, sensor-based control},
	pages = {582--596},
}

@inproceedings{ghosh_joint_2017,
	title = {Joint perception and planning for efficient obstacle avoidance using stereo vision},
	doi = {10.1109/IROS.2017.8202271},
	abstract = {Stereo vision is commonly used for local obstacle avoidance of autonomous mobile robots: stereo images are first processed to yield a dense 3D reconstruction of the observed scene, which is then used for navigation planning. Such an approach, which we term Sequential Perception and Planning (SPP), results in significant unnecessary computations as the navigation planner only needs to explore a small part of the scene to compute the shortest obstacle-free path. In this paper, we introduce an approach to Joint Perception and Planning (JPP) using stereo vision, which performs disparity checks on demand, only as necessary while searching on a planning graph. Furthermore, obstacle checks for navigation planning do not require full 3D reconstruction: we present in this paper how obstacle queries can be decomposed into a sequence of confident positive stereo matches and confident negative stereo matches, which are significantly faster to compute than the exact depth of points. The resulting complete JPP formulation is significantly faster than SPP, while still maintaining correctness of planning. We also show how the JPP works with different planners, including search-based and sampling-based planners. We present extensive experimental results from real robot data and simulation experiments, demonstrating that the JPP requires less than 10\% of the disparity computations required by SPP.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Ghosh, Sourish and Biswas, Joydeep},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Collision avoidance, Planning, Robot kinematics, Robot sensing systems, Stereo vision},
	pages = {1026--1031},
}

@misc{murali_active_2021,
	title = {Active {Visuo}-{Tactile} {Point} {Cloud} {Registration} for {Accurate} {Pose} {Estimation} of {Objects} in an {Unknown} {Workspace}},
	url = {http://arxiv.org/abs/2108.04015},
	doi = {10.48550/arXiv.2108.04015},
	abstract = {This paper proposes a novel active visuo-tactile based methodology wherein the accurate estimation of the time-invariant SE(3) pose of objects is considered for autonomous robotic manipulators. The robot equipped with tactile sensors on the gripper is guided by a vision estimate to actively explore and localize the objects in the unknown workspace. The robot is capable of reasoning over multiple potential actions, and execute the action to maximize information gain to update the current belief of the object. We formulate the pose estimation process as a linear translation invariant quaternion filter (TIQF) by decoupling the estimation of translation and rotation and formulating the update and measurement model in linear form. We perform pose estimation sequentially on acquired measurements using very sparse point cloud as acquiring each measurement using tactile sensing is time consuming. Furthermore, our proposed method is computationally efficient to perform an exhaustive uncertainty-based active touch selection strategy in real-time without the need for trading information gain with execution time. We evaluated the performance of our approach extensively in simulation and by a robotic system.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Murali, Prajval Kumar and Gentner, Michael and Kaboli, Mohsen},
	month = aug,
	year = {2021},
	note = {arXiv:2108.04015 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{murali_deep_2022,
	title = {Deep {Active} {Cross}-{Modal} {Visuo}-{Tactile} {Transfer} {Learning} for {Robotic} {Object} {Recognition}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3191408},
	abstract = {We propose for the first time, a novel deep active visuo-tactile cross-modal full-fledged framework for object recognition by autonomous robotic systems. Our proposed network xAVTNet is actively trained with labelled point clouds from a vision sensor with one robot and tested with an active tactile perception strategy to recognise objects never touched before using another robot. We propose a novel visuo-tactile loss (VTLoss) to minimise the discrepancy between the visual and tactile domains for unsupervised domain adaptation. Our framework leverages the strengths of deep neural networks for cross-modal recognition along with active perception and active learning strategies for increased efficiency by minimising redundant data collection. Our method is extensively evaluated on a real robotic system and compared against baselines and other state-of-art approaches. We demonstrate clear outperformance in recognition accuracy compared to the state-of-art visuo-tactile cross-modal recognition method.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Murali, Prajval Kumar and Wang, Cong and Lee, Dongheui and Dahiya, Ravinder and Kaboli, Mohsen},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Active visuo-tactile object recognition, Object recognition, Point cloud compression, Robot sensing systems, Robots, Training, Vision sensors, Visualization, perception for grasping and manipulation, transfer learning, visuo-tactile cross-modal learning},
	pages = {9557--9564},
}

@article{murali_active_2022,
	title = {Active {Visuo}-{Tactile} {Interactive} {Robotic} {Perception} for {Accurate} {Object} {Pose} {Estimation} in {Dense} {Clutter}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/2202.02207},
	doi = {10.1109/LRA.2022.3150045},
	abstract = {This work presents a novel active visuo-tactile based framework for robotic systems to accurately estimate pose of objects in dense cluttered environments. The scene representation is derived using a novel declutter graph (DG) which describes the relationship among objects in the scene for decluttering by leveraging semantic segmentation and grasp affordances networks. The graph formulation allows robots to efficiently declutter the workspace by autonomously selecting the next best object to remove and the optimal action (prehensile or non-prehensile) to perform. Furthermore, we propose a novel translation-invariant Quaternion filter (TIQF) for active vision and active tactile based pose estimation. Both active visual and active tactile points are selected by maximizing the expected information gain. We evaluate our proposed framework on a system with two robots coordinating on randomized scenes of dense cluttered objects and perform ablation studies with static vision and active vision based estimation prior and post decluttering as baselines. Our proposed active visuo-tactile interactive perception framework shows upto 36\% improvement in pose accuracy compared to the active vision baseline.},
	number = {2},
	urldate = {2022-09-23},
	journal = {IEEE Robotics and Automation Letters},
	author = {Murali, Prajval Kumar and Dutta, Anirvan and Gentner, Michael and Burdet, Etienne and Dahiya, Ravinder and Kaboli, Mohsen},
	month = apr,
	year = {2022},
	note = {arXiv:2202.02207 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {4686--4693},
}

@article{zhang_aerial_2022,
	title = {Aerial additive manufacturing with multiple autonomous robots},
	volume = {609},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-04988-4},
	doi = {10.1038/s41586-022-04988-4},
	abstract = {Additive manufacturing methods1–4 using static and mobile robots are being developed for both on-site construction5–8 and off-site prefabrication9,10. Here we introduce a method of additive manufacturing, referred to as aerial additive manufacturing (Aerial-AM), that utilizes a team of aerial robots inspired by natural builders11 such as wasps who use collective building methods12,13. We present a scalable multi-robot three-dimensional (3D) printing and path-planning framework that enables robot tasks and population size to be adapted to variations in print geometry throughout a building mission. The multi-robot manufacturing framework allows for autonomous three-dimensional printing under human supervision, real-time assessment of printed geometry and robot behavioural adaptation. To validate autonomous Aerial-AM based on the framework, we develop BuilDrones for depositing materials during flight and ScanDrones for measuring the print quality, and integrate a generic real-time model-predictive-control scheme with the Aerial-AM robots. In addition, we integrate a dynamically self-aligning delta manipulator with the BuilDrone to further improve the manufacturing accuracy to five millimetres for printing geometry with precise trajectory requirements, and develop four cementitious–polymeric composite mixtures suitable for continuous material deposition. We demonstrate proof-of-concept prints including a cylinder 2.05 metres high consisting of 72 layers of a rapid-curing insulation foam material and a cylinder 0.18 metres high consisting of 28 layers of structural pseudoplastic cementitious material, a light-trail virtual print of a dome-like geometry, and multi-robot simulations. Aerial-AM allows manufacturing in-flight and offers future possibilities for building in unbounded, at-height or hard-to-access locations.},
	language = {en},
	number = {7928},
	urldate = {2022-09-22},
	journal = {Nature},
	author = {Zhang, Ketao and Chermprayong, Pisak and Xiao, Feng and Tzoumanikas, Dimos and Dams, Barrie and Kay, Sebastian and Kocer, Basaran Bahadir and Burns, Alec and Orr, Lachlan and Choi, Christopher and Darekar, Durgesh Dattatray and Li, Wenbin and Hirschmann, Steven and Soana, Valentina and Ngah, Shamsiah Awang and Sareh, Sina and Choubey, Ashutosh and Margheri, Laura and Pawar, Vijay M. and Ball, Richard J. and Williams, Chris and Shepherd, Paul and Leutenegger, Stefan and Stuart-Smith, Robert and Kovac, Mirko},
	month = sep,
	year = {2022},
	keywords = {Aerospace engineering, Composites, Computational science},
	pages = {709--717},
}

@article{jiang_layercam_2021,
	title = {{LayerCAM}: {Exploring} {Hierarchical} {Class} {Activation} {Maps} for {Localization}},
	volume = {30},
	issn = {1941-0042},
	shorttitle = {{LayerCAM}},
	doi = {10.1109/TIP.2021.3089943},
	abstract = {The class activation maps are generated from the final convolutional layer of CNN. They can highlight discriminative object regions for the class of interest. These discovered object regions have been widely used for weakly-supervised tasks. However, due to the small spatial resolution of the final convolutional layer, such class activation maps often locate coarse regions of the target objects, limiting the performance of weakly-supervised tasks that need pixel-accurate object locations. Thus, we aim to generate more fine-grained object localization information from the class activation maps to locate the target objects more accurately. In this paper, by rethinking the relationships between the feature maps and their corresponding gradients, we propose a simple yet effective method, called LayerCAM. It can produce reliable class activation maps for different layers of CNN. This property enables us to collect object localization information from coarse (rough spatial localization) to fine (precise fine-grained details) levels. We further integrate them into a high-quality class activation map, where the object-related pixels can be better highlighted. To evaluate the quality of the class activation maps produced by LayerCAM, we apply them to weakly-supervised object localization and semantic segmentation. Experiments demonstrate that the class activation maps generated by our method are more effective and reliable than those by the existing attention methods. The code will be made publicly available.},
	journal = {IEEE Transactions on Image Processing},
	author = {Jiang, Peng-Tao and Zhang, Chang-Bin and Hou, Qibin and Cheng, Ming-Ming and Wei, Yunchao},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Convolution, Image segmentation, Location awareness, Reliability, Semantics, Spatial resolution, Task analysis, Weakly-supervised object localization, class activation maps},
	pages = {5875--5888},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2022-09-22},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {336--359},
}

@misc{zhou_learning_2015,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {http://arxiv.org/abs/1512.04150},
	doi = {10.48550/arXiv.1512.04150},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
	urldate = {2022-08-29},
	publisher = {arXiv},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = dec,
	year = {2015},
	note = {arXiv:1512.04150 [cs]},
	keywords = {CAM, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{abnar_quantifying_2020,
	title = {Quantifying {Attention} {Flow} in {Transformers}},
	url = {http://arxiv.org/abs/2005.00928},
	doi = {10.48550/arXiv.2005.00928},
	abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Abnar, Samira and Zuidema, Willem},
	month = may,
	year = {2020},
	note = {arXiv:2005.00928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{kelestemur_tactile_2022,
	title = {Tactile {Pose} {Estimation} and {Policy} {Learning} for {Unknown} {Object} {Manipulation}},
	url = {https://arxiv.org/abs/2203.10685v1},
	doi = {10.48550/arXiv.2203.10685},
	abstract = {Object pose estimation methods allow finding locations of objects in unstructured environments. This is a highly desired skill for autonomous robot manipulation as robots need to estimate the precise poses of the objects in order to manipulate them. In this paper, we investigate the problems of tactile pose estimation and manipulation for category-level objects. Our proposed method uses a Bayes filter with a learned tactile observation model and a deterministic motion model. Later, we train policies using deep reinforcement learning where the agents use the belief estimation from the Bayes filter. Our models are trained in simulation and transferred to the real world. We analyze the reliability and the performance of our framework through a series of simulated and real-world experiments and compare our method to the baseline work. Our results show that the learned tactile observation model can localize the pose of novel objects at 2-mm and 1-degree resolution for position and orientation, respectively. Furthermore, we experiment on a bottle opening task where the gripper needs to reach the desired grasp state.},
	language = {en},
	urldate = {2022-09-22},
	author = {Kelestemur, Tarik and Platt, Robert and Padir, Taskin},
	month = mar,
	year = {2022},
}

@article{vizzo_make_2022,
	title = {Make it {Dense}: {Self}-{Supervised} {Geometric} {Scan} {Completion} of {Sparse} {3D} {LiDAR} {Scans} in {Large} {Outdoor} {Environments}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {Make it {Dense}},
	doi = {10.1109/LRA.2022.3187255},
	abstract = {Mapping systems that turn sensor data into a model of the environment are standard components in mobile robotics. Outdoor robots are often equipped with 3D LiDAR sensors to obtain accurate range measurements at a high frame rate. The price for a robotic LiDAR sensor scales roughly linearly with the number of beams and thus the vertical resolution of the scanner. In general, the cheaper the sensors, the sparser the point cloud. In this letter, we address the problem of building dense models from sparse range data. Instead of requiring the vehicle to move slowly through the environment or to traverse the scene multiple times to cover the space densely, we investigate geometric scan completion through a learning-based approach. We revisit the traditional volumetric fusion pipeline based on truncated signed distance fields (TSDF) and propose a neural network to aid the 3D reconstruction on a frame-to-frame basis by completing each scan towards a dense TSDF volume. We propose a geometric scan completion network that is trained in a self-supervised fashion without labels. Our experiments illustrate that such frame-wise completion leads to maps that are on-par or even better compared to maps generated using a higher resolution LiDAR sensor. We additionally show that our system can be used to improve the performance of SLAM systems.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Vizzo, Ignacio and Mersch, Benedikt and Marcuzzi, Rodrigo and Wiesmann, Louis and Behley, Jens and Stachniss, Cyrill},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {AI-enabled robotics, Laser radar, Mapping, Pipelines, Robot sensing systems, Robots, Semantics, Simultaneous localization and mapping, Three-dimensional displays},
	pages = {8534--8541},
}

@misc{cordonnier_relationship_2020,
	title = {On the {Relationship} between {Self}-{Attention} and {Convolutional} {Layers}},
	url = {http://arxiv.org/abs/1911.03584},
	doi = {10.48550/arXiv.1911.03584},
	abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	month = jan,
	year = {2020},
	note = {arXiv:1911.03584 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mersch_receding_2022,
	title = {Receding {Moving} {Object} {Segmentation} in {3D} {LiDAR} {Data} {Using} {Sparse} {4D} {Convolutions}},
	url = {http://arxiv.org/abs/2206.04129},
	doi = {10.48550/arXiv.2206.04129},
	abstract = {A key challenge for autonomous vehicles is to navigate in unseen dynamic environments. Separating moving objects from static ones is essential for navigation, pose estimation, and understanding how other traffic participants are likely to move in the near future. In this work, we tackle the problem of distinguishing 3D LiDAR points that belong to currently moving objects, like walking pedestrians or driving cars, from points that are obtained from non-moving objects, like walls but also parked cars. Our approach takes a sequence of observed LiDAR scans and turns them into a voxelized sparse 4D point cloud. We apply computationally efficient sparse 4D convolutions to jointly extract spatial and temporal features and predict moving object confidence scores for all points in the sequence. We develop a receding horizon strategy that allows us to predict moving objects online and to refine predictions on the go based on new observations. We use a binary Bayes filter to recursively integrate new predictions of a scan resulting in more robust estimation. We evaluate our approach on the SemanticKITTI moving object segmentation challenge and show more accurate predictions than existing methods. Since our approach only operates on the geometric information of point clouds over time, it generalizes well to new, unseen environments, which we evaluate on the Apollo dataset.},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Mersch, Benedikt and Chen, Xieyuanli and Vizzo, Ignacio and Nunes, Lucas and Behley, Jens and Stachniss, Cyrill},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04129 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{attia_global_2018,
	title = {Global overview of {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1801.06503},
	doi = {10.48550/arXiv.1801.06503},
	abstract = {Imitation Learning is a sequential task where the learner tries to mimic an expert's action in order to achieve the best performance. Several algorithms have been proposed recently for this task. In this project, we aim at proposing a wide review of these algorithms, presenting their main features and comparing them on their performance and their regret bounds.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Attia, Alexandre and Dayan, Sharone},
	month = jan,
	year = {2018},
	note = {arXiv:1801.06503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ross_efficient_2010,
	title = {Efficient {Reductions} for {Imitation} {Learning}},
	url = {https://proceedings.mlr.press/v9/ross10a.html},
	abstract = {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.},
	language = {en},
	urldate = {2022-09-08},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Ross, Stephane and Bagnell, Drew},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {661--668},
}

@misc{mandlekar_human---loop_2020,
	title = {Human-in-the-{Loop} {Imitation} {Learning} using {Remote} {Teleoperation}},
	url = {http://arxiv.org/abs/2012.06733},
	doi = {10.48550/arXiv.2012.06733},
	abstract = {Imitation Learning is a promising paradigm for learning complex robot manipulation skills by reproducing behavior from human demonstrations. However, manipulation tasks often contain bottleneck regions that require a sequence of precise actions to make meaningful progress, such as a robot inserting a pod into a coffee machine to make coffee. Trained policies can fail in these regions because small deviations in actions can lead the policy into states not covered by the demonstrations. Intervention-based policy learning is an alternative that can address this issue -- it allows human operators to monitor trained policies and take over control when they encounter failures. In this paper, we build a data collection system tailored to 6-DoF manipulation settings, that enables remote human operators to monitor and intervene on trained policies. We develop a simple and effective algorithm to train the policy iteratively on new data collected by the system that encourages the policy to learn how to traverse bottlenecks through the interventions. We demonstrate that agents trained on data collected by our intervention-based system and algorithm outperform agents trained on an equivalent number of samples collected by non-interventional demonstrators, and further show that our method outperforms multiple state-of-the-art baselines for learning from the human interventions on a challenging robot threading task and a coffee making task. Additional results and videos at https://sites.google.com/stanford.edu/iwr .},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Mandlekar, Ajay and Xu, Danfei and Martín-Martín, Roberto and Zhu, Yuke and Fei-Fei, Li and Savarese, Silvio},
	month = dec,
	year = {2020},
	note = {arXiv:2012.06733 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{spencer_learning_2020,
	title = {Learning from {Interventions}: {Human}-robot interaction as both explicit and implicit feedback},
	shorttitle = {Learning from {Interventions}},
	url = {https://collaborate.princeton.edu/en/publications/learning-from-interventions-human-robot-interaction-as-both-expli},
	doi = {10.15607/RSS.2020.XVI.055},
	language = {English (US)},
	urldate = {2022-09-08},
	booktitle = {Robotics: {Science} and {Systems} {XVI}},
	publisher = {MIT Press Journals},
	author = {Spencer, Jonathan and Choudhury, Sanjiban and Barnes, Matthew and Schmittle, Matthew and Chiang, Mung and Ramadge, Peter and Srinivasa, Siddhartha},
	year = {2020},
}

@misc{kelly_hg-dagger_2019,
	title = {{HG}-{DAgger}: {Interactive} {Imitation} {Learning} with {Human} {Experts}},
	shorttitle = {{HG}-{DAgger}},
	url = {http://arxiv.org/abs/1810.02890},
	doi = {10.48550/arXiv.1810.02890},
	abstract = {Imitation learning has proven to be useful for many real-world problems, but approaches such as behavioral cloning suffer from data mismatch and compounding error issues. One attempt to address these limitations is the DAgger algorithm, which uses the state distribution induced by the novice to sample corrective actions from the expert. Such sampling schemes, however, require the expert to provide action labels without being fully in control of the system. This can decrease safety and, when using humans as experts, is likely to degrade the quality of the collected labels due to perceived actuator lag. In this work, we propose HG-DAgger, a variant of DAgger that is more suitable for interactive imitation learning from human experts in real-world systems. In addition to training a novice policy, HG-DAgger also learns a safety threshold for a model-uncertainty-based risk metric that can be used to predict the performance of the fully trained novice in different regions of the state space. We evaluate our method on both a simulated and real-world autonomous driving task, and demonstrate improved performance over both DAgger and behavioral cloning.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Kelly, Michael and Sidrane, Chelsea and Driggs-Campbell, Katherine and Kochenderfer, Mykel J.},
	month = mar,
	year = {2019},
	note = {arXiv:1810.02890 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{mandlekar_what_2021,
	title = {What {Matters} in {Learning} from {Offline} {Human} {Demonstrations} for {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2108.03298},
	doi = {10.48550/arXiv.2108.03298},
	abstract = {Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at https://arise-initiative.github.io/robomimic-web/},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Mandlekar, Ajay and Xu, Danfei and Wong, Josiah and Nasiriany, Soroush and Wang, Chen and Kulkarni, Rohun and Fei-Fei, Li and Savarese, Silvio and Zhu, Yuke and Martín-Martín, Roberto},
	month = sep,
	year = {2021},
	note = {arXiv:2108.03298 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{de_haan_causal_2019,
	title = {Causal {Confusion} in {Imitation} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html},
	abstract = {Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.},
	urldate = {2022-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
	year = {2019},
}

@misc{liu_search-based_2017,
	title = {Search-based {Motion} {Planning} for {Aggressive} {Flight} in {SE}(3)},
	url = {http://arxiv.org/abs/1710.02748},
	doi = {10.48550/arXiv.1710.02748},
	abstract = {Quadrotors with large thrust-to-weight ratios are able to track aggressive trajectories with sharp turns and high accelerations. In this work, we develop a search-based trajectory planning approach that exploits the quadrotor maneuverability to generate sequences of motion primitives in cluttered environments. We model the quadrotor body as an ellipsoid and compute its flight attitude along trajectories in order to check for collisions against obstacles. The ellipsoid model allows the quadrotor to pass through gaps that are smaller than its diameter with non-zero pitch or roll angles. Without any prior information about the location of gaps and associated attitude constraints, our algorithm is able to find a safe and optimal trajectory that guides the robot to its goal as fast as possible. To accelerate planning, we first perform a lower dimensional search and use it as a heuristic to guide the generation of a final dynamically feasible trajectory. We analyze critical discretization parameters of motion primitive planning and demonstrate the feasibility of the generated trajectories in various simulations and real-world experiments.},
	urldate = {2022-09-04},
	publisher = {arXiv},
	author = {Liu, Sikang and Mohta, Kartik and Atanasov, Nikolay and Kumar, Vijay},
	month = oct,
	year = {2017},
	note = {arXiv:1710.02748 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{zhang_perception-aware_2018,
	title = {Perception-aware {Receding} {Horizon} {Navigation} for {MAVs}},
	doi = {10.1109/ICRA.2018.8461133},
	abstract = {To reach a given destination safely and accurately, a micro aerial vehicle needs to be able to avoid obstacles and minimize its state estimation uncertainty at the same time. To achieve this goal, we propose a perception-aware receding horizon approach. In our method, a single forward-looking camera is used for state estimation and mapping. Using the information from the monocular state estimation and mapping system, we generate a library of candidate trajectories and evaluate them in terms of perception quality, collision probability, and distance to the goal. The best trajectory to execute is then selected as the one that maximizes a reward function based on these three metrics. To the best of our knowledge, this is the first work that integrates active vision within a receding horizon navigation framework for a goal reaching task. We demonstrate by simulation and real-world experiments on an actual quadrotor that our active approach leads to improved state estimation accuracy in a goal-reaching task when compared to a purely-reactive navigation system, especially in difficult scenes (e.g., weak texture).},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, Zichao and Scaramuzza, Davide},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Measurement, Navigation, Planning, State estimation, Task analysis, Trajectory},
	pages = {2534--2541},
}

@inproceedings{badki_binary_2021,
	title = {Binary {TTC}: {A} {Temporal} {Geofence} for {Autonomous} {Navigation}},
	shorttitle = {Binary {TTC}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Badki_Binary_TTC_A_Temporal_Geofence_for_Autonomous_Navigation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-09-04},
	author = {Badki, Abhishek and Gallo, Orazio and Kautz, Jan and Sen, Pradeep},
	year = {2021},
	pages = {12946--12955},
}

@misc{badki_bi3d_2020,
	title = {{Bi3D}: {Stereo} {Depth} {Estimation} via {Binary} {Classifications}},
	shorttitle = {{Bi3D}},
	url = {http://arxiv.org/abs/2005.07274},
	doi = {10.48550/arXiv.2005.07274},
	abstract = {Stereo-based depth estimation is a cornerstone of computer vision, with state-of-the-art methods delivering accurate results in real time. For several applications such as autonomous navigation, however, it may be useful to trade accuracy for lower latency. We present Bi3D, a method that estimates depth via a series of binary classifications. Rather than testing if objects are at a particular depth \$D\$, as existing stereo methods do, it classifies them as being closer or farther than \$D\$. This property offers a powerful mechanism to balance accuracy and latency. Given a strict time budget, Bi3D can detect objects closer than a given distance in as little as a few milliseconds, or estimate depth with arbitrarily coarse quantization, with complexity linear with the number of quantization levels. Bi3D can also use the allotted quantization levels to get continuous depth, but in a specific depth range. For standard stereo (i.e., continuous depth on the whole range), our method is close to or on par with state-of-the-art, finely tuned stereo methods.},
	urldate = {2022-09-04},
	publisher = {arXiv},
	author = {Badki, Abhishek and Troccoli, Alejandro and Kim, Kihwan and Kautz, Jan and Sen, Pradeep and Gallo, Orazio},
	month = jun,
	year = {2020},
	note = {arXiv:2005.07274 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{loquercio_deep_2020,
	title = {Deep {Drone} {Racing}: {From} {Simulation} to {Reality} with {Domain} {Randomization}},
	volume = {36},
	issn = {1552-3098, 1941-0468},
	shorttitle = {Deep {Drone} {Racing}},
	url = {http://arxiv.org/abs/1905.09727},
	doi = {10.1109/TRO.2019.2942989},
	abstract = {Dynamically changing environments, unreliable state estimation, and operation under severe resource constraints are fundamental challenges that limit the deployment of small autonomous drones. We address these challenges in the context of autonomous, vision-based drone racing in dynamic environments. A racing drone must traverse a track with possibly moving gates at high speed. We enable this functionality by combining the performance of a state-of-the-art planning and control system with the perceptual awareness of a convolutional neural network (CNN). The resulting modular system is both platform- and domain-independent: it is trained in simulation and deployed on a physical quadrotor without any fine-tuning. The abundance of simulated data, generated via domain randomization, makes our system robust to changes of illumination and gate appearance. To the best of our knowledge, our approach is the first to demonstrate zero-shot sim-to-real transfer on the task of agile drone flight. We extensively test the precision and robustness of our system, both in simulation and on a physical platform, and show significant improvements over the state of the art.},
	number = {1},
	urldate = {2022-09-04},
	journal = {IEEE Transactions on Robotics},
	author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, René and Dosovitskiy, Alexey and Koltun, Vladlen and Scaramuzza, Davide},
	month = feb,
	year = {2020},
	note = {arXiv:1905.09727 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1--14},
}

@article{foehn_agilicious_2022,
	title = {Agilicious: {Open}-source and open-hardware agile quadrotor for vision-based flight},
	volume = {7},
	issn = {2470-9476},
	shorttitle = {Agilicious},
	url = {https://www.science.org/doi/10.1126/scirobotics.abl6259},
	doi = {10.1126/scirobotics.abl6259},
	abstract = {Autonomous, agile quadrotor flight raises fundamental challenges for robotics research in terms of perception, planning, learning, and control. A versatile and standardized platform is needed to accelerate research and let practitioners focus on the core problems. To this end, we present Agilicious, a codesigned hardware and software framework tailored to autonomous, agile quadrotor flight. It is completely open source and open hardware and supports both model-based and neural network–based controllers. Also, it provides high thrust-to-weight and torque-to-inertia ratios for agility, onboard vision sensors, graphics processing unit (GPU)–accelerated compute hardware for real-time perception and neural network inference, a real-time flight controller, and a versatile software stack. In contrast to existing frameworks, Agilicious offers a unique combination of flexible software stack and high-performance hardware. We compare Agilicious with prior works and demonstrate it on different agile tasks, using both model-based and neural network–based controllers. Our demonstrators include trajectory tracking at up to 5
              g
              and 70 kilometers per hour in a motion capture system, and vision-based acrobatic flight and obstacle avoidance in both structured and unstructured environments using solely onboard perception. Last, we demonstrate its use for hardware-in-the-loop simulation in virtual reality environments. Because of its versatility, we believe that Agilicious supports the next generation of scientific and industrial quadrotor research.
            
          , 
            We provide a codesigned hardware and software framework tailored to autonomous, agile quadrotor flight.},
	language = {en},
	number = {67},
	urldate = {2022-09-04},
	journal = {Science Robotics},
	author = {Foehn, Philipp and Kaufmann, Elia and Romero, Angel and Penicka, Robert and Sun, Sihao and Bauersfeld, Leonard and Laengle, Thomas and Cioffi, Giovanni and Song, Yunlong and Loquercio, Antonio and Scaramuzza, Davide},
	month = jun,
	year = {2022},
	pages = {eabl6259},
}

@article{adamkiewicz_vision-only_2022,
	title = {Vision-{Only} {Robot} {Navigation} in a {Neural} {Radiance} {World}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3150497},
	abstract = {Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. Neural Radiance Fields (NeRFs) represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot’s objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot’s full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through a narrow gap.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Adamkiewicz, Michal and Chen, Timothy and Caccavale, Adam and Gardner, Rachel and Culbertson, Preston and Bohg, Jeannette and Schwager, Mac},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Collision avoidance, Navigation, Pipelines, Planning, Robot vision systems, Robots, localization, motion and path planning, neural radiance fields, vision-based navigation},
	pages = {4606--4613},
}

@misc{song_flightmare_2021,
	title = {Flightmare: {A} {Flexible} {Quadrotor} {Simulator}},
	shorttitle = {Flightmare},
	url = {http://arxiv.org/abs/2009.00563},
	doi = {10.48550/arXiv.2009.00563},
	abstract = {State-of-the-art quadrotor simulators have a rigid and highly-specialized structure: either are they really fast, physically accurate, or photo-realistic. In this work, we propose a novel quadrotor simulator: Flightmare. Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently of each other. This makes our simulator extremely fast: rendering achieves speeds of up to 230 Hz, while physics simulation of up to 200,000 Hz on a laptop. In addition, Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) integration with a virtual-reality headset for interaction with the simulated environment. We demonstrate the flexibility of Flightmare by using it for two different robotic tasks: quadrotor control using deep reinforcement learning and collision-free path planning in a complex 3D environment.},
	urldate = {2022-09-03},
	publisher = {arXiv},
	author = {Song, Yunlong and Naji, Selim and Kaufmann, Elia and Loquercio, Antonio and Scaramuzza, Davide},
	month = may,
	year = {2021},
	note = {arXiv:2009.00563 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@misc{kaufmann_deep_2020,
	title = {Deep {Drone} {Acrobatics}},
	url = {http://arxiv.org/abs/2006.05768},
	doi = {10.48550/arXiv.2006.05768},
	abstract = {Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3g.},
	urldate = {2022-09-03},
	publisher = {arXiv},
	author = {Kaufmann, Elia and Loquercio, Antonio and Ranftl, René and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	month = jun,
	year = {2020},
	note = {arXiv:2006.05768 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{guzman-rivera_multiple_2012,
	title = {Multiple {Choice} {Learning}: {Learning} to {Produce} {Multiple} {Structured} {Outputs}},
	volume = {25},
	shorttitle = {Multiple {Choice} {Learning}},
	url = {https://papers.nips.cc/paper/2012/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html},
	abstract = {The paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade. Given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one. The standard approach for handling this scenario is to learn a single model and then produce M-best Maximum a Posteriori (MAP) hypotheses from this model. In contrast, we formulate this multiple \{{\textbackslash}em choice\} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem. We present a max-margin formulation  that minimizes an upper-bound on this loss-function. Experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this  scenario and leads to substantial improvements in prediction accuracy.},
	urldate = {2022-09-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Guzmán-rivera, Abner and Batra, Dhruv and Kohli, Pushmeet},
	year = {2012},
}

@misc{sadeghi_cad2rl_2017,
	title = {{CAD2RL}: {Real} {Single}-{Image} {Flight} without a {Single} {Real} {Image}},
	shorttitle = {{CAD2RL}},
	url = {http://arxiv.org/abs/1611.04201},
	doi = {10.48550/arXiv.1611.04201},
	abstract = {Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD\${\textasciicircum}2\$RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Sadeghi, Fereshteh and Levine, Sergey},
	month = jun,
	year = {2017},
	note = {arXiv:1611.04201 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{ross_learning_2013,
	title = {Learning monocular reactive {UAV} control in cluttered natural environments},
	doi = {10.1109/ICRA.2013.6630809},
	abstract = {Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Ross, Stéphane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
	month = may,
	year = {2013},
	note = {ISSN: 1050-4729},
	keywords = {Cameras, Optical imaging, Sensors, Training, Trajectory, Vegetation, Visualization},
	pages = {1765--1772},
}

@incollection{florence_integrated_2020,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {Integrated {Perception} and {Control} at {High} {Speed}: {Evaluating} {Collision} {Avoidance} {Maneuvers} {Without} {Maps}},
	isbn = {978-3-030-43089-4},
	shorttitle = {Integrated {Perception} and {Control} at {High} {Speed}},
	url = {https://doi.org/10.1007/978-3-030-43089-4_20},
	abstract = {We present a method for robust high-speed quadrotor flight through unknown cluttered environments using integrated perception and control. Motivated by experiments in which the difficulty of accurate state estimation was a primary limitation on speed, our method forgoes maintaining a map in favor of using only instantaneous depth information in the local frame. This provides robustness in the presence of significant state estimate uncertainty. Additionally, we present approximation methods augmented with spatial partitioning data structures that enable low-latency, real-time reactive control. The probabilistic formulation provides a natural way to integrate reactive obstacle avoidance with arbitrary navigation objectives. We validate the method using a simulated quadrotor race through a forest at high speeds in the presence of increasing state estimate noise. We pair our method with a motion primitive library and compare with a global path-generation and pathfollowing approach.},
	language = {en},
	urldate = {2022-09-02},
	booktitle = {Algorithmic {Foundations} of {Robotics} {XII}: {Proceedings} of the {Twelfth} {Workshop} on the {Algorithmic} {Foundations} of {Robotics}},
	publisher = {Springer International Publishing},
	author = {Florence, Pete and Carter, John and Tedrake, Russ},
	editor = {Goldberg, Ken and Abbeel, Pieter and Bekris, Kostas and Miller, Lauren},
	year = {2020},
	doi = {10.1007/978-3-030-43089-4_20},
	pages = {304--319},
}

@article{pfeiffer_visual_2022,
	title = {Visual {Attention} {Prediction} {Improves} {Performance} of {Autonomous} {Drone} {Racing} {Agents}},
	volume = {17},
	issn = {1932-6203},
	url = {http://arxiv.org/abs/2201.02569},
	doi = {10.1371/journal.pone.0264471},
	abstract = {Humans race drones faster than neural networks trained for end-to-end autonomous flight. This may be related to the ability of human pilots to select task-relevant visual information effectively. This work investigates whether neural networks capable of imitating human eye gaze behavior and attention can improve neural network performance for the challenging task of vision-based autonomous drone racing. We hypothesize that gaze-based attention prediction can be an efficient mechanism for visual information selection and decision making in a simulator-based drone racing task. We test this hypothesis using eye gaze and flight trajectory data from 18 human drone pilots to train a visual attention prediction model. We then use this visual attention prediction model to train an end-to-end controller for vision-based autonomous drone racing using imitation learning. We compare the drone racing performance of the attention-prediction controller to those using raw image inputs and image-based abstractions (i.e., feature tracks). Comparing success rates for completing a challenging race track by autonomous flight, our results show that the attention-prediction based controller (88\% success rate) outperforms the RGB-image (61\% success rate) and feature-tracks (55\% success rate) controller baselines. Furthermore, visual attention-prediction and feature-track based models showed better generalization performance than image-based models when evaluated on hold-out reference trajectories. Our results demonstrate that human visual attention prediction improves the performance of autonomous vision-based drone racing agents and provides an essential step towards vision-based, fast, and agile autonomous flight that eventually can reach and even exceed human performances.},
	number = {3},
	urldate = {2022-09-02},
	journal = {PLOS ONE},
	author = {Pfeiffer, Christian and Wengeler, Simon and Loquercio, Antonio and Scaramuzza, Davide},
	month = mar,
	year = {2022},
	note = {arXiv:2201.02569 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Robotics},
	pages = {e0264471},
}

@article{pfeiffer_human-piloted_2021,
	title = {Human-{Piloted} {Drone} {Racing}: {Visual} {Processing} and {Control}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Human-{Piloted} {Drone} {Racing}},
	url = {http://arxiv.org/abs/2103.04672},
	doi = {10.1109/LRA.2021.3064282},
	abstract = {Humans race drones faster than algorithms, despite being limited to a fixed camera angle, body rate control, and response latencies in the order of hundreds of milliseconds. A better understanding of the ability of human pilots of selecting appropriate motor commands from highly dynamic visual information may provide key insights for solving current challenges in vision-based autonomous navigation. This paper investigates the relationship between human eye movements, control behavior, and flight performance in a drone racing task. We collected a multimodal dataset from 21 experienced drone pilots using a highly realistic drone racing simulator, also used to recruit professional pilots. Our results show task-specific improvements in drone racing performance over time. In particular, we found that eye gaze tracks future waypoints (i.e., gates), with first fixations occurring on average 1.5 seconds and 16 meters before reaching the gate. Moreover, human pilots consistently looked at the inside of the future flight path for lateral (i.e., left and right turns) and vertical maneuvers (i.e., ascending and descending). Finally, we found a strong correlation between pilots eye movements and the commanded direction of quadrotor flight, with an average visual-motor response latency of 220 ms. These results highlight the importance of coordinated eye movements in human-piloted drone racing. We make our dataset publicly available.},
	number = {2},
	urldate = {2022-09-02},
	journal = {IEEE Robotics and Automation Letters},
	author = {Pfeiffer, Christian and Scaramuzza, Davide},
	month = apr,
	year = {2021},
	note = {arXiv:2103.04672 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {3467--3474},
}

@inproceedings{teed_droid-slam_2021,
	title = {{DROID}-{SLAM}: {Deep} {Visual} {SLAM} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Teed, Zachary and Deng, Jia},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {SLAM, vSLAM},
	pages = {16558--16569},
}

@misc{wang_adaafford_2022,
	title = {{AdaAfford}: {Learning} to {Adapt} {Manipulation} {Affordance} for {3D} {Articulated} {Objects} via {Few}-shot {Interactions}},
	shorttitle = {{AdaAfford}},
	url = {http://arxiv.org/abs/2112.00246},
	doi = {10.48550/arXiv.2112.00246},
	abstract = {Perceiving and interacting with 3D articulated objects, such as cabinets, doors, and faucets, pose particular challenges for future home-assistant robots performing daily tasks in human environments. Besides parsing the articulated parts and joint parameters, researchers recently advocate learning manipulation affordance over the input shape geometry which is more task-aware and geometrically fine-grained. However, taking only passive observations as inputs, these methods ignore many hidden but important kinematic constraints (e.g., joint location and limits) and dynamic factors (e.g., joint friction and restitution), therefore losing significant accuracy for test cases with such uncertainties. In this paper, we propose a novel framework, named AdaAfford, that learns to perform very few test-time interactions for quickly adapting the affordance priors to more accurate instance-specific posteriors. We conduct large-scale experiments using the PartNet-Mobility dataset and prove that our system performs better than baselines.},
	urldate = {2022-08-20},
	publisher = {arXiv},
	author = {Wang, Yian and Wu, Ruihai and Mo, Kaichun and Ke, Jiaqi and Fan, Qingnan and Guibas, Leonidas and Dong, Hao},
	month = jul,
	year = {2022},
	note = {arXiv:2112.00246 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{ganapathi_learning_2020,
	title = {Learning {Dense} {Visual} {Correspondences} in {Simulation} to {Smooth} and {Fold} {Real} {Fabrics}},
	url = {http://arxiv.org/abs/2003.12698},
	doi = {10.48550/arXiv.2003.12698},
	abstract = {Robotic fabric manipulation is challenging due to the infinite dimensional configuration space, self-occlusion, and complex dynamics of fabrics. There has been significant prior work on learning policies for specific deformable manipulation tasks, but comparatively less focus on algorithms which can efficiently learn many different tasks. In this paper, we learn visual correspondences for deformable fabrics across different configurations in simulation and show that this representation can be used to design policies for a variety of tasks. Given a single demonstration of a new task from an initial fabric configuration, the learned correspondences can be used to compute geometrically equivalent actions in a new fabric configuration. This makes it possible to robustly imitate a broad set of multi-step fabric smoothing and folding tasks on multiple physical robotic systems. The resulting policies achieve 80.3\% average task success rate across 10 fabric manipulation tasks on two different robotic systems, the da Vinci surgical robot and the ABB YuMi. Results also suggest robustness to fabrics of various colors, sizes, and shapes. See https://tinyurl.com/fabric-descriptors for supplementary material and videos.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Ganapathi, Aditya and Sundaresan, Priya and Thananjeyan, Brijen and Balakrishna, Ashwin and Seita, Daniel and Grannen, Jennifer and Hwang, Minho and Hoque, Ryan and Gonzalez, Joseph E. and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Goldberg, Ken},
	month = nov,
	year = {2020},
	note = {arXiv:2003.12698 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{han_differential_2022,
	title = {Differential {Flatness}-{Based} {Trajectory} {Planning} for {Autonomous} {Vehicles}},
	url = {http://arxiv.org/abs/2208.13160},
	doi = {10.48550/arXiv.2208.13160},
	abstract = {As a core part of autonomous driving systems, motion planning has received extensive attention from academia and industry. However, there is no efficient trajectory planning solution capable of spatial-temporal joint optimization due to nonholonomic dynamics, particularly in the presence of unstructured environments and dynamic obstacles. To bridge the gap, we propose a versatile and real-time trajectory optimization method that can generate a high-quality feasible trajectory using a full vehicle model under arbitrary constraints. By leveraging the differential flatness property of car-like robots, we use flat outputs to analytically formulate all feasibility constraints to simplify the trajectory planning problem. Moreover, obstacle avoidance is achieved with full dimensional polygons to generate less conservative trajectories with safety guarantees, especially in tightly constrained spaces. We present comprehensive benchmarks with cutting-edge methods, demonstrating the significance of the proposed method in terms of efficiency and trajectory quality. Real-world experiments verify the practicality of our algorithm. We will release our codes as open-source packages with the purpose for the reference of the research community.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Han, Zhichao and Wu, Yuwei and Li, Tong and Zhang, Lu and Pei, Liuao and Xu, Long and Li, Chengyang and Ma, Changjia and Xu, Chao and Shen, Shaojie and Gao, Fei},
	month = aug,
	year = {2022},
	note = {arXiv:2208.13160 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{li_ego-exo_2021,
	title = {Ego-{Exo}: {Transferring} {Visual} {Representations} {From} {Third}-{Person} to {First}-{Person} {Videos}},
	shorttitle = {Ego-{Exo}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.html?ref=https://githubhelp.com},
	language = {en},
	urldate = {2022-08-29},
	author = {Li, Yanghao and Nagarajan, Tushar and Xiong, Bo and Grauman, Kristen},
	year = {2021},
	pages = {6943--6953},
}

@misc{geng_is_2021,
	title = {Is {Attention} {Better} {Than} {Matrix} {Decomposition}?},
	url = {http://arxiv.org/abs/2109.04553},
	doi = {10.48550/arXiv.2109.04553},
	abstract = {As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants.},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Geng, Zhengyang and Guo, Meng-Hao and Chen, Hongxu and Li, Xia and Wei, Ke and Lin, Zhouchen},
	month = dec,
	year = {2021},
	note = {arXiv:2109.04553 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_joint_2022,
	title = {Joint {Hand} {Motion} and {Interaction} {Hotspots} {Prediction} {From} {Egocentric} {Videos}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Joint_Hand_Motion_and_Interaction_Hotspots_Prediction_From_Egocentric_Videos_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-28},
	author = {Liu, Shaowei and Tripathi, Subarna and Majumdar, Somdeb and Wang, Xiaolong},
	year = {2022},
	pages = {3282--3292},
}

@inproceedings{luo_learning_2022,
	title = {Learning {Affordance} {Grounding} {From} {Exocentric} {Images}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Luo_Learning_Affordance_Grounding_From_Exocentric_Images_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-28},
	author = {Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
	year = {2022},
	pages = {2252--2261},
}

@inproceedings{zha_contrastively_2021,
	title = {Contrastively {Learning} {Visual} {Attention} as {Affordance} {Cues} from {Demonstrations} for {Robotic} {Grasping}},
	doi = {10.1109/IROS51168.2021.9636760},
	abstract = {Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work, our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective, there is a close association between attention and affordance. Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks, instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment. Our project website can be accessed at1.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zha, Yantian and Bhambri, Siddhant and Guan, Lin},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Affordances, Deep learning, Grasping, Neural networks, Psychology, Trajectory, Visualization},
	pages = {7835--7842},
}

@article{cattaneo_lcdnet_2022,
	title = {{LCDNet}: {Deep} {Loop} {Closure} {Detection} and {Point} {Cloud} {Registration} for {LiDAR} {SLAM}},
	volume = {38},
	issn = {1941-0468},
	shorttitle = {{LCDNet}},
	doi = {10.1109/TRO.2022.3150683},
	abstract = {Loop closure detection is an essential component of simultaneous localization and mapping (SLAM) systems, which reduces the drift accumulated over time. Over the years, several deep learning approaches have been proposed to address this task; however, their performance has been subpar compared to handcrafted techniques, especially while dealing with reverse loops. In this article, we introduce the novel loop closure detection network (LCDNet) that effectively detects loop closures in light detection and ranging (LiDAR) point clouds by simultaneously identifying previously visited places and estimating the six degrees of freedom relative transformation between the current scan and the map. LCDNet is composed of a shared encoder, a place recognition head that extracts global descriptors, and a relative pose head that estimates the transformation between two point clouds. We introduce a novel relative pose head based on the unbalanced optimal transport theory that we implement in a differentiable manner to allow for end-to-end training. Extensive evaluations of LCDNet on multiple real-world autonomous driving datasets show that our approach outperforms state-of-the-art loop closure detection and point cloud registration techniques by a large margin, especially while dealing with reverse loops. Moreover, we integrate our proposed loop closure detection approach into a LiDAR SLAM library to provide a complete mapping system and demonstrate the generalization ability using different sensor setup in an unseen city.},
	number = {4},
	journal = {IEEE Transactions on Robotics},
	author = {Cattaneo, Daniele and Vaghi, Matteo and Valada, Abhinav},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Deep learning, Feature extraction, Laser radar, Point cloud compression, Protocols, Simultaneous localization and mapping, Standards, Task analysis, loop closure detection, place recognition, point cloud registration, simultaneous localization and mapping (SLAM)},
	pages = {2074--2093},
}

@article{engel_direct_2018,
	title = {Direct {Sparse} {Odometry}},
	volume = {40},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2017.2658577},
	abstract = {Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Computational modeling, Geometry, Optimization, Robustness, Three-dimensional displays, Visual odometry, SLAM, 3D reconstruction, structure from motion, Visualization},
	pages = {611--625},
}

@article{teed_raft_2020,
	title = {{RAFT}: {Recurrent} {All}-{Pairs} {Field} {Transforms} for {Optical} {Flow}},
	shorttitle = {{RAFT}},
	url = {https://arxiv.org/abs/2003.12039v3},
	doi = {10.48550/arXiv.2003.12039},
	abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
	language = {en},
	urldate = {2022-08-27},
	author = {Teed, Zachary and Deng, Jia},
	month = mar,
	year = {2020},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2022-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
}

@article{valsesia_deep_2020,
	title = {Deep {Graph}-{Convolutional} {Image} {Denoising}},
	volume = {29},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3013166},
	abstract = {Non-local self-similarity is well-known to be an effective prior for the image denoising problem. However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information. In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields. The graph convolution operation generalizes the classic convolution to arbitrary graphs. In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns. We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution. Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.},
	journal = {IEEE Transactions on Image Processing},
	author = {Valsesia, Diego and Fracastoro, Giulia and Magli, Enrico},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Biological neural networks, Computer architecture, Convolution, Convolutional neural networks, Graph neural networks, Image denoising, Noise reduction, graph convolution, image denoising},
	pages = {8226--8237},
}

@misc{teed_deep_2022,
	title = {Deep {Patch} {Visual} {Odometry}},
	url = {http://arxiv.org/abs/2208.04726},
	doi = {10.48550/arXiv.2208.04726},
	abstract = {We propose Deep Patch Visual Odometry (DPVO), a new deep learning system for monocular Visual Odometry (VO). DPVO is accurate and robust while running at 2x-5x real-time speeds on a single RTX-3090 GPU using only 4GB of memory. We perform evaluation on standard benchmarks and outperform all prior work (classical or learned) in both accuracy and speed. Code is available at https://github.com/princeton-vl/DPVO.},
	urldate = {2022-08-26},
	publisher = {arXiv},
	author = {Teed, Zachary and Lipson, Lahav and Deng, Jia},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04726 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_pogo-net_2021,
	title = {{PoGO}-{Net}: {Pose} {Graph} {Optimization} {With} {Graph} {Neural} {Networks}},
	shorttitle = {{PoGO}-{Net}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_PoGO-Net_Pose_Graph_Optimization_With_Graph_Neural_Networks_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-08-26},
	author = {Li, Xinyi and Ling, Haibin},
	year = {2021},
	pages = {5895--5905},
}

@misc{li_3d_2021,
	title = {{3D} {Neural} {Scene} {Representations} for {Visuomotor} {Control}},
	url = {http://arxiv.org/abs/2107.04004},
	doi = {10.48550/arXiv.2107.04004},
	abstract = {Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.},
	urldate = {2022-08-25},
	publisher = {arXiv},
	author = {Li, Yunzhu and Li, Shuang and Sitzmann, Vincent and Agrawal, Pulkit and Torralba, Antonio},
	month = nov,
	year = {2021},
	note = {arXiv:2107.04004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{goh_additive_2017,
	title = {Additive manufacturing in unmanned aerial vehicles ({UAVs}): {Challenges} and potential},
	volume = {63},
	issn = {1270-9638},
	shorttitle = {Additive manufacturing in unmanned aerial vehicles ({UAVs})},
	url = {https://www.sciencedirect.com/science/article/pii/S127096381630503X},
	doi = {10.1016/j.ast.2016.12.019},
	abstract = {Unmanned aerial vehicles (UAV) are gaining popularity due to their application in military, private and public sector, especially being attractive for fields where human operator is not required. Light-weight UAVs are more desirable as they have better performance in terms of shorter take-off range and longer flight endurance. However, light weight structures with complex inner features are hard to fabricate using conventional manufacturing methods. The ability to print complex inner structures directly without the need of a mould gives additive manufacturing (AM) an edge over conventional manufacturing. Recent development in composite and multi-material printing opens up new possibilities of printing lightweight structures and novel platforms like flapping wings with ease. This paper explores the impact of additive manufacturing on aerodynamics, structures and materials used for UAVs. The review will discuss state-of-the-art AM technologies for UAVs through innovations in materials and structures and their advantages and limitations. The role of additive manufacturing to improve the performance of UAVs through smart material actuators and multi-functional structures will also be discussed.},
	language = {en},
	urldate = {2022-08-26},
	journal = {Aerospace Science and Technology},
	author = {Goh, G. D. and Agarwala, S. and Goh, G. L. and Dikshit, V. and Sing, S. L. and Yeong, W. Y.},
	month = apr,
	year = {2017},
	keywords = {3D printing, Additive manufacturing, Multi-functional, Rapid prototyping, Smart materials, Structures, UAV},
	pages = {140--151},
}

@inproceedings{xavier_accuracy_2017,
	title = {Accuracy {Analysis} of {Augmented} {Reality} {Markers} for {Visual} {Mapping} and {Localization}},
	doi = {10.1109/WVC.2017.00020},
	abstract = {An important problem that can be solved by Computer Vision algorithms is computing a 3D reconstruction of a scene captured by an ordinary camera. While some approaches seek solutions based on natural features (projections of the captured scene in the image), artificial markers can be used as an attractive alternative because they are easier to detect and less susceptible to error. Several artificial markers systems are available as computer libraries with different approaches of camera pose estimation, although their capabilities are still unexplored by Simultaneous Localization and Mapping (SLAM) solutions. This work presents a solution to the problem of SLAM using artificial markers. The automatically computed camera poses by the ARUCO library are assessed on data depicting commonly encountered scenarios in Robotics and Augmented/Virtual Reality applications, such as small to medium sized rooms and a research laboratory. The evaluation considers an established criteria that should clarify the accuracy of the use of artificial markers in visual mapping and localization.},
	booktitle = {2017 {Workshop} of {Computer} {Vision} ({WVC})},
	author = {Xavier, Rodrigo S. and da Silva, Bruno M. F. and Goncalves, Luiz M. G.},
	month = oct,
	year = {2017},
	keywords = {Augmented reality, Cameras, Libraries, Simultaneous localization and mapping, Three-dimensional displays, Transforms, Visualization},
	pages = {73--77},
}

@article{campos_orb-slam3_2021,
	title = {{ORB}-{SLAM3}: {An} {Accurate} {Open}-{Source} {Library} for {Visual}, {Visual}-{Inertial} and {Multi}-{Map} {SLAM}},
	volume = {37},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{ORB}-{SLAM3}},
	url = {http://arxiv.org/abs/2007.11898},
	doi = {10.1109/TRO.2021.3075644},
	abstract = {This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
	number = {6},
	urldate = {2022-08-26},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and Montiel, José M. M. and Tardós, Juan D.},
	month = dec,
	year = {2021},
	note = {arXiv:2007.11898 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1874--1890},
}

@misc{tiryaki_printing-while-moving_2018,
	title = {Printing-while-moving: a new paradigm for large-scale robotic {3D} {Printing}},
	shorttitle = {Printing-while-moving},
	url = {http://arxiv.org/abs/1809.07940},
	doi = {10.48550/arXiv.1809.07940},
	abstract = {Building and Construction have recently become an exciting application ground for robotics. In particular, rapid progress in materials formulation and in robotics technology has made robotic 3D Printing of concrete a promising technique for in-situ construction. Yet, scalability remains an important hurdle to widespread adoption: the printing systems (gantry- based or arm-based) are often much larger than the structure to be printed, hence cumbersome. Recently, a mobile printing system - a manipulator mounted on a mobile base - was proposed to alleviate this issue: such a system, by moving its base, can potentially print a structure larger than itself. However, the proposed system could only print while being stationary, imposing thereby a limit on the size of structures that can be printed in a single take. Here, we develop a system that implements the printing-while-moving paradigm, which enables printing single-piece structures of arbitrary sizes with a single robot. This development requires solving motion planning, localization, and motion control problems that are specific to mobile 3D Printing. We report our framework to address those problems, and demonstrate, for the first time, a printing-while-moving experiment, wherein a 210 cm x 45 cm x 10 cm concrete structure is printed by a robot arm that has a reach of 87 cm.},
	urldate = {2022-08-26},
	publisher = {arXiv},
	author = {Tiryaki, Mehmet Efe and Zhang, Xu and Pham, Quang-Cuong},
	month = sep,
	year = {2018},
	note = {arXiv:1809.07940 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{gawel_fully-integrated_2019,
	title = {A {Fully}-{Integrated} {Sensing} and {Control} {System} for {High}-{Accuracy} {Mobile} {Robotic} {Building} {Construction}},
	doi = {10.1109/IROS40897.2019.8967733},
	abstract = {We present a fully-integrated sensing and control system which enables mobile manipulator robots to execute building tasks with millimeter-scale accuracy on building construction sites. The approach leverages multi-modal sensing capabilities for state estimation, tight integration with digital building models, and integrated trajectory planning and whole-body motion control. A novel method for high-accuracy localization updates relative to the known building structure is proposed. The approach is implemented on a real platform and tested under realistic construction conditions. We show that the system can achieve sub-cm end-effector positioning accuracy during fully autonomous operation using solely onboard sensing.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Gawel, Abel and Blum, Hermann and Pankert, Johannes and Krämer, Koen and Bartolomei, Luca and Ercan, Selen and Farshidian, Farbod and Chli, Margarita and Gramazio, Fabio and Siegwart, Roland and Hutter, Marco and Sandy, Timothy},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {2300--2307},
}

@inproceedings{sustarevas_youwasps_2019,
	title = {{YouWasps}: {Towards} {Autonomous} {Multi}-{Robot} {Mobile} {Deposition} for {Construction}},
	shorttitle = {{YouWasps}},
	doi = {10.1109/IROS40897.2019.8967766},
	abstract = {Mobile multi-robot construction systems offer new ways to optimise the on-site construction process. In this paper we begin to investigate the functionality requirements for controlling a team of robots to build structures much greater than their individual workspace. To achieve these aims, we present a mobile extruder robot called YouWasp. We also begin to explore methods for collision aware printing and construction task decomposition and allocation. These are deployed via YouWasp and enable it to deposit material autonomously. In doing so, we are able to evaluate the potential for parallelization of tasks and printing autonomy in simulation as well as physical team of robots. Altogether, these results provide a foundation for future work that enable fleets of mobile construction systems to cooperate and help us shape our built environment in new ways.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Sustarevas, Julius and Benjamin Tan, K. X. and Gerber, David and Stuart-Smith, Robert and Pawar, Vijay M.},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {2320--2327},
}

@article{dorfler_mobile_2019,
	title = {Mobile robotic fabrication beyond factory conditions: case study {Mesh} {Mould} wall of the {DFAB} {HOUSE}},
	volume = {3},
	issn = {2509-8780},
	shorttitle = {Mobile robotic fabrication beyond factory conditions},
	url = {https://doi.org/10.1007/s41693-019-00020-w},
	doi = {10.1007/s41693-019-00020-w},
	abstract = {The development of novel robotic fabrication technologies in architecture concentrates largely on integrating stationary industrial-type robots into off-site prefabrication processes. By contrast, few enabling robotic technologies exist today that allow robotic fabrication processes to be mobile and implemented directly on building sites. While mobile in situ fabrication offers a large range of architectural potentials, its realization requires to address fundamental challenges. First, the production of large-scale and potentially monolithic structures on-site requires an advanced robotic fabrication system that can fulfill the material, structural- and architectural-related demands associated with it. Second, the poorly structured nature of building sites requires mobile robotic systems to be equipped with advanced sensing and control solutions to contend with uncertain conditions found on-site. The research discussed in this paper addresses both of these subjects. It applies a novel construction system for non-standard reinforced concrete structures, termed Mesh Mould, to explore the fabrication of large-scale and monolithic building structures using a mobile robot on site. It further investigates sensor-integrated adaptive fabrication strategies to achieve the accurate fabrication of such a large-scale structure, and this is done despite prevalent uncertainties related to the building site environment, the mobile robotic system, and the material behavior during fabrication. The results of this research were realized in a slender, doubly curved, reinforced concrete wall at the DFAB HOUSE at NEST. This research demonstrator provides the unique opportunity to present robotic in situ fabrication not merely as a future possibility, but as a reality applied to a tangible construction project.},
	language = {en},
	number = {1},
	urldate = {2022-08-26},
	journal = {Construction Robotics},
	author = {Dörfler, Kathrin and Hack, Norman and Sandy, Timothy and Giftthaler, Markus and Lussi, Manuel and Walzer, Alexander N. and Buchli, Jonas and Gramazio, Fabio and Kohler, Matthias},
	month = dec,
	year = {2019},
	keywords = {Adaptive fabrication, Construction robotics, Digital fabrication, In situ fabrication, Mobile manipulation},
	pages = {53--67},
}

@article{dorfler_additive_2022,
	title = {Additive {Manufacturing} using mobile robots: {Opportunities} and challenges for building construction},
	volume = {158},
	issn = {0008-8846},
	shorttitle = {Additive {Manufacturing} using mobile robots},
	url = {https://www.sciencedirect.com/science/article/pii/S0008884622000631},
	doi = {10.1016/j.cemconres.2022.106772},
	abstract = {In situ Additive Manufacturing (AM) has developed into an active research and industry-transfer area, mainly due to the increasing need for productive and sustainable methods in the concrete construction industry in combination with novel technological enablers. While current systems for in situ AM are often single large-scale stationary facilities, Mobile Additive Manufacturing (MAM) systems are an emerging technology that could provide scalability for AM processes on construction sites. This scalability is achievable through cooperability of multiple mobile robots on individual 3D printing tasks, while their mobility and autonomy enable deployment in both new and existing contexts, and coordination of operations for the direct and indirect interaction with humans. To ensure applicability and scalability, MAM closely integrates architectural, mechanical, and materials design, the manufacturing process, sensing, and control. With this paper, we give a comprehensive review of research trends, open questions and key performance indicators. We support the discussion with potential architectural application scenarios. Overall, we aim to indicate why addressing MAM is an inherently multidisciplinary challenge.},
	language = {en},
	urldate = {2022-08-26},
	journal = {Cement and Concrete Research},
	author = {Dörfler, Kathrin and Dielemans, Gido and Lachmayer, Lukas and Recker, Tobias and Raatz, Annika and Lowke, Dirk and Gerke, Markus},
	month = aug,
	year = {2022},
	keywords = {Additive Manufacturing, Architecture and digital fabrication, Computational design, Mobile robotics, Robotic fabrication},
	pages = {106772},
}

@article{urhal_robot_2019,
	title = {Robot assisted additive manufacturing: {A} review},
	volume = {59},
	issn = {0736-5845},
	shorttitle = {Robot assisted additive manufacturing},
	url = {https://www.sciencedirect.com/science/article/pii/S0736584518303636},
	doi = {10.1016/j.rcim.2019.05.005},
	abstract = {The additive manufacturing and the robotic applications are tremendously increasing in the manufacturing field. This review paper discusses the concept of robotic-assisted additive manufacturing. The leading additive manufacturing methods that can be used with a robotic system are presented and discussed in detail. The information flow required to produce an object from a CAD model through a robotic-assisted system, different from the traditional information flow in a conventional additive manufacturing approach is also detailed. Examples of the use of robotic-assisted additive manufacturing systems are presented.},
	language = {en},
	urldate = {2022-08-26},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Urhal, Pinar and Weightman, Andrew and Diver, Carl and Bartolo, Paulo},
	month = oct,
	year = {2019},
	keywords = {3D printing, Additive manufacturing, Multi-axis printing, Robots},
	pages = {335--345},
}

@inproceedings{ye_whats_2022,
	title = {What's in {Your} {Hands}? {3D} {Reconstruction} of {Generic} {Objects} in {Hands}},
	shorttitle = {What's in {Your} {Hands}?},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ye_Whats_in_Your_Hands_3D_Reconstruction_of_Generic_Objects_in_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-25},
	author = {Ye, Yufei and Gupta, Abhinav and Tulsiani, Shubham},
	year = {2022},
	pages = {3895--3905},
}

@inproceedings{sun_scalability_2020,
	title = {Scalability in {Perception} for {Autonomous} {Driving}: {Waymo} {Open} {Dataset}},
	shorttitle = {Scalability in {Perception} for {Autonomous} {Driving}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html},
	urldate = {2022-08-23},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	year = {2020},
	pages = {2446--2454},
}

@inproceedings{qian_understanding_2022,
	title = {Understanding {3D} {Object} {Articulation} in {Internet} {Videos}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Qian_Understanding_3D_Object_Articulation_in_Internet_Videos_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-20},
	author = {Qian, Shengyi and Jin, Linyi and Rockwell, Chris and Chen, Siyi and Fouhey, David F.},
	year = {2022},
	pages = {1599--1609},
}

@inproceedings{sun_loftr_2021,
	title = {{LoFTR}: {Detector}-{Free} {Local} {Feature} {Matching} {With} {Transformers}},
	shorttitle = {{LoFTR}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Sun_LoFTR_Detector-Free_Local_Feature_Matching_With_Transformers_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-08-20},
	author = {Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei},
	year = {2021},
	pages = {8922--8931},
}

@inproceedings{shridhar_cliport_2022,
	title = {{CLIPort}: {What} and {Where} {Pathways} for {Robotic} {Manipulation}},
	shorttitle = {{CLIPort}},
	url = {https://proceedings.mlr.press/v164/shridhar22a.html},
	abstract = {How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.},
	language = {en},
	urldate = {2022-08-18},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {894--906},
}

@misc{pathak_zero-shot_2018,
	title = {Zero-{Shot} {Visual} {Imitation}},
	url = {http://arxiv.org/abs/1804.08606},
	doi = {10.48550/arXiv.1804.08606},
	abstract = {The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/},
	urldate = {2022-08-06},
	publisher = {arXiv},
	author = {Pathak, Deepak and Mahmoudieh, Parsa and Luo, Guanghao and Agrawal, Pulkit and Chen, Dian and Shentu, Yide and Shelhamer, Evan and Malik, Jitendra and Efros, Alexei A. and Darrell, Trevor},
	month = apr,
	year = {2018},
	note = {arXiv:1804.08606 [cs, stat]},
	keywords = {Imitation},
}

@inproceedings{young_visual_2021,
	title = {Visual {Imitation} {Made} {Easy}},
	url = {https://proceedings.mlr.press/v155/young21a.html},
	abstract = {Visual imitation learning provides a framework for learning complex manipulation behaviors by leveraging human demonstrations. However, current interfaces for imitation such as kinesthetic teaching or teleoperation prohibitively restrict our ability to efficiently collect large-scale data in the wild. Obtaining such diverse demonstration data is paramount for the generalization of learned skills to novel scenarios. In this work, we present an alternate interface for imitation that simplifies the data collection process while allowing for easy transfer to robots. We use commercially available reacher-grabber assistive tools both as a data collection device and as the robot’s end-effector. To extract action information from these visual demonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in addition to training a finger detection network. We experimentally evaluate on two challenging tasks: non-prehensile pushing and prehensile stacking, with 1000 diverse demonstrations for each task. For both tasks, we use standard behavior cloning to learn executable policies from the previously collected offline demonstrations. To improve learning performance, we employ a variety of data augmentations and provide an extensive analysis of its effects. Finally, we demonstrate the utility of our interface by evaluating on real robotic scenarios with previously unseen objects and achieve a 87\% success rate on pushing and a 62\% success rate on stacking. Robot videos are available at our  project website: https://sites.google.com/view/visual-imitation-made-easy .},
	language = {en},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Young, Sarah and Gandhi, Dhiraj and Tulsiani, Shubham and Gupta, Abhinav and Abbeel, Pieter and Pinto, Lerrel},
	month = oct,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {Imitation},
	pages = {1992--2005},
}

@misc{bukschat_efficientpose_2020,
	title = {{EfficientPose}: {An} efficient, accurate and scalable end-to-end {6D} multi object pose estimation approach},
	shorttitle = {{EfficientPose}},
	url = {http://arxiv.org/abs/2011.04307},
	doi = {10.48550/arXiv.2011.04307},
	abstract = {In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-the-art accuracy of 97.35\% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at https://github.com/ybkscht/EfficientPose.},
	urldate = {2022-08-06},
	publisher = {arXiv},
	author = {Bukschat, Yannick and Vetter, Marcus},
	month = nov,
	year = {2020},
	note = {arXiv:2011.04307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sarlin_superglue_2020,
	title = {{SuperGlue}: {Learning} {Feature} {Matching} with {Graph} {Neural} {Networks}},
	shorttitle = {{SuperGlue}},
	url = {http://arxiv.org/abs/1911.11763},
	doi = {10.48550/arXiv.1911.11763},
	abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork.},
	urldate = {2022-08-07},
	publisher = {arXiv},
	author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv:1911.11763 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{deng_3d_2021,
	title = {{3D} {AffordanceNet}: {A} {Benchmark} for {Visual} {Object} {Affordance} {Understanding}},
	shorttitle = {{3D} {AffordanceNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Deng_3D_AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-08-06},
	author = {Deng, Shengheng and Xu, Xun and Wu, Chaozheng and Chen, Ke and Jia, Kui},
	year = {2021},
	pages = {1778--1787},
}

@inproceedings{xu_rnnpose_2022,
	title = {{RNNPose}: {Recurrent} 6-{DoF} {Object} {Pose} {Refinement} {With} {Robust} {Correspondence} {Field} {Estimation} and {Pose} {Optimization}},
	shorttitle = {{RNNPose}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-06},
	author = {Xu, Yan and Lin, Kwan-Yee and Zhang, Guofeng and Wang, Xiaogang and Li, Hongsheng},
	year = {2022},
	pages = {14880--14890},
}

@inproceedings{ehsanpour_jrdb-act_2022,
	title = {{JRDB}-{Act}: {A} {Large}-{Scale} {Dataset} for {Spatio}-{Temporal} {Action}, {Social} {Group} and {Activity} {Detection}},
	shorttitle = {{JRDB}-{Act}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ehsanpour_JRDB-Act_A_Large-Scale_Dataset_for_Spatio-Temporal_Action_Social_Group_and_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-08-16},
	author = {Ehsanpour, Mahsa and Saleh, Fatemeh and Savarese, Silvio and Reid, Ian and Rezatofighi, Hamid},
	year = {2022},
	pages = {20983--20992},
}

@article{bao_instereo2k_2020,
	title = {{InStereo2K}: a large real dataset for stereo matching in indoor scenes},
	volume = {63},
	issn = {1869-1919},
	shorttitle = {{InStereo2K}},
	url = {https://doi.org/10.1007/s11432-019-2803-x},
	doi = {10.1007/s11432-019-2803-x},
	abstract = {Deep neural networks have shown great success in stereo matching in recent years. On the KITTI datasets, most top performing methods are based on neural networks. However, on the Middlebury datasets, these methods usually do not perform well. The KITTI datasets are collected in outdoor scenes while the Middlebury datasets are collected in indoor scenes. It is commonly believed that the community still lacks a large labelled dataset for stereo matching in indoor scenes. In this paper, we introduce a new stereo dataset called InStereo2K. It contains 2050 pairs of stereo images with highly accurate groundtruth disparity maps, including 2000 pairs for training and 50 pairs for test. Experimental results show that our dataset can significantly improve the performance of several latest networks (including StereoNet and PSMNet) on the Middlebury 2014 dataset. The large scale, high accuracy and rich diversity of the proposed InStereo2K dataset provide new opportunities to researchers in the area of stereo matching and beyond. It also takes end-to-end stereo matching methods a step towards practical applications.},
	language = {en},
	number = {11},
	urldate = {2022-08-16},
	journal = {Science China Information Sciences},
	author = {Bao, Wei and Wang, Wei and Xu, Yuhua and Guo, Yulan and Hong, Siyu and Zhang, Xiaohu},
	month = jul,
	year = {2020},
	keywords = {convolutional neural network, dataset, depth estimation, stereo matching},
	pages = {212101},
}

@inproceedings{scharstein_high-resolution_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {High-{Resolution} {Stereo} {Datasets} with {Subpixel}-{Accurate} {Ground} {Truth}},
	isbn = {978-3-319-11752-2},
	doi = {10.1007/978-3-319-11752-2_3},
	abstract = {We present a structured lighting system for creating high-resolution stereo datasets of static indoor scenes with highly accurate ground-truth disparities. The system includes novel techniques for efficient 2D subpixel correspondence search and self-calibration of cameras and projectors with modeling of lens distortion. Combining disparity estimates from multiple projector positions we are able to achieve a disparity accuracy of 0.2 pixels on most observed surfaces, including in half-occluded regions. We contribute 33 new 6-megapixel datasets obtained with our system and demonstrate that they present new challenges for the next generation of stereo algorithms.},
	language = {en},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer International Publishing},
	author = {Scharstein, Daniel and Hirschmüller, Heiko and Kitajima, York and Krathwohl, Greg and Nešić, Nera and Wang, Xi and Westling, Porter},
	editor = {Jiang, Xiaoyi and Hornegger, Joachim and Koch, Reinhard},
	year = {2014},
	pages = {31--42},
}

@inproceedings{sharma_third-person_2019,
	title = {Third-{Person} {Visual} {Imitation} {Learning} via {Decoupled} {Hierarchical} {Controller}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/8a146f1a3da4700cbf03cdc55e2daae6-Abstract.html},
	abstract = {We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling what to achieve (intended task) from how to perform it (controller). We propose a hierarchical setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box. Project video is available at https://pathak22.github.io/hierarchical-imitation/},
	urldate = {2022-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sharma, Pratyusha and Pathak, Deepak and Gupta, Abhinav},
	year = {2019},
	keywords = {Imitation},
}

@misc{frey_locomotion_2022,
	title = {Locomotion {Policy} {Guided} {Traversability} {Learning} using {Volumetric} {Representations} of {Complex} {Environments}},
	url = {http://arxiv.org/abs/2203.15854},
	doi = {10.48550/arXiv.2203.15854},
	abstract = {Despite the progress in legged robotic locomotion, autonomous navigation in unknown environments remains an open problem. Ideally, the navigation system utilizes the full potential of the robots' locomotion capabilities while operating within safety limits under uncertainty. The robot must sense and analyze the traversability of the surrounding terrain, which depends on the hardware, locomotion control, and terrain properties. It may contain information about the risk, energy, or time consumption needed to traverse the terrain. To avoid hand-crafted traversability cost functions we propose to collect traversability information about the robot and locomotion policy by simulating the traversal over randomly generated terrains using a physics simulator. Thousand of robots are simulated in parallel controlled by the same locomotion policy used in reality to acquire 57 years of real-world locomotion experience equivalent. For deployment on the real robot, a sparse convolutional network is trained to predict the simulated traversability cost, which is tailored to the deployed locomotion policy, from an entirely geometric representation of the environment in the form of a 3D voxel-occupancy map. This representation avoids the need for commonly used elevation maps, which are error-prone in the presence of overhanging obstacles and multi-floor or low-ceiling scenarios. The effectiveness of the proposed traversability prediction network is demonstrated for path planning for the legged robot ANYmal in various indoor and natural environments.},
	urldate = {2022-08-05},
	publisher = {arXiv},
	author = {Frey, Jonas and Hoeller, David and Khattak, Shehryar and Hutter, Marco},
	month = aug,
	year = {2022},
	note = {arXiv:2203.15854 [cs]},
	keywords = {Computer Science - Robotics},
}

@inproceedings{eppenberger_leveraging_2020,
	title = {Leveraging {Stereo}-{Camera} {Data} for {Real}-{Time} {Dynamic} {Obstacle} {Detection} and {Tracking}},
	doi = {10.1109/IROS45743.2020.9340699},
	abstract = {Dynamic obstacle avoidance is one crucial component for compliant navigation in crowded environments. In this paper we present a system for accurate and reliable detection and tracking of dynamic objects using noisy point cloud data generated by stereo cameras. Our solution is real-time capable and specifically designed for the deployment on computationally-constrained unmanned ground vehicles. The proposed approach identifies individual objects in the robot's surroundings and classifies them as either static or dynamic. The dynamic objects are labeled as either a person or a generic dynamic object. We then estimate their velocities to generate a 2D occupancy grid that is suitable for performing obstacle avoidance. We evaluate the system in indoor and outdoor scenarios and achieve real-time performance on a consumergrade computer. On our test-dataset, we reach a MOTP of 0.07 ± 0.07m, and a MOTA of 85.3\% for the detection and tracking of dynamic objects. We reach a precision of 96.9\% for the detection of static objects.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Eppenberger, Thomas and Cesari, Gianluca and Dymczyk, Marcin and Siegwart, Roland and Dubé, Renaud},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Collision avoidance, Dynamics, Real-time systems, Reliability, Tracking, Two dimensional displays, Vehicle dynamics},
	pages = {10528--10535},
}

@article{buchner_3d_2022,
	title = {{3D} {Multi}-{Object} {Tracking} {Using} {Graph} {Neural} {Networks} {With} {Cross}-{Edge} {Modality} {Attention}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3191558},
	abstract = {Online 3D multi-object tracking (MOT) has witnessed significant research interest in recent years, largely driven by demand from the autonomous systems community. However, 3D offline MOT is relatively less explored. Labeling 3D trajectory scene data at a large scale while not relying on high-cost human experts is still an open research question. In this work, we propose Batch3DMOT which follows the tracking-by-detection paradigm and represents real-world scenes as directed, acyclic, and category-disjoint tracking graphs that are attributed using various modalities such as camera, LiDAR, and radar. We present a multi-modal graph neural network that uses a cross-edge attention mechanism mitigating modality intermittence, which translates into sparsity in the graph domain. Additionally, we present attention-weighted convolutions over frame-wise k-NN neighborhoods as suitable means to allow information exchange across disconnected graph components. We evaluate our approach using various sensor modalities and model configurations on the challenging nuScenes and KITTI datasets. Extensive experiments demonstrate that our proposed approach yields an overall improvement of 3.3\% in the AMOTA score on nuScenes thereby setting the new state-of-the-art for 3D tracking and further enhancing false positive filtering.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Büchner, Martin and Valada, Abhinav},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Computer vision, graph neural networks, autonomous driving, Image edge detection, Laser radar, Radar tracking, Three-dimensional displays, Tracking, Trajectory},
	pages = {9707--9714},
}

@article{sakr_quantifying_2022,
	title = {Quantifying {Demonstration} {Quality} for {Robot} {Learning} and {Generalization}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3191950},
	abstract = {Learning from Demonstration (LfD) seeks to democratize robotics by enabling non-expert end-users to teach robots. However, most LfD techniques assume users provide optimal demonstrations, which may not be accurate. Demonstration quality plays a crucial role in robot learning and generalization. Hence, it is important to quantify the quality of the provided demonstrations before using them for robot learning. In this letter, we propose quantifying the generalizability of the demonstrations based on how well they perform in the learned task. The proposed approach is validated in a user study (N = 27). Participants with different robotics expertise levels were recruited to teach a PR2 robot a generic task (pressing a button) under different task constraints. They taught the robot in two sessions on two different days to capture their teaching behaviour across sessions. The task performance was utilized to classify the provided demonstrations into high-quality and low-quality sets. The results show a significant correlation between task performance and generalization performance across all participants. We also found that users clustered into two groups: Users who provided high-quality demonstrations from the first session (the fast-adapters), and users who provided low-quality demonstrations in the first session and then improved with practice (the slow-adapters). This approach for assessing demonstrations allows us to determine whether users require more training in order to provide high-quality demonstrations.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sakr, Maram and Li, Zexi Jesse and Van der Loos, H. F. Machiel and Kulić, Dana and Croft, Elizabeth A.},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Education, Learning from demonstration, Mathematical models, Measurement, Robot kinematics, Robot learning, Robots, Task analysis, design and human factors, physical human-robot interaction},
	pages = {9659--9666},
}

@article{alliegro_end--end_2022,
	title = {End-to-{End} {Learning} to {Grasp} via {Sampling} {From} {Object} {Point} {Clouds}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3191183},
	abstract = {The ability to grasp objects is an essential skill that enables many robotic manipulation tasks. Recent works have studied point cloud-based methods for object grasping by starting from simulated datasets and have shown promising performance in real-world scenarios. Nevertheless, many of them still rely on ad-hoc geometric heuristics to generate grasp candidates, which fail to generalize to objects with significantly different shapes with respect to those observed during training. Several approaches exploit complex multi-stage learning strategies and local neighborhood feature extraction while ignoring semantic global information. Furthermore, they are inefficient in terms of number of training samples and time required for inference. In this letter, we propose an end-to-end learning solution to generate 6-DOF parallel-jaw grasps starting from the 3D partial view of the object. Our Learning to Grasp (L2G) method gathers information from the input point cloud through a new procedure that combines a differentiable sampling strategy to identify the visible contact points, with a feature encoder that leverages local and global cues. Overall, L2G is guided by a multi-task objective that generates a diverse set of grasps by optimizing contact point sampling, grasp regression, and grasp classification. With a thorough experimental analysis, we show the effectiveness of L2G as well as its robustness and generalization abilities.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Alliegro, Antonio and Rudorfer, Martin and Frattin, Fabio and Leonardis, Aleš and Tommasi, Tatiana},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Deep learning for visual perception, Feature extraction, Grasping, Grippers, Point cloud compression, Shape, Three-dimensional displays, Training, deep learning in grasping and manipulation, grasping},
	pages = {9865--9872},
}

@inproceedings{zhu_hierarchical_2021,
	title = {Hierarchical {Planning} for {Long}-{Horizon} {Manipulation} with {Geometric} and {Symbolic} {Scene} {Graphs}},
	doi = {10.1109/ICRA48506.2021.9561548},
	abstract = {We present a visually grounded hierarchical planning algorithm for long-horizon manipulation tasks. Our algorithm offers a joint framework of neuro-symbolic task planning and low-level motion generation conditioned on the specified goal. At the core of our approach is a two-level scene graph representation, namely geometric scene graph and symbolic scene graph. This hierarchical representation serves as a structured, object-centric abstraction of manipulation scenes. Our model uses graph neural networks to process these scene graphs for predicting high-level task plans and low-level motions. We demonstrate that our method scales to long-horizon tasks and generalizes well to novel task goals. We validate our method in a kitchen storage task in both physical simulation and the real world. Experiments show that our method achieves over 70\% success rate and nearly 90\% of subgoal completion rate on the real robot while being four orders of magnitude faster in computation time compared to standard search-based task-and-motion planner. 1},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhu, Yifeng and Tremblay, Jonathan and Birchfield, Stan and Zhu, Yuke},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Automation, Computational modeling, Conferences, Graph neural networks, Planning, Predictive models, Visualization},
	pages = {6541--6548},
}

@inproceedings{johns_coarse--fine_2021,
	title = {Coarse-to-{Fine} {Imitation} {Learning}: {Robot} {Manipulation} from a {Single} {Demonstration}},
	shorttitle = {Coarse-to-{Fine} {Imitation} {Learning}},
	doi = {10.1109/ICRA48506.2021.9560942},
	abstract = {We introduce a simple new method for visual imitation learning, which allows a novel robot manipulation task to be learned from a single human demonstration, without requiring any prior knowledge of the object being interacted with. Our method models imitation learning as a state estimation problem, with the state defined as the end-effector’s pose at the point where object interaction begins, as observed from the demonstration. By modelling a manipulation task as a coarse, approach trajectory followed by a fine, interaction trajectory, this state estimator can be trained in a self-supervised manner, by automatically moving the end-effector’s camera around the object. At test time, the end-effector is moved to the estimated state through a linear path, at which point the demonstration’s end-effector velocities are simply repeated, enabling convenient acquisition of a complex interaction trajectory without actually needing to explicitly learn a policy. Real-world experiments on 8 everyday tasks show that our method can learn a diverse range of skills from just a single human demonstration, whilst also yielding a stable and interpretable controller. Videos at: www.robot-learning.uk/coarse-to-fine-imitation-learning.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Johns, Edward},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Computer vision, Conferences, End effectors, Learning systems, Three-dimensional displays, Trajectory, Visualization},
	pages = {4613--4619},
}

@inproceedings{jang_bc-z_2022,
	title = {{BC}-{Z}: {Zero}-{Shot} {Task} {Generalization} with {Robotic} {Imitation} {Learning}},
	shorttitle = {{BC}-{Z}},
	url = {https://proceedings.mlr.press/v164/jang22a.html},
	abstract = {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pre-trained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44\%, without any robot demonstrations for those tasks.},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the 5th {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Imitation},
	pages = {991--1002},
}

@inproceedings{sieb_graph-structured_2020,
	title = {Graph-{Structured} {Visual} {Imitation}},
	url = {https://proceedings.mlr.press/v100/sieb20a.html},
	abstract = {We cast visual imitation as a visual correspondence problem. Our robotic agent is rewarded when its actions result in better matching of relative spatial configurations for corresponding visual entities detected in its workspace and the teacher’s demonstration. We build upon recent advances in Computer Vision, such as human finger keypoint detectors, object detectors trained on-the-fly with synthetic augmentations, and point detectors supervised by viewpoint changes [1] and learn multiple visual entity detectors for each demonstration without human annotations or robot interactions. We empirically show that the proposed factorized visual representations of entities and their spatial arrangements drive successful imitation of a variety of manipulation skills within minutes, using a single demonstration and without any environment instrumentation. It is robust to background clutter and can effectively generalize across environment variations between demonstrator and imitator, greatly outperforming unstructured non-factorized full-frame CNN encodings of previous works [2].},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Sieb, Maximilian and Xian, Zhou and Huang, Audrey and Kroemer, Oliver and Fragkiadaki, Katerina},
	month = may,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Imitation},
	pages = {979--989},
}

@inproceedings{nguyen_translating_2018,
	title = {Translating {Videos} to {Commands} for {Robotic} {Manipulation} with {Deep} {Recurrent} {Neural} {Networks}},
	doi = {10.1109/ICRA.2018.8460857},
	abstract = {We present a new method to translate videos to commands for robotic manipulation using Deep Recurrent Neural Networks (RNN). Our framework first extracts deep features from the input video frames with a deep Convolutional Neural Networks (CNN). Two RNN layers with an encoder-decoder architecture are then used to encode the visual features and sequentially generate the output words as the command. We demonstrate that the translation accuracy can be improved by allowing a smooth transaction between two RNN layers and using the state-of-the-art feature extractor. The experimental results on our new challenging dataset show that our approach outperforms recent methods by a fair margin. Furthermore, we combine the proposed translation module with the vision and planning system to let a robot perform various manipulation tasks. Finally, we demonstrate the effectiveness of our framework on a full-size humanoid robot WALK-MAN.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Nguyen, Anh and Kanoulas, Dimitrios and Muratore, Luca and Caldwell, Darwin G. and Tsagarakis, Nikos G.},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	pages = {3782--3788},
}

@inproceedings{lee_learning_2017,
	title = {Learning robot activities from first-person human videos using convolutional future regression},
	doi = {10.1109/IROS.2017.8205953},
	abstract = {We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Lee, Jangwon and Ryoo, Michael S.},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Imitation},
	pages = {1497--1504},
}

@inproceedings{chen_unsupervised_2021,
	title = {Unsupervised {Learning} of {Visual} {3D} {Keypoints} for {Control}},
	url = {https://proceedings.mlr.press/v139/chen21b.html},
	abstract = {Learning sensorimotor control policies from high-dimensional images crucially relies on the quality of the underlying visual representations. Prior works show that structured latent space such as visual keypoints often outperforms unstructured representations for robotic control. However, most of these representations, whether structured or unstructured are learned in a 2D space even though the control tasks are usually performed in a 3D environment. In this work, we propose a framework to learn such a 3D geometric structure directly from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints via a differentiable encoder which is trained to optimize both a multi-view consistency loss and downstream task objective. These discovered 3D keypoints tend to meaningfully capture robot joints as well as object movements in a consistent manner across both time and 3D space. The proposed approach outperforms prior state-of-art methods across a variety of reinforcement learning benchmarks. Code and videos at https://buoyancy99.github.io/unsup-3d-keypoints/.},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Boyuan and Abbeel, Pieter and Pathak, Deepak},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1539--1549},
}

@inproceedings{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {https://proceedings.mlr.press/v139/radford21a.html},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
	language = {en},
	urldate = {2022-07-29},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {CLIP},
	pages = {8748--8763},
}

@misc{smith_avid_2020,
	title = {{AVID}: {Learning} {Multi}-{Stage} {Tasks} via {Pixel}-{Level} {Translation} of {Human} {Videos}},
	shorttitle = {{AVID}},
	url = {http://arxiv.org/abs/1912.04443},
	doi = {10.48550/arXiv.1912.04443},
	abstract = {Robotic reinforcement learning (RL) holds the promise of enabling robots to learn complex behaviors through experience. However, realizing this promise for long-horizon tasks in the real world requires mechanisms to reduce human burden in terms of defining the task and scaffolding the learning process. In this paper, we study how these challenges can be alleviated with an automated robotic learning framework, in which multi-stage tasks are defined simply by providing videos of a human demonstrator and then learned autonomously by the robot from raw image observations. A central challenge in imitating human videos is the difference in appearance between the human and robot, which typically requires manual correspondence. We instead take an automated approach and perform pixel-level image translation via CycleGAN to convert the human demonstration into a video of a robot, which can then be used to construct a reward function for a model-based RL algorithm. The robot then learns the task one stage at a time, automatically learning how to reset each stage to retry it multiple times without human-provided resets. This makes the learning process largely automatic, from intuitive task specification via a video to automated training with minimal human intervention. We demonstrate that our approach is capable of learning complex tasks, such as operating a coffee machine, directly from raw image observations, requiring only 20 minutes to provide human demonstrations and about 180 minutes of robot interaction.},
	urldate = {2022-07-29},
	publisher = {arXiv},
	author = {Smith, Laura and Dhawan, Nikita and Zhang, Marvin and Abbeel, Pieter and Levine, Sergey},
	month = jun,
	year = {2020},
	note = {arXiv:1912.04443 [cs]},
	keywords = {Imitation},
}

@inproceedings{liu_imitation_2018,
	title = {Imitation from {Observation}: {Learning} to {Imitate} {Behaviors} from {Raw} {Video} via {Context} {Translation}},
	shorttitle = {Imitation from {Observation}},
	doi = {10.1109/ICRA.2018.8462901},
	abstract = {Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, object positions and types, and other factors. We term this kind of imitation learning “imitation-from-observation,” and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations in the same environment configuration, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show the effectiveness of our approach in learning a wide range of real-world robotic tasks modeled after common household chores from videos of a human demonstrator, including sweeping, ladling almonds, pushing objects as well as a number of tasks in simulation.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Imitation},
	pages = {1118--1125},
}

@inproceedings{xiong_learning_2021,
	title = {Learning by {Watching}: {Physical} {Imitation} of {Manipulation} {Skills} from {Human} {Videos}},
	shorttitle = {Learning by {Watching}},
	doi = {10.1109/IROS51168.2021.9636080},
	abstract = {Learning from visual data opens the potential to accrue a large range of manipulation behaviors by leveraging human demonstrations without specifying each of them mathe-matically, but rather through natural task specification. In this paper, we present Learning by Watching (LbW), an algorithmic framework for policy learning through imitation from a single video specifying the task. The key insights of our method are two-fold. First, since the human arms may not have the same morphology as robot arms, our framework learns unsupervised human to robot translation to overcome the morphology mis-match issue. Second, to capture the details in salient regions that are crucial for learning state representations, our model performs unsupervised keypoint detection on the translated robot videos. The detected keypoints form a structured representation that contains semantically meaningful information and can be used directly for computing reward and policy learning. We evaluate the effectiveness of our LbW framework on five robot manipulation tasks, including reaching, pushing, sliding, coffee making, and drawer closing. Extensive experimental evaluations demonstrate that our method performs favorably against the state-of-the-art approaches. More results and analysis are available at pair.toronto.edu/lbw-kp/.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Xiong, Haoyu and Li, Quanzhou and Chen, Yun-Chun and Bharadhwaj, Homanga and Sinha, Samarth and Garg, Animesh},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Imitation},
	pages = {7827--7834},
}

@inproceedings{pavlakos_human_2022,
	title = {Human {Mesh} {Recovery} {From} {Multiple} {Shots}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Pavlakos_Human_Mesh_Recovery_From_Multiple_Shots_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-29},
	author = {Pavlakos, Georgios and Malik, Jitendra and Kanazawa, Angjoo},
	year = {2022},
	pages = {1485--1495},
}

@inproceedings{taheri_goal_2022,
	title = {{GOAL}: {Generating} {4D} {Whole}-{Body} {Motion} for {Hand}-{Object} {Grasping}},
	shorttitle = {{GOAL}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Taheri_GOAL_Generating_4D_Whole-Body_Motion_for_Hand-Object_Grasping_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-29},
	author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios},
	year = {2022},
	pages = {13263--13273},
}

@inproceedings{georgakis_cross-modal_2022,
	title = {Cross-{Modal} {Map} {Learning} for {Vision} and {Language} {Navigation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Georgakis_Cross-Modal_Map_Learning_for_Vision_and_Language_Navigation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-29},
	author = {Georgakis, Georgios and Schmeckpeper, Karl and Wanchoo, Karan and Dan, Soham and Miltsakaki, Eleni and Roth, Dan and Daniilidis, Kostas},
	year = {2022},
	pages = {15460--15470},
}

@misc{harley_particle_2022,
	title = {Particle {Video} {Revisited}: {Tracking} {Through} {Occlusions} {Using} {Point} {Trajectories}},
	shorttitle = {Particle {Video} {Revisited}},
	url = {http://arxiv.org/abs/2204.04153},
	doi = {10.48550/arXiv.2204.04153},
	abstract = {Tracking pixels in videos is typically studied as an optical flow estimation problem, where every pixel is described with a displacement vector that locates it in the next frame. Even though wider temporal context is freely available, prior efforts to take this into account have yielded only small gains over 2-frame methods. In this paper, we revisit Sand and Teller's "particle video" approach, and study pixel tracking as a long-range motion estimation problem, where every pixel is described with a trajectory that locates it in multiple future frames. We re-build this classic approach using components that drive the current state-of-the-art in flow and object tracking, such as dense cost maps, iterative optimization, and learned appearance updates. We train our models using long-range amodal point trajectories mined from existing optical flow data that we synthetically augment with multi-frame occlusions. We test our approach in trajectory estimation benchmarks and in keypoint label propagation tasks, and compare favorably against state-of-the-art optical flow and feature tracking methods.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Harley, Adam W. and Fang, Zhaoyuan and Fragkiadaki, Katerina},
	month = jul,
	year = {2022},
	note = {arXiv:2204.04153 [cs]},
	keywords = {Optical Flow},
}

@inproceedings{jiang_bongard-hoi_2022,
	title = {Bongard-{HOI}: {Benchmarking} {Few}-{Shot} {Visual} {Reasoning} for {Human}-{Object} {Interactions}},
	shorttitle = {Bongard-{HOI}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Jiang_Bongard-HOI_Benchmarking_Few-Shot_Visual_Reasoning_for_Human-Object_Interactions_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-28},
	author = {Jiang, Huaizu and Ma, Xiaojian and Nie, Weili and Yu, Zhiding and Zhu, Yuke and Anandkumar, Anima},
	year = {2022},
	keywords = {HOI},
	pages = {19056--19065},
}

@misc{ren_corri2p_2022,
	title = {{CorrI2P}: {Deep} {Image}-to-{Point} {Cloud} {Registration} via {Dense} {Correspondence}},
	shorttitle = {{CorrI2P}},
	url = {http://arxiv.org/abs/2207.05483},
	doi = {10.48550/arXiv.2207.05483},
	abstract = {Motivated by the intuition that the critical step of localizing a 2D image in the corresponding 3D point cloud is establishing 2D-3D correspondence between them, we propose the first feature-based dense correspondence framework for addressing the image-to-point cloud registration problem, dubbed CorrI2P, which consists of three modules, i.e., feature embedding, symmetric overlapping region detection, and pose estimation through the established correspondence. Specifically, given a pair of a 2D image and a 3D point cloud, we first transform them into high-dimensional feature space and feed the resulting features into a symmetric overlapping region detector to determine the region where the image and point cloud overlap each other. Then we use the features of the overlapping regions to establish the 2D-3D correspondence before running EPnP within RANSAC to estimate the camera's pose. Experimental results on KITTI and NuScenes datasets show that our CorrI2P outperforms state-of-the-art image-to-point cloud registration methods significantly. We will make the code publicly available.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Ren, Siyu and Zeng, Yiming and Hou, Junhui and Chen, Xiaodong},
	month = jul,
	year = {2022},
	note = {arXiv:2207.05483 [cs]},
	keywords = {Correspondence, Registration},
}

@inproceedings{wang_translating_2022,
	title = {Translating a {Visual} {LEGO} {Manual} to a {Machine}-{Executable} {Plan}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Wang, Ruocheng and Zhang, Yunzhi and Mao, Jiayuan and Cheng, Chin-Yi and Wu, Jiajun},
	year = {2022},
}

@inproceedings{yao_mvsnet_2018,
	title = {{MVSNet}: {Depth} {Inference} for {Unstructured} {Multi}-view {Stereo}},
	shorttitle = {{MVSNet}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.html},
	urldate = {2022-07-27},
	author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Fang, Tian and Quan, Long},
	year = {2018},
	pages = {767--783},
}

@misc{krishnan_deep_2015,
	title = {Deep {Kalman} {Filters}},
	url = {http://arxiv.org/abs/1511.05121},
	doi = {10.48550/arXiv.1511.05121},
	abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing MNIST" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	month = nov,
	year = {2015},
	note = {arXiv:1511.05121 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ahmadi_unsupervised_2016,
	title = {Unsupervised convolutional neural networks for motion estimation},
	doi = {10.1109/ICIP.2016.7532634},
	abstract = {Traditional methods for motion estimation estimate the motion field F between a pair of images as the one that minimizes a predesigned cost function. In this paper, we propose a direct method and train a Convolutional Neural Network (CNN) that when, at test time, is given a pair of images as input it produces a dense motion field F at its output layer. In the absence of large datasets with ground truth motion that would allow classical supervised training, we propose to train the network in an unsupervised manner. The proposed cost function that is optimized during training, is based on the classical optical flow constraint. The latter is differentiable with respect to the motion field and, therefore, allows backpropagation of the error to previous layers of the network. Our method is tested on both synthetic and real image sequences and performs similarly to the state-of-the-art methods.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Ahmadi, Aria and Patras, Ioannis},
	month = sep,
	year = {2016},
	note = {ISSN: 2381-8549},
	keywords = {Adaptive optics, Convolutional Neural Network, Convolutional codes, Cost function, Motion Estimation, Motion estimation, Neural networks, Optical imaging, Training, Unsupervised Training},
	pages = {1629--1633},
}

@inproceedings{zhou_unsupervised_2017,
	title = {Unsupervised {Learning} of {Stereo} {Matching}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Unsupervised_Learning_of_ICCV_2017_paper.html},
	urldate = {2022-07-27},
	author = {Zhou, Chao and Zhang, Hong and Shen, Xiaoyong and Jia, Jiaya},
	year = {2017},
	keywords = {Stereo},
	pages = {1567--1575},
}

@inproceedings{tonioni_real-time_2019,
	title = {Real-{Time} {Self}-{Adaptive} {Deep} {Stereo}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.html},
	urldate = {2022-07-27},
	author = {Tonioni, Alessio and Tosi, Fabio and Poggi, Matteo and Mattoccia, Stefano and Stefano, Luigi Di},
	year = {2019},
	keywords = {Stereo},
	pages = {195--204},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {Meta Learning},
	pages = {1126--1135},
}

@article{yu_one-shot_2018,
	title = {One-{Shot} {Imitation} from {Observing} {Humans} via {Domain}-{Adaptive} {Meta}-{Learning}},
	url = {https://arxiv.org/abs/1802.01557v1},
	doi = {10.48550/arXiv.1802.01557},
	abstract = {Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.},
	language = {en},
	urldate = {2022-07-26},
	author = {Yu, Tianhe and Finn, Chelsea and Xie, Annie and Dasari, Sudeep and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
	month = feb,
	year = {2018},
	keywords = {Imitation},
}

@inproceedings{pastor_learning_2009,
	title = {Learning and generalization of motor skills by learning from demonstration},
	doi = {10.1109/ROBOT.2009.5152385},
	abstract = {We provide a general approach for learning robotic motor skills from human demonstration. To represent an observed movement, a non-linear differential equation is learned such that it reproduces this movement. Based on this representation, we build a library of movements by labeling each recorded movement according to task and context (e.g., grasping, placing, and releasing). Our differential equation is formulated such that generalization can be achieved simply by adapting a start and a goal parameter in the equation to the desired position values of a movement. For object manipulation, we present how our framework extends to the control of gripper orientation and finger position. The feasibility of our approach is demonstrated in simulation as well as on the Sarcos dextrous robot arm. The robot learned a pick-and-place operation and a water-serving task and could generalize these tasks to novel situations.},
	booktitle = {2009 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Pastor, Peter and Hoffmann, Heiko and Asfour, Tamim and Schaal, Stefan},
	month = may,
	year = {2009},
	note = {ISSN: 1050-4729},
	keywords = {Imitation},
	pages = {763--768},
}

@inproceedings{gartner_trajectory_2022,
	title = {Trajectory {Optimization} for {Physics}-{Based} {Reconstruction} of {3D} {Human} {Pose} {From} {Monocular} {Video}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Gartner_Trajectory_Optimization_for_Physics-Based_Reconstruction_of_3D_Human_Pose_From_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-25},
	author = {Gärtner, Erik and Andriluka, Mykhaylo and Xu, Hongyi and Sminchisescu, Cristian},
	year = {2022},
	keywords = {Trajectory},
	pages = {13106--13115},
}

@misc{bahl_human--robot_2022,
	title = {Human-to-{Robot} {Imitation} in the {Wild}},
	url = {http://arxiv.org/abs/2207.09450},
	abstract = {We approach the problem of learning by watching humans in the wild. While traditional approaches in Imitation and Reinforcement Learning are promising for learning in the real world, they are either sample inefficient or are constrained to lab settings. Meanwhile, there has been a lot of success in processing passive, unstructured human data. We propose tackling this problem via an efficient one-shot robot learning algorithm, centered around learning from a third-person perspective. We call our method WHIRL: In-the-Wild Human Imitating Robot Learning. WHIRL extracts a prior over the intent of the human demonstrator, using it to initialize our agent's policy. We introduce an efficient real-world policy learning scheme that improves using interactions. Our key contributions are a simple sampling-based policy optimization approach, a novel objective function for aligning human and robot videos as well as an exploration method to boost sample efficiency. We show one-shot generalization and success in real-world settings, including 20 different manipulation tasks in the wild. Videos and talk at https://human2robot.github.io},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Bahl, Shikhar and Gupta, Abhinav and Pathak, Deepak},
	month = jul,
	year = {2022},
	note = {arXiv:2207.09450 [cs, eess]},
	keywords = {Imitation},
}

@inproceedings{song_talking_2022,
	title = {Talking {Face} {Generation} {With} {Multilingual} {TTS}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Song_Talking_Face_Generation_With_Multilingual_TTS_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-19},
	author = {Song, Hyoung-Kyu and Woo, Sang Hoon and Lee, Junhyeok and Yang, Seungmin and Cho, Hyunjae and Lee, Youseong and Choi, Dongho and Kim, Kang-wook},
	year = {2022},
	pages = {21425--21430},
}

@inproceedings{preechakul_diffusion_2022,
	title = {Diffusion {Autoencoders}: {Toward} a {Meaningful} and {Decodable} {Representation}},
	shorttitle = {Diffusion {Autoencoders}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-18},
	author = {Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
	year = {2022},
	keywords = {autoencoder},
	pages = {10619--10629},
}

@inproceedings{worchel_multi-view_2022,
	title = {Multi-{View} {Mesh} {Reconstruction} {With} {Neural} {Deferred} {Shading}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Worchel_Multi-View_Mesh_Reconstruction_With_Neural_Deferred_Shading_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-17},
	author = {Worchel, Markus and Diaz, Rodrigo and Hu, Weiwen and Schreer, Oliver and Feldmann, Ingo and Eisert, Peter},
	year = {2022},
	keywords = {NeRF},
	pages = {6187--6197},
}

@article{labbe_rtab-map_2019,
	title = {{RTAB}-{Map} as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation},
	volume = {36},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21831},
	doi = {10.1002/rob.21831},
	abstract = {Distributed as an open-source library since 2013, real-time appearance-based mapping (RTAB-Map) started as an appearance-based loop closure detection approach with memory management to deal with large-scale and long-term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual- or lidar-based, comparison is difficult. Therefore, we decided to extend RTAB-Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB-Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
	language = {en},
	number = {2},
	urldate = {2022-07-16},
	journal = {Journal of Field Robotics},
	author = {Labbé, Mathieu and Michaud, François},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21831},
	keywords = {SLAM, vSLAM},
	pages = {416--446},
}

@article{hornung_octomap_2013,
	title = {{OctoMap}: an efficient probabilistic {3D} mapping framework based on octrees},
	volume = {34},
	issn = {1573-7527},
	shorttitle = {{OctoMap}},
	url = {https://doi.org/10.1007/s10514-012-9321-0},
	doi = {10.1007/s10514-012-9321-0},
	abstract = {Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and models the data consistently while keeping the memory requirement at a minimum.},
	language = {en},
	number = {3},
	urldate = {2022-07-16},
	journal = {Autonomous Robots},
	author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
	month = apr,
	year = {2013},
	keywords = {Octree},
	pages = {189--206},
}

@inproceedings{teed_raft_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{RAFT}: {Recurrent} {All}-{Pairs} {Field} {Transforms} for {Optical} {Flow}},
	isbn = {978-3-030-58536-5},
	shorttitle = {{RAFT}},
	doi = {10.1007/978-3-030-58536-5_24},
	abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Teed, Zachary and Deng, Jia},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {402--419},
}

@inproceedings{you_learning_2022,
	title = {Learning {To} {Detect} {Mobile} {Objects} {From} {LiDAR} {Scans} {Without} {Labels}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/You_Learning_To_Detect_Mobile_Objects_From_LiDAR_Scans_Without_Labels_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-14},
	author = {You, Yurong and Luo, Katie and Phoo, Cheng Perng and Chao, Wei-Lun and Sun, Wen and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	year = {2022},
	keywords = {Point Cloud},
	pages = {1130--1140},
}

@inproceedings{diaz-ruiz_ithaca365_2022,
	title = {Ithaca365: {Dataset} and {Driving} {Perception} {Under} {Repeated} and {Challenging} {Weather} {Conditions}},
	shorttitle = {Ithaca365},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Diaz-Ruiz_Ithaca365_Dataset_and_Driving_Perception_Under_Repeated_and_Challenging_Weather_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-14},
	author = {Diaz-Ruiz, Carlos A. and Xia, Youya and You, Yurong and Nino, Jose and Chen, Junan and Monica, Josephine and Chen, Xiangyu and Luo, Katie and Wang, Yan and Emond, Marc and Chao, Wei-Lun and Hariharan, Bharath and Weinberger, Kilian Q. and Campbell, Mark},
	year = {2022},
	keywords = {Point Cloud},
	pages = {21383--21392},
}

@misc{you_hindsight_2022,
	title = {Hindsight is 20/20: {Leveraging} {Past} {Traversals} to {Aid} {3D} {Perception}},
	shorttitle = {Hindsight is 20/20},
	url = {http://arxiv.org/abs/2203.11405},
	doi = {10.48550/arXiv.2203.11405},
	abstract = {Self-driving cars must detect vehicles, pedestrians, and other traffic participants accurately to operate safely. Small, far-away, or highly occluded objects are particularly challenging because there is limited information in the LiDAR point clouds for detecting them. To address this challenge, we leverage valuable information from the past: in particular, data collected in past traversals of the same scene. We posit that these past data, which are typically discarded, provide rich contextual information for disambiguating the above-mentioned challenging cases. To this end, we propose a novel, end-to-end trainable Hindsight framework to extract this contextual information from past traversals and store it in an easy-to-query data structure, which can then be leveraged to aid future 3D object detection of the same scene. We show that this framework is compatible with most modern 3D detection architectures and can substantially improve their average precision on multiple autonomous driving datasets, most notably by more than 300\% on the challenging cases.},
	urldate = {2022-07-14},
	publisher = {arXiv},
	author = {You, Yurong and Luo, Katie Z. and Chen, Xiangyu and Chen, Junan and Chao, Wei-Lun and Sun, Wen and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.11405 [cs]},
	keywords = {Point Cloud},
}

@inproceedings{li_bnv-fusion_2022,
	title = {{BNV}-{Fusion}: {Dense} {3D} {Reconstruction} using {Bi}-level {Neural} {Volume} {Fusion}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Kejie and Tang, Yansong and Prisacariu, Victor Adrian and Torr, Philip HS},
	year = {2022},
	keywords = {NeRF},
}

@article{hu_dynamic_2021,
	title = {Dynamic {Point} {Cloud} {Denoising} via {Manifold}-to-{Manifold} {Distance}},
	volume = {30},
	doi = {10.1109/TIP.2021.3092826},
	journal = {IEEE Transactions on Image Processing},
	author = {Hu, Wei and Hu, Qianjiang and Wang, Zehua and Gao, Xiang},
	year = {2021},
	keywords = {Point Cloud},
	pages = {6168--6183},
}

@inproceedings{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = jul,
	year = {2017},
	keywords = {Point Cloud},
}

@inproceedings{yu_plenoctrees_2021,
	title = {{PlenOctrees} for {Real}-{Time} {Rendering} of {Neural} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yu, Alex and Li, Ruilong and Tancik, Matthew and Li, Hao and Ng, Ren and Kanazawa, Angjoo},
	month = oct,
	year = {2021},
	keywords = {NeRF, Octree},
	pages = {5752--5761},
}

@misc{braun_n-qgn_2022,
	title = {N-{QGN}: {Navigation} {Map} from a {Monocular} {Camera} using {Quadtree} {Generating} {Networks}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2202.11982},
	publisher = {arXiv},
	author = {Braun, Daniel and Morel, Olivier and Vasseur, Pascal and Demonceaux, Cédric},
	year = {2022},
	doi = {10.48550/ARXIV.2202.11982},
	keywords = {Monocular Depth Estimation, Octree},
}

@article{long_fully_2014,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {abs/1411.4038},
	url = {http://arxiv.org/abs/1411.4038},
	journal = {CoRR},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2014},
	note = {arXiv: 1411.4038},
	keywords = {FCN},
}

@inproceedings{chitta_quadtree_2020,
	title = {Quadtree {Generating} {Networks}: {Efficient} {Hierarchical} {Scene} {Parsing} with {Sparse} {Convolutions}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chitta, Kashyap and Alvarez, Jose M. and Hebert, Martial},
	month = mar,
	year = {2020},
	keywords = {Octree},
}

@article{chen_deeplab_2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	doi = {10.1109/TPAMI.2017.2699184},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	year = {2018},
	keywords = {CRF, Image Segmentation},
	pages = {834--848},
}

@inproceedings{krahenbuhl_efficient_2011,
	title = {Efficient {Inference} in {Fully} {Connected} {CRFs} with {Gaussian} {Edge} {Potentials}},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper/2011/file/beda24c1e1b46055dff2c39c98fd6fc1-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krähenbühl, Philipp and Koltun, Vladlen},
	editor = {Shawe-Taylor, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	keywords = {CRF},
}

@inproceedings{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	keywords = {Pyramid},
}

@article{he_spatial_2015,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {37},
	doi = {10.1109/TPAMI.2015.2389824},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	keywords = {Pyramid},
	pages = {1904--1916},
}

@inproceedings{grauman_pyramid_2005,
	title = {The pyramid match kernel: discriminative classification with sets of image features},
	volume = {2},
	doi = {10.1109/ICCV.2005.239},
	booktitle = {Tenth {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}'05) {Volume} 1},
	author = {Grauman, K. and Darrell, T.},
	year = {2005},
	keywords = {Pyramid},
	pages = {1458--1465 Vol. 2},
}

@article{liu_learning_2016,
	title = {Learning {Depth} from {Single} {Monocular} {Images} {Using} {Deep} {Convolutional} {Neural} {Fields}},
	volume = {38},
	doi = {10.1109/TPAMI.2015.2505283},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian},
	year = {2016},
	keywords = {Monocular Depth Estimation},
	pages = {2024--2039},
}

@inproceedings{saxena_learning_2005,
	title = {Learning {Depth} from {Single} {Monocular} {Images}},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Saxena, Ashutosh and Chung, Sung and Ng, Andrew},
	editor = {Weiss, Y. and Schölkopf, B. and Platt, J.},
	year = {2005},
	keywords = {Monocular Depth Estimation},
}

@article{wang_o-cnn_2017,
	title = {O-{CNN}: {Octree}-based {Convolutional} {Neural} {Networks} for {3D} {Shape} {Analysis}},
	volume = {36},
	number = {4},
	journal = {ACM Transactions on Graphics (SIGGRAPH)},
	author = {Wang, Peng-Shuai and Liu, Yang and Guo, Yu-Xiao and Sun, Chun-Yu and Tong, Xin},
	year = {2017},
	keywords = {3D Reconstruction, Octree, Sparse},
}

@inproceedings{graham_3d_2018,
	title = {{3D} {Semantic} {Segmentation} {With} {Submanifold} {Sparse} {Convolutional} {Networks}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Graham, Benjamin and Engelcke, Martin and van der Maaten, Laurens},
	month = jun,
	year = {2018},
	keywords = {Sparse},
}

@inproceedings{wei_nerfingmvs_2021,
	title = {{NerfingMVS}: {Guided} {Optimization} of {Neural} {Radiance} {Fields} for {Indoor} {Multi}-{View} {Stereo}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wei, Yi and Liu, Shaohui and Rao, Yongming and Zhao, Wang and Lu, Jiwen and Zhou, Jie},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5610--5619},
}

@inproceedings{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5855--5864},
}

@inproceedings{liu_editing_2021,
	title = {Editing {Conditional} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Steven and Zhang, Xiuming and Zhang, Zhoutong and Zhang, Richard and Zhu, Jun-Yan and Russell, Bryan},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5773--5783},
}

@article{zhang_nerf_2020,
	title = {{NeRF}++: {Analyzing} and {Improving} {Neural} {Radiance} {Fields}},
	volume = {abs/2010.07492},
	url = {https://arxiv.org/abs/2010.07492},
	journal = {CoRR},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	year = {2020},
	note = {arXiv: 2010.07492},
	keywords = {NeRF},
}

@inproceedings{meng_gnerf_2021,
	title = {{GNeRF}: {GAN}-{Based} {Neural} {Radiance} {Field} {Without} {Posed} {Camera}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Meng, Quan and Chen, Anpei and Luo, Haimin and Wu, Minye and Su, Hao and Xu, Lan and He, Xuming and Yu, Jingyi},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {6351--6361},
}

@inproceedings{zhu_nice-slam_2022,
	title = {{NICE}-{SLAM}: {Neural} {Implicit} {Scalable} {Encoding} for {SLAM}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {12786--12796},
}

@inproceedings{sucar_imap_2021,
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {6229--6238},
}

@article{wang_nerf-_2021,
	title = {{NeRF}-: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	volume = {abs/2102.07064},
	url = {https://arxiv.org/abs/2102.07064},
	journal = {CoRR},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	year = {2021},
	note = {arXiv: 2102.07064},
	keywords = {NeRF},
}

@inproceedings{yen-chen_inerf_2021,
	title = {{iNeRF}: {Inverting} {Neural} {Radiance} {Fields} for {Pose} {Estimation}},
	doi = {10.1109/IROS51168.2021.9636708},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yen-Chen, Lin and Florence, Pete and Barron, Jonathan T. and Rodriguez, Alberto and Isola, Phillip and Lin, Tsung-Yi},
	year = {2021},
	keywords = {NeRF},
	pages = {1323--1330},
}

@inproceedings{jain_putting_2021,
	title = {Putting {NeRF} on a {Diet}: {Semantically} {Consistent} {Few}-{Shot} {View} {Synthesis}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Jain, Ajay and Tancik, Matthew and Abbeel, Pieter},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5885--5894},
}

@inproceedings{wang_ibrnet_2021,
	title = {{IBRNet}: {Learning} {Multi}-{View} {Image}-{Based} {Rendering}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Qianqian and Wang, Zhicheng and Genova, Kyle and Srinivasan, Pratul P. and Zhou, Howard and Barron, Jonathan T. and Martin-Brualla, Ricardo and Snavely, Noah and Funkhouser, Thomas},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {4690--4699},
}

@inproceedings{yu_pixelnerf_2021,
	title = {{pixelNeRF}: {Neural} {Radiance} {Fields} {From} {One} or {Few} {Images}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yu, Alex and Ye, Vickie and Tancik, Matthew and Kanazawa, Angjoo},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {4578--4587},
}

@inproceedings{du_neural_2021,
	address = {Los Alamitos, CA, USA},
	title = {Neural {Radiance} {Flow} for {4D} {View} {Synthesis} and {Video} {Processing}},
	url = {https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.01406},
	doi = {10.1109/ICCV48922.2021.01406},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE Computer Society},
	author = {Du, Y. and Zhang, Y. and Yu, H. and Tenenbaum, J. B. and Wu, J.},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {14304--14314},
}

@inproceedings{li_neural_2021,
	title = {Neural {Scene} {Flow} {Fields} for {Space}-{Time} {View} {Synthesis} of {Dynamic} {Scenes}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Li, Zhengqi and Niklaus, Simon and Snavely, Noah and Wang, Oliver},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {6498--6508},
}

@article{liu_neural_2021,
	title = {Neural {Actor}: {Neural} {Free}-{View} {Synthesis} of {Human} {Actors} with {Pose} {Control}},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3478513.3480528},
	doi = {10.1145/3478513.3480528},
	abstract = {We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is developed upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as a proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high-fidelity dynamic geometry and appearance, NA leverages 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports shape control on the free-view synthesis of human actors.},
	number = {6},
	journal = {ACM Trans. Graph.},
	author = {Liu, Lingjie and Habermann, Marc and Rudnev, Viktor and Sarkar, Kripasindhu and Gu, Jiatao and Theobalt, Christian},
	month = dec,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {NeRF},
}

@inproceedings{tretschk_non-rigid_2021,
	title = {Non-{Rigid} {Neural} {Radiance} {Fields}: {Reconstruction} and {Novel} {View} {Synthesis} of a {Dynamic} {Scene} {From} {Monocular} {Video}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Tretschk, Edgar and Tewari, Ayush and Golyanik, Vladislav and Zollhöfer, Michael and Lassner, Christoph and Theobalt, Christian},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {12959--12970},
}

@inproceedings{gafni_dynamic_2021,
	title = {Dynamic {Neural} {Radiance} {Fields} for {Monocular} {4D} {Facial} {Avatar} {Reconstruction}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Gafni, Guy and Thies, Justus and Zollhofer, Michael and Niessner, Matthias},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {8649--8658},
}

@inproceedings{pumarola_d-nerf_2021,
	title = {D-{NeRF}: {Neural} {Radiance} {Fields} for {Dynamic} {Scenes}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Pumarola, Albert and Corona, Enric and Pons-Moll, Gerard and Moreno-Noguer, Francesc},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {10318--10327},
}

@inproceedings{park_nerfies_2021,
	title = {Nerfies: {Deformable} {Neural} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5865--5874},
}

@inproceedings{sun_direct_2022,
	title = {Direct {Voxel} {Grid} {Optimization}: {Super}-{Fast} {Convergence} for {Radiance} {Fields} {Reconstruction}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {5459--5469},
}

@inproceedings{sitzmann_light_2021,
	title = {Light {Field} {Networks}: {Neural} {Scene} {Representations} with {Single}-{Evaluation} {Rendering}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/a11ce019e96a4c60832eadd755a17a58-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sitzmann, Vincent and Rezchikov, Semon and Freeman, Bill and Tenenbaum, Josh and Durand, Fredo},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {NeRF},
	pages = {19313--19325},
}

@article{lombardi_mixture_2021,
	title = {Mixture of {Volumetric} {Primitives} for {Efficient} {Neural} {Rendering}},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3450626.3459863},
	doi = {10.1145/3450626.3459863},
	abstract = {Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.},
	number = {4},
	journal = {ACM Trans. Graph.},
	author = {Lombardi, Stephen and Simon, Tomas and Schwartz, Gabriel and Zollhoefer, Michael and Sheikh, Yaser and Saragih, Jason},
	month = jul,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {NeRF},
}

@inproceedings{reiser_kilonerf_2021,
	title = {{KiloNeRF}: {Speeding} {Up} {Neural} {Radiance} {Fields} {With} {Thousands} of {Tiny} {MLPs}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Reiser, Christian and Peng, Songyou and Liao, Yiyi and Geiger, Andreas},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {14335--14345},
}

@inproceedings{garbin_fastnerf_2021,
	title = {{FastNeRF}: {High}-{Fidelity} {Neural} {Rendering} at {200FPS}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Garbin, Stephan J. and Kowalski, Marek and Johnson, Matthew and Shotton, Jamie and Valentin, Julien},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {14346--14355},
}

@inproceedings{rebain_derf_2021,
	title = {{DeRF}: {Decomposed} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Rebain, Daniel and Jiang, Wei and Yazdani, Soroosh and Li, Ke and Yi, Kwang Moo and Tagliasacchi, Andrea},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {14153--14161},
}

@inproceedings{liu_neural_2020,
	title = {Neural {Sparse} {Voxel} {Fields}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/b4b758962f17808746e9bb832a6fa4b8-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Lingjie and Gu, Jiatao and Zaw Lin, Kyaw and Chua, Tat-Seng and Theobalt, Christian},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {NeRF},
	pages = {15651--15663},
}

@inproceedings{lindell_autoint_2021,
	title = {{AutoInt}: {Automatic} {Integration} for {Fast} {Neural} {Volume} {Rendering}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lindell, David B. and Martel, Julien N. P. and Wetzstein, Gordon},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {14556--14565},
}

@inproceedings{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	booktitle = {{ECCV}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	year = {2020},
	keywords = {NeRF},
}

@inproceedings{chen_mvsnerf_2021,
	title = {{MVSNeRF}: {Fast} {Generalizable} {Radiance} {Field} {Reconstruction} {From} {Multi}-{View} {Stereo}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Anpei and Xu, Zexiang and Zhao, Fuqiang and Zhang, Xiaoshuai and Xiang, Fanbo and Yu, Jingyi and Su, Hao},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {14124--14133},
}

@inproceedings{lin_barf_2021,
	title = {{BARF}: {Bundle}-{Adjusting} {Neural} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Lin, Chen-Hsuan and Ma, Wei-Chiu and Torralba, Antonio and Lucey, Simon},
	month = oct,
	year = {2021},
	keywords = {NeRF},
	pages = {5741--5751},
}

@inproceedings{martin-brualla_nerf_2021,
	title = {{NeRF} in the {Wild}: {Neural} {Radiance} {Fields} for {Unconstrained} {Photo} {Collections}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
	month = jun,
	year = {2021},
	keywords = {NeRF},
	pages = {7210--7219},
}

@inproceedings{fridovich-keil_plenoxels_2022,
	title = {Plenoxels: {Radiance} {Fields} {Without} {Neural} {Networks}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Fridovich-Keil, Sara and Yu, Alex and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {5501--5510},
}

@inproceedings{johari_geonerf_2022,
	title = {{GeoNeRF}: {Generalizing} {NeRF} {With} {Geometry} {Priors}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Johari, Mohammad Mahdi and Lepoittevin, Yann and Fleuret, François},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {18365--18375},
}

@inproceedings{hu_efficientnerf_2022,
	title = {{EfficientNeRF} {Efficient} {Neural} {Radiance} {Fields}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Hu, Tao and Liu, Shu and Chen, Yilun and Shen, Tiancheng and Jia, Jiaya},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {12902--12911},
}

@article{tancik_block-nerf_2022,
	title = {Block-{NeRF}: {Scalable} {Large} {Scene} {Neural} {View} {Synthesis}},
	journal = {arXiv},
	author = {Tancik, Matthew and Casser, Vincent and Yan, Xinchen and Pradhan, Sabeek and Mildenhall, Ben and Srinivasan, Pratul and Barron, Jonathan T. and Kretzschmar, Henrik},
	year = {2022},
	keywords = {NeRF},
}

@inproceedings{chibane_stereo_2021,
	title = {Stereo {Radiance} {Fields} ({SRF}): {Learning} {View} {Synthesis} from {Sparse} {Views} of {Novel} {Scenes}},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chibane, Julian and Bansal, Aayush and Lazova, Verica and Pons-Moll, Gerard},
	month = jun,
	year = {2021},
	keywords = {NeRF},
}

@article{muller_instant_2022,
	title = {Instant {Neural} {Graphics} {Primitives} with a {Multiresolution} {Hash} {Encoding}},
	volume = {41},
	url = {https://doi.org/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	number = {4},
	journal = {ACM Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	month = jul,
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: ACM},
	keywords = {NeRF},
	pages = {102:1--102:15},
}

@inproceedings{deng_depth-supervised_2022,
	title = {Depth-{Supervised} {NeRF}: {Fewer} {Views} and {Faster} {Training} for {Free}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Deng, Kangle and Liu, Andrew and Zhu, Jun-Yan and Ramanan, Deva},
	month = jun,
	year = {2022},
	keywords = {NeRF},
	pages = {12882--12891},
}

@article{martel_acorn_2021,
	title = {{ACORN}: {Adaptive} {Coordinate} {Networks} for {Neural} {Representation}},
	journal = {ACM Trans. Graph. (SIGGRAPH)},
	author = {Martel, Julien N.P. and Lindell, David B. and Lin, Connor Z. and Chan, Eric R. and Monteiro, Marco and Wetzstein, Gordon},
	year = {2021},
	keywords = {3D Reconstruction, NeRF, Octree},
}

@misc{lee_big_2019,
	title = {From {Big} to {Small}: {Multi}-{Scale} {Local} {Planar} {Guidance} for {Monocular} {Depth} {Estimation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1907.10326},
	publisher = {arXiv},
	author = {Lee, Jin Han and Han, Myung-Kyu and Ko, Dong Wook and Suh, Il Hong},
	year = {2019},
	doi = {10.48550/ARXIV.1907.10326},
	keywords = {Monocular Depth Estimation},
}
