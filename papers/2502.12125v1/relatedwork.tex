\section{RELATED WORKS}
\textbf{Hierarchical classification}. Errors in trained classifiers tend to occur
more frequently between closely related classes, such as different species of
animals or various types of vehicles. This pattern is evident in the
block-diagonal structure of the confusion matrix, provided that the classes are
properly sorted. Initially, the ImageNet dataset was organized according to the
lexical database of semantic relations WordNet \citep{fellbaum2010wordnet} under
assumption that it will help to build new efficient classification algorithms
\citep{russakovsky2015imagenet}. Since then lots of works tried to incorporate
hierarchy bias into the training process of deep neural networks: either in
architecture \citep{hinton2015distilling, Malashin2016, xiao2014error} or in
loss function \citep{redmon2017yolo9000, brust2019integrating}. Some authors
reported promising results. However these findings are often derived in
nonoptimal training settings \citep{brust2019integrating} or from closed
datasets, which complicates fair comparison (e.g. \citep{hinton2015distilling}
as example). It is also well-known that, in few-shot leaning settings removing
hierarchically related classes from the pretraining process significantly
impacts the performance on these classes \citep{vinyals2016matching}. This
suggests that features of the network inherently capture hierarchical
relationships. \textit{We believe our result provide insight into why
  hierarchical structure is not used in state-of-the-art solutions}.

\textbf{Frequency Principle}. The Frequency Principle (FP) or Spectral bias is a
phenomenon in neural network training, independently discovered in
\citep{rahaman2019spectral} and \citep{xu2019frequency}, which states that the
low-frequency components of the target function are learned first by the neural
network. The principle is formulated for the multidimensional spectrum of data
(where the number of dimensions equals the number of elements in the input
feature vector). However, its interpretation is often associated with
one-dimensional (time sequences) or two-dimensional (images) spaces
\citep{xu2024overview}.

Theoretical justification of FP remains challenging \citep{xu2024overview} and
its applicability may be limited in some cases \citep{zhang2020rethink, wang2023neural}.
% \cite{wang2023neural} study the
% classification task of a synthetic dataset generated with different frequencies,
% showing that the network quickly recognizes frequencies that are unique,
% regardless of whether they are low or high.
% it is shown that spectral
% dynamics are not monotonicâ€”high frequencies may disappear in the final phase of
% training; the authors use a portion of the data with random labels to control
% memorization and observe that the double descent phenomenon on test examples,
% associated with memorizing random examples, is accompanied by the disappearance
% of high frequencies from the neural network's spectrum.
Nevertheless, FP serves as a valuable tool to think about training dynamics,
providing intuitive interpretations for existing results. For example, deep
image prior (DIP) \citep{ulyanov2018deep} leverages randomly initialized network
to restore image. It can be explained more generally by FP: by inherently
learning low-frequency components before high-frequency details, neural networks
used in DIP naturally reconstruct the essential structures of images first.
\cite{beyazit2024inductive} use FP to explain the poorer performance of neural
networks on tabular data compared to other methods (such as decision trees)
\citep{shwartz2022tabular}. \cite{benjdiraa2023guided} suggest to use high-pass
filter before calculating the similarity metric between the reconstructed and
reference images to improve reconstruction.

% Tabular data typically contains heterogeneous information that is not spatially
% ordered and is of various natures, which results in the absence of a meaningful
% low-frequency component, making it challenging for neural networks to extract
% this information.

% The data itself may give clue: DL architectures nicely generalize from training
% on natural data, such as images or sound, but struggle with tabular data
% . In \citep{beyazit2024inductive} frequency
% principle is used to explain this fact.

\textit{Interpreting hypernym bias as a manifestation of Frequency Principle is
  complicated} by two factors: a) the need of Fourier Transform computation of
high-dimensional data, which suffers from the curse of dimensionality and b)
challenges in applying the principle to naturally unordered datasets (which is
true for image classification).
% \textit{We show that hypernym bias doesn't simplify to 2D image frequency bias}.

% It can be naturally interpreted for regression tasks of ordered datasets %  (such as time sequences), but classification datasets are naturally unordered.


\textbf{Simplicity bias}. \cite{arpit2017closer} have shown that simple patterns
are learned first before the network transitions into the noise memorization
phase. Simple patterns are associated with those that frequently occur across
many examples. \cite{arpit2017closer} compare learning dynamics on real data
with dynamics on noise (where no patterns are present). They suggest that
datasets contain a large number of simple examples with common patterns, which
are learned during the first epoch, after which the network spends most of its
time learning more complex examples. \cite{hu2020surprising} theoretically and
practically demonstrate that during the initial phase of training, a neural
network learns a linear function. In \citep{mangalam2019deep}, a more general
observation is experimentally demonstrated: during the early iterations,
networks learn to correctly recognize examples that can be distinguished by
shallow classifiers (such as SVMs or Random Forests). \cite{zhang2021linear}
emphasize that this property of neural networks helps prevent overfitting, even
in conditions of significant overparameterization. Therefore, early stopping can
be considered a method to prevent the learning of high-frequency noise
components in individual signal examples. Simplicity bias sometimes is
considered to be a pitfall \citep{shah2020pitfalls,pezeshki2022dynamics},
because it allows learning spurious correlations (shortcuts) and harms
generalization. These features are often not useful for generalization and are
referred as shortcuts \citep{geirhos2020shortcut, wang2022frequency}. A separate
research direction focuses on finding ways to reduce false correlations and
studying learning dynamics in their presence \citep{pezeshki2022dynamics,
  qiu2024complexity}.
% Frequency shortcut are special case \cite{wang2022frequency}
% , a special case of shortcuts in general .


The Frequency Principle and Simplicity Bias are closely related; the latter can
be thought of as more general but less quantifiable of the former. \cite{xu2021deep}
demonstrate that, in regression tasks, low frequencies carry the majority of the
information needed for signal reconstruction and are easier to learn.
% Although many researchers believe that Simplicity Bias positively affects the
% generalization capabilities of neural networks, \cite{shah2020pitfalls} shows
% that this bias can actually harm generalization, as the Simplicity Bias
% persists even when simple features have lower generalization capability than
% more complex ones.

%  For instance, in \cite{qiu2024complexity}, it is
% shown that the presence of false correlations slows down the learning of true
% features. In \cite{wang2022frequency}, an iterative algorithm is proposed to
% detect such shortcuts, characterized by the fact that high accuracy for
% individual classes can be achieved based on a small number of frequencies. In
% \cite{pezeshki2022dynamics}, the negative effect of quickly learning simple
% features is linked to gradient starvation in the cross-entropy loss function.

\textit{In simplicity bias terminology frequent features are called simple, and
  simplicity bias should manifest in accelerated learning of hypernym features
  common among many classes. This is what we robustly observe in neural networks
  trained in practical settings}.


\textbf{Neural collapse}. Neural collapse (NC) phenomenon was first described
by \cite{papyan2020prevalence} and then was further extended to broader settings
in \citep{dang2024neural}, \citep{fang2021exploring} and other works. Neural
collapse is a state of the last layer of the neural classifier, which can be
observed in the terminal phase of training. It is characterized by zero
within-class variability of features, when class means form vertices of the
Equiangular Tight Frame (ETF), and classification decision is similar to
prototypes \citep{snell2017prototypical}. While the ETF configuration is optimal
from the perspective of robustness to adversarial attacks
\citep{papyan2020prevalence}, its contribution to generalization is arguable, as
NC is observed only when accuracy on a balanced train set reaches 100\%
\citep{hui2022limitations}. Nevertheless knowledge about dynamics of the last
layer is helpful, for example, for designing few-shot learning algorithms, where
often only last layer is tuned \citep{yang2023neural}.

\textit{Neural collapse of penultimate layer can be seen as deeper
  representation of perfectly diagonal confusion matrix, which doesn't posses
  any information about hyponymy relations. From the perspective of hypernym
  bias NC is the final stage of top-to-bottom hierarchical label clustering
  process}.