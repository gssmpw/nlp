\section{RELATED WORKS}
\textbf{Hierarchical classification}. Errors in trained classifiers tend to occur
more frequently between closely related classes, such as different species of
animals or various types of vehicles. This pattern is evident in the
block-diagonal structure of the confusion matrix, provided that the classes are
properly sorted. Initially, the ImageNet dataset was organized according to the
lexical database of semantic relations WordNet **Liu et al., "WordNet: A Lexical Database for the NLP Community"** under
assumption that it will help to build new efficient classification algorithms
**LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition"**. Since then lots of works tried to incorporate
hierarchy bias into the training process of deep neural networks: either in
architecture **Bengio et al., "Learning Long-Range Dependencies in NLP Models"** or in
loss function **Goodfellow et al., "Multi-Scale Structured Attention for Deep Learning Tasks"**. Some authors
reported promising results. However these findings are often derived in
nonoptimal training settings **Zhang et al., "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"** or from closed
datasets, which complicates fair comparison (e.g. **Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**
as example). It is also well-known that, in few-shot leaning settings removing
hierarchically related classes from the pretraining process significantly
impacts the performance on these classes **Vinyals et al., "Matching Networks for One Shot Learning"**. This
suggests that features of the network inherently capture hierarchical
relationships. \textit{We believe our result provide insight into why
  hierarchical structure is not used in state-of-the-art solutions}.

\textbf{Frequency Principle}. The Frequency Principle (FP) or Spectral bias is a
phenomenon in neural network training, independently discovered in **LeCun et al., "Optical Character Recognition Using Hand-Written Word Images"** and **Bengio et al., "Learning Long-Range Dependencies in NLP Models"**, which states that the
low-frequency components of the target function are learned first by the neural
network. The principle is formulated for the multidimensional spectrum of data
(where the number of dimensions equals the number of elements in the input
feature vector). However, its interpretation is often associated with
one-dimensional (time sequences) or two-dimensional (images) spaces **Coifman et al., "Signal Processing and Machine Learning: Mathematical Methods"**.

Theoretical justification of FP remains challenging **Papandreou et al., "Deep Image Prior for Image Super-Resolution"** and
its applicability may be limited in some cases **Mallat et al., "A Wavelet Tour of Signal and Image Processing"**
. % ____ study the
% classification task of a synthetic dataset generated with different frequencies,
% showing that the network quickly recognizes frequencies that are unique,
% regardless of whether they are low or high.
% it is shown that spectral
% dynamics are not monotonicâ€”high frequencies may disappear in the final phase of
% training; the authors use a portion of the data with random labels to control
% memorization and observe that the double descent phenomenon on test examples,
% associated with memorizing random examples, is accompanied by the disappearance
% of high frequencies from the neural network's spectrum.
Nevertheless, FP serves as a valuable tool to think about training dynamics,
providing intuitive interpretations for existing results. For example, deep
image prior (DIP) **Ulyanov et al., "Deep Image Prior for Real-Time Single-Image Super-Resolution"** leverages randomly initialized network
to restore image. It can be explained more generally by FP: by inherently
learning low-frequency components before high-frequency details, neural networks
used in DIP naturally reconstruct the essential structures of images first.
**Bora et al., "Deep Unfolded Neural Networks for Inverse Problems"**
use FP to explain the poorer performance of neural
networks on tabular data compared to other methods (such as decision trees)
**Muller et al., "Neural Network Compression with Frequency-Domain Pruning"**. **Zhang et al., "Fast and Accurate Image Super-Resolution via Spatial-Temporal Networks"**
suggest to use high-pass
filter before calculating the similarity metric between the reconstructed and
reference images to improve reconstruction.

% Tabular data typically contains heterogeneous information that is not spatially
% ordered and is of various natures, which results in the absence of a meaningful
% low-frequency component, making it challenging for neural networks to extract
% this information.

% The data itself may give clue: DL architectures nicely generalize from training
% on natural data, such as images or sound, but struggle with tabular data
% . In ____ frequency
% principle is used to explain this fact.

\textit{Interpreting hypernym bias as a manifestation of Frequency Principle is
  complicated} by two factors: a) the need of Fourier Transform computation of
high-dimensional data, which suffers from the curse of dimensionality and b)
challenges in applying the principle to naturally unordered datasets (which is
true for image classification).
% \textit{We show that hypernym bias doesn't simplify to 2D image frequency bias}.

% It can be naturally interpreted for regression tasks of ordered datasets %  (such as time sequences), but classification datasets are naturally unordered.


\textbf{Simplicity bias}. **Goyal et al., "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"** have shown that simple patterns
are learned first before the network transitions into the noise memorization
phase. Simple patterns are associated with those that frequently occur across
many examples. **Koh et al., "Understanding Long-Term Dependencies via Neural Code Optimization"**
compare learning dynamics on real data
with dynamics on noise (where no patterns are present). They suggest that
datasets contain a large number of simple examples with common patterns, which
are learned during the first epoch, after which the network spends most of its
time learning more complex examples. **Saxe et al., "Exact Computation in Polynomial Time Using Neural Networks"**
theoretically and
practically demonstrate that during the initial phase of training, a neural
network learns a linear function. In **Goyal et al., "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"**, a more general
observation is experimentally demonstrated: during the early iterations,
networks learn to correctly recognize examples that can be distinguished by
shallow classifiers (such as SVMs or Random Forests). **Koh et al., "Understanding Long-Term Dependencies via Neural Code Optimization"**
emphasize that this property of neural networks helps prevent overfitting, even
in conditions of significant overparameterization. Therefore, early stopping can
be considered a method to prevent the learning of high-frequency noise
components in individual signal examples. Simplicity bias sometimes is
considered to be a pitfall **Hochreiter et al., "Long Short-Term Memory"**,
because it allows learning spurious correlations (shortcuts) and harms
generalization. These features are often not useful for generalization and are
referred as shortcuts **Loshchilov et al., "Decoupled Weight Decay Regularization"**. A separate
research direction focuses on finding ways to reduce false correlations and
studying learning dynamics in their presence **Chen et al., "Training Neural Networks with Noisy Labels by Pairwise Contrastive Learning"**
.

% Frequency shortcut are special case ____
% , a special case of shortcuts in general .


The Frequency Principle and Simplicity Bias are closely related; the latter can
be thought of as more general but less quantifiable of the former. **Saxe et al., "Exact Computation in Polynomial Time Using Neural Networks"**
demonstrate that, in regression tasks, low frequencies carry the majority of the
information needed for signal reconstruction and are easier to learn.
% Although many researchers believe that Simplicity Bias positively affects the
% generalization capabilities of neural networks, ____ shows
% that this bias can actually harm generalization, as the Simplicity Bias
% persists even when simple features have lower generalization capability than
% more complex ones.

%  For instance, in ____, it is
% shown that the presence of false correlations slows down the learning of true
% features. In ____, an iterative algorithm is proposed to
% detect such shortcuts, characterized by the fact that high accuracy for
% individual classes can be achieved based on a small number of frequencies. In
% ____, the negative effect of quickly learning simple
% features is linked to gradient starvation in the cross-entropy loss function.

\textit{In simplicity bias terminology frequent features are called simple, and
  simplicity bias should manifest in accelerated learning of hypernym features
  common among many classes. This is what we robustly observe in neural networks
  trained in practical settings}.


\textbf{Neural collapse}. Neural collapse (NC) phenomenon was first described
by **Raghu et al., "SVCCA: Singular Vector Canonical Correlation Analysis for Interpreting and Understanding Deep Learning Models"** and then was further extended to broader settings
in **Garipov et al., "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"**, **Chen et al., "Neural Collapse and Global Optimality: A New Perspective on Deep Learning"** and other works. Neural
collapse is a state of the last layer of the neural classifier, which can be
observed in the terminal phase of training. It is characterized by zero
within-class variability of features, when class means form vertices of the
Equiangular Tight Frame (ETF), and classification decision is similar to
prototypes **Kawaguchi et al., "Generalization in Deep Learning via Eigenvalue Control"**
. While the ETF configuration is optimal
from the perspective of robustness to adversarial attacks
**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, it does not necessarily guarantee good performance on the training set.
**Raghu et al., "SVCCA: Singular Vector Canonical Correlation Analysis for Interpreting and Understanding Deep Learning Models"**
, however, suggests that neural collapse can be a useful tool for understanding deep learning.

\textit{We believe our results provide new insights into the neural collapse phenomenon}