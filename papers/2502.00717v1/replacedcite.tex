\section{Related Work}
\label{related work}
\subsection{Large Vision-Language Models}
The existing boom of LVLMs ____ could be chiefly attributed to the tremendous success of LLMs technology, such as Llama ____, Vicuna ____, Qwen ____, etc. These LVLMs try to build up a connection between the pre-trained LLMs and the off-the-shelf vision encoders ____, utilizing a lightweight adapter, such as MLP ____ or Qformer ____, to translate visual features into language tokens understandable for LLMs. Therefore, they can perform next-token generation just like LLMs, iteratively predicting tokens based on produced probability distribution. 
%LVLMs are expected to comprehend the linguistic nuances of the question, extract needed visual information, and exploit in-built open-world knowledge, thus conquering visual tasks such as image captioning, visual question answering, etc. 
Recent visual instruction tuning ____ has further enhanced LVLMs' ability to follow human's various instructions. Besides, a suite of works ____ has also focused on improved projection modules to convert the aligned visual features effectively yet efficiently, retaining more detailed information contained in images for perception enhancement. Another series of studies ____ has additionally explored equipping LVLMs with richer capabilities to conduct vision-centric tasks including referring object detection, reasoning segmentation, etc. Despite the ongoing evolution of LVLMs, it is still found that they are subject to hallucinations, generating content that is inconsistent with facts in images. 

\subsection{Hallucination Mitigation of LVLMs}
The phenomenon of hallucination in LVLMs ____ refers to the contradiction between the textual response and the factual information of the input image. Different origins are focused on, which we divide into \textit{imperfect alignment}, \textit{defective perception mechanism}, and \textit{over-reliance on language prior}. 

\textbf{Imperfect Alignment.} 
The most direct reason for hallucination in LVLMs is that there are still knowledge gaps between vision and language modalities. Therefore, high-quality curated datasets ____ and evolutionary fine-tuning methods ____ are introduced to further enrich the knowledge of LVLMs. 
%Furthermore, Q-Instruct ____ reveals LVLMs' poor identification of low-level visual attributes (e.g., clarity, brightness, etc.) and thus performs instruction tuning with the constructed Q-Pathway dataset, which includes detailed human feedback on images with diverse low-level appearance. Blink ____ finds that most multimodal LLMs struggle with visual tasks (e.g., depth estimation, spatial reasoning, etc.) that humans can solve within a blink. 
For example, SpatialVLM ____ significantly enhances the model's spatial reasoning capability by training LVLMs with Internet-scale 3D spatial knowledge. Despite satisfactory improvement in a specific domain, these methods are mainly dependent on data-driven principles, ignoring the inherent problems of current LVLMs.

\textbf{Defective Perception Mechanism.}
As we know, the visual perception of LVLMs is achieved through integration with models like Vision Transformer (ViT) ____ as the backbone visual encoders. However, biases such as high-norm outlier tokens are found in ViT ____, which could be abused in downstream tasks. Simultaneously, V* ____ discovers severe visual information loss in high-resolution scenarios due to image compression of visual encoders. Thus, extra vision models are involved to detect task-relevant regions and feed them back to the LVLM. In contrast, LLaVA-Next ____ and InternLM-XComposer2-4KHD ____ achieve performance gains by treating each sub-region of the image as a single image, thereby increasing the image input resolution of the LVLMs. 
%Moreover, MMVP ____ finds that LVLMs cannot distinguish two similar images with completely different semantic information and proposes Mixture-of-Features (MoF), in which CLIP-ViT-based and DINOV2-based visual representations are combined. Similarly, Vcoder proposes feeding extra perception modalities (e.g., depth, segmentation, etc.) as control inputs through additional vision encoders for improved object perception performance. To conclude, the current perception mechanism of LVLMs is still not perfect enough with specific problems to be solved.

\textbf{Over-reliance on language prior.}
Another stubborn drawback of LVLMs is that they tend to generate textual responses directly depending on their constituent LLM's in-built knowledge, thus weakening the visual perception and exacerbating the occurrence of hallucination.
% Concretely, some frequently occurring knowledge has been deeply planted in the language representation space of these LLMs, resulting in an extremely high probability distribution of tokens that could hardly be influenced. To highlight the impact of visual information, a suite of decoding-based methods has been proposed, circumventing the need for further training or the usage of external models. For example, 
To highlight the impact of the image, VCD ____ introduces contrastive decoding ____ to utilize the discrepancy between two probability distributions, respectively, obtained from the original input image and a Gaussian blurred one, as the instructive value for probability refinement. In addition, ICD ____ uses the prompt ``You are a confused object detector'' to get the distorted distribution. Differently, PAI ____ proposes increasing LVLMs' attention values of the corresponding image tokens to obtain a vision-centric distribution for contrastive decoding. Unlike PAI, our approach aims to dynamically make the LVLM focus on key regions.