[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024improved-llava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "bai2023qwen-vl",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      },
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "dong2024internlmxcomposerhd",
        "author": "Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and xinyue zhang and Wei Li and Li Jingwen and Wenhai Wang and Kai Chen and Conghui He and Xingcheng ZHANG and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang",
        "title": "Intern{LM}-{XC}omposer2-4{KHD}: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K {HD}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "vicuna2023",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.",
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others",
        "title": "Qwen technical report"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dosovitskiy2020vit",
        "author": "Dosovitskiy, Alexey",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      },
      {
        "key": "radford2021clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024improved-llava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bai2023qwen-vl",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "liu2024improved-llava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cha2024honeybee",
        "author": "Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm"
      },
      {
        "key": "yao2024deco",
        "author": "Yao, Linli and Li, Lei and Ren, Shuhuai and Wang, Lean and Liu, Yuanxin and Sun, Xu and Hou, Lu",
        "title": "DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lai2024lisa",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya",
        "title": "Lisa: Reasoning segmentation via large language model"
      },
      {
        "key": "zhang2025llava-grounded",
        "author": "Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Li, Chunyuan and Yang, Jainwei and others",
        "title": "Llava-grounding: Grounded visual chat with large multimodal models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "fu2025blink",
        "author": "Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay",
        "title": "Blink: Multimodal large language models can see but not perceive"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024improved-llava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "gunjal2024fdpo",
        "author": "Gunjal, Anisha and Yin, Jihan and Bas, Erhan",
        "title": "Detecting and preventing hallucinations in large vision language models"
      },
      {
        "key": "wu2024q-instruct",
        "author": "Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Xu, Kaixin and Li, Chunyi and Hou, Jingwen and Zhai, Guangtao and others",
        "title": "Q-instruct: Improving low-level visual abilities for multi-modality foundation models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hu2022lora",
        "author": "Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "Lo{RA}: Low-Rank Adaptation of Large Language Models"
      },
      {
        "key": "yu2024rlhf",
        "author": "Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others",
        "title": "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback"
      },
      {
        "key": "wu2024reft",
        "author": "Zhengxuan Wu and Aryaman Arora and Zheng Wang and Atticus Geiger and Dan Jurafsky and Christopher D Manning and Christopher Potts",
        "title": "Re{FT}: Representation Finetuning for Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "fu2025blink",
        "author": "Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay",
        "title": "Blink: Multimodal large language models can see but not perceive"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chen2024spatialvlm",
        "author": "Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei",
        "title": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "dosovitskiy2020vit",
        "author": "Dosovitskiy, Alexey",
        "title": "An image is worth 16x16 words: Transformers for image recognition at scale"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "darcet2024vit-flaw",
        "author": "Darcet, Timoth{\\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr",
        "title": "Vision Transformers Need Registers"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wu2024vstar",
        "author": "Wu, Penghao and Xie, Saining",
        "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dong2024internlmxcomposerhd",
        "author": "Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and xinyue zhang and Wei Li and Li Jingwen and Wenhai Wang and Kai Chen and Conghui He and Xingcheng ZHANG and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang",
        "title": "Intern{LM}-{XC}omposer2-4{KHD}: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K {HD}"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "tong2024mmvp",
        "author": "Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining",
        "title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "leng2024vcd",
        "author": "Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong",
        "title": "Mitigating object hallucinations in large vision-language models through visual contrastive decoding"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2023cd",
        "author": "Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori B and Zettlemoyer, Luke and Lewis, Mike",
        "title": "Contrastive Decoding: Open-ended Text Generation as Optimization"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "wang2024ICD",
        "author": "Wang, Xintong and Pan, Jingheng and Ding, Liang and Biemann, Chris",
        "title": "Mitigating hallucinations in large vision-language models with instruction contrastive decoding"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "liu2025pai",
        "author": "Liu, Shi and Zheng, Kecheng and Chen, Wei",
        "title": "Paying more attention to image: A training-free method for alleviating hallucination in lvlms"
      }
    ]
  }
]