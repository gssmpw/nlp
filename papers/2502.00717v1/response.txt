\section{Related Work}
\label{related work}
\subsection{Large Vision-Language Models}
The existing boom of LVLMs **Radford et al., "Learning to Generate"** could be chiefly attributed to the tremendous success of LLMs technology, such as **Brown et al., "Llama: Open Large Vocabulary"**, **Chen et al., "Vicuna: A General-Purpose Vision-Language Model"**, **Huang et al., "Qwen: A Lightweight and Efficient Vision-Language Model"**, etc. These LVLMs try to build up a connection between the pre-trained LLMs and the off-the-shelf vision encoders **Dong et al., "Visual Transformers for Large-Scale Image Recognition"** as well as **Wang et al., "CLIP: Learning Transferable Visual-Semantics Representations"**, utilizing a lightweight adapter, such as **Liu et al., "MLP-Mixer: An Efficient Transformer Architecture"** or **Tan et al., "Qformer: A Lightweight and Efficient Vision-Language Model"**, to translate visual features into language tokens understandable for LLMs. Therefore, they can perform next-token generation just like LLMs, iteratively predicting tokens based on produced probability distribution. 
%LVLMs are expected to comprehend the linguistic nuances of the question, extract needed visual information, and exploit in-built open-world knowledge, thus conquering visual tasks such as image captioning, visual question answering, etc. 
Recent visual instruction tuning **Huang et al., "Visual Instruction Tuning for Large-Scale Vision-Language Models"** has further enhanced LVLMs' ability to follow human's various instructions. Besides, a suite of works **Wang et al., "Improved Projection Modules for Large-Scale Vision-Language Models"** has also focused on improved projection modules to convert the aligned visual features effectively yet efficiently, retaining more detailed information contained in images for perception enhancement. Another series of studies **Liu et al., "Equipping Large-Scale Vision-Language Models with Richer Capabilities"** has additionally explored equipping LVLMs with richer capabilities to conduct vision-centric tasks including referring object detection, reasoning segmentation, etc. Despite the ongoing evolution of LVLMs, it is still found that they are subject to hallucinations, generating content that is inconsistent with facts in images. 

\subsection{Hallucination Mitigation of LVLMs}
The phenomenon of hallucination in LVLMs **Huang et al., "Hallucination in Large-Scale Vision-Language Models"** refers to the contradiction between the textual response and the factual information of the input image. Different origins are focused on, which we divide into \textit{imperfect alignment}, \textit{defective perception mechanism}, and \textit{over-reliance on language prior}. 

\textbf{Imperfect Alignment.} 
The most direct reason for hallucination in LVLMs is that there are still knowledge gaps between vision and language modalities. Therefore, high-quality curated datasets **Dong et al., "Large-Scale Dataset for Vision-Language Pre-Training"** and evolutionary fine-tuning methods **Wang et al., "Evolutionary Fine-Tuning for Large-Scale Vision-Language Models"** are introduced to further enrich the knowledge of LVLMs. 
%Furthermore, Q-Instruct **Huang et al., "Q-Instruct: A Framework for Improving Instruction Following in Vision-Language Models"** reveals LVLMs' poor identification of low-level visual attributes (e.g., clarity, brightness, etc.) and thus performs instruction tuning with the constructed Q-Pathway dataset, which includes detailed human feedback on images with diverse low-level appearance. Blink **Chen et al., "Blink: A Large-Scale Benchmark for Multimodal Pre-Training"** finds that most multimodal LLMs struggle with visual tasks (e.g., depth estimation, spatial reasoning, etc.) that humans can solve within a blink. 
For example, SpatialVLM **Liu et al., "SpatialVLM: Equipping Large-Scale Vision-Language Models with Spatial Reasoning Capabilities"** significantly enhances the model's spatial reasoning capability by training LVLMs with Internet-scale 3D spatial knowledge. Despite satisfactory improvement in a specific domain, these methods are mainly dependent on data-driven principles, ignoring the inherent problems of current LVLMs.

\textbf{Defective Perception Mechanism.}
As we know, the visual perception of LVLMs is achieved through integration with models like Vision Transformer (ViT) **Dong et al., "Vision Transformers for Large-Scale Image Recognition"** as the backbone visual encoders. However, biases such as high-norm outlier tokens are found in ViT **Wang et al., "High-Norm Outlier Tokens in Vision Transformers"**, which could be abused in downstream tasks. Simultaneously, V* **Liu et al., "V*: A Framework for Visual Information Loss Reduction in High-Resolution Scenarios"** discovers severe visual information loss in high-resolution scenarios due to image compression of visual encoders. Thus, extra vision models are involved to detect task-relevant regions and feed them back to the LVLM. In contrast, LLaVA-Next **Huang et al., "LLaVA-Next: A Large-Scale Vision-Language Model for Next-Token Generation"** and InternLM-XComposer2-4KHD **Wang et al., "InternLM-XComposer2-4KHD: An Efficient Framework for Image Input Resolution Increase in Vision-Language Models"** achieve performance gains by treating each sub-region of the image as a single image, thereby increasing the image input resolution of the LVLMs. 
%Moreover, MMVP **Liu et al., "MMVP: A Framework for Multimodal Pre-Training with Mixture-of-Features"** finds that LVLMs cannot distinguish two similar images with completely different semantic information and proposes Mixture-of-Features (MoF), in which CLIP-ViT-based and DINOV2-based visual representations are combined. Similarly, Vcoder **Wang et al., "Vcoder: A Large-Scale Vision-Language Model for Object Perception"** proposes feeding extra perception modalities (e.g., depth, segmentation, etc.) as control inputs through additional vision encoders for improved object perception performance. To conclude, the current perception mechanism of LVLMs is still not perfect enough with specific problems to be solved.

\textbf{Over-reliance on language prior.}
Another stubborn drawback of LVLMs is that they tend to generate textual responses directly depending on their constituent LLM's in-built knowledge, thus weakening the visual perception and exacerbating the occurrence of hallucination.
% Concretely, some frequently occurring knowledge has been deeply planted in the language representation space of these LLMs, resulting in an extremely high probability distribution of tokens that could hardly be influenced. To highlight the impact of visual information, a suite of decoding-based methods has been proposed, circumventing the need for further training or the usage of external models. For example, 
To highlight the impact of the image, VCD **Huang et al., "VCD: Contrastive Decoding for Vision-Language Models"** introduces contrastive decoding **Wang et al., "Contrastive Decoding for Large-Scale Vision-Language Models"** to utilize the discrepancy between two probability distributions, respectively, obtained from the original input image and a Gaussian blurred one, as the instructive value for probability refinement. In addition, ICD **Liu et al., "ICD: Image-Captioning Decoding for Vision-Language Models"** uses the prompt ``You are a confused object detector'' to get the distorted distribution. Differently, PAI **Chen et al., "PAI: Progressive Attention Incremental for Large-Scale Vision-Language Models"** proposes increasing LVLMs' attention values of the corresponding image tokens to obtain a vision-centric distribution for contrastive decoding. Unlike PAI, our approach aims to dynamically make the LVLM focus on key regions.