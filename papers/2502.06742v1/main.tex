

\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}


\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{theconference2024}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[textsize=tiny]{todonotes}

\usepackage{bm}
\theconferencetitlerunning{Arxiv Preprint}
\usepackage{enumitem}



\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\begin{document}

\twocolumn[
\theconferencetitle{Gradient Multi-Normalization for Stateless and Scalable LLM Training}
\theconferencesetsymbol{equal}{*}

\begin{theconferenceauthorlist}
\theconferenceauthor{Meyer Scetbon}{equal,msr}
\theconferenceauthor{Chao Ma}{equal,msr}
\theconferenceauthor{Wenbo Gong}{equal,msr}
\theconferenceauthor{Edward Meeds}{msr}

\end{theconferenceauthorlist}

\theconferenceaffiliation{msr}{Microsoft Research}


\theconferencecorrespondingauthor{Meyer Scetbon}{t-mscetbon@microsoft.com}
\theconferencecorrespondingauthor{Chao Ma}{chaoma@microsoft.com}
\theconferencecorrespondingauthor{Wenbo Gong}{wenbogong@microsoft.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\theconferencekeywords{Machine Learning, theconference}

\vskip 0.3in
]

% \twocolumn[
% \theconferencetitle{Gradient Multi-Normalization for Stateless and Scalable LLM Training}

% \theconferencesetsymbol{equal}{*}

% \begin{theconferenceauthorlist}
% \theconferenceauthor{Meyer Scetbon}{aff}{t-mscetbon@microsoft.com}
% \theconferenceauthor{Chao Ma}{aff}{chaoma@microsoft.com}
% \theconferenceauthor{Wenbo Gong}{aff}{wenbogong@microsoft.com} \\
% \theconferenceauthor{Edward Meeds}{aff}{ted.meeds@microsoft.com}
% \end{theconferenceauthorlist}

% \theconferenceaffiliation{aff}{Open Research}

% \vskip 0.3in
% ]

\printAffiliationsAndNotice{\theconferenceEqualContribution}

\begin{abstract}
Training large language models (LLMs) typically relies on adaptive optimizers like Adam~\citep{adam}, which store additional state information to accelerate convergence but incur significant memory overhead. Recent efforts, such as SWAN~\citep{ma2024swansgdnormalizationwhitening}, address this by eliminating the need for optimizer states while achieving performance comparable to Adam via a multi-step preprocessing procedure applied to instantaneous gradients. Motivated by the success of SWAN, we introduce a novel framework for designing stateless optimizers that normalizes stochastic gradients according to multiple norms. To achieve this, we propose a simple alternating scheme to enforce the normalization of gradients w.r.t these norms. We show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem, and that SWAN is a particular instance of our approach with carefully chosen norms, providing a deeper understanding of its design. However, SWAN’s computationally expensive whitening/orthogonalization step limit its practicality for large LMs. Using our principled perspective,  we develop of a more efficient, scalable, and practical stateless optimizer. Our algorithm relaxes the properties of SWAN, significantly reducing its computational cost while retaining its memory efficiency, making it applicable to training large-scale models. Experiments on pre-training LLaMA models with up to 1 billion parameters demonstrate a $3$× speedup over Adam with significantly reduced memory requirements, outperforming other memory-efficient baselines.
\end{abstract}

\input{sections/intro}
\input{sections/related_work}
\input{sections/background}
\input{sections/contrib}
\input{sections/experiment}
\input{sections/conclusion}




\clearpage
\newpage
\bibliography{biblio}
\bibliographystyle{theconference2024}

\newpage
\appendix
\onecolumn
\input{sections/appendix}


\end{document}

