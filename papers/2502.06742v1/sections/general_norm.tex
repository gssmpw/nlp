Given $K$ norms, $(g_1,\dots, g_K)$, in this section we are interested in solving:
\begin{align}
\label{eq:multi-norm-opt-convex-app}
 \argmax_{z} \langle \nabla, z\rangle\quad \text{s.t.}~\forall~~i\in [|1,K|],~~g_i(z)\leq 1
\end{align}
which as stated in the main paper is equivalent to solve
\begin{align*}
% \label{eq:multi-norm-opt-convex}
 \argmax_{z} \langle \nabla, z\rangle\quad \text{s.t.}~\Vert z\Vert\leq 1
\end{align*}
where
\begin{align}
\label{eq:def-gen-norm}
\Vert z\Vert := \max_{i\in[|1,K|]}g_i(z)\; .
\end{align}

Note that this constrained optimization problem is exactly finding the subdifferential of the dual norm $\Vert \cdot \Vert$. To see this, let us recall the following Lemma with its proof~\cite{watson1992characterization}.
\begin{lemma}
The subdifferential of a norm $\Vert \cdot \Vert$ at $x$ is given by
\begin{align*}
    \partial\Vert \cdot\Vert(x)=\{p\in\mathbb{R}^d:~\Vert p\Vert_*\leq 1\text{, } \langle p, x\rangle = \Vert x\Vert\}
\end{align*}
where the dual norm is defined as
\begin{align*}
    \Vert x\Vert_*:=\max_{\Vert z\Vert\leq 1}\langle z, x\rangle 
\end{align*}
\end{lemma}

\begin{proof}
We can show this result by double inclusion. Let us define the subdifferential of a norm as
\begin{align*}
    \partial\Vert \cdot\Vert(x):=\{p:~\Vert y\Vert \geq \Vert x \Vert + \langle p, y-x\rangle~\forall y\}
\end{align*}
and let us denote our set of interest as
\begin{align*}
    \mathcal{V}(x):=\{p\in\mathbb{R}^d:~\Vert p\Vert_*\leq 1\text{, } \langle p, x\rangle = \Vert x\Vert\}
\end{align*}

Let $p\in\mathcal{V}(x)$. Then we have
\begin{align*}
    \Vert x \Vert + \langle p, y-x\rangle &= \langle p, y\rangle \\
    &\leq \Vert p\Vert_* \Vert y\Vert\\
    &\leq \Vert y\Vert
\end{align*}
where the first equality comes from the definition of $p$, the first inequality comes from Holder, and the last one is obtained by definition of $p$. So we deduce that $\mathcal{V}(x)\subset  \partial\Vert \cdot\Vert(x)$.
Let us now take $p\in \partial\Vert \cdot\Vert(x)$, that is $p$ such that for all $y$ 
$\Vert y\Vert \geq \Vert x \Vert + \langle p, y-x\rangle$. Then we have for all $y$ that:
\begin{align*}
    \langle p,x\rangle - \Vert x\Vert &\geq \langle p,y\rangle 
 - \Vert y\Vert\\
 & \geq \sup_y\langle p,y\rangle 
 - \Vert y\Vert\\
 &\geq \Vert p\Vert^{*}
\end{align*}
where $\Vert \cdot\Vert^{*}$ is the Fenchel-Legendre transform of the norm $\Vert\cdot \Vert$. From Lemma~\ref{lemma-fenchel-norm}, we deduce that
\begin{align*}
    \langle p,x\rangle - \Vert x\Vert \geq \bm{1}_{\mathcal{B}_{1}}(p)
\end{align*}
where $\mathcal{B}_{1}$ is the unit ball associated with the dual norm $\Vert \cdot\Vert_{*}$. As the left-hand side is finite, we deduce that $p\in\mathcal{B}_1$. Then we deduce that 
\begin{align*}
   \Vert x\Vert \geq  \langle p, x\rangle \geq \Vert x\Vert
\end{align*}
where the left inequality is obtained by applying Holder to the inner-product, from which follows the result.

\end{proof}

For simple norms, such as the $\ell_p$-norm with $1< p\leq +\infty$, obtaining an element of $\partial\Vert \cdot\Vert_{*}(\nabla)$ can be done in closed-form. For $\ell_p$ norms, recall the their dual norm are the $\ell_q$ norms with $q$ the dual exponent respectively. The following Lemma provide analytic formulas for such norms. 

\begin{lemma}
For $x\in\mathbb{R}^d$, let us define the $\ell_p$-norm as 
$$\Vert x\Vert_p:=\left(\sum_{i=1}^d |x_i|^p\right)^{1/p}$$

\textbf{Case 1: \boldmath{$1 < p < \infty$}.}
Let us define the dual exponent $q$ by
\[
\frac{1}{p} + \frac{1}{q} = 1.
\]
Then the subdifferential of $\|x\|_p$ is
\[
\partial \|x\|_p \;=\;
\begin{cases}
\displaystyle
\left\{
  \frac{\bigl(|x_1|^{p-2}x_1,\dots,|x_d|^{p-2}x_d\bigr)}{\|x\|_p^{\,p-1}}
\right\}, 
& x \neq 0,
\\
\bigl\{\,g \in \mathbb{R}^d : \|g\|_q \,\le\, 1 \bigr\}, 
& x = 0.
\end{cases}
\]

\textbf{Case 2: \boldmath{$p = 1$}.}
For the $\ell_1$-norm, the subdifferential at $x \in \mathbb{R}^n$ is given by
\[
\partial \|x\|_1 
\;=\;
\Bigl\{
   g \in \mathbb{R}^n : 
   g_i = \mathrm{sign}(x_i)
\Bigr\}.
\]
Here, $\mathrm{sign}(x_i)$ is $+1$ if $x_i > 0$, $-1$ if $x_i < 0$, and can be any value in $[-1,1]$ if $x_i=0$.

\textbf{Case 3: \boldmath{$p = \infty$}.}  
For the $\ell^\infty$-norm, let $M = \|x\|_\infty$ and let us define
\begin{align*}
\mathcal{S}(x):=\{
  g: 
  g_i=0 \text{ if } |x_i|<M,\,
  g_i = \mathrm{sign}(x_i)~\text{else},\,
  \|g\|_1 = 1
\}
\end{align*}

Then by denoting for any set $A\subset\mathbb{R}^d$, $\text{conv}(A)$ the convex hull of the set $A$, we have:
\[
\partial \|x\|_\infty
\;=\;
\begin{cases}
\displaystyle
\mathrm{conv}\!(\mathcal{S}(x))
& x \neq 0,
\\
\bigl\{\, g \in \mathbb{R}^n : \|g\|_1 \le 1 \bigr\},
& x = 0.
\end{cases}
\]
\end{lemma}

However, for general norms, there are not known closed-form solutions of their associated subdifferentials. In particular, if the norm is defined as in~\eqref{eq:def-gen-norm}, even when the $g_i$'s are simple norms (i.e. norms for which we can compute the subdifferential of their dual norms), then no closed-form solution can be obtained in general.  



\subsection{A Dual Perspective}


In this section, we propose an algorithmic approach to solve the convex relaxation of the problem introduced in~\eqref{eq:multi-norm-opt}. More formally, given a family of simple norms $(g_i)_{i=1}^K$ and some positive constants $(\varepsilon_i)_{i=1}^K$, we consider the following problem:
\begin{align}
\label{eq:proj-gen-opt}
 \max_{d\theta\in\mathbb{R}^d} \langle \nabla, d\theta\rangle\quad \text{s.t.}~g(d\theta)\leq 1\; .
\end{align}
where
\begin{align*}
g(x):= \max_{i\in[|1,K|]}\frac{g_i(x)}{\varepsilon_i}
\end{align*}
which is also a norm. For such problems, as long as $\nabla\neq 0$, then the solutions lies in the level set $\{d\theta:~g(d\theta)=1\}$. Even if the subdifferentials of (the dual norm of) each $g_i$ can be derived in closed form, there is not known closed-form for the subdifferential of (the dual norm of) $g$. To solve~\eqref{eq:proj-gen-opt}, we propose to consider a coordinate gradient descent on the dual. A simple application of the Fenchel duality~\cite{rockafellar1974conjugate} leads to the following equivalent optimization problem:
\begin{align}
\label{dual-gen-proj}
\inf_{\lambda_1,\dots,\lambda_K}\sum_{i=1}^K \epsilon_i g^\dagger_i(\lambda_i)\quad\text{s.t.}\quad \nabla\mathcal{L}(\theta) = \sum_{i=1}^K \lambda_i
\end{align}
where $g_i^{\dagger}$ is the dual norm of $g_i$ and so for all $i\in[|1,K|]$, from which a primal solution can be recovered by simply finding $y_i$ s.t. $\lambda_i>0$ and such that $\langle \lambda_i, y_i\rangle=\varepsilon_ig_i^{\dagger}(y_i)$ under the condition that $g_i(y_i)=\varepsilon_i$, which is equivalent to solve:
\begin{align*}
    y_i^{*}:=\varepsilon_i\argmax_{z:~g_i(z)\leq 1} \langle z,\lambda_i\rangle \; .
\end{align*}


\begin{proof}
Let $(\mathcal{B}_i(\epsilon_i))_{i=1}^K$ the ball associated with the norm $(g_i)_{i=1}^K$ with radius $(\varepsilon_i)_{i=1}^K$ respectively. Let us also denote for any set $\mathcal{A}\subset\mathbb{R}^d$, the indicator function as 
$$\bm{1}_{A}(x)=\begin{cases} 
          0 ~~\text{if}~~x\in A\\
          +\infty ~~\text{otherwise}
       \end{cases}$$

In the following we denote $f(x):=\langle x, \nabla\rangle$. Then~\eqref{eq:proj-gen-opt} can be reformulated as the following optimization problem:
\begin{align*}
    -\inf_{d\theta} f(d\theta) +\sum_{i=1}^K \bm{1}_{\mathcal{B}_i(\epsilon_i)}(
d\theta)
\end{align*}
which can be again reparameterized (up to the sign) as 
\begin{align*}
    \inf_{x=y_i, ~ \forall i\in[|1,K|]} f(x) +\sum_{i=1}^K \bm{1}_{\mathcal{B}_i(\epsilon_i)}(y_i)
\end{align*}
Now the Lagrangian associated with this problem is:
\begin{align*}
    &\mathcal{F}((\lambda_i)_{i=1}^{K}, (y_i)_{i=1}^K, x):= \\
    & f(x) - \langle x,\sum_{i=1}^K\lambda_i\rangle  +\sum_{i=1}^K \bm{1}_{\mathcal{B}_i(\epsilon_i)}(y_i) + \langle y_i, \lambda_i\rangle 
\end{align*}

And taking the infimum of the Lagrangian w.r.t the primal variables leads to the following optimization problem:
\begin{align*}
    \inf_{x} f(x) - \langle x,\sum_{i=1}^K\lambda_i\rangle + \sum_{i=1}^K \inf_{y_i} \bm{1}_{\mathcal{B}_i(\epsilon_i)}(y_i) + \langle y_i, \lambda_i\rangle 
\end{align*}

Now observe that
\begin{align*}
    \inf_{x} f(x) - \langle x,\sum_{i=1}^K\lambda_i\rangle &= -\sup_{x}\langle x,\sum_{i=1}^K\lambda_i\rangle - f(x)\\
    &=-f^*(\sum_{i=1}^K\lambda_i)
\end{align*}
where $f^*$ is the Fenchel-Legendre transform of $f$. Similarly, we have:
\begin{align*}
     \inf_{y_i} \bm{1}_{\mathcal{B}_i(\epsilon_i)}(y_i) + \langle y_i, \lambda_i\rangle &= -\sup_{y_i}\langle y_i,-\lambda_i\rangle - \bm{1}_{\mathcal{B}_i(\epsilon_i)}(y_i)\\
    &=-\bm{1}_{\mathcal{B}_i(\epsilon_i)}^*(-\lambda_i)
\end{align*}
Finally the dual of the problem is:
\begin{align*}
   \sup_{\lambda_1,\dots,\lambda_K} -f^*(\sum_{i=1}^K\lambda_i) -\sum_{i=1}^K \bm{1}_{\mathcal{B}_i(\epsilon_i)}^*(-\lambda_i)
\end{align*}
Now recall that $f(x):=\langle x, \nabla\rangle$, therefore we have that
\begin{align*}
    f^*(x)=\bm{1}_{\{\nabla\}}(x)
\end{align*}
Also, we have that
\begin{align*}
\bm{1}_{\mathcal{B}_i(\epsilon_i)}^*(x)=\varepsilon_i g_i^\dagger(x)
\end{align*}
where $g_i^\dagger$ is the dual norm of $g_i$, from which it follows the final dual formulation:
\begin{align*}
% \label{dual-gen-proj}
\inf_{\lambda_1,\dots,\lambda_K}\sum_{i=1}^K \epsilon_i g^\dagger_i(\lambda_i)\quad\text{s.t.}\quad \nabla\mathcal{L}(\theta) = \sum_{i=1}^K \lambda_i.
\end{align*}
Finally, Slater condition are verified, thus strong duality holds, and the KKT conditions gives the following primal-dual conditions:
\begin{align*}
    \begin{cases} 
          \nabla\mathcal{L}(\theta)=\sum_{i=1}^K \lambda_i\\
          \lambda_i\in\partial\bm{1}_{\mathcal{B}_i(\varepsilon_i)}(y_i)~~\forall i\\
          x=y_i~~\forall i
       \end{cases}
\end{align*}
Now according to Lemma~\ref{lemma-subdiff-indicator}, we have that


\begin{align*}
\partial\bm{1}_{\mathcal{B}_i(\varepsilon_i)}(x)=\begin{cases} 
          \{0\}\text{ if } g_i(x)< \varepsilon_i\\
          \emptyset \text{ if } g_i(x)> \varepsilon_i\\
          \{p:~\langle p, x\rangle =\varepsilon_ig_i^{\dagger}(p)\} \text{ if } g_i(x)=\varepsilon_i
       \end{cases}
\end{align*}
from which follows that one can recover a primal solution by simply finding $y_i$ s.t. $\lambda_i>0$ and such that $\langle \lambda_i, y_i\rangle=\varepsilon_ig_i^{\dagger}(y_i)$ under the condition that $g_i(y_i)=\varepsilon_i$, which is equivalent to solve:
\begin{align*}
    y_i^{*}:=\varepsilon_i\argmax_{z:~g_i(z)\leq 1} \langle z,\lambda_i\rangle \; .
\end{align*}
\end{proof}


\begin{algorithm}[tb]
   \caption{Primal-Dual Algorithm to solve~\eqref{update-cgd}}
   \label{alg:primal-dual}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\beta_k\in\mathbb{R}^d$,   $\epsilon_1,\epsilon_k>0$, $\eta_1, \eta_2>0$, s.t. $\eta_1\eta_2<1$.
   \STATE Initialize $\lambda=z=u=0_d$.
   \FOR{$i=1$  {\bfseries to} $L$}
   \STATE $\lambda_{\text{old}} \gets \lambda$
    \STATE $z\gets \text{proj}_{\mathcal{B}_{1}(\epsilon_1)}(z + \eta_1 (u - \beta_k)) $
    \STATE $\lambda\gets \text{prox}_{\eta_2\epsilon_k g_k^{\dagger}}(\lambda - \eta_2 z)$
    \STATE $u\gets 2\lambda - \lambda_{\text{old}}$
   \ENDFOR
   \STATE Return $\lambda$
\end{algorithmic}
\end{algorithm}


To solve the dual problem introduced in~\eqref{dual-gen-proj}, we apply a coordinate gradient descent on the $\lambda_i$. More precisely, we can reformulate the problem as an unconstrained optimization one by considering:
\begin{align*}
\inf_{\lambda_2,\dots,\lambda_K} \epsilon_1 g_1^{\dagger}\left(\nabla\mathcal{L}(\theta) - \sum_{i=2}^K\lambda_i\right) +\sum_{i=2}^K \epsilon_i g^\dagger_i(\lambda_i)
\end{align*}
Starting with $\lambda_2^{(0)}=\dots=\lambda_K^{(0)}=0_d$, we propose to apply the following updates at time $t\geq 0$ and so for all $k\in[|2,K|]$:
\begin{align}
\label{update-cgd}
\lambda_{k}^{(t+1)}=\argmin_{\lambda_k} \epsilon_1 g_1^{\dagger}\left(\beta_k^{(t)}- \lambda_k \right) + \epsilon_k g^\dagger_k(\lambda_k)
\end{align}
where $\beta_k^{(t)}:=\nabla\mathcal{L}(\theta) - \sum\limits_{i\neq k}\lambda_i^{(t)}$. In order to solve~\eqref{update-cgd}, we leverage the so-called Chambolle-Pock algorithm~\cite{chambolle2011first}. Let us denote $h_1(\lambda):=\varepsilon_1 g_1^{\dagger}(\beta_k^{(t)}-\lambda)$ and $h_k(\lambda):=\varepsilon_k g_k^{\dagger}(\lambda)$. Then we can write
\begin{align*}
    \inf_{\lambda} h_1(\lambda) + h_k(\lambda) = \inf_{\lambda} h_k(\lambda) + \sup_{z} \langle z, \lambda\rangle - h_1^*(z)\\
    = \inf_{\lambda}\sup_{z} \langle z,\lambda\rangle - h_1^{*}(z) + h_k(\lambda)
\end{align*}
where $h_1^*$ is the Fenchel-Legendre transform of $h_1$ given by $h_1^{*}(x)=\langle x, \beta_k^{(t)}\rangle + \bm{1}_{\mathcal{B}_{1}(\epsilon_1)}(x)$
where $\mathcal{B}_{1}(\epsilon_1)$ is the ball induced by the norm $g_1$ of radius $\varepsilon_1$. We are now ready to present the Chambolle-Pock algorithm for our setting as presented in Algorithm~\ref{alg:primal-dual}. This algorithm requires to have access to the projection operation w.r.t the norm $g_1$ and the proximal operator w.r.t the norm $g_k^{\dagger}$, that is, it requires to have access to:
\begin{align*}
    \text{proj}_{\mathcal{B}_1(\varepsilon_1)}(x)&:=\argmin_{z:~g_1(z)\leq \varepsilon_1}\Vert z - x\Vert_2\\
    \text{prox}_{\lambda g_k^{\dagger}}(x)&:=\argmin_{z}\frac{\Vert z - x\Vert_2^2}{2} + \lambda g_k^{\dagger}(z)
    \end{align*}

Computing proximal and projection operators of norms and their duals can also be done using the Moreau decomposition property which states that:
\begin{align*}
    \text{prox}_{f}(x) + \text{prox}_{f^*}(x) = x
\end{align*}
in particular if $f:=\Vert \cdot \Vert$ is a norm, we have:
\begin{align*}
    \text{prox}_{\Vert \cdot \Vert}(x) + \text{proj}_{\mathcal{B}_{*}(1)}(x) = x
\end{align*}
where $\mathcal{B}_{*}(1)$ is the unit ball of the dual norm of $\Vert \cdot \Vert$. Finally, the full coordinate gradient scheme is presented in Algorithm~\ref{alg:cd-relax} which returns a solution of the primal problem defined in~\eqref{eq:multi-norm-opt-convex-app}.

\begin{algorithm}[tb]
   \caption{Coordinate Gradient Descent to solve~\eqref{dual-gen-proj}}
   \label{alg:cd-relax}
\begin{algorithmic}
   \STATE {\bfseries Input:} the gradient$\nabla\mathcal{L}(\theta)$ and  $\epsilon_1,\dots,\epsilon_K>0$
   \STATE Initialize $\lambda_2=\dots=\lambda_K=0_d$.
   \FOR{$t=1$  {\bfseries to} $T$}
   \FOR{$k=2$ {\bfseries to} $K$}
   \STATE $\beta_k^{(t)}\gets\nabla\mathcal{L}(\theta) - \sum\limits_{i\neq k}\lambda_i^{(t)}$
   \STATE $\lambda_k^{(t+1)} \gets \argmin_{\lambda} h_1(\lambda) + h_k(\lambda)$~~with Alg.~\ref{alg:primal-dual}
   \ENDFOR
    \ENDFOR
    \STATE Find $k$ such that $\lambda_k>0$
    \STATE Return $x^*:=\varepsilon_k\argmax\limits_{z:~g_k(z)\leq 1} \langle z,\lambda_k\rangle$
\end{algorithmic}
\end{algorithm}


\begin{lemma}
\label{lemma-fenchel-norm}
Let $\Vert\cdot\Vert$ be a norm on $\mathbb{R}^d$ with dual norm $\Vert x\Vert_{*}:=max_{z:\Vert z\Vert\leq 1} \langle z, x\rangle$, then the Fenchel-Legendre transform of $\Vert\cdot\Vert$ is the indicator function of the unit ball induced by its dual norm. More formally, we have
\begin{align*}
    \sup_{z\in\mathbb{R}^d}\langle z, x\rangle - \Vert z\Vert =  \begin{cases} 
          0 ~~\text{if}~~ \Vert x\Vert_* \leq 1\\
          +\infty ~~\text{otherwise}
       \end{cases}
\end{align*}
\end{lemma}
\begin{proof}
Using the fact that $\Vert x\Vert=\sup_{z:\Vert z\Vert_{*}\leq 1} \langle z,x\rangle$, we have:
\begin{align*}
   \sup_{z\in\mathbb{R}^d}\langle z, x\rangle - \Vert z\Vert & = \max_{z\in\mathbb{R}^d}\langle z, x\rangle  - \sup_{y:\Vert y\Vert_{*}\leq 1} \langle y,z\rangle\\
   &= \sup_{z\in\mathbb{R}^d}\inf_{y:\Vert y\Vert_{*}\leq 1} \langle z, x-y\rangle \\
   &=\inf_{y:\Vert y\Vert_{*}\leq 1}\sup_{z\in\mathbb{R}^d}\langle z, x-y\rangle\\
   &=\inf_{y:\Vert y\Vert_{*}\leq 1} \begin{cases} 
          0 ~~\text{if}~~y=x\\
          +\infty ~~\text{otherwise}
       \end{cases}
\end{align*}
which gives the desired result. Note that the third equality follows from Sion's minimax theorem. 
\end{proof}


\begin{lemma}
\label{lemma-subdiff-indicator}
Let $\Vert \cdot\Vert$ a norm on $\mathbb{R}^d$ and $\varepsilon>0$. Then we have:
\begin{align*}
\partial\bm{1}_{\mathcal{B}(\varepsilon)}(x)=\begin{cases} 
          \{0\}\text{ if } \Vert x\Vert< \varepsilon\\
          \emptyset \text{ if } \Vert x\Vert> \varepsilon\\
          \{p:~\langle p, x\rangle =\varepsilon\Vert p\Vert_*\} \text{ if } \Vert x\Vert=\varepsilon
       \end{cases}
\end{align*}
where $\Vert \cdot \Vert_*$ is the dual norm of $\Vert \cdot \Vert$, and $\mathcal{B}(\varepsilon)$ is the ball of radius $\varepsilon$ w.r.t the norm $\Vert \cdot \Vert$.
\end{lemma}

\begin{proof}
Recall that the definition of the subdifferential is:
\begin{align*}
    \partial\bm{1}_{\mathcal{B}(\varepsilon)}(x):=\{p:~\bm{1}_{\mathcal{B}(\varepsilon)}(y)\geq \bm{1}_{\mathcal{B}(\varepsilon)}(x) + \langle p, y-x\rangle~~\forall y\}
\end{align*}
If $\Vert x\Vert < \varepsilon$, then we have that $p$ must satisfy for all $y\in\mathcal{B}(\varepsilon)$:
\begin{align*}
    \langle p, y-x\rangle\leq 0
\end{align*}
By taking $\gamma$ sufficiently small we can therefore choose $y=x+\gamma \frac{p}{\Vert p \Vert_2}\in\mathcal{B}(\varepsilon)$ which leads to 
\begin{align*}
    \gamma \Vert p\Vert_2\leq 0  
\end{align*}
which is only true for $p=0$ as $\gamma$ can be selected to be negative or positive. Now if $\Vert x\Vert > \varepsilon$, then the subdifferential is clearly empty. Finally, let us consider the case where $\Vert x\Vert = \varepsilon$. We deduce that:
\begin{align*}
    \langle p, x\rangle \geq \langle p, y\rangle -\bm{1}_{\mathcal{B}(\varepsilon)}(y)
\end{align*}
and so for all $y$. Therefore we obtain that
\begin{align*}
    \langle p, x\rangle  &\geq \sup_y \langle p, y\rangle -\bm{1}_{\mathcal{B}(\varepsilon)}(y)\\
    &=\varepsilon\Vert p\Vert_*
\end{align*}
But we also have that:
\begin{align*}
    \varepsilon\Vert p\Vert_*=\Vert p\Vert_* \Vert x\Vert \geq \langle p, x\rangle
\end{align*}
from which follows that $ \langle p, x\rangle=\varepsilon\Vert p\Vert_*$ which conclude the proof.
\end{proof}

