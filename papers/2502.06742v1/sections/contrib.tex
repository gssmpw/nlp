\section{Multi-Normalized Gradient Descent} \label{sec: mngd}
Before presenting our approach, let us first introduce some clarifying notations.

\paragraph{Notations.} For a vector $x\in\mathbb{R}^d$, we call its normalized projection w.r.t to a given norm $\Vert \cdot\Vert$, the solution to the following optimization problem:
\begin{align}
\label{eq-single-proj}
\mathcal{P}_{\Vert \cdot \Vert}(x):=\argmax_{z:~\Vert z \Vert = 1} \langle x, z\rangle    
\end{align}
We also extend the definition of this notation if $x\in\mathbb{R}^{m\times n}$ is a matrix and $\Vert \cdot \Vert$ is a matrix norm. 


\subsection{Gradient Multi-Normalization}

Let us now consider a finite family of $K\geq 1$ norms $(g_1,\dots,g_K)$. In order to pre-process the gradient $\nabla$ jointly according to these norms, we propose to consider the following optimization problem:
\begin{align}
\label{eq:multi-norm-opt}
 \argmax_{z} \langle \nabla, z\rangle~ \text{s.t.}~\forall~i\in [|1,K|],~g_i(z)=1\; .
\end{align}
Assuming the constraint set is non-empty, the existence of a maximum is guaranteed. However, this problem is NP-hard and non-convex due to the constraints, making it hard to solve efficiently for the general case of arbitrary norms.


\begin{algorithm}[!t]
   \caption{$\texttt{MultiNorm}(\nabla,L, \bm{g})$}
   \label{alg:alt-proj}
\begin{algorithmic}
   \STATE {\bfseries Input:} the stochastic gradient $\nabla_\theta\mathcal{L}(\theta_t,x^{(t)})$, the norms $\bm{g}:=(g_1,\dots,g_K)$, and $L\geq 1$ the number of iterations.
   \STATE Initialize $x=\nabla_\theta\mathcal{L}(\theta_t,x^{(t)})$.
   \FOR{$\ell=1$  {\bfseries to} $L$}
   \FOR{$i=1$ {\bfseries to} $K$}
   \STATE $x\gets \mathcal{P}_{g_i}(x):=\argmax\limits_{z:~g_i(z) = 1} \langle x, z\rangle $
   \ENDFOR
    \ENDFOR
    \STATE Return $x$
\end{algorithmic}
\end{algorithm}

\begin{remark}
Observe that when $K=1$, the problem~\eqref{eq:multi-norm-opt} recovers exactly the single normalization step used in~\cite{bernstein2024old}, as presented in~\eqref{eq:single-norm}.
\end{remark}

\begin{remark}
The convex relaxation of~\eqref{eq:multi-norm-opt}, defined as  
\begin{align}
\label{eq:multi-norm-opt-convex}
 \argmax_{z} \langle \nabla, z\rangle\quad \text{s.t.}~\forall~~i\in [|1,K|],~~g_i(z)\leq 1
\end{align}
is in fact equivalent to the single normalization case discussed in Section~\ref{sec:single-norm}, where the norm considered is $\Vert x\Vert:=\max\limits_{i\in[|1,K]} g_i(x)$. Thus, solving~\eqref{eq:multi-norm-opt-convex} is equivalent to computing the projection $\mathcal{P}_{\Vert \cdot \Vert}(\nabla)$. In Appendix~\ref{sec:convex-relaxation}, we provide a general approach to compute it using the so-called Chambolle-Pock algorithm~\cite{chambolle2011first}.
\end{remark}

While solving~\eqref{eq:multi-norm-opt} exactly might not be practically feasible in general, we propose a simple alternating projection scheme, 
presented in Algorithm~\ref{alg:alt-proj}. Notably, our method assumes that the projections $\mathcal{P}_{g_i}(\cdot)$  can be efficiently computed for all $i\in[|1,K|]$. Fortunately, when the $g_i$'s correspond to $\ell_p$-norms with $p\in[|1,+\infty|]$, or Schatten $p$-norms for matrices, closed-form solutions for these projections exist. See Appendix~\ref{sec:convex-relaxation} for more details.


\paragraph{SWAN: an Instance of $\texttt{MultiNorm}$.}  SWAN~\cite{ma2024swansgdnormalizationwhitening} applies two specific pre-processing steps to the raw gradients in order to update the weight matrices. In fact, each of these pre-processing steps can be seen as normalized projections with respect to a specific norm. More precisely, for $W\in\mathbb{R}^{m\times n}$ and $m\leq n$, let us define
\begin{align*}
g_1(W):=\frac{\max\limits_{i\in[|1,m|]} \Vert W_{i,:}\Vert_2}{\sqrt{n}}\; ,~ \text{and}~~ 
g_2(W):=\frac{\Vert W\Vert_{\sigma, \infty}}{\sqrt{n}}\; .
\end{align*}
where for $p\in [1,+\infty]$, $\Vert W\Vert_{\sigma,p}$ is the Schatten $p$-norm of $W$. Simple derivations leads to the following equalities:
\begin{align*}
    \mathcal{P}_{g_1}(W)&= \sqrt{n} Q(W)^{-1}W\\
    \mathcal{P}_{g_2}(W)&=\sqrt{n}(WW^\top)^{-1/2}W
\end{align*}
%
Therefore applying a single iteration ($L=1$) of Algorithm~\ref{alg:alt-proj} with norms $g_1$ and $g_2$ as defined above on the raw gradient $\nabla_t$ exactly leads to the SWAN update (Eq.~\eqref{eq:swan-update}).


\subsection{On the Convergence of \texttt{MultiNorm}}
We aim now at providing some theoretical guarantees on the convergence of  $\texttt{MultiNorm}$ (Algorithm~\ref{alg:alt-proj}). More precisely, following the SWAN implementation~\cite{ma2024swansgdnormalizationwhitening}, we focus on the specific case where $K=2$ and the normalized projections associated with the norms $g_1$ and $g_2$ have constant $\ell_2$-norm. More formally, we consider the following assumption.
\begin{assumption}
\label{assump-norm}
Let $g$ be a norm on $\mathbb{R}^d$. We say that it satisfies the assumption if for all $x\in\mathbb{R}^d$, $\Vert \mathcal{P}_{g}(x) \Vert_2 = c $ where $c>0$ is an arbitrary positive constant independent of $x$ and $\Vert\cdot\Vert_2$ represents the Euclidean norm.
\end{assumption}

\begin{remark}
Observe that both norms in SWAN satisfies Assumption~\ref{assump-norm} and their normalized projections have the same $\ell_2$-norm, as for any $W\in\mathbb{R}^{m\times n}$ with $m\leq n$, we have $\Vert \mathcal{P}_{g_1}(W) \Vert_2 = \Vert \mathcal{P}_{g_2}(W)\Vert_2 = \sqrt{nm}$.
\end{remark}


This assumption enables to obtain useful properties on $\mathcal{P}_{g}$ as we show in the following Lemma:
\begin{lemma}
\label{lem:properties-proj}
Let $g$ a norm satisfying Assumption~\ref{assump-norm}. Then
\begin{align*}
    \mathcal{P}_{g}\circ\mathcal{P}_{g} =\mathcal{P}_{g}
\end{align*}
and for all $x\in\mathbb{R}^d$, $g^*(\mathcal{P}_g(x))=\Vert \mathcal{P}_g(x)\Vert_2^2=c^2$, 
where $g^*$ is the dual norm associated with $g$.
\end{lemma}

Let us now introduce some additional notation to clearly state our result. Let $x_0\in\mathbb{R}^d$ and let us define for $n\geq 0$:
\begin{equation}
\begin{aligned}
\label{eq:seq}
    x_{2n+1}&:=\mathcal{P}_{g_1}(x_{2n})\\
    x_{2n+2}&:= \mathcal{P}_{g_2}(x_{2n+1})
    \end{aligned}
\end{equation}

which is exactly the sequence generated by Algorithm~\ref{alg:alt-proj} when $K=2$ and $x_0=\nabla_\theta\mathcal{L}(\theta_t,x^{(t)})$. Let us now show our main theoretical result, presented in the following Theorem.
\begin{theorem}
\label{thm:cvg}
Let $g_1$ and $g_2$ two norms on $\mathbb{R}^d$ satisfying Assumption~\ref{assump-norm} and such that their normalized projections have the same $\ell_2$ norm. Let also $(x_n)_{n_\geq 0}$ be defined as in~\eqref{eq:seq} and let us define the set of fixed-point as:
\begin{align*}
    \mathcal{F}:=\{x:~\mathcal{P}_{g_1}(x)=\mathcal{P}_{g_2}(x)=x\}
\end{align*}
Then by denoting $d(x,\mathcal{F}):=\min\limits_{z\in\mathcal{F}}\Vert x - z\Vert_2$ we have 
\begin{align*}
d(x_n,\mathcal{F}) \xrightarrow[n\to\infty]{} 0\; .
\end{align*}
% the sequence $(\Vert x_{n+1} - x_{n}\Vert_2)_{n\geq 0}$ monotonically decreases towards $0$ and \begin{align*}
%     g_1(x_n)\xrightarrow[n\to\infty]{} 1,~\text{and}~
%     g_2(x_n)\xrightarrow[n\to\infty]{} 1\; .
% \end{align*}
% Additionally, any cluster point $x$ of $(x_{n})_{n\geq 0}$ verifies:
% \begin{align*}
%     \mathcal{P}_{g_1}(x)=\mathcal{P}_{g_2}(x)=x\; .
% \end{align*}
\end{theorem}

This Theorem states that if $\texttt{MultiNorm}$ runs for a sufficient amount of time, then the returned point $x$ can be arbitrarily close to a fixed-point solution. While we cannot guarantee that it solves~\eqref{eq:multi-norm-opt}, we can assert that our algorithm converges to a fixed-point solution with arbitrary precision, and as a by-product produces a solution $x$ normalized w.r.t both norms $g_1$, $g_2$ (up to an arbitrary precision).

\begin{remark}
Note that in Theorem~\ref{thm:cvg} we assume that the normalized projections associated to $g_1$ and $g_2$ have the same $\ell_2$-norms. However, given two norms $g_1$ and $g_2$ satisfying Assumption~\ref{assump-norm}, i.e. such that for all $x$:
\begin{align*}
    \Vert \mathcal{P}_{g_1}(x) \Vert_2 &= c_1\\
    \Vert \mathcal{P}_{g_2}(x) \Vert_2 &= c_2
\end{align*}
for some $c_1,c_2>0$, and given a target value $a>0$, one can always rescale the norms such that their normalized projections have the same $\ell_2$ norm equal to $a$. More formally, by denoting $\tilde{g_1} = \frac{c_1}{a} g_1$ and $\tilde{g_2} = \frac{c_2}{a} g_2$, we obtain that
\begin{align*}
    \Vert \mathcal{P}_{\tilde{g}_1}(x)\Vert_2 =  \Vert \mathcal{P}_{\tilde{g}_2}(x)\Vert_2 = a .
\end{align*}
\end{remark}




\begin{remark}
It is worth noting that, for squared matrices ($m=n$), a single iteration ($L=1$) of \texttt{MultiNorm} using the norms considered in~\cite{ma2024swansgdnormalizationwhitening}, immediately converges to a fixed-point---precisely recovering SWAN.
\end{remark}



\subsection{MNGD: a New Family of Stateless Optimizers.} 
% We are now ready to present our family optimizers, called the \emph{Multi-Normalized Gradient Descents} (MNGDs), which is detailed in Algorithm~\ref{alg:multi-normalized-gd}. The main difference with the framework proposed in~\cite{bernstein2024old}, is that here we enable the normalization of the gradient w.r.t multiple norms thanks to the $\texttt{MultiNorm}$ step, while in~\cite{bernstein2024old}, the gradient is normalized according to a single norm as presented in~\eqref{eq:single-norm}.


We now introduce our family of optimizers: \emph{Multi-Normalized Gradient Descents} (MNGDs) (Algorithm~\ref{alg:multi-normalized-gd}). The key distinction from the framework proposed in~\cite{bernstein2024old} is that MNGDs normalize the gradient with respect to multiple norms using the 
$\texttt{MultiNorm}$ step, whereas in~\cite{bernstein2024old}, the gradient is normalized using a single norm, as shown in~\eqref{eq:single-norm}.
\begin{algorithm}[!t]
   \caption{Multi-Normalized GD ($\texttt{MNGD}$)}
   \label{alg:multi-normalized-gd}
\begin{algorithmic}
   \STATE {\bfseries Input:} $T\geq 1$ the number of updates, $(\eta_t)_{0\leq t\leq T}$ the global step-sizes, $\mathcal{L}$ the loss to minimize, $L\geq 1$ the number of iterations for the multi-normalization, and $\bm{g}:=(g_1,\dots,g_K)$ the norms.
   \STATE Initialize $\theta_0$
   \FOR{$t=1$  {\bfseries to} $T$}
    \STATE $\nabla_t\gets \nabla_{\theta}\mathcal{L}(\theta_t, x^{(t)})$ with $x^{(t)}\sim P_x$
    \STATE $\hat{\nabla}_t \gets \texttt{MultiNorm}(\nabla_t,L, \bm{g})$ as defined in Alg.~\ref{alg:alt-proj}.
    \STATE $\theta_{t+1} \gets \theta_t - \eta_t \hat{\nabla}_t$
    \ENDFOR
    \STATE Return $x$
\end{algorithmic}
\end{algorithm}

In the following, we focus on the MNGD scheme with a specific choice of norms, for which we can efficiently compute the gradient multi-normalization step. This enables the application of stateless optimizers to large LMs.

\section{Sinkhorn: a Multi-Normalization Procedure}
As in SWAN~\cite{ma2024swansgdnormalizationwhitening}, we propose to normalize the weight matrices according to multiple norms. We still leverage the row-wise $\ell_2$-norm to pre-process raw gradients, however, rather than using the spectral norm, we propose to consider instead a relaxed form of this constraint and use the column-wise $\ell_2$-norm. More formally, let us consider the two following norms on matrices of size $\mathbb{R}^{m\times n}$:
\begin{align*}
g_1(W):=\frac{\max\limits_{i\in[|1,m|]} \Vert W_{i,:}\Vert_2}{\sqrt{n}}\; ,\quad 
g_2(W):=\frac{\max\limits_{j\in[|1,n|]} \Vert W_{:,j}\Vert_2}{\sqrt{m}}\; ,
\end{align*}
which leads to the following two normalized projections:
\begin{align*}
    \mathcal{P}_{g_1}(W)&= \sqrt{n} Q(W)^{-1}W\\
    \mathcal{P}_{g_2}(W)&=\sqrt{m}W R(W)^{-1}
\end{align*}
where $R(W):=\text{Diag}(\Vert W_{:,1}\Vert_2,\dots,\Vert W_{:,n}\Vert_2)\in\mathbb{R}^{n\times n} $ is the diagonal matrix of size $n$ with the $\ell_2$-norm of the columns of $W$ as diagonal coefficients. For such a choice of norms, the $\texttt{MultiNorm}$ reduces to a simple procedure as presented in Algorithm~\ref{alg:Sinkhorn}.


\begin{remark}
For such a choice of norms, we obtain $\Vert \mathcal{P}_{g_1}(W) \Vert_2 = \Vert \mathcal{P}_{g_2}(W)\Vert_2 = \sqrt{nm}$ for any $W\in\mathbb{R}^{m\times n}$. In other words, both norms satisfy Assumption~\ref{assump-norm} and their $\ell_2$ norms are equal to $\sqrt{nm}$.
\end{remark}


For completeness we include  the MNGD scheme (Algorithm~\ref{alg:multi-normalized-sinkhorn}) that replaces the $\texttt{MultiNorm}$ step with $\texttt{SR-Sinkhorn}$ (Algorithm~\ref{alg:Sinkhorn}).


\begin{algorithm}[!t]
   \caption{$\texttt{SR-Sinkhorn}(\nabla,L)$}
   \label{alg:Sinkhorn}
\begin{algorithmic}
   \STATE {\bfseries Input:} the stochastic gradient $\nabla_W\mathcal{L}(W_t,x^{(t)})$, and $L\geq 1$ the number of iterations.
   \STATE Initialize $X=\nabla_W\mathcal{L}(W_t,x^{(t)})\in\mathbb{R}^{m\times n}$.
   \FOR{$\ell=1$  {\bfseries to} $L$}
   \STATE $X\gets  \sqrt{n} Q(X)^{-1}X$
   \STATE $X\gets  \sqrt{m} XR(X)^{-1}$
   \ENDFOR
    \STATE Return $X$
\end{algorithmic}
\end{algorithm}


\paragraph{The Sinkhorn Algorithm.} Before explicitly showing the link between Algorithm~\ref{alg:Sinkhorn} and the Sinkhorn algorithm, let us first recall the Sinkhorn theorem~\cite{sinkhorn1964relationship} and the Sinkhorn algorithm~\cite{sinkhorn1967concerning}. Given a positive coordinate-wise matrix $A\in\mathbb{R}_{+}^{m\times n}$, there exists a unique matrix $P\in\mathbb{R}_{+}^{m\times n}$ of the form $P=QAR$ with $Q$ and $R$ positive coordinate-wise and diagonal matrices of size $m$ and $n$ respectively, such that $P\bm{1}_n=n\bm{1}_m$ and $P^\top\bm{1}_m=m\bm{1}_n$. To find $P$, one can use the Sinkhorn algorithm that initializes $P_0:=A$ and computes for $k\geq 0$:
\begin{align*}
    P_{k+1/2}&=n\text{Diag}(P_k\bm{1}_n)^{-1}P_k\\
    P_{k+1}&=m P_{k+1/2}\text{Diag}(P_{k+1/2}^\top\bm{1}_m)^{-1}\; .
\end{align*}
Equivalently, these updates on $P$ can be directly expressed as updates on the diagonal coefficients of $Q=\text{Diag}(u)$ and $R=\text{Diag}(v)$ with $u\in\mathbb{R}_{+}^m$ and $v\in\mathbb{R}_{+}^n$. By initializing $u_0=\bm{1}_m$ an $v_0=\bm{1}_m$, the above updates can be reformulated as follows:
\begin{align}
\label{eq:update-diag-sin}
    u_{k+1} = n\frac{ \bm{1}_m}{Av_k},~~v_{k+1} = m\frac{\bm{1}_n}{ A^\top u_{k+1}}
\end{align}
where $/$ denote the coordinate-wise division.~\citet{franklin1989scaling} show the linear convergence of Sinkhornâ€™s iterations. More formally, they show that $(u_k, v_k)$ converges to some $(u^*,v^*)$ such that $P:=\text{Diag}(u^*)A\text{Diag}(v^*)$ satisfies $P\bm{1}_n=n\bm{1}_m$ and $P^\top\bm{1}_m=m\bm{1}_n$, and:
\begin{align*}
    d_\mathcal{H}(u_k, u^*)\in\mathcal{O}(\lambda(A)^{2k})~, \text{ and } ~d_\mathcal{H}(v_k, v^*)\in\mathcal{O}(\lambda(A)^{2k})\; ,
\end{align*}
where $d_{\mathcal{H}}$ is the Hilbert projective metric~\cite{de1993hilbert} and $\lambda(A)<1$ is a contraction factor associated with the matrix $A$.


\paragraph{Links between Sinkhorn and Algorithm~\ref{alg:Sinkhorn}.} Algorithm~\ref{alg:Sinkhorn} can be seen as a simple reparameterization of the updates presented in~\eqref{eq:update-diag-sin}. More precisely, given a gradient $\nabla\in\mathbb{R}^{m\times n}$ and denoting $A:=\nabla^{\odot 2}$, we obtain that the iterations of Algorithm~\ref{alg:Sinkhorn} exactly compute:
\begin{align}
\label{eq:update-diag-sin-sr}
    u_{k+1}^{1/2} = \sqrt{n\frac{ \bm{1}_m}{Av_k}},~~v_{k+1}^{1/2} = \sqrt{m\frac{\bm{1}_n}{ A^\top u_{k+1}}}
\end{align}
where the square-root is applied coordinate-wise, and returns after $L$ iterations $X_L=\text{Diag}(u_{L}^{1/2})\nabla \text{Diag}(v_{L}^{1/2})$. Therefore the linear convergence of Algorithm~\ref{alg:Sinkhorn} follows directly from the convergence rate of Sinkhorn, and Algorithm~\ref{alg:Sinkhorn} can be thought as applying the square-root Sinkhorn algorithm, thus the name $\texttt{SR-Sinhkorn}$. Note also that at convergence ($L\to+\infty$) we obtain $X^{*}\in\mathbb{R}^{m\times n}$ which is a fixed-point of both normalized projections, that is $\mathcal{P}_{g_1}(X^*)=\mathcal{P}_{g_2}(X^*)=X^*$,
from which we deduce that
\begin{align*}
\Vert X^*_{i,:}\Vert_2 = \sqrt{n}\;, \quad \text{and}\quad   \Vert X^*_{:,j}\Vert_2 = \sqrt{m}\;
\end{align*}
as demonstrated in Theorem~\ref{thm:cvg}.



\textbf{On the Importance of the Scaling.} Now that we have shown the convergence $\texttt{SR-Sinkhorn}$, let us explain in more detail the scaling considered for both the row-wise and column-wise normalizations. First recall that both norm $g_1$ and $g_2$ satisfy Assumption~\ref{assump-norm} and that the $\ell_2$ norm of their normalized projections is equal to $\sqrt{nm}$. The reason for this specific choice of scaling ($\sqrt{nm}$) is due to the global step-size in Algorithm~\ref{alg:multi-normalized-sinkhorn}. In our proposed MNGD, we did not prescribe how to select $\eta_t$. In practice, we aim to leverage the same global step-sizes as those used in Adam~\cite{adam} for training LLMs, and therefore we need to globally rescale the (pre-processed) gradient accordingly. To achieve that, observe that when EMAs are turned-off, Adam corresponds to a simple signed gradient descent, and therefore the Frobenius norm of the pre-processed gradient is simply $\sqrt{nm}$. Thus, when normalizing either the rows or the columns, we only need to rescale the normalized gradient accordingly.


\begin{algorithm}
   \caption{Sinkhorn GD ($\texttt{SinkGD}$)}
   \label{alg:multi-normalized-sinkhorn}
\begin{algorithmic}
   \STATE {\bfseries Input:} $T\geq 1$ the number of updates, $(\eta_t)_{0\leq t\leq T}$ the global step-sizes, $\mathcal{L}$ the loss to minimize, and $L\geq 1$ the number of iterations for the SR-Sinkhorn procedure.
   \STATE Initialize $\theta_0$
   \FOR{$t=1$  {\bfseries to} $T$}
    \STATE $\nabla_t\gets \nabla_{\theta}\mathcal{L}(\theta_t, x^{(t)})$ with $x^{(t)}\sim P_x$
    \STATE $\hat{\nabla}_t \gets \texttt{SR-Sinkhorn}(\nabla_t,L)$ as defined in Alg.~\ref{alg:Sinkhorn}.
    \STATE $\theta_{t+1} \gets \theta_t - \eta_t \hat{\nabla}_t$
    \ENDFOR
    \STATE Return $x$
\end{algorithmic}
\end{algorithm}
\vspace{-0.2cm}

\textbf{Computational Efficiency of SinkGD over SWAN.} Compared to SWAN~\cite{ma2024swansgdnormalizationwhitening}, the proposed approach,  \texttt{SinkGD}, is more efficient as it only requires $\mathcal{O}(nm)$ numerical operations. In contrast, SWAN, even when implemented with Newton-Schulz, still requires performing matrix-matrix multiplications, which have a time complexity of $\mathcal{O}(m^2(m+n))$. In the next section, we will demonstrate the practical effectiveness of MNGD with $\texttt{SR-Sinkhorn}$, that is \texttt{SinkGD}. This approach manages to be on par with, and even outperforms,  memory-efficient baselines for pretraining the family of LLaMA models up to 1B scale.









