\section*{Appendix}


\section{Implementation details}\label{app: code}


\paragraph{General setup} We describe the implementation setups for the LLM pre-training tasks. To enable a more straightforward and comparable analysis, we simply replicate the setting of \cite{Zhao2024GaLoreML}, under exactly the same model configs and optimizer hyperparameter configs, whenever possible. This includes the same model architecture, tokenizer, batch size, context length, learning rate scheduler, learning rates, subspace scaling, etc.  

\paragraph{Precision} All baselines uses BF16 for model weights, gradients, and optimizer states storage. For SWAN and SWAN$^\dag$, we follow the original paper and use FP32 in there whitening step.

\paragraph{Learning rate scheduling} we use exactly the same scheduler as in \cite{Zhao2024GaLoreML} for all methods.



\paragraph{Hyperparameters} Since \textbf{SinkGD} utilizes matrix-level operations on gradients, it can only be applied to 2D parameters. Therefore, in our experiments, we only apply \textbf{SinkGD} on all linear projection weights in transformer blocks. Similar to Galore \citep{Zhao2024GaLoreML}, the rest of the non-linear parameters still uses Adam as the default choice. Therefore, we follow the learning rate setup of Galore, where we fix some global learning rate across all model sizes and all modules. Then, for the linear projection modules where \textbf{SinkGD} is applied, we simply apply a scaling factor $\alpha$ on top of the global learning rate.  For all \textbf{SinkGD}, we adopt a \emph{lazy-tuning approach} (hyperparameters are set without extensive search), as detailed below. This helps to reduce the possibility of unfair performance distortion due to excessive tuning.   

\begin{itemize}
    \item \textbf{Adam}  For Adam we use same learning rate tuning procedure as suggested by \cite{Zhao2024GaLoreML} and \cite{ma2024swansgdnormalizationwhitening} (i.e., performing grid search over $\{0.01, 0.005, 0.001, 0.0005, 0.0001\}$). We found that the optimal learning rates for Adam is 0.001. The only exception is that for a model of size 1.3B: as we already know that a larger model requires smaller learning rates, we conduct a learning search for Adam over a smaller but more fine-grained grid of $\{ 0.001, 0.0007, 0.0005, 0.0003, 0.0001\}$. As a result, the optimal learning rate found for Adam on 1.3B is 0.0007.
    
    \item \textbf{SWAN}$^\dag$, is the tuned version of SWAN presented in \cite{ma2024swansgdnormalizationwhitening}. The original results of \textbf{SWAN} from \citet{ma2024swansgdnormalizationwhitening} assumes no learning rate warm-up and no learning rate tuning, in order to demonstrate the robustness of the method. This setting is more challenging than the setting of the usual literature \citep{Zhao2024GaLoreML, zhu2024apollo}. Hence, for fair comparison we relax those constraints and matches the setting of Galore and Apollo: we now allow learning rate warm-up (set to the same as Adam and Apollo), as well as larger learning rates for SWAN. This improved version of SWAN is denoted by \textbf{SWAN}$^\dag$. We use a global learning rate of 0.02, as well as the scaling factor $\alpha = 0.05$. This is selected by simply searching the learning rate over a constraint grid $\{0.01, 0.02, 0.05\}$, and then setting $\alpha = 0.05$ such that the effective learning rate is scaled back to 0.001. Finally, for other hyperparameters, we follow \cite{ma2024swansgdnormalizationwhitening}. 

    \item Finally, \textbf{SinkGD}, we use the same global learning rate of 0.02, as well as the scaling factor $\alpha = 0.05$ which are the same as \textbf{SWAN}$^\dag$, across all model sizes. We suspect with more careful tuning, its performance can be significantly improved; however, this is out of the scope of the paper. For $\texttt{SR-Sinkhorn}(\nabla,L)$ operation used in \textbf{SinkGD}, we simply use 5 steps.
\end{itemize}


\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:properties-proj}}


\begin{proof}
Let us assume that $\Vert\mathcal{P}_{g}(x)\Vert_2=c$ and so for any $x$. Then we have that:
\begin{align*}
    \Vert \Vert\mathcal{P}_{g}\circ \mathcal{P}_{g}(x)\Vert_2 \Vert \mathcal{P}_{g}(x) \Vert_2  &\geq g^*(\mathcal{P}_g(x)):=\langle \mathcal{P}_{g}\circ \mathcal{P}_{g}(x),\mathcal{P}_{g}(x)\rangle \\
    &\geq \sup_{z:~g(z)\leq 1} \langle z, \mathcal{P}_g(x)\rangle 
\end{align*}
where the first inequality follows from Cauchyâ€“Schwarz and the second inequality follows from the definition of $\mathcal{P}_g$. Now recall by definition, that $g(\mathcal{P}_g(x))\leq 1 $, and therefore we can select $z=\mathcal{P}_g(x)$ in the right inequality which gives:
\begin{align*}
     \Vert\mathcal{P}_{g}\circ \mathcal{P}_{g}(x)\Vert_2 \Vert \mathcal{P}_{g}(x) \Vert_2  &\geq g^*(\mathcal{P}_g(x))\\
    &\geq \Vert \mathcal{P}_g(x)\Vert_2^2 
\end{align*}
However because $\Vert\mathcal{P}_{g}\circ \mathcal{P}_{g}(x)\Vert_2 = \Vert\mathcal{P}_{g}(x)\Vert_2 = c$, we obtain that 
\begin{align*}
    g^*(\mathcal{P}_g(x))=\Vert \mathcal{P}_g(x)\Vert_2^2
\end{align*}
and by optimality, we also deduce that $\mathcal{P}_g\circ \mathcal{P}_g(x)=\mathcal{P}_g(x)$.
\end{proof}

\subsection{Proof of Thoerem~\ref{thm:cvg}}



\begin{proof}
First observe that thanks to Lemma~\ref{lem:properties-proj}, we have for any $n\geq 1$:
\begin{align}
\label{eq-proof-lem}
    \Vert x_n\Vert_2^2 = g_1^{*}(x_{2n-1})=g_2^{*}(x_{2n}) = c^2
\end{align}
where $g_1^*$ and $g_2^*$ are the dual norms of $g_1$ and $g_2$ respectively. We also have that for $n\geq 1$
\begin{align}
\label{eq-proof-ineq}
g_2(x_{2n}) \leq 1,~g_1(x_{2n+1})\leq 1
\end{align}
by definition of the normalized projections. We even have $$g_2(x_{2n})=g_1(x_{2n+1})=1$$ by optimality of the normalized projections. Let assume now that $n\geq 2$ is even, then we have that:
\begin{align*}
    \langle x_{n+1}, x_n\rangle &= \langle \mathcal{P}_{g_1}(x_{n}), x_n\rangle = g_1^{*}(x_n)\\
    &\geq \langle z ,x_n\rangle~\forall~z\in\mathcal{B}_1(0_d)
\end{align*}
where $\mathcal{B}_{g_1}(0_d)$ is the unit ball centered in $0_d$ associated to the norm $g_1$ and the inequality follows from the definition of $\mathcal{P}_{g_1}$. In particular by taking $z=x_{n-1}=\mathcal{P}_{g_1}(x_{n-2})\in\mathcal{B}_{g_1}(0_d)$, we obtain that:
\begin{align*}
    \langle x_{n+1}, x_n\rangle \geq \langle x_{n-1} ,x_n\rangle
\end{align*}
A similar proof can be conducted when $n$ is odd using the definition of $\mathcal{P}_{g_2}$. Therefore the sequence $(\langle x_{n+1}, x_n\rangle)_{n\geq 1}$ is increasing and bounded so it converges to a certain constant $r>0$. From this result we directly deduces that:
\begin{itemize}
    \item $(g_1^{*}(x_{2n}))_{n\geq 1}$ is monotonic increasing and converges towards $r$.
    \item $(g_2^{*}(x_{2n+1}))_{n\geq 1}$ is monotonic increasing and converges towards $r$.
\end{itemize}


Because $(x_{2n+1})_{n\geq 0}$ and $(x_{2n})_{n\geq 0}$ are bounded, we can extract a common subsequence $(x_{2\phi(n)+1})_{n\geq 1}$ and $(x_{2\phi(n)})_{n\geq 1}$ that converge to  some cluster points $x_1$ and $x_2$ respectively. 

Now by continuity of the dual norms and of the inner product we obtain that:
\begin{align*}
    \lim_{n\to\infty} g_2^{*}(x_{2\phi(n)+1})=g_2^{*}(x_1)\\
    \lim_{n\to\infty} g_1^{*}(x_{2\phi(n)})=g_1^{*}(x_2)\\
    \lim_{n\to\infty}\langle x_{2\phi(n)+1}, x_{\phi(n)}\rangle = \langle x_1, x_2\rangle 
\end{align*}
However observe that these three sequences are subsequences of $(\langle x_n,x_{n+1}\rangle)_{n\geq 0}$ which converges towards $r$, therefore we obtain that:
\begin{align*}
    r = g_2^{*}(x_1) = g_1^{*}(x_2) = \langle x_1, x_2\rangle 
\end{align*}
Additionally, remark that 
\begin{align}
\label{eq-proof=1}
g_2^{*}(x_{2\phi(n)+1})=g_2^*(\mathcal{P}_{g_1}(x_{2\phi(n)}))
\end{align}

Let us now show that $x_{2\phi(n)+1}=\mathcal{P}_{g_1}(x_{2\phi(n)})\xrightarrow[n\to\infty]{}\mathcal{P}_{g_1}(x_2)$. Indeed we have that:
\begin{align*}
    \langle \mathcal{P}_{g_1}(x_{2\phi(n)}), x_{2\phi(n)}\rangle = g_1^{*}(x_{2\phi(n)})\xrightarrow[n\to\infty]{}g_1^{*}(x_2)
\end{align*}
Then, because $(\mathcal{P}_{g_1}(x_{2\phi(n)}))_{n\geq 0}$ is bounded, we can extract a subsequence that converges towards $z$ such that $g_1(z)\leq 1$, from which follows that:
\begin{align*}
    \langle z,  x_2\rangle = \langle \mathcal{P}_{g_1}(x_2),x_2\rangle
\end{align*}
then by optimality of $\mathcal{P}_{g_1}(x_2)$ over the unit ball induced by $g_1$, we deduce that $z= \mathcal{P}_{g_1}(x_2)$. This is true for all converging sub-sequences of $(\mathcal{P}_{g_1}(x_{2\phi(n)}))_{n\geq 0}$, therefore we have that $\mathcal{P}_{g_1}(x_{2\phi(n)})\xrightarrow[n\to\infty]{}\mathcal{P}_{g_1}(x_2)$, and by unicity of the limit, it follows that 
$$x_1 =\mathcal{P}_{g_1}(x_2)\; .$$
Now from the equality $g_2^{*}(x_1) =  \langle x_1, x_2\rangle $, and given the fact that $g_2(x_2)\leq 1$ (as for all $n$ $g_2(x_{2\phi(n)})\leq 1$ which is obtained from~\eqref{eq-proof-ineq}), we deduce that 
$$x_2 = \mathcal{P}_{g_2}(x_1)$$
thanks to the optimality of $\mathcal{P}_{g_2}$. Now observe now that:
\begin{align*}
    g_2^*(x_1) = g_2^*(\mathcal{P}_{g_1}(x_2)) &= \langle \mathcal{P}_{g_2}\circ  \mathcal{P}_{g_1}(x_2), \mathcal{P}_{g_1}(x_2)\rangle\\
    & = \langle x_2, \mathcal{P}_{g_2}(x_2)\rangle 
\end{align*}
where the equality follows from the fact that:
\begin{align*}
    \mathcal{P}_{g_2}\circ  \mathcal{P}_{g_1}(x_2) = \mathcal{P}_{g_2}(x_1) = x_2
\end{align*}
and the two equalities follows the previous results obtained.
Therefore we obtain that
\begin{align*}
    g_2^*(x_1)  = g_2^{*}(x_2)=c^2
\end{align*}
where the last equality follows from~\eqref{eq-proof-lem}. Thus, we obtain that
\begin{align*}
 r = c^2 = g_2^*(x_1) =\langle x_1,x_2\rangle \leq \Vert x_1\Vert_2 \Vert x_2\Vert_2 
\end{align*}
but from~\eqref{eq-proof-lem}, $\Vert x_1\Vert_2=\Vert x_2\Vert_2=c$, from which follows that $x_1=x_2=x$, and $\mathcal{P}_{g_1}(x)=\mathcal{P}_{g_2}(x)=x$. As a by-product, we also obtain that $\langle x_n, x_{n+1}\rangle \xrightarrow[n\to\infty]{} r=c^2$, and therefore $\Vert x_n - x_{n-1}\Vert_2^2 = 2c^2 - 2\langle x_n,x_{n+1}\rangle \xrightarrow[n\to\infty]{}0$.

From the above proof, we also conclude that if $y$ is a cluster point of $(x_n)_{n\geq 0}$, then there exists $\psi$ such that $(x_{\psi(n)})_{n\geq 0}$ converges towards $y$ that satisfies:
$\mathcal{P}_{g_1}(y)=\mathcal{P}_{g_2}(y)=y$. Indeed this follows simply from the fact that we can extract a subsequence of $(x_{\psi(n)})_{n\geq 0}$ which has all indices that are either even or odd.

Let us now show that 
\begin{align*}
    g_1(x_n)\xrightarrow[n\to\infty]{} 1,~\text{and}~
    g_2(x_n)\xrightarrow[n\to\infty]{} 1\; .
\end{align*}

Indeed for a convergent subsequence, if the subsequence has infinitely many odd indices the result is trivial from the fact $g_1(x_{2n+1})=1$. Now if the indices are even, we obtain that $g_1(x_{2\phi(n)})\xrightarrow[n\to\infty]{}g_1(x)$, however $x$ has to be a fixed-point so $g_1(x)=g_1(\mathcal{P}_{g_1}(x))=1$. This hold for any subsequences, therefore we have $g_1(x_n)\xrightarrow[n\to\infty]{}1$. Similarly, we can apply the same reasoning for $g_2(x_n)$.

Let us now show the following Lemma.

\begin{lemma}
\label{lem-set-view}
Let $g_1$ and $g_2$ two norms satisfying the same assumption as in Theorem~\ref{thm:cvg}, that is for all $x$, $\Vert \mathcal{P}_{g_1}(x) \Vert_2 = \Vert \mathcal{P}_{g_2}(x)\Vert_2 = c$ with $c>0$. Then by denoting $\mathcal{S}_{g}$ the unit sphere associated to a norm $g$, we have:
\begin{align*}
    \mathcal{S}_{g_1}\cap \mathcal{S}_{g_2}\cap\mathcal{S}_{c\ell_2} = \mathcal{F}~~\text{where  }\mathcal{F}:=\{x:~\mathcal{P}_{g_1}(x)=\mathcal{P}_{g_2}(x)=x\}\; .
\end{align*}
\end{lemma}
\begin{proof}
Indeed $\mathcal{F}\subset \mathcal{S}_{g_1}\cap \mathcal{S}_{g_2}\cap\mathcal{S}_{c\ell_2} $ follows directly from the definition of $\mathcal{P}_{g_1}$, $\mathcal{P}_{g_1}$, and from Assumption~\ref{assump-norm}. Now let $z\in  \mathcal{S}_{g_1}\cap \mathcal{S}_{g_2}\cap\mathcal{S}_{c\ell_2}$. Observe that
\begin{align*}
    c^2\geq \langle z, \mathcal{P}_{1}(z)\rangle = \sup_{q:~g_1(q)=1}\langle z, q\rangle 
\end{align*}
where the inequality follows from the assumption on $\mathcal{P}_{g_1}$ and from the definition of $z$. Then as $g_1(z)=1$, we deduce that:
\begin{align*}
     c^2\geq \langle z, \mathcal{P}_{1}(z)\rangle \geq \Vert z\Vert_2^2=c^2
\end{align*}
from which folows that $\mathcal{P}_{g_1}(z)=z$. Similarly we deduce that $\mathcal{P}_{g_2}(z)=z$, and thus we have $\mathcal{S}_{g_1}\cap \mathcal{S}_{g_2}\cap\mathcal{S}_{c\ell_2}\subset \mathcal{F} $.
\end{proof}


Now observe that $d(x_n,\mathcal{S}_{g_1})\xrightarrow[n\to \infty]{} 0 $, and $d(x_n,\mathcal{S}_{g_1})\xrightarrow[n\to \infty]{} 0 $. Additionally, from~\eqref{eq-proof=1}, we have $d(x_n,\mathcal{S}_{c\ell_2})=0$, therefore we have that $d(x,\mathcal{S}_{g_1}\cap \mathcal{S}_{g_2}\cap\mathcal{S}_{c\ell_2})\xrightarrow[n\to\infty]{}0$ since all these spaces are closed, and the result follows from  Lemma~\ref{lem-set-view}.  

\end{proof}


\section{On the Convex Relaxation of Problem~\eqref{eq:multi-norm-opt}}
\label{sec:convex-relaxation}
\input{sections/general_norm}


% \subsection{Link with Hessian Approximation}

% In this work, we consider the problem of training LLMs using efficient gradient-based algorithm. To achieve this, we propose to consider a general framework. Let us denote $\theta\in\Theta\subset \mathbb{R}^d\to \mathcal{L}(\theta)\in\mathbb{R}$ the objective function we aim at minimizing. Given a certain point $\theta$, we can derive the second-order approximation of $\mathcal{L}$ which gives:
% \begin{align*}
%     \mathcal{L}(\theta + d\theta)=\mathcal{L}(\theta) + \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{1}{2} \Vert d\theta\Vert^2_{\textbf{H}_{\theta}} + o(\Vert d\theta\Vert^2)
%  \end{align*}
% where $\textbf{H}_\theta$ is the Hessian of $\mathcal{L}$ at $\theta$ and $\Vert \cdot \Vert_{\textbf{S}}=\sqrt{\langle \cdot, \textbf{S}\cdot\rangle }$ for $\textbf{S}$ an PD matrix. A natural way to update the parameter is to find $d\theta$ that minimizes the second-order approximation of the loss, that is:
% \begin{align}
% \label{eq:newton-opt}
%     \min_{d\theta\in\mathbb{R}^d} \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{1}{2} \Vert d\theta\Vert^2_{\textbf{H}_{\theta}}
% \end{align}


% Assuming one could compute the Hessian and invert it, this optimization problem admits a closed form solution given by: 
% \begin{align*}
%     d\theta = -\textbf{H}_\theta^{-1} \nabla \mathcal{L}(\theta)
% \end{align*}
% which leads to the following update:
% \begin{align*}
%     \theta \gets \theta - \eta \textbf{H}_\theta^{-1} \nabla \mathcal{L}(\theta)
% \end{align*}
% where $\eta>0$ is an optional the gradient step. However, in practice, it is often impracticable to estimate the Hessian $\textbf{H}_\theta$. To address this, a lot of effort has been put to approximate efficiently the Hessian under some structural assumptions. 

% Another approach consists in building surrogates of the objective function. For example, if $\mathcal{L}$ is $\lambda$-smooth w.r.t the Euclidean norm, one obtains that
% \begin{align*}
%     \mathcal{L}(\theta + d\theta)\leq \mathcal{L}(\theta) + \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{\lambda}{2} \Vert d\theta\Vert^2_{2}
% \end{align*}
% where $\Vert \cdot \Vert_{2}$ is the Euclidean norm, and one can decrease the objective by solving 
% \begin{align*}
% % \label{eq:gd-opt}
%     \min_{d\theta\in\mathbb{R}^d} \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{\lambda}{2} \Vert d\theta\Vert^2_{2}
% \end{align*}
% which leads to the standard gradient descent update
% \begin{align*}
%     \theta \gets \theta - \frac{1}{\lambda}\nabla \mathcal{L}(\theta)\; .
% \end{align*}

% A generalization of the above approach consists in considering an arbitrary norm $\Vert\cdot\Vert$ rather than the Euclidean one, which leads to the following optimization problem:
% \begin{align}
% \label{eq:norm-opt}
%     \min_{d\theta\in\mathbb{R}^d} \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{\lambda}{2} \Vert d\theta\Vert^2\; .
% \end{align}



% One way to connect the proposed approach with the original Newton method is to observe that one
% can decompose $\textbf{H}_{\theta}$ as a nonnegative weighted sum of different geometries, which gives:
% \begin{align*}
%     \textbf{H}_\theta = \sum_{i=1}^n \lambda_i \textbf{H}^{(i)}_\theta
% \end{align*}
% where each $\lambda_i >0$ and $\textbf{H}^{(i)}$ is PD. This leads to the following optimization problem:
% \begin{align*}
%     \min_{d\theta\in\mathbb{R}^d} \langle \nabla \mathcal{L}(\theta), d\theta\rangle +  \frac{1}{2} \sum_{i=1}^n \lambda_i \Vert d\theta\Vert^2_{\textbf{H}^{(i)}_{\theta}}
% \end{align*}
% which can be seen as the Lagragian formulation of the following projection problem:
% \begin{align*}
% \min_{d\theta\in\mathbb{R}^d} \langle \nabla \mathcal{L}(\theta), d\theta\rangle ~~\text{s.t.}~~ \forall~i~\Vert d\theta\Vert_{\textbf{H}^{(i)}_\theta} \leq 1\; .
% \end{align*}