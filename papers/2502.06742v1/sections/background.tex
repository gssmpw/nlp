\section{Background}

\subsection{From Adam to Stateless Optimizers}

\paragraph{Adam Optimizer.} Adam~\citep{adam} relies on accumulating internal states throughout training in order to improve the convergence. More formally, given a loss function $(\theta,x)\in\Theta\times\mathcal{X}\to \mathcal{L}(\theta,x)\in\mathbb{R}$, where $\Theta\subset\mathbb{R}^d$ is the set of learnable parameters and $\mathcal{X}$ is the set where the data resides, Adam aims at minimizing $\theta\to\mathbb{E}_{x\sim \mathbb{P}_x}(\mathcal{L}(\theta,x))$ where $\mathbb{P}_x$ is the distribution of data on $\mathcal{X}$. To achieve this, Adam computes at every step $t\geq 1$ a stochastic gradient associated with a mini-batch of input data $x^{(t)}$, and performs the following updates:
\begin{align*}
    \nabla_t &= \nabla_\theta \mathcal{L}(\theta_t,  x^{(t)})\\
    \text{m}_t & = \beta_1 \text{m}_{t-1} + (1-\beta_1) \nabla_t,\quad \hat{\text{m}}_t=\frac{\text{m}_t}{1-\beta_1^{t}} \\ 
    \text{s}_t & =  \beta_2 \text{s}_{t-1} + (1-\beta_2) \nabla_t^{\odot 2},\quad \hat{\text{s}}_t=\frac{\text{s}_t}{1-\beta_2^{t}}\\ 
    \theta_{t+1} &= \theta_t - \eta_t \frac{\hat{\text{m}}_t}{\sqrt{\hat{\text{s}}_t} +\varepsilon} 
\end{align*}
where $\odot$ is the Hadamard product, $\eta_t>0$ are global step-sizes, and $\beta_1, \beta_2>0$ are the weights of the exponential moving averages (EMAs) for the first and second moments respectively. During training, Adam optimizer stores two additional states $(\text{m}_t,\text{s}_t)$, effectively tripling the memory required to train the model compared to a simple stochastic gradient descent (SGD) scheme.

\paragraph{SWAN: a Stateless Optimizer.} Recently,~\citet{ma2024swansgdnormalizationwhitening} propose to move away from the paradigm of keeping track of internal states during the training of LLMs, and propose SWAN, a stateless optimizer that only pre-processes the stochastic gradients before updating the parameters. More precisely, they propose
to update the learnable weight matrices involved in the model using two matrix operators. Given a weight matrix $W\in\mathbb{R}^{m\times n}$, with $m\leq n$, at time $t\geq 1$, the SWAN update is:
\begin{equation}
\begin{aligned}
\label{eq:swan-update}
    \nabla_t &= \nabla_W \mathcal{L}(W_t,  x^{(t)})\\
    \tilde{\nabla}_t &=\sqrt{n} Q(\nabla_t)^{-1}\nabla_t\\
    \hat{\nabla}_t &=  \sqrt{n}(\tilde{\nabla}_t\tilde{\nabla}_t^\top)^{-1/2}\tilde{\nabla}_t\\
    W_{t+1}&= W_t - \eta_t  \hat{\nabla}_t\; , 
\end{aligned}
\end{equation}

where for a matrix $W\in\mathbb{R}^{m\times n}$, $Q(W):=\text{Diag}(\Vert W_{1,:}\Vert_2, \dots, \Vert W_{m,:}\Vert_2)$ is the diagonal matrix of size $m$ where the diagonal coefficients are the $\ell_2$-norm of the rows of $W$. To compute $(\hat{\nabla}_t\hat{\nabla}_t^\top)^{-1/2}$, the authors leverage the Newton-Schulz algorithm~\citep{song2022fast, li2018towards, huang2019iterative} instead of computing the SVD. While this approach does not require storing any additional states, it still suffers from a computational burden due to the $\mathcal{O}( m^2(n+m))$ computation of $(\nabla_t\nabla_t^\top)^{-1/2}\nabla_t$ which may limit usage for training large models.

\subsection{Steepest Descent as Gradient Normalization}
\label{sec:single-norm}
\citet{bernstein2024old} interpret several gradient descent schemes as steepest descent methods under specific norms. More formally, they propose to minimize a local quadratic model of the loss $\mathcal{L}(\cdot,x^{(t)})$ at $\theta_t$ w.r.t to a given norm $\Vert \cdot \Vert$, that is:
\begin{align*}
   \mathcal{Q}_{\Vert\cdot\Vert}(z):=\mathcal{L}(\theta_t, x^{(t)}) +\langle \nabla_t, z\rangle + \frac{\lambda_t}{2} \Vert z \Vert^2 
\end{align*}
where $\lambda_t>0$ are the sharpness parameters and $\nabla_t:=\nabla_{\theta} \mathcal{L}(\theta_t, x^{(t)})$ is the current stochastic gradient. 


As shown in~\citep{bernstein2024old}, finding a minimizer of $\mathcal{Q}_{\Vert\cdot\Vert}$ can be equivalently formulated as solving:
\begin{align}
\label{eq-steepest}
  -\frac{\Vert \nabla_t\Vert_{*}}{\lambda_t} \argmax_{z\in\mathbb{R}^d:~\Vert z\Vert= 1} \langle \nabla_t, z\rangle 
\end{align}
where $\Vert x \Vert_{*}:=\sup_{z\in\mathbb{R}^d:~\Vert z\Vert= 1} \langle x, z\rangle$ is the dual norm of $\Vert x\Vert$. Their framework encompasses a large family of optimizers that perform the following update: 
\begin{equation}
\begin{aligned}
\label{eq:single-norm}
    \nabla_t &= \nabla_{\theta} \mathcal{L}(\theta_t, x^{(t)}) \\
\hat{\nabla}_t &=\argmax_{z\in\mathbb{R}^d:~\Vert z\Vert= 1} \langle \nabla_t, z\rangle\\
\theta_{t+1} &= \theta_t -\frac{\Vert \nabla_t\Vert_{*}}{\lambda_t}\hat{\nabla}_t
\end{aligned}
\end{equation}

Several popular gradient-descent schemes can be recovered using the above approach. For example, when the $\ell_2$-norm is used, one recovers standard gradient descent, while the $\ell_{\infty}$ leads to signed gradient descent~\citep{carlson2015stochastic}. However, this framework considers only  a single norm for pre-processing the raw gradient $\nabla_t$. In the following, we extend this approach to incorporate multiple norms for gradient pre-processing, enabling the design of efficient and stateless optimizers for LLM training. 


