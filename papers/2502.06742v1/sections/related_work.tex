\subsection{Related Work}

\paragraph{Gradient Normalization.} Gradient normalization has emerged as a key technique in optimization, complementing its well-established role in forward-pass operations such as Layer Normalization (LayerNorm) \citep{Ba2016LayerN}. LARS and LAMB \citep{you2017lars, you2019lamb} employ global normalization to raw gradients and Adam's layer-wise updates, respectively, improving convergence and mitigating gradient pathologies in large-batch training. Apollo \citep{zhu2024apollo} introduces a channel-wise scaling approach, while SWAN \citep{ma2024swansgdnormalizationwhitening} replaces Adam's first-moment estimate with normalized gradients to stabilize gradient distributions. Theoretical analyses further underscore the importance of gradient normalization. \citet{hazan2015beyond} study its convergence properties in SGD, while \citet{cutkosky2020momentum} demonstrate that incorporating momentum enhances convergence without requiring large batches. \citet{bernstein2024old} interpret normalization in certain optimizers as a form of steepest descent under a specific norm, with SignSGD \citep{bernstein2018signsgd}, or standard gradient descent, serving as examples of gradient normalization.

\paragraph{Memory Efficient Optimizers.} Optimizers for large-scale training can reduce memory consumption primarily through two approaches: (1) low-rank approximation and (2) elimination of internal state dependencies. Low-rank optimizers project gradients onto a reduced subspace, allowing internal state updates within this subspace. ReLoRA \citep{Lialin2023ReLoRAHT} periodically merges LoRA \citep{hu2021lora} weights to restore full-rank representations. FLoRA \citep{Hao2024FloraLA} employs random Gaussian projections, whereas GaLore \citep{Zhao2024GaLoreML} utilizes singular value decomposition (SVD) for structured projections, further improved by Fira \citep{chen2024fira} via a compensation term. Apollo \citep{zhu2024apollo} minimizes memory overhead using rank-1 state representations. An alternative approach eliminates the need for internal states altogether. SWAN \citep{ma2024swansgdnormalizationwhitening} removes Adam's first and second moments through gradient normalization and whitening. Adam-mini \citep{zhang2024adam} reduces memory by leveraging block-wise second moment estimation. SGD-SaI \citep{xu2024no} obviates Adam’s second moment by precomputing learning rate scaling. Sign-based optimization \citep{chen2024symbolic} enables large-scale training using only first-moment updates. Muon \citep{jordan2024muon}, a simplification of Shampoo \citep{gupta2018shampoopreconditionedstochastictensor}, accelerates large model training via whitened first-moment updates, further demonstrating the viability of reduced-memory optimizers.


\paragraph{Alternating Projection.} Many iterative fixed-point algorithms employ alternating updates to enforce constraints or refine estimates. A classical example is the Von Neumann algorithm~\cite{von1950functional}, which alternates projections onto affine subspaces and converges to their intersection. The Sinkhorn algorithm~\cite{sinkhorn1967concerning} similarly alternates row and column normalizations, which can be seen as Bregman projections~\cite{benamou2015iterative} onto affine spaces, to approximate entropy-regularized optimal transport. While effective in Hilbert spaces, these algorithms do not generalize to arbitrary convex sets. Dykstra’s algorithm~\cite{dykstra1983algorithm} extends these methods by introducing correction terms, ensuring convergence to the exact projection. More generally, alternating projection methods have been extended through Pierra’s product space reformulation~\cite{pierra1984decomposition}, as well as modern techniques like ADMM~\cite{boyd2011distributed} and block-coordinate methods~\cite{tibshirani2017dykstra} in large-scale optimization. Despite these theoretical advances, extending alternating projection methods to non-convex settings remains a significant challenge. Recent progress includes manifold-based projection methods~\cite{ lewis2008alternating}, and proximal alternating techniques~\cite{bolte2014proximal}, which aim to improve convergence in non-convex problems, yet a comprehensive theory for convergence remains an open question.

