\section{Introduction}

The training of Large Language Models (LLMs) relies heavily  on adaptive optimization algorithms, such as Adam~\cite{adam}, which dynamically adjust learning rates for each parameter based on past gradient information, leading to faster convergence and improved stability. However, these optimizers introduce substantial memory overhead due to the storage of internal states, typically moment estimates of gradients, a challenge that becomes particularly pronounced in distributed training settings where memory constraints and communication overhead are critical concerns~\cite{rajbhandari2020zero, korthikanti2023reducing, llama3}. In contrast, simpler first-order optimization methods such as Stochastic Gradient Descent (SGD) require significantly less memory but fail to adequately train LLMs~\cite{zhao2024deconstructing, zhang2020adaptive, kunstner2023noise, kunstner2024heavy}. As a result, there is an ongoing need for developing new optimization strategies that resolves the memory efficiency v.s. training performance dilemma for large-scale models training.



Recent research has made significant strides in improving the efficiency of optimization methods by reducing the memory overhead associated with saving optimizer states~\cite{hu2021lora,Lialin2023ReLoRAHT, Zhao2024GaLoreML, Hao2024FloraLA, xu2024adamlearningratescaling, jordan2024muon, zhang2024adam, ma2024swansgdnormalizationwhitening, zhu2024apollo}. Among these advancements,  ~\citet{ma2024swansgdnormalizationwhitening} introduce SWAN, a stateless optimizer that only performs pre-processing operations on the instantaneous gradients, achieving the same memory footprint as SGD while delivering comparable or even better performances than Adam. Collectively, these advances demonstrate that memory efficiency and loss throughput are not mutually exclusive, opening pathways for efficient optimization in large-scale deep learning.

% However, several key questions remain regarding the stateless approach proposed by SWAN. Firstly, the underlying mathematical framework for the stateless processing of instantaneous gradients remains unexplored, preventing us to further generalize SWAN and design better stateless optimizers. Secondly, SWAN relies on the computationally expensive operation of whitening, which has a complexity of $\mathcal{O}(m^2(m + n))$ for gradients of size $m$ by $n$. In practical terms, this necessitates the implementation of whitening in a distributed manner, thereby hinder its scalability to large-scale training due to additional infrastructure burdens.





\captionsetup[subfigure]{labelformat=empty}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Sinkhorn_evaluation_ppl_comparison_350M_preview.png}
        \caption*{(a) 350M LLaMA model}
    \end{minipage}\hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Sinkhorn_evaluation_ppl_comparison_1B_preview_3.png}
        \caption*{(b) 1.3B LLaMA model}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sinkhorn_throughput.pdf}
        \caption*{(c) Training Throughputs}
    \end{minipage}\hfill
\caption{\textbf{\texttt{SinkGD} performance preview on LLM pretraining.}  \textbf{(a)} and \textbf{(b)}: Comparison of the test perplexities obtained by Adam~\cite{adam, Zhao2024GaLoreML}, SWAN \citep{ma2024swansgdnormalizationwhitening}, Apollo \citep{zhu2024apollo},  and our proposed \texttt{SinkGD} (Algorithm~\ref{alg:multi-normalized-sinkhorn}) on 1B LLaMA pretraining task with C4 dataset. All loss curves of Adam and Apollo-mini are reproduced from the corresponding opensource codes. We also compare with their official results in \Cref{tab:main}. On both 350M and 1.3B LLama architectures, \texttt{SinkGD}  achieves $>$ 2× speed-up vs Adam in terms of tokens seen; and 1.3 to 1.5 X speed-up vs SWAN and Apollo. \textbf{(c)}: Training throughput analysis on training 1.3 B model on 8 $\times$ A100, under constant batch size = 130K tokens. We present two metrics: raw throughput, measured by number of training tokens consumed per second; and effective throughput, which is raw throughput adjusted by the token efficiency of optimizer relative to Adam. \texttt{SinkGD} has a raw throughput that is marginally higher than Adam, while improving the effective throughput by $>$ 3×.
}
\label{fig: preview}
\end{figure*}


\paragraph{Contributions.} Motivated by the recent success of SWAN~\cite{ma2024swansgdnormalizationwhitening}, we introduce a framework for designing stateless optimizers based on a novel multi-normalization scheme. Unlike standard first-order methods that can be interpreted as gradient normalization according to a single norm ~\cite{bernstein2024old}, our approach aims at normalizing gradients according to multiple norms. We demonstrate that SWAN is a specific instance of our general framework. However, a key limitation of SWAN is its computational overhead: it relies on whitening/orthogonalization operation which has complexity $\mathcal{O}(m^2(m+n))$. This may hinder its scalability to large-scale training. To overcome this, we propose a new stateless optimizer that achieves Adam-level computational cost ($\mathcal{O}(mn)$), while having the same memory footprint as SGD. Moreover, it achieves on par or even outperforms Adam in LLM pretraining tasks, as well as various existing memory-efficient baselines under LLaMA architecture. Our contributions are summarized below:
% that incurs small additional computational cost compared to standard SGD, while having the same memory footprint, and that achieves on par or even outperforms various existing memory-efficient baselines in LLaMA training at the 1B scale. Our contributions are summarized below:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Multi-Normalized Gradient Descent.} 
    \begin{enumerate}[leftmargin=1em]
         \item We propose a novel family of first-order  methods, called \emph{Multi-Normalized Gradient Descent} (MNGD), that aims at normalizing gradients according to multiple norms. Our framework generalizes the steepest descent viewpoint of~\cite{bernstein2024old} that recasts popular first-order optimizers as normalization of gradients under a single norm.
    
        \item We then propose a simple alternating scheme in order to effectively compute the multi-normalization of gradients, and show that our algorithm can provide a fixed-point solution up to an arbitrary precision, ensuring the normalization of its output with respect to the norms considered. 
        
        \item We demonstrate that SWAN~\cite{ma2024swansgdnormalizationwhitening} is a particular instance of MNGD where the gradient is normalized according to two well-chosen norms: (1) the row-wise $\ell_2$-norm, and (2) the spectral norm. 
    \end{enumerate}

    \item \textbf{\texttt{SinkGD}: an efficient, scalable, and stateless optimizer.}

    \begin{enumerate}[leftmargin=1em]
         \item We leverage our framework and design a new stateless optimizer that relaxes the constraint of SWAN to improve computational efficiency. Our algorithm, namely \texttt{SinkGD}~(Algorithm~\ref{alg:multi-normalized-sinkhorn}), alternatively performs row-wise and column-wise normalization according to the Euclidean geometry. We show that \texttt{SinkGD} exactly recovers the square-root iterates of the Sinkhorn algorithm~\cite{sinkhorn1964relationship}.

    \item Finally, we evaluate our Sinkhorn-based stateless optimizer \texttt{SinkGD} by training LlaMA models on various scales, from 60m to 1.3B. Results (\Cref{fig: preview}) show that \texttt{SinkGD} manages to be on par or even outperforms the Adam optimizer, as well as other memory-efficient baselines, achieving a $3$× speedup over Adam at 1B scale, with significantly reduced memory requirements.
    \end{enumerate}
    
    
   
\end{itemize}
