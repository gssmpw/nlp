\paragraph{Conclusion.}
In this work, we present a novel framework for designing stateless optimization algorithms tailored for LLM training. Our approach is based on the normalization of stochastic gradients with respect to multiple norms, and we propose an alternating optimization procedure to achieve this normalization efficiently. We establish that our multi-normalization scheme can approximate, to arbitrary precision, a fixed point of the optimization problem, thereby ensuring that the gradient is appropriately scaled according to both norms. Furthermore, we extend and improve upon SWAN~\cite{ma2024swansgdnormalizationwhitening}, a specific instance of our framework, by introducing SinGD, a stateless optimizer that enforces row-wise and column-wise $\ell_2$ normalization. We demonstrate that this procedure is theoretically guaranteed to converge and provide empirical evidence that it outperforms SoTA memory-efficient optimizers, as well as Adam, in training a 1B-parameter LLaMA model on the C4 dataset. Future research directions include exploring alternative normalization schemes to further enhance the efficiency of stateless optimizers and extending the applicability of SinGD to other training regimes beyond LLM pre-training.


% In this work, we introduce a new framework to design stateless optimizers from LLM training. Our approach aims at normalizing stochastic gradients w.r.t multiple norms and we derive a simple alternating procedure in order to achieve it. We show that our alternative multi-normalization scheme can produce, up to an arbitrary precision, a fixed-point of the problem, ensuring that the gradient is effectively properly normalized according to both norms. Additionally, we improve upon SWAN~\cite{ma2024swansgdnormalizationwhitening}, which is a particular instance of this framework, and propose SR-Sinkhorn, that performs row-wise and column-wise $\ell_2$ normalization. This procedure is guarented to converge, and we show experimentally that this optimizer outperforms all current memory-efficient optimizers as well as Adam on training 1B LLaMA model on C4 in term of test perplexity, achieving 3Ã— speedup compared to Adam. Future work may explore other choice of norms to design more efficient stateless optimizers and further expand SR-Sinkhorn's applicability to other complex training regimes beyond LLM pre-training.


