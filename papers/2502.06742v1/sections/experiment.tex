% \begin{figure}[t!]
%     \centering
%     \includegraphics[scale=0.3]{figures/Sinkhorn_evaluation_ppl_comparison_1B_preview_2.png}
%     \caption{Comparison of the test perplexities obtained by Adam~\cite{adam} and our proposed SinkGD (Algorithm~\ref{alg:multi-normalized-sinkhorn}) on 1B LLaMA pretraining task with C4 dataset.}
%     \label{fig:1b-training}
% \vspace{-0.8cm}
% \end{figure}

\section{Experimental Results}
\label{sec:pretrain}
In this section, we evaluate the empirical performance of applying \texttt{SinkGD} optimizer to LLM pretraining tasks. All experiments were performed on NVIDIA A100 GPUs.


\subsection{LlaMA Pre-training Tasks} 

\paragraph{Setup.} We evaluate \textbf{SinkGD} on LLM pre-training tasks using a LLaMA-based architecture~\cite{touvron2023llama} with RMSNorm and SwiGLU activations \citep{zhang2019root, gao2023eigenvalue}. We consider models with 60M, 130M, 350M, and 1.3B parameters, all trained on the C4 dataset~\cite{2020t5} using an effective token batch size of 130K tokens (total batch size 512, context length 256). Specifically, for both 130M and 350M, we use 128 batch size with 4 accumulations. For 60M and 1B, we uses 256 batch with 2 accumulation, and 32 per-device batch size with 2 accumulation and 8xA100s, respectively.
Following the setup of \cite{Zhao2024GaLoreML, zhu2024apollo}, $\textbf{SinkGD}$ is applied to all linear modules in both attention and MLP blocks with $L=5$ iterations for the $\texttt{SR-Sinkhorn}$ procedure. For all other modules, that are the embedding layer, the RMSnorm layers, and the last output layer, \textbf{Adam} optimizer~\cite{adam} is used. We use exactly the same cosine learning rate scheduler as in \cite{Zhao2024GaLoreML}, where $10\%$ of total training steps is used for warm-up. Note that, as in~\cite{Zhao2024GaLoreML,zhu2024apollo}, we use a group-wise learning rate for our optimizer. The effective learning rate used for linear modules in the transformer blocks is of the form $\alpha \eta_t$ where $\eta_t$ is global learning rate provided by the scheduler and $\alpha$ is fixed hyperparameter that we set to $\alpha=0.05$. For Adam, we use $\eta_t$ as the learning rate. 

\paragraph{Baselines.} We consider the following memory-efficient optimizers baselines: \textbf{Adam} \citep{adam};  \textbf{Galore}~\cite{Zhao2024GaLoreML}; \textbf{Fira}~\cite{chen2024firaachievefullranktraining}, \textbf{Apollo} and \textbf{Apollo-mini} \cite{zhu2024apollo}, and  \textbf{SWAN}~\cite{ma2024swansgdnormalizationwhitening}. For all methods, training uses BF16 precision for weights, gradients and optimizer states by default, except for $\textbf{SWAN}$ that uses FP32 precision to pre-process the gradient~\cite{ma2024swansgdnormalizationwhitening}. We also perform a grid search of learning rate for Adam over $\{0.01, 0.005, 0.001, 0.0005, 0.0001\}$, except for 1B model which we search over $\{ 0.001, 0.0007, 0.0005, 0.0003, 0.0001\}$. We do not perform any weight decay for all optimizers. 

\begin{table*}[t!]
    \centering
    \caption{\small{Comparison with Adam and  memory-efficient baselines on pre-training various sizes of LLaMA models with C4 dataset. Test perplexity is reported, along with a memory estimate of the total of parameters and optimizer states based on BF16 format. The perplexities reported for all competitive methods are taken from \citet{Zhao2024GaLoreML, zhu2024apollo}, as well as the \textbf{SWAN} results taken from \citet{ma2024swansgdnormalizationwhitening}. Remarks: 1) As we cannot reproduce the Adam results from \citet{Zhao2024GaLoreML}, we report both the reported Adam results from \citet{Zhao2024GaLoreML} and our reproduced result; 2) The memory estimations from \citet{Zhao2024GaLoreML, zhu2024apollo} did not consider the fact that Adam optimizer was used for embedding layers. This is corrected in our memory estimates. 3) The results of \textbf{SWAN} from \citet{ma2024swansgdnormalizationwhitening} assumes no learning rate warm-up and no learning rate tuning. For fair comparison, we also report the performance of \textbf{SWAN} that matches the setting of \textbf{Galore} and \textbf{Apollo}, where in with learning rate warm-up and larger learning rates are allowed. This is denoted by \textbf{SWAN}$^\dag$. }  }
    \label{tab:main}
    \begin{tabular}{|c|cc|cc|cc|cc|}
    \toprule
               Methods & \multicolumn{2}{c|}{\textbf{60M}} & \multicolumn{2}{c|}{\textbf{130M}} & \multicolumn{2}{c|}{\textbf{350M}} & \multicolumn{2}{c|}{\textbf{1.3B}} \\
  
    & PPL & MEM & PPL & MEM & PPL & MEM & PPL & MEM\\
     \midrule 
    Adam (reproduced) & 33.94 & 0.32G  & 25.03 &0.75G        & 19.24 & 2.05G  & 16.44 &7.48G \\
    Adam (cited) &  34.06 & 0.32G  & 25.08 &0.75G        & 18.80 & 2.05G  & 15.56 &7.48G \\
    \midrule 
     
     Galore & 34.88 &0.26G& 25.36& 0.57G &18.95& 1.29G& 15.64& 4.43G\\
    Fira & 31.06 &0.26G& \textbf{22.73} & 0.57G& 17.03& 1.29G& 14.31& 4.43G \\
    Apollo-mini &  31.93 &0.23G& 23.53& 0.43G& 17.18& 0.93G& 14.17& 2.98G \\
    Apollo &  31.55 &0.26G& 22.94& 0.57G& 16.85& 1.29G& 14.20& 4.43G \\

    
    SWAN &  32.28 &0.23G& 24.13 &0.43G& 18.22 &0.93G& 15.13& 2.98G \\

    SWAN$^\dag$ &  \textbf{30.00} &0.23G& 22.83 &0.43G& 17.14 &0.93G& 14.42& 2.98G \\
    
    \midrule
    SinkGD & $30.99$ & 0.23G & \textbf{22.75} &0.43G &  
    \textbf{16.51} &0.93G & \textbf{13.51}  &2.98G \\
    \bottomrule
    SinkGD speed up v.s. Adam (reproduced) & \multicolumn{2}{c|}{1.60 X} & \multicolumn{2}{c|}{1.56 X} & \multicolumn{2}{c|}{2.42 X} & \multicolumn{2}{c|}{2.79 X} \\
    SinkGD speed up v.s. Adam (cited) & \multicolumn{2}{c|}{1.66 X} & \multicolumn{2}{c|}{1.73 X} & \multicolumn{2}{c|}{2.10 X} & \multicolumn{2}{c|}{2.17 X} \\
    Total Training Steps & \multicolumn{2}{c|}{10K} & \multicolumn{2}{c|}{20K} & \multicolumn{2}{c|}{60K} & \multicolumn{2}{c|}{100K}  \\ 
    \bottomrule
    \end{tabular}
\end{table*}


\begin{table}[H]
\vspace{-0.5cm}
\centering
\caption{Comparison of the test perplexities obtained during training when training 1B LLaMA with SinkGD v.s. 7B LLaMA using different baselines. For Apollo, Apollo-mini, 8-bit Adam and Galore, we cite the number from \citet{zhu2024apollo}.}
\label{tab: 7B preformance}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|llll}
\hline
Method           & Mem.   & 40K & 80K & 120K & 150K \\ \hline
8-bit Adam (7B)&26G &18.09 & 15.47 & 14.83 & 14.61  \\
8-bit Galore (7B) &18G & 17.94 & 15.39 &14.95 &14.65\\
Apollo (7B)      & 15.03G &  17.55   &  14.39   &  13.23    &   13.02   \\
Apollo-mini (7B) & 13.53G &  18.03   &  14.60   &  13.32    &  13.09    \\\hline
SinkGD (1B)    & 2.98G   &  \textbf{16.44}   &  \textbf{14.27}   & \textbf{13.17}     & \textbf{12.97}    \\   

\hline
\end{tabular}}
\end{table}

\paragraph{Performance evaluation and memory efficiency analysis.} The results presented in~\Cref{tab:main} demonstrate the effectiveness of \textbf{SinkGD} in terms of both computational efficiency and model performance. Notably, \textbf{SinkGD} achieves competitive performance while maintaining the lowest estimated memory consumption, comparable to that of SGD. Across all evaluated models, our method performs on par with or even surpasses \textbf{Adam} and other memory-efficient baselines in terms of test perplexity. In particular, \textbf{SinkGD} outperforms all other baselines in this experimental setup for the 350M and 1.3B model variants. Additionally, we quantify the computational efficiency of \textbf{SinkGD} by measuring the speed-up relative to \textbf{Adam}. This is determined by computing the ratio of the total training steps of \textbf{Adam} to the number of steps needed for \textbf{SinkGD} to reach the same final test perplexity. Note also that the reported memory consumption values in~\Cref{tab:main} account for three components: (1) memory allocated for model parameters, (2) optimizer-related costs for linear modules within transformer blocks, and (3) the Adam optimizer's memory footprint for the remaining parameters.


\begin{table}[H]
\vspace{-0.5cm}
\centering
\caption{Raw and effective throughput analysis. }
\label{tab: throughput}
\begin{tabular}{l|l}
\hline
      Method   & Raw / eff. throughput \\ \hline
Adam & 53047 / 53047 (tokens/s)          \\
SinkGD     & 57982 / 161769  (tokens/s)         \\ \hline
\end{tabular}
\end{table}

\paragraph{Comparative analysis of 1B and 7B LLaMA training.} To further evaluate the efficacy of our proposed optimizer, we replicate the experimental setup of \cite{zhu2024apollo}, but instead train a 1B-parameter LLaMA model using \textbf{SinkGD} and compare its performance against a 7B-parameter LLaMA model trained with \textbf{Apollo}, \textbf{Apollo-mini}, \textbf{8-bit Adam}, and \textbf{8-bit Galore}. As shown in Table \ref{tab: 7B preformance}, the 1B model trained with \textbf{SinkGD} achieves comparable test perplexities to those of the 7B model trained with \textbf{Apollo} after 150K optimization steps, while incurring significantly lower costs. Notably, training the 7B LLaMA model with Apollo requires \textbf{15} days on an 8xA100 GPU setup to complete 150K steps, whereas our approach achieves a similar loss in \textbf{3.3} days. The reported memory estimates correspond to the total memory cost detailed in the previous paragraph.


\subsection{Ablation Study}
\label{sec:ablation}

\paragraph{Throughput analysis.} We also assess throughput when training a 1.3B-parameter model on 8xA100 GPUs. We use two metrics: (1) the \emph{raw throughput} which is the number of tokens processed per second, and (2) the \emph{effective throughput} defined as the total training token used by Adam divided by the time (in seconds) used by \textbf{SinkGD} to reach the same test perplexities.
These metrics evaluate the impact of the multi-normalization step on training speed, and also account for the fact that some optimizers make more effective use of training tokens. As shown in \Cref{tab: throughput}, \textbf{SinkGD} achieves competitive raw throughput compared to \textbf{Adam}, suggesting the multi-normalization step does not require expensive computations. Furthermore, \textbf{SinkGD} exhibits a 3 $\times$ higher effective throughput than Adam, indicating a significantly faster wall-clock time convergence.


\paragraph{On the effect of the number of iterations.} In this experiment, we measure the effect of applying different iterations of our proposed $\texttt{MultiNorm}$ (Algorithm~\ref{alg:alt-proj}) scheme in the specific case of the \textbf{SWAN} and \textbf{SinkGD} methods. More specifically, we train a 130M LLaMA model on C4 datasets and compare the test perplexities obtained after 10K steps. We observe that the number of iterations has marginal effect on the performance of the algorithms. However, as we still observe a consistent improvement when using $L=5$ iterations, we decide to use this number of iterations in our benchmark evaluation, as reported in table~\ref{tab:main}.  

\begin{table}[H]
\vspace{-0.5cm}
\centering
\caption{Comparison of the test perplexities obtained during training at 10K steps when training 130M LLaMA model with either SWAN or SinkGD using different number of iterations in \texttt{MultiNorm} procedure.}
\label{tab:num-iter}
\begin{tabular}{|c|c|}
\hline
Method           & PPL   \\ 
\hline
SWAN ($L=1$) & 26.79 \\ 
SWAN ($L=5$) & 26.56\\ 
\hline
SinkGD ($L=1$)     & 26.21  \\ 
SinkGD ($L=5$)    & 26.13  \\ 
\hline
\end{tabular}
\end{table}