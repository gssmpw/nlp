\section{Related Work}
\label{sec:relatedwork}
In this section, we review several relevant topics of our method, including knowledge distillation and reinforcement learning from external feedback. 


\textbf{Knowledge Distillation.} Recent studies on knowledge distillation for language models have primarily focused on transferring reasoning capabilities from large language models (LLMs) to smaller models**Vinyals et al., "Matching Networks for One Shot Learning"**. For example, **Bertinetto et al., "Dense Discriminative Models for Human Video Segmentation"** employed semantic decompositions to distill reasoning skills. Most existing approaches rely on supervised fine-tuning**Sun et al., "Meta-Learning with Memory-Augmented Neural Networks"** and  leverages advanced LLMs such as the GPT series**Radford et al., "Improving Language Understanding by Generative Pre-Training"** as the guidance to generate high-quality data **. Symbolic Chain-of-Thought Distillation**Veen et al., "Symbolic Reasoning in Deep Learning"** introduced a step-by-step reasoning framework, highlighting the potential of distilling complex reasoning processes, while FLD**Hausknecht et al., "Deep Reinforcement Learning for Large-Scale Multi-Agent Systems"** focused on logical deductive reasoning.
More recently, **Sukhbaatar et al., "Learning Multi-Level Distributed Representations Inside A Single Neural Network"** proposed distilling multiple small models via supervised fine-tuning (SFT) over reasoning data generated by DeepSeek-R1. However, most methods treat LLMs merely as sources of reasoning chains, optimizing student models exclusively through supervised fine-tuning. Additionally, some works have explored reinforcement learning to further enhance reasoning capabilities; for instance, MARIO**Hou et al., "MARIO: A Meta-Reinforcement Learning Framework for AI"** employs multiple external reward models to improve self-rationalization.
In contrast, our work takes a different approach by leveraging LLMs not only for response generation but also for extracting reward signals, enabling a more comprehensive distillation process.


\textbf{Reinforcement learning from External Feedback.} Following the success of Reinforcement Learning from Human Feedback (RLHF)**Jiang et al., "Robust Meta-Optimization via Reinforcement Learning"** in enabling the widespread application of large language models, researchers have increasingly explored ways to reduce human involvement in training through Reinforcement Learning from AI Feedback (RLAIF).
As a pioneer in RLAIF, Constitutional AI**Huang et al., "Constitutional AI: A Framework for Explainable and Fair Decision-Making"** has demonstrated improved performance in tasks such as summarization, helpful dialogue generation, and harmless dialogue generation. **Sukhbaatar et al., "Learning Multi-Level Distributed Representations Inside A Single Neural Network"** further showed that RLAIF can achieve performance comparable to or even surpassing RLHF, as evaluated by human judges. 
% Moreover, RLAIF has been shown to outperform a supervised fine-tuning baseline, even when the LLM preference labeler is the same size as the policy model.
SPIN**Melo et al., "Self-Paced Iterative Neural Network"** eliminates the need for explicit reward models by adopting an iterative DPO-like framework, where human-labeled winning responses are paired with the previous iterationâ€™s generations as losing responses. However, those method do not focus on training smaller models through knowledge distillation.


% \textbf{LLM-as-a-Judge} Using LLM-as-a-Judge prompting to evaluate language models has become a standard approach**Fernandes et al., "Improving Language Understanding by Large-Scale Meta-Learning"**, and is being used to train reward models or curate data as well, as described above **Chen et al., "Meta-Learning for Adaptive Control of Dynamical Systems"**. While some works such as Kim et al. **Kim et al., "Efficient Neural Architecture Search via Hierarchical Graph Learning"** create training data to train an LLM to perform well as a judge, to our knowledge it is not common to combine this training with general instruction following skills as in our work.


\vspace{-2pt}