\section{Related Work}
\label{sec:relatedwork}
In this section, we review several relevant topics of our method, including knowledge distillation and reinforcement learning from external feedback. 


\textbf{Knowledge Distillation.} Recent studies on knowledge distillation for language models have primarily focused on transferring reasoning capabilities from large language models (LLMs) to smaller models____. For example, ____ employed semantic decompositions to distill reasoning skills. Most existing approaches rely on supervised fine-tuning____ and  leverages advanced LLMs such as the GPT series____ as the guidance to generate high-quality data ____. Symbolic Chain-of-Thought Distillation____ introduced a step-by-step reasoning framework, highlighting the potential of distilling complex reasoning processes, while FLD____ focused on logical deductive reasoning.
More recently, ____ proposed distilling multiple small models via supervised fine-tuning (SFT) over reasoning data generated by DeepSeek-R1. However, most methods treat LLMs merely as sources of reasoning chains, optimizing student models exclusively through supervised fine-tuning. Additionally, some works have explored reinforcement learning to further enhance reasoning capabilities; for instance, MARIO____ employs multiple external reward models to improve self-rationalization.
In contrast, our work takes a different approach by leveraging LLMs not only for response generation but also for extracting reward signals, enabling a more comprehensive distillation process.


\textbf{Reinforcement learning from External Feedback.} Following the success of Reinforcement Learning from Human Feedback (RLHF)____ in enabling the widespread application of large language models, researchers have increasingly explored ways to reduce human involvement in training through Reinforcement Learning from AI Feedback (RLAIF).
As a pioneer in RLAIF, Constitutional AI____ has demonstrated improved performance in tasks such as summarization, helpful dialogue generation, and harmless dialogue generation. ____ further showed that RLAIF can achieve performance comparable to or even surpassing RLHF, as evaluated by human judges. 
% Moreover, RLAIF has been shown to outperform a supervised fine-tuning baseline, even when the LLM preference labeler is the same size as the policy model.
SPIN____ eliminates the need for explicit reward models by adopting an iterative DPO-like framework, where human-labeled winning responses are paired with the previous iterationâ€™s generations as losing responses. However, those method do not focus on training smaller models through knowledge distillation.


% \textbf{LLM-as-a-Judge} Using LLM-as-a-Judge prompting to evaluate language models has become a standard approach____[Dubois et al., 2023, Li et al., 2023, Fernandes et al., 2023, Bai et al., 2023, Saha et al., 2023], and is being used to train reward models or curate data as well, as described above [Lee et al., 2023, Chen et al., 2024a, Li et al., 2024]. While some works such as Kim et al. [2023] create training data to train an LLM to perform well as a judge, to our knowledge it is not common to combine this training with general instruction following skills as in our work.


\vspace{-2pt}