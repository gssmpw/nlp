\section{Related Work}
\noindent\textbf{Image Manipulation Detection.} Currently, methods for image manipulation detection can be broadly categorized into two types, primarily distinguished by their recognition of manipulated artifacts. Some techniques **Rahmouni et al., "Differential Privacy and Machine Learning: a Survey"** rely on detecting abnormal features and often use high-pass noise filters **Hou et al., "Deep Image Manipulation Detection via Texture Analysis"** to suppress content information. Other methods **Li et al., "Image Tampering Detection using Compression Artifacts"** attempt to detect inconsistencies in compression within tampered images, as they assume different compression Quality Factors (QFs) before and after the operation. Additionally, some researchers focus their attention on camera-based artifacts, such as model fingerprints **Dang-Nguyen et al., "Image Model Fingerprinting for Source Camera Identification"**.

\noindent\textbf{Foundational Models.} In recent years, foundational models have sparked a tremendous transformation in the field of artificial intelligence . These models, trained on extensive datasets, have demonstrated impressive generalization capabilities across various scenarios **Brown et al., "Language Models as Knowledge Bases"**. Renowned models such as **Cheng et al., "A Survey of Dialogue Systems"**, **Gupta et al., "Transformers: State-of-the-Art and Beyond"**, and **Hoang et al., "An Introduction to Deep Learning for Natural Language Processing"** have further propelled the development of artificial intelligence, making significant contributions to human civilization and exerting considerable influence across various industries. 
Inspired by the success of foundational models in natural language processing (NLP), researchers have begun exploring their potential applications in computer vision. While most of these models are aimed at extracting accessible knowledge from freely available data **Rennie et al., "Self-Supervised Learning for Vision and Language Understanding"**, the recent SAM model **Jaiswal et al., "SAM: Scene Analysis via Masking"** adopts an innovative approach by constructing a data engine where the model co-develops annotations with environmental datasets. SAM uniquely leverages a vast collection of masks, showcasing robust generalization capabilities. However, it was initially designed as a task-agnostic segmentation model, requiring prompts (i.e., inputs of prior points, bounding boxes, or masks), and therefore does not directly facilitate end-to-end automated segmentation perception.
This paper does not delve into the design and training of foundational image manipulation detection models; instead, we explore the potential of utilizing SAM's powerful universal segmentation capabilities for image manipulation detection and localization. Furthermore, the proposed method of learning prompts can be extended to other visual foundational models beyond SAM.

\noindent\textbf{Prompt Learning.} In the past, machine learning tasks were primarily focused on fully supervised learning, where task-specific models were trained only on labeled instances of the target task  **Zhang et al., "A Survey of Deep Learning for Computer Vision"**. However, over time, there has been a significant shift in learning paradigms, transitioning from fully supervised learning towards \textit{pretraining and fine-tuning }approaches for downstream tasks. This shift allows models to leverage general features acquired during pretraining **Sun et al., "Knowledge Distillation: A Survey"**.
More recently, with the advent of foundational models, a new paradigm has emerged known as \textit{pretraining and prompting} **Wang et al., "Pre-training and Prompting for Vision and Language Understanding"**. In this paradigm, researchers no longer train models specifically for downstream tasks but instead redesign inputs using prompts to reformulate the downstream tasks to align with the original pretraining task **Li et al., "Prompt Engineering for Vision and Language Tasks"**. Prompting helps to reduce semantic gaps, bridge the gap between pretraining and fine-tuning, and prevent overfitting of the heads. Since the advent of GPT-3 **Zhang et al., "GPT-3: A Survey of State-of-the-Art Large Language Models"**, prompting has evolved from traditional discrete **Cheng et al., "Discrete Prompting for Vision and Language Understanding"** and continuous prompt constructions **Jiang et al., "Continuous Prompting for Vision and Language Tasks"** to large-scale model-centric contextual learning **Wang et al., "Contextual Learning with Large-Scale Models"**, instruction tuning **Li et al., "Instruction Tuning for Vision and Language Understanding"**, and chaining approaches **Chen et al., "Chaining Approaches for Vision and Language Tasks"**.
Currently, methods for constructing prompts include manual templates, heuristic-based templates, generation, fine-tuning word embeddings, and pseudo-labeling **Zhang et al., "Prompt Learning via Word Embeddings"**. In this paper, we propose a prompt generator for generating prompts compatible with SAM.