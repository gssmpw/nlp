\section{Related Work}
\noindent\textbf{Image Manipulation Detection.} Currently, methods for image manipulation detection can be broadly categorized into two types, primarily distinguished by their recognition of manipulated artifacts. Some techniques \cite{wu2019mantra, chen2021image, wu2022robust, bi2019rru, hu2020span, yang2020constrained, marra2020full} rely on detecting abnormal features and often use high-pass noise filters \cite{yang2020constrained, li2019localization} to suppress content information. Other methods \cite{park2018double, kwon2022learning, mareen2022comprint} attempt to detect inconsistencies in compression within tampered images, as they assume different compression Quality Factors (QFs) before and after the operation. Additionally, some researchers focus their attention on camera-based artifacts, such as model fingerprints \cite{mareen2022comprint, cozzolino2019noiseprint, cozzolino2015splicebuster}.

\noindent\textbf{Foundational Models.} In recent years, foundational models have sparked a tremendous transformation in the field of artificial intelligence . These models, trained on extensive datasets, have demonstrated impressive generalization capabilities across various scenarios \cite{kirillov2023segment,cai2024biosam,liu2024pq,pan2024conv,pan2024codev,pan2025code}. Renowned models such as Chat-GPT \cite{ouyang2022training}, GPT-4 \cite{achiam2023gpt}, and Stable Diffusion \cite{rombach2022high} have further propelled the development of artificial intelligence, making significant contributions to human civilization and exerting considerable influence across various industries. 
Inspired by the success of foundational models in natural language processing (NLP), researchers have begun exploring their potential applications in computer vision. While most of these models are aimed at extracting accessible knowledge from freely available data \cite{alayrac2022flamingo, radford2021learning, chen2023ovarnet}, the recent SAM model \cite{kirillov2023segment} adopts an innovative approach by constructing a data engine where the model co-develops annotations with environmental datasets. SAM uniquely leverages a vast collection of masks, showcasing robust generalization capabilities. However, it was initially designed as a task-agnostic segmentation model, requiring prompts (i.e., inputs of prior points, bounding boxes, or masks), and therefore does not directly facilitate end-to-end automated segmentation perception.
This paper does not delve into the design and training of foundational image manipulation detection models; instead, we explore the potential of utilizing SAM's powerful universal segmentation capabilities for image manipulation detection and localization. Furthermore, the proposed method of learning prompts can be extended to other visual foundational models beyond SAM.

\noindent\textbf{Prompt Learning.} In the past, machine learning tasks were primarily focused on fully supervised learning, where task-specific models were trained only on labeled instances of the target task  \cite{krizhevsky2017imagenet}. However, over time, there has been a significant shift in learning paradigms, transitioning from fully supervised learning towards \textit{pretraining and fine-tuning }approaches for downstream tasks. This shift allows models to leverage general features acquired during pretraining \cite{russakovsky2015imagenet, simonyan2014very, he2016deep}.
More recently, with the advent of foundational models, a new paradigm has emerged known as \textit{pretraining and prompting} \cite{chen2023ovarnet, lester2021power, liu2023pre, zhou2022learning}. In this paradigm, researchers no longer train models specifically for downstream tasks but instead redesign inputs using prompts to reformulate the downstream tasks to align with the original pretraining task \cite{radford2021learning, devlin2018bert, radford2019language}. Prompting helps to reduce semantic gaps, bridge the gap between pretraining and fine-tuning, and prevent overfitting of the heads. Since the advent of GPT-3 \cite{brown2020language}, prompting has evolved from traditional discrete \cite{liu2023pre} and continuous prompt constructions \cite{chen2023ovarnet, zhou2022learning} to large-scale model-centric contextual learning \cite{alayrac2022flamingo}, instruction tuning \cite{liu2024visual, peng2023instruction, gupta2022instructdial}, and chaining approaches \cite{wei2022chain, wang2022self, zhang2022automatic}.
Currently, methods for constructing prompts include manual templates, heuristic-based templates, generation, fine-tuning word embeddings, and pseudo-labeling \cite{liu2023pre, wang2022learning}. In this paper, we propose a prompt generator for generating prompts compatible with SAM.
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.