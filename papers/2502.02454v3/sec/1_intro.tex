\section{Introduction}\label{sec:intro}
With the continuous emergence of powerful editing tools, manipulating images has become unprecedentedly simple. These new opportunities have sparked creativity among both benevolent and malevolent users. Previously, orchestrating multimedia disinformation campaigns required complex skills, and attackers could only splice, copy, or delete objects. With the explosive growth of deep learning, image editing tools have become more user-friendly and potent, allowing users to generate instant images of non-existent individuals or achieve convincing deepfakes. Generative models can produce realistic image edits using natural language prompts, seamlessly integrating inserted elements to match the style and lighting of the environment \cite{avrahami2023blended, nichol2021glide}.

The risks posed by these tools falling into the wrong hands are evident. In fact, in recent years, governments and funding agencies have shown increasing interest in developing forensic tools capable of addressing such attacks. The primary focus is on local image manipulation, particularly alterations that change the semantic content of images. To tackle these challenges, activities in the fields of multimedia forensics and related sciences have rapidly expanded, proposing numerous methods and tools for image manipulation detection and localization \cite{verdoliva2020media,cozzolino2022data,huh2018fighting,wu2019mantra}. Despite considerable progress in this field, the performance of current state-of-the-art detectors is insufficient for field deployment, primarily due to several shortcomings that require further research: i) limited generalizability; ii) limited robustness; and iii) inadequate detection performance.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{sec/fig/intro.pdf}
	\caption{Improvements of IMDPrompter over baseline methods: a.Automated prompt learning without the need for manual input; b.Integration of semantic-agnostic information crucial for Image Manipulation Detection.}
	\label{fig:intro}
\end{figure}

To address the shortcomings of existing image manipulation detection methods, we turn our attention to foundational models. Benefiting from large-scale pretraining, foundational models such as GPT-4 \cite{achiam2023gpt}, Flamingo \cite{alayrac2022flamingo}, and SAM \cite{kirillov2023segment} have made significant advancements and contributed greatly to societal progress. Among these, SAM, trained on 1 billion masks, and demonstrating outstanding generalization capabilities, has inspired our research. However, SAM's interactive framework requires pre-provided cues such as points, boxes, or masks alongside input images. Operating as a category-agnostic segmentation method, as shown in Figure 1, these limitations render SAM unsuitable for fully automated understanding in image manipulation detection. Furthermore, we note that unlike previous tasks like semantic or instance segmentation, image manipulation detection methods require the introduction of semantic-agnostic features; relying solely on RGB visual features is insufficient to address image manipulation detection problems  \cite{chen2021image}, posing significant challenges for SAM in image manipulation detection work.


To enhance the image manipulation detection capabilities of foundational models, we propose a new method called IMDPrompter for learning how to generate prompts that enhance the functionality of the SAM framework. Our research primarily focuses on the SAM framework, which is a category-agnostic interactive segmentation model. To introduce semantic-agnostic features, we incorporate three noise views, SRM Filter \cite{zhou2018learning}, Bayer Conv \cite{bayar2018constrained}  and Noiseprint \cite{cozzolino2019noiseprint}. For integrating RGB visual features with semantic-agnostic features, we build the Semantic-Agnostic Feature Fusion (SAF) module. For RGB views and three noise views, we sequentially acquire image manipulation localization masks. Additionally, we construct an ensemble mask outputting from four branches for selecting the optimal localization map to generate prompts, we construct the Optimal Prompt Selection (OPS) module based on minimizing localization loss. After obtaining the optimal prompt, to achieve cross-view prompt consistency enhancement, we introduce the Cross-View Prompt Consistency (CPC) loss constraint, aligning the localization maps of the four branches to the optimal localization map. Prompt Mixing Module is proposed to integrate multiple types of prompt information.

In summary, the main contributions are listed as follows.
\begin{itemize}
    \item We propose a learning paradigm called IMDPrompter for utilizing SAM in image manipulation detection, marking the first attempt to apply SAM to this field.
    \item We introduce an automated prompt learning method, which eliminates the need for manual guidance and simplifies the implementation of IMDPrompter.
    \item We develop components such as Semantic-Agnostic Feature Fusion, Optimal Prompt Selection, Cross-View Prompt Consistency, and Prompt Mixing Module to fully integrate various semantic-agnostic information.
    \item We demonstrate extensive results across five different image manipulation detection datasets, thoroughly validating the strong in-distribution and out-of-distribution image manipulation detection and localization capabilities of IMDPrompter.
\end{itemize}


% \begin{itemize}
%     \item 
%     We propose a method for learning prompts that incorporate semantic-agnostic information, enhancing the capability of the SAM model for image manipulation detection.
        
%     \item
%     We introduce methods such as Optimal Prompt Selection and Cross-View Prompt Consistency Enhancement to achieve optimized prompts and enhance cross-view consistency.
    
%     \item  
%     We propose a Prompt Mixing module to integrate prompts of multiple types.

%     \item  We demonstrate extensive results on five different image manipulation detection datasets, fully validating IMDPrompter's strong capabilities for in-distribution and out-of-distribution image manipulation detection and localization.
% \end{itemize}

