\begin{figure*}[ht]
	\centering
	\includegraphics[width=\linewidth]{sec/fig/Pipeline.pdf}
	\caption{Overall framework of IMDPrompter. The prompter part consists of four views: RGB, SRM, Bayar and Noiseprint. OPS selects the optimal prediction from the four views to generate the best prompt. CPC enhances cross-view consistency. SAF introduces semantic-agnostic features. PMM achieves a mixture of multiple prompt information.}
	\label{fig:pipe}
\end{figure*}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{sec/fig/sae.pdf}
	\caption{Architecture of the Semantic-Agnostic Feature Fusion (SAF) unit.}
	\label{fig:saf}
\end{figure}
\section{Method}
As shown in Figure \ref{fig:pipe}, We achieve integration of information from the RGB view and three noise views, facilitating the automation of SAM prompts. Specifically, through the SAF module, we integrate the information from the noise views into the RGB view; through the optimal prompt selection module, we select the best prompts from multiple sparse prompts; via CPC, we enhance consistency across multiple views; and through the PMM module, we organize a mix of various types of prompts, achieving efficient, automated image manipulation detection and localization based on SAM.


\subsection{Semantic-Agnostic Feature Fusion}
Current technology finds that exploring semantic information from images works well for in-domain (IND) manipulation detection but performs poorly in out-of-domain (OOD) detection. Additionally, utilizing image noise maps to learn semantic-agnostic information can yield strong performance. Given these findings, we hypothesize that relying solely on semantically related information is insufficient for detecting and localizing manipulations. To address this issue, we introduce information from three semantic-agnostic views, the SRM noise map, the Bayer noise map, and the Noiseprint  noise map, with the process as follows:
\begin{equation}
F_{\text{sam}} = \Phi_{\text{sam-img}}(\mathcal{I})
\end{equation}
\begin{equation}
f_{\text{RGB}} = \Phi_{\text{Seg-RGB}}(\mathcal{I})
\end{equation}
\begin{equation}
f_{\text{SRM}} = \Phi_{\text{Seg-SRM}}\left(\Phi_{\text{SRM}}(\mathcal{I})\right)
\end{equation}
\begin{equation}
f_{\text{Bayer}} = \Phi_{\text{Seg-Bayer}}\left(\Phi_{\text{Bayer}}(\mathcal{I})\right)
\end{equation}
\begin{equation}
f_{\text{Noiseprint}} = \Phi_{\text{Seg-Noiseprint}}\left(\Phi_{\text{Noiseprint}}(\mathcal{I})\right)
\end{equation}
\begin{equation}
F_{\text{SAF}} = \Phi_{\text{SAF}}\left(F_{\text{sam}}, f_{\text{RGB}}, f_{\text{SRM}}, f_{\text{Bayer}},f_{\text{Noiseprint}}\right)
\end{equation}
In the system, $\mathcal{I}$ represents the input image, and $\Phi_{\text{sam-img}}$ denotes the SAM image encoder. $F_{\text{sam}}$ represents the features encoded by the SAM image encoder. $\Phi_{\mathrm{SRM}}$, $\Phi_{\text{Bayer}}$ and $\Phi_{\text{Noiseprint}}$ respectively represent the SRM noise map extractor, the Bayer noise map extractor and the Noiseprint noise map extractor. $\Phi_{\text{Seg-RGB}}$, $\Phi_{\text{Seg-SRM}}$, $\Phi_{\text{Seg-Bayer}}$ and $\Phi_{\text{Seg-Noiseprint}}$ each represent the segmenter for the RGB view, SRM view, Bayer view and Noiseprint view, with all four segmenters having identical structures but non-shared parameters. $f_{\mathrm{RGB}}$, $f_{\mathrm{SRM}}$, $f_{\mathrm{Bayer}}$ and $f_{\mathrm{Noiseprint}}$ represent the features from the RGB, SRM, Bayer, and Noiseprint views, respectively. $\Phi_{\mathrm{SAF}}$ stands for the module that integrates semantic-agnostic features, and $F_{\mathrm{SAF}}$ represents the fused features after integration by SAF. The structure diagram of the SAF module is shown in Figure \ref{fig:saf}.

Existing technology has shown that exploring semantic information from images is effective for in-domain (IND) manipulation detection but performs poorly in out-of-domain (OOD) detection. Additionally, utilizing noise maps from images to learn semantic-agnostic information can yield significant performance gains. Given these findings, we hypothesize that relying solely on semantically related information is insufficient for detecting and locating manipulations. To address this issue, we have incorporated information from two semantic-agnostic views, the SRM noise map ,the Bayer noise map and Noiseprint noise map. The entire process is illustrated in Figure \ref{fig:saf}.

First, the features $f_{\text{RGB}}$, $f_{\text{SRM}}$, $f_{\text{Bayer}}$, and $f_{\text{Noiseprint}}$ are aligned with $F_{\text{sam}}$ through bicubic interpolation, followed by channel concatenation with $F_{\text{sam}}$ after passing through a $1 \times 1$ convolution. The concatenated features then pass through two stacked convolution blocks (comprising $3 \times 3$ convolution, instance normalization, and GELU activation), and the output from the semantic selection attention branch (consisting of global average pooling, a fully connected layer, GELU, and sigmoid) is weighted onto the shortcut branch to produce the integrated feature $F_{\text{SAF}}$. Through the SAF module, features from multiple views are effectively integrated.

\subsection{Optimal Prompt Selection}
In the process of generating masked prompts, we utilize the mask probability distributions from four views, $P_{\mathrm{RGB}}$, $P_{\mathrm{SRM}}$, $P_{\mathrm{Bayar}}$, and $P_{\mathrm{Noiseprint}}$, and construct an integrated mask probability distribution $P_{\text{Ens}}$. We then select the optimal mask probability segmentation $P_{\mathrm{opt}}$ based on the principle of minimizing segmentation loss. Further, we obtain the mask $M_{\mathrm{opt}}$ and bounding box prompts $\mathcal{B}_{\mathrm{opt}}$, which are input into SAM's prompt encoder to obtain the prompt encoding $F_{\mathrm{opt}}$. The process is as follows:
\begin{equation}
P_{\mathrm{RGB}} = \Phi_{\mathrm{RGB}-\mathrm{CLS}}(f_{\mathrm{RGB}}),
\end{equation}
\begin{equation}
P_{\mathrm{SRM}} = \Phi_{\mathrm{SRM-CLS}}(f_{\mathrm{SRM}}),
\end{equation}
\begin{equation}
P_{\mathrm{Bayer}} = \Phi_{\mathrm{Bayer-CLS}}(f_{\mathrm{Bayer}}),
\end{equation}
\begin{equation}
P_{\mathrm{Noiseprint}} = \Phi_{\mathrm{Noiseprint-CLS}}(f_{\mathrm{Noiseprint}}),
\end{equation}
\begin{equation}
P_{\text{Ens}} = \frac{P_{\mathrm{RGB}} + P_{\mathrm{SRM}} + P_{\mathrm{Bayer}}+P_{\mathrm{Noiseprint}}}{4},
\end{equation}
\begin{equation}
\mathcal{P}=\{P_{\mathrm{RGB}}, P_{\mathrm{SRM}}, P_{\mathrm{Bayer}},P_{\mathrm{Noiseprint}}, P_{\text{Ens}}\},
\end{equation}
\begin{equation}
P_{\text{opt}} = \underset{P \in \mathcal{P}}{\operatorname{argmin}} L_{\mathrm{Seg}}(P, G),
\end{equation}
\begin{equation}
M_{\mathrm{opt}} = \Phi_{\mathrm{mask}}(P_{\mathrm{opt}}),
\end{equation}
\begin{equation}
\mathcal{B}_{\mathrm{opt}} = \Phi_{\mathrm{box}}(M_{\mathrm{opt}}),
\end{equation}
\begin{equation}
F_{\mathrm{opt}} = \Phi_{\mathrm{p-enc}}(M_{\mathrm{opt}}, \mathcal{B}_{\mathrm{opt}}),
\end{equation}
where $\Phi_{\mathrm{RGB-CLS}}$, $\Phi_{\mathrm{SRM-CLS}}$, $\Phi_{\mathrm{Bayer-CLS}}$ and $\Phi_{\mathrm{Noiseprint-CLS}}$ represent classifiers for the RGB, SRM, Bayer, and Noiseprint views, respectively, $G$ represents the one-hot encoded mask labels, $L_{\mathrm{Seg}}$ is the segmentation loss function, $\Phi_{\mathrm{mask}}$ represents the mask generation operation, $\Phi_{\mathrm{box}}$ is the bounding box generation operation, and $\Phi_{\mathrm{p-enc}}$ represents SAM's prompt encoder.

\subsection{Cross-View Prompt Consistency}
Ideally, the segmentation masks from the four views should be consistent with the optimal segmentation mask. Therefore, we constructed a cross-view prompt consistency enhancement loss to achieve enhanced prompt consistency across views. The $\mathrm{CPC}$ loss function is expressed as follows:
\begin{equation}
\mathcal{L}_{\mathrm{CPC}} = \mathcal{L}_{\mathrm{Seg}}(P_{\mathrm{RGB}}, P_{\text{opt}}) + \mathcal{L}_{\mathrm{Seg}}(P_{\mathrm{SRM}}, P_{\text{opt}}) + \mathcal{L}_{\mathrm{Seg}}(P_{\mathrm{Bayer}}, P_{\text{opt}}) +
\mathcal{L}_{\mathrm{Seg}}(P_{\mathrm{Noiseprint}}, P_{\text{opt}})
\end{equation}

Where $\mathcal{L}_{\mathrm{Seg}}$ is the segmentation loss function.

\subsection{Prompt Mixing Module}
Since $F_{\mathrm{SAF}}$ aggregates semantically related and unrelated features, it aids the guidance of SAM for image manipulation detection. Therefore, we constructed the PMM module, which is primarily based on MLP, to integrate multiple types of prompt information. First, the two types of prompt inputs, $F_{\mathrm{SAF}}$ and $F_{\mathrm{opt}}$, are concatenated, followed by an MLP layer to perform dimension transformation to align with the original SAM prompt encoder's prompt embedding. Then, $F_{\mathrm{mix}}$ and $F_{\mathrm{SAF}}$ are input into SAM's mask decoder to achieve the image manipulation localization process. The process is as follows:
\begin{equation}
F_{\mathrm{mix}} = \Phi_{\mathrm{MLP}}\left([F_{\mathrm{SAF}} ; F_{\mathrm{opt}}]\right),
\end{equation}
\begin{equation}
P_{\mathrm{sam}} = \Phi_{\mathrm{sam-dec}}(F_{\mathrm{SAF}}, F_{\mathrm{mix}}),
\end{equation}
where $\Phi_{\mathrm{MLP}}$ represents the MLP layer operation, $F_{\mathrm{mix}}$ represents the integrated prompt embedding, and $\Phi_{\mathrm{sam-dec}}$ represents SAM's mask decoder, with $P_{\mathrm{sam}}$ representing the mask probability prediction output of the mask decoder.

\subsection{Training and Inference Pipeline}
The training loss function of IMDPrompter includes the segmentation loss from four prompt views $\mathcal{L}_{\text{Seg-p}}$, the CPC loss $\mathcal{L}_{\mathrm{CPC}}$, the SAM decoder segmentation loss $\mathcal{L}_{\text{Seg-sam}}$, and the image-level prediction loss $\mathcal{L}_{\text{Img-level}}$. The formula is expressed as follows:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{Seg-sam}} + \lambda_1 \mathcal{L}_{\text{Seg-p}} + \lambda_2 \mathcal{L}_{\mathrm{CPC}} + \lambda_3 \mathcal{L}_{\text{Img-level}},
\end{equation}
where
\begin{equation}
L_{\text{Seg-sam}} = L_{\text{Seg}}(P_{\text{sam}}, G)
\end{equation}
\begin{equation}
L_{\text{Seg-p}} = L_{\text{Seg}}(P_{\text{RGB}}, G) + L_{\text{Seg}}(P_{\text{Bayar}}, G) + L_{\text{Seg}}(P_{\text{SRM}}, G)+ L_{\text{Seg}}(P_{\text{Noiseprint}}, G)
\end{equation}

In this paper, we use Focal Loss as our $L_{\mathrm{Seg}}$.

For image-level detection, following the work of \cite{zhai2023towards}, we adopt an adaptive pooling based on minimizing intra-class prediction variance. The overall computational process is as follows, first using Otsu's method \cite{otsu1975threshold} to find a threshold $\omega_0$ that minimizes the intra-class prediction variance:
\begin{equation}
\begin{array}{r}
\omega_0=\underset{\omega \in\left\{\hat{p}_{i, j}\right\}}{\arg \min }\left|\left\{\hat{p}_{i, j} \mid \hat{p}_{i, j}<\omega\right\}\right| \operatorname{var}\left(\left\{\hat{p}_{i, j} \mid \hat{p}_{i, j}<\omega\right\}\right)+ \\
\left|\left\{\hat{p}_{i, j} \mid \hat{p}_{i, j} \geq \omega\right\}\right| \operatorname{var}\left(\left\{\hat{p}_{i, j} \mid \hat{p}_{i, j} \geq \omega\right\}\right),
\end{array}
\end{equation}
where $\operatorname{var}(\cdot)$ denotes variance, $\hat{p}_{i, j}$ is the pixel-level response at position $(i, j)$.
Then the image-level prediction is aggregated from pixel-level responses above the threshold and the image-level loss is:
\begin{equation}
\hat{y}_{\mathrm{A}} = \frac{1}{\left| \mathbb{P}_{\mathrm{h}} \right|} \sum_{\hat{p} \in \mathbb{P}_{\mathrm{h}}} \hat{p}; \mathbb{P}_{\mathrm{h}} = \left\{\hat{p}_{i, j} \mid \hat{p}_{i, j} \geq \omega_0\right\},
\end{equation}
\begin{equation}
\mathcal{L}_{\text{Img-level}} = \mathcal{L}_{\mathrm{BCE}}(y, \hat{y}_{\mathrm{A}}),
\end{equation}
where $\hat{y}_{\mathrm{A}}$ is the image-level prediction, $\mathcal{L}_{\mathrm{BCE}}$ denotes the binary cross-entropy loss.

During the inference process of IMDPrompter, since there are no true labels, our OPS module defaults to selecting $P_{\text{Ens}}$ for generating  masks and bounding box prompts, with other components functioning as during the training process.


