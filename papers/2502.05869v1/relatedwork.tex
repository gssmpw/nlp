\section{Related Work}
\subsection{Skeleton-based Action Recognition}
Skeleton-based human action recognition (HAR) has been extensively studied, evolving through multiple deep learning paradigms \citep{du2015skeleton,wang2018action,li2017joint,li2017adaptive,li2017skeleton}. Early methods for skeleton-based human action recognition relied primarily on convolutional neural networks (CNNs) to capture basic spatial interactions among skeleton points  \citep{du2015skeleton}. With the advent of Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM),  \citep{li2017adaptive,li2017skeleton} leveraged them to model temporal interactions. To better account for the topological structures of skeleton data, Graph Convolutional Networks (GCNs) have been extensively applied in this domain, achieving significant performance improvements  \citep{yan2018spatial,cheng2020skeleton,chen2021channel,zhou2024blockgcn}.  \citep{yan2018spatial} introduced ST-GCN, a spatiotemporal graph model that connects skeleton joints based on the natural body structure and temporal continuity.   \citep{cheng2020skeleton} proposed a network that contains spatial and temporal shift graph convolution.  \citep{chen2021channel} proposed a channel-wise topology graph convolution network (CTR-GCN) to dynamically capture spatial features at different levels of granularity.  \citep{zhou2024blockgcn} developed BlockGCN, a network designed to enhance the learning and retention of critical skeleton attributes. Collectively, these contributions represent significant advancements in utilizing the inherent graph structure of skeleton data for action recognition tasks. 

In contrast to the aforementioned approaches, our work introduces a novel hyperbolic-space-based linear attention mechanism. Benefiting from the linear attention design, our method achieves superior modeling of temporal dependencies compared to graph convolutional networks (GCNs), while leveraging the hyperbolic space to capture the hierarchical structure of skeletal data more effectively. Additionally, our approach is significantly more lightweight than transformer-based methods, addressing the challenges of high memory consumption without compromising much performance. To the best of our knowledge, this is the first application of linear attention mechanisms within the Poincar√© model in hyperbolic space, addressing the dual challenges of high memory usage and hierarchical information modeling limitations in existing methods. 

\subsection{Hyperbolic Transformer}
In recent years, hyperbolic geometry has demonstrated significant potential for modeling complex structured data, particularly those with tree-like or hierarchical structures \citep{yang2024hypformer}. Numerous studies have begun exploring the application of transformers in hyperbolic space. For instance,  \citep{huang2023hyperbolic} employed a hyperbolic transformer for music generation, while  \citep{chen2024hyperbolic} utilized it for pre-trained language models. Additionally, hyperbolic geometry has been applied to model hierarchical skeleton data in skeleton-based human action recognition.  \citep{ermolov2022hyperbolic} introduced a hyperbolic vision transformer model featuring a novel metric learning loss that combines the representational power of hyperbolic space with the simplicity of cross-entropy loss.  \citep{chen2022hmanet} leveraged hyperbolic space mapping to enhance spatiotemporal feature representation, and  \citep{qu2024llms} integrated large language models with hyperbolic space to improve feature representation. In contrast to these approaches, our work is the first to explore linear attention mechanisms in hyperbolic spaces for skeleton-based human action recognition.