\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{db.pdf}
    \caption{Database Design}
    \label{fig:design}
\end{figure}

\section{Design of Post-Relational Data Management Systems}
Figure~\ref{fig:design} shows an overview of how we envision a post-relational data management system to look like. 

The system consists of three main workflows (1) database design, (2)  data ingestion and (3) query answering. The design workflow is in charge of deciding the logical and physical design of the database based on user-provided data and query definitions, as well as (potential) database redesign based on newly observed queries and data records. The data ingestion workflow is in charge of parsing data records to map them to the logical and physical design. The query answering workflow is in charge of answering user queries. We first discuss the user interface of the system, and then discuss in more details the internals of the database for each of the three workflows mentioned. We end the section by discussing some algorithmic components common to the different workflows within the system.

%The first two are instigated by user interaction (e.g., database setup, new record insertion, asking new query), the third can happen in the background but can take user interactions into account (e.g., database agent can decide to reorganize database after observing a number of queries to be better optimized for a workload). 


\subsection{Interface}
The user interacts with the system to either define what data and queries are to be handled database, or to query or modify the data in the database. We call the former the Data and Query Definition Interface, and the latter two comprise the Data Manipulation Interface. These interfaces support natural language inputs from the user and may provide natural language outputs. Thus, an important aspect of the two interfaces is having the human in the loop to ensure correctness of the results. We first describe the two interfaces, and then provide more details on the human in the loop nature of the interfaces. 

%Data ingestion part of the system is a collection of components in the system that take (1) data definition and (2) data records as input and create (1) metadata describing the data and how its stored and (2) physical blocks where data is stored.

%In relational dbms this layer is straight forward. Data definition is done using DDL in SQL and the DML layer provides instruction to support modification and query of new data records as defined by DDL instructions. Simple implementation, just requires parsing, and data layout design decides how to store data. Different storage choices are made to improve efficiency, but no discussion of accuracy since everything is mathematically well-defined. Extensions beyond relatinoal models increases importance of this layer, no well-defined mapping of how things should logically be stored



\subsubsection{Data and Query Definition Interface}
Data and Query Definition Interface allows users to define how to insert or query their data. The interface itself can be in different levels of structure, from supporting only structured operations such as DDL in SQL in one end to just providing a natural language interface where users can provide a description of the data to be inserted and queries to be performed on the system. 

The interface has two important features. First, because the interface support natural language the system may need to ask follow up questions to resolve ambiguities, understand intent or allow user to validate the outputs. As such, the interface can be interactive, as further discussed in Sec.~\ref{sec:hil}.  Second, the interface allows decoupled Data and Query Definitions. That is, logical data representation does not need to correspond to the logical query representation. This allows the user to define SQL schema on unstructured data, or natural language schema on structured data. Allowing different query and data definitions is important and gives the database system the agency to decide how the data should be mapped to the query. That is, by having a decoupled query and data interface, the system allows all data transformation, extraction, loading, and integration choices to be differed from the data management system. We provide a two examples below. 


\textbf{SQL Queries on Unstructured Data}. The system may define a schema so that queries are performed in SQL, while the input to the system may be unstructured data, as done in ZenDB \cite{lin2024towards} and ELEET \cite{urban2024efficient}. This defers to the system to decide how to map the unstructured data to the query schema. Although performing ETL upfront is possible, both approaches advocate against such a methodology and perform retrieval and extraction on the fly. This shows how decoupling data and query definition allows the system to perform the mapping between the two themselves, providing a richer optimization space that can be exploited by the database system.  

\textbf{Definitions for Unstructured Query and Data}. When either query or data are unstructured, providing additional information about the data or query are important. For example, when doing natural language query on rational data additional meta-data containing description of tables, columns and extra information that can be used to query is important to allow for query answering (for example, BIRD \cite{li2024can} benchmark for NL2SQL provides meta-data about columns and tables). Even when data and queries are unstructured users can provide instructions on how they should be processed. In our police records example, users can provide hints for the system to cluster different police cases. Such definitions allow users to provide additional domain knowledge and information about future queries that help the system decide how to organize the data. 


Overall the data and query definition interface allows users to navigate between a fully relational database and a RAG system. As different application utilize different levels of structure in either query or data, we envision a database system that supports all such specifications. 

%The interface allows users to defined what data will be inserted into the system and what queries will be performed. Users can define query interface and data insertion interface that are different (i.e., do not adhere to the same schema). Overall, the users can define \textit{Structured Query and Data}: This is how data is traditionally defined in relational dbms. Users to define tables, data types, constraints, etc. \textit{Structured Query on Unstructured Data}: Users can define a structured query interface while allowing unstructured data to be inserted. ZenDB, ELEET. \textit{Unstructured Query on structured datar}: Users can define schema to insert data, but allow users to perform unstructured query NL2SQL, TableQA, dataset search. \textit{Unstructured Query on Unstructured Data}: Retrieal/RAG applications. \textit{Semi structured data/queries}: There is usually some amount of structure. Even in the classical unstructured text retrieval, a corpus is divided into a set of documents, rarely it's just a blob of text with nothing known about it. One can do things like DocETL which uses some structure (e.g., applying the same operation across documents), or define additional operations (e.g., some predefined LLM call user can do). 



%The DDI interface itself can be structured or unstructured, just provide data description. Can defin base

%DDI allows users to define how they interact with DBMS and how data can be inserted and accessed. The users can define interfaces that are



\subsubsection{Data Manipulation Interface}
After data and query definition, the user can interact with the system to insert/update/delete data (data ingestion) and to perform query queries. The interfaces follow their respective definitions where data ingestion and/or query interface can be require structured inputes or not. In addition to (or instead of) performing the operations requested by the user and returning the result, the interface may need to ask follow-up questions to do intent discovery or disambiguation. The interface may also need to provide tools for the user to validate the operations performed by the database. The system may raise errors if it cannot semantically parse and perform the operation. We further discuss the interactive interface in Sec.~\ref{sec:hil}

%. (1) when users perform queries or insertions, the interface provides tools for users to verify the operations, for example by showing provenance. And (2) the systems may engage in interactive querying if the system decide the operation cannot be performed just based on user input. This may require additional follow ups to the user to do intent discovery, to disambiguate the query or the data. 


\subsubsection{Human in the Loop}\label{sec:hil}
Because the interface allows natural language input, understanding what the user and/or data mean may require additional interactions with the user. This may require intent discovery to understand user queries and disambiguation of data. Moreover, the user needs to be able to verify the operations performed by the user. Thus, the system needs to provide tools for the user to verify operations. This can be in various forms, for example, asking users for labels, and measuring accuracy to ensure it satisfy some user defined requirement. It can also be in terms of showing provenance to the user, or other explainability artifacts to show what the system has done. An example is showing the SQL query to user if the system perform NL2SQL. 


\subsection{Database Design workflow}
The database design workflow takes instructions provided by the user in the data and query definition interface and designs the logical and physical representation of the data. It physically creates and stores the relevant meta-data that will later be used to decide how and where to insert new data records, as well as how to map queries to the observed data points.

In relational dbms this workflow is straightforward. Data definition is done using DDL in SQL, and the system only needs to parse DDL instructions and create the relevant meta-data (i.e., schema) to be stored physically. Design choices on how the data is stored are for efficiency purposes only (e.g., columnar, materialize views, index, etc.). %, where the main question if physical data representation, given that logical representation is given/known. 
%and the DML layer provides instruction to support modification and query of new data records as defined by DDL instructions. Simple implementation, just requires parsing, and data layout design decides how to store data. Different storage choices are made to improve efficiency, but no discussion of accuracy since everything is mathematically well-defined. 
On the other hand, extensions beyond the relational model increase the importance of this workflow since the user may not have provided a mathematical definition of the logical data structure. As such, the system needs to decide how to logically and physically store the data. Internally, this workflow consists of three components: (1) parsing user input, (2) logical representation design and (3) physical representation design. 

\subsubsection{Parsing}
This component parses user instructions, decides if additional user feedback is needed (e.g., in cases of ambiguity), and raises errors if it cannot parse user instructions (errors can be both from inability to parse semantically or syntactically.). 

%Syntax check, check for potential ambiguity, ask follow up question (similar to other parsing components mentioned in more details in the other two workflows)

\subsubsection{Logical Representation Design}
The system then decides what the logical representation of the data should be. In cases when the user defines the logical representation, the system just relies on user provided definitions. However, the system can be responsible for designing a logical representation of the data,

\textbf{User-defined Representations}
If the user specifies the logical representation of the data, then the system relies on the provided definition. This can be data schema in a relational setting, but there are many other possibilities. For example, the user can simply specify that data is a collection of documents, each broken into smaller text chunks of a certain size, to store the data in a vector database. Moreover, unstructured data can be defined as an additional column in an already existing schema to combine structured and unstructured data. 

\textbf{Automated Representation Design}
Alternatively, when given unstructured data, the system can decide how to logically represent the data. The system can decide to create a schema based on the data and create an information extraction pipeline to transform unstructured data to a structured format, or can decide to keep data as a collection of unstructured text chunks and store them in a vector database. Such a choice can be made based on the amount of structure available in the data and the required task.

For example, in cases of semi-structured data, EVAPORATE \cite{arora2023language} extracts information and inserts them into a structured table. On the other hand, Semantic Hierarchical Trees in \cite{lin2024towards} create a logical view of the information in a document by identifying different sections in the document and building a tree hierarchy that represent information in the text. Moreover, the data representation can be self-improving, and specialized for tasks. For instance, NUDGE \cite{zeighami2024nudge} optimizes data representations for the specific query workload in hand. 

Overall, users during data and query definition, users can provide a task (or set of tasks they want to do) and description of the data and the system needs to break down the task and decide how data should be stored based on the task. This can require deciding what information is relevant, and how it should be extracted and stored it. Besides the system may also need to decide how different records are related (e.g., orginze different files in police records), and decide to build different collections of data records used for downstream tasks.


\subsubsection{Physical Storage Design}
The system also needs to decide what needs to be physically stored and how to stored. If the system is adhering to user-defined representation, then physical storage may be well-understood, e.g., in case of relational or vector databases. However, when system automatically design the logical data representation, it further needs to decide what needs to be stored and how. Moreover,  need to decide what level of user feedback and workload to store to allow the database to improve over time based on queries and user interactions. Overall, physically, the system will store meta-data, data and any query workload and user feedback information needed. 
%Physical data storage based on logical data representation. Need to decide how to physically store the logical representation as meta-data and the corresponding data. Also 

\textbf{Meta-Data}. Meta-data includes information about the logical data representation, and allows the system to use the logical data representation to physically find relevant data. In relational dbms, this includes schema information and where the corresponding data is stored. In post-relational dbms, the meta-data can include additional information such as extracted structure, summaries of records, user-provided description, virtual views and mappings between logical data representation and possible locations to find the data (For example, as done in ZenDB\cite{lin2024towards}). 


\textbf{Data}. Based on logical data representation, the system may decide to store multiple physical data representations. Each data representation may be useful for different workloads, or can provide varying levels of accuracy when answering queries. For example, system may extract some of the fields, create data embeddings, summaries, or may simply store text as is.

\textbf{Query workload and user feedback}. System also decide how to store the query workload and user provided feedback. A simple approach is to just store all queries and any feedback the user provides. This may get too costly, and the system can decide what needs to be kept based on the data definition and existing levels of ambiguity (e.g., store more feedback on ambiguous records, where more impact is expected).

\subsection{Data Ingestion workflow}
Data ingestion takes data records as input and stores it in physical blocks in the database and performs any meta-data changes needed. The parsing component may decide to ask follow-up questions from the user, or raise errors if it cannot parse the data. The storage components uses the meta-data to figure out where and how the data needs to be stored, and performs necessary operations to translate the input into the required format. The operations here can include a combination of the traditional Extract, Transform and Load operations when data is unstructured.



\subsubsection{Parsing}
This component takes data input from user, and attempts to map it to the logical representation designed for the data. In addition to syntactic errors, there can be semantic errors, where data may not adhere to existing or predefined semantics. Semantic assertions such as in SPADE\cite{shankar2024spade} can be used to ensure data quality. 

The system performs both syntactic parsing (e.g., for cases where there is any expected pre-defined structure) or semantic parsing to understand the logical representation of the data and how it fits in within the existing database. In our police records examples, this means finding the correct file where a new record belongs, or potentially raising an error if it cannot be assigned to any existing file. Other parsing strategies are possible, for instance ZenDB\cite{lin2024towards} extracts structure from a templatized document and checks if the template matches templates in other existing documents.  
%\textbf{Syntactic Parsing}. If there is any amount of structure there can be syntax checks to match the structure

%\textbf{Semantic Parsing}. Parse the data to make sure it matches data definition. This can be semantic operations, e.g., if there are collection of different police records, this step can semantically understand where the record belongs, or raise an error if the record does not belong to any of the predefined categories. It can also figure out if there is an error, e.g., if a police record doesn't have a date.

%If the system needs to do some sort of semantic mapping, then the system can ask the user for verification, or provide some sort of provenance for why the type of mapping happened for users to check. The system should decide when/how often the validation should happen, what records to validate to minimize human involvement while maximizing accuracy. 

%Can do zendb's template matching to see if the data matches existing templates

%The system can also check for ambiguity and raise errors if it the level of ambiguity passes some amount of certainty

\subsubsection{Storage}
If the data record is semantically valid and understandable by the system, then the system needs to store it in the database based on the designed physical storage scheme. This may require extracting certain information and/or performing data transformations. This stage performs such transformations and writes the data to the physical storage, with any additional meta-data and user feedback generated. This process is reminiscent of existing ELT workflows and can be done using methods such as DocETL~\cite{shankar2024docetl}. However, in contrast with ETL pipelines, this process need not be a one-off ETL of all existing data. Instead, the system can decide if any ETL is needed what operations to perform and can perform various Extract, Transform or Load operations throughout the lifecycle of the database. 

\subsection{Query Processing}
At query time, the database uses the stored meta-data and data to answer queries. The system may ask follow-up questions and interact with the user in multiple rounds after searching the database. Thus, in general, query answering can be a multi-step process. Intrnally, to process the query, the system performs (1) parsing to understand the query, (2) planning to decide how to answer the query and (3) execution which performs certain planned steps on the data. The three steps are not necessarily done once and sequentially and may be interleaved with each other. 

\subsubsection{Parsing}
Similar to data ingestion, parsing is an important component that goes beyond syntactic parsing. This step requires mapping the query to a logical representation that can be used to find the relevant data. This step can require semantic operations to map the query to the logical data representation. Similar to all other user interactions in the system, the system may ask follow-up questions to resolve ambiguities. Moreover, the system may need to use the data and perform intent discovery to understand what information the user is looking for. 


\subsubsection{Planning and Execution}
Given a (possibly partial) logical representation of the query, the system needs to decide what operations to perform. The system may not be generating a complete a complete query plan, since the query may require asking follow-up from the user. This step outputs specific LLM call or mathematical operations that can be performed on the database and/or user query. The execution engine then executes the plan. 

To maximize accuracy of the system and minize the cost, the system can try to optimize the plan and its execution.  This can be done using rewrite methods, such as in DocETL~\cite{shankar2024docetl}, cascade, caching, compression, filtering, etc. We give an overview of various algorithmic components that can be used for optimization throughout the database in Sec.~\ref{sec:alg_opt}. 




%\subsubsection{Verification and Provenance}
%User should be able to verify operations. e.g., show SQL to user if we end up doing NL2SQL or other forms of provenance. Can be other forms of provenance, e.g., show text where data was extracted in ZenDB. 

\subsection{Algorithmic Componenets}\label{sec:alg_opt}
Having a good system requires in general to have methods to maximize accuracy while minimizing both amount of compute and human involvement. Below is general discussion of various tools in our disposal 

\textbf{Utilizing Hidden Structure}. Important tool when operating on unstructured data is that there is often hidden structure. Extracting and using the structure often helps improve accuracy and/or cost. It helps decomposing and rewriting tasks into simpler logical unit and decomposing data into logical units, which both help with reducing cost (i.e., to avoid looking at entire dataset) and improving accuracy, since model is better at doing simpler tasks. DocETL, ZenDB

\textbf{Reducing amount of data processed}. We can often filter and/or compress data to reduce the amount of information llms needs to look at. This can be done by extracting relevant fields, filtering using embeddings, creating summaries or doing other prompt compression techniques (e.g., stuff by Trummer). 

\textbf{Re-using processed data}. We can use caching (prompt caching, prefix matching stuff) or creating materialized views to avoid reprocessing the data


\textbf{Model-based improvements}. We can fine-tune models to get better for the data. We can use cheaper models 

\textbf{Human Cost Optimizations} Deciding what should be labeled to minimize interactions with user, probably cite some crowdsourcing work

\textbf{LLM Agents}. Various parts of the system requires performing open-ended tasks, give that inputs are not necessarily structured. e.g., how to breakdown query, how to structure the data, figuring out if can the query be answered given the data, or if the logical structure of data is good based on user feedback.  Performing such actions can be done by LLM agents, that are good at making such open-ended operations, perhaps by additional guidance (e.g., DocETL). Agents don't necessarily have to be LLM agents, nor are a new thing (RL agents were used for query optimization XXX or database tuning XXXX), but LLM agents are good/new because they can perform open-ended tasks and operate in an unstructured space.


