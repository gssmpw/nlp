%!TEX root=../main.tex


\section{Proactive Operation Understanding}\label{sec:op}
In reactive data systems, the onus
is on the user to author queries involving
the ``right'' LLM-powered operators,
with the system then determining how to execute
these in conjunction with other relational operators.
However, even in cases where users
are able to specify clear, unambiguous operations,
the granularity at which they specify them may
not be optimal for execution.
Fundamentally, this stems from a lack
of understanding of what LLMs can do well
versus what they can't---something most users
are not aware of.
In this section, we discuss various 
approaches to 
operation reformulation 
that can improve accuracy 
while maintaining or reducing computational costs.
We first describe new operators that
we may introduce, and then methods for 
assessing and improving cost and accuracy when leveraging
new or existing operators.

\subsection{How and Where to Introduce New Operators}
We now describe ways to reformulate existing LLM-powered
operations into different ones. 

\topic{Decomposition into simpler LLM operations}
In Example~\ref{ex:police}, we described
how sometimes journalists want to extract
dozens of fields from a given document
(e.g., police officer names, descriptions
of misconduct, locations, use of firearms, among others).
The journalist may specify the entire list of fields
along with instructions within a prompt.
However, executing this as is in a single LLM call
may lead to poor accuracies
as LLMs often struggle to identify multiple concepts simultaneously~\cite{shankar2024docetl}.
A proactive system can decompose such an operation
into separate focused ones that
each extract one type of information, improving
accuracy.
An LLM can be asked rewrite a prompt that
says ``extract fields f$_1$, ..., f$_n$ from the following ...''
into ``extract field f$_i$ from the following ...''.
In prior work, we have identified
several new decomposition-based rewrite rules~\cite{shankar2024docetl}
that can lead to higher accuracies,
when coupled with LLMs being used to instantiate
the rewrites themselves.
Recent work on Text-2-SQL also leverages 
similar ideas~\cite{pourreza2024chase}: 
rather than attempting translation 
in ``one-shot'' (i.e., single LLM call), 
which often fails due to schema mismatches 
or poorly-named schemas~\cite{luoma2025snails}, 
it is beneficial to 
parse the operation into smaller units, e.g.,
given ``find all employees who joined before 2020'', 
first identify the core concepts: employee records 
and hire dates,
and treat each separately.

\topic{Leveraging non-LLM components}
In certain cases, operations
that are assigned to an LLM may be better done
through other means, e.g., through SQL.
For example, finding the average settlement amount 
for misconduct cases can be decomposed 
into an LLM operation to 
identify relevant cases and their settlement amounts, 
followed by a SQL aggregation to compute the average.
We have employed similar techniques where
we separate operations that require real-world reasoning (suited for LLMs) from mathematical computations (better handled by traditional database engines or calculators) in
our work on ZenDB~\cite{lin2024towards}.
Determining how to do this automatically
is challenging. 

\topic{Leveraging reasoning or data feedback for reformulation}
As described avove, a proactive data system
must reformulate (e.g., decompose or rework) operations to 
execute them better. 
However, finding good reformulations is difficult. 
Current approaches to discovering good reformulations 
or decompositions
are quite naive. Systems like DocETL~\cite{shankar2024docetl} 
simply prompt LLMs to suggest rewrites, 
either taking the first 
suggestion or selecting from multiple candidates. One option is 
to use a powerful ``reasoning'' model like OpenAI's o1 model to 
rewrite the task, but this still fundamentally relies on one-shot 
prompting, and is unaware of how the rewrite will actually perform on the data. We need approaches that can learn what 
characteristics of the data make tasks challenging, what types of 
LLM errors occur in different contexts, and use this knowledge to 
guide reformulation---perhaps even in an agentic fashion.

\topic{Calibrating LLM outputs}
Independent of which decomposition or reformulation is used,
when LLMs are independently being applied
to a set of items (documents, tuples),
the outputs can often be
inconsistent and non-calibrated.
For example, if we ask LLMs to rate
the severity of every incident
in our document collection,
it often gives all of them the same score,
or worse, gives them scores that
are only losely correlated with the severity.
To remedy this lack of consistency,
we can take various actions.
We can leverage the LLMs themselves to pick 
representative examples that indicate
the full range of the categories of interest, provided
as few-shot examples.
Or they can rework the prompt to describe in more detail
the criteria used for evaluation---to ensure consistency.
Finally, they can also
restrict the space of possible outputs
(e.g., when LLMs are asked to extract state information
from a collection of US addresses, they may extract CA in some
cases and California in others).
While this doesn't change the semantic meaning of the operation, it can significantly improve LLM accuracy by providing better context and guidance, as has
also been explored in work on prompt optimization~\cite{khattab2023dspy}.

\subsection{Assessing and Improving Performance with Reformulation}
Next, we describe ways to 
assess the benefits of reformulations,
and reduce cost while preserving accuracy. 

\topic{Leveraging LLMs to assess benefits}
One question that naturally emerges when considering
decomposing operations into smaller units
is how to assess the benefits of such decompositions.
While it is a-priori hard to tell whether
a decomposition will help, we can run
both the non-decomposed and decomposed
variants on a sample. 
LLMs are much better at evaluating outputs
than generating them, so LLMs can be used
to tell which version performs better.
For example, 
one can employ a ``generate-fix \& rewrite-verify'' pattern: 
generate an initial operation formulation, 
verify its correctness (e.g., through automated checks or LLM 
verification), and if verification fails, attempt alternative 
formulations~\cite{chung2025long}. 
This pattern, 
which we also use in DocETL~\cite{shankar2024docetl}, 
allows a system to systematically explore the space of possible 
formulations until finding one 
that passes verification, effectively optimizing for accuracy 
through trial and refinement.
However, doing evaluation in a cost-effective manner remains
a challenge. 





\topic{Deferring to cheaper LLMs for the ``easy bits''}
One concern with decomposition is that
it may increase the cost of the overall pipeline,
since one LLM call per document may now become
multiple. 
One way to defray the cost is to couple decomposition
with cost optimization:
for the simpler newly decomposed operations,
we can alternatively use cheaper and smaller
models to handle them, and only use the more expensive model 
for the most complicated operations.
For example, when we're trying to extract many fields
from a police record document, we can use a cheaper
model for extraction of locations and dates, while using
a more expensive model for harder tasks such as
determining the type of misconduct incident.
While the idea of cheaper proxy models isn't new~\cite{kang2017noscope, patel2024lotus},
here, since the space of decompositions is infinite,
and for each decomposition (or sequences thereof), 
we could use different
models and different confidence thresholds, each with 
different cost-accuracy tradeoffs, the problem
becomes a lot more compliacated.
Additionally, unlike previous settings which focused primarily on 
tasks with well-defined accuracy metrics, we now must provide 
guarantees for open-ended generative operations---where quality 
is harder to quantify.

 
\topic{Expensive predicate ordering, but with synthesized predicates}
For operations that involve subselecting documents
based on certain criteria (all expressed
together in one prompt), we can leverage existing
related work on expensive predicate ordering~\cite{hellerstein1993predicate, raman1999online};
however, in our context, we can introduce an arbitrary
number of new dependent predicates (that are potentially easier
to check and therefore cheaper).
For example, instead of using an expensive model
to examine each police record document to extract
medical impacts to the victims, if any,
we can consider cheaper filters
that are easier to check, for example, 
if the document contains any medical information at all.
This check could potentially be done by a cheaper
model and rule out a substantial fraction of the documents.
Similarly, when decomposing a complex filter like ``find 
incidents involving both use of force and drug use'' into two 
filters, one for ``use of force'' and one for ``drug use,'' the 
system can evaluate the more selective filter first to minimize expensive LLM calls.









%\topic{Coalescing into more complex operations}

















