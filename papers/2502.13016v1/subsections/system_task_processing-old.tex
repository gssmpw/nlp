%!TEX root=../main.tex


\section{Proactive Operation Understanding and Reformulation}
In reactive data systems, the onus
is on the user to author queries involving
the ``right'' LLM-powered operators,
with the system then determining how to execute
these in conjunction with other relational operators.
However, even in cases where users
are able to specify clear, unambiguous operations,
the granularity at which they specify them may
not be optimal for execution.
Fundamentally, this stems from a lack
of understanding of what LLMs can do well
versus what they can't---something most users
are not aware of.
In this section, we discuss various 
approaches to 
operation reformulation 
that can improve accuracy 
while maintaining or reducing computational costs.

\topic{Decomposition into simpler LLM operations}
In Example~\ref{ex:police}, we described
how sometimes journalists want to extract
dozens of fields from a given document
(e.g., police officer names, descriptions
of misconduct, locations, use of firearms, among others).
The journalist may specify the entire list of fields
along with instructions within a prompt.
However, executing this as is in a single LLM call
may lead to poor accuracies
as LLMs often struggle to identify multiple concepts simultaneously~\cite{shankar2024docetl}.
A proactive system can decompose such an operation
into separate focused ones that
each extract one type of information, improving
accuracy.
An LLM can be asked rewrite a prompt that
says ``extract fields f$_1$, ..., f$_n$ from the following ...''
into ``extract field f$_i$ from the following ...''.
In prior work, we have identified
several new decomposition-based rewrite rules~\cite{shankar2024docetl}
that can lead to higher accuracies,
when coupled with LLMs being used to instantiate
the rewrites themselves.
Recent work on Text-2-SQL also leverages 
similar ideas~\cite{pourreza2024chase}: 
rather than attempting translation 
in ``one-shot'' (i.e., single LLM call), 
which often fails due to schema mismatches 
or poorly-named schemas~\cite{luoma2025snails}, 
it is beneficial to 
parse the operation into smaller units, e.g.,
given ``find all employees who joined before 2020'', 
first identify the core concepts: employee records 
and hire dates,
and treat each separately.

\topic{Leverating non-LLM components}
In certain cases, operations
that are assigned to an LLM may be better done
through other means, e.g., through SQL.
For example, finding the average settlement amount 
for misconduct cases can be decomposed 
into an LLM operation to 
identify relevant cases and their settlement amounts, 
followed by a SQL aggregation to compute the average.
We have employed similar techniques where
we separate operations that require real-world reasoning (suited for LLMs) from mathematical computations (better handled by traditional database engines or calculators) in
our work on ZenDB~\cite{lin2024towards}.
Determining how to do this automatically
may be challenging, but we may be able to use 
reasoning models for this purpose. 

\topic{Leveraging LLMs to assess benefits}
One question that naturally emerges when considering
decomposing operations into smaller units
is how to assess the benefits of such decompositions.
While it is a-priori hard to tell whether
a decomposition will help, we can run
both the non-decomposed and decomposed
variants on a sample. 
LLMs are much better at evaluating outputs
than generating them, so LLMs can be used
to tell which version performs better.
For example, 
one can employ a ``generate-fix \& rewrite-verify'' pattern: generate an initial operation formulation, 
verify its correctness (e.g., through automated checks or LLM verification), and if verification fails, attempt alternative formulations~\cite{chung2025long}. 
This pattern, which we also use in DocETL~\cite{shankar2024docetl}, allows a system to systematically explore the space of possible task formulations until finding one that passes verification, effectively optimizing for accuracy through trial and refinement.
However, doing evaluation in a cost-effective manner remains
a challenge. 


\topic{Calibrating LLM outputs}
Independent of which decomposition is used,
when LLMs are independently being applied
to a set of items (documents, tuples),
the outputs can often be
inconsistent and non-calibrated.
For example, if we ask LLMs to rate
the severity of every incident
in our document collection,
it often gives all of them the same score,
or worse, gives them scores that
are only losely correlated with the severity.
To remedy this lack of consistency,
we can take various actions.
We can leverage the LLMs themselves to pick 
representative examples that indicate
the full range of the categories of interest, provided
as few-shot examples.
Or they can rework the prompt to describe in more detail
the criteria used for evaluation---to ensure consistency.
Finally, they can also
restrict the space of possible outputs
(e.g., when LLMs are asked to extract state information
from a collection of US addresses, they may extract CA in some
cases and California in others).
While this doesn't change the semantic meaning of the operation, it can significantly improve LLM accuracy by providing better context and guidance, as has
also been explored in work on prompt optimization~\cite{khattab2023dspy}.


\topic{Deferring to cheaper LLMs for the ``easy bits''}
One concern with decomposition is that
it may increase the cost of the overall pipeline,
since one LLM call per document may now become
multiple. 
One way to defray the cost is to couple decomposition
with cost optimization:
for the simpler newly decomposed operations,
we can alternatively use cheaper and smaller
models to handle them, and only use the more expensive model 
for the most complicated operations.
For example, when we're trying to extract many fields
from a police record document, we can use a cheaper
model for extraction of locations and dates, while using
a more expensive model for harder tasks such as
determining the type of misconduct incident.
While the idea of cheaper proxy models isn't new~\cite{},
here, since the space of decompositions is infinite,
and for each decomposition (or sequences thereof), 
we could use different
models and different confidence thresholds, each with 
different cost-accuracy tradeoffs, the problem
becomes a lot more compliacated.
Additionally, unlike previous settings which focused primarily on tasks with well-defined accuracy metrics, we now must provide guarantees for open-ended generative operations---where quality is harder to quantify.

 
\topic{Expensive predicate ordering, but with synthesized predicates}
For operations that involve subselecting documents
based on certain criteria (all expressed
together in one prompt), we can leverage existing
related work on expensive predicate ordering~\cite{};
however, in our context, we can introduce an arbitrary
number of new dependent predicates (that are potentially easier
to check and therefore cheaper).
For example, instead of using an expensive model
to examine each police record document to extract
medical impacts to the victims, if any,
we can consider cheaper filters
that are easier to check, for example, 
if the document contains any medical information at all.
This check could potentially be done by a cheaper
model and rule out a substantial fraction of the documents.
Similarly, when decomposing a complex filter like ``find 
incidents involving both use of force and drug use'' into two 
filters, one for ``use of force'' and one for ``drug use,'' the 
system can evaluate the more selective filter first to minimize expensive LLM calls.








\topic{Leveraging reasoning for reformulate}
As described above, a proactive data system
must reformulate (e.g., decompose or rework) operations to better execute them. However, finding good reformulations is difficult. Current approaches to discovering task reformulations are quite naive. Systems like DocETL simply prompt LLMs to suggest rewrites, either taking the first suggestion or selecting from multiple candidates. One option is to use a powerful ``reasoning'' model like OpenAI's o1 model to rewrite the task, but this still fundamentally relies on one-shot prompting, and is unaware of how the rewrite will actually perform on the data. We need approaches that can learn what characteristics of the data make tasks challenging, what types of LLM errors occur in different contexts, and use this knowledge to guide task reformulation---perhaps even in an agentic fashion.


\topic{Coalescing into more complex operations}

















\agp{old content follows}
\subsection{Semantic Operation Rewrites}

One strategy to improve operation accuracy is to rewrite it based on our {\em semantic} understanding of what it does. There are several strategies to perform semantic rewrites:

\textbf{Decomposition into Simpler Operations}. Consider Example~\ref{ex:police} where journalists extract various information like locations and police officers from misconduct records. While a user might write this as a single well-specified operation (``find locations and police officers mentioned in the case''), executing this prompt directly may lead to poor accuracy since LLMs often struggle to identify multiple concepts simultaneously~\cite{shankar2024docetl}. By semantically understanding the operation, a proactive system can decompose it into separate focused operations that each extract one type of information, improving both accuracy---and even enable parallelization, improving latency.

\textbf{Hybrid Operation Decomposition}. Semantic understanding also enables decomposing operations into LLM and non-LLM components. Systems like ZenDB~\cite{lin2024towards} demonstrate the value of separating operations that require real-world reasoning (suited for LLMs) from mathematical computations (better handled by traditional database engines). For example, finding the average settlement amount for misconduct cases can be decomposed into an LLM operation to identify relevant cases and their settlement amounts, followed by a SQL aggregation to compute the average. 

\textbf{Semantic Parsing for Operation Translation}. Semantic understanding can also improve accuracy when translating natural language to SQL queries~\cite{pourreza2024chase}. Rather than attempting translation in a ``one-shot'' manner (i.e., single LLM call), which often fails due to schema mismatches or poorly-named schemas~\cite{luoma2025snails}, a proactive system could first parse the operation into semantic components. For instance, given ``find all employees who joined before 2020'', the system identifies the core concepts: it needs employee records and hire dates. Only then does it map these semantic concepts to actual database objects.

Semantic rewrites can be combined with database optimization techniques to reduce costs. For example, when decomposing a complex filter like ``find incidents involving both use of force and drug use'' into two filters, one for ``use of force'' and one for ``drug use,'' the system can apply traditional query optimization principles like filter reordering. By understanding that these are independent predicates, it can evaluate the more selective filter first to minimize expensive LLM calls. \shreya{Of course, there might be correlation between the predicates}

\subsection{Non-Semantic Rewrites}

Beyond semantic understanding, systems can improve operation accuracy through systematic reformulation and verification:

\textbf{Generate-Fix-Verify Pattern}. Some NL-to-SQL approaches employ a ``generate-fix \& rewrite-verify'' pattern: they generate an initial operation formulation, verify its correctness (e.g., through automated checks or LLM verification), and if verification fails, they attempt alternative formulations~\cite{chung2025long}. This pattern, also seen in DocETL~\cite{shankar2024docetl}, allows a system to systematically explore the space of possible task formulations until finding one that passes verification, effectively optimizing for accuracy through trial and refinement.

\textbf{Prompt Engineering}. Operations can be reformulated by enriching them with carefully selected examples, failure cases, or more expressive descriptions. While this doesn't change the semantic meaning of the operation, it can significantly improve LLM accuracy by providing better context and guidance~\cite{khattab2023dspy}.

\shreya{Stopped here for now}

\subsection{Challenges and Future Directions}

\textbf{Systematically exploring Task Reformulations}. A proactive database system reformulates tasks so that it can better execute them. However, finding good reformulations is difficult. Current approaches to discovering task reformulations are quite naive. Systems like DocETL simply prompt LLMs to suggest rewrites, either taking the first suggestion or selecting from multiple candidates. One option is to use a powerful ``reasoning'' model like OpenAI's o1 model to rewrite the task, but this still fundamentally relies on one-shot prompting, and is unaware of how the rewrite will actually perform on the data. We need approaches that can learn what characteristics of the data make tasks challenging, what types of LLM errors occur in different contexts, and use this knowledge to guide task reformulation---perhaps even in an agentic fashion.

%Moreover, we need better ways to predict LLM behavior on different types of tasks---both to avoid costly reformulations that won't improve accuracy, and to identify when verification steps needed to . %Second, these agents need cost models that can reason about the tradeoffs between different LLM models and traditional database operations in a unified way. Current approaches either ignore costs entirely or treat LLMs as black-box UDFs with fixed costs, neither of which captures the complexity of modern LLM pipelines.


\textbf{Cost-Aware Reformulations}. Moreover, oprations involving LLM operations can be prohibitively expensive, especially when processing large datasets. We can turn to AQP. By understanding which tasks (or components of tasks) can be approximated using cheaper LLMs or simpler operations, we can reduce cost without significantly impacting result quality. For instance, we might use smaller models for initial filtering steps, or perform operations on a sample of the data when high precision isn't required. While these strategies are well-known, modern LLM-based systems pose new challenges. For one, instead of simply choosing between a cheap proxy model and an expensive oracle, systems must now reason about hundreds of potential models and multi-step pipelines, each with different cost-accuracy tradeoffs. Additionally, unlike traditional AQP which focused primarily on tasks with well-defined accuracy metrics, we now must provide guarantees for open-ended generative operations---where quality is harder to quantify.

%Data systems increasingly support tasks specified in various ways, ranging from pure SQL queries, to natural language questions, to SQL queries augmented with AI-based functions or predicates for natural language reasoning (e.g., {\tt SELECT * FROM documents WHERE llm\_classify(text, "this document contains police misconduct") = TRUE}). However, the way users specify these tasks may not be optimal for execution, whether due to limitations of the underlying data model, capabilities of the execution engine, or ambiguity in the specification itself. 

% \subsubsection{Breaking Down Complex Operations}
% A proactive database system can decompose complex operations, that is, break them down into series of steps, in order to improve accuracy and lower costs. 
 
% \textbf{Decomposition into Simpler Tasks}. Consider our Example~\ref{ex:police} where the journalists are interested in extracting various information, such as locations, police officers involved, etc., from police misconduct records. A user might write this as a single operation, executing the prompt ``find locations and police officers mentioned in the case'' against all police incident records. While simple to write, executing this prompt on the documents as-is may lead to poor accuracy, if the LLM struggles to identify multiple concepts simultaneously~\cite{shankar2024docetl}. A proactive system could rewrite this into separate focused operations that each extract one type of clause, thereby improving both accuracy and even latency by parallelizing the task.

% \textbf{Decomposition with Tool Use}. This decomposition principle extends beyond just splitting LLM operations. Systems like ZenDB~\cite{lin2024towards} demonstrate the value of separating operations that require real-world ``reasoning'' (suited for LLMs) from mathematical computations (better handled by traditional database engines). For example, finding the average settlement amount for misconduct cases might involve an LLM to identify relevant cases, followed by a SQL aggregation function to correctly compute the average. \shreya{TODO: discuss tool usage}

% \textbf{Optimizing Cost}. Breaking down tasks also enable cost optimization. In our Example~\ref{ex:police}, consider a journalist looking for incidents involving both use of force and drug use. Writing an LLM-based pipeline to execute the prompt ``Does this incident involve use of force and drug use'' against all documents, can lead to unnecessary expensive solutions if one of the conditions, e.g., ``use of force'', is highly selective. An LLM might be able to quickly filter out many documents if it first decomposes the prompt into ``Does this incident involve use of force`` and  ``Does this incident involve drug use''. That is, by reordering these filters and minimizing cost by pushing selective filters down in the query plan~\cite{liu2024declarative}. Beyond enabling such filter reordering, decomposing tasks can unlock other optimization opportunities—from parallelizing independent LLM operations, to caching intermediate results, to selecting different LLMs for different subtasks based on their complexity.

% \if 0
% if one of the filters is very selective. For example, if very few incidents involve use of force, find relevant legal documents: {\tt SELECT * FROM documents WHERE llm\_filter(text, "discusses patent infringement AND involves pharmaceutical companies")}. Even if the ``patent infringement'' condition is highly selective and could quickly filter out many documents, the LLM evaluates both conditions together for every document, if it is treated like a black box. Ideally, a proactive system would rewrite this into two separate filters:

% \begin{verbatim}
% SELECT * FROM documents 
% WHERE llm_filter(text, "discusses patent infringement")
% AND llm_filter(text, "involves pharmaceutical companies") 
% \end{verbatim}

% Then,  we can reorder these filters---minimizing cost by pushing selective filters down in the query plan~\cite{liu2024declarative}. 
% \fi



% \subsubsection{Semantic Parsing}
% A proactive database system can semantically parse a user input to be able to map it better to the data. Consider performing natural language queries on a relational database. Modern NL-to-SQL systems rely heavily on LLMs to generate SQL queries directly from the user's query and database information. However, enterprise settings often involve hundreds of tables with complex schemas, making it impossible to include complete schema information in LLM prompts \shreya{TODO: cite}. Even when schema information fits in the context window, these systems often fail when schema details don't match the LLM's expectations---even subtle variations in column names (e.g., ``rec\_id'' versus ``record\_id'') can cause incorrect SQL generation \shreya{TODO: cite arun kumar paper}. 

% Rather than trying to generate SQL directly, a proactive system first understands the {\em meaning} of the query. For example, given ``find all employees who joined before 2020'', the system might first decompose this into its {\em conceptual} components: it needs an entity representing people employed by the company, and a temporal attribute representing when they started. Only then does it map these semantic concepts to actual database objects---finding that {\tt employees} maps to the {\tt hr\_personnel} table and {\tt joined} maps to {\tt start\_date}. Such a semantic parsing of the task into a logical representation can help bridge the gap between how users express their information needs, and how data is structured in the database. It can also allow the system to ensure the operations performed are semantically correct. The system can through semantic errors if it cannot semantically understand user inputs. 


% \subsubsection{Task Rewrites}
% %Semantic rewrites can be particularly powerful when combined with verification. 
% A proactive database system can rewrite the task so that it can be better executed by the system. The task statement can be enriched with examples, failure possibilities, more expressive descriptions, etc. to improve accuracy.

% Such rewrites are often coupled with a verification mechanism. In many cases, we can verify the correctness of an LLM's output, for example through automated checks (e.g., testing if a generated SQL statement is valid), comparing results against known examples, or using another LLM as a verifier. This ability to verify opens up new optimization strategies. Systems like CHASE-SQL and various NL2SQL approaches \shreya{TODO: cite} employ a``generate-fix-verify'' pattern: they generate an initial task formulation, verify its correctness, and if verification fails, they attempt alternative formulations. For instance, if an LLM generates incorrect SQL due to misunderstanding column names, verification would catch this error, and the system could try reformulating the query. This pattern, also seen in DocETL~\cite{shankar2024docetl}, allows a system to systematically explore the space of possible task formulations until finding one that passes verification, effectively optimizing for accuracy through trial and refinement.


% \subsubsection{Challenges and Future Directions}

% \textbf{Systematically exploring Task Reformulations}. A proactive database system reformulates tasks so that it can better execute them. However, finding good reformulations is difficult. Current approaches to discovering task reformulations are quite naive. Systems like DocETL simply prompt LLMs to suggest rewrites, either taking the first suggestion or selecting from multiple candidates. One option is to use a powerful ``reasoning'' model like OpenAI's o1 model to rewrite the task, but this still fundamentally relies on one-shot prompting, and is unaware of how the rewrite will actually perform on the data. We need approaches that can learn what characteristics of the data make tasks challenging, what types of LLM errors occur in different contexts, and use this knowledge to guide task reformulation---perhaps even in an agentic fashion.

% %Moreover, we need better ways to predict LLM behavior on different types of tasks---both to avoid costly reformulations that won't improve accuracy, and to identify when verification steps needed to . %Second, these agents need cost models that can reason about the tradeoffs between different LLM models and traditional database operations in a unified way. Current approaches either ignore costs entirely or treat LLMs as black-box UDFs with fixed costs, neither of which captures the complexity of modern LLM pipelines.


% \textbf{Cost-Aware Reformulations}. Moreover, oprations involving LLM operations can be prohibitively expensive, especially when processing large datasets. We can turn to AQP. By understanding which tasks (or components of tasks) can be approximated using cheaper LLMs or simpler operations, we can reduce cost without significantly impacting result quality. For instance, we might use smaller models for initial filtering steps, or perform operations on a sample of the data when high precision isn't required. While these strategies are well-known, modern LLM-based systems pose new challenges. For one, instead of simply choosing between a cheap proxy model and an expensive oracle, systems must now reason about hundreds of potential models and multi-step pipelines, each with different cost-accuracy tradeoffs. Additionally, unlike traditional AQP which focused primarily on tasks with well-defined accuracy metrics, we now must provide guarantees for open-ended generative operations---where quality is harder to quantify.



% \shreya{Outline:
% \begin{itemize}
%     \item Users specify tasks in many ways; in SQL, in natural language, or some hybrid combination.
%     \item However these tasks are specified, it might be suboptimal given the data model (or lack of data model), or given the capabilities of the execution engine. For example, imagine the LLM is the execution engine; scanning a legal document to extract many types of clauses. We know LLMs are bad at doing many things in one go, so we may rewrite this query to be k independent queries, each of which extract a small number of clauses. Moreover, tasks can also be ambiguously specified---especially if natural language---and a rewrite of the task can bring at least some specificity to the query, thereby improving the quality of the result (give an example here)
%     \item There are several ``types'' of query rewrites, or ways to think about query rewrites. One is along logically decomposing a complex task into a sequence of simpler tasks. Another is to separate ``reasoning'' from mathematical operations, that databases and data processing engines will do well (e.g., ZenDB), or tasks well-executed by other tools (e.g., getting the weather). Another is to approximate definitions when the query is ambiguous (todo: figure out how to say this better)
%     \item What's key here is to do this {\em agentically}, that is, semantically understand the task, rather than rely on user-specified decompositions. In a simple example, imagine the user has written a pipeline with 2 filter operations. A system like Palimpzest will reorder the filters cost-optimally. However imagine they wrote the pipeline with one filter operation, where the operation was described by a prompt that effectively squishes both the predicates together, joined by the word ``and.'' We should ``agentically'' break this prompt into its 2 different operations, perhaps with an LLM, and then apply our cost optimization---rather than leave the operation as-is.
%     \item NL to SQL: it seems like being ``aware'' of the data model is very difficult, especially in enterprise data, where there are too many tables to fit in a prompt, and it's not even clear what the tables are. cite also Arun Kumar's work of LLMs getting confused by column names, like ``rec\_id'' and ``record id'', LLMs do not correctly generate the SQL if the column name is the former
%     \item Generate-fix-verify pattern in CHASE-SQL and NL2SQL; if we have a verifier (can even be an LLM), then we can put the system to keep trying alternative task formulations in a loop. DocETL also leverages this
%     \item Future directions: how to effectively search over different task decompositions---DocETL's strategy of ``just ask the LLM for different decompositions'' is a start, but how can we be smarter about it, find these decompositions in a data-driven way or based on what the LLM gets wrong? , approximate query processing---figure out what tasks (or part of tasks) are easy to approximate with cheaper LLMs, cost and accuracy aware optimization, developing agents for task rewriting (that can consider the appropriate context) with the generate-fix-verify or TAG paradigm
% \end{itemize}
% }

% \textbf{Query rewrites} DocETL things that go here, rewrites to approximate user intent, rewrites to enjoy better system performance, rewrites to improve accuracy (given limitations of LLMs)

% \textbf{Task Decomposition} DocETL things that go here, also ZenDB-style (and LLM UDFs) hybrid LLM + SQL queries

% \textbf{Future Directions}