%!TEX root=../main.tex


\section{Proactive User Understanding }\label{sec:intent}

Even with best-effort operation reformulation 
and data understanding capabilities, 
a fundamental challenge remains: the gap between what users specify 
and what they actually need. 
This challenge manifests in multiple ways---users may provide ambiguous specifications, 
fail to articulate implicit assumptions, 
or simply not know how to express their requirements fully~\cite{papicchio2024evaluating, shankar2024validates}. 
To bridge this gap between the user's intent and the operations performed, 
a proactive data system needs to be internally 
aware of this gap when executing the task (Section~\ref{sec:internal}), 
leverage user feedback to bridge the gap (Section~\ref{sec:feedback}) 
and provide mechanisms for users to externally validate query results (Section~\ref{sec:verification}). 

\subsection{Imprecision-Aware Processing}\label{sec:internal}

Unlike reactive data systems
that only execute the query as stated, proactive
data systems can leverage LLMs to help truly
identify user intent,
despite the user-provided tasks representing
an imprecise or incomplete specification thereof. 
The system should therefore internally consider 
multiple possible user intents when processing tasks, 
potentially providing different answers that correspond 
to these different interpretations. This can be done to varying degrees. 
In the simplest form, the system can consider multiple interpretations 
of the statements provided by the user. 
In Example~\ref{ex:police}, the system can consider 
various spellings of the same name, 
either in the input provided by the user or derived from the data. 
Such attempts are similar to 
possible world notions in the database community~\cite{suciu2022probabilistic}, 
where the database can consider various possibilities for ``fuzzy'' data and queries.

A proactive system can take 
more aggressive steps to understand user intent. 
For example, the system can consider adding new predicates 
that the user may be interested in---e.g., 
if a user has previously asked several questions about police misconduct 
in a specific city but submits a new query without specifying location, the system might prioritize results from that city. 
This intent discovery can be data-driven---the system might determine that records from certain cities are more relevant 
or interesting 
and prioritize them in the output. 
Moreover, if the output for an operation is too large, the system can 
selectively display what it determines to be the most relevant answers 
or provide appropriate summaries or sample outputs.

The system can also anticipate user questions, for example, ``why was a certain record not provided in the answer'', 
and proactively relevant records that, 
while not strictly matching the query, might be of interest to the user. 
Additionally, the system can modify queries by dropping or relaxing certain predicates---e.g., if the user has specified a predicate that leads to empty results. 
Or, the system might expand query scope, for example, geographically, to include potentially interesting results 
(such as when a specific type of police misconduct, while not present in the queried city, occurred in neighboring jurisdictions).


\subsection{Leveraging User Feedback}\label{sec:feedback}
A proactive system can leverage feedback from users to clarify intent in a lightweight manner.
This feedback serves two purposes: improving accuracy for the current task, and enhancing the system's understanding of user intent for future queries.


To improve the accuracy on the current task, the system can decide to ask follow-up questions~\cite{li2024llms}. This might include asking for clarification about task goals, gathering additional specifications, or presenting example results for users to indicate which best match their needs. The important challenge is balancing the need for clarity with minimizing user burden: for example, when processing police misconduct documents, rather than asking multiple detailed questions, the system might show representative document types and let users select which are most relevant---then apply this learning broadly across the document collection. Similarly, when encountering potential name variations in Example~\ref{ex:police}, the system might ask a single question about handling typos that can inform its overall matching strategy, rather than asking the user to confirm every typo correction. While LLMs offer promising capabilities for generating targeted feedback requests, automatically determining what feedback to request and when remains an open research challenge. Prior work
on predictive interaction is highly relevant here~\cite{2015-predictive-interaction}.


User feedback can also be leveraged to improve system performance on {\em future} tasks. For example, if the user provides feedback that a certain document is relevant or not relevant to a task, the system can then update its data and task processing mechanism to take that feedback into account. This can, for instance, change operation rewrite rules used internally for processing (Section~\ref{sec:op}), modify data representation~\cite{zeighami2024nudge} or change how semantic structure is extracted from the data (Section~\ref{sec:data}). While this approach shares similarities with query-by-example systems that learn from user-provided examples, e.g., \cite{fariha2018squid}, it extends the concept more broadlyâ€”--allowing the system to refine its understanding of user intent across a diverse range of tasks and feedback types.

\subsection{Verifying Execution}\label{sec:verification}

A proactive system must provide users with the means to verify that their tasks were executed correctly. Verification is particularly important when the system makes autonomous decisions---for instance, when correcting potential typos in names, the system should clearly show which corrections were made to allow users to catch any incorrect modifications. 
If, during processing, the system encountered anomalous documents, it's best to indicate them as such to the users so that they don't pollute the rest of the analysis.

While the system can provide comprehensive execution traces, including details of LLM operations performed and data sources accessed~\cite{tan2007provenance}, presenting this information in a user-friendly way remains challenging. Simply showing raw execution traces or complete datasets is overwhelming and impractical, as users cannot reasonably review large amounts of data to verify correctness. An interesting open challenge
is to determine a small subset or explanation that conveys the same information as the entire provenance; we can always verify such explanations using an LLM. 


% % We have discussed how the data system can attempt to proactively understand the data and the operations to improve the correctness of the system by breaking down complex operations, rewriting them, etc. However, even if such reformulations are done to the best of the system's ability, the effort is in vain if the user's specification is ambiguous, or it does not actually match their true intent \cite{papicchio2024evaluating, shankar2024validates}. As Example~\ref{ex:police} shows, even in very simple tasks such as looking up records by name, the system needs to proactively ensure that the operations it performs match the user intent. 

% \if 0
% %\sep{not sure if sematic gap is the phraing we want to go with, but keeping it as that rn}
% An important characteristic of performing natural language operations, either when the data is unstructured or when task specification is unstructured  is the semantic gap between the user specification and what the system perceives the user specification to be. This semantic gap can exist even in very simple query scenarios:

% \begin{example}\label{ex:typo}
%      On either a structured or unstructured data source containing names of people, consider the simple query of a user asking for records of ``Adam Smiht'', where ``Smiht'' is a misspelling of ``Smith''. The system needs to understand user intent (which name they are asking for). This may need to be done with respect to data (e.g., if neither ``Smith'' or `'Smiht'' exist, the answer is the same for both queries). The system cannot ensure correctness by always correcting what it perceives as typos, there may be deviations from common name spelling that are not typos. 
% \end{example}

% The example shows that even in simple query-answering scenarios, the correct answer cannot be ensured without taking the user into account. In a proactive data system, the data system has the agency to ensure the operations are performed correctly. This means the system needs to (1) provide an interface where it can collect additional feedback and allow the user to verify answers and (2) have internal mechanisms to take such feedback into account. 
% \fi



% %\subsubsection{Interface}
% %We envision an interface where the system is aware of imprecision in the unstructured tasks and the data definition, and provides tools for the users to reach their correct answer while keeping the imprecision in mind. Here, we discuss various frameworks that allow us to achieve this.

% %\textbf{Possible Worlds}. 
% \subsection{Imprecision-Aware Processing}\label{sec:internal}

% \shreya{Seems like the overall takeaway here is; LLMs allow us to be more aggressive in determining the user's intent.}

% User-provided tasks often represent imprecise or incomplete specifications of their true intent. The system should therefore internally consider multiple possible user intents when processing tasks, potentially providing different answers that correspond to these different interpretations. This can be done to varying degrees. In the simplest form, the system can consider multiple interpretations of the statements provided by the user. In Example~\ref{ex:police}, the system can consider various spellings of the same name, either in the input provided by the user or in the data. Such attempts are similar to the possible world's notion in the database community \cite{suciu2022probabilistic}, where the database can consider various possibilities for a data point and the query.

% % The task provided by the user can be an imprecise representation of their intent. The system needs to internally consider possible user intents when performing the task and find answers that are likely to match the user intent, potentially providing multiple different answers to users for their different possible intents. 

% % This can be done to varying degrees. In the simplest form, the system can consider multiple interpretations of the statements provided by the user. In Example~\ref{ex:police}, the system can consider various spellings of the same name, either in the input provided by the user or in the data. Such attempts are similar to the possible world's notion in the database community \cite{suciu2022probabilistic}, where the database can consider various possibilities for a data point and the query.

% Beyond simple matching of different interpretations, a proactive system can take more aggressive steps to understand user intent. For example, the system can consider adding new predicates a statement that the user may be interested in---e.g., if a user has previously asked several questions about police misconduct in a specific city but submits a new query without specifying location, the system might prioritize results from that city. This intent discovery can be data-driven---the system might determine that records from certain cities are more relevant and prioritize them in the output. Moreover, if the output for an operation is too large, the system can selectively display what it determines to be the most relevant answers or provide appropriate summaries.



% % Such intent discovery can be done data dependently, for example, the system can decide based on the data that the records for a certain city are more interesting, and put the results for that city at the top of the output. Moreover, if the output for a task is too large the system can decide to only show answers that the system thinks the user is interested, or summarize some of the answers. 

% The system can anticipate user questions, for example, ``why a certain record was not provided in the answer'', and proactively relevant records that, while not strictly matching the query, might be of interest to the user. Additionally, the system can modify queries by dropping or relaxing certain predicates---particularly if the user has incorrectly specified the predicate. \shreya{The prior sentence needs an example; it is weird for it to not have an example while the following sentence has an example.} Or, the system might expand query scope, for example geographically, to include potentially interesting results (such as when a specific type of police misconduct, while not present in the queried city, occurred in neighboring jurisdictions).


% % The system can also drop or relax certain predicates. The user may have wrongly specified a predicate and dropping such a predicate may lead to answers that match the user intents better. The system can similarly expand the scope of a query, e.g., geographically, to include results that the user may find interesting (e.g., a specific form of police misconduct may not have happened in a particular city but did happen in a neighboring city).

% %has been proposed to deal with uncertainty in query answer. The notion allows the system to consider various interpretations of the data and query, and show to the user the answers to all \textit{possible} interpretations. However, in the case of unstructured data and queries, having such an interface comes with its own challenges. Although LLMs can be used to decide what is a ``possible answer'', the notion of what is a possible interpretation is not well-defined, and on its own may require further feedback from the user. Moreover, such an approach can lead to large output sets that are hard to interpret for the user, as the user may need to understand why an answer was possible. 

% %In addition to decomposing complex operations, task rewrites can address ambiguity in user specifications. When tasks are specified in natural language, they often contain ambiguity that impacts result quality. Rather than executing these tasks directly, proactive systems can rewrite them to explicitly introduce specificity. For instance, a natural language query about ``find police misconduct cases'' might be rewritten into a series of more specific operations that check for use of force incidents, procedural violations, and subsequent investigations. The challenge of ambiguity is particularly evident in natural language to SQL translation. 


% %Existing work \cite{madden2024databases, liu2024declarative} suggests using a declarative interface, inspired by the declarative interface of relational databases, to perform data processing tasks. The goal of such a declarative interface is to allow the user to specify their task, while the system is in charge of how to perform the task. However, Example~\ref{ex:typo} shows how difficult such a separation can be even in a simple query answering scenario, where ``how'' to perform a task requires detailed instructions about the task from the user. We believe interfaces that allow the system to understand user intent, and allow the user to verify system operations are crucial for successful data processing. The database system cannot assume that the user input corresponds to well-defined logical operations, but needs to take the agency to ensure the correct interpretation of user inputs. 

% \subsection{Leveraging User Feedback}\label{sec:feedback}

% \shreya{Seems like the takeaway here is; ask for low-effort feedback and apply this across tasks, not just for the current task.}

% A proactive system can leverage feedback from users to clarify intent. This feedback serves two purposes: improving accuracy for the current task, and enhancing the system's understanding of user intent for future queries.

% % The user can additionally interact with the user to obtain clarifications of the user intent.  User feedback can be used to improve the accuracy on the current task and to better understand user intents in general for future tasks. 

% To improve the accuracy on the current task, the system can decide to ask follow-up questions~\cite{li2024llms}. This might include asking for clarification about task goals, gathering additional specifications, or presenting example results for users to indicate which best match their needs. The important challenge is balancing the need for clarity with minimizing user burden: for example, when processing police misconduct documents, rather than asking multiple detailed questions, the system might show representative document types and let users select which are most relevant---then apply this learning broadly across the document collection. Similarly, when encountering potential name variations in Example~\ref{ex:police}, the system might ask a single question about handling typos that can inform its overall matching strategy, rather than asking the user to confirm every typo correction. While LLMs offer promising capabilities for generating targeted feedback requests, automatically determining what feedback to request and when remains an open research challenge. \shreya{We need better connection to PBE.}


% % All such feedback should attempt to minimize the need for user feedback, and the system needs to decide to maximally use the feedback. For instance, by showing different types of documents to the user, the system may decide a certain type of document is useful for finding misconduct, applying user feedback broadly to new documents to be processed. %how to generalize feedback to n. example results to show the user to choose from, among many other intent discovery works. Using LLMs, the space of potential interactions with the user is large, and the database system needs to decide what the best way to elicit feedback from the user to get to the correct answer is. 
% % In Example~\ref{ex:police} where there can be a type in the names of police officers, the system may succinctly ask the user if typos should be corrected. Automatically finding how to generate such follow-up questions using LLMs based on the data and the task is challenging and requires further research. 

% %\textbf{Taking User Feedback into Account}. More importantly, the system needs to be able to take user feedback into account. We envision two modes, where the system is either getting feedback to interactively improve the accuracy on the current task, or getting feedback to improve the accuracy on future task.
% %In the first setting, the system can take feedback from the user and internally rewrite queries. It can ask the user follow up questions and rewrite the data. \sep{Probably just connect this back to the two previous sections}

% User feedback can also be leveraged to improve system performance on {\em future} tasks. For example, if the user provides feedback that a certain document is relevant or not relevant to a task, the system can then update its data and task processing mechanism to take that feedback into account. This can, for instance, change operation rewrite rules used internally for processing (discussed in Sec.~\ref{sec:op}), modify data representation \cite{zeighami2024nudge} or change how semantic structure is extracted from the data (discussed in Sec.~\ref{sec:data}). While this approach shares similarities with query-by-example systems that learn from user-provided examples \cite{fariha2018squid}, it extends the concept more broadlyâ€”allowing the system to refine its understanding of user intent across a diverse range of tasks and feedback types.

% %An example solution is NUDGE \cite{zeighami2024nudge}, where the users provide feedback on what documents are the correct answer to a query, and the system modifies the internal data representation (in the case of NUDGE the vector embeddings of data records) to improve accuracy on such a dataset. Overall, NUGDE is a specific that shows how to fine-tune vector data representations for the user to ensure that the system is well-opteimized for the specific workload the user is interested in. Ideas from such an approach can be applicable to other data representation, for example, to fine-tune extracted summaries, modify schemas and the extracted structure, to ensure that the internal representation of the data and tasks in the system is better aligned with the user after obtaining feedback from the user.     

% \subsection{Verifying Execution}\label{sec:verification}

% A proactive system must provide users with the means to verify that their tasks were executed correctly. Verification is particularly important when the system makes autonomous decisions---for instance, when correcting potential typos in names, the system should clearly show which corrections were made to allow users to catch any incorrect modifications. 

% While the system can provide comprehensive execution traces, including details of LLM operations performed and data sources accessed \cite{tan2007provenance}, presenting this information in a user-friendly way remains challenging. Simply showing raw execution traces or complete datasets is overwhelming and impractical, as users cannot reasonably review large amounts of data to verify correctness. \shreya{Ends abruptly}


% % No matter how the system performs a task, the user needs to be able to verify that the operation was done correctly. For example, if the system asks fixes typos in name, it should be able to show the user what typos were fixed, in case the system incorrectly changed the spelling of a name. The system can in general show the user the operations it performed (e.g., what LLM calls), and what part of the data were used~\cite{tan2007provenance}. However, doing so in a way that allows easy validation by the user is difficult~\cite{shankar2024validates}. For example, although showing the entire dataset allows user to validate what information was extracted, it is not helpful since user would need to spend time reading through the document. %We envision a proactive data system that will decide what to show to the user, both informative and concise. 

% %For query-answering tasks, the system can show the user the answer provenance, so the user can verify how the answer was generated. However, how the provenance should be presented to the user is non-trivial, since the answer may have been generated from a large span of text and verification from a large text span may be difficult and time-consuming. 
% %The interface may require the system to perform additional operations while answering queries. For example, the system may need to keep track of the LLM calls it did internally to show to the user for validation, or keep track of data provenance to show to the user. It may need to consider possible world semantics, especially if multiple different answers are to be shown to the user.  


