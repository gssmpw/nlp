\subsection{Compose Theory}
ComposeOn promotes people without a music background to compose their own music. We design the system to detect melody users put in and give suggestions for continuations based on basic music and harmony progression theory. "Western music written during Baroque, Classical, and Romantic periods (ca.1650-ca.1900) is called tonal music, which has a point of gravitation called tonic." \citep{laitz2008complete} The keys and scales gradually formed based on that and thus formed tonal hierarchy and harmony function theory.

"The music of the tonal era is almost exclusively tertian, which means being constructed of stacked 3rd." \citep{kostka2006materials} Chords are marked with their root notes (where the stack begins) using roman numerals. Functionally, they are basically divided into Tonic chords: I, Dominant chord: V, and predominant/subdominant chords: ii, iv, vi. The harmony often progresses as Tonic–Subdominant–Dominant–Tonic. Sometimes may use iii or Vii, and different 7th chords, but their function is various depending on the texture \citep{aldwell2010harmony}.

There are also uses out of this basic progression, such as sequence, modulation or transportation. We also considered those situations with limited possibilities within our database.

Based on the above theory, we suggest notes within the key and the possible harmony progressions. As to music phrases, we followed basic 2+2/4+4 (refer to measure numbers) to form the music phrase and thus sentence and sections. \citep{schoenberg1967fundamentals} We mainly focus on songwriting, so the intro, verse — chorus — verse — chorus —bridge — chorus — outro structure of song is also being considered. \citep{masterclass2021songwriting}

\subsection{Voice to MIDI Technology and Its Applications}

MIDI (Musical Instrument Digital Interface) is a technical standard that describes a protocol, digital interface, and connectors, allowing various electronic musical instruments, computers, and other related devices to connect and communicate with each other \citep{midi1996complete}. Voice to MIDI technology is the process of converting vocal or other audio signals into this MIDI data format, playing a crucial role in music production and analysis. This technology involves multiple steps, including pitch detection, note segmentation, quantization, and MIDI conversion. Pitch detection typically employs algorithms such as YIN \citep{decheveigne2002yin} or pYIN \citep{mauch2014pyin} to estimate the fundamental frequency of audio. Subsequently, the continuous pitch sequence is segmented into discrete notes, which are then time-aligned to a musical grid. Finally, the detected note information is converted into MIDI events. Voice-to-MIDI technology has found applications in various fields, such as quickly converting hummed melodies into MIDI for song composition, providing instant feedback to students in music education, and generating real-time music based on user voice input in interactive music systems. This technology not only simplifies the music creation process but also provides powerful tools for music analysis and education.

\subsection{Automated Melody Analysis}

Automated melody analysis is a significant branch of music information retrieval, aimed at extracting and analyzing melodic features from musical data. This process typically includes the analysis of notes, chords, and chord progressions. Note analysis involves extracting attributes such as pitch, duration, and velocity. Chord analysis focuses on identifying combinations of simultaneously sounding notes, often using algorithms like Chordino \citep{mauch2010simultaneous}. Chord progression analysis examines patterns in the series of chords, utilizing methods such as hidden Markov models \citep{rohrmeier2012comparing}. In this field, Musicpy, a powerful Python music programming library, provides extensive functional support. It not only has concise syntax for representing various musical elements but also incorporates a complete music theory system supporting advanced musical operations. Musicpy's core data structures include notes, keys, chords, scales, etc., offering various practical functions including chord identification, melody analysis, and automatic composition. Through Musicpy, researchers and music creators can conveniently achieve automated melody analysis, explore musical structures and characteristics. Notably, the ComposeOn project extensively utilizes Musicpy's powerful capabilities, particularly in chord extraction and chord progression analysis. ComposeOn employs Musicpy's algorithms to identify and analyze the progression patterns of these chords, thereby providing an important foundation for music analysis and creation. This application demonstrates the practicality and effectiveness of Musicpy in real-world music analysis projects.

\subsection{Automatic Accompaniment Generation}
Accompaniment generation is referred to as "the audio realization of a chord sequence"
by systems like MySong \cite{simon2008mysong}, which represents a significant advancement in the field of automatic accompaniment generation for vocal melodies. MySong allows users to input vocal melodies, which the system then inputs to a hidden Markov model to recommend chords. However, MySong's capabilities are limited to generating accompaniments, whereas our ComposeOn system empowers users to easily extend and develop their melodic ideas into complete compositions, providing a more comprehensive music creation and learning experience.
