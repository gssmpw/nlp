\begin{table*}
    
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{m{3.3cm}|b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}b{0.6cm}}
    \multicolumn{1}{l}{}  & \rot{SingleOp} & \rot{SingleEq} & \rot{MultiArith} & \rot{SVAMP} & \rot{GSM8K} & \rot{MMLU HS Math} & \rot{Logic Ded. 3-Obj} & \rot{Object Counting} & \rot{Navigate} & \rot{TabFact} & \rot{HotpotQA} & \rot{SQuAD2.0} & \rot{DROP} & \rot{Winograd WSC} \\
    \toprule
Avg \# errors, original & 2.5 & 1.1 & 3.5 & 18.8 & 10.1 & 20.5 & 2.4 & 6.3 & 5.9 & 17.3 & 26.4 & 56.2 & 28.4 & 16.6 \\
     Avg \# errors, cleaned &  0.3 & 0.1 & 0.4 & 4.3 & 5.2 & 19.5 & 2.4 & 4.8 & 5.9 & 4.3 & 2.3 & 9.9 & 6.6 & 15.0 \\ \midrule
     
    \textbf{\% errors caused by \newline benchmark errors}  &  90\% & 93\% & 89\% & 77\% & 48\% & 5\% & 0\% & 24\% & 0\% & 75\% & 91\% & 82\% & 77\% & 10\% \\
    
    \bottomrule
    \end{tabular}
    \caption{Here we report the average number of errors across models on each benchmark before and after our revision process. For most benchmarks the number of model errors decreased significantly, often by over 50\%, suggesting that the majority of errors in the original benchmarks can be attributed to label noise rather than genuine model failures. We exclude VQA V2.0, as the original benchmark did not include one single ground-truth label to compare against (see Appendix \ref{app:vqa}).
    For reading comprehension benchmarks, there are often many potential correct answers that are not enumerated within the original label (e.g. answering ``five'' when the solution is ``5''). We use an LLM to resolve such cases in the original benchmark; see Appendix \ref{app:hotpotqa} for details.}
    \label{tab:original_vs_cleaned}
\end{table*}
