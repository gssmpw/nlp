In this work, we propose the construction of carefully curated \textit{platinum} benchmarks to test the reliability of LLMs, and make an initial effort to create such benchmarks by revising fifteen existing datasets. We demonstrate that frontier models continue to exhibit failures on basic tasks from these ``saturated'' benchmarks, showing a gap in current benchmarking practices. We hope that our paper motivates the adoption of platinum benchmarks for evaluating LLMs to ensure they meet the high reliability standards required in real-world applications. \aleks{``Why not combine this with Section 4''}

