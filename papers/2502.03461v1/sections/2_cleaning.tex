\input{figures/error_examples}

To make a noisy benchmark ``platinum,'' we need to update it to remove or correct label errors. In this section, we categorize common types of errors and specify our approach for detecting and correcting them. In Section \ref{sec:results}, we will then leverage these platinum benchmarks in order to evaluate the reliability of frontier models.

\footnotetext{We mask identities for privacy.}

\subsection{Experimental setup}

\paragraph{Benchmarks considered} We investigate fifteen benchmarks covering six categories of capabilities: mathematics (SingleOp~\cite{roy2015reasoning}, SingleEq~\cite{koncel2015parsing}, MultiArith~\cite{roy2016solving}, SVAMP~\citep{patel2021nlp}, GSM8K~\citep{cobbe2021training}, MMLU High School Math~\citep{hendrycks2020measuring}), logic (BIG-bench Object Counting, BIG-bench Logical Deduction, BIG-bench Navigate ~\citep{srivastava2022beyond, suzgun2022challenging}), table understanding (TabFact~\citep{chen2019tabfact}), reading comprehension (SQuAD2.0 \cite{rajpurkar2018know}, HotPotQA~\citep{yang2018hotpotqa}, DROP~\citep{dua2019drop}), commonsense reasoning (Winograd WSC~\citep{levesque2012winograd}), and visual understanding (VQA v2.0~\citep{goyal2017making}). For datasets with publicly available test splits with solutions, we use the test split, otherwise we use the validation split. Many of these benchmarks are large (e.g., the VQA v2.0 validation set has over 200,000 questions). In order to ensure the quality of our cleaning process, we select smaller subsets at random from many of these benchmarks.

\paragraph{Models tested}
We test a variety of current frontier models (as of January 2025), including both popular proprietary LLMs \cite{openai2023gpt4,Claude35ModelCardAddendum,reid2024gemini,openai2024o1,openai2025o3mini} and open-weights LLMs \cite{dubey2024llama,qwen2.5,deepseekai2024deepseekv3technicalreport,guo2025deepseek}. The full list of models is included in Table \ref{tab:error_results}.

\paragraph{Prompting}
We use chain-of-thought prompting~\cite{wei2022chain} (i.e., asking the model to think step-by-step) for all models except ``reasoning'' models (the o1 series, DeepSeek-R1, Gemini Thinking), as step-by-step thinking is instilled in them through their training. Furthermore, o1's official prompting guide specifically recommends omitting chain-of-thought prompting\footnote{see \url{https://platform.openai.com/docs/guides/reasoning/advice-on-prompting\#advice-on-prompting}.}. All
questions are asked in a zero-shot setting and using a temperature of 0.5. The exact prompts we use are provided in Appendix \ref{app:template}.

\paragraph{Metrics}
All the benchmarks we use are either multiple choice or have a single correct answer (except for reading comprehension datasets such as SQuAD2.0, for which we manually expand the set of correct responses---see Appendix \ref{app:hotpotqa} for further details). Thus, we can simply compare this answer to the model prediction to evaluate correctness. We  report the number of errors rather than accuracy to better differentiate between models, as for many benchmarks we expect models to have accuracies in the high 90s.

\input{tables/cleaned_datasets}

\subsection{Identifying Errors in Benchmarks}\label{sec:identify-bad-questions}
We consider a model to be reliable on some task category if it can always (or nearly always) correctly answer questions within that category, provided that they are unambiguous and clearly defined. Measuring such reliability requires carefully curated benchmarks where every incorrect answer definitively indicates a model failure rather than an issue with the question itself.


\paragraph{What makes a question bad?} Before correcting errors in benchmarks, we first need to categorize the kinds of issues we aim to resolve. Each example in a benchmark consists of a question and a solution. Sometimes, a question can be well-written, but the solution is mislabeled. For instance, Figure \ref{fig:benchmark_errors}(a) shows a question from SVAMP for which the given solution is incorrect. For such examples we can simply re-label the solution, allowing us to keep the example in the benchmark. 


In other cases, though, the question itself is poorly written, so simply re-labeling the solution is inadequate. Figures \ref{fig:benchmark_errors}(b-d) illustrate three common categories of such issues: (1) a logical contradiction in the problem statement, (2) ambiguity that allows for multiple plausible solutions, or (3) a clear flaw in the question's construction. 
We opt to remove all poorly written examples during our revision process, as in many cases there is no simple way to fix them. For example, fixing Figure \ref{fig:benchmark_errors}(d) would require coming up with a set of equations and corresponding question from scratch.

\paragraph{How do we efficiently identify errors in a benchmark?} 

Revising large datasets can be time-consuming, especially when the questions take time to verify (e.g., challenging math problems or retrieval tasks with long contexts). We devise a simple and scalable strategy to find problematic questions by examining the agreement among multiple LLMs.

As described above, we divide potential issues with examples from a benchmark into two categories: (1) mislabeled solutions, and (2) poorly written questions (e.g., ones that are ambiguous or ill-posed). To detect these issues, we give each question to several frontier LLMs. We then manually inspect any example on which at least one LLM makes an error. We expect that when a solution is incorrect, frontier models will often disagree with the given solution, and when the question itself is poorly written, models should disagree among themselves. For every example we inspect, we either mark that example as ``bad'' and remove it if the question is poorly written or relabel the solution if it is incorrect. 
Note that the number of errors that we identify for each benchmark is only a lower bound, as our approach could miss any bad question for which all LLM solutions happen to agree with the benchmark solution. For example, all models could interpret an ambiguous question in the same way as that question's given solution. Further details of the cleaning protocols used for each benchmark are in Appendix \ref{app:exp-details}.


\paragraph{How noisy are saturated LLM benchmarks?}
In Table \ref{tab:cleaning}, we report the number of poorly written and mislabeled questions we identify in each of the fifteen benchmark subsets we investigate. We find that, indeed, many of these benchmarks have a substantial rate of errors, confirming suspicions of the presence of flaws in these benchmarks commonly held by the community. For example, the percentage of mislabeled or poorly written questions that we find in GSM8K, SVAMP, VQA v2.0 and TabFact is greater than the percentage of errors reported by frontier models on these benchmarks.


For reading comprehension datasets (SQuAD2.0, HotpotQA, DROP), we identify issues with up to 30\% of examples. Largely, these issues arise from questions that are sufficiently open-ended such that it is difficult to exhaustively list all possible responses. Additionally, SQuAD2.0 intentionally adds questions that are unanswerable from the given passage to test whether models can abstain from answering (i.e., return N/A). However, the process of making these unanswerable questions often leads to them being highly ambiguous or even nonsensical. For example, one such question asks, ``What isn't the gender income inequality in Bahrain?''; this question can be traced to a worker replacing "is" with "isn't" from an original question within SQuAD~\cite{rajpurkar2016squad}\footnote{For the original question from SQuAD, \href{https://huggingface.co/datasets/rajpurkar/squad/viewer/plain\_text/validation?p=74&row=7464}{see this link}.}. Following our cleaning protocol, we omit such poorly written questions.
In Appendix \ref{app:example-failures}, we show specific examples of bad questions that we identify across the fifteen benchmarks.




