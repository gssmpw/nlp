\subsection{Benchmark processing} \label{app:benchmarks}
Here, we discuss additional processing and cleaning steps specific to certain of the chosen benchmarks.

\subsubsection{VQA v2.0} \label{app:vqa}

\paragraph{Re-labeling} Rather than a single ground truth label per example, the VQA~\cite{antol2015vqa} and updated VQA v2.0~\citep{goyal2017making} datasets collect ten separate crowd-annotated labels per image-question query. Their accuracy metric then assigns a score to a model prediction based on the overlap between the prediction and these ten labels. As we assign a single ground-truth label to each question, we manually re-label all the VQA v2.0 queries we include in our revised subset rather than only inspecting ones for which some model failed.

\paragraph{Selection of queries} The VQA v2.0 dataset is designed to mitigate common biases in visual question answering datasets by balancing the original VQA dataset with complementary images that break a given bias. To maintain this construction, we randomly select from these image pairs in VQA v2.0, and reject a given pair if either image deemed ambiguous.

To further improve the clarity and ease of labeling of our samples, we also limit our subset to only `yes/no' questions within VQA v2.0, as open-ended queries have a greater potential for ambiguity and can often have multiple correct answers.

\subsubsection{Reading Comprehension Benchmarks} \label{app:hotpotqa}
\paragraph{Re-labeling to account for multiple correct responses} SQuAD2.0~\cite{rajpurkar2018know}, HotPotQA~\citep{yang2018hotpotqa}, and DROP~\citep{dua2019drop} are all question answering benchmarks based on background knowledge provided in-context. Most of these questions are open-ended, so there are often multiple valid responses. For instance, consider the following example from HotPotQA:
\\
\begin{tcolorbox}[colback=gray!6, colframe=gray!50, arc=2mm, boxrule=0.5pt]
    \textbf{Paragraph A:} \textit{Ethel Houbiers}
    
    Ethel Houbiers is a French voice actress.  She is the French voice of Penélope Cruz and Salma Hayek.
    
    \medskip
    
    \textbf{Paragraph B:} \textit{Salma Hayek}
    
    Salma Hayek Pinault ( Hayek Jiménez) (born September 2, 1966), known professionally as Salma Hayek, is a Mexican and American film actress, producer, and former model\ldots[\textit{continued}]
    
    \medskip
    
    \textbf{Question:} which Mexican and American film actress is Ethel Houbiers French voice of?
    
    \medskip
    
    \textbf{Answer:} Salma Hayek Pinault

\end{tcolorbox}
\vspace{\baselineskip}
 
\noindent While ``Salma Hayek Pinault'' is a valid answer, ``Salma Hayek'' is a second answer that should also be considered valid. To address these cases, when re-labeling examples, we set the revised label to be a list of possible valid responses. Specifically, if any LLM answer doesn't match the original label but is also a valid solution, we include this answer as part of the updated label. We also make an effort to further list additional valid options. If the question is sufficiently open-ended and ambiguous that too many possible options might be valid, we mark the question as bad. 

\paragraph{Marking example as mislabeled} As discussed above, the original reading comprehension benchmarks include many questions for which there might be multiple possible equivalent answers that are not listed as the correct solution, such as the following example from DROP:

\begin{tcolorbox}[colback=gray!6, colframe=gray!50, arc=2mm, boxrule=0.5pt]
\textbf{Context:} Coming off their overtime win over the Bills, the Steelers flew to M\&T Bank Stadium\ldots Pittsburgh trailed in the first quarter as Ravens quarterback Joe Flacco completed a 14-yard touchdown pass to wide receiver Anquan Boldin.  After a scoreless second quarter, Pittsburgh answered in the third quarter\ldots With the win, not only did the Steelers improve to 9-3, but it also allowed them to take the AFC North division lead for the first time since week 4.

\medskip

\textbf{Question:} How many scoring drives took place in the first half?

\medskip
    
\textbf{Answer:} 1
\end{tcolorbox}

\noindent Here, ``one'' (the word rather than the number) would also be a valid solution. However, it might be unfair for us to consider this a benchmark error, as this ambiguity could potentially be overcome by altering our prompting strategy (e.g., specifying to provide numerical answers). 

For our platinum benchmarks we still revise such examples; in this particular case, we mark any of the following solutions as valid: ``1,'' ``one,'' ``1 scoring drive,'' ``one scoring drive.'' However, when counting the number of mislabeled examples, we only consider cases where the original label is not included within the list of valid labels we identify. For instance, if for this example we had revised the label to ``2'' or ``two,'' we would have considered the original example mislabeled.

\paragraph{Answer-matching for the original benchmark}
In Table \ref{tab:original_vs_cleaned}, we compare the number of errors on each of the original and revised benchmarks. However, as we discuss in the paragraph above, the original reading comprehension benchmarks often include ambiguity in the form of many equivalent correct solutions that may have been clarified through better prompting. To more accurately calculate the number of model errors on these original benchmarks, we ask an LLM (GPT-4o) to identify models' answers that are ``equivalent'' to the benchmarks' solution, and consider any such equivalent answer as correct. We only apply this automated equivalence-checking process to calculate the error counts on the original benchmark. For our platinum benchmarks, we instead manually enumerated all valid responses during the revision process.


\subsection{Chain-of-Thought prompt template} \label{app:template}
We use a chain-of-thought prompt for evaluation on all datasets except for VQA V2.0~\citep{goyal2017making}, for which there is general no need for multiple reasoning steps. The specific prompt varies slightly between benchmarks, however the general templates are as follows:\\

\noindent\textbf{Open-ended Question:}
\begin{tcolorbox}[colback=gray!6, colframe=gray!50, arc=2mm, boxrule=0.5pt]
    \texttt{Answer the following \{category\} question.\\\\
    \{question\}\\\\
    Think step-by-step.\textsf{ }Then, answer in the format "Answer:\textsf{ }XXX".}
\end{tcolorbox}

\vspace{\baselineskip}

\noindent\textbf{Multiple-Choice Question:}
\begin{tcolorbox}[colback=gray!6, colframe=gray!50, arc=2mm, boxrule=0.5pt]
    \texttt{Answer the following \{category\} question.\\\\
    \{question\}\\\\
    Options:\\
    A) \{option A\}\\
    B) \{option B\}\\
    C) \{option C\}\\
    D) \{option D\}\\\\
    Think step-by-step.\textsf{ }Then, provide the final answer in the format "Answer:\textsf{ }X" where X is the correct letter choice.}

\end{tcolorbox}

\vspace{\baselineskip}

\noindent For BIG-bench~\cite{srivastava2022beyond}, we use (A) instead of A) for multiple choice style to align with the prompt used by the original authors. For open-ended math questions, we additionally specify to respond with an integer and to exclude additional formatting, as model often style outputs with latex styling. We exclude chain-of-thought prompting for VQA v2.0, as the questions rarely require any explicit reasoning to answer. We also find that, in practice, models rarely actually think step-by-step for simple visual reasoning questions, even when prompted to do so.
