Large language models (LLMs) have demonstrated impressive capabilities in areas such as problem solving \cite{hao2023reasoning,openai2024o1}, knowledge retrieval \cite{gao2023retrieval}, and code generation \cite{guo2024deepseek,jimenez2024swebench}. Major research effort continues to advance the frontier of LLM capabilities \cite{Claude35ModelCardAddendum,openai2023gpt4}. However, these models still sometimes exhibit failures even on tasks far simpler than these frontier capabilities \cite{yang2024can,nezhurina2024alice}. 
Practitioners might thus be worried whether this unreliability can pose significant risks, especially in accuracy- and safety-critical applications.





Indeed, in contexts such as healthcare, finance, insurance, and legal services, model errors can lead to serious ramifications (e.g., jeopardizing patient outcomes or causing substantial financial losses).
In fact, mistakes made by LLMs in real-world deployments have already caused legal liability \cite{moffatt2024civil} and generated controversy \cite{masse2023a}. In light of these issues, it is important to understand when we can confidently deploy LLMs in such situations. These concerns motivate the central question of our work:


\begin{center}
    {\em On what kinds of tasks are frontier models actually reliable?}
\end{center}

To identify such tasks, a natural approach would be to examine existing benchmarks on which current models already perform well.
Specifically, we might want to investigate older benchmarks (e.g., GLUE~\citep{wang2018glue}, SQuAD~\citep{rajpurkar2016squad}, GSM8K~\citep{dvijotham2018training}) that tend to evaluate simpler capabilities than current ones.
These benchmarks are rarely used today due to the commonly held view that performance on them has ``saturated''---that is, that models have reached a sufficient or ``human-level'' performance on the benchmark, and remaining errors can be attributed to  label noise or ambiguity in the benchmark itself. 
Indeed, some recent releases of frontier LLMs have excluded evaluations on GSM8K (a dataset of grade-school level math word problems), for example \cite{openai2024o1,anthropic2024claude}, following concerns that it has reached saturation~\cite{hendersontweet} (current frontier models achieve $\sim$95\% accuracy~\cite{dubey2024llama,Claude35ModelCardAddendum}).




Since we are interested in reliability though,
we need to ensure that models can execute tasks with near-perfect accuracy.
So, we would like to know if models are truly reliable on benchmarks once they reach saturation (e.g., achieving 95\% on GSM8K), or if we should be worried about lingering model errors in the remaining 5\%, hidden among the label noise. 














\subsection{Contributions}
In this work, we demonstrate that, indeed, the remaining failures on these older benchmarks are not just label errors, and more broadly, that current benchmarks are not well equipped for testing model reliability. 
We then propose a new style of benchmarking to rigorously quantify model reliability, and make an initial effort towards constructing such benchmarks.

Before we discuss this new framework for benchmarking, it is instructive to understand what has precipitated the current state of affairs.

\vspace{-4pt}
\paragraph{The status quo of LLM benchmarking} %


The difficulty of LLM benchmarks has increased over time to track the progression of their frontier capabilities. For example, the complexity of math and science tasks that these models are evaluated on has grown from elementary and middle school level (e.g., SVAMP~\citep{patel2021nlp}, GSM8K~\citep{cobbe2021training}), to high school and college level (e.g., MATH~\citep{hendrycksmath2021}, MMLU~\cite{hendrycks2020measuring}), to graduate level problems (e.g., GPQA~\cite{rein2023gpqa})
\footnote{The original state-of-the-art accuracies on these benchmarks were reported at below $50\%$ (and as low as $6.2\%$ on MATH), and today the accuracies on all of them other than GPQA have exceeded 90\% (see Table \ref{tab:accuracy_comparison}).}.







 a result of rapidly progressing model capabilities, current benchmarks (like the ones mentioned above) follow a consistent pattlife cycledevelopment, progress, and eventual retirement thai
Specifically, a benchmark is first created to test a frontier capability for which current LLMs achieve a low accuracy (i.e., below 50\%). Models' performance on the benchmark increases over time and eventually plateaus, often around 90-95\%. At this point, the benchmark faces the same fate that we discussed earlier with older benchmarks like SQuAD and GSM8K: it is deemed ``saturated,'' with remaining errors attributed to label noise, and gradually retired as the community shifts focus to newer, more difficult benchmarks.

We believe that this life cycle has led to a gap in benchmarking: since benchmarks are discarded when models achieve a sufficient, but not perfect, performance (i.e., when they are ``saturated''), model developers are never encouraged to achieve proper reliability on them. 




\vspace{-4pt}
\paragraph{Platinum benchmarks} In order to bridge this gap and better evaluate model reliability, 
we introduce the concept of \textit{platinum} benchmarks, i.e., benchmarks that are carefully curated to minimize label errors and ambiguity such that 100\% performance is attainable. Unlike traditional benchmarks, which become "saturated" when models reach high (but imperfect) performance, platinum benchmarks are intended to remain relevant until models achieve full reliability.
While current iterations of benchmarks quantify the \textit{capabilities frontier} of LLMs---the most advanced tasks models are able to perform---platinum benchmarks allow us to identify their \textit{reliability frontier}: the most advanced tasks models can perform consistently without error.





We demonstrate our approach by constructing platinum versions of fifteen ``saturated'' benchmarks across six categories of capabilities by systematically re-labeling subsets of these benchmarks. We find that that many of the original benchmarks are indeed riddled with errors (e.g., 5\% of GSM8K). In fact, for the majority of benchmarks we investigate, more than half of model failures can be attributed to label noise.


However, hidden within this label noise, there still remain real model failures.
Indeed, despite the relative simplicity of these tasks, 
for most tasks {\em no} state-of-the-art model that we evaluate is able to pass (i.e., achieve 100\% accuracy). 
For instance, the majority of models, including o3-mini, Gemini 2.0 Flash, and DeepSeek-V3 fail at the following basic coreference resolution task from Winograd WSC~\citep{levesque2012winograd}:

\begin{quote}
\small
In the phrase, "I couldn't find a spoon, so I tried using a pen to stir my coffee. But that turned out to be a bad idea, because it got full of ink," what does "it" in "it got full of ink" refer to in the phrase? [A: The pen,  B: The coffee]
\end{quote}

\noindent
Moreover, our analysis of model failures reveals previously unidentified patterns of questions that frontier models consistently struggle with. For example, we find that when asked to identify the second of two events chronologically, multiple models instead answer with the first event over 85\% of the time.
We view our work as the first step in a new practice of quantifying LLM reliability.











