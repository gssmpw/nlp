\paragraph{Errors in machine learning benchmarks} Previous works have studied the identification of errors in machine learning benchmarks, as well as the resulting impact of these label errors on the quality of model evaluations. \citet{tsipras2020from} investigate the original ImageNet labeling process and %
release a refined, multi-label re-labeling of the ImageNet validation set. \citet{northcutt2021pervasive} identify errors in commonly used machine learning benchmarks, and then find that evaluating on benchmarks with significant rates of errors can lead practitioners to incorrectly select less performant models. \citet{bowman2021will} raise concerns similar to ours over issues in benchmarking for NLP tasks, and lay out a set of criteria that good benchmarks should satisfy.
However, they focus on overall design and social impact of benchmarks, whereas we focus specifically on better assessing model reliability. Additionally, while their work identifies similar flaws in NLP benchmarks, we propose an approach that can address some of these flaws (in particular, erroneous examples and ambiguity). %

Recently, \citet{gema2024we} released MMLU-Redux, a re-annotated subset of the MMLU benchmark~\cite{hendrycks2020measuring} created through manual assessment by 14 human experts. Our re-labeling of the MMLU high school mathematics subset actually intersects with MMLU-Redux on 100 examples. Our revised annotations align with MMLU-Redux on all but one example, on which we find that one of their human experts accidentally re-annotated a correct solution to make it incorrect\footnote{See the question marked ``wrong\_groundtruth'' here: \url{https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux/viewer/high_school_mathematics?row=52}}. This slight remaining inconsistency highlights the difficulty of avoiding errors when creating and revising benchmarks.

\paragraph{LLM failures on simple tasks} It is generally known and often discussed that LLMs fail in surprising and unintuitive ways even on simple tasks. For instance, the common example of LLMs failing on the query ``how many r's are there in the word strawberry'' has circulated both social media and news outlets \cite{silberling2024strawberry}. Previous works have investigated specific instantiations of such failures. \citet{yang2024can} find that models frequently fail on simple problems even when they can solve harder versions of these same problems, suggesting inconsistency in their reasoning abilities. 
\citet{nezhurina2024alice} raise similar concerns over breakdowns in LLM reasoning behavior by identifying a specific category of logic tasks on which current frontier LLMs fail consistently.


\paragraph{Adversarial examples}
Adversarial attacks are small, sometimes imperceptible perturbations to model inputs that can drastically change their behavior---these lightly perturbed inputs are referred to as \textit{adversarial examples}. There has been significant work studying adversarial attacks and defenses against them in computer vision (and other) domains \cite{szegedy2014intriguing,carlini2017towards,madry2018towards,papernot16jsma}, and recent work has demonstrated successful adversarial attacks on LLMs, especially in the context of breaking safety alignment~\cite{zou2023universal,xu2023llm}. As a result, one possible approach to identifying LLM failures on simple queries might be to adversarially optimize for queries that result in model failures. However, we aim for our benchmark to assess whether models can be deployed reliably on real-world tasks, and adversarial examples generally do not align with the ``corner cases'' that models might face in the real world (unless the users themselves adversarially optimize their own inputs).

