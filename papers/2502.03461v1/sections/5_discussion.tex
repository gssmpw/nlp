

Below, we provide further discussion including limitations of our work and potential avenues for future work.

\subsection{Limitations} We view our work as an initial effort towards constructing platinum benchmarks. Here, we briefly discuss the limitations of our work and areas for potential improvement by future works.
\begin{enumerate}
    \item  \textbf{Capabilities and levels of difficulty covered } Our set of fifteen benchmarks misses a number of relevant capabilities of LLMs, such as coding and tool use. Indeed, we focus on covering a wide range of difficulty levels for mathematics, but not necessarily any other capability. %

    \item \textbf{Number of examples per benchmark } Some of our revised benchmarks include as little as 100 examples, often limited by the size of the original benchmark. This limits our ability to quantify reliability with certainty: there may not be a large gap between models that have zero percent or one percent error rate on such a small sample size.

    \item \textbf{Only re-labeling errors } As we re-label all examples for which some model failed, we can be confident that every error we report is genuine. However, there may still be poorly written questions in our platinum benchmarks among those we did not revise, where, despite error or ambiguity, all models agreed with the stated ground truth.

    \item \textbf{Benchmark difficulty } As of today, frontier LLMs still fail on sufficiently simple questions that their errors can be quantified without specific expertise. For example, the most difficult task we annotate is high school level mathematics. However, once the reliability frontier of models is sufficiently advanced, expensive expert annotation will be required to construct platinum benchmarks for expert capabilities.

\end{enumerate}



\subsection{Discussion}
Here we discuss further observations and implications of our findings.

\paragraph{There is a significant gap between capability and reliability} Frontier LLMs can solve graduate-level problems (e.g., GPQA~\cite{rein2023gpqa}), but we find they can still fail on basic logic tasks and elementary-level problems. This discrepancy indicates a wide gap between the capability and reliability of frontier LLMs that does not seem to be addressed by scaling models further. 


\paragraph{How do we account for prompt brittleness?}
A common strategy for further improving performance of LLMs is to carefully adjust prompts, such as by tuning structure and wording individualized to a specific target model (i.e., prompt engineering).
But as long as the instructions are clearly stated and the task is explicitly defined, it is reasonable to expect reliable models to perform well regardless of minor variations in prompt wording. Since we are not focused on assessing how well models can follow formatting directions, however, we experimented to choose a prompting strategy that ensures our models do not fail due to output formatting errors. We do not further engineer prompts beyond this; see Appendix \ref{app:template} for our specific template. Nevertheless, it is plausible that a specific choice of prompt will affect a model's reliability. We encourage future work to both investigate prompting strategies that elicit more reliable behavior, and develop models that are less brittle to specific prompt types.

\paragraph{Framing reliability as a deployment metric}%
Quantifying reliability is a common practice for traditional deployed systems. Within the context of software, for instance, an entire dedicated field of site reliability engineering has emerged to ensure that systems are reliable, with a reliability goal measured by the number of nines of uptime (e.g., five nines, or 99.999\% uptime). We hope our work can be a first step towards building out this level of reliability quantification for LLMs.

\subsection{Conclusions } In this work, we propose the construction of carefully curated platinum benchmarks to test the reliability of LLMs, and make an initial effort to create such benchmarks by revising fifteen existing datasets. We hope that our work leads to the broader adoption and construction of platinum benchmarks for evaluating LLMs to ensure they meet the high reliability standards required in real-world applications.
