\input{tables/error_results}
\input{tables/original_acc}

Now that we have constructed a set of platinum benchmarks, we can use them to measure the reliability of frontier LLMs.

\subsection{Pinpointing the Reliability Frontier}
Within a given category of capabilities (e.g., solving math problems), there exist tasks of a wide spectrum of difficulty (e.g., ranging from a simple addition problem to graduate-level math problems).
So, in order to identify the reliability frontier with greater granularity, we need platinum benchmarks at varying levels of difficulty. Then, we can estimate a model's reliability frontier by identifying the most difficult benchmark it is able to pass (i.e., score 100\% on).


Towards this end, six of the benchmarks that we revised are mathematics benchmarks ranging in difficulty from single operations (SingleOP~\cite{roy2015reasoning}) to high school math problems (MMLU High School Math~\citep{hendrycks2020measuring}). We expect that models might exhibit reliability on sufficiently simple math problems (e.g., current models can generally complete single-digit multiplication 100\% of the time). So, this range should allow is to pinpoint the difficulty at which models begin to lose reliability.






\subsection{Findings}
In Table~\ref{tab:error_results} we report the number of errors made by each model on the revised benchmarks, and in Table~\ref{tab:original_vs_cleaned} we compare the frequency of errors on these benchmarks to the original versions before our revisions. 
We manually inspected all model errors during our revision process, so we can be confident that every model failure we report on our platinum benchmarks is genuine.
Our primary findings are:

\begin{enumerate}
    \item \textbf{Reliability challenges are significant and widespread.} Almost every model makes simple mistakes on almost {\em every} dataset, with the exception of particularly simple math datasets (SingleOP, SingleEq, MultiArith). Considering that these frontier models are now evaluated with PhD-level questions~\cite{rein2023gpqa}, it is alarming that they continue to make such simple mistakes. Several examples of these failures are presented in Appendix~\ref{app:example-failures}.
    \item \textbf{Most ``saturated'' benchmarks are too noisy to evaluate reliability.} Table~\ref{tab:original_vs_cleaned} confirms significant differences in the number of errors made by models on the original and cleaned benchmarks. For most of the original benchmarks, the majority of errors can be attributed to mislabeling or bad questions. For instance, about 75\% the errors that models make on the original SVAMP benchmark are on poorly written or mislabeled questions. Logic datasets show few or no errors, but only because these datasets were programmatically generated~\cite{srivastava2022beyond}.

    \item \textbf{More capable models are also more reliable.} Models that are considered to be more capable (e.g., GPT-4o is more capable than GPT-4o mini, Llama 3.1 405B Instruct is more capable than Llama 3.1 70B Instruct) tend to perform better on our benchmarks. Notably, these more capable models are able to perform perfectly on a few of the benchmarks, whereas our least capable models cannot do so for any benchmark.

    \item \textbf{Models' reliability varies depending on the specific capability.} We find that the o1 series and DeepSeek-R1 exhibit the greatest reliability among mathematics benchmarks, while Claude 3.5 Sonnet (Oct) exhibits the greatest reliability for commonsense reasoning. This finding emphasizes the importance of diverse reliability benchmarking; one might want to choose a different frontier LLM depending on the specific task of interest.
\end{enumerate}


\subsection{Platinum benchmarks allow us to discover new patterns of model failures} 
By investigating model errors on platinum benchmarks and their corresponding chain of thought processes, we can discover \textit{patterns} of failures. We identify two such patterns of questions that lead to consistent collapses in reasoning of frontier LLMs. We initially found an instance of each failure mode by examining models' reasoning processes on failures from our platinum benchmarks. We then verified the consistency of such failures by procedurally constructing similar examples. We outline these failure modes below, and provide further details and a more complete analysis in Appendix \ref{app:patterns}.

\paragraph{Example pattern 1: First event bias} We find that when asked: "What happened second: \{\textit{some event}\} or \{\textit{some other event}\}" given some context, three models (Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Small) almost always answer with the first event, and will even explicitly acknowledge they are identifying the first event rather than the second:

\begin{quote}
\small
\textbf{Question:} \textit{\{context paragraph\}} What happened second: Russians blocked Azov or Treaty of Constantinople? \\
\textbf{Solution:}  Treaty of Constantinople\\
\textbf{Gemini 1.5 Pro:} \ldots we can conclude that the Russians blocking Azov happened before the Treaty of Constantinople. Answer: Russians blocked Azov
\end{quote}

\noindent We procedurally generate further examples in this question format and find that each of these three models fails on over 85\% of the examples (see Figure \ref{appfig:event_bias}). Notably, this style of question elicits high failure rates in some models while having no detrimental effect on others.

\paragraph{Example pattern 2: Rounding up primes} We find that when Claude 3.5 Sonnet (June) is asked to solve grade-school math problems involving division, it often incorrectly rounds up the quotient even though the division already resulted in a whole number, leading to an answer greater than the solution by one:

\begin{quote}
\small
\textbf{Question:} The school is planning a field trip. The school has 67 classrooms. There are 66 students in each classroom in the school. If there are 6 seats on each school bus. How many buses are needed to take the trip?\\%(Solution: 737)\\
\textbf{Solution:} 737\\
\textbf{Claude 3.5 Sonnet:} \ldots To find the number of buses needed, we divide the total number of students by the number of seats per bus:
4,422 รท 6 = 737. However, since we can't have a fraction of a bus, we need to round up to the next whole number to ensure all students have a seat. Therefore, the school needs 738 buses to take all students on the trip. Answer: 738
\end{quote}

\noindent Interestingly, we find that this behavior is significantly more common when the final answer is closer to being prime (i.e., it has few divisors). For example, when we query Claude 3.5 Sonnet on similarly constructed problems with an answer that is prime, the model makes this exact rounding error 20\% of the time (see Figure \ref{appfig:rounding}).
