\section{Related Works}
% IL/IRL (shorter blob
%  While some works focus on training a visual encoding. IRL fundamentally is a distribution matching game. Distribution matching game. maxentIRL --> GAIL. Learning from a single demo.  is a good way to optimize. 
% OT (state action --> visual ones) 
% 
% Learning from a single demo. 
%   Roboclip, Rank2Reward. Focus on directly using a visual encoder to 
% But not principled! And IRL is more principle. 

% Optimal Transport and friends (state action --> visual ones) 

\textbf{Learning From a Few Video Demonstrations.} 
% IRL approaches typically learn a reward function from demonstrations and train an agent through online interactions~\cite{ng2000algorithms, abbeel2004apprenticeship, ziebart2008maximum}. 
Some works focus on training a reward model that takes videos as input and outputs scalar rewards for RL~\cite{sontakke2024roboclip, rank2reward}, but these models often require fine-tuning on task-specific data to improve performance~\cite{fu2024furlvisuallanguagemodelsfuzzy}. 
Instead, we follow IRL's formulation where we aim to match the learner and demonstration distributions, which is equivalent to optimizing the Integral Probabitliy Metrics (IPMs)~\cite{sun2019provably, swamy2021moments}.

\textbf{Optimal Transport Used In IRL.} Recent works leverage OT~\cite{peyr√©2020computationaloptimaltransport} to optimize IPMs. 
In the imitation learning setting, where a teleoperated demonstration dataset contains state-action labels, prior works can directly minimize the Wasserstein distance between the learner and the demonstration's state-action distributions~\cite{xiao2019wassersteinadversarialimitationlearning, dadashi2020primal, papagiannis2022imitation, luo2023optimal, bobrin2024alignintentsofflineimitation}.
These approaches do not work given only a single visual demonstration.
Without access to privileged states, recent works instead uses a distance function that measures the transport cost between two visual embeddings~\cite{cohen2022imitation, haldar2023teach, haldar2023watch, guzey2024see, tian2024what, liu2024imitation, fu2024robot, kedia2024oneshotimitationmismatchedexecution}.
Both types of approaches assume that a Markovian function measuring transport cost between two timesteps is sufficient. 
However, our work tackles sequence-matching problems, where agents must follow the subgoals from a temporally misaligned demonstration in the correct order. 
OT with a Markovian function fails because the true reward function now depends on the entire trajectory, a limitation that our approach addresses. 

% Existing works mitigate OT's failure modes by incorporating sequence-level constraints into its distribution-matching formulation. 
% Dynamic Time Warping (DTW)~\cite{dtw} poses hard temporal constraints to align the start and end of the learner trajectories with those of the demonstrations, and \citet{cohen2021aligning} shows successes using DTW distance to solve imitation learning problems. 
% Meanwhile, TemporalOT~\cite{fu2024robot} proposes a temporal mask to constrain how frames can be matched by assuming that the learner trajectories and demonstrations are temporally aligned. 
% However, both approaches fail under sequence-matching problems where demonstrations could be temporally misaligned. 
% All these approaches attempt matching at the frame levels.
% In contrast, our approach lifts the matching to the sequence level, proposing a principled practical reward function that can flexibly work with misaligned demonstrations.


% \subsection{Visual Rewards}
% Recent advancements in pretrained vision and multimodal models have raised interest in leveraging their features as reward sources for RL, especially when traditional reward functions are challenging to define. Prior works use reward functions 
% derived from these models to directly specify tasks \citep{mahmoudieh2022zero, rocamonde2024visionlanguage, baumli2023vision} or to provide supplementary reward signals \citep{lubana2023fomo, klissarov2023motif}. 
% Similar to our goal, \citet{sontakke2024roboclip} learn a policy for manipulation tasks from a demonstration video using a pretrained video model \citep{xie2018rethinking} to generate trajectory-level sparse rewards.
% However, embeddings derived from these models can produce noisy rewards that hinder RL training \citep{fu2024furlvisuallanguagemodelsfuzzy, wang2024rl}, and sparse rewards can fail to effectively train a policy in more complex environments. 
