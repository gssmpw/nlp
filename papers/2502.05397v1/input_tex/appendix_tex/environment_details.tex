
\section{Environment Details\label{app:env_details}}

We run experiments in two different environments: Meta-world~\cite{yu2021metaworldbenchmarkevaluationmultitask}, a manipulation environment, and \texttt{Humanoid-v4}~\citep{mujoco}, a more difficult control environment. Following prior work \cite{fu2024robot}, we test 10 different tasks in Meta-world. To show that \orca{} works in more general domains, we additionally design 4 tasks in the humanoid environment that require moving the arms of the humanoid.

\subsection{Visual Encoder}
\orca{}, as well as all baselines except \roboclip{}, requires a distance function defined on the space of images. We use a visual encoder to obtain embeddings for each image in the learner and expert trajectory, and find the pairwise distances between them to obtain the distance matrix.

In Meta-world, we follow prior work \cite{fu2024robot} in using an off-the-shelf Resnet50~\cite{he2015deepresiduallearningimage} as the visual encoder. We use the cosine distance between embeddings to produce a cost matrix. 

In Humanoid, we find that off-the-shelf visual encoders do not capture the fine-grained details necessary for the more difficult control task. Instead, we train a model to predict the joint positions of the humanoid, and use the Euclidean distance between joint predictions as the distance function. To address out-of-distribution samples, we train a separate network to predict the model confidence, which is used to scale the final rewards. For more details on the joint predictor, see \ref{subsubsec:mujoco_visual_encoder}.

\subsection{Meta-world}
\subsubsection{Tasks}
We run experiments on 10 different tasks in the \texttt{Meta-world} \cite{yu2021metaworldbenchmarkevaluationmultitask} environment. In addition to the 9 tasks in~\citet{fu2024robot}, we added \textit{Door-close}. We classify them into easy, medium, and hard based on factors like their visual difficulty, necessity for precise motor control, and interaction with other objects. For further information on the tasks, we refer the reader to \cite{fu2024robot}. Below is a brief description of the objective for each task:
\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Task} & \textbf{Difficulty} & \textbf{Success Criteria} \\
\midrule
Button-press & Easy & The button is pushed. \\
Door-close & Easy & The door is fully closed. \\
Door-open & Medium & The door is fully open. \\
Window-open & Medium & The window is slid fully open. \\
Lever-pull & Medium & The lever is pulled up. \\
Hand-insert & Medium & The brown block is inserted into the hole in the table. \\
Push & Medium & The cylinder is moved to the target location. \\
Basketball & Hard & The basketball is in the hoop. \\
Stick-push & Hard & The bottle is pushed to the target location using the stick. \\
Door-lock & Hard & The locking mechanism is engaged (pushed down). \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Training Details}

In order to improve performance across all methods, we employ two training strategies on top of the reward model and RL algorithm:

1) \textbf{Context embedding}: We use the context embedding-based cost matrix proposed in \cite{fu2024robot}, which can be interpreted as a diagonal smoothing kernel. Specifically, the distance between two frames is expressed as the average distance over the next $c_w$ learner and demonstration frames (where $c_w$ refers to the context window):
$$d_{window}(o_i^L, o_j^D) = \frac{1}{c_w} \sum_{k=1}^{c_w} d(o_{i+k}^L, o_{j+k}^D) $$
We choose a context window of length 3, which resulted in the best performance in \cite{fu2024robot}. Although a longer context window could damage performance on extremely mismatched tasks, we find that a small window helps regularize the noisiness of the visual distance metric.

2) \textbf{Timestep in agent state}: By nature of the sequence-following task, the reward at a given time step depends on the states visited by the learner in previous time steps. Thus, if the policy or value estimator cannot observe the entire trajectory, then it does not have enough information to model the reward. In practice, we find that including the current time step (as a percentage of the episode length) in the state observation of the agent allows it to reasonably estimate the value function. We emphasize that, although the agent has access to the ground truth state and time step, \textit{the reward model only sees the visual learner rollout and demonstration.} Because the purpose of this paper is to examine specific sequence-matching reward functions, we choose to use this empirical trick for our experiments, leaving further investigations into RL algorithms given non-Markovian rewards as future work.


\subsection{Humanoid}
\subsubsection{Tasks}

We use the MuJoCo \texttt{Humanoid-v4} environment~\citep{mujoco,humanoidgym}. In order to improve visual encoder performance, we modify the environment textures and camera angle, as described in \cite{rocamonde2024visionlanguage}. At the beginning of an episode, the humanoid is spawned upright, slightly above the ground, with its arms curled towards its chest. 

\input{figure_table_tex/mujoco_demos}
The humanoidâ€™s goal is to follow the motion of a demonstration trajectory within a maximum of 120 timesteps. We define 4 motions, corresponding to 4 demonstration trajectories. The humanoid must remain standing while doing all tasks. The visual demonstrations for each task are shown in \ref{fig:mujoco_demos}.
\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Task} & \textbf{Success Criteria} \\
\midrule
Arm up (L) & The left arm is raised above the head and the right arm is down. \\
Arm up (R) & The right arm is raised above the head and the left arm is down. \\
Arms out & Both arms are raised to shoulder height. (T-pose) \\
Arms down & Both arms are lowered to the side. \\
\bottomrule
\end{tabular}
\end{table}
%     \item (1) left arm up, (2) right arm up, (3) left arm out, (4) right arm out, (5) both arms out, and (6) both arms down. 
These demonstration trajectories each have a length of 10 and are generated by interpolating between the initial and final poses.
%Fig.~\ref{fig:joint_based_result}
Fig.~\ref{fig:all_mujoco_demos} shows snapshots of these trajectories. 

\input{figure_table_tex/vit_rewards}

% \subsubsection{Visual Encoder Training Details}
% \input{figure_table_tex/rl_uncertainty}
\subsubsection{Confidence-Scaled Visual Rewards.}
\label{subsubsec:mujoco_visual_encoder}
% xpos and joint state
Empirically, we found that off-the-shelf visual encoders produced noisy rewards in the Mujoco environment, as shown in Fig.~\ref{fig:vit_rewards}. This resulted in training failure regardless of the distribution-matching or sequence-matching function. To solve this problem, we train a network that predicts the joint positions of the humanoid given an image observation. To address distribution shift during RL training, we use an autoencoder to predict a model confidence score, which we use to scale the final rewards. We additionally assume access to a stability reward function, which includes a control cost and a reward for remaining standing:
\begin{equation}
\mathcal{R}_{stability} = \exp(- (h_{torso} - 1.3)^2)  - c_{ctrl}
\end{equation}
where $h_{torso}$ is the height of the humanoid torso, and $c_{ctrl}$ is a control cost provided by the environment.

Given a learner observation $o^L_t$, demonstration subgoal $o^D_j$, visual backbone $\phi$, and joint predictor $f$, we let 
\begin{equation}d(o^L_t, o^D_j) = ||f(\phi(o^L_t)) - f(\phi(o^D_j)) ||_2\end{equation}
This distance metric is used for by the distribution-matching algorithms and \orca{}. Then, we compute the confidence of the learner observation embedding $c(\phi(o^L_t))$ according to \ref{eq:conf_scaling}. This results in the final reward function:
\begin{equation}
\mathcal{R} = c(\phi(o^L_t)) * \mathcal{R}_{seq}(o_t^L, \xi^D) + \lambda \mathcal{R}_{stability}
\end{equation}


\subsubsection{Joint Predictor Training Details}

\textbf{Dataset}: We collect a dataset $\mathcal{D}=\{(o_i, j_i)\}_{i=1}^N$ containing MuJoCo images $o_i$ of various poses and corresponding joint positions $j_i$. The dataset includes in total 9{,}038 samples. To build the dataset, we utilize a set of rollout trajectories covering a set of goal reaching tasks (such as different hand poses, doing splits, etc.), We include both successful and unsuccessful trajectories.
% , generated using ground truth rewards, dreamsim runs and some old OT runs.
To ensure diversity among samples representing different stages of a trajectory, we select one frame every $k$ frames (here $k = 5$), encouraging the network to differentiate between similar images.
Given the similarity of initial trajectories, we retain the first four frames only 25\% of the time, and in those cases, select a random frame from a five-frame interval.

\textbf{Training}: To train our joint predictor $f \circ \phi$, we fully fine-tune a ResNet50 backbone \cite{he2015deepresiduallearningimage} pre-trained on ImageNet-1K \cite{deng2009imagenet} with a 3-layer MLP head that projects to the joint dimension. 
The MLP head has layers of shape (2048, 1024), (1024, 1024), and (1024, 54), where 54 represents the number of joints (18) multiplied by the dimension per joint (3). Optimization is performed over 100 epochs using SGD with learning rate .008, batch size 16, and momentum 0.875. After training the joint predictor, we freeze the backbone weights, and train a shallow autoencoder architecture with two linear layers of shapes $(d_\phi, 32)$ and $(32, d_\phi)$ using the same parameters, where $d_\phi$ is the dimension of the backbone (2048 in this case). This provides the reconstruction loss that is used for confidence estimation, as described in \ref{subsubsec:conf_scaling}
% \begin{itemize}
%     \item Every 
%     \item As the beginning of trajectories are similar, we choose to only keep the four first frames 25\% of the time.
%     \item saved xpos for joint predictions
% \end{itemize}


\input{figure_table_tex/conf_scaling}

\subsubsection{Confidence Estimate as Reward Scaling}
\label{subsubsec:conf_scaling}
The use of a joint-position predictor results in an additional challenge: in an environment with unstable dynamics, there is a large space of image observations with very different joint positions, many of which are difficult to reach through robot play. During RL training, a policy can reach a state outside of $\mathcal{D}$, resulting in noisy joint predictions and rewards. To solve this problem, prior work has estimated the epistemic uncertainty of the visual model using an autoencoder architecture \citep{andrews2016autoencoder, frey2023fasttraversabilityestimationwild}. Given that the training converges, if an embedding $\mathbf{z}$ is in the domain, then the autoencoder will be able to achieve low reconstruction loss. Contrapositively, if it has high reconstruction loss on an embedding \textbf{z'}, then \textbf{z'} must not be in domain. 

First, we train an autoencoder to reconstruct image embeddings on the offline dataset. After training coverages, we compute the final mean $\mu_{reco}$ and standard deviation $\sigma_{reco}$ of the loss on this dataset. Then, we use a formulation inspired by \citet{frey2023fasttraversabilityestimationwild} to compute a confidence score $u(\mathbf{z_i})$:
\begin{equation}
\label{eq:conf_scaling}
c(\mathbf{z_i}) = 
\begin{cases} 
1, & \text{if} \mathcal{L}_{reco}(\mathbf{z_i}) < \mu_{reco}
\\
 \exp{\left(-\frac{(\mathcal{L}_{reco}(\mathbf{z_i}) - \mu_{reco})^2}{2(\sigma_{reco}k_\sigma)^2}\right)}, & \text{otherwise}
\end{cases}
\end{equation} 
% \begin{equation}
% \label{eq:conf_scaling}
% c(\mathbf{z_i}) = 
% \begin{cases} 
% 1, & \text{if} \mathcal{L}_{reco}(\mathbf{z_i}) < \mu_{reco}
% \\
% \exp{\left(\frac{(\mathcal{L}_{reco}(\mathbf{z_i}) - \mu_{reco})^2}{2(\sigma_{reco}k_\sigma)^2}\right)}, & \text{otherwise}
% \end{cases}
% \end{equation} 
where $k_\sigma$ is a hyperparameter that controls the spread of the confidence function. In our experiments, we set $k_\sigma=2$. 



We observe that the reconstruction losses over a set of trajectories sampled from partially trained policies are skewed towards low confidence. In-domain images tend to have low reconstruction loss that is tightly clustered around the average offline loss, while out-of-domain images tend to have higher and more spread out loss. Figure \ref{fig:conf_scaling} shows a qualitative example of how uncertainty scaling fixes incorrect reward predictions on out-of-distribution images. The shape of the uncertainty scaled reward curve better matches the ground truth.

% Figure \ref{fig:rl_uncertainty} shows that confidence decreases during RL training, indicating that there is distribution shift. We additionally visualize the confidence function $c(\mathbf{z})$, which we define as $1 - (u(\mathbf{z}) + \epsilon)$. The confidence uses the mean and standard deviation of the offline uncertainties and $k=2$. This demonstrates that the confidence function (and thus uncertainty function) is appropriate for the uncertainty distribution encountered during RL.  



