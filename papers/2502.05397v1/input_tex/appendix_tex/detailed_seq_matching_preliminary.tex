\section{Detailed Analysis of \orca{}}

We prove propositions \ref{prop:ordering} and \ref{prop:progress}, showing that the \orca{} reward encourages the agent to complete all subgoals in the correct order, thus addressing the limitations of baselines that use frame-level matching.
First, we derive basic properties of the \orca{} coverage matrix. Then, we restate the propositions and prove them using these properties.

\subsection{Proofs of \orca{} Desiderata}
\begin{lemma}
For all t, $C_{t, j} = \max_{i=1}^{t} C_{i, j-1} P_{i, j}$.
\label{lemma:induction}
\end{lemma}

\begin{proof}
We prove this by induction on $t$.

\textbf{Base case:} For $t=1$, $C_{1, j} = C_{1, j-1} P_{1, j}$ by (\ref{eq:coverage_recursive}), which satisfies the statement.

\textbf{Inductive step:} Assume $C_{t, j} = \max_{i=1}^t C_{i, j-1} P_{i, j}$ holds for $t$. For $t+1$, by the recursive definition (\ref{eq:coverage_recursive}):
$$C_{t+1, j} = \max \{ C_{t, j}, C_{t+1, j-1} P_{t+1, j} \}.$$

Substituting the inductive hypothesis:
$$C_{t+1, j} = \max \{ \max_{i=1}^t C_{i, j-1} P_{i, j}, C_{t+1, j-1} P_{t+1, j} \} = \max_{i=1}^{t+1} C_{i, j-1} P_{i, j}.$$

Thus, the statement holds for $t+1$. By induction, the lemma is proven.
\end{proof}

\begin{corollary}
\label{coro:max_prob}
If at time $t$, $\max_{i=1}^{t} P_{i, j} = P_{t, j} $, then $C_{t, j} = C_{t, j-1}P_{t,j} $.
\end{corollary}
\begin{proof}
By the non-decreasing property of coverage along the learner axis (\ref{eq:coverage_recursive}),
\begin{equation}
\max_{i=1}^{t} C_{i, j-1}  = C_{t, j-1}.
\end{equation}
By lemma \ref{lemma:induction},
\begin{equation}
C_{t, j} = C_{t, j-1} P_{t, j}.
\end{equation}
\end{proof}

\begin{corollary}
\label{coro:coverage}
If two trajectories $\xi^{+}$ and $\xi^{-}$ are identical, except a subgoal $\od_{j}$ is covered at time $t$ in $\xi^{+}$ and is not covered in $\xi^{-}$, then $C^{+}_{t, j} > C^{-}_{t, j}$.
\end{corollary}
\begin{proof}
$\xi^{+}$ achieves coverage of $\od_{j}$ at time $t$, so by corollary \ref{coro:max_prob}:
\begin{equation}
C^{+}_{t, j} = C^{+}_{t, j-1} P^{+}_{t, j} > C^{+}_{t-1, j}.
\end{equation}
Since the trajectories are identical prior to $t$,
we have:
\begin{equation}
C^{+}_{t-1, j} = C^{-}_{t, j-1}.
\end{equation}

Moreover, $P^{+}_{t, j} > P^{-}_{t, j}$ implies
\begin{equation}
C^{+}_{t, j-1} P^{+}_{t, j} > C^{-}_{t, j-1} P^{-}_{t, j}.
\end{equation}

It follows that
\begin{equation}
C^{+}_{t, j-1} P^{+}_{t, j} > \max\{ C^{-}_{t, j-1}, C^{-}_{t, j-1} P^{-}_{t, j} \}. 
\end{equation}

\text{We conclude from the coverage definition (\ref{eq:coverage_recursive})}:
\begin{equation}
C^{+}_{t, j} > C^{-}_{t, j}.
\end{equation}
\end{proof}

\begin{proposition}[\textbf{\orca{} enforces subgoal ordering} (Restating Prop.~\ref{prop:ordering})]
Let $\xi^{-}$ be a trajectory that is out of order; specifically, there exists a subgoal $\od_{j}$ such that $\od_{j}$ is occupied at time $t$ and $\od_{j-1}$ is not yet covered. Let $\xi^{+}$ be a trajectory that is identical to $\xi^{-}$, except that it covers $\od_{j-1}$ before time $t$. Then, $\mathcal{R}_{ORCA}(o^{+}_t, \xid) > \mathcal{R}_{ORCA}(o^{-}_t, \xid)$.
\end{proposition}

\begin{proof} \label{proof:ordering}
Because $\od_j$ is occupied at time $t$ in $\xi^+$ ($\max_{i=1}^{t} P^+_{i, j} = P^+_{t, j}$),  by corollary \ref{coro:max_prob},
\begin{equation} \label{eq:coverage1}
C^{+}_{t, j} = C^{+}_{t, j-1}P^{+}_{t, j}.
\end{equation}

According to the DP recurrence relation (\ref{eq:coverage_recursive}), there are two cases for $C^{-}_{t, j}$:

\textbf{Case 1.} $C^{-}_{t, j} = C^{-}_{t-1, j}$

By lemma \ref{lemma:induction}, 
\begin{equation}
\label{eq:order1}
C^{-}_{t-1, j} = \max_{i=1}^{t-1} C^{-}_{i, j-1} P^{-}_{i, j}.
\end{equation}
By corollary \ref{coro:coverage}, and the fact that coverage is nondecreasing along the demonstration,
\begin{equation}
\label{eq:order2}
C^{+}_{t, j-1} > C^{-}_{t, j-1} \geq \max_{i=1}^{t} C^{-}_{i, j-1}.
\end{equation}
Because $\xi^{+}$ and $\xi^{-}$ both occupy subgoal $\od_j$ at time $t$:

\begin{equation}
\label{eq:order3}
P^{+}_{t, j} = \max_{i=1}^{t} P^{-}_{i, j}.
\end{equation}

Multiplying (\ref{eq:order2}) and (\ref{eq:order3}) lets us establish a bound on (\ref{eq:order1}):
\begin{equation}
    C^{+}_{t, j-1}P^{+}_{t, j} > \max_{i=1}^{t} C^{-}_{i, j-1} \max_{i=1}^{t} P^{-}_{i, j} \geq
    \max_{i=1}^{t-1} C^{-}_{i, j-1} P^{-}_{i, j}.
\end{equation}

Substituting (\ref{eq:coverage1}), we get:
\begin{equation}
C^{+}_{t, j}  > C^{-}_{t, j}.
\end{equation}

\textbf{Case 2.} $C^{-}_{t, j} =  C^{-}_{t, j-1}P^{-}_{t, j}$

Because $P^{+}_{t, j} = P^{-}_{t, j}$, and by corollary \ref{coro:coverage},
\begin{equation}
C^{+}_{t, j-1} P^{+}_{t, j} > C^{-}_{t, j-1}P^{-}_{t, j}.
\end{equation}

Substituting (\ref{eq:coverage1}), we get:
\begin{equation}
C^{+}_{t, j} > C^{-}_{t, j}.
\end{equation}
Since $C^{+}_{t, j} > C^{-}_{t, j}$ in both cases, and the trajectories are otherwise identical,
\begin{equation}
\mathcal{R}_{ORCA}(o^{+}_t, \xid) > \mathcal{R}_{ORCA}(o^{-}_t, \xid).
\end{equation}
\end{proof}

\begin{proposition}[\textbf{\orca{} enforces subgoal coverage.} (Restating Prop.~\ref{prop:progress})]
Let $\xi^{-}$ be a trajectory that occupies $\od_{j-1}$ at time $t-1$ and continues to occupy $\od_{j-1}$ at time $t$, instead of progressing towards $\od_{j}$. Let $\xi^{+}$ be an identical trajectory that progresses towards $\od_{j}$ at $t$, and assume that neither trajectory has been closer to $\od_{j}$ before. Then, $\mathcal{R}_{ORCA}(o^{+}_t, \xid) > \mathcal{R}_{ORCA}(o^{-}_t, \xid)$.
\end{proposition}

\begin{proof}
\label{proof:progress}
At time $t$, $\xi^{+}$ moves closer to $\od_{j}$ than it has previously been. Thus, by corollary \ref{coro:max_prob}:
\begin{equation} \label{eq:coverage2}
C^{+}_{t, j} = C^{+}_{t, j-1} P^{+}_{t, j} > C^{+}_{t-1, j}.
\end{equation}

There are two cases for $C^{-}_{t, j}$.

\textbf{Case 1.} $C^{-}_{t, j} = C^{-}_{t-1, j}$

Because $\xi^{+}$ is identical to $\xi^{-}$ prior to $t$,
\begin{equation}C^{+}_{t-1, j} = C^{-}_{t-1, j}\end{equation}.

Thus, by (\ref{eq:coverage2}),
\begin{equation}
C^{+}_{t, j}  > C^{+}_{t-1, j}  = C^{-}_{t-1, j} = C^{-}_{t, j}.
\end{equation}

\textbf{Case 2.} $C^{-}_{t, j} =  C^{-}_{t, j-1}P^{-}_{t, j}$

Since $\xi^{+}$ is identical to $\xi^{-}$ prior to $t$, and at time $t$ neither improves its coverage of subgoals prior to $\od_{j}$:
\begin{equation}
C^{+}_{t, j-1} = C^{-}_{t, j-1}.
\end{equation}
However, because $\xi^{+}$ moves closer to $\od_{j}$ than $\xi^{-}$ at time t,
\begin{equation}
P^{+}_{t, j} > P^{-}_{t, j}.
\end{equation}
We conclude that
\begin{equation}
C^{+}_{t, j} = C^{+}_{t, j-1}P^{+}_{t, j} > C^{-}_{t, j-1}P^{-}_{t, j} = C^{-}_{t, j}.
\end{equation}

Since $C^{+}_{t, j} > C^{-}_{t, j}$ in both cases, and the trajectories are otherwise identical,
\begin{equation}
\mathcal{R}_{ORCA}(o^{+}t, \xid) > \mathcal{R}_{ORCA}(o^{-}_t, \xid).
\end{equation}
\end{proof}

\subsection{Toy Examples of \orca{} Overcoming Failure Cases of Existing Approaches\label{app:toy_orca_success}}
We present complete figures showing how \orca{} overcomes OT's failure to enforce subgoal ordering and DTW/TemporalOT's failure to enforce full subgoal coverage. The distance function between each state is the Manhattan distance, which is Markovian.  

\input{figure_table_tex/appendix_qualitative_examples/ot_failure_detailed}
In Fig~\ref{fig:ot_fail_full}, the suboptimal learner trajectory completes the demonstration subgoals in the wrong order compared to the optimal learner trajectory that completes the task in the correct order. Because OT treats the learner and trajectory distribution as two unordered sets, and the distance function does not encode any temporal information, it fails to penalize the suboptimal trajectory, giving both equally high rewards. In contrast, because DTW enforces temporal constraint in its alignment, the final alignment is the same for both trajectories, resulting in a lower DTW reward for the suboptimal trajectory. Similarly, \orca{} measures the probability that \emph{all subgoals are covered in the correct order}. Although the suboptimal learner trajectory perfectly occupies the third subgoal at time 3, because it has not occupied the second subgoal, its overall ordered coverage is still low, thereby penalizing the suboptimal learner trajectory for covering out of order.

\input{figure_table_tex/appendix_qualitative_examples/dtw_failure_detailed}
In Fig.~\ref{fig:dtw_fail_full}, the suboptimal learner trajectory stalls at the second subgoal while the optimal trajectory makes consistent progress towards covering all subgoals. DTW fails because it does not constrain the number of learner frames that can be matched with each subgoal. Consequently, its alignment matrix matches most learner frames to the second subgoal, which has no cost since they perfectly occupy it, resulting in equally high DTW rewards for both trajectories. In contrast, because \orca{} rewards depends on \emph{all} subgoals to be covered, the suboptimal trajectory receives low rewards for most timestep since it has not covered the final subgoal.

\input{figure_table_tex/appendix_qualitative_examples/tot_failure_detailed}
In Fig.~\ref{fig:tot_fail_full}, the video demonstration is temporally misaligned because the agent can move one cell at a time, and the subgoals would require the agent to take multiple timesteps to reach. The suboptimal learner trajectory is slow, spending 2 timesteps at earlier subgoals and failing to solve the tasks in time. Meanwhile, the optimal learner trajectory makes consistent progress and succeeds. Because TemporalOT assumes that the learner and demonstration demonstrations are temporally aligned, the mask window that it then defines causes the coupling matrix to also approximate a diagonal matrix. Such coupling matrix would encourage the agent to spend equal amount of time matching each subgoal, thereby rewarding the suboptimal trajectory that does so perfectly even though they did not finish the task. In contrast, \orca{}'s rewards depend on \emph{all} subgoals to be covered, so the suboptimal trajectory receives low rewards for all timesteps since it has not covered the final subgoal.

% \begin{definition}
%     Let $\R^*$ be the ground-truth reward function. 
%     Let $\tilde{\R}$ be the sequence matching reward function.
%     % Let $\hat{\pi}$ be some policy in the policy class $\hat{\pi} \in \Pi$. 
%     $\tilde{\R}$ is hackable with respect to $\R^*$ if there exist $\pi$ and $\pi' \in \Pi$ such that
%     \begin{equation*}
%         J_{\R^*}(\pi) < J_{\R^*}(\pi') \And J_{\tilde{\R}}(\pi) > J_{\tilde{\R}}(\pi').
%     \end{equation*}
% \end{definition}


% In the following proofs, we show that DTW and OT rewards are hackable with respect to the ground truth reward. Specifically, let $\pi^*$ be a deterministic policy that is obtained by optimizing the ground truth reward $\R^*$, resulting in a trajectory $\xi^{L*}$ that successfully reaches all the demonstration states in the correct order. We show that there exists a policy $\pi'$ such that $J_{\R^*}(\pi') < J_{\R^*}(\pi^*)$ and $J_{\tilde{\R}}(\pi') > J_{\tilde{\R}}(\pi^*)$.


% \textbf{Detailed Problem Setup.} 
% \wh{IN PROGRESS}
% \begin{proposition}

% The OT reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.

% \end{proposition}

% \begin{proof}

% Let $\pi'$ be a deterministic policy that produces $\xi^{L-}$, a trajectory with the same embedded occupancy distribution as $\xi^{L*}$. This occurs if $\xi^L*$ and $\xi^{L-}$ reach the same states for the same amount of time. For example, $\xi^{L-}$ could complete all the states in the wrong order, as in the left of Fig.~\ref{fig:ot_fail_full}. If the demonstration has twice as many frames as the learner has steps, $\xi^{L-}$ could complete all the demonstration states during the first half of the trajectory, and then complete them backwards during the second half of the trajectory, as in \wh{ref OT humanoid going backwards fig}.

%  \citet{chen2023neuralapproximationwassersteindistance} show that the Wasserstein distance is a function of the embedded occupancy distribution of its point sets, so it is invariant to permutations in the ordering of these points. Thus, for the trajectories $\xi^{L*}$ and $\xi^{L-}$, the Wasserstein distance is the same. Consequently, the cumulative OT rewards are also the same, even though $\xi^{L-}$ is not a successful rollout.
% \end{proof}

% Note that DTW can overcome this limitation because of its time consistency constraints, as shown in the right figure of Fig.~\ref{fig:ot_fail_full}. 
% It aligns the two trajectories in a time-consistent way, ensuring that $\xi^{L*}$ (the trajectory in the correct order) is rewarded more than $\xi^L$ (the trajectory in the incorrect order), thereby overcoming the failure case of OT.



% \subsection{DTW Reward Hacking}

% \textbf{Detailed Problem Setup.} 

% \textbf{Assumption 1:} $\mathcal{S}$ is a finite discrete state space: $S \subset \mathbb{Z}^{N}$ and $\mathcal{S}$ is bounded in all dimensions. $\mathcal{A}$ is an action space that allows the agent to move 1 space in any cardinal direction. 2D-Navigation is an example of such an environment. 

% \textbf{Assumption 2}: $d:S\times  S \rightarrow \mathbb{R}$ is a metric on $S$. For simplicity, we assume $d$ is the Manhattan distance, but the proof can be extended to other distance functions. 

% % The proof proceeds by constructing an adversarial trajectory $\xi^{L-}$ and showing that its cost is bounded above by the cost of $\xi^{L*}$.    
% % \textit{Constructing the Adversarial Trajectory.}  


% \begin{proposition}
% The DTW reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.
% % and $\pi' \in \argmax J(\pi, \tilde{\R})$. 
% \end{proposition} 
% \begin{proof}
% Let $\tilde{\R}$ be the $\dtw$ reward function given a learner trajectory $\xi^L$ of length T, a reference trajectory $\xid$ of length T', and a learner timestep t:
% $$\tilde{\R}(\xi^L, \xid, t) = -\frac{1}{T'T} A^*_t(\xi^L, \xid) \cdot D_t(\xi^L, \xid)$$
% where $\mathbf{A}^*_t(\xi^L, \xid)$ is the DTW assignment between $\xi^L$ and $\xid$ at learner step t, and $\mathbf{D}_t(\xi^L, \xid)$ is the distance vector between $\xi^L$ and $\xid$ at learner step t. 

% Let $\pi'$ be a deterministic policy that produces the adversarial learner trajectory $\xi^{L-}$ as follows:
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item Visit all the same states as $\xi^{L*}$ until reaching the second-to-last state of the demonstration trajectory, $\xid_{T'-1}$.
%     \item Stay in $\xid_{T'-1}$ until the final timestep.
%     \item Take an action towards the final demonstration state $s^D_{T'}$ in the final timestep.
% \end{enumerate}

% The first part of the proposition follows by definition: $\pi'$ does not reach every reference state and $\pi^*$ reaches them all in the correct order, so $J_{\R^*}(\pi') < J_{\R^*}(\pi^*)$. 

% For the second part of the proposition, we compute the returns of the policies. The distance matrices $\textbf{D}$ for the two trajectories are as follows, where $d_{rem} = d(\xid_{T'-1}, \xid_{T'})$

% \begin{center}
% $\mathbf{D}(\mathbf{\xi^{L*}}, \xid)$ = 
% $\begin{bmatrix}
% \mathbf{\mathbf{D}(\xi^{L*}_\leq, \xid)} & ? & ?\\
% ? & \mathbf{0} & d_{rem} \\
% \vdots & \vdots & \vdots \\
% ? & d_{rem}-1 & \mathbf{1}\\
% ? & d_{rem} & \mathbf{0}
% \end{bmatrix}$
% \hspace{1cm} % Add space between the two matrices
% $\mathbf{D}(\mathbf{\xi^{L-}}, \xid)$ = 
% $\begin{bmatrix}
% \mathbf{\mathbf{D}(\xi^{L*}_\leq, \xid)} & ? & ?\\
% ? & \mathbf{0} & d_{rem} \\
% \vdots & \vdots & \vdots \\
% ? & \mathbf{0} & d_{rem} \\
% ? & 1 & d_{rem}-1
% \end{bmatrix}$
% \end{center}

% Before $\xi^{L-}$ and $\xi^{L*}$ reach $\xi_{T'-1}^D$, they are identical, and thus accumulate the same reward. To simplify the proof, we refer to the time when they reach $\xi_{T'-1}^D$ as $t=0$ (effectively removing the identical parts of the trajectory). The optimal policy makes continuous progress for T timesteps, at which point it reaches $\xi_{T'}$. For the first half of the timesteps, it is closest to $\xi_{T'-1}$ (and is thus assigned there), and for the second half, it is closest to $\xi_{T'}$. 
% \begin{align*}
%     J_{\tilde{R}}(\pi^*) &= -\sum_{t=1}^{T/2} t \gamma^t + t \gamma^{T-t} \\
%     & = -\gamma - \gamma^{T-1} -\sum_{t=2}^{T/2} t \gamma^t + t \gamma^{T-t}.
% \end{align*}

% For $1 \leq t  < T$, $\xi^{L-}$ gets 0 reward because it remains in $\xi_{T'-1}^D$. The last learner frame $\xi^{L-}_T$ must be assigned to $\xi_{T'}^D$ to satisfy the DTW constraint, and it makes 1 move of progress (out of T necessary moves), thus achieving a discount reward of $-\gamma^T(T-1)$.
% % Adversarial Policy Return:
% \begin{align*}
%     J_{\tilde{R}}(\pi') &= -\gamma^T(T-1) = \gamma^T - 2 \gamma^T - \sum_{t=2}^{T/2} 2 \gamma^T \\
%     &= -\gamma^T - \sum_{t=2}^{T/2} 2 \gamma^T. \\
% \end{align*}

% Note that $-\gamma^T > -\gamma - \gamma^{T-1}$, and that $  - \sum_{t=2}^{T/2} 2 \gamma^T > -\sum_{t=2}^{T/2} t \gamma^t + t \gamma^{T-t}$.
% It follows that $J_{\tilde{R}}(\pi') > J_{\tilde{R}}(\pi^*)$. Thus, the DTW reward is hackable with respect to the ground truth reward.

% \end{proof}


% \subsection{TemporalOT Reward Hacking\label{proof:suboptimal_temporal_ot}}
% \textbf{Detailed Problem Setup.} 

% Let $\xid$ be a demonstration trajectory and $\xi^{L*}$ be the optimal learner trajectory that correctly follows this demonstration. Let $d(\xid_{t}, \xid_{j})$ be a distance metric on the observation manifold. 

% \textbf{Assumption 1:} $\mathcal{S}$ is a state space.

% \textbf{Assumption 2}: $d:S\times  S \rightarrow \mathbb{R}$ is a metric on $S$. 
% \begin{proposition}
% The TemporalOT reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.
% % and $\pi' \in \argmax J(\pi, \tilde{\R})$. 
% \end{proposition} 

% \begin{proof}
% The optimal TemporalOT coupling matrix \cite{fu2024robot} is defined as:
% $$
% \mu^* = \arg\min_{\mu} \langle M \odot \mu, \hat{C} \rangle_F - \epsilon \mathcal{H}(M \odot \mu), \quad \text{s.t.} \quad \mu \mathbf{1} = \mu^\top \mathbf{1} = s,
% $$
% where $\mathcal{H}(\mu)$ is the entropy of the coupling matrix $\mu$, and $M$ is the mask matrix. For a high $\epsilon$, the entropy term dominates, and the reward is optimized when $M \odot \mu$ is uniformly distributed. Since M is a predefined mask, $\mu$ must be uniformly distributed over the mask. Applying the definition of the variant of the diagonal mask used in \citet{fu2024robot}:
% $$
% \mu^*_{t,j} =
% \begin{cases} 
% \frac{1}{s} & \text{if } j \in [t - k_m, t + k_m] \\
% 0 & \text{otherwise.}
% \end{cases}
% $$
% By definition of the reward,
% $$\tilde{R}_t =  - \sum_{t'=1}^{T'} (\mu^* \odot D)_{t, t'} $$
% $$\tilde{R}_t = - \frac{1}{s} \sum_{i=-k_m}^{k_m} d(\xi^L_t, \xid_{t+i})$$

% By the above definition, an agent that stays within the mask window for the entire trajectory will have a higher reward than one that exits the mask window for some period. If $k_m > 0$, an unsuccessful policy  $\pi'$ can always be constructed that stays within the mask window for the entire trajectory (since $k_m > 0$, it can be within the mask window without reaching the final demonstration state). If a successful policy $\pi^*$ is outside the mask window for at least one learner step, then:
% $$J_{\tilde{R}}(\pi') > J_{\tilde{R}}(\pi^*)$$
% This happens when $\pi$ moves significantly faster or slower than the demonstration (for example, if the agent requires more than $k_m$ steps to move between two subgoals). Under these conditions, the TemporalOT reward is hackable with respect to the ground truth reward.
% \end{proof} 
