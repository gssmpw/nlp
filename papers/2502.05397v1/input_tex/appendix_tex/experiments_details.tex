

% \textbf{Privileged Distance Metric.} This experiment investigates the effectiveness of the sequence-following reward function in isolation, so we assume access to a distance metric based on privilege states. The 2D-Navigation environment's distance metric $d(s^R_t, s^D_{t'})$ is the shortest path from the agent location in $s^R_t$ to its location in $s^D_{t'}$. The Humanoid environment uses a joint-based euclidean distance metric $d(s^R_t, s^D_t)$ between the learner trajectory's arm joint position and demonstration trajectory's arm joint position. We investigate the visual distance metric in Section~\ref{exp:visual}.

\section{Experiments}

\subsection{RL Policy Details.\label{app:rl}}

% In the 2D-Navigation task, the policy is trained with PPO for 100k steps. The hyperparameters used are: \todo{placeholder}.
 
% For the Humanoid control task, the policy is trained with SAC for over 2 million steps. 

% For both tasks, the evaluation is done across 8 seeds randomly chosen.

% Table~\ref{tab_hyperp} outlines the specific hyperparameters used for the tasks in both environments.
In the Meta-world environment, we use an identical setup and hyperparameters to~\citet{fu2024robot}, training the policy with DrQ-v2 over 1 million steps. We also follow~\citet{fu2024robot} to repeat a policy's predicted action 2 times, which effectively shortens the episode length by half. This effect is applied both in RL training and generating demonstrations from hand-engineered policies. In MuJoCo, the policy is trained with SAC over 2 million steps, and we slightly modify the parameters from \citet{rocamonde2024visionlanguage} to adapt to the shorter training period (2 million steps vs 10 million steps). All policies are trained with ground truth state observations, but we emphasize that the reward function only sees visual observations.


\begin{table}[h]
\centering
\caption{Training hyperparameters used for experiments on both environments.}\label{tab_hyperp}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Meta-world (DrQ-v2)} & \textbf{Humanoid (SAC)} \\
\midrule
Total environment steps   & 1,000,000            & 2,000,000      \\
Learning rate             & 1e-4   & 1e-3 \\
Batch size                & 512                 & 256 \\
Gamma ($\gamma$)          & 0.9                & 0.99           \\
Learning starts           & 500                & 6000          \\
Soft update coefficient   & 5e-3  & 5e-3 \\
Actor/Critic architecture & (256, 256)         & (256, 256)    \\
Episode length            & 125 or 175         & 120            \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Approach Details.\label{app:baselines}}
We describe the hyperparameter details for each baseline as needed. 
\begin{itemize}
    \item \orca{}. We use $\lambda=1$ for temperature turn. For Meta-world tasks, we initialize the policy by loading \tot{}'s checkpoint at 500k steps before training on ORCA rewards. For Humanoid tasks, we load the checkpoint at 1M steps because the total training steps is 2M.
    \item \ot{}. We solve the entropic regularized optimal transport problem shown in (\ref{eq:ot_mu_star}). In Meta-world, the weight on the entropic regularization term is $\epsilon=1$, and in Humanoid, it is $\epsilon=0$.
    \item \tot{}. TemporalOT \cite{fu2024robot} was originally designed for learner and reference sequences of the same length. In this paper, we compare against a slightly stronger version of TemporalOT, which allows for a learner that is linear in the speed of the expert (either faster or slower). Specifically, we mask with a windowed diagonal matrix stretched along the longer of the learner and demonstration axis. Because TemporalOT also formulates the entropic regularized OT problem (\ref{eq:tot_mu_star}), the entropic weight is also $\epsilon=1$. We use a varying mask window size, depending on the reference length. \citet{fu2024robot} used $k_m=10$ for metaworld matched demos. We use the same for matched demos, and for mismatched demos, we let $k_m \approx \lceil\frac{|\tilde{\xi}|}{10}\rceil$.
    \item \threshold{}. This is a hand-engineered reward function with simple conditionals. \threshold{} contains two terms: the number of subgoals completed and the reward with respect to the current subgoal to follow. It initializes the subgoal to follow as the first subgoal, and it starts tracking the next subgoal, when the current subgoal's reward is above a predefined threshold. We set the threshold as $0.90$ for all Meta-world tasks and $0.70$ for all Humanoid tasks because it is a more challenging environment where it is difficult to achieve high rewards.
    \item \roboclip{}. \roboclip{}~\citep{sontakke2024roboclip} uses a pretrained video-and-language model \citep{xie2018rethinking} to directly encode the video. For videos with more than 32 frames, \roboclip{} downsample them to 32 frames before passing them to the encoder. 
    It defines the reward for the last timestep as the cosine similarity between the learner video's and the demonstration video's embeddings, while all previous timesteps have zero as the reward. Due to Python version issues, we train all \roboclip{} policies using SAC~\cite{haarnoja2018softactorcriticoffpolicymaximum}, following the setup and code base from \citet{sontakke2024roboclip}.
\end{itemize}

\subsection{Varying Temporal Alignment Level Experiment Details.\label{app:random_mismatch_setup}}
We study the effect of how increasingly misaligned demonstrations would affect \orca{} and \tot{}'s performances.
We identify two types of temporal misalignment: either the demonstration contains pauses and is slower, or the demonstration accelerates through a segment and is faster.
We randomly perturb the original demonstrations of three Meta-world tasks (\textit{Door-open}, \textit{Window-open}, \textit{Lever-pull}), generating 6 demonstrations per task per speed type (i.e, slow or fast).
Concretely, we first evenly split the original demonstration into five segments.
To get the first three randomly perturbed demonstrations, we randomly select \emph{one} segment and randomly change their speed.
We can speed up a segment by 2, 4, 6, 8, or 10 times, and we can slow down a segment by 2, 3, 4, 5, or 6 times. 
To get the rest of the randomly perturbed demonstrations, we randomly select \emph{three} segments and randomly change their speed.
Then, for each task and speed type, we categorize 3 of the perturbed demonstrations as having ``Low" level of misalignment and the other 3 as having ``High" level of misalignment by ranking the demonstrations based on the mean absolute deviation between the segment length.
The more varied the segment lengths are with respect to each other, the more temporally misaligned this demonstration is. 
Each perturbed demonstration is trained on a random seed, and we ensure that \orca{} and \tot{} are trained on that same seed for fair comparison. 





\subsection{Additional Experiment Results}
\label{app:additional_experiments}
\textbf{Temporally Misaligned Experiment.}

\input{figure_table_tex/metaworld_mismatched_all_training_curves}
Fig. \ref{fig:meta_all_training} shows training curves of all methods given temporally misaligned demonstrations. The \orca{} plot diverges directly from \tot{} at 500k steps because it is initialized with these \tot{} policies, and trained for an additional 500k stops. In tasks where \tot{} achieves some success by this point steps, \orca{} performs at least as well as \orcanp{}, because it can take advantage of the better initialization. In tasks like basketball and door-open, where \tot{} is unsuccessful after 500k steps, \orcanp{} performs better because it is trained with the \orca{} reward for more steps. All methods fail to learn the push task, which we hypothesize is due to the visual encoder.

\input{figure_table_tex/mujoco_all_training_curves}
\input{figure_table_tex/mujoco_table_full}
Fig.~\ref{fig:humanoid_all_training} shows training curves of all methods given interpolated demonstrations in the humanoid environment, which are naturally temporally misaligned. Tab. \ref{tab:mujoco_table_full} shows the final cumulative reward of all methods. Across all tasks, \orcanp{} performs the best. In general, \orcanp{} performs better than \orca{} because \tot{} achieves near 0 performance, and \orca{} cannot reap the benefits of better initialization. All methods perform poorly on arms down because it is an extremely unstable position. 

Fig.~\ref{fig:humanoid_qualitative} shows a qualitative comparison between frame-level matching approaches and ORCA on the left arm up task. ORCA quickly solves the task, while the other approaches have the failure modes described in \ref{sec:dist_fail}.

\input{figure_table_tex/appendix_qualitative_examples/mujoco_qualitative}


\textbf{Temporally Aligned Experiment.}
\input{figure_table_tex/metaworld_matched_all_training_curves}
\input{figure_table_tex/metaworld_matched_table}
Fig. \ref{fig:meta_matched_all_training} shows the training curves for all methods in Meta-world with temporally aligned demonstrations, and Tab.\ref{tab:metaworld_matched} shows the final cumulative reward of all methods. Across all tasks, \orca{} performs the best. Since \tot{} achieves good baseline performance on most of these tasks, \orca{} is able to take advantage of pretraining, and outperforms \orcanp{} on most tasks.

\subsection{Frame-Level Failure Modes in Meta-world\label{app:metaworld_qual}}
We show qualitative examples of the failure modes of \ot{}, \tot{}, and \dtw{} described in Sec. \ref{sec:dist_fail} occuring during Meta-world policy training. 

\textbf{OT and TemporalOT Fail to Enforce Ordering.}

Fig. \ref{fig:meta_order_fail} shows how \ot{} and \tot{} with a large $k_m$ both fail to enforce subgoal ordering, thus rewarding trajectories that do not complete subgoals in the correct order.
\input{figure_table_tex/appendix_qualitative_examples/metaworld_order_fails}

\textbf{DTW Fails to Enforce Subgoal Coverage.}
 
Fig. \ref{fig:meta_dtw_fail} shows how \dtw{} fails to enforce full subgoal coverage, getting stuck in an intermediate subgoal. 
\input{figure_table_tex/appendix_qualitative_examples/metaword_dtw_fails}

\subsection{The importance of pretraining for \orca{}.}

Fig. \ref{fig:meta_no-pretrain_fail} shows a qualitative example of how pretraining on \tot{} reward helps initialize \orca{} in the correct basin. Without pretraining, \orcanp{} gets partial credit for earlier subgoals, and gets stuck in a local minimum of immediately pushing the bottle without picking up the stick. Meanwhile, \tot{} on its own moves slowly and does not pick up the stick. \orca{} picks up the stick and completes the task quickly.

\input{figure_table_tex/appendix_qualitative_examples/metaworld_no-pretrain_fails}

% \subsection{Humanoid Failure Mode Examples}
% \textbf{OT Failure in the Humanoid Environment\label{app:ot_humanoid_exp_fail}}

% \input{figure_table_tex/ot_humanoid_failure_case_app}

% Figure~\ref{fig:ot_fail_case_app} shows an example of failure case for OT reward in the Mujoco \texttt{Humanoid-v4} environment.

% \subsection{Toy Environment Experiments\label{app:toy}}

% Figure \ref{fig:toy_rollouts_appendix} shows example trajectories for each method on the two 2D-Navigation environment tasks. In both cases, \sdtwplus{} is the only method that reaches every reference state in the correct order. 

% \input{figure_table_tex/old_figs_tables/toy_rollouts_appendix}

% \subsection{Unbalanced Matching in DTW+ \label{app:dtw_unbalanced}}

% % from the ``Right Arm Up'' task
% Figure~\ref{fig:dtw+_unbalanced} shows an example of trajectory, where the DTW+ coupling matrix assigns most of the learner timesteps to the frames towards the end of the reference trajectory.

% \input{figure_table_tex/dtw+_sending_frames_to_the_end_app}


