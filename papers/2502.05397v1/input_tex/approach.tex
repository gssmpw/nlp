\section{Approach}
% Reward functions that measure matching at the frame level  fail to satisfy the desiderata in Sec.~\ref{sec:desiderata}. For sequence-matching tasks, the reward function needs to measure matching at the trajectory level. Specifically, we claim that a trajectory perfectly matches a temporally misaligned demonstration if it successfully covers every subgoal in the same order as the demonstration. 
% The problem of minimizing an arbitrary distance function over the space of learner trajectories naturally lends itself to RL. 
% By the desiderata in \ref{subsec:desiderata}, the optimal learner trajectory should cover every subgoal in the same order as the demonstration. 
% However, efficient RL requires dense rewards to help guide the agent. 
We introduce \ours{} (ORdered Coverage Alignment), a reward function that measures, at each time step, the probability that (1) the learner currently occupies the subgoal specified by the final video frame, and (2) it has already covered all prior frames in the correct order. Since the \orca{} reward at time $t$ depends on the learner trajectory up until $t$, it models the non-Markovian nature of sequence-matching tasks.
In Sec.~\ref{subsec:orca_analysis}, we theoretically analyze and prove that \orca{} satisfies the desiderata proposed in Sec.~\ref{sec:desiderata}.


% \yw{A snippet: a solution to the ordered coverage problem naturally satisfies the desiderata. (1) must cover in order (2) we don't care when it covers (3) you have to cover all the subgoals.}

% \wh{Paragraph introducing the ordered coverage problem (as the same as the 3 desiderata).}

% 1. Temporal constraints: in order to achieve a high reward for frame t', the learner must have already followed the video up to t'-1

% 2. Mismatched speed: it must not matter when in the learner's trajectory it reached any reference frame t'. 

% 3. Progress: the reward at a given frame must be based on it completing all the reference frames by that frame.

\subsection{\ours{}: Ordered Coverage Reward Function}
\input{input_tex/approach_tex/coverage_approach}


\subsection{Analysis\label{subsec:orca_analysis}}
\input{input_tex/approach_tex/coverage_analysis}

\subsection{Pretraining}
% To obtain dense rewards, the final coverage value must be nonzero for all subgoals, regardless of the learner trajectory. 

We observe that in practice, \orca{} can have multiple local minima.
This is because the agent is trying to achieve high coverage for many subgoals. It has a trade-off between covering all subgoals equally well, which is often slower, or quickly achieving high coverage for most subgoals while a small portion of them get lower (but still nonzero) coverage.
Some of these minima are undesirable, e.g. if the small portion of subgoals that the agent only partially covers are vital to the task and require the agents to match them more perfectly, the agent is more likely to fail.

To initialize the agent in a better basin, we first bias the agent towards spending an equal amount of time attempting to cover each subgoal, and then train with the \orca{} reward. 
Specifically, we pretrain the agent on a reward function that assumes the video demonstration is temporally aligned  (e.g., \tot{} rewards). 
In Section~\ref{sec:exp}, we empirically show the importance of pretraining. 


% To avoid any assumptions about the temporal alignment, the \orca{} reward (\ref{eq:orca_reward}) is the probability that \emph{all} the subgoals have been covered. Thus, the agent is motivated to quickly achieve high coverage for as many subgoals as possible, even if this means achieving lower (but still nonzero) coverage for a small portion of earlier subgoals. 

% In practice, this can sometimes lead \orca{} to undesirable local minima. If the small portion of subgoals that the agent only partially covers are vital to the task and require the agents to match them more perfectly, the agent is more likely to fail. To initialize the agent in a better basin, we first bias the agent towards spending an equal amount of time attempting to cover each subgoal, and then train with the \orca{} reward. 
% Specifically, we pretrain the agent on a reward function that assumes the video demonstration is temporally aligned  (e.g., \tot{} rewards). 
% In Section~\ref{sec:exp}, we empirically show the importance of pretraining. 

% then the probability of the learner trajectory covering states closer to \(o^D_j\) satisfies \(P(G^{A}_{t,j}) > P(G^{B}_{t,j})\). By the recursive definition of coverage, \(C^{A}_{t,j-1}P(G^{A}_{t,j}) > C^{B}_{t,j-1}P(G^{B}_{t,j})\), implying \(C^{A}_{t,j} > C^{B}_{t,j}\). Thus, the ORCA reward favors trajectories that demonstrate consistent progress.

% \begin{proposition}
% \ours{} is agnostic to temporal misalignment
% \label{prop:misalignment}
% \end{proposition}
% % \orca{} assigns identical rewards to trajectories that have made the same amount of progress along the demonstration, regardless of the time step. 
% By propositions \ref{prop:ordering} and \ref{prop:progress}, coverage measures the ordered progress along the demonstration trajectory. If a trajectory $\xi^A$ covers a sequence of subgoals at time $t_1$, and another trajectory $\xi^B$ covers the same subgoals at a different time $t_2$, then the reward for $\xi^A$ at time $t_1$ is the same as the reward for $\xi^B$ at time $t_2$. This overcomes the failure mode of TemporalOT detailed \ref{sec:preliminaries}.

 

% 1. \textit{Enforces temporal ordering}: A temporally inconsistent trajectory is one where some demonstration subgoal $o^D_{j}$ is covered in the learner trajectory at time $t$, and the previous demonstration subgoal $o^D_{j-1}$ is covered after time $t$. Since $o^D_{j-1}$ is not yet covered at time $t$, $C_{t, j-1}$ and $C_{t-1, j}$ are both lower than if the $(j-1)$-th subgoal is covered due to (\ref{eq:coverage_recursive}). Thus, a temporally consistent trajectory will always achieve a higher reward.

% 2. \textit{Allows for mismatched execution speeds}: Under mismatched execution speeds, an optimal learner policy $\pi^{L}$ requires $N$ steps to move from $o^D_{j}$ to $o^D_{j+1}$ (whereas the demonstration only takes 1 step). A reward model can handle mismatched execution if it assigns the same reward to the optimal learner policy at time $t+N$ as the demonstration policy $\pi^D$ at time $t+1$. In other words, the reward does not depend on the time at which a demonstration subgoal is covered. By the \ours{} recurrence relation (\ref{eq:coverage_recursive}), the ordered coverage of a given demonstration subgoal $j$ is nondecreasing along the learner trajectory, so $C^L_{t+N, j} = C^D_{t+1, j}$ as long as $\pi^L$ does not cover any previously uncovered reference states. When the optimal learner policy and the demonstration policy reach the next demnstration subgoal $j+1$, $P(G^L_{t+N, j+1}) = P(G^D_{t+1, j+1})$ by definition.
% Then, $C^L_{t+N, j} P(G^L_{t+N, j+1}) = C^D_{t+1, j}P(G^D_{t+1, j+1})$, so $C^L_{t+N, j+1} = C^M_{t+1, j+1}$. 

% 3. \textit{Assigns higher reward to trajectories that make more progress}: Suppose a trajectory $\xi^{L+}$ and an adversarial trajectory $\xi^{L-}$ are identical up until time $t-1$. However, at time $t$, $\xi^{L+}$ makes progress towards the demonstration subgoal $o^D_{j}$, whereas $\xi^{L-}$ does not.
% By definition, $P(G^{L+}_{t, j}) > P(G^{L-}_{t, j})$. 
% Because $\xi^{L+}$ moves closer to $o^D_{j}$ than it has been before, $P(G^{L+}_{t, j}) > P(G^{L+}_{i, j})~\forall i\leq t$, and consequently $C^{L+}_{t,j-1}P(G^{L+}_{t, j}) > C^{L+}_{t-1, j}$.
% Thus, comparing $\xi^{L+}$ against the adversarial trajectory $\xi^{L-}$, $C^{L+}_{t,j-1}P(G^{L+}_{t, j}) > C^{L-}_{t,j-1}P(G^{L-}_{t, j})$, so $C^{L+}_{t,j} > C^{L-}_{t,j}$. 
% The coverage of $o^D_{j}$ is higher for the trajectory that makes progress because the probability that it occupies the next state is increased.


% \textbf{\ours{} probabilistic bounds.}: In addition to analyzing the benefits of \ours{} over other distribution matching algorithms, we provide a probabilistic basis for it. Specifically, we define a success at time $t$ with respect to reference $t'$ as the event that, at time $t$, the agent has occupied every subgoal up to and including the $t'$-th subgoal:
% $$S_{t,t'} = \bigcap_{j=1}^{t'} \bigcup_{i=1}^{t} G_{i, j} $$
% It can be shown that the coverage reward is a lower bound on the probability of successfully following the trajectory: 
% $$P(S_{t,t'}) \geq \max(P(G_{t, t'})P(S_{t, t'-1}),  P(S_{t-1, t'}))$$
% See appendix \ref{app:probabilistic_proof} for the full proof.


% \yw{Potential subsections: (Option A) \textbf{ Analysis.} Can we show that coverage reward satisfies the 3 desiderata? (Option B) \textbf{ Practical Implementation.} e.g. introducing the timestep to state}



% We propose \sdtwplus{}, a sequence-matching reward function that enforces temporal constraints while encouraging continuous progress in Section~\ref{sec:approach:seq}, and we introduce a finetuning framework to train a robust visual encoder that allows \sdtwplus{} to take videos as input and measure the visual distance $d_v(o^L_t, o^D_{t'})$ between the learner and demonstration video frames in Section~\ref{sec:approach:image}. 

% Section~\ref{sec:approach:seq} introduces the \sdtwplus{} reward function, which solves for the optimal soft alignment matrix $A^*$ between the two trajectories and computes the per-timestep reward with a cumulative reward bonus.
% Section~\ref{sec:approach:image} defines a distance function $d_v(o^L_t, o^D_{t'})$ that discounts the visually inferred distance between two frames $o^L_t$ and $o^D_{t'}$ by the predicted epistemic uncertainty of the model. Fig.~\ref{fig:main} illustrates the full pipeline for training a reinforcement learning agent from a demonstration video using the visual distance function and the \sdtwplus{} reward.

% \subsection{TODO}\label{sec:approach:seq}
% \input{input_tex/approach_tex/seq_matching_approach}

% \subsection{Visual Distance Metric}\label{sec:approach:image}
% \input{input_tex/approach_tex/visual_distance_metric_approach}
