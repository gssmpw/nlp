\subsection{Experimental Setup}
\textbf{Environments.} We evaluate our approach across two environments (details in Appendix~\ref{app:env_details}):
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Meta-World~\cite{yu2021metaworldbenchmarkevaluationmultitask}}. Following~\citet{fu2024robot}, we use ten tasks from the Meta-world environment to evaluate the effectiveness of \ours{} reward in the robotic manipulation domain. 
    We classify the tasks into three difficulty levels based on the number of types of motions required and whether the task requires precision. Visual demonstrations are generated using hand-engineered policies provided by the environment.
    \item \textbf{Humanoid.} We define four tasks in the MuJoCo \texttt{Humanoid-v4} environment~\citep{mujoco} to examine how well \ours{} works with precise motion. Because there is no predefined expert, we obtain visual demonstrations by rendering an interpolation between the initial and goal joint state.
    % The goal is to follow the demonstration motion within the maximum 120 timesteps.
\end{itemize}

\textbf{RL Policy.} For Meta-world, we follow the RL setup in ~\citet{fu2024robot}. We train DrQ-v2 \cite{yarats2021masteringvisualcontinuouscontrol} with state-based input for 1M steps and evaluate the policy every 10k steps on 10 randomly seeded environments. 
For the Humanoid environment, we train SAC~\cite{haarnoja2018softactorcriticoffpolicymaximum} for 2M steps and evaluate the policy every 20k steps on 8 environments.
All policies use state-based input, and in metaworld we include an additional feature that represents the percentage of total timesteps passed. 
Appendix~\ref{app:rl} contains RL training details and hyperparameters.

\textbf{Baselines.} We compare \ours{} against baselines that use frame-level matching algorithms: \ot{}~\cite{tian2024what}, \tot{}~\cite{fu2024robot}, and \dtw{}~\cite{dtw}. 
We also compare \orca{}, which is first trained on TemporalOT rewards for half of the total timesteps, then ORCA rewards for the remaining timesteps, against \orcanp{}, which fully trains on ORCA rewards without any initialization.ompared to 
All approaches use the pretrained ResNet50~\cite{he2015deepresiduallearningimage} to extract visual features and cosine similarity as the distance function. 
% For Meta-world, all approaches use the pretrained ResNet50~\cite{he2015deepresiduallearningimage} to extract visual features and cosine similarity as the distance function. 
% For the Humanoid environment, we further finetune the pretrained ResNet50~\cite{he2015deepresiduallearningimage} on domain-specific data. 
% We apply a diagonal smoothing kernel over the distance matrix of length 3, as described in~\citet{fu2024robot}.
We also include a simple baseline \threshold{}, which tracks the subgoals completed based on a threshold distance, and a transformer-based approach \roboclip{}~\citep{sontakke2024roboclip}, which directly encodes an entire video.
% \threshold{} contains two terms: the number of subgoals completed and the reward with respect to the current subgoal to follow; advancing to the next subgoal requires the current subgoal's reward to be above a predefined threshold.  
% Meanwhile, \roboclip{}~\citep{sontakke2024roboclip} uses a pretrained video-and-language model \citep{xie2018rethinking} to directly encode the video. 
% It defines the reward for the last timestep as the cosine similarity between the learner video's and the demonstration video's embeddings, while all previous timesteps have zero as the reward. 
Details in Appendix~\ref{app:baselines}.

\textbf{Metrics.} We evaluate the final checkpoints of all approaches on cumulative binary rewards, or \textbf{returns}, so a policy that succeeds quickly and remains successful is better.
In Meta-world, we use the ground-truth sparse rewards and report the normalized return, which is the return as a fraction of the expert's return on the same task, given the same number of timesteps.
We define a success metric for Humanoid, using privileged states, which no approaches have access to.
An agent is successful if it can remain standing (torso height above 1.1) and its arm joint position is close to the goal joint position (Euclidean distance less than 1). There is no expert in for Humanoid tasks, so we report unnormalized returns.
