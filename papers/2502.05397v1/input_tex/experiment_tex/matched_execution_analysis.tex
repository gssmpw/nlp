\subsection{How does pretraining affect ORCA's performance?}
Pretraining leads to better \orca{} performance when the pretraining strategy is able to obtain some success. In Table.~\ref{tab:metaworld_mismatched}, \orca{} is equal or better than \orcanp{} on temporally misaligned demonstrations. In Humanoid tasks, \orcanp{} achieves higher overall performance because \tot{} almost entirely fails on every task. The effect of pretraining is thus most apparent when the demonstrations are temporally aligned, where \tot{} performs well because the setting satisfies its assumption. 
In Fig.~\ref{fig:meta_aligned_bar}, \orca{} achieves an average normalized return of $0.57$ compared to \orcanp{} ($0.33$) on aligned demonstrations.

\orcanp{} fails due to undesirable local minima.
Consider the \textit{Stick-Push} task in Fig.~\ref{fig:meta_no-pretrain_fail} of Appendix~\ref{app:metaworld_qual}, where the robot arm needs to grasp the stick before pushing the water bottle with that stick.
The \orcanp{} policy directly moves to push the water bottle without the stick.
Because the robot arm initially seems close to the stick, the \orcanp{} policy could get partial coverage on earlier subgoals while collecting high coverage on later subgoals for pushing the bottle. 
Meanwhile, \orca{} is initialized with the \tot{} policy that fails the task but spends equal time attempting each subgoal.
\orca{} is able to refine the policy, finding the middle ground between \orcanp{} and \tot{}and quickly grasping the stick before pushing the water bottle with it. 
Overall, pretraining initializes \orca{} in a better basin, allowing it to train successful and efficient policies.