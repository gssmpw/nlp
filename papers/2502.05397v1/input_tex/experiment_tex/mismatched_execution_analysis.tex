\input{figure_table_tex/tot_slow_filmstrip}

\subsection{How well does \orca{} perform given temporally misaligned demonstrations?}

% \yw{Results: (1) Metaworld table + Training curve for 3 selected tasks; (2) Humanoid IQM bar plot}

\textbf{Metaworld.} For each original demonstration, we subsample it such that the first one-fifth retains the original execution speed, but the rest is sped up by five to ten times. 
% We also analyze how the levels of misalignment affect performance in Sec.~\ref{sec:exp_random_mismatched}.
In Table~\ref{tab:metaworld_mismatched}, the \orca{} reward significantly outperforms other baselines and trains agents to acquire the highest average normalized return of $0.50$.
% Agents with the simple \threshold{} reward achieve an average cumulative return of $4.39$, and they can effectively solve easy tasks (e.g., \textit{Door-close}) while dropping performances on more difficult tasks. 
The \textit{Push} task is difficult for all approaches and the only one where \orca{} or \orcanp{} does not achieve top performance.
As analyzed by~\citet{fu2024robotrebuttal}, the distance matrix is generally noisier for the \textit{Push} task due to the small size of the target object, which benefits a sparser reward function like \threshold{}.
Meanwhile, rewards based on frame-level matching algorithms perform poorly on all tasks, revealing their weakness when encountering temporal misalignment. 

\textbf{Humanoid.} Because there does not exist an expert policy for the Humanoid tasks, we generate demonstrations by interpolating ten frames between the initial and final joint positions. 
These demonstrations are naturally temporally misaligned because the environment is unstable, making it impossible to follow the subgoals at the same speed.
Table~\ref{tab:mujoco} shows that agents trained with \orcanp{} achieve the highest average cumulative return of $62.9$. Due to poor \tot{} performance, \orca{} does not benefit from pretraining, and \orcanp{} thus performs better because it is trained on the ordered coverage reward for more steps. We further investigate the failure of \tot{} in Sec.~\ref{exp:progress}. Meanwhile, Fig.~\ref{fig:humanoid_qualitative} in Appendix~\ref{app:additional_experiments} shows how the \orca{} reward satisfies Prop.~\ref{prop:ordering} and \ref{prop:progress}, covering the subgoals as quickly as possible and successfully completing the task.

\textbf{Varying Misalignment Level.}
We identify two types of temporal misalignment: either a slower demonstration that contains pauses, or a faster demonstration that accelerates through a segment.
For each misalignment type, we randomly perturb the original demonstrations of three Meta-world tasks (\textit{Door-open}, \textit{Window-open}, \textit{Lever-pull}), where the misalignment level controls how varied and nonlinear speed changes are. 
See appendix~\ref{app:random_mismatch_setup} for details.
In Fig.~\ref{fig:meta_random_mismatched}, \orca{} consistently maintains a higher return compared to \tot{} as the demonstrations become more misaligned. 
Meanwhile, \tot{}'s performance significantly deteriorates when there is any level of misalignment.
When the demonstrations are sped up, because \tot{} encourages agents to spend an equal amount of time at each subgoal (in-depth discussion in Sec~\ref{exp:progress}), \tot{} agents often cannot finish the task in time. 
This problem is further exacerbated when the demonstrations are slowed down and longer than the learner trajectory.
\orca{}'s performance also worsens more given slower demonstrations compared to faster ones.
In addition to being affected by poor initialization due to \tot{}'s poor performance, we observe that when \orca{} agents fail, they successfully follow the general motions of the demonstration, but they miss details (e.g., aligning the gripper with the target object). 
We hypothesize that this behavior is caused by the frame-level distance metric, which pays more attention to the general robot arm motions than details, allowing most subgoals to achieve relatively good coverage. 

% \orca{} has a more varied performance given slower, misaligned demonstrations. 
% When \orca{} agents fail, they successfully follow the general motions of the demonstration, but they have missed details (e.g., aligning the gripper with the target object). 
% We hypothesize that this behavior is caused by the visual frame-level distance metric, which pays more attention to the general robot arm motions than details, allowing most subgoals to achieve relatively good coverage. 

\subsection{How does failure to enforce subgoal ordering affect performance?}
Fig.~\ref{fig:meta_order_fail} in Appendix~\ref{app:metaworld_qual} shows a key failure point for \ot{}: its matching matrix can match later subgoals to earlier learner frames and vice versa.
The optimal matching matrix minimizes the transport cost regardless of the order in which subgoals are completed, thereby giving higher \ot{} rewards to trajectories that violate the temporal ordering. In both Meta-world and Humanoid environments, the OT rewards create a difficult optimization landscape that causes the agents to learn undesirable behaviors. 
Fig.~\ref{fig:meta_order_fail} also demonstrates that \tot{} can violate temporal ordering depending on the demonstration length and the mask window size. 
% The mask window regularizes the rewards over the demonstration trajectory, but higher levels of regularization lead to the same problems as \ot{}. 
Tuning this value is a major drawback of \tot{} because it requires prior knowledge of the temporal alignment.

\input{figure_table_tex/metaworld_random_mismatch_fig}
\input{figure_table_tex/metaworld_matched_bar}

\subsection{How does failure to enforce full subgoal coverage affect performance? \label{exp:progress}}
Fig.~\ref{fig:tot_slow} shows that \orca{} successfully trains an efficient agent, while the \tot{} agent opens the door much more slowly and fails. 
The \ot{}/\tot{} formulation assumes that the learner and demonstration distributions are uniform in time. 
% An equal portion of the learner distribution must be matched to each demonstration subgoal.
However, given a temporally misaligned demonstration, different portions of learner frames should be matched to different subgoals, which is impossible using this formulation.
Empirically, successful trajectories produce coupling matrices that approximate the diagonal matrix, as shown in Fig.~\ref{fig:tot_slow}. 
The subsequent rewards teach the agent to spend an equal amount of time at each subgoal instead of following the demonstration as fast as possible, which can cause the agent to exhaust the timesteps before completing the task. 
\tot{} also exhibits poor performance for Humanoid tasks. We hypothesize that the agent fails because \tot{} rewards force the agent to spend an equal amount of time in each intermediate subgoal, which is difficult in a highly unstable environment. 
We show \dtw{}'s failure case in Fig.~\ref{fig:meta_dtw_fail} of Appendix~\ref{app:metaworld_qual}.
% We hypothesize that this is due to the instability of the environment. Under mismatched execution, the diagonal nature of the \tot{} assignment matrix forces the agent to remain in intermediate subgoals. In a highly unstable environment, it is extremely difficult to move towards a subgoal, stop moving and stay there for a few timesteps, and then start moving again. 

% The \dtw{} formulation does not limit the number of learner frames assigned to each subgoal. Fig.~\ref{fig:meta_dtw_fail} shows that it fails to encourage progress as it matches a disproportionate number of frames to one subgoal. Thus, the \dtw{} rewards train the agent to remain stuck at this subgoal instead of continuing and completing the task.
