% From the meeting with Sanjiban:
% 4.1 Motivate what is ordered coverage in english.
% Now, why is it picked

% How to compute efficiently, and why is final reward .

% We propose that ordered coverage can act as a dense RL reward function for learning to follow a video demonstration.
We define the ordered coverage for two sequences as the probability that one sequence covers all the subgoals in the same order as the other sequence.
Ordered coverage shares the same recursive nature as the true reward function of sequence-matching tasks: it depends on how well the final subgoal is covered and how well previous subgoals have already been covered. 
We next present how to calculate ordered coverage and use it to compute rewards.

\textbf{Calculate ordered coverage via dynamic programming.}
Given a video demonstration and a learner trajectory, we can calculate ordered coverage between the two sequences by computing the matrix $C_{t,j}$, which is the probability that the segment of the learner trajectory from time $1$ to $t$ has covered the first to the $j$-th subgoals in the correct order.
% We define the ordered coverage success metric $C_{t,j}$ at timestep $t$ as: the probability that, during the segment of learner trajectory $\{o_{i}\}^{t}_{i=1}$, the learner has finished covering the first to the $j$-th subgoals in the same order as the demonstration.
By the recursive definition of ordered coverage, $C_{t,j}$ can be expressed with two components: (1) the ordered coverage until the ($j-1$)-th subgoal and (2) how well the learner is currently occupying the $j$-th subgoal. 
In addition, ordered coverage should be nondecreasing over time: i.e., once the learner has covered a subgoal at a timestep, it is always at least equally successful in future timesteps.
Let $G_{t,j}$ denote the event that the learner occupies the $j$-th demonstration subgoal at timestep $t$.
We assume that the probability for this event is proportional to the negative exponent distance:
$P(G_{t,j}) \propto \exp(-\lambda d(o_t, \od_{j}))$, where $\lambda$ is a temperature hyperparameter. 
For simplicity, we substitute $P(G_{t,j})$ with $P_{t,j}$.
Thus, we recursively calculate ordered coverage:
\begin{equation}\label{eq:coverage_recursive}
    C_{t,j} = \max \{C_{t-1, j}, C_{t, j-1}P_{t,j}\}.
\end{equation}
Algorithm~\ref{alg:dp_coverage} shows the entire computation. 

\textbf{\ours{} reward function.}
In most robotics tasks, the learner should stay in the final demonstration subgoal after covering all previous subgoals. 
A reward function that directly uses the ordered coverage for the final subgoal does not satisfy this requirement: 
even if the learner does not remain occupying the final subgoal, as long as it has covered the subgoal at one timestep, the maximum operator in (\ref{eq:coverage_recursive}) keeps the reward high for remaining timesteps. 
% Since the ordered coverage of a given demonstration state is nondecreasing over the learner trajectory, the RL reward function must additionally ensure that the agent occupies the final demonstration subgoal. 
Instead of simply equating the reward to $C_{t, \Td}$, the final \ours{} reward at timestep $t$ is:
\begin{equation}\label{eq:orca_reward}
    \mathcal{R}_\text{ORCA}(o_t, \xi, \xid) = C_{t, \Td-1}P_{t,\Td}.
\end{equation}

\input{input_tex/approach_tex/coverage_algorithm}

% \wh{I would argue for a section on the tricks we've learned, like timestep in state and applying a diagonal+horizontal smoothing kernel over the cost matrix (as in temporalOT). This will of course require ablations for these things, but I think they're super important, and are kinda ignored in previous papers.}

% \yw{We currently attempt to describe coverage in an intuitive way, BUT \textbf{it currently looks like an ``engineered solution" and lacks the mathematic principles.} It will be helpful to get Will to incorporate some of the set notations/probability from the appendix so that people can appreciate the cleanliness of the math!}


% We propose equating the RL reward function at a learner timestep $t$ with the ordered coverage success metric for a learner trajectory and a demonstration trajectory with $\Td$ subgoals.
% The ordered coverage metric $C_{t,\Td}$ at timestep $t$ represents the probability that, during the segment of learner trajectory $\{o_{i}^L\}^{t}_{i=1}$, the learner has finished covering the first to the $\Td$-th subgoal in the same order as the demonstration and 
% Thus, the reward function simply is:
% \begin{equation}
%     \mathcal{R}_\text{coverage}(o^L_i, \xid) = C_{i,\Td}.
% \end{equation}


% \wh{Appendix: examples are fixed with ordered coverage}

% \yw{As we workshop the approach, make sure that we are emphasizing coverage has 2 temporal components: (1) covering the 1st subgoal to the \Td-th subgoal. (2) covering is not happening/acheived in 1 learner timestep. It's calculated by looking at the entire learner trajectory from timestep 0 to i}


% \textbf{Calculate ordered coverage via dynamic programming.}
% The ordered coverage metric $C_{i,j}$ at the learner timestep $i$ covering until the $j$-th demonstration subgoal can be \textit{expressed recursively with two components}: (1) the ordered coverage until the $(j-1)$-th subgoal and (2) how well the learner is currently covering the $j$-th subgoal. 
% In addition, the ordered coverage metric is nondecreasing over time: i.e., once the learner has covered a subgoal at a timestep, it is always at least equally successful in future timesteps.
% Let $G_{i,j}$ denote the event that, at the learner timestep $i$, the learner occupies the $j$-th demonstration subgoal.
% We assume that the probability for this event is proportional to the negative exponent distance:
% $P(G_{i,j}) \propto \exp(-\lambda d(o^L_i, o^D_{j}))$, where $\lambda$ is a temperature hyperparameter. 
% Thus, we can recursively calculate ordered coverage as:
% \begin{equation}
%     C_{i,j} = \max \{C_{i-1, j}, C_{i, j-1}P(G_{i,j})\}.
% \end{equation}

% Algorithm~\ref{alg:dp_coverage} shows the entire computation. For simplicity, we substitute $P(G_{i,j})$ with $P_{i,j}$

