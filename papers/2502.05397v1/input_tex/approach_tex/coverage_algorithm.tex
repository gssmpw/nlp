\begin{algorithm}[tb]
   \caption{\orca{} Rewards.}
   \label{alg:dp_coverage}
    \begin{algorithmic}
       \STATE {\bfseries Input:} learner traj $\xi = \{o_t\}^{T}_{t=1}$, demo traj $\xid = \{\od_{j}\}^{\Td}_{j=1}$, distance function $d(\cdot, \cdot)$, temperature term $\lambda$
       \STATE \algcommentlight{Calc probability matrix}
       \STATE $P_{i,j}=\exp(-\lambda d(o_t, \od_{j})) \text{ for } t \in [[T]], j \in [[\Td]]$
       \STATE \algcommentlight{Init coverage at learner time $t=1$}
       \STATE $C_{1, 1} = P_{1,1}$
       \STATE $C_{1, j} = C_{1, j-1} P_{1, j} \text{ for } j \in \{2, 3, \ldots, \Td\}$
       \STATE \algcommentlight{Init coverage of first demo frame $j=1$}
        \STATE $C_{t, 1} = \max\{C_{t-1, 1}, P_{t, 1}\} \text{ for } t \in \{2, 3, \ldots, T\}$
       \STATE \algcommentlight{Solve recurrence relation (\ref{eq:coverage_recursive})}
       \FOR{$t=1$ {\bfseries to} $T$}
       \FOR{$j=1$ {\bfseries to} $T'$}
       \STATE $C_{t,j} = \max\{C_{t-1,j}, C_{t,j-1}P_{t,j}\}$
       \ENDFOR
       \ENDFOR
       \STATE \algcommentlight{Compute the final \orca{} rewards (\ref{eq:orca_reward})}
       \STATE {\bfseries Return} Rewards $R_\text{ORCA} = \{C_{t, \tilde{T}-1}P_{t, \tilde{T}} \mid t \in [[T]]\}$ 
    \end{algorithmic} 
\end{algorithm}
