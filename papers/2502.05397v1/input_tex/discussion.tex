\section{Discussion}
We investigate sequence-matching tasks, where the learner must follow a expert video demonstration that may be temporally misaligned. 
We analyze how algorithms that match the learner and expert distribution at the frame level (e.g., optimal transport) result in reward functions that fail in this setting.
% do not respect the subgoal orderings or do not incentivize full subgoal coverage. 
Following our key insight that matching should be defined at the sequence level, we present \orca{}: a principled reward function that computes the probability that the learner has covered every subgoal in the correct order. 
Experiments on Meta-world and Humanoid tasks show that \orca{} rewards train agents that complete the tasks efficiently regardless of the level of temporal misalignment.
We recognize a few limitations: (1) \orca{} relies on a good visual distance metric that measures the similarity between two frame embeddings. Future work will explore using online finetuning to improve the encoder~\cite{fu2024furlvisuallanguagemodelsfuzzy} and solve cross-embodiment tasks where temporal misalignment is common. (2) If the demonstration contains unrealizable subgoals that the policy can never achieve (e.g., due to physical limitations), the policy will fail. We plan to explore how \orca{} can help iteratively identify realizable subgoals.

% We investigate how to define the reward function for tasks that require an RL agent, such as a humanoid robot, to follow the sequence specified in a video demonstration.
% We present the \sdtwplus{} reward function, which utilizes SDTW to enforce time constraints so that the learner must follow the order in the demonstration and accumulates a reward bonus to encourage the learner to make meaningful progress. 
% Our approach outperforms baselines that use other sequence-matching algorithms in \wh{\# of wins} tasks across two distinct domains. 
% With the \sdtwplus{} reward function, the RL agent is able to learn how to follow the video demonstration effectively. 

% In future directions, we are looking to (1) apply the \sdtwplus{} reward function to longer-horizon, more difficult tasks, such as those requiring periodic motions. Solving these tasks would require incorporating a memory module into the RL policy or a temporal encoding into the state representation because the action at a state will differ depending on the history of states visited. We also plan to (2) relax the dataset assumption needed to finetune the visual encoder. Currently, we rely on the offline robot play dataset to finetune the encoder and estimate the model's uncertainty. Future work would explore using online finetuning during RL training, inspired by~\cite{fu2024furlvisuallanguagemodelsfuzzy}, to improve performance.  
