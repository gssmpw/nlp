\section{Introduction}

%%% Hook: RL reward design + why demonstrations + misaligned demonstrations
Designing reward functions for reinforcement learning (RL) is tedious~\cite{eschmann2021reward}, especially for tasks that require completing subgoals in a strict order. A more scalable way is to learn rewards from a video demonstration. However, learning from videos is challenging because demonstrations are often \emph{temporally misaligned—}variations in timing, embodiment, or execution mean that frame-level matching fails to enforce correct sequencing. To follow a demonstration, an agent must not only match states but also make consistent progress through the subgoals in the right order.

% especially when a task requires explicitly defining high-dimensional subgoals and the conditions for transitioning between them. 
% Many tasks (e.g., assembly, tool use) also require the agent to complete ever subgoal in the correct order.
% These are \emph{sequence-matching problems}, where the true reward function depends both on the agent's current progress along the entire trajectory and whether it has completed all previous subgoals in the past. 
% A more natural and efficient alternative to hand-designing reward functions is to provide a video demonstration, which directly conveys the desirable behaviors. 
% % To imitate, the policy should aim to match all the demonstration subgoals in the same order as the video. 
% Learning from videos poses a unique challenge: Demonstrations are often \emph{temporally misaligned} compared to a learner's trajectory due to variations in timing, differences in embodiment, or suboptimality in execution. For example, a demonstrator might speed through familiar subtasks or take unnecessary pauses during unfamiliar ones.


% variations in when to transition to the next subgoals, differences in embodiment, or inconsistencies in how tasks are performed. 

%%% Paradigm that we are in: IRL. 
Inverse reinforcement learning (IRL)~\citep{abbeel2004apprenticeship, ziebart2008maximum} provides a principled way to infer rewards by matching the learner’s trajectory to expert demonstrations. 
Recent IRL approaches apply optimal transport (OT)~\cite{peyré2020computationaloptimaltransport} to align visual embeddings between frames~\cite{cohen2022imitation, haldar2023teach, haldar2023watch, guzey2024see, tian2024what, liu2024imitation, fu2024robot}. 
However, these methods operate at the frame level, ignoring temporal dependencies: an agent can revisit subgoals, skip ahead, or stall without penalty. As a result, frame-matching approaches fail on sequence-matching tasks, where the correct subgoal order is crucial for success.
% We present analysis showing that the OT reward fails to enforce the order of subgoal completion, while attempts to improve OT by temporally constraining the frame level matching fail under temporal misalignment.

% However, to follow the video demonstration, the agent must complete a demonstration's subgoal in the correct order.
% Thus, the video-following task requires non-Markovian reward functions that depend on trajectory history. 


% We claim that, Because frame-level matching approaches assume a Markovian frame-level distance function, they are fundamentally limited in sequence-following tasks. 

% We present both theoretical and empirical evidence that OT faces two significant challenges: 
% First, OT fails to enforce subgoal ordering because its formulation is invariant to orders. 
% Second, given a temporally misaligned video, mitigations to make OT enforce ordering proposed by~\citet{fu2024robot} have poor performance because they rely on the learner and demonstration having similar movement speed.  

%%% Key insight
Our key insight is that \emph{\textbf{matching should be defined at the sequence level instead of the frame level}} when learning rewards for sequence-matching tasks. Under temporal misalignment, we claim that two sequences only match if one sequence aligns with all of the other sequence’s subgoals in the same order. This leads to an intuitive and principled reward function for sequence-matching tasks: an agent should be rewarded for covering all subgoals in order, without requiring precise timing alignment.

% Two sequences are only matched if one sequence completes all of the other sequence's subgoals in the same order. A reward function that uses frame-level matching cannot satisfy this definition because it lacks important information about the learner and demonstration history. We present analysis showing that the OT reward fails to enforce the order of subgoal completion, while attempts to improve OT by temporally constraining the frame level matching fail under temporal misalignment. Our key insight is that \emph{\textbf{when used as a reward function, matching should be defined at the sequence level instead of the frame level.}} 
% A reward function corresponding to this definition should respect the order of the subgoals and increase as the agent covers more subgoals. However, it should not force the agent to precisely match the video's timing.
% Our key insight is that \emph{\textbf{the agent successfully follows a video if and only if it covers all the demonstration frames in the correct order.}}
% The reward function should increase as the agent covers more frames, and it should respect the order of the frames. However, it should not force the agent to precisely match the video's timing. 

\input{figure_table_tex/main_figure}
We propose \orca{} (ORdered Coverage Alignment), which computes dense rewards given a single video demonstration and a visual learner trajectory, as shown in Fig.~\ref{fig:main}.
Concretely, the reward function is recursively defined as the probability that (1) the learner currently occupies the subgoal specified by the final video frame, and (2) the learner has already covered all prior frames in the correct order. 
% Concretely, the learner's ordered coverage of a video demonstration is recursively defined as the probability that (1) it currently occupies the subgoal specified by the final video frame, and (2) it has already achieved ordered coverage of the video prior to the final frame. 
% When defined as a reward function, \orca{} can be optimized via RL.
In practice, we train an \orca{} policy via RL in two stages.
First, we initialize the policy by training with rewards that assume a temporally aligned demonstration.
Second, we refine this policy with \orca{} rewards to significantly improve its efficiency and performance.
Our key contributions are:
\vspace{-1em}
\begin{enumerate}[leftmargin=*]
    \setlength{\itemsep}{0pt}
    \item A novel, principled reward function class \orca{} that formulates the reward as an ordered coverage problem. 
    \item Analysis on the weakness of rewards based on optimal transport and other frame-level matching algorithms. 
    \item Experiments showing that the \orca{} reward can effectively and efficiently train RL agents to achieve $4.5$x improvement ($0.11 \rightarrow 0.50$ average normalized return) for Meta-world tasks and $6.6$x improvement ($6.55 \rightarrow 43.3$ average return) for \texttt{Humanoid-v4} tasks compared to the best frame-level matching approach.
\end{enumerate}

%%%%% v1 before emphasizing more on misaligned demo + the big picture wrt IRL
% 1. Hook/Motivation: RL reward design is hard (especially when there is a series of subgoals). Demos are a more powerful/intuitive way
% 2. Paradigm that we are in: IRL. 
% 3. Challenges/Desiderata:
% - orders matter (correspond to OT's main weakness)
% - expert demo has different execution speed (correspond to TemporalOT's weakness)
% 4. Key insight. 
% 5. Contribution/Proposed method. How does it address the challenge?
% %%% Hook: RL reward design + why demonstrations
% Designing reward functions for reinforcement learning (RL) is a tedious process~\cite{eschmann2021reward}, especially when a task requires explicitly defining high-dimensional subgoals and the conditions for transitioning between them. 
% A more natural and efficient alternative is to provide a video demonstration, which directly conveys the desired behaviors. 
% However, video demonstrations pose unique challenges.
% Demonstrations are often \emph{temporally misaligned} compared to a learner's trajectory due to variations in when to transition to the next subgoals, differences in embodiment, or inconsistencies in how tasks are performed. 

% %%% Paradigm that we are in: IRL. 
% %%% BUT traditional IRL does work because we care a lot of the order. 
% %%% Distribution matching algorithms offer a solution (but it has its own prob
% Inverse reinforcement learning (IRL)~\citep{abbeel2004apprenticeship, ziebart2008maximum} provides a foundational framework for learning reward functions from expert demonstrations. 
% At its core, IRL can be viewed as a distribution matching problem between the learner and the demonstrator, often formulated as optimizing Integral Probability Metrics (IPMs)~\citep{sun2019provably, swamy2021moments}.

% Recent works have leveraged optimal transport (OT)~\cite{peyré2020computationaloptimaltransport} to do such matching in high-dimensional feature space and use the matching to construct rewards~\cite{xiao2019wassersteinadversarialimitationlearning, dadashi2020primal, papagiannis2022imitation, cohen2022imitation, luo2023optimal, haldar2023teach, haldar2023watch}.
% %%% Challenges of current approaches
% However, current approaches face two significant challenges:
% %% 1. orders matter
% First, certain tasks require completing subgoals in a specific order.
% OT fails to enforce this ordering because its formulation lacks temporal consistency and is invariant to orders, as also noticed by~\citet{fu2024robot}. 
% %% 2. mismatched execution
% Second, expert demonstrations often have a different execution speed than the learner: they may complete some subgoals at a pace faster or slower than the learner's capability. 
% Mitigations to make OT enforce ordering proposed by~\citet{fu2024robot} have poor performance in these cases because they rely on the learner and expert having similar execution speeds.  

% %%% Key insight 
% Our key insight is that \emph{\textbf{following a demonstration trajectory is an ordered coverage problem, not a distribution matching problem.}}
% % reaching a series of subgoals in a specified order is similar to an ordered coverage problem.
% % framing the task of reaching a specified sequence of subgoals as an ordered coverage problem reveals a principled solution.
% % the ordered coverage problem offers the principled lens to define a reward function that enforces reaching each subgoal. 
% \yw{By covering in order, ... (this sentence should embellish the key insight instead of repeating what it says above.)}
% In order to correctly follow a trajectory, the agent must cover all the subgoals in the same order as the demonstration. We present both theoretical and empirical evidence that OT fails to enforce ordering, while existing solutions to this problem require strong assumptions about the similarity in the execution speed between the demonstrator and the learner. 

% \yw{Mechanically tell the reader what the method is doing: how many demo? when to compute the reward?}
% We propose \ours{} (ORdered Coverage Alignment), which computes the probability that, at a given timestep, the learner has fully covered all subgoals in the correct order. Since this is a dense success metric over the agent's trajectory, it can be optimized using reinforcement learning. We show that agents trained with \ours{} rewards outperform OT by $TODO\%$, achieving $TODO\%$ success rate in the Metaworld and $TODO\%$ in the Mujoco \texttt{Humanoid-v4} environment. 
% \input{figure_table_tex/main_figure}
% Our key contributions are:
% \vspace{-1em}
% \begin{enumerate}[leftmargin=*]
%     \setlength{\itemsep}{0pt}
%     \item A novel, principled reward function class \ours{} that formulates the reward as an ordered coverage problem. 
%     \item Theoretical and empirical analysis on the weakness of rewards based on optimal transport and other distribution matching algorithms. 
%     % \item A robust visual distance metric for calculating the distance between two frames based on a visual encoder with uncertainty estimation finetuned on privileged robot state. 
%     % \item A practical approach to finetune a visual encoder as a distance metric between two frames and increase its reliability on out-of-distribution data by estimating its prediction confidence. 
%     \item Experiments showing that the \ours{} reward can effectively and efficiently train RL agents to achieve $TODO\%$ success rate in the Metaworld environments and $91.7\%$ success rate in the MujoCo \texttt{Humanoid-v4} environment.
%     % \item Evaluations in 2D Navigation environments and the MujoCo \texttt{Humanoid-v4} environments on 8 sequence-following tasks. 
% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%% Workshop Intro
% \SC{
% The arguments don't make sense to me. I would actually follow a style similar to Andrea's paper: https://arxiv.org/pdf/2310.07932
% 1. Demonstrations are a powerful way to program behavior (example)
% 2. IRL provides a foundation, think of it as distribution matching, optimal transport have been recently used (cite all the paper)
% 3. Two fundamental challenges
% Challenge 1: How do we enforce temporal constraints? Make the argument that standard OT does not 
% Challenge 2: How do we capture fine-grained robot state? 
% Make the argument that VLM rewards capture semantics. 
% 4. Our key insight ... 
% 5. What is your proposed method? How does it address the challenges?
% }

%%% Hook why demonstrations
% Designing reward functions for reinforcement learning (RL) agents is a tedious process, especially when controlling humanoid robots with high degrees of freedom.
% For example, training a humanoid to perform specific motion sequences, such as coordinated hand and arm signals, requires explicitly defining high-dimensional subgoals and the conditions for transitioning between them. A more natural and efficient alternative is to provide a video demonstration, which directly conveys the desired behaviors without the need for manually crafted task specifications.
% % However, because a demonstration video does not have state or action labels, it is not trivial to measure the performance of a learner trajectory with respect to that video. 

% %%% Prior works: where is our work in the bigger context
% Inverse reinforcement learning (IRL)~\citep{abbeel2004apprenticeship, ziebart2008maximum} provides a foundational framework for learning reward functions from expert demonstrations. At its core, IRL can be viewed as a distribution matching problem between the learner and the demonstrator, often formulated as optimizing Integral Probability Metrics (IPMs)~\citep{sun2019provably, swamy2021moments}.
% Recent works have leveraged optimal transport (OT)~\cite{peyré2020computationaloptimaltransport} to do such matching in high-dimensional feature space~\citep{xiao2019wassersteinadversarialimitationlearning, dadashi2020primal, papagiannis2022imitation, cohen2022imitation, luo2023optimal, haldar2023teach, haldar2023watch}.
% %% What are the two fundamental challenges
% However, current approaches face two significant challenges:
% %% 1: temporal constraint
% First, certain tasks require completing subgoals in a specific order.
% OT fails to enforce this ordering because its formulation lacks temporal constraints, allowing it to ignore the order of subgoals when matching distributions between the learner and the demonstration. 
% %% 2: encourage progress
% Second, the reward function must avoid local minima that may lead to reward hacking, where the agent learns to exploit undesired loopholes~\citep{clark2016faultyreward, amodei2016concrete, yuan2019novel}.
% A naive reward function that only enforces temporal constraints can lead to an agent that lingers at subgoals and collects immediate rewards without making meaningful progress.

% \textbf{\emph{Our key insight is to structure the reward function to enforce temporal consistency.}} 
% Instead of using OT, which matches trajectories without considering temporal order, we build on Soft Dynamic Time Warping (SDTW)~\cite{soft-dtw} to constrain alignment paths, ensuring that the agent follows the correct sequence of actions. However, we find that SDTW alone does not sufficiently drive task completion. To address this, we introduce a cumulative reward bonus that encourages the agent to make continuous progress, preventing it from stalling at intermediate subgoals.

% We propose a method, Soft-DTW Plus (\sdtwplus{}), that (1) uses a visual encoder to compute the distance for all pairs of frames between the learner and demonstration trajectories, (2) based on the distances, solves for the optimal alignment path using SDTW and accumulates reward bonuses to calculate the per-timestep reward. 
% We show that agents trained with the \sdtwplus{} rewards outperform OT by $56.2\%$, achieving $91.7\%$ success rate in the Mujoco \texttt{Humanoid-v4} environment. 
% % ... (describe your method in a few lines. talk about it linearly. End with "We show that SDTW+ is X\% better.." )


% Our key contributions are:
% \begin{enumerate}[leftmargin=*]
%     \setlength{\itemsep}{0pt}
%     \item A novel sequence-matching reward function class \sdtwplus{} that enforces the agent to follow the demonstration in the correct order and encourages it to continuously make meaningful progress. 
%     % A sequence-matching reward function that uses SDTW to enforces the agent to follow a visual demonstration and incorporates a cumulative reward bonus to encourages making progress.
%     \item A robust visual encoder, finetuned on privileged robot state, that estimates its uncertainty to calculate reliable visual distance between two frames. 
%     % \item A robust visual distance metric for calculating the distance between two frames based on a visual encoder with uncertainty estimation finetuned on privileged robot state. 
%     % \item A practical approach to finetune a visual encoder as a distance metric between two frames and increase its reliability on out-of-distribution data by estimating its prediction confidence. 
%     \item Experiments showing that the \sdtwplus{} reward can effectively and efficiently train RL agents to achieve 100.0\% success rate in the 2D-Navigation environments and $91.7\%$ success rate in the MujoCo \texttt{Humanoid-v4} environment.
%     % \item Evaluations in 2D Navigation environments and the MujoCo \texttt{Humanoid-v4} environments on 8 sequence-following tasks. 
% \end{enumerate}



%%%%%%%%%%%%%%%%%%%%%
% %%% Hook
% Defining reward functions to train reinforcement learning (RL) agents is a tedious process, especially for controlling humanoid robots with high degrees of freedom.
% For instance, training a humanoid robot to follow a specific motion sequence (e.g. making hand and arm signals) requires expert knowledge in understanding and defining the task in the robot's state space. A more intuitive, natural method would be to provide a video that directly showcases the desirable behaviors.
% However, because a demonstration video does not have state or action labels, it is not trivial to measure the performance of a learner trajectory with respect to that video. 
% % \aw Later on: we could add an example

% %%% Prior works: where is our work in the bigger context
% Recent work show promising results in utilizing image encoders and Visual-Language Models (VLMs) to generate rewards for tasks defined via a short language description or a goal image. 
% Given an image representing a timestep in the learner trajectory, the model can either directly output scalar values as rewards~\citep{du2023visionlanguagemodelssuccessdetectors, wang2024rl} or  use similarity scores between the embedding of the goal and the current frame as rewards~\citep{rocamonde2024visionlanguage, baumli2023vision, mahmoudieh2022zero, fu2024furlvisuallanguagemodelsfuzzy}. 
% However, current approaches do not account for two challenges. 
% First, image encoders and VLMs alone are insufficient as reward models for tasks require the agent to follow a specific sequence defined by a demonstration video. 
% Second, although these models can more reliably detect changes in semantics (e.g. whether a drawer was open or not), they struggle to provide accurate reward for out-of-domain tasks that require fine-grained motions.

% \textbf{\emph{Our key insight is to provide a structured reward formulation that utilizes vision models.}}
% Concretely, we propose a reward formulation that (1) uses, Soft Dynamic Time Warping (SDTW)~\cite{soft-dtw}, a sequence matching algorithm, as the basis to define how well the learner's trajectory can overall be matched with the demonstration trajectory and (2) utilizes vision models as a distance metric needed by SDTW to measure how close each frame in the two sequences are. 
% We also provide a practical approach to finetune the vision model and increase its accuracy as a distance metric. 

% Our key contributions are:
% \begin{itemize}[leftmargin=*]
%     \item A sequence-matching reward function based on SDTW that encourages the agent to make progress and follow a demonstration video.
%     \item A practical approach to finetune a vision model as a distance metric between two frames and increase its reliability by estimating its prediction uncertainty. 
%     \item Evaluations in 2D Navigation environments and the MujoCo \texttt{Humanoid-v4} environments on 8 sequence-following tasks. 
% \end{itemize}

% %%% Hook
% Defining reward functions to train reinforcement learning (RL) agents is a tedious process, especially for controlling humanoid robots with high degrees of freedom.
% For instance, training a humanoid robot to follow a specific motion sequence (e.g. making hand and arm signals) requires expert knowledge in understanding and defining the task in the robot's state space. A more intuitive, natural method would be to provide a video that directly showcases the desirable behaviors.
% However, because a demonstration video does not have state or action labels, it is not trivial to measure the performance of a learner trajectory with respect to that video. 
% % \aw Later on: we could add an example

% %%% Prior works: where is our work in the bigger context
% Recent work show promising results in utilizing image encoders and Visual-Language Models (VLMs) to generate rewards for tasks defined via a short language description or a goal image. 
% Given an image representing a timestep in the learner trajectory, the model can either directly output scalar values as rewards~\citep{du2023visionlanguagemodelssuccessdetectors, wang2024rl} or  use similarity scores between the embedding of the goal and the current frame as rewards~\citep{rocamonde2024visionlanguage, baumli2023vision, mahmoudieh2022zero, fu2024furlvisuallanguagemodelsfuzzy}. 
% However, current approaches do not account for two challenges. 
% First, image encoders and VLMs alone are insufficient as reward models for tasks require the agent to follow a specific sequence defined by a demonstration video. 
% Second, although these models can more reliably detect changes in semantics (e.g. whether a drawer was open or not), they struggle to provide accurate reward for out-of-domain tasks that require fine-grained motions.

% \textbf{\emph{Our key insight is to provide a structured reward formulation that utilizes vision models.}}
% Concretely, we propose a reward formulation that (1) uses, Soft Dynamic Time Warping (SDTW)~\cite{soft-dtw}, a sequence matching algorithm, as the basis to define how well the learner's trajectory can overall be matched with the demonstration trajectory and (2) utilizes vision models as a distance metric needed by SDTW to measure how close each frame in the two sequences are. 
% We also provide a practical approach to finetune the vision model and increase its accuracy as a distance metric. 

% Our key contributions are:
% \begin{itemize}[leftmargin=*]
%     \item A sequence-matching reward function based on SDTW that encourages the agent to make progress and follow a demonstration video.
%     \item A practical approach to finetune a vision model as a distance metric between two frames and increase its reliability by estimating its prediction uncertainty. 
%     \item Evaluations in 2D Navigation environments and the MujoCo \texttt{Humanoid-v4} environments on 8 sequence-following tasks. 
% \end{itemize}

%%%%%%%%%%%%%%%%

% VLM reward
% How are these different paper? 
% Why do so many papers exist? There seems to be a limited axis of things to do. 
% VLM spitting numbers
% Embedding

% Nothing works super well

% 1. RL, you can't use fancy VLM, due to speed concern
% 2. Rewards are noisy
% 3. Andrea: VLMs totally suck until you fine-tune them on privileged state
% --> VLMs can't pick up privileged states
% --> These models are designed to give same caption for different prospective. It abstract away spatial details. But we would care about the small spatial details. They are insensitive to precision.
% --> But then why do you need VLMs? Language input or 

% Little to do with VLM reward: main meat is: how do we match sequences. 
% If you care about following a sequence, how do you follow a structural constraints on top of a VLM model 
% \textbf{How do you put structure on top of the VLM model to follow a specific sequence? What's the space of reward function that we can use}
% While OT is able to ... (give you a rich class of reward function), these reward functions don't guarantee the agent is follow the right sequence.
% How do we put constraint on OT?
% --> You can think of OT path 
% Observation: soft-dtw is a way to constrain OT
% --> We can establish it formally 

% How do we represent/capture temporal constraint
% VLM 

% Hierarchical methods: 
% >> soft-dtw
% - scale the

% On the VLM front:
% Can we use the text caption? To show that it can't hurt and 
% Ensemble: 
% add in the CLIP reward 

% v0: no text, what we have right now
% v1: add text for each ref image (add the CLIP reward)

% GPT
% "are the arms the same?"
% probability? logits?

% (image1, image2, question) --> score

% (image 1, question) --> emb1
% (image 2, question) --> emb2
% emb1 * emb2

% CLIP style model
% high level concept

% What are simple things we can try to get things to work? (Is my performance getting better once I 
% - did we ever take the masked humanoid image through CLIP
% - overlay pink for the left arm, blue for the right arm

% Story: 
% SAM tracking

% Pretrained image model (image model pretrained on large scale of data) shows a lot of promising in capturing representation that are robust across domain + semantic aspect of the task.
% --> Else ResNet. We would have to fine-tune for every new tasks

% 2 problems with VLMs
% 1. (image models) natively insufficient to use as reward fn --> bc they are natively insufficient over reward models for following sequence. They are not trained on sequence data. They look at frames individually. They are innately not keeping track of instances. Primary shortcoming. So we build an architecture of the reward function.  Uses any VLM models underneath the hood to output how well the sequence to match. Some extension: a class of constrained OT (--> soft DT). 

% What are the desirata:
% (1) reward progress
% (2) match sequence
% (3) easy to compute / be robust to this thing
% 2. (a dataset that evaluates sequence following ability)
% 3. (make it a bit pointed: how to make the underlying VLMs robust?)
% make the VLM focus on salient aspect of the image (task-aware reward fun). We are the practitioner. 

% VLMs don't work perfectly. You need to think about the reward function much harder than you have been.

% Our competitor would be a transformer that 

