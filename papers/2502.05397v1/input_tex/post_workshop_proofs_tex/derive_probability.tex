% \subsection{Probabilistic Derivation of \ours{}}
% \label{app:probabilistic_proof}

% Let $O_{t, t'}$ be the event that the agent occupies reference $t'$ at time $t$. Let $S_{t,t'}$ be a success at time $t$ with respect to reference $t'$. Specifically, $S_{t,t'}$ is the event that the agent currently occupies reference t', and for each reference before t', the agent reached that reference at some point on or before t. In other words, the agent is currently in the final reference states, and has full coverage over previous reference states:

% We define a success $S_{t,t'}$ to be the event that the agent reached t' and all previous subgoals at time t.

% $$S_{t,t'} = \bigcap_{j=1}^{t'} \bigcup_{i=1}^{t} O_{i, j} $$

% Factor out time t'
% $$ \bigcup_{i=1}^{t} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j}$$

% Factor out time t from the left side
% $$  (O_{t, t'} \cup  \bigcup_{i=1}^{t-1} O_{i, t'}) \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j} $$
% Distribute the intersection
% $$  (O_{t, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j}) \cup  (\bigcup_{i=1}^{t-1} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j}) $$
% Expanded success definition:
% $$  (O_{t, t'} \cap S_{t, t'-1}) \cup  (\bigcup_{i=1}^{t-1} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j}) $$

% To simplify, we introduce a lower bound on the right part of this equation:
% $$ P(\bigcup_{i=1}^{t} O_{i, j}) \geq P(\bigcup_{i=1}^{t-1} O_{i, j}) \text{ for all j}$$ 

% Thus:
% $$P(\bigcup_{i=1}^{t-1} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t} O_{i, j}) \geq P(\bigcup_{i=1}^{t-1} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t-1} O_{i, j}) $$

% Plugging this term into the expanded success definition from above:
% $$P(S_{t,t'}) \geq P ((O_{t, t'} \cap S_{t, t'-1}) \cup  (\bigcup_{i=1}^{t-1} O_{i, t'} \cap \bigcap_{j=1}^{t'-1} \bigcup_{i=1}^{t-1} O_{i, j})) $$
% Simplify the right side:
% $$P(S_{t,t'}) \geq P((O_{t, t'} \cap S_{t, t'-1}) \cup  (\bigcap_{j=1}^{t'} \bigcup_{i=1}^{t-1} O_{i, j}) )$$
% $$P(S_{t,t'}) \geq P((O_{t, t'} \cap S_{t, t'-1}) \cup  S_{t-1, t'}) $$
% Apply max lower bound on joint event likelihood and occupancy independence given observation:
% $$P(S_{t,t'}) \geq \max(P(O_{t, t'}) * P(S_{t, t'-1}),  P(S_{t-1, t'}))$$
% With base cases:

% $P(S_{0,t'}) = 0 $ because the probability of being in a reference after 0 learner timesteps is 0

% $P(S_{t, 0}) = 1 $ because the probability of being in no specific reference is 1

% If we let $\mathcal{R}^*_t = P(S_{t, T'})$, we obtain the \ours{} reward function.

% \[S_{t,t'} =  (O_{t, t'} \cap S_{t, t'-1}) \cup S_{t-1,t'}\]



% \wh{Probability-based reward (not coverage)}
% The true reward is the probability of a success with respect to the final reference state, T':
% $$\mathcal{R}^*_t = \mathbb{P}(S_{t,T'})$$

% The event that the agent reached a reference $j$ at any time prior to the current time is independent from the event that it reached a different reference $j'$ given the history. Thus, $\bigcup_{i=1}^{t} O_{i, j}$ is independent from $ \bigcup_{i=1}^{t} O_{i, j'}$ given the history:
% \begin{equation*}
% \mathbb{P}(S_{t,t'} | \mathcal{H}) = \mathbb{P}(O_{t,t'} |  \mathcal{H}) * \prod_{j=1}^{t'} \mathbb{P} (\bigcup_{i=1}^{t} O_{i, j'} |  \mathcal{H})
% \end{equation*}

% It is intractable to calculate $\mathbb{P} (\bigcup_{i=1}^{t} O_{i, j'} | \mathcal{H})$ because $O_{i, j}$ and $ O_{i', j}$ are not disjoint events. Instead, a lower bound can be established on the probability of a success. 

% $$\mathbb{P} (\bigcup_{i=1}^{t} O_{i, j} |  \mathcal{H}) \geq  \max_{i \leq t}{\mathbb{P}(O_{i,j}} |  \mathcal{H}) $$

% We assume that the probability of occupying a specific reference state given the current observation is equal to the probability of occupying the reference state given the agent's history. I.e., $\mathbb{P}(O_{i,j} | \mathcal{H}) = \mathbb{P}(O_{i,j} | \xi^L_i, \xi^D_j)$.

% $$\mathbb{P}(S_{t,t'}) \geq \mathbb{P}(O_{t,t'} ) * \prod_{j=1}^{t'-1} \max_{i \leq t}{\mathbb{P}(O_{i,j}}) $$

% The lower bound on $\mathbb{P}(S_{t,T'})$ is used to obtain a tractable estimate of the true reward. In practice, we approximate $\mathbb{P}(O_{i, j})$ as $\exp(-d(\xi^L_i, \xi^D_j)/\tau)$, where $d$ is a distance metric defined on the observation space and $\tau$ is a smoothing parameter. \textbf{This gives the final reward function:}
% $$\mathcal{R}_t = \exp(-\frac{d(\xi^L_t, \xi^D_{T'})}{\tau}) * \prod_{j=1}^{T'-1} \max_{i \leq t}\exp(-\frac{d(\xi^L_i, \xi^D_{j})}{\tau}) $$


% Note that if we define our ground truth reward to be the log probability of success:
% $\mathcal{R}^*_t(\pi) = \log \mathbb{P}(S_{t,T'} | \pi)$, then the reward function becomes:
% $$\mathcal{R}_t = -\frac{d(\xi^L_t, \xi^D_{T'})}{\tau} - \sum_{j=1}^{T'-1} \min_{i \leq t}\frac{d(\xi^L_i, \xi^D_j)}{\tau}$$

% To ensure that rewards remain bounded (for the purposes of optimization), we choose to use the multiplication formulation for our reward function.

% \wh{Connection to \dtwplus} The sum formulation of the reward suggests a connection to the cumulative reward bonus of \dtwplus{}. In the probability based reward, the cost is defined as the distance from the final reference, plus the min distance from the previous reference, for all previous references. In \dtwplus{}, the cost is defined as the distance from the currently assigned reference, plus the min distance from learner rollouts assigned to the previous reference, for all previous references. The difference is simply that $\dtwplus{}$ depends on the assignment. 





% \wh{Reweighting}

% Currently, we define occupancy as the exponentiated distance from the reference:
% $$\mathbb{P}(O_{t, t'}) = \exp(-d(\xi^L_t, \xi^D_{t'})/\tau)$$
% But the probability of occupying a given reference state should decrease as the probability of occupying other reference states increases (i.e., as other reference states come closer).

% $$\mathbb{P}(O_{t, t'}) = \exp(-d(\xi^L_t, \xi^D_{t'})/\sum_{j=1}^{T'} d(\xi_j^D, \xi^D_{t'}))$$

% In other words, we use an adaptive temperature based on the reference state: 
% $$\tau_{t'} = \sum_{j=1}^{T'} d(\xi_j^D, \xi^D_{t'})$$

% \wh{Definition of the problem that explicitly includes completion efficiency:}
% The efficient sequence following problem is equivalent to:
% \[
% \argmax_{\pi} \mathbb{P}(S_{t, T'} | \pi)  \quad \text{subject to} \quad t < \tau
% \]
% This is converted to a decision problem by defining the instantaneous payoff as the probability of success: \wh{What is the connection? Can we reduce to this from the lagrangian}
% \[
% \argmax_{\pi} \sum_{t=1}^T \gamma^t \mathbb{P}(S_{t, T'} | \pi)
% \]



