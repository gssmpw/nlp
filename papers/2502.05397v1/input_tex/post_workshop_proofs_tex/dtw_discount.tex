

% \subsection{OT Reward Hacking\label{proof:suboptimal_ot}}
% \input{figure_table_tex/ot_failure_detailed}


% \textbf{Detailed Problem Setup.} 
% \wh{IN PROGRESS}
% \begin{proposition}

% The OT reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.

% \end{proposition}

% \begin{proof}

% Let $\pi'$ be a deterministic policy that produces $\xi^{L-}$, a trajectory with the same embedded occupancy distribution as $\xi^{L*}$. This occurs if $\xi^L*$ and $\xi^{L-}$ reach the same states for the same amount of time. For example, $\xi^{L-}$ could complete all the states in the wrong order, as in the left of Fig.~\ref{fig:ot_fail_full}. If the demonstration has twice as many frames as the learner has steps, $\xi^{L-}$ could complete all the demonstration states during the first half of the trajectory, and then complete them backwards during the second half of the trajectory, as in \wh{ref OT humanoid going backwards fig}.

%  \citet{chen2023neuralapproximationwassersteindistance} show that the Wasserstein distance is a function of the embedded occupancy distribution of its point sets, so it is invariant to permutations in the ordering of these points. Thus, for the trajectories $\xi^{L*}$ and $\xi^{L-}$, the Wasserstein distance is the same. Consequently, the cumulative OT rewards are also the same, even though $\xi^{L-}$ is not a successful rollout.
% \end{proof}

% Note that DTW can overcome this limitation because of its time consistency constraints, as shown in the right figure of Fig.~\ref{fig:ot_fail_full}. 
% It aligns the two trajectories in a time-consistent way, ensuring that $\xi^{L*}$ (the trajectory in the correct order) is rewarded more than $\xi^L$ (the trajectory in the incorrect order), thereby overcoming the failure case of OT.



% \subsection{DTW Reward Hacking}

% \textbf{Detailed Problem Setup.} 

% \textbf{Assumption 1:} $\mathcal{S}$ is a finite discrete state space: $S \subset \mathbb{Z}^{N}$ and $\mathcal{S}$ is bounded in all dimensions. $\mathcal{A}$ is an action space that allows the agent to move 1 space in any cardinal direction. 2D-Navigation is an example of such an environment. 

% \textbf{Assumption 2}: $d:S\times  S \rightarrow \mathbb{R}$ is a metric on $S$. For simplicity, we assume $d$ is the Manhattan distance, but the proof can be extended to other distance functions. 

% % The proof proceeds by constructing an adversarial trajectory $\xi^{L-}$ and showing that its cost is bounded above by the cost of $\xi^{L*}$.    
% % \textit{Constructing the Adversarial Trajectory.}  


% \begin{proposition}
% The DTW reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.
% % and $\pi' \in \argmax J(\pi, \tilde{\R})$. 
% \end{proposition} 
% \begin{proof}
% Let $\tilde{\R}$ be the $\dtw$ reward function given a learner trajectory $\xi^L$ of length T, a reference trajectory $\xi^D$ of length T', and a learner timestep t:
% $$\tilde{\R}(\xi^L, \xi^D, t) = -\frac{1}{T'T} A^*_t(\xi^L, \xi^D) \cdot D_t(\xi^L, \xi^D)$$
% where $\mathbf{A}^*_t(\xi^L, \xi^D)$ is the DTW assignment between $\xi^L$ and $\xi^D$ at learner step t, and $\mathbf{D}_t(\xi^L, \xi^D)$ is the distance vector between $\xi^L$ and $\xi^D$ at learner step t. 

% Let $\pi'$ be a deterministic policy that produces the adversarial learner trajectory $\xi^{L-}$ as follows:
% \begin{enumerate}[noitemsep, topsep=0pt, partopsep=0pt]
%     \item Visit all the same states as $\xi^{L*}$ until reaching the second-to-last state of the demonstration trajectory, $\xi^D_{T'-1}$.
%     \item Stay in $\xi^D_{T'-1}$ until the final timestep.
%     \item Take an action towards the final demonstration state $s^D_{T'}$ in the final timestep.
% \end{enumerate}

% The first part of the proposition follows by definition: $\pi'$ does not reach every reference state and $\pi^*$ reaches them all in the correct order, so $J_{\R^*}(\pi') < J_{\R^*}(\pi^*)$. 

% For the second part of the proposition, we compute the returns of the policies. The distance matrices $\textbf{D}$ for the two trajectories are as follows, where $d_{rem} = d(\xi^D_{T'-1}, \xi^D_{T'})$

% \begin{center}
% $\mathbf{D}(\mathbf{\xi^{L*}}, \xi^D)$ = 
% $\begin{bmatrix}
% \mathbf{\mathbf{D}(\xi^{L*}_\leq, \xi^D)} & ? & ?\\
% ? & \mathbf{0} & d_{rem} \\
% \vdots & \vdots & \vdots \\
% ? & d_{rem}-1 & \mathbf{1}\\
% ? & d_{rem} & \mathbf{0}
% \end{bmatrix}$
% \hspace{1cm} % Add space between the two matrices
% $\mathbf{D}(\mathbf{\xi^{L-}}, \xi^D)$ = 
% $\begin{bmatrix}
% \mathbf{\mathbf{D}(\xi^{L*}_\leq, \xi^D)} & ? & ?\\
% ? & \mathbf{0} & d_{rem} \\
% \vdots & \vdots & \vdots \\
% ? & \mathbf{0} & d_{rem} \\
% ? & 1 & d_{rem}-1
% \end{bmatrix}$
% \end{center}

% Before $\xi^{L-}$ and $\xi^{L*}$ reach $\xi_{T'-1}^D$, they are identical, and thus accumulate the same reward. To simplify the proof, we refer to the time when they reach $\xi_{T'-1}^D$ as $t=0$ (effectively removing the identical parts of the trajectory). The optimal policy makes continuous progress for T timesteps, at which point it reaches $\xi_{T'}$. For the first half of the timesteps, it is closest to $\xi_{T'-1}$ (and is thus assigned there), and for the second half, it is closest to $\xi_{T'}$. 
% \begin{align*}
%     J_{\tilde{R}}(\pi^*) &= -\sum_{t=1}^{T/2} t \gamma^t + t \gamma^{T-t} \\
%     & = -\gamma - \gamma^{T-1} -\sum_{t=2}^{T/2} t \gamma^t + t \gamma^{T-t}.
% \end{align*}

% For $1 \leq t  < T$, $\xi^{L-}$ gets 0 reward because it remains in $\xi_{T'-1}^D$. The last learner frame $\xi^{L-}_T$ must be assigned to $\xi_{T'}^D$ to satisfy the DTW constraint, and it makes 1 move of progress (out of T necessary moves), thus achieving a discount reward of $-\gamma^T(T-1)$.
% % Adversarial Policy Return:
% \begin{align*}
%     J_{\tilde{R}}(\pi') &= -\gamma^T(T-1) = \gamma^T - 2 \gamma^T - \sum_{t=2}^{T/2} 2 \gamma^T \\
%     &= -\gamma^T - \sum_{t=2}^{T/2} 2 \gamma^T. \\
% \end{align*}

% Note that $-\gamma^T > -\gamma - \gamma^{T-1}$, and that $  - \sum_{t=2}^{T/2} 2 \gamma^T > -\sum_{t=2}^{T/2} t \gamma^t + t \gamma^{T-t}$.
% It follows that $J_{\tilde{R}}(\pi') > J_{\tilde{R}}(\pi^*)$. Thus, the DTW reward is hackable with respect to the ground truth reward.

% \end{proof}


% \subsection{TemporalOT Reward Hacking\label{proof:suboptimal_temporal_ot}}
% \textbf{Detailed Problem Setup.} 

% Let $\xi^D$ be a demonstration trajectory and $\xi^{L*}$ be the optimal learner trajectory that correctly follows this demonstration. Let $d(\xi^D_{t}, \xi^D_{t'})$ be a distance metric on the observation manifold. 

% \textbf{Assumption 1:} $\mathcal{S}$ is a state space.

% \textbf{Assumption 2}: $d:S\times  S \rightarrow \mathbb{R}$ is a metric on $S$. 
% \begin{proposition}
% The TemporalOT reward $\tilde{R}$ is hackable with respect to the ground truth reward $R^*$.
% % and $\pi' \in \argmax J(\pi, \tilde{\R})$. 
% \end{proposition} 

% \begin{proof}
% The optimal TemporalOT coupling matrix \cite{fu2024robot} is defined as:
% $$
% \mu^* = \arg\min_{\mu} \langle M \odot \mu, \hat{C} \rangle_F - \epsilon \mathcal{H}(M \odot \mu), \quad \text{s.t.} \quad \mu \mathbf{1} = \mu^\top \mathbf{1} = s,
% $$
% where $\mathcal{H}(\mu)$ is the entropy of the coupling matrix $\mu$, and $M$ is the mask matrix. For a high $\epsilon$, the entropy term dominates, and the reward is optimized when $M \odot \mu$ is uniformly distributed. Since M is a predefined mask, $\mu$ must be uniformly distributed over the mask. Applying the definition of the variant of the diagonal mask used in \citet{fu2024robot}:
% $$
% \mu^*_{t,t'} =
% \begin{cases} 
% \frac{1}{s} & \text{if } t' \in [t - k_m, t + k_m] \\
% 0 & \text{otherwise.}
% \end{cases}
% $$
% By definition of the reward,
% $$\tilde{R}_t =  - \sum_{t'=1}^{T'} (\mu^* \odot D)_{t, t'} $$
% $$\tilde{R}_t = - \frac{1}{s} \sum_{i=-k_m}^{k_m} d(\xi^L_t, \xi^D_{t+i})$$

% By the above definition, an agent that stays within the mask window for the entire trajectory will have a higher reward than one that exits the mask window for some period. If $k_m > 0$, an unsuccessful policy  $\pi'$ can always be constructed that stays within the mask window for the entire trajectory (since $k_m > 0$, it can be within the mask window without reaching the final demonstration state). If a successful policy $\pi^*$ is outside the mask window for at least one learner step, then:
% $$J_{\tilde{R}}(\pi') > J_{\tilde{R}}(\pi^*)$$
% This happens when $\pi$ moves significantly faster or slower than the demonstration (for example, if the agent requires more than $k_m$ steps to move between two subgoals). Under these conditions, the TemporalOT reward is hackable with respect to the ground truth reward.
% \end{proof} 






% \begin{proof}
% \wh{Proof in the case of $\gamma$-normalized return (correlated proxies definition)}:
% We assume $\tilde{\R}$ is r-correlated with $\R^*$. If this were not true, then the $\dtw$ reward function would be a bad reward function, and there would be no need to show that reward hacking occurs.

% The first part of the proposition follows by definition: $\pi'$ does not reach every reference state and $\pi^*$ reaches them all in the correct order, so $J_{\R^*}(\pi') < J_{\R^*}(\pi^*)$. 

% The policy and transition function are deterministic, so:
% $$J_\pi(\tilde{\R}) = (1-\gamma) \sum_{t=0}^T \gamma^t \tilde{\R}(s_t, a_t) $$
% Following the previous definitions of $\xi^{L*}$ and $\xi^{L-}$, we let the trajectories follow the same path until time $t=1$, at which point they diverge. For the rest of this proof, we only pay attention to the return starting from $t=1$, since the DTW return before this point is the same for both trajectories. 
% % Note that $d_{rem} = T$. 

% \textbf{Successful trajectory return:}
% $$J_{\tilde{\R}}(\pi^*) = (1-\gamma) * (\sum_{t=1}^{T / 2} \gamma^t * (-t) + \sum_{t=1}^{T / 2} \gamma^{T-t} * (-t)) $$
% $$J_{\tilde{\R}}(\pi^*) =\sum_{t=1}^{T/2} \gamma^t t (\gamma-1)  + \gamma^{T-t} t (\gamma-1) $$



% \textbf{Adversarial trajectory return:}
% $$J_{\tilde{\R}}(\pi') = (1-\gamma) * (\sum_{t=1}^{T -1} 0 + \gamma^T*(-(T-1)) = -\gamma^T(\gamma-1) + \gamma^T T(\gamma - 1) $$
% % Rewriting the last term as a summation:
% $$J_{\tilde{\R}}(\pi') = -\gamma^T(\gamma-1) + 2 \sum_{t=1}^{T/2} \gamma^T (\gamma-1) $$

% \textbf{Bounds:}
% Let $J^*=\sum_{t=1}^{T/2}  \gamma^t t (\gamma-1) $. Then, $J_{\tilde{\R}}(\pi^*) < J^*$ because $ (\gamma-1) < 0$, so $ \gamma^{T-t} t (\gamma-1) < 0$ for $t > 0$. We show that $$J^* < J_{\tilde{\R}}(\pi')$$

% Remove $t=1$ from both summation terms:
% $$ \gamma(\gamma - 1) + \sum_{t=2}^{T/2} \gamma^t t (\gamma-1) < -\gamma^T(\gamma-1)  + 2 \gamma^T(\gamma-1) + \sum_{t=2}^{T/2} 2 \gamma^T (\gamma-1)  $$

% $$ \gamma(\gamma - 1) + \sum_{t=2}^{T/2} \gamma^t t (\gamma-1) < \gamma^T(\gamma-1) + \sum_{t=2}^{T/2} 2\gamma^T (\gamma-1)  $$


% \textbf{Lemma 1: } $\gamma(\gamma - 1) \leq \gamma^T(\gamma-1) $ because $\gamma \geq \gamma^T$, and $\gamma-1 < 0$.

% \textbf{Lemma 2: } $\gamma^t t (\gamma-1)< 2\gamma^T(\gamma-1)$ for all $2\leq t < T$. This is because $\gamma^t > \gamma^T$ when $t < T$, so $\gamma^t t > 2\gamma^T$ when $2 \leq t < T$, and $\gamma-1 < 0$.

% The inequality holds directly by Lemma 1 and 2, so $J^* < J_{\tilde{\R}}(\pi')$. Thus, 
% $$J_{\tilde{\R}}( \pi^*) < J_{\tilde{\R}}(\pi')$$


% \end{proof}


