% \subsection{Definition of Reward Hacking \label{def:reward_hacking}}

% Following the problem formulation in Section~\ref{sec:preliminaries}, let the normalized expected return given a policy $\pi$ and a reward function $\R$ be $J_\R(\pi) = \E_{\xi^L \sim p(\xi^L | \pi)}[\sum_{t=0}^{T-1} \gamma^t \R(o^L_t, \xi^D)]$. 
% Adopting the definition of reward hacking in~\cite{skalse2022defining}, we define reward hacking in the context of optimizing a sequence-matching reward function as:
% \begin{definition}
%     Let $\R^*$ be the ground-truth reward function, and $\pi^*$ be the optimal policy for the ground-truth reward function $\pi^* = \arg\max_\pi J_{\R^*}(\pi)$. 
%     Let $\tilde{\R}$ be the sequence matching reward function.
%     Let $\hat{\pi}$ be some policy in the policy class $\hat{\pi} \in \Pi$. $\tilde{\R}$ is hackable with respect to $\R^*$ when
%     \begin{equation*}
%         J_{\R^*}(\hat{\pi}) < J_{\R^*}(\pi^*) \And J_{\tilde{\R}}(\hat{\pi}) > J_{\tilde{\R}}(\pi^*).
%     \end{equation*}
% \end{definition}


% We restate the definition of reward hacking supplied in \cite{anonymous2024correlated}.

% Let $\pi_\text{base}$ be some base policy for the given task (i.e., a policy with reasonable behavior).

% Let $R$ be the ground truth reward function and $\tilde{R}$ be an $r$-correlated proxy reward with respect to $\pi_\text{base}$

% Let J be the normalized return of a policy $\pi$ under $R$ with discount factor $\gamma\in (0,1]$: 
% $$J(R,\pi) = (1-\gamma) \mathbb{E} \sum_{t=0}^T \gamma^t R(s_t, a_t) $$
% Then, reward hacking occurs if 
% $$J(\pi, R) < J(\pi_\text{base}, R) \ \text{for some} \ \pi \in \argmax J(\pi, \tilde{R})$$