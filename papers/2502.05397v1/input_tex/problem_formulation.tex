\section{Problem Formulation}
Sequence-matching problems focus on tasks where it is critical to follow the entire sequence in the correct order: e.g., for assembly tasks, robots must assemble all parts in the demonstrated order. 
% Fig.~\ref{fig:main} shows a similar phenomenon: the robot must pick up the tool before pushing the water bottle; directly manipulating the bottle with its end-effector can damage it. 
We model the problem as a Markov Decision Process (MDP) $(\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)$. At time $t$, the agent at state $s_t \in \mathcal{S}$ receives an \emph{image} observation $o_t \in \mathcal{O}$ of the state. On taking action $a_t \in \mathcal{A}$, it transitions to a new state $s_{t+1} \in \mathcal{S}$ and gets a new observation $o_{t+1} \in \mathcal{O}$.

\emph{We assume the reward function for this task is unknown.} Instead, we assume access to a \emph{single} visual demonstration of the task, $\xid = \{\od_1, \dots, \od_{\tilde{T}}\}$.
The robot policy must follow all subgoals in the same order as the demonstration.
Importantly, this demonstration may be \emph{temporally misaligned} with the learner’s trajectory and have a different execution speed. 
% \SC{one more sentence to elaborate, For example, ...}
% For example, demonstrations often vary in timing because a demonstrator might take unnecessary pauses or speed through familiar subgoals. 
Additionally, the demonstration lacks state and action labels, rendering classical imitation learning inapplicable. 
Instead, we approach this as an inverse reinforcement learning (IRL) problem, where the reward $\mathcal{R}(o_{1:t}, \xid)$ is defined as a function of the visual demonstration $\xid$ and the learner's observations.
The goal is to learn a policy $\pi^*(a | s)$ that maximizes the expected discounted sum of rewards:
\begin{equation*}
    \pi^* = \arg\max_\pi \mathbb{E}_{\pi} \left[ \sum_{t=1}^{T} \gamma^t \mathcal{R}(o_{1:t}, \xid) \right].
\end{equation*}

% \section{Failures with Sequence-Matching Tasks}
\section{Desiderata of Sequence-Matching Rewards\label{sec:desiderata}}
\label{sec:dist_fail}

IRL can be formulated as a distribution matching problem between the learner and the demonstrator on ``moments", i.e., expectations of reward basis functions~\cite{swamy2021moments}. 
Hence, solving IRL is equivalent to optimizing Integral Probability Metrics (IPMs) between the learner and demonstrator distributions~\cite{sun2019provably}. Recent works use optimal transport (OT) by minimizing the Wasserstein distance (an IPM) between embeddings of the frames in the learner and demonstration trajectories. 
They work with Markovian moments that depend on only a single frame.

However, \emph{the true reward function for a sequence-matching task depends on trajectory history} to determine whether the agent has followed the subgoals in order, so it is not in the span of Markovian moments.
Instead of matching the distribution over frames, we should be matching the distribution over trajectories.
% If the learner and demonstration trajectories were temporally aligned, we could simply match the learner and demonstration states in each timestep, but this method fails under temporal misalignment.
% Instead, we define the optimal policy as one that can cover all the same subgoals as the demonstration in the correct order. 
We propose that the corresponding reward should be a measure of:
\begin{enumerate}[noitemsep, topsep=0pt]
    \item \textbf{Subgoal Ordering}: A learner trajectory that completes the subgoals in the correct order should receive a higher cumulative reward than one that completes the subgoals in the wrong order.
    \item \textbf{Subgoal Coverage}: A learner trajectory that completes more of the subgoals should receive a higher cumulative reward than one that completes less of them.
\end{enumerate}
% If a learner trajectory maximizes such a reward function with respect to the demonstration, it must have covered all the subgoals in the correct order, thus completing the task.

Rewards that use frame-level matching fail to satisfy these desiderata: OT fails to enforce subgoal covering (Sec.~\ref{subsec:ot_fail}), and mitigations to OT's problem fail to enforce full subgoal coverage (Sec.~\ref{subsec:dtw_fail} and~\ref{subsec:tot_fail}). 
% Rewards that use frame-level matching fail to satisfy these desiderata. Specifically, rewards defined via OT fail to enforce the temporal order of demonstration subgoals (Sec.~\ref{subsec:ot_fail}). Solutions to this problem attempt to incorporate temporal constraints on the frame-level matching, but they fail to encourage full subgoal coverage given temporally misaligned demonstrations (Sec.~\ref{subsec:dtw_fail} and~\ref{subsec:tot_fail}). 
% , breaking the IPM framework. 
% We show that these methods result in policies that fail to reach all subgoals (Sec.~\ref{subsec:dtw_fail} and~\ref{subsec:tot_fail}).  
% Moments from single observations cannot possibly span the class containing the expert's objective function. Thus, frame level matching is not sufficient for the sequence-following task. Furthermore, this implies that \emph{any} IRL method that assumes Markovian moments, including Maximum Entropy IRL \cite{ziebart2008maximum} and GAIL \cite{ho2016generative}, cannot optimize the expert's objective in the sequence following task.

% In this section, we show a practical example of how minimizing the optimal transport distance between frames results in policies that do not respect the temporal order of subgoals. Prior work, such as TemporalOT \cite{fu2024robot}, has also noticed this limitation of optimal transport. However, solutions to this problem attempt to place temporal constraints on frame-level matching, without recognizing that frame-level matching is fundamentally limited. We show examples of how two matching functions with temporal constraints (DTW and TemporalOT) can result in policies that do not visit every subgoal in the expert demonstration.

\input{figure_table_tex/ot_failure}
% \input{figure_table_tex/dtw_failure}. 
\subsection{Optimal Transport Fails to Respect Ordering}
\label{subsec:ot_fail}
% Optimal Transport (OT)~\cite{peyré2020computationaloptimaltransport} finds the optimal coupling to transport one distribution to another distribution. 
Prior works in imitation learning use Optimal Transport (OT)~\cite{peyré2020computationaloptimaltransport}, specifically the 
Wasserstein distance, to measure how closely a learner trajectory matches a demonstration trajectory~\citep{papagiannis2022imitation, tian2024what, fu2024robot, kedia2024oneshotimitationmismatchedexecution}.
These approaches assume a Markovian distance metric $d(\cdot,\cdot)$ that measures the distance between the embeddings of a learner and demonstration frame. 
Given a learner trajectory $\xi = \{o_{t}\}^{T}_{t=1}$ and a video demonstration $\xid = \{\od_j\}^{\Td}_{j=1}$, they define corresponding learner and demonstration distributions that are uniform in time. Specifically, $\rho = \frac{1}{T} \sum^{T}_{t=1} \delta_{o_t}$, where $\delta_{o_t}$ is a Dirac distribution centered on $o_t$. 
The demonstration similarly has a distribution of $\tilde{\rho} = \frac{1}{\Td} \sum^{\Td}_{j=1} \delta_{\od_{j}}$. 
Consequently, the matching matrices are evenly weighted, defined as the set $M = \{\mu \in \mathbb{R}^{T \times \Td}: \mu \mathbf{1} = \frac{1}{T}, \mu^T \mathbf{1} = \frac{1}{\Td}\}$.
% Given a distance matrix $D$ such that $D_{i,j} = d(o_i, \od_j)$,
With an entropy regularizer $\mathcal{H(\cdot)}$, OT solves for the optimal matching matrix:
\begin{equation}\label{eq:ot_mu_star}
    \mu^* = \arg\min_{\mu \in M} \sum_{t=1}^{T}\sum_{j=1}^{\Td} d(o_t, \od_j) \mu_{t, j} - \epsilon \mathcal{H}(\mu).
\end{equation}
The OT reward is:
\begin{equation}\label{eq:distr_rew}
     \mathcal{R}_\text{OT}(o_t,\xi, \xid) = - \sum_{j=1}^{\Td} d(o_t,\od_j) \mu^*_{i,j}.
\end{equation}
Although this is a non-Markovian reward function, \textit{it still matches Markovian moments}. This is clear in its failure to enforce subgoal ordering.
\begin{counterexample}
There exists an MDP (Fig.~\ref{fig:ot_fail}) where OT fails to penalize violations of temporal ordering.
\end{counterexample}
The OT reward uses a Markovian distance based on two frame embeddings, and there are no temporal constraints on $\mu^*$ in (\ref{eq:ot_mu_star}).
% In (\ref{eq:ot_mu_star}), there are no temporal constraints on $\mu$.
Thus, OT does not penalize trajectories that complete subgoals in the wrong order. Fig.~\ref{fig:ot_fail} shows an example where a suboptimal trajectory that visits the subgoals in the \emph{reversed} order has the same OT reward as the optimal one. 

% Fig.~\ref{fig:ot_fail} shows that OT fails for tasks that require completing subgoals in specific orders.
% A suboptimal trajectory that visits the subgoals in the \emph{reversed} order has the same OT reward as the optimal trajectory. 
% Appendix~\ref{proof:suboptimal_ot} generalizes this observation.

\subsection{Dynamic Time Warping Fails to Cover All Subgoals}
\label{subsec:dtw_fail}
% \wh{I think we should just switch to DTW since the proof is for that. Nobody uses SDTW anyways, and this keeps it simple}
Dynamic Time Warping (DTW)~\cite{dtw} overcomes the subgoal ordering problem of OT by temporally aligning the learner and demonstration trajectories, while allowing for flexible distribution of the assignment over each demonstration frame.
Specifically, each matching matrix $\mu$ must align the beginning and the end of two trajectories (i.e., $\mu_{1, 1} = 1$ and $\mu_{T, \Td} = 1$),
% each learner frame must be coupled with at least one demonstration frame; 
and the indices of its positive entries must be non-decreasing in time.
Subject to these constraints, DTW solves for the optimal matching:
$\mu^* = \arg\min_\mu \sum_{t=1}^{T} \sum_{j=1}^{\Td} \mu_{t,j} d(o_t, \od_j)$.

Given $\mu^*$, the DTW reward is the same as (\ref{eq:distr_rew}).
% Note that $\mu^*$ need not be evenly weighted (i.e., it may not be in $M$).
DTW's temporal constraints allow it to avoid the example failure of OT (shown in Fig.~\ref{fig:ot_fail_full} of Appendix~\ref{app:toy_orca_success}), but they lead to a new failure mode.

\begin{counterexample}
There exists an MDP (Fig.~\ref{fig:dtw_fail_full}, App.~\ref{app:toy_orca_success}) where DTW fails to penalize incomplete subgoal coverage.
\end{counterexample}
Although DTW constrains the order of assignment, it does not limit the number of learner frames matched with each subgoal. Once the learner reaches an intermediate subgoal, it can achieve high DTW reward by remaining in that subgoal until the last few frames of the trajectory. Fig.~\ref{fig:dtw_fail_full} of Appendix~\ref{app:toy_orca_success} shows an example where a trajectory stuck in the second subgoal achieves the same DTW reward as a trajectory that completes all subgoals. This is a local minimum that could cause RL agents to stall at earlier subgoals.  
% Appendix~\ref{proof:suboptimal_dtw} proves that there always exists an unsuccessful trajectory with the same total DTW reward as the successful trajectory in the 2D-Navigation environment. 


% \subsection{TemporalOT~\cite{fu2024robot}, a mitigation to incorporate temporal order into OT, fails under temporal misalignment.}

\subsection{TemporalOT Fails to Cover All Subgoals under Temporal Misalignment}
\label{subsec:tot_fail}
Recognizing the limitations of OT, \citet{fu2024robot} propose TemporalOT, which alters the OT objective to optimize over a temporally masked cost matrix. 
Specifically, the mask is a variant of the diagonal matrix with a hyperparameter $k_w$ to define the width of the diagonal:
\begin{equation}
    W_{t,j} = 
\begin{cases} 
1, & \text{if } j \in [t - k_w, t + k_w], \\
0, & \text{otherwise}. 
\end{cases}
\end{equation}
Consequently, TemporalOT solves for the optimal matching matrix $\mu^*\in M$ with the same constraints as (\ref{eq:ot_mu_star}):
\begin{equation}\label{eq:tot_mu_star}
\mu^* = \arg\min_{\mu \in M} \sum_{t=1}^{T}\sum_{j=1}^{\Td} W_{t,j} d(o_t, \od_j) \mu_{t,j} - \epsilon \mathcal{H}(W \odot \mu).
\end{equation}
% \begin{equation}\label{eq:tot_mu_star}
% \mu^* = \arg\min_{\mu} \langle M \odot \mu, D \rangle_F - \epsilon \mathcal{H}( M \odot \mu)
% \end{equation}
% \SC{TOT should look exactly the same as OT with the addition of the mask. So please make them consistent. Also M is used both as coupling matrices and as the mask}
Given $\mu^*$, the TemporalOT reward is the same as (\ref{eq:distr_rew}). 

% \SC{Can you formalize this as a counter example please like the others?}
\begin{counterexample}
There exists an MDP (Fig.~\ref{fig:tot_fail_full}, App.~\ref{app:toy_orca_success}) where TemporalOT fails to penalize trajectories that do not reach every subgoal given a temporally misaligned demonstration.    
\end{counterexample}
To define the mask window, TemporalOT makes the strong assumption that the demonstration is temporally aligned with the learner trajectory. 
Regardless of how temporally misaligned a demonstration is, TemporalOT's matching matrix always approximates a diagonal matrix, matching an equal proportion of learner frames to each subgoal. 
Fig.~\ref{fig:tot_fail_full} of Appendix~\ref{app:toy_orca_success} shows an example where a suboptimal, slower policy that does not reach later subgoals has a higher TemporalOT reward than an optimal policy that makes consistent progress to complete the task. 

% The diagonal nature of the mask and TemporalOT's formulation
% Both OT and TemporalOT's formulation assumes that the learner and demonstration distributions are uniform in time, its optimal coupling matrix 
% TemporalOT was originally designed for learner and reference sequences of the same length. In this paper, we compare against a slightly stronger version of TemporalOT, which allows for a learner that is linear in the speed of the expert (either faster or slower). Specifically, we mask with a windowed diagonal matrix stretched along the longer of the learner and demonstration axis. 

% \subsection{Sequence Matching Desiderata}\label{subsec:desiderata}
% \subsection{Desiderata for Sequence-Matching Rewards}\label{subsec:desiderata}
% The true reward function for a sequence-matching task depends on trajectory history to determine whether the agent has followed the subgoals in order, so it is not in the span of Markovian moments. 
% Instead of matching the distribution over frames, we should be matching the distribution over trajectories (or, given just one demonstration, matching a single trajectory).
% If the learner and demonstration trajectories were temporally aligned, we could simply match the learner and demonstration states in each timestep. 
% % When $k_w=0$, and for small levels of entropy regularization, this is exactly how \tot{} works: $\mathcal{R}_t=d(o_t, \od_t)$. 

% However, under temporal misalignment, the per-timestep state distribution cannot be matched. 
% Instead, we define the optimal policy as one that can cover all the same subgoals as the demonstration in the correct order. 
% We propose that the corresponding reward should be a measure of:
% % Instead, we define a \textit{temporally aligned expert} to be any policy that covers all the same subgoals as the expert in the correct order, and is temporally realizable to the learner. Matching this new expert naturally leads to two reward function desiderata:
% % Given a single video demonstration, this amounts to defining and matching trajectory moments with the demonstration. This can be viewed as defining a reward function over a trajectory and optimizing it via RL. 
% % We propose that this reward should be a measure of:
% % The expert's sequence matching objective is not contained in the span of Markovian moments, so frame-level matching approaches are destined to fail. In the sequence following task, \emph{IPMs should be optimized over the distributions of learner and expert trajectories}, not the distributions of frames.

% % In this problem setting, we consider deterministic policies and environments where only a single expert demonstration is given. Thus, optimizing IPMs over the trajectory distributions simply amounts to minimizing the distance between the moments of a single policy rollout and the given demonstration. 

% % The problem of minimizing an arbitrary distance function over the space of learner trajectories naturally lends itself to RL. Motivated by the observed limitations of OT, DTW, and TemporalOT, we propose that this reward should be a measure of:
% \begin{enumerate}[noitemsep, topsep=0pt]
%     \item \textbf{Subgoal Ordering}: a learner trajectory that completes the subgoals in the correct order should receive a higher cumulative reward than one that completes the subgoals in the wrong order.
%     \item \textbf{Subgoal Coverage}: a learner trajectory that completes more of the subgoals should receive a higher cumulative reward than one that completes less of them.
% \end{enumerate}
% If a learner trajectory maximizes such a reward function with respect to the demonstration, it must have covered all the subgoals in the correct order, thus completing the task.

% The key problem, and core methodological contribution of this paper, is defining a distance function over the space of trajectories that accurately models the sequence following task.



% \textbf{Even distribution fails under mismatched execution speed.} 

% The simplest case of progressive assignment is an assignment matrix that is evenly distributed over the reference trajectory. When $T\geq T'$, the assignment matrix is defined as:

% \begin{equation}
% \mu^*_{t, t'} =
% \begin{cases}
% 1 & (t'-1)*N \lt t \leq t' * N \\
% 0 & \text{otherwise}.
% \end{cases}
% \end{equation}

% where 
% $N=\left\lfloor \frac{T}{T'} \right\rfloor$

% For example, if there are 2 reference frames, the assignment matrix would appear as follows:
% \[
% \mu^* =
% \begin{bmatrix}
% 1 & 0  \\
% \vdots & \vdots \\
% 1 & 0  \\
% 0 & 1  \\
% \vdots & \vdots\\
% 0 & 1 \\
% \end{bmatrix}.
% \]
% If the learner and reference trajectories have the same length ($T=T'$), this becomes the identity matrix.  

% Giving the coupling matrix, the final per-timestep reward is computed in the same manner as optimal transport.

% This approach assumes that the speed of the learner is linear in the speed of the demonstration. If it takes the learner more steps to complete a given expert frame than all the other frames, fig \wh{Even dist Failure} shows how the learner can fail. 

% \textbf{TemporalOT fails under mismatched execution speed}
% \wh{proof of OT beocomes even distribution}


% In order to fix the time inconsistency of OT, progressive assignment approaches place constraints on the assignment matrix. Even-distribution and TemporalOT explicitly constrain the reference frames to which each learner frame can be assigned. These methods assume that the learner and demonstrator have equal execution speeds, but this is not always true. DTW fixes this problem by only constraining the assignment of the first and last learner frame. However, this allows it to put an abnormally large amount of assignment on intermediate frames, resulting in agents that get stuck. An assignment matrix that encourages progress does not allow for mismatched execution, and vice versa. \textit{ Under the class of distribution matching algorithms, desiderata 2 and 3 are in direct conflict.}

%%%%%%%%%%%%% SDTW version
% Given two trajectories $\xi^L$ and $\xi^D$, and a distance metric $d(o^L_t, o^D_{t'})$, SDTW solves for the optimal probability distribution $p^*$ over all alignment paths $A \in \Omega$.
% \citet{pmlr-v80-mensch18a} shows that the SDTW objective function can be written in an entropy regularized formulation controlled by a temperature term $\lambda \geq 0$: $p^* = \arg\max_p \sum_{A \in \Omega} \sum_{t=0}^{T-1} \sum_{t'=0}^{T'-1} [p(A) A]_{t,t'} d(o^L_t, o^D_{t'}) - \lambda H(p)$.
% Because of its time consistency constraints, SDTW can be solved in polynomial time via dynamic programming~\cite{soft-dtw} using the soft-min operator: $\min^\lambda\{x_1, \dots, x_n\} = -\lambda \log \sum_{i=1}^n e^{-x_i/\lambda}$.
% Note that Dynamic Time Warping (DTW), which uses the minimum operator instead, is a special case of SDTW when $\lambda = 0$~\cite{soft-dtw}. 

% Given the optimal probability distribution $p^*$ over all paths, the SDTW reward function is:
% \begin{equation}\label{eq:sdtw_reward}
%     \mathcal{R}_{SDTW}(o^L_t, \xi^D) = - \sum_{t'=0}^{T'} d(o^L_t, o^D_{j}) A^*_{t, t'} \text{\space where \space} A^* = \sum_{A \in \mathcal{A}} p^*(A) A
% \end{equation}
% where $A^*$ is the optimal soft alignment path defined based on the optimal distribution $p^*$. 

% \input{figure_table_tex/sdtw_failure}
% SDTW's time consistency constraints allow it to avoid the example failure case of OT (shown in Fig.~\ref{fig:ot_fail_full}). However, Fig.~\ref{fig:sdtw_fail} demonstrates SDTW's limitation: \emph{it fails to incentivize progress along the demonstrated trajectory}. Consider a trajectory that makes one move of progress past the first demonstration state and another trajectory that just gets stuck in the first demonstration state. Even if the first trajectory does not reach the next demonstration state yet, it should have a higher reward because it makes more progress. Instead, the second trajectory has a higher SDTW reward. This is an example of a local minimum that could cause the agent to stall at intermediate states instead of making meaningful progress during RL training. Appendix~\ref{proof:suboptimal_dtw} proves that, when SDTW has temperature 0, there will always exist an unsuccessful trajectory with the same total DTW reward as the successful trajectory in the 2D-Navigation environment. 


%%%%%%%%%%%%%%%
% In order to solve the problems of OT, it is necessary to include some constraints on the temporal ordering of the assignment. An alignment matrix $A$ between $\xi^L_i$ and  $\xi^D_j$ is a binary matrix where $A_{ij} = 1$ if and only if $\xi^L_i$ is assigned to $\xi^D_j$. An alignment matrix must additionally satisfy time consistency constraints, which require that (1) the beginning and the end of the two trajectories are aligned (i.e., $A_{0, 0} = 1$ and $A_{T-1, T'-1} = 1)$ and (2) the alignment indices are monotonically non-decreasing in time. 

% Dynamic Time Warping (DTW) \cite{dtw} finds the alignment path $A^*$ that minimizes the distance $d(o^L_t, o^D_{t'})$ among the set of all possible paths $\Omega$. Soft Dynamic Time Warping (SDTW) solves for the optimal probability distribution $p^*$ over alignment paths. ~\citet{pmlr-v80-mensch18a} shows that SDTW is an entropy-regularized formulation of DTW that controls the proportion of the learner trajectory that can be aligned to one timestep of the demonstration trajectory. 
% Let $\mathcal{P}$ be the set of probability distributions over all alignment paths $\mathcal{A}$ between the learner and demonstration trajectories. When written with a temperature term $\lambda$, the objective of SDTW solves for $p \in \mathcal{P}$ that minimizes $\sum_{A \in \Omega} \sum_{t=0}^{T-1} \sum_{t'=0}^{T'-1} [p(A) A]_{t,t'} d_v(o^L_i, o^D_{j}) - \lambda H(p)$.
% % $\arg\min_{p \in \mathcal{P}} \sum_{A \in \mathcal{A}} \sum_{t=0}^{T-1} \sum_{t'=0}^{T'-1} [p(A) A]_{t,t'} d_v(o^L_i, o^D_{j}) - \lambda H(p)$.
% Given the optimal probability distribution $p^*$ over all paths, the SDTW reward function is:
% \begin{equation}
%     \mathcal{R}_{SDTW}(o^L_t, \xi^D) = - \sum_{t'=0}^{T'} d(o^L_t, o^D_{j}) A^*_{t, t'} \text{\space where \space} A^* = \sum_{A \in \mathcal{A}} p^*(A) A
% \end{equation}
% % \input{figure_table_tex/sdtw_failure}
%  $A^*$ is the optimal soft alignment path defined based on the optimal distribution $p^*$. Although this appears to require an inner loop over the set of all possible alignment matrices, SDTW can be solved in polynomial time via dynamic programming~\cite{soft-dtw} using the soft-min operator: $\min^\lambda\{x_1, \dots, x_n\} = -\lambda \log \sum_{i=1}^n e^{-x_i/\lambda}$. Note that DTW is a special case of SDTW when $\lambda = 0$ \cite{soft-dtw}.

%  These additional constraints allow SDTW to avoid the example failure case of OT, as shown in fig. \ref{fig:ot_fail_full}. Since the assignment cannot go backwards in time, the reverse order trajectory is no longer able to achieve the same reward as the successful trajectory. 

% \textbf{Soft Dynamic Time Warping fails to encourage progress. }
% Although SDTW results in a temporally consistent assignment, fig. ~\ref{fig:sdtw_fail} demonstrates that it has a new failure mode: \emph{it fails to incentivize progress along the demonstrated trajectory}. Consider two suboptimal trajectories that make the same progress overall. The first suboptimal trajectory moves one step forward at an earlier timestep, while the second suboptimal trajectory moves one step forward at the last timestep. Ideally, the first trajectory should have higher reward because it makes immediate progress. Instead, the second trajectory has a higher SDTW reward, so during RL training, the agent is likely stuck in this suboptimal trajectory, making it more difficult to explore and escape this local minimum. Appendix~\ref{proof:suboptimal_dtw} proves that, when SDTW has temperature is 0, there always exists an unsuccessful trajectory with the same total reward as the successful trajectory in the 2D-Navigation environment. 

% %%%%% Conclusion: what's our desiderata
% The investigation reveals two key desiderata for the reward function: 
% (1) It must \emph{enforce temporal constraints}, and
% (2) It must \emph{assign higher reward to trajectories that make more progress}.



% The optimal probability distribution $p^*$ overall all paths, by proxy, defines the optimal soft alignment matrix $A^* = \sum_{A \in \mathcal{A}} p^*(A) A$. Then, the SDTW reward function is defined as:
% \begin{equation}
%     \mathcal{R}_{SDTW}(o^L_t, \xi^D) = - \sum_{(i, j) \in A} p^*(A) d_v(o^L_i, o^D_{j}).
% \end{equation}




% Each alignment path $A \in \Omega$ is a sequence of $K$ tuples of indices that satisfies time consistency constraints, where the beginning and the end of two trajectories must be aligned (i.e., $A_0=(i_0, j_0)$ and $A_{K-1}=(i_{T-1}, j_{T'-1})$) and the indices for both trajectories must monotonically increase.
% % (i.e., $\forall (i_k, j_k) \in A, i_{k-1} \leq i_k \leq i_{k-1} + 1, j_{k-1} \leq j_{k} \leq j_{k-1} + 1$). 
% The objective of DTW can be written as $A^* = \arg\min_{A \in \Omega} \sum_{(i, j) \in A} d(o^L_i, o^D_{j})$.
% DTW can be solved via dynamic programming by choosing the indices that have the minimum cumulative cost. Similar to OT, based on the solved optimal alignment path $A^*$, the DTW-based reward function is:
% \begin{equation}
%     \mathcal{R}_{DTW}(o^L_t, \xi^D) = - \sum_{(t, j) \in A^*} d(o^L_t, o^D_{j}).
% \end{equation} 
% where $(t, j) \in A^*$ represents tuples that is aligning the learner $o^L_t$ with the demonstration $o^L_j$.



% In Fig.~\ref{fig:seq_algo_fail} (a), DTW-based reward function demonstrates how it enforces time consistency as the optimal trajectory gets higher reward compared to the reversed, suboptimal trajectory. 
% However, Fig.~\ref{fig:seq_algo_fail} (b) reveals the weakness of DTW-based reward function. 
% Consider a suboptimal trajectory that is stuck at the initial position until the last timestep. 
% OT limits the proportion of the learner trajectory that can get assigned to one timestep of the demonstration trajectory, but DTW poses no such constraint.
% % Thus, DTW aligns most of the suboptimal trajectory to the first timestep of the demonstration trajectory and only the last timestep of the learner trajectory to the last timestep due to the constraint on a valid alignment path (i.e. $A_{K-1}=(i_{T-1}, j_{T'-1})$).
% Thus, DTW outputs an alignment path that causes the suboptimal trajectory to have the same reward as the optimal trajectory. In Appendix~\ref{proof:suboptimal_dtw}, we prove that there always exists a suboptimal trajectory that receives the same DTW-based reward as the optimal trajectory in the 2D navigation domain. 


% In Fig.~\ref{fig:seq_algo_fail} (a), DTW-based reward function demonstrates how it enforces time consistency as the optimal trajectory gets higher reward compared to the reversed, suboptimal trajectory. 
% However, Fig.~\ref{fig:seq_algo_fail} (b) reveals the weakness of DTW-based reward function. 
% Consider a suboptimal trajectory that is stuck at the initial position until the last timestep. 
% OT limits the proportion of the learner trajectory that can get assigned to one timestep of the demonstration trajectory, but DTW poses no such constraint.
% % Thus, DTW aligns most of the suboptimal trajectory to the first timestep of the demonstration trajectory and only the last timestep of the learner trajectory to the last timestep due to the constraint on a valid alignment path (i.e. $A_{K-1}=(i_{T-1}, j_{T'-1})$).
% Thus, DTW outputs an alignment path that causes the suboptimal trajectory to have the same reward as the optimal trajectory. In Appendix~\ref{proof:suboptimal_dtw}, we prove that there always exists a suboptimal trajectory that receives the same DTW-based reward as the optimal trajectory in the 2D navigation domain. 

