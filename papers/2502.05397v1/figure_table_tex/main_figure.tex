\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/main_fig.png}
    \vskip -0.005in
    \caption{\small \textbf{\orca{} overview.} The expert video demonstrates the \textit{Stick-Push} task, where the robot must grasp the tool before pushing the water bottle. However, this demonstration is \emph{temporally misaligned} because it contains long pauses before picking up the tool. To learn from this demonstration, \orca{} provides a per-timestep reward for the visual learner trajectory $\xi = \{o_{t}\}^{T}_{t=1}$. (1) Each frame is passed through an off-the-shelf visual encoder before calculating its distance with respect to the other frames. (2) The \orca{} reward calculates the probability that the learner has covered \emph{all} the frames in the correct order.
    % The goal is to define a per-timestep sequence-matching reward for a visual learner trajectory $\xi^L = \{o_{t}^L\}^{T}_{t=0}$ given a visual demonstration trajectory $\xi^D = \{o_{t'}^D\}^{T'}_{t'=0}$. (1) Each frame is passed through the visual encoder to predict the robot state and estimate uncertainty. (2) Given the uncertainty-scaled visual distance matrix, SDTW computes the optimal alignment matrix. (3) This matrix is used to compute per-timestep rewards and apply reward bonuses, resulting in the final sequence-matching reward.
    }
    \label{fig:main}
\end{figure*}
