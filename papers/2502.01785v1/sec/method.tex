\section{Proposed AquaticCLIP Model}
\label{sec:method}
This work introduces the Aquatic Contrastive Language-Image Pre-training (AquaticCLIP) model, which leverages a collection of 2M aquatic image-text pairs curated form several heterogeneous resources. 
The ground truth descriptions are also enriched by harnessing the existing VLM MarineGPT.
The primary objective is to pre-train AquaticCLIP using diverse aquatic data sourced from various platforms, enhancing its capability for zero-shot transfer across different aquatic imagery tasks.
This is particularly beneficial for recognizing unfamiliar marine species and coral reef categories that were not encountered during training.
Fig. \ref{introFigure} illustrates the key components of the proposed model, which include the construction of the 2M aquatic image-text paired dataset further enriched by unsupervised generated textual descriptions, caption cleaning, a lightweight prompt-guided vision encoder, a vision-guided text encoder, and the pre-training process. 
The contrastive learning approach aligns positive image-text pairs while separating negative ones. 
The details of these processes are discussed in the subsequent sections.

\subsection{Aquatic Dataset Construction and Curation}
\label{sec:dataset}
Our dataset construction pipeline involves two main steps: gathering image-text paired data from multiple sources and cleaning and filtering the collected data.
We assembled a dataset of 2 million aquatic image-text pairs from various resources, including YouTube videos, marine and ocean sciences textbooks and articles, the Corals of the World \cite{veron2016corals}, Fishes of Australia \cite{van2014family, schodde1997zoological, merrick2006australasian,shelley2017revision}, Marine Twitter, Netflix, and National Geographic (NatGeo) \cite{doe2023coral, national_geographic_coral, ng_coral_bleaching, ng_ocean_acidification,ng_great_barrier_reef,ng_coral_conservation,ng_ocean_biodiversity}.

For YouTube videos, we searched using keywords such as ``underwater world'', ``marine documentary'',``deep oceans'', ``great barrier reef'', ``aquatic scenes'', and ``coral reefs'' etc. 
For Netflix videos, we explored hundreds of documentaries, including ``My Octopus Teacher'', ``Last Breath'', and ``Wonders of the Reef'', etc.
Subtitles provided by both resources were used to generate aligned image-text pairs, which were manually checked and refined. 
Unique frames were extracted every 50 seconds from the videos, which often contained challenges like low visibility, motion blur, background clutter, and color distortions.

Additionally, We utilized 1200 diverse textbooks on marine biology and oceanography, along with research articles from ocean and marine journals and NatGeo magazines, to further enrich the dataset. 
Figures and captions were extracted using PDF-Figures 2.0 tool \cite{clark2016pdffigures}, and we manually refined the data to ensure the selected images had meaningful associated text. 
Images not related to aquatic environments were discarded.

We also included image-text pairs from the Corals of the World repository and Fishes of Australia, only selecting pairs with detailed descriptions. 
Furthermore, we used the Twitter platform to search for relevant content using hashtags like $\#$MarineBiology, $\#$Oceans, and $\#$Fisheries, considering only channels with over 100 followers. 
After a thorough cleaning and filtering process, we retained 2 million high-quality image-text pairs representing a diverse range of aquatic scenes.
\textit{More details are provided in the supplementary material.}
%%\vspace{-5mm}
\subsection{Unsupervised Generation of Image and Instance-Level Descriptions (Fig. \ref{introFigure} (b))} 
\label{sec:generation}
%%\vspace{-2mm}
In order to enrich the ground-truth textual descriptions, we generated additional textual descriptions at both the image and instance levels using a VLM MarineGPT \cite{zheng2023marinegpt}, which includes a frozen ViT image encoder and Q-former.
%For each image in our dataset, we manually verified ground truth textual descriptions, which were then combined with the pseudo-textual descriptions during the pre-training stage. 
%\textit{Our AquaticCLIP model is pre-trained on 2M image-text pairs, where the ground-truth descriptions of the images were combined with pseudo-textual descriptions.}
At the image level, each image $\textbf{I}_{i}$ is input into MarineGPT to generate its corresponding textual descriptions. 

For the instance level, we pre-trained MRegionCLIP, an object detector based on RegionCLIP \cite{zhong2022regionclip} and MarineDet \cite{haixin2023marinedet} and applied it to our 2M imagery dataset to detect all instances in zero-shot settings. 
Each instance was then passed through MarineGPT to generate a textual description.
Specifically, we used the following prompt template: ``The image is $<\textbf{image}>$. Describe the object in this image:'', where $<\textbf{image}>$ is the image token.
The generated textual descriptions $\textbf{S}_{i}$ at the image and instance levels were then cleaned using our cleaning module, which is explained in the following subsection.
%were combined with ground-truth descriptions $\textbf{G}_{i}$ to generate more enriched and comprehensive textual data $\textbf{C}_{i}$ for further processing. 
%The generated textual descriptions $\textbf{S}_{i}$ were cleaned using our cleaning module, which is explained in the following section.

\subsection{Semantic Filtering and Cleaning of Generated Textual Descriptions (Figs. \ref{introFigure} (c)-(f))}
\label{sec:cleaning}
The generated textual descriptions $\textbf{S}_{i}$ may contain noise, such as broken sentences, incorrect descriptions, or irrelevant keywords. 
To address these issues, we developed a textual description cleaning module aimed at identifying the semantically closest and most relevant keywords.

In this process, each generated textual description $\textbf{S}_{i}$ is broken down into a set of $k$-keywords ($\{\textbf{s}_{i}^{j}\}_{j=1}^{k}$).
For each keyword, we compute its cosine similarity with the image embedding as follows:
%%\vspace{-2mm}
\begin{equation}
\hat{\textbf{S}}_{i}= \argmax_{s\in S}< \Phi_{v}(\textbf{I}_{i})\cdot  \Phi_{t}(\textbf{s}_{i}^{j})>, 
\label{eqn1}
%%\vspace{-3mm}
\end{equation}

\noindent where $\Phi_{v}$ is a vision encoder followed by an MLP, and $\Phi_{t}$ is a text encoder from the CLIP model. 
We retain the top-p$\%$ of keywords in $\hat{\textbf{S}}_{i}$, discarding the rest as noise. 
This cleaning process ensures that the remaining keywords are semantically aligned with the visual content, improving the quality of the textual descriptions.


For each image in our dataset, we manually verified ground truth textual descriptions $\textbf{G}_{i}$. 
During the pre-training stage, the refined keywords $\hat{\textbf{S}}_{i}$ both at the image and instance levels were combined with ground-truth descriptions $\textbf{G}_{i}$ to generate more enriched and comprehensive textual data $\textbf{C}_{i}$ for further processing. 
\textit{Our AquaticCLIP model is pre-trained on 2M image-text pairs, where the ground-truth descriptions of the images were combined with refined keywords.}

\subsection{Prompt-guided Vision Encoder (Fig. \ref{encoder} (a))}
\label{sec:pgve}
To generate efficient visual embeddings for each aquatic image $\textbf{I}_{i}$, we aggregate the patch features of the input image using learned visual prompts. 
First, the input image $\textbf{I}_{i}$ is divided into $n_{p}$ non-overlapping patches $\{w_{i}^{j}\}_{j=1}^{n_{p}}$, each of size $m \times m$.
These patches are fed into the pre-trained image encoder $\Phi_{v}$ to produce embeddings $\textbf{P}_i=\{\textbf{p}_{i}^{j}\}\in \mathbb{R}^{d_{p} \times n_p}$.

To effectively aggregate these patch embeddings into a final image-level embedding for similarity calculation, we designed a prompt-guided image encoder, as illustrated in Fig. \ref{encoder} (a).
We randomly initialize a set of learnable prompt features $\textbf{Q}_{i}=\{r_{i}^{j}\}_{j=1}^{n_{r}} \in \mathbb{R}^{d_p \times n_r}$, where $n_{r}$ represents the number of learnable prompts. 
These prompts guide the progressive fusion of patch embeddings. 
Cross-attention is then computed using the visual embeddings as keys $\textbf{K}_{i}=\textbf{P}_i$ and values $\textbf{V}_{i}=\textbf{P}_i$, while the prompts $\textbf{Q}_{i}$ serve as queries.
%%\vspace{-5mm}
\begin{equation}
\textbf{E}_{i}=\textrm{Softmax} \Bigg( \frac{\textbf{Q}_{i}\textbf{K}_{i}^\top}{\sqrt{d_{p}}}\Bigg)\textbf{V}_{i},\\
\textbf{E}_{i}=\textrm{Norm}(\textbf{E}_{i})+\textbf{Q}_i,
\label{eqn2}
%%\vspace{-3mm}
\end{equation}
\noindent The learnable prompts help prioritize patches with high semantic similarity, resulting in a more meaningful image-level representation that captures global contextual information. 
The final image-level features are derived using an attention-based feature fusion method, as shown below:
%%\vspace{-3mm}
\begin{equation}
\textbf{E}^{'}_{i}=\textbf{W}_{1}\textbf{E}_{i},\textbf{e}^{}_{i}=\exp(\textbf{W}_{3}^{\top}(\textrm{tanh}(\textbf{W}_{2}\textbf{E}^{'}_{i}))),
%%\vspace{-1mm}
\end{equation}

\noindent Here, $\textbf{W}_{1}$, $\textbf{W}_{2} \in \mathbb{R}^{d_p\times d_p}$ and $\textbf{W}_{3 }\in \mathbb{R}^{1\times d_p}$ are learnable matrices, and the softmax function is used to compute attention weights  $\textbf{a}_i(j)$. 
The image-level representation $\textbf{f}_{i}$ is then computed as follows:
%%\vspace{-3mm}
\begin{equation}
\textbf{a}_i(j)=\frac{\textbf{e}_i(j)}{\sum_{k=1}^{n_r} \textbf{e}_i(k)},~\textrm{and}~\textbf{f}_{i}=\textbf{W}_{4} \sum_{j=1}^{n_{r}}\textbf{a}_i(j)\textbf{E}^{'}_{i}(j),
%%\vspace{-4mm}
\end{equation}
\noindent where $\textbf{W}_{4}\in \mathbb{R}^{d_{p} \times d_p}$ is a learnable weight matrix and $\textbf{E}^{'}_{i}(j)$ is a column vector of $\textbf{E}^{'}_{i}$.

\subsection{Vision-guided Text Encoder (Fig. \ref{encoder} (b))}
\label{sec:vgte}
In the text encoder branch, the enriched textual descriptions $\textbf{C}_{i}$ are fed into the CLIP text encoder to obtain textual representations $\textbf{T}_{i}$  corresponding to the descriptions of the $i$-th image. 
These representations are then passed through a vision-guided attention layer for refinement. 
The patch features $\textbf{P}_{i}$ and the learned prompts $\textbf{E}_{i}$ are concatenated as $\textbf{V}_i$, which serves as the key $\textbf{K}_{t}$ and value $\textbf{V}_{t}$, while the textual representations $\textbf{T}_{i}$ are used as the query, as shown in (Fig. \ref{encoder} (b)).
The vision-guided attention mechanism is computed as follows:
%%\vspace{-5mm}
\begin{equation}
\textbf{U}_{i}=\textrm{Softmax} \Bigg( \frac{\textbf{T}_{i}\textbf{K}_{t,i}^\top}{\sqrt{d_{p}}}\Bigg)\textbf{V}_{t,i},\\
\textbf{T}_{i}=\textbf{T}_{i}+\textbf{U}_i,
\label{eqn}
%%\vspace{-2mm}
\end{equation}
\noindent This context-guided text encoder further refines the textual features by incorporating image context and learned visual prompts.
This process enhances the alignment between images and texts, improving the performance of the AquaticCLIP model.
%%%\vspace{-1mm}

\subsection{Cross-Modal Contrastive Loss for Vision-Language Alignment (Fig. \ref{introFigure} (i))}
\label{loss}
We pre-train our prompt-guided vision encoder and vision-guided text encoder using a cross-modal contrastive loss function. 
This loss is formulated as a temperature-scaled vision-language pre-training loss, similar to $W$-way classification, where $W$ represents the batch size of image-text pairs involved in the training process \cite{radford2021learning, chen2020simple, tian2020contrastive}.
Given a batch of $W$ paired normalized image and text embeddings $\{\textbf{f}_{i},\textbf{T}_{i}\}$, we minimize the contrastive loss in two directions: image-to-text ($i \rightarrow t$) and text-to-image ($t \rightarrow i$) as:
%The image-to-text loss is defined as:
%%\vspace{-2mm}
\begin{equation}
\mathcal{L}_{i2t}= -\frac{1}{W}\sum_{i=1}^{W} \log \frac{\exp (\tau \textbf{T}_{i}^{\top}\textbf{f}_{i} )}{\sum_{j=1}^{W}\exp(\tau \textbf{T}_{i}^{\top}\textbf{f}_{j})},
%%%\vspace{-2mm}
\end{equation}
%\noindent The text-to-image loss is defined as:
%%\vspace{-2mm}
\begin{equation}
\mathcal{L}_{t2i}=-\frac{1}{W}\sum_{j=1}^{W} \log \frac{\exp (\tau \textbf{f}_{j}^{\top}\textbf{T}_{j} )}{\sum_{i=1}^{W}\exp(\tau \textbf{f}_{j}^{\top}\textbf{T}_{i})},  
%%\vspace{-2mm}
\end{equation}

\noindent where $\tau$ is a learnable temperature parameter that controls the smoothness of the distribution \cite{radford2021learning}.
The overall contrastive loss, $\mathcal{L}_{cont}$ , is the sum of both losses: $\mathcal{L}_{cont}=\mathcal{L}_{i2t}+\mathcal{L}_{t2i}$.
This loss minimizes the distance between embeddings of positive image-text pairs and maximizes the distance between negative pairs $(\textbf{f}_{i},\textbf{T}_{j})$, where $i\ne j$, ensuring that images and texts with the same semantic content have similar representations in the feature space.

\subsection{Zero-shot Transfer for Image Classification}
%%\vspace{-2mm}
Radford \textit{et al.} introduced the concept of zero-shot classification using a prompt-based approach \cite{radford2021learning}.
In our method, each class in the test dataset is converted into one or more text prompts using predefined templates, such as ``An image of \{Sea Urchins\}." or ``An image of \{Oyster\}." 
For each test image, we compute the $\ell_{2}$ normalized embeddings using our prompt-guided vision encoder and vision-guided text encoder. 
We then calculate the cosine similarity between the test image and the set of testing prompts to find the best match, resulting in zero-shot classification. 
Additional details are provided in the supplementary material.
%%\vspace{-4mm}

\begin{figure}[t!]
    \centering    
    \includegraphics[width=3.5in, height=2.8in]{encoder.pdf}
    %%%\vspace{-8mm}
    \caption{\textbf{(a) Prompt-Guided Vision Encoder:} The prompt-guided attention mechanism combines patch features $\textbf{P}_{i}$ with initialized prompts $\textbf{Q}_{i}$ through layer normalization and an MLP, followed by softmax to produce the final image features $\textbf{f}_{i}$.
\textbf{(b) Vision-Guided Text Encoder:} Text embeddings $\textbf{T}_{i}$ are refined using a vision-guided attention mechanism, where patch features $\textbf{P}_{i}$, learned prompts $\textbf{E}_{i}$, and text embeddings $\textbf{T}_{i}$ are concatenated to compute attention $\textbf{U}_{i}$, which further enhances $\textbf{T}_{i}$.}
    \label{encoder}
    %%\vspace{-10mm}
    \end{figure}