\section{Experimental Evaluations}
\label{sec:results}
%\vspace{-2mm}
We conducted extensive experiments to evaluate the performance of the proposed AquaticCLIP model across various tasks, including zero-shot classification of marine species, fine-grained fish, and coral species classification. 
Additionally, we performed zero-shot cross-modal retrieval tasks for aquatic images (\textit{see supplementary material}). 
For downstream tasks, we applied fine-tuned instance segmentation, semantic segmentation of underwater imagery, as well as marine object detection, classification, and counting (\textit{see supplementary material}). 
These experiments, covering a range of classification, detection, and segmentation tasks, allowed us to thoroughly assess AquaticCLIP's performance. 
We also compared our results with SOTA methods, including both VLMs and vision-only approaches.

\begin{table*}[t!]
\caption{\textbf{Architectural ablation} highlighting the importance of the Prompt-Guided Vision Encoder (PGVE) and Vision-Guided Text Encoder (VGTE), as well as the impact of frozen versus fine-tuned image $\Phi_{v}$ and text encoders $\Phi_{t}$.
The results, reported as $F_{1}$ scores, reflect zero-shot classification performance on six datasets.
The fully fine-tuned AquaticCLIP model with both PGVE and VGTE achieves the highest scores, demonstrating the effectiveness of fine-tuning and these key components.}
%\vspace{-7mm}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.90}{
\begin{tabu}{|[2pt]c|c|c|c|c|[2pt]c|c|c|c|c|c|[2pt]}
\tabucline[0.5pt]{-}
Variants&Frozen ($\Phi_{v}$)&Frozen ($\Phi_{t}$)&PGVE &VGTE&MAI&SAI&FishNet&FNOI&LSF&CC\\\tabucline[0.5pt]{-}
Frozen CLIP&\checkmark&\checkmark&$\times$&$\times$&0.692&0.702&0.651&0.622&0.772&0.752\\\tabucline[0.5pt]{-}
AquaticCLIP$_{1}$&\checkmark&\checkmark&\checkmark&\checkmark&0.736&0.754&0.762&0.672&0.831&0.823\\\tabucline[0.5pt]{-}
AquaticCLIP$_{2}$&\checkmark&\checkmark&\checkmark&$\times$&0.721&0.725&0.667&0.651&0.780&0.783\\\tabucline[0.5pt]{-}
AquaticCLIP$_{3}$&\checkmark&\checkmark&$\times$&\checkmark&0.718&0.716&0.731&0.696&0.807&0.811\\\tabucline[0.5pt]{-}
Finetune CLIP&Finetune&Finetune&$\times$&$\times$&0.772&0.847&0.771&0.750&0.853&0.831\\\tabucline[0.5pt]{-}
AquaticCLIP$_{4}$&Finetune&Finetune&\checkmark&$\times$&0.838&\underline{0.890}&\underline{0.821}&0.753&0.892&0.910\\\tabucline[0.5pt]{-}
AquaticCLIP$_{5}$&Finetune&Finetune&$\times$&\checkmark&\underline{0.842}&0.876&0.808&\underline{0.766}&\underline{0.912}&\underline{0.932}\\\tabucline[0.5pt]{-}
AquaticCLIP&Finetune&Finetune&\checkmark&\checkmark&\textbf{0.871}&\textbf{0.923}&\textbf{0.842}&\textbf{0.801}&\textbf{0.934}&\textbf{0.953}\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table1}
%\vspace{-5mm}
\end{table*}
%\vspace{-2mm}
\subsection{Training and Implementation Details}
%\textbf{Instance Detection:} We fine-tuned the existing object detector, RegionCLIP \cite{zhong2022regionclip}, using the CLIP text encoder and ResNet50 as the vision backbone. 
%The fine-tuning was done on publicly available marine datasets containing 228,363 images and 664,411 instances (details are provided in the supplementary material). This fine-tuned model, named %Marine RegionCLIP (MRegionCLIP), followed the same experimental protocols as MarineDET \cite{haixin2023marinedet} and MaineInst \cite{ziqiang2024marineinst}.
%The learning rate was initially set to $5 \times 10^{-2}$, gradually reduced to $5 \times 10^{-4}$ and $5 \times 10^{-6}$ as needed. 
%The fine-tuning was conducted over 9,000 iterations with a batch size of 256 on four A100 GPUs.
%The quantitative object detection results of MRegionCLIP are included in Table \ref{table11}.
%MRegionCLIP was then used to detect instances in our 2M images dataset in a zero-shot fashion.
%The instances detected from each image were then input to the MarineGPT \cite{zheng2023marinegpt} for generating instance-level captions.
The architecture of AquaticCLIP consists of a frozen domain-specific captions generator MarineGPT \cite{zheng2023marinegpt}, the CLIP \cite{radford2021learning} image encoder using the ViT-B/16-224 \cite{dosovitskiy2020image}, and a transformer-based text encoder \cite{radford2019language}. 
We fine-tuned four components: the image encoder, text encoder, prompt-guided vision encoder, and vision-guided text encoder, all using the cross-modal contrastive loss described in Sec. \ref{loss}.
We employed the Adam optimizer \cite{loshchilov2017decoupled} with an initial learning rate of $1 \times 10^{-4}$ and a weight decay of $1 \times 10^{-5}$.
The model was trained for 80 epochs on four A100 GPUs with a batch size of 512. 
We set the number of prompts to 20. 
For fair comparisons, we utilized the same evaluation metrics as used by the SOTA methods.
For the classification tasks, we reported both accuracy and $F_{1}$ measure scores. 
For object detection, we reported mAP$_{50}$ metric.
\textit{Additional details are provided in supplementary material.}
%\vspace{-2mm}
%For pre-training, all images were pre-processed to a size of $512 \times 512$.
%Larger images were resized to 512 on the short side and center-cropped. 
%Data augmentation, including horizontal and vertical flips, was applied to both images and captions. 
%A linear projection head was used to map the text and image embeddings into a 512-dimensional latent space for alignment. 
%The image and text representations were aligned using the cross-modal contrastive loss (Sec. \ref{loss}), and the model was implemented using the PyTorch library.
%In a second experiment, we only fine-tuned our image and text encoders $\Phi_{v}$ and $\Phi_{t}$ using similar settings to CLIP \cite{radford2021learning} with our 2M images and generated text dataset. 
%The settings included a batch size, weight decay of 0.2, a temperature of 0.07, a peak learning rate of $1 \times 10^{-4}$, AdamW optimizer with an initial learning rate of $5 \times 10^{-6}$, and %a cosine decay scheduler.

\subsection{Underwater Datasets and Tasks}
For the zero-shot marine species classification task, we utilized two datasets: Marine Animal Images (MAI) \cite{marine_animal} and Sea Animals Images (SAI) \cite{sea_animal}.
For zero-shot fine-grained classification, we employed three datasets: FishNet \cite{khan2023fishnet}, FishNet Open Images (FNOI) \cite{kay2021fishnet}, and Large-Scale Fish (LSF) \cite{ulucan2020large}.
For zero-shot fine-grained coral species classification, we used the Coral Species Classification (CSC) \cite{coral-species-classification_dataset} and Coral Classification (CC) \cite{coral_classification} datasets.
For object detection and classification, we used four datasets including FishNet, DeepFish \cite{saleh2020realistic}, Brackish \cite{Pedersen_2019_CVPR_Workshops}, and URPC \cite{urpc}.
\textit{Additional details are provided in supplementary material.}
%\vspace{-2mm}
\begin{comment}
\subsection{Downstream Datasets}
For supervised salient object segmentation of underwater images, we used the USOD10K dataset \cite{usod10k}.
For supervised instance segmentation of marine images, we employed the Underwater Image Instance Segmentation (UIIS) \cite{Lian_2023_ICCV}, and for semantic segmentation, we used the SUIM \cite{islam2020suim} dataset. 
For fine-tuned underwater object detection and classification tasks, we used four datasets: FishNet \cite{khan2023fishnet}, DeepFish \cite{saleh2020realistic}, URPC \cite{urpc}, and Brackish \cite{Pedersen_2019_CVPR_Workshops}. 
Lastly, for the biodiversity-related marine object counting task, we used the IOCFish5K \cite{sun2023indiscernible} dataset.
\textbf{\textit{See more details in our supplementary material.}}
\end{comment}

\begin{table*}[t!]
\caption{\textbf{Ablation study} showing the effect of the Textual Descriptions Cleaning Module (TDCM) and the use of instance-level and image-level textual descriptions on AquaticCLIP zero-shot classification performance ($F_{1}$ scores). 
The full AquaticCLIP model, with TDCM and both text levels, delivers the best results across all datasets, especially on MAI (0.871), SAI (0.923), and CC (0.953). 
Variants with components removed or altered exhibit reduced performance, highlighting the importance of each component.}
%\vspace{-7mm}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.90}{
\begin{tabu}{|[2pt]c|c|c|c|[2pt]c|c|c|c|c|c|[2pt]}
\tabucline[0.5pt]{-}
Variants&TDCM&Instance Text&Image Text&MAI&SAI&FishNet&FNOI&LSF&CC\\\tabucline[0.5pt]{-}
AquaticCLIP&\checkmark&\checkmark&\checkmark&\textbf{0.871}&\textbf{0.923}&\underline{0.842}&\textbf{0.801}&\textbf{0.934}&\textbf{0.953}\\\tabucline[0.5pt]{-}
AquaticCLIP$_{6}$&$\times$&\checkmark&\checkmark&\underline{0.854}&0.891&0.804&\underline{0.786}&0.897&\underline{0.934}\\\tabucline[0.5pt]{-}
AquaticCLIP$_{7}$&\checkmark&$\times$&\checkmark&0.853&0.906&0.804&0.765&0.911&0.933\\\tabucline[0.5pt]{-}
AquaticCLIP$_{8}$&\checkmark&\checkmark&$\times$&0.840&0.892&0.823&0.784&0.921&0.915\\\tabucline[0.5pt]{-}
%AquaticVision&$\times$&$\times$&$\times$&0.852&\underline{0.915}&\textbf{0.856}&\underline{0.786}&\underline{0.924}&0.928\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table2}
%\vspace{-8mm}
\end{table*}

\subsection{Ablation Studies}
We conducted ablation studies to highlight the contributions of each component of the proposed AquaticCLIP model. 
The evaluations were performed on zero-shot classification tasks, reporting $F_{1}$ scores across six independent external datasets (MAI, SAI, FishNet, FNOI, LSF, and CC) that were not used during the pre-training phase. \noindent \textbf{1. FrozenCLIP vs. FinetuneCLIP (Table \ref{table1})}: In FrozenCLIP, pre-trained image and text encoders were used to generate visual and text embeddings. 
In FinetuneCLIP, these encoders were fine-tuned using the contrastive pre-training loss on our 2M dataset. 
We observed that fine-tuning resulted in improved performance across all datasets.
\noindent \textbf{2. AquaticCLIP$_{1}$ vs. AcquaticCLIP (Table \ref{table1})}: AquaticCLIP$_{1}$ used frozen image and text encoders, with only the PGVE (Prompt-Guided Vision Encoder) and VGTE (Vision-Guided Text Encoder) fine-tuned. 
Significant performance improvements were observed when all four encoders were fine-tuned in AquaticCLIP. \noindent \textbf{3. Importance of PGVE and VGTE Components (Table \ref{table1})}: In AquaticCLIP$_{2}$ and AquaticCLIP$_{4}$, VGTE was removed, and original textual embeddings were used, while in AquaticCLIP$_{3}$ and AquaticCLIP$_{5}$, PGVE was removed and image-level embeddings from MLP were used. 
Both cases showed performance reductions compared to AquaticCLIP$_{1}$ and AquaticCLIP, highlighting the importance of these components.\\
\noindent \textbf{4. Importance of Textual Description Cleaning Module (TDCM) (Table \ref{table2})}: In AquaticCLIP$_{6}$, removing the TDCM module led to decreased performance compared to the full AquaticCLIP model. \noindent \textbf{5. Importance of Instance-level Text and Image-level Text (Table \ref{table2})}: In AquaticCLIP$_{7}$ and AquaticCLIP$_{8}$, either instance-level or image-level text descriptions were removed. 
Performance dropped in both cases compared to the proposed AquaticCLIP, which utilized both levels of text. 
For fine-grained classification (FishNet, FNOI, LSF), instance-level descriptions performed better, while for coarse-grained classification (MAI, SAI), image-level captions were more effective.
\textit{More ablations are provided in supplementary material.}
%\vspace{-2mm}


\begin{table*}[t!]
\centering
\caption{Zero-shot and supervised classification performance comparison in terms of accuracy and $F_{1}$ score of AquaticCLIP against SOTA VLMs and vision-only models. 
AquaticCLIP consistently outperforms all models across multiple datasets, excelling in both zero-shot and supervised tasks. 
It achieves top $F_{1}$ scores, particularly in MAI, SAI, FishNet, and CC datasets, demonstrating superior generalization and classification accuracy compared to traditional vision-only models and other VLMs. 
Family classification performance is reported for FishNet, while CSC is excluded from supervised methods due to its small size.}
%\vspace{-3mm}
\makebox[\linewidth]{
\scalebox{0.70}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Zero-Shot VLMs} &MAI \cite{marine_animal}&SAI \cite{sea_animal}&FishNet \cite{khan2023fishnet}&FNOI \cite{kay2021fishnet}&LSF \cite{ulucan2020large}&CSC \cite{coral-species-classification_dataset}&CC \cite{coral_classification} \\
\hline
Frozen CLIP \cite{radford2021learning}&0.702$|$0.692&0.711$|$0.702&0.663$|$0.651&0.642$|$0.622&0.770$|$0.772&0.809$|$0.783&0.763$|$0.752\\
Finetune CLIP&0.802$|$0.772&0.856$|$0.847&0.770$|$0.771&0.763$|$0.750&0.864$|$0.853&0.862$|$0.841&0.845$|$0.831\\
CoOp \cite{zhou2022learning}&0.853$|$0.822&0.866$|$0.853&0.752$|$0.744&0.764$|$0.752&0.863$|$0.854&0.903$|$0.888&0.868$|$0.866\\
MAPLE \cite{khattak2023maple}&0.861$|$0.834&0.867$|$0.860&0.750$|$0.748&0.774$|$0.769&0.864$|$0.859&0.903$|$0.893&0.881$|$0.876\\
GPT4V \cite{yang2023dawn}&0.831$|$0.811&0.832$|$0.834&0.801$|$0.791&0.758$|$0.743&0.892$|$0.881&0.854$|$0.841&0.881$|$0.876\\
BLIP2 \cite{li2023blip}&0.813$|$0.801&0.821$|$0.818&0.783$|$0.788&0.728$|$0.727&0.893$|$0.880&0.793$|$0.782&0.862$|$0.853\\
MarineGPT \cite{zheng2023marinegpt}&0.862$|$0.844&0.892$|$0.883&\underline{0.823}$|$0.815&0.776$|$0.769&0.912$|$0.905&0.918$|$0.903&0.881$|$0.876\\
AquaticCLIP$_{1}$&0.766$|$0.736&0.746$|$0.754&0.772$|$0.762&0.684$|$0.672&0.844$|$0.831&0.897$|$0.882&0.835$|$0.823\\
AquaticCLIP$_{7}$&\underline{0.879}$|$0.853&\underline{0.912}$|$0.906&0.814$|$0.804&0.773$|$0.765&0.917$|$0.911&\underline{0.944}$|$0.932&\underline{0.938}$|$0.933\\
AquaticCLIP$_{8}$&0.855$|$0.840&0.898$|$0.892&0.821$|$0.823&\underline{0.796}$|$0.784&\underline{0.932}$|$0.921&0.942$|$0.938&0.926$|$0.915\\
AquaticCLIP&\textbf{0.892}$|$\textbf{0.871}&\textbf{0.935}$|$\textbf{0.923}&\textbf{0.850}$|$\textbf{0.842}&\textbf{0.822}$|$\textbf{0.801}&\textbf{0.942}$|$\textbf{0.934}&\textbf{0.968}$|$\textbf{0.964}&\textbf{0.961}$|$\textbf{0.953}\\
\hline 
\textbf{Supervised Vision Models} &MAI \cite{marine_animal}&SAI \cite{sea_animal}&FishNet \cite{khan2023fishnet}&FNOI \cite{kay2021fishnet}&LSF \cite{ulucan2020large}&CSC \cite{coral-species-classification_dataset}&CC \cite{coral_classification} \\
\hline 
ResNet-34 \cite{he2016deep}&0.821$|$0.802&0.731$|$0.726&0.408$|$0.423&0.475$|$0.456&0.761$|$0.756&-&0.833$|$0.821\\
ResNet-50 \cite{he2016deep}&0.830$|$0.811&0.739$|$0.728&0.403$|$0.428&0.504$|$0.488&0.773$|$0.764&-&0.847$|$0.840\\
ResNet-101 \cite{he2016deep}&0.842$|$0.816&0.745$|$0.740&0.363$|$0.354&0.511$|$0.496&0.809$|$0.792&-&0.872$|$0.861\\
ViT-S \cite{alexey2020image}&0.862$|$0.856&0.783$|$0.774&0.379$|$0.367&0.652$|$0.633&0.844$|$0.833&-&0.891$|$0.883\\
ViT-B \cite{alexey2020image}&0.880$|$0.873&0.827$|$0.812&0.429$|$0.412&0.671$|$0.665&0.881$|$0.876&-&0.910$|$0.903\\
ViT-L \cite{alexey2020image}&0.893$|$0.881&0.856$|$0.833&0.484$|$0.476&0.714$|$0.706&0.914$|$0.902&-&0.919$|$0.920\\
BeiT \cite{bao2021beit}&0.881$|$0.873&0.856$|$0.845&0.542$|$0.522&0.704$|$0.688&0.871$|$0.867&-&0.895$|$0.883\\
ConvNeXt \cite{liu2022convnet}&0.847$|$0.852&0.812$|$0.803&0.606$|$0.587&0.714$|$0.702&0.827$|$0.807&-&0.914$|$0.902\\
ConvNeXt \cite{liu2022convnet}+ FL \cite{lin2017focal}&0.881$|$0.870&0.834$|$0.822&0.551$|$0.544&0.738$|$0.722&0.831$|$0.842&-&0.905$|$0.903\\
ConvNeXt \cite{liu2022convnet}+ CB \cite{cui2019class}&0.883$|$0.872&0.851$|$0.842&0.613$|$0.605&0.745$|$0.731&0.870$|$0.855&-&0.917$|$0.905\\
AquaticVision (Linear Probing)&\underline{0.912}$|$\underline{0.890}&\underline{0.945}$|$\underline{0.924}&\underline{0.922}$|$\underline{0.902}&\underline{0.846}$|$\underline{0.833}&\underline{0.942}$|$\underline{0.934}&-&\underline{0.947}$|$\underline{0.931}\\
AquaticCLIP (Linear Probing)&\textbf{0.915}$|$\textbf{0.893}&\textbf{0.951}$|$\textbf{0.944}&\textbf{0.934}$|$\textbf{0.923}&\textbf{0.882}$|$\textbf{0.867}&\textbf{0.968}$|$\textbf{0.961}&-&\textbf{0.963}$|$\textbf{0.958}\\
\hline  
      \end{tabular}
      }}
    \label{table6}
    %\vspace{-5mm}
\end{table*}



\begin{comment}
\noindent \textbf{6. Comparison of Heterogeneous Textual Sources (Table \ref{table3})}: We evaluated AquaticCLIP with text descriptions generated by MarineGPT (MGPT), GPT4V, and BLIP2. 
MGPT provided the best results due to its aquatic domain-specific pre-training, while GPT4V and BLIP2 were trained on out-of-domain data. 
Additionally, combining ground truth and MGPT-generated texts produced the best performance, as they complemented each other in generating more accurate textual descriptions.\\
\noindent \textbf{7. Effect of the number of learnable prompts on performance (Table \ref{table5})}: We varied the number of learnable prompts $n_{r}$ in the Prompt-Guided Vision Encoder (PGVE) from 5 to 30, as shown in Table \ref{table5}. 
Performance improved with an increasing number of prompts, peaking at a value of 0.842 for $n_{r}=20$ on the FishNet dataset. 
With fewer prompts, visual features were not efficiently represented, while beyond 20 prompts, additional prompts became redundant and did not contribute to performance gains.\\
\noindent \textbf{8. Performance variation with varying top-p$\%$ keywords (Table \ref{table5})}: In the image-text caption cleaning module, we retained different percentages of top matching keywords, ranging from 90$\%$ to 10$\%$, as shown in Table \ref{table5}.
The best performance was achieved when 20$\%$ of the top keywords were retained. 
Higher percentages likely included noisy keywords that degraded performance, while lower percentages discarded too many important keywords, causing a performance drop.\\
\end{comment}

\begin{comment}
\noindent \textbf{9. AquaticCLIP vs. AquaticVision using Linear Probing (Table \ref{table4})}: %We pre-trained a vision-only model, AquaticVision, using contrastive loss on our \textcolor{blue}{2M} dataset in a self-supervised learning setting, with the ViT-B/16-224 backbone.
We pre-trained a vision-only model, AquaticVision, using contrastive loss on our 2M images dataset in a self-supervised learning setting. 
For this purpose, a DINOv2 pre-training paradigm is utilized based on ViT architecture \cite{oquab2023dinov2}.
For downstream classification tasks, we trained a linear layer for each dataset in a supervised manner for both AquaticVision and AquaticCLIP. 
As shown in Table \ref{table4}, AquaticCLIP consistently outperformed AquaticVision, highlighting the importance of integrating vision-language supervision.
\end{comment}

\subsection{SOTA Methods for Comparison}
We compared our AquaticCLIP model to a wide range of SOTA methods across various underwater image analysis tasks. 
For zero-shot classification, we compared the performance of our AquaticCLIP model with existing VLMs, including Frozen CLIP \cite{radford2021learning}, Finetune CLIP \cite{radford2021learning}, and prompt-based VLMs like CoOp \cite{zhou2022learning} and MAPLE \cite{khattak2023maple}, as well as GPT4V \cite{yang2023dawn}, BLIP2 \cite{li2023blip}, and MarineGPT \cite{zheng2023marinegpt}.
We fine-tuned CoOp and MAPLE using the original author's provided source codes. 
For supervised classification tasks, we compared AquaticCLIP with models such as ResNet-34/50/101 \cite{he2016deep}, ViT-S/B/L \cite{alexey2020image}, BeiT \cite{bao2021beit}, ConvNeXt \cite{liu2022convnet}, ConvNeXt \cite{liu2022convnet} + Focal Loss (FL) \cite{lin2017focal}, and ConvNeXt \cite{liu2022convnet} + Class-Balanced (CB) \cite{cui2019class}.
To ensure a fair comparison, we used the same settings as FishNet \cite{khan2023fishnet}. 
For object detection, we compared with FasterRCNN \cite{ren2016faster}, YOLOF \cite{chen2021you}, TOOD \cite{feng2021tood}, MarintInst \cite{ziqiang2024marineinst}, MarineDet \cite{haixin2023marinedet}.
\textit{Further details are given in the supplementary material.}
%\vspace{-2mm}
\subsection{Zero-shot Comparisons}
Table \ref{table6} presents the zero-shot classification results and comparisons with SOTA VLM-based methods across seven datasets. 
AquaticCLIP consistently outperformed existing SOTA VLMs by a significant margin on all datasets. 
Significantly, on the CSC dataset, AquaticCLIP achieved a zero-shot performance of 96.80$\%$ accuracy and 96.40$\%$ $F_{1}$ score, the highest across all datasets. 
In more challenging fine-grained fish classification tasks on the FishNet, FNOI, and LSF datasets, AquaticCLIP achieved $F_{1}$ scores of 84.20$\%$, 80.10$\%$, and 93.40$\%$, respectively, due to the incorporation of instance-level captions in the model.
%\vspace{-2mm}
\subsection{Linear Probe Evaluations}
In this experiment, we conducted a linear probe evaluation of the AquaticCLIP model and compared it against several SOTA supervised vision-only models.
For this purpose, the vision encoder of the AquaticCLIP model is kept frozen and only a linear classifier is trained on top of the pre-extracted visual representations.
We also pre-trained our vision-only model, AquaticVision, using contrastive loss on our 2M images dataset in a self-supervised learning setting. 
For this purpose, DINOv2 pre-training paradigm is utilized based on ViT architecture \cite{oquab2023dinov2}.

The supervised comparisons between AquaticCLIP and existing SOTA vision-only models are shown in Table \ref{table6}. 
AquaticCLIP outperformed SOTA vision-based methods, achieving $F_{1}$ scores of 92.30$\%$, 86.70$\%$, and 96.10$\%$ on fine-grained classification datasets. 
We observed that AquaticCLIP also outperforms the AquaticVision model. 
While AquaticVision ranked as the second-best performer, it significantly outperformed other vision-only models due to its contrastive pre-training in a self-supervised learning manner on the 2M aquatic images.
%\vspace{-2mm}

%\subsection{Supervised Segmentation}
%\textbf{\textit{Please see our supplementary material.}}

%For the supervised segmentation task, we utilized the SAM foundational model \cite{Kirillov_2023_ICCV_SAM}, which includes a prompt encoder, an image encoder, and a lightweight mask decoder. 
%In our fine-tuning experiments, we replaced SAM's original image encoder with the proposed AquaticCLIP vision encoder $\Psi_{v}$, which is specially tuned for aquatic scenes. The remaining settings and %implementation details followed SAM's original setup.
%We dubbed our instance segmentation model as AquaticSAM. 
%For semantic segmentation, we introduced AquaticSeg, which adds a classification head fine-tuned for pixel-wise classification. 
%Following MarineInst’s \cite{ziqiang2024marineinst} experimental protocols, these segmentation experiments were conducted using the training splits for each dataset.
%Table \ref{table8} shows the results of salient object segmentation on the USOD10K \cite{usod10k} dataset, where AquaticSAM consistently outperformed six existing SOTA methods across all evaluation metrics, %without using additional information like edge or depth maps. 
%MarineInst also performed well but lagged behind AquaticSAM. 
%In Table \ref{table9}, the underwater instance segmentation results on the UIIS \cite{Lian_2023_ICCV} dataset show that AquaticSAM achieved the best mAP, AP$_{50}$, and AP$_{75}$ scores, though it ranked %second-best for small object segmentation (APS), with WaterMask R-CNN and its cascaded variant and MarineInst also performing well. 
%Table \ref{table10} highlights the semantic segmentation results on the SUIM \cite{islam2020suim} dataset, where AquaticSeg achieved the highest mIOU and F-score, with Mask2Former and MarineInst as the %second and third best performers.
%Overall, both AquaticSAM and AquaticSeg benefited from pre-training on the 2M aquatic images dataset. 
%Additionally, the use of both instance-level and image-level captions for language supervision contributed significantly to performance improvements, compared to using only image-level captions. 
%\textcolor{blue}{Some recent methods, like MarineInst, could not be compared due to unavailable code.}

\subsection{Object Detection and Classification Results}
In this experiment, we replaced ResNet-50 in MRegionCLIP with our pre-trained image encoder $\Phi_{v}$ and applied the same fine-tuning settings as discussed in the supplementary. 
We named this model AquaticDet and compared it against SOTA methods.
Table \ref{table11} shows object detection and classification results across four different datasets, with AquaticDet achieving the best mAP$_{50}$ scores across all compared datasets by a significant margin. 
This superior performance is attributed to pre-training on the 2M image-text paired dataset, which allowed AquaticDet to extract highly efficient and effective visual features. The inclusion of the prompt-guided vision encoder and vision-guided text encoder, along with comprehensive captions at both the image and instance levels, contributed to substantial performance improvements, even under challenging conditions.
%\vspace{-2mm}


%\subsection{Object Counting in Underwater Scenes}
%\textbf{\textit{Please see our supplementary material.}}
%For object counting in underwater scenes, we utilized a crowd localization transformer method \cite{cltr_liang2022end}, which includes a CNN-based backbone, a %transformer encoder, a transformer decoder, and a nearest neighbors matching component. 
%In our experiments, we replaced the original backbone and encoder with the AquaticCLIP pre-trained vision encoder $\Psi_{v}$, specifically fine-tuned for aquatic %environments. 
%The rest of the settings followed the original implementation. 
%We refer to our object counting model as AquaticOC.

%Table \ref{table12} presents the object counting results on the IOCFish5K \cite{sun2023indiscernible} dataset and compares AquaticOC with existing SOTA methods. 
%AquaticOC achieved the best results in terms of MAE and MSE, showing a significant improvement over the baseline CLTR model. 
%AquaticOC represents a strong contribution to efficient and accurate object counting in aquatic scenes.

\subsection{Computational Complexity}
We also evaluated the computational complexity of AquaticCLIP during the inference stage across various tasks, including zero-shot classification, supervised object detection and classification, supervised segmentation, and object counting. 
For zero-shot classification, AquaticCLIP took 0.80 seconds, AquaticSAM took 1.23 seconds, AquaticDet took 1.19 seconds, while the AquaticOC model took 1.31 seconds to count objects.
We used the same hardware settings as discussed above.
%\vspace{-4mm}



\begin{comment}
    
\begin{table}[t!]
\caption{\textbf{Ablation study} comparing ground truth (GT) text with generated text from MarineGPT (MGPT), GPT4V, and BLIP2 for zero-shot classification ($F_{1}$ scores). 
The combination of GT and MGPT yields the best performance across all datasets, with top scores on MAI (0.871), SAI (0.923), and CC (0.953). 
Generated text from GPT4V and BLIP2 shows lower performance, highlighting the advantage of using domain-specific MGPT with GT..}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.88}{
\begin{tabu}{|[2pt]c|c|c|c|c|c|[2pt]}
\tabucline[0.5pt]{-}
Variants&GT&GT+MGPT&MGPT&GPT4V&BLIP2\\\tabucline[0.5pt]{-}
MAI&\underline{0.861}&\textbf{0.871}&0.844&0.811&0.801\\\tabucline[0.5pt]{-}
SAI&\underline{0.902}&\textbf{0.923}&0.883&0.834&0.818\\\tabucline[0.5pt]{-}
FishNet&\underline{0.821}&\textbf{0.842}&0.815&0.791&0.788\\\tabucline[0.5pt]{-}
FNOI&\underline{0.785}&\textbf{0.801}&0.769&0.743&0.727\\\tabucline[0.5pt]{-}
LSF&\underline{0.917}&\textbf{0.934}&0.905&0.881&0.880\\\tabucline[0.5pt]{-}
CC&\underline{0.936}&\textbf{0.953}&0.917&0.876&0.853\\\tabucline[0.5pt]{-}
\end{tabu}
}
}
\end{center}
\label{table3}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[t!]
\caption{\textbf{Ablation study} comparing the performance of the vision-only model (AquaticVision) with the vision-language model (AquaticCLIP) for zero-shot classification ($F_{1}$ scores) on three datasets: FishNet, FNOI, and LSF. 
AquaticCLIP outperforms AquaticVision across all datasets, achieving the highest $F_{1}$ scores of 0.923 on FishNet, 0.867 on FNOI, and 0.961 on LSF.}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.80}{
\begin{tabu}{|c|c|c|c|}
\tabucline[1.5pt]{-}
Varaints&FishNet&FNOI&LSF\\\tabucline[0.5pt]{-}
AquaticCLIP&\textbf{0.923}&\textbf{0.867}&\textbf{0.961}\\\tabucline[0.5pt]{-}
AquaticVision&0.902&0.833&0.934\\\tabucline[1.5pt]{-}
%AquaticVision&0&0&0&0&0\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table4}
\end{table}
\end{comment}


\begin{comment}
\begin{table}[t!]
\caption{\textbf{Ablation study} on AquaticCLIP, detailing the impact of varying the number of learnable prompts $n_{r}$ in the Prompt-Guided Vision Encoder (PGVE) and the percentage of top-p$\%$ keywords. 
$F_{1}$ scores for zero-shot classification on the FishNet dataset indicate optimal performance at 20 prompts and 30$\%$ keyword retention, both achieving an $F_{1}$ score of 0.842. Changes in these parameters lead to minor performance variations.}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.80}{
\begin{tabu}{|c|c|c|c|c|c|c|}
\tabucline[1.5pt]{-}
Prompts ($n_{r}$)&5&10&15&20&25&30\\\tabucline[0.5pt]{-}
AquaticCLIP&0.822&0.835&0.839&\textbf{0.842}&0.840&0.841\\\tabucline[1.5pt]{-}
Top-p$\%$ keywords&90&70&50&30&20&10\\\tabucline[0.5pt]{-}
AquaticCLIP&0.791&0.811&0.830&0.837&\textbf{0.842}&0.826\\\tabucline[1.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table5}
\end{table}
\end{comment}




\begin{comment}
\begin{table}[t!]
\caption{Supervised salient object segmentation results on the USOD10K dataset \cite{usod10k}.
AquaticSAM outperforms other methods, including SVAM-Net \cite{jahidul2020svam}, CTDNet \cite{zhao2021complementary}, CDINet \cite{zhang2021cross}, SGL-KRN \cite{xu2021locate}, TC-USOD \cite{usod10k}, and MarineInst \cite{ziqiang2024marineinst}, achieving the best $S_{m}$ (0.943), $E^{max}_{\epsilon}$ (0.978), $\max F$ (0.942), and lowest MAE (0.018), despite not using additional cues like depth or edge information.}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.90}{
\begin{tabu}{|c|c|c|c|c|}
\tabucline[1.5pt]{-}
Methods&$S_{m} \uparrow$&$E^{max}_{\epsilon} \uparrow$&$\max F \uparrow$&MAE $\downarrow$\\\tabucline[0.5pt]{-}
SVAM-Net \cite{jahidul2020svam}&0.746&0.764&0.645&0.091 \\
CTDNet \cite{zhao2021complementary}&0.908&0.953&0.907&0.028\\
CDINet \cite{zhang2021cross}&0.704&0.864&0.736&0.090\\
SGL-KRN \cite{xu2021locate}&0.921&0.963&0.924&0.023\\
TC-USOD \cite{usod10k}&0.921&\underline{0.968}&0.923&0.020\\
DUAL-SAM \cite{zhang2024fantastic}&\underline{0.923}&\underline{0.968}&\underline{0.931}&\underline{0.018}\\
MarineInst \cite{ziqiang2024marineinst}&0.910&0.941&0.887&0.025\\
AquaticSAM&\textbf{0.943}&\textbf{0.978}&\textbf{0.942}&\textbf{0.012}\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table8}
\end{table}
\end{comment}



\begin{comment}
\begin{table}[t!]
    \centering
   \caption{Fine-tune Coral Segmentation: Comparison of various SOTA methods on the Coral Mask dataset \cite{zheng2024coralscop} Generation Quality.}
   \adjustbox{width=0.45\textwidth}{
    \begin{tabular}{ l c| c c c}
    \hline
    \multicolumn{1}{c}{Method} & Backbone & IoU $\uparrow$ & Accuracy $\uparrow$ & MAE $\downarrow$ \\
    \hline
    DeeplabV3 \cite{deeplab} & R101-D8 & 59.39 & 71.76 & 0.1634 \\
    CGNet \cite{cgnet} & FCN-4xB8 & 59.63 & 79.84 & 0.1197 \\
    GCNet \cite{gcnet_Cao2019} & R101-D8 & 61.75 & 78.35 & 0.1206 \\
    BiSeNetV2 \cite{bisenetv2_Yu2021} & FCN-FP16 & 60.90 & 78.11 & 0.1129 \\
    MaskFormer \cite{cheng2021maskformer} & R101-D32 & 67.21 & 79.46 & 0.0868 \\
    Mask2Former \cite{cheng2021mask2former} & R101-D32 & 69.17 & 77.63 & 0.0842 \\
    SegFormer \cite{segformer} & Mit-B5 & 71.51 & 84.29 & 0.0776 \\
    SAM \cite{Kirillov_2023_ICCV_SAM} & Vit-H & 31.16 & 38.05 & 0.5057 \\
    CoralSCOP \cite{zheng2024coralscop} & Vit-B & 59.02 & 79.88 & 0.1243 \\
    CoralSCOP \cite{zheng2024coralscop} & Vit-L & 65.87 & 78.45 & 0.1204 \\
    \hline
     & & & & \\
    \hline
    \end{tabular}}
    \label{finetune_coral_seg}
\end{table}
\end{comment}




\begin{comment}
\begin{table*}[t!]
    \centering
   \caption{Comparison of instance segmentation results on the UIIS dataset \cite{Lian_2023_ICCV} between AquaticSAM and other SOTA methods, all using ResNet101 as a backbone. 
AquaticSAM outperforms the other methods in several key metrics, demonstrating its superiority in small object segmentation and overall accuracy.}
   \adjustbox{width=0.95\textwidth}{
    \begin{tabular}{ l c| c c | c c c | c c c |}
    \hline
    \multicolumn{1}{c}{Method} & mAP $\uparrow$ & AP\textsubscript{50} $\uparrow$ & AP\textsubscript{75} $\uparrow$ & AP\textsubscript{\textit{S}} $\uparrow$ & AP\textsubscript{\textit{M}} $\uparrow$ & AP\textsubscript{\textit{L}} $\uparrow$ & AP\textsubscript{\textit{f}} $\uparrow$ & AP\textsubscript{\textit{h}} $\uparrow$ & AP\textsubscript{\textit{r}} $\uparrow$  \\
    \hline
   Point Rend \cite{pointrend_Kirillov_2020_CVPR} & 0.259 & 0.434 & 0.276 & 0.820 & 0.202 & 0.386 & 0.433 & 0.541 & 0.206 \\
   SOLOv2 \cite{solov2_Wang2020} & 0.245 & 0.409 & 0.251 & 0.560 & 0.194 & 0.376 & 0.364 & 0.483 & 0.206  \\
   QueryInst \cite{query_inst_Fang_2021_ICCV}  & 0.260 & 0.428 & 0.273 & 0.820 & 0.217 & 0.351 & 0.433 & 0.541 & 0.206 \\
   Mask Transfiner \cite{mask_transfiner_Ke_2022_CVPR}  & 0.246 & 0.421 & 0.260 & 0.720 & 0.194 & 0.361 & 0.438 & 0.263 & 0.198  \\
   Mask2Former \cite{cheng2021mask2former}  & 0.257 & 0.380 & 0.277 & 0.630 & 0.189 & 0.381 & 0.411 & 0.519 & 0.231  \\
   WaterMask R-CNN \cite{sun2023indiscernible}  & \underline{0.272} & \underline{0.437} & 0.293 & \textbf{0.900} & \underline{0.218} & \underline{0.389} & 0.463 & 0.548 & 0.209 \\
   Cascade WaterMask R-CNN \cite{sun2023indiscernible}  & 0.271 & 0.429 & \underline{0.304} & 0.830 & 0.210 & \underline{0.389} & \underline{0.470} & \underline{0.558} & \underline{0.225} \\
   MarineInst \cite{ziqiang2024marineinst} & 0.266& 0.434& 0.298 & 0.832& 0.183& 0.367& 0.450 & 0.521 & 0.198\\
   AquaticSAM &\textbf{0.293}& \textbf{0.451} & \textbf{0.330} & \underline{0.870} & \textbf{0.236} & \textbf{0.403} & \textbf{0.513} & \textbf{0.576} & \textbf{0.252} \\
       \hline
    \end{tabular}}
    \label{table9}
\end{table*}
\end{comment}

\begin{comment}
\begin{table}[t!]
\caption{Comparison of semantic segmentation results on the SUIM \cite{islam2020suim} dataset between AquaticSeg and SOTA methods. AquaticSeg achieves the highest mIoU and F-score, demonstrating its superior performance in underwater semantic segmentation tasks.}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.90}{
\begin{tabu}{|c|c|c|}
\tabucline[1.5pt]{-}
Methods&mIoU $\uparrow$&F-score $\uparrow$\\\tabucline[0.5pt]{-}
PSPNet \cite{p2pnet_Song_2021_ICCV}& 0.774 & 0.760\\
DeepLabv3 \cite{deeplab}&0.791&0.812\\
SUIM-Net \cite{islam2020suim}&0.841&0.869\\
Mask2Former \cite{cheng2021mask2former}&\underline{0.855}&\underline{0.896}\\
MarinsInst \cite{ziqiang2024marineinst}&0.851&0.882\\
AquaticSeg&\textbf{0.881}&\textbf{0.921}\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table10}
\end{table}
\end{comment}



   
\begin{table}[t!]
\caption{Object detection and classification results (mAP$_{50}$).}
%\vspace{-7mm}
\begin{center}
\makebox[\linewidth]{
\scalebox{0.78}{
\begin{tabu}{|c|c|c|c|c|}
\tabucline[1.0pt]{-}
Methods&FishNet&DeepFish&Brackish&URPC\\\tabucline[0.5pt]{-}
FasterRCNN \cite{ren2016faster}&0.284&0.814&0.788&0.475\\
YOLOF \cite{chen2021you}&0.672&0.806&0.813&0.511\\
TOOD \cite{feng2021tood}&0.811&0.766&0.805&0.507\\
MRegionCLIP&\underline{0.867}&0.855&\underline{0.842}&0.758\\
MarineInst \cite{ziqiang2024marineinst}&\underline{0.868}&0.854&0.841&\underline{0.779}\\
MarineDet \cite{haixin2023marinedet}&-&-&-&0.706\\
AquaticDet &\textbf{0.903}&\textbf{0.891}&\textbf{0.877}&\textbf{0.837}\\\tabucline[0.5pt]{-}
%DeepLabv3 \cite{deeplab}&0.791&0.812\\
%SUIM-Net \cite{islam2020suim}&0.841&0.869\\
%Mask2Former \cite{cheng2021mask2former}&\underline{0.855}&\underline{0.896}\\
%AquaticSeg&\textbf{0.881}&\textbf{0.921}\\\tabucline[0.5pt]{-}
\end{tabu}
}}
\end{center}
\label{table11}
%\vspace{-10mm}
\end{table}





\begin{comment}

\begin{table*}[t!]
    \centering
   \caption{Semantic Segmentation: Quantitative performance comparison for semantic segmentation and saliency prediction on the SUIM \cite{islam2020suim} dataset: scores are shown as $mean \pm \sqrt{variance}$.}
   \adjustbox{width=0.95\textwidth}{
    \begin{tabular}{ l c| c c| c c}
    \hline
    \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c|}{All Categories} & \multicolumn{2}{c}{Saliency Prediction} \\
     & & mIoU $\uparrow$ & F-Score $\uparrow$ & mIoU $\uparrow$ & F-Score $\uparrow$ \\
    \hline
    %FCN8 \cite{fcn8_Long_2015_CVPR} & CNN & $66.86\pm2.62$ & $64.86\pm2.52$ & $75.63\pm1.89$ & $75.62\pm1.79$ \\
    %FCN8 \cite{fcn8_Long_2015_CVPR} & VGG-16 & $79.42\pm2.02$ & $82.96\pm2.02$ & $85.22\pm1.24$ & $89.63\pm1.24$ \\
    %SegNet \cite{segnet_Badrinarayanan2015} & CNN & $58.42\pm2.71$ & $46.97\pm2.25$ & $65.90\pm2.12$ & $56.96\pm1.58$ \\
    %SegNet \cite{segnet_Badrinarayanan2015} & ResNet-50 & $77.58\pm2.39$ & $76.88\pm2.66$ & $83.09\pm1.96$ & $86.88\pm1.83$ \\
    %UNet\textsubscript{\textit{GRAY}} \cite{unet} & CNN & $75.74\pm2.38$ & $75.12\pm2.30$ & $82.77\pm1.59$ & $83.96\pm1.40$ \\
    %UNet\textsubscript{\textit{RGB}} \cite{unet} & CNN & $79.66\pm2.24$ & $83.05\pm2.14$ & $85.85\pm1.54$ & $89.99\pm1.29$ \\
    PSPNet \cite{p2pnet_Song_2021_ICCV} & MobileNet & $77.41\pm1.56$ & $76.01\pm1.67$ & $80.87\pm1.56$ & $78.42\pm1.59$ \\
    DeepLabv3 \cite{deeplab} & ResNet-101 & $79.10\pm2.34$ & $81.27\pm2.30$ & $83.55\pm1.65$ & $85.94\pm1.72$ \\
    %SUIM-Net \cite{islam2020suim} & RSB & $77.77\pm1.64$ & $78.86\pm1.79$ & $80.86\pm1.64$ & $81.36\pm1.72$ \\
    SUIM-Net \cite{islam2020suim} & VGG-16 & $84.14\pm1.43$ & $86.97\pm1.15$ & $87.67\pm1.24$ & $91.91\pm0.85$ \\
    \hline
    &  & & & & \\
    \hline
    \end{tabular}}
    \label{table10}
\end{table*}
\end{comment}




\begin{comment}
    \begin{table}[t!]
    \centering
   \caption{Object counting comparison of various SOTA methods on the IOCFish5k \cite{sun2023indiscernible} test set.
AquaticOC achieves the best results with the lowest Mean Absolute Error (MAE: 13.50) and Mean Squared Error (MSE: 36.10), outperforming other methods.}
   \adjustbox{width=0.4\textwidth}{
    \begin{tabular}{ l| c c }
    \hline
    \multicolumn{1}{c}{Method} & MAE $\downarrow$ & MSE $\downarrow$  \\
    \hline
  %  MCNN \cite{mcnn_Zhang_2016_CVPR} & 72.93 & 129.43 & 4.90 \\
    %CSRNet \cite{csrnet_Li_2018_CVPR} & 38.12 & 69.75 & 2.48 \\
    %LCFCN \cite{lcfcn_Laradji_2018_ECCV} & 28.05 & 68.24 & 1.12 \\
    %CAN \cite{can_Liu_2019_CVPR} & 42.02 & 74.46 & 2.58 \\
    %DSSI-Net \cite{dssi_Liu_2019_ICCV} & 31.04 & 69.11 & 1.68 \\
    %BL \cite{bl_Ma_2019_ICCV} & 20.03 & 46.08 & 0.55 \\
    %NoisyCC \cite{noisycc} & 19.73 & 46.85 & 0.46 \\
    %DM-Count \cite{dmcount_Wang2020a} & 19.52 & 45.52 & 0.55 \\
    %GL \cite{gl_Wan_2021_CVPR} & 18.80 & 46.19 & 0.47 \\
    %P2PNet \cite{p2pnet_Song_2021_ICCV} & 20.74 & 47.90 & 0.48 \\
    KDMG \cite{kdmg} & 0.227 & 0.499  \\
    MPS \cite{mps} & 0.335 & 0.550  \\
    %MAN \cite{man_Liu2020} & 25.82 & 45.82 & 3.16 \\
    CLTR \cite{cltr_liang2022end} & 0.180 & 0.419  \\
    IOCFormer \cite{sun2023indiscernible} &\underline{ 0.171} & \underline{0.412}  \\
    AquaticOC&\textbf{0.135}&\textbf{0.361} \\
    \hline
        \end{tabular}}
    \label{table12}
\end{table}
\end{comment}










