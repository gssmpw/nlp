\section{Conclusion}
\label{sec:conclusion}
\vspace{-1mm}
Current aquatic and underwater VLMs rely on paired image-text data for pre-training.
In this work, we introduced AquaticCLIP, pre-trained using real aquatic image-text pairs and additional generated textual descriptions at both image and instance levels. 
To achieve this, we built a 2M image-text paired dataset sourced from various online repositories. 
We proposed a novel vision-language alignment model where the vision encoder is guided by learned prompts, and the text encoder benefits from visual prompts. 
Both lightweight encoders are pre-trained using cross-modal contrastive supervision for enhanced vision-language alignment.
AquaticCLIP was evaluated across a diverse set of marine vision tasks, including zero-shot fine-grained object classification, fine-tuned instance and semantic segmentation, and object detection, and counting.
Our model consistently delivered superior results compared to existing SOTA methods designed specifically for marine environments, demonstrating its robustness and effectiveness across multiple aquatic vision tasks.




















