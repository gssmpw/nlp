\section{Related Work}
\label{sec:relatedwork}
Numerous deep learning-based approaches have been proposed for various aquatic scene understanding tasks \cite{saleh2022computer, wang2023deep, zhang2024webuot, li2023underwater, gonzalez2023survey} including underwater image enhancement  \cite{desai2022aquagan, wang2022agcyclegan, islam2020fast}, species classification \cite{li2023deep, xu2023systematic}, underwater object recognition \cite{khan2023fishnet, jalal2020fish}, coral segmentation \cite{zheng2024coralscop}, Object counting \cite{sun2023indiscernible}, and underwater tracking \cite{zhang2024webuot}.
%While CNNs have traditionally been used, more recent approaches have incorporated vision transformers.
%Below, we discuss key categories of methods used in specific aquatic computer vision applications.

\noindent \textbf{1. CNN-based Methods:} %CNNs have gained popularity due to their ability to automatically learn and extract features from aquatic images, improving accuracy in various tasks.
For underwater image enhancement and restoration, many CNN-based approaches have been proposed including WaterGAN \cite{li2017watergan}, Image-2-Image Translation \cite{cho2020underwater}, AquaGAN \cite{desai2022aquagan}, CycleGAN \cite{lu2019multi}, AGCycleGAN \cite{wang2022agcyclegan} and others \cite{hu2018underwater}.
%Hu \textit{et al.} proposed a cross-layer multi-scale CNN using separate networks for ambient light and blue channel transmission estimation \cite{hu2018underwater}.
%Li \textit{et al.} introduced WaterGAN, a model that performs color correction by estimating attenuation, backscattering, and camera characteristics \cite{li2017watergan}.
%Cho \textit{et al.} utilized image-to-image translation via GANs for image correction and enhancement \cite{cho2020underwater}, while Desai \textit{et al.} presented AquaGAN, which estimates attenuation coefficients using a UNet CNN based on the image formation model \cite{desai2022aquagan}.
%Other notable approaches include, Lu \textit{et al.} CycleGAN model combined with the dark channel prior for underwater image restoration \cite{lu2019multi} and 
%Wang \textit{et al.} AGCycleGAN for restoration \cite{wang2022agcyclegan}.
For underwater object detection and classification, several methods and datasets have led to significant progress \cite{khan2023fishnet, yeh2021lightweight, jalal2020fish}. 
Khan \textit{et al.} introduced FishNet, a benchmark dataset for fish recognition and functional trait prediction \cite{khan2023fishnet}.
%Yeh \textit{et al.} proposed a lightweight deep network for simultaneous object detection and color conversion \cite{yeh2021lightweight}, while Jalal \textit{et al.} applied deep networks with %temporal information for fish detection and species classification \cite{jalal2020fish}.

Species classification is crucial for underwater monitoring and conservation and a number of methods have been proposed in this directions \cite{10612606, Pedersen_2019_CVPR_Workshops, siddiqui2018automatic}.  
%Prathima \textit{et al.} employed models like Yolo, FRCNN, and RetinaNet for marine animal detection \cite{10612606}. 
%Pedersen \textit{et al.} released a dataset for species classification including fish, crabs, and starfish in varying visibility conditions \cite{Pedersen_2019_CVPR_Workshops}. 
%Siddiqui \textit{et al.} used a cross-layer pooling method with a pre-trained CNN for generalized species classification \cite{siddiqui2018automatic}.
Coral reefs, known as the ``rainforests of the sea'', play a critical role in biodiversity. 
However, coral segmentation is a challenging task due to complex underwater visual conditions.
Many methods are recently proposed for this task \cite{zhong2023combining, ziqiang2023coralvos}.
%Zhong \textit{et al.} developed 3D models and performed millimeter-accurate rugosity evaluation of coral habitats \cite{zhong2023combining}, while Ziqiang \textit{et al.} introduced a new dataset %and benchmark for coral video object segmentation \cite{ziqiang2023coralvos}..
Object counting is essential for marine biology, fisheries management, and environmental monitoring. 
Most approaches focus on fish and coral counting such as \cite{burguera2024deep, babu2023computer, modasshir2018coral}. 
%Burguera \textit{et al.} proposed a CNN-based method for lobster counting in deep sea environments \cite{burguera2024deep}, and Babu \textit{et al.} introduced a juvenile fish counting model using object detection \cite{babu2023computer}.
%Other approaches include simultaneous fish detection and counting \cite{babu2023computer} and coral identification and counting \cite{modasshir2018coral}.
Underwater visual tracking has also seen significant advances using CNN-based trackers and new datasets \cite{9499961, cai2023semi, hao2022umotma, lee2024detection}.
%Panetta \textit{et al.} proposed the UOT100 underwater tracking dataset and a GAN-based tracker \cite{9499961, cai2023semi, hao2022umotma, lee2024detection}.
%Cai \textit{et al.} developed a semi-supervised visual tracker for marine animals using autonomous underwater vehicles \cite{cai2023semi}, and Hao \textit{et al.} introduced a memory aggregation network for multi-object tracking \cite{hao2022umotma}.
%Further developments in underwater tracking can be found in \cite{lee2024detection}.

\noindent \textbf{2. Transformer-based Methods:} Many ViT-based techniques for detection, segmentation, counting, tracking, and classification have been proposed for underwater scenes \cite{peng2023u, zhang2024webuot, sun2023indiscernible, yang2024density, ai2024novel}.
For instance, UShape transformer \cite{peng2023u} and transformer-driven GAN \cite{ummar2023window} have been proposed for underwater image restoration tasks.
%Peng \textit{et al.} introduced the UShape transformer architecture for underwater image enhancement \cite{peng2023u}, while Mehnaz \textit{et al.} proposed a transformer-driven GAN for underwater image %restoration \cite{ummar2023window}.
Alawode \textit{et al.} proposed a combined approach for underwater image enhancement and visual tracking \cite{alawode2022utb180}, and Zhang \textit{et al.}  developed a large-scale benchmark for advancing underwater object tracking \cite{zhang2024webuot}.
A dense object counter \cite{sun2023indiscernible} and density-guided attention \cite{yang2024density} methods are proposed for underwater object counting task.
%Sun \textit{et al.} introduced a dense object counter designed for underwater scenes , and Yang \textit{et al.} developed a density-guided temporal attention transformer for underwater object counting .
For fish detection and classification \cite{liu2024dp, liu2024cffi}, and coral reef classification \cite{ai2024novel, shao2024deep} tasks, ViT-based models have also recently been proposed. 
%Liu \textit{et al.} presented a dual-path pyramid vision transformer network for fish detection \cite{liu2024dp}, as well as an enhanced vision transformer architecture for fish classification in aquaculture %\cite{liu2024cffi}.
%Bo \textit{et al.} proposed a vision transformer-based method for coral reef classification \cite{ai2024novel}, while Shao \textit{et al.} applied transformers for multi-label classification of coral %conditions \cite{shao2024deep}. 
A token-based selective ViT \cite{si2023token} and cascaded attention \cite{zhang2024catnet} models are proposed for fine-grained marine species classification.
%Si \textit{et al.} introduced a token-based selective transformer for fine-grained marine organism classification \cite{si2023token}, and Zhang \textit{et al.} proposed a cascaded attention mechanism for %marine species classification \cite{zhang2024catnet}.
Additionally, some self-supervised learning methods have also been utilized for underwater image analysis \cite{huang2023contrastive, saleh2022transformer}.\\
\noindent \textbf{3. Vision-Language Models (VLMs):} In the context of safeguarding aquatic biodiversity, there is an increasing need for VLMs to facilitate AI-based aquatic scene understanding systems. 
However, VLMs have only been sparsely applied to aquatic scene analysis \cite{zheng2024coralscop, zheng2023marinegpt,ziqiang2024marineinst}.
For instance, Zheng \textit{et al.} introduced CoralSCOP, a foundational model for the automatic segmentation of coral reefs \cite{zheng2024coralscop}, and MarineGPT, a multimodal large language model for marine object classification and question-answering \cite{zheng2023marinegpt}.
Zheng \textit{et al.} further evaluated GPT-4V for marine images, but found its performance unsatisfactory for domain-specific needs of marine biologists \cite{zheng2024exploring}. 
Recently, the MarineInst foundational model was proposed, pre-trained on 2M images for segmenting and captioning marine imagery \cite{ziqiang2024marineinst}.
To the best of our knowledge, VLMs have not been thoroughly explored for aquatic scene understanding, except for MarineGPT and MarineInst. 
Our work is the first to introduce AquaticCLIP, with comprehensive analysis and comparisons to existing SOTA methods.
%\vspace{-13mm}

\begin{figure*}[t!]
    \centering    
    \includegraphics[width=0.96\linewidth]{CVPR_QauaticClip_methodology.png}
    \vspace{-2mm}
    \caption{\textbf{Overview of AquaticCLIP architecture and training process}. 
\textbf{(a)} Shows a set of input image-text pairs. 
\textbf{(b)} A caption model (MarineGPT) generates textual descriptions for the images. 
\textbf{(c)} Input images are divided into patches and processed by the image encoder $\Phi_v$ to produce patch embeddings $\textbf{P}_{i}$.
\textbf{(d)} The generated textual descriptions $\textbf{S}_{i}$ are processed by the text encoder $\Phi_{t}$ to produce text embedings. 
\textbf{(e)-(f)} The textual description $\textbf{S}_{i}$ is then cleaned by an image-text caption cleaning module to produce refined descriptions $\hat{\textbf{S}}_{i}$ which are then combined with groundtruth descriptions $\textbf{G}_{i}$ to produce enriched textual description data $\textbf{C}_{i}$ .  
Both image and text embeddings are refined using \textbf{(h)} vision-guided text encoding and \textbf{(g)} prompt-guided vision encoding. 
The learned prompts $\textbf{E}_i$ guide the fusion of patch embeddings, while initialized prompts $\textbf{Q}_{i}$ are used to enhance the visual representation. 
\textbf{(i)} The final image and text features are aligned using a cross-modal contrastive pre-training loss $\mathcal{L}_{cont}$, ensuring a stronger association between text and image representations. }\label{introFigure}
%\vspace{-3mm}
\end{figure*}






 

























