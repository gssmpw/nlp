\section{Introduction}
\label{sec:intro}
Global aquatic\footnote{Aquatic is a broad term encompassing all water-based environments, including both freshwater and saltwater habitats. It encompasses a vast range of ecosystems, from rivers and lakes to oceans and coral reefs.}  ecosystems are under severe threats from human activities such as overfishing and coastal development, along with climate change impacts \cite{doney2012climate, halpern2008global, jennings1998effects, zhou2022geological}. Effective conservation efforts depend on precise monitoring, which requires an accurate and automatic aquatic scene understanding system \cite{duarte2020rebuilding,grorud2021mpa}. However, the complexity of understanding aquatic environments demands significant expertise from ocean scientists and marine biologists, creating challenges for efficient monitoring \cite{zheng2023marinegpt,saleh2022computer}.


\captionsetup{type=figure}
\begin{figure*}
\begin{center}
    \centering
    \begin{subfigure} %[b]{0.76\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=0.75\textwidth, height=7.5cm]{introDiagram_4.png} 
    \end{subfigure}      
    \begin{subfigure} %[b]{0.23\textwidth}
        \centering
        \includegraphics[width=0.23\textwidth, height=7.5cm]{marineClip-2_1.pdf}
    \end{subfigure}
     \vspace{-4mm}
     \caption{\textbf{(a) Step 1:} \textit{Two Million Aquatic Image-Text Pairs.} 
The dataset consists of paired aquatic images and enriched textual descriptions, which serve as input to the model. 
\textbf{(b) Step 2:} \textit{Contrastive Loss Pretraining}. Text and image pairs are processed by a text encoder and an image encoder.
The embeddings are aligned through contrastive loss, reducing the distance between matching pairs and improving the model's ability to associate images with their corresponding textual descriptions. \textbf{(c) Step 3:} \textit{Downstream Analysis.} AquaticCLIP performance is evaluated across various tasks such as zero-shot marine species classification, fine-tuned instance and semantic segmentation, object detection, and biodiversity counting in underwater imagery.}
\vspace{-1mm}
    \label{fig1}    
\end{center}
\end{figure*}


Recently, Vision-Language Models (VLMs) have gained increasing attention in the computer vision field \cite{radford2021learning, sammani2022nlx, mu2024embodiedgpt, maniparambil2023enhancing, yan2021videogpt}.
These models are typically pre-trained using large-scale image-text paired data, readily available online \cite{radford2021learning, singh2022flava, zhang2024vision}.
Pre-trained VLMs have been successfully applied to downstream tasks such as image classification \cite{naeem2023i2mvformer}, detection \cite{zhu2020don}, tracking \cite{li2023citetracker}, and human action recognition \cite{sun2022human}.
Their success is primarily attributed to contrastive pre-training loss, which pulls paired images and texts closer while pushing unrelated ones apart in the embedding space \cite{li2021supervision, radford2021learning}.
A notable example is Contrastive Language-Image Pre-training (CLIP), which captures rich vision-language correspondence and enables zero-shot predictions by matching the embeddings of images and texts \cite{radford2021learning,zhang2024vision}.

While significant progress has been made in extending CLIP to various computer vision tasks, few works have applied VLMs to aquatic environments \cite{zheng2024coralscop, zheng2023marinegpt,ziqiang2024marineinst}.
For example, MarineGPT, pre-trained on 5M images, was introduced for marine image-question answering tasks \cite{zheng2023marinegpt}.
More recently, MarineInst, a marine foundational model, has been proposed for segmentation and caption generation tasks \cite{ziqiang2024marineinst}.
However, the development of marine VLMs has lagged behind terrestrial VLMs due to the unique challenges of aquatic environments. 
Existing open-air datasets like COCO \cite{lin2014microsoft} cannot be used to train aquatic VLMs, and large-scale, domain-specific aquatic image-text pairs are not readily available. 
This scarcity of data makes pre-training aquatic VLMs particularly challenging.

To bridge this gap, we propose a large-scale aquatic image-text paired dataset comprising 2 million image-text pairs. 
We then introduce AquaticCLIP, a model that efficiently aligns aquatic images and texts for several downstream tasks, as shown in Fig. \ref{fig1}.
Our dataset is collected from publicly available resources such as National Geographic (NatGeo) \cite{doe2023coral, national_geographic_coral, ng_coral_bleaching, ng_ocean_acidification,ng_great_barrier_reef,ng_coral_conservation,ng_ocean_biodiversity}, aquatic biology textbooks and journals, YouTube aquatic documentary videos, Fishes of Australia \cite{van2014family, schodde1997zoological, merrick2006australasian,shelley2017revision}, Marine Twitter, Netflix, and the Corals of the World \cite{veron2016corals}.
To our knowledge, no such paired dataset exists for aquatic scenes, except MarineInst \cite{ziqiang2024marineinst}, which contains only images. 
To further enrich the textual descriptions, we  generate additional descriptions for aquatic images using MarineGPT \cite{zheng2023marinegpt} both at the image level and the instance level. 
For instance-level descriptions, we detect objects within an aquatic image using a pre-trained object detector and then MarineGPT is employed for each instance.
These additional descriptions are then refined using a custom cleaning module to remove irrelevant keywords while retaining those relevant to the aquatic imagery.

For our AquaticCLIP model, we introduced two lightweight learnable encoders for the image and text branches, respectively. 
The key idea behind these encoders is to effectively transfer the VLM into the aquatic domain by leveraging the prior knowledge of the existing MarineGPT. 
To enable the VLM to process aquatic images more efficiently, we designed a prompt-guided vision encoder based on prompt-based learning. 
Specifically, for the image branch, we aggregate all patch features using learnable weights. 
A set of learnable prompt vectors is introduced to progressively guide the fusion of patch features, grouping similar ones together. 
This method allows each prompt to capture more global contextual information for final similarity computation.

For the text branch, we propose a vision-guided text encoder that integrates corresponding image information into the text encoder. 
By employing a multi-modal text encoder for guidance, knowledge is more effectively transferred to the VLM's text encoder. 
The visual representations and textual descriptions learned by these two encoders are then aligned using a contrastive pre-training loss, similar to CLIP \cite{radford2021learning}.
The main goal is to bring similar visual and textual concepts of aquatic images closer and push dissimilar ones apart.


We conducted extensive experimental evaluations of the AquaticCLIP model on various downstream tasks, including zero-shot and fine-tuning scenarios. 
Zero-shot tasks included aquatic species recognition, fine-grained fish classification, coral species classification, and cross-modal retrieval.
Fine-tuning tasks involved coral segmentation, instance segmentation of aquatic imagery, semantic segmentation, aquatic object detection and classification, and aquatic object counting. 
Our results demonstrate significant performance improvements compared to existing State-Of-The-Art (SOTA) methods in aquatic settings.

The key contributions of this work include: 
\begin{comment}
 \begin{enumerate}
\item We propose a large-scale 2M image-text paired dataset specifically designed for pre-training aquatic VLMs (Sec. \ref{sec:dataset}).
\item \textcolor{blue}{To further enrich the textual descriptions, for the aquatic images in our dataset, we also generate pseudo-descriptions using MarineGPT in zero-shot settings at the image and instance levels (Sec. \ref{sec:generation}).
These descriptions are then refined to remove undesired keywords, improving overall data quality (Sec. \ref{sec:cleaning}).}
\item We introduce a learnable prompts-guided image encoder that enhances the encoding of aquatic images. 
By utilizing learnable prompts, irrelevant image regions are suppressed, resulting in more effective visual representations (Sec. \ref{sec:pgve}).
\item We propose a vision-guided text encoder that aligns textual descriptions with corresponding visual context more accurately (Sec. \ref{sec:vgte}). 
This refined textual representation is used for efficient pre-training of the VLM (Sec. \ref{loss}).
\item  Extensive experimental evaluations are conducted on various downstream aquatic computer vision tasks, including instance segmentation, semantic segmentation, zero-shot aquatic species recognition, aquatic object counting, and challenging aquatic object detection. 
Our results show significant performance improvements compared to existing SOTA methods in aquatic environments (Sec. \ref{sec:results}).
\end{enumerate}
\end{comment}

\begin{enumerate}
\item We propose a large-scale dataset of 2 million image-text pairs for aquatic VLM pre-training, with enriched textual descriptions generated by MarineGPT in zero-shot settings at both the image and instance levels and refined to exclude undesired keywords, enhancing data quality (Sec. \ref{sec:dataset}-\ref{sec:cleaning}).
\item We introduce a dual-encoder approach with a prompts-guided image encoder to suppress irrelevant image regions and a vision-guided text encoder for improved alignment of visual and textual representations (Sec. \ref{sec:pgve}-\ref{loss}).
\item  Extensive evaluations on diverse aquatic vision tasks show significant gains over existing SOTA methods (Sec. \ref{sec:results}).
\end{enumerate}

\noindent The remainder of this paper is organized as follows: Sec. \ref{sec:relatedwork} reviews related work. 
Sec. \ref{sec:method} presents our proposed dataset and the AquaticCLIP. 
Sec. \ref{sec:results} presents the experiments and results, and Sec. \ref{sec:conclusion} concludes our work.

