\section{Related Work}
Transferring robot manipulation knowledge between different embodiments can improve the policy's flexibility and generalizability for adapting to new tasks or hardware settings____, including the knowledge transitions between different grippers____ or robot manipulators____. For example, Zakka et al.____ combine the temporal cycle-consistency method with imitation learning to learn an invariant feature space for different embodiments, empowering robot manipulators (target domain) to perform the tasks by imitating video demonstrations of human experts (source domain). While the feature correspondence between the source and target domains has to be re-trained if a new source or target domain is introduced. Wang et al.____ mitigate this limitation by learning a feature latent space for the source (a 7DoF manipulator) and target (a 6DoF manipulator) domains alongside the encoder and decoder for each domain. Then, they employ adversarial training and cycle consistency for latent alignment, which aligns the target encoder-decoder to that of the source domain, respectively. Similarly, in____, the authors employ latent alignment for source and target domains, achieving knowledge transferring between heterogeneous manipulators. By leveraging large-scale heterogeneous robot data, Wang et al.____ propose a heterogeneous pre-trained transformer (HPT) that can perform different tasks across heterogeneous robot manipulators. 

However, the manipulation knowledge transition of these approaches relies on transfer learning or domain adaption/alignment____, where introducing new hardware configuration (e.g., grippers) to the policy require fine-tuning policy____ or retraining auxiliary networks (e.g., encoder and decoder)____, limiting the flexibility of the policy. Our method circumvents these fine-tuning and retraining operations by integrating a learning-optimization hybrid strategy into diffusion policy____, necessitating the introduction of task-oriented constraints for new hardware configurations and employing optimization during the policy online inference phase to ensure completion of the task.

Diffusion models have shown promise for solving decision-making tasks, including motion planning____, imitation learning____, and reinforcement learning____. When solving decision-making tasks through generative models, the policy needs not only to accomplish the task goal but also to satisfy certain task constraints, such as collision avoidance. Some works add a residual loss to the training objectives if the task constraints are consistent during the policy training and inference____. A more flexible method is training diffusion models via classifier-free guidance____, introducing conditioning variables that represent constraints into the policy training, such as physical constraints for guiding human motion generation____. However, the model conditioning encourages the generated samples to adhere to task constraints rather than strict guarantee constraints____. Alternative post-processing methods draw constraints into the last denoising stage of the sample-generating process and obtain samples that satisfy the constraints by solving an optimization problem____. As the optimization problem does not consider the unknown data likelihood, post-processing may result in samples significantly deviating from the data distribution____. To mitigate this issue, an iterative projection is integrated into the denoising process, confining each generated sample to the constraints. Some works employ it for sequential decision-making____, which is time-consuming. Or model-based trajectory control____, falling short of seamlessly adapting to a new model since introducing a new gripper changes the robot dynamics. Our approach utilizes projection alongside optimization techniques in the denoising process but mitigates the time consumption and makes the policy seamlessly adaptable to new grippers.