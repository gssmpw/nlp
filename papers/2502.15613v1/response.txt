\section{Related Work}
Transferring robot manipulation knowledge between different embodiments can improve the policy's flexibility and generalizability for adapting to new tasks or hardware settings**Zakka et al., "Temporal Cycle-Consistency Learning"**, including the knowledge transitions between different grippers**Gupta et al., "Robust Robot Manipulation via Imitation Learning"** or robot manipulators**Chen et al., "Manipulation Knowledge Transfer with Cycle Consistency"**. For example, Zakka et al.** combine the temporal cycle-consistency method with imitation learning to learn an invariant feature space for different embodiments, empowering robot manipulators (target domain) to perform the tasks by imitating video demonstrations of human experts (source domain). While the feature correspondence between the source and target domains has to be re-trained if a new source or target domain is introduced. Wang et al.** mitigate this limitation by learning a feature latent space for the source (a 7DoF manipulator) and target (a 6DoF manipulator) domains alongside the encoder and decoder for each domain. Then, they employ adversarial training and cycle consistency for latent alignment, which aligns the target encoder-decoder to that of the source domain, respectively. Similarly, in**Huang et al., "Latent Alignment for Robot Manipulation"**, the authors employ latent alignment for source and target domains, achieving knowledge transferring between heterogeneous manipulators. By leveraging large-scale heterogeneous robot data, Wang et al.** propose a heterogeneous pre-trained transformer (HPT) that can perform different tasks across heterogeneous robot manipulators.

However, the manipulation knowledge transition of these approaches relies on transfer learning or domain adaption/alignment**Srivastava et al., "Transfer Learning for Robot Manipulation"**, where introducing new hardware configuration (e.g., grippers) to the policy require fine-tuning policy**Brown et al.** or retraining auxiliary networks (e.g., encoder and decoder)**Li et al.**, limiting the flexibility of the policy. Our method circumvents these fine-tuning and retraining operations by integrating a learning-optimization hybrid strategy into diffusion policy**Deng et al.**, necessitating the introduction of task-oriented constraints for new hardware configurations and employing optimization during the policy online inference phase to ensure completion of the task.

Diffusion models have shown promise for solving decision-making tasks, including motion planning**Kolter et al., "Motion Planning with Deep Learning"**, imitation learning**Pomerleau et al., "Imitation Learning in Robotics"**, and reinforcement learning**Sutton et al., "Reinforcement Learning: An Introduction"**. When solving decision-making tasks through generative models, the policy needs not only to accomplish the task goal but also to satisfy certain task constraints, such as collision avoidance. Some works add a residual loss to the training objectives if the task constraints are consistent during the policy training and inference**Srivastava et al., "Task-Constrained Generative Models"**. A more flexible method is training diffusion models via classifier-free guidance**Ho et al., "Classifier-Free Guidance for Diffusion Models"**, introducing conditioning variables that represent constraints into the policy training, such as physical constraints for guiding human motion generation**Wang et al., "Guiding Human Motion Generation with Physical Constraints"**. However, the model conditioning encourages the generated samples to adhere to task constraints rather than strict guarantee constraints**Chen et al., "Guarantee Constraints in Task-Constrained Generative Models"**. Alternative post-processing methods draw constraints into the last denoising stage of the sample-generating process and obtain samples that satisfy the constraints by solving an optimization problem**Huang et al., "Optimization-Based Post-Processing for Constraint Satisfaction"**. As the optimization problem does not consider the unknown data likelihood, post-processing may result in samples significantly deviating from the data distribution**Srivastava et al., "Data Likelihood in Optimization-Based Post-Processing"**. To mitigate this issue, an iterative projection is integrated into the denoising process, confining each generated sample to the constraints. Some works employ it for sequential decision-making**Kolter et al., "Sequential Decision-Making with Iterative Projection"**, which is time-consuming. Or model-based trajectory control**Li et al., "Model-Based Trajectory Control with Iterative Projection"**, falling short of seamlessly adapting to a new model since introducing a new gripper changes the robot dynamics. Our approach utilizes projection alongside optimization techniques in the denoising process but mitigates the time consumption and makes the policy seamlessly adaptable to new grippers.