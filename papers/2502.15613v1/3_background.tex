\section{Background on diffusion policy} 
%Denoising diffusion probabilistic model (DDPM)~\cite{ho2020denoising} is a generative model that iteratively generates samples matching data distribution, including forward and reverse processes. During the forward process $q(x^k|x^{k-1})$, the original data is iteratively destroyed by adding Gaussian noise to the previous data $x^{k-1}$. While in the reverse process $p_{\theta}(x^{k-1}|x^k)$, the model iteratively denoises the destroyed data to recover the original data $x^0$. This process is formulated by $p_{\theta}(x^{k-1}|x^k)=\mathcal{N}(x^{k-1};\mu_{k}(x^k,\epsilon_\theta(x^k, k)),\Sigma^k)$, where $\epsilon_\theta$ is noise prediction network with parameters $\theta$, predicting the added noise $\epsilon$ that convert $x^0$ to $x^k$, and $\mu_{k}(\cdot)$ maps the $k$-th step data $x^k$ and predicted $\epsilon_\theta$ to the next mean value. Diffusion policy~\cite{chi2023diffusion} extends DDPM into robotic manipulators, training a policy $p_{\theta}(A_t|S_t)$ that generates action sequences conditioned on visual and robot states. The policy is trained via behavior cloning by fitting the noise prediction $\epsilon_\theta(a^k_t,s_t,k)$ to predict the added noise. During inference, the denoising process is iteratively repeated for $K$ times, generating a noiseless action sequence $A_t^0$ in the end. Specifically, given a state sequence $S_t = \{s_t-T_a+1,s_t-T_a+2,...,s_t\}$ within a duration of $T_a$, the policy generates a action sequence $A_t = \{a_t,a_t+1,...,a_t+T_a-1\}$. After the agent executes $A_t$, the policy predicts the sequential action sequence $A_{t+T_a}$ based on a new state sequence $S_{t+T_a}$. 



% by a neural network that parameterized by $\theta$. 
%Denoising diffusion probabilistic model (DDPM)~\cite{ho2020denoising} is a generative model that iteratively generates samples matching data distribution, including forward and reverse processes. During the forward process $q(x^k|x^{k-1})$, the original data is iteratively destroyed by adding Gaussian noise to the previous data $x^{k-1}$. While in the reverse process $p_{\theta}(x^{k-1}|x^k)$, the model iteratively denoises the destroyed data to recover the original data $x^0$. This process is formulated by $p_{\theta}(x^{k-1}|x^k)=\mathcal{N}(x^{k-1};\mu_{\theta}(x^k,k),\Sigma^k)$, where $\mu_{\theta}(x^k,k)$ maps the $k$-th step data $x^k$ to the next mean by a neural network that parameterized by $\theta$. 
%This process is parameterized by a neural network $\epsilon_{\theta}(x^k,k)$ to predict each step noise $\epsilon$ in the forward process, formulating by $p_{\theta}(x^{k-1}|x^k)=\mathcal{N}(x^{k-1};\mu_{\theta}(x^k,k),\Sigma^k)$. %Here, we provide a brief overview of DDPM and diffusion policy. 


%% extened version

% Denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising} are a class of generative models that produce samples closely matching a target data distribution through an iterative refinement process. Such models rely on two complementary Markovian processes: a forward diffusion process and a corresponding reverse denoising process. In the forward process, denoted by $q(x^k \mid x^{k-1})$, gradually increasing levels of Gaussian noise are added to the original data sample $x^0$ over $k$ steps, yielding increasingly perturbed data $x^k$. Conversely, in the reverse process $p_{\theta}(x^{k-1} \mid x^k)$, a learnable denoising network iteratively removes noise to recover the underlying data distribution. Formally, the reverse step is: 
% \begin{equation} 
%     p_{\theta}(x^{k-1}\mid x^k) = \mathcal{N}\big(x^{k-1}; \mu_{k}(x^k, \epsilon_{\theta}(x^k, k)), \Sigma_k\big), 
% \end{equation}
% where $\epsilon_{\theta}$, parameterized by $\theta$, is a noise-prediction neural network that estimates the noise $\epsilon$ added in the forward process. The function $\mu_{k}(\cdot)$ maps the current noisy sample $x^k$ and the predicted noise $\epsilon_{\theta}(x^k, k)$ to a mean value from which a cleaner sample $x^{k-1}$ can be drawn, and $\Sigma_k$ is the covariance of the conditional Gaussian distribution.

%Building upon DDPMs, Diffusion Policy~\cite{chi2023diffusion} extends these principles to the domain of robotic manipulation. In this framework, the policy $\pi_{\theta}(A_t \mid S_t)$ is trained to generate action sequences conditioned on visual and robot state observations using a behavior cloning objective. Specifically, the noise prediction network $\epsilon_{\theta}(a_t^k, s_t, k)$ is trained to predict the artificially injected noise across multiple denoising steps. At inference time, the learned reverse denoising process is iterated $K$ times to produce a sequence of actions $A_t^0$ that is free of noise and suitable for execution. Concretely, given a state sequence $S_t = \{ s_{t - T_a + 1}, s_{t - T_a + 2}, \ldots, s_t \}$ over a horizon $T_a$, the policy generates a corresponding action sequence $A_t = \{ a_t, a_{t+1}, \ldots, a_{t + T_a - 1} \}$. After executing the actions $A_t$ on the robot, new sensor readings produce a subsequent state sequence $S_{t + T_a}$, from which the policy then infers the next action sequence $A_{t+T_a}$. By iteratively applying this diffusion-based inference step, the policy can continuously produce coherent action trajectories that respond adaptively to changing environment states.

%% simplified version for diffusion policy
Denoising diffusion probabilistic models (DDPMs) \cite{ho2020denoising} are generative models that iteratively refine noisy data samples to match a target distribution.  They employ two complementary Markovian processes: a forward diffusion process that incrementally adds Gaussian noise to an original data sample $x^0$ over $k$ steps, and a reverse denoising process that removes the accumulated noise step by step. Building on this foundation, Diffusion Policy \cite{chi2023diffusion} adapts DDPMs for robotic manipulation. In this setting, a policy $\pi_{\theta}(A_t \mid S_t)$ is trained via behavior cloning to generate action sequences conditioned on visual and robot state observations. The model injects noise into the action samples during training and learns to predict this noise over multiple denoising steps, thereby guiding the policy to produce coherent actions at inference. Specifically, at test time, the reverse denoising process is iterated $K$ times to times to obtain a ``clean'' action sequence $A^0_t$. Concretely, assume a horizon $T_a$ and a state sequence $S_t = \{ s_{t - T_a + 1}, s_{t - T_a + 2}, \ldots, s_t \}$. The policy generates an action sequence $A_t = \{ a_t, a_{t+1}, \ldots, a_{t + T_a - 1} \}$ based on the learned diffusion mechanism. After executing $A_t$ on the robot, new sensor readings yield a subsequent state sequence $S_{t + T_a}$, from which the policy infers the next action sequence. By iterating this diffusion-based inference, the robot continuously adapts its actions to changing environment states, enabling robust long-horizon control.