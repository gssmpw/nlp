\section{Introduction}

Robotic pick-and-place operations form the cornerstone of automated manipulation, which is essential for activities from household to industrial scenarios. A variety of methods have been proposed to endow robots with pick-and-place capabilities, including reinforcement learning~\cite{10341769,10160626,meng2025preserving}, imitation learning~\cite{10685120}, and motion planning~\cite{migimatsu2020object}. While these methods have demonstrated the ability to manipulate multiple objects (known or unknown)~\cite{9131812,zeng2021transporter}, operate in dynamic environments~\cite{migimatsu2020object}, or perform one-shot or few-shot tasks~\cite{di2023one,10202179}, they typically assume fixed end-effector morphology throughout training and deployment. This assumption restricts the transfer of pick-and-place skills across grippers and compromises overall generalizability.


Recent research mitigates this limitation through multi-embodiment learning strategies. For example, HPT trains a transformer-based policy with large-scale heterogeneous robot datasets covering diverse embodiments and tasks~\cite{wang2024scaling}. Wang et al.~\cite{wang2024cross} employ latent space alignment to transfer pick-and-place skills between different robot manipulators. Despite promising results, these methods often require substantial embodiment-specific data \cite{wang2024scaling} or policy fine-tuning to accommodate new gripper configurations \cite{wang2024scaling,wang2024cross}, leading to additional adaptation costs and limiting deployment flexibility. This challenge is particularly pronounced for behavior cloning algorithms, such as diffusion policies~\cite{chi2023diffusion,Ze2024DP3}. In online developments, switching from a base gripper to one with a different morphology can shift the end-effector’s tool-center-point (TCP), causing collisions or missed grasps if a pretrained diffusion policy is naively applied to generate trajectories, as shown in Fig. \ref{fig:first-figure}\textcolor{blue}{a}.



We bridge this gap through an integration of diffusion policy~\cite{chi2023diffusion,Ze2024DP3} with a learning-optimization hybrid strategy. This strategy allows the trained policy to fit new grippers without policy retraining or fine-tuning, as shown in Fig.\ref{fig:first-figure}\textcolor{blue}{b}. The policy first learns pick-and-place manipulation primitives following diffusion policy, where the training demonstration data is collected via a base gripper. During the policy's online inference, the base gripper is replaced with a new one. To ensure the generative trajectories are valid for new gripper configurations, we first incorporate the gripper-specific dimensional offsets into the policy inputs, and recast the traditional diffusion sampling process as a constrained-optimization process. The optimization introduces mathematical encoding of task and safety constraints (e.g.,  object grasping, avoiding collisions) to the denoising process, utilizing a quadratic programming optimization method to incrementally refine an initially noisy trajectory to conform to unseen gripper geometries. Crucially, this approach does not require policy retraining or fine-turning, thereby preserving flexibility for real-world deployment with minimal overhead.

We validate our approach with a Franka Panda manipulator equipped with various grippers for pick-and-place tasks—including 3D-printed fingertips of different heights, flexible fingertips, and the Robotiq 2F gripper—achieving robust performance without policy retraining. Key contributions are summarized as follows.
\begin{itemize}
    \item A learning-optimization strategy is introduced into the diffusion policy, dynamically optimizing the generation of trajectories to counteract the TCP offset problem caused by switching grippers, thus ensuring task completion. This ensures the trained policy can solve pick-and-place tasks with new grippers without retaining and fine-tuning operations.
    \item A general formulation of the task and safety constraints is designed for the optimization process. Moreover, a cumulative optimization is utilized to ensure the temporal consistency of the generated trajectory.
    \item Real-world experiments across different grippers show our method achieves 93.3\% and 70.0\% success rates for seen and unseen objects, respectively--outperforming baseline diffusion policies (23.3\% and 20.0\%).
\end{itemize} 

