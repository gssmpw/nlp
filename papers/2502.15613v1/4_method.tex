

\section{Methodology}
\subsection{Problem Statement}

We formalize the challenge of gripper-agnostic manipulation via diffusion policies through three core components:

\begin{itemize}
    \item \textbf{Gripper Configuration}: Let $\mathbb{G} \subset \mathbb{R}^{d_g}$ denote the space of two-finger gripper parameters encoding morphology, maximum width $w^{\max}$, and tool-center-point (TCP) $z^{\text{}}$, as shown in Fig. \ref{fig:grippers}. 
    \item \textbf{Observation Domain}: $\mathcal{O} = \mathcal{S}_{\text{sce}} \times \mathcal{S}_{\text{rob}}$ where $\mathcal{S}_{\text{sce}}$ represents scene observations (3D point clouds) and $\mathcal{S}_{\text{rob}} = SE(3) \times [0,g]$ the robot state (end-effector pose $\mathbf{x}_{\text{ee}} \in SE(3)$, gripper width $g$)\footnote{This paper forces on two-finger grippers problems in SE(3) space. SE(6)-space and three-finger grippers implementation is future work.}. $\mathbf{x}_{\text{ee}}$ is gripper-agnostic and reading from the robot. $g$ is specific for grippers.
    \item \textbf{Action Space}: $\mathcal{A} \subset \mathbb{R}^{d_a}$ containing end-effector displacements $\Delta\mathbf{x}_{\text{ee}}$ and gripper commands. 
\end{itemize}

The policy $\pi_\theta$ is trained on demonstrations $\mathcal{D} = \{\tau^{(i)}\}_{i=1}^N$ collected with a reference gripper $\mathbb{G}_0 \in \mathbb{G}$. Each trajectory $\tau = \{(\mathbf{o}_t, \mathbf{a}_t)\}_{t=0}^T$ satisfies: $\mathbf{o}_t = (\mathcal{S}^0_{\text{sce}}, \mathbf{x}_{\text{ee}}^0, g_t^0)$ and $\mathbf{a}_t \sim \pi_{\text{expert}}(\cdot|\mathbf{o}_t)$,
where superscript $0$ indicates $\mathbb{G}_0$ parameters.

During deployment with novel gripper $\mathbb{G}_i \neq \mathbb{G}_0$, the observation-action distribution shifts due to (1) visual/kinematic differences $\mathcal{O}_i \neq \mathcal{O}_0$, and (2) policy mismatch $p_{\theta}(\mathcal{A}|\mathcal{O}_i) \neq p_{\theta}(\mathcal{A}|\mathcal{O}_0)$. This manifests as trajectory divergence:
\begin{equation}
\|\tau_{1:T}^{\mathbb{G}_i} - \tau_{1:T}^{\mathbb{G}_0}\|_{\mathcal{W}} > \delta_{\text{tol}}\text{,}
\end{equation}
where $\mathcal{W}$ is the task-specific metric space and $\delta_{\text{tol}}$ the success threshold, e.g., objects cannot be grasped with shorter grippers, and collisions can result from using longer grippers.

\input{figure-exp-gripper}


To mitigate these issues, 
we develop policy $\pi_\theta^*$ that maintains task performance under gripper variation, combining (1) {gripper-invariant knowledge learning} and (2) {morphology-aware trajectory optimization} to achieve $\pi_\theta^*$ without policy retraining. The overview framework is shown in Fig. \ref{fig:framework}.

\input{figure-framework}
\input{figure-gmap}

\subsection{Learning Gripper-agnostic Grasping Knowledge}
Visuomotor policies, like Diffusion policy and 3D Diffusion Policy~\cite{chi2023diffusion,Ze2024DP3}, depend on visual observations to generate robot trajectories. However, swapping out different grippers during the online execution can alter visual observations (both RGB and point cloud inputs), as shown in Fig.\ref{fig:grippers}. Such alterations lead to out-of-distribution trajectory generations, reducing the task success rate\cite{liu2023towards}. To mitigate this limitation, we introduce a gripper-invariant \textit{grasping probability map} $\mathcal{G}_{\text{prob}}$ as an additional observation component, which captures object-centric grasp affordances  that are independent of end-effector geometry, thereby guiding the policy to focus on relevant object features rather than gripper-specific visual patterns. By decoupling object-related cues from the gripper's appearance, $\mathcal{G}_{\text{prob}}$ enhances the policy's robustness to variations in gripper morphology, maintaining stable task performance across different grippers. 


We adopt the Generative Grasping CNN (GG-CNN) \cite{morrison2020learning} for $\mathcal{G}_{\text{prob}}$ synthesis from depth images. GG-CNN is pre-trained on the Cornell Grasping Dataset~\cite{lenz2015deep}, which contains 885 RGB-D images with annotated parallel-jaw grasps across 240 objects. However, real-world pick-and-place manipulations introduce two key challenges: (1) the hand-eye camera moving with the robot, causing scale variations in object pixels, and (2) lighting changes disturb depth sensor readings. These factors degrade GG-CNN's output stability, i.e., $\mathcal{G}_{\text{prob}}$, and destabilize policy training and inference performance. To address this issue, our solution involves: (1) threshold filtering: discard pixels with $\mathcal{G}_{\text{prob}} < 0.7$, (2) centroid computation: $\mathds{O} = \frac{1}{N}\sum_{i=1}^N (u_i,v_i)$ for remaining pixels, and (3) region masking: generate $\mathcal{G}^*_{\text{prob}}$ through circular masking ($r=30$ pixels) about $\mathds{O}$. The map $\mathcal{G}^*_{\text{prob}}$ satisfies:
\begin{equation}
    \mathcal{G}^*_{\text{prob}}(u,v) = \begin{cases}
        1 & \text{, if } \|(u,v) - \mathds{O}\|_2 \leq 30 \\
        0 & \text{, otherwise}
    \end{cases}
\end{equation}


This spatial filtering maintains grasp affordance information while eliminating outlier predictions caused by sensor noise, as shown in Fig.\ref{fig:gmap}. The policy observation consists of:
\begin{equation}
    \label{eq:obs}
    \mathcal{O}^* = \mathcal{G}^*_{\text{prob}}\times \mathcal{S}_{\text{sce}}\times \mathcal{S}_{\text{rot}},
\end{equation}

\subsection{Optimizing Trajectory Generation} 
\label{sec:optimization}
The training scheme of our policy $\pi_{\theta}$ is consistent with that of Diffusion Policy (DP)~\cite{chi2023diffusion}, i.e. DDPM, with observations $\mathcal{O}^*$ and MSE training loss. During inference, $\pi_{\theta}$ employs Denoising Diffusion Implicit Models (DDIM)\cite{song2021denoising} sampling with 10 denoising iterations. However, our policy introduces a strategy to enforce the generative trajectory to fit different grippers, comprising gripper-geometry aware mapping and safety-constrained trajectory  projection. 


\textbf{Gripper mapping:} Gripper morphological variations induce the end-effector's pose discrepancies during identical object manipulation, primarily along: (1) vertical axis ($z$): tool-center-point offset, and (2) gripper state ($g$): grasping width differences. The discrepancies causes inconsistent actions predicted by $\pi_\theta$ across grippers.

Let $\mathbb{G}_{(0)}$ denote the reference gripper used during training, and $\mathbb{G}_{(i)}$ represent a novel gripper of category $i$. We define differentiable mapping functions $\mathds{M}_h(\cdot)$ and $\mathds{M}_g(\cdot)$ that project $\mathbb{G}_{(i)}$ parameters to the $\mathbb{G}_{(0)}$ basis: 
\begin{equation}
\label{eq:gripper_mapping}
\begin{aligned}
    z'_{(i)} &= \mathds{M}_h(z_{(i)}) = z_{(i)} + \Delta h_{(i)}, \\
    g'_{(i)} &= \mathds{M}_g(g_{(i)}) = \alpha_{(i)} g_{(i)},
\end{aligned}
\end{equation}
where $z_{(i)}$ is the measured height of end-effector equipped with $\mathbb{G}_{(i)}$, $\Delta h_{(i)} = z_{(0)} - z_{(i)}$ is the height offset from reference gripper, $g_{(i)} \in [g^{\min}, g^{\max}]$ is the real-time grasping width, $\alpha_{(i)} = {g^{\max}_{(0)}}/{g^{\max}_{(i)}}$ scales widths. The mapping parameters $\{\Delta h_{(i)}, \alpha_{(i)}\}$ are obtained through offline calibration with mechanical measurement of gripper dimensions. 

This transformation preserves the policy's internal representation while adapting to physical gripper properties, enabling zero-shot generalization to novel end-effectors. Current implementation focuses on translational pose adaptation, and the rotational compensation remains future work. During policy execution, the transformed pose $\mathcal{S}'_{\text{rob}} = (x,y,z'_{(i)},g'_{(i)})$ is fed to $\pi_\theta$ instead of $\mathcal{S}_{\text{rob}}$ in \eqref{eq:obs}, maintaining observation-space consistency across grippers.


\textbf{Safety-Constrained Trajectory Projection} 
While gripper mapping aligns geometric parameters, visual perception differences from gripper morphology can still induce unsafe trajectory variations. To guarantee constraint satisfaction, we integrate a projection layer into the DDIM denoising process~\cite{song2021denoising}. The modified reverse diffusion step becomes:
%
\begin{equation}\label{eq:modified_ddim}
  \mathbf{a}^{k-1}_{t} = \text{Proj}_{\mathcal{C}}\left(\mu_{k}(\mathbf{a}^k_{t}, \epsilon_{\theta}(\mathbf{a}^k_{t},\mathbf{o}_{t},k))\right),
\end{equation}
where $\text{Proj}_{\mathcal{C}}(\cdot)$ enforces safety constraints $\mathcal{C}$ through the following two steps.

\noindent\paragraph{\textbf{Constraint-aware denoising}}
For efficiency, projection activates only in the final denoising steps ($k \leq 2$). At each step $k$, we solve a quadratic program problem:

\begin{equation}\label{eq:safety_optimization}
\begin{aligned}
    \nu^{k*}_t &= \underset{\nu^k_t}{\arg\min} \|\nu^k_t\|^2_2 \\
    \text{s.t. } & \mathcal{S}'_{\text{rob}}(z)_t + \Phi\left(\mathbf{a}^{k}_t\right) + \nu^k_t \geq \epsilon_{\text{safe}}
\end{aligned}
\end{equation}
where $\Phi(\cdot)$ maps latent actions to Cartesian displacement, which is denormalization in our case, $\epsilon_{\text{safe}} = 0.01$ m (safety margin), and $\nu^k_t$ is the minimal corrective offset.


\noindent\paragraph{\textbf{Temporal consistency enforcement}} 
To maintain safety over the policy's $T_a$-step action horizon ($j \in [0,T_a-1]$), we extend \eqref{eq:safety_optimization} with cumulative constraints:
%
\begin{equation}\label{eq:sequence_constraint}
\mathcal{S}'_{\text{rob}}(z)_{t} + \sum_{r=0}^{j}\Phi(\mathbf{a}^{k}_{t+r}) + \nu^k_{t+j} \geq \epsilon_{\text{safe}}
\end{equation}

The projected actions $\mathbf{a}^{k*}_t = \Phi^{-}[\Phi(\mathbf{a}^{k}_t) + \nu^{k*}_t]$ guarantee:
\begin{equation}
\mathbb{P}\left(\bigcap_{j=0}^{T_a} \{\mathcal{S}'_{\text{rob}}(z)_{t+j} \geq \epsilon_{\text{safe}}\}\right) = 1,
\end{equation} 
indicating the cumulative trajectory is always safe, with an example of $\mathcal{S}'_{\text{rob}}(z)_{t+1}=\mathcal{S}'_{\text{rob}}(z)_t+\Phi(\mathbf{a}^{k*}_t)$.


\begin{algorithm}[!t]
    \caption{Gripper-Aware Trajectory Generation}\label{alg:safe_diffusion}
    \setstretch{1}
    \begin{algorithmic}[1]
    
        \Require Novel gripper $\mathbb{G}_i$, Observation $\mathbf{o}_t$, Safety margin $\epsilon_{\text{safe}}$, trained noise predicted $\epsilon_\theta$
        \Ensure Safe trajectory $\tau = \{\mathbf{a}_{t:t+T_a-1}\}$
        
        \noindent \textcolor{gray}{// \textbf{Online Inference:}}
        \State $\mathcal{S}'_{\text{rob}} \leftarrow \mathds{M}_h(z_{(i)}) = z_{(i)} + \Delta h_{(i)}$ \textcolor{gray}{// \textbf{Gripper Mapping:}}
        \State $g'_{(i)} \leftarrow \mathds{M}_g(g_{(i)}) = \alpha_{(i)} g_{(i)}$
        \State $\mathbf{\tilde{o}}_t \leftarrow (\mathcal{G}^*_{\text{prob}}, \mathcal{S}^{ }_{\text{sce}}, \mathcal{S}'_{\text{rob}})$
    
        \State $\mathbf{a}^{K}_t \sim \mathcal{N}(0, \mathbf{I})$ \textcolor{gray}{// \textbf{Diffusion Process:}}
        \Repeat
            \State $k \leftarrow K-1$, and $K \leftarrow K-1$
            \State $\mathbf{a}^{k}_t \leftarrow \mathcal{N}\big(\mu_{k}(\mathbf{a}^{k+1}_{t}, \epsilon_{\theta}(\mathbf{a}^{k+1}_{t},\mathbf{\tilde{o}}_t,k+1)), 0\big)$
            \State \textbf{if} $k \leq 1$: \textcolor{gray}{// \textbf{Safety Projection:}}
            \State\quad\ \textbf{for} $j \leftarrow 0$ \textbf{to} $T_a-1$ \textbf{do}
            \State\quad\quad\ \ $\nu^{k*}_{t+j} \leftarrow \arg\min\limits_{\nu} \|\nu^k_{t+j}\|^2_2$
            \State\quad\quad\ \ $\text{s.t. } \mathcal{S}'_{\text{rob}}(z)_{t}+\sum\limits_{r=0}^{j}\Phi(\mathbf{a}^{k}_{t+r}) + \nu^k_{t+j} \geq \epsilon_{\text{safe}}$
            \State\quad\ $\mathbf{a}^{k}_{t:t+T_a-1} \leftarrow \Phi^{-}[\Phi(\mathbf{a}^{k}_{t:t+T_a-1}) + \nu^{k*}_{t:t+T_a-1}]$
            %\State\quad$a^{k}_{t+j} \leftarrow a^{k*}_{t+j}$
        \Until{$K=0$}
        \State $\tau \leftarrow \text{Decode}(\mathbf{a}^{0}_{t:t+T_a-1})$
    \end{algorithmic}
\end{algorithm} 
