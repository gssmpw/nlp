\documentclass[10pt,conference]{IEEEtran}


\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

% \usepackage[a4paper,right=0.5in, left=0.5in, top=1.92cm, bottom=4.4cm]{geometry}
\usepackage[letterpaper, left=0.625in, right=0.625in, top=0.75in, bottom=0.95in]{geometry}

\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{amsmath,amsfonts}
\usepackage{tikz-cd}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning, shapes, arrows.meta}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{makecell}
\usepackage{amsthm}
\usepackage{adjustbox}
% \usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
\usepackage{stackengine}
% \newcommand\xrowht[2][0]{\addstackgap[.5\dimexpr#2\relax]{\vphantom{#1}}}
\usepackage{tabularx}
\usepackage{amssymb}

\usepackage{amsmath} 
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx,booktabs,multirow,bm}
\usepackage{subfig}
\usepackage{xcolor}
\usepackage{scalerel}
\usepackage{cite}
\usepackage{colortbl}
\newcommand{\bb}[1]{\textbf{#1}}
\newcommand{\uu}[1]{\underline{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{example}{Example}


\renewcommand{\baselinestretch}{0.88}

\input{math_command}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\IEEEoverridecommandlockouts
\begin{document}

% \title{Distribution Shift in Task-Oriented Communication:\\ An Information Bottleneck Solution}
% \title{Robust Distributed Co-Inference in Task-Oriented Communication with Information Bottleneck}
% \title{Accelerating Remote Training in Task-Oriented Communication: A Task-Agnostic Mutual Information Maximization Approach}
\title{Remote Training in Task-Oriented Communication: Supervised or Self-Supervised with Fine-Tuning?}

%% Multi-view
%% Robust
%% Real-time
%% Unsupervised
%% Task-oriented communication



% \title{Tackling Distribution Shifts in Task-Oriented Communication}
\author{\IEEEauthorblockN{Hongru~Li, Hang~Zhao, Hengtao~He, Shenghui~Song,\\ Jun Zhang,~\textit{Fellow,~IEEE,} and {Khaled B.~Letaief,~\textit{Fellow,~IEEE}}\\\IEEEauthorblockA{Dept. of Electronic and Computer Engineering,
The Hong Kong University of Science and Technology, Hong Kong\\
Email: \{hlidm, hzhaobi\}@connect.ust.hk, \{eehthe, eeshsong, eejzhang, eekhaled\}@ust.hk}}\\ \vspace*{-12mm}\thanks{This work was supported in part by the Hong Kong Research Grants Council under the Areas of Excellence scheme grant AoE/E-601/22-R, in part by NSFC/RGC Collaborative Research Scheme under grant CRS\_HKUST603/22, and in part by NSFC/RGC Joint Research Scheme under grant N\_HKUST656/22.}}

% \thanks{The authors are with the Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology (HKUST), Hong Kong (e-mail: hlidm@connect.ust.hk, jiawei.shao@connect.ust.hk, eehthe@ust.hk, eeshsong@ust.hk, eejzhang@ust.hk, eekhaled@ust.hk). (The corresponding author is Hengtao He.)}}


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.


\maketitle

\pagestyle{plain}  % no page number for the second and the later pages
\thispagestyle{empty}
% \begin{abstract}
% Task-oriented communication aims to extract and transmit only task-relevant information from the input data and ignore irrelevant information, which will significantly reduce the communication overhead. Most exsiting task-oriented communication methods focus on the communication overhead and system performance during inference time under the assumption that local training is feasible or the communication overhead during training is negligible. Furthermore, these task-oriented communication methods need to access the donwstream task and label information for training. However, in practical wireless communication systems, the connection topology among transceivers are dynamic and retraining the model from strach for each new connection is impractical while the label information and the downstream task are not always available before the connection is established. Therefore, how to reduce the communication overhead incurred during traininig and how to train the model without the need of the label information and the downstream task are two critical challenges in task-oriented communication.
% In this paper, we propose a mutual information maximization approach based on self-supervised learning to address these challenges, supported by information-theortical analysis. Specifically, we propose an efficient strategy that first pre-train the transmitter in a task-agnostic and label-free way, and then fine-tune the transmitter and receiver jointly with a task-specific and label-aware way. Simulation results show that the proposed approach can reduce the communication overhead during training to \textbf{about $\frac{1}{10}$ of the original with SGD optimizer} and achieve a comparable performance with the state-of-the-art full-supervised method.
% \end{abstract}
\begin{abstract}
% Task-oriented communication focuses on extracting and transmitting only task-relevant information from the input data, which significantly reduces communication overhead. Most exsiting task-oriented methods primarily emphasize minimizing communication overhead and optimizing system performance during inference, which always assuming feasible local training or negligible communication overhead during training. 
% Additionally, these methods typically require access to downstream tasks and labeled data for training. However, in practical wireless communication systems, the connection topology among transceivers is dynamic, making it impractical to retrain the model from scratch for each new connection. Additionally, task labels and downstream task information are often unavailable prior to connection establishment. Consequently, reducing communication overhead during training and training models without task labels or downstream task are two key challenges in the practical application of task-oriented communication. 
% Task-oriented communication aims to extract and transmit only task-relevant information, which effectively reduces communication overhead. Most of existing task-oriented communication methods focus on minimizing this overhead during inference, assuming feasible local training or negligible training communication resouces, and often require access to downstream task information and labeled data. However, in practical wireless systems with dynamic connection topologies, retraining models for each new connection is impractical, and task information is often unavailable before establishing connections. Thus reducing training overhead and enabling label-free, task-agnostic training are critical for task-oriented communication.
% In this paper, we address these challenges by leveraging mutual information maximization approach based on self-supervised learning and information-theoretic analysis. Specifically, we propose an efficient strategy that pre-trains the transmitter in a task-agnostic and label-free manner, followed by jointly fine-tuning of both the transmitter and receiver in a task-specific and label-aware manner. Simulation results demonstrate that our approach reduces training communication overhead to approximately half of the full-supervised method with an SGD optimizer, substantially improving the training efficiency.
Task-oriented communication focuses on extracting and transmitting only the information relevant to specific tasks, effectively minimizing communication overhead. Most existing methods prioritize reducing this overhead during inference, often assuming feasible local training or minimal training communication resources.
However, in real-world wireless systems with dynamic connection topologies, training models locally for each new connection is impractical, and task-specific information is often unavailable before establishing connections. Therefore, minimizing training overhead and enabling label-free, task-agnostic pre-training before the connection establishment are essential for effective task-oriented communication. In this paper, we tackle these challenges by employing a mutual information maximization approach grounded in self-supervised learning and information-theoretic analysis. We propose an efficient strategy that pre-trains the transmitter in a task-agnostic and label-free manner, followed by joint fine-tuning of both the transmitter and receiver in a task-specific, label-aware manner. Simulation results show that our proposed method reduces training communication overhead to about half that of full-supervised methods using the SGD optimizer, demonstrating significant improvements in training efficiency.
\end{abstract}
\begin{IEEEkeywords}
Mutual information optimization, self-supervised learning, task-oriented communication, training acceleration.
\end{IEEEkeywords}

\vspace{0.2cm}
\input{section/introduction}
\input{section/system.tex}
\input{section/method.tex}
\input{section/simulation.tex}
\input{section/conclusion.tex}
\input{section/appendix.tex}



\linespread{0.85}{
\bibliographystyle{./bibtex/IEEEtran}
%\bibliography{/Users/yuanming/OneDrive/Paper/topics/Reference}
% \bibliography{ref}
\bibliography{./bibtex/IEEEabrv,ref}
}
\end{document}


