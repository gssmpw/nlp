% \newpage
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{image/pgm.pdf}
%     \caption{The probabilistic graphical model of the considered task-oriented communication scheme.}
%     \end{figure}
% \begin{definition}{\textbf{Sufficiency:}}
%     A random variable $Z$ is a sufficient representation of $X$ for predicting $Y$ if and only if $I(Z;Y)=I(X;Y)$.
% \end{definition}

% As discussed in~\cite{shao2023task, alemi2016deepVIB,e22090999_ceb}, an ideal representation $Z_k$ should satisfied
% \begin{equation}
%     \begin{aligned}
%         \rlap{$\underbrace{\phantom{{I(X_{k} ; Z_{k}) =I(Y;X_{k})}}}_{\textbf{Minimality}}$} I(X_{k} ; Z_{k}) = \overbrace{I(Y;X_{k}) =I(Y ; Z_{k})}^{\textbf{Sufficiency}},\ k \in \{1,\ldots,K\},
%     \end{aligned}
% \end{equation}

% \begin{definition}{\textbf{Consistency:}}
    
% \end{definition}

% \begin{definition}{\textbf{Redundancy:}}
%     Given two views $X$ and $X'$, the mutual information between $X$ and its representation $Z$ can be decomposed by
%     \begin{equation}
%         I(X ; Z)=\underbrace{I(X ; Z| X')}_{\text {superfluous information }}+\underbrace{I(X' ; Z)}_{\text {predictive information }},
%     \end{equation}
%     The superfluous part inforamtion is the redundancy between $X$ and $X'$.
% \end{definition}



\appendices
\section{Proof of Theorem 1}

\begin{proof} The proof is mainly based on the chain rule of mutual information and the following PGM, 
    \begin{equation} 
    X' \leftarrow Y \rightarrow X \rightarrow Z \rightarrow \hat{Z}.
    \label{eq:proof_pgm} 
    \end{equation}
    Given label information \( Y \), we have \( \max I(Z^{\text{sup}};Y) = I(X;Y) \). Considering perfect information transmission, we have \( \max I(\hat{Z}^{\text{sup}};Y) = \max I(Z^{\text{sup}};Y) = I(X;Y) \), i.e., \( I(\hat{Z}^{\text{opt}}_{\text{sup}};Y) = I(\hat{Z}^{\text{opt}}_{\text{sup}_{\text{min}}};Y) = I(X;Y) \). With imperfect information transmission, we decompose \( I(\hat{Z}^{\text{ssl}}, X') \) given label information $Y$ by the chain rule of mutual information as follows, \begin{equation} I(\hat{Z}^{\text{ssl}}; X') = I(\hat{Z}^{\text{ssl}}; Y; X') + I(\hat{Z}^{\text{ssl}}; X' | Y). \end{equation} Furthermore, with imperfect information transmission, the information loss is \( I(Z^{\text{ssl}}; X') - I(\hat{Z}^{\text{ssl}}; X') = \epsilon_c \), and according to the PGM in \eqref{eq:proof_pgm}, we have \begin{equation} \max I(\hat{Z}^{\text{ssl}}; X') = \max I(Z^{\text{ssl}}; X') - \epsilon_c = I(X; X') - \epsilon_c. \end{equation} By the chain rule of mutual information, we decompose \( I(\hat{Z}^{\text{ssl}}; X'; Y) \) as \begin{equation} I(\hat{Z}^{\text{ssl}}; X'; Y) = I(\hat{Z}^{\text{ssl}}; X') - I(\hat{Z}^{\text{ssl}}; X' | Y), \end{equation} and have \( I(\hat{Z}^{\text{ssl}}; X' | Y) = 0 \) according to the \( d \)-separation principle. Thus, we have \begin{equation} \max I(\hat{Z}^{\text{ssl}}; X'; Y) = \max I(\hat{Z}^{\text{ssl}}; X') = I(X; X') - \epsilon_c. \end{equation} Then, \begin{equation} \begin{aligned} &I(\hat{Z}^{\text{opt}}_{\text{ssl}}; X') = I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; X') = I(X; X') - \epsilon_c, \\ &I(\hat{Z}^{\text{opt}}_{\text{ssl}}; X'; Y) = I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; X'; Y) = I(X; X'; Y) - \epsilon_c. \end{aligned} \label{eq:proof_interMI} \end{equation} By decomposing the second equation in~\eqref{eq:proof_interMI}, we have \begin{equation} \begin{aligned} I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y) &= I(\hat{Z}^{\text{opt}}_{\text{ssl}}; X'; Y) + I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y | X') \\ &= -\epsilon_c + I(X; X'; Y) + I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y | X') \\ &= -\epsilon_c + I(X; Y) - I(X; Y | X') + I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y | X'). \label{eq:proof_ssl} \end{aligned} \end{equation} A similar derivation can be made for \( I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; Y) \), \begin{equation} I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; Y) = -\epsilon_c + I(X; Y) - I(X; Y | X') + I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; Y | X'). \end{equation} According to the the data processing inequality with \eqref{eq:proof_pgm} and~\eqref{eq:proof_ssl}, we have \begin{equation} \begin{aligned} &I(X; Y | X') \geq I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y | X'), \\ &I(\hat{Z}^{\text{opt}}_{\text{sup}_{\text{min}}}; Y) = I(\hat{Z}^{\text{opt}}_{\text{sup}}; Y) = I(X; T) > I(\hat{Z}^{\text{opt}}_{\text{ssl}}). \label{eq:proof_comb1} \end{aligned} \end{equation} Furthermore, according to~\eqref{eq:proof_ssl}, \begin{equation} \begin{aligned} I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}) &= -\epsilon_c + I(X; Y) - I(X; Y | X') + I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; Y | X') \\ &\geq -\epsilon_c + I(X; Y) - I(X; Y | X'). \label{eq:proof_comb2} \end{aligned} \end{equation} Finally, combining~\eqref{eq:proof_comb1} and~\eqref{eq:proof_comb2}, we have \begin{equation} \begin{aligned} I(X; Y) &= I(\hat{Z}^{\text{opt}}_{\text{sup}}; Y) = I(\hat{Z}^{\text{opt}}_{\text{sup}_{\text{min}}}; Y) \geq I(\hat{Z}^{\text{opt}}_{\text{ssl}}; Y) \\ &\geq I(\hat{Z}^{\text{opt}}_{\text{ssl}_{\text{min}}}; Y) \geq I(X; Y) - I(X; Y | X') - \epsilon_c. \end{aligned} \end{equation} \end{proof}
% \begin{proof}
    
% The proof is mainly based on the the following PGM,
% \begin{equation}
%     X'\leftarrow Y \rightarrow X \rightarrow Z \rightarrow \hat{Z}
%     \label{eq:proof_pgm}
% \end{equation} and the chain rule of mutual information.

% Given label information $Y$, we have $\max I(Z;Y)=I(X;Y)$. Considering the perfect information transmission, we have $\max I(\hat{Z};Y)=\max I(Z;Y)=I(X;Y)$, i.e., $I(\hat{Z}^{opt};Y)=I(\hat{Z}^{opt}_{min};Y)=I(X;Y)$.

% Then, when without label information $Y$ and imperfect information transmission, we decompose $I(\hat{Z},X')$ by the chain rule of mutual information as following,
% \begin{equation}
% I(\hat{Z};X')=I(\hat{Z};Y;X')+I(\hat{Z};X'|Y).
% \end{equation}
% Furthermore, under the assumption that during imperfect information transmission, the information loss is $I(Z;X')-I(\hat{Z};X')=\epsilon_c$, and according to the PGM in \eqref{eq:proof_pgm}, we have
% \begin{equation}
%     \max I(\hat{Z};X') = \max I(Z;X')-\epsilon_c = I(X;X')-\epsilon_c.
% \end{equation}
% By the chain rule of mutual information, we decompose $I(\hat{Z};X';Y)$ as
% \begin{equation}
%     I(\hat{Z};X';Y) = I(\hat{Z};X')-I(\hat{Z};X'|Y),
% \end{equation}
% according to the $d$-separation principle, $I(\hat{Z};X'|Y)=0$, thus we have
% \begin{equation}
%     \max I(\hat{Z};X';Y) = \max I(\hat{Z};X') = I(X;X')-\epsilon_c
% \end{equation}
% Hence,
% \begin{equation}
%     \begin{aligned}
%         &I(\hat{Z}'^{opt};X')=I(\hat{Z}'^{opt}_{min};X')=I(X;X')-\epsilon_c,\\
%         &I(\hat{Z}'^{opt};X';Y)=I(\hat{Z}'^{opt}_{min};X';Y)=I(X;X';Y)-\epsilon_c.
%     \end{aligned}
%     \label{eq:proof_interMI}
% \end{equation}

% By decomposing the second equation in~\eqref{eq:proof_interMI}, we have
% \begin{equation}
%     \begin{aligned}
%         I(\hat{Z}'^{opt};Y)&=I(\hat{Z}'^{opt};X';Y)+I(\hat{Z}'^{opt};Y|X')\\
%         &=-\epsilon_c + I(X;X';Y)+I(\hat{Z}'^{opt};Y|X')\\
%         &=-\epsilon_c + I(X;Y) - I(X;Y|X')+I(\hat{Z}'^{opt};Y|X')
%         \label{eq:proof_ssl}
%     \end{aligned}
% \end{equation}
% Similar derivation can be made for $I(\hat{Z}'^{opt}_{min};Y)$,
% \begin{equation}
%     I(\hat{Z}'^{opt}_{min};Y) = -\epsilon_c + I(X;Y) - I(X;Y|X') +I(\hat{Z}'^{opt}_{min};Y|X').
% \end{equation}
% According to the PGM~\eqref{eq:proof_pgm}, data processing inequality and~\eqref{eq:proof_ssl}, we have
% \begin{equation}
%     \begin{aligned}
%         &I(X;Y|X')\geq I(\hat{Z}'^{opt};Y|X'),\\
%         &I(\hat{Z}^{opt}_{min};Y)=I(\hat{Z}^{opt};Y)=I(X;T)>I(\hat{Z}'^{opt}).
%         \label{eq:proof_comb1}
%     \end{aligned}
% \end{equation}
% Furthermore, according to~\eqref{eq:proof_ssl},
% \begin{equation}
%     \begin{aligned}
%         I(\hat{Z}'^{opt}_{min})&=-\epsilon_c + I(X;Y) - I(X;Y|X') +I(\hat{Z}'^{opt}_{min};Y|X')\\
%         &\geq -\epsilon_c + I(X;Y) - I(X;Y|X').
%         \label{eq:proof_comb2}
%     \end{aligned}
% \end{equation}
% Finally, combine~\eqref{eq:proof_comb1} and~\eqref{eq:proof_comb2}, we have
% \begin{equation}
%     \begin{aligned}
%         I(X;Y)&=I(\hat{Z}^{opt};Y)=I(\hat{Z}^{opt}_{min};Y)\geq I(\hat{Z}'^{opt};Y)\\
%         &\geq I(\hat{Z}'^{opt}_{min};Y)\geq I(X;Y)-I(X;Y|X')-\epsilon_c
%     \end{aligned}
% \end{equation}
% \end{proof}



% \section{Lower Bound of Mutual Information}
% We begin with the definition of mutual information,
% \begin{equation}
%     I(X;Y) = \mathbb{E}_{p(x,y)}[\log \frac{p(x,y)}{p(x)p(y)}].
% \end{equation}
% The mutual information neural esimation (MINE) is propsoed to estimate the mutual information between two random variables $X$ and $Y$ by a neural network. The MINE objective is to minimize the following loss function,
% % \begin{equation}
% %     \mathcal{L}_{MINE} = \mathbb{E}_{p(x,y)}[T(x,y)] - \log (\mathbb{E}_{p(x)p(y)}[e^{T(x,y)}]),
% % \end{equation}
% \begin{equation}
%     \mathbb{E}_{p(x,y)}[T(x,y)] - \log (\mathbb{E}_{p(x)p(y)}[e^{T(x,y)}]) \triangleq I_{MINE},
% \end{equation}
% considering that 
% \begin{equation}
%     \log(x)\leq \frac{x}{e} + \log(e) -1
% \end{equation}

% \begin{equation}
%     \mathbb{E}_{p(x, y)}[f(x, y)]-e^{-1} \mathbb{E}_{p(y)}[Z(y)] \triangleq I_{NWJ}
% \end{equation}

% % \begin{equation}
% %     \mathcal{L}_{MC}=\frac{1}{2 B} \sum_{i=1}^{2 B}-\log \left(\frac{\exp \left(\frac{\mathbf{z}_i \cdot \mathbf{z}_j}{\tau}\right)}{\sum_{k \neq i} \exp \left(\frac{\mathbf{z}_i \cdot \mathbf{z}_k}{\tau}\right)}\right)
% % \end{equation}

% \begin{equation}
%     I(X ; Y) \geq \mathbb{E}\left[\frac{1}{K} \sum_{i=1}^K \log \frac{e^{f\left(x_i, y_i\right)}}{\frac{1}{K} \sum_{j=1}^K e^{f\left(x_i, y_j\right)}}\right]
% \end{equation}


% \section{Derivation of Redundancy}
% \label{appendix:decomp}