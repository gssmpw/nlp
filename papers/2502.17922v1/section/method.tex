\section{Mutual Information Maximization for Task-Agnostic Feature Extraction}
\label{sec:method}
In this section, we develop a two-stage algorithm for accelerating remote training in task-oriented communication, based on a mutual infomration maximization approach. The proposed method consists of two stages: 1) task-agnostic pre-training for transmitter and 2) task-specific fine-tuning for transmitter and receiver. We first introduce the self-supervised feature extraction by data augmentation and utilize a tractable mutual information to maximize the shared information between the data and its augmentation. We then present the overall algorithm for accelerating remote training in task-oriented communication.
% Fine-tuning then adapts this feature extractor to specific downstream tasks by incorporating task-specific information. The overall algorithm is summarized in Algorithm~\ref{alg:overall}.
% % We first reformulate the problem from a multi-view perspective and then derive mutual information maximization for multi-view feature extraction. Finally, the overall algorithm for accelerating remote training in task-oriented communication will be introduced.
% Particularly, in the first stage, we first reformulate the task-agnostic feature extraction problem from a multi-view perspective and then derive mutual information maximization for multi-view feature extraction. Then in the second stage, we 


% Our proposed approach consists of two stages: 1) task-agnostic pre-training, and 2) task-specific fine-tuning. During pre-training, we aim to develop a universal feature extractor that captures the intrinsic structure of input data without requiring downstream task information. Fine-tuning then adapts this feature extractor to specific downstream tasks by incorporating task-specific information. 

% The summary of the proposed method is shown in Fig.~\ref{fig:sys}.
% We begin by reformulating the problem from a multi-view perspective, deriving mutual information maximization for multi-view feature extraction, and presenting an overall algorithm for accelerating remote training in task-oriented communication. A summary of the proposed method is shown in Fig.~\ref{fig:sys}.

% The pre-training stage aims to learn a universal feature extractor that can capture the intrinsic structure of the input data without the need for the downstream task information. The fine-tuning stage aims to adapt the pre-trained feature extractor to the specific downstream task by leveraging the task-specific information. We begin with reformulating the problem from the multi-view perspective, followed by the detailed derivation of mutual information maximization for multi-view feature extraction, we then present the overall algorithm for accelerating remote training in task-oriented communication. A summary of the proposed method is provided in Fig.~\ref{fig:sys}.


\subsection{Self-Supervised Feature Extraction}
\label{sec:method:multi-view}

The minimality and sufficiency of the extracted features are two crucial properties for task-oriented communication systems.
Minimality ensures the features $\hz$ to be as compact as possible to reduce the communication overhead, i.e., $I(X;\hZ)$ should be minimized~\cite{vfe, li2024tackling}. Additionally, the sufficiency requires $\hat{Z}$ is a sufficient representation of $X$ for predicting $Y$ if and only if $I(\hat{Z};Y)=I(X;Y)$.

Among all sufficient representations, the minimal representation is the one that has minimal complexity, i.e., $I(X;Z)$ is minimized. As discussed in~\cite{shao2023task, alemi2016deepVIB,e22090999_ceb}, an ideal representation $\hat{Z}$ should satisfy
\begin{equation}
    \begin{aligned}
        \label{eq:ideal}
        \rlap{$\underbrace{\phantom{{I(X ; \hat{Z}) =I(Y;X)}}}_{\textbf{Minimality}}$} I(X ; \hat{Z}) = \overbrace{I(Y;X) =I(Y ; \hat{Z})}^{\textbf{Sufficiency}},
    \end{aligned}
\end{equation}
and can be achieved by minimizing the IB objective function~\cite{vfe, li2024tackling}.

% However, the IB objective function~\eqref{eq:ideal} requires label information $Y$ for optimizaiton. To address this challenge, we propose a self-supervised feature extraction approach that leverages the multi-view learning framework. 

However, optimizing the IB objective function in \eqref{eq:ideal} requires the known label information $Y$, which is not practical in task-agnostic feature extraction scenarios. To this end, we propose a self-supervised feature extraction approach that leverages a multi-view learning framework~\cite{chen2020simple,tsai2020self}. Inspired by the success of multi-view feature coding for task-oriented communication~\cite{shao2022task}, we reformulate the problem of task-agnostic feature extraction as a multi-view feature extraction problem. 
Specifically, we consider the input data $X$ and its augmented version $X' = Aug(X)$ as two views of the same data. Thus the self-supervised feature extraction problem can be reformulated from the multi-view perspective, e.g., from view $X$ and view $X'$~\cite{chen2020simple,federici2020learning}. We assume the task-relevant information is included in the shared information between the two views, which is reasonable since $X$ and the augmented data $X'$ convey the same semantic content from human perception. Thus the unique information in each view can be considered as the redundant information~\cite{federici2020learning}, allowing us to decompose the mutual information between $X$ and its representation $Z$ as follows,
\vspace{-0.1cm}
\begin{equation}
        \label{eq:redundancy}
        \begin{aligned}
        I(X ; \hat{Z})&\overset{(a)}{=}\ \ \ \ I(X,X';\hat{Z})\\
        &\overset{(b)}{=}\underbrace{I(X ; \hat{Z}| X')}_{\text {\textbf{Redundant Information }}}+\underbrace{I(X' ; \hat{Z})}_{\text {\textbf{Shared Information }}},
        \end{aligned}
    \end{equation}
where (a) holds because $\hat{Z}$ is a noise-corrupted representation of $X$ and makes $\hat{Z}$ conditionally independent of $X'$ given $X$; (b) follows from the chain rule of mutual information.
The redundant information represents the surplus content in the representation $\hat{Z}$ of view $X$ that is not shared by view $X'$, where the shared information is task-relevant that should be extracted. Therefore, to learn the cross-view task-relevant information, we aim to maximize the predictive information $I(X' ; \hat{Z})$. However, directly optimizing $I(X';\hat{Z})$ or find a tractable variational lower bound like that in the IB objective function~\eqref{eq:ideal} is challenging. To address this problem, we propose to maximize the mutual information between the two representations $\hat{Z},\hat{Z'}$ from the two views $X$ and $X'$, i.e., $I(\hat{Z};\hat{Z'})$ as the followling inequality shows,
\begin{equation}
    \begin{aligned}
        I(\hat{Z};\hat{X'})&\overset{(a)}{=}I(\hat{Z};X',\hat{Z'}) - I(\hat{Z};\hat{Z'}|X')\\
        &= I(\hat{Z};X',\hat{Z'})- (H(\hat{Z'}|X')-H(\hat{Z'}|\hat{Z},X'))\\
        &\overset{(b)}{=} I(\hat{Z};X',\hat{Z'})\\
        &\overset{(c)}{=} I(\hat{Z};\hat{Z'}) + I(Z;X'|Z')\\
        &\geq I(\hat{Z},\hat{Z'}),
    \end{aligned}
\end{equation}
where (a) and (c) are obtained by the chain rule of mutual information, and (b) follows from the fact that $\hat{Z'}$ is the received feature of $X'$, thus $\hat{Z'}$ is conditionally independent of $\hat{Z}$ given $X'$. This derivation shows that $I(\hat{Z},\hat{Z'})$ can be considered as a surrogate objective for maximizing the predictive information $I(X';\hat{Z})$. 
To further support the feasiblity of the proposed self-supervised learning framework, similar to~\cite{tsai2020self}, we present the following theorem to analyze the information loss due to wireless channel and internal noise within the dataset.

% We further support the feasiblity of the proposed self-supervised learning framework, we provide the following theorem by analysis the information loss due to wireless channel and internal noise of dataset.

% \begin{theorem}
%     For a specific view $X$ with perfect information transmission, the optimal learned representations from supervised learning satisfy
%     \begin{equation}
%         \hat{Z}^{opt}_{sup}\triangleq \mathop{\arg\max}_{\hat{Z}} I(\hat{Z};Y),
%     \end{equation}
%     Then, given $X$ and $X'=Aug(X)$ with information loss $I(Z;X')-I(\hat{Z};X')=\epsilon_c$ due to the wireless channel, the optimal learned representations from self-supervised learning satisfy
%     \begin{equation}
%         \hat{Z}^{opt}_{ssl}\triangleq \mathop{\arg\max}_{\hat{Z}}I(\hat{Z};X').
%     \end{equation}
%     With the minimal optimal learned representations $\hat{Z}^{opt}_{sup_{min}}$ and $\hat{Z}^{opt}_{ssl_{min}}$ satisfies $\hat{Z}^{opt}_{min}\triangleq \mathop{\arg\min}_{\hat{Z}}H(\hat{Z}|Y)$ for both supervised and self-supervised learning, we have
%     \begin{equation}
%         \begin{aligned}
%             I(X;Y)&=I(\hat{Z}^{opt}_{sup};Y)=I(\hat{Z}^{opt}_{{sup}_{min}};Y)\geq I(\hat{Z}^{opt}_{ssl};Y)\\
%             &\geq I(\hat{Z}^{opt}_{ssl_{min}};Y)\geq I(X;Y)-I(X;Y|X')-\epsilon_c.
%         \end{aligned}
%     \end{equation}
% \end{theorem}

\begin{theorem} 
    For a specific view \( X \) with perfect information transmission, i.e., \( I(Z;X') - I(\hat{Z};X') = 0 \), the optimal learned representations from supervised learning satisfy 
    \begin{equation} 
    \hat{Z}^{\textnormal{opt}}_{\textnormal{sup}} \triangleq \mathop{\arg\max}_{\hat{Z}} I(\hat{Z};Y). 
    \end{equation} 
    Then, given \( X \) and \( X' = \textnormal{Aug}(X) \) with information loss \( I(Z;X') - I(\hat{Z};X') = \epsilon_c \) due to the wireless channel, the optimal learned representations from self-supervised learning satisfy 
    \begin{equation} 
    \hat{Z}^{\textnormal{opt}}_{\textnormal{ssl}} \triangleq \mathop{\arg\max}_{\hat{Z}} I(\hat{Z};X'). 
    \end{equation} 
    With the minimal optimal learned representations \( \hat{Z}^{\textnormal{opt}}_{\textnormal{sup}_{\textnormal{min}}} \) and \( \hat{Z}^{\textnormal{opt}}_{\textnormal{ssl}_{\textnormal{min}}} \) satisfying \( \hat{Z}^{\textnormal{opt}}_{\textnormal{min}} \triangleq \mathop{\arg\min}_{\hat{Z}} H(\hat{Z}|Y) \) for both supervised and self-supervised learning, we have 
    \begin{equation} 
    \begin{aligned} 
    I(X;Y) &= I(\hat{Z}^{\textnormal{opt}}_{\textnormal{sup}};Y) = I(\hat{Z}^{\textnormal{opt}}_{\textnormal{sup}_{\textnormal{min}}};Y) \geq I(\hat{Z}^{\textnormal{opt}}_{\textnormal{ssl}};Y) \\ 
    &\geq I(\hat{Z}^{\textnormal{opt}}_{\textnormal{ssl}_{\textnormal{min}}};Y) \geq I(X;Y) - I(X;Y|X') - \epsilon_c. 
    \end{aligned} 
    \end{equation} 
\end{theorem}
    
\textbf{Theorem 1} shows that the optimal learned representations from self-supervised learning can achieve the same performance as supervised learning with a small information loss. This result further supports the feasibility of the proposed self-supervised learning framework for task-agnostic feature extraction.

\subsection{Tractable Mutual Information Maximization}
The surrogate objective $I(\hat{Z},\hat{Z'})$ is symmetric but challenging to optimize due to the difficulty in estimating high-dimensional mutual information. To address this challenge, we use the InfoNCE~\cite{oord2018representation} to approximate the mutual information between the two representations $\hat{Z}$ and $\hat{Z'}$. Specifically, we maximize the following objective function,
\begin{equation}
    \label{eq:infonce_objective}
    \mathcal{L}_{InfoNCE}(\theta) = \mathbb{E}_{p(x,x')}[\log\frac{e^{<f_\theta(x),f_\theta(x')>}}{\sum_{x_j\in \mathcal{X}}e^{<f_\theta(x),f_\theta(x_j)>}}],
\end{equation}
where $f_\theta(\cdot)$ is the feature extractor parameterized by $\theta$, and $<\cdot,\cdot>$ denotes cosine similarity. 
% The underlying intuition of the InfoNCE estimator is that the cosine similarity between the two representations $\hat{Z}$ and $\hat{Z'}$ from the same data (i.e., the positive pair $(f_\theta(x),f_\theta(x'))$) should be maximized and the cosine similarity between the two representations from different data (i.e., the negative pair $(f_\theta(x),f_\theta(x_j)), for x_j \in \mathcal{X}$) should be minimized to ensure the shared information between the two views can be captured.
The underlying intuition of the InfoNCE estimator is to maximize the cosine similarity between representations $\hat{Z}$ and $\hat{Z'}$ from the same data (i.e., the positive pair $(f_\theta(x),f_\theta(x'))$) while minimizing the cosine similarity between representations from different data (i.e., the negative pair $(f_\theta(x),f_\theta(x_j)), \text{for } x_j \in \mathcal{X}$). This ensures that the shared information between the two views is captured. The InfoNCE estimator provides a lower bound on the mutual information between the representations $Z$ and $\hat{Z}$.
To make this objective tractable, we apply Monte Carlo sampling and temperature scaling, yielding the following practical optimization objective,
\begin{equation}
    \label{eq:infonce_objective_mc}
    \mathcal{L}_{InfoNCE}(\theta)\simeq\frac{1}{2 B} \sum_{i=1}^{2 B}\log \left(\frac{e^{\left(<f_\theta(x_i),f_\theta(x_i')>/\tau\right)}}{\sum_{j=1}^{2B}e^{\left(<f_\theta(x_i),f_\theta(x_j)>/\tau\right)}}\right)
\end{equation}
% where $B$ is the batch size of a mini-batch of data sampled from $\mathcal{X},\mathcal{X'}$, and $\tau$ is the temperature scaling factor. The above objective function can be optimized by stochastic gradient descent (SGD) algorithm thus we can maximize the mutual information between the two views $X$ and $X'$ tractably.
where $B$ is the mini-batch size, and $\tau$ is the temperature scaling factor. This objective function can be optimized using the stochastic gradient descent (SGD) algorithm, enabling a tractable mutual information maximization between the two views $X$ and $X'$.
\subsection{Overall Algorithm}
In previous sections, we have analyzed the feasiblity of learning shared information between two views $X$ and $X'$ for task-agnostic feature extraction and introduced a self-supervised feature approach based on mutual information maximization. Here, we present the overall algorithm designed to accelerate remote training in task-oriented communication. 

The overall algorithm comprises two main stages: 1) task-agnostic pre-training and 2) task-specific fine-tuning. In the pre-training stage, we first pre-train the feature extractor $f_\theta(\cdot)$ with InfoNCE estimator in~\eqref{eq:infonce_objective_mc}, creating a universal feature extractor that captures the intrinsic structure of the input data without requiring downstream task information. In the fine-tuning stage, we update the pre-trained feature extractor in device $f_{\theta}(\cdot)$ and the random inferencer $g_{\phi}(\cdot)$ in server jointly with the task-specific information. The overall algorithm is summarized in Algorithm~\ref{alg:overall}. 

\vspace{5pt}
\begin{algorithm}
    \caption{Self-Supervised Learning for Task-Oriented Communication}
    \label{alg:overall}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Training data $\mathcal{D}=\{x_i,x'_i, y_i\}_{i=1}^{N}$, batch size $B$, temperature scaling factor $\tau$, number of epochs $E_1,E_2$.
        % \State \textbf{Output:} Pre-trained feature extractor $f_\theta(\cdot)$.
        \State \textbf{Pre-training:}
        \State \texttt{\# No information transmission during pre-training}
        \State Initialize $f_\theta(\cdot)$ with random weights.
        \For{$e=1$ to $E_1$}
        \State Sample a mini-batch of data $\{(x_i,x_i')\}_{i=1}^{B}$ from $\mathcal{D}$.
        \State Compute the InfoNCE estimator~\eqref{eq:infonce_objective_mc} with the mini-batch data.
        \State Update the feature extractor $f_\theta(\cdot)$ by minimizing the InfoNCE estimator with SGD.
        \EndFor
        \State \textbf{Fine-tuning:}
        \State \texttt{\# Information transmission is needed during fine-tuning for updating $g_\phi(\cdot)$}
        \State Initialize $f_\theta(\cdot)$ with the pre-trained weights.
        \State Initialize $g_\phi(\cdot)$ with random weights.
        \For{$e=1$ to $E_2$}
        \State Sample a mini-batch of data $\{(x_i,y_i)\}_{i=1}^{B}$ from $\mathcal{D}$.
        \State Compute the task-specific loss with $f_\theta(\cdot)$ and $g_\phi(\cdot)$.
        \State Update $f_\theta(\cdot)$ and $g_\phi(\cdot)$ by minimizing the task-specific loss with SGD.
        \EndFor
    \end{algorithmic}
\end{algorithm}



