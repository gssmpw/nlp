\section{Performance Evaluation}
\label{sec:simulation}

% We consider the classification task over Ominiglot dataset~\cite{lake2015human} in the following performance evaluation. The Ominiglot dataset contains 1623 different handwritten characters from 50 different alphabets including 964 training characters and 659 testing characters. We consider AWGN channel and Rayleigh fading channel as the wireless channel model. We set the $P=1$ and $\tau =0.1$.
We evaluate performance on a classification task using the Omniglot dataset~\cite{lake2015human}, which contains 1,623 different handwritten characters across 50 alphabets, including 964 training characters and 659 testing characters. For the wireless channel, we consider both the AWGN channel and the Rayleigh fading channel with $P=1$ and $\tau =0.1$.

Performance metrics include test accuracy and task-specific loss (cross-entropy loss for classification) versus the number of communication rounds. One communication round includes both forward and backward passes for a mini-batch data\footnote{As the central server has greater computation and communication resources, we assume no gradient information loss over the wireless channel, with exact gradient recovery at the edge device.}.
\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{labelformat=empty}
    \subfloat[(a) AWGN, $\textrm{SNR}= 0$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_0dB_awgn_acc.pdf}
    }
    \subfloat[(b) AWGN, $\textrm{SNR}= 10$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_10dB_awgn_acc.pdf}
    }
    \subfloat[(c) AWGN, $\textrm{SNR}= 20$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_20dB_awgn_acc.pdf}
    }

    \subfloat[(d) Rayleigh, $\textrm{SNR}= 0$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_0dB_rayleigh_acc.pdf}
    }
    \subfloat[(e) Rayleigh, $\textrm{SNR}= 10$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_10dB_rayleigh_acc.pdf}
    }
    \subfloat[(f) Rayleigh, $\textrm{SNR}= 20$ dB]{
        \centering
        \includegraphics[width=0.32\linewidth]{result/1000_60_SGD_Omniglot_20dB_rayleigh_acc.pdf}
    }

    % \caption{The rate-distortion curves for image classification tasks with $\text{PSNR}_{\text{test}}=\text{PSNR}_{\text{train}}$. (a) Colored-MNIST, $\textrm{PSNR}= 10\textrm{dB}$, (b) Colored-MNIST, $\textrm{PSNR}= 20\textrm{dB}$, (c) Colored-Object, $\textrm{PSNR}= 10\textrm{dB}$ and (d) Colored-Object, $\textrm{PSNR}= 20\textrm{dB}$.}
    \caption{The communication round versus the test accuracy for the four methods with $\text{SNR}_{\text{test}}=\text{SNR}_{\text{train}}$.}
    \label{fig:results1}
\end{figure*}
We compare the performance of the four methods as follows,
\begin{itemize}
    \item \textbf{Full Supervised (Sup)}: The transmitter and receiver are trained from strach with supervised learning, full label information, and the downstream task.
    \item \textbf{Full Supervised with 60\% Label \stretchrel*{\textbf{(}}{\strut}Sup (60\%)\stretchrel{\textbf{)}}{\strut}}: The transmitter and receiver are trained from strach with supervised learning, 60\% label information, and the downstream task.
    \item \textbf{Self-Supervised and Fine-Tune (SSL-FT)}: The transmitter is pre-trained in a task-agnostic and label-free way, and then fine-tuned with the downstream task and label information.
    \item \textbf{Self-Supervised and Fine-Tune with 60\% Label \stretchrel*{\textbf{(}}{\strut}SSL-FT (60\%)\stretchrel{\textbf{)}}{\strut}}: The transmitter is pre-trained in a task-agnostic and label-free way, and then fine-tuned with 60\% label information and the downstream task.
\end{itemize}
where \textbf{SSL-FT} and \textbf{SSL-FT (60\%)} are our proposed training methods. All methods are trained with the same backbone and optimized using the SGD optimizer with a learning rate of 0.001. The batch size is set to 512, and the total training epochs are 1,000. In each communication round, only one mini-batch is used for training and each epoch-level parameters update requires 7 communication rounds.

In Fig.~\ref{fig:results1}, we illustrate test accuracy versus communication rounds for the four methods under different SNRs. The proposed methods show significantly faster test accuracy growth compared to fully supervised methods, consistently achieving superior test accuracy across all training stages and channel conditions. This is because the proposed methods have captured task-relevant information during the pre-training stage, resulting in faster convergence in the fine-tuning stage. Moreover, with only 60\% of the label information, the proposed \textbf{SSL-FT (60\%)} method outperforms the \textbf{Sup} method that uses full label information, demonstrating the effectiveness of the proposed method in learning with limited labeled data.
% the proposed method's effectiveness in learning with limited labeled data.

Tables I and II shows the results of different methods in terms of loss value under different SNRs. Each element of the table is the minimal communication round needed to achieve a specific training loss; bolded and underlined values indicate the best and second-best performance, respectively. We observe that the proposed methods reach target training loss with fewer communication rounds than fully supervised methods. This is because the proposed methods have already learned a universal representation of input data during the pre-training stage, which can be fine-tuned with fewer labels to perform well on specifc tasks. Additionally, \textbf{SSL-FT (60\%)} even achieves target training loss with fewer communication rounds than the \textbf{Sup} method with full label information, which demonstrates that the proposed method learn more efficiently with less label information.



% Fig. 3(a) ∼ 3(c) show the PSNR performance versus SNR overthe AWGN channel, and Fig. 3(d) ∼ 3(f) present the Rayleighfast fading channel. For the WITT scheme, a single model cancover a range of SNR from 1dB to 13dB. For the “BPG + LDPC”scheme, according to adaptive modulation and coding (AMC) standard [21], we choose the best-performing configuration of codingrate and modulation (the green dashed lines) under each specific SNR and plot the envelope. Compared to the CNN-based deep JSCC scheme, we achieve much better performance for all SNRs. Due to the enhanced model capacity by incorporating Transformers, it can be seen that the performance gap increases with the growth of image resolution. For the CIFAR10 dataset, WITT and deep JSCC scheme significantly outperform the “BPG + LDPC” and “BPG + Capacity”. However, for high-resolution images, the performance of CNN-based deep JSCC degrades a lot and falls behind to the separation-based scheme. Our proposed maintains a considerable performance, especially in the low SNR regions.

% Fig. 4(a) ∼ 4(c) demonstrate the PSNR performance versusthe CBR over the AWGN channel, and Fig. 4(d) ∼ 4(f) show theRayleigh fast fading channel. For the CIFAR10 dataset, ourproposed model can generally outperform deep JSCC for all CBRs.Meanwhile, our model achieves considerable gains compared tothe existing classical separation-based schemes, especially on theRayleigh channel. For high-resolution image datasets, our proposedmodel outperforms the CNN-based deep JSCC scheme. Comparedto “BPG + LDPC”, our WITT model achieves comparable or betterperformance and coding gain. Moreover, WITT cannot providecomparable coding gain as that of the “BPG + Capacity” scheme,i.e., the slope of the performance curve slows down with the increase of SNR. Nevertheless, the performance of WITT approachesthe “BPG + Capacity” in the low CBR regions and obviously improves compared to CNN-based deep JSCC.



% than the full supervised methods \texttt{Sup} and \texttt{Sup-60\%}. The proposed methods achieve better performance with fewer communication rounds, which indicates that the proposed methods can learn more efficiently with less label information.

% The proposed methods achieve better performance with fewer communication rounds, which indicates that the proposed methods can learn more efficiently with less label information.


% achieve better performance than the full supervised methods \textbf{Sup} and \textbf{Sup-60\%}. The proposed methods achieve better performance with fewer communication rounds, which indicates that the proposed methods can learn more efficiently with less label information.



% We show the test accuracy and the task-specific loss versus the number of communication rounds. We observe that the proposed \textbf{SSL} and \textbf{SSL-60\%} methods achieve better performance than the full supervised methods \textbf{Sup} and \textbf{Sup-60\%}. The proposed methods achieve better performance with fewer communication rounds, which indicates that the proposed methods can learn more efficiently with less label information.
% There is no overlap between the training and testing characters. 

% We choose the CIFAR-10 dataset as the ID dataset, and the LSUN-resized dataset and Tiny ImageNet-resized dataset as the OoD dataset. For fairness, we use ResNet-18 as the backbone network in all experiments. Considering the transmit power limitation, we use the $sigmoid(\cdot)$ function as the activation function in the last layer of the JSCC encoder. Furthermore, we use the same communication setting as~\cite{shao2021learning} where the symbol rate is 9600 Baud\footnote{The code is available at {github.com/hlidmhkust/VCCIB}.}.



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\begin{table*}[h]
    \centering
    \caption{The communication rounds to achieve different training loss, AWGN Channel}
    \begin{tabular}{@{}c|cccc|cccc|cccc@{}}
    \toprule
    SNR           & \multicolumn{4}{c|}{0dB}   & \multicolumn{4}{c|}{10dB}  & \multicolumn{4}{c}{20dB}  \\ \midrule
    Training Loss & 4.00        & 3.00      & 2.00      & 1.00      & 4.00      & 3.00      & 2.00      & 1.00      & 4.00      & 3.00      & 2.00      & 1.00 \\ \midrule
    SSL-FT        & \bb{560}     & \bb{882}   & \bb{1512}  & \bb{3507}  & \bb{441}   & \bb{679}   & \bb{1099}  & \bb{2100}  & \bb{413}   &\bb{637}    & \bb{1015}  & \bb{1890} \\
    Sup           & \uu{840}     & \uu{1344}  & \uu{2408}  & 6230      & \uu{686}   & \uu{1106}  & 1897      & 4291      & \uu{588}   &\uu{973}    & \uu{1708}  & 3801 \\
    SSL-FT (60\%)  & 980         & 1547      & 2618      & \uu{5957}  & 770       & 1169      & \uu{1855}  & \uu{3514}  & 721       & 1113      & 1750      & \uu{3213} \\
    Sup (60\%)     & 1526        & 2373      & 4144      & 7000      & 1162      & 1890      & 3213      & 6762      & 1029      & 1701      & 2919      & 6055 \\ \bottomrule
    \end{tabular}
\end{table*}


\begin{table*}[h]
    \centering
    \caption{The communication rounds to achieve different training loss, Rayleigh Fading Channel}
    \begin{tabular}{@{}c|cccc|cccc|cccc@{}}
        \toprule
        SNR           & \multicolumn{4}{c|}{0dB}  & \multicolumn{4}{c|}{10dB} & \multicolumn{4}{c}{20dB}  \\ \midrule
        Training Loss & 4.00 & 3.00 & 2.00 & 1.00 & 4.00 & 3.00 & 2.00 & 1.00 & 4.00 & 3.00 & 2.00 & 1.00 \\ \midrule
        SSL-FT        & \bb{658}  & \bb{1022} & \bb{1729} & \bb{4361} & \bb{504}  & \bb{777}  & \bb{1253} & \bb{2590} & \bb{483}  & \bb{749}  & \bb{1218} & \bb{2415} \\
        Sup           & \uu{924}  & \uu{1470} & \uu{2695} & 7000 & \uu{742}  & \uu{1190} & \uu{2086} & 5068 & \uu{672}  & \uu{1120} & \uu{2002} & 4816 \\
        SSL-FT (60\%)  & 1127 & 1750 & 2961 & 7000 & 854  & 1323 & 2121 & \uu{4235} & 868  & 1330 & 2107 & \uu{4095} \\
        Sup (60\%)     & 1589 & 2527 & 4578 & 7000 & 1260 & 2023 & 3472 & 7000 & 1260 & 2016 & 3395 & 7000 \\ \bottomrule
        \end{tabular}
\end{table*}

% \begin{table*}[]
%     \centering

% \end{table*}