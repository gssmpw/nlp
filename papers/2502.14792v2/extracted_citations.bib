@INPROCEEDINGS{godard_digging_2019,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3827-3837},
  keywords={Training;Estimation;Predictive models;Cameras;Image color analysis;Image reconstruction;Image matching},
  doi={10.1109/ICCV.2019.00393}}

@article{gosala_birds-eye-view_2022,
	title = {Bird’s-{Eye}-{View} {Panoptic} {Segmentation} {Using} {Monocular} {Frontal} {View} {Images}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3142418},
	abstract = {Bird’s-Eye-View (BEV) maps have emerged as one of the most powerful representations for scene understanding due to their ability to provide rich spatial context while being easy to interpret and process. Such maps have found use in many real-world tasks that extensively rely on accurate scene segmentation as well as object instance identification in the BEV space for their operation. However, existing segmentation algorithms only predict the semantics in the BEV space, which limits their use in applications where the notion of object instances is also critical. In this work, we present the first BEV panoptic segmentation approach for directly predicting dense panoptic segmentation maps in the BEV, given a single monocular image in the frontal view (FV). Our architecture follows the top-down paradigm and incorporates a novel dense transformer module consisting of two distinct transformers that learn to independently map vertical and flat regions in the input image from the FV to the BEV. Additionally, we derive a mathematical formulation for the sensitivity of the FV-BEV transformation which allows us to intelligently weight pixels in the BEV space to account for the varying descriptiveness across the FV image. Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach exceeds the state-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gosala, Nikhil and Valada, Abhinav},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Geometry, Head, Image segmentation, Semantic scene understanding, Semantics, Task analysis, Three-dimensional displays, Transformers, deep learning for visual perception, object detection, segmentation and categorization},
	pages = {1968--1975},
}

@inproceedings{gosala_skyeye_2023,
	address = {Vancouver, BC, Canada},
	title = {{SkyEye}: {Self}-{Supervised} {Bird}'s-{Eye}-{View} {Semantic} {Mapping} {Using} {Monocular} {Frontal} {View} {Images}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {{SkyEye}},
	url = {https://ieeexplore.ieee.org/document/10205409/},
	doi = {10.1109/CVPR52729.2023.01431},
	abstract = {Bird’s-Eye-View (BEV) semantic maps have become an essential component of automated driving pipelines due to the rich representation they provide for decision-making tasks. However, existing approaches for generating these maps still follow a fully supervised training paradigm and hence rely on large amounts of annotated BEV data. In this work, we address this limitation by proposing the first selfsupervised approach for generating a BEV semantic map using a single monocular image from the frontal view (FV). During training, we overcome the need for BEV ground truth annotations by leveraging the more easily available FV semantic annotations of video sequences. Thus, we propose the SkyEye architecture that learns based on two modes of self-supervision, namely, implicit supervision and explicit supervision. Implicit supervision trains the model by enforcing spatial consistency of the scene over time based on FV semantic sequences, while explicit supervision exploits BEV pseudolabels generated from FV semantic annotations and self-supervised depth estimates. Extensive evaluations on the KITTI-360 dataset demonstrate that our self-supervised approach performs on par with the state-of-the-art fully supervised methods and achieves competitive results using only 1\% of direct supervision in BEV compared to fully supervised approaches. Finally, we publicly release both our code and the BEV datasets generated from the KITTI-360 and Waymo datasets.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gosala, Nikhil and Petek, Kürsat and Drews-Jr, Paulo L. J. and Burgard, Wolfram and Valada, Abhinav},
	month = jun,
	year = {2023},
	pages = {14901--14910},
}

@inproceedings{harley_simple-bev_2023,
	title = {Simple-{BEV}: {What} {Really} {Matters} for {Multi}-{Sensor} {BEV} {Perception}?},
	shorttitle = {Simple-{BEV}},
	url = {https://ieeexplore.ieee.org/document/10160831},
	doi = {10.1109/ICRA48891.2023.10160831},
	abstract = {Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably “lifted” from the multi-camera images onto the 2D ground plane, yielding a “bird's eye view” (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel “lifting” methods, but we observe that other details in the training setups have shifted at the same time, making it unclear what really matters in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like radar have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect-even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.},
	urldate = {2024-06-13},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Harley, Adam W. and Fang, Zhaoyuan and Li, Jie and Ambrus, Rares and Fragkiadaki, Katerina},
	month = may,
	year = {2023},
	keywords = {Buildings, Cameras, Laser radar, Protocols, Space vehicles, Three-dimensional displays, Training},
	pages = {2759--2765},
}

@incollection{li_bevformer_2022,
	address = {Cham},
	title = {{BEVFormer}: {Learning} {Bird}’s-{Eye}-{View} {Representation} from {Multi}-camera {Images} via {Spatiotemporal} {Transformers}},
	volume = {13669},
	isbn = {978-3-031-20076-2 978-3-031-20077-9},
	shorttitle = {{BEVFormer}},
	url = {https://link.springer.com/10.1007/978-3-031-20077-9_1},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Qiao, Yu and Dai, Jifeng},
	year = {2022},
	doi = {10.1007/978-3-031-20077-9_1},
	pages = {1--18},
}

@article{li_delving_2024,
	title = {Delving {Into} the {Devils} of {Bird}’s-{Eye}-{View} {Perception}: {A} {Review}, {Evaluation} and {Recipe}},
	volume = {46},
	issn = {1939-3539},
	shorttitle = {Delving {Into} the {Devils} of {Bird}’s-{Eye}-{View} {Perception}},
	url = {https://ieeexplore.ieee.org/document/10321736},
	doi = {10.1109/TPAMI.2023.3333838},
	abstract = {Learning powerful representations in bird’s-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) how to adapt and generalize algorithms as sensor configurations vary across different scenarios. In this survey, we review the most recent works on BEV perception and provide an in-depth analysis of different solutions. Moreover, several systematic designs of BEV approach from the industry are depicted as well. Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera, LiDAR and fusion inputs. At last, we point out the future research directions in this area. We hope this report will shed some light on the community and encourage more research effort on BEV perception.},
	number = {4},
	urldate = {2024-06-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Hongyang and Sima, Chonghao and Dai, Jifeng and Wang, Wenhai and Lu, Lewei and Wang, Huijie and Zeng, Jia and Li, Zhiqi and Yang, Jiazhi and Deng, Hanming and Tian, Hao and Xie, Enze and Xie, Jiangwei and Chen, Li and Li, Tianyu and Li, Yang and Gao, Yulu and Jia, Xiaosong and Liu, Si and Shi, Jianping and Lin, Dahua and Qiao, Yu},
	month = apr,
	year = {2024},
	keywords = {3D detection and segmentation, Autonomous vehicles, Cameras, Laser radar, Pipelines, Surveys, Task analysis, Three-dimensional displays, autonomous driving challenge, bird ’s-eye-view (BEV) perception},
	pages = {2151--2170},
}

@article{lu_monocular_2019,
	title = {Monocular {Semantic} {Occupancy} {Grid} {Mapping} {With} {Convolutional} {Variational} {Encoder}–{Decoder} {Networks}},
	volume = {4},
	issn = {2377-3766},
	doi = {10.1109/LRA.2019.2891028},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lu, Chenyang and van de Molengraft, Marinus Jacobus Gerardus and Dubbelman, Gijs},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Feature extraction, Image segmentation, Measurement, Neural networks, Semantic scene understanding, Semantics, Training, baseline, computer vision for transportation, object detection, segmentation and categorization},
	pages = {445--452},
}

@article{mallot_inverse_1991,
	title = {Inverse perspective mapping simplifies optical flow computation and obstacle detection},
	volume = {64},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00201978},
	doi = {10.1007/BF00201978},
	abstract = {We present a scheme for obstacle detection from optical flow which is based on strategies of biological information processing. Optical flow is established by a local “voting” (non-maximum suppression) over the outputs of correlation-type motion detectors similar to those found in the fly visual system. The computational theory of obstacle detection is discussed in terms of space-variances of the motion field. An efficient mechanism for the detection of disturbances in the expected motion field is based on “inverse perspective mapping”, i.e., a coordinate transform or retinotopic mapping applied to the image. It turns out that besides obstacle detection, inverse perspective mapping has additional advantages for regularizing optical flow algorithms. Psychophysical evidence for body-scaled obstacle detection and related neurophysiological results are discussed.},
	language = {en},
	number = {3},
	urldate = {2022-11-17},
	journal = {Biological Cybernetics},
	author = {Mallot, Hanspeter A. and Bülthoff, H. H. and Little, J. J. and Bohrer, S.},
	month = jan,
	year = {1991},
	keywords = {Biological Information, Efficient Mechanism, Motion Detector, Optical Flow, Visual System},
	pages = {177--185},
}

@inproceedings{mildenhall_nerf_2020,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}

@article{pan_cross-view_2020,
	title = {Cross-view {Semantic} {Segmentation} for {Sensing} {Surroundings}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1906.03560},
	doi = {10.1109/LRA.2020.3004325},
	number = {3},
	urldate = {2023-06-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
	month = jul,
	year = {2020},
	note = {arXiv:1906.03560 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {4867--4873},
}

@inproceedings{philion_lift_2020,
	address = {Cham},
	title = {Lift, {Splat}, {Shoot}: {Encoding} {Images} from {Arbitrary} {Camera} {Rigs} by {Implicitly} {Unprojecting} to {3D}},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	shorttitle = {Lift, {Splat}, {Shoot}},
	url = {https://link.springer.com/10.1007/978-3-030-58568-6_12},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {{ECCV} 2020},
	author = {Philion, Jonah and Fidler, Sanja},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_12},
}

@inproceedings{reiher_sim2real_2020,
	title = {A {Sim2Real} {Deep} {Learning} {Approach} for the {Transformation} of {Images} from {Multiple} {Vehicle}-{Mounted} {Cameras} to a {Semantically} {Segmented} {Image} in {Bird}’s {Eye} {View}},
	doi = {10.1109/ITSC45102.2020.9294462},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Reiher, Lennart and Lampe, Bastian and Eckstein, Lutz},
	month = sep,
	year = {2020},
	keywords = {Cameras, Image segmentation, Neural networks, Roads, Semantics, Task analysis, Vehicle dynamics},
	pages = {1--7},
}

@misc{sirko-galouchenko_occfeat_2024,
	title = {{OccFeat}: {Self}-supervised {Occupancy} {Feature} {Prediction} for {Pretraining} {BEV} {Segmentation} {Networks}},
	shorttitle = {{OccFeat}},
	url = {http://arxiv.org/abs/2404.14027},
	abstract = {We introduce a self-supervised pretraining method, called OccFeat, for camera-only Bird’s-Eye-View (BEV) segmentation networks. With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks. Occupancy prediction provides a 3D geometric understanding of the scene to the model. However, the geometry learned is class-agnostic. Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model. Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in lowdata scenarios. Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Sirko-Galouchenko, Sophia and Boulch, Alexandre and Gidaris, Spyros and Bursuc, Andrei and Vobecky, Antonin and Pérez, Patrick and Marlet, Renaud},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14027 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

