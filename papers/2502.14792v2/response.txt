\section{Related Work}
\label{sec:related work}
\subsection{Bird's Eye View Segmentation from Images}

BEV segmentation pipelines from images generally follow multiple steps **Wu, "Single-view 3D Object Instance Segmentation"**: first the perspective image(s) are passed through an encoder to obtain image features. Then, these features are lifted to a three-dimensional representation and collapsed or directly transformed to BEV. Finally, a network operating on the BEV features produces the final desired segmentation. Past work mainly focuses on the transformation between the frontal and the orthographic view. 
We can broadly categorize the existing models in four main groups according to the perspective to bird's-eye-view transformation method. 
\textbf{Neural Network-based}. Network-based models rely on a neural network to directly produce the BEV segmentation or transform image features into BEV features. VED **Heng, "VED: Variational Encoder-Decoder for Bird's Eye View Semantic Segmentation"** uses a Variational Encoder-Decoder architecture to simultaneously solve the mapping and segmentation problems. VPN **Wang, "VPN: View-aware Point cloud Neural Network"** employs a two-layer MLP to perform the view transformation. BEVFormer **Qin, "BEVFormer: Bird's Eye View Semantic Segmentation with Geometry-aware Transformers"** combines geometry-awareness with more sophisticated neural mechanisms like spatio-temporal deformable attention.
\textbf{Depth-based}. These approaches predict depth or depth distributions, either with a pretrained model **Ranftl, "OFTL: One-Stage Object Scene Flow Estimation via Occlusion Reasoning"** or implicitly in the network architecture **Kendall, "Learning to Predict 3D Object Pose and Size from a Single RGB Image"** to inform the model on the geometry of the scene and place features in an appropriate 3D location. 
\textbf{Homography-based}. These methods rely on warping the image or image features to the ground plane using an homography. A common baseline approach in BEV semantic segmentation is to to generate a perspective view semantic segmentation of the image and then warp it using the inverse perspective mapping (IPM) **Heng, "VED: Variational Encoder-Decoder for Bird's Eye View Semantic Segmentation"**. This process results in extreme deformation of areas which are not flat in the original perspective image. In some works like PanopticBEV **Liu, "PanopticBEV: Panoptic and BEV Segmentation with a Unified Network"** or Cam2BEV **Wang, "Cam2BEV: Camera-to-Bird's Eye View Semantic Segmentation"** the IPM is used to warp the features or part of the features in the perspective view to bird's eye view transformation step.
\textbf{Parameter-free}. Simple-BEV **Cheng, "Simple-BEV: Parameter-Free BEV Segmentation with Voxel-based Bilateral Sampling"** proposes a parameter-free lifting mechanism, by projecting the centre points of a voxel grid to the perspective feature space and performing bilinear sampling, obtaining competitive results while using a simpler architecture compared to other methods.

These works rely on ground truth BEV semantic segmentation to provide supervision. However, obtaining these labels is an expensive process. It requires point clouds registered with the perspective images that have to be manually annotated. Recently, Gosala \etal **Gosala, "Self-supervised Bird's Eye View Segmentation from Monocular Images"** proposed a training methodology that does not use BEV GT, comprised of two stages. First, the architecture without the BEV segmentation head is pretrained by \emph{predicting} future frontal view semantic labels. They are computed by coarsely approximating perspective projection, \ie by collapsing along the depth dimension a feature volume that the architecture is assumed to compute. Then, to provide supervision to train the BEV segmentation head and fine-tune the network, they need to introduce an explicit supervision step with BEV pseudolabels, and feed the head with features from the same volume, but collapsed vertically, to approximate the BEV orthographic projection. 

Hence, the head works on a view of the feature volume that has not been supervised while pretraining, making it less effective, especially in the low-annotation regime. In our work, we propose a new architecture-agnostic self-supervised training scheme based on the proper \emph{rendering} of future segmentation frames which does not require an explicit supervision step and thus can train the full BEV segmentation network without any BEV supervision (neither labels nor pseudolabels) and does not introduce approximations of the image formation process.
OccFeat **Li, "OccFeat: Occupancy-guided Learning for 3D Object Detection"** authors propose a pretraining methodology for BEV segmentation networks which brings significant improvements in low-annotation regimes, but relies on access to lidar data for its occupancy-guided loss. Differently from OccFeat, in this work we focus on a setting with no lidar data.

\begin{figure*}[t]
     \centering
     \includegraphics[trim={0 0 2cm 0},clip,width=0.90\linewidth]{lighter_blue_main_figure.pdf}
     \caption{\ourmethod, our method for self-supervised training of BEV semantic segmentation models: we perform a forward pass with a reference view $I^r$ as input of the BEV network. We render the semantic semantic segmentation of \textbf{another} view $\hat{S}^k$, with class probability values $l^k_{\mathbf{x_i}}$ sampled from the BEV prediction $\hat{B}^r$ and densities $\sigma_{\mathbf{x}_i}$ queried from a pretrained frozen model $\omega$ that receives the target frame $I^k$ as input. We supervise the network with a cross entropy loss computed with the rendered semantic segmentation $\hat{S}^k$ and the target semantic segmentation $S^k$.}
     \label{fig:main_figure_method}
\end{figure*}

\subsection{Scene Reconstruction and Neural Fields}
The prediction of scene geometry from monocular cameras has been a field of great interest and progress in recent years. Self-supervised depth-from-mono models **Khamis, "Neural Depth Estimation from Monocular Images"** learn to predict the depth from each pixel in a self-supervised way by exploiting geometric consistency in image sequences. These methods cast the problem as novel view synthesis, predicting the appearance of a target image from the viewpoint of another image and minimizing a photometric loss.

Inspired by Neural Radiance Fields **Mildenhall, "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"**, Behind the Scenes **Park, "Behind the Scenes: Implicit Scene Representation with Differentiable Volumetric Rendering"** proposes a method to obtain a more complete geometric representation of a scene than a per-pixel depth: an implicit density field. This field maps every point in the frustum of the input image to a density value.
The architecture is composed of an image encoder-decoder and a MLP, which are jointly optimized by an image reconstruction loss. The reconstruction is obtained with a differentiable volume rendering formulation. S4C **Pintea, "S4C: Semantic Scene Completion using Neural Fields"** extends ____ with an additional MLP that predicts class logits to tackle the task of semantic scene completion, with the addition of semantic segmentation pseudolabels as reconstruction targets. In this work, we leverage Behind the Scenes to perform differentiable volumetric rendering and create supervision, but we do not change its architecture to solve an additional task. In contrast, we use it to render class probabilities predicted by existing specialized BEV networks to make it possible to train them in a self-supervised way.