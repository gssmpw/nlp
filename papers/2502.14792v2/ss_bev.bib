@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@InProceedings{waymo_2020_CVPR, author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir}, title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2020} }

@article{10.1145/964965.808594,
author = {Kajiya, James T. and Von Herzen, Brian P},
title = {Ray tracing volume densities},
year = {1984},
issue_date = {July 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {0097-8930},
url = {https://doi.org/10.1145/964965.808594},
doi = {10.1145/964965.808594},
abstract = {This paper presents new algorithms to trace objects represented by densities within a volume grid, e.g. clouds, fog, flames, dust, particle systems. We develop the light scattering equations, discuss previous methods of solution, and present a new approximate solution to the full three-dimensional radiative scattering problem suitable for use in computer graphics. Additionally we review dynamical models for clouds used to make an animated movie.},
journal = {SIGGRAPH Comput. Graph.},
month = {jan},
pages = {165–174},
numpages = {10},
keywords = {Stochastic modeling, Simulation of natural phenomena, Ray tracing, Raster graphics, Radiative transport, Particle systems, Light scattering, Computer graphics, Clouds}
}

@inproceedings{volumetric_rendering_Kajiya_1984,
author = {Kajiya, James T. and Von Herzen, Brian P},
title = {Ray tracing volume densities},
year = {1984},
isbn = {0897911385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800031.808594},
doi = {10.1145/800031.808594},
abstract = {This paper presents new algorithms to trace objects represented by densities within a volume grid, e.g. clouds, fog, flames, dust, particle systems. We develop the light scattering equations, discuss previous methods of solution, and present a new approximate solution to the full three-dimensional radiative scattering problem suitable for use in computer graphics. Additionally we review dynamical models for clouds used to make an animated movie.},
booktitle = {Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {165–174},
numpages = {10},
keywords = {Stochastic modeling, Simulation of natural phenomena, Ray tracing, Raster graphics, Radiative transport, Particle systems, Light scattering, Computer graphics, Clouds},
series = {SIGGRAPH '84}
}



@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@INPROCEEDINGS{wang_towards_2023,
  author={Wang, Shuo and Zhao, Xinhai and Xu, Hai–Ming and Chen, Zehui and Yu, Dameng and Chang, Jiahao and Yang, Zhen and Zhao, Feng},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View}, 
  year={2023},
  volume={},
  number={},
  pages={13333-13342},
  keywords={Training;Industries;Representation learning;Three-dimensional displays;Estimation;Object detection;Cameras;3D from multi-view and sensors},
  doi={10.1109/CVPR52729.2023.01281}}


@inproceedings{cordts_cityscapes_2016,
title={The Cityscapes Dataset for Semantic Urban Scene Understanding},
author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2016}
}


@inproceedings{cheng2020panoptic,
  title={Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation},
  author={Cheng, Bowen and Collins, Maxwell D and Zhu, Yukun and Liu, Ting and Huang, Thomas S and Adam, Hartwig and Chen, Liang-Chieh},
  booktitle={CVPR},
  year={2020}
}
@INPROCEEDINGS {caesar_nuscenes_2020,
author = { Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar },
booktitle = { 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ nuScenes: A Multimodal Dataset for Autonomous Driving }},
year = {2020},
volume = {},
ISSN = {},
pages = {11618-11628},
abstract = { Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online. },
keywords = {Sensors;Laser radar;Three-dimensional displays;Cameras;Radar tracking;Autonomous vehicles},
doi = {10.1109/CVPR42600.2020.01164},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01164},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


@InProceedings{Sun_2020_CVPR, author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir}, title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2020} }


@inproceedings{hu_fiery_2021,
  title     = {{FIERY}: Future Instance Segmentation in Bird's-Eye view from Surround Monocular Cameras},
  author    = {Anthony Hu and Zak Murez and Nikhil Mohan and Sofía Dudas and 
               Jeffrey Hawke and Vijay Badrinarayanan and Roberto Cipolla and Alex Kendall},
  booktitle = {Proceedings of the International Conference on Computer Vision ({ICCV})},
  year = {2021}
}

@article{roddick_orthographic_2018,  
  title={Orthographic feature transform for monocular 3d object detection},  
  author={Roddick, Thomas and Kendall, Alex and Cipolla, Roberto},  
  journal={British Machine Vision Conference},  
  year={2019}  
}

@inproceedings{reiher_sim2real_2020,
	title = {A {Sim2Real} {Deep} {Learning} {Approach} for the {Transformation} of {Images} from {Multiple} {Vehicle}-{Mounted} {Cameras} to a {Semantically} {Segmented} {Image} in {Bird}’s {Eye} {View}},
	doi = {10.1109/ITSC45102.2020.9294462},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Reiher, Lennart and Lampe, Bastian and Eckstein, Lutz},
	month = sep,
	year = {2020},
	keywords = {Cameras, Image segmentation, Neural networks, Roads, Semantics, Task analysis, Vehicle dynamics},
	pages = {1--7},
}

@article{pan_cross-view_2020,
	title = {Cross-view {Semantic} {Segmentation} for {Sensing} {Surroundings}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1906.03560},
	doi = {10.1109/LRA.2020.3004325},
	number = {3},
	urldate = {2023-06-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
	month = jul,
	year = {2020},
	note = {arXiv:1906.03560 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {4867--4873},
}

@inproceedings{saha_translating_2022,
	title = {Translating {Images} into {Maps}},
	doi = {10.1109/ICRA46639.2022.9811901},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Saha, Avishkar and Mendez, Oscar and Russell, Chris and Bowden, Richard},
	month = may,
	year = {2022},
	keywords = {Automation, Cameras, Grounding, Training, Transformers, Video sequences},
	pages = {9200--9206},
}

@INPROCEEDINGS{roddick_predicting_2020,
  author={Roddick, Thomas and Cipolla, Roberto},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks}, 
  year={2020},
  volume={},
  number={},
  pages={11135-11144},
  keywords={Semantics;Feature extraction;Roads;Three-dimensional displays;Automobiles;Task analysis;Image segmentation},
  doi={10.1109/CVPR42600.2020.01115}}

@article{lu_monocular_2019,
	title = {Monocular {Semantic} {Occupancy} {Grid} {Mapping} {With} {Convolutional} {Variational} {Encoder}–{Decoder} {Networks}},
	volume = {4},
	issn = {2377-3766},
	doi = {10.1109/LRA.2019.2891028},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Lu, Chenyang and van de Molengraft, Marinus Jacobus Gerardus and Dubbelman, Gijs},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Feature extraction, Image segmentation, Measurement, Neural networks, Semantic scene understanding, Semantics, Training, baseline, computer vision for transportation, object detection, segmentation and categorization},
	pages = {445--452},
}


@InProceedings{siddiqui_panoptic_2022,
    author    = {Siddiqui, Yawar and Porzi, Lorenzo and Bul\`o, Samuel Rota and M\"uller, Norman and Nie{\ss}ner, Matthias and Dai, Angela and Kontschieder, Peter},
    title     = {Panoptic Lifting for 3D Scene Understanding With Neural Fields},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {9043-9052}
}


@article{liao_kitti-360_2023,
	title = {{KITTI}-360: {A} {Novel} {Dataset} and {Benchmarks} for {Urban} {Scene} {Understanding} in {2D} and {3D}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {{KITTI}-360},
	doi = {10.1109/TPAMI.2022.3179507},
	abstract = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over 150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liao, Yiyi and Xie, Jun and Geiger, Andreas},
	month = mar,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Annotations, Benchmark testing, Cameras, Computer vision, Point cloud labeling, Semantics, Task analysis, Three-dimensional displays, datasets, performance evaluation, scene understanding, self-driving, semantic label transfer},
	pages = {3292--3310},
}

@inproceedings{gosala_skyeye_2023,
	address = {Vancouver, BC, Canada},
	title = {{SkyEye}: {Self}-{Supervised} {Bird}'s-{Eye}-{View} {Semantic} {Mapping} {Using} {Monocular} {Frontal} {View} {Images}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	shorttitle = {{SkyEye}},
	url = {https://ieeexplore.ieee.org/document/10205409/},
	doi = {10.1109/CVPR52729.2023.01431},
	abstract = {Bird’s-Eye-View (BEV) semantic maps have become an essential component of automated driving pipelines due to the rich representation they provide for decision-making tasks. However, existing approaches for generating these maps still follow a fully supervised training paradigm and hence rely on large amounts of annotated BEV data. In this work, we address this limitation by proposing the first selfsupervised approach for generating a BEV semantic map using a single monocular image from the frontal view (FV). During training, we overcome the need for BEV ground truth annotations by leveraging the more easily available FV semantic annotations of video sequences. Thus, we propose the SkyEye architecture that learns based on two modes of self-supervision, namely, implicit supervision and explicit supervision. Implicit supervision trains the model by enforcing spatial consistency of the scene over time based on FV semantic sequences, while explicit supervision exploits BEV pseudolabels generated from FV semantic annotations and self-supervised depth estimates. Extensive evaluations on the KITTI-360 dataset demonstrate that our self-supervised approach performs on par with the state-of-the-art fully supervised methods and achieves competitive results using only 1\% of direct supervision in BEV compared to fully supervised approaches. Finally, we publicly release both our code and the BEV datasets generated from the KITTI-360 and Waymo datasets.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gosala, Nikhil and Petek, Kürsat and Drews-Jr, Paulo L. J. and Burgard, Wolfram and Valada, Abhinav},
	month = jun,
	year = {2023},
	pages = {14901--14910},
}

@inproceedings{philion_lift_2020,
	address = {Cham},
	title = {Lift, {Splat}, {Shoot}: {Encoding} {Images} from {Arbitrary} {Camera} {Rigs} by {Implicitly} {Unprojecting} to {3D}},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	shorttitle = {Lift, {Splat}, {Shoot}},
	url = {https://link.springer.com/10.1007/978-3-030-58568-6_12},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {{ECCV} 2020},
	author = {Philion, Jonah and Fidler, Sanja},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_12},
}

@incollection{li_bevformer_2022,
	address = {Cham},
	title = {{BEVFormer}: {Learning} {Bird}’s-{Eye}-{View} {Representation} from {Multi}-camera {Images} via {Spatiotemporal} {Transformers}},
	volume = {13669},
	isbn = {978-3-031-20076-2 978-3-031-20077-9},
	shorttitle = {{BEVFormer}},
	url = {https://link.springer.com/10.1007/978-3-031-20077-9_1},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Qiao, Yu and Dai, Jifeng},
	year = {2022},
	doi = {10.1007/978-3-031-20077-9_1},
	pages = {1--18},
}

@inproceedings{harley_simple-bev_2023,
	title = {Simple-{BEV}: {What} {Really} {Matters} for {Multi}-{Sensor} {BEV} {Perception}?},
	shorttitle = {Simple-{BEV}},
	url = {https://ieeexplore.ieee.org/document/10160831},
	doi = {10.1109/ICRA48891.2023.10160831},
	abstract = {Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably “lifted” from the multi-camera images onto the 2D ground plane, yielding a “bird's eye view” (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel “lifting” methods, but we observe that other details in the training setups have shifted at the same time, making it unclear what really matters in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like radar have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect-even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.},
	urldate = {2024-06-13},
	booktitle = {2023 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Harley, Adam W. and Fang, Zhaoyuan and Li, Jie and Ambrus, Rares and Fragkiadaki, Katerina},
	month = may,
	year = {2023},
	keywords = {Buildings, Cameras, Laser radar, Protocols, Space vehicles, Three-dimensional displays, Training},
	pages = {2759--2765},
}


@INPROCEEDINGS {reading_categorical_2021,
author = {C. Reading and A. Harakeh and J. Chae and S. L. Waslander},
booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Categorical Depth Distribution Network for Monocular 3D Object Detection},
year = {2021},
volume = {},
issn = {},
pages = {8551-8560},
abstract = {Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird’s-eye-view projection and single-stage detector to produce the final output detections. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available.},
keywords = {computer vision;three-dimensional displays;codes;estimation;object detection;distribution networks;detectors},
doi = {10.1109/CVPR46437.2021.00845},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00845},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@article{mallot_inverse_1991,
	title = {Inverse perspective mapping simplifies optical flow computation and obstacle detection},
	volume = {64},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/BF00201978},
	doi = {10.1007/BF00201978},
	abstract = {We present a scheme for obstacle detection from optical flow which is based on strategies of biological information processing. Optical flow is established by a local “voting” (non-maximum suppression) over the outputs of correlation-type motion detectors similar to those found in the fly visual system. The computational theory of obstacle detection is discussed in terms of space-variances of the motion field. An efficient mechanism for the detection of disturbances in the expected motion field is based on “inverse perspective mapping”, i.e., a coordinate transform or retinotopic mapping applied to the image. It turns out that besides obstacle detection, inverse perspective mapping has additional advantages for regularizing optical flow algorithms. Psychophysical evidence for body-scaled obstacle detection and related neurophysiological results are discussed.},
	language = {en},
	number = {3},
	urldate = {2022-11-17},
	journal = {Biological Cybernetics},
	author = {Mallot, Hanspeter A. and Bülthoff, H. H. and Little, J. J. and Bohrer, S.},
	month = jan,
	year = {1991},
	keywords = {Biological Information, Efficient Mechanism, Motion Detector, Optical Flow, Visual System},
	pages = {177--185},
}

@article{li_delving_2024,
	title = {Delving {Into} the {Devils} of {Bird}’s-{Eye}-{View} {Perception}: {A} {Review}, {Evaluation} and {Recipe}},
	volume = {46},
	issn = {1939-3539},
	shorttitle = {Delving {Into} the {Devils} of {Bird}’s-{Eye}-{View} {Perception}},
	url = {https://ieeexplore.ieee.org/document/10321736},
	doi = {10.1109/TPAMI.2023.3333838},
	abstract = {Learning powerful representations in bird’s-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) how to adapt and generalize algorithms as sensor configurations vary across different scenarios. In this survey, we review the most recent works on BEV perception and provide an in-depth analysis of different solutions. Moreover, several systematic designs of BEV approach from the industry are depicted as well. Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera, LiDAR and fusion inputs. At last, we point out the future research directions in this area. We hope this report will shed some light on the community and encourage more research effort on BEV perception.},
	number = {4},
	urldate = {2024-06-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Li, Hongyang and Sima, Chonghao and Dai, Jifeng and Wang, Wenhai and Lu, Lewei and Wang, Huijie and Zeng, Jia and Li, Zhiqi and Yang, Jiazhi and Deng, Hanming and Tian, Hao and Xie, Enze and Xie, Jiangwei and Chen, Li and Li, Tianyu and Li, Yang and Gao, Yulu and Jia, Xiaosong and Liu, Si and Shi, Jianping and Lin, Dahua and Qiao, Yu},
	month = apr,
	year = {2024},
	keywords = {3D detection and segmentation, Autonomous vehicles, Cameras, Laser radar, Pipelines, Surveys, Task analysis, Three-dimensional displays, autonomous driving challenge, bird ’s-eye-view (BEV) perception},
	pages = {2151--2170},
}

@INPROCEEDINGS{godard_digging_2019,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3827-3837},
  keywords={Training;Estimation;Predictive models;Cameras;Image color analysis;Image reconstruction;Image matching},
  doi={10.1109/ICCV.2019.00393}}


@inproceedings{mildenhall_nerf_2020,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}


@INPROCEEDINGS {wimbauer_behind_2023,
author = { Wimbauer, Felix and Yang, Nan and Rupprecht, Christian and Cremers, Daniel },
booktitle = { 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ Behind the Scenes: Density Fields for Single View Reconstruction }},
year = {2023},
volume = {},
ISSN = {},
pages = {9076-9086},
abstract = { Inferring a meaningful geometric scene representation from a single image is a fundamental problem in computer vision. Approaches based on traditional depth map prediction can only reason about areas that are visible in the image. Currently, neural radiance fields (NeRFs) can capture true 3D including color, but are too complex to be generated from a single image. As an alternative, we propose to predict an implicit density field from a single image. It maps every location in the frustum of the image to volumetric density. By directly sampling color from the available views instead of storing color in the density field, our scene representation becomes significantly less complex compared to NeRFs, and a neural network can predict it in a single forward pass. The network is trained through self-supervision from only video data. Our formulation allows volume rendering to perform both depth prediction and novel view synthesis. Through experiments, we show that our method is able to predict meaningful geometry for regions that are occluded in the input image. Additionally, we demonstrate the potential of our approach on three datasets for depth prediction and novel-view synthesis. },
keywords = {Geometry;Computer vision;Three-dimensional displays;Image color analysis;Neural networks;Predictive models;Rendering (computer graphics)},
doi = {10.1109/CVPR52729.2023.00876},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00876},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


@article{gosala_birds-eye-view_2022,
	title = {Bird’s-{Eye}-{View} {Panoptic} {Segmentation} {Using} {Monocular} {Frontal} {View} {Images}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3142418},
	abstract = {Bird’s-Eye-View (BEV) maps have emerged as one of the most powerful representations for scene understanding due to their ability to provide rich spatial context while being easy to interpret and process. Such maps have found use in many real-world tasks that extensively rely on accurate scene segmentation as well as object instance identification in the BEV space for their operation. However, existing segmentation algorithms only predict the semantics in the BEV space, which limits their use in applications where the notion of object instances is also critical. In this work, we present the first BEV panoptic segmentation approach for directly predicting dense panoptic segmentation maps in the BEV, given a single monocular image in the frontal view (FV). Our architecture follows the top-down paradigm and incorporates a novel dense transformer module consisting of two distinct transformers that learn to independently map vertical and flat regions in the input image from the FV to the BEV. Additionally, we derive a mathematical formulation for the sensitivity of the FV-BEV transformation which allows us to intelligently weight pixels in the BEV space to account for the varying descriptiveness across the FV image. Extensive evaluations on the KITTI-360 and nuScenes datasets demonstrate that our approach exceeds the state-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gosala, Nikhil and Valada, Abhinav},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Geometry, Head, Image segmentation, Semantic scene understanding, Semantics, Task analysis, Three-dimensional displays, Transformers, deep learning for visual perception, object detection, segmentation and categorization},
	pages = {1968--1975},
}


@misc{sirko-galouchenko_occfeat_2024,
	title = {{OccFeat}: {Self}-supervised {Occupancy} {Feature} {Prediction} for {Pretraining} {BEV} {Segmentation} {Networks}},
	shorttitle = {{OccFeat}},
	url = {http://arxiv.org/abs/2404.14027},
	abstract = {We introduce a self-supervised pretraining method, called OccFeat, for camera-only Bird’s-Eye-View (BEV) segmentation networks. With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks. Occupancy prediction provides a 3D geometric understanding of the scene to the model. However, the geometry learned is class-agnostic. Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model. Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in lowdata scenarios. Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Sirko-Galouchenko, Sophia and Boulch, Alexandre and Gidaris, Spyros and Bursuc, Andrei and Vobecky, Antonin and Pérez, Patrick and Marlet, Renaud},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14027 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@INPROCEEDINGS {hayler_s4c_2023,
author = { Hayler, Adrian and Wimbauer, Felix and Muhle, Dominik and Rupprecht, Christian and Cremers, Daniel },
booktitle = { 2024 International Conference on 3D Vision (3DV) },
title = {{ S4C: Self-Supervised Semantic Scene Completion With Neural Fields }},
year = {2024},
volume = {},
ISSN = {},
pages = {409-420},
abstract = { 3D semantic scene understanding is a fundamental challenge in computer vision. It enables mobile agents to autonomously plan and navigate arbitrary environments. SSC formalizes this challenge as jointly estimating dense geometry and semantic information from sparse observations of a scene. Current methods for SSC are generally trained on 3D ground truth based on aggregated LiDAR scans. This process relies on special sensors and annotation by hand which are costly and do not scale well. To overcome this issue, our work presents the first self-supervised approach to SSC called S4C that does not rely on 3D ground truth data. Our proposed method can reconstruct a scene from a single image and only relies on videos and pseudo segmentation ground truth generated from off-the-shelf image segmentation network during training. Unlike existing methods, which use discrete voxel grids, we represent scenes as implicit semantic fields. This formulation allows querying any point within the camera frustum for occupancy and semantic class. Our architecture is trained through rendering-based self-supervised losses. Nonetheless, our method achieves performance close to fully supervised state-of-the-art methods. Additionally, our method demonstrates strong generalization capabilities and can synthesize accurate segmentation maps for far away viewpoints. },
keywords = {Training;Image segmentation;Three-dimensional displays;Navigation;Semantics;Rendering (computer graphics);Sensors},
doi = {10.1109/3DV62453.2024.00133},
url = {https://doi.ieeecomputersociety.org/10.1109/3DV62453.2024.00133},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}
