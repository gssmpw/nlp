\section{Experimental Results}
\label{sec:experimental_results}
Often a motion capture systems are chosen because of their low noise levels and low latency which is ideally suited for robot control. Therefore, we demonstrate the performance of the developed system, by flying the small drone depicted in Fig.~\ref{fig:overview} in closed-loop with the event-camera motion capture being the only source of state estimation. We use a Prophesee Gen 3.1 event-camera ($\unit[640\times 480]{px}$, \unit[3/4]{inch} sensor) with either a \unit[25]{mm} or a \unit[50]{mm} lens resulting in a horizontal FoV (field of view) of \unit[22]{deg} and \unit[11]{deg}, respectively.

\subsection{Pose Estimation Noise}
In a first experiment the drone is rigidly placed at various distances in front of the camera. Then, 10 seconds of data are recorded with the event-camera and we calculate the standard deviation of the pose estimate. Since the drone is static, all deviations from the mean are only due to noise. The results of this analysis summarized in Fig.~\ref{fig:pose_noise}. A few interesting observations can be made which are subsequently discussed in detail:
\begin{enumerate}
    \item The noise levels along the $z_\cfr$ axis are much larger compared to the $x_\cfr$ and $y_\cfr$ axis.
    \item The SqPnP~\cite{terzakis2020sqpnp} algorithm achieves a much better performance than EPnP~\cite{lepetit2009epnp}.
    \item For SqPnP the noise levels in position scale quadratically with the distance from the camera, while the orientation noise scales linearly.
\end{enumerate}

\begin{figure}
    \centering
    \input{media/Plots_NoiseAnalysis}
    \caption{The object is placed at distances between \unit[70]{cm} and \unit[5]{m} statically in front of the camera (with the \unit[25]{mm} lens). The plots show the standard deviation in the position measurement ($z_\cfr$ and $x_\cfr$, $y_\cfr$) as well as the orientation measurements. We can clearly see that SqPnP~\cite{terzakis2020sqpnp} outperforms EPnP~\cite{lepetit2009epnp} by a large margin.}
    \label{fig:pose_noise}
\end{figure}

The $z_\cfr$-axis is the optical axis of the camera and hence the $z_\cfr$ coordinate can only be inferred from the scale of the object. Assuming that elongation of the object along the optical axis is small compared to the distance to the camera (i.e. the object is nearly flat), a well-known result from stereo-vision applies~\cite{zisserman2004multipleview}: for a given inter-marker distance $d$, focal length $f$ and a marker detection with uncertainty $\sigma_u$ (in pixels) the depth uncertainty $\sigma_{pz}$ scales with the square of the distance $z$ as
\begin{equation}
    \sigma_{pz} = \frac{\partial z_\cfr}{\partial u} \cdot \sigma_u = \frac{b \cdot f}{z_\cfr^2} \cdot \sigma_u \sim \frac{1}{z^2}\;.
\end{equation}
For the positional errors in $x_\cfr$ and $y_\cfr$ we observe a similar quadratic dependency on the camera-object distance, however with much less noise. Intuitively, this makes sense as the translation along $x_\cfr$ and $y_\cfr$ are directly observable from each marker and thus the estimate is much more accurate.

The position noise plots also highlight the superior performance of SqPnP for this task: the optimization-based approach is able to estimate the position with much less variance given the same input data. This discrepancy becomes even larger when considering the orientation estimation shown at the bottom plot of Fig.~\ref{fig:pose_noise}. SqPnP dramatically outperforms EPnP which performs between two and four times worse. Interestingly, we observe that EPnP shows a large but nearly constant orientation uncertainty after \unit[2]{m}, whereas SqPnP shows a linear increase in the noise standard deviation. Note that we do not compare against EPnP with nonlinear refinement as the OpenCV implementation requires at least six points for iterative refinement.

\subsection{Closed-Loop Deployment}

\begin{figure}[t]
    \centering
    \input{media/Plots_ClosedLoopFlight}
    \vspace*{-18pt}
    \caption{Closed-loop experiments: the drone flies a rectangular pattern starting at (2,0.1) and then lands at a distance of \unit[2.5]{m}. The event-camera is located at the origin of the coordinate system at $(x_\wfr, y_\wfr) = (0,0)$ and the optical axis of the \unit[50]{mm} lens is aligned with the $x_\wfr$ direction. The bottom plot shows the roll and pitch angle measurements during the flight.}
    \label{fig:closed_loop}
\end{figure}

In this experiment we fly the small drone shown in Fig.~\ref{fig:overview} in closed-loop and the event-camera motion capture runs at \unit[400]{Hz} leading to a system latency of \unit[2.5]{ms}. The monocular event-camera motion capture system is the only source of state-estimation for the MPC controller~\cite{foehn2022agilicious} of the quadrotor. The drone is tasked to fly a rectangular pattern, hover and then land. The trajectory is shown in the top plot Fig.~\ref{fig:closed_loop} and the roll and pitch angle measurements are shown at the bottom. When the drone is further away at {$x_\wfr=\unit[3]{m}$} the position and orientation estimate become more noisy but overall we find that the system is able to safely fly the small drone, thereby demonstrating the performance of the developed system.

