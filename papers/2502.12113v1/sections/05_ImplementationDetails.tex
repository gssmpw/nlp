\section{Implementation Details}
\label{sec:implementation_details}
To ensure real-time operation of the event-camera motion capture system the delay in the processing pipeline must be kept to a minimum. This is achieved through an efficient, multi-threaded implementation in combination with filtering data early on in the pipeline. This section gives an overview of the most important concepts, specifically the data representation, the filtering and the multi-threading.

\subsection{Event Data Representation}
\label{subsec:event_data_representation}
A single event as supplied by the camera is given as a four-tuple, consisting of an $x$ and a $y$ coordinate (both \texttt{uint16\_t}), a polarity $p$ which is either -1 or 1 (\texttt{int8\_t}), and a timestamp $t$ (\texttt{uint64\_t}). Additionally, 3 bytes of padding are included for 16-byte alignment. 
The event camera supplies a stream of such raw events. For the following discussion of different data representations for blinking LED detection, an event stream containing $k$ events over a time period $T$ coming from an event camera with image width $W$, height $H$ and $N = W \times H$ pixels is considered.

\subsubsection*{1D Representations}
The \emph{event stream} is the most basic and raw representation of events and has recently gained some attention in combination with spiking neural networks~\cite{gehrig2020eventbasedangular}. For LED detection with classical CPU architectures however this representation is completely unsuitable. To extract any spatial information, the entire event stream must be searched for pixels with matching coordinates. Furthermore, having a memory layout where each event is stored serially is not efficiently using the cache: if we search for a given x coordinate, the remaining 14 bytes of the raw event representation are unused and just occupy cache space.

\subsubsection*{2D Representations}
In the \emph{event frame} representation the events are stored as a 2D grid by either summing the polarity or by counting the number of events. The accumulation is done for the time window of length $t$ which represents the equivalent of the exposure time. The conversion from an event stream to an event frame is fast as can be done in linear time $\mathcal{O}(k)$ by iterating once over the stream. This representation is suitable for filtering out which pixels have a sufficiently high number of events to be candidates for a blinking LED, however it does not include any time information which would allow robust frequency detection.

A \emph{time-surface} representation is also a 2D image, but each pixel in this 2D grid is assigned the value of the latest timestamp. For detecting blinking LEDs this representation is unsuitable as it contains no information related to periodic on-off transitions of a pixels.

\subsubsection*{3D Representations}
In an \emph{event volume}, events are stored as a 3D grid in a form that can be thought of as a stack of multiple event frames. This representation also includes time information and, given a sufficiently fine binning in the time domain, could be used to detect the frequency of a blinking LED. However, to accurately detect the frequency the binning would have to be very fine, yielding a huge memory footprint. For an accuracy of \unit[5]{$\mu s$}, a window length $t = \unit[2]{ms}$, VGA resolution and \texttt{uint8\_t} storage, the event volume would occupy \unit[117]{MB} of memory. This size exceeds all levels of the processor cache, potentially affecting runtime adversely.

\subsubsection*{Signed Delta-Time Volume}
To get past the shortcomings of those widely used event representation we propose a data representation that is ideally suited for the task of blinking LED detection: the \emph{signed delta-time volume (SDTV)}. It is a 3D volume of size {$W \times H \times D$} where $D$ is the stack depth. For each pixel the time difference to the last event is stored and the polarity of the event is encoded in the sign of this time difference. This is possible because time must be monotonically increasing, so we can re-purpose the sign-bit for polarity encoding. The idea is illustrated in Fig.~\ref{fig:sdtv} for a single pixel stack of the SDTV.
\begin{figure}
    \centering
    \input{media/Fig_SDTVRepresentation}
    \vspace*{-18pt}
    \caption{Illustration on the construction of the \emph{Signed Delta-Time Volume (SDTV)} from an event stream. \textbf{a)} The LED is blinking with a period of $\unit[300]{\mu s}$ with a duty cycle of \unit[10]{\%}. \textbf{b)} A single pixel of the event camera records a noisy signal of this blinking LED. False double events (e.g. at $t = \unit[150]{\mu s}, \unit[165]{\mu s}$) and spurious events (e.g. at $t = \unit[630]{\mu s}$) are included. \textbf{c)} Construction of the SDTV illustrated before processing the latest time window and after processing the time window. \textbf{d)} Periods robustly identified from the SDTV by summing up absolute time differences between negative $\rightarrow$ positive transitions (the first positive value is included). All events until the first positive $\rightarrow$ negative transition are discarded.}
    \label{fig:sdtv}
\end{figure}

Because of the fast blinking of the LEDs the time differences (in microseconds) between consecutive events are always within \texttt{int16\_t} range, making storage compact. As most operations are done per pixel-stack, the memory layout is such that the $D$-dimension is consecutive. Similar to the other representations, converting an event stream to SDTV is linear in the number of events.
The signed delta-time volume is not computed per window of length $T$ but updated as a cyclic buffer. This increases the accuracy of the frequency detection for LEDs blinking at a lower frequency than $f_\text{max}$ since the amount of LED periods available for frequency identification is independent of the frequency.

The minimal depth $D$ can be calculated based on the window length $T$ and the frequency of the fastest LED $f_\text{max}$ as $D = 2\,T\,f_\text{max}$ because every LED should trigger two events (once on and once off) per period. Typical values of $D$ are between 4 and 16, reducing the memory footprint by a factor of 25 to 100 compared to a event volume.


\subsection{Filtering}
When computing the signed delta-time volume representation from the event stream for a time window $t$ of events, we also compute an event frame based on the event count of each pixel. Only pixels with more than {$\beta \cdot 2 t f_\text{min}$} events are considered further where $\beta$ is the probability that a transition triggers an event. We use {$\beta = 0.8$} to purposely underestimate the detection probability.

For all selected pixels the SDTV is used to calculate mean, median and standard deviation of the period. The period is defined as the time between two on-events with at least one off-event in between as illustrated in Fig.~\ref{fig:sdtv}d). In agreement with~\cite{censi2013activeled} we find that this is a robust measure. After rejecting pixels with a too-large standard deviation in the period, pixels are clustered together. Too small and too large clusters are rejected as the expected size of an LED is roughly known a priori. Clusters are then assigned to the individual LEDs by matching the measured average period in a cluster with the blinking frequencies of the LEDs. Each LED is tracked by a particle filter that gets the assigned clusters for each LED as an input.

\subsection{Multi-Theading}
\label{subsec:multi_threading}
In a real-time application like this, relying on generic multi-threading tools such as OpenMP can be problematic. For this reason, the threading is manually implemented to ensure optimal performance. Each thread in the pipeline shares its memory with the next thread in the pipeline. To ensure threads do not block each other, each thread allocates the required memory two times. During operation, the thread writes to one of its allocations while the other memory chunk is processed by the next pipeline step. Subsequently, the memory pointers are swapped and the newly filled batch processed. 

The pipeline primarily consists of three threads. They
\begin{enumerate}
    \item copy events from event camera driver into a buffer,
    \item convert a linear event buffer into the optimized SDTV representation described in Section~\ref{subsec:event_data_representation}, and
    \item process the accumulated data to detect the LEDs, assign the LED clusters and solve the PnP (perspective-n-points) problem.
\end{enumerate}

This design makes it possible to run the pipeline at hight speeds on a modern laptop. Speeds exceeding \unit[1]{kHz} are possible, but due the slowest LED blinking at \unit[1700]{Hz} increasing the processing frequency beyond \unit[800]{Hz} might degrade robustness.