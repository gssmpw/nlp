\section{Introduction}
\label{sec:introduction}
Motion capture systems are external camera systems that provide high-accuracy, low-latency pose estimates of an object by tracking markers attached to the object. The 3D position and orientation of the tracked object is then computed through multi-view triangulation. Such motion capture systems are ubiquitous in mobile robotics and they are used for platforms ranging from autonomous drones~\cite{ducard2009fma, kaufmann23champion}, remotely controlled (RC) cars~\cite{froehlich2022contextual}, and swimming vehicles~\cite{menolotto2020mocapreview} to manipulation~\cite{aljalbout2024transfer}. However, such motion capture systems require the object to be in view of multiple cameras, making deployment in narrow, confined spaces extremely difficult. Furthermore, the portability of the systems is negatively impacted by the long set-up time of the multi-camera system.

To overcome these drawbacks we developed the monocular, event-camera motion capture system shown in Fig.~\ref{fig:overview}. In contrast to multi-camera motion-capture systems that rely on triangulation, a monocular system must solve the PnP (perspective-n-point problem) problem~\cite{zisserman2004multipleview} to obtain the pose of the object in the camera frame. For a unique solution, at least four 3D~$\leftrightarrow$~2D correspondances must be known~\cite{gao2003complete}. Put differently, it is not enough to detect the markers, but the markers must also be uniquely identified. Note, in the PnP-setting it is always assumed that the locations of the markers on the tracking object are known, for example from a CAD model.

Event-cameras are an excellent tool to overcome the above limitations and build a monocular motion capture system. By using blinking LEDs as active markers the markers can be easily detected by the event-camera. To uniquely identify each marker, the LEDs blink at different, known frequencies~\cite{censi2013activeled}. The blinking frequency for each detected marker is then measured from the event stream to associate the detection in the image with an LED marker. The high temporal resolution of event cameras makes it possible to use very fast blinking frequencies and obtain a low-latency system, which is ideally suited for challenging real-world robotics tasks. 

The idea to use blinking LEDs in combination with an event camera for localization has been explored for over a decade~\cite{censi2013activeled, ebmer2024realtime6dof, loch2023eventbasedfiducial, salah2022neuromorphicvisionbased}, but most prior works focus on using the event camera on a mobile robot to localize w.r.t fixed markers. In this setting, the compute is constrained but markers can be large and consist of multiple LEDs. The availability of IMU measurements from the robot additionally simplifies the task when the event-camera is mounted the robot itself. Our system is different as it uses a static event camera and active markers on the robot, and as such presents a much improved version of~\cite{censi2013activeled} featuring true 6-DOF tracking, a 50-fold improvement in accuracy, and a 4-fold improvement in update rate. To the best of the authors knowledge, it is the first true monocular motion-capture system providing millimeter-accuracy 6-DOF pose estimates at update rates as high as \unit[1]{kHz}. This advance is enabled by modern sensors, an efficient, cache-friendly event-processing pipeline, utilizing a novel \emph{signed delta-time volume} event representation in combination with the very accurate and robust SqPnP~\cite{terzakis2020sqpnp} algorithm to estimate the pose of the tracked object.