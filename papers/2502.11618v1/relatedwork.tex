\section{Related Work}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/overview_rev_tm.pdf}
    \caption{This figure provides an overview of the proposed method. A space-partitioned point cloud is projected onto the camera view using frustum culling, generating RGB, depth (D) and binary fill mask (A). The depth channel is used for depth filtering, as detailed in Fig. \ref{fig:depthfiltering_step}. A fully convolutional U-Net then processes the resulting depth-filtered RGBDA image to produce the final output.}
    \label{fig:overview}
\end{figure*}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/fig2.pdf}
    \caption{On the left, the hierarchical structure is illustrated, where we use minpooling with kernel size 2$\times$2 and stride 2 to downsample the image n times. Upsampling then occurs until the original image size is reached. On the right, an upsampling step is depicted. First, a Laplacian kernel is applied to the min-pooled(i-1) image for edge detection. If the pixel being upsampled in upsampled(i-1) is a non-edge pixel, its value is compared to the corresponding four pixels in min-pooled(i). For edge pixels, these four values are also compared with the eight neighboring pixels in upsampled(i-1). A filtering factor, filter\_strength, determines how many points are retained, with smaller values producing sparser results. This process generates an upsampled image, where unfiltered pixels remain empty. If this is not the final upsampling step, empty pixels are filled using linear interpolation from upsampled(i-1).}
    \label{fig:depthfiltering_step}
\end{figure*}