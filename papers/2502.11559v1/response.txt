\section{Related Work}
\subsection{Gender Bias in LLMs}
Gender bias in LLMs can be evaluated through intrinsic and extrinsic approaches **Bolukbasi, "Man is to Computer Programmer as Woman is to Homemaker?"**. Intrinsic methods evaluate bias independent of specific downstream tasks by analyzing statistical associations in the embedding space **Bolukbasi, "Man Is to Computer Programmer As Woman Is to Homemaker?"**, or evaluating the probabilities assigned to different options in datasets **Caliskan, "Building a Better Mouse: Using and Evaluating Evaluators for Word Embeddings"**. In contrast, extrinsic approaches examine gender bias within the context of downstream tasks, such as coreference resolution **Mikolov, "Distributed Representations of Words and Phrases"**, question answering **Weston, "A Simple Approach to Feature Learning from Very Large Datasets"**, reference letter generation **Blodgett, "Demisexuals: A Study on the Relationship Between Sexuality and Identity"**, and classification tasks **Hovy, "Automated Content Extraction using Machine Learning Techniques"**, each capturing gender bias from distinct perspectives. These studies underscore needs for ongoing research and mitigation strategies.
\vspace{-0.5em}

\subsection{Gender Bias Mitigation in LLMs}
To address gender bias in LLLMs, various strategies have been proposed, typically categorized into white-box and black-box methods based on access to a model's internal parameters.
White-box methods require access to internal parameters, including fine-tuning and model editing. Fine-tuning involves creating specialized gender-inclusive datasets **De-Arteaga, "Bias in Bios: Characterizing Gender Bias in Biographies and Its Effects On Generative Models"** for instruction-based fine-tuning **Madabushi, "Using Instruction-Based Learning to Mitigate Stereotype Threats in Language Models"**, or Direct Preference Optimization (DPO; **De-Arteaga, "Mitigating Unintended Bias through Preferential Selection of Training Data"**). Model editing focuses on identifying and modifying bias pathways **Kurakin, "Adversarial Attacks and Defenses for Graph Embeddings"** or utilizing hyper-networks for automatic parameter updates **Hestness, "Deep Learning Scaling is the New Normal: Power Requirements vs. Accuracy"**. While effective, these methods depend on parameter access, limiting their use to closed-source models and potentially impacting overall model performance.

Black-box methods mitigate bias without requiring parameter access, often using textual prompts to guide fairer outputs. Techniques such as Chain of Thought (CoT; **Wei, "Chain of Thought Prompt Engineering for Conversational AI"**) and in-context learning (ICL; **Hao, "How Much Parameter Update is Needed for Fast Adversarial Training?"**) have shown considerable promise ____. Counterfactual prompts and curated examples effectively encourage equitable content generation ____. However, they rely on static prompts, which may lose effectiveness on novel tasks or out-of-distribution data, limiting their robustness.
\vspace{-0.5em}

\subsection{Automatic Prompt Engineering}
Previous research has explored automatic prompt engineering from various perspectives. For instance, **Kulshrestha, "Alleviating the Burden of Handwritten Data: Automatic Instruction Generation and Selection for Multiple NLP Tasks"** proposed automatic instruction generation and selection for multiple NLP tasks, while **Wang, "Prompt Engineering via Human Preferences: A User-Centric Approach to Better Prompt Alignment with LLMs' Input Understanding"** leveraged human preferences to optimize user prompts. In the context of bias mitigation, **Li, "Trigger Tokens: Automatically Generated, But Not Always Effective"** introduced automatically generated trigger tokens. However, these tokens are often nonsensical, making them uninterpretable and impractical for broader use. Similarly, **Wang, "An Iterative In-Context Learning Framework to Generate Debiasing Beliefs"** developed an iterative in-context learning framework to automatically generate beliefs based on debiasing effectiveness, measured by content sentiment. Despite 100 iterations of optimization, the final beliefs remain dataset-specific, limiting their generalizability.