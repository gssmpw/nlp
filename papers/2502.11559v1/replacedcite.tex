\section{Related Work}
\subsection{Gender Bias in LLMs}
Gender bias in LLMs can be evaluated through intrinsic and extrinsic approaches ____. Intrinsic methods evaluate bias independent of specific downstream tasks by analyzing statistical associations in the embedding space ____ or evaluating the probabilities assigned to different options in datasets ____.
In contrast, extrinsic approaches examine gender bias within the context of downstream tasks, such as coreference resolution ____, question answering ____, reference letter generation ____, and classification tasks ____, each capturing gender bias from distinct perspectives. These studies underscore needs for ongoing research and mitigation strategies.
\vspace{-0.5em}

\subsection{Gender Bias Mitigation in LLMs}
To address gender bias in LLMs, various strategies have been proposed, typically categorized into white-box and black-box methods based on access to a model's internal parameters.
White-box methods require access to internal parameters, including fine-tuning and model editing. Fine-tuning involves creating specialized gender-inclusive datasets ____ for instruction-based fine-tuning ____ or Direct Preference Optimization (DPO; ____). Model editing focuses on identifying and modifying bias pathways ____ or utilizing hyper-networks for automatic parameter updates ____. While effective, these methods depend on parameter access, limiting their use to closed-source models and potentially impacting overall model performance.

Black-box methods mitigate bias without requiring parameter access, often using textual prompts to guide fairer outputs. Techniques such as Chain of Thought (CoT; ____) and in-context learning (ICL; ____) have shown considerable promise ____. Counterfactual prompts and curated examples effectively encourage equitable content generation ____. However, they rely on static prompts, which may lose effectiveness on novel tasks or out-of-distribution data, limiting their robustness.
\vspace{-0.5em}

\subsection{Automatic Prompt Engineering}
Previous research has explored automatic prompt engineering from various perspectives. For instance, ____ proposed automatic instruction generation and selection for multiple NLP tasks, while ____ leveraged human preferences to optimize user prompts for better alignment with LLMs' input understanding.
In the context of bias mitigation, ____ introduced automatically generated trigger tokens. However, these tokens are often nonsensical, making them uninterpretable and impractical for broader use. Similarly, ____ developed an iterative in-context learning framework to automatically generate beliefs based on debiasing effectiveness, measured by content sentiment. Despite 100 iterations of optimization, the final beliefs remain dataset-specific, limiting their generalizability.