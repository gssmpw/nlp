@misc{anonymous2024Editbias,
  title={EDITBIAS: Debiasing Stereotyped Language Models via Model Editing},
  author={Anonymous},
  year={2024},
  note={OpenReview Preprint},
  url={https://openreview.net/pdf/85b5b92f3386df93e99f72d4d1641134327097c7.pdf},
}

@article{bartl2024showgirls,
  title={From'Showgirls' to'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs},
  author={Bartl, Marion and Leavy, Susan},
  journal={arXiv preprint arXiv:2407.04434},
  year={2024}
}

@article{bauer2024believe,
  title={BELIEVE: Belief-enhanced instruction generation and augmentation for zero-shot bias mitigation},
  author={Bauer, Lisa and Mehrabi, Ninareh and Goyal, Palash and Chang, Kai-Wei and Galstyan, Aram and Gupta, Rahul},
  year={2024}
}

@inproceedings{cai2024locating,
  title={Locating and mitigating gender bias in large language models},
  author={Cai, Yuchen and Cao, Ding and Guo, Rongxi and Wen, Yaqin and Liu, Guiquan and Chen, Enhong},
  booktitle={International Conference on Intelligent Computing},
  pages={471--482},
  year={2024},
  organization={Springer}
}

@article{cheng2023black,
  title={Black-box prompt optimization: Aligning large language models without model training},
  author={Cheng, Jiale and Liu, Xiao and Zheng, Kehan and Ke, Pei and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.04155},
  year={2023}
}

@inproceedings{de2019bias,
  title={Bias in bios: A case study of semantic representation bias in a high-stakes setting},
  author={De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  booktitle={proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={120--128},
  year={2019}
}

@article{dong2024disclosure,
  title={Disclosure and mitigation of gender bias in llms},
  author={Dong, Xiangjue and Wang, Yibo and Yu, Philip S and Caverlee, James},
  journal={arXiv preprint arXiv:2402.11190},
  year={2024}
}

@article{dwivedi2023breaking,
  title={Breaking the bias: Gender fairness in LLMs using prompt engineering and in-context learning},
  author={Dwivedi, Satyam and Ghosh, Sanjukta and Dwivedi, Shivam},
  journal={Rupkatha Journal on Interdisciplinary Studies in Humanities},
  volume={15},
  number={4},
  year={2023}
}

@article{feng2023pretraining,
  title={From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.08283},
  year={2023}
}

@article{ganguli2023capacity,
  title={The capacity for moral self-correction in large language models},
  author={Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and others},
  journal={arXiv preprint arXiv:2302.07459},
  year={2023}
}

@inproceedings{kotek2023gender,
  title={Gender bias and stereotypes in large language models},
  author={Kotek, Hadas and Dockum, Rikker and Sun, David},
  booktitle={Proceedings of the ACM collective intelligence conference},
  pages={12--24},
  year={2023}
}

@article{kurita2019measuring,
  title={Measuring bias in contextualized word representations},
  author={Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:1906.07337},
  year={2019}
}

@article{levy2021collecting,
  title={Collecting a large-scale gender bias dataset for coreference resolution and machine translation},
  author={Levy, Shahar and Lazar, Koren and Stanovsky, Gabriel},
  journal={arXiv preprint arXiv:2109.03858},
  year={2021}
}

@article{li2023survey,
  title={A survey on fairness in large language models},
  author={Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Wang, Ying},
  journal={arXiv preprint arXiv:2308.10149},
  year={2023}
}

@article{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  journal={arXiv preprint arXiv:1903.10561},
  year={2019}
}

@article{nadeem2020stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  journal={arXiv preprint arXiv:2004.09456},
  year={2020}
}

@article{nangia2020crows,
  title={CrowS-pairs: A challenge dataset for measuring social biases in masked language models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.00133},
  year={2020}
}

@inproceedings{oba2024contextual,
  title={In-Contextual Gender Bias Suppression for Large Language Models},
  author={Oba, Daisuke and Kaneko, Masahiro and Bollegala, Danushka},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={1722--1742},
  year={2024}
}

@article{raza2024mbias,
  title={MBIAS: Mitigating Bias in Large Language Models While Retaining Context},
  author={Raza, Shaina and Raval, Ananya and Chatrath, Veronica},
  journal={arXiv preprint arXiv:2405.11290},
  year={2024}
}

@article{sant2024power,
  title={The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs},
  author={Sant, Aleix and Escolano, Carlos and Mash, Audrey and Fornaciari, Francesca De Luca and Melero, Maite},
  journal={arXiv preprint arXiv:2407.18786},
  year={2024}
}

@article{sheng2020towards,
  title={Towards controllable biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:2005.00268},
  year={2020}
}

@article{si2022prompting,
  title={Prompting gpt-3 to be reliable},
  author={Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan},
  journal={arXiv preprint arXiv:2210.09150},
  year={2022}
}

@article{thakur2023language,
  title={Language models get a gender makeover: Mitigating gender bias with few-shot data interventions},
  author={Thakur, Himanshu and Jain, Atishay and Vaddamanu, Praneetha and Liang, Paul Pu and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2306.04597},
  year={2023}
}

@article{wan2023kelly,
  title={" kelly is a warm person, joseph is a role model": Gender biases in llm-generated reference letters},
  author={Wan, Yixin and Pu, George and Sun, Jiao and Garimella, Aparna and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:2310.09219},
  year={2023}
}

@inproceedings{zayed2024fairness,
  title={Fairness-aware structured pruning in transformers},
  author={Zayed, Abdelrahman and Mordido, Gon{\c{c}}alo and Shabanian, Samira and Baldini, Ioana and Chandar, Sarath},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={20},
  pages={22484--22492},
  year={2024}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

