\section{Related Work}
\subsection{Gender Bias in LLMs}
Gender bias in LLMs can be evaluated through intrinsic and extrinsic approaches \cite{li2023survey, zayed2024fairness}. Intrinsic methods evaluate bias independent of specific downstream tasks by analyzing statistical associations in the embedding space \cite{kurita2019measuring, may2019measuring} or evaluating the probabilities assigned to different options in datasets \cite{nangia2020crows, nadeem2020stereoset}.
In contrast, extrinsic approaches examine gender bias within the context of downstream tasks, such as coreference resolution \cite{levy2021collecting, kotek2023gender}, question answering \cite{feng2023pretraining}, reference letter generation \cite{wan2023kelly}, and classification tasks \cite{de2019bias}, each capturing gender bias from distinct perspectives. These studies underscore needs for ongoing research and mitigation strategies.
\vspace{-0.5em}

\subsection{Gender Bias Mitigation in LLMs}
To address gender bias in LLMs, various strategies have been proposed, typically categorized into white-box and black-box methods based on access to a model's internal parameters.
White-box methods require access to internal parameters, including fine-tuning and model editing. Fine-tuning involves creating specialized gender-inclusive datasets \cite{bartl2024showgirls, dong2024disclosure} for instruction-based fine-tuning \cite{raza2024mbias, thakur2023language} or Direct Preference Optimization (DPO; \citealp{zhang2024genderalign, allam2024biasdpo}). Model editing focuses on identifying and modifying bias pathways \cite{cai2024locating} or utilizing hyper-networks for automatic parameter updates \cite{anonymous2024Editbias}. While effective, these methods depend on parameter access, limiting their use to closed-source models and potentially impacting overall model performance.

Black-box methods mitigate bias without requiring parameter access, often using textual prompts to guide fairer outputs. Techniques such as Chain of Thought (CoT; \citealp{wei2022chain}) and in-context learning (ICL; \citealp{brown2020language}) have shown considerable promise \cite{sant2024power, ganguli2023capacity}. Counterfactual prompts and curated examples effectively encourage equitable content generation \cite{si2022prompting, dwivedi2023breaking, oba2024contextual}. However, they rely on static prompts, which may lose effectiveness on novel tasks or out-of-distribution data, limiting their robustness.
\vspace{-0.5em}

\subsection{Automatic Prompt Engineering}
Previous research has explored automatic prompt engineering from various perspectives. For instance, \citet{zhou2022large} proposed automatic instruction generation and selection for multiple NLP tasks, while \citet{cheng2023black} leveraged human preferences to optimize user prompts for better alignment with LLMs' input understanding.
In the context of bias mitigation, \citet{sheng2020towards} introduced automatically generated trigger tokens. However, these tokens are often nonsensical, making them uninterpretable and impractical for broader use. Similarly, \citet{bauer2024believe} developed an iterative in-context learning framework to automatically generate beliefs based on debiasing effectiveness, measured by content sentiment. Despite 100 iterations of optimization, the final beliefs remain dataset-specific, limiting their generalizability.