@inproceedings{zayed2024fairness,
  title={Fairness-aware structured pruning in transformers},
  author={Zayed, Abdelrahman and Mordido, Gon{\c{c}}alo and Shabanian, Samira and Baldini, Ioana and Chandar, Sarath},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={20},
  pages={22484--22492},
  year={2024}
}

@article{li2023survey,
  title={A survey on fairness in large language models},
  author={Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Wang, Ying},
  journal={arXiv preprint arXiv:2308.10149},
  year={2023}
}

@article{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  journal={arXiv preprint arXiv:1903.10561},
  year={2019}
}

@article{kurita2019measuring,
  title={Measuring bias in contextualized word representations},
  author={Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:1906.07337},
  year={2019}
}

@article{nadeem2020stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  journal={arXiv preprint arXiv:2004.09456},
  year={2020}
}

@article{nangia2020crows,
  title={CrowS-pairs: A challenge dataset for measuring social biases in masked language models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2010.00133},
  year={2020}
}

@inproceedings{kotek2023gender,
  title={Gender bias and stereotypes in large language models},
  author={Kotek, Hadas and Dockum, Rikker and Sun, David},
  booktitle={Proceedings of the ACM collective intelligence conference},
  pages={12--24},
  year={2023}
}

@article{levy2021collecting,
  title={Collecting a large-scale gender bias dataset for coreference resolution and machine translation},
  author={Levy, Shahar and Lazar, Koren and Stanovsky, Gabriel},
  journal={arXiv preprint arXiv:2109.03858},
  year={2021}
}

@article{feng2023pretraining,
  title={From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models},
  author={Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2305.08283},
  year={2023}
}

@article{wan2023kelly,
  title={" kelly is a warm person, joseph is a role model": Gender biases in llm-generated reference letters},
  author={Wan, Yixin and Pu, George and Sun, Jiao and Garimella, Aparna and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:2310.09219},
  year={2023}
}

@article{chu2024fairness,
  title={Fairness in Large Language Models: A Taxonomic Survey},
  author={Chu, Zhibo and Wang, Zichong and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2404.01349},
  year={2024}
}

@inproceedings{de2019bias,
  title={Bias in bios: A case study of semantic representation bias in a high-stakes setting},
  author={De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  booktitle={proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={120--128},
  year={2019}
}

@article{thakur2023language,
  title={Language models get a gender makeover: Mitigating gender bias with few-shot data interventions},
  author={Thakur, Himanshu and Jain, Atishay and Vaddamanu, Praneetha and Liang, Paul Pu and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2306.04597},
  year={2023}
}

@article{dong2024disclosure,
  title={Disclosure and mitigation of gender bias in llms},
  author={Dong, Xiangjue and Wang, Yibo and Yu, Philip S and Caverlee, James},
  journal={arXiv preprint arXiv:2402.11190},
  year={2024}
}

@misc{anonymous2024Editbias,
  title={EDITBIAS: Debiasing Stereotyped Language Models via Model Editing},
  author={Anonymous},
  year={2024},
  note={OpenReview Preprint},
  url={https://openreview.net/pdf/85b5b92f3386df93e99f72d4d1641134327097c7.pdf},
}

@inproceedings{cai2024locating,
  title={Locating and mitigating gender bias in large language models},
  author={Cai, Yuchen and Cao, Ding and Guo, Rongxi and Wen, Yaqin and Liu, Guiquan and Chen, Enhong},
  booktitle={International Conference on Intelligent Computing},
  pages={471--482},
  year={2024},
  organization={Springer}
}

@article{allam2024biasdpo,
  title={BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization},
  author={Allam, Ahmed},
  journal={arXiv preprint arXiv:2407.13928},
  year={2024}
}

@article{zhang2024genderalign,
  title={GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models},
  author={Zhang, Tao and Zeng, Ziqian and Xiao, Yuxiang and Zhuang, Huiping and Chen, Cen and Foulds, James and Pan, Shimei},
  journal={arXiv preprint arXiv:2406.13925},
  year={2024}
}

@article{raza2024mbias,
  title={MBIAS: Mitigating Bias in Large Language Models While Retaining Context},
  author={Raza, Shaina and Raval, Ananya and Chatrath, Veronica},
  journal={arXiv preprint arXiv:2405.11290},
  year={2024}
}

@inproceedings{zmigrod2019counterfactual,
  title={Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology},
  author={Zmigrod, Ran and Mielke, Sabrina J and Wallach, Hanna and Cotterell, Ryan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1651--1661},
  year={2019}
}

@article{bartl2024showgirls,
  title={From'Showgirls' to'Performers': Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs},
  author={Bartl, Marion and Leavy, Susan},
  journal={arXiv preprint arXiv:2407.04434},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{sant2024power,
  title={The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs},
  author={Sant, Aleix and Escolano, Carlos and Mash, Audrey and Fornaciari, Francesca De Luca and Melero, Maite},
  journal={arXiv preprint arXiv:2407.18786},
  year={2024}
}

@article{ganguli2023capacity,
  title={The capacity for moral self-correction in large language models},
  author={Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and others},
  journal={arXiv preprint arXiv:2302.07459},
  year={2023}
}

@article{si2022prompting,
  title={Prompting gpt-3 to be reliable},
  author={Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan},
  journal={arXiv preprint arXiv:2210.09150},
  year={2022}
}

@article{dwivedi2023breaking,
  title={Breaking the bias: Gender fairness in LLMs using prompt engineering and in-context learning},
  author={Dwivedi, Satyam and Ghosh, Sanjukta and Dwivedi, Shivam},
  journal={Rupkatha Journal on Interdisciplinary Studies in Humanities},
  volume={15},
  number={4},
  year={2023}
}

@inproceedings{oba2024contextual,
  title={In-Contextual Gender Bias Suppression for Large Language Models},
  author={Oba, Daisuke and Kaneko, Masahiro and Bollegala, Danushka},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={1722--1742},
  year={2024}
}

@article{sheng2020towards,
  title={Towards controllable biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:2005.00268},
  year={2020}
}

@article{bauer2024believe,
  title={BELIEVE: Belief-enhanced instruction generation and augmentation for zero-shot bias mitigation},
  author={Bauer, Lisa and Mehrabi, Ninareh and Goyal, Palash and Chang, Kai-Wei and Galstyan, Aram and Gupta, Rahul},
  year={2024}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

@article{cheng2023black,
  title={Black-box prompt optimization: Aligning large language models without model training},
  author={Cheng, Jiale and Liu, Xiao and Zheng, Kehan and Ke, Pei and Wang, Hongning and Dong, Yuxiao and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.04155},
  year={2023}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}

@article{luccioni2021s,
  title={What's in the box? a preliminary analysis of undesirable content in the common crawl corpus},
  author={Luccioni, Alexandra Sasha and Viviano, Joseph D},
  journal={arXiv preprint arXiv:2105.02732},
  year={2021}
}

@misc{anthropicClaude,
 author = {Anthropic},
 title = {{C}laude 2 --- anthropic.com},
 howpublished = {\url{https://www.anthropic.com/news/claude-2}},
 year = {},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}


@article{conover2023free,
  title={Free dolly: Introducing the worldâ€™s first truly open instruction-tuned llm},
  author={Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  journal={Company Blog of Databricks},
  year={2023}
}

@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{openaiOpenAI,
	author = {OpenAI},
	howpublished = {\url{https://openai.com}},
	year = {2024},
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@misc{yang2024qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL}

}

@article{parrish2021bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{wang2023pandalm,
  title={Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization},
  author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others},
  journal={arXiv preprint arXiv:2306.05087},
  year={2023}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}