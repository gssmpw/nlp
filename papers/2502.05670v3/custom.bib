@article{Wasow_1997, title={Remarks on grammatical weight}, volume={9}, DOI={10.1017/S0954394500001800}, number={1}, journal={Language Variation and Change}, author={Thomas Wasow}, year={1997}, pages={81–105}}

@misc{hu2023promptingsubstituteprobabilitymeasurements,
      title={Prompting is not a substitute for probability measurements in large language models}, 
      author={Jennifer Hu and Roger Levy},
      year={2023},
      eprint={2305.13264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13264}, 
}

 @inbook{Wasow_Arnold_2003, address={Berlin, New York}, title={Post-verbal constituent ordering in English}, ISBN={978-3-11-090001-9}, url={https://doi.org/10.1515/9783110900019.119}, DOI={doi:10.1515/9783110900019.119}, booktitle={Determinants of Grammatical Variation in English}, publisher={De Gruyter Mouton}, author={Wasow, Thomas and Arnold, Jennifer}, year={2003}, pages={119–154}}

 @article{Wasow_endweight, title={End-Weight from the Speaker’s Perspective}, volume={26}, ISSN={1573-6555}, DOI={10.1023/A:1025080709112}, abstractNote={Explanations of the tendency to put long, complex constituents at the ends of sentences (“end-weight”) usually take the listener’s perspective, claiming it facilitates parsing. I argue for a speaker-oriented explanation of end-weight, based on how it facilitates utterance planning. Parsing is facilitated when as much tree structure as possible can be determined early in the string, but production is easiest when options for how to continue are kept open. That is, listeners should prefer early commitment and speakers should prefer late commitment. Corpus data show that different verbs exhibit different rates of word-order variation that are systematically related to differences in subcategorization possibilities in just the way predicted by a strategy of late commitment. Thus, a speaker-based account of lexical preferences in word ordering does a better job of explaining variation in weight effects than a listener-based account.}, number={3}, journal={Journal of Psycholinguistic Research}, author={Thomas Wasow}, year={1997}, month=may, pages={347–361}}

 @article{Gibson_1998, title={Linguistic complexity: locality of syntactic dependencies}, volume={68}, ISSN={0010-0277}, DOI={https://doi.org/10.1016/S0010-0277(98)00034-1}, abstractNote={This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory – the Syntactic Prediction Locality Theory (SPLT) – has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.}, number={1}, journal={Cognition}, author={Edward Gibson}, year={1998}, pages={1–76}}

@book{hawkins2004,
    author = {John A. Hawkins},
    title = "{Efficiency and Complexity in Grammars}",
    publisher = {Oxford University Press},
    year = {2004},
    month = {11},
    abstract = "{This book addresses a question fundamental to any discussion of grammatical theory and grammatical variation: to what extent can principles of grammar be explained through language use? The book argues that there is a profound correspondence between performance data and the fixed conventions of grammars. Preferences and patterns found in the one, the book shows, are reflected in constraints and variation patterns in the other. The theoretical consequences of the proposed ‘performance-grammar correspondence hypothesis’ are far-reaching — for current grammatical formalisms, for the innateness hypothesis, and for psycholinguistic models of performance and learning. Drawing on empirical generalizations and insights from language typology, generative grammar, psycholinguistics, and historical linguistics, this book demonstrates that the assumption that grammars are immune to performance is false. It presents detailed empirical case studies and arguments for an alternative theory in which performance has shaped the conventions of grammars and thus the variation patterns found in the world’s languages. The innateness of language, the book argues, resides primarily in the mechanisms human beings have for processing and learning it.}",
    isbn = {9780199252695},
    doi = {10.1093/acprof:oso/9780199252695.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780199252695.001.0001},
}

@article{1909bezie,
url = {https://doi.org/10.1515/9783110242652.110},
title = {Beziehungen zwischen Umfang und Reihenfolge von Satzgliedern.},
author = {Otto Behaghel},
pages = {110--142},
volume = {25},
number = {1909},
journal = {Indogermanische Forschungen (1909)},
doi = {doi:10.1515/9783110242652.110},
year = {1909},
lastchecked = {2024-10-02}
}

@article{lau2017,
  author    = {J. H. Lau and A. Clark and S. Lappin},
  title     = {Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge},
  journal   = {Cognitive Science},
  year      = {2017},
  volume    = {41},
  number    = {5},
  pages     = {1202--1241},
  doi       = {10.1111/cogs.12414},
  pmid      = {27732744},
  month     = {July},
  eprint    = {2016 Oct 12}
}


@book{behaghel1932,
  author    = {Otto Behaghel},
  title     = {Deutsche Syntax: Eine geschichtliche Darstellung. Band IV: Wortstellung, Periodenbau},
  year      = {1932},
  publisher = {Carl Winters Universitätsbuchhandlung},
  address   = {Heidelberg}
}

@incollection{gundel1988,
  author    = {Gundel, Jeannette K.},
  title     = {Universals of topic-comment structure},
  booktitle = {Studies in Syntactic Typology},
  editor    = {Michael Hammond and Edith A. Moravcsik and Jessica R. Wirth},
  pages     = {209--239},
  year      = {1988},
  publisher = {Benjamins},
  address   = {Amsterdam}
}

@incollection{chafe1976,
  address            = {New York, NY},
  author             = {Chafe, Wallace L.},
  booktitle          = {Subject and topic: A new typology of language},
  editor             = {Li, Charles N.},
  journal            = {Subject and Topic},
  pages              = {25-55},
  publisher          = {Academic Press},
  title              = {Givenness, contrastiveness, definiteness, subjects, topics, and point of view},
  year               = {1976},
  citekeys           = {glossa4988:B2 glossa5356:B19 glossa5361:B35 glossa5373:B14 glossa5409:B7 glossa5465:B15 glossa5727:B20 glossa5778:B12 glossa8242:B9 glossa8514:B7 glossa9883:B7},
  guldemann_location = {TG},
  inlg               = {English [eng]},
  isreferencedby     = {glossa4988 glossa5356 glossa5361 glossa5373 glossa5409 glossa5465 glossa5727 glossa5778 glossa8242 glossa8514 glossa9883},
  location           = {New York},
  src                = {glossa, guldemann, langsci}
}

@book{knud1994, place={Cambridge}, series={Cambridge Studies in Linguistics}, title={Information Structure and Sentence Form: Topic, Focus, and the Mental Representations of Discourse Referents}, publisher={Cambridge University Press}, author={Lambrecht, Knud}, year={1994}, collection={Cambridge Studies in Linguistics}} <div></div>

@article{Hawkins_1999, title={The relative order of prepositional phrases in English: Going beyond Manner–Place–Time}, volume={11}, DOI={10.1017/S0954394599113012}, number={3}, journal={Language Variation and Change}, author={Hawkins, John A.}, year={1999}, pages={231–266}} <div></div>

@book{Hawkins_1995, place={Cambridge}, series={Cambridge Studies in Linguistics}, title={A Performance Theory of Order and Constituency}, publisher={Cambridge University Press}, author={John A. Hawkins}, year={1995}, collection={Cambridge Studies in Linguistics}} <div></div>


@article{misra2022minicons,
    title={minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models},
    author={Kanishka Misra},
    journal={arXiv preprint arXiv:2203.13112},
    year={2022}
}

@misc{futrell2018rnnslearnhumanlikeabstract,
      title={Do RNNs learn human-like abstract word order preferences?}, 
      author={Richard Futrell and Roger P. Levy},
      year={2018},
      eprint={1811.01866},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.01866}, 
}

@book{pinker2007language,
  title={The Language Instinct},
  author={Steven Pinker},
  year={2007},
  publisher={HarperCollins}
}

 @article{Medeiros_Mains_McGowan_2021, title={Ceiling Effects on Weight in Heavy NP Shift}, volume={52}, ISSN={0024-3892}, DOI={10.1162/ling_a_00382}, abstractNote={This squib tests theories of heavy NP shift that link constituent order to parsing. Our results indicate that increasing the weight of an object NP cannot make a heavy NP shift construction more acceptable than a comparison sentence in the canonical order, contra parsing-based theories. In addition, we examine acceptability as it relates to verb disposition. We find no significant differences between different subcategorizations, contrary to the findings of corpus studies. These results reveal a disconnect between production and comprehension; we further conclude that parser sensitivity to constituent structure is unlikely to affect speaker production of heavy NP shift.}, number={2}, journal={Linguistic Inquiry}, author={David J. Medeiros and Paul Mains and Kevin B. McGowan}, year={2021}, month=mar, pages={426–440}}

@misc{treebank_2,
title= {Penn Treebank II 2 - LDC95T7},
author= {Mitchell P. Marcus and Beatrice Santorini and Mary Ann Marcinkiewicz},
year= {1995},
doi= {10.35111/wf9p-g717},
isbn= {1-58563-054-3},
islrn= {650-146-755-602-3}}

@misc{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@misc{groeneveld2024olmoacceleratingsciencelanguage,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00838}, 
}

@article{Manetta2012ReconsideringRS,
  title={Reconsidering Rightward Scrambling: Postverbal Constituents in Hindi-Urdu},
  author={Emily Walker Manetta},
  journal={Linguistic Inquiry},
  year={2012},
  volume={43},
  pages={43-74},
  url={https://api.semanticscholar.org/CorpusID:57569578}
}

@misc{namboodiripad2019gradient,
  title={A gradient approach to flexible constituent order},
  author={Savithry Namboodiripad},
  year={2019},
  publisher={PsyArXiv}
}

@book{namboodiripad2017experimental,
  title={An experimental approach to variation and variability in constituent order},
  author={Savithry Namboodiripad},
  year={2017},
  publisher={University of California, San Diego}
}

@article{bakker1998flexibility,
  title={Flexibility and consistency in word order patterns in the languages of Europe},
  author={Dik Bakker},
  journal={Empirical Approaches to Language Typology},
  volume={20},
  pages={383--420},
  year={1998},
  publisher={Mouton de Gruyter}
}

@misc{nonverbpred,
author = {Kees Hengeveld},
year = {1992},
month = {01},
title = {Non-Verbal Predication: Theory, Typology, Diachrony},
isbn = {9783110883282},
doi = {10.1515/9783110883282}
}

@misc{wilcox2019syntacticstructuresblockdependencies,
      title={What Syntactic Structures block Dependencies in RNN Language Models?}, 
      author={Ethan Wilcox and Roger Levy and Richard Futrell},
      year={2019},
      eprint={1905.10431},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.10431}, 
}

@book{chomsky2002syntactic,
  title={Syntactic Structures},
  author={Noam Chomsky},
  isbn={9783110172799},
  lccn={2002043087},
  series={A Mouton classic},
  url={https://books.google.ca/books?id=a6a_b-CXYAkC},
  year={2002},
  publisher={Mouton de Gruyter}
}

@article{gams,
author = {Trevor Hastie and Robert Tibshirani},
title = {{Generalized Additive Models}},
volume = {1},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {297 -- 310},
keywords = {generalized linear models, nonlinearity, Nonparametric regression, partial residuals, smoothing},
year = {1986},
doi = {10.1214/ss/1177013604},
URL = {https://doi.org/10.1214/ss/1177013604}
}

@inproceedings{kamath-bradford-2023-neural,
    title = "Does a neural model understand the de re / de dicto distinction?",
    author = "Gaurav Kamath and
      Laurestine Bradford",
    editor = "Hunter, Tim  and
      Prickett, Brandon",
    booktitle = "Proceedings of the Society for Computation in Linguistics 2023",
    month = jun,
    year = "2023",
    address = "Amherst, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.scil-1.6",
    pages = "69--84",
}

@misc{zhou2024constructionsdifficultlargelanguage,
      title={Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons}, 
      author={Shijia Zhou and Leonie Weissweiler and Taiqi He and Hinrich Schütze and David R. Mortensen and Lori Levin},
      year={2024},
      eprint={2403.17760},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.17760}, 
}

@article{wallenberg2016extraposition,
  title={Extraposition is disappearing},
  author={Joel C. Wallenberg},
  journal={Language},
  volume={92},
  number={4},
  pages={e237--e256},
  year={2016},
  publisher={Linguistic Society of America},
  doi={10.1353/lan.2016.0079}
}

@article{spearmanC,
    author = {Charles Spearman},
    title = "{The proof and measurement of association between two things}",
    journal = {International Journal of Epidemiology},
    volume = {39},
    number = {5},
    pages = {1137-1150},
    year = {2010},
    month = {10},
    issn = {0300-5771},
    doi = {10.1093/ije/dyq191},
    url = {https://doi.org/10.1093/ije/dyq191},
    eprint = {https://academic.oup.com/ije/article-pdf/39/5/1137/18481215/dyq191.pdf},
}

@article{10.1162/kamath2024,
    author = {Kamath, Gaurav and Schuster, Sebastian and Vajjala, Sowmya and Reddy, Siva},
    title = "{Scope Ambiguities in Large Language Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {738-754},
    year = {2024},
    month = {06},
    abstract = "{Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2, and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90\% in some cases).1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00670},
    url = {https://doi.org/10.1162/tacl\_a\_00670},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00670/2377773/tacl\_a\_00670.pdf},
}





@book{haegeman1991,
  author    = {Haegeman, Liliane},
  title     = {Introduction to Government and Binding Theory},
  year      = {1991},
  publisher = {Blackwell},
  address   = {Oxford}
}

@book{wood2017generalized,
  title={Generalized Additive Models: An Introduction with R, Second Edition},
  author={Wood, Simon N.},
  edition={2},
  year={2017},
  publisher={Chapman and Hall/CRC},
  doi={10.1201/9781315370279}
}

@misc{sóskuthy2017generalisedadditivemixedmodels,
      title={Generalised additive mixed models for dynamic analysis in linguistics: a practical introduction}, 
      author={Márton Sóskuthy},
      year={2017},
      eprint={1703.05339},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/1703.05339}, 
}

@article{logstruct,
author = {Noam Chomsky},
year = {2008},
month = {12},
pages = {795-814},
title = {The Logical Structure of Linguistic Theory},
volume = {84},
journal = {Language},
doi = {10.1353/lan.0.0075}
}

@misc{quirketal,
author = {Randolph Quirk and Sidney Greenbaum and Geoffrey Leech and Jan Svartvik},
year = {1975},
month = {08},
pages = {277 - 280},
title = {Review of: A Grammar of Contemporary English, by Randolph Quirk, Sidney Greenbaum, Geoffrey Leech and Jan Svartvik},
volume = {11},
doi = {10.1017/S002222670000459X}
}

@article{wasow2002,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/4489669},
 editor = {Georgia M. Green},
 journal = {Language},
 number = {2},
 pages = {327--331},
 publisher = {Linguistic Society of America},
 author = {Thomas Wasow},
 urldate = {2024-10-02},
 volume = {80},
 year = {2002},
 title = {Postverbal Behavior}
}

@article{ARNOLD200455,
title = {Avoiding attachment ambiguities: The role of constituent ordering},
journal = {Journal of Memory and Language},
volume = {51},
number = {1},
pages = {55-70},
year = {2004},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2004.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X04000282},
author = {Jennifer E. Arnold and Thomas Wasow and Ash Asudeh and Peter Alrenga},
abstract = {Three experiments investigated whether speakers use constituent ordering as a mechanism for avoiding ambiguities. In utterances like “Jane showed the letter to Mary to her mother,” alternate orders would avoid the temporary PP-attachment ambiguity (“Jane showed her mother the letter to Mary,” or “Jane showed to her mother the letter to Mary”). A preference judgment experiment confirmed that comprehenders prefer the latter orders for dative utterances when the former order would have contained an ambiguity. Nevertheless, speakers in two on-line production experiments showed no evidence of an ambiguity avoidance strategy. In fact, they were slightly more likely to use the former order when it was ambiguous than when it was not. Speakers’ failure to disambiguate with ordering cannot be explained by the use of other ambiguity mechanisms, like prosody. A prosodic analysis of the responses in Experiment 3 showed that while speakers generally produced prosodic patterns that were consistent with the syntactic structure, these patterns would not strongly disambiguate the PP-attachment ambiguity. We suggest that speakers do not consistently disambiguate local PP-attachment ambiguities of this type, and in particular do not use constituent ordering for this purpose. Instead, constituent ordering is driven by factors like syntactic weight and lexical bias, which may be internal to the production system.}
}

@inproceedings{Fujihara2022TopicalizationIL,
  title={Topicalization in Language Models: A Case Study on Japanese},
  author={Riki Fujihara and Tatsuki Kuribayashi and Kaori Abe and Ryoko Tokuhisa and Kentaro Inui},
  booktitle={International Conference on Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252819390}
}

@article{WangLiu,
url = {https://doi.org/10.1515/psicl-2014-0024},
title = {The effects of length and complexity on constituent ordering in written English},
author = {Hua Wang and Haitao Liu},
pages = {477--494},
volume = {50},
number = {4},
journal = {Poznan Studies in Contemporary Linguistics},
doi = {doi:10.1515/psicl-2014-0024},
year = {2014},
lastchecked = {2024-10-02}
}


@misc{faghiri,
  TITLE = {{Relative weight and givenness in constituent ordering of typologically different languages: Evidence from French and Persian}},
  AUTHOR = {Pegah Faghiri and Juliette Thuilier},
  URL = {https://shs.hal.science/halshs-02605180},
  NOTE = {Poster},
  HOWPUBLISHED = {{The 31st Annual CUNY Conference on Human Sentence Processing}},
  YEAR = {2018},
  MONTH = Mar,
  HAL_ID = {halshs-02605180},
  HAL_VERSION = {v1},
}


@article{arnold_et_al,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/417392},
 abstract = {Variations in postverbal constituent ordering have been attributed to both grammatical complexity (heaviness) and discourse status (newness), although few studies compare the two factors explicitly. Through corpus analysis and experimentation, we demonstrate that both factors simultaneously and independently influence word order in two English constructions. While past investigations of these factors have focused on their effects in language comprehension, we argue that postponing heavy and new constituents facilitates processes of planning and production.},
 author = {Jennifer E. Arnold and Anthony Losongco and Thomas Wasow and Ryan Ginstrom},
 journal = {Language},
 number = {1},
 pages = {28--55},
 publisher = {Linguistic Society of America},
 title = {Heaviness vs. Newness: The Effects of Structural Complexity and Discourse Status on Constituent Ordering},
 urldate = {2024-10-02},
 volume = {76},
 year = {2000}
}


@misc{llama3modelcard,
title={Llama 3 Model Card},
author={AI@Meta},
year={2024},
url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Alex Warstadt and Aaron Mueller and Leshem Choshen and Ethan Wilcox and Chengxu Zhuang and Juan Ciro and Rafael Mosquera and Bhargavi Paranjabe and Adina Williams and Tal Linzen and Ryan Cotterell",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@article{futrell2015,
author = {Richard Futrell  and Kyle Mahowald  and Edward Gibson },
title = {Large-scale evidence of dependency length minimization in 37 languages},
journal = {Proceedings of the National Academy of Sciences},
volume = {112},
number = {33},
pages = {10336-10341},
year = {2015},
doi = {10.1073/pnas.1502134112},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1502134112},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1502134112},
abstract = {We provide the first large-scale, quantitative, cross-linguistic evidence for a universal syntactic property of languages: that dependency lengths are shorter than chance. Our work supports long-standing ideas that speakers prefer word orders with short dependency lengths and that languages do not enforce word orders with long dependency lengths. Dependency length minimization is well motivated because it allows for more efficient parsing and generation of natural language. Over the last 20 y, the hypothesis of a pressure to minimize dependency length has been invoked to explain many of the most striking recurring properties of languages. Our broad-coverage findings support those explanations. Explaining the variation between human languages and the constraints on that variation is a core goal of linguistics. In the last 20 y, it has been claimed that many striking universals of cross-linguistic variation follow from a hypothetical principle that dependency length—the distance between syntactically related words in a sentence—is minimized. Various models of human sentence production and comprehension predict that long dependencies are difficult or inefficient to process; minimizing dependency length thus enables effective communication without incurring processing difficulty. However, despite widespread application of this idea in theoretical, empirical, and practical work, there is not yet large-scale evidence that dependency length is actually minimized in real utterances across many languages; previous work has focused either on a small number of languages or on limited kinds of data about each language. Here, using parsed corpora of 37 diverse languages, we show that overall dependency lengths for all languages are shorter than conservative random baselines. The results strongly suggest that dependency length minimization is a universal quantitative property of human languages and support explanations of linguistic variation in terms of general properties of human information processing.}}

@inproceedings{xu-futrell-2024-syntactic,
    title = "Syntactic dependency length shaped by strategic memory allocation",
    author = "Weijie Xu and Richard Futrell",
    editor = "Hahn, Michael  and
      Sorokin, Alexey  and
      Kumar, Ritesh  and
      Shcherbakov, Andreas  and
      Otmakhova, Yulia  and
      Yang, Jinrui  and
      Serikov, Oleg  and
      Rani, Priya  and
      Ponti, Edoardo M.  and
      Murado{\u{g}}lu, Saliha  and
      Gao, Rena  and
      Cotterell, Ryan  and
      Vylomova, Ekaterina",
    booktitle = "Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP",
    month = mar,
    year = "2024",
    address = "St. Julian's, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.sigtyp-1.1",
    pages = "1--9",
    abstract = "Human processing of nonlocal syntactic dependencies requires the engagement of limited working memory for encoding, maintenance, and retrieval. This process creates an evolutionary pressure for language to be structured in a way that keeps the subparts of a dependency closer to each other, an efficiency principle termed dependency locality. The current study proposes that such a dependency locality pressure can be modulated by the surprisal of the antecedent, defined as the first part of a dependency, due to strategic allocation of working memory. In particular, antecedents with novel and unpredictable information are prioritized for memory encoding, receiving more robust representation against memory interference and decay, and thus are more capable of handling longer dependency length. We examine this claim by analyzing dependency corpora of 11 languages, with word surprisal generated from GPT-3 language model. In support of our hypothesis, we find evidence for a positive correlation between dependency length and the antecedent surprisal in most of the languages in our analyses. A closer look into the dependencies with core arguments shows that this correlation consistently holds for subject relations but not for object relations.",
}



@article{baars1975output,
  title={Output editing for lexical status in artificially elicited slips of the tongue},
  author={Bernard J Baars and Michael T Motley and Donald G MacKay},
  journal={Journal of verbal learning and verbal behavior},
  volume={14},
  number={4},
  pages={382--391},
  year={1975},
  publisher={Elsevier}
}

@article{kayne1983,
 ISSN = {00243892, 15309150},
 URL = {http://www.jstor.org/stable/4178324},
 author = {Richard S. Kayne},
 journal = {Linguistic Inquiry},
 number = {2},
 pages = {223--249},
 publisher = {The MIT Press},
 title = {Connectedness},
 urldate = {2024-10-05},
 volume = {14},
 year = {1983}
}


@article{hartsuiker2005lexical,
  title={The lexical bias effect is modulated by context, but the standard monitoring account doesn’t fly: Related beply to Baars et al.(1975)},
  author={Hartsuiker, Robert J and Corley, Martin and Martensen, Heike},
  journal={Journal of Memory and Language},
  volume={52},
  number={1},
  pages={58--70},
  year={2005},
  publisher={Elsevier}
}

@article{dell1981stages,
  title={Stages in sentence production: An analysis of speech error data},
  author={Gary S Dell and Peter A Reich},
  journal={Journal of verbal learning and verbal behavior},
  volume={20},
  number={6},
  pages={611--629},
  year={1981},
  publisher={Elsevier}
}


@inproceedings{marvin-linzen-2018-targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Rebecca Marvin and
      Tal Linzen",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{linzen-etal-2016-assessing,
    title = "Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies",
    author = "Linzen, Tal  and
      Dupoux, Emmanuel  and
      Goldberg, Yoav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1037",
    doi = "10.1162/tacl_a_00115",
    pages = "521--535",
    abstract = "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture{'}s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1{\%} errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",
}

@misc{baroni2022properrolelinguisticallyorienteddeep,
      title={On the proper role of linguistically-oriented deep net analysis in linguistic theorizing}, 
      author={Marco Baroni},
      year={2022},
      eprint={2106.08694},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.08694}, 
}

@inproceedings{wilcox-etal-2018-rnn,
    title = "What do {RNN} Language Models Learn about Filler{--}Gap Dependencies?",
    author = "Wilcox, Ethan  and
      Levy, Roger  and
      Morita, Takashi  and
      Futrell, Richard",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5423",
    doi = "10.18653/v1/W18-5423",
    pages = "211--221",
    abstract = "RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance \textbf{filler{--}gap dependencies} and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler{--}gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler{--}gap dependencies, known as \textbf{island constraints}: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.",
}


@misc{futrell2018rnnspsycholinguisticsubjectssyntactic,
      title={RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency}, 
      author={Richard Futrell and Ethan Wilcox and Takashi Morita and Roger Levy},
      year={2018},
      eprint={1809.01329},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.01329}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}


@misc{linzen2016assessingabilitylstmslearn,
      title={Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies}, 
      author={Tal Linzen and Emmanuel Dupoux and Yoav Goldberg},
      year={2016},
      eprint={1611.01368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1611.01368}, 
}

@article{Pedersen2019HierarchicalGA,
  title={Hierarchical generalized additive models in ecology: an introduction with mgcv},
  author={Eric J. Pedersen and David L. Miller and Gavin L. Simpson and Noam Ross},
  journal={PeerJ},
  year={2019},
  volume={7},
  url={https://api.semanticscholar.org/CorpusID:174804005}
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}


@inproceedings{schuster2022sentence,
  title={When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it},
  author={Schuster, Sebastian and Linzen, Tal},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={969--982},
  year={2022}
}

