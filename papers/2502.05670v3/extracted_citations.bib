@article{10.1162/kamath2024,
    author = {Kamath, Gaurav and Schuster, Sebastian and Vajjala, Sowmya and Reddy, Siva},
    title = "{Scope Ambiguities in Large Language Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {738-754},
    year = {2024},
    month = {06},
    abstract = "{Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2, and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90\% in some cases).1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00670},
    url = {https://doi.org/10.1162/tacl\_a\_00670},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00670/2377773/tacl\_a\_00670.pdf},
}

@article{1909bezie,
url = {https://doi.org/10.1515/9783110242652.110},
title = {Beziehungen zwischen Umfang und Reihenfolge von Satzgliedern.},
author = {Otto Behaghel},
pages = {110--142},
volume = {25},
number = {1909},
journal = {Indogermanische Forschungen (1909)},
doi = {doi:10.1515/9783110242652.110},
year = {1909},
lastchecked = {2024-10-02}
}

@article{ARNOLD200455,
title = {Avoiding attachment ambiguities: The role of constituent ordering},
journal = {Journal of Memory and Language},
volume = {51},
number = {1},
pages = {55-70},
year = {2004},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2004.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X04000282},
author = {Jennifer E. Arnold and Thomas Wasow and Ash Asudeh and Peter Alrenga},
abstract = {Three experiments investigated whether speakers use constituent ordering as a mechanism for avoiding ambiguities. In utterances like “Jane showed the letter to Mary to her mother,” alternate orders would avoid the temporary PP-attachment ambiguity (“Jane showed her mother the letter to Mary,” or “Jane showed to her mother the letter to Mary”). A preference judgment experiment confirmed that comprehenders prefer the latter orders for dative utterances when the former order would have contained an ambiguity. Nevertheless, speakers in two on-line production experiments showed no evidence of an ambiguity avoidance strategy. In fact, they were slightly more likely to use the former order when it was ambiguous than when it was not. Speakers’ failure to disambiguate with ordering cannot be explained by the use of other ambiguity mechanisms, like prosody. A prosodic analysis of the responses in Experiment 3 showed that while speakers generally produced prosodic patterns that were consistent with the syntactic structure, these patterns would not strongly disambiguate the PP-attachment ambiguity. We suggest that speakers do not consistently disambiguate local PP-attachment ambiguities of this type, and in particular do not use constituent ordering for this purpose. Instead, constituent ordering is driven by factors like syntactic weight and lexical bias, which may be internal to the production system.}
}

@inproceedings{Fujihara2022TopicalizationIL,
  title={Topicalization in Language Models: A Case Study on Japanese},
  author={Riki Fujihara and Tatsuki Kuribayashi and Kaori Abe and Ryoko Tokuhisa and Kentaro Inui},
  booktitle={International Conference on Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252819390}
}

@book{Hawkins_1995, place={Cambridge}, series={Cambridge Studies in Linguistics}, title={A Performance Theory of Order and Constituency}, publisher={Cambridge University Press}, author={John A. Hawkins}, year={1995}, collection={Cambridge Studies in Linguistics}}

@article{Hawkins_1999, title={The relative order of prepositional phrases in English: Going beyond Manner–Place–Time}, volume={11}, DOI={10.1017/S0954394599113012}, number={3}, journal={Language Variation and Change}, author={Hawkins, John A.}, year={1999}, pages={231–266}}

@article{Manetta2012ReconsideringRS,
  title={Reconsidering Rightward Scrambling: Postverbal Constituents in Hindi-Urdu},
  author={Emily Walker Manetta},
  journal={Linguistic Inquiry},
  year={2012},
  volume={43},
  pages={43-74},
  url={https://api.semanticscholar.org/CorpusID:57569578}
}

@article{Medeiros_Mains_McGowan_2021, title={Ceiling Effects on Weight in Heavy NP Shift}, volume={52}, ISSN={0024-3892}, DOI={10.1162/ling_a_00382}, abstractNote={This squib tests theories of heavy NP shift that link constituent order to parsing. Our results indicate that increasing the weight of an object NP cannot make a heavy NP shift construction more acceptable than a comparison sentence in the canonical order, contra parsing-based theories. In addition, we examine acceptability as it relates to verb disposition. We find no significant differences between different subcategorizations, contrary to the findings of corpus studies. These results reveal a disconnect between production and comprehension; we further conclude that parser sensitivity to constituent structure is unlikely to affect speaker production of heavy NP shift.}, number={2}, journal={Linguistic Inquiry}, author={David J. Medeiros and Paul Mains and Kevin B. McGowan}, year={2021}, month=mar, pages={426–440}}

@article{WangLiu,
url = {https://doi.org/10.1515/psicl-2014-0024},
title = {The effects of length and complexity on constituent ordering in written English},
author = {Hua Wang and Haitao Liu},
pages = {477--494},
volume = {50},
number = {4},
journal = {Poznan Studies in Contemporary Linguistics},
doi = {doi:10.1515/psicl-2014-0024},
year = {2014},
lastchecked = {2024-10-02}
}

@article{Wasow_1997, title={Remarks on grammatical weight}, volume={9}, DOI={10.1017/S0954394500001800}, number={1}, journal={Language Variation and Change}, author={Thomas Wasow}, year={1997}, pages={81–105}}

@inbook{Wasow_Arnold_2003, address={Berlin, New York}, title={Post-verbal constituent ordering in English}, ISBN={978-3-11-090001-9}, url={https://doi.org/10.1515/9783110900019.119}, DOI={doi:10.1515/9783110900019.119}, booktitle={Determinants of Grammatical Variation in English}, publisher={De Gruyter Mouton}, author={Wasow, Thomas and Arnold, Jennifer}, year={2003}, pages={119–154}}

@article{Wasow_endweight, title={End-Weight from the Speaker’s Perspective}, volume={26}, ISSN={1573-6555}, DOI={10.1023/A:1025080709112}, abstractNote={Explanations of the tendency to put long, complex constituents at the ends of sentences (“end-weight”) usually take the listener’s perspective, claiming it facilitates parsing. I argue for a speaker-oriented explanation of end-weight, based on how it facilitates utterance planning. Parsing is facilitated when as much tree structure as possible can be determined early in the string, but production is easiest when options for how to continue are kept open. That is, listeners should prefer early commitment and speakers should prefer late commitment. Corpus data show that different verbs exhibit different rates of word-order variation that are systematically related to differences in subcategorization possibilities in just the way predicted by a strategy of late commitment. Thus, a speaker-based account of lexical preferences in word ordering does a better job of explaining variation in weight effects than a listener-based account.}, number={3}, journal={Journal of Psycholinguistic Research}, author={Thomas Wasow}, year={1997}, month=may, pages={347–361}}

@article{arnold_et_al,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/417392},
 abstract = {Variations in postverbal constituent ordering have been attributed to both grammatical complexity (heaviness) and discourse status (newness), although few studies compare the two factors explicitly. Through corpus analysis and experimentation, we demonstrate that both factors simultaneously and independently influence word order in two English constructions. While past investigations of these factors have focused on their effects in language comprehension, we argue that postponing heavy and new constituents facilitates processes of planning and production.},
 author = {Jennifer E. Arnold and Anthony Losongco and Thomas Wasow and Ryan Ginstrom},
 journal = {Language},
 number = {1},
 pages = {28--55},
 publisher = {Linguistic Society of America},
 title = {Heaviness vs. Newness: The Effects of Structural Complexity and Discourse Status on Constituent Ordering},
 urldate = {2024-10-02},
 volume = {76},
 year = {2000}
}

@misc{faghiri,
  TITLE = {{Relative weight and givenness in constituent ordering of typologically different languages: Evidence from French and Persian}},
  AUTHOR = {Pegah Faghiri and Juliette Thuilier},
  URL = {https://shs.hal.science/halshs-02605180},
  NOTE = {Poster},
  HOWPUBLISHED = {{The 31st Annual CUNY Conference on Human Sentence Processing}},
  YEAR = {2018},
  MONTH = Mar,
  HAL_ID = {halshs-02605180},
  HAL_VERSION = {v1},
}

@misc{futrell2018rnnslearnhumanlikeabstract,
      title={Do RNNs learn human-like abstract word order preferences?}, 
      author={Richard Futrell and Roger P. Levy},
      year={2018},
      eprint={1811.01866},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.01866}, 
}

@book{haegeman1991,
  author    = {Haegeman, Liliane},
  title     = {Introduction to Government and Binding Theory},
  year      = {1991},
  publisher = {Blackwell},
  address   = {Oxford}
}

@book{hawkins2004,
    author = {John A. Hawkins},
    title = "{Efficiency and Complexity in Grammars}",
    publisher = {Oxford University Press},
    year = {2004},
    month = {11},
    abstract = "{This book addresses a question fundamental to any discussion of grammatical theory and grammatical variation: to what extent can principles of grammar be explained through language use? The book argues that there is a profound correspondence between performance data and the fixed conventions of grammars. Preferences and patterns found in the one, the book shows, are reflected in constraints and variation patterns in the other. The theoretical consequences of the proposed ‘performance-grammar correspondence hypothesis’ are far-reaching — for current grammatical formalisms, for the innateness hypothesis, and for psycholinguistic models of performance and learning. Drawing on empirical generalizations and insights from language typology, generative grammar, psycholinguistics, and historical linguistics, this book demonstrates that the assumption that grammars are immune to performance is false. It presents detailed empirical case studies and arguments for an alternative theory in which performance has shaped the conventions of grammars and thus the variation patterns found in the world’s languages. The innateness of language, the book argues, resides primarily in the mechanisms human beings have for processing and learning it.}",
    isbn = {9780199252695},
    doi = {10.1093/acprof:oso/9780199252695.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780199252695.001.0001},
}

@article{lau2017,
  author    = {J. H. Lau and A. Clark and S. Lappin},
  title     = {Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge},
  journal   = {Cognitive Science},
  year      = {2017},
  volume    = {41},
  number    = {5},
  pages     = {1202--1241},
  doi       = {10.1111/cogs.12414},
  pmid      = {27732744},
  month     = {July},
  eprint    = {2016 Oct 12}
}

@misc{linzen2016assessingabilitylstmslearn,
      title={Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies}, 
      author={Tal Linzen and Emmanuel Dupoux and Yoav Goldberg},
      year={2016},
      eprint={1611.01368},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1611.01368}, 
}

@article{logstruct,
author = {Noam Chomsky},
year = {2008},
month = {12},
pages = {795-814},
title = {The Logical Structure of Linguistic Theory},
volume = {84},
journal = {Language},
doi = {10.1353/lan.0.0075}
}

@inproceedings{marvin-linzen-2018-targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Rebecca Marvin and
      Tal Linzen",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1151",
    doi = "10.18653/v1/D18-1151",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{quirketal,
author = {Randolph Quirk and Sidney Greenbaum and Geoffrey Leech and Jan Svartvik},
year = {1975},
month = {08},
pages = {277 - 280},
title = {Review of: A Grammar of Contemporary English, by Randolph Quirk, Sidney Greenbaum, Geoffrey Leech and Jan Svartvik},
volume = {11},
doi = {10.1017/S002222670000459X}
}

@misc{wilcox2019syntacticstructuresblockdependencies,
      title={What Syntactic Structures block Dependencies in RNN Language Models?}, 
      author={Ethan Wilcox and Roger Levy and Richard Futrell},
      year={2019},
      eprint={1905.10431},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.10431}, 
}

