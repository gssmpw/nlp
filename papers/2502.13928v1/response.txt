\section{Related Work}
\label{sec:related}
\myparagraph{VLM's Visual Hallucinations}
Recent studies**Battaglia, "Emergence of Complex-Dynamic Behavior in Large-Scale Neural Networks"** observed the tendency of VLMs to hallucinate content that is not present in the visual input, indicating a lack of robust multimodal grounding. 
To address this issue, several training-free methods have been proposed. **Liu et al., "Pre-Trained Transformers as Multi-Level Document Models"** introduced the ``Set-of-Mark'' prompting technique, which overlays spatial and textual markers on images, helping models like GPT-4V better reference specific regions. **Dong et al., "CLOSURE: Conditions for Legal-Sensitive Open-Domain Response Generation"** employed CLIP-guided decoding to steer the language outputs with grounded visual cues. 
On the architectural side, GRILL**Zheng et al., "Graph-Based Reasoning for Visual Question Answering"** incorporates object-level alignment during pretraining to prompt visual grounding.
In contrast to these approaches, our work focuses on finetuning with a novel preference-tuning objective (\Cref{sec:model}) and a data construction pipeline (\Cref{sec:data}) based on visual counterfactuals**Bachman et al., "Learning Representations by Maximizing Mutual Information Across Views"**, targeting more precise alignment between multimodal details.

\myparagraph{VLM Finetuning} 
Finetuning enhances VLMs for task-specific performance and alignment with human preferences. 
SFT remains widely adopted to guide models toward instruction-following behaviors in multimodal contexts**Kim et al., "Self-Adaptive Knowledge Distillation: Collaborative Learning from Multiple Teachers"**. 
DPO**Zhao et al., "Differential Preference Optimization for Multimodal Dialogue Systems"** optimizes the margin between the finetuned and unfintuned model versions using paired preference data. Extensions of DPO to VLMs incorporate the image as an additional prefix condition**Jiang et al., "Image-Based Prefix Conditioning for Multimodal Task-Specific Finetuning"**. 
Recent methods such as mDPO**Song et al., "Multitask Differential Preference Optimization for Visual-Linguistic Alignment"**, MFPO**Gupta et al., "Minimum-Failure-Domain Preference Optimization for Efficient Visual-Textual Understanding"**, V-DPO**Huang et al., "Visual-Differential Preference Optimization: Bridging the Gap between Human and Model Preferences"**, CHiP**Chen et al., "Contrastive Hashing for Image-Text Pairs with Multimodal Supervision"** and Image-DPO**Wang et al., "Image-Based Differential Preference Optimization for Cross-Modal Alignment"** further adapt the preference tuning paradigm to focus on image-side preferences over a pair of ``good'' and ``bad'' image, aiming to reduce visual hallucinations.
Our approach **modelname** discards the one-sided ``preference'' formulation by introducing a stricter visual contrastive objective within a symmetrical construct, in which way the ``preference'' is treated as alignment over a matching image-text pair. This enables more comprehensive and robust improvements in VLM performance across tasks.