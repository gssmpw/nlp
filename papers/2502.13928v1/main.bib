@article{imagedpo,
  title={Probing Visual Language Priors in VLMs},
  author={Luo, Tiange and Cao, Ang and Lee, Gunhee and Johnson, Justin and Lee, Honglak},
  journal={arXiv preprint arXiv:2501.00569},
  year={2024}
}

@article{chip,
  title={CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs},
  author={Fu, Jinlan and Huangfu, Shenzhen and Fei, Hao and Shen, Xiaoyu and Hooi, Bryan and Qiu, Xipeng and Ng, See-Kiong},
  journal={arXiv preprint arXiv:2501.16629},
  year={2025}
}


@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@InProceedings{MAEprepre,
    author    = {Singh, Mannat and Duval, Quentin and Alwala, Kalyan Vasudev and Fan, Haoqi and Aggarwal, Vaibhav and Adcock, Aaron and Joulin, Armand and Dollar, Piotr and Feichtenhofer, Christoph and Girshick, Ross and Girdhar, Rohit and Misra, Ishan},
    title     = {The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {5484-5494}
}

@article{mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@inproceedings{mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@inproceedings{textvqa,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@inproceedings{ai2d,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@misc{realworldqa,
  title = {Grok-1.5 Vision Preview},
  author = {{xAI}},
  year = {2024},
  month = {April},
  url = {https://x.ai/blog/grok-1.5v},
  note = {Accessed: 2024-12-12}
}


@article{sqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@article{dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

@article{dalle3,
  title={Improving Image Generation with Better Captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
  url={https://cdn.openai.com/papers/dall-e-3.pdf}
}

@inproceedings{gligen,
  title={Gligen: Open-set grounded text-to-image generation},
  author={Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22511--22521},
  year={2023}
}

@inproceedings{powerpaint,
  title={A task is worth one word: Learning with task prompts for high-quality versatile image inpainting},
  author={Zhuang, Junhao and Zeng, Yanhong and Liu, Wenran and Yuan, Chun and Chen, Kai},
  booktitle={European Conference on Computer Vision},
  pages={195--211},
  year={2025},
  organization={Springer}
}


@article{d3,
  title={Detect, Describe, Discriminate: Moving Beyond VQA for MLLM Evaluation},
  author={Gaur, Manu and Tapaswi, Makarand and others},
  journal={arXiv preprint arXiv:2409.15125},
  year={2024}
}

@article{dpa,
  title={Mitigating Object Hallucination via Data Augmented Contrastive Tuning},
  author={Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Ar{\i}k, Sercan {\"O} and Pfister, Tomas},
  journal={arXiv preprint arXiv:2405.18654},
  year={2024}
}



@inproceedings{rlhfv,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13807--13816},
  year={2024}
}

@article{povid,
  title={Aligning modalities in vision large language models via preference fine-tuning},
  author={Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2402.11411},
  year={2024}
}

@article{silkie,
  title={Silkie: Preference distillation for large visual language models},
  author={Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2312.10665},
  year={2023}
}

@article{finecopsref,
  title={FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension},
  author={Liu, Junzhuo and Yang, Xuzheng and Li, Weiwei and Wang, Peng},
  journal={arXiv preprint arXiv:2409.14750},
  year={2024}
}

@article{mfpo,
  title={Modality-Fair Preference Optimization for Trustworthy MLLM Alignment},
  author={Jiang, Songtao and Zhang, Yan and Chen, Ruizhe and Jin, Yeying and Liu, Zuozhu},
  journal={arXiv preprint arXiv:2410.15334},
  year={2024}
}

@article{countercurate,
  title={CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples},
  author={Zhang, Jianrui and Cai, Mu and Xie, Tengyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2402.13254},
  year={2024}
}

@article{chen2024quantifying,
  title={Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective},
  author={Chen, Meiqi and Cao, Yixin and Zhang, Yan and Lu, Chaochao},
  journal={arXiv preprint arXiv:2403.18346},
  year={2024}
}

@inproceedings{mmvp,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9568--9578},
  year={2024}
}

@article{molmo,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}

@article{cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{franken2024self,
  title={Self-supervised alignment with mutual information: Learning to follow principles without preference labels},
  author={Fr{\"a}nken, Jan-Philipp and Zelikman, Eric and Rafailov, Rafael and Gandhi, Kanishk and Gerstenberg, Tobias and Goodman, Noah D},
  journal={arXiv preprint arXiv:2404.14313},
  year={2024}
}

@inproceedings{llava1.5,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@misc{llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@article{llavaonevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{vdpo,
  title={V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization},
  author={Xie, Yuxi and Li, Guanzhen and Xu, Xiao and Kan, Min-Yen},
  journal={arXiv preprint arXiv:2411.02712},
  year={2024}
}

@article{llavanextinterleave,
  title={Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models},
  author={Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan},
  journal={arXiv preprint arXiv:2407.07895},
  year={2024}
}

@article{mdpo,
  title={mDPO: Conditional Preference Optimization for Multimodal Large Language Models},
  author={Wang, Fei and Zhou, Wenxuan and Huang, James Y and Xu, Nan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2406.11839},
  year={2024}
}

@article{symdpo,
  title={SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization},
  author={Jia, Hongrui and Jiang, Chaoya and Xu, Haiyang and Ye, Wei and Dong, Mengfan and Yan, Ming and Zhang, Ji and Huang, Fei and Zhang, Shikun},
  journal={arXiv preprint arXiv:2411.11909},
  year={2024}
}

@inproceedings{robinson2023leveraging,
  title={Leveraging Large Language Models for Multiple Choice Question Answering},
  author={Robinson, Joshua and Wingate, David},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{zhao2023pytorch,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{thomee2016yfcc100m,
  title={Yfcc100m: The new data in multimedia research},
  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={64--73},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@misc{deepspeed,
  author = {Microsoft},
  title = {DeepSpeed: A Deep Learning Optimization Library},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/microsoft/DeepSpeed}}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	year={2019}
}

@inproceedings{yang2019xlnet,
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
	volume = {32},
	year = {2019}
}

@book{gut2013probability,
	title={Probability: a graduate course},
	author={Gut, Allan},
	volume={75},
	year={2013},
	publisher={Springer Science \& Business Media}
}

% vim:ts=4:sw=4
@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{bara2011cognitive,
  title={Cognitive pragmatics: The mental processes of communication},
  author={Bara, Bruno G},
  year={2011},
  publisher={Walter de Gruyter GmbH \& Co. KG}
}

@article{Bradley1952,
  title = {Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
  volume = {39},
  ISSN = {0006-3444},
  url = {http://dx.doi.org/10.2307/2334029},
  DOI = {10.2307/2334029},
  number = {3/4},
  journal = {Biometrika},
  publisher = {JSTOR},
  author = {Bradley,  Ralph Allan and Terry,  Milton E.},
  year = {1952},
  month = dec,
  pages = {324}
}

@book{austin1962how,
  added-at = {2013-08-04T14:36:29.000+0200},
  author = {Austin, John Langshaw},
  biburl = {https://www.bibsonomy.org/bibtex/24d84385e4dd3d9b870d7fec2d8afd197/porta},
  file = {austin1962how.pdf:austin1962how.pdf:PDF},
  groups = {public},
  interhash = {63fdb4ecfc0f5cf8546b4547a0bbdc68},
  intrahash = {4d84385e4dd3d9b870d7fec2d8afd197},
  keywords = {act dialogue speech theory},
  publisher = {Oxford University Press},
  series = {William James Lectures},
  timestamp = {2013-08-09T11:53:22.000+0200},
  title = {How to do things with words},
  url = {http://scholar.google.de/scholar.bib?q=info:xI2JvixH8_QJ:scholar.google.com/&output=citation&hl=de&as_sdt=0,5&ct=citation&cd=1},
  username = {porta},
  year = 1962
}


@article{Corona2019ModelingCU,
  title={Modeling Conceptual Understanding in Image Reference Games},
  author={Rodolfo Corona and Stephan Alaniz and Zeynep Akata},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.04872},
  url={https://api.semanticscholar.org/CorpusID:202782284}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{wu2023diffuvst,
  title={DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models},
  author={Wu, Shengguang and Yuan, Mei and Su, Qi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1885--1896},
  year={2023}
}

@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@online{rmsdrop,
  author       = {Geoffrey Hinton},
  title        = {Coursera Lecture Slides - Neural Networks for Machine Learning lecture 6},
  year         = {2014},
  url          = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@article{dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2023pandalm,
  title={Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization},
  author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others},
  journal={arXiv preprint arXiv:2306.05087},
  year={2023}
}

@inproceedings{hu2023prompting,
  title={Prompting is not a substitute for probability measurements in large language models},
  author={Hu, Jennifer and Levy, Roger},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5040--5060},
  year={2023}
}

@inproceedings{
carenini2023towards,
title={Towards a better Rational Speech Act framework for context-aware modeling of metaphor understanding},
author={Gaia Carenini and Luca Bischetti and Walter Schaeken and Valentina Bambini},
booktitle={First Workshop on Theory of Mind in Communicating Agents},
year={2023},
url={https://openreview.net/forum?id=x4YpVxafEc}
}


@misc{liu2023computational,
      title={Computational Language Acquisition with Theory of Mind}, 
      author={Andy Liu and Hao Zhu and Emmy Liu and Yonatan Bisk and Graham Neubig},
      year={2023},
      eprint={2303.01502},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sap2019socialiqa,
  title="{SocialIQA}: Commonsense Reasoning about Social Interactions",
  author="Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi",
  year="2019",
  booktitle="EMNLP"
}

@misc{xu2024opentom,
      title={OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models}, 
      author={Hainiu Xu and Runcong Zhao and Lixing Zhu and Jinhua Du and Yulan He},
      year={2024},
      eprint={2402.06044},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{fried2023pragmatics,
      title={Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches}, 
      author={Daniel Fried and Nicholas Tomlin and Jennifer Hu and Roma Patel and Aida Nematzadeh},
      year={2023},
      eprint={2211.08371},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{qi-etal-2023-pragmaticqa,
    title = "{P}ragmati{CQA}: A Dataset for Pragmatic Question Answering in Conversations",
    author = "Qi, Peng  and
      Du, Nina  and
      Manning, Christopher  and
      Huang, Jing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.385",
    doi = "10.18653/v1/2023.findings-acl.385",
    pages = "6175--6191",
    abstract = "Pragmatic reasoning about another speaker{'}s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks {``}do you have a minute?{''}, instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics. We designed innovative crowdsourcing mechanisms for interest-based and task-driven data collection to address the common issue of incentive misalignment between crowdworkers and potential users. To compare computational models{'} capability at pragmatic reasoning, we also propose several quantitative metrics to evaluate question answering systems on PragmatiCQA. We find that state-of-the-art systems still struggle to perform human-like pragmatic reasoning, and highlight their limitations for future research.",
}

@misc{sravanthi2024pub,
      title={PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities}, 
      author={Settaluri Lakshmi Sravanthi and Meet Doshi and Tankala Pavan Kalyan and Rudra Murthy and Pushpak Bhattacharyya and Raj Dabre},
      year={2024},
      eprint={2401.07078},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{text-davinci-003,
  author = {OpenAI},
  title = {Text-DaVinci-003},
  year = {2022},
  howpublished = {OpenAI API},
  url = {https://beta.openai.com}
}


@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{roller2020recipes,
  title={Recipes for building an open-domain chatbot},
  author={Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M and others},
  journal={arXiv preprint arXiv:2004.13637},
  year={2020}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{ruis2023goldilocks,
      title={The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs}, 
      author={Laura Ruis and Akbir Khan and Stella Biderman and Sara Hooker and Tim Rocktäschel and Edward Grefenstette},
      year={2023},
      eprint={2210.14986},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hu2023finegrained,
      title={A fine-grained comparison of pragmatic language understanding in humans and language models}, 
      author={Jennifer Hu and Sammy Floyd and Olessia Jouravlev and Evelina Fedorenko and Edward Gibson},
      year={2023},
      eprint={2212.06801},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2023define,
      title={Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models}, 
      author={Lingjun Zhao and Khanh Nguyen and Hal Daumé III au2},
      year={2023},
      eprint={2301.05149},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hoyle2023natural,
      title={Natural Language Decompositions of Implicit Content Enable Better Text Representations}, 
      author={Alexander Hoyle and Rupak Sarkar and Pranav Goel and Philip Resnik},
      year={2023},
      eprint={2305.14583},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gabriel2022misinfo,
      title={Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines}, 
      author={Saadia Gabriel and Skyler Hallinan and Maarten Sap and Pemi Nguyen and Franziska Roesner and Eunsol Choi and Yejin Choi},
      year={2022},
      eprint={2104.08790},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ravfogel2023retrieving,
      title={Retrieving Texts based on Abstract Descriptions}, 
      author={Shauli Ravfogel and Valentina Pyatkin and Amir DN Cohen and Avshalom Manevich and Yoav Goldberg},
      year={2023},
      eprint={2305.12517},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sieker-zarriess-2023-language,
    title = "When Your Language Model Cannot {E}ven Do Determiners Right: Probing for Anti-Presuppositions and the Maximize Presupposition! Principle",
    author = "Sieker, Judith  and
      Zarrie{\ss}, Sina",
    editor = "Belinkov, Yonatan  and
      Hao, Sophie  and
      Jumelet, Jaap  and
      Kim, Najoung  and
      McCarthy, Arya  and
      Mohebbi, Hosein",
    booktitle = "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.blackboxnlp-1.14",
    doi = "10.18653/v1/2023.blackboxnlp-1.14",
    pages = "180--198",
    abstract = "The increasing interest in probing the linguistic capabilities of large language models (LLMs) has long reached the area of semantics and pragmatics, including the phenomenon of presuppositions. In this study, we investigate a phenomenon that, however, has not yet been investigated, i.e., the phenomenon of anti-presupposition and the principle that accounts for it, the Maximize Presupposition! principle (MP!). Through an experimental investigation using psycholinguistic data and four open-source BERT model variants, we explore how language models handle different anti-presuppositions and whether they apply the MP! principle in their predictions. Further, we examine whether fine-tuning with Natural Language Inference data impacts adherence to the MP! principle. Our findings reveal that LLMs tend to replicate context-based n-grams rather than follow the MP! principle, with fine-tuning not enhancing their adherence. Notably, our results further indicate a striking difficulty of LLMs to correctly predict determiners, in relatively simple linguistic contexts.",
}

@misc{liu2024large,
      title={How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?}, 
      author={Ryan Liu and Theodore R. Sumers and Ishita Dasgupta and Thomas L. Griffiths},
      year={2024},
      eprint={2402.07282},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{saha2023language,
      title={Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization}, 
      author={Swarnadeep Saha and Peter Hase and Mohit Bansal},
      year={2023},
      eprint={2306.09299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{buschoff2024visual,
      title={Visual cognition in multimodal large language models}, 
      author={Luca M. Schulze Buschoff and Elif Akata and Matthias Bethge and Eric Schulz},
      year={2024},
      eprint={2311.16093},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hase-etal-2023-methods,
    title = "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
    author = "Hase, Peter  and
      Diab, Mona  and
      Celikyilmaz, Asli  and
      Li, Xian  and
      Kozareva, Zornitsa  and
      Stoyanov, Veselin  and
      Bansal, Mohit  and
      Iyer, Srinivasan",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.199",
    doi = "10.18653/v1/2023.eacl-main.199",
    pages = "2714--2731",
    abstract = "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
}

@misc{gandhi2023understanding,
      title={Understanding Social Reasoning in Language Models with Language Models}, 
      author={Kanishk Gandhi and Jan-Philipp Fränken and Tobias Gerstenberg and Noah D. Goodman},
      year={2023},
      eprint={2306.15448},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{leer2023violation,
      title={Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models}, 
      author={Courtland Leer and Vincent Trost and Vineeth Voruganti},
      year={2023},
      eprint={2310.06983},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{le2019revisiting,
  title={Revisiting the evaluation of theory of mind through question answering},
  author={Le, Matthew and Boureau, Y-Lan and Nickel, Maximilian},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5872--5877},
  year={2019}
}

@misc{alpaca_eval_2_lc_notebook,
  author = {Yann Dubois},
  title = {Length controlled AlpacaEval},
  year = {2024},
  url = {https://github.com/tatsu-lab/alpaca_eval/blob/main/notebooks/length_controlled.ipynb},
}

@misc{pragmega,
  author = {Sammy Floyd},
  title = {PragMega Materials},
  year = {2022},
  url = {https://osf.io/6abgk/?view_only=42d448e3d0b14ecf8b87908b3a618672},
}


@misc{zhou2023far,
      title={How FaR Are Large Language Models From Agents with Theory-of-Mind?}, 
      author={Pei Zhou and Aman Madaan and Srividya Pranavi Potharaju and Aditya Gupta and Kevin R. McKee and Ari Holtzman and Jay Pujara and Xiang Ren and Swaroop Mishra and Aida Nematzadeh and Shyam Upadhyay and Manaal Faruqui},
      year={2023},
      eprint={2310.03051},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qiu2023minddial,
      title={MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation}, 
      author={Shuwen Qiu and Song-Chun Zhu and Zilong Zheng},
      year={2023},
      eprint={2306.15253},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{qiu-etal-2022-towards,
    title = "Towards Socially Intelligent Agents with Mental State Transition and Human Value",
    author = "Qiu, Liang  and
      Zhao, Yizhou  and
      Liang, Yuan  and
      Lu, Pan  and
      Shi, Weiyan  and
      Yu, Zhou  and
      Zhu, Song-Chun",
    editor = "Lemon, Oliver  and
      Hakkani-Tur, Dilek  and
      Li, Junyi Jessy  and
      Ashrafzadeh, Arash  and
      Garcia, Daniel Hern{\'a}ndez  and
      Alikhani, Malihe  and
      Vandyke, David  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
    booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = sep,
    year = "2022",
    address = "Edinburgh, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigdial-1.16",
    doi = "10.18653/v1/2022.sigdial-1.16",
    pages = "146--158",
    abstract = "Building a socially intelligent agent involves many challenges. One of which is to track the agent{'}s mental state transition and teach the agent to make decisions guided by its value like a human. Towards this end, we propose to incorporate mental state simulation and value modeling into dialogue agents. First, we build a hybrid mental state parser that extracts information from both the dialogue and event observations and maintains a graphical representation of the agent{'}s mind; Meanwhile, the transformer-based value model learns human preferences from the human value dataset, ValueNet. Empirical results show that the proposed model attains state-of-the-art performance on the dialogue/action/emotion prediction task in the fantasy text-adventure game dataset, LIGHT. We also show example cases to demonstrate: (i) how the proposed mental state parser can assist the agent{'}s decision by grounding on the context like locations and objects, and (ii) how the value model can help the agent make decisions based on its personal priorities.",
}

@article{moghaddam2023boosting,
  title={Boosting theory-of-mind performance in large language models via prompting},
  author={Moghaddam, Shima Rahimi and Honey, Christopher J},
  journal={arXiv preprint arXiv:2304.11490},
  year={2023}
}

@misc{sclar2023minding,
      title={Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker}, 
      author={Melanie Sclar and Sachin Kumar and Peter West and Alane Suhr and Yejin Choi and Yulia Tsvetkov},
      year={2023},
      eprint={2306.00924},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2021fewshot,
      title={Few-shot Language Coordination by Modeling Theory of Mind}, 
      author={Hao Zhu and Graham Neubig and Yonatan Bisk},
      year={2021},
      eprint={2107.05697},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{takmaz2023speaking,
      title={Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind}, 
      author={Ece Takmaz and Nicolo' Brandizzi and Mario Giulianelli and Sandro Pezzelle and Raquel Fernández},
      year={2023},
      eprint={2305.19933},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{hessel-etal-2021-clipscore,
    title = "{CLIPS}core: A Reference-free Evaluation Metric for Image Captioning",
    author = "Hessel, Jack  and
      Holtzman, Ari  and
      Forbes, Maxwell  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.595",
    doi = "10.18653/v1/2021.emnlp-main.595",
    pages = "7514--7528",
    abstract = "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",
}

@misc{bao2022learning,
      title={Learning to Mediate Disparities Towards Pragmatic Communication}, 
      author={Yuwei Bao and Sayan Ghosh and Joyce Chai},
      year={2022},
      eprint={2203.13685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{singh2023know,
      title={Know your audience: specializing grounded language models with listener subtraction}, 
      author={Aaditya K. Singh and David Ding and Andrew Saxe and Felix Hill and Andrew K. Lampinen},
      year={2023},
      eprint={2206.08349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Bara_2021,
   title={MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks},
   url={http://dx.doi.org/10.18653/v1/2021.emnlp-main.85},
   DOI={10.18653/v1/2021.emnlp-main.85},
   booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
   publisher={Association for Computational Linguistics},
   author={Bara, Cristian-Paul and CH-Wang, Sky and Chai, Joyce},
   year={2021} }


@misc{jin2024mmtomqa,
      title={MMToM-QA: Multimodal Theory of Mind Question Answering}, 
      author={Chuanyang Jin and Yutong Wu and Jing Cao and Jiannan Xiang and Yen-Ling Kuo and Zhiting Hu and Tomer Ullman and Antonio Torralba and Joshua B. Tenenbaum and Tianmin Shu},
      year={2024},
      eprint={2401.08743},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@inproceedings{jones2023epitome,
title={{EPITOME}: Experimental Protocol Inventory for Theory Of Mind Evaluation},
author={Cameron Robert Jones and Sean Trott and Ben Bergen},
booktitle={First Workshop on Theory of Mind in Communicating Agents},
year={2023},
url={https://openreview.net/forum?id=e5Yky8Fnvj}
}

@misc{shapira2023clever,
      title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models}, 
      author={Natalie Shapira and Mosh Levy and Seyed Hossein Alavi and Xuhui Zhou and Yejin Choi and Yoav Goldberg and Maarten Sap and Vered Shwartz},
      year={2023},
      eprint={2305.14763},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kim2023fantom,
      title={FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions}, 
      author={Hyunwoo Kim and Melanie Sclar and Xuhui Zhou and Ronan Le Bras and Gunhee Kim and Yejin Choi and Maarten Sap},
      year={2023},
      eprint={2310.15421},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kosinski2024evaluating,
      title={Evaluating Large Language Models in Theory of Mind Tasks}, 
      author={Michal Kosinski},
      year={2024},
      eprint={2302.02083},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt-3.5-turbo,
  author = {OpenAI},
  title = {ChatGPT: GPT-3.5 Turbo},
  year = {2022},
  howpublished = {OpenAI API},
  url = {https://chat.openai.com}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{ullman2023large,
      title={Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks}, 
      author={Tomer Ullman},
      year={2023},
      eprint={2302.08399},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{sap2023neural,
      title={Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs}, 
      author={Maarten Sap and Ronan LeBras and Daniel Fried and Yejin Choi},
      year={2023},
      eprint={2210.13312},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jamali2023unveiling,
      title={Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain}, 
      author={Mohsen Jamali and Ziv M. Williams and Jing Cai},
      year={2023},
      eprint={2309.01660},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{mahowald2024dissociating,
      title={Dissociating language and thought in large language models}, 
      author={Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
      year={2024},
      eprint={2301.06627},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ma2023holistic,
      title={Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models}, 
      author={Ziqiao Ma and Jacob Sansom and Run Peng and Joyce Chai},
      year={2023},
      eprint={2310.19619},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luketina2019survey,
      title={A Survey of Reinforcement Learning Informed by Natural Language}, 
      author={Jelena Luketina and Nantas Nardelli and Gregory Farquhar and Jakob Foerster and Jacob Andreas and Edward Grefenstette and Shimon Whiteson and Tim Rocktäschel},
      year={2019},
      eprint={1906.03926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kosinski2023theory,
  title={Theory of mind might have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023}
}

@misc{dolly,
  author = {Databricks},
  title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {Blog post},
  url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}
}

@misc{sharegpt,
  author = {},
  title = {ShareGPT},
  url = {https://sharegpt.com/},
}

@article{longpre2023flan,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@misc{köpf2023openassistant,
      title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, 
      author={Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
      year={2023},
      eprint={2304.07327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{levinson1983pragmatics,
  title={Pragmatics},
  author={Levinson, Stephen C},
  year={1983},
  publisher={Cambridge university press}
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@book{wittgenstein2010philosophical,
  title={Philosophical Investigations},
  author={Wittgenstein, L. and Hacker, P.M.S. and Schulte, J.},
  isbn={9781444307979},
  lccn={2009023572},
  url={https://books.google.com/books?id=XN9yyyhYMDoC},
  year={2010},
  publisher={Wiley}
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Vigliocco2014LanguageAA,
  title={Language as a multimodal phenomenon: implications for language learning, processing and evolution},
  author={Gabriella Vigliocco and Pamela Perniss and David P. Vinson},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  year={2014},
  volume={369},
  url={https://api.semanticscholar.org/CorpusID:941544}
}


@book{Clark1996, place={Cambridge}, series={“Using” Linguistic Books}, title={Using Language}, publisher={Cambridge University Press}, author={Clark, Herbert H.}, year={1996}, collection={“Using” Linguistic Books}} <div></div>

@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{eelen2014critique,
  title={A Critique of Politeness Theory: Volume 1},
  author={Eelen, Gino},
  year={2014},
  publisher={Routledge}
}

@book{yule1996pragmatics,
  title={Pragmatics},
  author={Yule, George},
  year={1996},
  publisher={Oxford university press}
}

@book{halliday2014cohesion,
  title={Cohesion in english},
  author={Halliday, Michael Alexander Kirkwood and Hasan, Ruqaiya},
  year={2014},
  publisher={Routledge}
}

@book{lakoff2008metaphors,
  title={Metaphors we live by},
  author={Lakoff, George and Johnson, Mark},
  year={2008},
  publisher={University of Chicago press}
}

@book{attardo2009linguistic,
  title={Linguistic theories of humor},
  author={Attardo, Salvatore},
  year={2009},
  publisher={Walter de Gruyter}
}

@misc{lazaridou2020emergent,
      title={Emergent Multi-Agent Communication in the Deep Learning Era}, 
      author={Angeliki Lazaridou and Marco Baroni},
      year={2020},
      eprint={2006.02419},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gibbs2000irony,
  title={Irony in talk among friends},
  author={Gibbs, Raymond W},
  journal={Metaphor and symbol},
  volume={15},
  number={1-2},
  pages={5--27},
  year={2000},
  publisher={Taylor \& Francis}
}


@incollection{baldwin2014understanding,
  title={Understanding the link between joint attention and language},
  author={Baldwin, Dare A},
  booktitle={Joint attention},
  pages={131--158},
  year={2014},
  publisher={Psychology Press}
}

@misc{andreas2022language,
      title={Language Models as Agent Models}, 
      author={Jacob Andreas},
      year={2022},
      eprint={2212.01681},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Potts2012GoalDrivenAI,
  title={Goal-Driven Answers in the CardsDialogue Corpus},
  author={Christopher Potts},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:17825221}
}

@misc{bisk2020experience,
      title={Experience Grounds Language}, 
      author={Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph Turian},
      year={2020},
      eprint={2004.10151},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lipkin2023evaluating,
  title={Evaluating statistical language models as pragmatic reasoners},
  author={Lipkin, Benjamin and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2305.01020},
  year={2023}
}

@book{brown1987politeness,
  title={Politeness: Some universals in language usage},
  author={Brown, Penelope and Levinson, Stephen C},
  volume={4},
  year={1987},
  publisher={Cambridge university press}
}

@misc{thrush2022winoground,
      title={Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality}, 
      author={Tristan Thrush and Ryan Jiang and Max Bartolo and Amanpreet Singh and Adina Williams and Douwe Kiela and Candace Ross},
      year={2022},
      eprint={2204.03162},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{rassin2022dalle2,
      title={DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models}, 
      author={Royi Rassin and Shauli Ravfogel and Yoav Goldberg},
      year={2022},
      eprint={2210.10606},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{potts2019case,
  title={A case for deep learning in semantics: Response to Pater},
  author={Potts, Christopher},
  journal={Language},
  volume={95},
  number={1},
  pages={e115--e124},
  year={2019},
  publisher={Linguistic Society of America}
}

@inbook{Noveck2007,
author = {Noveck, Ira and Sperber, Dan},
year = {2007},
month = {01},
pages = {},
title = {The why and how of experimental pragmatics: The case of 'scalar inferences'},
isbn = {9780521766777},
doi = {10.1017/CBO9781139028370.018}
}

@article{FélixBrasdefer2015,
url = {https://doi.org/10.1515/multi-2014-1017},
title = {Istvan Kecskes: Intercultural pragmatics},
title = {},
author = {J. César Félix-Brasdefer},
pages = {611--615},
volume = {34},
number = {4},
journal = {Multilingua},
doi = {doi:10.1515/multi-2014-1017},
year = {2015},
lastchecked = {2024-04-19}
}


@book{Wierzbicka1991,
url = {https://doi.org/10.1515/9783112329764},
title = {Cross-Cultural Pragmatics},
title = {The Semantics of Human Interaction},
author = {Anna Wierzbicka},
editor = {Werner Winter},
publisher = {De Gruyter Mouton},
address = {Berlin, Boston},
doi = {doi:10.1515/9783112329764},
isbn = {9783112329764},
year = {1991},
lastchecked = {2024-04-19}
}


@book{tannen2013discourse,
  title={Discourse 2.0: Language and new media},
  author={Tannen, Deborah and Trester, Anna Marie},
  year={2013},
  publisher={Georgetown University Press}
}

@book{seargeant2014language,
  title={The language of social media: Identity and community on the internet},
  author={Seargeant, Philip and Tagg, Caroline},
  year={2014},
  publisher={Springer}
}


@book{brown1987politeness,
  title={Politeness: Some universals in language usage},
  author={Brown, Penelope and Levinson, Stephen C},
  volume={4},
  year={1987},
  publisher={Cambridge university press}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@incollection{searle1975indirect,
  title={Indirect speech acts},
  author={Searle, John R},
  booktitle={Speech acts},
  pages={59--82},
  year={1975},
  publisher={Brill}
}

@incollection{grice1975logic,
  title={Logic and conversation},
  author={Grice, Herbert P},
  booktitle={Speech acts},
  pages={41--58},
  year={1975},
  publisher={Brill}
}

@book{sperber1986relevance,
  title={Relevance: Communication and cognition},
  author={Sperber, Dan and Wilson, Deirdre},
  volume={142},
  year={1986},
  publisher={Harvard University Press Cambridge, MA}
}

@book{levinson2000presumptive,
  title={Presumptive meanings: The theory of generalized conversational implicature},
  author={Levinson, Stephen C},
  year={2000},
  publisher={MIT press}
}

@book{mey,
author = {Mey, Jacob},
year = {2001},
month = {01},
pages = {},
title = {Pragmatics: An Introduction}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{
burkholz2024batch,
title={Batch normalization is sufficient for universal function approximation in {CNN}s},
author={Rebekka Burkholz},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=wOSYMHfENq}
}

@article{giannou2023expressive,
  title={The expressive power of tuning only the normalization layers},
  author={Giannou, Angeliki and Rajput, Shashank and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2302.07937},
  year={2023}
}

@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@book{firth1957synopsis,
  title={A Synopsis of Linguistic Theory, 1930-1955},
  author={Firth, J.R.},
  url={https://books.google.com/books?id=T8LDtgAACAAJ},
  year={1957}
}


@article{koechlin2003architecture,
  title={The architecture of cognitive control in the human prefrontal cortex},
  author={Koechlin, Etienne and Ody, Chrystele and Kouneiher, Fr{\'e}d{\'e}rique},
  journal={Science},
  volume={302},
  number={5648},
  pages={1181--1185},
  year={2003},
  publisher={American Association for the Advancement of Science}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@article{xu2019understanding,
  title={Understanding and improving layer normalization},
  author={Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={782--791},
  year={2021}
}

@article{premack1978does,
  title={Does the chimpanzee have a theory of mind?},
  author={Premack, David and Woodruff, Guy},
  journal={Behavioral and brain sciences},
  volume={1},
  number={4},
  pages={515--526},
  year={1978},
  publisher={Cambridge University Press}
}



@article{green1998direct,
  title={Direct reference and implicature},
  author={Green, Mitchell S},
  journal={Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition},
  volume={91},
  number={1},
  pages={61--90},
  year={1998},
  publisher={JSTOR}
}

@article{carston2004stephen,
  title={Stephen C. Levinson, Presumptive meanings: the theory of generalized conversational implicature. Cambridge, MA: MIT Press, 2000. Pp. xxiii+ 480.},
  author={Carston, Robyn},
  journal={Journal of linguistics},
  volume={40},
  number={1},
  pages={181--186},
  year={2004},
  publisher={Cambridge University Press}
}


@inproceedings{Horn1972OnTS,
  title={On the semantic properties of logical operators in english' reproduced by the indiana university lin},
  author={Laurence R. Horn},
  year={1972},
  url={https://api.semanticscholar.org/CorpusID:59844243}
}

@article{goffman1959moral,
  title={The moral career of the mental patient},
  author={Goffman, Erving},
  journal={Psychiatry},
  volume={22},
  number={2},
  pages={123--142},
  year={1959},
  publisher={Taylor \& Francis}
}


@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}


@misc{dubois2024lengthcontrolled,
      title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators}, 
      author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2404.04475},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

Balazs Galambosi. Advanced length-normalized alpacaeval 2.0, 2024. 
@misc{alpaca_eval_length_norm,
  author       = {Balazs Galambosi},
  title        = {Advanced length-normalized alpacaeval 2.0},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval/issues/225}},
  year         = 2024,
}


@misc{dubois2023alpacafarm,
  title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year={2023},
  eprint={2305.14387},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@misc{shridhar2023distilling,
      title={Distilling Reasoning Capabilities into Smaller Language Models}, 
      author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
      year={2023},
      eprint={2212.00193},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2023far,
      title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, 
      author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.04751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}


@article{dai2024instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@article{schuhmann2021laion,
  title={Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
  author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:2111.02114},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}


@article{santurkar2018does,
  title={How does batch normalization help optimization?},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{gadre2024datacomp,
  title={Datacomp: In search of the next generation of multimodal datasets},
  author={Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}



@article{kupcsik2018learning,
  title={Learning dynamic robot-to-human object handover from human feedback},
  author={Kupcsik, Andras and Hsu, David and Lee, Wee Sun},
  journal={Robotics Research: Volume 1},
  pages={161--176},
  year={2018},
  publisher={Springer}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.463",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{openchat,
  title = {{OpenChat: Advancing Open-source Language Models with Imperfect Data}},
  author = {Wang, Guan and Cheng, Sijie and Yu, Qiying and Liu, Changling},
  doi = {10.5281/zenodo.8105775},
  url = {https://github.com/imoneoi/openchat},
  version = {pre-release},
  year = {2023},
  month = {7},
}

@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{xu2023baize,
  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},
  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  journal={arXiv preprint arXiv:2304.01196},
  year={2023}
}

@inproceedings{yuan-etal-2023-exploring,
    title = "Exploring Partial Knowledge Base Inference in Biomedical Entity Linking",
    author = "Yuan, Hongyi  and
      Lu, Keming  and
      Yuan, Zheng",
    booktitle = "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bionlp-1.3",
    doi = "10.18653/v1/2023.bionlp-1.3",
    pages = "37--49",
    abstract = "Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference; training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop.Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with little computational overhead.",
}


@misc{instructgpt,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bai2022constitutional,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2024iterative,
  title={Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level},
  author={Liu, Jie and Zhou, Zhanhui and Liu, Jiaheng and Bu, Xingyuan and Yang, Chao and Zhong, Han-Sen and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.11817},
  year={2024}
}

@article{lu2024eliminating,
  title={Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence},
  author={Lu, Junru and Li, Jiazheng and An, Siyu and Zhao, Meng and He, Yulan and Yin, Di and Sun, Xing},
  journal={arXiv preprint arXiv:2406.10957},
  year={2024}
}

@misc{llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{fu2023specializing,
      title={Specializing Smaller Language Models towards Multi-Step Reasoning}, 
      author={Yao Fu and Hao Peng and Litu Ou and Ashish Sabharwal and Tushar Khot},
      year={2023},
      eprint={2301.12726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{song2023preference,
      title={Preference Ranking Optimization for Human Alignment}, 
      author={Feifan Song and Bowen Yu and Minghao Li and Haiyang Yu and Fei Huang and Yongbin Li and Houfeng Wang},
      year={2023},
      eprint={2306.17492},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yuan2023rrhf,
      title={RRHF: Rank Responses to Align Language Models with Human Feedback without tears}, 
      author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
      year={2023},
      eprint={2304.05302},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{holtzman2021surface,
  title={Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right},
  author={Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7038--7051},
  year={2021}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}



@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}



@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{longpre2023flan,
      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, 
      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
      year={2023},
      eprint={2301.13688},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{wang2023selfinstruct,
      title={Self-Instruct: Aligning Language Models with Self-Generated Instructions}, 
      author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
      year={2023},
      eprint={2212.10560},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@misc{2023openassistant,
      title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, 
      author={Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
      year={2023},
      eprint={2304.07327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, 
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2023aligning,
  title={Aligning Large Language Models with Human: A Survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@article{wang2021phrase,
  title={Phrase-bert: Improved phrase embeddings from bert with an application to corpus exploration},
  author={Wang, Shufan and Thompson, Laure and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2109.06304},
  year={2021}
}

@article{lee2020learning,
  title={Learning dense representations of phrases at scale},
  author={Lee, Jinhyuk and Sung, Mujeen and Kang, Jaewoo and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.12624},
  year={2020}
}

@article{hahsler2019dbscan,
  title={dbscan: Fast density-based clustering with R},
  author={Hahsler, Michael and Piekenbrock, Matthew and Doran, Derek},
  journal={Journal of Statistical Software},
  volume={91},
  pages={1--30},
  year={2019}
}

@article{han2000mining,
  title={Mining frequent patterns without candidate generation},
  author={Han, Jiawei and Pei, Jian and Yin, Yiwen},
  journal={ACM sigmod record},
  volume={29},
  number={2},
  pages={1--12},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations}
}

@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{honovich2022unnatural,
  title={Unnatural instructions: Tuning language models with (almost) no human labor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022}
}



@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{
  dmcc,
  author = {Yujia Li  and David Choi  and Junyoung Chung  and Nate Kushman  and Julian Schrittwieser  and R{\'e}mi Leblond  and Tom Eccles  and James Keeling  and Felix Gimeno  and Agustin Dal Lago  and Thomas Hubert  and Peter Choy  and Cyprien de Masson d’Autume  and Igor Babuschkin  and Xinyun Chen  and Po-Sen Huang  and Johannes Welbl  and Sven Gowal  and Alexey Cherepanov  and James Molloy  and Daniel J. Mankowitz  and Esme Sutherland Robson  and Pushmeet Kohli  and Nando de Freitas  and Koray Kavukcuoglu  and Oriol Vinyals },
  title = {Competition-level code generation with AlphaCode},
  journal = {Science},
  volume = {378},
  number = {6624},
  pages = {1092-1097},
  year = {2022},
  doi = {10.1126/science.abq1158},
  URL = {https://www.science.org/doi/abs/10.1126/science.abq1158},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.abq1158},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.}}

@InProceedings{Yasunaga20DrRepair,
  author =  {Michihiro Yasunaga and Percy Liang},
  title =   {Graph-based, Self-Supervised Program Repair from Diagnostic Feedback},
  year =    {2020},  
  booktitle =   {International Conference on Machine Learning (ICML)},  
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{meng2024simpo,
  title={SimPO: Simple Preference Optimization with a Reference-Free Reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@article{xu2023some,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{leng2024mitigating,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13872--13882},
  year={2024}
}
@article{deng2024seeing,
  title={Seeing is believing: Mitigating hallucination in large vision-language models via clip-guided decoding},
  author={Deng, Ailin and Chen, Zhirui and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.15300},
  year={2024}
}
@article{jin2023grill,
  title={Grill: Grounded vision-language pre-training via aligning text and image regions},
  author={Jin, Woojeong and Mukherjee, Subhabrata and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Awadallah, Ahmed Hassan and Jose, Damien and Ren, Xiang},
  journal={arXiv preprint arXiv:2305.14676},
  year={2023}
}
@article{liu2024reducing,
  title={Reducing hallucinations in vision-language models via latent space steering},
  author={Liu, Sheng and Ye, Haotian and Zou, James},
  journal={arXiv preprint arXiv:2410.15778},
  year={2024}
}
@article{sun2024layoutvlm,
  title={LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models},
  author={Sun, Fan-Yun and Liu, Weiyu and Gu, Siyi and Lim, Dylan and Bhat, Goutam and Tombari, Federico and Li, Manling and Haber, Nick and Wu, Jiajun},
  journal={arXiv preprint arXiv:2412.02193},
  year={2024}
}

@article{jiang2024supervised,
  title={Supervised fine-tuning in turn improves visual foundation models},
  author={Jiang, Xiaohu and Ge, Yixiao and Ge, Yuying and Shi, Dachuan and Yuan, Chun and Shan, Ying},
  journal={arXiv preprint arXiv:2401.10222},
  year={2024}
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}