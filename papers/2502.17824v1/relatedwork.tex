\section{Literature Review}
Recent advancements in computer vision and explainable AI have significantly improved object segmentation and interpretation in medical and general imaging. However, many state-of-the-art methods still rely on pixel-level annotations, which are costly and time-consuming \cite{Rahimi21}. Additionally, neural networks often lack transparency, limiting their applicability in high-stakes domains like healthcare \cite{alzubaidi2023towards}. This section reviews key developments in semantic segmentation, weakly supervised learning, generative adversarial networks, and uncertainty quantification, highlighting their strengths and limitations. The identified research gaps provide the foundation for our proposed approach, which integrates ensemble learning, XGrad-CAM explainability, and Monte Carlo Dropout-based uncertainty estimation to generate reliable pixel-level annotations using only image-level labels.

\subsection{Semantic Segmentation}

Recent advances in semantic segmentation have improved the accuracy of object localization in medical imaging. For example, GroupViT \cite{ref12} employs a Hierarchical Grouping Vision Transformer that segments image regions using text supervision. While effective, its reliance on pre-trained object detectors with bounding box annotations limits its applicability in scenarios where pixel-level annotations are unavailable. In medical imaging, SMR-UNet \cite{ref13} uses self-attention mechanisms, multi-scale feature integration, and residual structures to improve the segmentation of lung nodules. Although it captures both local and global contextual information, its performance depends on detailed pixel-level annotations, which are costly to obtain.

USegTransformer-P and USegTransformer-S \cite{ref14} combine transformers with CNNs to improve segmentation accuracy. However, both models require large, annotated datasets and high computational resources, making them less practical in data-limited or resource-constrained environments. Similarly, the hybrid attention-based residual UNet \cite{ref15} enhances brain tumor segmentation but relies on resized inputs, which may omit diagnostic information critical for clinical applications.

While these methods have advanced pixel-level segmentation, they rely heavily on pixel-level annotations and are sensitive to data quality and computational constraints. In contrast, our approach addresses these limitations by using a weakly supervised learning framework that requires only image-level labels. By integrating XGrad-CAM within an ensemble of ResNet50, EfficientNet, and DenseNet, our method produces pixel-level masks without manual annotations, reducing data preparation costs while maintaining interpretability and reliability.

\subsection{Weakly Supervised Semantic Segmentation}

Weakly supervised semantic segmentation (WSSS) aims to segment objects or regions of interest without relying on detailed pixel-level annotations. This is particularly important in medical imaging, where obtaining pixel-level labels is time-consuming and resource-intensive.

The Multi-class Token Transformer (MCTformer) \cite{ref16} enhances object localization using class-specific attention mechanisms, improving segmentation precision. However, it still requires class labels and struggles with complex boundaries, limiting its applicability in medical contexts. Causal Class Activation Maps (C-CAM) \cite{ref17} address WSSS challenges in medical imaging by leveraging anatomy and co-occurrence causalities, generating pseudo-segmentation masks with clearer boundaries. Yet, C-CAMâ€™s reliance on heatmap thresholding can reduce accuracy for small or overlapping regions.

ReFit \cite{ref18} integrates unsupervised segmentation and saliency methods to create edge maps that refine object boundaries using Grad-CAM. Despite improved boundary delineation, its effectiveness depends on the quality of edge maps, which can be inconsistent in noisy medical images. For brain tumour segmentation, \cite{ref19} proposes classifiers trained with only image-level labels, producing heatmaps that guide ROI segmentation. However, its performance is limited by the thresholding process, which may exclude subtle features. A 3D segmentation technique \cite{ref20} combines semi-supervised and self-supervised learning to generate pseudo-labels, but its reliance on ground truth labels for central slices restricts its use in datasets with specific annotations.

Although these methods reduce annotation requirements, they still depend on partial pixel-level labels or struggle with accurate segmentation from image-level annotations. Our approach overcomes these limitations by generating pixel-level masks using only image-level labels. By integrating XGrad-CAM within an ensemble of ResNet50, EfficientNet, and DenseNet, we produce accurate annotations without manual pixel-level labeling. Additionally, using Monte Carlo Dropout quantifies uncertainty, ensuring reliable segmentation even in open-set scenarios where unseen data may occur.

\subsection{Generative Adversarial Networks and Interpretability}

Generative Adversarial Networks (GANs) \cite{ref21} consists of two components: a generator that creates synthetic data and a discriminator that distinguishes between real and generated data. GANs have been applied across various domains, including realistic image synthesis \cite{ref22}, domain translation, and medical image generation \cite{ref23}. However, their limited explainability hinders their use in critical applications like medical imaging. Existing explainability methods for GANs, such as image-to-image translation \cite{ref24,ref25}, struggle to generate accurate pixel-level masks, reducing their effectiveness in visualizing decision-making processes.

CycleGAN \cite{ref26}, designed for unpaired image-to-image translation, has improved tissue segmentation and disease detection in cardiac, liver, and retinal imaging \cite{ref27}. Although it enhances visual interpretability by revealing disease impacts, it does not produce binary masks, limiting its use for precise annotations. Class Activation Maps (CAM) \cite{ref28} provide visual explanations by highlighting class-specific image regions using the final convolutional layer. Grad-CAM \cite{ref29} extends this approach by leveraging gradient information to generate coarse localization maps. However, CAM and Grad-CAM only offer heatmaps and cannot produce binary masks, limiting their utility for pixel-level annotation.

MDVA-GAN \cite{ref30} addresses this limitation by integrating CycleGAN with Grad-CAM to visualize multi-class features and generate binary masks. Despite this improvement, MDVA-GAN often misclassifies images from unseen classes, reducing its reliability in open-set scenarios. Additionally, its reliance on adversarial training can lead to unstable results, especially when training data is limited.

These limitations highlight the need for an approach to generate pixel-level annotations from image-level labels while maintaining reliability in open-set scenarios. Our method addresses these challenges by integrating XGrad-CAM within an ensemble of ResNet50, EfficientNet, and DenseNet to generate pixel-level masks without adversarial training. Unlike MDVA-GAN, our approach quantifies prediction uncertainty using Monte Carlo Dropout, enabling it to flag ambiguous or novel data, ensuring more reliable annotations even when encountering unseen classes.

\subsection{Uncertainty Quantification}

Deep learning has advanced diagnostic evaluations in medical imaging, including CT, MRI, ultrasound, and histopathology \cite{ref31}. Despite these advancements, neural networks often function as "black boxes," offering limited insight into their decision-making processes \cite{ref32}. This opacity raises safety and reliability concerns, as models can overestimate their confidence when processing anomalous data \cite{ref33} and are vulnerable to adversarial attacks \cite{ref34}. Identifying these limitations is critical for ensuring the reliable integration of DL models into clinical workflows.

Uncertainty Quantification (UQ) techniques address these challenges by estimating the confidence of model predictions, enabling the identification of ambiguous cases that require human review \cite{ref36}. This is especially important in healthcare, where undetected errors can lead to misdiagnoses and inappropriate treatments \cite{ref37}. Traditional neural networks use the Softmax function to output probability distributions across classes, but these probabilities often do not reflect true uncertainty, particularly in open-set scenarios where the input data differs from the training distribution.

The Uncertainty-Inspired Open Set (UIOS) model \cite{ref38} improves the detection and classification of retinal anomalies by using evidential uncertainty estimation. While UIOS is trainable and computationally efficient, its reliance on specific annotations, such as central slice labels, and the need for manual threshold tuning limit its applicability to datasets with predefined reference points. This dependency restricts its generalizability to broader medical imaging tasks.

Our approach overcomes these limitations by integrating Monte Carlo Dropout into an ensemble of ResNet50, EfficientNet, and DenseNet models. Unlike UIOS, our method does not require specific annotations or manual thresholding. By performing multiple stochastic forward passes during inference, Monte Carlo Dropout estimates the variance of predictions, providing a measure of uncertainty. This uncertainty flags ambiguous cases for human review, improving reliability when the system encounters novel data outside its training distribution. Additionally, integrating UQ within an ensemble framework enhances interpretability, as uncertainty estimates can be analyzed alongside visual explanations generated by XGrad-CAM, ensuring that both the confidence and rationale behind predictions are transparent to clinicians.

\subsection{Research Gaps}

Existing segmentation methods rely heavily on pixel-level annotations or bounding boxes, increasing the time, cost, and expertise required for data preparation. Explainability techniques like CAM and Grad-CAM generate heatmaps but cannot produce pixel-level binary masks. At the same time, MDVA-GAN, despite addressing this limitation, suffers from instability and misclassification of unseen data. Open-set detection remains challenging, as models often fail to identify novel inputs outside the training distribution. Although UIOS estimates uncertainty for open-set scenarios, its dependence on specific annotations and manual thresholding limits its generalizability. Moreover, few methods integrate uncertainty quantification and visual explainability within the same framework, reducing reliability and transparency. This study addresses these gaps by developing a weakly supervised approach that uses only image-level labels to generate pixel-level annotations. By integrating Monte Carlo Dropout within an ensemble of ResNet50, EfficientNet, and DenseNet models, the system estimates prediction uncertainty to improve reliability in open-set scenarios. Additionally, XGrad-CAM enhances interpretability by providing visual explanations, ensuring both confidence and reasoning behind predictions are transparent and accessible.