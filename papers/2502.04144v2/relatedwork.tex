\section{Related Work}
\vspace{-0.3em}

With the rise of foundation models \cite{radford2021learning,singh2022flava,alayrac2022flamingo,wang2023internimage,zhang2023video,achiam2023gpt,chen2024internvl,xiao2024florence,dubey2024llama,cheng2024videollama}, there has been a recent influx of %
benchmarks \cite{zhao2024videoniah, kesen2023vilma,mangalam2023egoschema,patraucean2023perception,li2024vitatecs,li2024seed,li2024mvbench,wang2024sok,chen2024rextime,fang2024mmbench,cai2024temporalbench,cores2024tvbench,Video-MME} aiming to test video understanding abilities. 
These benchmarks evaluate diverse  capabilities \eg physics \cite{patraucean2023perception}, counting \cite{fang2024mmbench}, temporal reasoning  \cite{cai2024temporalbench,chen2024rextime} and long video \cite{mangalam2023egoschema,Video-MME,fang2024mmbench}.


A few benchmarks test embodied or egocentric understanding. 
\cite{Ego4D2022CVPR} released a Natural Language Queries (NLQ) benchmark (19.2K queries) centred around episodic memory of objects.
\cite{OpenEQA2023}~collects 1.6K human-made questions and answers on topics such as relative object locations, episodic memory, and spatial reasoning. However, it uses views from the HM3D \cite{ramakrishnan2021habitat} and ScanNet~\cite{dai2017scannet} datasets, so these questions are based on passive views of a static environment. \cite{mangalam2023egoschema, ye2024mm} auto-generate 5K and 7K questions based on Ego4D narrations. Whilst this approach is efficient, it is limited to these short narrations.
\cite{cai2024temporalbench} collects its own annotations for videos from several datasets, including Ego4D. %
Their benchmark is solely focused on temporal questions related to ordering, counting, causality and direction.


To evaluate a wider range of capabilities, a wider range of annotations are required. 
Of particular note are 3D grounding annotations.
Ego4D~\cite{Ego4D2022CVPR} contains some environment scans and static 3D object locations. With SLAM-equipped devices \cite{zhao2024instance} builds a benchmark for 3D object tracking; \cite{pan2023aria} contains an office and living room digital twin; and \cite{EgoExo4d} contains ego- and exo- views of expert tasks.

In contrast to these works which focus on only a few annotation types, we collect the most comprehensive set of annotations in one dataset, including highly detailed narrations, object and hand segmentations, and a comprehensive 3D digital twin of the scene and objects, all from unscripted egocentric footage in participants' homes. 



\vspace{-0.5em}