
\documentclass[10pt,twocolumn,letterpaper]{article}

 \usepackage{cvpr}              %

\usepackage{pifont}%
\usepackage{multicol}
\usepackage{color, colortbl}

\usepackage{siunitx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{float}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{afterpage}

\newcommand{\cmark}{\textcolor{ForestGreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{BrickRed}{\ding{55}}}%
\definecolor{LightGrey}{rgb}{0.9,0.9,0.9}
\definecolor{figred}{rgb}{1.0,0.4,0.4}
\definecolor{figblue}{rgb}{0.2,0.6,1.0}
\definecolor{figpurple}{rgb}{0.7,0.3,1.0}

\definecolor{vqagreen}{rgb}{0.6, 0.8, 0.4}
\definecolor{vqagemini}{rgb}{0.50, 0.46, 0.78}
\definecolor{vqavlama}{rgb}{0.71, 0.74, 0.54}
\definecolor{vqalngva}{rgb}{0.62, 0.06, 0.06}
\definecolor{vqallama}{rgb}{1.0, 0.4, 1.0}

\input{preamble}

\newcommand*{\assign}{\textcolor{Cyan}}
\newcommand*{\DName}{HD-EPIC\xspace}

\newcommand*{\toby}{\textcolor{Purple}}

\addtocontents{toc}{\setcounter{tocdepth}{-10}}



\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{fdsymbol}

\def\paperID{0058} %
\def\confName{CVPR}
\def\confYear{2025}

\title{%
HD-EPIC: A Highly-Detailed Egocentric Video Dataset}


\author{
Toby Perrett$^{*\clubsuit}$$\quad$Ahmad Darkhalil$^{*\clubsuit}$$\quad$Saptarshi Sinha$^{*\clubsuit}$$\quad$Omar Emara$^{*\clubsuit}$$\quad$Sam Pollard$^{*\clubsuit}$\\ Kranti Kumar Parida$^{*\clubsuit}$$\quad$Kaiting Liu,$^{*\spadesuit}$$\quad$Prajwal Gatti$^{*\clubsuit}$$\quad$Siddhant Bansal$^{*\clubsuit}$$\quad$Kevin Flanagan$^{*\clubsuit}$\\Jacob Chalk$^{*\clubsuit}$$\quad$Zhifan Zhu$^{*\clubsuit}$$\quad$Rhodri Guerrier$^{*\clubsuit}$$\quad$Fahd Abdelazim$^{*\clubsuit}$$\quad$Bin Zhu{$^\vardiamondsuit$}\\Davide Moltisanti{$^\varheartsuit$} $\quad$ Michael Wray{$^\clubsuit$} $\quad$ Hazel Doughty{$^\spadesuit$} $\quad$ Dima Damen{$^\clubsuit$}\\
\small{
\noindent $^{\clubsuit}$Uni. of Bristol$\quad^{\spadesuit}$ Leiden Uni.$\quad^{\vardiamondsuit}$Singapore Management Uni. $\quad^{\varheartsuit}$Uni. of Bath $\quad$ $^*$: Equal Contribution}\\
\small{\url{http://hd-epic.github.io}}
}

\begin{document}
\maketitle

\begin{figure*}[b]
\setlength{\fboxsep}{0pt}
    \centering
    \includegraphics[width=\linewidth]{figures/f1.jpg}
    \vspace{-21pt}
\captionsetup{type=figure}
\captionof{figure}{\textbf{Annotation Highlights.} We capture multi-day recordings of unscripted activities. \colorbox{figbackgroundblue}{\textbf{Centre-Top}}: Recipes are recorded with steps and their preparation temporally annotated, along with ingredient addition. Ingredients are weighed and nutrition recorded. \colorbox{figbackgroundyellow}{\textbf{Centre-Middle}}: Dense fine-grained narrations detailing what, how, and why are parsed and clustered. Audio events are also annotated. \colorbox{figbackgroundpink}{\textbf{Centre-Bottom}}: Object movements are temporally annotated with bounding boxes and hands and object masks. \textbf{Right-Top}: All annotations are temporally grounded in a 3D digital twin. We show trajectories of 3 (masked) objects: \textcolor{figred}{Sweet potato}, \textcolor{figblue}{Food processor} and \textcolor{figpurple}{Spoon}, highlighting relevant kitchen fixtures. \textbf{Right-Bottom}: Gaze captures when objects are primed (\ie looked at) before being taken/placed.}
    \label{fig:f1}
    \vspace*{-12pt}
\end{figure*}


\input{sec/0_abstract}   

\vspace{-16pt}
\section{Introduction}
\vspace{-0.4em}
\label{sec:Intro}

Detailed understanding of videos, from the brief fine-grained action to the overarching hour-long activity, is effortless for humans but currently out of reach for both foundational and specialised models.
Egocentric videos, in particular, 
introduce additional challenges to general video understanding, including significant camera motion, subtle action motion, objects occluded during manipulations and frequently going out of view.
Understanding such videos requires disentangling the combined signals of head motion, hand interactions and a global understanding of the dynamic scene.
This makes ego videos a great testbed for a comprehensive evaluation of video perception models.

Egocentric vision has recently been fuelled by an influx of datasets~\cite{Damen2022RESCALING,Ego4D2022CVPR,EgoExo4d,HoloAssist2023}.
While large-scale, making them ideal for training, these datasets 
are sparsely annotated, particularly for tasks which link various parts of the long video, or those requiring 3D grounding.
In contrast, richly annotated datasets tend to be synthetic or collected in controlled settings~\cite{pan2023aria,sener2022assembly101,banerjee2024hot3d} which limits their realism. %
We bridge this gap by 
presenting the \textbf{most densely annotated dataset of unscripted recordings}, ideal for comprehensive \textbf{validation} of video-only and video-language models.

We collect new videos, allowing us to capture additional meta-data and to ensure these videos have not already been used to train existing models.
Following EPIC-Kitchens~\cite{damen2018scaling}, participants collect all kitchen activities for three  days.
We thus term our dataset Highly-Detailed EPIC (\DName). 
Fig.~\ref{fig:f1} provides an overview of the multi-tiered annotations, several of which are novel:
\begin{itemize}[label=$\star$]
    \item Recipe steps are temporally annotated, and linked to annotations of all preparatory actions that relate to the step.
    \item Ingredients are weighed in videos %
    and labelled with nutrition. We track dish nutrition as ingredients are added.
    \item Each action has a dense description capturing the what, how, and why of actions along the start and end time.
    \item For each kitchen, we curate a digital twin with labelled fixtures. These are associated with actions (\eg open/close) and the taking/placing of objects.
    \item All moved objects are tracked, with manual masks lifted to 3D bounding boxes.
    \item We associate gaze with object movements, labelling when objects are spotted before take/place actions. %
\end{itemize}




With these dense annotations, we design a challenging Visual Question Answering (VQA) benchmark of 26K questions. 
We purposefully do not use LLMs to generate negatives, instead using similar annotations. We highlight a few novel question types:
\begin{itemize}[label=$\star$]
    \item Recipe nutrition: we question the change in the recipe nutrition as one or more ingredients are added.
    \item Multi-video: we question recipes prepared across recordings, with a VQA that spans multiple long videos.
    \item Object itinerary: we question multi-hop object movements over a long video, relative to kitchen fixtures.
    \item Fixture interactions: we question how many times a particular cupboard/drawer is opened/closed.
    \item Action how/why: we question how/why an action was carried out, using participant-narrated  manners/reasons. %
    \item Anticipation with gaze:  With gaze priming, we query next-object movement, offering evidenced anticipation.
\end{itemize}
Additionally, we report results on action recognition, sound recognition, and long-term video object segmentation.

This paper thus contributes: (i) 41 hours of multi-day unscripted egocentric recordings, (ii) highly-detailed annotations including novel labels (\eg ingredient nutrition, digital twin, gaze prime) and (iii) a challenging VQA benchmark including novel Qs (\eg object itinerary, recipe nutrition changes) along with 3 standard video benchmarks.

\vspace{-0.5em}
\section{Related Work}
\vspace{-0.3em}

With the rise of foundation models \cite{radford2021learning,singh2022flava,alayrac2022flamingo,wang2023internimage,zhang2023video,achiam2023gpt,chen2024internvl,xiao2024florence,dubey2024llama,cheng2024videollama}, there has been a recent influx of %
benchmarks \cite{zhao2024videoniah, kesen2023vilma,mangalam2023egoschema,patraucean2023perception,li2024vitatecs,li2024seed,li2024mvbench,wang2024sok,chen2024rextime,fang2024mmbench,cai2024temporalbench,cores2024tvbench,Video-MME} aiming to test video understanding abilities. 
These benchmarks evaluate diverse  capabilities \eg physics \cite{patraucean2023perception}, counting \cite{fang2024mmbench}, temporal reasoning  \cite{cai2024temporalbench,chen2024rextime} and long video \cite{mangalam2023egoschema,Video-MME,fang2024mmbench}.


A few benchmarks test embodied or egocentric understanding. 
\cite{Ego4D2022CVPR} released a Natural Language Queries (NLQ) benchmark (19.2K queries) centred around episodic memory of objects.
\cite{OpenEQA2023}~collects 1.6K human-made questions and answers on topics such as relative object locations, episodic memory, and spatial reasoning. However, it uses views from the HM3D \cite{ramakrishnan2021habitat} and ScanNet~\cite{dai2017scannet} datasets, so these questions are based on passive views of a static environment. \cite{mangalam2023egoschema, ye2024mm} auto-generate 5K and 7K questions based on Ego4D narrations. Whilst this approach is efficient, it is limited to these short narrations.
\cite{cai2024temporalbench} collects its own annotations for videos from several datasets, including Ego4D. %
Their benchmark is solely focused on temporal questions related to ordering, counting, causality and direction.


To evaluate a wider range of capabilities, a wider range of annotations are required. 
Of particular note are 3D grounding annotations.
Ego4D~\cite{Ego4D2022CVPR} contains some environment scans and static 3D object locations. With SLAM-equipped devices \cite{zhao2024instance} builds a benchmark for 3D object tracking; \cite{pan2023aria} contains an office and living room digital twin; and \cite{EgoExo4d} contains ego- and exo- views of expert tasks.

In contrast to these works which focus on only a few annotation types, we collect the most comprehensive set of annotations in one dataset, including highly detailed narrations, object and hand segmentations, and a comprehensive 3D digital twin of the scene and objects, all from unscripted egocentric footage in participants' homes. 



\vspace{-0.5em}
\section{Data Collection}
\vspace{-0.3em}
\label{sec:Data}

\noindent\textbf{Recruitment and Equipment. }
Each participant engaged in a long commitment ($\sim$50 hours) involving  data recording and providing detailed narrations, recipes and nutrition information. %
Data was collected with Project Aria glasses~\cite{Somasundaram2023ProjectAA}---a multi-sensor platform %
with 3 forward cameras (1 RGB and 2 SLAM), 7 microphones and inward cameras for gaze estimation. %
We collected 30 FPS RGB videos at $1408\times1408$ resolution, 60 FPS eye tracking and 30 FPS SLAM.
We supplied participants with multiple devices including scales for nutritional tracking (see Fig.~\ref{fig:backpack}).

\begin{figure*}[t]
    \captionsetup[subfigure]{labelformat=empty,position=top}
    \centering
    \vspace{-1em}
    \subfloat[Recorded over 3 days]{\includegraphics[height=125pt]{figures/d_days.jpg} }    
    \subfloat[Objects]{ \includegraphics[height=125pt]{figures/d_objects.jpg} }    
    \subfloat[Activities]{ \includegraphics[height=125pt]{figures/d_activities.jpg} }    
    \subfloat[Recipes]{ \includegraphics[height=125pt]{figures/d_recipes.jpg} }   
    \vspace{-0.8em}
    \caption{Diversity in \DName, which is filmed over 3 days in-the-wild, resulting in many objects, activities and recipes.}
    \vspace{-1em}
    \label{fig:diversity}
\end{figure*}


\noindent\textbf{Instructions and Collected Data. }
Participants recorded all their daily kitchen activities for at least 3 consecutive days.  
All 9 participants were asked to wear the glasses each time they walked into their kitchen, pressing record upon entering, and stopping the recording when they left the kitchen.
Participants recorded for 3.5 to 7.2 hours (avg. 4.6). 
Overall, we collected 156 videos, with an average length of 15.9 ($\pm$ 14.5) minutes totalling 41.3 hours (4.46M frames).
Fig. \ref{fig:diversity} shows the diversity in the collected data.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/RecipeFig-v3.png}
    \vspace{-1em}
    \caption{Recipe modification in ingredients and steps.}
    \vspace*{-12pt}
    \label{fig:recipe}
\end{figure}







Following data collection, participants provided the recipes they freely prepared, citing the source (\eg website) and any modifications (see Fig.~\ref{fig:recipe}). %
We collected a total of 69 recipes  
covering various cuisines. %
On average, recipes contained 6.6 steps, 8.1 ingredients, and took 4 hours 48 minutes across 2.1 videos from preparation to serving. %
Our longest recipe %
took 2 days and 6 hours to complete.



To track nutrition of recipes, participants weighed and manually logged ingredients with MyFitnessPal~\cite{myfitness}, giving us detailed nutrition information and adding an additional dimension to the dataset. %
In total, participants used 558 ingredients including ingredients high in protein, \eg tuna and kidney beans;  carbohydrates, \eg dates and flour; and fat \eg sour cream and pine nuts. %
Participants prepared both high calorie dishes \eg Lazy Cake %
(4.8K calories)
and low calorie dishes \eg Crispy Cucumber Salad %
(274 calories).


\noindent\textbf{Narrations. }
We follow prior datasets~\cite{damen2018scaling,Damen2022RESCALING,Ego4D2022CVPR}, asking participants to watch their recordings and narrate with a web-based narrator tool~\cite{Ego4D2022CVPR}.
We expand on this by asking participants to describe \textit{what} they are doing, along with \textit{how} and \textit{why}.
This results in a rich set of narrations that are denser, and more detailed than previous datasets (\eg 3.8$\times$ more words/min than Ego4D).
See stats in Supp.~\ref{sec:data_collection_sup}.


\noindent\textbf{Post-Processing---Multi-Video Slam and Gaze. }
We use Aria %
MPS~\cite{ariamps} to process videos obtaining singular multi-day point clouds per kitchen; 1kHz 6DoF camera trajectories; and eye gaze direction.
We post-process VRS files, converting videos to mp4, removing the gaze camera input for anonymity. %
Further details are in Supp.~\ref{sec:data_collection_sup}.



\vspace{-0.5em}
\section{Annotation Pipeline}
\vspace{-0.3em}
\label{sec:annotations}

We collect extensive multi-tiered annotations to achieve the level of detail that distinguishes \DName from other video understanding datasets. Here we detail our pipeline.

\vspace{-0.3em}
\subsection{Annotating Recipe Steps and Ingredients}
\vspace{-0.2em}
\label{sec:annotations-recipe}




Our videos are distinct from short recipe videos found online, which are typically trimmed to only crucial steps, and often edited further or sped up. Videos in \DName include a wider range of recipe-relevant activities, such as fetching or prepping ingredients. %
To comprehensively annotate these videos, we introduce prep and step pairs. 

 The \textit{prep} of a corresponding \textit{step} is defined as all essential actions the participant takes to get ready to execute a given step. For example, the prep of the step `chop tomato', includes retrieving the tomato from storage, washing it, and gathering the knife and chopping board. 
However, if the step is `Add chopped onions and stir', then the chopping of onions is part of the prep for that step. %
This introduces a more fine-grained understanding of all steps, unexplored in prior datasets~\cite{song2024ego4d,lai2023lego,EgoExo4d}.
\begin{figure*}
    \centering
    \vspace{-0.5em}
    \includegraphics[width=1.0\linewidth]{figures/prep_stepv5.png}
    \vspace{-2em}
    \caption{For the `Carbonara' recipe, we visualise the \textit{\textcolor{gray}{prep}} and \textit{step} time segments for three consecutive steps (left), along with sample frames with corresponding action narrations (top). 
    The interleaving of different \textit{\textcolor{gray}{preps}}/steps is evident in the annotations.}
\vspace{-0.8em}
    \label{fig:prep-step}
\end{figure*}
Fig.~\ref{fig:prep-step} shows sample prep-step annotations for 3 steps. %
Nearly all steps (93.1\%) have paired prep annotations.
Typically, prep is shorter than a step:
avg. prep is   54.5s ($\pm$95.3s), avg. step is 78.2s ($\pm$100.7s). %
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]
    {figures/verb_noun_distribution4}
    \vspace{-2.1em}
    \caption{Frequency of verb clusters (top) and noun clusters (bottom) in narrated sentences by category, shown on a logarithmic scale.}
    \vspace{-12pt}
    \label{fig:verb-noun-dist}
\end{figure*}




\begin{figure}
    \centering
    \vspace{-0.6em}
    \includegraphics[width=1\linewidth]{figures/NutritionTrackerV7.jpg} %
    \vspace{-2em}
    \caption{Nutrition is monitored throughout recipes as ingredients are incorporated into dishes. Here we show Banana Bread Chocolate Chip Cookies. We annotate when ingredients are weighed, document their nutrition, and locate their adding time, giving us overall dish nutrition at each stage.}
     \vspace{-12pt}
    \label{fig:nutritionTracker}
\end{figure}



 We also annotate \textit{weighing} and \textit{adding} temporal segments which enables monitoring the nutrition of the full dish as ingredients are incorporated (see
Fig.~\ref{fig:nutritionTracker}).
In total, we annotate 283 in-view weighing sequences (avg. 18.9s) and 501 adding sequences (avg. 31.6s), excluding spices. %
Details of the annotation process are in Supp.~\ref{sec:annotations_pipeline_supp}.




\vspace{-0.3em}
\subsection{Fine-Grained Actions}
\vspace{-0.2em}
\label{sec:annotations-narrations}


\noindent \textbf{Transcription.} We automatically transcribe and manually check and correct 
all audio narrations provided by participants, to obtain detailed action descriptions. %

\noindent \textbf{Action Boundaries.} For all narrations, we label precise start and end times. %
In total, we obtain segments for 59,454 actions, with a mean duration of 2.0s (${\pm}$3.4s).



{
\setlength{\fboxsep}{1pt}
\noindent \textbf{Parsing.}
We parse \colorbox{verb}{\vphantom{Ay}verbs}, 
\colorbox{noun}{\vphantom{Ay}nouns} and \colorbox{hand}{\vphantom{Ay}hands} from open vocabulary narrations so they can be used for closed vocabulary tasks, such as action recognition. We also extract \colorbox{how}{\vphantom{Ay}how} and \colorbox{why}{\vphantom{Ay}why} clauses from 16,004 and 11,540 narrations, respectively. 
For example, ``\colorbox{verb}{\vphantom{Ay}Turn} the \colorbox{noun}{\vphantom{Ay}salt container} \colorbox{how}{clockwise by pushing it} with my \colorbox{hand}{\vphantom{Ay}left hand} \colorbox{why}{\vphantom{Ay}so that the lid} \colorbox{why}{is aligned with the container opening.}''}






\noindent \textbf{Clustering.} 
Fig.~\ref{fig:verb-noun-dist} shows the distribution of clusters (\ie classes) across all videos in \DName, along with hierarchical clusters~\cite{Damen2022RESCALING}. As with prior datasets~\cite{Damen2022RESCALING,Ego4D2022CVPR}, our highly diverse actions and objects are long-tailed.

\noindent\textbf{Sound Annotations. }
We follow~\cite{EPICSOUNDS2023} to collect audio annotations.
These capture start-end times of audio events along with a class name (e.g. `click', `rustle', `metal-plastic collision', `running water').
Overall, we have 50,968 audio annotations from 44 classes. %

Full details of transcription, boundary labelling, parsing, clustering and sound annotations are in Supp.~\ref{sec:annotations_pipeline_supp}.

\vspace{-0.3em}
\subsection{Digital Twins: Scene \& Object Movements}
\vspace{-0.2em}

\noindent \textbf{Scene.}
We create digital copies of participants' kitchens by reconstructing the surfaces and manually curating every fixture (\eg cupboard, drawer), storage space (\eg shelves, hooks) and large appliance (\eg fridge, microwave).
This is distinct from digital twins that rely on known environments with replicas. 
Our digital twin is created in Blender \cite{blender} on top of the multi-video SLAM point clouds from recordings. %

Each kitchen contains an average of 45.9 labelled fixtures (min 31, max 62), including 14.2 counters/surfaces, 12.2 cupboards, 7.8 drawers and 5.2 appliances (sample in Fig. \ref{fig:3D-twin}). We refer to these annotations as Fixtures $F$.

We then associate narrations which describe scene interactions with $F$. We find actions where a noun indicates a fixture, \eg ``open drawer'', identify the exact ``drawer'' in the digital twin (\eg drawer.001) and update its state. Following studies showing humans fixate up to 1 second before interacting \cite{land1999roles}, we take the fixture $f{\in}F$ with the highest cumulative gaze intersection for the 1s before the narration.

\begin{figure}
    \centering
    \vspace{-1em}
    \includegraphics[width=1.0\linewidth]{figures/blender.jpg}
    \vspace{-2em}
    \caption{Digital Twin: from point cloud (left), to surfaces (middle) and labelled fixtures (right). We show two moved objects (masks on top) at fixtures: \textcolor{figblue}{cheese} and \textcolor{figred}{pan}.
    Body poses from~\cite{yi2024egoallo}.
    }
    \vspace{-10pt}
    \label{fig:3D-twin}
\end{figure}

\noindent \textbf{Hand Masks.} We annotate a handful of frames per video for %
both hands.
Frames are selected to cover various actions and kitchen locations. %
We use these to automatically segment, and manually correct a selected subset. %
In total, our dataset contains 
7.7M hand masks: 3.9M right and 3.8M left of which 11K are
manually annotated (details in Supp.~\ref{sec:annotations_pipeline_supp}).

\noindent \textbf{Moving Objects in 2D.} To generate 3D object movement annotations, we first annotate when objects move. %
Annotators %
label a temporal segment each time an object is moved until it is set, along with 2D object bounding boxes at the onset and end of motion. %
For example, if a person moves a cup from a countertop to the sink, one bounding box captures the cup on the countertop and another when in the sink.
Tracks are annotated even for slight shifts/pushes, and thus offer full annotations of all object movements.

Overall, we collected 19.9K object movement tracks and 36.9K bounding boxes.
We label an average of 9.2 objects taken and 9.0 objects placed per minute.
On average, tracks are 9.0s long, the longest is 461.5s. 

\noindent \textbf{Object Masks.} 
Despite progress in segmentation~\cite{kirillov2023segment, ravi2024sam} and available annotations~\cite{VISOR2022,EgoExo4d}, models perform poorly in egocentric video, particularly under occlusions.
We obtain 
pixel-level segmentations from each bounding box by
initialising with iterative SAM2~\cite{ravi2024sam} then manually correcting. 
Annotators corrected 74\% of masks; the IoU between SAM2 and the manual masks is 0.82.

\noindent \textbf{Masks to 3D.} We lift object masks to 3D using dense depth estimates and 2D-to-3D sparse correspondences provided by MPS. %
Given metric depth from~\cite{depthanything}, we identify $S$, the set of pixels within or around the object with 3D correspondences. We then find the linear transformation coefficients:
$\alpha, \beta = \underset{\alpha, \beta}{\mathrm{argmin}} \, \| (\alpha \hat{D}_S + \beta ) - D_S \|^2.
$ where $\hat D_S$ are estimated depth values and $D_S$ are existing depth values, followed by RANSAC to remove outliers.


\noindent \textbf{3D Object Motion. } Objects move 61.4cm ($\pm$84.5cm) on average, 27.6\%  move $\leq$10cm, while 7.6\% move $\geq$2m.

\noindent \textbf{Object-Scene Interactions.} With the 3D object locations, we associate locations with the closest fixture $f{\in}F$, subject to fixture-specific heuristics (\eg objects must be within a counter's x-y plane). 
We manually verify all assignments, correcting any errors. %
On average objects move between 1.8 different fixtures per video (see Supp.~\ref{sec:annotations_pipeline_supp} for stats).







\begin{table*}[t]
\vspace{-1em}
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Val\&Test} & \textbf{Action} & \textbf{Unscripted} & \textbf{Free} & \textbf{Recipe} & \textbf{Nutrition} & \textbf{Gaze} & \textbf{Audio} & \textbf{Object} & \textbf{Hand} & \textbf{3D object} & \textbf{Labelled 3D} & \textbf{Camera} & \textbf{Fully} \\
& \textbf{Hours} & \textbf{Segments} & & \textbf{Setting} & & & & \textbf{Labels} & & & \textbf{over time} & \textbf{environment} & \textbf{pose} & \textbf{annotated} \\
\midrule
HOI4D~\cite{liu2022hoi4d} & 11.4 & \cmark &\xmark &\xmark & \xmark & \xmark & \xmark & \xmark & Mask & Mask &  \cmark & \cmark & \xmark & \cmark \\
Assembly101~\cite{sener2022assembly101} & 66.8 & \cmark &\xmark &\xmark & \xmark & \xmark & \xmark & \xmark & \xmark & 3D pose & \xmark & \xmark & \xmark & \cmark \\
EPIC-KITCHENS-100~\cite{Damen2022RESCALING} & 25.3 & \cmark &\cmark &\cmark & \xmark & \xmark & \xmark & \cmark & Mask & Mask & \cmark & \xmark & \cmark & \xmark \\
Ego4D~\cite{Ego4D2022CVPR} & 288.7 & \cmark &\cmark &\cmark & \xmark & \xmark & \xmark & \xmark & B-Box & B-Box &  \xmark & \xmark & \xmark & \xmark \\
HoloAssist~\cite{HoloAssist2023} & 49.8 & \cmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark  & 3D pose & \xmark & \xmark & \cmark & \cmark \\ 
Aria Digital Twin~\cite{pan2023aria} & 8.1 & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & Mask & \xmark &  \cmark & \cmark & \cmark  & \cmark \\
Aria Everyday Activities~\cite{lv2024aria} & 7.3 & \xmark & \xmark &\cmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark &  \xmark \\
Aria Everyday Objects~\cite{straub2024efm3d} & 0.4 & \xmark &\cmark &\cmark & \xmark & \xmark & \xmark & \xmark & B-Box & \xmark & \cmark & \xmark & \cmark  & \cmark \\
Ego-Exo4D~\cite{EgoExo4d} & 85.1 & \xmark & \cmark & \cmark & \cmark & \xmark & \cmark & \xmark & Mask & 3D pose & \xmark & \xmark & \cmark  & \xmark \\ \midrule
\textbf{\DName} & 41.3 & \cmark & \cmark & \cmark &\cmark & \cmark & \cmark & \cmark & Mask & Mask &  \cmark & \cmark & \cmark  & \cmark \\ \bottomrule
\end{tabular}%
}
\vspace*{-10pt}
\caption{\textbf{Comparison of Egocentric Video Datasets} (see full table~\ref{tab:dataset_comparison_full} in Supp.). 
}
\vspace*{-12pt}
\label{tab:dataset_comparison}
\end{table*}

\begin{figure}
    \centering
    \vspace{-1em}
    \includegraphics[width=1.0\linewidth]{figures/prime_figure.png}
    \vspace{-2em}
    \caption{\textbf{Priming Object Interaction Through Gaze}. Top: Camera position with projected eye-gaze and object positions in 3D. Middle: 2D gaze location. Bottom: Timeline for priming object movement \eg the glass is primed 8.3s before taking.}
    \vspace{-6pt}
    \label{fig:prime_fig}
\end{figure}

\noindent \textbf{Priming Object Movement.}
The behaviour of gaze when picking up and placing objects is well-studied~\cite{land1999roles,johansson2001eyehand}. %
We combine eye-gaze and 3D object locations, to find when an object is \textit{primed}, \ie the moment in time when the gaze attends to the object's location before picking it up (\textit{pick-up priming}) or when the gaze attends to the future location of an object before it's put down (\textit{put-down priming}).

We calculate the \textit{priming time} for all objects, excluding those taking or placed off screen. 
Additionally, at times, a person is already manipulating an object well before picking it up.
We thus exclude objects with a pick up location already close to the gaze 10s earlier.
 In Fig.~\ref{fig:prime_fig} we show gaze priming for two objects: milk bottle and glass. %
The glass's end location, a cupboard, 
is primed 3s before the glass is put away.
Fig.~\ref{fig:prime_stats} displays priming statistics. %
Of those objects feasible for priming, 94.8\%
are primed, an average of 4.0s before being picked up, compared to 88.5\%
primed an average of 2.6s before being placed.



\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|cc|cc|c}
        \toprule
        Location  
        & \makecell{Filtered\\(\% Total)} & \makecell{Feasible\\(\% Total)} & \makecell{Primed\\(\% Feasible)} & \makecell{Not Primed\\(\% Feasible)} &  Avg. Time (s) \\
        \midrule
        Start & 
        29.40 & 70.60 & 94.82 & 5.18 & $3.99 (\pm2.94)$ \\
        End & 
        66.92 & 33.08 & 88.46 & 11.54 & $2.62 (\pm2.05)$ \\
        \bottomrule
        \end{tabular}
    }
    \includegraphics[width=1.0\linewidth]{figures/prime_gap_hist.png}
    \vspace{-2.4em}
    \caption{(Top) Priming Statistics for both start and end locations (Bottom) Histogram showing the difference in time when an object is primed before it is picked up (blue) or placed (red).}
    \vspace{-0.8em}
    \label{fig:prime_stats}
\end{figure}

\noindent \textbf{Long Term Object Tracking.}
\label{sec:long_term_obj_tracks}
We connect object movements and form longer trajectories, \ie object itineraries, to capture sequences of an object's movement.
Our efficient pipeline utilises our lifted 3D locations  
and allows annotating a 1-hour long video in minutes (details in Supp.~\ref{sec:annotations_pipeline_supp}).













\vspace{-0.3em}
\subsection{\DName vs Prior Egocentric Datasets}
\vspace{-0.2em}
Tab.~\ref{tab:dataset_comparison} compares \DName to other egocentric datasets (full table in Supp.).
Compared to the largest dataset with labelled 3D environments (Aria Digital Twin~\cite{pan2023aria}), \DName contains 5x more footage; has more annotations; and importantly was collected in an unscripted manner in the participants' homes. %
In particular, \DName is the first to annotate recipes, nutritional values, detailed action segments, gaze and audio labels on the same set of videos. 
With these diverse and dense annotations, \DName constitutes a true zero-shot benchmark for video understanding.


\vspace{-0.5em}
\section{Benchmarks and Results}
\vspace{-0.3em}
\label{sec:benchmarks}

We show the potential of \DName as a validation dataset with benchmarks on
 general Video Question Answering (VQA) (Sec~\ref{sec:benchmark-vqa}), action and sound recognition (Sec~\ref{sec:benchmark-recognition}) and long-term video object segmentation (Sec~\ref{sec:benchmark-long}).%

\begin{figure}
    \centering
    \vspace{-1em}
    \includegraphics[width=0.8\linewidth]{figures/q_pie_with_dist_jan.pdf}
    \vspace{-1.5em}
    \caption{\textbf{VQA Question Prototypes}. We show our 30 question prototypes by category alongside the number of questions. Outer bars indicate the distribution over input lengths for each question. %
    }
    \label{fig:vqa_prototypes}
    \vspace{-0.5em}
\end{figure}

\vspace{-0.3em}
\subsection{\DName VQA Benchmark and Analysis}
\vspace{-0.2em}
\label{sec:benchmark-vqa}
\setlength{\fboxsep}{2pt}
\noindent\textbf{Benchmark Creation}. We take the dense output of our annotation pipeline and construct a comprehensive VQA benchmark around 7 types of annotations:
\begin{enumerate}
    \item \recipe{Recipe}. Questions on temporally localising, retrieving, or recognising recipes and their steps.
    \item \ingredient{Ingredient}. Questions on the ingredients used, their weight, their adding time and order.
    \item \nutrition{\vphantom{Ay}Nutrition}. Questions on nutrition of ingredients and nutritional changes as ingredients are  added to recipes.
    \item \fine{Fine-grained action}. What, how, and why of actions and their temporal localisation.
    \item \threed{3D perception}. Questions that require the understanding of relative positions of objects in the 3D scene. %
    \item \object{Object motion}. Questions on where, when and how many times objects are moved across long videos.
    \item \gaze{Gaze}. Questions on estimating the fixation on large landmarks and anticipating future object interactions.
\end{enumerate}
For each question type, we define prototypes to sample questions, correct answers, and strong negatives from our annotations. For example, \object{Object Movement Counting} asks ``How many times did the object $<$bbox$>$ seen at $<$time$>$ move in the video?''. This uses long videos, requiring multiple hops to be correctly answered. In contrast, \fine{How Recognition} asks ``What is the best description for how the person carried out the action $<$verb, noun$>$?'' to test a model's ability to capture intricate details of actions.

\begin{figure*}
\vspace{-1em}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results_per_prototype_latest_final.png}
    \vspace{-2.6em}
    \caption{\textbf{VQA Results per Question Prototype}. Our benchmark contains many challenging questions for current models.} %
    \vspace{-1em}
    \label{fig:results_per_prototype}
\end{figure*}

Each question prototype is 5-way multiple choice. We generate hard negatives for prototypes by sampling within the dataset for difficult answers.
For example, we take 4 different answers of how participants performed the same action. This ensures realistic negatives and challenging questions. %
In total, we have 30 prototypes, and generate 26,650 multiple-choice questions. 
This makes it one of the largest VQA video benchmarks, but keeps it tractable particularly to evaluate closed-source VLMs.
Due to the density of our annotations, we estimate an upper bound of 100,000 possible unique questions with this set of prototypes. 

Fig.~\ref{fig:vqa_prototypes} shows the distribution of 
questions per category alongside the distribution of input lengths which varies from single frames to 7+ hours. Details of each prototype's sampling can be found in Supp.~\ref{sec:benchmark_supp}.
A sample of our questions and answers can be seen in Fig.~\ref{fig:vqa_qual_res}. %






\noindent \textbf{VLM Models.} Due to the size and long-term nature of many question prototypes in our benchmark, we use 5 representative models as baselines (more details in Supp.~\ref{sec:benchmark_supp}):
\begin{itemize}
    \item Llama 3.2 90B~\cite{dubey2024llama}. We use this as a strong open-source (OS) text-only baseline, as LLMs can perform well on visual QA benchmarks \emph{without any visual input}~\cite{xiao2024can}.
    \item VideoLlama 2 7B \cite{cheng2024videollama}. OS short context model.
    \item LongVA \cite{zhang2024long}. Longest context OS model.
    \item LLaVa-Video \cite{zhang2024videoinstructiontuningsynthetic}. OS model trained also on  ego data.
    \item Gemini Pro \cite{team2024gemini}. Closed source, longest context of any model, and state-of-the-art on long-video \cite{Video-MME}.
\end{itemize}

\begin{table}[]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{\linewidth}{!}
    {\begin{tabular}{lcccccccc}
    \toprule
    Model & \recipe{Recipe} & \ingredient{Ingredient} & \nutrition{\vphantom{Ay}Nutrition} & \fine{\vphantom{Ay}Action} & \threed{\vphantom{Ay}3D} & \object{\vphantom{Ay}Motion} & \gaze{\vphantom{Ay}Gaze} & Avg.\\
 \midrule
\rowcolor{LightGrey} \multicolumn{9}{l}{\textbf{Blind - Language Only}} \\
 Llama 3.2 & 33.5 & 25.0 & 36.7 & 23.3 & 22.3 & 25.5 & 19.5 & 26.5 \\
  Gemini Pro & 38.0 & 26.8 & 30.0 & 22.1 & 21.5 & 27.7 & 20.5 & 26.7\\
\rowcolor{LightGrey} \multicolumn{9}{l}{\textbf{Video-Language}} \\
 VideoLlama 2 & 30.8 & 25.7 & 32.7 & 27.2 & 25.7 & 28.5 & 21.2 & 27.4\\
  LongVA & 29.6 & 30.8 & 33.7 & 30.7 & 32.9 & 22.7 & 24.5 & 29.3\\
  LLaVA-Video & 36.3 & 33.5 & 38.7 & 43.0 & 27.3 & 18.9 & 29.3 & 32.4 \\
  Gemini Pro & 60.5 & 46.2 & 34.7 & 39.6 & 32.5 & 20.8 & 28.7 & 37.6\\
  \color{Gray}{\textit{Sample Human Baseline}} & \color{Gray}{\textit{96.7}} & \color{Gray}{\textit{96.7}} & \color{Gray}{\textit{85.0}} & \color{Gray}{\textit{92.5}} & \color{Gray}{\textit{93.8}} & \color{Gray}{\textit{92.7}} & \color{Gray}{\textit{75.0}} & \color{Gray}{\textit{90.3}}\\
         \bottomrule
    \end{tabular}}
    \vspace{-1em}
    \caption{\textbf{VQA Results per Category} (\% Acc.). Our VQA benchmark  cannot be solved blind or by external knowledge and is a challenge for state-of-the-art video VLM models.}
    \vspace{-1.5em}
    \label{tab:results_main}
\end{table}

\noindent\textbf{VQA Results Per Category and Per Prototype.} Tab. \ref{tab:results_main} provides overall and per-category accuracy averaged over the prototype results shown in Fig.~\ref{fig:results_per_prototype}. %
Both language-only models only achieve 26.5\% and 26.7\%, only 6.7\% above random. %
Open-source video VLMs (VideoLlama, LongVA, LlaVA-Video) perform similarly (27.4\%, 29.3\%, 32.4\%) but have different strengths as shown in Fig.~\ref{fig:results_per_prototype}. For example, Llama better estimates nutrition, while the video is necessary to get above random performance on action recognition and gaze estimation. Gemini achieves the best performance, particularly for \recipe{Recipe} and \ingredient{Ingredient} where external knowledge helps. However, the average performance (37.6\%) and the gap to our sample human baseline (90.3\%) shows the challenge posed by our VQA benchmark. 


\begin{figure}
\vspace{-0.3em}
    \centering
    \includegraphics[width=\linewidth]{figures/acc_time_hist_latest_final.png}
    \vspace{-2.7em}
    \caption{\textbf{Effect of Input Length}. Models struggle with questions of all video input lengths. s=second, m=minute, 
h=hour.}
    \vspace{-1em}
    \label{fig:result_length}
\end{figure}



\noindent\textbf{Video Length.} Fig.~\ref{fig:result_length} shows
models struggle with all video lengths but are worst with inputs $\geq$1 minute.

\begin{figure*}
    \centering
    \vspace{-1em}
    \includegraphics[width=\linewidth]{figures/qual_res_v13.jpg}
    \vspace{-2.5em}
    \caption{\textbf{VQA Qualitative Results}. We mark GT answers with a  \colorbox{vqagreen}{green} background, and predictions from different models, \ie, \textcolor{vqallama}{LLaMA 3.2}, \textcolor{vqavlama}{VideoLLaMA 2}, \textcolor{vqalngva}{LongVA}, \textcolor{vqagemini}{Gemini Pro} with coloured dots. Note: Under Nutrition, [fat] values are not provided to the model.
    } 
    \vspace{-2em}
    \label{fig:vqa_qual_res}
\end{figure*}

{\setlength{\fboxsep}{1pt}
\noindent\textbf{Common Failures.} Fig.~\ref{fig:vqa_qual_res} shows qualitative results. In \recipe{Recipe}, models struggle when steps have common objects or actions. In \ingredient{Ingredient}, models guess weights (readable from the scale by humans) poorly, also causing errors in \nutrition{\vphantom{Ay}Nutrition}. \fine{Fine-grained action} is hard when answers share nouns. %
In \gaze{\vphantom{Ay}Gaze}, models just select recently moved objects. Confusion in \threed{\vphantom{Ay}3D} and \object{Object motion} occurs with directions (right/left) and fixtures (counters/drawers).  }

\begin{table}[t]
    \footnotesize
    \centering
    \setlength{\tabcolsep}{6pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l c c c c | c }
        \toprule
        Model & Modality & Verb & Noun & Action & {\parbox{2cm}{\centering Unseen\\EPIC-100 Action}}\\
        \midrule
                \rowcolor{LightGrey} \multicolumn{5}{l|}{\textbf{EPIC-KITCHENS-100 SOTA}} & \\
        TIM~\cite{Chalk_2024_CVPR} & A+V & 77.1 & 67.2 & 57.5 & 44.6 \\
        \rowcolor{LightGrey} \multicolumn{5}{l|}{\textbf{\DName}} & \\
        Chance & - & 10.9 & 1.8 & 0.0 & - \\
        SlowFast~\cite{Feichtenhofer_2019_ICCV} & V & 29.2 & 10.6 & 5.3 & 29.0 \\
        Omnivore~\cite{girdhar2022omnivore} & V & 19.5 & 17.1 & 8.7 & 28.7 \\
        MotionFormer-HR~\cite{patrick2021keeping} & V & 35.7 & 20.0 & 10.2 & 32.2 \\
        VideoMAE-L~\cite{tong2022videomae} & V & 47.5 & 29.4 & 17.9 & 29.3 \\
        TIM~\cite{Chalk_2024_CVPR} & A+V & \textbf{51.3} & 36.1 & 23.4 & \textbf{44.6} \\
        TIM~\cite{Chalk_2024_CVPR} & V & 51.2 & \textbf{36.5} & \textbf{23.9} & 44.4 \\
        \bottomrule
    \end{tabular}}
    \vspace{-1.2em}
    \caption{\textbf{Action Recognition Benchmark} (\% Acc.). \DName provides a significant challenge for state-of-the-art models.}
    \vspace{-2em}
    \label{tab:action_recognition}
\end{table}

\vspace{-0.3em}
\subsection{Recognition Benchmarks}
\vspace{-0.2em}
\label{sec:benchmark-recognition}

\noindent \textbf{Action Recognition.} We assess 5  action recognition methods~\cite{Feichtenhofer_2019_ICCV,patrick2021keeping,girdhar2022omnivore,tong2022videomae,Chalk_2024_CVPR}, using publicly available checkpoints fine-tuned on EPIC-KITCHENS-100.
Results are shown in Tab.~\ref{tab:action_recognition}. For context we show the results from EPIC-KITCHENS-100 (top row) and on the unseen kitchens subset of EPIC-KITCHENS-100 (last col.).
Best performance on \DName is only 51\% for verbs, 37\%  for nouns and 24\% for actions leaving plenty of room for improvement.

\noindent \textbf{Sound Recognition.} We evaluate 3 audio models~\cite{gong2022ssast,Kazakos2021SlowFastAuditory,Chalk_2024_CVPR}, 
all trained on EPIC-Sounds. 
Tab.~\ref{tab:sound_recognition} %
shows a large gap in performance comparing \DName to EPIC-Sounds for SSAST (-28.4), ASF (-25.9) and TIM (-26.4).
This shows audio is not sufficiently robust to new scenes or devices.




\begin{table}[t]
    \footnotesize
    \centering
    \setlength{\tabcolsep}{6pt}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
         Model & Modality & Top-1 & Top-5 & mCA & mAP & mAUC \\
         \midrule
                 \rowcolor{LightGrey} \multicolumn{7}{l}{\textbf{EPIC-Sounds SOTA}} \\
         TIM~\cite{Chalk_2024_CVPR} & A+V & 58.3 & 86.0 & 25.8 & 30.6 & 0.879 \\
        \rowcolor{LightGrey} \multicolumn{7}{l}{\textbf{\DName}} \\
         Chance & - & 6.9 & 29.4  & 2.2 & 2.3 & 0.500 \\
         SSAST~\cite{gong2022ssast} & A & 25.1 & 59.8 & 10.8 & 13.5 & 0.748 \\
         TIM~\cite{Chalk_2024_CVPR} & A & 26.9 & 56.9 & 12.4 & 11.4 & 0.689 \\
         ASF~\cite{Kazakos2021SlowFastAuditory} & A & 27.9 & \textbf{64.0} & 11.9 & 14.0 & 0.741 \\
         TIM~\cite{Chalk_2024_CVPR} & A+V & \textbf{31.9} & 61.0 & \textbf{14.4} & \textbf{15.7} & \textbf{0.765} \\
        \bottomrule
    \end{tabular}}
    \vspace{-1em}
    \caption{\textbf{Sound Recognition Benchmark}. Current models struggle on \DName compared to the EPIC-Sounds state-of-the-art.}
    \vspace{-12pt}
    \label{tab:sound_recognition}
\end{table}



\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lccccccccccc}
    \toprule
           & \multicolumn{3}{c}{Total} && \multicolumn{3}{c}{Hands} && \multicolumn{3}{c}{Objects} \\
    \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
    Model & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J\&F}$ & & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J\&F}$ & & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J\&F}$  \\
    \midrule
    Static & 8.0 & 10.3 & 9.2 && 14.6 & 14.4 & 14.5 && 4.8 & 8.4 & 6.6 \\
    Cutie~\cite{cheng2024putting} & 44.8 & \textbf{52.3} & \textbf{48.6} && 74.8 & 79.5 & 77.2 && \textbf{30.1} & \textbf{39.0} & \textbf{34.6} \\
    SAM2~\cite{ravi2024sam} & \textbf{45.2} & 49.6 & 47.4 && \textbf{87.5} & \textbf{90.8} & \textbf{89.1} && 24.5 & 29.5 & 27.0 \\
    \bottomrule
    \end{tabular}}
    \includegraphics[width=0.9\linewidth]{figures/vos_benchmark_figure_2.png}
    \vspace{-0.7em}
    \caption{\textbf{Long-Term VOS}. jaccard index $\mathcal{J}$ \& contour accuracy $\mathcal{F}$ show Cutie and SAM2 struggle with segmenting objects.
    }
    \vspace{-12pt}
    \label{fig:vos_benchmark}
\end{figure}

\vspace{-0.3em}
\subsection{Long-Term VOS Benchmark}
\vspace{-0.2em}
\label{sec:benchmark-long}
We construct a long-term video object segmentation  benchmark using our segmentations and track associations (Sec. \ref{sec:long_term_obj_tracks}). Our benchmark has 1000 sequences, each with 1-5 objects and 2 hand masks (see Supp.~\ref{sec:benchmark_supp} for details). 
While we have a lot more tracks, we keep it comparable to current benchmarks in size.
We evaluate two models~\cite{ravi2024sam,cheng2024putting} with a naive baseline where object masks are kept static. 
Fig.~\ref{fig:vos_benchmark} shows the results. %
SAM2 \cite{ravi2024sam} surpasses Cutie \cite{cheng2024putting} for hands, but does worse on objects. Overall, objects have added challenge in
 diversity in perspective, lighting, location and occlusion.%


\vspace{-0.3em}
\section{Onwards...}
\vspace{-0.2em}
HD-EPIC is available from: \url{http://dx.doi.org/10.5523/bris.3cqb5b81wk2dc2379fx1mrxh47} -- \ie the videos, audio, gaze, blender digital twin, camera pose estimates. 
Annotations are available at: \url{http://hd-epic.github.io} -- \ie object movements, object masks and 3D locations, long-object tracks, and object-action-fixture labels.
We hope \DName will direct future research to a more holistic perception of egocentric videos.

\section*{Acknowledgements}



Research at Bristol is supported by EPSRC Fellowship UMPIRE (EP/T004991/1), EPSRC Program Grant Visual AI (EP/T028572/1) and EPSRC Doctoral Training Program.
O Emara, K Flanagan and F Abdelazim are supported by UKRI CD in Interactive AI (EP/S022937/1). 
The project is also supported by a unrestricted charitable donation from Meta (Aria Project Partnership) to the University of Bristol. 
Gemini Pro results are supported by a research credits grant from Google DeepMind.

Research at Leiden is supported by the Dutch Research Council (NWO) under a Veni grant (VI.Veni.222.160). Research at Singapore is supported by Singapore Ministry of Education (MOE) Academic Research Fund (AcRF)
Tier 1 grant (No. MSS23C018).


We thank Rajan from Elancer and his team, for their huge assistance with temporal and audio annotation.
We thank Srdjan Delic and his team for their assistance with mask annotations.
We thank Damian Steer and the University of Bristol's RDSF team for hosting and maintaining the dataset.
We also thank Owen Tyley for the 3D Digital Twin of the kitchen environments using Blender.

We thank David Fouhey and Evangelos Kazakos for early feedback on the project.
We thank Pierre Moulon, Vijay Baiyya and Cheng Peng from the Aria team for technical assistance in using the MPS code and services.

We acknowledge the usage of EPSRC Tier-2 Jade clusters.
The authors also acknowledge the use of Isambard-AI National AI Research Resource (AIRR). Isambard-AI is operated by the University of Bristol and is funded by the UK Government’s Department for Science, Innovation and Technology (DSIT) via UK Research and Innovation; and the Science and Technology Facilities Council [ST/AIRR/I-A-I/1023].
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

 \input{sec/X_suppl_Jan2025}

\end{document}
