[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      },
      {
        "key": "singh2022flava",
        "author": "Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe",
        "title": "{FLAVA: A foundational language and vision alignment model}"
      },
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      },
      {
        "key": "wang2023internimage",
        "author": "Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others",
        "title": "{InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions}"
      },
      {
        "key": "zhang2023video",
        "author": "Zhang, Hang and Li, Xin and Bing, Lidong",
        "title": "{Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "{GPT-4} technical report"
      },
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "{InternVL}: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "xiao2024florence",
        "author": "Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu",
        "title": "Florence-2: Advancing a unified representation for a variety of vision tasks"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "{The Llama 3 Herd of Models}"
      },
      {
        "key": "cheng2024videollama",
        "author": "Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others",
        "title": "{VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhao2024videoniah",
        "author": "Zhao, Zijia and Lu, Haoyu and Huo, Yuqi and Du, Yifan and Yue, Tongtian and Guo, Longteng and Wang, Bingning and Chen, Weipeng and Liu, Jing",
        "title": "{Needle In A Video Haystack: A Scalable  Synthetic Framework for Benchmarking Video MLLMs}"
      },
      {
        "key": "kesen2023vilma",
        "author": "Ilker Kesen and Andrea Pedrotti and Mustafa Dogan and Michele Cafagna and Emre Can Acikgoz and Letitia Parcalabescu and Iacer Calixto and Anette Frank and Albert Gatt and Aykut Erdem and Erkut Erdem",
        "title": "{ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models}"
      },
      {
        "key": "mangalam2023egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra",
        "title": "{EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}"
      },
      {
        "key": "patraucean2023perception",
        "author": "Viorica P\u0103tr\u0103ucean and Lucas Smaira and Ankush Gupta and Adri\u00e0 Recasens Continente and Larisa Markeeva and Dylan Banarse and Skanda Koppula and Joseph Heyward and Mateusz Malinowski and Yi Yang and Carl Doersch and Tatiana Matejovicova and Yury Sulsky and Antoine Miech and Alex Frechette and Hanna Klimczak and Raphael Koster and Junlin Zhang and Stephanie Winkler and Yusuf Aytar and Simon Osindero and Dima Damen and Andrew Zisserman and Jo\u00e3o Carreira",
        "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models"
      },
      {
        "key": "li2024vitatecs",
        "author": "Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu",
        "title": "{VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models}"
      },
      {
        "key": "li2024seed",
        "author": "Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying",
        "title": "{SEED-Bench: Benchmarking Multimodal Large Language Models}"
      },
      {
        "key": "li2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "{MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}"
      },
      {
        "key": "wang2024sok",
        "author": "Wang, Andong and Wu, Bo and Chen, Sunli and Chen, Zhenfang and Guan, Haotian and Lee, Wei-Ning and Li, Li Erran and Gan, Chuang",
        "title": "{SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge}"
      },
      {
        "key": "chen2024rextime",
        "author": "Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank",
        "title": "{ReXTime}: A benchmark suite for reasoning-across-time in videos"
      },
      {
        "key": "fang2024mmbench",
        "author": "Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai",
        "title": "{MMBench-Video}: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
      },
      {
        "key": "cai2024temporalbench",
        "author": "Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and Dou, Yao and Park, Jaden and Gao, Jianfeng and Lee, Yong Jae and Yang, Jianwei",
        "title": "{TemporalBench}: Towards Fine-grained Temporal Understanding for Multimodal Video Models"
      },
      {
        "key": "cores2024tvbench",
        "author": "Cores, Daniel and Dorkenwald, Michael and Mucientes, Manuel and Snoek, Cees GM and Asano, Yuki M",
        "title": "{TVBench}: Redesigning Video-Language Evaluation"
      },
      {
        "key": "Video-MME",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "{Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "patraucean2023perception",
        "author": "Viorica P\u0103tr\u0103ucean and Lucas Smaira and Ankush Gupta and Adri\u00e0 Recasens Continente and Larisa Markeeva and Dylan Banarse and Skanda Koppula and Joseph Heyward and Mateusz Malinowski and Yi Yang and Carl Doersch and Tatiana Matejovicova and Yury Sulsky and Antoine Miech and Alex Frechette and Hanna Klimczak and Raphael Koster and Junlin Zhang and Stephanie Winkler and Yusuf Aytar and Simon Osindero and Dima Damen and Andrew Zisserman and Jo\u00e3o Carreira",
        "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fang2024mmbench",
        "author": "Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai",
        "title": "{MMBench-Video}: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "cai2024temporalbench",
        "author": "Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and Dou, Yao and Park, Jaden and Gao, Jianfeng and Lee, Yong Jae and Yang, Jianwei",
        "title": "{TemporalBench}: Towards Fine-grained Temporal Understanding for Multimodal Video Models"
      },
      {
        "key": "chen2024rextime",
        "author": "Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank",
        "title": "{ReXTime}: A benchmark suite for reasoning-across-time in videos"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mangalam2023egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra",
        "title": "{EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}"
      },
      {
        "key": "Video-MME",
        "author": "Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others",
        "title": "{Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis}"
      },
      {
        "key": "fang2024mmbench",
        "author": "Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai",
        "title": "{MMBench-Video}: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Ego4D2022CVPR",
        "author": "Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Fuegen, Christian and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra",
        "title": "{Ego4D}: Around the {W}orld in 3,000 {H}ours of {E}gocentric {V}ideo"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "OpenEQA2023",
        "author": "Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and Yadav, Karmesh and Li, Qiyang and Newman, Ben and Sharma, Mohit and Berges, Vincent and Zhang, Shiqi and Agrawal, Pulkit and Bisk, Yonatan and Batra, Dhruv and Kalakrishnan, Mrinal and Meier, Franziska and Paxton, Chris and Sax, Sasha and Rajeswaran, Aravind",
        "title": "{OpenEQA: Embodied Question Answering in the Era of Foundation Models}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ramakrishnan2021habitat",
        "author": "Ramakrishnan, Santhosh K and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X and others",
        "title": "{Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dai2017scannet",
        "author": "Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\\ss}ner, Matthias",
        "title": "{ScanNet}: Richly-annotated {3D} reconstructions of indoor scenes"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "mangalam2023egoschema",
        "author": "Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra",
        "title": "{EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}"
      },
      {
        "key": "ye2024mm",
        "author": "Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others",
        "title": "{MM-Ego: Towards Building Egocentric Multimodal LLMs}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "cai2024temporalbench",
        "author": "Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and Dou, Yao and Park, Jaden and Gao, Jianfeng and Lee, Yong Jae and Yang, Jianwei",
        "title": "{TemporalBench}: Towards Fine-grained Temporal Understanding for Multimodal Video Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Ego4D2022CVPR",
        "author": "Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Fuegen, Christian and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra",
        "title": "{Ego4D}: Around the {W}orld in 3,000 {H}ours of {E}gocentric {V}ideo"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhao2024instance",
        "author": "Zhao, Yunhan and Ma, Haoyu and Kong, Shu and Fowlkes, Charless",
        "title": "{Instance tracking in 3D scenes from egocentric videos}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pan2023aria",
        "author": "Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Yuheng Carl",
        "title": "{Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "EgoExo4d",
        "author": "Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and Byrne, Eugene and Chavis, Zach and Chen, Joya and Cheng, Feng and Chu, Fu-Jen and Crane, Sean and Dasgupta, Avijit and Dong, Jing and Escobar, Maria and Forigua, Cristhian and Gebreselasie, Abrham and Haresh, Sanjay and Huang, Jing and Islam, Md Mohaiminul and Jain, Suyog and Khirodkar, Rawal and Kukreja, Devansh and Liang, Kevin J and Liu, Jia-Wei and Majumder, Sagnik and Mao, Yongsen and Martin, Miguel and Mavroudi, Effrosyni and Nagarajan, Tushar and Ragusa, Francesco and Ramakrishnan, Santhosh Kumar and Seminara, Luigi and Somayazulu, Arjun and Song, Yale and Su, Shan and Xue, Zihui and Zhang, Edward and Zhang, Jinxu and Castillo, Angela and Chen, Changan and Fu, Xinzhu and Furuta, Ryosuke and Gonzalez, Cristina and Gupta, Prince and Hu, Jiabo and Huang, Yifei and Huang, Yiming and Khoo, Weslie and Kumar, Anush and Kuo, Robert and Lakhavani, Sach and Liu, Miao and Luo, Mi and Luo, Zhengyi and Meredith, Brighid and Miller, Austin and Oguntola, Oluwatumininu and Pan, Xiaqing and Peng, Penny and Pramanick, Shraman and Ramazanova, Merey and Ryan, Fiona and Shan, Wei and Somasundaram, Kiran and Song, Chenan and Southerland, Audrey and Tateno, Masatoshi and Wang, Huiyu and Wang, Yuchen and Yagi, Takuma and Yan, Mingfei and Yang, Xitong and Yu, Zecheng and Zha, Shengxin Cindy and Zhao, Chen and Zhao, Ziwei and Zhu, Zhifan and Zhuo, Jeff and Arbelaez, Pablo and Bertasius, Gedas and Damen, Dima and Engel, Jakob and Farinella, Giovanni Maria and Furnari, Antonino and Ghanem, Bernard and Hoffman, Judy and Jawahar, C.V. and Newcombe, Richard and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Savva, Manolis and Shi, Jianbo and Shou, Mike Zheng and Wray, Michael",
        "title": "{Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives}"
      }
    ]
  }
]