\section{Related Work}
\vspace{-0.3em}

With the rise of foundation models ____, there has been a recent influx of %
benchmarks ____ aiming to test video understanding abilities. 
These benchmarks evaluate diverse  capabilities \eg physics ____, counting ____, temporal reasoning  ____ and long video ____.


A few benchmarks test embodied or egocentric understanding. 
____ released a Natural Language Queries (NLQ) benchmark (19.2K queries) centred around episodic memory of objects.
____~collects 1.6K human-made questions and answers on topics such as relative object locations, episodic memory, and spatial reasoning. However, it uses views from the HM3D ____ and ScanNet____ datasets, so these questions are based on passive views of a static environment. ____ auto-generate 5K and 7K questions based on Ego4D narrations. Whilst this approach is efficient, it is limited to these short narrations.
____ collects its own annotations for videos from several datasets, including Ego4D. %
Their benchmark is solely focused on temporal questions related to ordering, counting, causality and direction.


To evaluate a wider range of capabilities, a wider range of annotations are required. 
Of particular note are 3D grounding annotations.
Ego4D____ contains some environment scans and static 3D object locations. With SLAM-equipped devices ____ builds a benchmark for 3D object tracking; ____ contains an office and living room digital twin; and ____ contains ego- and exo- views of expert tasks.

In contrast to these works which focus on only a few annotation types, we collect the most comprehensive set of annotations in one dataset, including highly detailed narrations, object and hand segmentations, and a comprehensive 3D digital twin of the scene and objects, all from unscripted egocentric footage in participants' homes. 



\vspace{-0.5em}