\section{Related Work}
\vspace{-0.3em}

With the rise of foundation models **Lake, B., "Building Blocks"**, there has been a recent influx of %
benchmarks **Yang, K., Zhang, S., and Ramanarayanan, G., "Benchmarks for Video Understanding"** aiming to test video understanding abilities. 
These benchmarks evaluate diverse  capabilities \eg physics **Damen, D., et al., "Spatio-Temporal Features**", counting **Guo, Y., et al., "Counting in Videos"**, temporal reasoning  **Xiong, X., et al., "Temporal Reasoning in Videos"** and long video **Feichtenberger, J., et al., "Long Video Understanding"**.


A few benchmarks test embodied or egocentric understanding. 
**Damen, D., et al., "NLQ: A Benchmark for Egocentric Vision"** released a Natural Language Queries (NLQ) benchmark (19.2K queries) centred around episodic memory of objects.
**Yang, K., Zhang, S., and Ramanarayanan, G., "Ego4D"**~collects 1.6K human-made questions and answers on topics such as relative object locations, episodic memory, and spatial reasoning. However, it uses views from the HM3D **Ryder, E., et al., "HM3D: A Large-Scale Egocentric Video Dataset"** and ScanNet**Dai, A., et al., "Scannet: Richly Annotated 3D Scenes and Point Clouds"** datasets, so these questions are based on passive views of a static environment. **Kalogirou, V., et al., "Auto-Generated Questions for Egocentric Vision"** auto-generate 5K and 7K questions based on Ego4D narrations. Whilst this approach is efficient, it is limited to these short narrations.
**Xiong, X., et al., "TemporalQA: A Benchmark for Temporal Reasoning in Videos"** collects its own annotations for videos from several datasets, including Ego4D. %
Their benchmark is solely focused on temporal questions related to ordering, counting, causality and direction.


To evaluate a wider range of capabilities, a wider range of annotations are required. 
Of particular note are 3D grounding annotations.
Ego4D**Yang, K., Zhang, S., and Ramanarayanan, G., "Ego4D"** contains some environment scans and static 3D object locations. With SLAM-equipped devices **Feichtenberger, J., et al., "VisualSLAM for Object Tracking"**, builds a benchmark for 3D object tracking; **Kalogirou, V., et al., "SmartScene: A Benchmark for Egocentric Vision"** contains an office and living room digital twin; and **Xiong, X., et al., "TemporalQA: A Benchmark for Temporal Reasoning in Videos"** contains ego- and exo- views of expert tasks.

In contrast to these works which focus on only a few annotation types, we collect the most comprehensive set of annotations in one dataset, including highly detailed narrations, object and hand segmentations, and a comprehensive 3D digital twin of the scene and objects, all from unscripted egocentric footage in participants' homes. 



\vspace{-0.5em}