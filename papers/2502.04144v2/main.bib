@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})x
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{damen2018scaling,
  title={Scaling Egocentric Vision: The {EPIC-KITCHENS} Dataset},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2018}
}

@inproceedings{EPICSOUNDS2023,
           title={{EPIC-SOUNDS}: {A} {L}arge-{S}cale {D}ataset of {A}ctions that {S}ound},
           author={Huh, Jaesung and Chalk, Jacob and Kazakos, Evangelos and Damen, Dima and Zisserman, Andrew},
           booktitle   = {IEEE International Conference on Acoustics, Speech, \& Signal Processing (ICASSP)},
           year      = {2023}
}

@inproceedings{dutta2019vgg,
  author = {Dutta, Abhishek and Zisserman, Andrew},
  title = {The {VIA} Annotation Software for Images, Audio and Video},
  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia (ACMMM)},
  year = {2019},
  url = {https://doi.org/10.1145/3343031.3350535},
}

@manual{duta20lisa,
      title  = "VGG List Annotator (LISA)",
      note   = "https://www.robots.ox.ac.uk/~vgg/software/lisa/",
      year   = "2022"
}

@article{Somasundaram2023ProjectAA,
  title={{Project Aria: A New Tool for Egocentric Multi-Modal AI Research}},
  author={Kiran K. Somasundaram and Jing Dong and Huixuan Tang and Julian Straub and Mingfei Yan and Michael Goesele and Jakob J. Engel and Renzo De Nardi and Richard A. Newcombe},
  journal={arXiv preprint arXiv:2308.13561},
  year={2023},
}

@inproceedings{thames2021nutrition5k,
  title={Nutrition5k: Towards automatic nutritional understanding of generic food},
  author={Thames, Quin and Karpur, Arjun and Norris, Wade and Xia, Fangting and Panait, Liviu and Weyand, Tobias and Sim, Jack},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={8903--8911},
  year={2021}
}

@article{ravi2024sam,
  title={{SAM} 2: Segment anything in images and videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@article{qin2024question,
  title={Question-Answering Dense Video Events},
  author={Qin, Hangyu and Xiao, Junbin and Yao, Angela},
  journal={arXiv preprint arXiv:2409.04388},
  year={2024}
}

@InProceedings{EgoExo4d,
    author    = {Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and Byrne, Eugene and Chavis, Zach and Chen, Joya and Cheng, Feng and Chu, Fu-Jen and Crane, Sean and Dasgupta, Avijit and Dong, Jing and Escobar, Maria and Forigua, Cristhian and Gebreselasie, Abrham and Haresh, Sanjay and Huang, Jing and Islam, Md Mohaiminul and Jain, Suyog and Khirodkar, Rawal and Kukreja, Devansh and Liang, Kevin J and Liu, Jia-Wei and Majumder, Sagnik and Mao, Yongsen and Martin, Miguel and Mavroudi, Effrosyni and Nagarajan, Tushar and Ragusa, Francesco and Ramakrishnan, Santhosh Kumar and Seminara, Luigi and Somayazulu, Arjun and Song, Yale and Su, Shan and Xue, Zihui and Zhang, Edward and Zhang, Jinxu and Castillo, Angela and Chen, Changan and Fu, Xinzhu and Furuta, Ryosuke and Gonzalez, Cristina and Gupta, Prince and Hu, Jiabo and Huang, Yifei and Huang, Yiming and Khoo, Weslie and Kumar, Anush and Kuo, Robert and Lakhavani, Sach and Liu, Miao and Luo, Mi and Luo, Zhengyi and Meredith, Brighid and Miller, Austin and Oguntola, Oluwatumininu and Pan, Xiaqing and Peng, Penny and Pramanick, Shraman and Ramazanova, Merey and Ryan, Fiona and Shan, Wei and Somasundaram, Kiran and Song, Chenan and Southerland, Audrey and Tateno, Masatoshi and Wang, Huiyu and Wang, Yuchen and Yagi, Takuma and Yan, Mingfei and Yang, Xitong and Yu, Zecheng and Zha, Shengxin Cindy and Zhao, Chen and Zhao, Ziwei and Zhu, Zhifan and Zhuo, Jeff and Arbelaez, Pablo and Bertasius, Gedas and Damen, Dima and Engel, Jakob and Farinella, Giovanni Maria and Furnari, Antonino and Ghanem, Bernard and Hoffman, Judy and Jawahar, C.V. and Newcombe, Richard and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Savva, Manolis and Shi, Jianbo and Shou, Mike Zheng and Wray, Michael},
    title     = {{Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives}},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2024}
}

@INPROCEEDINGS{Ego4D2022CVPR,
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Fuegen, Christian and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
  title     = {{Ego4D}: Around the {W}orld in 3,000 {H}ours of {E}gocentric {V}ideo},
  booktitle   = {IEEE/CVF Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022}
}

@ARTICLE{Damen2022RESCALING,
           title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for {EPIC-KITCHENS-100}},
           author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino 
           and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
           journal   = {International Journal of Computer Vision (IJCV)},
           year      = {2022},
           Url       = {https://doi.org/10.1007/s11263-021-01531-2}
} 

@inproceedings{Kazakos2021SlowFastAuditory,
   title={{Slow-Fast Auditory Streams For Audio Recognition}},
   author={Kazakos, Evangelos and Nagrani, Arsha and Zisserman, Andrew and Damen, Dima},
           booktitle   = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
           volume    = {abs/2103.03516},
           year      = {2021},
           ee        = {https://arxiv.org/abs/2103.03516},
}

@InProceedings{Chalk_2024_CVPR,
    author    = {Chalk, Jacob and Huh, Jaesung and Kazakos, Evangelos and Zisserman, Andrew and Damen, Dima},
    title     = {{TIM}: {A} {T}ime {I}nterval {M}achine for {A}udio-{V}isual {A}ction {R}ecognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024}
}

@inproceedings{gong2022ssast,
  title={{SSAST: Self-Supervised Audio Spectrogram Transformer}},
  author={Gong, Yuan and Lai, Cheng-I and Chung, Yu-An and Glass, James},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},  
  year={2022}
}

@inproceedings{girdhar2022omnivore,
  title={{Omnivore: A Single Model for Many Visual Modalities}},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and van der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@InProceedings{Feichtenhofer_2019_ICCV,
author = {Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
title = {{SlowFast} Networks for Video Recognition},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{patrick2021keeping,
   title={Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers}, 
   author={Mandela Patrick and Dylan Campbell and Yuki M. Asano and Ishan Misra Florian Metze and Christoph Feichtenhofer and Andrea Vedaldi and João F. Henriques},
   year={2021},
   booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{kitaev2018constituency,
  title={Constituency parsing with a self-attentive encoder},
  author={Kitaev, Nikita and Klein, Dan},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2018}
}

@article{honnibal2017spacy,
  title={{spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing}},
  author={Honnibal, Matthew and Montani, Ines},
  year={2020},
  doi={"10.5281/zenodo.1212303"}
}

@article{song2024ego4d,
  title={{Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities}},
  author={Song, Yale and Byrne, Eugene and Nagarajan, Tushar and Wang, Huiyu and Martin, Miguel and Torresani, Lorenzo},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@InProceedings{HoloAssist2023,
    author    = {Wang, Xin and Kwon, Taein and Rad, Mahdi and Pan, Bowen and Chakraborty, Ishani and Andrist, Sean and Bohus, Dan and Feniello, Ashley and Tekin, Bugra and Frujeri, Felipe Vieira and Joshi, Neel and Pollefeys, Marc},
    title     = {{HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World}},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2023}
}

@inproceedings{banerjee2024hot3d,
  title={{HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos}},
  author={Banerjee, Prithviraj and Shkodrani, Sindi and Moulon, Pierre and Hampali, Shreyas and Zhang, Fan and Fountain, Jade and Miller, Edward and Basol, Selen and Newcombe, Richard and Wang, Robert and Engel, Jakob Julian and Hodan, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}

@inproceedings{lai2023lego,
  title={{LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning}},
  author={Lai, Bolin and Dai, Xiaoliang and Chen, Lawrence and Pang, Guan and Rehg, James M and Liu, Miao},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2023}
}

@inproceedings{VISOR2022,
           title={{EPIC-KITCHENS VISOR} Benchmark: VIdeo Segmentations and Object Relations},
           author={Darkhalil, Ahmad and Shan, Dandan and Zhu, Bin and Ma, Jian and Kar, Amlan and Higgins, Richard and Fidler, Sanja and Fouhey, David and Damen, Dima},
           booktitle   = {Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks},
           year      = {2022}
} 

@inproceedings{Plizzari2024, title={{Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind}},
author={Plizzari, Chiara and Goel, Shubham and Perrett, Toby and Chalk, Jacob and Kanazawa, Angjoo and Damen, Dima},
booktitle={{Proceedings of the IEEE International Conference on 3D Vision (3DV)}},
year={2025} }

@inproceedings{pirsiavash2012detecting,
  title={Detecting activities of daily living in first-person camera views},
  author={Pirsiavash, Hamed and Ramanan, Deva},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2012},
}

@inproceedings{perrett2024unique,
          title={It's Just Another Day: Unique Video Captioning by Discriminitave Prompting},
          author={Perrett, Toby and Han, Tengda and Damen, Dima and Zisserman, Andrew},
          booktitle={Proceedings of the Asian Conference on Computer Vision (ACCV)},
          year={2024},
        }

@inproceedings{lee2012discovering,
  title={Discovering important people and objects for egocentric video summarization},
  author={Lee, Yong Jae and Ghosh, Joydeep and Grauman, Kristen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year={2012},
}

@inproceedings{fathi2012social,
  title={Social interactions: A first-person perspective},
  author={Fathi, Alireza and Hodgins, Jessica K and Rehg, James M},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year={2012},  
}

@inproceedings{singh2016krishnacam,
  title={Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks},
  author={Singh, Krishna Kumar and Fatahalian, Kayvon and Efros, Alexei A},
  booktitle={Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)},  
  year={2016},  
}

@article{ragusa2020ego,
  title={{EGO-CH: Dataset and fundamental tasks for visitors behavioral understanding using egocentric vision}},
  author={Ragusa, Francesco and Furnari, Antonino and Battiato, Sebastiano and Signorello, Giovanni and Farinella, Giovanni Maria},
  journal={Pattern Recognition Letters},
  year={2020},  
}

@article{li2021eye,
  title={In the eye of the beholder: Gaze and actions in first person video},
  author={Li, Yin and Liu, Miao and Rehg, James M},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year={2021},
}

@inproceedings{liu2022hoi4d,
  title={{HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction}},
  author={Liu, Yunze and Liu, Yun and Jiang, Che and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Fu, Zhoujie and Wang, He and Yi, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@inproceedings{sener2022assembly101,
  title={Assembly101: A large-scale multi-view video dataset for understanding procedural activities},
  author={Sener, Fadime and Chatterjee, Dibyadip and Shelepov, Daniel and He, Kun and Singhania, Dipika and Wang, Robert and Yao, Angela},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year={2022}
}

@inproceedings{ragusa2021meccano,
  title={{The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain}},
  author={Ragusa, Francesco and Furnari, Antonino and Livatino, Salvatore and Farinella, Giovanni Maria},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2021}
}

@inproceedings{silva2018weighted,
  title={A weighted sparse sampling and smoothing frame transition approach for semantic fast-forward first-person videos},
  author={Silva, Michel and Ramos, Washington and Ferreira, Joao and Chamone, Felipe and Campos, Mario and Nascimento, Erickson R},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@inproceedings{schoonbeek2024industreal,
  title={{IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting}},
  author={Schoonbeek, Tim J and Houben, Tim and Onvlee, Hans and van der Sommen, Fons and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2024}
}

@inproceedings{zhou2018towards,
  title={Towards automatic learning of procedures from web instructional videos},
  author={Zhou, Luowei and Xu, Chenliang and Corso, Jason},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{Video-MME,
  title={{Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis}},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}

@inproceedings{tong2022videomae,
  title={Video{MAE}: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author={Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{zhang2024long,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{cheng2024videollama,
  title={{VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs}},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{dubey2024llama,
  title={{The Llama 3 Herd of Models}},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{sigurdsson2018actor,
  title={Actor and observer: Joint modeling of first and third-person videos},
  author={Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}

@inproceedings{pan2023aria,
  title={{Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception}},
  author={Pan, Xiaqing and Charron, Nicholas and Yang, Yongqian and Peters, Scott and Whelan, Thomas and Kong, Chen and Parkhi, Omkar and Newcombe, Richard and Ren, Yuheng Carl},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023}
}

@article{straub2024efm3d,
  title={{EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models}},
  author={Straub, Julian and DeTone, Daniel and Shen, Tianwei and Yang, Nan and Sweeney, Chris and Newcombe, Richard},
  journal={arXiv preprint arXiv:2406.10224},
  year={2024}
}


@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023}
}

@inproceedings{cheng2024putting,
  title={Putting the object back into video object segmentation},
  author={Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Lee, Joon-Young and Schwing, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@article{de2009guide,
  title={{Guide to the Carnegie Mellon University Multimodal activity (CMU-MMAC) database}},
  author={De la Torre, Fernando and Hodgins, Jessica and Bargteil, Adam and Martin, Xavier and Macey, Justin and Collado, Alex and Beltran, Pep},
  year={2009},
}

@article{lv2024aria,
  title={{Aria Everyday Activities Dataset}},
  author={Lv, Zhaoyang and Charron, Nickolas and Moulon, Pierre and Gamino, Alexander and Peng, Cheng and Sweeney, Chris and Miller, Edward and Tang, Huixuan and Meissner, Jeff and Dong, Jing and others},
  journal={arXiv preprint arXiv:2402.13349},
  year={2024}
}


@misc{projectaria,
      title  ={Project Aria academic partners},
      howpublished   =  {https://www.projectaria.com/research-kit/}
}

@misc{ariamps,
  title = {Project Aria Machine Perception Services},
  howpublished = {\url{https://facebookresearch.github.io/projectaria\_tools/docs/ARK/mps}}
}

@inproceedings{EPICFields2023,
           title={{EPIC Fields}: {M}arrying {3D} {G}eometry and {V}ideo {U}nderstanding},
           author={Tschernezki, Vadim and Darkhalil, Ahmad and Zhu, Zhifan and Fouhey, David and Larina, Iro and Larlus, Diane and Damen, Dima and Vedaldi, Andrea},
           booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
           year      = {2023}
} 

@inproceedings{schoenberger2016sfm,
    author={Sch\"{o}nberger, Johannes Lutz and Frahm, Jan-Michael},
    title={Structure-from-Motion Revisited},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016},
}

@inproceedings{schoenberger2016mvs,
    author={Sch\"{o}nberger, Johannes Lutz and Zheng, Enliang and Pollefeys, Marc and Frahm, Jan-Michael},
    title={Pixelwise View Selection for Unstructured Multi-View Stereo},
    booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    year={2016},
}

@misc{myfitness,
  title = {My Fitness App},
  howpublished = {\url{https://www.myfitnesspal.com/}}
}

@inproceedings{perazzi2016benchmark,
  title={A benchmark dataset and evaluation methodology for video object segmentation},
  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023},
}

@article{chu2023qwen,
  title={{Qwen-Audio}: Advancing universal audio understanding via unified large-scale audio-language models},
  author={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.07919},
  year={2023}
}

@article{kuchaiev2019nemo,
  title={{NeMo: a toolkit for building AI applications using Neural Modules}},
  author={Kuchaiev, Oleksii and Li, Jason and Nguyen, Huyen and Hrinchuk, Oleksii and Leary, Ryan and Ginsburg, Boris and Kriman, Samuel and Beliaev, Stanislav and Lavrukhin, Vitaly and Cook, Jack and Castonguay, Patrice and
Popova, Mariya and Huang, Jocelyn and Cohen, Jonathan M.},
  journal={arXiv preprint arXiv:1909.09577},
  year={2019}
}

@article{JUST1976441,
title = {Eye fixations and cognitive processes},
journal = {Cognitive Psychology},
volume = {8},
number = {4},
pages = {441-480},
year = {1976},
issn = {0010-0285},
doi = {https://doi.org/10.1016/0010-0285(76)90015-3},
url = {https://www.sciencedirect.com/science/article/pii/0010028576900153},
author = {Marcel Adam Just and Patricia A Carpenter},
abstract = {This paper presents a theoretical account of the sequence and duration of eye fixation during a number of simple cognitive tasks, such as mental rotation, sentence verification, and quantitative comparison. In each case, the eye fixation behavior is linked to a processing model for the task by assuming that the eye fixates the referent of the symbol being operated on.}
}

@article{hoffman1998visual,
  title={Visual attention and eye movements.},
  author={Hoffman, James E},
  year={1998},
  publisher={Psychology Press/Erlbaum (UK) Taylor \& Francis}
}

@article{johansson2001eyehand,
author = {Johansson, Roland and Westling, Göran and Bäckström, Anders and Flanagan, John},
year = {2001},
title = {{Eye–Hand Coordination in Object Manipulation}},
journal = {The Journal of neuroscience: the official journal of the Society for Neuroscience},
}

@inproceedings{wang2023internimage,
  title={{InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions}},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023}
}

@inproceedings{singh2022flava,
  title={{FLAVA: A foundational language and vision alignment model}},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@inproceedings{chen2024internvl,
  title={{InternVL}: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@inproceedings{xiao2024florence,
  title={Florence-2: Advancing a unified representation for a variety of vision tasks},
  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{zhang2023video,
  title={{Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{achiam2023gpt,
  title={{GPT-4} technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},
}

@inproceedings{li2024seed,
  title={{SEED-Bench: Benchmarking Multimodal Large Language Models}},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{patraucean2023perception,
      title={Perception Test: A Diagnostic Benchmark for Multimodal Video Models}, 
      author={Viorica Pătrăucean and Lucas Smaira and Ankush Gupta and Adrià Recasens Continente and Larisa Markeeva and Dylan Banarse and Skanda Koppula and Joseph Heyward and Mateusz Malinowski and Yi Yang and Carl Doersch and Tatiana Matejovicova and Yury Sulsky and Antoine Miech and Alex Frechette and Hanna Klimczak and Raphael Koster and Junlin Zhang and Stephanie Winkler and Yusuf Aytar and Simon Osindero and Dima Damen and Andrew Zisserman and João Carreira},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
      year={2023},
}

@article{land1999roles,
  title={The roles of vision and eye movements in the control of activities of daily living},
  author={Land, Michael and Mennie, Neil and Rusted, Jennifer},
  journal={Perception},  
  year={1999},
}

@article{mangalam2023egoschema,
  title={{EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{li2024vitatecs,
  title={{VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models}},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2024}
}

@inproceedings{li2024mvbench,
  title={{MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@inproceedings{kesen2023vilma,
    title={{ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models}},
    author={Ilker Kesen and Andrea Pedrotti and Mustafa Dogan and Michele Cafagna and Emre Can Acikgoz and Letitia Parcalabescu and Iacer Calixto and Anette Frank and Albert Gatt and Aykut Erdem and Erkut Erdem},
    year={2024},
    booktitle={International Conference on Learning Representations (ICLR)},
}

@inproceedings{wang2024sok,
  title={{SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge}},
  author={Wang, Andong and Wu, Bo and Chen, Sunli and Chen, Zhenfang and Guan, Haotian and Lee, Wei-Ning and Li, Li Erran and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@inproceedings{chen2024rextime,
   title={{ReXTime}: A benchmark suite for reasoning-across-time in videos},
   author={Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank},
   booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
   year={2024}
}

@inproceedings{zhao2024videoniah,
  title={{Needle In A Video Haystack: A Scalable  Synthetic Framework for Benchmarking Video MLLMs}},
  author={Zhao, Zijia and Lu, Haoyu and Huo, Yuqi and Du, Yifan and Yue, Tongtian and Guo, Longteng and Wang, Bingning and Chen, Weipeng and Liu, Jing},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@inproceedings{fang2024mmbench,
  title={{MMBench-Video}: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
   booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@article{cai2024temporalbench,
      title={{TemporalBench}: Towards Fine-grained Temporal Understanding for Multimodal Video Models},
      author={Cai, Mu and Tan, Reuben and Zhang, Jianrui and Zou, Bocheng and Zhang, Kai and Yao, Feng and Zhu, Fangrui and Gu, Jing and Zhong, Yiwu and Shang, Yuzhang and Dou, Yao and Park, Jaden and Gao, Jianfeng and Lee, Yong Jae and Yang, Jianwei},
      journal={arXiv preprint arXiv:2410.10818},
      year={2024}
}

@inproceedings{ye2024mm,
  title={{MM-Ego: Towards Building Egocentric Multimodal LLMs}},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

 @inproceedings{OpenEQA2023,
        title         = {{OpenEQA: Embodied Question Answering in the Era of Foundation Models}}, 
        booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        author        = {Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and Yadav, Karmesh and Li, Qiyang and Newman, Ben and Sharma, Mohit and Berges, Vincent and Zhang, Shiqi and Agrawal, Pulkit and Bisk, Yonatan and Batra, Dhruv and Kalakrishnan, Mrinal and Meier, Franziska and Paxton, Chris and Sax, Sasha and Rajeswaran, Aravind},
        year          = {2024},
    }

@inproceedings{zhao2024instance,
  title={{Instance tracking in 3D scenes from egocentric videos}},
  author={Zhao, Yunhan and Ma, Haoyu and Kong, Shu and Fowlkes, Charless},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@article{cores2024tvbench,
  title={{TVBench}: Redesigning Video-Language Evaluation},
  author={Cores, Daniel and Dorkenwald, Michael and Mucientes, Manuel and Snoek, Cees GM and Asano, Yuki M},
  journal={arXiv preprint arXiv:2410.07752},
  year={2024}
}

@inproceedings{xiao2024can,
  title={{Can I trust your answer? Visually grounded video question answering}},
  author={Xiao, Junbin and Yao, Angela and Li, Yicong and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}

@inproceedings{depthanything,
  title={{Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}},
  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}
@misc{torontoannotsuite,
  title = {Toronto Annotation Suite},
  howpublished = {\url{https://aidemos.cs.toronto.edu/toras}},
  author = {
     Kar, Amlan and
     Kim, Seung Wook and
     Boben, Marko and
     Gao, Jun and
     Li, Tianxing and
     Ling, Huan and
     Wang, Zian and
     Fidler, Sanja},
  year = {2021}
}

@inproceedings{dai2017scannet,
  title={{ScanNet}: Richly-annotated {3D} reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{ramakrishnan2021habitat,
  title={{Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI}},
  author={Ramakrishnan, Santhosh K and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alex and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, Angel X and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year={2021}
}

@article{xu2018youtube,
  title={You{T}ube-{VOS}: A large-scale video object segmentation benchmark},
  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},
  journal={arXiv preprint arXiv:1809.03327},
  year={2018}
}

@manual{blender,
      title  = "Blender",
      note   = {\url{https://www.blender.org/}},
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1728--1738},
  year={2021}
}

@inproceedings{yi2024egoallo,
    title={Estimating Body and Hand Motion in an Ego-sensed World},
    author={Brent Yi and Vickie Ye and Maya Zheng and Lea M\"uller and Georgios Pavlakos and Yi Ma and Jitendra Malik and Angjoo Kanazawa},
    year={2025},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
} 

@inproceedings{captaincook4d,
                title={{CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities}},
                author={Rohith Peddi and Shivvrat Arya and Bharath Challa and Likhitha Pallapothula and Akshay Vyas and Bhavya Gouripeddi and Jikai Wang and Qifan Zhang and Vasundhara Komaragiri and Eric Ragan and Nicholas Ruozzi and Yu Xiang and Vibhav Gogate},
                year={2024},
           booktitle   = {Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks},
}

@article{zhang2024videoinstructiontuningsynthetic,
    title={Video Instruction Tuning With Synthetic Data}, 
    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
    year={2024},
    journal={arXiv preprint arXiv:2410.02713},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.02713}, 
}

@misc{vos2019,
  author       = "Linjie Yang and Yuchen Fan and Ning Xu",
  title        = "The 2nd Large-scale Video Object Segmentation Challenge - video object segmentation track",
  year         = "2019",
}

@article{Pont-Tuset_arXiv_2017,
              author = {Jordi Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\'aez and Alexander Sorkine-Hornung and Luc {Van Gool}},
              title = {The 2017 DAVIS Challenge on Video Object Segmentation},
              journal = {arXiv:1704.00675},
              year = {2017}
            }