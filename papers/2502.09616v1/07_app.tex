\section*{Appendix: Variational Rectified Flow Matching}

This appendix is structured as follows: 
in \cref{app:preserve_marginal} we show that our approach maintains the marginal distribution; 
in \cref{app:addrel} we discuss additional related work;
in \cref{app:addexp} we provide additional experimental analysis;
in \cref{app:implement_all} we provide more implementation details; 
in \cref{app:addqual} we list additional qualitative results.

\section{On Preserving the Marginal Data Distribution}
\label{app:preserve_marginal}
We obtain samples by numerically solving the ordinary differential equation
$$
du_t = v_\theta(x_t,t,z)dt \quad\text{with}\quad z\sim p(z) = {\cal N}(z;0,I).
$$
This differs slightly from Theorem 3.3 of \citet{liu2023flow} because the velocity $v_\theta$ depends on a latent variable $z$ drawn from a standard Gaussian. 
However, Theorem 3.3 of \citet{liu2023flow} can be extended to fit this setting as follows.

First, note that we have $v^\ast(x_t,t,z) = \mathbb{E}[\dot X_t|X_t,Z]$ where $X_t$ and $Z$ are random variables corresponding to instances $x_t$ and $z$.

Incorporating the velocity field depending on the latent variable $z$ into the transport problem defined in \cref{eq:transportpde} and taking an expectation over the latent variable, we obtain the continuity equation
\begin{equation}
    \dot p_t + \di (\mathbb{E}_Z[v_\theta(x_t,t,z)]p_t) = 0.
    \label{eq:app:cont}
\end{equation}
Following \citet{liu2023flow}, one can show equivalence to the following equality, which uses any compactly supported continuously differentiable test function $h$:
$$
\frac{d}{dt}\mathbb{E}[h(X_t)] = \mathbb{E}[\nabla h(X_t)^T \dot X_t] = \mathbb{E}[\nabla h(X_t)^T v^\ast(X_t,t)] = \mathbb{E}_X[\nabla h(X_t)^T \mathbb{E}_Z[v^\ast(X_t,t,Z)]].
$$
Concretely, equivalence can be shown via
$$
0 = \mathbb{E}_Z\left(\int_{x_t}h(\dot p_t + \di (v^\ast(X_t,t,Z)p_t)\right) = \frac{d}{dt}\mathbb{E}[h(X_t)] - \mathbb{E}_X[\nabla h(X_t)^T \mathbb{E}_Z[v^\ast(X_t,t,Z)]].
$$

Note, different from \citet{liu2023flow}, in our case $U_t$ is driven by a velocity field $v(x_t,t,z)$ that depends on a latent variable. 
Averaging over  instantiations of the random latent variable $Z$ leads to the 
same marginal velocity that appears in the continuity equation (\cref{eq:app:cont}). Therefore, we solve the same equation with the same initial condition ($X_0 = U_0$). Equivalence follows if the solution to \cref{eq:app:cont} is unique.


\section{Additional Related Work Discussion}
\label{app:addrel}




\begin{wrapfigure}{r}{0.3\textwidth}
    \vspace{-5mm}
    \centering
    \includegraphics[width=\linewidth]{figs/1d/baseline_std_ALL_consistency_FM_num2.png} %
    \vspace{-7mm}
    \captionof{figure}{Velocity distribution of consistency flow matching~\citep{yang2024consistency}.}
    \label{fig:appendix_fm_v_distribution}
    \vspace{-1em}
\end{wrapfigure}
Here, we discuss related work aimed at improving the sample efficiency of diffusion and flow matching models, either via consistency modeling or via distillation. %




\noindent\textbf{Consistency models.} Consistency models, such as those by \citet{song2023consistency} and \citet{yang2024consistency}, enforce self-consistency across timesteps, ensuring trajectories map back to the same initial point. %
Moreover, \citet{kimconsistency} ensure consistent trajectories for probability flow ODEs. 
While consistency models focus on improving results via trajectory alignment if few function evaluations are used, they don't model the multi-modal ground-truth velocity distribution, which is our goal. %



To illustrate this, we train the recently developed consistency flow matching model proposed by \citet{yang2024consistency} (which improves upon work by \citet{song2023consistency} and \citet{kimconsistency}; both are not flow matching based; it also improves upon distillation work by \citet{nguyenbellman}) on the data for which V-RFM results are presented in \cref{fig:1d_analysis,fig:1d_unimodal_to_bimodal}. Specifically, we used the publicly available baseline.\footnote{https://github.com/YangLing0818/consistency\_flow\_matching} We obtain the results illustrated in \cref{fig:appendix_fm_v_distribution}. As expected, we observe that classic consistency modeling does not capture the multi-modal velocity distribution, unlike the proposed V-RFM.

Furthermore, we conduct additional experiments with consistency flow matching across multiple datasets, summarizing the results in \cref{app:consistency_model}. We observe that the consistency flow matching method performs well in the low function evaluation regime (i.e., NFE = 2 or 5), but its performance degrades as the NFEs increase. Most notably, its best performance across all NFEs does not surpass that of classic rectified flow matching or our proposed variational rectified flow matching. Based on the empirical evidence and the key differences in capturing multi-modal velocity distributions, we believe consistency models are orthogonal to our proposed variational formulation. Therefore, we find it exciting to explore future research on combining variational flow matching with consistency models, which is beyond the scope of this paper.



\noindent\textbf{Distillation.} \citet{nguyenbellman} perform distillation by optimizing step sizes in pretrained flow-matching models to refine trajectories and improve training dynamics. Moreover, \citet{yan2024perflow} perform distillation by introduceing a piecewise rectified flow mechanism to accelerate flow-based generative models. Note, both methods distill useful information from a pretrained model, either by using dynamic programming to optimize the step size or by applying reflow to straighten trajectories, i.e., they focus on distilling already learned models. In contrast, our V-RFM focuses on learning via single-stage training, directly from ground-truth data, and without use of a pre-trained deep net, a flow-matching model, which captures a multi-modal velocity distribution. 
More research on the distillation of a V-RFM model is required to assess how multi-modality can be maintained in the second distillation step. We think this is exciting future research, which is beyond the scope of this paper.









\section{Additional Experimental Results and Analysis}
\label{app:addexp}

\subsection{Comparison to Consistency Flow Matching}
\label{app:consistency_model}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/1d/comparison_plot_1d_num2.pdf} \\
    \vspace{-1em}
    \caption{Additional quantitative evaluation with the consistency flow matching baseline on synthetic 1D data. Higher values are better for True and Parzen Window Log-Likelihood, while lower values are preferred for Wasserstein Distance.}
    \label{app:fig:1d_quant_result}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.28\textwidth} %
        \captionof{figure}{Additional quantitative evaluation with the consistency flow matching baseline on synthetic 2D data. Metrics are averaged over three runs with different random seeds.}
        \label{app:fig:2d_quant_result}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.7\textwidth} %
        \centering
        \includegraphics[width=0.9\textwidth]{figs/2d/comparison_plot_2d_num2.pdf} %
    \end{minipage}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.5\textwidth} %
        \captionof{figure}{FID score evaluation for the MNIST experiment, including the additional consistency flow matching baseline. Our model with a latent dimension of 2 outperforms the baselines, except at 2 evaluation steps where Consistency FM performs best. %
        Note, the latent dimension of 2 is chosen for a controllability analysis rather than being optimized for FID score improvement. }
        \label{app:fig:mnist_quant}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\textwidth} %
        \centering
        \includegraphics[width=0.9\textwidth]{figs/mnist/comparison_plot_mnist_num2.pdf} %
    \end{minipage}
\end{figure}

We conduct additional experiments to compare our approach with  consistency models across multiple datasets. For this, we use the recently developed consistency flow matching model from \citet{yang2024consistency} as a representative baseline, as it advances earlier consistency modeling efforts by \citet{song2023consistency, kimconsistency} and distillation work by \citet{nguyenbellman}. Specifically, we used the publicly available implementation.\footnote{https://github.com/YangLing0818/consistency\_flow\_matching}

The results are summarized as follows: Synthetic 1D data in \cref{app:fig:1d_quant_result}, Synthetic 2D data in \cref{app:fig:2d_quant_result}, MNIST data in \cref{app:fig:mnist_quant}, and CIFAR-10 data in \cref{app:tab:cifar10}. These results demonstrate that V-RFM outperforms the consistency flow matching baseline across various evaluation steps for synthetic data, with V-RFM showing superior performance when the number of evaluation steps exceeds 2 for MNIST and 5 for CIFAR-10. Importantly, while consistency flow matching achieves strong performance for a low number of evaluation steps, its best performance still does not surpass that of  classic rectified flow matching or our proposed variational rectified flow matching with a high number of evaluation steps. This highlights its distinct nature as an orthogonal research direction to our method. As discussed in \cref{app:addrel}, we believe that combining variational formulations with consistency models presents an exciting avenue for future research, though it is beyond the scope of this paper.



\begin{table}[t]
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{cccccccccc}
        \toprule
        & NFE / sample & $\#$ Params. & 2 & 5 & 10 & 50 & 100 & 1000 & Adaptive \\
        \midrule
        & \begin{tabular}{@{}c@{}}OT-FM \\ ~\citep{LipmanICLR2023,tongimproving} \end{tabular} & 36.5M & 166.655 & 36.188 & 14.396 & 5.557 & 4.640 & 3.822 & 3.655\\
        & \begin{tabular}{@{}c@{}}I-CFM \\ ~\citep{liu2023flow, tongimproving} \end{tabular}& 36.5M & 168.654 & 35.489 & 13.788 & \underline{5.288} & 4.461 & 3.643 & 3.659 \\
        \midrule
        & \begin{tabular}{@{}c@{}}Consistency-FM \\ ~\citep{yang2024consistency} \end{tabular}& 36.5M & \underline{15.758} & \underline{14.588} & 24.107 & 36.800 & 38.675 & 40.486 & 40.711\\
        & \begin{tabular}{@{}c@{}}Consistency-FM-XL \\ ~\citep{yang2024consistency} \end{tabular} & 61.8M & \textbf{5.323} & \textbf{11.412} & 23.948 & 36.652 & 38.680 & 40.402 & 40.677\\
        \midrule
        \texttt{1}& V-RFM (adaptive norm, $x_1$, 2e-3)& 37.2M & 135.275 & 28.912 & \textbf{13.226} & 5.382 & \underline{4.430} & 3.642 & 3.545\\
        \texttt{2}& V-RFM (adaptive norm, $x_1$, 5e-3)& 37.2M & 159.940 & 35.293 & 14.061 & \textbf{5.265} & \textbf{4.349} & \textbf{3.582} & 3.561\\
        \texttt{3}& V-RFM (adaptive norm, $x_1 + t$, 5e-3)& 37.2M  & 117.666 & 27.464 & 13.632 & 5.512 & 4.484 & 3.614 & \textbf{3.478}\\
        \texttt{4}& V-RFM (bottleneck sum, $x_1 + t$, 2e-3)& 37.0M  & 104.634 & 25.841 & \underline{13.508} & 5.618 & 4.540 & \underline{3.596} & \underline{3.520}\\
        \bottomrule
    \end{tabular}
    \caption{Additional quantitative evaluation of the consistency flow matching baseline on CIFAR-10. The consistency flow matching method performs well in the low function evaluation regime (NFE = 2 or 5), but its performance degrades as NFEs increase. Notably, its best performance across all NFEs does not surpass that of classic rectified flow matching (OT-FM, I-CFM) or our proposed variational rectified flow matching (V-RFM).}
    \label{app:tab:cifar10}
\end{table}

\subsection{1D Velocity Ambiguity Analysis}
\label{app:full_1d_analysis}
As discussed in \cref{sec:method:train}, the posterior \(q_\phi\) can be conditioned in different ways. To understand the implications, we performed ablation studies and visualize the velocity distribution maps in \cref{fig:1d_analysis_complete} (c)-(f). For \(x_0\) conditioning (d), the model struggles to predict the bi-modal distribution at early timesteps (\(x_t=0.0, t=0.0\)) due to the absence of \(x_1\) information. However, when \(t\) is sufficiently large, the model can infer \(x_1\) from \(x_t\), enabling it to predict a bi-modal distribution again at (\(x=0.0, t=0.5\)). Conversely, with \(x_1\) conditioning (e), the model fails to capture the ground-truth distribution at later timesteps (\(x=-1.0, t=0.95\)) as the influence of \(x_1\) diminishes. With \(x_t\) conditioning (f), the ambiguity plot follows the baseline as no extra data is provided to the posterior. 


\begin{figure*}[t]
    \vspace{-0.2cm}
    \centering
    \begin{tabular}{ccccccc}
    \hspace{-0.02\linewidth}\includegraphics[width=0.085\linewidth]{figs/1d/gt_std_ALL_left_axis_2.png} &
    \hspace{-0.025\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/gt_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/baseline_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x0+x1+xt_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x0_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x1_std_ALL_left_crop.png} & \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/xt_std_ALL_left_crop.png} \\
    \hspace{-0.02\linewidth}\includegraphics[width=0.085\linewidth]{figs/1d/gt_std_ALL_left_axis_3.png} &
    \hspace{-0.025\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/gt_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/baseline_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x0+x1+xt_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x0_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/x1_std_ALL_right.png} & \hspace{-0.015\linewidth}\includegraphics[width=0.14\linewidth]{figs/1d/xt_std_ALL_right.png}  \vspace{-0.8em}\\
    & (a) & (b) & (c) & (d) & (e) & (f) \\
    \end{tabular}
    \vspace{-0.8em}
    \caption{1D velocity ambiguity analysis with various conditioning options and sampling strategies. (a) Ground Truth (GT), (b) Baseline (Rectified Flow), (c) Ours ($x_0 + x_1 + x_t$), (d) Ours ($x_0$), (e) Ours ($x_1$), (f) Ours ($x_t$). The heatmap illustrates the velocity standard deviation for sampled bins in data-domain-time-domain, along with histograms of the velocity at four sampled locations. Our method effectively models velocity ambiguity, while the baseline  produces deterministic outputs. %
    }
    \vspace{1em}
    \label{fig:1d_analysis_complete}
\end{figure*}

\subsection{Qualitative Results of Synthetic 1D Experiment}
\label{app:qual_1d}

\begin{figure}[t]
    \centering
    \begin{tabular}{ccc}
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/gt.png} &
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/baseline.png} &
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/consistency_fm_num2.png} \\
    (a) Ground Truth &(b) Rectified FM &(c) Consistency FM \\
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/x0.png} &
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/x1.png} &
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/xt.png} \\
    (d) Ours ($x_0$) &(e) Ours ($x_1$) &(f) Ours ($x_t$) \\
     &
    \includegraphics[width=0.28\linewidth]{figs/1d/traj/x0x1xt_1.png} &
     \\
    & (g) Ours ($x_0 + x_1 + x_t$)& \\
    \end{tabular}
    \caption{1D flow visualization for uni-modal Gaussian to bi-modal Gaussian.}
    \label{fig:1d_unimodal_to_bimodal}
\end{figure}
We provide qualitative flow visualizations from the synthetic 1D experiment in \cref{fig:1d_unimodal_to_bimodal}. Our method effectively captures velocity ambiguity and predicts crossing flows, whereas the baselines produce deterministic outputs.


\subsection{Quantitative Results of MNIST Experiment}
\label{app:mnist_quant}
We evaluate the FID scores of our method using this 2-dimensional conditional latent space and report the results in 
\cref{app:fig:mnist_quant}. Despite the small latent dimension, it still enables the velocity model \(v_\theta\) to achieve better FID scores than the baselines, except at 2 evaluation steps where consistency flow matching~\citep{yang2024consistency} performs best. %


















\begin{table}[t]
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccccccc}
        \toprule
        & NFE / sample & 2 & 5 & 10 & 50 & 100 & 1000 & Adaptive \\
        \midrule
        & \begin{tabular}{@{}c@{}}I-CFM \\ ~\citep{liu2023flow, tongimproving} \end{tabular}& 2.786 & 7.143 & 8.326 & 8.770 & 8.872 & 9.022 & 9.041 \\
        \midrule
        \texttt{1}& V-RFM (adaptive norm, $x_1$, 2e-3) & {3.943} & {7.728} & {8.499} & {8.973} & {9.050} & {9.168} & 9.171 \\
        \texttt{2}& V-RFM (adaptive norm, $x_1$, 5e-3) & 3.083 & 7.202 & 8.342 & 8.868 & 8.997 & 9.166 & {9.183} \\
        \texttt{3}& V-RFM (adaptive norm, $x_1 + t$, 5e-3) & \underline{4.460} & \underline{7.930} & \textbf{8.583} & \underline{9.007} & \underline{9.104} & \underline{9.220} & \underline{9.238}\\
        \texttt{3}& V-RFM (bottleneck sum, $x_1 + t$, 2e-3) & \textbf{4.831} & \textbf{7.996} & \underline{8.529} & \textbf{9.062} & \textbf{9.150} & \textbf{9.293} & \textbf{9.308}\\
        \bottomrule
    \end{tabular}
    \caption{Inception Score evaluation of our method compared to the baseline on CIFAR-10, using fixed-step Euler and adaptive-step Dopri5 ODE solvers. Higher scores indicate better performance.}
    \label{tab:rebuttal_inception_score}
\end{table}

\begin{table}[t]
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccccccc}
        \toprule
        & NFE / sample & 2 & 5 & 10 & 50 & 100 & 1000 & Adaptive \\
        \midrule
        & \begin{tabular}{@{}c@{}}OT-FM \\ ~\citep{LipmanICLR2023,tongimproving} \end{tabular} & 166.655 & 36.188 & 14.396 & 5.557 & 4.640 & 3.822 & 3.655\\
        & \begin{tabular}{@{}c@{}}I-CFM \\ ~\citep{liu2023flow, tongimproving} \end{tabular}& 168.654 & 35.489 & 13.788 & \textbf{5.288} & 4.461 & \underline{3.643} & 3.659 \\
        \midrule
        \texttt{1}& V-RFM-L (100\% Posterior Model) & \textbf{135.275} & \textbf{28.912} & \textbf{13.226} & 5.382 & \underline{4.430} & \textbf{3.642} & \textbf{3.545}\\
        \texttt{2}& V-RFM-M (17.5\% Posterior Model) & \underline{135.983} & \underline{30.106} & 13.783 & 5.486 & 4.500 & 3.697 & \underline{3.607}\\
        \texttt{3}& V-RFM-S (6.7\% Posterior Model) & 144.676 & 31.224 & \underline{13.406} & \underline{5.289} & \textbf{4.398} & 3.699 & 3.639\\
        \bottomrule
    \end{tabular}
    \caption{We use the same flow matching model $v_\theta$ and pair it with different sizes of encoders $q_\phi$ during training while maintaining the exact same hyper-parameters. We report the FID scores for our method and the baseline using both fixed-step Euler and adaptive-step Dopri5 ODE solvers. }
    \label{tab:rebuttal_ablation_encoder_size}
\end{table}





\subsection{Inception Score Evaluation of CIFAR-10 Experiment}
We evaluate the Inception Score of our model trained on  CIFAR-10 data and present results in~\cref{tab:rebuttal_inception_score}. This score quantifies the distribution of predicted labels for the generated samples. Compared to the vanilla rectified flow baseline, our method consistently achieves higher Inception Scores, reflecting improved diversity in the generated samples.




\subsection{Ablation on Posterior Model Size}

We conducted ablations to study the impact of varying the  size of the encoder $q_\phi$, reducing it to 6.7\% and 17.5\% of its original size. The results reported in~\cref{tab:rebuttal_ablation_encoder_size} demonstrate that our model maintains comparable performance across these variations, highlighting the flexibility and robustness of our approach. 



\subsection{Reconstruction Loss Visualizations}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/rebuttal/training_losses.pdf} \\
    \caption{Reconstruction loss  for MNIST (left) and CIFAR-10 (right). We observe lower reconstruction losses for the variational formulation, indicating a better fit. }
    \label{fig:rebuttal_training_loss}
    \vspace{2em}
\end{figure}
We present the reconstruction loss curves for our model and the baseline trained on  MNIST and CIFAR-10 data in \cref{fig:rebuttal_training_loss}. We observe better reconstruction losses of our model compared to vanilla rectified flow, indicating that the predicted velocities more accurately approximate the ground-truth velocities.






























\section{Implementation Details}
\label{app:implement_all}
\subsection{Synthetic data}
\label{app:implement_1d2d}

In the rectified flow baseline, the velocity network \(v_\theta\) features separate encoders for time \(t\) and data \(x\). Each encoder consists of a sinusoidal positional encoding layer followed by two MLP layers with GeLU activation. The resulting time and data embeddings are concatenated and passed into a four-layer MLP, also utilizing GeLU activations. Both the positional embedding and hidden dimensions of the encoder and decoder are set to 64. The training batch size is 1000, and we employ the standard rectified flow objective, i.e., we compute the current data via \(x_t = (1-t)x_0 + tx_1\), the ground truth velocity via \(v(x_0,x_1,t) = x_1 - x_0\), and we use the L2 loss for supervision. 

For consistency flow matching, we adopt the same velocity network \(v_\theta\) and modify the loss function to incorporate the velocity consistency loss proposed by~\citet{yang2024consistency}. We find the  hyperparameter settings suggested by the publicly available codebase to work best. Specifically, we use \(\Delta t = 1 \times 10^{-3}\), \(N_{\textit{segments}} = 2\), and \(\textit{boundary} = 0.0\) for the first training stage, transitioning to \(\textit{boundary} = 0.9\) in the second stage. Additionally, the loss weighting factor \(\alpha\) is set to \(1 \times 10^{-5}\). For complete implementation details, we kindly direct readers to the open-source repository which we used to obtain the reported results.\footnote{https://github.com/YangLing0818/consistency\_flow\_matching} 

In both cases, the AdamW optimizer is used with the default weight decay and a learning rate of \(1 \times 10^{-3}\), over a total of 20,000 training iterations.

In our variational flow matching approach, the velocity network \(v_\theta\) incorporates an additional latent encoding module comprising three MLP layers with a hidden dimension of 128. The conditional latent embedding \(z\) is concatenated with the embeddings for time \(t\) and data \(x\). The decoder maintains the same structure as the baseline, with the first MLP layer adjusted to accommodate the increased channel input. For the posterior model \(q_\phi\), we employ a similar architecture, designing a separate encoder for each possible input selected from \([x_0, x_1, x_t, t]\). Each encoder consists of a sinusoidal positional encoder layer followed by two MLP layers with GeLU activation. The output embeddings are concatenated along the channel dimension and processed through three MLP layers to produce the predicted \(\mu_\phi\) and \(\sigma_\phi\). The latent dimension of \(z\) is set to 4 for 1D experiments and 8 for 2D experiments. During training, we utilize the reparameterization trick to sample \(z\) from the predicted posterior distribution; during inference, the posterior model \(q_\phi\) is omitted, and sampling is performed from a unit variance Gaussian prior distribution. The loss is defined as the sum of the rectified flow reconstruction loss and the KL divergence loss, with the KL loss weighted at 1.0 for the 1D experiments and 0.1 for the 2D experiments. We employ AdamW as the optimizer with a learning rate of \(1 \times 10^{-3}\) and train the two networks $q_\phi$ and $v_\theta$ jointly for 20,000 iterations.

\subsection{MNIST}
\label{app:implement_mnist}

In the rectified flow baseline, the velocity network \(v_\theta\) uses separate encoders for time \(t\) and data \(x\). The time \(t\) encoder consists of a sinusoidal positional encoding layer followed by two MLP layers with SiLU activation. The data \(x\) encoder includes a convolutional in-projection layer, five consecutive ResNet~\cite{he2015deepresiduallearningimage} blocks (each consisting of two convolutional layers with a kernel size of 3, group normalization, and SiLU activation), followed by a convolutional out-projection layer. The time and data embeddings are concatenated and passed to a decoder composed of a convolutional in-projection layer, five consecutive ResNet blocks, and a convolutional out-projection layer with a kernel size of 1 and an output channel of 1. The hidden dimension is set to 64. MNIST data is normalized to the \([-1,1]\) range. We adopted the  consistency velocity loss from the consistency flow matching baseline  used for synthetic data experiments. We train the network for 100,000 iterations using the AdamW optimizer with a learning rate of \(1 \times 10^{-3}\) and batch size of 256.

In our variational flow matching approach, the velocity network \(v_\theta\) includes an additional latent encoding module consisting of a sinusoidal positional encoding layer followed by two MLP layers with SiLU activation. The conditional latent embedding \(z\) is concatenated with the embeddings for time \(t\) and data \(x\). The decoder structure mirrors the baseline, with the first in-projection layer adjusted to handle the increased channel input. The posterior model \(q_\phi\) follows a similar architecture, with separate encoders for each input \( [x_0, x_1, x_t] \). The resulting embeddings are concatenated and passed through a decoder consisting of a convolutional in-projection layer, followed by three consecutive interleaving ResNet blocks and average pooling layers. The final hidden activation is flattened and processed by two linear MLP layers to predict the 1D latent \(z\) with a dimension of 2. The two networks are trained jointly for 100,000 iterations using the AdamW optimizer with a learning rate of \(1 \times 10^{-3}\) and a batch size of 256. The KL loss weight is set to \(1 \times 10^{-3}\).


\subsection{CIFAR-10}
\label{app:implement_cifar10}

For the rectified flow baseline, we directly use the OT-FM and I-CFM models from \cite{tongimproving} and evaluate their performance under different NFEs. For the consistency flow matching model, we take the public implementation from \cite{yang2024consistency} and integrate the consistency loss into the same I-CFM model, naming it Consistency-FM. Additionally, we evaluate the original model from \cite{yang2024consistency} with a larger parameter count, referring to it as Consistency-FM-XL.

For our V-RFM model variants, we adopt the I-CFM model from \cite{tongimproving} and add modules to incorporate conditional signals from a 1D latent \( z \). For both conditioning mechanisms discussed in \cref{sec:cifar10}, the sampled latent is processed through two MLP layers with SiLU activation, with both hidden and output dimensions set to 512.

In the adaptive norm variant, the latent embedding \(z\) is combined with the time embedding from \(v_\theta\) to regress the learnable scale and shift parameters \(\gamma\) and \(\beta\) for the adaptive group norm layers. For the bottleneck sum variant, the latent is added to the bottleneck feature of \(v_\theta\). Since the lowest spatial resolution of the baseline network is \(4 \times 4\), the 1D latent is spatially repeated and fused with the bottleneck feature via a weighted sum. To ensure effective use of the latent, we assign a weighting of 0.9 to the latent and 0.1 to the original velocity feature. 

The posterior model \(q_\phi\) shares a similar encoder structure to \(v_\theta\) but omits the decoder. To achieve greater spatial compression, we increase the number of downsampling blocks, predicting features at a \(1 \times 1\) spatial resolution. The base channel size is set to 16. Both networks are trained jointly for 600,000 iterations using the Adam optimizer with a learning rate of \(2 \times 10^{-4}\) and a batch size of 128. The KL loss weighting is presented alongside the results in \cref{tab:cifar10}.


\subsection{ImageNet}
\label{app:implement_imagenet}
We build upon the open-source SiT-XL model \cite{ma2024sit} by incorporating additional modules to integrate conditional signals from the sampled 1D latent variable \( z \). The sampled latent is processed through two MLP layers with SiLU activation, with both the hidden and output dimensions set to 1152. The processed latent is then directly added to the original conditional latent \( c \), which contains timestep and class label information. The resulting conditional feature is used to predict the learnable scale and shift parameters, \( \gamma \) and \( \beta \), for the adaptive group normalization layers.  

The posterior model \( q_\phi \) shares the SiT-XL architecture but uses only half the number of transformer blocks. To achieve greater spatial compression, we apply an average pooling layer to compress the latent representation into a 1D vector, which is then processed by an MLP layer to predict \( \mu_\phi \) and \( \sigma_\phi \). The base channel size is set to 1152, the patch size to 2, and the number of heads to 16. Both networks are trained jointly for 800,000 iterations using the AdamW optimizer with a learning rate of \( 1 \times 10^{-4} \) and a global batch size of 256. The KL loss weight is set to \( 2 \times 10^{-3} \), and the posterior model \( q_\phi \) takes \( x_1 \) as input. \textit{To ensure a fair comparison, we strictly adhere to the original training recipe of SiT~\cite{ma2024sit}, i.e., we don't tune learning rate, decay or warm-up schedules, AdamW parameters, or employ additional data augmentation or gradient clipping during training.} 

\section{Qualitative Results}
\label{app:addqual}
\subsection{CIFAR-10}
\label{app:qual_cifar10}

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \includegraphics[width=0.8\linewidth]{figs/cifar10/output_7.png} \\
    \end{tabular}
    \caption{Randomly selected samples generated from our model trained on CIFAR-10 data. }
    \label{fig:cifar10_qual}
\end{figure}

We present  qualitative results of our model trained on  CIFAR-10 data in \cref{fig:cifar10_qual}. 



\subsection{ImageNet}
\label{app:qual_imagenet}

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \includegraphics[width=0.8\linewidth]{figs/sit/combined_image_seed_5.png} \\
    \end{tabular}
    \caption{Randomly selected samples generated from our model trained on ImageNet data. }
    \label{fig:imagenet_qual}
\end{figure}

We present  qualitative results of our model trained on  ImageNet data in \cref{fig:imagenet_qual}. 
