\section{Variational Rectified Flow Matching}
\label{sec:method}
Our goal is to  capture the multi-modality inherent in ``ground-truth'' velocity vector-fields obtained from typically used couplings $(x_0,x_1)$ that connect source distribution samples $x_0\sim p_0$ with target data samples $x_1\in{\cal D}$. Here, $p_0$ is a known source distribution and ${\cal D}$ is a considered dataset. 
This differs from classic rectified flow matching which does not capture this multi-modality even for simple distributions as shown in \cref{fig:intuition} and as discussed in \cref{sec:prelim}.  
The struggle to capture multi-modality leads to velocity vector fields that may be more curve and consequently more difficult to integrate at inference time. 
In turn, this leads to distributions that may not fit the data as well. We will show evidence for both, more difficult integration and less accurately captured data distributions in \cref{sec:exp}. 

To achieve our goal we combine rectified flow matching and variational auto-encoders. In the following we first discuss the objective before detailing training and inference.

\subsection{Objective}
\label{sec:method:obj}

The goal of flow matching is to learn a velocity vector-field $v_\theta(x_t,t)$ that transports samples from a known source distribution $p_0$ at time $t=0$ to samples from a commonly unknown probability density function $p_1(x_1)$ at time $t=1$. The probability densities $p_0, p_1$ and the velocity vector-field $v_\theta$ are related to each other via the transport problem
\begin{equation}
\frac{\partial \log p_t(x_t)}{\partial t} = - \di v_\theta(x_t,t),
\label{eq:transportpde}
\end{equation}
or its integral form given in \cref{eq:transportintegral}.

Solving the partial differential equation given in \cref{eq:transportpde} in general analytically is challenging, even when assuming availability of the probability density functions, i.e., when addressing a classic boundary value problem.

However, if we assume the probability density functions to be Gaussians and if we restrict the velocity vector-field to be constant, i.e., of the simple parametric form $v_\theta(x_t,t) = \theta$, we can obtain an analytic solution. This is expressed in the following claim:

\begin{claim}
Consider two Gaussian probability density functions $\tilde{p}_0 = {\cal N}(\xi_0; x_0,I)$ and $\tilde{p}_1 = {\cal N}(\xi_1; x_1,I)$ with mean $x_0$ and $x_1$ respectively. %
Assume 
a constant velocity vector-field $v_\theta(\xi_t,t)=\theta$. Then $\theta = x_1 - x_0$ solves the partial differential equation given in \cref{eq:transportpde} and its integral form given in \cref{eq:transportintegral} and $x_t = (1-t)x_0 + tx_1$.
\label{clm:simple}
\end{claim}
\textbf{Proof:} Given the constant velocity vector-field $v_\theta(\xi_t,t) = \theta$, we have $\int_1^0 \di v_\theta(\xi_t,t)dt \equiv 0$.  Plugging this and both probability density functions into \cref{eq:transportintegral} yields $(\xi_0 - x_0)^2 -(\xi_1 - x_1)^2\equiv 0$ $\forall \xi_0, \xi_1$. Using $\xi_1 = \xi_0 + \int_0^1 v_\theta(\xi_t,t)dt = \xi_0 + \theta$ leads to $(\xi_0-x_0)^2 - (\xi_0 - x_1+\theta)^2 \equiv 0$ $\forall \xi_0$ which is equivalent to $(x_1-x_0-\theta)(2\xi_0-x_0-x_1+\theta)\equiv 0$ $\forall \xi_0$. This can only be satisfied $\forall \xi_0$ if $\theta = x_1-x_0$, leading to $x_t = x_0 + t\theta = (1-t)x_0+tx_1$, which proves the claim.\hfill$\blacksquare$


The arguably very simple setup in \cref{clm:simple} provides intuition for the objective of classic rectified flow matching and offers an alternative way to interpret the flow matching procedure. %
Specifically, instead of two Gaussian probability density functions $\tilde{p}_0$ and $\tilde{p}_1$, we assume the real probability density functions for the source and target data are composed of  Gaussians centered at given data points $x_0$ and $x_1$ respectively, e.g., $p_0(\xi_0) = \sum_{x_0\in{\cal S}} {\cal N}(\xi_0; x_0,I)/|{\cal S}|$. Moreover, importantly, let us assume that the velocity vector-field $v_\theta(x_t,t)$ at a data-domain-time-domain location $(x_t,t)$ is characterized by a uni-modal standard Gaussian
$$
p(v|x_t,t) = {\cal N}(v; v_\theta(x_t,t),I)
$$
with a parametric mean $v_\theta(x_t,t)$. Maximizing the log-likelihood of the empirical ``velocity data''
 is equivalent to the following objective
\begin{equation}
\begin{split}
&\mathbb{E}_{t,x_0,x_1}\left[\log p(x_1-x_0|x_t,t)\right] \\
&\propto -\mathbb{E}_{t,x_0,x_1}\left[\|v_\theta(x_t,t) - x_1 + x_0\|_2^2\right].
\end{split}
\label{eq:rf}
\end{equation}

Note that this objective is identical to classic rectified flow matching. Moreover, note our use of the standard rectified flow velocity vector-field, also derived in \cref{clm:simple}.

This derivation highlights a key point: because the vector field is parameterized via a  Gaussian at each data-domain-time-domain location, multi-modality cannot be captured: the Gaussian distribution is uni-modal. Hence, classic rectified flow matching  averages the ``ground-truth'' velocities.

As mentioned before, this can be sub-optimal. 
To capture multi-modality, we study the use of a mixture model over velocities at each data-domain-time-domain location. For this, we assume an \emph{unobserved} continuous random variable $z$, drawn from a prior distribution $p(z)$, governs the mean of the \emph{conditional} distribution of the velocity vector-field, i.e., 
$$
p(v|x_t,t,z) = {\cal N}(v; v_\theta(x_t,t,z),I).
$$
Note, this model captures multi-modality as $p(v|x_t,t) = \int p(v|x_t,t,z)p(z)dz$ is a Gaussian mixture. %

We now derive the variational flow matching objective. Since the random variable $z$ is not observed, at training time, we introduce a  recognition model $q_\phi(z|x_0,x_1,x_t,t)$ a.k.a.\ an encoder. It is parameterized by $\phi$ and  approximates the intractable true posterior.

Using this setup, the marginal likelihood of an individual data point can be lower-bounded by 
\begin{equation}
\begin{split}
\log p(v|x_t,t) & \geq \mathbb{E}_{z\sim q_\phi}\left[\log p(v|x_t,t,z)\right] \\
&- D_\text{KL}(q_\phi(\cdot|x_0,x_1,x_t,t)|p(\cdot)).
\end{split}
\label{eq:lb}
\end{equation}

Replacing the log-probability of the Gaussian in the derivation of \cref{eq:rf} with the lower bound given in \cref{eq:lb} immediately leads to the variational rectified flow matching objective $\mathbb{E}_{t,x_0,x_1}\left[\log p(x_1-x_0|x_t,t)\right] \geq $
\begin{equation}
\begin{split}
&\mathbb{E}_{t,x_0,x_1}[-\mathbb{E}_{z\sim q_\phi}\left[\|v_\theta(x_t,t,z) - x_1 + x_0\|_2^2\right] \\
& - D_\text{KL}(q_\phi(\cdot|x_0,x_1,x_t,t)|p(\cdot))].
\end{split}
\label{eq:vrfmobj}
\end{equation}
We note that this objective could be extended in a number of ways: for instance, the prior $p(z)$ could be a trainable deep net conditioned on $x_0$ and/or $t$. Note however that this leads to a more complex optimization problem with a moving target. We leave a study of  extensions to future work.

In \cref{app:preserve_marginal}, we provide a theoretical proof demonstrating that the distribution learned by the variational objective preserves the marginal data distribution, as previously established for classic rectified flow matching~\citep{liu2023flow}. 

In the following we first discuss optimization of this objective before detailing the inference procedure.

\subsection{Training}
\label{sec:method:train}




\begin{algorithm}[tb]
\caption{Variational Rectified Flow Matching Training}
\label{alg:training}
\begin{algorithmic}
   \STATE {\bfseries Data:} source distribution $p_0$ and target sample dataset ${\cal D}$
   \WHILE{stopping conditions not satisfied}
       \STATE sample $x_0 \sim p_0, x_1 \in {\cal D}$ \COMMENT{we use a mini-batch}
       \STATE sample $t \sim U(0,1)$ \COMMENT{different $t$ for each mini-batch sample}
       \STATE $x_t = (1-t)x_0 + tx_1$\;
       \STATE get latent  $z = \mu_\phi(x_0,x_1,x_t,t) + \epsilon\sigma_\phi(x_0,x_1,x_t,t)$ with $\epsilon\sim{\cal N}(0,1)$\label{alg:training:new} \COMMENT{reparameterization trick}
       \STATE compute loss following \cref{eq:vrfmobj}\;
       \STATE perform gradient update on $\theta$, $\phi$\;
   \ENDWHILE
\end{algorithmic}
\end{algorithm}


To optimize the objective given in \cref{eq:vrfmobj}, we follow the classic VAE setup. Specifically, we let the prior $p(z)={\cal N}(z;0,I)$ and we let the approximate posterior $q_\phi(z|x_0,x_1,x_t,t) = {\cal N}(z; \mu_\phi(x_0,x_1,x_t,t),\sigma_\phi(x_0,x_1,x_t,t))$. This enables analytic computation of the KL-divergence in \cref{eq:vrfmobj}. Note that the mean of the approximate posterior is obtained from the deep net $\mu_\phi(x_0,x_1,x_t,t)$ and the standard deviation is obtained from $\sigma_\phi(x_0,x_1,x_t,t)$. Further, we use the re-parameterization trick to enable optimization of the objective w.r.t.\ the trainable parameters $\theta$ and $\phi$. Moreover, we use a single-sample estimate for the expectation over the unobserved variable $z$.  We summarize the training procedure in \cref{alg:training}. Note, it's more effective to work with a mini-batch of samples rather than a single data point, which was merely used for readability in \cref{alg:training}.

Note that variational rectified flow matching training differs from training of classic rectified flow matching in only a single step: computation of a latent sample $z$ in \cref{alg:training:new}. From a computational point of view we add a deep net forward pass  to obtain the mean $\mu_\phi$ and standard deviation $\sigma_\phi$ of the approximate posterior, and a backward pass to obtain the gradient w.r.t.\ $\phi$. Also note that the velocity vector-field architecture $v_\theta(x_t,t,z)$ might be more complex as the latent variable $z$ needs to be considered. However, the additional amount of computation is likely not prohibitive. %



We provide implementation details for the deep nets $v_\theta(x_t,t,z)$, $\mu_\phi(x_0,x_1,x_t,t)$, and $\sigma_\phi(x_0,x_1,x_t,t)$ in \cref{sec:exp}, as their architecture depends on the data.





\begin{algorithm}[t]

\caption{Variational Rectified Flow Matching Inference}\label{alg:inference}
\begin{algorithmic}
   \STATE {\bfseries Data:} source distribution $p_0$
   \STATE sample $x_0\sim p_0$\;
   \STATE get latent $z\sim p(z)$\;
   \STATE ODE integrate $x_0$ from $t=0$ to $t=1$ using velocity vector-field $v_\theta(x_t,t,z)$\;
\end{algorithmic}

\end{algorithm}


\subsection{Inference}
\label{sec:method:infer}




We summarize the inference procedure in \cref{alg:inference}. Note that we sample a latent variable only once prior to classic ODE integration of a random sample $x_0\sim p_0$ drawn from the source distribution $p_0$. To obtain the latent $z$ we sample from the prior $z \sim p(z) = \mathcal{N}(z; 0,I)$. Subsequently, we ODE integrate the velocity field $v_\theta(x_t,t,z)$ from time $t = 0$ to time $t = 1$ starting from a random sample $x_0$ drawn from the source distribution.



