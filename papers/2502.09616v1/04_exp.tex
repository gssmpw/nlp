\section{Experiments}
\label{sec:exp}

We evaluate the efficacy of  variational rectified flow matching and compare to the classic rectified flow~\citep{LipmanICLR2023,liu2023flow,albergo2023building} 
across multiple datasets and model architectures. %
Our experiments show that variational rectified flow matching is able to capture the multi-modal velocity in the data-domain-time-domain, leading to compelling evaluation results. %
Moreover, we demonstrate that explicitly modeling multi-modality through a conditional latent $z$ can enhance the interpretability of flow matching models, leading to  controllability. Implementation details for all experiments are provided in   \cref{app:implement_all}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/1d/comparison_plot_Sep30_2.png} \\
    \vspace{-0.5em}
    \caption{Quantitative evaluation on synthetic 1D data for varying evaluation steps. Metrics are averaged over three runs. For True and Parzen Window Log-Likelihood, higher values are better.}
    \label{fig:1d_quant_result}
    \vspace{0.5em}
\end{figure}



\subsection{Synthetic 1D Data}
\label{sec:1d_exp}


For synthetic 1D experiments, the source distribution is a zero-mean, unit-variance Gaussian, while the target distribution is bimodal, with modes centered at $-1.0$ and $1.0$. 


For the rectified flow  baseline, we use a multi-layer MLP network \(v_\theta\) to model the velocity. The network operates on inputs \(x_t\) and \(t\)  and predicts the velocity through a series of MLP layers. We follow this structure in our variational rectified flow matching, but add an encoding layer for the latent variable \(z\). The posterior model \(q_{\phi}\) follows a similar design as \(v_\theta\), outputting \(\mu_\phi\) and \(\sigma_\phi\). %
At inference time, $q_\phi$ isn't used. Instead, we sample directly from the prior distribution \(p(z) = \mathcal{N}(z; 0, I)\). The KL loss weight is $1.0$.  


We assess the performance using the Euler ODE solver and vary the evaluation steps. Results are presented in \cref{fig:1d_quant_result}. Across both metrics, i.e., True Log-Likelihood and Parzen Window Log-Likelihood, and most evaluation steps, our method outperforms the baseline. Notably, as the model  handles multi-modality in the data-domain-time-domain, it produces reasonable results even for 2 or 5 evaluation steps. Qualitative visualizations of flow trajectories are provided in \cref{app:qual_1d}. 



\begin{figure}[t]
    \vspace{-0.2cm}
    \centering
    \begin{tabular}{ccccccc}
    \hspace{-0.02\linewidth}\includegraphics[width=0.12\linewidth]{figs/1d/gt_std_ALL_left_axis_2.png} &
    \hspace{-0.025\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/gt_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/baseline_std_ALL_left_crop.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/x0+x1+xt_std_ALL_left_crop.png} \\
    \hspace{-0.02\linewidth}\includegraphics[width=0.12\linewidth]{figs/1d/gt_std_ALL_left_axis_3.png} &
    \hspace{-0.025\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/gt_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/baseline_std_ALL_right.png} &
    \hspace{-0.015\linewidth}\includegraphics[width=0.2\linewidth]{figs/1d/x0+x1+xt_std_ALL_right.png} \\
    & (a) & (b) & (c) \\
    \end{tabular}
    \vspace{-0.8em}
    \caption{1D velocity ambiguity analysis with various conditioning options and sampling strategies. (a) Ground Truth, (b) Baseline (Rectified Flow), 
    (c) Ours (Variational Rectified Flow)
    . The heatmap illustrates the velocity standard deviation for sampled bins in data-domain-time-domain, along with histograms of the velocity at four sampled locations. Our method effectively models velocity ambiguity, while the baseline  produces deterministic outputs. %
    }
    \vspace{0.5em}
    \label{fig:1d_analysis}
\end{figure}

To better understand the multi-modality of the velocity and to assess the efficacy of our model in handling it, we randomly sample different trajectories and plot the velocity range standard deviation across predefined bins in the data-domain-time-domain, as shown in \cref{fig:1d_analysis}. The ground-truth flow in \cref{fig:1d_analysis}(a) shows that the standard deviation increases with time, peaking at (\(x=0.0, t=0.75\)). The velocity distribution transitions from a bi-modal distribution at early times \(t\) to a uni-modal distribution at later times \(t\). %
\cref{fig:1d_analysis}(b) shows that the rectified flow baseline, which uses  an MSE loss, fails to model the velocity distribution faithfully, collapsing to a Dirac-delta distribution as expected. 
In contrast, \cref{fig:1d_analysis}(c) demonstrates that our model  captures the  distribution with higher velocity standard deviation range,  matching the ground-truth  reasonably, albeit not perfectly.
The complete ablation study on various conditioning options is provided in \cref{app:full_1d_analysis}.










\begin{figure*}[t]
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{ccc}
    \includegraphics[width=0.24\linewidth]{figs/2d/gt_adv_False.png} &
    \includegraphics[width=0.24\linewidth]{figs/2d/baseline_traj.png} &
    \includegraphics[width=0.24\linewidth]{figs/2d/traj_2.png} \vspace{-0.8em} \\
    
    (a) Ground Truth &(b) Rectified FM &(c) Variational Rectified FM (Ours)
    \end{tabular}
    \vspace{-0.8em}
    \caption{Flow  visualization for synthetic 2D data using the Euler solver with 20 function evaluations. Sampled points from the source distribution are shown in red, and points from the target distribution in purple. Different from Rectified FM, which predicts flow trajectories with sharp curvature and  U-turns to avoid crossings, our model  captures velocity ambiguity and predicts flows that  intersect. %
    }
    \vspace{-1.em}
    \label{fig:2d_result}
\end{figure*}

\subsection{Synthetic 2D Data}
\label{sec:2d_exp}

We further test  efficacy using  synthetic 2D data. Following \citet{liu2023flow}, we model the source distribution as a mixture of Gaussian components positioned at six equidistant points along a circle with a radius of 1/3, shown in red in \cref{fig:2d_result}(a). The target distribution follows a similar structure, but with components arranged along a larger circle with a radius of 1, shown in purple. %

For the architecture we follow \cref{sec:1d_exp} and condition the posterior on \([x_0, x_1, x_t]\). We report the True Log-Likelihood and the Parzen Window Log-Likelihood for various evaluation steps of the Euler ODE solver, as shown in \cref{fig:2d_quant_result}. Compared to the 1D data, our model shows a more significant performance boost here. 
We hypothesize that this  is due to the increased complexity of the task: explicitly modeling multi-modality avoids the need for curved trajectories, making it easier to fit the target distribution. The qualitative flow visualization in \cref{fig:2d_result} supports this hypothesis: the rectified flow  requires a U-turn to avoid collisions, while our model, aided by the variational training objective, moves   in trajectories that intersect and aren't as curved.




\begin{figure}[t]
    \vspace{-2mm}
    \centering
    \includegraphics[width=\linewidth]{figs/2d/comparison_plot.png} %
    \vspace{-2.5em}
    \caption{Quantitative evaluation on  synthetic 2D data for varying evaluation steps. Metrics are averaged over three runs with different random seeds.}
    \label{fig:2d_quant_result}
    
    \vspace{0.5em}
\end{figure}

\begin{figure}[t]
    \vspace{-2mm}
    \centering
    \setlength{\tabcolsep}{3pt}  %
    \begin{tabular}{cc}
    \includegraphics[width=0.45\linewidth]{figs/mnist/grid_0.png} &
    \includegraphics[width=0.45\linewidth]{figs/mnist/grid_1.png} \\
    (a) $x^0_0$ & (b) $x^1_0$ \\
    \end{tabular}
    \vspace{-2mm}
    \caption{Visualization of learned MNIST manifold with different random noise $x_0$. %
    }
    \label{fig:MNIST_manifold}
    \vspace{0.5em}
\end{figure}












\subsection{MNIST}
\label{sec:mnist}




Modeling multi-modality  also enables more explicit control %
without additional conditioning signals. To show this we use variational rectified flow matching to train a vanilla convolutional net with residual blocks \citep{he2015deepresiduallearningimage} on  MNIST data \citep{lecun1998gradient}. We use $(x_0, x_1, x_t)$ as input to $q_\phi$ and set the  KL loss weight to $1e^{-3}$. %

Following \citet{KingmaICLR2014}, we set the latent variable \(z\) to be 2-dimensional. During inference, we sample linearly spaced coordinates on the unit square, transforming them through the inverse CDF of the Gaussian to generate latents \(z\). Using these latents, we integrate the samples with an ODE solver and plot the generated samples %
in \cref{fig:MNIST_manifold}. To show the effects of the source distribution sample \(x_0\) and the latent \(z\), we visualize the learned MNIST manifold for two randomly sampled \(x_0\) values in \cref{fig:MNIST_manifold}(a,b). The results demonstrate that the latent space \(z\) enables smooth interpolation between different digits within the 2D manifold, providing  control over the generated images. By adjusting \(z\), we can  transition between various shapes and styles. %
The initial noise \(x_0\) enhances the generation process by introducing additional variations in character styles, allowing the model to better capture the target data distribution. 
Our method achieves better FID scores than the baseline, even with a 2-dimensional conditional latent space. Further details and results can be found in \cref{app:mnist_quant}.





\begin{table*}[t]
\vspace{-3mm}
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{cccccccccc}
        \toprule
        & NFE / sample & $\#$ Params. & 2 & 5 & 10 & 50 & 100 & 1000 & Adaptive \\
        \midrule
        & OT-FM ~\citep{LipmanICLR2023} & 36.5M & 166.655 & 36.188 & 14.396 & 5.557 & 4.640 & 3.822 & 3.655\\
        & I-CFM ~\citep{tongimproving} & 36.5M & 168.654 & 35.489 & 13.788 & \underline{5.288} & 4.461 & 3.643 & 3.659 \\
        \midrule
        \texttt{1}& V-RFM (adaptive norm, $x_1$, 2e-3)& 37.2M & 135.275 & 28.912 & \textbf{13.226} & 5.382 & \underline{4.430} & 3.642 & 3.545\\
        \texttt{2}& V-RFM (adaptive norm, $x_1$, 5e-3)& 37.2M & 159.940 & 35.293 & 14.061 & \textbf{5.265} & \textbf{4.349} & \textbf{3.582} & 3.561\\
        \texttt{3}& V-RFM (adaptive norm, $x_1 + t$, 5e-3)& 37.2M  & \underline{117.666} & \underline{27.464} & 13.632 & 5.512 & 4.484 & 3.614 & \textbf{3.478}\\
        \texttt{4}& V-RFM (bottleneck sum, $x_1 + t$, 2e-3)& 37.0M  & \textbf{104.634} & \textbf{25.841} & \underline{13.508} & 5.618 & 4.540 & \underline{3.596} & \underline{3.520}\\
        \bottomrule
    \end{tabular}
    \vspace{-0.7em}
    \caption{Following \citet{tongimproving}, we train the same UNet model and reported the FID scores for our method and the baselines using both fixed-step Euler and adaptive-step Dopri5 ODE solvers. The baselines checkpoint was directly taken from \citet{tongimproving}. We present four model variants of our V-RFM, which differ in fusion mechanism, posterior model input, and KL loss weight. }
    \label{tab:cifar10}
    \vspace{-1em}
\end{table*}


\subsection{CIFAR-10}
\label{sec:cifar10}


Next, we evaluate on  CIFAR-10, a widely used benchmark in prior work \citep{LipmanICLR2023, tongimproving}. For a fair comparison, we use the architecture and training paradigm of \citet{tongimproving}, but train the UNet model with the variational rectified flow loss detailed in \cref{eq:vrfmobj}. The UNet consists of downsampling and upsampling residual blocks with skip connections, and a self-attention block  added after the residual block at $16 \times 16$ resolution and in the middle bottleneck layer. The model takes both \(x_t\) and \(t\) as input, with the time embedding $t$ used to regress learnable scale and shift parameters \(\gamma\) and \(\beta\) for adaptive group norm layers.


The posterior model \(q_\phi\) shares a similar encoder structure as \(v_\theta\):  image space inputs are chosen from \([x_0, x_1, x_t]\) and concatenated along the channel dimension, while time \(t\) is conditioned using adaptive group normalization. The network predicts \(\mu_\phi\) and \(\sigma_\phi\) with dimensions \(1 \times 1 \times 768\). During training, the conditional latent \(z\) is sampled from the predicted posterior, and at test time, from a standard Gaussian prior. The latent is processed through two MLP layers and serves as a conditional signal for the velocity network \(v_\theta\). We identify two effective approaches as conditioning mechanisms: adaptive normalization, where \(z\) is added to the time embedding before computing shift and offset parameters, and bottleneck sum, which fuses the latent with intermediate activations at the lowest resolution using a weighted sum before upsampling. 





We evaluate results  using FID scores computed for varying numbers of function evaluations, as shown in \cref{tab:cifar10}. Four model variants were tested, differing in fusion mechanisms, posterior model \(q_\phi\) inputs, and KL loss weighting. %
Compared to prior work \citep{LipmanICLR2023, liu2023flow, tongimproving}, model \texttt{1} achieves superior FID scores with fewer function evaluations and performs comparably at higher evaluations. Using the adaptive Dopri5 solver further improves scores, highlighting the importance of capturing flow ambiguity. Model \texttt{2} increases the KL loss weight, improving performance at higher function evaluations but reducing effectiveness at lower evaluations, likely due to reduced information from latent \(z\). Model \texttt{3}, with additional time conditioning, significantly improves FID at low evaluations and performs best with the adaptive solver. Model \texttt{4}, incorporating bottleneck sum fusion, delivers robust FID scores across evaluation settings, demonstrating the flexibility of the variational rectified flow objective with different fusion strategies. 






Similar to the  MNIST results in \cref{sec:mnist}, we observe clear patterns in color and content for the generated samples \(x_1\), demonstrating a degree of controllability. %
\cref{fig:cifar10_control} visualizes three sets of images (a)–(c). Each set is conditioned on a different latent \(z\), while the starting noise \(x_0\) varies across individual images within each set. The same noise \(x_0\) is applied to images at the same grid location across all subplots. Images conditioned on the same latent exhibit consistent color patterns, while images at the same grid location display similar content, as highlighted in the last row.


        

\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{2pt}  %
    \begin{tabular}{ccc}
    \includegraphics[width=0.3\linewidth]{figs/cifar10/image_grid_3.png} &
    \includegraphics[width=0.3\linewidth]{figs/cifar10/image_grid_46.png} &
    \includegraphics[width=0.3\linewidth]{figs/cifar10/image_grid_14.png} \\
    (a) $z^0$ & (b) $z^1$ & (c) $z^2$
    \end{tabular}
    \vspace{-1em}
    \caption{Varying $x_0$ while keeping the latent $z$ fixed. Images at the same position across panels share the same  $x_0$, while  images within a panel share the same  latent sampled from the prior distribution. }
    \label{fig:cifar10_control}
    \vspace{-1mm}
\end{figure}

\subsection{ImageNet}


To assess efficacy on large-scale data, we use ImageNet $256 \times 256$ data and SiT-XL \citep{ma2024sit}, a recent transformer-based model that has shown strong results in image generation. For a fair comparison, we strictly follow the original training recipe in the open-source SiT repository and replicate the training process from the SiT paper, while introducing our model, V-SiT-XL, by substituting the classic rectified flow loss with the variational rectified flow loss in \cref{eq:vrfmobj}. The posterior model $q_\phi$ also utilizes an SiT transformer architecture but with half the number of blocks. In the final layer, the features are average pooled and passed through an MLP layer to predict $\mu_\phi$ and $\sigma_\phi$. We sample the latent variable $z$ from the posterior during training and from the prior distribution during inference. This latent variable $z$ is then processed by two MLP layers and fused with the velocity network $v_\theta$ via adaptive normalization. By default, we use the Euler-Maruyama sampler with the SDE solver and 250 integration steps, as described by \citet{ma2024sit}. 




Following the evaluation protocol of \citet{ma2024sit}, we randomly generate 50K images from the models and report the FID scores in \cref{tab:cond_imagenet_sit}. V-SiT-XL consistently outperforms both DiT-XL and SiT-XL, achieving   gains under the same training conditions, with and without classifier-free guidance. These results underscore the importance of modeling multi-modality in the velocity vector field, which contributes to a substantial improvement in generation quality, particularly in the large-scale high-resolution data domain. Additionally, we analyze the model's performance across different training iterations and varying numbers of function evaluations, presenting the findings in \cref{fig:sit_plot}. The results reveal a consistent performance boost, further highlighting the effectiveness of our approach. 










\begin{table}[t]
\vspace{-3mm}
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccccc}
        \toprule
        & Model & Params (M) & Training Steps & FID $\downarrow$ \\
        \midrule
        & DiT-XL  & 675 & 400K & 19.5 \\
        & SiT-XL  & 675 & 400K & 17.2\\
        & \textbf{V-SiT-XL}   & 677 & 400K & \textbf{14.6}\\
        \midrule
        & SiT-XL  & 675 & 800K & 13.1\\
        & \textbf{V-SiT-XL}  & 677 & 800K & \textbf{10.6}\\
        \midrule
        & SiT-XL\textsubscript{cfg=1.5}  & 675 & 400K & 5.40\\
        & \textbf{V-SiT-XL}\textsubscript{cfg=1.5}  & 677 & 400K & \textbf{4.91}\\
        \midrule
        & SiT-XL\textsubscript{cfg=1.5}  & 675 & 800K & 3.43\\
        & \textbf{V-SiT-XL}\textsubscript{cfg=1.5}  & 677 & 800K & \textbf{3.22}\\
        \bottomrule
    \end{tabular}
    \vspace{-2mm}
    \caption{FID-50K score evaluation of class-conditional generation on ImageNet $256\times256$, comparing the baselines (DiT-XL, SiT-XL) with our proposed model \textbf{V-SiT-XL}.}
    \label{tab:cond_imagenet_sit}
    \vspace{-1mm}
\end{table}



