\section{Preliminaries}
\label{sec:prelim}
Given a dataset ${\cal D} = \{(x_1)\}$ consisting of data samples $x_1$, e.g., an image, %
generative models learn a distribution $p(x_1)$, often by maximizing the likelihood. 
In the following we discuss how  this distribution is learnt with variational auto-encoders and rectified flow matching, and why the corresponding modeled data distribution is multi-modal. %

\subsection{Variational Auto-Encoders (VAEs)}
Variational inference generally and variational auto-encoders (VAEs)~\citep{KingmaICLR2014} specifically have been shown to learn multi-modal distributions. %
This is achieved by introducing a latent variable $z$. At inference time, a latent  $z$ is obtained by sampling from the prior distribution $p(z)$, typically a zero mean unit covariance Gaussian. %
A decoder which characterizes a distribution $p(x_1|z)$ over the output space is then used to obtain an output space sample $x_1$. %

At training time, variational auto-encoders use an encoder to compute an approximate posterior distribution $q_\phi(z|x_1)$ over the latent space. As the approximate posterior distribution is only needed at training time, the data $x_1$ %
can be leveraged. Note, the approximate posterior distribution is often a Gaussian with parameterized mean and covariance. A sample from this approximate posterior distribution is then used as input in the  distribution $p_\theta(x_1|z)$ characterized by the decoder. The loss encourages a high probability of the output space samples while favoring an approximate posterior distribution $q_\phi(z|x_1,c)$ that is similar to the prior distribution $p(z)$. To achieve this, formally, VAEs maximize a lower-bound on the log-likelihood, i.e., 
\begin{align*}
&\mathbb{E}_{x_1\sim{\cal D}}\log p(x_1) \\
&\geq \mathbb{E}_{x_1\sim{\cal D}}\left[\mathbb{E}_{z\sim q_\phi}\left[\log p_\theta(x_1|z)\right] - D_\text{KL}(q_\phi(\cdot|x_1)|p(\cdot))\right].
\end{align*}



\subsection{Rectified Flow Matching}

For flow matching, at inference time, a source distribution $p_0(x_0)$ is queried to obtain a sample $x_0$. This is akin to sampling of a latent variable from the prior in VAEs. Different from VAEs which perform a single forward pass through the decoder, in flow matching, the source distribution sample $x_0$ is used as the boundary condition for an ordinary differential equation (ODE). This ODE is `solved' by pushing the sample $x_0$ forward from time zero to time one via integration along a trajectory specified via a learned velocity vector-field $v_\theta(x_t,t)$ defined at time $t$ and location $x_t$,  and commonly parameterized by deep net weights $\theta$. Note, the velocity vector-field is queried many times during integration. 
The likelihood of a data point $x_1$ can be assessed via the instantaneous change of variables formula~\citep{ChenARXIV2018,SongICLR2021,LipmanICLR2023}, 
\begin{equation}
\log p_1(x_1) = \log p_0(x_0) + \int_1^0 \di v_\theta(x_t,t) dt,
\label{eq:transportintegral}
\end{equation}
which is commonly~\citep{GrathwohlICLR2018} approximated via the Skilling-Hutchinson trace estimator~\citep{Skilling1989,Hutchinson1990}. Here, $\di$ denotes the divergence vector operator. %

Intuitively, by pushing forward samples $x_0$, randomly drawn from the source distribution $p_0(x_0)$, ambiguity in the data domain is captured as one expects from a generative model. 


At training time the parametric velocity vector-field $v_\theta(x_t,t)$ needs to be learnt. For this, 
coupled sample pairs $(x_0, x_1)$ are  constructed by randomly drawing  from the source and the target distribution, often independently from each other. 
A coupled sample $(x_0,x_1)$ and a time $t\in[0,1]$ is then used to compute a time-dependent location $x_t$ at time $t$ via a function $\phi(x_0, x_1, t) = x_t$. Recall, rectified flow matching uses $x_t = \phi(x_0,x_1,t) = (1-t)x_0 + tx_1$. %
Interpreting $x_t$ as a location, intuitively, the ``ground-truth'' velocity vector-field $v(x_0,x_1,t)$ is readily available via $v(x_0,x_1,t) = \partial\phi(x_0,x_1,t)/\partial t$, and can be used as the target to learn the parametric velocity vector-field $v_\theta(x_t,t)$. 
Concretely, flow matching learns the parametric velocity vector field $v_\theta(x_t,t)$ by matching the target via an $\ell_2$ loss, i.e., by minimizing w.r.t.\ trainable parameters $\theta$ the objective
$$
\mathbb{E}_{t,x_0,x_1}\left[\|v_\theta(x_t,t) - v(x_0,x_1,t)\|_2^2\right].
$$
Consider two different couplings that lead to different ``ground-truth'' velocity vectors at the same data-domain-time-domain point $(x_t,t)$. The parametric velocity vector-field $v_\theta(x_t,t)$ is then asked to match/regress to a different target given the same input $(x_t,t)$. This leads to averaging and the optimal functional velocity vector-field $v^\ast(x_t,t) = \mathbb{E}_{\{(x_0,x_1,t) : \phi(x_0,x_1,t) = x_t\}}\left[v(x_0,x_1,t)\right]$. Hence,  multi-modality in the data-domain-time-domain is not captured. In the following we discuss and study a method that is able to model this multi-modality. 

