\section{Related Work}
\label{sec:rel}

Generative modeling has advanced significantly in the last decade, thanks in part due to seminal works like generative adversarial nets~\citep{goodfellow2014generative}, variational auto-encoders~\citep{KingmaICLR2014}, and normalizing flows~\citep{rezende2015variational}.

More recently, score matching~\citep{song2019generative} and diffusion models~\citep{ho2020denoising} were introduced. They can be viewed as augmenting variational auto-encoders hierarchically~\citep{luo2022understanding} while restricting involved distributions to be Gaussian. Notably, and analogously to classic discrete normalizing flows, the number of hierarchy levels, i.e., the number of time steps, remained discrete, which introduced complications.

Flow matching~\citep{LipmanICLR2023} was introduced recently as a compelling alternative to avoid some of these complications. It formulates an ordinary differential equation (ODE) in continuous time. This ODE connects a source distribution to a target distribution. Solving the ODE via forward integration through time permits to obtain samples from the target distribution, essentially by `moving' samples from the known source distribution to the target time along a learned velocity field.

To learn the velocity field, various mechanisms to interpolate between the source distribution and the target distribution have been considered~\citep{LipmanICLR2023, liu2023flow, tongimproving}. Rectified flow matching emerged as a compelling variant, which linearly interpolates between samples from the two distributions. For instance, it was used to attain impressive results on large scale data~\citep{ma2024sit, esser2024scaling}. Different from other techniques, linear interpolation encourages somewhat straight flows, which simplifies numerical solving of the ODE.

The importance of straight flows was further studied in ReFlow~\citep{liu2023flow}, which sequentially formulates multiple ODEs and learns velocity fields by adjusting the interpolations and `re-training.'
Consistency models~\citep{song2023consistency, kimconsistency, yang2024consistency} strive for straight flows by modifying the loss to encourage self-consistency across timesteps. More details and comparisons are provided in \cref{app:addrel} and \ref{app:consistency_model}.


While the aforementioned works aim to establish straight flows either via `re-training' or `re-parameterizing' of an already existing flow, differently, in this work we study the results of enabling a rectified flow to capture the ambiguity inherent in the usually employed ground-truth flow fields. %

Structurally similar to this idea is work by
\citet{preechakul2022diffusion}. In a first stage, an autoencoder is trained to compress images into a latent space. The resulting latents then serve as a conditioning signal for diffusion model training in a second stage. Note, this two-stage approach doesn't directly model ambiguity in the data-domain-time-domain. %
In similar spirit is work by \citet{pandey2022diffusevae}. A VAE and a diffusion model are trained in two separate stages, with the goal to enable controllability of diffusion models. %
Related is also work by \citet{eijkelboom2024variational} which focuses on flow matching  only for categorical data, achieving compelling results on graph generation tasks. 

Concurrently, \citet{ZhangICLR2025} also study a method to model multi-modal velocity vector fields. In this paper, we discuss how to use a lower-dimensional latent space to enable modeling of the velocity distribution via a variational approach. Differently, \citet{ZhangICLR2025} study use of a hierarchy of ordinary differential equations. The variational approach enables to capture semantics while use of a hierarchy of ordinary differential equations permits to more accurately model the velocity distribution.