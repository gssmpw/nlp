\section{Related Work}
~\label{sec:Related Work} 


% In this section, we will review the most relevant research topics related to our paper, including Event Camera based Tracking, Knowledge Distillation, and Test-Time Tuning. More related works can be found in surveys**Braun et al., "A Survey on Event-Based Vision Systems"**.


\subsection{Event Camera based Tracking}  \label{subsec:eventtracking}
With the development of bio-inspired event cameras, event camera based visual object tracking has gradually become a focus of attention. The early event camera based tracking algorithm ESVM**Wright et al., "An Event-Guided Support Vector Machine for High-Speed Object Tracking"** proposed an event-guided support vector machine (ESVM) for tracking high-speed moving objects, which solves the tracking problem caused by motion blur and large displacement in low frame rate cameras. Recently, Chen et al.**Chen et al., "Adaptive Time Surface with Linear Time Decay for Event-to-Frame Conversion"** adopt the Adaptive Time Surface with Linear Time Decay (ATSLTD) algorithm for event-to-frame conversion, the spatiotemporal information of asynchronous retinal events is transformed into an ATSLTD frame sequence for efficient object tracking. Zhu et al.**Zhu et al., "Event Camera Based Object Tracking Using Graph Network Embedding"** propose a new end-to-end learning framework for object tracking based on event cameras, which improves tracking accuracy and speed through key event sampling and graph network embedding. Zhang et al.**Zhang et al., "STNet: A Spiking Neural Network-Transformer-Based Visual Tracker"** propose a novel tracking network STNet based on a spiking neural network and Transformer network, which can effectively extract spatiotemporal information from events and achieve a better tracking performance. Wang et al.**Wang et al., "Hierarchical Knowledge Distillation for Event-Based Multi-Modal Tracking"** propose a novel hierarchical knowledge distillation framework, combined with multi-modal/multi-view information, high-speed and low latency visual tracking can be achieved during the testing phase using only event signals. 


For event based multi-modal tracking, Zhang et al.**Zhang et al., "Cross-Domain Attention Mechanism for Single-Object Tracking"** propose a multi-modal fusion method that combines visual cues from frame and event domains to improve single object tracking performance under degraded conditions, which also enhances the effect through cross-domain attention mechanism and adaptive weighting scheme. 
VisEvent**Wang et al., "Event-Based Image Generation and Fusion Network"** proposed by Wang et al., transforms the event stream into images and extends a single-modal tracker to a dual-modal version, with a cross-modal converter enabling better fusion of RGB and event data. 
AFNet**Zhang et al., "Asynchronous Frame Rate Tracking Network"**, proposed by Zhang et al., a framework for high frame rate tracking, event alignment, and fusion network has been proposed, which significantly improves the performance of high frame rate tracking by combining the advantages of traditional frameworks and event cameras.  
Gehrig et al.**Gehrig et al., "Event-Based Asynchronous Photometric Feature Tracking"** propose EKLT, an asynchronous photometric feature tracking method that combines the advantages of event cameras and RGB cameras to achieve visual feature tracking at high temporal resolution and improve tracking accuracy. 
% Tang et al.**Tang et al., "Unified Event Camera and Color Frame Tracker with Visual Transformer Networks"** propose a single-stage network architecture CEUTrack for unified tracking of color frames and event camera, which simultaneously implements functions such as feature extraction, feature fusion, matching, and interactive learning by visual Transformer networks. The prompt learning strategy is adopted in 
ViPT**Wang et al., "Vision-Photo Transformers: Unifying Vision and Photo with a Unified Framework"**, which adjusts the pre-trained RGB base model by introducing a small number of trainable parameters to adapt the different multi-modal tracking tasks. 
% Zhu et al.**Zhu et al., "Uncertainty-Aware Event Camera Tracker for Robust Visual Object Tracking"** propose a novel misaligned tracking method that extracts template and search regions from RGB and event data, respectively. Subsequently, uncertainty-aware modules are proposed to encode RGB and event features, and a modal uncertainty fusion module is further proposed to integrate these two modalities. 
% Wu et al.**Wu et al., "Unified Event-Data-Based Tracker for Robust Visual Object Tracking"** propose UnTrack, which is a single model unified tracker that can handle any modal data (including event data), maintain performance in any missing modality through shared representations, and does not require modality-specific fine-tuning. 
%%
Different from existing works, we propose to conduct a hierarchical knowledge distillation strategy from multi-modal or multi-view in the training phase and only utilize the event data for efficient and low-latency tracking. 




\subsection{Knowledge Distillation}  \label{subsec:kd}
The knowledge distillation strategy has been widely proven to be an effective method of knowledge transfer.
%%
Deng et al.**Deng et al., "Image-Based Knowledge Distillation Learning Framework"** propose an image-based knowledge distillation learning framework that improves the performance of event camera models in feature extraction by extracting knowledge from the image domain, achieving significant improvements in the performance of event cameras in target classification and optical flow prediction tasks. 
%%%
In visual object tracking, Shen et al.**Shen et al., "Distillation Learning Method for Siamese Network Trackers"** propose a distillation learning method for learning small, fast, and accurate Siamese network trackers. 
Chen et al.**Chen et al., "Lightweight Network Based on Teacher-Student Knowledge Distillation Framework"** propose a lightweight network based on the teacher-student knowledge distillation framework to accelerate visual trackers based on correlation filters while maintaining tracking performance.
Zhuang et al.**Zhuang et al., "Ensemble Learning Method for Visual Tracking Tasks"** propose an ensemble learning method based on Siamese architecture, which effectively improves the accuracy of visual tracking tasks and solves the limitations of knowledge distillation in visual tracking tasks.
Sun et al.**Sun et al., "Cross-Modal Knowledge Distillation Framework for RGB-TIR Datasets"** distill the pre-trained RGB modality onto the TIR modality on unlabeled RGB-TIR datasets, utilize the two branches of the network to process data from different modalities to transfer cross-modal knowledge. 
Wang et al.**Wang et al., "Knowledge Distillation Framework with Model Compression and Transfer"** propose a knowledge distillation framework that combines model compression and transfer. 
Zhao et al.**Zhao et al., "Siamese Network-Based Tracking Method with Balance Between Efficiency and Complexity"** propose a tracking method based on a Siamese network, which optimizes the balance between tracking efficiency and model complexity through distillation, integration, and framework selection, achieving better tracking performance and speed. 
Ge et al.**Ge et al., "Channel Distillation Framework for Depth Trackers"** propose a novel framework called ``channel distillation", which optimizes the performance of depth trackers by adaptively selecting information feature channels, achieving accurate, fast, and low memory required visual tracking. 
Cui et al.**Cui et al., "Efficient Visual Tracking with MixFormerV2"** introduce special prediction tokens and distillation model compression methods in MixFormerV2, which significantly improved efficiency while ensuring accuracy. 
Different from these works, in this paper, we propose a framework for knowledge distillation from multi-modal or multi-view to unimodal, which achieves efficient visual object tracking through hierarchical knowledge distillation. 



\subsection{Test-Time Tuning} \label{subsec:ttt} 
The idea of Test Time Tuning is to use self-supervised learning to continue training the model on test sets. 
%%%
% In the 1990s, Bottou et al.**Bottou et al., "Local Learning for Machine Learning"** introduced the pioneering concept of Local Learning, a methodology that emphasizes training on neighboring data points before making predictive inferences. This work stands as a seminal paper, marking the first instance where locality is articulated as a foundational principle in machine learning.
% Building on this idea, Vladimir Vapnik**Vladimir Vapnik, "Try to Get the Answer You Really Need"** emphasizes the principle of "Try to get the answer that you really need but not a more general one." He advocates for the utilization of test data to impose additional constraints upon the decision boundary, thereby enhancing the model's ability to generalize effectively while remaining focused on its immediate task.
TTT**Yu et al., "Test-Time Tuning Method"**, introduces a Test Time Tuning method that uses self-supervised learning to continue training the model on test sets. 
Meanwhile, it benefits from knowledge distillation from the teacher network. It is trained with tracking loss functions and achieves better tradeoff between accuracy and model complexity. 
Our tracker achieves a better tradeoff between accuracy and model complexity.