\section{Related work}
Currently, the most objective and commonly used VAD datasets are mainly sourced from various surveillance cameras. For example, the UCSD Li et al., "UCSD Pedestrian Dataset" and ShanghaiTech Li et al., "ShanghaiTech Part A and B Datasets" datasets have recorded anomalous situations under different lighting conditions and scenes through campus surveillance cameras. The UCF-Crime Smeureanu et al., "UCF Crime Dataset" dataset focuses on public safety, including anomalous events such as traffic accidents, burglaries, and explosions.


With the development of assisted driving technology, anomaly detection in first-person perspective traffic videos has attracted widespread attention. Chan et al., "A New Dataset for Road Traffic Accident Detection" proposed a dataset of road traffic accident videos recorded by dash cameras, with anomalous frames annotated. {data5} collected 1,500 video clips of road anomalies recorded by dash cameras from East Asia on YouTube, with the start and end times of anomalous events labeled. Additionally, Zhang et al., "Anomaly Detection in Road Traffic Videos" extracted 803 clips from the BDD100K Bordone et al., "BDD100K: A Large-Scale Driving Video Dataset" dataset to construct a new dataset.

In the realm of driver video detection, L. Lyu et al., "NTHU-DDD: A New Benchmark for Driver Drowsiness Detection" presented the NTHU-DDD dataset, which is designed to detect driver drowsiness. This dataset captures a variety of driver behaviors under different lighting conditions, including normal driving, yawning, slow blinking, and dozing off.The KMU-FED dataset Wang et al., "KMU-FED: A Facial Expression Dataset for Driver Fatigue Detection" captures the facial expressions of drivers using an infrared camera while driving, including 55 image sequences from 12 subjects.Although there is currently a lack of publicly available annotated datasets specifically targeting facial expressions and speech anomalies of drivers, there are many valuable datasets in the field of human video and audio emotion classification research that can serve as references. For example, video datasets annotated based on discrete emotion classification, such as RAVDESS Livingstone et al., "RAVDESS: A Spoken Emotion Dataset" and RAV-DB  Mohammadi et al., "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAV14)" provide valuable resources for research. Additionally, some datasets employ more complex emotion classification methods, such as valence and arousal-based emotion classification, like AffectNet Mollahosseini et al., "AffectNet: An Affective Emotion Dataset" and RECOLA Ringeval et al., "RECOLA: A Multimodal Database for Emotion Recognition". These datasets offer important references and insights for exploring anomaly detection in drivers' emotional states.

Extensive research has been devoted to addressing road incident detection and multimodal recognition.These efforts contribute significantly to the development of intelligent transportation systems by leveraging advanced sensor technologies and sophisticated algorithms to promptly identify potential hazards on the road.

The EfficientFace framework used in video branch processing is a highly efficient network structure that balances computational efficiency and recognition accuracy. It utilizes EfficientNet-B5 as its backbone network and employs depthwise separable convolutions to reduce the number of parameters while maintaining a high level of feature extraction capability. Additionally, it incorporates residual connections to facilitate information flow and gradient propagation within the network.

Furthermore, EfficientFace integrates an attention mechanism module, which enhances its ability to detect occluded parts in images and recognize key regions. The aim of EfficientFace is to provide a compact and efficient image detection framework by optimizing the sharing and learning of features of different scales and modes within the network structure Tan et al., "EfficientFace: A Compact Network for Image Detection".

Mel-Frequency Cepstral Coefficients (MFCC) are a commonly used acoustic feature in tasks like speech recognition, speaker identification, and other speech-related tasks. MFCC is used to extract features from speech data by first taking the logarithm of the speech spectrum on the Mel frequency scale, and then applying a discrete cosine transform to the processed results to obtain the cepstral coefficients Mermelstein et al., "Analysis of Experiments on the Recognition of Spoken English Using an HMM-Based Pattern-Recognition System".

% The process of extracting MFCC is as follows:

% \begin{enumerate}
%     \item \textbf{Pre-emphasis}: Apply a high-pass filter to the speech signal to enhance the high-frequency parts of the speech data.
%     \item \textbf{Framing}: Divide the speech signal into short frames, each with a length of about 20 to 30 milliseconds.
%     \item \textbf{Windowing}: Apply a Hamming window to each speech segment that was framed in the previous step.
%     \item \textbf{Short-Time Fourier Transform (STFT)}: Perform a Fast Fourier Transform (FFT) on the signal data processed by windowing to obtain the spectrum.
%     \item \textbf{Mel-scale Filterbank}: Pass the transformed spectrum data through a set of triangular filters to obtain the Mel spectrum.
% \end{enumerate}

% MFCC is an auditory feature that fully considers the hearing characteristics of the human ear, making the data processing more aligned with the human auditory perception process. MFCC is widely used in tasks such as speech emotion recognition and speaker recognition. By processing speech data in this way to extract MFCC features, it not only retains the important and key information of the speech signal but also enhances the discriminability of the extracted features, thus performing excellently in various speech processing tasks.

For intermediate feature fusion, the commonly used approach involves sharing features at the intermediate layers of a neural network. This process begins with performing feature extraction on each modality or data source separately. After extracting the features, they are fused to jointly learn the feature representations of different modalities. The fusion methods include concatenation, addition, weighted average, and the attention mechanism, which will be introduced later. Using intermediate feature fusion can retain the feature representations of each modality or data source to a certain extent, thereby preserving their advantages. Additionally, the choice of fusion layers is relatively flexible.Mohammadi et al., "A Survey on Multimodal Fusion Techniques for Visual and Auditory Emotion Recognition".

In the field of modality fusion, the use of self-attention mechanisms is currently a relatively effective fusion method Vaswani et al., "Attention Is All You Need". The formula \( A_n \) represents the output of a self-attention mechanism, which is a commonly used technique in sequence modeling. This formula calculates attention weights by applying the softmax function to the dot product of the query vectors \( q \) and the key vectors \( k \), and then these weights are applied to the value vectors \( v \), resulting in a weighted output.

Specifically, \( qk^T \) captures the similarity between the queries and the keys, computed via the dot product. This is then divided by \( \sqrt{d} \), where \( d \) represents the dimensionality of the latent space, to control the scaling of the variables before the softmax function, aiding in gradient stability. The softmax function is then applied to ensure that the sum of all weights equals 1, representing a probability distribution.

Thus, \( A_n \) reflects a weighted representation of the input features after undergoing a series of learnable transformations, allowing the model to dynamically allocate importance among different parts of the input. This mechanism is a cornerstone of the Transformer architecture, which has demonstrated superior performance across a variety of tasks and applications.

\begin{equation}
A_n = \text{softmax}\left(\frac{qk^T}{\sqrt{d}}\right)v,
\end{equation}
where \( d \) is the dimensionality of a latent space, for the vector \( d \) is the dimensionality of the vector, and for matrix the \( d \) respresents the dimensionality of the column of the matrix.

Considering the task of fusion of two modalities \( a \) and \( b \), self-attention can be utilized as a fusion approach by calculating queries from modality \( a \) and keys and values from modality \( b \). This results in representation learnt from modality \( a \) attending to corresponding modality \( b \) and further applying the obtained attention matrix to the representation learnt from modality \( b \).

This process of calculating attention weights and applying them to the value vectors allows the model to focus on different parts of the input sequence with varying degrees of importance, thereby capturing complex dependencies and relationships within the data.And the self-attention mechanism (Self-Attention) has achieved remarkable performance in multimodal fusion in recent years.