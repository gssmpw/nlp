\appendix
\newpage
\section{Implementation}
\label{sec:imp}
% 我们使用Mistral 7B Instruct 模型～\cite{}作为基座模型用于微调。在训练阶段，我们follow前人的工作，只微调了output tokens上的loss，没有微调input tokens上的loss，即监督式微调SFT。我们在Xtuner这个框架下进行微调，学习率为2e-5, warmup ratio是0.03，batch size为1，我们采用Lora的形式训练，r为64，alpha为16，dropout为0.05。 在生成阶段，我们利用vllm在温度系数T=0.7条件下生成.
In data synthesis, we sample 15 seed queries, and the maximum number of conversation turns is three. We iterate for three rounds to refine the answers.

We utilize the Mistral 7B Instruct model~\cite{jiang2023mistral} as the base model for fine-tuning. In the training phase, following prior works~\cite{taori2023stanford,Wang2022SelfInstructAL,Xu2023WizardLMEL}, we apply supervision on the output tokens' loss. The fine tuning is performed using the Xtuner framework~\cite{2023xtuner} with a learning rate $lr=2e-5$, a warm-up ratio of 0.03, and a batch size of 1. We employ the LORA training method with hyper-parameters rank $r$ set to 64, $\alpha$ set to 16, and dropout rate $p$ set to 0.05. During the generation phase, text generation is performed using vLLM~\cite{kwon2023efficient} with a temperature coefficient $T=0.7$.
% 在RAG设置中，我们利用langchain框架，采用CharacterTextSplitter在chunk size为512， overlap为32的条件下，对Domain data进行切分成文本chunks。我们通过all-mpnet-base-v2将chunk向量化，并且保存在Chroma数据库中。在检索时，通过计算检索的question与chunks之间的相似度，取top 3的chunks作为检索结果。
In the context of Retrieval-Augmented Generation (RAG), we utilize the LangChain framework to process domain-specific data. We employ CharacterTextSplitter to segment the data into text chunks with a chunk size of 512 and an overlap of 32. These chunks are then embedded using the pre-trained all-mpnet-base-v2 model\footnote{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}, and the embeddings are stored in a Chroma database. During retrieval, we calculate the similarity between the question and the stored chunks, selecting the top 3 most similar chunks as the retrieval results. 


\section{Baselines}
\label{sec:baselinesDetailed}
We compare \ourmodel with the following instruction synthesis baselines.
\begin{itemize}
    \item Self Instruct~\cite{zhang2023self}: a method which leverages a small set of seed data and a pretrained language model to synthesize a large amount of instructional data for fine-tuning.
    \item Evol Instruct~\cite{Xu2023WizardLMEL}: a method that starts with a basic set of instructions and employs a large language model to iteratively rewrite them, progressively enhancing their complexity. This approach generates a wide array of instructional data with varying levels of complexity. 
    \item Magpie~\cite{xu2024magpie}: a self-synthesis method that leverages the autoregressive feature of aligned LLMs like Llama-3-Instruct to auto-generate 4 million high-quality instructions, with 300K selected for fine-tuning.
\end{itemize}
We also compare \ourmodel with the following baselines that use RAG to augment SFT. 
\begin{itemize}
    \item RAFT~\cite{zhang2024raft}: a training method that enhances large language models (LLMs) for open-book question answering by utilizing Chain-of-Thought (CoT) during the Supervised Fine-Tuning (SFT) phase. It incorporates both relevant and irrelevant documents in the context, training the model to ignore the irrelevant ones and focus on citing useful information in its output. 
    \item DSF: performing standard supervised finetuning, without documents in context. We follow the same setting as mentioned in RAFT.
\end{itemize}
\textbf{Remarks.} The goal of RAFT is to train the model to distinguish which documents are relevant to the question so that the model can answer based on these documents. On the other hand, \ourmodel aims to help the model identify knowledge in the documents that is not only relevant to the question but also aligns with the user's intent (since the answers after CDR incorporate conversational information, uncovering the deep user intent in the question). We aim for this process to be implicit, avoiding the reliance on explicit CoT, which can sometimes be inaccurate. User intentions are complex and diverse, and inappropriate or stereotypical CoT reasoning may hinder the model's ability to fully capture the user's true intent~\cite{turpin2023language}. Besides, unlike RAFT, we did not deliberately introduce irrelevant documents in instructions that could confuse the model.


\section{Comparison of Response Quality Evaluated by Different LLMs}
\label{sec:r1result}
% 为了更进一步验证\ourmodel的有效性，我们利用slow-thinking reasoning model进行评估。具体来说，我们使用DeepSeek-R1\footnode{我们本地部署了DeepSeek开源的DeepSeek-R1-Qwen-32B}使用和Section~\ref{sec:comp_study}相同的prompt来指导DeepSeek-R1评估一些代表性的baselines以及\ourmodel在Recent数据集上的表现。如Table~\ref{tab:r1comp}所示。结果表明，在给予slow-thinking reasoning的DeepSeek-R1中，\ourmodel依然可以得到和在GPT-4-turbo相同的结论。
To further validate the effectiveness of \ourmodel, we conducted an additional evaluation using a slow-thinking reasoning model. Specifically, we employed DeepSeek-R1~\cite{guo2025deepseek}\footnote{We locally deployed the open-source DeepSeek-R1-Distill-Qwen-32B model and DeepSeek-R1-Distill-Llama-70B model} and Llama-3.1-405B to assess the performance of representative baseline methods and \ourmodel on the Recent dataset, following the same evaluation prompt template described in Section~\ref{sec:comp_study}. As demonstrated in Table~\ref{tab:r1comp}, the experimental results reveal that when evaluated through the slow-thinking reasoning framework of DeepSeek-R1, \ourmodel achieves consistent conclusions with those obtained from GPT-4-turbo. This alignment persists across multiple evaluation dimensions, suggesting that our method maintains robust performance even under more deliberate and systematic reasoning paradigms and different LLM-based evaluators.



\begin{table}[htbp]
\caption{Performance of each component in \ourmodel on Recent dataset evaluated by Different LLM Evaluators.}
\label{tab:r1comp}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllllll}
\hline
          Evaluator & Setting & Rel. & Comp. & Clar. & Acc. & Act. \\
           \hline
           \multirow{5}{*}{Llama-3.1-405B} &GPT-4-turbo & 4.50 & 3.79 & 4.60 & 4.52 &3.94 \\
           &Self Instruct & 4.67 & 4.01 & 4.78 & 4.62 & 4.17 \\
           &Evol Instruct & 4.63 & 3.99 & 4.77 & 4.63 &4.17 \\
           &RAFT & 4.60 & 3.81 & 4.63 & 4.59 & 4.05 \\ &\ourmodel  & \textbf{4.78} & \textbf{4.25}  & \textbf{4.79}  & \textbf{4.73} & \textbf{4.57} \\
          
           \hline
            \multirow{5}{*}{\shortstack{DeepSeek-R1-\\Distill-Qwen-32B}} &GPT-4-turbo & 4.43 & 3.83 & 4.55 & 4.56 &4.14 \\
           &Self Instruct & 4.53 & 3.92 & 4.72 & 4.57 & 4.27 \\
           &Evol Instruct & 4.48 & 3.86 & 4.70 & 4.52 &4.27 \\
           &RAFT & 4.60 & 3.92 & 4.70 & 4.72 & 4.31 \\
            & \ourmodel  & \textbf{4.75} & \textbf{4.26}  & \textbf{4.82}  & \textbf{4.77} & \textbf{4.65}

           \\ \hline
           \multirow{5}{*}{\shortstack{DeepSeek-R1-\\Distill-Llama-70B}} &GPT-4-turbo & 4.40 & 3.71 & 4.67 & 4.61 &4.07 \\
           &Self Instruct & 4.42 & 3.79 & 4.84 & 4.55 & 4.23 \\
           &Evol Instruct & 4.36 & 3.72 & 4.83 & 4.49 &4.22 \\
           &RAFT & 4.48 & 3.74 & 4.78 & 4.68 & 4.22 \\
                  
&\ourmodel  & \textbf{4.60} & \textbf{4.06}  & \textbf{4.86}  & \textbf{4.70} & \textbf{4.56} \\
\hline
\end{tabular}}
\end{table}


% 我们分别从\ourmodel，Self Instruct和Evol Instruct这三种合成数据集和Recent评估数据集中采样了1000个问题，然后利用all-mpnet-base-v2 model获得它们的embedding。我们使用t-SNE分别\ourmodel，Self Instruct和Evol Instruct与Recent的分布相似情况。更进一步的，后我们计算每一个数据集的质心embedding，然后分别计算\ourmodel，Self Instruct，Evol Instruct与Recent的质心相似度，来评判哪种数据合成方式的合成出来的数据更接近用户真实的问题。
\section{Comparison of Synthetic Data Quality}
We sample 1000 questions each from three synthetic datasets: \ourmodel, Self Instruct, and Evol Instruct, as well as from the Recent evaluation dataset. Using the all-mpnet-base-v2 model, we obtain embeddings for these questions. We then use t-SNE to assess the distribution similarity between \ourmodel, Self Instruct, Evol Instruct, and Recent datasets. Furthermore, we calculate the centroid embedding for each dataset and assessed the similarity of the centroid embeddings between \ourmodel, Self Instruct, Evol Instruct, and Recent. This approach allows us to evaluate which synthetic data generation method yields data that is more comparable to real user questions.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{figures/quality_compare.pdf}
    \caption{Similarity between different synthetic data methods and real user questions}
    \label{fig:qualComp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{figures/DeepthinkPie.pdf}
    \caption{Top 10 Most Common Root Verbs (Inner) and Their Top 3 Direct Noun Objects (Outer) in \ourmodel}
    \label{fig:deepthinkPie}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{figures/EvolPie.pdf}
    \caption{Top 10 Most Common Root Verbs (Inner) and Their Top 3 Direct Noun Objects (Outer) in Evol Instruct}
    \label{fig:evolPie}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\linewidth]{figures/SelfPie.pdf}
    \caption{Top 10 Most Common Root Verbs (Inner) and Their Top 3 Direct Noun Objects (Outer) in Self Instruct}
    \label{fig:selfPie}
\end{figure}


% 如图~\ref{fig:qualComp}所示，通过对比\ourmodel与其他方法如Self Instruct和Evol Instruct所生成的指令数据，我们可以明显看到，\ourmodel生成的指令数据与真实用户问题的相关性更高。这种显著的相关性表明，\ourmodel在构建指令时，通过模拟用户在真实广告平台上对话形式，语气风格等的数据构造方法，生成的指令不仅更符合用户的表达方式，还能满足用户在使用场景中可能提出的真实需求。
As depicted in Figure~\ref{fig:qualComp}, a comparison between \ourmodel and other methods such as Self Instruct and Evol Instruct reveals that the instruction data generated by \ourmodel exhibits significantly higher relevance to actual user questions. The centroid smilarity of \ourmodel is 0.93, while Self Instruct got -0.97 and Evol Instruct got 0.72. This marked relevance demonstrates that \ourmodel, through its data construction approach that simulates conversational formats and style as found on real advertising platforms, generates instructions that not only better reflect user expression but also satisfy the actual demands users may present in specific scenarios.



Furthermore, we follow the previous work~\cite{xu2024magpie} and show the visualization of root verbs and their direct noun objects. Figure ~\ref{fig:deepthinkPie},~\ref{fig:evolPie} and~\ref{fig:selfPie} visualize the top common root verbs and their direct noun objects of DeepThink,  Evol Instruct and Self Instruct dataset, respectively.
A notable finding is that in \ourmodel, the verb "provide" holds a significantly larger proportion compared to other synthesis approaches. Additionally, expressions such as "-guidance" and "-example" are types of questions that users are more inclined to ask in the advertising domain. This result further validates that \ourmodel can generate more questions that users would actually ask in this field.


\section{Full Parameters vs. LoRA Finetuning}
% 为了回答RQ5，我们分别在Self Instruct， Evol Instruct 和 \ourmodel上进行全参数微调和LoRA微调，并在Recent数据集上进行评估。对于全参数微调，我们选择的基座模型是Mistral 7B base~\cite{}, 对于LoRA微调，我们选择的基座模型是Mistral 7B Instruct~\cite{}。指标上我们关注于模型回答关于问题的相关度，这是因为回答的相关度直接反映了...结果如Figure~\ref{fig:fullandlora}所示，我们可以发现全参数微调显著低于LoRA微调的效果。可能的原因是模型在合成数据上进行训练，问题分布与用户真实问题的分布存在偏离，全参微调在泛化到用户真实分布的能力上差劲~\cite{},此外，我们发现相比较Self Instruct和Evol Instruct，\ourmodel在全参数微调上表现要优异不少，这也一定程度反映了我们这种Imitation-based的指令合成方法的有效性，且更加接近用户真实的分布。
We conduct two types of fine-tuning, full-parameter fine-tuning and LoRA fine-tuning, on Self Instruct, Evol Instruct, and our proposed model, \ourmodel. Specifically, we employ QLoRA~\cite{dettmers2024qlora}, a quantization-based efficient finetuning improvement of LoRA.These are subsequently evaluated on the Recent dataset. For full-parameter fine-tuning, we employ the Mistral 7B base model, while for LoRA fine-tuning, the Mistral 7B Instruct is selected as the foundation model. Our evaluation focus primarily on the relevance of the model's responses, as this metric is a crucial indicator of the model's accuracy and utility in understanding and generating answers. Relevance of the responses is critical because it directly influences the model’s capability to solve problems, authenticity, and user satisfaction. As depicted in Figure, we observe that full-parameter fine-tuning significantly underperformed compared to LoRA fine-tuning. One possible reason for this discrepancy is the divergence between the synthetic training data and the distribution of real user questions, which hampers the model's ability to generalize to authentic user data in full-parameter tuning. Additionally, our \ourmodel displays superior performance in full-parameter fine-tuning compared to Self Instruct and Evol Instruct, which partially demonstrates the effectiveness of our imitation-based instruction synthesis method, as it yields instructions that more closely align with the distribution of real user questions.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{figures/FullLoRA.pdf}
    \caption{Performance between Full parameters and QLoRA finetuning}
    \label{fig:fullandlora}
\end{figure}



\section{Case Study}

\subsection{Synthesized Conversations and Refining Responses}
\label{sec:scrr}
We present a conversation synthesized by DeepThink, as shown in Figure~\ref{fig:goodCDS}. In this conversation, the user initially asks a rather vague question, leading to a less helpful and broad response. In the next round, the user clarifies the question, receiving a more precise answer. Finally, the user follows up on a specific detail in the response, obtaining a deeper and more detailed answer. This type of conversation closely mirrors the scenarios that people encounter when using language models, as described in Section~\ref{sec:intro}, and $\ourmodel$ successfully synthesizes such conversations. Furthermore, Figure~\ref{fig:goodCDR} shows that $\ourmodel$ improves the original answer to the question using Conversation-based Data refinement.

\subsection{Online platform vs. DeepThink}
We demonstrate the answers for the online advertising platform(GPT-4-turbo+RAG) and $\ourmodel$ regarding the same real user questions. They are shown in Figure~\ref{fig:case1} and~\ref{fig:case2}. The cases show answers generated by $\ourmodel$ better than the online advertising platform in completeness and actionability.



\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{figures/goodCDS.pdf}
    \caption{The case of conversation between the user and the assistant synthesized by $\ourmodel$.}
    \label{fig:goodCDS}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{figures/goodCDR.pdf}
    \caption{The case of the response after Conversation-based Data Refinement.}
    \label{fig:goodCDR}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{figures/case1.pdf}
    \caption{Online platform answer vs. DeepThink(Case 1)}
    \label{fig:case1}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{figures/case2.pdf}
    \caption{Online platform answer vs. DeepThink(Case 2)}
    \label{fig:case2}
\end{figure*}




\section{Prompts}
\label{sec:appPrompts}
\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/docQAsPrompt.pdf}
    \caption{The prompt of extracting Seed QAs from documents}
    \label{fig:docQAsPrompts}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/userAgentPrompt.pdf}
    \caption{The prompt of $\Inquirer$ in Conversation-based Data Synthesis}
    \label{fig:userAgentPrompt}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/assitantAgentPrompt.pdf}
    \caption{The prompt of $\Assistant$ in Conversation-based Data Synthesis}
    \label{fig:assitantAgentPrompt}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/refinerPrompt.pdf}
    \caption{The prompt of Conversation-based Data Refinement}
    \label{fig:refinerPrompt}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/evalDocPrompt.pdf}
    \caption{The prompt of the evaluation prompt based on the relevant document}
    \label{fig:evalDocPrompt}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\linewidth]{prompts/evalwinratePrompt.pdf}
    \caption{The prompt of the winrate evaluation prompt}
    \label{fig:evalwinratePrompt}
\end{figure*}

% \begin{figure*}[htbp]
% \centering
% \includegraphics[width=1\linewidth]{prompts/inferPrompt.pdf}
%     \caption{Inference prompt}
%     \label{fig:inferPrompt}
% \end{figure*}

