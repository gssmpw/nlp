\section{Experiments}
\subsection{Experimental Setups}
\label{sec:setups}
\textbf{Dataset}. We consider the Advertising Domain in this paper. We have obtained relevant documentation from an advertising platform, consisting of a total of 7.5k text segments covering Setup and Basics, Manage Ads, Measuring Results, and Billing and Payments. For testing, we have collected real user questions from our advertising platform, spanning the period from March 27 to June 6. The dataset underwent a cleaning process, including deduplication and filtering out questions unrelated to advertising, resulting in a refined collection of 7,801 user questions. Subsequently, we order these questions chronologically based on the time they were posted and divide them into two subsets: (1) \textit{Historic}: consists of the earliest 90\% of data (i.e., 6,617 questions). This subset allows us to gauge the model's response quality over a broad range of topics, thus offering a comprehensive view of the model's overall capabilities. (2) \textit{Recent}: consists of the most recent 10\% of data (i.e., 1,184 questions), representing the latest user needs on the platform. It is used to assess the LLM's response quality for recent user activities. This subset helps detect any potential degradation in the model's performance over time, ensuring its continued reliability and alignment with user expectations.


\textbf{Imitation seed data.} Following prior work~\cite{zhang2023self}, we prompt GPT-4-turbo to generate at least one high-quality instruction for each text segment from the domain data. To approximate the distribution of real user inquiries, we randomly select 15 authentic user questions from our advertising platform to prompt GPT-4-turbo to generate seed questions. Our conversation-based Data synthesis also employs the same set of 15 real user questions throughout the process. We obtain approximately 5k seed data. Detailed prompts are provided in Figure~\ref{fig:docQAsPrompts}. 

\begin{table}[!t]
\caption{Statistics of Instruction Data Generated by Different Methods}
\label{tab:genQAsStatistic}
\resizebox{\linewidth}{!}{
\begin{tabular}{lllll}
\hline
              & \# Examples & \# Domain & \makecell[l]{Instruction \\Length} & \makecell[l]{Output\\ Length} \\
\hline
Seed data     & 5k          & advertising       & 10±3              & 87±20         \\
Self Instruct & 23k         & advertising       & 15±13             & 56±27         \\
Evol Instrcut & 15k         & advertising       & 18±8              & 74±24         \\
Magpie        & 300k        & general   & 12±6              & 377±76            \\
\ourmodel     & 12k         & advertising       & 18±6              & 90±20        \\
\hline
\end{tabular}
}
\end{table}

\begin{table*}[htbp]
\caption{Performance of different methods}
\label{tab:ovrPerf}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{lllllllllllll}
\hline
                                Type & Model         & \multicolumn{5}{l}{Historic} & \multicolumn{5}{l}{Recent}         & Avg  \\
                                &               & Rel. & Comp. & Clar. & Acc. & Act. & Rel. & Comp. & Clar. & Acc. & Act. & Ovr. \\ \hline
\multirow{4}{*}{LLMs}    & GPT-4-turbo    & 4.28 & 3.70  & 4.59  & 4.55 & 3.89 & 4.24 & 3.67  & 4.54  & 4.50 & 3.87 & 4.18 \\
                                & GPT-3.5-turbo  & 3.80 & 3.29  & 4.05  & 4.03 & 3.47 & 4.22 & 3.66  & 4.54  & 4.49 & 3.85 & 3.94 \\
                                & Mistral 7B    & 4.23 & 3.51  & 4.70  & 4.60 & 3.90 & 3.97 & 3.24  & 4.44  & 4.37 & 3.60 & 4.05 \\
                                & Llama3 8B     & 4.06 & 3.37  & 4.51  & 4.35 & 3.66 & 3.97 & 3.41  & 4.42  & 4.29 & 3.67 & 3.97 \\ \hline
\multirow{4}{*}{Data Synthesis+SFT} & Self Instruct & 4.29 & 3.77  & 4.79  & 4.51 & 4.09 & 4.25 & 3.74  & 4.75  & 4.46 & 4.06 & 4.27 \\
                                & Evol Instruct & 4.28 & 3.83  & 4.78  & 4.52 & 4.06 & 4.23 & 3.73  & 4.73  & 4.44 & 4.01 & 4.26 \\
                                & Magpie        & 4.02 & 3.73  & 4.52  & 4.21 & 4.05 & 3.97 & 3.65  & 4.45  & 4.16 & 3.96 & 4.07 \\ 
                                & \ourmodel-S        & \underline{4.31} & \underline{3.99}  & \underline{4.83}  & 4.55 & \underline{4.33} & \underline{4.27} & \underline{3.95}  & \underline{4.80}  & 4.52 & \underline{4.29} & \underline{4.38} \\                                 
                                \hline
\multirow{2}{*}{RAG-augmenting-SFT}     & RAFT          & 4.27 & 3.66  & 4.66  & \underline{4.60} & 3.95 & 4.22 & 3.62  & 4.63  & \underline{4.54} & 3.93 & 4.21 \\
                                & DSF           & 4.19 & 3.64  & 4.69  & 4.43 & 3.90 & 4.09 & 3.52  & 4.58  & 4.32 & 3.80 & 4.12 \\ \hline
\makecell[l]{Data Synthesis\\+RAG-augmenting-SFT}       & \ourmodel     & \textbf{4.44} & \textbf{4.16}  & \textbf{4.89}  & \textbf{4.72} & \textbf{4.43} & \textbf{4.37} & \textbf{4.10}  & \textbf{4.86}  & \textbf{4.68} & \textbf{4.40} & \textbf{4.50} \\
\hline
\end{tabular}}
\end{table*}


\textbf{Baseline}. (1) \textit{Proprietary and open-source LLMs} including GPT-4-turbo, GPT-3.5-turbo, Mistral 7B, and Llama3 8B. (2) \textit{Data synthesis+SFT} methods which synthesize instructions and utilize Mistral 7B as the base model for SFT, including Self Instruct~\cite{zhang2023self}, Evol Instruct~\cite{Xu2023WizardLMEL}, and Magpie~\cite{xu2024magpie}. Note that these data-synthesis strategies do not incorporate Retrieval Augmented Generation (RAG) in data synthesis. It is impossible to include retrieved contents as part of the question during the SFT phase. To ensure a fair comparison, we also implement a \textit{variant of the proposed model} \ourmodel-S, which does not use retrieved content in the question. The statistics of generated instructions are shown in Table~\ref{tab:genQAsStatistic}. (3) \textit{RAG-augmenting-SFT} baselines, which utilize Mistral 7B as the base model for retrieval augmented SFT, including RAFT~\cite{zhang2024raft} and DSF~\cite{zhang2024raft}. More details about baselines are discussed in Appendix~\ref{sec:baselinesDetailed}




\textbf{Evaluation}. 
Following previous works~\cite{zhu2023judgelm,zheng2024judging}, we leverage GPT-4-turbo to evaluate the quality of model-generated responses. Specifically, we input the question, the most relevant documents, and the model's response into GPT-4-turbo, prompting it to score the model's answer based on relevance, completeness, clarity, accuracy, and actionability. We further evaluate \ourmodel using DeepSeek-R1 and Llama-3.1-405B, with results provided in the Appendix~\ref{sec:r1result}.


\subsection{Comparative Study}
\label{sec:comp_study}
\textbf{Comparison of Response Quality}. We first compare the performance of 
\ourmodel with various baseline models regarding multi-facet evaluation. The results are shown in Table~\ref{tab:ovrPerf}. We have the following observations.

%（1）\ourmodel 在综合表现和最近表现都取得最好的表现。在Historic数据集中，在Relvance，Completeness，Clarify，Accuracy和Actionability，分别取得4.44，4.16，4.89，4.72和4.43分。在Recent数据集中，在Relvance，Completeness，Clarify，Accuracy和Actionability，分别取得4.37，4.10，4.86，4.68，4.40分。相比较与专有模型，平均分Overall比表现最好的GPT-4-turbo，提升7.66%，这表明\ourmodel相比较于专有模型在广告领域具有更专业的更高质量的回答。相比较于数据合成的其他基线，Overall比表现最好的Self-Instruct提升5.39%，这表明相比较于其他垂直领域常用的数据合成方式，\ourmodel合成出来的指令质量更高。
(1) \ourmodel achieves superior performance in the advertising domain. Compared with proprietary LLMs, \ourmodel achieved improvements of 3.43\%, 12.09\%, 6.69\%, 3.74\%, and 13.69\% over the best-performing GPT-4-turbo in relevance, completeness, clarity, accuracy, and actionability metrics, respectively. It indicates proprietary LLMs only focus on general domain knowledge and do not perform well in the vertical domain, i.e., the advertising domain. Supervised Fine-Tuning (SFT) is necessary for the advertising domain and \ourmodel proposes an efficient data synthesis strategy for SFT.
Compared with other data synthesis strategies, \ourmodel achieved improvements of 3.15\%, 9.98\%, 2.21\%, 4.77\%, and 8.30\% over the best-performing Self Instruct in relevance, completeness, clarity, accuracy, and actionability, respectively. Besides, \ourmodel-S also achieved average improvements of 2.68\% over Self Instruct. This means that our model benefits from conversation-based synthetic data, which enables it to gain insights into users' hidden interests and provide higher-quality responses.


%（2）\ourmodel在Completeness和Actionability上提升显著。相比较于其他最好的基线(除了\ourmodel-S)，在Completeness和Actionability上分别提升了9.13%和8.34%。可能的原因是\ourmodel利用对话信息增强回答质量。由于对话信息主题高度集中，且与问题息息相关，所以有利于从多个方面对问题进行回答，因而Completeness分数提升显著。此外，由于一场对话中，往往会深入到"How to do"这样的问题，因此可以整场对话的Actionability也是显著的。
(2) Our model demonstrates significant improvements in Completeness and Actionability, outperforming other baselines by at least 9.13\% and 8.34\%, respectively. These gains are likely due to our use of conversation data, which enhances response quality in two key ways. First, conversation data is highly focused on specific topics, enabling the model to provide more comprehensive and detailed answers, thus improving Completeness. Second, because conversations often explore practical "how-to" details, the model generates more actionable responses, boosting Actionability. 

(3) In our model, RAG-augmenting-SFT has resulted in significant performance improvements. Specifically, \ourmodel demonstrates an overall improvement of 2.73\% compared with \ourmodel-S, which does not use RAG. This indicates that RAG helps the model generate higher-quality responses. 



\begin{figure}[!t]
\centering
\includegraphics[width=0.98\linewidth]{figures/winrate.pdf}
    \caption{Human Preference Evaluation (WinRate models vs. GPT-4-turbo \%)}
    \label{fig:winrate}
\end{figure}


\textbf{Comparison of Human Preference}. 
% 对于人类偏好评估，我们计算每一个模型与GPT-4-turbo(线上广告助手平台所使用的LLM）的winrate，以展现人类对模型生成的回答偏好程度。
We calculate the WinRate of each model in comparison with GPT-4-turbo (the LLM used by the online advertising assistant platform). To reflect the degree of human preference, we use the judgments from GPT-4-turbo. Detailed prompts
are provided in Figure~\ref{fig:evalwinratePrompt}. We report the WinRate on the historical and recent subset in Figure~\ref{fig:winrate}. We also color the baselines by the average win rate. We have the following observations.


% (1).用户会更加偏好DeepThink的回答。DeepThink在Historic和Recent数据集中都取得最好的winrate，分别是89.69%和87.58%，即相比较原来的广告助手，用户会更加偏好DeepThink的回答。可能的原因是DeepThink的回答包括了更多有利于解答用户问题的信息，更加的全面。
(1) Users exhibit a stronger preference for responses generated by \ourmodel. \ourmodel achieves the highest WinRates on both the Historic and Recent datasets, with scores of 89.69\% and 87.58\%, respectively. This indicates that, compared to the original advertising assistant, users prefer the responses from \ourmodel. %A possible reason for this preference is that \ourmodel's responses include more comprehensive information that is beneficial for addressing user questions.

% 在广告领域，用户更加偏好经过SFT后的模型的回答。几乎所有的SFT后的模型（除了RAFT），都取得了至少55.77%的WinRate，高于50%的Winrate表示优于在线广告助手（基于GPT-4-turbo）的用户偏好表现。RAFT只有38.35%的WinRate，可能的是因为识别检索中哪些是有效的内容这一思维链对于只有7B参数大小的模型难度过大，容易丢失有效的检索内容，进而生成不够完善的回答。总的来说，用户更加偏好SFT后的模型回答，这表明不经过垂直领域指令微调的GPT-4-turbo生成出来的内容可能过于宽泛和不够专业。
%(2) In the advertising domain, users show a clear preference for responses generated by models fine-tuned with Supervised Fine-Tuning (SFT). Almost all SFT-based models (except for RAFT) achieve a WinRate of at least 55.77\%, which is higher than the 50\% threshold, indicating that these models outperform the GPT-4-turbo-based online advertising assistant. RAFT, with a WinRate of only 38.35\%, may struggle due to its smaller parameter size (Mistral 7B), making it difficult to effectively handle the reasoning chain required to identify relevant retrieval content. This often results in the loss of critical information and the generation of suboptimal responses. Overall, the higher user preference for SFT-based models suggests that GPT-4-turbo, without domain-specific instruction tuning, may produce content that is too generic and lacks domain expertise.
(2) Models fine-tuned by synthesized instructions generally achieve better performance. For example, Self Instruct, Evol Instruct, Magpie, \ourmodel-S, and \ourmodel all achieve a WinRate of at least 70\% on both datasets. 

% （3）\ourmodel相比较其他指令合成的方法表现更好。Self Instruct取得75.32%的WinRate， MagPie取得76.84%的WinRate，Evol Instruct取得80.65%的WinRate。然而，\ourmodel取得88.64%的WinRate。相比较表现最好的Evol Instruct提升了9.90%。这表明我们提出的这种通过conversation挖掘用户深层感兴趣内容，并通过迭代式refiner不断优化回答的策略，质量更高，更能挖掘用户问题背后更加感兴趣的关注点。
(3) \ourmodel demonstrates superior performance compared with other instruction synthesis methods for SFT. \ourmodel achieves a 9.90\% improvement over the best-performing baseline, Evol-Instruct. This improvement highlights the effectiveness of our proposed strategy, which leverages conversation data to uncover users' deeper interests and employs an iterative refiner to optimize answers continuously. Our approach not only generates higher-quality answers but also better captures the underlying concerns and interests behind user queries.


\begin{table}[!t]
\caption{Performance of each component in \ourmodel on Recent dataset}
\label{tab:ablation}
\resizebox{0.96\linewidth}{!}{
\begin{tabular}{llllll}
\hline
           & Rel. & Comp. & Clar. & Acc. & Act. \\
           \hline
\ourmodel  & \textbf{4.37} & \textbf{4.10}  & \textbf{4.86}  & \textbf{4.68} & \textbf{4.40} \\
w/o \convaugment     & 4.19 & 3.89  & 4.73  & 4.49 & 4.21 \\
w/o \convrefine     & 4.21 & 3.76  & 4.72  & 4.45 & 4.07 \\
w/o \convaugment, \convrefine & 4.14 & 3.45  & 4.65  & 4.38 & 3.85 \\
\hline
\end{tabular}}
\end{table}

\subsection{Impact of Conversation-base Data Synthesis and Refinement}
% 我们在Recent数据集上，依次消融(1)利用对话合成的数据增强模块(CDS),(2)利用对话精进回答质量模块(CDR)和（3）同时消融CDS和CDR。结果如表~\ref{}所示，我们有如下结论。
We conduct extensive experiments to show the effectiveness of different components in \ourmodel. We conduct a series of ablation studies that involve: (1) removing \convaugmentfull (w/o \convaugment), (2) removing \convrefinefull (w/o \convrefine), and (3) the simultaneous removal of \convaugment and \convrefine in the recent dataset. The results are presented in Table~\ref{tab:ablation}. From these experiments, we draw the following conclusions.

% (1) 每一个component都是有用的。移除\convaugment后，模型的回答中Relevance下降了4.12%，Completeness下降了5.12%，Clarity下降了2.67%，Accuracy下降了4.06%和Actionability下降了4.32%。移除\convrefine后，模型的回答中Relevance下降了3.72%，Completeness下降了8.34%，Clarity下降了2.83%，Accuracy下降了4.88%和Actionability下降了7.52%。同时移除\convaugment和\convrefine，模型的回答质量进一步下降。Relevance下降了5.32%，Completeness下降了15.90%，Clarity下降了4.27%，Accuracy下降了6.37%和Actionability下降了12.52%。这证明了每一个component都是有用的。
Every component in our model contributes significantly to its performance. When \convaugment is removed, the model exhibits notable declines in response quality: Relevance drops by 4.12\%, Completeness by 5.12\%, Clarity by 2.67\%, Accuracy by 4.06\%, and Actionability by 4.32\%. Similarly, removing \convrefine results in reductions of 3.72\% in Relevance, 8.34\% in Completeness, 2.83\% in Clarity, 4.88\% in Accuracy, and 7.52\% in Actionability. Furthermore, when both \convaugment and \convrefine are removed simultaneously, the model's performance degrades even more significantly, with Relevance decreasing by 5.32\%, Completeness by 15.90\%, Clarity by 4.27\%, Accuracy by 6.37\%, and Actionability by 12.52\%. These results clearly demonstrate the importance and effectiveness of each component in our model.

% (2) 去掉\convrefine影响最大。当移除\convrefine,模型回答的质量中Relevance下降了3.72%，Completeness下降了8.34%，Clarity下降了2.83%，Accuracy下降了4.88%和Actionability下降了7.52%。这表明利用对话内容提升回答质量，以及通过assessment的反馈消除对话中无意义信息对精炼回答的影响，这一步骤非常重要。
(2) The removal of \convrefine has the most significant impact on the model's performance. This demonstrates the critical role of \convrefine in leveraging conversational context to enhance response quality. Specifically, \convrefine refines responses by utilizing assessment feedback to filter out irrelevant or meaningless information from dialogues, thereby significantly improving the overall quality of the generated answers.

% (3) \convaugment对模型回答的relevance作用更大。移除\convaugment后模型回答的relevance下降幅度最大，下降了4.17%。这是可能是因为\convaugment通过对话的形式，模拟用户在生活中的对话形式，这样得到的指令更加广泛且与贴近用户真实的提问。
(3) \convaugment has a particularly strong impact on improving the relevance of the model's responses. When \convaugment is removed, the relevance of the model's answers shows the most significant decline, dropping by 4.17\%. This is likely because \convaugment generates a broader range of high-quality instructions by simulating real-world user conversations. By closely mimicking how users naturally communicate, \convaugment ensures that the generated instructions are more aligned with actual user queries, thereby enhancing the relevance of the model's responses.

% 更进一步，我们进一步消融了\convrefine中refiner的初始效果以及经过feedback指导的refiner效果。如图~\ref{fig:scoreDist}所示。我们有如下的观察.
% (1) refiner的初始效果已经非常显著。不经过refiner的原始回答的综合分数为$r_0=4.63$,经过refiner初始化后的回答的综合分数为$r_1=4.75$，提升了2.59%。这表明利用对话的相关信息增强回答的质量的refiner是十分意义的，而非trivial。
% （2）利用assessment的feedback指导refiner可进一步提升refiner的精炼回答的能力。相比较于refiner初始化后的回答的综合分数为$r_1=4.75$，经过$T$次根据feedback指导refiner后的回答的综合分数提升至$r_T=4.77$，提升了0.42%。这表明assessment的feedback指导refiner是效果的，可以进一步微调答案是其更加符合用户的偏好。

\begin{figure}[!t]
\centering
\includegraphics[width=0.98\linewidth]{figures/distribution.pdf}
    \caption{Score distribution of the instructions}
    \label{fig:scoreDist}
\end{figure}

Furthermore, we conduct ablation studies on the refiner in \convrefinefull. Specifically, we implement the three variants: (1) the original answers obtained by synthesized conversation without refinement (w/o \convrefine), (2) answers initially refined by only the conversation contexts (w/ \convrefine initialization), and (3) answers iteratively refined by assessment feedback (w/ \convrefine). We report the distribution of assessment scores in Figure~\ref{fig:scoreDist}, and we make the following observations.

(1) The initial effect of the refiner is significant. The average score has been increased from $4.63$ to $4.75$ after refiner initialization, representing an improvement of 2.59\%. This indicates that refinement leveraging conversational context enhances response quality.

(2) Feedback-guided refiner further increases the ratio of high-quality answers, i.e., with a score of five. This demonstrates that the assessment feedback effectively guides the refiner to fine-tune responses, making them better aligned with user preferences.



\begin{table}[!t]
\caption{Performance of Imitation-based and Synthesis-only seed data}
\label{tab:imqVsSynQ}
\resizebox{1\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
                & Sim. &Rel. &Comp. &Clar. &Acc. &Act.\\
\hline
Synthesis-only  & 0.76 & 4.14 & 3.42  & 4.63 & \textbf{4.37} & 3.81 \\
Imitation-based & \textbf{0.79} & \textbf{4.15} & \textbf{3.46}  & \textbf{4.64}  & \textbf{4.37} & \textbf{3.84}
\\
\hline
\end{tabular}}
\end{table}
\subsection{Necessary of Imitation}
We analyze the differences between instructions generated by GPT-4-turbo using two distinct approaches: imitation-based, which replicates the style of real user questions, and synthesis-only, i.e., \textit{SELF-QA}, which generates instructions without such imitation. Utilizing the all-mpnet-base-v2 model, we obtain embeddings for each instruction and for real user questions from the Recent evaluation dataset. In addition to the five evaluation dimensions, we also calculate the similarity(Sim.) between the centroid of the imitation-based instruction embeddings and the centroid of real user question embeddings, as well as between the centroid of synthesis-only instruction embeddings and the centroid of real user question embeddings.

As shown in Table~\ref{tab:imqVsSynQ}, imitation-based instructions exhibit higher similarity (Sim.=0.79) to actual user questions compared with synthesis-only instructions (Sim.=0.76). Additionally, models fine-tuned on imitation-based data demonstrate improved performance across various metrics, including Relevance, Completeness, Clarity, Accuracy, and Actionability, compared with those trained with synthesis-only data. Specifically, imitation-based methods achieve improvements of 0.24\% in Relevance, 1.17\% in Completeness, and 0.22\% in Clarity. These results indicate that imitation-based instruction data more closely align with real user queries, leading to enhanced model performance.

\subsection{Performance of RAG-augmenting-SFT }
\label{sec:rag_in_sft}
%Incorporating Retrieval-Augmented Generation (RAG) into the Supervised Fine-Tuning (SFT) process enhances the model's ability to leverage external knowledge bases for generating more accurate and contextually relevant responses. By retrieving pertinent documents during fine-tuning, the model can produce answers that are not only coherent but also grounded in up-to-date and domain-specific information.


To evaluate the impact of Retrieval-Augmented Generation (RAG) on Supervised Fine-Tuning, we first compare the loss of \ourmodel-S  (i.e., uses only the original questions without any retrieved documents) with \ourmodel (i.e., uses retrieved documents). As shown in Figure~\ref{fig:trainloss}, the training loss for \ourmodel-S is significantly higher than that for \ourmodel, with an average increase of 37.28\%. This discrepancy can be attributed to the distinct reliance on knowledge sources during the Supervised Fine-Tuning phase. Specifically, solely relying on the knowledge poses a non-trivial challenge for LLMs. In contrast, integrating retrieved documents within the instructional contexts allows learning objectives to align effectively with more accurate responses, reinforcing LLM's ability during the SFT phase. 


\begin{figure}[htbp]
\centering
\includegraphics[width=0.96\columnwidth]{figures/trainloss.pdf}
    \caption{Training loss trend of \ourmodel with and without RAG-augmenting-SFT on Recent}
    \label{fig:trainloss}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.96\linewidth]{figures/DSFVSDSFR.pdf}
    \caption{Performance of SFT and RAG-augmenting-SFT on Recent}
    \label{fig:ragVsnorag}
\end{figure}


We have shown that \ourmodel achieves a notable improvement regarding all performance metrics compared with \ourmodel-S in Table~\ref{tab:ovrPerf}. We further remove the conversation component and implement (1) SFT that uses only seed instructions for fine-tuning and (2) RAG-augmenting-SFT that uses seed instructions along with retrieved documents. As shown in Figure~\ref{fig:ragVsnorag}, incorporating relevant documents as part of the input on lower-quality instructions also helps the model better understand contextual relationships and enhances QA capabilities.












