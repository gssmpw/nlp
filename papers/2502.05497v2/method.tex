\section{Approach}
As illustrated in Figure~\ref{fig:framework}, \ourmodel consists of four key stages: (1) Seed Question and Answer Synthesis, (2) Conversation-based Data Synthesis, (3) Conversation-based Data Refinement, and (4) Retrieval-augmenting-SFT.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.98\linewidth]{figures/framework.pdf}
    \caption{The framework of \ourmodel}
    \label{fig:framework}
\end{figure*}

\subsection{Seed Question and Answer Synthesis}
We leverage GPT-4-turbo to synthesize instructions. Existing studies such as SELF-QA~\cite{zhang2023self} utilize LLMs to extract questions from documents, enabling the automatic generation of seed questions. Unfortunately, the questions generated through existing approaches exhibit significant linguistic style discrepancies compared with those found in genuine user-LLM interactions. To resolve this issue, we randomly sample a few authentic user questions and prompt GPT-4-turbo to generate queries that mirror the linguistic style and structure of these samples. This method ensures that the generated questions reflect the realistic language and format of the actual user queries. Details of the specific prompts used are in Appendix~\ref{sec:appPrompts}.

\subsection{Conversation-based Data Synthesis}
We implement a dual-role conversation framework based on GPT-4-turbo, where one role is designated as the \Inquirer and the other as the \Assistant. \ourmodel guides the two roles to emulate authentic and high-quality conversations. Specifically, in guiding the \Inquirer, \ourmodel instructs it to mimic the style of actual user inquiries by incorporating real user questions into \Inquirer's prompt. This stylistic imitation distinguishes from earlier methods~\cite{Wang2022SelfInstructAL} without instructing the LLM using actual user queries, resulting in dialogues that more closely reflect real-world conversational dynamics.

Previous methods rely on the inherent knowledge of LLMs to generate answers and often lead to hallucinations, especially in vertical domains where LLMs lack direct training data~\cite{abdullin2024synthetic,liu2024chatqa}. In generating the \Assistant's responses, \ourmodel incorporates a retrieval-augmented generation framework. By retrieving domain-relevant documents to ground responses, this approach mitigates the risk of hallucination and enhances the response's accuracy.

Furthermore, to maintain engagement and progressively deepen the dialogue, \ourmodel instructs the \Assistant to suggest follow-up questions based on topics that may interest the user. The \Inquirer then has the option to (1) choose from these suggestions, (2) generate a new question, or (3) respond with "No more questions" to end the conversation. This structured interaction ensures that the conversation flow remains natural. The interaction is enforced to end when exceeding a predefined maximal number of turns because long dialogues are likely to drift from the original topic. The prompt is shown in Figure~\ref{fig:assitantAgentPrompt}, and cases are shown in Appendix~\ref{sec:scrr}.


\subsection{Conversation-based Data Refinement}
The answers generated from the above procedures face several critical issues: (1) they merely provide superficial responses to user queries without capturing the underlying intent behind the questions, and (2) they fail to address ambiguous or unclear user queries, resulting in answers that do not align with the user's expectations. To mitigate these challenges, leveraging question-and-answer pairs from other turns in the conversation to supplement the current response presents a natural solution. However, this process is inherently complex, as the content from other turns may not always align perfectly with the current question, and irrelevant information should not be incorporated into the refinement. To address this, we propose an iterative answer refinement strategy based on the synthesized conversation. In each iteration of the refinement process, the refiner is prompted to refine the answer based on the conversational context, followed by an assessment phase where the refined content is evaluated and constructive feedback is generated. This feedback is then utilized as input for the subsequent iteration, guiding the refiner to improve the response further.


\textbf{Initialization.} Refinement focuses on enriching the current answer by incorporating relevant information from the conversation's other turns. Specifically, \ourmodel feeds synthesized questions, corresponding answers, and the related conversation context into GPT-4-turbo (Refiner). By designing specific prompts that guide the Refiner to mimic the linguistic style of real user inquiries, we ensure that the generated answers are both comprehensive and stylistically consistent with authentic user interactions. Additionally, to minimize irrelevant interference and prevent potential hallucinations, we retrieve documents closely aligned with the current question and include them in the input.

\textbf{Feedback-based Refinement}. As previous studies~\cite{zheng2024judging,mao2023gpteval} have demonstrated GPT-4-turbo's capability to emulate human evaluation preferences, we employ it as an effective assessor. GPT-4-turbo evaluates responses across five dimensions: relevance, completeness, clarity, accuracy, and actionability, providing an overall score and detailed feedback. This feedback is subsequently utilized as input for the refiner to further refine the response in the next iteration. This multi-faceted assessment allows for targeted refinements, ensuring that each aspect of the response aligns with user expectations and the conversational context. The iterative process continues for a maximum of rounds $T$.


\textbf{Instruction Update and Filtering.} We calculate the overall score $r_0$ of the original answer $a_0$ and put the original answer in the selection pool $\pool$. We also obtain the assessment score $r_t$ for the refined answer $a_t$ in each iteration, where $t\in[1,T]$. We put these answers in $\pool$. We select the best answer with the highest score in the pool that exceeds a predefined quality threshold, i.e., $r = r_{\arg\max_{0 \leq t\leq T}r_t,r_t> \filterthreshold}$. 



\subsection{Retrieval-Augmented Supervised Fine-Tuning}
% \subsection{基于检索增强的监督微调}为了有效捕捉和利用垂直领域的特定知识，我们提出的\ourmodel在监督微调（SFT）框架中集成了检索增强生成（RAG）方法。与传统的仅依赖模型内在知识生成（问题，回答）对的方法不同，\ourmodel利用相关的外部文档来提高回答的准确性和上下文感知能力。
To effectively capture and utilize domain-specific knowledge, our proposed \ourmodel integrates Retrieval-Augmented Generation (RAG) within the Supervised Fine-Tuning (SFT) framework. Unlike traditional SFT approaches that rely solely on (question, answer) pairs generated from the model's inherent knowledge, \ourmodel leverages relevant external documents to enhance answer accuracy and context awareness.

% 动机。 在广告、医疗和金融等垂直领域，用户查询通常需要依赖最新的、领域特定的信息来生成精准且富有上下文的回答。传统的SFT方法在这些场景中表现不佳，因为它们不利用外部知识源，限制了模型根据提供的上下文生成准确和相关回答的能力。
\textbf{Motivation for RAG Integration.} In vertical domains such as advertising, healthcare, and finance, user queries often require precise and contextually rich responses that depend on up-to-date and domain-specific information. Traditional SFT methods fall short in these scenarios as they do not utilize external knowledge sources, limiting the model's ability to generate accurate and relevant answers based on the provided context.

% 方法 我们的方法涉及在指令微调过程中整合检索到的文档。对于每一个训练实例，给定一个问题$\question_i$，我们首先从策划的知识库中检索最相关的文档$\documents_i$。然后，模型基于问题和检索到的文档生成回答$\answer_i$。这形成了一个（问题，文档，回答）的三元组，作为我们微调数据的基础。
\textbf{RAG-Augmenting-SFT.} Our approach involves incorporating retrieved documents into the supervised fine-tuning process. For each training instance, given a question $\question_i$, we first retrieve the most relevant documents $\documents_i$ from a curated knowledge base. The model then generates an answer $\answer_i$ conditioned on both the question and the retrieved documents. This results in a (question, document, answer) triplet that forms the basis of our fine-tuning data.

Formally, the training loss is defined as: \begin{equation} \trainingloss(\Phi) = -\sum_{(\question_i,\documents_i,\answer_i)}\log p(\answer_i|\question_i,\documents_i,\Phi), \end{equation} where $\Phi$ represents the LLM parameters, $\answer_i$ is the answer generated by the LLM, and $p(\cdot)$ is the likelihood of the answer given the question and documents.

% 通过在\ourmodel中嵌入检索增强生成（RAG）方法，我们确保每个生成的回答不仅在上下文上相关，而且基于可靠的外部信息。具体而言，检索到的文档提供了必要的证据，使模型在进行监督微调（SFT）时能够更好地理解经过对话精炼得到的用户问题的回答。这些文档中的信息帮助模型挖掘用户在对话中隐含的需求和兴趣，从而生成更加精准和相关的回答。此外，RAG方法使得模型能够有效地过滤和利用检索到的内容，避免生成与上下文无关或信息不足的回答。这种结合不仅提升了模型在理解和回应复杂用户需求方面的能力，还增强了其在垂直领域中提供高质量、上下文感知的回答的表现。
%\textbf{Integration with Conversation-Based Refinement.} By embedding Retrieval-Augmented Generation (RAG) into \ourmodel, we ensure that each generated answer is not only contextually relevant but also grounded in reliable external information. Specifically, the retrieved documents provide essential evidence that enables the model to understand better and address user questions refined through the conversation-based data refinement process. These documents supply detailed and accurate information that helps the model uncover and respond to the users' hidden needs and interests, which may not be explicitly stated in their initial queries. Additionally, integrating RAG allows the model to effectively filter out irrelevant content and focus on pertinent information, thereby enhancing the precision and relevance of the answers. This synergy between RAG and conversation-based refinement empowers \ourmodel to generate high-quality, context-aware responses that align closely with user intents and the nuanced demands of specialized vertical domains.

