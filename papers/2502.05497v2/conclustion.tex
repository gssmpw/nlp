\section{Conclusion}
In this paper, we propose \ourmodel, a novel framework designed to improve the performance of large language models (LLMs) in domain-specific question-answering tasks. By integrating three key components: data synthesis based on conversations, data refinement based on conversations, and supervised fine-tuning (SFT) enhanced with retrieval, \ourmodel addresses the critical challenge of adapting LLM to understand and meet hidden user needs in vertical domains. Our experiments demonstrate that \ourmodel outperforms GPT-4-turbo+RAG by 7.92\% across the evaluation metrics. 

\section{Limitations}
% 不足1: 只在advertising这个domain下进行了实验->未来：在更多的vertical domain下进行实验，验证方法的有效性。
% 不足2: 评估手段多样性不足，只是利用GPT-4-turbo进行了评估->未来：在更多更强LLM（例如GPT-4-o1，Claude-3.5-sonnet以及DeepSeek-R1等）上进行评估，同时招募avertising上真实的广告用户进行评估等等。
% 不足3：没有进行A/B test。
This study has several limitations. First, the experimental validation was exclusively conducted within the advertising domain, which may constrain the generalizability of our methodology to other vertical domains (e.g., e-commerce, education, or healthcare). Future research should extend the evaluation framework by conducting cross-domain experiments to verify the robustness of our approach. Second, the assessment protocol relied primarily on GPT-4-turbo, DeepSeek-R1, and Llama-3.1-405B for automated evaluation, potentially introducing model-specific biases. Future work can explore(1) implementing human-in-the-loop evaluation with advertising professionals to assess practical utility, and (2) incorporating real-world A/B testing with actual advertisers to measure performance metrics in production environments.