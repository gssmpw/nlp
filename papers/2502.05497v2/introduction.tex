\section{Introduction}
\label{sec:intro}
Large language models (LLMs) such as GPT-4~\cite{OpenAI:2023ktj} have achieved remarkable advancements in question-answering (QA) tasks. Commercial and open-source LLMs are primarily trained on general-domain data and perform less effectively in vertical domains such as healthcare, finance and advertising~\cite{goyal2024healai,xu2024mental,wu2023bloomberggpt}. Supervised Fine-Tuning~\cite{sanh2022multitask,wang2024instruct,Wang2022SelfInstructAL} (SFT) has been widely adopted to optimize the LLM's parameters on a curated set of instructions or task examples to enhance LLMs' ability to answer domain-specific questions. 

Due to the high cost of collecting instruction data, recent SFT methods generate synthetic data. They typically start with a few \textit{seed instructions}, either constructed manually~\cite{ouyang2022training} or generated by LLMs from available documents~\cite{wang2024instruct}. The seed questions are then expanded~\cite{Wang2022SelfInstructAL} or evolved~\cite{Xu2023WizardLMEL} to provide greater complexity and diversity. However, the synthesized instructions deviate from user questions and expected answers.

\begin{figure}[!t]
\centering
\includegraphics[width=1\columnwidth]{figures/intro_case.pdf} % 图片文件名和宽度
\caption{Three phenomena on real-world advertising platforms}
\label{fig:discoverys}
\end{figure}

We now characterize the user demands of domain-specific QA by analyzing an advertising platform. As illustrated in Figure~\ref{fig:discoverys}(a), users usually start with a brief question and ask for more details mentioned in the assistant's responses. Consequently, multi-turn dialogues constitute a substantial portion (Figure~\ref{fig:discoverys}(b)), and most conversations, no matter how many turns, focus on one topic theme  (Figure~\ref{fig:discoverys}(c)). \footnote{The platform currently uses GPT-4-turbo with Retrieval Augmented Generation (RAG) as an intelligent assistant. We prompt GPT-4-turbo to analyze whether the dialogue is closely centered around a single topic theme and assign a topic consistency score. The scores range from 1 to 5, where a score of 3 or below indicates a lower level of thematic consistency, and a score of 5 represents very high thematic consistency.}  

The above observations reveal a critical challenge of QA in vertical domains: Questions are vague and incomplete and do not reflect the user's hidden interests. Because specialized knowledge is required in vertical domains, users do not always possess such expertise, and they are incapable of asking concise and accurate questions. LLMs fine-tuned by conventional instructions (e.g., synthetic questions that mismatch actual user behavior patterns and answers that fall short of user expectations) tend to provide broad responses, which will increase the cost of consultation and harm user experience. The question naturally arises. \textit{
Could we use high-quality synthetic instructions to fine-tune the LLM to capture the user's hidden interests and give a precise and relevant answer?} 

Figure~\ref{fig:discoverys}(a) also demonstrates that authentic user interests are gradually exposed through conversations and satisfying answers are obtained by continuously expanding on a question and delving into technical details. Inspired by this insight, we propose a novel supervised fine-tuning method named \ourmodel. \ourmodel first generates a few seed questions guided by actual user questions. Then, to simulate human conversations, \ourmodel designs a dual-role (i.e., the inquirer and the assistant) framework to generate dialogues on the seed questions. An evaluator assesses each answer based on the dialogue context and provides revision suggestions for a refiner to enhance the quality of the answers. Finally, the questions in each turn of the simulated dialogues and the corresponding refined answers are employed for supervised fine-tuning.

We evaluate the performance of \ourmodel on an online advertising platform. As shown in Figure~\ref{fig:comp}, \ourmodel surpasses commercial LLM, i.e., GPT-4-turbo+RAG and achieves improvements of 3.43\%, 12.09\%, 6.69\%, 3.74\%, and 13.69\% regarding relevance, completeness, clarity, accuracy, and actionability, respectively.

% contribution
% 我们提出了一种基于模拟对话扩展并enhance answer的方法 \ourmodel 来进行命令数据合成，该方法有效捕捉了用户在垂直领域中对问题的真实偏好。我们对真实用户提出的问题中，评估了\ourmodel与在线广告平台助手的回答。结果显示，\ourmodel在相关性、完整性、清晰性、准确性和可操作性五个维度分别提升了3.43%、12.09%、6.69%、3.74%和13.69%。
In summary, the main contribution of \ourmodel is threefold. (1) \ourmodel presents a novel instruction synthesis method by simulating real-world user queries and follow-up conversations for supervised fine-tuning. (2) \ourmodel refines the synthesized answers based on the conversation contexts to ensure that the LLMs can generate more comprehensive answers and address user interests in vertical domains. (3) A large-scale evaluation on an advertising platform of real user questions verifies the effectiveness of \ourmodel.    


\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/abstract.pdf} % 图片文件名和宽度
\caption{Performance comparison of DeepThink and GPT-4-turbo across five evaluation dimensions over different time spans ( "Historic," and "Recent."). DeepThink performs better than GPT-4-turbo in relevance, completeness, clarity, accuracy, and actionability.}
\label{fig:comp}
\end{figure}


