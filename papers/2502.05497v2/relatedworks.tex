\section{Related Works}
% \subsection{Supervised Fine-Tuning}
% % 指令微调
% % 指令微调对LLM具有重要的作用，具体是什么？
% % 或者模仿Magpie的写法，这一段就纯讲作用，以及对应的工作有哪些

% % 指令微调的作用->sft技术分类->特别介绍conversation based prompt,因为我们也在用
% A series of studies find that if adjusted with annotated "instructional" data, LMs can effectively generate responses aligned with human values~\cite{sanh2022multitask, weifinetuned,ouyang2022training}. The performance of Supervised Fine-Tuning depends not only on the quality of the dataset~\cite{Zhou2023LIMALI} but also on various contextual prompting techniques, such as conversation-based prompts~\cite{sreedhar2024canttalkaboutthis, Wei2023ZeroShotIE}, chain-of-thought~\cite{Wei2022ChainOT}, and contextual calibration~\cite{Zhao2021CalibrateBU}.
% % 因为要对齐deepthink，这边强调一下conversation-based prompts
% Specifically, more models now use conversation-based prompts as the default for QA model deployment~\cite{wu2023brief,liu2024chatqa}, because they enhance the user experience by handling follow-up questions, providing clarifications, and reducing hallucinations.

% 数据合成->分为人工标注和LLM自己生成->人工标注成本高，LLM自己生成会有一些幻觉sample->我们在真实的QA下用rag来避免幻觉并且使用refiner来保证前后topic一致性以及保证数据真实性。（保证数据真实性是因为refiner前后能看到的rag的信息更广，引入更多事实数据）
\subsection{Instruction Data Synthesis}
To address the issue of limited training samples in specific domains, various works have proposed using additional data, such as manual annotation~\cite{Zhao2024WildChat1C,zheng2023lmsys} and automatic generation by LLMs~\cite{Mekala2022LeveragingQD, Wang2021TowardsZL, Wang2022SelfInstructAL, Xu2023WizardLMEL}. However, manual annotation is expensive~\cite{honovich-etal-2023-unnatural}, and iterative generation by LLMs frequently introduces the risk of hallucinations.


Our work falls into the category of automatic generation by LLMs. However, our work differs from previous approaches in two main aspects. (1) We synthesize instructions by simulating conversations closer to real-world scenarios. (2) We adopt several techniques to improve the quality of synthesized instruction. We integrate Retrieval-Augmented Generation (RAG) to mitigate hallucination in conversation-based synthesis. We apply a Conversation-based Data Refiner for filtering, ensuring topic consistency and data authenticity.
% RAG的作用->早期关注于检索器本身->现在专注于when and how ->分别举两个例子验证when and how -> 我们是在sft阶段使用rag的
\subsection{Retrieval-Augmented Generation}
Retrieval augmentation has become a standard solution to address hallucinations in LLMs by introducing external knowledge to compensate for factual shortcomings~\cite{Asai2023SelfRAGLT,ma2023query, Izacard2021UnsupervisedDI, Ram2023InContextRL}.
Early Retrieval Augmentation efforts focus primarily on the retriever itself, where both the neural retriever and generator are typically trainable Pretrained Language Models (PrLMs), such as BERT ~\cite{Devlin2019BERTPO} or BART ~\cite{Lewis2019BARTDS}. In contrast, modern Retrieval Augmentation applied to LLMs emphasizes determining when and how to retrieve relevant information~\cite{fatehkia2024t, Asai2023SelfRAGLT, Xu2024LargeLM}. For example, Self-RAG enables on-demand retrieval and generates more accurate, fact-based text through fine-grained self-reflection~\cite{Asai2023SelfRAGLT}. 
% mHyER bridges the semantic gap between learner input and practice content by generating hypothetical exercises related to the learner's input, 
% thereby improving retrieval relevance~\cite{Xu2024LargeLM}. 

Our approach uses RAG throughout the data synthesis, SFT, and inference stages. This not only improves the authenticity of the synthesized data but also helps the LLM learn how to effectively utilize the retrieved knowledge during the SFT stage. In contrast, previous research only used RAG during the inference stage, relying heavily on the LLM's ability to discern the retrieved knowledge. This can lead to insufficient utilization of relevant knowledge, especially when dealing with domain knowledge that was not included in the pretraining process.

