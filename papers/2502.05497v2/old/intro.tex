\section{Introduction}
\label{sec:intro}
% 框架
% 一句话介绍LLM：近期，随着大型语言模型（LLMs）取得显著进展，以ChatGPT~\cite{}和GPT-4~\cite{}为例，它们能够理解并回答各种问题，极大便利了人们的日常生活。~\cite{}，但是这些模型只在general domain进行学习，在垂直领域，例如医疗，金融，广告上表现不佳，所以最近很多研究工作关注于如何提升LLM在垂直领域的表现~\cite{}.
Recently, large language models (LLMs), e.g., ChatGPT~\cite{gpt2020} and GPT-4~\cite{OpenAI:2023ktj} have achieved remarkable advancements across a wide range of tasks and applications. Despite their success, these models are primarily trained on general-domain data, which limits their effectiveness in specialized domains such as healthcare, finance, and advertising. This limitation has prompted a growing body of research focused on enhancing the performance of LLMs in these vertical domains~\cite{moore2023empowering,harris2023large,llmRec,huang2023dsqa}.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/intro_case.png} % 图片文件名和宽度
\caption{Three phenomena on real-world advertising platforms}
\label{fig:discoverys}
\end{figure}

% RAG 是一种在垂直领域中非常成功的方法，能够有效解决因模型缺乏垂直领域知识而无法准确回答专业问题的挑战。一句话RAG的介绍。RAG的应用~\cite{}, Microsoft 365 Copilot~\cite{},Microsoft Ads Copilot..., 但是这类工作存在什么问题？GPT-4在vertical domain的指令遵循不行，SFT干嘛的，最近的研究表明ft + rag vs rag？raft，对LLM在特定领域SFT后再进行RAG要比直接在LLM进行RAG的表现更好。
% Retrieval-Augmented Generation (RAG) is a highly effective method in vertical domains, addressing the challenge of models lacking domain-specific knowledge by integrating external information to accurately answer specialized questions. Applications of RAG, such as mHyER~\cite{xu2024large}, Telco-RAG~\cite{bornea2024telco}, and Microsoft Copilot~\cite{kyto2024copilot}, demonstrate its effectiveness, but they do not address LLMs' instruction-following limitations in vertical domains. Supervised Fine-Tuning (SFT) tackles this by retraining the model on domain-specific labeled data, improving its ability to follow specialized instructions. Recent studies also show that applying RAG after performing Supervised Fine-Tuning (SFT) on LLMs in specific domains yields better results than directly using RAG on the LLM.~\cite{zhang2024raft,balaguer2024rag}


% RAG（检索增强生成）与SFT（监督式微调）是提升LLM性能的两种有效策略。RAG通过检索与问题相关的知识片段，增强了LLM的回答能力，使其能够整合最新或特定信息，以生成更精确的答案。而SFT则是在特定领域的指令和数据上进行精细调整，使LLM更契合该领域的专业知识与语言风格。然而，这两种方法并非完美无缺。在LLMs，尤其是参数较少的模型中，仅依赖上下文学习的RAG难以有效识别和利用有用的知识片段，可能导致回答的准确性下降。另一方面，通过SFT训练LLM时，指令数据的合成成为了一个关键瓶颈。先前的研究通过self-instruct或evol-instruct生成的问题，可能与实际用户在特定领域中的提问存在较大差异，例如难以生成涉及专有名词的问题。此外，生成的回答可能无法完全捕捉用户的兴趣点，特别是在细分领域，用户关注的细节可能更为精细，而未经专门优化的LLM生成的回答可能过于泛泛或不够精确，从而与用户的需求产生偏差。
%SFT effective strategy for domain-specific tuning. SFT requires task aligned instruction data QA. due to data availability/cost, synthesis~\cite{}. synthesis 方法，从种子Q->expand Q, GPT A（收集来的真实或者根据已有文档提示GPT合成的）开始（1）扩展，（2） 或者进化Q更复杂更多样性,GPT A。 依赖于初始种子本身模式和GPT自己的知识容量

RAG (Retrieval Augmented Generation) and SFT (Supervised Fine-Tuning) are two effective strategies for enhancing the performance of LLMs~\cite{}. RAG strengthens the LLM’s response capabilities by retrieving knowledge fragments related to the question, enabling it to integrate the latest or specific information to generate more accurate answers~\cite{}. SFT, on the other hand, involves fine-tuning on instructions and data within specific domains, allowing LLMs to better align with the professional knowledge and linguistic style of that domain~\cite{}. However, neither of these methods is without its flaws. In LLMs, RAG’s reliance on context learning alone can struggle to effectively identify and utilize useful knowledge fragments~\cite{Asai2023SelfRAGLT,zhang2024raft}, potentially leading to less accurate answers. Moreover, when training LLMs through SFT, the synthesis of instruction data becomes a critical bottleneck. Previous research using self-instruct~\cite{Wang2022SelfInstructAL} or evol-instruct~\cite{Xu2023WizardLMEL} to generate questions may differ significantly from actual user inquiries in specific domains, such as the inability to generate questions involving proper nouns. Additionally, the generated responses may fail to fully capture the user’s points of interest, particularly in niche areas where users may focus on more nuanced details. Without specialized optimization, the responses generated by LLMs, e.g., GPT-4 might be too general or lack precision, resulting in a deviation from user expectations.



%常见的SFT（Supervised Fine-Tuning）数据合成方法包括self-instruct和evol-instruct，这些方法通过seed question提示LLM（例如GPT-3.5）生成指令数据。然而，这些合成策略可能存在一些局限性。首先，由于LLM在垂直领域（如广告领域）的训练数据较为有限，所合成的问题可能与实际用户在该领域中的提问有较大差异，例如无法合成一些专有名词相关的问题等，其次，生成的回答往往不能完全捕捉用户的兴趣点和关切。尤其在细分领域，用户关注的重点可能更为细腻，而LLM在未专门优化的情况下，生成的内容可能过于笼统或不够精准，导致与用户预期的需求存在偏差。
% Common methods for Supervised Fine-Tuning (SFT) data synthesis include self-instruct and evol-instruct~\cite{Xu2023WizardLMEL, Wang2022SelfInstructAL}, where instruction data is generated by prompting LLMs (e.g., GPT-3.5) with seed questions. Unfortunately, these synthesis strategies may have some limitations. First, due to the limited training data of LLMs in vertical domains (such as the advertising domain), the synthesized questions may significantly differ from the actual queries users pose in these fields. Second, the generated responses often fail to capture users' points of interest and concerns fully. In specialized domains, users' focus may be more nuanced, and without specific optimization, the content generated by the LLM may be too general or imprecise, leading to a gap between the output and users' expected needs.


%以一个垂直领域的数据分析，（1）用户喜欢多轮提问（2）用户多轮提问针对同一个话题。这说明用户提问的不好，且不能从GPT中获得满意的答案。因此SFT必须让LLM能够去理解用户的兴趣，能够回答（预知能力？）。但是目前的SFT 生成数据方式是做不到的。因为在垂直领域，GPT Q和用户的Q有偏差（包括风格、内容、兴趣点），A 质量很难保证 

%我们进一步分析了真实广告平台上用户与广告助手的对话数据，发现了三个有趣的现象：1）对话往往不局限于单轮交互；2）对话主题始终围绕一个核心话题展开；3）用户经常基于助手之前回答中的具体细节进行进一步追问。这些现象引发了我们思考：现有助手的回答为何总是无法一次性解决用户的所有疑问？是不是因为助手的回答不够全面，无法覆盖用户潜在的关注点，才导致用户反复追问？如果助手能够更好地预判用户的需求，提前深入思考用户可能会进一步关注的内容，并在回答中进行更全面的回应，是否能够显著提升用户的满意度？换句话说，我们是否可以利用对话历史和上下文来增强助手的回答，使其更加贴合用户的实际需求？
Our analysis of user interactions with ad assistants on real-world advertising platforms has revealed three interesting phenomena: (1) \textbf{P1: conversations often extend beyond a single round of interaction}. We recorded the number of conversation turns between users and the advertising assistant over the course of one day. It is shown in Figure ~\ref{fig:discoverys}(a). Although the number of single-turn dialogues has reached the scale of thousands, the number of multi-turn dialogues has also reached the scale of hundreds. Multi-turn dialogues should not be overlooked; (2) \textbf{P2: conversation tends to revolve consistently around a central topic.} We prompt GPT-4-turbo to analyze whether the dialogue is closely centered around a single theme and to assign a consistency score. A score of 5 indicates a very high degree of thematic consistency, while a score of 1 indicates a very low degree of thematic consistency. As shown in Figure~\ref{fig:discoverys}(b), the thematic consistency of multi-turn dialogues is notably high. When the number of dialogue turns is 2, the proportion of dialogues receiving a score of 5 is 96\%. Even in cases where the number of dialogue turns is greater than or equal to 5, the proportion of dialogues scoring 5 remains at 39\%; (3) \textbf{P3: users frequently follow up on specific details mentioned in the assistant's previous responses}. Figure ~\ref{fig:discoverys}(c) illustrates a real conversation. Approximately 78.49\% of the dialogues exhibit this condition. These observations raise the following thinking: 

\textit{
Could we potentially use the conversation to refine instructions that better capture user interests, given that the assistant’s initial responses may not fully address their underlying concerns?}


%基于这些观察和思考，我们提出了一种基于对话协同形式的指令数据合成framework，named deepthink. It consists of three key components. (1)Conversation-based Data Synthesis. 它利用了双agent对话的形式来模拟垂直领域下用户真实对话场景, 为保证合成的问题风格与现实世界用户提问风格一致，我们采样少量用户真实的提示“用户agent”合成类似风格的问题。 为保证合成的答案的真实性与准确性，我们通过RAG的形式提示“助手agent”进行回答。 (2)Conversation-based Data Refiner。提出了基于conversation上下文对合成数据进行refine 来保证answer更加全面，更加符合垂直领域的用户关注的内容。此外，通过利用了一个多维度的评分器对answer进行打分并给出修改意见，该修改意见同步送入refiner中，模拟一个垂直领域专家对Refiner进行指导来进一步提升answer的质量。(3)Retrival-augment SFT。 在SFT（Supervised Fine-Tuning）阶段引入与问题最相关的文档，通过检索增强的SFT，使模型的训练目标与RAG（Retrieval-Augmented Generation）对齐，从而更好地让模型学会如何有效利用文档进行回答问题。

%motivation: 从人类的conversation获得启发：人类通过对话对一个Q不断进行related扩展。 expand发散，对话更深入，更细节。围绕focused。A能够比较全面。

%因此，工作（1）conversation QA synthesis framework (2) conversation answer refinement (3) RAG SFT

Based on this insight, we propose a dialogue-collaborative framework for command data synthesis, named \ourmodel. It comprises three key components:

(1) Conversation-based Data Synthesis: This component employs a dual-agent dialogue format to simulate real user dialogue scenarios in vertical domains. To ensure that the synthesized questions align with the prompt styles of real-world users, we sample a limited number of genuine user questions to prompt the “user agent” to generate questions in a similar style. To guarantee the authenticity and accuracy of the synthesized answers, we prompt the “assistant agent” to respond using a Retrieval-Augmented Generation (RAG) approach.

(2) Conversation-based Data Refiner: We propose refining the synthesized data based on the dialogue context to ensure that the answers are more comprehensive and align with the content of interest for users in vertical domains. Additionally, by utilizing a multi-dimensional scorer to evaluate the answers and provide revision suggestions, these suggestions are fed back into the refiner, simulating guidance from a domain expert to further enhance the quality of the answers.

(3) Retrieval-augmented SFT: During the Supervised Fine-Tuning (SFT) phase, we introduce the most relevant documents related to the questions. This retrieval-augmented SFT aligns the training objectives of the model with those of the Retrieval-Augmented Generation (RAG), thus enabling the model to learn more effectively how to utilize documents in answering questions.

% contribution
% 我们提出了一种基于模拟对话扩展并enhance answer的方法 \ourmodel 来进行命令数据合成，该方法有效捕捉了用户在垂直领域中对问题的真实偏好。我们对真实用户提出的问题中，评估了\ourmodel与在线广告平台助手的回答。结果显示，\ourmodel在相关性、完整性、清晰性、准确性和可操作性五个维度分别提升了3.43%、12.09%、6.69%、3.74%和13.69%。
To summarize, the main contribution of this work is three-fold.
\begin{itemize}
    \item We present a conversation-based approach \ourmodel to command data synthesis that effectively captures the genuine preferences of users regarding questions in vertical domains.
    \item We propose a refined strategy that utilizes dialogues to enhance the quality of  responses, ensuring that the answers are more comprehensive and address content of potential interest to users.
    \item We evaluated the responses of \ourmodel and the assistant of an online advertising platform(GPT-4+RAG) against real user questions. The results indicate that \ourmodel achieved improvements of 3.43\%, 12.09\%, 6.69\%, 3.74\%, and 13.69\% in the dimensions of relevance, completeness, clarity, accuracy, and actionability, respectively.
   
\end{itemize}


