\input{figures/results_plot.tex}

\section{Experiments}

\subsection{Dataset}

To train the model, we create a diverse dataset of aerial imagery from various locations across North America. Figure \ref{fig:usa-plot} in Appendix \ref{appendix:dataset} shows the exact locations where the data is collected. The dataset consists of 121 high-resolution stitched orthomosacis of large-scale solar farms captured using a fixed-wing aircraft with an RGB camera at approximately 2.5cm ground sample distance (GSD).

\subsection{Dataset Preprocessing}
We randomly split the 121 orthomosaics into train, validation, and test sets using an approximate split ratio of 80/10/10. We then preprocess each orthomosaic into 512x512 patches. Annotating large scale orthomosaics is time consuming and expensive. Additionally, since visual features of solar panels do not vary in comparison to natural images, we find it to be redundant to fully annotate and train on all patches from each site. Consequently, we randomly sample 10 unique foreground patches containing solar panels from each orthomosaic for annotating. We additionally sample 5 unique background patches from each orthomosaic to increase model precision on images of solar arrays with substantial portions of background information. We find that with proper augmentation, described in Appendix ~\ref{section:augmentation}, we achieve better results with a sampled dataset from a larger set of orthomosaics than we do with a smaller set of fully annotated orthomosaics. Finally, to test the efficacy of the proposed solar panel mapping framework, we create complete georeferenced maps of the 10 test orthomosaics containing the geospatial coordinates of the vertices of every solar panel. The spread of all three of our datasets is depicted in Table \ref{tab:dataset} in Appendix \ref{appendix:dataset}.

\subsection{Metrics}
To evaluate our model, we report mean average precision (mAP) and mean average recall (mAR) as our primary performance metrics. We also report mAP and mAR with an IoU threshold of 0.75, notated as $\text{mAP}_{75}$ and $\text{mAR}_{75}$, respectively. This is because we are interested in the performance of the model at high IoU thresholds as a proxy for evaluating how tight the localizations are.

\input{tables/results.tex}

\subsection{Results}
We fine-tune the model using several different combinations of ResNet backbone networks, all of which are pretrained on the ImageNet dataset \citep{imagenet_cvpr09}:

\begin{itemize}
    \item C4: A conv4 backbone with a conv5 head as described in the original Faster R-CNN paper \citep{FasterRCNN}.
    \item DC5: A conv5 backbone with dilations in the conv5 layer and fully connected layers in the box regression head \citep{DC}. 
    \item FPN: A feature pyramid network backbone with fully connected layers in the box regression head \citep{FPN}.
\end{itemize}

We run an ablation study to investigate the effects of the choice of backbone on model performance. We donâ€™t run any hyperparameter tuning to keep things fair and we use the same hyperparameters for every model, with the exception that we reduce the batch size for deformable convnet backbones which require more memory during processing. The ResNet-50-FPN network comfortably achieves the best results on the validation set and comparable results on the test set.

\subsection{Discussion}
The growth in cumulative solar capacity has made it difficult to scale up the inspection and maintenance of these valuable assets. In 2022, we estimate that 3.5 GW of potential solar power was lost due to faulty solar panels. By automating the initial mapping phase of the inspection process, we estimate that analysts will have a 43\% increase in efficiency. This increase in analyst efficiency decreases the time that faulty solar panels are losing potential power. Finally, we estimate that our increase in inspection efficiency will lead to a potential retention of 1.4 GW of solar power.

%