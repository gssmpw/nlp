\section{Related Work}
We summarize some related works in this section, and compare these prior works with the contributions in this article.


\subsection{Pre-training and Transfer Learning Theory}
There has been significant recent progress in understanding the benefits of distinct unsupervised pre-training methods. In \citet{lee2021predicting}, the authors provide rigorous evidence of the benefits of self-supervised pre-training (SSL). They explain the benefits of SSL via specific conditional independence relations between two sets of observed features, given the response. In a related direction, \citet{arora2019theoretical,tosh2021contrastive,tosh2021contrastive2} examine the benefits of contrastive pretraining, while \citet{zhai2023understanding} examines the effects of augmentation-based self-supervised representation learning. In \citet{wei2021pretrained}, the authors explore the benefits pre-trained language models, while  \citet{zhang2021inductive}, explores the inductive bias of masked language modeling by connecting to the statistical literature on learning graphical models. Finally, we highlight the work \cite{azar2024semi}, which  exhibits provable computational benefits of semi-supervised learning under the low-degree likelihood hardness conjecture.


The paucity of high-quality labeled data has directly motivated inquiries into the properties of transfer learning across diverse application domains. The recent literature focuses on several distinct notions of transfer learning (e.g., covariate shift \cite{heckman1979sample,huang2006correcting,shimodaira2000improving}, model shift \cite{wang2015generalization,wang2014active}, target shift \cite{maity2022minimax}, conditional shift  \cite{quinonero2022dataset,storkey2008training} etc) and develops distinct rigorous methods to ensure successful knowledge transfer in these settings (see \cite{shimodaira2000improving,wu2019domain,sagawa2019distributionally,ganin2016domain,long2017deep} and the references therein for an incomplete list). From a learning theoretic perspective, recent works study the generalization performance as a function of the discrepancy between the source and the target domains \cite{albuquerque2019generalizing,ben2010theory,david2010impossibility,hanneke2019value,tachet2020domain,zhao2019learning}.


In \cite{damian2022neural}, the authors study the benefits of transfer learning in the setting of single/multi-index models. They keep the representation fixed across the source and target, and vary the link function across the two tasks. In contrast, we keep the link function constant (and assume that the link is known), and study settings with distinct (but correlated) representations in the source and target tasks. 


\subsection{Understanding Sample Complexity for single-index models}

 Single-index models have emerged as popular toy-models for understanding the sample complexity of training of neural networks. This is due to the fact that they are both high-dimensional and non-convex. From a statistical perspective there has been work on the fundamental thresholds of inference in these problems \cite{barbier2019optimal,maillard2020phase} and its landscape geometry \cite{sun2018geometric,maillard2019landscape,dudeja2018learning}. From the perspective of sample complexity, a substantial amount of deep work in this direction focused on the sample complexity  of spectral methods or related algorithms, particularly in relation to the Phase Retrieval problem, \cite{candes2015phase,barbier2019optimal, lu2020phase}.  


More recently there have been tight analyses of the sample complexity for online stochastic gradient descent from random initialization.
In particular, it was shown in \cite{arous2021online} that the sample complexity in the online setting is characterized by the Information Exponent. Since then there has been a tremendous body of work around complexity exponents, such as the Information Exponent, Leap Exponent \cite{abbe2023sgd}, or Generative Exponent \cite{damian2024computational}. In particular, these exponents have enabled studies which contrast the performance of various learning paradigms such as Correlational Statistical Query (CSQ) versus Statistical Query (SQ) bounds \cite{damian2024computational}, feature learning versus kernel methods \cite{ba2024learning}, better choices of loss function \cite{damian2024smoothing}, and the importance of data reuse \cite{dandi2024benefits,lee2024neural}.  We note here that there has been quite a lot of recent important work on the case of multi-index models which we do not explore here, see, e.g., \cite{abbe2023sgd,bietti2023learning,ren2024learning} for a small selection of this rich literature.  


To our knowledge, most of this work has focused largely on the setting of isotropic Gaussian features (though note \cite{zweig2024single} for work on universality). However, given that pre-training only has access to the features, one requires that the features have some correlation with the underlying spike. Inspired by the recent works of \cite{mousavi2023gradient, ba2024learning}, we model this via a spiked covariance model.