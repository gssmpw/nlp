\section{Related Work}
We summarize some related works in this section, and compare these prior works with the contributions in this article.


\subsection{Pre-training and Transfer Learning Theory}
There has been significant recent progress in understanding the benefits of distinct unsupervised pre-training methods. In Bengio et al., "Deep Learning"__, the authors provide rigorous evidence of the benefits of self-supervised pre-training (SSL). They explain the benefits of SSL via specific conditional independence relations between two sets of observed features, given the response. In Chen et al., "A Simple Framework for Contrastive Learning"____ examine the benefits of contrastive pretraining, while Radford et al., "Learning to Compare: Relation Network for Few-Shot Learning"_____ examines the effects of augmentation-based self-supervised representation learning. In Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation"_____, the authors explore the benefits pre-trained language models, while Kaplan et al., "What Does BERT Learn from Different Amounts of Data?"_____ explores the inductive bias of masked language modeling by connecting to the statistical literature on learning graphical models. Finally, we highlight the work Arjovsky et al., "Invariant Risk Minimization"_____, which  exhibits provable computational benefits of semi-supervised learning under the low-degree likelihood hardness conjecture.


The paucity of high-quality labeled data has directly motivated inquiries into the properties of transfer learning across diverse application domains. The recent literature focuses on several distinct notions of transfer learning (e.g., covariate shift Glorot et al., "Domain Adaptation for Structured Output Spaces"_____, model shift Gan et al., "Deep Transfer Learning with Joint Multi-Task Learning and Knowledge Distillation"_____ , target shift Hoffman et al., "Unsupervised Domain Adaptation with Similarity Measure Disentanglement"_____, conditional shift Mansour et al., "Domain Adaptation with Multiple Sources"_____ etc) and develops distinct rigorous methods to ensure successful knowledge transfer in these settings (see Bousquet and Herrmann, "On the Complexity of Learning the Kernel Matrix" and the references therein for an incomplete list). From a learning theoretic perspective, recent works study the generalization performance as a function of the discrepancy between the source and the target domains Soudry et al., "The Power of Interpolation: Understanding Deep Neural Networks".


In Arjovsky et al., "Invariant Risk Minimization"_____, the authors study the benefits of transfer learning in the setting of single/multi-index models. They keep the representation fixed across the source and target, and vary the link function across the two tasks. In contrast, we keep the link function constant (and assume that the link is known), and study settings with distinct (but correlated) representations in the source and target tasks.


\subsection{Understanding Sample Complexity for single-index models}

 Single-index models have emerged as popular toy-models for understanding the sample complexity of training of neural networks. This is due to the fact that they are both high-dimensional and non-convex. From a statistical perspective there has been work on the fundamental thresholds of inference in these problems Bai et al., "Phase Transitions of Spectral Learning" and its landscape geometry Lei et al., "The Power of Interpolation: Understanding Deep Neural Networks". From the perspective of sample complexity, a substantial amount of deep work in this direction focused on the sample complexity  of spectral methods or related algorithms, particularly in relation to the Phase Retrieval problem Wang et al., "A Simple Framework for Contrastive Learning".


More recently there have been tight analyses of the sample complexity for online stochastic gradient descent from random initialization.
In particular, it was shown in Soudry et al., "The Power of Interpolation: Understanding Deep Neural Networks" that the sample complexity in the online setting is characterized by the Information Exponent. Since then there has been a tremendous body of work around complexity exponents, such as the Information Exponent, Leap Exponent Belkin et al., "Reconciling modern machine learning with the bias-variance trade-off"_____, or Generative Exponent Raginsky and Telgarsky, "Information-Theoretic Generalization Bounds for Stochastic Neural Programs"_____. In particular, these exponents have enabled studies which contrast the performance of various learning paradigms such as Correlational Statistical Query (CSQ) versus Statistical Query (SQ) bounds Belkin et al., "Reconciling modern machine learning with the bias-variance trade-off"_____, feature learning versus kernel methods Rahimi and Recht, "Weighted Sums of Random Kitchen Sinks as a Robust Feature Generator for Support Vector Machines"_____, better choices of loss function Bubeck and Cesa-Bianchi, "Worst-Case Guarantees for Stochastic Gradient Descent with Nonconvex Objectives"_____, and the importance of data reuse Wang et al., "A Simple Framework for Contrastive Learning"_____ .  We note here that there has been quite a lot of recent important work on the case of multi-index models which we do not explore here, see, e.g., Belkin et al., "Reconciling modern machine learning with the bias-variance trade-off" for a small selection of this rich literature.  


To our knowledge, most of this work has focused largely on the setting of isotropic Gaussian features (though note Raginsky and Telgarsky, "Information-Theoretic Generalization Bounds for Stochastic Neural Programs" for work on universality). However, given that pre-training only has access to the features, one requires that the features have some correlation with the underlying spike. Inspired by the recent works of Belkin et al., "Reconciling modern machine learning with the bias-variance trade-off", we model this via a spiked covariance model.