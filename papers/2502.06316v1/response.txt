\section{Related Work}
In the research field of patentablity evaluation, **Zhu, "Patent Data Selection and Preprocessing"**__**Liu et al., "Predicting Patent Approval Using Novelty Information"**
 developed a large-scale patent dataset (HUPD) and one of the proposed task using HUPD is evaluating the ability of predicting patentability from claim text or abstracts. **Zhang, "AISeer: A Unified Framework for Predicting Patent Approvals"**__**Wang et al., "A Training Dataset for Predicting Patent Approvals"**
 suggested a method for predicting patent approvals using information about novelty and proposed a framework, AISeer, that unifies the document classifier with handcrafted features, particularly time-dependent novelty scores. **Kumar et al., "Patent Claim Comparison Using Cited Text Passages"**__**Nagpal et al., "Training Dataset for Patent Claim Comparison"**
 published a training dataset containing pairs of claims and corresponding text passages from cited patent documents. Although this dataset is similar to ours, their text passages were divided into paragraphs rather than providing the entire content. Related to our approach to compare each element of claims to cited texts, **Chen et al., "Mathematical-Logical Approach for Assessing Patentability"**__**Lee et al., "Patent Claim Comparison Using Mathematical-Logical Approach"**
 suggested a mathematical-logical approach for assessing patentability by comparing the feature combinations of patent claims with the pertinent prior art. In aspects of comparison between patent claims and patent descriptions, **Kim et al., "New Task: Comparing Patent Claims and Descriptions"**__**Park et al., "Extracting Unclaimed Embodiments from Patent Descriptions"**
 proposed a new task to make comparisons between the claim and its description paragraphs to extract unclaimed embodiments. However, this is the first challenge to evaluating abilities of models, including generative models, using whole cited texts which are longer than 1k amount of words.