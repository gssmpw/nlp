\section{Related Work}
In the research field of patentablity evaluation, ~\citet{suzgun2024harvard} developed a large-scale patent dataset (HUPD) and one of the proposed task using HUPD is evaluating the ability of predicting patentability from claim text or abstracts. ~\citet{gao2022towards} suggested a method for predicting patent approvals using information about novelty and proposed a framework, AISeer, that unifies the document classifier with handcrafted features, particularly time-dependent novelty scores. ~\citet{risch2020patentmatch} published a training dataset containing pairs of claims and corresponding text passages from cited patent documents. Although this dataset is similar to ours, their text passages were divided into paragraphs rather than providing the entire content. Related to our approach to compare each element of claims to cited texts, ~\citet{schmitt2023assessment} suggested a mathematical-logical approach for assessing patentability by comparing the feature combinations of patent claims with the pertinent prior art. In aspects of comparison between patent claims and patent descriptions, ~\citet{hashimoto2023hunt} proposed a new task to make comparisons between the claim and its description paragraphs to extract unclaimed embodiments. However, this is the first challenge to evaluating abilities of models, including generative models, using whole cited texts which are longer than 1k amount of words.