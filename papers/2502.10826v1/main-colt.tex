\PassOptionsToPackage{dvipsnames}{xcolor}
%
%
\documentclass[final,12pt]{colt2025-arxiv} %

\input{defs-kjun-v5-overleaf-colt}

%
%

%
\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{booktabs} %

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
%
\newcommand{\qed}{\hfill\ensuremath{\jmlrQED}}
\newcommand{\qedhere}{\ifmmode\hfill\ensuremath{\jmlrQED}\else\unskip\nobreak\hfill\ensuremath{\jmlrQED}\fi}




%
%
\let\set\undefined
\usepackage{commath}

\newenvironment{revised}
{\colorlet{kjsaved}{.}\color{MidnightBlue}}%
{\color{kjsaved}}%

\renewcommand{\cite}{\citep}
%

\usepackage{enumitem}
\setlist{nolistsep} %
\setlist{itemsep=.0em} %
\setlist[itemize]{topsep=.5pt,itemsep=0pt,parsep=2pt}
\setlist[enumerate]{topsep=.5pt,itemsep=0pt,parsep=2pt}


%
%
%


%
\newif\ifFINAL
%
\FINALtrue %
%

\ifFINAL
  \def\blue#1{#1}
  \def\guide#1{}
  \def\gray#1{}
%
%
%
\else
  \usepackage{transparent}
  \usepackage[inline]{showlabels}
  \usepackage{rotating}
  \renewcommand{\showlabelsetlabel}[1]
  {{\raisebox{0.3ex}{\begin{turn}{3}\showlabelfont #1\end{turn}}
        }}
  \renewcommand{\showlabelfont}{\transparent{0.8}\scriptsize\bf\slshape\color{Lavender}}
  
\fi

%
%
%
%
%
%

\def\bias{\ensuremath{bias}}
\def\tilsfU{{\tilde{\mathsf{U}}}}
\def\tilsfW{{\tilde{\mathsf{W}}}}

%
\newcommand{\defeq}{\triangleq}
\newcommand{\E}{\EE}
\newcommand{\ie}{i.e., }
\newcommand{\wealth}{\mathsf{W}}
\newcommand{\wealthcrp}[1]{\mathsf{W}^{\mathsf{CRP}(#1)}}
\newcommand{\wealthup}{\mathsf{W}^{\mathsf{UP}}}
\newcommand{\wealthpcrp}{\mathsf{W}^{\mathsf{pCRP}^\star}}
\newcommand{\wealthsup}{\mathsf{W}^{\mathsf{sup}}}

\newcommand{\piref}{\pi_{\mathsf{ref}}}
\newcommand{\ips}{\tilr}

\newcommand{\mean}{v}
\newcommand{\var}{\mathsf{V}}
\usepackage{ifthen}
\newcommand{\lcbup}[2]{%
  \ifthenelse{\equal{#2}{}}%
    {\hat{\mean}_{\mathsf{UP}}}%
    {\hat{\mean}_{\mathsf{UP}}^{(#2)}}%
}
\newcommand{\lcbpcrp}[2]{%
  \ifthenelse{\equal{#2}{}}%
    {\hat{\mean}_{\mathsf{pCRP}^\star}}%
    {\hat{\mean}_{\mathsf{pCRP}^\star}^{(#2)}}%
}

\newcommand{\pcrplcb}{pCRP$^\star$-LCB}

\newcommand{\selectup}{\blue{{\hpi}_{\mathsf{UP}}^{(\dt)}}}
\newcommand{\selectupsimple}{\blue{{\hpi}_{\mathsf{UP}}}}

\newcommand{\selectpcrp}{\blue{{\hpi}_{\mathsf{pCRP}^\star}^{(\dt)}}}
\newcommand{\selectpcrpsimple}{\blue{{\hpi}_{\mathsf{pCRP}^\star}}}

\newcommand{\lcbeb}[2]{\hat{\mean}_{\mathsf{EB}}^{(#2)}}

\def\dtp{{\dt'}}
\def\dtpp{{\dt''}}
\def\barb{{\overline b}}

\newcommand{\Gc}{\cG}

\newcommand{\up}{\mathsf{UP}}

\raggedbottom

\usepackage{scrwfile}
\TOCclone[\appendixname]{toc}{atoc}
\newcommand\StartAppendixEntries{}
\AfterTOCHead[toc]{%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=-10000\relax}%
}
\AfterTOCHead[atoc]{%
  \edef\maintocdepth{\the\value{tocdepth}}%
  \value{tocdepth}=-10000\relax%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=\maintocdepth\relax}%
}

%
\renewcommand*{\bf}{\textbf}
%
%
%
%
%

%
\usepackage{xparse}
\RenewDocumentEnvironment{proof}{ o }
  {%
    \par\noindent
    {\bfseries\upshape
      \IfNoValueTF{#1}
         {\proofname\ }
         {Proof of~#1\ }%
    }%
  }
  {%
    \jmlrQED
  }



%
%
%
\newtheorem{assumption}[theorem]{Assumption}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}


%
\title[Improved Offline Contextual Bandits with Second-Order Bounds]
{Improved Offline Contextual Bandits with Second-Order Bounds:\\ Betting and Freezing}
%
\usepackage{times}
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%

%
\coltauthor{%
 \Name{J. Jon Ryu} \Email{jongha@mit.edu}\\
 \addr Massachusetts Institute of Technology
 \AND
 \Name{Jeongyeol Kwon} \Email{jeongyeol.kwon@wisc.edu}\\
 \addr University of Wisconsin-Madison%
 \AND
 \Name{Benjamin Koppe} \Email{bek76@cornell.edu}\\
 \addr Cornell University%
 \AND
 \Name{Kwang-Sung Jun} \Email{kjun@cs.arizona.edu}\\
 \addr University of Arizona%
 \AND%
}


\begin{document}

\maketitle

\begin{abstract}%
We consider the off-policy selection and learning in contextual bandits where the learner aims to select or train a reward-maximizing policy using data collected by a fixed behavior policy. Our contribution is two-fold. First, we propose a novel off-policy selection method that leverages a new betting-based confidence bound applied to an inverse propensity weight sequence. Our theoretical analysis reveals that our method achieves a significantly better, variance-adaptive guarantee upon prior art. Second, we propose a novel and generic condition on the optimization objective for off-policy learning that strikes a difference balance in bias and variance.
One special case that we call freezing tends to induce small variance, which is preferred in small-data regimes.
Our analysis shows that they match the best existing guarantee. 
In our empirical study, our selection method outperforms existing methods, and freezing exhibits improved performance in small-sample regimes.
\end{abstract}

\begin{keywords}%
offline contextual bandits; confidence bounds; martingale; second-order bounds%
\end{keywords}

\textfloatsep=.5em

\section{Introduction}

%
The offline contextual bandit problem has emerged as a critical area of study in sequential decision-making, with significant implications for decision systems for various domains including recommendation~\citep{li10acontextual} and online advertising~\citep{schwartz2017customer}.
In this problem, a behavior policy $\piref(a|x)$ is deployed in the environment for a nontrivial period of time, where the policy defines a conditional distribution over the actions $a$ (e.g., items to be recommended) given each context information $x$ (e.g., user being served).
%
Specifically, at each time step $t\in[n] \defeq \{1,\ldots,n\}$, an agent observes a context $x_t\sim \cD$ from an unknown distribution $\cD$, takes an action $a_t \sim \piref(a|x_t)$, and then receives a reward $r_t = r(x_t, a_t) \in [0,1]$ where $r$ is an unknown  (possibly stochastic) reward function. 
Given the offline logs of interactions via the behavior policy $D_n\defeq \{(x_t,a_t,r_t)\}_{t=1}^n$, we wish to find a policy $\pi$ that maximizes the expected reward $\mean(\pi)\defeq \EE_{x\sim\cD,a\sim \pi(a|x)}[r(x,a)]$, which we call the \emph{value} of the policy. 
This setting is called \emph{off-policy}, in contrast to its online counterpart, where a policy can be updated with the feedback from the environment throughout.
While online interactions may allow to find the best policy more effectively, 
in many real-world scenarios, it is generally either infeasible due to system constraints or costly considering operational risks. 
The offline problem naturally arises as a viable alternative in this context.


%
%

%
\begin{figure*}
  \begin{center}
  \scalebox{.975}{
    \begin{tabular}{c}
      $\underbrace{\sqrt{\EE\Bigl[\fr{(\ips_1^{\pi^*} - v(\pi^*))^2}{1 + \beta 
          (\ips_1^{\pi^*}\!-\!v(\pi^*))}\Bigr]}}_{\tsty\text{PUB (\textbf{ours}; see Eq.~\eqref{eq:selection_refined})}} 
      \le \displaystyle\EE\Bigl[\fr{(\ips_1^{\pi^*} - v(\pi^*))^2}{1 + \beta 
          (\ips_1^{\pi^*}\!-\!v(\pi^*))}\Bigr]
      \lesssim\!\! \underbrace{1 \!+\! \EE\Bigl[\fr{(\ips_1^{\pi^*})^2}{1 + \beta 
          \ips_1^{\pi^*}}\Bigr]}_{\tsty\text{\citet{sakhi24logarithmic}} } 
      \le\!\! \underbrace{1 \!+\! \EE\Bigl[
        \frac{(\ips_1^{\pi^\star})^2}{\pi^\star r+\beta\ips_1^{\pi^\star}}
        \Bigr]}_{\tsty\text{\citet{gabbianelli24importance}} }$.
\vspace{.5em}\\
(a) \textbf{Off-Policy Selection} 
%
\vspace{1em}\\
       $  
       %
       %
       %
       %
       %
       %
       %
         %
        \underbrace{\EE\Bigl[
        \frac{(\ips_1^{\pi^\star})^2}{c_u + \gamma\ips_1^{\pi^\star}}
        \Bigr]}_{ 
             \tsty\shortstack{\text{Solve \eqref{eq:op-learning-alg-0} + Assumption~\ref{ass:phi-new} (\textbf{ours}; see Eq.~\eqref{eq:learning})} \\ \text{\& \citet{sakhi24logarithmic}}} 
         } 
       \le \underbrace{
       %
       \EE\Bigl[
        \frac{(\ips_1^{\pi^\star})^2}{\pi^\star r+\gamma\ips_1^{\pi^\star}}
        \Bigr]
        }_{\tsty\text{\citet{gabbianelli24importance}}}. 
       $
       \vspace{.5em}\\
       (b) \textbf{Off-Policy Learning}
    \end{tabular}}
  \end{center}\vspace{-1em}
  \caption{
  Comparison of different bounds on the offline regret (see Eq.~\eqref{eq:offline_regret}) for the off-policy (a) selection and (b) learning where $\beta \approx \sqrt{1/n}$ in (a), and $\gamma>0$ in (b) is a hyperparameter.
  We hide a factor of $\sqrt{1/n}$ and other constants. 
  The symbol above $\lesssim$ holds for $n$ sufficiently large. %
  %
  For selection, our method achieves an improved bound.
  For learning, we propose a broad family of methods that achieves the same order of bound as~\citet{sakhi24logarithmic}.
  }
  \label{fig:comparison}
\end{figure*}

%
The main challenge of such offline problem is in the discrepancy of the behavior policy used to log the offline data and the set of candidate policies which we wish to evaluate the performance.
%
That is, we cannot simply use the offline data $D_n$ to estimate the expected reward of an arbitrary policy $\pi$, since $\pi$ may choose actions that are different from $a_t$'s chosen by $\piref$, in which case we have not observed rewards for them.
This is in stark contrast to the supervised learning setup where a classifier's generalization error can be estimated by simply computing the average error on a test dataset.
To circumvent the problem, researchers have proposed unbiased estimators of the expected reward of a policy, such as the Importance Weighted (IW) estimator\footnote{
Also known as Inverse Propensity Score or Inverse Propensity Weighting estimators.
}~\citep{horvitz52generalization,liu19competing} 
and Doubly Robust (DR) estimator~\citep{robins95semiparametric}, with numerous extensions of them.
IW is defined as:
%
%
%
\begin{align}\label{eq:iw}
    \hat{v}_n^{\mathsf{IW}}(\pi) 
    \defeq 
    \frac{1}{n}\sum_{t=1}^n \ips_t^\pi \text{~~ where ~~} \ips_t^\pi = \fr{\pi(a_t|x_t) }{\piref(a_t|x_t)} r_t
\end{align}
is called importance-weighted reward.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%


%
Depending on the specific goal and setting, there are three representative types of problems in the off-policy setting:
\begin{itemize}
  \item \textbf{Off-policy evaluation}: 
      Given a policy $\pi$, estimate its value (i.e., expected reward).
      In supervised learning, this corresponds to estimating the generalization error of a classifier using a test split or obtaining confidence bounds for it.
      %
  \item \textbf{Off-policy selection}: 
      Given a set $\Pi$ of finite number of policies, select the policy with the largest value.
      In supervised learning, this corresponds to performing model selection using a validation split.
  \item \textbf{Off-policy learning/optimization}: 
      Given a policy class $\Pi$ (typically $|\Pi|=\infty$ like for neural network policies), find the policy $\pi$ with the largest value.
      In supervised learning, this corresponds to learning a classifier using a train split.
\end{itemize}

In selection and learning, we wish to establish a guarantee on the \emph{offline regret} (or suboptimality gap) for the policy $\hat{\pi} \in \Pi$ selected by an algorithm, which is defined as 
\begin{align}
\label{eq:offline_regret}
    \mathsf{Reg}_n(\hat{\pi}) 
    \defeq \mean(\pi^\star)-\mean(\hat{\pi})
\end{align}
where $\pi^\star \defeq  \arg\max_{\pi\in\Pi} \mean(\pi)$.

The reason for differentiating learning from selection parallels the reason for distinguishing training from model selection in supervised learning.
In the binary classification setting, the proper objective is the 0-1 loss, which can be \textit{evaluated} for model selection but is difficult to \textit{optimize} for training.
Thus, for learning, we often resort to convex surrogates such as the logistic loss or, more generally, any function that at least admits gradient-based optimization.
Analogously, we can use any objective function for off-policy selection as long as it can be evaluated. 
For off-policy learning, we prefer an objective that admits convex optimization, or at least is amenable to stochastic gradient-based optimization.
%



%
In this paper, we make two main contributions.
%
First, we propose a novel OP selection method called PUB (\textbf{P}essimism via semi-\textbf{U}nbounded-coin-\textbf{B}etting).
PUB is an algorithm for computing a lower confidence bound (LCB) of any nonnegative random variable and is based on a variation of betting-based confidence bound~\citep{orabona24tight,Ryu--Bhatt2024,Waudby-Smith--Ramdas2020b}.
By applying our new LCB to the importance-weighted rewards $\{\til r_t^\pi\}_{t=1}^n$ (defined in Eq.~\eqref{eq:iw}) for each policy under consideration, we can establish a guarantee on the performance measure called offline regret (defined in Eq.~\eqref{eq:offline_regret}), which is strictly tighter than existing works to our knowledge.
We highlight two features in our guarantee. First, our regret bound scales with the \emph{standard deviation} of $\ips^\pi$, significantly improving the prior art scaling with the raw second moment. Second, more crucially, we achieve the improved guarantee \emph{without any hyperparameter tuning}. This is crucial in practice as tuning a parameter in the existing estimators is infeasible in general due to the lack of knowledge on the second-order statistics of $\ips^\pi$.
%
We summarize the comparison in Figure~\ref{fig:comparison}(a) and provide details in Section~\ref{sec:op-selection}.
Our LCB can be also applied to OP evaluation to construct both lower- and upper- confidence bounds, provably converging to the value of a target policy; we defer this discussion to Appendix~\ref{sec:evaluation}. 
%

%
Second, we propose a broad family of optimization objectives for OP learning.
Optimal policies defined with these objectives attains the second-moment-based bound, matching the rate of the state-of-the-art method called \emph{logarithmic smoothing}~\citep{sakhi24logarithmic}.
In particular, we propose to learn a policy by solving
\begin{align}\label{eq:op-learning-alg-0}
    \hpi_n\defeq \arg\max_{\pi \in \Pi} 
    \sum_{t=1}^n \phi (\beta \ips_t^\pi),
\end{align}
where $\beta$ is a hyperparameter and $\phi\colon \mathbb{R}_+\to\mathbb{R}$ is a \emph{score function}.
%
%
%
%
%
%
One extreme instance of our generic family is called \textit{freezing} whose $\phi(x)$ zeros out when $x$ is too large.
This greatly reduce variance at the price of bias, and we empirically show that freezing results in the best performance especially in the small-data regime.
Our analysis not only achieves the same smoothed second-order bound as the state-of-the-art~\citep{sakhi24logarithmic} but also reveals that, depending on the problem instance, one may prefer aggressive methods such as freezing.
We summarize our achieved bounds along with those of existing work in Figure~\ref{fig:comparison}(b) and explain details in Section~\ref{sec:learning}.

%
%
%
%

%
Finally, we perform empirical evaluation of the proposed selection and learning methods, following the experiment suite of~\citet{wang24oracle}.
We empirically show that PUB outperforms all existing methods, and new learning methods outperform or on par with baseline methods.
We conclude the paper with exciting future research directions.

\paragraph{Notation.} For a random variable $X$, we denote its expectation and variance by $\EE[X]$ and $\VV[X]=\EE[X^2]-\EE[X]^2$, respectively.
We use $a_{1:n}$ to denote a sequence of numbers $a_1,\ldots,a_n$.
For real numbers $a,b\in\mathbb{R}$, we use shorthand notations $a\wed b\defeq \max\{a,b\}$ and $a\vee b \defeq \min\{a,b\}$.

\vspace{-.5em}
\section{Problem Setting}
We have a log of interactions $D_n=\{(x_t,a_t,r_t)\}_{t=1}^n$ with a contextual bandit using a behavior (or reference) policy $\piref(a|x)$, \ie for each $t\ge 1$, 
\[
(x_t,a_t,r_t)\sim p(x) \piref(a|x) p(r|x,a) ~.
\]
Based on the bandit-logged data $D_n$, we wish to evaluate the value of a target policy $\pi(a|x)$:
%
\[
\mean(\pi)\defeq \E_{(x,a,r)\sim p(x)\pi(a|x)p(r|a,x)}[r] ~.
\]
With a slight abuse of notation, we will occasionally write $r=r(x,a)$, where $r(x,a) \in[0,1]$ denotes a (possibly stochastic) reward function.
One simple and popular unbiased estimator for $v(\pi)$ is the importance weighted (IW) estimator~\citep{horvitz52generalization} defined in~\eqref{eq:iw}.
%
%
%
%
For convenience, we define the importance weight as
\begin{align}
    w_t^\pi\defeq \frac{\pi(a_t|x_t)}{\piref(a_t|x_t)}
    %
    %
    \label{eq:iw_reward}
\end{align}
so that we can write $\ips_t^\pi = w_t^\pi r_t$.
Hereafter, we denote the variance of the importance weighted reward by
$
\tilde{\var}(\pi)\defeq \VV[\ips_1^\pi].$
While the IW estimator is unbiased, i.e., $\EE[\hat{v}_n^{\mathsf{IW}}(\pi) ]=\mean(\pi)$, the variance $\tilde{\var}(\pi)$ can be undesirably large for policies that frequently choose different actions not explored by $\piref$.
The effect is exacerbated when the IW estimator is used for the objective function for the selection or learning task.
That is, we may choose a bad policy because, if its IW has a disproportionately high variance, then the IW can be largely overrepresented with nontrivial probability.

This led to development of the \textit{pessimism} principle~\citep{swaminathan15batch} where we find the policy with the largest lower confidence bound of the IW estimator.
This has the benefit of penalizing policies with large variance, which can be seen as a form of regularization for stability.
Theoretically, pessimism is known to enjoy a property called \textit{single-policy concentrability}, which means that the main factor determining the convergence of the offline regret (see Eq.~\eqref{eq:offline_regret}) scales with a quantity that depends on the variability of the best policy $\pi^*$ rather than the maximum variability of all policies $\pi\in\Pi$ that is commonly referred to as \textit{all-policy concentrability}.
That is, the mere existence of an ill-behaved policy will not slow down the convergence.
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Off-Policy Selection}
\label{sec:op-selection}
%
For the selection problem, we wish to choose a policy that would maximize the expected reward.
Our strategy is the standard \emph{pessimism} under uncertainty, \ie to construct the LCB on the expected reward for each policy, and choose the policy that maximizes the LCB.
The proposed selection method plugs our novel one-sided-betting lower confidence bound into the pessimism.
%
%
%
%
In what follows, we first introduce a betting-based (time-uniform) confidence bound for mean-parameter estimation when the random variables are $[0,\infty)$-valued.
We then show how the pessimism strategy with our confidence bound performs in the selection task (Theorem~\ref{thm:main_selection}) and discuss its superiority against existing methods.

\subsection{New Betting-Based Lower Confidence Bound for \texorpdfstring{$[0,\infty)$}{[0,infty]}-Valued Random Variables}
%
To construct a confidence bound, 
we will use the idea of gambling and the martingale theory, which has been extensively used in the literature recently~\citep{orabona24tight,Waudby-Smith--Ramdas2020a,Waudby-Smith--Ramdas2020b,Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022,Ryu--Bhatt2024,Ryu--Wornell2024}.
The most general form of gambling is the stock market investment~\citep{Cover--Thomas2006}, and here we will consider betting over a two-stock market, following the convention of \citet{Ryu--Bhatt2024}.

%
%
%
    
%
%
%
%
%
%
%
%


%
%
%
%
%
%

\subsubsection{A Generic Betting-Based Construction}
Suppose that there are two stocks, say stock 1 and stock 2. 
On each day $t\in\mathbb{N}$, a gambler must make her betting $\bb_t=(b_t,1-b_t)$, for $b_t\in[0,1]$, over the two stocks at the beginning.
At the end of the day, the \emph{price relative vector} $\bx_t=(x_{t1},x_{t2})\in\mathbb{R}_+^2$ is revealed, where $x_{ti}>0$ captures the multiplicative change in the price of stock $i$.
Note that the betting $\bb_t$ must be causal, that is, $\bb_t$ can be only a function of the past observations $\bx_{1:t-1}$.
If we denote the gambler's wealth at day $t$ by $\wealth_t$, then the multiplicative gain of the wealth can be written as 
\[
\frac{\wealth_t}{\wealth
_{t-1}}=\bb_t^\intercal\bx_t
=b_t x_{t1} + (1-b_t) x_{t2}.
\]
Here, if we assume that the stock market, \ie $(\bx_t)_{t=1}^\infty$, is stochastic and satisfies $\EE[\bx_t|\bx_{1:t-1}]\le [1,1]^\intercal$ (coordinate-wise), 
then it is easy to check that the wealth process $(\wealth_t)_{t=1}^\infty$ is \emph{super-martingale}, \ie $\EE[\wealth_t|\bx_{1:t-1}]\le \wealth_{t-1}$.

In the case of $[0,1]$-valued random variables $(Y_t)_{t=1}^{\infty}$ such that $\EE[Y_t|Y^{t-1}] = \EE[Y_1] \defeq \blue{\mu} > 0$ for any $t\ge1$, one can set $x_{t} = [\fr{Y_t}{\mu}, \fr{1-Y_t}{1-\mu}]^\intercal$.
Then, applying Ville's inequality~\citep{ville39etude} with $(\wealth_t)_{t=1}^{\infty}$  leads to a confidence sequence for $\mu$~\citep{Waudby-Smith--Ramdas2020a,Waudby-Smith--Ramdas2020b,orabona24tight,Ryu--Bhatt2024}.
In particular, we obtain
\begin{align*}
  1-\dt \le \PP\del[3]{ \sup_{n\ge 1} \wealth_n(Y_{1:n}; \mu) < \fr1\dt }
\end{align*}
where we have made explicit the dependency of $Y_{1:n}$ and $\mu$ on $\wealth_n$.
Since the inequality inside $\PP(\cd)$ holds for $\mu$, the set of $\nu$'s satisfying the inequality with $\mu \larrow \nu$ must include $\mu$ with high probability, thus forming a confidence set $S$.
Simply choosing the smallest and the largest value of $S$ leads to a lower and upper confidence bound respectively.
\citet{orabona24tight} have shown that a suitable choice of the betting strategy $\{\bb_t\}_{t=1}^\infty$ leads to confidence bounds that are of empirical-Bernstein type (i.e., confidence width adapts to the empirical variance), and provably never worse than the Bernoulli-KL-based confidence bound.
The latter property does not hold for the empirical Bernstein bound~\cite{maurer09empirical} in the small-sample regime.

What if, however, we want confidence bounds for a nonnegative random process \((Y_t)_{t=1}^{\infty}\) that may be unbounded (i.e., \(Y_t \in [0,\infty)\))? 
The unbounded nature of \(Y_t\) breaks the nonnegativity of \(\wealth_t\) in the construction above, thereby violating Ville's inequality and preventing us from obtaining confidence bounds.  
Instead, \citet{Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022} consider a stock market of the form
%
%
%
\begin{align*}
\bx_t(\nu)\defeq \Bigl[\frac{Y_t}{\nu},1\Bigr]^\intercal,
\end{align*}
which leads to a nonnegative wealth sequence.
Here, the first stock depends on the underlying process $Y_t$, where the second stock can be understood as \emph{cash}.
We call this setting an \emph{one-sided} betting.
For a given causal betting strategy $(\bx_{1:t-1}(\nu)\mapsto \bb_t)_{t=1}^\infty$, the cumulative wealth can be written as
\begin{align*}
\wealth_n(Y_{1:n}; \nu)
\defeq \prod_{t=1}^n \Bigl(1-b_t+b_t\frac{Y_t}{\nu}\Bigr),
\end{align*}
assuming that we start from a unit initial wealth $\wealth_0=1$.
Thus, by Ville's inequality, we have $1-\dt \le \PP( \sup_{n\ge 1} \wealth_n(Y_{1:n}; \mu) < 1/\dt )$.
Thus, for $\dt\in(0,1)$, defining
%
%
%
%
\begin{align*}
C_{\mathsf{bet}}^{(\dt)}(Y_{1:n})
\defeq \Bigl\{
\nu\in(0,1)\colon \wealth_n(Y_{1:n};\nu)\le \frac{1}{\dt}
\Bigr\}
\quad\text{and}\quad
\hat{v}_{\mathsf{bet}}^{(\dt)}(Y_{1:n})\defeq \inf C_{\mathsf{bet}}^{(\dt)}(Y_{1:n}),
\end{align*}
we have that
$\hat{v}_{\mathsf{bet}}^{(\dt)}(Y_{1:n})$ is a $(1-\dt)$-lower confidence bound (LCB) for $\EE[Y_1]$: 
%
\begin{proposition}[{\cite[Proposition~1]{Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022}}]
\label{prop:lcb}
{Let $(Y_t)_{t=1}^\infty$ a non-negative real-valued random process $(Y_t)_{t=1}^\infty$ such that $\EE[Y_t|Y^{t-1}]\defeq \EE[Y_1]=\mu>0$ for any $t\ge 1$.}
For any causal betting strategy, $C_{\mathsf{bet}}^{(\dt)}(Y_{1:n})$ is a (time-uniform) lower confidence set at level $1-\dt$, that is,
\begin{align*}
\PP\Bigl(\forall t \ge 1, \mu \ge \hat{v}_{\mathsf{bet}}^{(\dt)}(Y_{1:t})\Bigr) \ge 1-\dt.
\end{align*}    
%
%
\end{proposition}
Note that obtaining an \textit{upper} confidence bound for $[0,\infty)$-valued random variables is nontrivial and is beyond the scope of our work.
The proof of the result above, deferred to Appendix, is based on a standard martingale argument.
We note in passing that $(\hat{v}_{\mathsf{bet}}^{(\dt)}(Y_{1:t}))_{t=1}^\infty$ satisfies the strong \emph{time-uniform} guarantee, but we only invoke this guarantee for a single time step in the offline setting.
%

%
While any choice of betting strategy leads to a correct LCB, we need an \emph{efficient} betting strategy to obtain a \textit{tight} (i.e., sample-efficient) LCB.
Specifically, since the LCB is a random variable as well, one may desire to prove the sample efficiency of the LCB by bounding the distance between the LCB and the mean.
%
Here, it is important for the bound to consist of \textit{deterministic} quantities (e.g., variance) because random quantities (e.g., empirical variance) may ill-behave even with a large number of samples in which case the bound is not meaningful.
This is important for $[0,\infty)$-valued random variables because the empirical variance may not converge.\footnote{
  For $[0,1]$-valued random variables, one can easily show that the empirical variance will converge to the true variance.
}
%
%
However, existing LCBs for this setting either do not provide any sample efficiency guarantee~\cite{Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022} or provide a sample efficiency guarantee that scales worse than $\sqrt{\Var(\ips^\pi_1)}$ (or something comparable) that seems natural in our opinion~\cite{gabbianelli24importance,sakhi24logarithmic}.
This observation is the motivation of our novel LCB described below.

%
%

%
%
%
%

%

%


\subsubsection{LCB Induced by Universal Portfolio}

For the betting strategy, we use Cover's universal portfolio~\citep{Cover1991}, as \citet{orabona24tight} first studied its application for bounded random processes.
A constant betting strategy $\bb_t=(b,1-b)$ for some $b\in[0,1]$, which is called a \emph{constantly rebalanced portfolio} (CRP), yields the cumulative wealth 
\begin{align}
\label{eq:wealth_crp}
\wealthcrp{b}_n(Y_{1:n};\nu)
&\defeq \prod_{t=1}^n \del[2]{1-b+b\frac{Y_t}{\nu}}.
\end{align}
\citet{Cover1991} proposed a strategy called the \emph{$w$-weighted universal portfolio}, or \emph{universal portfolio} (UP) in short,
%
%
to track the wealth achieved by the best CRP in hindsight asymptotically up to the first-order exponent.
Cover's UP is defined as the \emph{mixture} of CRP wealths with respect to a mixture distribution $w(b)$ over $b\in[0,1]$, that is,
\begin{align}
\label{eq:wealth_up}
\wealthup_n(Y_{1:n};\nu)
&\defeq \int_0^1 \wealthcrp{b}_n(Y_{1:n};\nu) w(b) db.
\end{align}
Intuitively, Cover's UP can be understood as a buy-and-hold strategy of the set of constant betting strategies, where a unit wealth is distributed according to the weight $w(b)$.
%
\citet{Cover--Ordentlich1996} showed that with the particular choice of weight distribution $w(b)=\frac{1}{\sqrt{\pi b(1-b)}}$, the density of the $\mathsf{Beta}(\frac12,\frac12)$ distribution, which we consider by default,
the UP's wealth is minimax optimal with respect to the class of CRPs.
In our specific context, the guarantee of \citet[Theorem~2]{Cover--Ordentlich1996} translates to: for any sequence $y_{1:n}\in\mathbb{R}_+^n$,
\begin{align}
{\wealthup_n(y_{1:n};\nu)}
\ge 
\wealthpcrp_n(y_{1:n};\nu) 
\defeq \frac{1}{\sqrt{\pi(n+1)}}\sup_{b\in(0,1)}\wealthcrp{b}(y_{1:n};\nu).
\label{eq:up_vs_pcrp}
\end{align}
In words, this shows that the Cover's UP can achieve the best CRP's performance within a polynomial factor $\sqrt{\pi(n+1)}$.
We call the right hand side the \emph{penalized best CRP wealth}.

%

%
In Figure~\ref{fig:ex_betting_lcb}, we visualize the logarithmic wealth functions $\nu\mapsto\ln\wealthup(Y_{1:n};\nu)$ of different CRPs and that of Cover's UP, at different time steps.
We first note that the wealth function of each CRP is log-convex and monotonically decreasing, and thus so is that of Cover's UP. 
This ensures that there exists a unique root for the equation $\wealthup(Y_{1:n};\nu)=\dt^{-1}$, which corresponds to the $(1-\dt)$-LCB denoted by 
\begin{align}\label{eq:uplcb}
    \lcbup{n}{\dt}(Y_{1:n})    \defeq \min\cbr[2]{\nu>0: \wealthup_n(Y_{1:n};\nu) \le \fr1\dt}.
\end{align}
We call the resulting bound the \emph{UP-LCB}.
We also remark that the curve of Cover's UP $\nu\mapsto\wealthup_n(Y_{1:n};\nu)$ closely approximates the frontier $\nu\mapsto\sup_{b\in[0,1]}\wealthcrp{b}_n(Y_{1:n};\nu)$, which is a consequence of that Cover's UP asymptotically tracks the best wealth of CRPs for any stock market as implied from Eq.~\eqref{eq:up_vs_pcrp}.

One can also use the penalized-best-CRP-wealth in Eq.~\eqref{eq:up_vs_pcrp} to construct a LCB, which is slightly looser than UP-LCB, yet simpler to compute; see below for computational aspects.
Specifically, 
\begin{align}
    \lcbpcrp{n}{\dt}(Y_{1:n}) \defeq \min\cbr[2]{\nu>0: \wealthpcrp(Y_{1:n};\nu) \le \fr{1}{\dt}}
\label{eq:pcrplcb}
\end{align}
is a $(1-\dt)$-LCB,
which we call the \emph{penalized-best-CRP-LCB} or \emph{\pcrplcb{}} in short.

{
\subsubsection{Finite-Sample Guarantees}
Our main technical contribution in this section is the following statement, which establishes the rate of convergence of UP-LCB and \pcrplcb{} to the true mean, automatically adapting to the underlying variance. 
For $n$ sufficiently large, we further show that the convergence is proportional to a \emph{smoothed variance}, defined as follows. This guarantee will be handy later for comparing the bound with \citet{sakhi24logarithmic}.
For a nonnegative random variable $Y$, we define a \emph{$b$-smoothed variance}
\begin{align*}
\WW_b[Y]\defeq \EE\Bigl[\fr{(Y-\EE[Y])^2}{1+b\fr{Y-\EE[Y]}{\EE[Y]}}\Bigr]
= \EE[Y]
\EE\Bigl[\fr{(Y-\EE[Y])^2}{bY+(1-b)\EE[Y]}\Bigr]
\end{align*}
for $b\in[0,1]$.
We note that $\WW_b[Y]$ interpolates the two extreme quantities $\WW_0[Y]=\VV[Y]$, the variance, and $\WW_1[Y] = \EE[Y]\EE[\fr{(Y-\EE[Y])^2}{Y}]$.
Under a mild regularity condition, i.e., $\EE[Y]<\infty$ and $\EE[\fr{1}{Y}]<\infty$,
%
we can show that $b\mapsto \WW_b[Y]$ is strictly convex, unless $Y$ is constant with probability 1.
Under such condition, for any $b\in(0,1)$,
\begin{align*}
\WW_b[Y] < \WW_0[Y]\wed \WW_1[Y] = \VV[Y] \wed \EE[Y]\EE\Bigl[\fr{(Y-\EE[Y])^2}{Y}\Bigr].
\end{align*}
%

In what follows, we assume that $(Y_t)_{t=1}^\infty$ is an independent and identically distributed (i.i.d.), nonnegative random process, with 
\[
\text{$\mu\defeq\EE[Y_1]$ 
\quad and \quad 
$\sigma^2\defeq \VV[Y_1].$}
\]
\begin{theorem}[Convergence rate of UP-LCB and \pcrplcb{}]
\label{thm:up_lcb_rate}
Let $n\ge 1$ and
define
$\blue{F_n^{(\dt)}} \defeq \ln\fr{\sqrt{\pi(n+1)}}{\dt^2}$.
%
%
%
Then, with probability $\ge 1-2\dt$,
\[
0
\le \mu - \lcbup{n}{\dt}(Y_{1:n})
\le \mu - \lcbpcrp{n}{\dt}(Y_{1:n})
\le \sqrt{\fr{48\sigma^2}{n} F_n^{(\dt)}} \vee \fr{12\mu}{n}F_n^{(\dt)}.
\]
Moreover, if 
$n\ge 108\bigl(1\vee 36\fr{\mu^2}{\sigma^2}\bigr) F_n^{(\dt)}$, for $b_n^{(\dt)}\defeq \sqrt{\fr{\mu^2}{2\sigma^2}\fr{F_n^{(\dt)}}{n}}$,
%
we further have
\begin{align}
\mu - \lcbpcrp{n}{\dt}(Y_{1:n})
&\le 
\inf_{b\in(0,\fr34]}
\Bigl\{
\frac{b}{\mu}
\WW_b[Y_1]
+\fr{\mu}{b}
\fr{F_n^{(\dt)}}{n}
\Bigr\}
%
\le 2\sqrt{
\frac{\WW_{b_n^{(\dt)}}[Y_1]}{n}
F_n^{(\dt)}}.
\label{eq:lcb_rate_refined}
\end{align}
\end{theorem}
This shows that both UP-LCB and \pcrplcb{} converge to the true mean from below at the rate of $O(\sqrt{\frac{\sigma^2}{n}\ln\frac{n^{1/4}}{\dt}}\vee \frac{\mu}{n}\ln\frac{n^{1/4}}{\dt})$.
This guarantee is somewhat analogous to Bernstein's inequality, but we remark that this holds uniformly over time, and more importantly, for any $[0,\infty)$-valued random variables.
This is the first LCB with finite-time \textit{sample efficiency} guarantee for $[0,\infty)$-valued random variables, which is much stronger than merely statistically correct bounds. 
%

%
}

%
\begin{remark}[Empirical-Bernstein-type relaxation]
\label{rem:eb}
We can also derive a loose outer bound of the UP-LCB and \pcrplcb{}, which is simply a function of the empirical mean and variance in a similar spirit to \citep[Theorem~6]{orabona24tight}.
We defer the statement to Appendix~\ref{app:sec:emp_bern_relaxation}; see Theorem~\ref{thm:emp_bern_relaxation}.
This relaxation can be understood as a statistically valid empirical Bernstein inequality for $[0,\infty)$-valued random variables.
While this bound is statistically valid, it does not characterize the rate of convergence or even the asymptotic consistency of the LCB as an estimator of the mean.
We further discuss the benefit of UP-LCB and \pcrplcb{} compared to this type of relaxation in Appendix~\ref{sec:counter-example}.
\end{remark}

%

%
%
%

%

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{figs/ex_up_vs_crp.pdf}
    \vspace{-1em}
    \caption{Example of the evolution of cumulative wealths achieved by different CRPs in Eq.~\eqref{eq:wealth_crp}, Cover's UP in Eq.~\eqref{eq:wealth_up}, and the penalized best CRP wealth in Eq.~\eqref{eq:up_vs_pcrp}.
    The underlying process is a sequence of independent and identically distributed (i.i.d.) Gamma random variables with shape and scale parameters of 6 and $1/8$, respectively, and thus mean $3/4$.
    }\label{fig:ex_betting_lcb}
\end{figure}

\begin{remark}[Implementation]
We can compute the UP-LCB up to numerical precision via a dynamic programming approach together with binary search over $\nu$, similar to described in \citet{Ryu--Bhatt2024}.
Its time complexity for processing a length-$n$ trajectory is, however, $O(n^2)$.
In practice, one can implement \pcrplcb{} (Eq.~\eqref{eq:pcrplcb}), or techniques in \citet{orabona24tight} or \citet{Ryu--Bhatt2024} to compute reasonably accurate proxy in linear complexity $O(n)$. 
%
In the experiment below, we used the pCRP$^\star$-LCB.
We defer the detailed discussion about implementation in Appendix~\ref{app:sec:implementation_lcbs}, including an adaptation of the fast proxy of \citet{Ryu--Bhatt2024}.
\end{remark}

\subsection{Off-Policy Selection with PUB (Pessimism via semi-Unbounded-coin-Betting)}
Our off-policy selection strategy is to apply our UP-LCB $\lcbup{n}{\dt}(\cdot)$ in Eq.~\eqref{eq:uplcb} and \pcrplcb{} $\lcbpcrp{n}{\dt}(\cdot)$ in Eq.~\eqref{eq:pcrplcb} to the underlying processes $\{\ips_{1:n}^\pi\}_{\pi\in\Pi}$ and to pick the best:
\begin{align}
\selectup\defeq \arg\max_{\pi\in\Pi} \lcbup{n}{\dt}(\ips_{1:n}^\pi)
\qquad\text{and}
\qquad
\hpi_{\mathsf{pCRP}^\star}^{(\dt)} \defeq \arg\max_{\pi\in\Pi} \lcbpcrp{n}{\dt}(\ips_{1:n}^\pi).
\label{eq:up_select}
\end{align}
We call these selection methods the \emph{\textbf{P}essimism by semi-\textbf{U}nbounded-coin-\textbf{B}etting} (PUB). 
%
%
Hereafter we will omit the superscript $^{(\dt)}$ to avoid clutters.
%

The following guarantee is immediate from Theorem~\ref{thm:up_lcb_rate}, and we defer the proof to Appendix.
\begin{theorem}[Selection]
\label{thm:main_selection} 
%
Let $\dt'=|\Pi|/\dt$.
With probability $\ge 1-2\dt$, 
for any $\pi^*\in \Pi$ and $\hpi\in \{\selectupsimple,\selectpcrpsimple\}$,
\begin{align*}
0\le \mean(\pi^*) - \mean(\hpi) 
%
&\le \sqrt{\fr{48\tilde{\var}(\pi^*)}{n} F_n^{(\dt')}
} \vee \fr{12\mean(\pi^*)}{n} F_n^{(\dt')}.
\end{align*}
Moreover, if 
$n\ge 108\bigl(1\vee 36\fr{\mean(\pi^*)^2}{\til{\mathsf{V}}(\pi^*)}\bigr) F_n^{(\dt')}$,
for $b_n^{(\dt)}\defeq \sqrt{\fr{\mu^2}{2\sigma^2}\fr{F_n^{(\dt)}}{n}}$, we further have
\begin{align}
\mean(\pi^*) - \mean(\hpi) 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
&\le 
\inf_{b\in(0,\fr34]}
\Bigl\{
%
%
%
%
\frac{b}{\mean(\pi^*)}
\WW_b[\ips_1^{\pi^*}]
+\fr{\mean(\pi^*)}{b}
\fr{F_n^{(\dt')}}{n}
\Bigr\}
%
\le 2\sqrt{
\frac{F_n^{(\dt')}}{n}
\WW_{b_n^{(\dt')}}[\ips_1^{\pi^*}]}.
\label{eq:selection_refined}
\end{align}
%
%
%
%
%
\end{theorem}

%
%
%
%
%
%
%
%
%
We compare the guarantee with that of the Logarithmic Smoothing (LS) estimator of \citet{sakhi24logarithmic}; see the definition in Appendix~\ref{app:sec:estimators}.
In Proposition~6 therein, it is proved that the estimator achieves the regret bound $\beta \EE[\fr{(\ips_1^{\pi^*})^2}{1+\beta\ips_1^{\pi^*}}] + \fr{2}{\beta n}\ln\fr{2|\Pi|}{\dt}$. First, we note that, our first term inside the infimum can be viewed as a \emph{centered} version of their first term. 
Also, similar to that the first term of their regret is always bounded by $\beta \mean(\pi^*)$, 
our first term is also bounded by $\frac{b}{\mean(\pi^*)}\WW_0[\ips_1^{\pi^*}]\wed \WW_1[\ips_1^{\pi^*}]$.
In the second part of the statement, we further show in Eq.~\eqref{eq:selection_refined} that, for $n$ sufficiently large, the regret scales as $\tilde{O}(\fr{1}{\sqrt{n}})$, where the leading factor is $\sqrt{\WW_{b_n^{(\dt')}}[\ips_1^{\pi^*}]}$, which can be rewritten as
\[
\sqrt{\WW_{b_n^{(\dt')}}[\ips_1^{\pi^*}]}
= \sqrt{\EE\Bigl[
\fr{(\ips_1^{\pi^*}-\mean(\pi^*))^2}{1+O(\sqrt{1/n})(\ips_1^{\pi^*}-\mean(\pi^*))}
\Bigr]}
\quad
\text{vs.}
\quad
\underbrace{1 + {\EE\Bigl[\fr{(\ips_1^{\pi^*})^2}{1 + O(\sqrt{1/n}) \ips_1^{\pi^*}}\Bigr]}}_{\text{\citet[Proposition~6]{sakhi24logarithmic}}}.
\]
As noted in Figure~\ref{fig:comparison}, we note that $\sqrt{\WW_{b_n^{(\dt')}}[\ips_1^{\pi^*}]}\approx \til{\mathsf{V}}(\pi^*)$ is strictly smaller than $1+\EE[(\ips_1^{\pi^*})^2]$ for $n$ sufficiently large.
%
%
%
%
%
%
Arguably the most appealing property of our selection method is that we achieve these bounds in a \emph{parameter-free} sense, unlike the existing methods that require tuning $\beta$.
This is implemented by the infimum in the bound unlike the LS estimator, which shows that our estimator automatically adapts to the ``optimal'' hyperparameter, as a consequence of applying the wealth of Cover's UP or penalized-best-CRP.
Note that the price of adaptivity is only a logarithmic factor.

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Off-Policy Learning}
\label{sec:learning}
%
%
The betting-based selection method in the previous section yields the tightest available bound.
For learning, while we can devise 
%
an optimization scheme using the betting-based formulation via Lagrange multipliers based on the following optimization formulation as follows:
\begin{align*}
  \max_{\pi\in\Pi} \max_{\alpha \ge 0} \min_\nu \Biggl\{\nu + \alpha\biggl(\max_{b\in[0,1]} \sum_{t=1}^n \ln\Bigl(1 + b \fr{\ips_t^{\pi}-\nu}{\nu}\Bigr) - \ln\fr{\sqrt{\pi(n+1)}}{(\dt/|\Pi|)^2}\biggr)
  \Biggr\},
\end{align*}
Since it is not easy to implement, however,
we propose in this section a broad class of pessimistic objective functions that take the following simple form involving a \textit{score function} $\phi\colon\mathbb{R}_+\to\mathbb{R}$:
%
%
%
%
%
%
%
%
%
%
\begin{align}\label{eq:op-learning-alg}
    \hpi_n \defeq \arg \max_{\pi \in \Pi} \sum_{t=1}^n %
    \phi (\beta \ips_t^\pi),
\end{align}
where $\beta\ge 0$ is a hyperparameter.
This form of objective is rather straightforward to implement with stochastic gradient-based algorithms.
To guarantee statistical efficiency of the algorithm, we restrict our attention to score functions that satisfy the following assumption.
\begin{assumption}\label{ass:phi-new}
For some $c_1, c_2 \in (0,1]$,
  a score function $\phi:\RR_+\rarrow\RR$ satisfies
  \begin{align*}
      -\ln\Bigl(1 - x + \fr{x^2}{c_1+c_2x}\Bigr) \le \phi(x) \le \ln(1+x).
  \end{align*}
\end{assumption}
Note that when $c_1=c_2=1$ the lower and the upper bound becomes equivalent and recovers the logarithmic smoothing algorithm~\citep{sakhi24logarithmic}.
We depict example score functions with various choices of $c_1$ and $c_2$ in Figure~\ref{fig:phi-second-order}.

%
\begin{proposition}[Examples of score functions]
\label{prop:phi}
  The following satisfies Assumption~\ref{ass:phi-new}:
  \begin{itemize}
    \item Logarithmic smoothing~\citep{sakhi24logarithmic}: $\phi^{\text{LS}} (x)=\ln(1+x)$ with $c_1 = c_2 = 1$.
    \item Clipping: $\phi^{\text{clipping}} (x) = \ln(1 + (x \wed 1))$ with $c_1 = c_2 = \fr12$.
    \item Freezing: $\phi^{\text{freezing}}(x) = \ln(1 + x\cd \onec{x \le 1})$ with $c_1 = c_2 = \fr12$.
  \end{itemize}
\end{proposition}

\begin{figure}[t]
  \centering
  %
    %
    %
    %
    %
     \includegraphics[width=.45\linewidth]{figs/score_functions.pdf}%
  %
  \caption{Examples of the score functions $\phi$ in Assumption~\ref{ass:phi-new}.
  %
  }
  \label{fig:phi-second-order}
\end{figure}

The clipping score function simply truncates the score function at $\ln 2$.
Note that the clipping here is done directly to $\ips^\pi$ (i.e., $\min\{\fr{\pi(a|x)}{\piref(a|x)}r(x,a),1\}$ rather than being applied to the probability ratio (i.e., $\min\{\fr{\pi(a|x)}{\piref(a|x)},1\}r(x,a)$).
We remark that this clipping score penalizes large values of $x$, i.e., implements a more aggressive pessimism, which may help reduce variance and improve sample efficiency.
The freezing score function implements an even more aggressive pessimism by zeroing out the score when it becomes higher than $1$.
The potential benefit of freezing is to effectively remove samples $(x_t,a_t,r_t)$ for policies $\pi$ whose IW score $\ips_t^{\pi}$ is too large.
This may have an even higher degree of variance reduction effect.

While \citet{sakhi24logarithmic} also propose a family of choices for inducing pessimism, their condition only guarantees \textit{correctness} of the pessimism~\cite[Corollary 4]{sakhi24logarithmic} and not the \textit{sample efficiency}.
Their sample efficiency result is only proved for the logarithmic smoothing.
In contrast, we demonstrate the sample efficiency for a broad class of score functions that satisfy Assumption~\ref{ass:phi-new}.

\paragraph{Main Result.}
%
We show a smoothed second-order bound only with respect to the optimal policy $\pi^\star$ for a fairly large class of score functions satisfying Assumption~\ref{ass:phi-new}.
\begin{theorem}[Learning]
\label{thm:learning-new}
  Let $\hpi_n$ be the estimator defined in Eq.~\eqref{eq:op-learning-alg} with a score function $\phi$ satisfying Assumption~\ref{ass:phi-new}, and let $\pi^\star$ be the optimal policy. 
  Then, 
  \begin{align}
  \label{eq:learning}
    \mean(\pi^\star) - \mean({\hpi_n}) 
    &\le \beta \EE\Bigl[\fr{(\ips^{\pi^*})^2}{c_1 + c_2 \ips^{\pi^*}}\Bigr] -  \underbrace{\fr1{\beta} \ln(\EE[e^{\phi(\beta \ips_1(\hpi_n)) - \EE  [\beta \ips_1(\hpi_n)]}] )}_{\tsty \defeq F_\beta(\phi)\ge 0} + \fr{2}{\beta n} \ln\fr{|\Pi| }{\dt},
  \end{align}
%
%
%
%
%
%
%
  where the functional $F_\beta(\phi)$ is the negative influence introduced by $\phi$ and satisfies $F_\beta(\phi) \ge 0$.
\end{theorem}
%
  %
  %
As a special case, our theorem recovers the guarantee of logarithmic smoothing~\cite[Proposition 6]{sakhi24logarithmic}.
The main term $\EE[\fr{(\ips^{\pi^*})^2}{c_1 + c_2 \ips^{\pi^*}}]$ is a smoothed second-moment term that specializes to that of~\citet{sakhi24logarithmic} when $c_1 = c_2 = 1$.
Thus, our bound inherits all the benefits such as being strictly better than IX~\citep{gabbianelli24importance} and being bounded with probability 1.

Additionally, our bound provides a nontrivial tradeoff made by different choices of $\phi$.
In particular, we have 
\begin{align*}
  F_\beta(\phi^{\text{freezing}}) \le F_\beta(\phi^{\text{clipping}})\le F_\beta(\phi^{\text{LS}}).
\end{align*}
Thus, by using a score function with $c_1,c_2<1$ (e.g., freezing or clipping), we gain a larger negative influence, which may lead to improved performance in practice, especially in a small-data regime as we demonstrate in our experiments below.
We believe that such a tradeoff arises from a delicate balance between bias and variance. Further characterization of this tradeoff is left for future work.

Finally, we note that 
$\beta$ is a hyperparameter that can be tuned using a holdout set, along with our proposed selection method from the previous section.

%

%
%
%

%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Experiments}
\label{sec:exp}
We demonstrate the efficacy of our ideas, betting and freezing, under a synthetic, controlled experiment setup.
%
%
%
Our experiments closely follow the setting of \citet{wang24oracle}, and thus we refer for detailed settings to the descriptions therein. 
We first discuss the learning experiment, followed by the selection experiment,
as the latter uses the best policies trained from the learning scenario by different baselines as a set of policies.
%




\subsection{Off-Policy Learning}

\paragraph{Datasets.} 
The contextual bandit data were simulated using three multi-class classification datasets from OpenML~\citep{OpenML2013}.
Each dataset has 1M data points.
Other statistics of the datasets are summarized in Table~\ref{tab:datasets} in Appendix.
For each learning method, we viewed each dataset as a multi-class regression problem, where each class corresponds to an action. We then treated a classifier, which maps a feature to a probability vector, as a deterministic policy that chooses the action of the maximum probability.
%
Among various configurations considered in \citet[Section~4]{wang24oracle}, we specifically considered the real-valued cost function and a single logging policy $\pi_{\text{good},\eps=0.1}$ therein.\footnote{
  The logging policy is defined as a random mixture of a deterministic policy and a uniform-random policy, where $\eps=0.1$ defines the probability of using the uniform-random policy.
  Here, the deterministic policy is induced by a ``good'' classifier trained separately; see \citet{wang24oracle} for the detail.
}
We tested different sample sizes by a fraction of datasets varying over $\{0.01,0.1,1\}$.

\paragraph{Baselines.}
We consider six different methods in the learning experiment.
The simplest baseline is the minimizer of the IW estimators without any regularizer (denoted as \texttt{IW}).
We then consider pseudo-loss (\texttt{PL})~\citep{wang24oracle}, clipped IW (\texttt{clippedIW}), 
Implicit Exploration (\texttt{IX})~\citep{gabbianelli24importance}, Logarithmic Smoothing (\texttt{LS})~\citep{sakhi24logarithmic}, and lastly LS with freezing (\texttt{LS+freezing}, which we propose in Section~\ref{sec:learning}. We include explicit definitions of the estimators in Appendix~\ref{app:sec:estimators}.
For optimization (i.e., learning), we used the linear regression approach. 
After training, we computed
the relative performance improvement for each estimator against the performance of the \texttt{IW} baseline:
\[
\text{(relative improvement of $\pi$)}\defeq \frac{\hat{\mean}(\pi_{\texttt{IW}})-\hat{\mean}(\pi)}{\hat{\mean}(\pi_{\texttt{IW}})}.
\]
Here, we used the IW estimator to estimate the value of each policy $\hat{\mean}(\pi)$.
%

Similar to \citet{wang24oracle}, the hyperparameter $\beta$ in each estimator (see Appendix~\ref{app:sec:estimators}) were tuned based on our PUB method with $\dt=0.1$, using a 50/50 split of the data.
For each learning method, we swept the hyperparameter $\beta$ over eight values $\{0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1\}$, and thus there are $48=6\times 8$ instances of methods in total.
For each dataset,
we ran the same experiment with 50 different seeds and report the averages with standard errors. 
%

\paragraph{Results.}
The results are summarized in Figure~\ref{fig:learning}.
As predicted by our analysis in Sec.~\ref{sec:learning}, \texttt{LS+freezing} (light blue) consistently improves the performance of \texttt{LS} (blue) in a small sample regime, even exhibiting competitive performance in general.
We believe this is due to the reduced variance thanks to the aggressive freezing that essentially removes problematic samples, which is particularly helpful in small-data regimes.

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{figs/offpolicy_learning.pdf}
\vspace{-.5em}
\caption{Evaluation results from the off-policy learning experiment: Relative improvement of each baseline and our method against the no-pessimism baseline, varying the fraction of data used over $\{0.01, 0.1, 1\}$. We highlight the consistent improvement of \texttt{LS+freezing} over \texttt{LS} in a small-sample regime.
}
\label{fig:learning}
\end{figure*}

\subsection{Off-Policy Selection}
\paragraph{Setup.}
%
We reused the synthetic bandit data from the learning experiment for off-policy selection.
For selection, we considered all the 48 policies from the learning experiment as a policy class.
%
For each dataset and each run, we selected the best method using \texttt{PUB}, \texttt{LS}, and the original empirical Bernstein (\texttt{EB}) of \citet[Theorem 4]{maurer09empirical}, all with $\dt=0.1$.
Here, we aim to simulate the scenario where the practitioner tries out various policies with various hyperparameters on the training data and then chooses the best policy using the validation set.

\paragraph{Evaluation.}
We again computed the relative performance improvement of each selected policy against that of the \texttt{IW} baseline.
%
To avoid misleading conclusion due to randomness, we performed the paired $t$-test for all pairs of the selection policies over the 50 random trials.
Below, we report the best-performing selection method and indicate statistically indistinguishable selection methods (with a $p$-value > 0.05) in parentheses, if any.

\paragraph{Results.}
The results are summarized in Table~\ref{tab:selection}.
Remarkably, for all cases, the proposed method \texttt{PUB} performs the best, or is statistically indistinguishable from the best.
This demonstrates that \texttt{PUB} is not only statistically valid, but also perform empirically well.
This corroborates the benefit of variance-adaptive guarantee.




%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{table}[t]\vspace{-.5em}
  \centering
  \renewcommand{\arraystretch}{1.2} %
  \scalebox{0.85}{
  \begin{tabular}{c|ccc|ccc|ccc}
    \toprule
%
    Dataset & \multicolumn{3}{c|}{PenDigits} & \multicolumn{3}{c|}{SatImage} & \multicolumn{3}{c}{JPVowel} \\
    \midrule
    Size & 0.01 & 0.1 & 1 & 0.01 & 0.1 & 1 & 0.01 & 0.1 & 1 \\
    \midrule
    \texttt{LS} &           34.93 & \textbf{\underline{23.60}} & 3.02    & 30.88 & 26.26 & \textbf{17.84} & 40.60 & \textbf{27.30} & \textbf{0.87} \\
    \texttt{EB}  &           41.34 & \textbf{22.67} & \textbf{3.77}       & 33.76 & 26.79 & \textbf{17.68} & \textbf{\underline{44.58}} & \textbf{27.84} & \textbf{\underline{1.59}} \\
    \texttt{PUB}  (ours) & \textbf{\underline{44.02}} & \textbf{22.74} & \textbf{\underline{4.01}} & \textbf{\underline{36.14}} & \textbf{\underline{28.16}} & \textbf{16.35} & \textbf{44.51} & \textbf{\underline{28.59}} & \textbf{1.40} \\
    \bottomrule
  \end{tabular}
  }
  \caption{
    Off-policy selection experiment. Size is the fraction the data used for training. For each column, the best is marked with underline and those who do not pass paired t-test with the best at significance level 0.05  are boldfaced. PUB is either the best or indistinguishable from the best.
    }
  \label{tab:selection}
\end{table}

\section{Related Work}
\label{sec:related}

Since the work of \citet{swaminathan15batch}, there have been numerous studies on off-policy contextual bandits and reinforcement learning.
An exhaustive literature review with a detailed comparison would warrant a separate survey paper.
Here, we focus on categorizing representative works from a theoretical perspective based on the level of guarantees they provide.

%
%
%

\paragraph{No Finite-Time Correctness Guarantee.}
Methods in this category do not provide a provable guarantee of correctness for the proposed confidence bound on the performance of a given policy under evaluation. Notable examples include the empirical likelihood approach~\citep{karampatziakis20empirical} and the self-normalized estimator~\citep{swaminathan15batch}. These lack explicit finite-time correctness guarantees, let alone sample efficiency guarantees. Moreover, coverage violation of \citet{karampatziakis20empirical} was empirically observed in \citet[Fig.~3]{kuzborskij21confident}.

\paragraph{Finite-Time Correctness Without Sample Efficiency Guarantee.}
Several approaches only come with a finite-sample correctness guarantee of the proposed confidence bound, but without a convergence rate guarantee, and consequently with no offline regret guarantee.
This includes the seminal work of \citet{london19bayesian} leveraging PAC-Bayesian bounds, exponential weighting~\citep{aouali23exponential}, empirical Bernstein style bound~\citep{sakhi23pac}, Efron-Stein semi-empirical bound for the self-normalized importance weight~\citep{kuzborskij19efron,kuzborskij21confident}, and betting-based bounds~\citep{karampatziakis21off,Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022}.

\paragraph{Sample Efficiency Guarantee Under Bounded Probability Ratios.}
Including many works mentioned above, several works have assumed a finite upper bound on the weights $w_{1:n}^\pi$.
%
Many works mentioned above make this assumption.
%
Of those that provide sample efficiency guarantees, the following works either assume bounded weight or their guarantees become vacuous when the weight is unbounded: \citet[Corollary 4.3]{jin22policy}, \citet{wang24oracle}, and \citet{zenati23sequential}. 

%


\paragraph{Sample Efficiency Guarantee Without Bounded Probability Ratios.}
Only recently have methods with sample efficiency guarantees that remain valid without the bounded probability ratio assumption been proposed. These methods allow the behavior policy to assign arbitrarily small probabilities to certain actions.
While such bounds can still be vacuous in the worst case, they may remain meaningful even when $\piref(a\mid x)$ approaches zero, depending on the distribution of the context $x$ and the reward function.
Early studies in this direction established sample efficiency guarantees that depend on empirical quantities, such as those in \citet[Theorem 4.1]{jin22policy}. However, these guarantees are challenging to interpret and compare with other bounds, as they depend on the specific randomness in the bound’s construction.
In a seminal work, \citet{gabbianelli24importance} provided the first deterministic sample efficiency bound, which was later improved by \citet{sakhi24logarithmic}.
Our work falls into this category, achieving the strongest sample efficiency guarantees for selection and evaluation while matching the bound of \citet{sakhi24logarithmic} for learning.


\section{Conclusion}
In this paper, we have established new state-of-the-art bounds for off-policy problems.
Our work opens up several interesting research directions.
First, since we have developed a selection method that adapts to the variance of the IW estimator, we can now aim to characterize the optimal rate for offline regret.
Second, an immediate next step is to investigate whether similar or stronger bounds can be achieved for the doubly robust method.
Third, a natural follow-up question is whether we can identify an objective that is favorable for learning (i.e., optimization) while attaining the same rate as the offline selection method.
Last but not least, our lower confidence bound for nonnegative random variables is novel and, to our knowledge, provides the strongest guarantee in the literature---particularly in terms of convergence, as it requires only the existence of variance rather than higher-order moments. Exploring potential applications or other learning-theoretic problems where this bound could lead to improved guarantees would be an interesting avenue for future work.




%
%
%
\bibliography{ref}


\clearpage
\appendix

\addtocontents{toc}{\protect\StartAppendixEntries}
\listofatoc
\vspace{1em}
\hrule

%
%
%
%
%
%
%
%
%
%
%
%
%








\section{Deferred Discussions and Proofs for Off-Policy Selection}

\subsection{Implementation of Proposed LCBs}
\label{app:sec:implementation_lcbs}

In this section, we discuss the implementation of the proposed LCBs, \ie UP-LCB and pCRP$^\star$-LCB, in detail and their complexity; see Table~\ref{tab:complexity} for a summary.
We make a distinction of the \emph{online complexity} (when constructing LCB at each time step) and \emph{offline complexity} (when constructing LCB only for the last time step).
We also provide a computationally efficient version, LBUP-LCB, based on a similar trick of \citet{Ryu--Bhatt2024}.


\begin{table}[htb]
    \centering
    \caption{Comparison of complexity of different LCBs. 
    We present the time complexity to compute a LCB for each time step in ``Online Complexity'' for a length-$n$ trajectory, and that to compute a LCB only for the last step with $n$ samples in ``Offline Complexity''.
    Here, $M$ denotes the maximum time complexity for a root finding procedure. For example, if we use a bisect algorithm with target precision $\eps$, $M=O(\ln \fr{1}{\eps})$.}
    %
    \begin{tabular}{c c c c}
    \toprule
    Algorithm & Online complexity & Offline complexity & Rate guarantee\\
    \midrule
    UP-LCB & $\Th(Mn^2)$ & $\Th(n^2 + Mn)$ & Yes\\
    pCRP$^\star$-LCB & $\Th(M^2n^2)$ & $\Th(M^2n)$ & Yes\\
    \midrule
    LBUP-LCB & $\Th(Mn)$ & $\Th(n+M)$ & No \\
    \bottomrule
    \end{tabular}
    \label{tab:complexity}
\end{table}

\newcommand{\ct}{\til{c}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\rhob}{\boldsymbol{\rho}}
\newcommand{\Natural}{\mathbb{N}}
\newcommand{\Real}{\mathbb{R}}

\subsubsection{Computing UP-LCB with Dynamic Programming}
As alluded to earlier, we can compute the exact UP wealth using dynamic programming. 
A similar statement was proved by \citet{Ryu--Bhatt2024} in the context of confidence sequences for bounded stochastic processes, and the original dynamic programming argument for UP can be found in \citep{Cover--Ordentlich1996}.

Recall that the wealth of UP is defined as a mixture wealth of CRPs:
\begin{align}
%
\nonumber
\wealthup_t(y_{1:t};\nu)
\defeq \int_0^1 \wealthcrp{b}_t(y_{1:t};\nu) w(b) db.
\end{align}
The following proposition holds for any weight distribution $w(b)$.
\begin{proposition}
\label{thm:exact_up}
The wealth of UP can be computed as
\begin{align}
\wealthup_t(y_{1:t};\nu)
=\sum_{k=0}^t 
\fr{1}{\nu^k}\psi_t^{w}(k)
y^{(t)}(k),
\label{eq:up_wealth_expression}
\end{align}
where we 
%
define
\begin{align}
\psi_t^{w}(k) &\defeq \int_0^1 b^k(1-b)^{t-k}\diff w(b),
\label{eq:mixture_wealth_individual}\\
y^{(t)}(k)&\defeq \sum_{x_{1:t}\in\{0,1\}^t\suchthat k(x_{1:t})=k} \prod_{i=1}^t y_i^{x_i},\label{eq:seq_k_statistics}
\end{align}
and $k(x_{1:t})\defeq \sum_{i=1}^t x_i$.
Furthermore, for each $t\ge 1$, we have
\begin{align}
y^{(t)}(k)=
\begin{cases}
y^{(t-1)}(0) & \text{if }k=0,\\
y_t y^{(t-1)}(k-1) + y^{(t-1)}(k) & \text{if }1\le k\le t-1\\
y_t y^{(t-1)}(t-1) & \text{if }k=t.
\end{cases}
\label{eq:up_recursive_update}
\end{align}
\end{proposition}

\begin{proof}
We first note that we can write the cumulative wealth of any constant bettor $b$ as
\begin{align}
\wealthcrp{b}_t(y_{1:t};\nu)
%
&=\sum_{x_{1:t}\in\{0,1\}^t} \prod_{i=1}^t \Bigl(\fr{y_i b}{\nu}\Bigr)^{x_i} (1-b)^{1-x_i},
\label{eq:distributive_law}
\end{align}
where the equality follows by the distributive law.
To see Eq.~\eqref{eq:up_wealth_expression}, we first note that continuing from Eq.~\eqref{eq:distributive_law}, we have
\begin{align}
\wealthcrp{b}_t(y_{1:t};\nu)
&= \sum_{k=0}^t 
\nu^{-k} b^k(1-b)^{t-k} 
\sum_{x_{1:t}\in\{0,1\}^t\suchthat k(x_{1:t})=k} \prod_{i=1}^t  y_i^{x_i}\nonumber\\
&= \sum_{k=0}^t 
\nu^{-k} b^k(1-b)^{t-k} 
y^{(t)}(k),
\label{eq:wealth_const_betting}
\end{align}
and thus integrating over $b$ with respect to $w(b)$ leads to \eqref{eq:up_wealth_expression}.
The recursive update  in Eq.~\eqref{eq:up_recursive_update} is straightforward.
\end{proof}

This proposition shows that, the recursive update takes $O(t)$ at time step $t$, and thus the online complexity is $O(Mn^2)$.
Even for the offline setting where we only need to compute the LCB with the entire samples once, we need to run the recursive update in Eq.~\eqref{eq:up_recursive_update} for each $t=1,\ldots,n$ and the wealth evaluation~\eqref{eq:up_wealth_expression} takes $O(t)$, which leads to the complexity $O(n^2+Mn)$.



\subsubsection{Computing pCRP\texorpdfstring{$^\star$}{*} Wealth}

Recall that the pCRP$^\star$-LCB in Eq.~\eqref{eq:pcrplcb}
is defined by the (unique) root $\nu$ of the equation
\[
\wealthpcrp(Y_{1:n};\nu) 
= \frac{1}{\sqrt{\pi(n+1)}}
\sup_{b\in(0,1)}\wealthcrp{b}(Y_{1:n};\nu)
= \fr{1}{\dt}.
\]
Here, for each $\nu$, the maximizer $b$ can be found by finding the root of the derivative
$\fr{d}{db}\wealthcrp{b}(Y_{1:n};\nu)=0$.
Hence, we can numerically find the root by the bisect algorithm over both $\nu>0$ and $b\in(0,1)$. 
Note that the CRP wealth evaluation takes $O(t)$ at time step $t$, and thus computing the LCB takes $O(M^2t)$. Therefore, the online and offline complexities are $O(M^2n^2)$ and $O(M^2n)$, respectively.

\newcommand{\apporder}{r}

\subsubsection{Lower-Bound Universal Portfolio: A Fast Alternative}
\label{app:sec:lbup}
Adapting the development of \citet{Ryu--Bhatt2024} for $[0,1]$-valued random processes, here we present a fast alternative approach that tightly approximates the UP wealth. 
The idea is to directly compute a mixture of very tight lower bounds on the CRP wealths.
The mixture of lower bounds can be computed efficiently by numerical integration, by viewing the lower bound as an (unnormalized) exponential family distribution.
While there is no guarantee on the approximation error, the resulting bound is empirically a very good proxy to the UP-LCB, even better than the pCRP$^\star$-UCB, when sample size is sufficiently large; see Figure~\ref{fig:ex_betting_lcb_lbup}.

\paragraph{Tight Lower-Bound on CRP Wealth.}
We start with the following lemma from \citep{Ryu--Bhatt2024}.
We note that \citep{sakhi24logarithmic} also proved a similar statement (see Lemma~10 therein), but the domain is restricted to $\Real_+$ and thus not sufficient for our purpose.

\begin{lemma}[{\citep[Lemma~25]{Ryu--Bhatt2024}}]
\label{lem:monotone}
For an integer $\ell\ge 1$, if we define
\[
f_\ell(t)\defeq
\begin{cases}
\displaystyle\frac{\ln(1+t)-\sum_{k=1}^{\ell-1}-\frac{(-t)^k}{k}}{\frac{(-t)^\ell}{\ell}} & \text{if $t>-1$ and $t\neq 0$,}\\
-1 &\text{if $t=0$},
\end{cases}
\]
then $t\mapsto f_\ell(t)$ is continuous and strictly increasing over $(-1,\infty)$.
\end{lemma}

We can then prove the following lower bound.
As noted in \citep{Ryu--Bhatt2024}, the positive integer $\apporder\ge1$ in the statement can be understood as the approximation order.
Empirical results show that a higher order $\apporder$ results in a tighter lower bound, but we do not have a formal proof.
\begin{lemma}
\label{lem:generalized_lower_bound}
For any $\apporder\in \Natural$, $b\in[0,1]$, and $z\ge 0$, we have
%
\begin{align*}
\ln(1-b +bz)
&\ge \sum_{k=1}^{2\apporder-1} \frac{b^k}{k} \{(1-z)^{2\apporder}
    -(1-z)^k\} + (1-z)^{2\apporder}\ln (1-b).
%
%
%
%
%
%
\end{align*}
\end{lemma}
\begin{proof}
Note that the right hand side diverges to $-\infty$ and thus the inequality becomes vacuously true for $b=1$.
We now assume that $b<1$, which ensures $b(z-1)\ge -b>-1$.
Hence, from Lemma~\ref{lem:monotone},
we have $f_{2r}(b(z-1)) \ge f_{2r}(-b)$, which is equivalent to
\begin{align*}
\frac{\ln(1+b(z-1))-\sum_{k=1}^{2r-1}-\frac{(-b(z-1))^k}{k}}{\frac{(-b(z-1))^{2r}}{2r}}
\ge 
\frac{\ln(1-b)-\sum_{k=1}^{2r-1}-\frac{(-b)^k}{k}}{\frac{(-b)^{2r}}{2r}}.
\end{align*}
Rearranging the terms concludes the proof.
\end{proof}

The lower bound in the statement can be understood as the logarithm of an unnormalized exponential family distribution over $z$, i.e.,
\begin{align}
\ln(1-b + bz)
&\ge 
\ln \psi_r(z|b),
\label{eq:long_eq2}
\end{align}
where $\psi_r(z|b)$ is an unnormalized exponential family distribution defined as
\begin{align}
\psi_r(z|b)
&\defeq\exp(\boldsymbol{\th}_r(b)^\intercal\bT_r(z)).
\end{align}
Here, $\boldsymbol{\th}_r(b)$ is the natural parameter defined as
\begin{align*}
\boldsymbol{\th}_r(b)
\defeq \begin{bmatrix}
b\\
{b^2}/{2}\\
\vdots\\
{b^{2r-1}}/{(2r-1)}\\
\ln (1-b)
\end{bmatrix},
\end{align*}
and
$\bT_r(z)$ is the sufficient statistics defined as 
\begin{align*}
\bT_r(z) \defeq \begin{bmatrix}
(1-z)^{2r} - (1-z)\\
(1-z)^{2r} - (1-z)^2\\
\vdots\\
(1-z)^{2r} - (1-z)^{2r-1}\\
(1-z)^{2r}
\end{bmatrix}
= \sum_{j=0}^{2r} \binom{2r}{j} (-1)^j z^j\begin{bmatrix}
1\\
1\\
\vdots\\
1\\
1
\end{bmatrix}
- \begin{bmatrix}
\displaystyle
\sum_{j=0}^1 \binom{1}{j} (-1)^j z^j\\
\displaystyle\sum_{j=0}^2 \binom{2}{j} (-1)^j z^j\\
\vdots\\
\displaystyle\sum_{j=0}^{2r-1} \binom{2r-1}{j} (-1)^j z^j\\
0
\end{bmatrix}.
\end{align*}
From this definition, it is easy to check that 
\begin{align*}
\prod_{t=1}^n \psi_r(z_t|b)
= \exp\Bigl(
\boldsymbol{\th}_r(b)^\intercal \sum_{t=1}^n \bT_r(z_t)
\Bigr),
\end{align*}
and $\sum_{t=1}^n \bT_r(z_t)$ is a function of $(s_j(z_{1:n}))_{j=0}^{2r}$, where we denote the (unnormalized) empirical $j$-th moment for $j\in\Natural$ by
\[
s_j(z_{1:n})
\defeq \sum_{t=1}^n z_t^j.
\]
This implies that the lower bound can be readily computed from the empirical moments, unlike the CRP wealth or UP wealth that requires storing the entire history $z_{1:n}$.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\newcommand{\alphab}{\boldsymbol{\alpha}}
\paragraph{Mixture of Lower-Bounds on CRP Wealths.}
For computational tractability, we now consider a mixture weight in the form of the \emph{conjugate prior} of $\psi_r(z|b)$, defined as
\begin{align}
w_r(b;\alphab)
\label{eq:general_prior}
&\defeq \frac{\exp(\boldsymbol{\th}_r(b)^\intercal\alphab)}{Z_r(\alphab)}.
\end{align}
Here, $\alphab\in\Real^{2r}$ is a hyperparamter of the conjugate prior, and
\[
Z_r(\alphab)\defeq \int_0^1 \exp(\boldsymbol{\th}_r(b)^\intercal\alphab) \diff b
\] 
is the \emph{partition function}.
By the following theorem, computing the mixture of the CRP wealths with respect to this conjugate prior only requires to compute the normalization constant efficiently:
\begin{theorem}
\label{thm:lbup}
%
%
Let $r\ge 1$.
For any $y_{1:t}\in\Real_{\ge 0}^t$ and $\nu>0$,
we have
\begin{align}
%
%
%
\int \wealthcrp{b}_n(y_{1:n};\nu) w_r(b;\alphab) \diff b
\ge\frac{
Z_r(\sum_{t=1}^n \bT_r(\fr{y_t}{\nu})+\alphab)}
{Z_r(\alphab)}.
\label{eq:lbup_wealth}
\end{align}
\end{theorem}
In the special case of $r=1$, we can compute $Z_1(\alphab)$ in an analytical form if $\alpha_1\ge 0$:
\begin{align}
Z_1(\alphab)= e^{\alpha_1}\alpha_1^{-\alpha_2-1}\gamma(\alpha_2+1,\alpha_1).
\label{eq:normal_cont_special}
\end{align} 
Here, $\gamma(s,x)\defeq \int_0^x t^{s-1}e^{-t}\diff t$ for $s>0$ denotes the lower incomplete gamma function.
For $r>1$, we need a numerical integration library to compute the partition function.

We note that the conjugate prior is not same as the beta prior of Cover's UP in general. 
In particular, however, if we set $\alphab=\boldsymbol{0}$, then the prior $w_r(b;\alphab)$ boils down the uniform distribution over $[0,1]$, and the resulting mixture wealth lower bound can be viewed as a lower bound to Cover's UP with the uniform prior (\ie $\mathsf{Beta}(1,1)$ prior).
Following \citet{Ryu--Bhatt2024}, we refer to the resulting wealth lower bound the \emph{lower-bound UP wealth of approximation order $r$}, or LBUP($r$) in short. We call the resulting LCB LBUP($r$)-LCB.

\paragraph{Implementation and Complexity.}
We can numerically compute the LBUP($r$)-LCB using the bisect method
%
Since we only need to keep track of the $2r$ empirical moments $(s_j(y_{1:t}))_{j=1}^{2\apporder}$, the storage complexity is $O(\apporder)$ and per-step time complexity for function evaluation is $O(\apporder)$ at any time step.
Consequently, for computing the LBUP($r$)-LCB, the online complexity is $O(Mnr)$ and the offline complexity is $O(n+Mr)$.

\paragraph{Simulation.}
We simulated the UP-LCB, pCRP$^\star$-LCB, and LBUP($r$)-LCB for $r\in\{1,2,3\}$ for the same synthetic setting used in Figure~\ref{fig:ex_betting_lcb}.
We generated $n=10^4$ i.i.d. Gamma random variables with shape and scale parameters of 6 and $1/8$, respectively, and thus of mean $3/4$.
The results are summarized in Figures~\ref{fig:ex_betting_lcb_lbup},~\ref{fig:lbup_relerr},~and~\ref{fig:lbup_time}.
In particular, we remark that LBUP($r$)-LCBs (especially with $r\ge 2$) very closely approximate the UP-LCB (Figure~\ref{fig:lbup_relerr}) better than pCRP$^\star$ in a large sample regime, exhibiting better scalability over $n$ (Figure~\ref{fig:lbup_time}).
We note, however, that we do not have a formal guarantee for the closeness of LBUP-LCB to UP-LCB, and LBUP-LCBs require some burn-in samples ($\sim 10^2$ samples in this example) to become sufficiently close to UP-LCB.
For an off-policy inference setting with large-scale data, practitioners may consider using LBUP-LCB if the sample trajectory is sufficiently long, and otherwise may prefer pCRP$^\star$ for guaranteed performance with moderate complexity.



\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{figs/ex_up_vs_lbup.pdf}
    \vspace{-1em}
    \caption{Example of the evolution of cumulative wealths achieved by Cover's UP in Eq.~\eqref{eq:wealth_up}, and the penalized best CRP wealth in Eq.~\eqref{eq:up_vs_pcrp}, and the lower-bound universal portfolio in Appendix~\ref{app:sec:lbup}.
    The setting is exactly same as Figure~\ref{fig:ex_betting_lcb}, except that we use larger time steps and depict with a different range for $\nu$.
    }\label{fig:ex_betting_lcb_lbup}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/ex_lbup_relerr.pdf}
    \caption{Convergence UP-LCB, pCRP$^\star$-LCB, and LBUP($r$)-LCB for $r\in\{1,2,3\}$. The left panel and right panel present the relative convergence of each LCB with respect to true mean and UP-LCB, respectively.}
    \label{fig:lbup_relerr}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{figs/ex_lbup_time.pdf}
    \caption{Online and offline time complexity for computing UP-LCB, pCRP$^\star$-LCB, and LBUP($r$)-LCB for $r\in\{1,2,3\}$.}
    \label{fig:lbup_time}
\end{figure}

\begin{comment}
This is wrong, as the left hand side is expectation of log wealth, not the log of expected wealth.
\begin{theorem}
Define $s_j(y_{1:t})
\defeq \sum_{i=1}^t y_i^j$ for $y_{1:t}\in \Real_+^t$ for $j=1,\ldots,2\apporder$.
Define $c_k \defeq \fr{B(k+\fr12,\fr12)}{B(\fr12,\fr12)}$ for $k\in\Natural$ and $d\defeq \psi(\fr12)-\psi(1)=-2\ln 2$, where $\psi(x)$ denotes the digamma function.
For any $r\in\Natural$ and any $y_{1:n}\in\Real_+^n$, we have 
\begin{align*}
\ln \wealthup_t(y_{1:t};\nu)
&\ge \sum_{j=0}^{2r}\biggl\{
\Bigl(
\sum_{k=1}^{2r-1} \fr{c_k}{k} + d\Bigr) \binom{2r}{j} 
- \sum_{k=j\vee 1}^{2r-1}\fr{c_k}{k} \binom{k}{j}
\biggr\}  
\fr{s_j(y_{1:t})}{(-\nu)^j}.
\end{align*}
\end{theorem}
\begin{proof}
Plugging in $z\gets \fr{y_i}{\nu}$ into the inequality in Lemma~\ref{lem:generalized_lower_bound}, summing over $j=1,\ldots,t$, and and integrating over $b\in(0,1)$ with respect to the density $w(b)=\mathsf{Beta}(b;\alpha,\beta)$, we have
\begin{align*}
\ln \wealthup_t(y_{1:t};\nu)
&\ge
\sum_{j=0}^{2r}\biggl\{
\Bigl(
\sum_{k=1}^{2r-1} \fr{\E_w[X^k]}{k} + \E_w[\ln(1-X)] \Bigr) \binom{2r}{j} 
- \sum_{k=j\vee 1}^{2r-1}\fr{\E_w[X^k]}{k} \binom{k}{j}
\biggr\}  
\fr{s_j(y_{1:t})}{(-\nu)^j},
\end{align*}
where $X\sim \mathsf{Beta}(\alpha,\beta)$.
The claim follows by noting that $\E_w[X^k]=\fr{B(k+\alpha,\beta)}{B(\alpha,\beta)}$ and $\E_w[\ln(1-X)]=\psi(\beta)-\psi(\alpha+\beta)$.
\end{proof}
\end{comment} 



\subsection{Proof of Theorem~\ref{thm:up_lcb_rate} (Convergence Rate Analysis for UP-LCB and pCRP\texorpdfstring{$^\star$}{*}-LCB)}

We restate Theorem~\ref{thm:up_lcb_rate} in two separate statements, and prove them separately. 
Technical lemmas are deferred to Appendix~\ref{app:sec:technical}.

\begin{theorem}[First part of Theorem~\ref{thm:up_lcb_rate}]
\label{thm:up_lcb_rate_1}
Let $n\ge 1$ and
define
$\blue{F_n^{(\dt)}} \defeq \ln\fr{\sqrt{\pi(n+1)}}{\dt^2}$.
%
%
%
Then, with probability $\ge 1-2\dt$,
\[
0
\le \mu - \lcbup{n}{\dt}(Y_{1:n})
\le \mu - \lcbpcrp{n}{\dt}(Y_{1:n})
\le \sqrt{\fr{48\sigma^2}{n} F_n^{(\dt)}} \vee \fr{12\mu}{n}F_n^{(\dt)}.
\]
\end{theorem}


Recall the definition of the smoothed variance
\[
\WW_b[Y]\defeq \EE\Bigl[\fr{(Y-\EE[Y])^2}{1+\fr{b}{\EE[Y]}(Y-\EE[Y])}\Bigr].
\]
\begin{theorem}[A Full Version of Second Part of Theorem~\ref{thm:up_lcb_rate}]
\label{thm:up_lcb_rate_2}
Pick any $\eps\in(0,\fr12]$.
Suppose that $(Y_t)_{t=1}^\infty$ is an independent identically distributed (i.i.d.), nonnegative random process, with $\mu\defeq\EE[Y_1]$ and $\sigma^2\defeq \VV[Y_1]$.
Let $b_n^{(\dt)}\defeq \sqrt{\fr{\mu^2}{2\sigma^2}\fr{F_n^{(\dt)}}{n}}$.
With probability $\ge 1-2\dt$, for any 
\[
n\ge \Bigl(12\Bigl(1+\fr{4}{\eps}\Bigr) \vee 48\Bigl(1+\fr{4}{\eps}\Bigr)^2\fr{\mu^2}{\sigma^2}\Bigr) F_n^{(\dt)},
\]
%
we have
\begin{align}
0\le \mu-\lcbup{n}{}(Y_{1:n})
&\le \mu-\lcbpcrp{n}{}(Y_{1:n})\nonumber\\
&\le 
\inf_{b\in(0,1-\eps]}
\Bigl\{
\frac{b}{\mu} \WW_b[Y_1]
+\fr{\mu}{b}
\fr{F_n^{(\dt)}}{n}
\Bigr\}\label{eq:refined_lcb1}\\
&\le 2\sqrt{
\frac{F_n^{(\dt)}}{n}
\WW_{b_n^{(\dt)}}[Y_1]
}.\label{eq:refined_lcb2}
\end{align}
\end{theorem}



\subsubsection{Proof of Theorem~\ref{thm:up_lcb_rate_1}}

Theorem~\ref{thm:up_lcb_rate_1} is an immediate consequence of Lemma~\ref{lem:lcb} and~\ref{lem:ucb} below.
%
\begin{lemma}\label{lem:lcb}
With probability $\ge 1-\dt$, $\mu\ge \lcbup{n}{\dt}(Y_{1:n}) \ge \lcbpcrp{n}{\dt}(Y_{1:n})$ for any $n\ge 1$.
\end{lemma}
\begin{proof}
Since $(\wealthup_t(Y_{1:t};\mu))_{t=1}^n$ is a nonnegative martingale,
by Ville's inequality, we have
\begin{align*}
\PP\Bigl(
\sup_{t\ge 1}\wealthup_t(Y_{1:t};\mu)\ge \frac{1}{\dt}
\Bigr)
\le \dt,
\end{align*}
which concludes the proof for the first inequality.
The second inequality is trivial by~\ref{eq:up_vs_pcrp}
\end{proof}

\begin{lemma}\label{lem:ucb}
Let
\begin{align*}
G_n^{(\dt)}\defeq \sqrt{\fr{12\sigma^2}{n} F_n^{(\dt)}} \vee \fr{6\mu}{n}F_n^{(\dt)}.
\end{align*}
With probability $\ge 1-\dt$,
\[
\mu
\le \lcbpcrp{n}{\dt}(Y_{1:n}) + 2G_n^{(\dt)} 
\le \lcbup{n}{\dt}(Y_{1:n}) + 2G_n^{(\dt)}
\]
for any $n\ge 1$. 
\end{lemma}

\begin{proof}
%
%
%
It suffices to prove the inequality for $\lcbpcrp{n}{\dt}(Y_{1:n})$ due to~\eqref{eq:up_vs_pcrp}.
We first note that, if $n < 12F_n^{(\dt)}(1\vee \frac{4\sigma^2}{\mu^2})$, 
we deterministically have $G_n^{(\dt)}> \fr{\mu}{2}$, which implies that
\begin{align*}
  \mu - \lcbpcrp{n}{\dt}(Y_{1:n})\le \mu < 2 G_n^{(\dt)},
\end{align*}
which proves the claim.

Hence, hereafter, we thus assume $n \ge 12F_n^{(\dt)}(1\vee \frac{4\sigma^2}{\mu^2})$ and show a slightly stronger bound
\begin{align}
\mu-\lcbpcrp{n}{\dt}(Y_{1:n}) \le G_n^{(\dt)}.
\label{eq:ub_claim}
\end{align}
In this regime, if we define
\begin{align*}
\nu_o\defeq\mu-G_n^{(\dt)},
\end{align*}
we have $\nu_o>0$, since $G_n^{(\dt)}\le\frac{\mu}{2}<\mu$.

%
%
%
%
%
%
Recall that $\nu\mapsto \wealthpcrp_n(Y_{1:n};\nu)$ is monotonically decreasing and $\lcbpcrp{n}{\dt}(Y_{1:n})$ is the unique root of $\wealthpcrp_n(Y_{1:n};\nu) =\frac{1}{\dt}$.
Therefore, to prove the desired claim in Eq.~\eqref{eq:ub_claim}, it suffices to show that 
%
\begin{align}
    \wealthpcrp_n(Y_{1:n};\nu_o) 
    >\frac{1}{\dt},
    \label{eq:ub_suff_cond}
\end{align}
since it implies that $\nu_o<\lcbpcrp{n}{\dt}(Y_{1:n})$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
By the definition of $\wealthpcrp_n$, it suffices to show that there exists $b^*\in(0,1)$ such that
\begin{align}
\label{eq:crp_wealth_larger_than_threshold}
\frac{1}{n}\ln\wealthcrp{b^*}_n(Y_{1:n};\nu_o) > \frac{1}{n}\ln\frac{\sqrt{\pi(n+1)}}{\dt}.
\end{align}
We will construct such $b^*$ below. 

Define
\begin{align*}
A\defeq \fr{\mu-\nu_o}{\nu_o} \quad\text{and}\quad 
B\defeq \fr{\sigma^2 + (\mu-\nu_o)^{2}}{\nu_o^2},
\end{align*}
and set
\[
b^*\defeq \fr{A}{2(A+2B)}\le \fr12.\footnote{The optimal choice of $b$ is $1-\sqrt{\frac{B}{A+B}}$, but the rate does not change in the current analysis.}
\]
By applying Lemma~\ref{lem:240903_concentration} with $\nu_o=\mu-G_n^{(\dt)}$ and $b^*$ chosen above, we have:
with probability $\ge 1-\dt$, for any $n\ge 1$, for any $\pi^*\in\Pi$,
\begin{align*}
\frac{1}{n}\ln\wealthcrp{b^*}_n(Y_{1:n};\nu_o)
&\ge
b^*\frac{\mu-\nu_o}{\nu_o} 
-\fr{(b^*)^2}{1-b^*} \fr{\sigma^2 + (\mu-\nu_o)^2}{\nu_o^2}
-\frac{1}{n}\ln\fr{1}{\dt}.\\
&=b^* A - \fr{(b^*)^2}{1-b^*} B - \frac{1}{n}\ln\fr{1}{\dt}
\\&= \fr{A^2}{2(A+2B)} - \frac{1}{n} \ln \fr{1}{\dt}.
\end{align*}
The last equality follows from the choice of $b^*$.

To show Eq.~\eqref{eq:crp_wealth_larger_than_threshold}, it remains to show that
\begin{align*}
\fr{A^2}{A+2B}\ge \frac{2F_n^{(\dt)}}{n}.
\end{align*}
We prove by contradiction:
if $\fr{A^2}{A+2B}< \frac{2F_n^{(\dt)}}{n}$, or equivalently
\begin{align}\label{eq:contradiction}
\frac{(\mu-\nu_o)^2}{2(\sigma^2 + (\mu-\nu_o)^2) + (\mu-\nu_o)\nu_o} < \frac{2F_n^{(\dt)}}{n},
\end{align}
then $G_n^{(\dt)}=\mu-\nu_o<G_n^{(\dt)}$.
We consider the following two cases separately.
  
  %
  %
  %
  %
  %
  %
  %
  %
  \paragraph{Case 1.} $\sigma^2 + (\mu-\nu_o)^2 \ge (\mu-\nu_o)\nu_o$.\\
  In this case, from Eq.~\eqref{eq:contradiction}, we have
\begin{align*}
\frac{2F_n^{(\dt)}}{n} > \frac{(\mu-\nu_o)^2}{3(\sigma^2 + (\mu-\nu_o)^2)},
\end{align*}
which implies that
  \begin{align*}
    (G_n^{(\dt)})^2=(\mu-\nu_o)^2
    < \frac{\frac{6F_n^{(\dt)}}{n}}{1-\frac{6F_n^{(\dt)}}{n}}\sigma^2 
    \le \frac{12\sigma^2 F_n^{(\dt)}}{n},
  \end{align*}
  which is a contradiction.
  Here, the last inequality follows from the assumption $n\ge 12F_n$.
  
  \paragraph{Case 2.} $\sigma^2 + (\mu-\nu_o)^2 < (\mu-\nu_o)\nu_o$.\\
  In this case, from Eq.~\eqref{eq:contradiction}, we have
\begin{align*}
\frac{2F_n^{(\dt)}}{n} > \frac{(\mu-\nu_o)^2}{3(\mu-\nu_o)\nu_o}=\frac{\mu-\nu_o}{3\nu_o},
\end{align*}
which implies that
\begin{align*}
G_n^{(\dt)}=\mu-\nu_o
< \frac{6\nu_o F_n^{(\dt)}}{n}
< \frac{6\mu F_n^{(\dt)}}{n},
\end{align*}
which is a contradiction.
Here, the last inequality follows since $\nu_o= \mu-G_n^{(\dt)}<\mu$.
This conclude the proof.
\end{proof}



\subsubsection{Proof of Theorem~\ref{thm:up_lcb_rate_2}}
%
Let $\hv\defeq \hv_{\mathsf{UP}}^{(\dt)}(Y_{1:n})$.
Note that 
$\wealthup_n(Y_{1:n};\hv)= \frac{1}{\dt}$ by the definition of UP-LCB.
%
Let $\blue{Z_1}\defeq \fr{Y_1}{\hv}$.
Since $\wealthup_n(Y_{1:n};\nu)\ge \wealthpcrp_n(Y_{1:n};\nu)$ from the regret guarantee of Cover's UP in Eq.~\eqref{eq:up_vs_pcrp}, by the definition of $\wealthpcrp_n(Y_{1:n};\nu)$, we have, for any $b\in(0,1)$, 
\begin{align*}
\fr{1}{n}\ln\fr{\sqrt{\pi(n+1)}}{\dt}
&\ge \fr{1}{n}\ln\wealthcrp{b}_n(Y_{1:n};\hv)\\
&=\fr{1}{n}\sum_{t=1}^n \ln\Bigl(
1-b+b\fr{Y_t}{\hv}
\Bigr)\\
&\ge b(\EE[Z_1]-1) - \EE\Bigl[\fr{b^2(Z_1-1)^2}{1+b(Z_1-1)}\Bigr] - \fr{1}{n}\ln\fr{1}{\dt},
\end{align*}
%
where the last inequality holds with probability $\ge 1-\dt$ by Lemma~\ref{lem:240903_concentration_new}.
We define $\blue{\Delta}\defeq \mu-\hv$, and we assume that $\Delta \ge 0$, which happens with probability $\ge 1-\dt$.
Note that $\fr{\Delta}{\hv} = \fr{\mu}{\hv}-1=\EE[Z_1]-1\ge 0$.
Rearranging the inequality, we then have
\begin{align}
\Delta
&\le \hv\Bigl(
b\EE\Bigl[\fr{(Z_1-1)^2}{1+b(Z_1-1)}\Bigr] + \fr{1}{b} \fr{F_n^{(\dt)}}{n}
\Bigr).
\label{eq:intermed2}
\end{align}
We bound the first term as follows:
\begin{align*}
\EE\Bigl[\fr{(Z_1-1)^2}{1+b(Z_1-1)}\Bigr]
&\le 2\EE\Bigl[\fr{(Z_1-\EE[Z_1])^2 + (\EE[Z_1]-1)^2}{1+b(Z_1-1)}\Bigr]
%
\\
&\le 2\EE\Bigl[\fr{(Z_1-\EE[Z_1])^2 }{1+b(Z_1-\EE[Z_1])}\Bigr]
+ \fr{2\Delta^2}{\hv^2}
\EE\Bigl[\fr{1}{1+b(Z_1-1)}\Bigr]
\tag{$\because \EE[Z_1]\ge 1$}
\\
&\sr{(a)}{\le} 2\EE\Bigl[\fr{(Z_1-\EE[Z_1])^2 }{1+b(Z_1-\EE[Z_1])}\Bigr]
+\fr{\Delta}{2\hv}.
\end{align*}
Here, we show that $(a)$ is true given $n\ge (\fr{12}{c} \vee \fr{48\sigma^2}{c^2\mu^2}) F_n^{(\dt)}$ for $c=\fr{\eps}{\eps+4}$ and $b\in(0,1-\eps]$.
%
%
To see this, Theorem~\ref{thm:up_lcb_rate} along with the requirement on $n$ ensures 
\[
0\le \Delta = \mu-\hv \le c\mu ~,
\]
which is equivalent to
\begin{align}
\fr{4}{\eps+4}\mu
=(1-c)\mu 
\le \hv \le \mu
\label{eq:intermed3}
\end{align}
or
\[
0\le \Delta \le \fr{c}{1-c}\hv = \fr{\eps}{4}\hv.
\]
This leads to, using $Z_1 \ge 0$ and $b \in (0,1-\eps]$,
\begin{align*}
\fr{2\Delta^2}{\hv^2}
\EE\Bigl[\fr{1}{1+b(Z_1-1)}\Bigr]
\le \fr{2\Delta^2}{\hv^2}
\fr{1}{1-b} 
\le \fr{2\Delta}{\hv^2}\fr{\Delta}{\eps}
\le \fr{2\Delta}{\hv^2}\fr{\hv}{4}
= \fr{\Delta}{2\hv}~,
\end{align*}
concluding the proof of $(a)$ above.

We now apply the upper bound of $\EE\Bigl[\fr{(Z_1-1)^2}{1+b(Z_1-1)}\Bigr]$ above to Eq.~\eqref{eq:intermed2} and solve it for $\Dt$ to obtain
\begin{align}
\Delta 
\le (2-b)\Delta
&\le 
\frac{4b}{\hv}\EE\Bigl[\fr{(Y_1-\mu)^2}{1+\fr{b}{\hv}(Y_1-\mu)}\Bigr]
+\fr{\hv}{b}\fr{F_n^{(\dt)}}{n}
\defeq h\Bigl(\fr{b}{\hv}\Bigr),
\end{align}
%
%
%
%
%
%
%
%
%
where $h(q)\defeq 4q\EE\Bigl[\fr{(Y_1-\mu)^2}{1+q(Y_1-\mu)}\Bigr]
    +\fr1q\fr{F_n^{(\dt)}}{n}$.
Taking infimum over $b\in(0,1-\eps]$,
%
\begin{align*}
\Delta
\le 
\inf_{b\in[0,1-\eps)}
h\Bigl(\fr{b}{\hv}\Bigr)
=\inf_{q\in[0,\fr{1-\eps}{\hv})} h(q)
\le \inf_{q\in[0,\fr{1-\eps}{\mu})} h(q)
= \inf_{b\in[0,{1-\eps})} h\Bigl(\fr{b}{\mu}\Bigr).
\end{align*}
%
%
%
The second inequality holds since we assume $\mu\ge \hv$.
This concludes the proof for the first inequality in Eq.~\eqref{eq:refined_lcb1}.
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

To prove the second inequality in Eq.~\eqref{eq:refined_lcb2}, we rewrite the inequality in Eq.~\eqref{eq:refined_lcb1} as
\begin{align}
\Delta
&\le 
\inf_{b\in(0,1-\eps]}
\{
f(b) + g(b)
\}
\le \inf_{b\in(0,\fr14]}
\{
f(b) + g(b)
\},
\label{eq:intermed4}
\end{align}
where $f(b)\defeq \frac{b}{\mu}\EE\Bigl[\fr{(Y_1-\mu)^2}{1+\fr{b}{\mu}(Y_1-\mu)}\Bigr]$
and $g(b)\defeq \fr{\mu}{b}
\fr{F_n^{(\dt)}}{n}$.
Note that $f(b)$ is monotonically increasing and $g(b)$ is monotonically decreasing over $b\in [0,1]$.
We now show that $f(\fr14) \ge g(\fr14)$, which implies that $f(b_o)=g(b_o)$ for some $0<b_o\le \fr14$.
To show this, note that 
\begin{align*}
f\Bigl(\fr14\Bigr)
&=  \EE\Bigl[\fr{(Y_1-\mu)^2}{4\mu+(Y_1-\mu)}\Bigr]\\
&= \EE\Bigl[\fr{(Y_1-\mu)^2}{3\mu+Y_1}\Bigr]\\
&\ge \EE\Bigl[\fr{(Y_1-\mu)^2}{2Y_1}\onec{Y_1\ge 3\mu}\Bigr]\\
&\ge \EE\Bigl[\fr{\fr{Y_1^2}{2}-\mu^2}{2Y_1}\onec{Y_1\ge 3\mu}\Bigr]\tag{$\because (a-b)^2\ge \fr12 c^2-b^2$}\\
&\ge \EE\Bigl[\fr{\fr{Y_1^2}{2}-\fr{Y_1^2}{9}}{2Y_1}\onec{Y_1\ge 3\mu}\Bigr]\\
&\ge\EE\Bigl[\fr{7}{36}Y_1\onec{Y_1\ge 3\mu}\Bigr]\\
&\ge \fr{7}{12}\mu\\
&\ge 4\mu \fr{F_n^{(\dt)}}{n}
= g\Bigl(\fr14\Bigr).
\end{align*}
Here, the last inequality follows from the assumption that $n\ge 7F_n^{(\dt)}$.
Hence, if we plug in the root $b_o$ to Eq.~\eqref{eq:intermed4}, then we have
\begin{align*}
\Delta \le f(b_o)+g(b_o) = 2\sqrt{\fr{F_n^{(\dt)}}{n} \EE\Bigl[\fr{(Y_1-\mu)^2}{1+\fr{b_o}{\mu}(Y_1-\mu)}\Bigr]}.
\end{align*}
To further upper bound this term, it suffices to find a deterministic lower bound on $b_o$, since, by Lemma~\ref{lem:basic} stated below, if $0\le b_\ell\le b_o$,
\begin{align*}
\EE\Bigl[\fr{(Y_1-\mu)^2}{1+\fr{b_o}{\mu}(Y_1-\mu)}\Bigr]
\le 2\EE\Bigl[\fr{(Y_1-\mu)^2}{1+\fr{2b_\ell}{\mu}(Y_1-\mu)}\Bigr].
\end{align*}
%
%
To find such a lower bound $b_\ell$, 
we note that, if we define $\eta(b)\defeq \fr{2\sigma^2}{\mu}b \ge f(b)$  
%
%
and $b\mapsto \eta(b)$ is monotonically increasing, and thus the root $b_o'$ of the equation $\eta(b)=g(b)$ must be smaller than $b_o$.
Hence, solving $\eta(b)=\fr{2\sigma^2}{\mu}b=\fr{\mu}{b}\fr{F_n^{(\dt)}}{n}=g(b)$ yields the root
\begin{align*}
b_o'=b_n^{(\dt)}\defeq \sqrt{\fr{\mu^2}{2\sigma^2}\fr{F_n^{(\dt)}}{n}}.
\end{align*}
Note that we require $n>\fr{\mu^2}{\sigma^2}F_n^{(\dt)}$ to ensure that the root $b_n^{(\dt)}$ lies in $(0,\fr12)$, which is assumed in the statement.
Finally, 
we have $\Delta\le f(b_o)+g(b_o) \le f(b_n^{(\dt)}) + g(b_n^{(\dt)})$, which concludes the proof.
\jmlrQED

\subsubsection{Technical Lemmas}
\label{app:sec:technical}

Here, we state and prove technical lemmas used in the proofs above.
\begin{lemma}
\label{lem:240903_concentration}
Let $Y_1,\ldots,Y_t$ be i.i.d. nonnegative random variables.
For any ``betting'' $b\in[0,1]$ and a ``reference'' mean $\nu>0$, we have
\begin{align*}
\PP\del[2]{
\frac{1}{n}\sum_{t=1}^n \ln\del[2]{1-b+b\frac{Y_t}{\nu}}
\ge 
b\frac{\mu-\nu}{\nu} 
-\fr{b^2}{1-b} \fr{\VV[Y_1] + (\mu-\nu)^2}{\nu^2}
%
-\frac{1}{n}\ln\frac{1}{\dt}
}\ge 1-\dt.
\end{align*}
\end{lemma}
\begin{proof}
Applying Lemma~\ref{lem:240903_var}  to Lemma~\ref{lem:240903_concentration_new} concludes the proof.
\end{proof}

\begin{lemma}
  \label{lem:240903_concentration_new}
  Let $Y_1,\ldots,Y_t$ be i.i.d. nonnegative random variables.
  For any ``betting'' $b\in[0,1]$ and a ``reference'' mean $\nu>0$, we have
  \begin{align*}
    \PP\del[2]{
      \frac{1}{n}\sum_{t=1}^n \ln\del[2]{1-b+b\frac{Y_t}{\nu}}
      \ge 
      b\frac{\mu-\nu}{\nu} 
      -\EE\sbr[2]{\fr{b^2 \fr{(Y_1 - \nu)^2}{\nu^2}}{1 + b\fr{Y_1 - \nu}{\nu}}} 
      -\frac{1}{n}\ln\frac{1}{\dt}
    }\ge 1-\dt.
  \end{align*}
\end{lemma}
\begin{proof}
  Use Lemma~\ref{lem:240903_concentration_basic} with $Z_t\gets b\frac{Y_t-\nu}{\nu}$.
\end{proof}



\begin{lemma}
\label{lem:240903_concentration_basic}
Let $Z_1,\ldots,Z_t$ be i.i.d. random variables supported over $(-1,\infty)$.
Then, we have
\begin{align*}
\PP\del[2]{
-\frac{1}{n}\sum_{t=1}^n \ln\del[1]{1+Z_t}
+\EE[Z_1] \le \EE\sbr[2]{\fr{Z_1^2}{1 + Z_1}} + \frac{1}{n}\ln\frac{1}{\dt}
}\ge 1-\dt.
\end{align*}
\end{lemma}
\begin{proof}
Note that the following is a nonnegative random variable.
\begin{align*}
M_n = \prod_{t=1}^n \fr{\fr{1}{1 + Z_t}}{\EE[\fr{1}{1 + Z_t}]} ~.
\end{align*}
Thus, by Markov's inequality $\PP(\ln\frac{M_n}{\EE[M_n]}\ge \ln \frac{1}{\dt})\le \dt$, or equivalently, w.p. at least $1-\dt$, we have
\begin{align*}
-\frac{1}{n}\sum_{t=1}^n \ln(1 + Z_t)  - \frac{1}{n}\ln\frac{1}{\dt}
&< \ln\EE\Bigl[\fr{1}{1 + Z_1}\Bigr]
\\&\le \EE\Bigl[\fr{1}{1 + Z_1}\Bigr] - 1 && (\because \ln(x)\le x-1)
\\&=   \EE\Bigl[\fr{-Z_1}{1 + Z_1}\Bigr]
\\&=   \EE\Bigl[\fr{Z_1^2}{1 + Z_1}\Bigr]  - \EE[Z_1].
\end{align*}
The last equality holds since $-\frac{t}{1+t}=\frac{t^2}{1+t}-t$.
\end{proof}

\begin{lemma}\label{lem:240903_var}
We have
\begin{align*}
\EE\sbr[3]{\fr{b^2 \fr{(Y_1 - \nu)^2}{\nu^2}}{1 + b\fr{Y_1 - \nu}{\nu}}}
\le \fr{b^2}{1-b} \fr{\VV[Y_1] + (\mu-\nu)^2}{\nu^2}.
\end{align*}
\end{lemma}
\begin{proof}
Consider
\begin{align*}
\fr{(Y_1 - \nu)^2}{1 + b\fr{Y_1 - \nu}{\nu}} 
= \fr{\nu{(Y_1-\nu)^2}}{b Y_1 + (1-b)\nu} 
\le \fr{{(Y_1-\nu)^2} }{(1-b)}. 
\end{align*}
Taking the expectation, we have $\EE[(Y_1-\nu)^2] = \EE[(Y_1-\mu+\mu-\nu)^2] = \VV[Y_1]+(\mu-\nu)^2$, which concludes the proof.
\end{proof}


\begin{lemma}
\label{lem:basic}
For any $y\ge 0$, $0\le b'\le b\le \fr12$, we have
\begin{align*}
\fr{1}{1+b\fr{y-\mu}{\mu}}
\le \fr{2}{1+2b'\fr{y-\mu}{\mu}}.
\end{align*}
\end{lemma}
\begin{proof}
Note that the denominators in both sides are positive.
Hence, the inequality is equivalent to
\begin{align*}
1+2b'\fr{y-\mu}{\mu} &\le 2+2b\fr{y-\mu}{\mu}
\Leftrightarrow (b-b')\Bigl(1-\fr{y}{\mu}\Bigr) \le 1.
\end{align*}
The last inequality readily follows from the assumptions $0\le b'\le b\le \fr12$ and $y\ge 0$.
\end{proof}



\subsection{Empirical-Bernstein-Type Relaxation of UP-LCB}
\label{app:sec:emp_bern_relaxation}
As alluded to earlier in Remark~\ref{rem:eb}, here we provide an empirical-Bernstein-type relaxation of pCRP$^\star$-LCB.
\begin{theorem}[Empirical-Bernstein-type relaxation of \pcrplcb{}]
\label{thm:emp_bern_relaxation}
%
Let $\hat{\mean}_n \defeq \frac{1}{n}\sum_{t=1}^n Y_t$ and $\hat{\mathsf{V}}_n\defeq \frac{1}{n}\sum_{t=1}^n (Y_t-\hat{\mean}_n)^2$ denote the empirical mean and variance, respectively.
Let $H_{n}^{(\dt)}\defeq\ln\frac{\sqrt{\pi(n+1)}}{\dt}$ and let $\lcbeb{n}{\dt}(Y_{1:n})
\defeq \hat{\mean}_n-\Delta_n^{(\dt)}$, where
\begin{align*}
\Delta_n^{(\dt)}
\defeq \frac{1}{1-\frac{2}{n}H_{n}^{(\dt)}} 
\Biggl(
\frac{\hat{\mean}_n}{n} H_{n}^{(\dt)}
+\sqrt{\frac{\hat{\mean}_n^2}{n^2} (H_{n}^{(\dt)})^2
+ \frac{4\hat{\mathsf{V}}_n}{n} H_{n}^{(\dt)} 
\Bigl(1-\frac{2}{n}H_{n}^{(\dt)}\Bigr) 
}
\Biggr).
\end{align*}
Under the same setting of Proposition~\ref{prop:lcb}, with probability at least $1-\dt$, for all $n\ge 1$ such that $H_{n}^{(\dt)}<\fr12$, we have $\mu\ge \lcbeb{n}{\dt}(Y_{1:n})$.
\end{theorem}

\begin{proof}
By Ville's inequality, with probability $1-\dt$, we have, for any $n\ge 1$,
\begin{align*}
\ln\frac{1}{\dt} {\ge} \ln\wealthup_t(Y_{1:n};\nu)
\stackrel{(a)}{\ge}
\ln\wealthpcrp(Y_{1:n};\nu)
={\sup_{b\in[0,1]}\ln\wealthcrp{b}(Y_{1:n};\nu)}
-\ln{\sqrt{\pi(n+1)}},
\end{align*}
which is equivalent to
\begin{align*}
\frac{1}{n}\sup_{b\in[0,1]} \sum_{t=1}^n \ln\Bigl(1-b+b\frac{Y_t}{\nu}\Bigr) 
\le \frac{1}{n} H_{n}^{(\dt)}.
\end{align*}
Here, $(a)$ follows from Eq.~\eqref{eq:up_vs_pcrp}.

Now, we apply Lemma~\ref{lem:generalized_lower_bound} for $n=1$ and obtain
\begin{align*}
\ln(1-b+bZ)
&\ge b((1-Z)^2-(1-Z)) + (1-Z)^2\ln(1-b)\\
&= b(Z^2-Z)+(Z^2-2Z+1)\ln(1-b),
\end{align*}
which holds for any $b\in[0,1)$ and $Z>0$.
Applying this inequality to each summand, we have
\begin{align*}
\fr{H_{n}^{(\dt)}}{n}
&\ge
\frac{1}{n}\sup_{b\in[0,1]} \sum_{t=1}^n \ln\Bigl(1-b+b\frac{Y_t}{\nu}\Bigr)\\
&\ge \frac{1}{\nu^2} \sup_{b\in[0,1]}\Bigl\{
((\hat{\mathsf{V}}_n+\hat{\mean}_n^2)-\hat{\mean}_n\nu)b
+((\hat{\mathsf{V}}_n+\hat{\mean}_n^2)-2\hat{\mean}_n\nu+\nu^2)\ln(1-b)\Bigr\}.\\
&= \frac{1}{\nu^2} \sup_{b\in[0,1]}\Bigl\{
B b
+(B-A)\ln(1-b)\Bigr\}\\
&\stackrel{(b)}{\ge} \frac{1}{\nu^2} \sup_{b\in[0,1]}\Bigl\{
B b
+(B-A)\frac{-b}{1-b}\Bigr\}\\
&\stackrel{(c)}{\ge} \frac{1}{\nu^2} \frac{A^2}{2(2B-A)},
\end{align*}
where $A\defeq (\hat{\mean}_n-\nu)\nu$ and $B\defeq (\hat{\mathsf{V}}_n+\hat{\mean}_n^2)-\hat{\mean}_n\nu$.
Note that $(b)$ follows from the elementary inequality $\ln(1-b) \ge \frac{-b}{1-b}$ for $b<1$, and $(c)$ follows by setting $b=\frac{A}{2B}$ to derive a lower bound.
We now wish to solve the equation 
\[
\fr{H_{n}^{(\dt)}}{n}\nu^2 = \frac{A^2}{2(2B-A)}
\]
with respect to $\nu$, which becomes equivalent to 
\[
\Bigl(1-2\fr{H_{n}^{(\dt)}}{n}\Bigr)x^2 - 2\fr{H_{n}^{(\dt)}}{n}\hat{\mean}_nx - 4\fr{H_{n}^{(\dt)}}{n}\hat{\mathsf{V}}_n=0,
\]
if we let $x\defeq \hat{\mean}_n-\nu$.
It is easy to check that $x=\hat{\mean}_n-\lcbeb{n}{\dt}(Y_{1:n})$ is the solution to this quadratic equation and thus a valid lower bound for $\mean$.
%
\end{proof}

\subsection{Proof for Theorem~\ref{thm:main_selection} (Regret Analysis for PUB)}

%
%
%
We provide a proof for $\hpi=\selectupsimple$, and the other case follows immediately by the same logic.
It suffices to show the second inequality.
Letting $2G_{n}^{(\dt)}[Y_1]$ denote the upper bound in Theorem~\ref{thm:up_lcb_rate},
we apply Theorem~\ref{thm:up_lcb_rate} to the process $\ips_{1:n}^\pi$ for each $\pi\in\Pi$ and take a union bound with $\dt\gets \dt'=\frac{\dt}{|\Pi|}$.
Under the good event with probability $\ge 1-2\dt$,
we have
\begin{align*}
\mean(\pi^*) - \mean(\selectupsimple) 
\stackrel{(a)}{\le} \mean(\pi^*) - \lcbup{n}{}(\ips_{1:n}^{\selectupsimple})  
%
\stackrel{(b)}{\le} \mean(\pi^*) - \lcbup{n}{}(\ips_{1:n}^{\pi^*})
\stackrel{(c)}{\le} 2G_n^{(\dt')}[\ips_1^{\pi^*}].
\end{align*}
Here, $(a)$ follows since $\mean(\ips^{\selectupsimple})\ge \lcbup{n}{}(\ips_{1:n}^{\selectupsimple})$, $(b)$ from the definition of the selection method in Eq.~\eqref{eq:up_select},
and $(c)$ from the upper bound of Theorem~\ref{thm:up_lcb_rate}.

The second part of the statement follows from the second part of Theorem~\ref{thm:up_lcb_rate} in place of the first part.
\jmlrQED

\subsection{Off-Policy Evaluation with Betting}
\label{sec:evaluation}
An immediate application of the UP-LCB and pCRP$^\star$-LCB is in off-policy evaluation.
Similar to \citet{Waudby-Smith--Wu--Ramdas--Karampatziakis--Mineiro2022}, we can construct the upper confidence bound (UCB) of the value of a policy using our LCB machinery, since 
\[
\breve{r}_t^\pi\defeq w_t^\pi (1-r_t) = w_t^\pi - \ips_t^\pi 
\]
is also a nonnegative random process.
Since $\EE[\breve{r}_t^\pi] = 1-\mean(\pi)$, we can construct the LCB from $\breve{r}_{1:n}^\pi$, from which we can construct the UCB of $\mean(\pi)$.
More precisely, we have:
\begin{proposition}
Pick any policy $\pi$.
With probability $\ge 1-2\dt$,
\begin{align*}
\lcbpcrp{n}{\dt}(\ips_{1:n}^\pi)
\le \lcbup{n}{\dt}(\ips_{1:n}^\pi) 
\le \mean(\pi)
\le 1-\lcbup{n}{\dt}({\breve{r}}_{1:n}^\pi)
\le 1-\lcbpcrp{n}{\dt}({\breve{r}}_{1:n}^\pi).
\end{align*}
\end{proposition}
Unlike \citet{sakhi24logarithmic}, our guarantee provides a direct control over the width of the confidence bounds.
The following guarantee is immediate from Theorem~\ref{thm:up_lcb_rate}:
\begin{theorem}[Evaluation]
\label{thm:evaluation} 
%
Pick any policy $\pi$.
Let $\breve{\var}(\pi)\defeq \VV[\breve{r}_1^\pi]$.
With probability $\ge 1-4\dt$, 
\begin{align*}
-\del[3]{\sqrt{\fr{48\breve{\var}(\pi)}{n} F_n^{(\dt)}} \vee \fr{12(1-\mean(\pi))}{n}F_n^{(\dt)} }
&\le \mean(\pi) - (1-\lcbpcrp{n}{\dt}({\breve{r}}_{1:n}^\pi))\\
&\le \mean(\pi) - (1-\lcbup{n}{\dt}({\breve{r}}_{1:n}^\pi))\\
&\le 0\\
&\le \mean(\pi) - \lcbup{n}{\dt}(\ips_{1:n}^\pi)\\
&\le \mean(\pi) - \lcbpcrp{n}{\dt}(\ips_{1:n}^\pi)
\le \sqrt{\fr{48\tilde{\var}(\pi)}{n} F_n^{(\dt)}} \vee \fr{12\mean(\pi)}{n}F_n^{(\dt)}.
\end{align*}
\end{theorem}
%



%
%

\section{Deferred Proofs for Off-Policy Learning}
\label{app:sec:proofs_learning}

\subsection{Proof of Proposition~\ref{prop:phi}}

\begin{proof}
  Logarithmic smoothing is trivial to show.
  For freezing, 
  the upper bound side is obvious.
  For the lower bound, we need to find $c_1$ and $c_2$ such that
  \begin{align*}
    f(x) \defeq\fr{\exp(-\phi(x)) - 1 + x}{x^2} \le \fr{1}{c_1 + c_2 x} 
  \end{align*}
  For this, if $x\le 1$ then we have $f(x) = \fr{1}{1+x}$.
  If $x > 1$, then we have $f(x) = 1/x$.
  Thus, using $\onec{x > 1} \le \fr{x}{1+x}$,
  \begin{align*}
    f(x) 
    &\le \onec{x\le 1} \fr{1}{1+x} + \onec{x > 1}  \fr{1}{x}
    \\&\le \onec{x\le 1} \fr{1}{1+x} + \fr{2}{1 + x} 
    \\&\le \fr{2}{1+x}.
  \end{align*}
  Thus, we have $c_1 = c_2 = \fr12$.
  For clipping, similar to freezing, if $x\le1$ then we have $f(x) = \fr{1}{1+x}$.
  If $x>1$, then we have $f(x) = \fr{-\fr12 + x}{x^2} \le \fr 1 x$.
  We can then proceed the identical derivation to Freezing to obtain $c_1 = c_2 = \fr12$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:learning-new} (Regret Analysis for Learning Algorithm)}

To derive the desired regret bound for our general estimator $\hpi_n \defeq \arg\max_{\pi\in\Pi} \sum_{t=1}^n \phi(\beta \ips_t^\pi)$, we consider the following two martingales:
\begin{align*}
  \text{(Upper deviation):}\quad
  U_n^\pi &\defeq
  \  \prod_{t=1}^n \frac{e^{\phi(\beta \ips_t^\pi)}}{\EE[e^{\phi (\beta \ips_t^\pi)}]}, \\
  \text{(Lower deviation):}\quad
  L_n^\pi &\defeq
  \  \prod_{t=1}^n \frac{e^{-\phi (\beta \ips_t^\pi)}}{\EE[e^{-\phi (\beta \ips_t^\pi)}]}.
\end{align*}
Throughout the proof we omit the subscript $t$ from $\ips_t^\pi$ inside the expectation, and use $\ips^\pi$ for simplicity.

By applying Ville's inequality~\citep{ville39etude} and taking the union bound over $\pi\in\Pi$, we have: with probability at least $1-2\dt$,
$U_n^\pi\le \frac{1}{\dt}$ and $L_n^\pi\le \frac{1}{\dt}$
for all $\pi \in \Pi$. 
Given this good event,
we have
\begin{align}
  -\ln\left(\EE[e^{-\phi(\beta \ips^{\pi^\star})}] \right) - \frac{1}{n}\ln\frac{|\Pi|}{\dt} 
  &\stackrel{(a)}{\le} \frac{1}{n} \sum_{t=1}^n \phi(\beta \ips_t^{\pi^\star}) \nonumber\\ 
  & \stackrel{(b)}{\le} \frac{1}{n} \sum_{t=1}^n \phi(\beta \ips_t^{\hpi_n}) \nonumber\\
  & \stackrel{(c)}{\le} \ln\left(\EE[e^{\phi(\beta \ips^{\hpi_n})}] \right) + \frac{1}{n}\ln\frac{|\Pi|}{\dt},
  \label{eq:ineq_chain_kj}
\end{align}
where $(a)$ follows from $L_n^{\pi^\star}\le \frac{1}{\dt}$,
$(b)$ follows by the definition of $\hpi_n$, and $(c)$ follows from $U_n^{\hpi_n}\le \frac{1}{\dt}$. 

We now further upper- and lower-bound this inequality.
%
%
Note that
\begin{align*}
  \ln(\EE[e^{\phi(\beta \ips^{\hpi_n})}])
  = \beta\EE[\ips^{\hpi_n}] + \ln (\EE[e^{\phi(\beta \ips^{\hpi_n}) - \EE  \beta \ips^{\hpi_n}}])
\end{align*}
Thus,
\begin{align}
  \fr1n\sum_t \fr1\beta \phi(\ips_t^{\hpi_n}) - \EE[\ips^{\hpi_n}] 
  &\le \underbrace{\fr1{\beta} \ln(\fr{1}{\EE[e^{\phi(\beta \ips^{\hpi_n}) - \EE  \beta \ips^{\hpi_n}}]} )}_{= -F_\beta(\phi)} + \fr{1}{n\beta} \ln(1/\dt) \label{eq:log_expected_inequality_ub_kj}
\end{align}
Note that $F_\beta(\phi) \ge 0$ by $\phi(x) \le \ln(1+x)$.


For the lower bound, we have
Note that
\begin{align*}
  \ln(\EE[e^{-\phi(\beta \ips^{\pi^*})}])
  &\le \EE[e^{-\phi(\beta \ips^{\pi^*})}] - 1
  \\&\le - \EE[\beta \ips^{\pi^*}] + \EE\Bigl[\fr{\beta^2(\ips^{\pi^*})^2}{c_1 + c_2 \beta \ips^{\pi^*}} \Bigr].
\end{align*}
Therefore,
\begin{align}
  \frac{1}{n}\sum_t -\fr{1}{\beta} \phi(\beta \ips_t^{\pi^*} ) + \EE[ \ips^{\pi^*}]\le \beta \EE\Bigl[\fr{(\ips^{\pi^*})^2}{c_1 + c_2 \beta \ips^{\pi^*}}\Bigr] +  \fr{1}{n\beta} \ln(1/\dt)~.
  \label{eq:log_expected_inequality_lb_kj}
\end{align}

By combining Eq.~\eqref{eq:log_expected_inequality_ub_kj} and Eq.~\eqref{eq:log_expected_inequality_lb_kj} through Eq.~\eqref{eq:ineq_chain_kj}, we have
\begin{align*}
  v(\pi^\star)-v(\hpi_n)
  &=\EE[\ips^{\pi^\star}] - \EE[\ips^{\hpi_n} ] \\
  &\le \beta \EE\Bigl[\fr{(\ips^{\pi^*})^2}{c_1 + c_2 \beta \ips^{\pi^*}}\Bigr] + F_\beta(\phi) + \fr{2}{\beta n} \ln\fr{|\Pi| }{\dt},
\end{align*}
which proves the desired claim.
%
\jmlrQED


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%


\clearpage
\section{On Experiments}

\subsection{On Datasets}
Table~\ref{tab:datasets} summarizes the dimensions of each dataset.

\begin{table}[!ht]
\centering
\caption{Summary statistics of the datasets.}\vspace{.5em}
\begin{tabular}{c c c c}
\toprule
Dataset & PenDigits & SatImage & JPVowel \\
\midrule
\# features & 16 & 36 & 14 \\
\# classes & 10 & 6 & 9 \\
\bottomrule
\end{tabular}
\label{tab:datasets}
\end{table}

\subsection{Learning Baselines}
\label{app:sec:estimators}
The estimators we tested in off-policy learning experiment are defined as follows:
%
%
%
%
%
%
\begin{align*}
\hat{\pi}_{\mathsf{PL}}&\defeq \arg\min_{\pi\in\Pi}  \sum_{t=1}^n \Bigl(\ips_t^\pi - {\beta}\sum_{a\in\mathcal{A}} \frac{\pi(a|x_t)}{\piref(a|x_t)}\Bigr),\\
\hat{\pi}_{\mathsf{clippedIW}}&\defeq \arg\min_{\pi\in\Pi}  \sum_{t=1}^n \frac{\piref(a_t|x_t)}{\piref(a_t|x_t) \vee \beta} r_t,\\
\hat{\pi}_{\mathsf{IX}}&\defeq \arg\min_{\pi\in\Pi} \sum_{t=1}^n \frac{\piref(a_t|x_t)}{\piref(a_t|x_t) + \beta} r_t,\\
\hat{\pi}_{\mathsf{LS}}&\defeq \arg\min_{\pi\in\Pi} \sum_{t=1}^n \ln(1+\beta \ips_t^\pi),\\
\hat{\pi}_{\mathsf{LS+freezing}}&\defeq \arg\min_{\pi\in\Pi} \sum_{t=1}^n \ln(1+\beta \ips_t^\pi)\onec{x \le \beta \ips_t^\pi}.
\end{align*}
In each case, $\beta>0$ is an hyperparameter.

\subsection{Comparison of Betting-Based LCB to Empirical Bernstein}
\label{sec:counter-example}
Here we empirically show that the betting-based LCB is not only statistically valid, but also converges to the target parameter in  a stable manner even for heavy-tailed data, where in sharp contrast, the empirical Bernstein bound behaves erratically due to the lack of convergence of empirical variance.
We construct a synthetic contextual bandit setting to demonstrate such robustness. 

Consider an infinite, discrete context space $\cX =\mathbb{N}= \{1,2,\ldots\}$ and a binary action space $\mathcal{A}=\{1,2\}$.
We study the following probability model:
\begin{itemize}
\item Context distribution: $p(x=i) = \frac{6}{\pi^2} \frac{1}{i^2}$ for $i\in\mathbb{N}$.
\item Behavior policy: $\piref(a|x=i)=\mathrm{Bern}(a|\frac{1}{i^\beta})$.
\item Reward distribution: $p(r=1|x=i,a=1)=1$,
$p(r|x=i,a=0)=\mathrm{Bern}(r|1-\frac{1}{i})$.
\end{itemize}
As we state below, below, the fourth raw moment $\EE[(\ips_t^\pi)^4]$ does not exist.
\begin{proposition}
\label{prop:counter_example}
Consider the discrete context space $\cX =\mathbb{N}= \{1,2,\ldots\}$ and a discrete action space $\mathcal{A}=\{1,\ldots,K\}$,
 where the context probability $p(x)$ is assigned such that
$p(x=i) \propto \fr{1}{i^2}$.
If we assume that $p(r=1|x=i,a=1)\ge \tau>0$, then for a behavior policy defined as $\piref(a=1|x=i)\defeq \frac{1}{i}$, there exists a policy $\pi$ which makes the fourth moment $\EE[(\ips_t^\pi)^4]$ not exist.    
\end{proposition}
\begin{proof}
If we consider $\pi$ such that $\pi(a=1|x)\ge c$ for any $x\in\mathcal{X}$ for some $c>0$, then we can show that the fourth raw moment $\EE[(\ips_t^\pi)^4]$ does not exist.
In particular, assume $r(x=i,a=1)=\frac{1}{\sqrt{i}}$.
\begin{align*}
\EE[(\ips_t^\pi)^4]
&= \EE_{p(x)\piref(a|x)p(r|a,x)}\Bigl[ \Bigl(\fr{\pi(A|X)}{\piref(A|X)}\Bigr)^4 R^4\Bigr]
\\&= \EE_{p(x)}\Bigl[ \sum_a \fr{\pi(a\mid X)^4}{\piref(a\mid X)^3} \E_{p(r|a,X)}[R^4]\Bigr]
\\&\ge \tau\EE_{p(x)}\Bigl[ \fr{\pi(a=1\mid X)^4}{\piref(a=1\mid X)^3} \Bigr]
\\&\gsim \tau\sum_{i=1}^\infty \fr{1}{i^2} \fr{\pi(a = 1\mid x=i)^4}{\piref(a=1\mid x=i)^3}
\\&\gsim \sum_{i=1}^\infty \fr{i^3}{i^2} \frac{1}{i^2} =\infty.
\end{align*}
This concludes the proof.
\end{proof}


\paragraph{Paragraph.} 
We simulated the environment using $\beta=3$, where a higher $\beta$ leads to a heavier tail behavior of $\tilde{r}_t^\pi$, and thus a more erratic behavior for EB.

We generated the trajectory of interactions of length $n=10^4$ for $N=100$ random trials, and visualize the sample mean trajectory, the UP-LCB, and the empirical Bernstein (EB) LCB, all averaged over the random trials,
in Figure~\ref{fig:counter_example}, 
with the shaded areas indicating one standard errors.
Here, following \citep{wang24oracle}, we used an empirical Bernstein type LCB
\[
\hat{\mean}_t(\pi)-\sqrt{2\hat{\var}_t(\pi)\ln\frac{2}{\dt}},
\]
where $\hat{\mean}_t(\pi)$ and $\hat{\var}_t(\pi)$ are the empirical mean and variance of the importance weighted rewards $(\ips_i^\pi)_{i=1}^t$.

Due to the heavy-tail nature of data, the estimates from the EB-LCB present erratic behaviors whenever we encounter a sample from the heavy tail.
In contrast, the UP-LCB provides a stable lower estimate of the target mean despite the heavy tail.
We also include some realizations of the experiments to further demonstrate the actual behavior without averaging. 


\begin{figure}[H]
    \centering
    \subfigure[]{%
        \includegraphics[width=.6\linewidth]{figs/unbdd_up_eb.png}\label{fig:unbdd_up_eb_main}
    }\\
    \subfigure[]{%
        \includegraphics[width=0.44\textwidth]{figs/unbdd_up_eb_0.png}%
        \label{fig:subfig1}%
    }
    %
    \subfigure[]{%
        \includegraphics[width=0.44\textwidth]{figs/unbdd_up_eb_2.png}%
        \label{fig:subfig2}%
    }\\
    %
    \subfigure[]{%
        \includegraphics[width=0.44\textwidth]{figs/unbdd_up_eb_3.png}%
        \label{fig:subfig3}%
    }
    %
    \subfigure[]{%
        \includegraphics[width=0.44\textwidth]{figs/unbdd_up_eb_5.png}%
        \label{fig:subfig4}%
    }
    \caption{Comparison of the UP-based LCB with the empirical-Bernstein-based LCB.
    The average behavior over $N=100$ random trials is presented in (a), and (b)-(d) show some realizations of the random runs. 
    These instances clearly demonstrate the failure cases of empirical-Bernstein-type bounds, which rely on the concentration of the empirical variance.
    See Figure~\ref{fig:counter_example} for an average behavior.}    \label{fig:counter_example}
\end{figure}


\end{document}
