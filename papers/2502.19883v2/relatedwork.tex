\section{Related Work}
%-----------------------------------------------------
\subsection{Jailbreak Attacks}
Jailbreak attacks, which transform harmful queries like ``How to make a bomb'' into more sophisticated prompts to deceive target models to generate toxic output, can be mainly classified into two categories: white-box methods and black-box methods. 

White-box methods generally rely on access to the internal states of LLMs to design attack strategies. 
These methods generally use gradients and logits of target LLMs as loss functions to optimize adversarial suffixes appended to malicious questions~\cite{ZWKF23, JDRS23, ZZAWBWHNS23, andriushchenko2024jailbreaking, geisler2024attacking, mangaokar2024prp}, or manipulate the output logits to enforce target LLMs to generate affirmative responses~\cite{HGXLC24, zhang2024jailbreak}.
However, white-box methods tend to generate irregular prompts that are easily detectable and cannot be optimized directly on black-box models like ChatGPT.

In contrast, black-box methods construct readable prompts in different ways and validate their effectiveness based on the responses of the target LLMs. 
Some studies employ heuristic strategies to rewrite malicious questions in other formats such as ASCII format~\cite{jiang2024artprompt}, code format~\cite{KLSGZH23, lv2024codechameleon}, encrypted format~\cite{YJWHHST24, liu2024making} and low-resource languages~\cite{DZPB24}, exploiting the insufficient safety alignment of target LLMs in these formats to bypass the defense mechanism.
Another line of research is to instruct an advanced LLM like GPT-4 to optimize jailbreak prompts by incorporating iterative refinement~\cite{jin2024guard}, genetic algorithms~\cite{LXCX23} and psychological expertise~\cite{ZLZYJS24}, or fine-tune another LLM with successful jailbreak templates to serve as an attacker to generate jailbreak prompts automatically~\cite{deng2024masterkey, ge2023mart}.
Compared with white-box methods, black-box methods can be applied to most models. For this reason, black-box methods are widely used in empirical experiments to evaluate the safety of LLMs.

To mitigate threats caused by jailbreak attacks, different defense techniques are proposed to ensure the security of LLMs. One line of work addresses the issue by detecting~\cite{inan2023llama} or perturbing~\cite{RWHP23} the jailbreak prompts to reduce the toxicity of the input, while another line of work directly enhances the robustness of LLMs by supervised fine-tuning~\cite{BSARJHZ24} or reinforcement learning from human feedback~\cite{Ouayng22}. 

\subsection{Small Language Models}
Similar to LLMs, SLMs are typically built upon decoder-only architectures, while they show diversity in implementation details such as the type of attention heads, layer numbers, dimension sizes, activation functions, and so on. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Overall.pdf}
    \caption{The family tree of the target SLMs we evaluate in our paper. The solid line represents the model is belonging to a certain family, while the dashed line indicates that the model is derived from that family with certain SLM technology.}
    \label{fig:overall}
\end{figure*}


To achieve competitive performance within the limited scale of SLMs, different model compression techniques are adopted to construct lightweight architectures efficiently. 
For instance, MobilLLaMA~\cite{thawakar2024mobillama} and MobileLLM~\cite{chu2023mobilevlm} introduce a parameter-sharing scheme in embedding blocks and attention head blocks to reduce the cost of GPU memory. 
TinyLLaMA~\cite{zhang2024tinyllama} optimizes memory load with the FlashAttention technique~\cite{dao2022flashattention}, which introduces an IO-aware attention algorithm to reduce the budget of high bandwidth memory. 
Quantization techniques, such as GPTQ~\cite{frantar2022gptq} and AWQ~\cite{lin2024awq}, can also effectively reduce memory loads by compressing the size parameters from 16 bits to 8 bits or even 4 bits.  
In model collections such as Llama 3~\cite{LLAMA23}, Qwen~\cite{bai2023qwen}, and MiniCPM~\cite{hu2024minicpm}, SLMs are generally designed and pre-trained following LLMs in the same family. 
Additionally, during the training phase, knowledge distillation techniques are widely used to derive performance from teacher LLMs to student SLMs, as models in the same family generally share similar tokenizers and architecture.

Recent research has demonstrated that SLMs can achieve comparable performance in some reasoning tasks, and can even outperform LLMs in specific scenarios~\cite{lu2024small}. 
Our study fills a gap in evaluating the security of SLMs from another perspective. In the following sections, we will demonstrate the security differences between various SLMs and delve into their underlying causes.

%-----------------------------------------------------