@article{GPT20,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{LLAMA23,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lu2024small,
  title={Small language models: Survey, measurements, and insights},
  author={Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D and Xu, Mengwei},
  journal={arXiv preprint arXiv:2409.15790},
  year={2024}
}

@article{vannguyen2024surveys,
      title={A Survey of Small Language Models}, 
      author={Chien Van Nguyen and Xuan Shen and Ryan Aponte and Yu Xia and Samyadeep Basu and Zhengmian Hu and Jian Chen and Mihir Parmar and Sasidhar Kunapuli and Joe Barrow and Junda Wu and Ashish Singh and Yu Wang and Jiuxiang Gu and Franck Dernoncourt and Nesreen K. Ahmed and Nedim Lipka and Ruiyi Zhang and Xiang Chen and Tong Yu and Sungchul Kim and Hanieh Deilamsalehy and Namyong Park and Mike Rimer and Zhehao Zhang and Huanrui Yang and Ryan A. Rossi and Thien Huu Nguyen},
      year={2024},
      journal={arXiv preprint arXiv:2410.20011},
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{zhang2024tinyllama,
  title={Tinyllama: An open-source small language model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}

@article{thawakar2024mobillama,
  title={Mobillama: Towards accurate and lightweight fully transparent gpt},
  author={Thawakar, Omkar and Vayani, Ashmal and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and Felsberg, Michael and Baldwin, Tim and Xing, Eric P and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2402.16840},
  year={2024}
}

@article{chu2023mobilevlm,
  title={Mobilevlm: A fast, strong and open vision language assistant for mobile devices},
  author={Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
  journal={arXiv preprint arXiv:2312.16886},
  year={2023}
}

@article{yi2024jailbreak,
  title={Jailbreak attacks and defenses against large language models: A survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{Yao2024,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@article{GAAPP23,
  title={From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy},
  author={Gupta, Maanak and Akiri, CharanKumar and Aryal, Kshitiz and Parker, Eli and Praharaj, Lopamudra},
  journal={IEEE Access},
  year={2023},
  publisher={IEEE}
}

@article{ZWKF23,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@inproceedings{JDRS23,
  title={Automatically auditing large language models via discrete optimization},
  author={Jones, Erik and Dragan, Anca and Raghunathan, Aditi and Steinhardt, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={15307--15329},
  year={2023},
  organization={PMLR}
}

@article{ZZAWBWHNS23,
  title={Autodan: Automatic and interpretable adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  journal={arXiv preprint arXiv:2310.15140},
  year={2023}
}

@article{andriushchenko2024jailbreaking,
  title={Jailbreaking leading safety-aligned llms with simple adaptive attacks},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2404.02151},
  year={2024}
}

@article{geisler2024attacking,
  title={Attacking large language models with projected gradient descent},
  author={Geisler, Simon and Wollschl{\"a}ger, Tom and Abdalla, MHI and Gasteiger, Johannes and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2402.09154},
  year={2024}
}

@inproceedings{mangaokar2024prp,
  author       = {Neal Mangaokar and
                  Ashish Hooda and
                  Jihye Choi and
                  Shreyas Chandrashekaran and
                  Kassem Fawaz and
                  Somesh Jha and
                  Atul Prakash},
  title        = {{PRP:} Propagating Universal Perturbations to Attack Large Language
                  Model Guard-Rails},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.591},
  pages = {10960--10976},
  year         = {2024},
}

@inproceedings{HGXLC24,
  author       = {Yangsibo Huang and
                  Samyak Gupta and
                  Mengzhou Xia and
                  Kai Li and
                  Danqi Chen},
  title        = {Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation},
  booktitle    = {The Twelfth International Conference on Learning Representations},
  year         = {2024},
}

@inproceedings{zhang2024jailbreak,
  title={Jailbreak open-sourced large language models via enforced decoding},
  author={Zhang, Hangfan and Guo, Zhimeng and Zhu, Huaisheng and Cao, Bochuan and Lin, Lu and Jia, Jinyuan and Chen, Jinghui and Wu, Dinghao},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5475--5493},
  url = {https://aclanthology.org/2024.acl-long.299/},
  year={2024}
}

@inproceedings{jiang2024artprompt,
  author       = {Fengqing Jiang and
                  Zhangchen Xu and
                  Luyao Niu and
                  Zhen Xiang and
                  Bhaskar Ramasubramanian and
                  Bo Li and
                  Radha Poovendran},
  title        = {ArtPrompt: {ASCII} Art-based Jailbreak Attacks against Aligned LLMs},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages = {15157--15173},
  url = {https://aclanthology.org/2024.acl-long.809/},
  year         = {2024},
}

@inproceedings{KLSGZH23,
  title={Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  booktitle={2024 IEEE Security and Privacy Workshops (SPW)},
  pages={132--143},
  year={2024},
  organization={IEEE}
}

@article{lv2024codechameleon,
  title={Codechameleon: Personalized encryption framework for jailbreaking large language models},
  author={Lv, Huijie and Wang, Xiao and Zhang, Yuansen and Huang, Caishuang and Dou, Shihan and Ye, Junjie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.16717},
  year={2024}
}

@inproceedings{YJWHHST24,
  title={GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year         = {2024},
}


@inproceedings{liu2024making,
  title={Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction},
  author={Liu, Tong and Zhang, Yingjie and Zhao, Zhe and Dong, Yinpeng and Meng, Guozhu and Chen, Kai},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={4711--4728},
  year={2024}
}
@inproceedings{DZPB24,
  title={Multilingual Jailbreak Challenges in Large Language Models},
  author={Deng, Yue and Zhang, Wenxuan and Pan, Sinno Jialin and Bing, Lidong},
  booktitle={The Twelfth International Conference on Learning Representations},
  year         = {2024},
}

@inproceedings{deng2024masterkey,
  title={Masterkey: Automated jailbreaking of large language model chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  booktitle={Proc. ISOC NDSS},
  year={2024}
}

@inproceedings{ZLZYJS24,
  author       = {Yi Zeng and
                  Hongpeng Lin and
                  Jingwen Zhang and
                  Diyi Yang and
                  Ruoxi Jia and
                  Weiyan Shi},
  title        = {How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing LLMs},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  url = {https://aclanthology.org/2024.acl-long.773/},
  pages = {14322--14350},
  year         = {2024},
}


@inproceedings{LXCX23,
  title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year         = {2024},
}

@inproceedings{jin2024guard,
  title={GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models},
  author={Jin, Haibo and Chen, Ruoxi and Zhou, Andy and Zhang, Yang and Wang, Haohan},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year     = {2024},
}

@inproceedings{ge2023mart,
  author       = {Suyu Ge and
                  Chunting Zhou and
                  Rui Hou and
                  Madian Khabsa and
                  Yi{-}Chia Wang and
                  Qifan Wang and
                  Jiawei Han and
                  Yuning Mao},
  title        = {{MART:} Improving {LLM} Safety with Multi-round Automatic Red-Teaming},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  year         = {2024},
  pages = {1927--1937},
  url = {https://aclanthology.org/2024.naacl-long.107/},
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{RWHP23,
  title={Smoothllm: Defending large language models against jailbreaking attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}

@article{Ouayng22,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{BSARJHZ24,
  title={Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
  author={Bianchi, Federico and Suzgun, Mirac and Attanasio, Giuseppe and Rottger, Paul and Jurafsky, Dan and Hashimoto, Tatsunori and Zou, James},
  booktitle={The Twelfth International Conference on Learning Representations},
  year         = {2024},
}


@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{Llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{SCBSZ23,
  title={Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@inproceedings{souly2024strongreject,
  title={A StrongREJECT for Empty Jailbreaks},
  author={Souly, Alexandra and Lu, Qingyuan and Bowen, Dillon and Trinh, Tu and Hsieh, Elvis and Pandey, Sana and Abbeel, Pieter and Svegliato, Justin and Emmons, Scott and Watkins, Olivia and others},
  booktitle={ICLR 2024 Workshop on Reliable and Responsible Foundation Models},
  year     = {2024},
}

@inproceedings{RKVABH23,
  author       = {Paul R{\"{o}}ttger and
                  Hannah Kirk and
                  Bertie Vidgen and
                  Giuseppe Attanasio and
                  Federico Bianchi and
                  Dirk Hovy},
  title        = {XSTest: {A} Test Suite for Identifying Exaggerated Safety Behaviours
                  in Large Language Models},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  year         = {2024},
  pages      = {5377--5400},
  url        = {https://aclanthology.org/2024.naacl-long.301/}
}

@article{LZZYLH23,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}