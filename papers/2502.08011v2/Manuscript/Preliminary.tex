\section{Preliminary}
\label{preliminary}

\begin{figure*}[t]{
    \centering
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\textwidth]{Figures/Method/algorithm.pdf}
            \caption{Schematic overview}
        \end{subfigure}    
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\textwidth]{Figures/Method/moons_trial_0_for_publication_v2.pdf}
            \caption{2-dimensional trajectory}
        \end{subfigure}
    \caption{An overview of the safe denoiser. (a) The safe denoiser $\mathbb{E}_{\text{safe}}$ negates the direction of the unsafe denoiser $\mathbb{E}_{\text{unsafe}}$ from the data denoiser $\mathbb{E}_{\text{data}}$. (b) Trajectories from data denoiser and safe denoiser, starting from the same initial point far from the data distribution, reveal distinct paths: while the sample path from the data denoiser falls into the unsafe region, the trajectory from the safe denoiser successfully avoids it.}
    \label{fig:schematic}
}
\end{figure*}

%
%
DMs generate samples through iterative decoding starting from random noise to data. This iterative process is a reverse of the forward data destruction process, 
given by  $\mathbf{x}_{t}=\alpha_{t}\mathbf{x}+\sigma_{t}\bm{\epsilon}$, where $\mathbf{x}$ follows the data distribution $p_{\text{data}}(\mathbf{x})$ and $\bm{\epsilon}$ follows the noise prior distribution $\mathcal{N}(0,I)$, which results in the perturbation kernel to be $q_{t}(\mathbf{x}_{t}\vert\mathbf{x})=\mathcal{N}(\mathbf{x}_{t};\alpha_{t}\mathbf{x},\sigma_{t}^{2}I)$. The specific choice of the coefficients $\alpha_{t}$ and $\sigma_{t}$ determine the variant of DMs. Depending on these parameters, the model may be referred to as Denoising Diffusion Probabilistic Models (DDPM) \cite{ho2020denoising}, Elucidating Diffusion Models (EDM) \cite{karras2022elucidating}, or flow matching \cite{lipman2022flow}. 

Regardless of whether the model is trained with noise-prediction~\cite{ho2020denoising}, data-prediction~\cite{karras2022elucidating}, or velocity-prediction~\cite{salimans2022progressive,lipman2022flow}, these approaches are fundamentally equivalent~\cite{kingma2021variational,kim2021soft}. This paper adopts the data-prediction framework due to its most intuitive interpretation. In data-prediction, the model approximates the \textit{denoiser} function, defined by
\begin{align}\label{eq:E_data}
\begin{split}
&\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]:=\int \mathbf{x} \frac{p_{\text{data}}(\mathbf{x})q_{t}(\mathbf{x}_{t}\vert\mathbf{x})}{p_{\text{data},t}(\mathbf{x}_{t})}\diff\mathbf{x}\\
&\quad\quad\approx\frac{1}{\alpha_{t}}\big(\mathbf{x}_{t}+\sigma_{t}^{2}\mathbf{s}_{\bm{\theta}}\big)=\frac{1}{\alpha_{t}}\big(\mathbf{x}_{t}-\sigma_{t}\bm{\epsilon}_{\bm{\theta}}\big), \end{split}
\end{align}
where $p_{\text{data},t}(\mathbf{x}_{t})$ is a marginal distribution of the noisy data distribution at time $t$, and $\mathbf{s}_{\bm{\theta}}$ and $\bm{\epsilon}_{\bm{\theta}}$ are score-prediction and noise-prediction, respectively.



DMs can be guided to produce samples~\cite{dhariwal2021diffusion,kim2022refining} that adhere more closely to a desired condition denoted by $\mathbf{c}$. A common approach in modern DMs is classifier-free guidance (CFG)~\cite{ho2021classifier}. The model is trained to learn both the unconditional denoiser $\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]$ and the conitional denoiser $\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}]$. The CFG modifies the sampling trajectory by
\begin{align*}
%
    \mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]+\lambda\big(\underbrace{\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}]}_{\text{positive}}-\underbrace{\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]}_{\text{uncond}}\big)
\end{align*}
allowing stronger alignment of the sample with the prompt $\mathbf{c}$ via the scale $\lambda$. The purpose of the additional term is to guide the unconditioinal denoiser in the \textit{sharpening direction} toward a desired condition $\mathbf{c}$.

%
%

Negative prompting \cite{liu2022compositional} reverses the CFG gradient direction for an undesired prompt denoted by $\mathbf{c}_{-}$. 
Formally, one replaces the standard CFG update with
\begin{align*}
%
    \mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]+\lambda\big(\underbrace{\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{+}]}_{\text{positive}}-\underbrace{\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{-}]}_{\text{negative}}\big),
\end{align*}
where $\mathbf{c}_{+}$ denotes a positive condition and $\mathbf{c}_{-}$ represents a negative context, such as low quality, watermark, logo, etc.
%



Recently, \citet{schramowski2023safe} introduced Safe Latent Diffusion (SLD), a new type of guidance, given by
\begin{align}\label{eq:SLD}
    &\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]+\underbrace{\lambda(\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{+}]-\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]}_{\text{CFG}})\\
    &-\underbrace{\mu(\mathbf{c}_{+},\mathbf{c}_{US};\gamma,\lambda)(\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{US}]-\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]}_{\text{SLD}}),\nonumber
\end{align}
where $\mathbf{c}_{US}$ represents a predefined set of unsafe prompts suggested by authors. Hypothetically, if we assume $\mu$ was set to be $\lambda$, the SLD guidance simplifies to
\begin{align*}
    \mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t}]+\lambda(\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{+}]-\mathbb{E}_{\text{data}}[\mathbf{x}\vert\mathbf{x}_{t},\mathbf{c}_{US}]).
    \end{align*}
    Instead of directly using $\mathbf{c}_{US}$ as $\mathbf{c}_{-}$, SLD introduces an adaptive weight $\mu(\mathbf{c}_{+}, \mathbf{c}_{US}; \gamma, \lambda)$ proportional to the denoiser difference norm, defined as $\Vert \mathbb{E}_{\text{data}}[\mathbf{x} \vert \mathbf{x}_{t}, \mathbf{c}_{+}] - \mathbb{E}_{\text{data}}[\mathbf{x} \vert \mathbf{x}_{t}, \mathbf{c}_{US}] \Vert$. The magnitude of this norm serves as an indicator of the proximity of the sampling trajectory to the unsafe region. Specifically, a larger norm suggests that the trajectory is likely to be safe, whereas a smaller norm indicates potential unsafety.

%


%

%
%
%
%


