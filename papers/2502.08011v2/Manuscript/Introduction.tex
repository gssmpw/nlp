\section{Introduction}
\label{sec_introduction}

Diffusion models (DMs) have emerged as a powerful class of generative models, consistently surpassing previous approaches on a variety of tasks, including text-to-image generation~\cite{rombach2022high}, audio synthesis~\cite{kong2021diffwave}, video synthesis~\cite{bartal2024lumiere}, and protein design \cite{watson2023denovo}. A significant factor contributing to their success is the flexible and controllable sampling with guidance \cite{dhariwal2021diffusion, ho2021classifier}. In particular, text-based guidance \cite{saharia2022photorealistic} has played a key role in the success of modern text-to-image models \cite{rombach2022high, podell2024sdxl}. 


Despite remarkable advancements, there is growing concern about the safety of content generated by these models.
The first concern is regarding not-safe-for-work (NSFW) content generation. To tackle the concern, negative prompts~\cite{gandikota2023erasing, ban2024understanding} have predominantly been used to guide models away from toxic text descriptions. Fine-tuning methods aimed at unlearning undesirable features~\cite{gandikota2023erasing, gandikota2023unified, gong2024reliable, kim2024race} have shown promise, too. However, 
their effectiveness is limited by adversarial attacks or jailbreaks that can circumvent safeguards~\cite{zhang2024generate, yang2024mma}.

Other safety concerns include DMs' generation of copyrighted content and data of individuals who wish to be excluded (machine unlearning). 
These two concerns are closely related to DMs's exceptional ability to memorize training data \cite{carlini2023extracting}. While differentially private training \cite{dockhorn2023differentially, liu2024differentially} could mitigate the danger of memorization, there is an inevitable performance drop due to the added noise to the training process.

%

%


\begin{figure*}[t]{
    \centering
        \begin{subfigure}{0.99\textwidth}
            \includegraphics[width=\textwidth]{Figures/Introduction/thumbnail.pdf}
            \caption{Nudity (Top) $\&$ Violence (Bottom)}
        \end{subfigure}    
        \begin{subfigure}{0.8\textwidth}
                \includegraphics[width=\textwidth]{Figures/Introduction/Ann.pdf}
            \caption{Data Memorization}
        \end{subfigure}
        \vskip -0.05in
    \caption{Our method \textit{Safe Denoiser} against existing methods. (a) Our method, incorporated with SLD~\cite{schramowski2023safe} and SAFREE~\cite{yoon2024safree}, does not generate inappropriate images. (b) Our method mitigates the memorization issue by negating the real image, resulting in a novel image that contains similar features like grey hair and formal outfit to those in the real image.}
    \label{fig:thumbnail}
}
\end{figure*}

%
%
%
%
%
%
%
%
%

In this work, we propose directly modifying the sampling trajectories of DMs such that the sampling trajectories adhere to theoretically safe distributions. The modification follows, what-we-call, \textit{safe denoiser}, which is derived from the relationship (in \thmref{safe}) between the expected denoised samples that are safe and those that are not safe. The final samples from the safe denoiser are theoretically guaranteed to be safe and away from the area to be negated. 
%
Based on this derivation, we develop a practical algorithm (in Algorithm~\ref{algo:safer}) that approximates the theoretically safe denoiser to generate safe images or combined with existing negative prompting methods to enhance the safety of text-to-image generation. 

In our experiments, we demonstrate that our safe denoiser achieves state-of-the-art performance in terms of its safe generation, in the tasks of concept erasing (a popular benchmark for avoiding NSFW images in text-to-image generation), class removal (object unlearning, a form of machine unlearning, in class conditional generation), and unconditional image generation. In what comes next, we start by describing background information. 
%

%

%

%

%



%
%
%
%
%
%

%
%

%

%

%

%

%

%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
        
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%

%

%
%
%
%
%
%