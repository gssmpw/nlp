\section{Experiments}
\label{sec:experiments}



In this section, we present the experimental results of our method, Safe Denoiser. Section~\ref{sec:t2i} details the outcomes of our text-to-image generation experiments, while the subsequent section explores both unconditional and conditional image generation.





%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsection{Text-to-Image Generation}
\label{sec:t2i}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
In this section, we conduct an in-depth analysis of the improvements achieved by additionally applying our safe denoiser in text-to-image models. As a baseline, we utilize Stable Diffusion (SD)~\cite{rombach2022high} v1.4\footnote{\url{https://huggingface.co/CompVis/stable-diffusion-v1-4}}. For our experiments, we employ the DDPM sampler. 



To assess the model safety, we evaluate Attack Success Rate (ASR) and Toxic Rate (TR)~\cite{yoon2024safree}. We measure ASR by the proportion of generated images that exceeds 0.6 nude class probability, measured by NudeNet\footnote{\url{https://github.com/notAI-tech/NudeNet}}. The TR is computed by the average of nude class probability, measured also by NudeNet. We select 515 unsafe images as the unsafe dataset of $\{\mathbf{x}^{(1)},\dots,\mathbf{x}^{(N)}\}$ from I2P~\cite{schramowski2023safe} that exceeds 0.6 nude class probability.

To evaluate the image quality, we calculate Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} and CLIP~\cite{radford2021learning}. We use a pytorch package~\cite{Seitzer2020FID} to compute the FID by comparing 10K reference images selected from the COCO-2014~\cite{lin2014microsoft} validation split and 10K generated images from the prompts identically selected from the same COCO dataset. Also, we evaluate the CLIP score using ViT-B-32\footnote{\url{https://huggingface.co/openai/clip-vit-base-patch32}} with the same dataset.

\paragraph{Quantitative Results} \tabref{t2i} summarizes our experimental findings. In these experiments, we utilize unsafe prompts proposed by Ring-A-Bell~\cite{tsai2024ringabell} (79 prompts), UnlearnDiff~\cite{zhang2024generate} (116 sexual prompts), and MMA-Diffusion~\cite{yang2024mma} (1000 prompts). For baseline comparisons, we consider both training-based approaches, specifically ESD~\cite{gandikota2023erasing} and RECE~\cite{gong2024reliable}, and training-free methods such as SLD~\cite{schramowski2023safe} and SAFREE~\cite{yoon2024safree}. Initially, we observe that using SDv1.4 results in a high percentage of unsafe images across all prompt datasets. As illustrated in \tabref{t2i}, existing text-based baselines demonstrate performance improvements over SD across each prompt dataset. 


%
%
%
%
%
%
%
%
%
%

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\textwidth]{Figures/Experiments/imagenet/vs_dogs/comparison_imagenet_chihuahua_vs_dogs.pdf}
    \caption{Generated samples when negating the Chihuahua class, primarily producing visually similar small dog breeds.}
    \label{fig:imagenet_comparison_dogs}
\end{figure*}

\begin{table*}[!t]
%
\begin{minipage}[t]{.45\textwidth}
\caption{Performance evaluation in FFHQ. We use ResNet18~\cite{he2016deep} to classify the sex of generated samples. We compute FID by comparing 1K male subset of CelebA validation and 1K generated images.}
    \label{tab:ffhq}
    \centering
    %
    \begin{tabular}{lccc}
        \toprule
        Models & Female & Male & FID $\downarrow$ \\\midrule
        Baseline (B)         & $64.0\%$ & $36.0\%$ & 109.07  \\
        B + SR   & $53.1\%$ & $46.9\%$ & 130.52   \\
        \cc{15}B + Ours       & \cc{15}$55.6\%$ & \cc{15}$44.4\%$ & \cc{15}96.57   \\
        \bottomrule
    \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{.53\textwidth}
    \caption{Experiments on ImageNet for the specific class (Chihuahua) negation task. Top-1 is the classification accuracy of the generated samples on 999 classes, and Top-1* indicates the accuarcy on the specific class.} \label{tab:imagenet} 
    \centering 
    \begin{tabular}{lccccc} 
        \toprule 
        Method & Prec $\uparrow$ & Rec $\uparrow$ & Top-1 $\uparrow$ & Top-1$^{*}$ $\downarrow$ \\ 
        \midrule 
        Baseline (B) & 0.72 & 0.63 & 0.76 & 0.68 \\ 
        B + SR & 0.59 & 0.54 & 0.01 & 0.0 \\ 
        \cc{15}B + Ours & \cc{15}0.62 & \cc{15}0.58 & \cc{15}0.14 & \cc{15}0.0 \\ \bottomrule 
    \end{tabular} 
    \end{minipage}%
\end{table*}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

Furthermore, our method significantly improves safety performance while maintaining image quality. Notably, the extent of improvement varies considerably depending on the characteristics of the prompts. For instance, with MMA-Diffusion prompts %
the performance of text-based baselines (like SLD) is markedly inferior compared to their performance on other prompt datasets such as Ring-A-Bell or UnlearnDiff. This discrepancy arises because MMA-Diffusion prompts lack explicit nudity information due to being part of a white-box adversarial attack, making it challenging for text-based methods to erase such content. In contrast, our approach employs purely image-based guidance, which, when combined with existing text-based methods, results in substantial performance gains from 0.88 to 0.48 in ASR on MMA-Diffusion that do not explicitly include unsafe text. Additionally, our method significantly improves the performance across all other prompt datasets, not limited to MMA-Diffusion.

\paragraph{Qualitative Results} 
Here, we present three ablation studies to evaluate the robustness and effectiveness of our method. First, \figref{ablation}‑(a) shows the effect of the number of unsafe data points on model performance. We observe that increasing the number of unsafe data points leads to better performance. %

%

We then explore the influence of the threshold parameter $\beta_{t}$, which governs the application of the safe denoiser. For simplicity, we fixed $\beta_{t}$ across all time steps. \figref{ablation}‑(b) shows the performance exhibits a U-shaped relationship to $\beta_{t}$. Specifically, when $\beta_{t} = 0$, the safe denoiser is applied to all samples $\mathbf{x}_{t}$ regardless of their safety status. Conversely, when $\beta_{t} = \infty$, the safe denoiser is not applied. At intermediate values of $\beta_{t}$, the safe denoiser is applied selectively to a certain proportion of unsafe samples $\mathbf{x}_{t}$. The U-shaped trend indicates 
%
that selectively applying the safe denoiser to unsafe samples based on an appropriate $\beta_{t}$ value is optimal, thereby balancing denoising efficacy and computational efficiency.
%

Finally, we assess the performance with varying the negative prompt weight in text-based methods. To establish that our approach consistently enhances performance across diverse setups when integrated with existing methodologies, we conduct a series of experiments. \figref{ablation}‑(c) shows as the weight of SLD increases from Weak to Max, SLD performs inadequately in MMA-Diffusion prompts. In contrast, our safe denoiser improves performance and widens the performance gap, underscoring its robustness and effectiveness as a superior enhancement.

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\subsection{Unconditional and Class Conditional Generation}
\label{sec:pixel}

In this section, we use our safe denoiser in the DMs without text inputs. Specifically, we employ experiments on FFHQ~\cite{karras2019style} and ImageNet~\cite{russakovsky2015imagenet} in the $256\times256$ resolution. We utilize the pretrained diffusion models from \citet{chung2022diffusion} for FFHQ and \citet{dhariwal2021diffusion} for ImageNet. For the experiments, we use a DPM solver~\cite{lu2022dpm} with 100 steps.

In FFHQ, we aim to prevent the generation of a specific sex. However, since the whole data points are used in training, we select 1K female images from CelebA~\cite{liu2015faceattributes} validation split to serve as unseen negative data, thereby establishing the negative dataset $\{\mathbf{x}^{(1)},...,\mathbf{x}^{(1000)}\}$. We then employ our safe denoiser to sample 1K images. As shown in Table~\ref{tab:ffhq}, classification accuracy for these generated samples reveal that our method more effectively avoids the female class compared to the baseline pretrained model. According to Table~\ref{tab:ffhq}, our algorithm generates more male images compared to Sparse Repellency (SR) while achieving a lower FID score. This suggests that the images produced by SR are of lower quality compared to those generated by ours, leading to increased confusion for the classifier.

In ImageNet, we focus on negating a specific Chihuahua class during generation. We select the validation set of Chihuahua class as the negative images. We generate 50 samples per class and classify samples from 999 classes by a classifier~\cite{dhariwal2021diffusion} and report the accuracy by Top-1. Also, we measure the Top-1 accuracy of 50 samples from Chihuahua class, reporting it by Top-1* in \tabref{imagenet}. From the result, we note that our method excels generating other 999 classes, while SR cannot generate images from those 999 classes. To evaluate the overall quality, \tabref{imagenet} further report the precision (sample accuracy) and recall (sample diversity)~\cite{kynkaanniemi2019improved} over 50K samples, indicating that our method is better than SR in negating a specific class. 
%
%
