\section{Experimental Details and Additional Results}
\label{suppsec:experiments}


\subsection{Implementation Details}
\paragraph{Text-to-Image Generation}

As outlined in the manuscript, we conduct the Text-to-Image experiment using SDv1.4, following the same model as the baselines for generating images from text, as referenced in \cite{schramowski2023safe, wu2024erasediff, gong2024reliable, yoon2024safree}. 
To ensure consistency, we adopt the generation procedure described in each baseline. 
Preliminary observing the sensitivity of nudity-related content, we employ the DDPM scheduler \cite{ho2020denoising}. %
For a fair comparison, we maintain the same number of inference steps, specifically $50$, aligning with the official implementations of both SLD and SAFREE, which also use 50 inference steps.

Regarding the \textit{Safe Denoiser}, the proposed model computes the transition kernel with an RBF kernel. 
%
%
The RBF kernel function is defined as follows:
\begin{equation}
\label{eq_rbf_kernel}
K(x, x') = \exp\left(-\frac{\lVert x - x'\rVert^2}{2\sigma^2}\right)
\end{equation}
For the bandwidth parameter $\sigma$, we set a value of 1.0 for SLD and 3.15 for SAFREE. 
Additionally, in case of SAFREE, we apply a scaling factor $\eta=0.33$, whereas for SLD, we use $\eta=0.03$ to regulate the strength of the repellency in \eqref{safer}.
For reference images, we utilize a total of 515 images sourced from the I2P dataset \cite{schramowski2023safe}, which were generated using SDv1.4. 
As stated in the manuscript, these reference images meet the criterion of having a nude class probability above 0.6, as determined by Nudenet. Sample images are shown below. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs.pdf}
    \caption{Samples of reference images by I2P dataset}
    \label{fig:i2p_ref_imgs}
\end{figure}

Empirically, we introduce a heuristic in which the proposed \textit{Safe Denoiser} is applied within critical timesteps $C=[780, ...,1000]$. %
In the early stages of diffusion, denoising process primarily establishes global structures rather than intricate details, while the later stages focus on refining fine-grained features. Since our approach aims to prevent the generation of globally harmful images rather than enhancing image quality or detail, we apply the denoiser at these later timesteps. 

Next, we briefly introduce the baseline models used in our experiments.
The first two approaches serve as comparisons for unlearning-based safe diffusion models \cite{gandikota2023erasing, gong2024reliable}.  
Specifically, we evaluate Erased Stable Diffusion (ESD) \cite{gandikota2023erasing} as a representative method. More recently, reliably trained safe diffusion (RECE) models have demonstrated improved performance, particularly in reducing the attack success rate \cite{gong2024reliable}. 
In addition to these unlearning-based approaches, we also include SLD and SAFREE as training-free safe diffusion models \cite{schramowski2023safe, yoon2024safree}. While both methods employ negative prompts, their underlying mechanisms differ significantly. 
In SLD, the set of unsafe prompts, denoted as  $c_{US}$, is designed to mitigate globally harmful image generation \cite{schramowski2023safe}.
In contrast, SAFREE focuses on more precise negative prompts specifically tailored to nudity-related content \cite{yoon2024safree}. Beyond negative prompts, SAFREE further enhances safety by applying an orthogonal projection technique in Euclidean space to shift text embeddings away from predefined toxic regions.
In the following, we provide an overview of the datasets used in our experiments.

\textbf{I2P}  
The I2P dataset consists of prompts related to seven unsafe concepts: hate, harassment, violence, self-harm, sexual content, shocking content, and illegal activity \cite{schramowski2023safe}. 
It contains a total of 4,703 prompts and was introduced in earlier stages of research, with subsequent studies primarily focusing on this dataset \cite{gong2024reliable, yoon2024safree}. 
In this work, we utilize the I2P dataset as a source of reference data points rather than for additional training.
The dataset was obtained from \url{https://huggingface.co/datasets/AIML-TUDA/i2p}

\textbf{Ring-A-Bell}
The Ring-A-Bell dataset was developed through a red-teaming approach that evaluates text-to-image diffusion models using black-box methods \cite{tsai2024ringabell}. 
The original dataset \url{Chia15/RingABell-Nudity} contains 285 prompts; however, we use a curated subset of 79 prompts, following prior baselines \cite{gong2024reliable, yoon2024safree}. 
This selection ensures a more equitable comparison of our method. 
The curated Ring-A-Bell dataset was obtained from either \url{https://github.com/CharlesGong12/RECE} or \url{https://github.com/jaehong31/SAFREE}.

\textbf{MMA-Diffusion}
MMA-Diffusion is another dataset generated via a red-teaming approach \cite{yang2024mma}. 
Unlike other datasets, it consists of adversarial prompts designed to include potentially harmful contexts without explicit expressions. 
Similar to the Ring-A-Bell dataset, we use a curated set of 1,000 prompts, consistent with prior baselines \cite{gong2024reliable, yoon2024safree}. 
The dataset was obtained from \url{https://github.com/CharlesGong12/RECE} or \url{https://github.com/jaehong31/SAFREE}.

\textbf{UnlearnDiff}
The UnlearnDiff dataset contains various harmful text prompts that can potentially generate NSFW images \cite{zhang2024generate}. 
Among its categories, we specifically focus on nudity-related prompts. 
The dataset includes a total of 116 nudity-related prompts, derived from an initial set of 143 prompts, from which 27 were excluded as they contained other NSFW categories such as self-harm and shocking content. 
This selection ensures that our numerical metrics remain unaffected by unrelated factors.
The dataset was obtained from \url{https://github.com/CharlesGong12/RECE} or \url{https://github.com/jaehong31/SAFREE}.

In Fig. \ref{fig:thumbnail}, we demonstrate that SD-1.4 exhibits trainig dataset memorization, as it is capable of regenerating an indentical images using the text prompt, (\textit{'Living in the light with Ann Graham Lotz <|startoftext|> lad mans'}). In this example, our method is applied with a bandwidth $\sigma=13.15$ and scaling factor of $0.69$. To construct a reference data for this case, we collected a total of 10 images from the internet. These are presented in Fig \ref{fig:ann_ref_imgs}. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.60\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_Ann.pdf}
    \caption{Reference images for Ann Graham Lotz case}
    \label{fig:ann_ref_imgs}
\end{figure}

\paragraph{Unconditional Generation}
For unconditional generation, we utilize the FFHQ dataset to evaluate whether the proposed method effectively mitigates sexual bias, using our method. Although FFHQ datset does not include explicit label information, Table~\ref{tab:ffhq} illustrates that the generated images exibit a noticiable bias toward female images over male ones. To address this imbalance, we we use 1000 male images from CelebA-HQ\footnote{https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256} test dataset as reference data. While both FFHQ and CelebAHQ are designed to capture similar distribution, they are not completely aligned. This distinction provides an advantageous experimental setup, where we assess the controllability of image generation using reference images. For performance evaluation, we compute FID score using 1000 male images from the CelebA-HQ dataset. 
For classification tasks, we train a ResNet18 model, as implemented in the PyTorch framework\footnote{\url{https://pytorch.org/vision/stable/index.html}} using the CelebA-HQ training dataset. 

\paragraph{Conditional Generation}
For conditional ImageNet~\cite{russakovsky2015imagenet} experiments at $256 \times 256$ resolution, we use a diffusion model trained on the full ImageNet-256 dataset guided by a classifier~\cite{dhariwal2021diffusion}. The diffusion backbone uses a linear noise schedule across 1000 diffusion steps. We condition on class labels by scaling the classifier guidance at 5.0, creating a strong pull towards the desired class during the sampling process. Each experiment generates 50 samples per class across all 1000 ImageNet classes, producing 50,000 samples that are then evaluated with a pretrained ImageNet classifier for precision, recall, and classification accuracy measurements ~\cite{he2016dppresnet50}. Our metrics include \textbf{(i) Precision: } the fraction of generated samples that match the designated ImageNet label when conditioned on the class, \textbf{(ii) Recall: } aims to evaluate the diversity and coverage of the targeted class distribution, and \textbf{(iii) Classification Accuracy: } the rate at which generated images are correctly identified as their conditioned label among the 999 classes (excluding the negated target class, i.e, Chihuahua). The classification accuracy on the hold-out negated class is also calculated, to evaluate how well the respective method does not generate the negated target class. To avoid unintended Chihuahua generation, these metrics aim to make sure that samples do not drift toward distinct Chihuahua-like features when conditioning on other classes as well. 

For the experiments, we focus on the Chihuahua class to investigate how effectively our proposed safe denoiser can repel a target class while preserving generative quality for other classes. To compare our approach we implement three variants of the conditional diffusion process: a baseline classifier-guided diffusion model without repellency mechanisms, the Sparse Repellency (SR)~\cite{kirchhof2024sparse} technique applied to the classifier-guided diffusion model, and our safe denoiser technique applied to the same diffusion process. In this experiment, the safe denoiser technique is applied on the 200 to 800 timesteps of the diffusion process. A $\beta$ of $\beta=0.02$ was chosen as to control the strength of the repellency away from the Chihuahua target class. %
In the SR variant of the experiment, a repellency scale of 0.01 is combined with a large radius of 300 to push generated samples out of regions resembling the negated target class.

%


\subsection{Additional Results}
We present additional qualitative results across three experimental scenarios: \textit{(1) Text-to-Image Generation for preventing nudity, (2) Sexual Debiasing in unconditional generation for facial images, and (3) Class-Conditional Generation, where reference images serve as constraints not to generate.} To systematically demonstrate the effectiveness of our approach, we present the results in sequence, beginning with text-to-image generation followed by unconditional generation and concluding with conditional generation.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_ring.pdf}
    \caption{Generated images by baselines and ours on Ring-A-Bell \cite{tsai2024ringabell}}
    \label{fig:ringabell}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_unlearn.pdf}
    \caption{Generated images by baselines and ours on UnlearnDiff \cite{zhang2024generate}}
    \label{fig:unlearndiff}
\end{figure*}



\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_ours_coco30k.pdf}
    \caption{Uncurated generated images by SAFREE+Ours on CoCo30K}
    \label{fig:coco30k_ours}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \begin{subfigure}{0.33\textwidth}
            \includegraphics[width=\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_uncond.pdf}
            \caption{Uncondtional FFHQ}
    \end{subfigure}   
    \begin{subfigure}{0.33\textwidth}
            \includegraphics[width=\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_uncond_sr.pdf}
            \caption{Sparse Repellency}
    \end{subfigure}   
    \begin{subfigure}{0.33\textwidth}
            \includegraphics[width=\textwidth]{Figures/Appendix/Implementation_detail/ref_imgs_uncond_ours_1.pdf}
            \caption{Ours}
    \end{subfigure}   
    \caption{Comparison of \textit{Safe Denoiser} against existing approaches when negation on female.}
    \label{fig:ffhq_comparision}
\end{figure*}


%
%
%
%
%
%


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/Experiments/imagenet/vs_others/comparison_imagenet_chihuahua_vs_others.pdf}
    \caption{Comparison of \textit{Safe Denoiser} against existing approaches when negation on Chihuahua. This comparison includes non-dog related ImageNet classes, which include Tench, Garbage Truck, Church, Spoonbill, and Great White Shark.}
    \label{fig:imagenet_comparison_others}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/Experiments/imagenet/vs_others/vanilla_imagenet_chihuahua_vs_others.pdf}
    \caption{Classifier guidance diffusion model generated samples when negating on Chihuahua. This comparison includes non-dog-related ImageNet classes mentioned in~\ref{fig:imagenet_comparison_others} along with the dog-related classes in Figure~~\ref{fig:imagenet_comparison_dogs} which are Pomeranian, Yorkshire Terrier, and Shih Tzu.}
    \label{fig:imagenet_vanilla_samples}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/Experiments/imagenet/vs_others/sparse_repellency_imagenet_chihuahua_vs_others.pdf}
    \caption{\textit{Sparse Repellency} generated samples when negating on Chihuahua. The same classes are selected as ~\ref{fig:imagenet_vanilla_samples}.}
    \label{fig:imagenet_sparse_repellency_samples}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/Experiments/imagenet/vs_others/safe_denoiser_imagenet_chihuahua_vs_others.pdf}
    \caption{\textit{Safe Denoiser} generated samples when negating on Chihuahua. The same classes are selected as ~\ref{fig:imagenet_vanilla_samples}.}
    \label{fig:imagenet_safe_denoiser_samples}
\end{figure*}