Developing audio signal compression codecs is one of the oldest signal processing problems still actively progressing. Over the years, it has passed many milestones. In the early days, conventional data transmission and storage were very expensive; thus, digital signal processing codecs aggressively focused on compression techniques for efficient audio storage and transmission. As storage technology developed and more bandwidth became available, new forms of audio codecs emerged. These shifted focus to strong audio quality with a lighter compression ratio, ranging from 2× (lossless codecs~\cite{mpeg4, flac}) to 10× (lossy codecs~\cite{ opus, amrwb, evs}). Most of these techniques trade off compression ratio (bitrate) for signal reconstruction and perceptual quality.

Recent advancements in storage, high-bandwidth transmission, and neural networks have led to data-driven or neural network-based codecs. These new codecs aim to achieve both low bitrates and high-quality waveform recovery simultaneously. Unlike traditional signal processing-based codecs—which use signal transform domains (wavelets) and handcrafted parameters (tone, pitch) for any audio signals—neural codecs optimize compression ratios and learn quality trade-offs through a data-driven approach. However, this approach has a common pitfall: neural codecs may produce poorer results when processing unfamiliar (out-of-domain) audio signals.

This issue becomes more problematic as neural codecs are increasingly adopted to provide discrete tokens for large language model-based audio generation techniques \cite{gslm, audiolm, valle, musicgen}. The obvious solution is to scale up training data and conduct large-scale training. However, this approach is both difficult and resource-intensive. Moreover, retraining with newly acquired data, maintaining domain balance, and avoiding model bias present additional challenges.

In this paper, we first investigate the out-of-domain robustness of the most commonly used techniques in neural codecs: autoencoder (AE) with residual vector quantizer (RVQ). We demonstrate that robustness issues arise from information loss during temporal and dimensional compression stages. We then propose an alternative approach called ComplexDec to encode speech in the complex spectral domain with a lower spectral dimension (e.g., 256-dim). Due to its inherently low temporal resolution, ComplexDec adopts a fully convolutional architecture without downsampling and upsampling layers. Consequently, it has a lower feature dimension than the code dimension typically used in RVQ, eliminating the need for dimensionality reduction or handcrafted codebook learning techniques. Moreover, complex spectral domain representations have limited information loss because of the linear short-time Fourier transform (STFT). Hence by easing the information loss and avoiding temporal and dimensional compressions improves the codec's out-of-domain robustness - a similar idea often used in conventional signal processing codecs.

We demonstrated the effectiveness of our approach using the reading-style VCTK~\cite{vctk2017} corpus as the training set and the expressive EARS~\cite{ears} corpus for testing. Our comprehensive validation reveals that the proposed ComplexDec method outperforms existing methods on several error metrics and in subjective evaluations.

The remainder of this paper is organized as follows: Section 2 provides background on out-of-domain robustness issues in AE-RVQ-based neural codecs. Section 3 details our proposed method, including the network architecture and data methods. In Section 4, we describe our experiments and present the quantitative and qualitative performance of the proposed method. Section 5 concludes the paper.

