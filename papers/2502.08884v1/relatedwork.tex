\section{Related Work}
\paragraph{Library learning.}

The core goal of our method is to learn a library of functions that can be used to procedurally represent any 3D shape in a given domain. Several prior methods~\cite{ellis2018library, ellis2021dreamcoder, topdownlib, babble, bellur2024leroy} have tackled the library learning problem in a more general setting. These methods take as input a set of simple tasks or programs that use only basic operators and find a library of more abstract functions that can represent the inputs more compactly.
Though these approaches have demonstrated impressive generality, their non-specialization limits their usability for 3D shapes. 
For example, they struggle to handle the complex parametric expressions that describe geometric relations between shape parts.
ShapeMOD~\cite{jones2021shapeMOD} and ShapeCoder~\cite{jones2023shapecoder} extend library learning to 3D shapes, using specialized search strategies over parametric expressions.

However, unlike our method, all of the methods above derive the library based only on the input examples, without using any additional priors like our LLM. Typically, their goal is to maximize the compression of input examples when represented with library functions. This limits the \emph{interpretability} of functions learned by these methods, i.e., how well their parameters and the operations they perform align with the semantics of 3D shapes. As a result, programs and their parameters are harder to understand and more difficult to use for both humans and LLMs, as we will demonstrate in our evaluation. Additionally, due to the lack of guidance from a strong prior, these methods require a much larger set of input examples and are more prone to get stuck in local minima, miss relevant functions, or produce duplicate functions.

More recently, Lilo~\cite{grand2024lilo} also proposes making use of an LLM prior for general (i.e., non-shape-specific) library learning. However, apart from not handling 3D shapes, the LLM is only used to \emph{name} functions that are found by one of the earlier methods~\cite{topdownlib}, and to apply these functions to given input examples. As the functions, along with their parameters, are still found without LLM guidance, the interpretability issue remains.


\paragraph{Shape program inference and generation.}
Once we have discovered a library of functions, we can use it to infer a program for a given input shape or to directly generate new shape programs. Several methods have tackled this problem of shape program inference and generation. Most methods assume that a low-level shape language is given, for example CAD languages~\cite{Fusion360Gallery, wu2021deepcad, xu2022skexgen, li2024sfmcad, xu2021inferring, li2023secad, ren2022extrudenet, uy2022point2cyl}, CSG operations~\cite{kania2020ucsg, ren2021csg, yu2023d, yu2022capri, du2018inversecsg}, or other languages~\cite{jones2020shapeAssembly, jones2024VPIEdit, ganeshan2023improving, tian2019learning, progrip2022}, including more domain-specific languages~\cite{plocharski2024faccaid, guo2020inverse, lee2023latent}. Shape programs are generated directly in this language, without learning a domain-specific function library that could be specialized to a given use case, usually by training a neuro-symbolic approach on a large set of examples.

However, both the lack of a domain-specific function library and the lack of a strong semantic prior like an LLM limit the \emph{interpretability} of the resulting programs, making it difficult to produce plausible edits, both manually or via natural language prompts: omitting a domain-specific function library reduces consistency and compositionality of the programs, making them less compact, and harder to interpret and work with. Thus, similar to prior library learning methods, the lack of a strong semantic prior reduces semantic alignment of operations and parameters.

Other methods assume shape programs are available and only focus on generating or inferring parameters for these programs~\cite{pearl2022geocode, infinigen2023infinite, infinigen2024indoors, MB:2021:DAGA}. Results are impressive, but require an expert to craft well-designed shape programs as input. In contrast, we only require high-level design intent as input, in the form of natural language descriptions and a set of seed shapes.

\paragraph{Scene layout generation using LLMs.}
LLMs have recently been used to directly generate 2D or 3D scene layouts from a natural language prompt~\cite{yang2024holodeck, feng2023layoutgpt, zhang2024scene, littlefair2025flairgpt, hu2024scenecraft, aguina2024open}. Note that in terms of priors used, this is the opposite of the approaches described in the previous paragraph: only an LLM prior is used, but no prior is obtained from a set of seed examples. 
This makes it hard to align the generated scene layouts to a target distribution, as examples can describe a target distribution much more efficiently than a text prompt. Additionally, the lack of a domain-specific function library results in the same problems described in the last paragraph: scene representations are not compact and have a large number of parameters, which makes them difficult to edit. 
In contrast, our approach combines the LLM prior with a prior obtained from a set of examples via our learned function library.
Recently, SceneMotifCoder~\cite{tam2024scenemotifcoder} also combines a LLM prior with a prior from a small set of $1$-$3$ examples. However, no function library is constructed, and they only produce very simple object layouts (e.g.,  stacks, rows, or grids).