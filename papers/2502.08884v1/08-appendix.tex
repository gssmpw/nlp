
\begin{figure*}[t!]
\centering
  \includegraphics[width=\linewidth]{figs/shapelib_qual_libs_v1.pdf}
   \caption{Examples of functions from the shape libraries discovered by \methodname. For each category, we show a function implementation, and a few example applications of the function. For each application, we show the full output shape, with parts corresponding to the function marked in the same color as the function name, and the function parameters. We can see that function applications are well-aligned with part semantics and that each function typically requires only a small set of parameters to represent a rich variety of part shapes.} 
  \label{fig:big_qual_libs}
\end{figure*}

\begin{figure*}[t!]
\centering
  \includegraphics[width=.85\linewidth]{figs/shapelib_qual_apps_v1.pdf}
   \caption{\methodname's abstraction functions provide a semantically aligned and interpretable interface that support downstream applications: text-based LLM editing and visual program induction from unstructured geometry. } 
  \label{fig:big_qual_apps}
\end{figure*}


\section{Additional Method Details}

\subsection{Objective Function }

When searching for programs that explain shapes, we need an objective function to guide the search. 
We take inspiration from prior approaches such as ~\cite{jones2023shapecoder}, and formulated an objective function as a weighted average of two terms.
One of these terms counts up the number of degrees of freedom in the program representation, for simplicity we treat every token in the program as a degree of freedom with the same weight (1.).
Another term ensures that the produced geometry does not deviate too far from the target structure. 
We calculate the geometric error (more on this in the next paragraph), and add that into our objective function with a weight of 10.

The geometric error function we use takes in two sets of unordered primitives. 
For every pair of primitives from the predicted to target set, we calculate the maximum minimum distance between any two corners from one primitive to the other. 
We then use a matching algorithm to assign a stable pairing between the two sets.
If any of the distances is above a threshold (0.25, where shapes are normalized to lie within the unit sphere), then we say that there is infinite geometric error.
Otherwise, the geometric error is an average of the maximum minimum corner distance (MMCD), calculated according to the best match.

\subsection{Network Design}

We implement all of our networks in PyTorch~\cite{paszke2017automatic}. 
All of our experiments are run on NVIDIA GeForce RTX 3090 graphic cards with 24GB of VRAM.
We use the Adam optimizer~\cite{Kingma2014AdamAM} with a learning rate of 1e-4.
We implement our recognition network as a Transformer decoder. 
Our network has 4 layers, 4 heads, model dim of 256, and a full feature dim of 1024.

This network has full attention over the conditioning information: each primitive in the input shape is quantized and treated as a discrete token.
We order the primitives according to their x-y-z positions, as we do not know how they should be ordered otherwise.
Programs are similarly tokenized, and our network is trained through teacher forcing. 
We use learned positional encodings, these cap the maximum sequence lengths and primitive amounts 
our network can reason over: 20 primitives and programs of up to length 64. 
We train with a batch size of 128.
For point cloud inputs, we replace the primitive token encodings with an embedding produced by a PointNet++~\cite{qi2017pointnet++} network. 
For voxel inputs, we replace the primitive token encodings with an embedding produced by a 3D-CNN. 
We train our networks for between 4-12 hours, depending on the category and task.

\subsection{Synthetic Data Sampler}

We perform two rounds of automated feedback for each `sample\_shape' function generated by the \textit{o1} LLM model.
This iterative approach aims to refine the sampler's outputs by addressing discrepancies and improving alignment with respect to seed set patterns. 
In each round of feedback, we evaluate the function by sampling a diverse set of shapes and assessing various aspects of its behavior. 
We examine whether all functions in the library were used, whether all parameter types were employed, and whether all output structures described in the function's documentation were produced. 
These checks are performed automatically.
Additionally, we analyze the structures generated by the sampled functions and determine their similarity to those observed during the validation stage. 
If significant deviations are detected, measured in the parameter space of each function, the sampler is instructed to update its logic to produce outputs closer to the expected structures.

\section{Additional Experimental Details  }

\subsection{Cost and Timing}

We provide detailed estimates for how expensive it is (from a time and API monetary expense perspective) to use our system to discover libraries of shape abstraction functions.
To produce 20 shape descriptions from images using gpt-4o: 10 cents and 1-2 minutes.
To create library interfaces from textual descriptions with o1mini: 25 cents, 2-4 minutes.
To propose function applications over (20) shapes with (1) o1mini call and (4) gpt-4o calls: \$2-3 
and 15-25 minutes.
To propose (4) implementations for each function with o1mini: \$2-4 and 15-30 minutes.
To propose a single program sampler with o1: 50 cents and 1 minute. In total, this amounts to \$5-8 and 30 minutes to 1 hour.

Notice that by default we use o1mini, but sometimes deviate based on our developmental experience. 
Making function applications without knowing function implementations is a `guess-based' exercise, so we are fine with the increased error rate that 4o produces in this step.
For the most complex tasks, like implementing a synthetic data sampler, we turn to o1 as we are able to provide enough task guidance and directives to make use of its `reasoning' capabilities.

\subsection{Data}

Collections of example shapes in the seed set are chosen by an expert user who has a design intent in mind (they also express this intent in natural language in the function descriptions).
Specifically, we have the user select 20 partNet shapes and put them in a list, and then we can automatically produce the rest of the structured data from the partNet annotations. 
Currently, we manually render associated ShapeNet meshes in MeshLab~\cite{meshlab}, but this could be easily relaxed for ease of use.

After we have selected these two shapes, we create separate `training' and 'validation' sets of shapes by randomly splitting up Partnet object instances.
We run all experiments over validation shapes, unless otherwise stated, and use the training shapes to get paired data for the visual program induction step that maps from unstructured geometry to a shape abstraction program.
The size of these train/val sets is 4000/1000 for chairs, 1216/400 for storage, 4000/1000 for tables, 434/400 for faucet, and 2625/656 for lamps.


\subsection{LLM-Direct Baseline}

The LLM-direct is an ablated version of our method that relies on only the prior of the LLM and the design intent of the expert user in the form of function descriptions.
We compare against it to validate the need for using the seed set of shapes alongside the natural language specification. 

This baseline, is equivalent to our method modulo a few critical changes. The interface creation step is exactly the same. 
After this step though, it immediately implements each function, without using any input/output guidance about how this function should constructed. 
As it has no seed set, it assumes that the LLM has perfectly implemented each function, and next advances to the synthetic sampler design stage where it prompts the LLM to produce a `sample\_shape' function from its constructed library.
Then, like the full~\methodname system, we can train a recognition network on data produced by this random sampling procedure.


\subsection{ShapeCoder}

In our comparisons against ShapeCoder we use the officially released implementation. 
The only change we make is removing the rotation operation from the base ShapeCoder language,
as we focus on structures of axis-aligned primitives in our experiments.
We develop ShapeCoder's library of abstraction over the same seed set of 20 shapes, which is much smaller than the large datasets used in the original ShapeCoder system (400 shapes).
Nevertheless, we find that ShapeCoder can generalize (in terms of compression, at least) fairly well even from these 20 shapes.

We experiment with discovering ShapeCoder libraries over a larger seed set of 400 shapes, and find that compression improves slightly on validation shapes, but not by a huge margin (Obj goes from 52.1 to 46.1, while the average library size grows from 19 to 24). 
Despite learning this library over a large collection of shapes, we still observe that this `ShapeCoder-400' variant does not find more semantically aligned function applications over validation structures.
In fact, its semantic entropy performance worsens (chair: 1.67 to 1.84, table: 1.578 to 2.16, storage: 2.07 to 2.08, lamp: 1.7 to 1.9, faucet: 2.1 to 2.3)
We view this result as lending our framing additional support: 
compression alone (even over a large dataset) is not enough to develop good shape abstraction libraries, top-down semantic guidance is also required.