
\section{Results and Evaluation}
\label{sec:results}

We run experiments over multiple categories of 3D shapes (\texttt{chair}, \texttt{table}, \texttt{storage}, \texttt{lamp}, \texttt{faucet}).
For each category, an expert user provides design intent as (a) natural language descriptions of functions that would be useful for this category and
(b) a set of 20 seed shapes 
sourced from PartNet~\cite{PartNet}, which has per-part annotations. 
We obtain corresponding renders of each shape from ShapeNet~\cite{chang2015shapenet}.
This input is provided to \methodname, which then produces libraries of abstraction functions for each category.
Unless otherwise noted, we use
OpenAI's o1-mini as the LLM.

We find that \methodname discovers libraries that match the design intent, with validated implementations for almost all of the functions specified in natural language (\texttt{chair} 8/8, \texttt{table} 5/6, \texttt{storage} 6/6, \texttt{faucet} 5/5, \texttt{lamp} 4/4). 
Figure~\ref{fig:big_qual_libs} shows examples of these implementations and applications.

We verify that our method is able to help realize the benefits of representing shapes in a procedural fashion with experiments that match our stated desiderata (see Figure~\ref{fig:big_qual_apps}.).
To evaluate generalization, we compare recognition networks that infer programs from structured inputs (Section~\ref{sec:res_prog_compression}) and from unstructured geometry (Section~\ref{sec:res_vpi}).
We then evaluate how well function applications are aligned with class semantics (Section~\ref{sec:res_semantics}).
Finally, we show that our interface is interpretable and maintains plausibility under manipulations with a perceptual study that evaluates how well an LLM can edit our shape programs compared to a baseline (Section~\ref{sec:res_llm_edit})


\subsection{Library Function Generalization}
\label{sec:res_prog_compression}

\input{tables/prog_compression.tex}

We measure how well our library generalizes beyond the patterns in the seed shapes.
We compare against three alternatives: \emph{Prims}, \emph{\llmbaseline}, and \emph{ShapeCoder}. 
\emph{Prims} refers to our representation of input shapes as collection of unordered primitives -- it is used as lower performance bound; 
\emph{\llmbaseline} is an ablated version of our method that only reasons over the natural language descriptions to discover a library of abstraction and does not use seed shapes; while \emph{ShapeCoder} only uses seed shapes.
In our evaluations we show that \methodname, which uses both forms of design intent, offers clear advantages over these alternatives.

We evaluate the ability of different methods to compress programs in Table~\ref{tab:prog_compression}.
We report this over two different shape sets: the seed set (20 shapes per category) and a held-out validation set (400-1000 shapes per category).
For~\methodname~and~\llmbaseline, program applications are found for validation shapes using the recognition network that takes as input a shape
represented
as a collection of unordered primitives (Section~\ref{sec:lib_usage}). 
ShapeCoder develops and learns such a recognition network during its `library learning' stage.
For both the seed set and the validation set, we report the total compression objective value (Obj).
This is a weighted sum of the degrees of freedom the program exposes (Prog DoF, weight 1),
and the geometric error of the reconstructions (Error, weight 10). 
We also report the number of functions used in each library (\# Lib fns), the time it took to discover each library (Dev time), the number of functions used per shape (\# Shape fns) and the average time it takes to infer a program for a validation shape (Inf Time).

From the results, we note \llmbaseline performs poorly, and its function implementations can't find applications that match well to real geometry (resulting in its limited objective improvement over Prims).
ShapeCoder is designed solely to perform well at program compression, but despite this, \methodname is able to match or slightly outperform ShapeCoder with respect to the objective.
Moreover, we achieve this result much faster, using a smaller collection of library functions, and require less function calls to reconstruct shapes during inference.
We find library implementations in under an hour, whereas ShapeCoder's bottom up procedure takes around a day to converge (though our LLM API calls cost \$5-10 per category).

\subsection{Shape Programs from Unstructured Geometry}
\label{sec:res_vpi}

\input{tables/vpi_struct}

So far, we demonstrated that our recognition network from Section~\ref{sec:lib_usage} can successfully convert semi-structure geometric inputs into programs, but what about completely unstructured geometry such as point clouds or voxels?
To support this application, we train new recognition networks that take either point clouds or voxels as input. We source training data using the original `structured' recognition network to annotate shapes in PartNet
with corresponding programs.
Per category, we use 400-4000 shapes for training and re-use the same 400-1000 shapes as described previously for validation.
We sample both point clouds and voxels for each of the shapes.


Table~\ref{tab:vpi} compares the reconstruction performance of a recognition network trained with function from \methodname to a recognition network trained with functions from ShapeCoder.
For point clouds, we track Chamfer distance~\cite{fan2017point} between input point cloud (sampled from mesh geometry) and point cloud sampled from abstracted cuboid outputs. 
For voxels, we track IoU between input voxelizations and voxelizations of program outputs. 
We find that functions from \methodname enable more accurate reconstructions compared to functions from ShapeCoder.
We visualize some qualitative results for some validation shapes in Figure~\ref{fig:big_qual_apps}. In addition to leading to better reconstructions, we also see that the application of our functions are more strongly correlated with class semantics.


\subsection{Sematic Consistency of Function Usages} 
\label{sec:res_semantics}

\input{tables/semantic_entropy}

Beyond reconstruction, the \textit{way} in which functions are used also impacts the usefulness of the resulting model.
We design an experiment to evaluate the \textit{semantic consistency} of function usages.
We track how each function is applied when reconstructing validation shapes, and record the semantic labels of the parts that it matches against. 
Then, for each semantic label, we analyze the distribution of functions that were used to construct parts of this type. 
If functions are well-aligned with semantics,
i.e. have a consistent usage pattern,
then this distribution should have low entropy.
We report results of this experiment in Table~\ref{tab:semantic_label_entropy}. 
Compared with ShapeCoder, \methodname has a much lower semantic entropy, indicating that its assignment of functions to part structures is more semantically aligned.

\paragraph{Semantic Segmentation}
\input{tables/semantic_seg}
Alternatively, we judge the semantic alignment of these libraries by using them to perform semantic segmentation.
We design an experiment to test these capabilities.
For each function, we look at validated applications made over the seed set, and record the semantic labels of parts that each function explains. 
We then aggregate this information by counting the most commonly covered part labels to produce a simple voting function to assign semantic labels when the function is applied.
We evaluate the semantic segmentation performance on fine-grained part labels from PartNet over validation shapes, and report results of this experiment in Table~\ref{tab:sem_seg}.
ShapeCoder and~\methodname achieve a similar recall, but~\methodname is twice as precise in its semantic predictions.
\llmbaseline is more precise then ShapeCoder,
however without access to seed set exemplars it cannot find many successful function application, resulting in poor recall.


\subsection{Editing Shape Programs with LLMs}
\label{sec:res_llm_edit}

\input{tables/llm_prog_edit}

In this section, we investigate two critical questions concerning our library: is it interpretable and does it help constrain shape plausibility.
We consider these questions under the framing of a shape editing study. 
First, we use the application network from Section~\ref{sec:lib_usage} to find programs that explain validation shapes, using either functions from ShapeCoder or~\methodname.
We then design a series of shape edit requests, and ask an LLM to edit the text of the shape program to meet the request (i.e. change function parameters and how functions are used, as depicted in Figure~\ref{fig:teaser}, for example).

To evaluate performance, we designed a two alternative forced choice perceptual study.
We choose 5 shapes from the validation set of each category, and consider 4 edits per shape, giving us a cross-product of 100 total comparison conditions.
We provide \textit{o1mini} with the fully implemented function library for both~\methodname and ShapeCoder conditions.
For the ShapeCoder condition, we observed that \textit{o1mini} produced a program that failed on execution for 11/100 editing tasks, so we omit those from the study. 
\textit{o1mini} never produced a program that failed on execution for the \methodname condition.
We recruited 13 participants who made 50 perceptual judgments each.
For each comparison, we show the original shape in the middle, and arrange edits made using ShapeCoder/\methodname programs on either side, randomizing the left/right order.
We then ask each participant to make two judgments: (i) which manipulated shape was more plausible, and (ii) which edit better matched the input edit request.

We report preference rates of \methodname over ShapeCoder along these two axes in Table~\ref{tab:llm_edit}. 
These results support our claim that our library of shape abstraction functions provides an interface that is easy to interpret and maintains strong plausibility under parameter variations.
We show qualitative demonstrations of these edits in Figure~\ref{fig:big_qual_apps}, and observe higher semantic alignment of LLM edits, when these edits are made over \methodname programs.

