

\section{Using the Library for Program Synthesis}
\label{sec:lib_usage}

In Section~\ref{sec:lib_design}, we constructed a library of functions that have meaningful signatures and structured doc-strings.
Each function has an implementation that is capable of producing structures that capture patterns observed in the seed set, but a question remains: how can we use these functions to represent new shapes?

In this section, we describe our strategy for expanding library function usage beyond the seed set (Fig.~\ref{fig:method_fig}, \textit{right}).
To begin, we once again make use of the strong prior of LLMs by providing it with our library interface and asking it to design a procedure that uses the abstraction functions to randomly synthesize synthetic shapes.
Once we've developed this synthetic data sampler, we can use it to produce paired training data for a recognition network that learns how to solve an inverse task: given an input shape structure, write a program using the library functions that explain its parts.


\paragraph{Generating a synthetic shape sampler}

In this step, we design a prompt that describes the library we've developed, including the interface of each function  and examples of how to use it (sourced from the validation stage).
We give this prompt to an LLM and ask it to write a `sample\_shape' function that randomly produces new shapes using the provided abstractions.
Interestingly, we find that frontier LLMs are able to provide useful implementations of such a `sample\_shape' function.
A shown in Figure~\ref{fig:method_fig}, some of these random outputs produce good shape abstractions, while other random samples violate class semantics.
With this in mind, instead of attempting to get the LLM to perfect its implementation, we treat its output as a synthetic data generator for a recognition network. 
To broaden the coverage and variety of structures that these `sample\_shape' functions produce, we employ an iterative refinement loop that provides automatic feedback to the LLM.
This refinement procedure ensures that all functions and parameters in the library get used, and instructs the `sample\_shape' function to produce outputs spanning the observed structures from validation step (see appendix).


\paragraph{Training a recognition network}

Once we've improved the `sample\_shape' function through rounds of iterative refinement, we can use it to produce training data for a recognition network.
This network takes as input a shape represented as a set of unordered primitives (e.g., Cuboid dimensions and positions).
It outputs a program that uses library functions to reconstruct this input shape.
We implement this network as an autoregressive Transformer decoder~\cite{att_is_all} with a causal prefix mask over the input shape representation.
We train this network from scratch, streaming random samples from the synthetic data generator: each program we sample becomes a target output and we execute the program to find the corresponding input.
Once trained, we can use this network to find library function applications that explain shapes from outside of the starting seed set (Fig.~\ref{fig:method_fig}, \textit{right-bottom}).  
Our inference procedure prompts the network with an input set of unordered primitives and samples a large number of programs according the network's predicted distribution.
We try executing each program, and we record its complexity (the number of tokens it uses) and its geometric error against the input set. 
We choose the program that minimizes an objective that is a simple weighted combination of these two values.





























