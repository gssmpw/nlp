% \subsection{Proof of Proposition \ref{proof:qc_packing}}
% \label{app:proof:qc_packing}

\subsection{Useful Lemmas}
\label{app:useful lemmas}
\begin{lemma}\label{app:lemma:traceinequality}
    Let $\rho$ and $\sigma$ be positive operators and $\Lambda$ a positive operator such that $0\leq \Lambda\leq I$. Then the following inequality holds
    $\Tr\{\Lambda \rho\} \leq \Tr\{\Lambda \sigma\}+\|\rho-\sigma\|_1.$
\end{lemma}
% \begin{proof}
%     Write the proof!
% \end{proof}
\begin{lemma}[Non-Commutative Union Bound \cite{sen2012achieving}]
   Let $\sigma$ be a sub-normalized state such that $\sigma\geq 0$ and $\Tr\{\sigma\}\leq 1$. Let $\Pi_1,\Pi_2, \Pi_N$ be projectors. Then the following holds 
   $$\Tr\{\sigma\} - \Tr\{\Pi_N\cdots\Pi_1\sigma\Pi_1\cdots\Pi_N\} \leq 2\sqrt{\sum_{i=1}^N\Tr\{(I-\Pi_i)\sigma\}}.$$
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Proposition \ref{prop:binning}}
\label{app:proof:prop:binning}
Before commencing the proof, it is noteworthy that if we assume $\calD(m,\zn) = \utilde^n$. Then, using upper bound on  total variation, we obtain the following expression:
\begin{equation}\label{eqn:packingbound}
    \sum_{\yn} \Big| P^n_{Y|UZ}(\yn|\un,\zn) - P^n_{Y|UZ}(\yn|\Tilde{u}^n,\zn)\Big| \leq 2 \cdot \I_{\set{\un \neq \Tilde{u}^n}}.
\end{equation}
Therefore, by utilizing \eqref{eqn:packingbound} and applying the union bound, we can rewrite the term $\peone$ as $\peone \leq \peoneone + \peonetwo$, where 
\begin{align*}
    \peoneone &\deq 2 \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{ \zn \\l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} \I_{\set{\xn \in \Txcond}}
    P_{Z|X}^n(\zn|\xn) P_{X|U}^n(\xn|\un) \\
    &\hspace{100pt} \times\I_{\set{(\un,\zn) \notin \Tuz}} \eqand\\
    %%%%%%%%%%%
    \peonetwo &\deq 2 \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }}\sum_{\substack{ \zn \\ l \in [\btheta]\\m\in [\theta]}} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} \I_{\set{\xn \in \Txcond}}
    P_{Z|X}^n(\zn|\xn) P_{X|U}^n(\xn|\un) \\
    &\hspace{100pt} \times \sum_{\ltilde \in [\btheta]} \sum_{\utilde^n \neq \un} \I_{\set{\Un(\ltilde) = \utilde^n}} \I_{\set{\calB(\ltilde) = m}} \I_{\set{(\utilde^n,\zn) \in \Tuz}}.
\end{align*}
We first show that the term $\peoneone$ can be made arbitrary small for sufficiently large $n$. Consider the following inequalities:
\begin{align*}
    \EE[&\Ipmf \peoneone] \\
    &\leq \sum_{\substack{\un \in \Tu  \\\xn \in \Txcond  }}  \sum_{\substack{ \zn \\l \in [\btheta]\\m\in [\theta] }} 
    \frac{2}{\btheta} \frac{P^n_U(\un)}{(1+\eta)} 
    \I_{\set{\calB(l) = m}} 
    P_{Z|X}^n(\zn|\xn) P_{X|U}^n(\xn|\un) \I_{\set{(\un,\zn) \notin \Tuz}} \\
    &\overset{a}{=} \sum_{\substack{\un \in \Tu  \\\xn \in \Txcond  }}
   \sum_{\substack{ \zn \\ l \in [\btheta]}}
    \frac{2}{\btheta} \frac{P^n_U(\un)}{(1+\eta)}  
    P_{XZ|U}^n(\xn,\zn|\un)  \I_{\set{(\un,\zn) \notin \Tuz}} \\
    &\overset{}{\leq} \sum_{\substack{\un \in \Tu}}
   \sum_{\substack{ \zn \\ l \in [\btheta]}}
    \frac{2}{\btheta} \frac{P_{UZ}^n(\un,\zn)}{(1+\eta)}  
      \I_{\set{(\un,\zn) \notin \Tuz}} \\
    &=  \frac{2}{(1+\eta)} \sum_{(\un,\zn) \notin \Tuz}P_{UZ}^n(\un,\zn) \overset{b}{\leq} 2\epsilon,
\end{align*}
for all sufficiently large $n$ and all $\delta>0$, where $(a)$ follows from the fact that $\sum_{m \in [\theta]} \I_{\set{\calB(l) = m}} \!= \!1$ and using the Markov chain $U-X-Z$ and $(b)$ follows from the standard joint typicality argument. We now proceed to bound the error term $\peonetwo$. Consider the following set of inequalities:
\begin{align*}
    \EE[&\Ipmf \peonetwo] \\
    &\overset{a}{\leq} \sum_{\substack{\un \in \Tu \\ \xn \in \Txcond}}\sum_{\substack{ \zn \\ l \in [\btheta]\\m\in [\theta]}} 
    \frac{2}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
     \I_{\set{(\utilde^n,\zn) \in \Tuz}} P_{XZ|U}^n(\xn,\zn|\un) \\
    &\hspace{80pt} \times \sum_{\ltilde \neq l} \sum_{\utilde^n \neq \un} \EE[ \I_{\set{\calB(l) = m}}\I_{\set{\calB(\ltilde) = m}} \I_{\set{\Un(l) = \un}} \I_{\set{\Un(\ltilde) = \utilde^n}}] \\
    %%%%%%%%%%%%%%
    &\overset{}{=} \sum_{\substack{\un \in \Tu \\ \xn \in \Txcond}}\sum_{\substack{ \zn \\ l \in [\btheta]\\m\in [\theta]}} 
    \frac{2}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
     \I_{\set{(\utilde^n,\zn) \in \Tuz}} P_{XZ|U}^n(\xn,\zn|\un) \\
    &\hspace{80pt} \times \sum_{\ltilde \neq l} \sum_{\utilde^n \neq \un} \text{Pr}\{\set{\calB(l) = m, \calB(\ltilde) = m} \}\frac{P^n_U(\un)P^n_U(\utilde^n)}{(1-\varepsilon)^2} \\
    %%%%%%%%%%%%%%%
    &\overset{}{\leq} \sum_{\substack{\un \in \Tu \\ \zn, \utilde^n \neq \un}}\sum_{\substack{l, \ltilde \\m\in [\theta]}} 
    \frac{2}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} \frac{1}{\theta^2}
     \I_{\set{(\utilde^n,\zn) \in \Tuz}} P_{UZ}^n(\un,\zn) \frac{P^n_U(\utilde^n)}{(1-\varepsilon)^2} \\
     %%%%%%%%%%%%%%%
    &\overset{}{\leq} \sum_{\substack{\ltilde \neq l}} \frac{1}{\theta}
    \frac{2}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
\sum_{\substack{(\utilde^n,\zn)\in \Tuz}} \frac{1}{(1-\varepsilon)} P_{Z}^n(\zn) P^n_U(\utilde^n)\\
      %%%%%%%%%%%%%%%
    &\overset{b}{\leq} \sum_{\substack{\ltilde \neq l }} 
    2 \frac{(1-\varepsilon)}{(1+\eta)} 
    \frac{(\btheta-1)}{\theta} \frac{2^{-n(I(U;Z) - \delta_1(\delta))}}{(1-\varepsilon)} \\
     %%%%%%%%%%%%%%%
     &\leq \frac{1}{(1+\eta)}2^{n(\Rbar - R - I(U;Z) + \delta_1(\delta) + 1/n)}. 
\end{align*}
where $(a)$ follows by using the Markov chain $U-X-Z$, $(b)$ follows from the properties of the joint typical sequences and $\delta_1(\delta)$ is a function that follows from the characterization of the size of the typical set \cite{cover2006elements}. Therefore, if $(\Rbar -R) < I(U;Z) - \delta_1 - 1/n$, the expected error $\EE[\Ipmf \peonetwo]$ decays exponentially and can be made arbitrarily small, for sufficiently large $n$. Hence, we get $\EE[\Ipmf \peone] \leq \EE[\Ipmf \peoneone] + \EE[\Ipmf \peonetwo] \leq 3\epsilon.$ This completes the proof of Proposition \ref{prop:binning}.


\subsection{Proof of Proposition \ref{prop:covering}}
\label{app:proof:prop:covering}
Consider the following inequalities:
    \begin{align*}
         \EE[&\Ipmf \ce] \\
         &\leq \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \EE[\I_{\set{\Un(l) = \un}}] \I_{\set{\calB(l) = m}} P_{YZ|U}^n(\yn,\zn|\un) \\
    & \hspace{100pt} \times \Big|\I_{\set{\xn \in \Txcond}} - \sum_{\substack{\sfxn \in \Txcond}} W^n_{X|YZ}(\sfxn
    |\yn,\zn)
    \Big|W^n_{X|YZ}(\xn|\yn,\zn)\\
    %%%%%%%%
    &= \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{1}{(1+\eta)} 
    \I_{\set{\calB(l) = m}} P_{UYZ}^n(\un,\yn,\zn) \\
    & \hspace{100pt} \times \Big|\I_{\set{\xn \in \Txcond}} - \sum_{\substack{\sfxn \in \Txcond}} W^n_{X|YZ}(\sfxn
    |\yn,\zn)
    \Big|W^n_{X|YZ}(\xn|\yn,\zn)\\
    & \overset{a}{\leq} \sum_{\substack{\un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{1}{(1+\eta)} 
    \I_{\set{\calB(l) = m}} P_{UYZ}^n(\un,\yn,\zn) \\
    &\hspace{100pt}\times 2  \!\!\!\!\! \sum_{\substack{\xn \in \Txcond \\ \sfxn \notin \Txcond}} \!\!\!\!\!W^n_{X|YZ}(\xn|\yn,\zn)W^n_{X|YZ}(\sfxn|\yn,\zn)\\
    &\overset{b}{=}\sum_{\substack{\un \in \Tu \\}} \sum_{\substack{\yn \\ \zn}} 
\frac{2}{(1+\eta)}  P_{UYZ}^n(\un,\yn,\zn)  \!\!\!\!\!\sum_{\substack{\xn \in \Txcond \\ \sfxn \notin \Txcond}}\!\!\!\!\! W^n_{X|YZ}(\xn|\yn,\zn)W^n_{X|YZ}(\sfxn|\yn,\zn)\\
&\overset{c}{\leq}\sum_{\substack{\un \in \Tu }}\sum_{\substack{\yn \\ \zn}}  
\frac{2}{(1+\eta)}  P_{UYZ}^n(\un,\yn,\zn) \!\!\!\!\! \sum_{\substack{\sfxn \notin \Txcond}}W^n_{X|YZ}(\sfxn|\yn,\zn)\\
&\overset{d}{\leq}
 2\frac{(1-\varepsilon)}{(1+\eta)} \sum_{\un \in \Tu} \frac{P_{U}^n(\un)}{(1-\varepsilon)} \sum_{\xn \notin\Txcond}W^n_{X|U}(\xn|\un)\\
 &\overset{e}{\leq}
 2\epsilon \frac{(1-\varepsilon)}{(1+\eta)} \sum_{\un \in \Tu} \frac{P_{U}^n(\un)}{(1-\varepsilon)} = 2\epsilon \frac{(1-\varepsilon)}{(1+\eta)} \leq 2\epsilon,
    \end{align*}
  for all sufficiently large $n$, and all $\eta,\delta > 0$, where $(a)$ follows by splitting the summation over $\xn \in \Tx$ as summation over $\set{\xn \in \Txcond} \cap \set{\xn \in \Tx}$ and $\set{\xn \notin \Txcond} \cap \set{\xn \in \Tx}$, $(b)$ follows from the fact that $\sum_{m \in [\theta]} \I_{\set{\calB(l) = m}} = 1$, $(c)$ follows by upper bounding the term $\sum_{\xn \in \Txcond} W^n_{X|YZ}(\xn|\yn,\zn)$ by $1$, $(d)$ follows from the Markov chain $U - (Y,Z) - X$ and writing $\sum_{\yn\zn} P_{UYZ}^n W^n_{X|UYZ} = P^n_U W^n_{X|U}$, and $(e)$ follows from the conditional typically argument. This completes the proof of Proposition \ref{prop:covering}.

  \subsection{Proof of Proposition \ref{prop:encoding_error}}\label{app:proof:prop:encoding_error}
 Consider the following set of inequalities:
\begin{align*}
    \EE[&\Ipmf \ee] \\
    &\overset{a}{\leq} \sum_{\substack{\xn \notin \Tx \\ \un \in \Tu \\ \sfxn \in \Txcond}}\sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{P^n_U(\un)}{(1+\eta)} 
    \I_{\set{\calB(l) = m}} 
    P_{XZ|U}^n(\sfxn,\zn|\un)P^n_{Y|UZ}(\yn|\un,\zn)W^n_{X|YZ}(\xn|\yn,\zn) \\
    &\overset{b}{\leq} \sum_{\substack{\xn \notin \Tx \\ \un \in \Tu }}\sum_{\substack{\yn\\\zn}} 
    \frac{1}{(1+\eta)} 
    P_{UYZ}^n(\un,\yn,\zn)W^n_{X|YZ}(\xn|\yn,\zn) \sum_{\sfxn \in \Txcond}W^n_{X|YZ}(\sfxn|\yn,\zn) \\
    &\overset{c}{\leq} \sum_{\substack{\xn \notin \Tx \\ \un \in \Tu }} 
    \frac{1}{(1+\eta)} 
    P_{U}^n(\un)W^n_{X|U}(\xn|\un)\\
    &\overset{}{\leq} \sum_{\substack{\xn \notin \Txcond \\ \un \in \Tu }} 
    \frac{1}{(1+\eta)} 
    P_{U}^n(\un)W^n_{X|U}(\xn|\un)\\
    &\overset{d}{\leq} 
    \epsilon\frac{(1-\varepsilon)}{(1+\eta)} 
    \sum_{\un\in \Tu}P_{U}^n(\un) =    \epsilon\frac{(1-\varepsilon)}{(1+\eta)} \leq \epsilon,
\end{align*}
where $(a)$ follows from the Markov chain $U-X-Z$, $(b)$ follows from the Markov chain $X - (U,Z) - Y$ and the fact that $\sum_{m\in [\theta]} \I_{\set{\calB(l) = m}} =1$, $(c)$ follows from the Markov chain $U- (Y,Z) - X$, and $(d)$ follows from the standard conditional typicality argument. This completes the proof of Proposition \ref{prop:encoding_error}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Proposition \ref{prop:code_dependent_RV}}
\label{app:prop:code_dependent_RV}
Consider the following inequalities:
\begin{align*}
    \EE_\PP[E_1] &= \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]} {(\Theta\bTheta)}^{-1}\EE_\PP[\Tr\{\rhotilde_{m,k}^{\refe B}\}]\\
    & = \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]} {(\Theta\bTheta)}^{-1}\!\!\!\!\!\!\sum_{\xn \in \Txqc}\!\!\!\frac{P^n_X(\xn)}{(1-\varepsilon)}\Tr\{\rhotilde_{\xn}^{\refe B}\}\\
    & \geq \!\!\!\!\!\!\sum_{\xn \in \Txqc}\!\!\!\frac{P^n_X(\xn)}{(1-\varepsilon)}(1-2\varepsilon-2\sqrt{\varepsilon}) \geq (1-\epsilon),
\end{align*}
where the first inequality follows from \cite[Eqn. 23]{wilde_e}. Now, using \eqref{eq:closeness_ref_SI}, we get 
\begin{align*}
    \EE_\PP[E_2] &= \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]}{(\Theta\bTheta)}^{-1}\ \EE_\PP[\|\rhotilde_{m,k}^{\refe B} - \calW^{\refe B}_{m,k}\|_1]\\
    &= \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]} {(\Theta\bTheta)}^{-1}\!\!\!\!\!\!\sum_{\xn \in \Txqc}\!\!\!\frac{P^n_X(\xn)}{(1-\varepsilon)}\|\rhotilde_{\xn}^{\refe B} - \calW^{\refe B}_{\xn}\|_1\leq \epsilon.
    % \\
    % \text{and }E_3&:=\sum_{m\in [\Theta]} \sum_{k\in[\bTheta]}{(\Theta\bTheta)}^{-1} \ \Tr\big\{(I-\dpovm_{k}^{(m)})\calW_{m,k}^B\big\},
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:qc_NC}}
\label{app:prop:proof:qc_NC}
    From equations \eqref{eqn:omegamk} and \eqref{eqn:lambdamk}, we rewrite $ \zeta_{\text{NC}}$ as
    \begin{align*}
        \zeta_{\text{NC}} &
        % =  \sum_{\substack{k'\in [\bTheta]\cup \{0\}}}\!\!\!\! \Tr\{\lambda_{(0,0),k'}^{\refe B}\}
        = \sum_{\substack{k'\in [\bTheta]\cup \{0\}}}\!\!\!\! \Tr\big\{\big({I^{\refe}}^{\tensor n} \tensor \Lambda_{k'}^{(m)}\big)\omega^{RB}_{0,0}\big\} = \Tr\big\{\big({I^{\refe}}^{\tensor n} \tensor \!\!\!\!\!\!\sum_{\substack{k'\in [\bTheta]\cup \{0\}}}\!\!\!\!\!\!\dpovm_{k'}^{(m)} \big)\omega^{RB}_{0,0}\big\} = \Tr\{\omega^{RB}_{0,0}\},
    \end{align*}
    where the equality follows because $\sum_{\substack{k'\in [\bTheta]\cup \{0\}}}\dpovm_{k'}^{(m)} = I$.
    Consider the following inequalities:
    \begin{align*}
        \EE_\PP[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{NC}}] \overset{a}{\leq} \EE_\PP[\Tr\{\omega^{RB}_{0,0}\}] &= 
        % \Tr\{\epovm_{0,0}^{A}\ {\rho^A}^{\tensor n}\} = 
        \EE_\PP\Big[\Tr\Big\{\Big(I-\sum_{m\in [\Theta]}\sum_{k\in[\bTheta]}\epovm_{m,k}^{\refe B}\Big) {\calW^{\refe B}}^{\tensor n}\Big\}\Big]\\
        &=
        1-\sum_{m\in [\Theta]}\sum_{k\in[\bTheta]}\EE_\PP\Big[\Tr\Big\{\epovm_{m,k}^{\refe B}{\calW^{\refe B}}^{\tensor n}\Big\}\Big]\\
        &= 1-\frac{(1-\varepsilon)}{(1+\eta)}\sum_{m\in [\Theta]}\sum_{k\in[\bTheta]}(\Theta\bTheta)^{-1}\EE_{\PP}[\Tr\{\rhotilde_{m,k}^{\refe B}\}]\\
        &\overset{b}{\leq}1-\frac{(1-\varepsilon)}{(1+\eta)}(1-\epsilon) \leq \tilde{\epsilon} \in (0,1),
    \end{align*}
    for all sufficiently large $n$ and all sufficiently small $\eta, \delta>0$, where $(a)$ is based on the fact that $\I_{\curly{\mbox{\normalfont sP}}}\leq 1$ and  $(b)$ follows from Proposition \ref{prop:code_dependent_RV}. This completes the proof of Proposition \ref{prop:qc_NC}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:qc_NP}}
\label{app:prop:proof:qc_NP}
Consider the following inequalities:
\begin{align*}
    \EE_\PP[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{NP}}] &\leq \EE_\PP\Big[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}
\Tr\big\{\big(I-\dpovm_{k}^{(m)} \big)\omega^{B}_{m,k}\big\}\}\Big]\\
&\overset{a}{\leq} \EE_\PP\bigg[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma\ \Tr\big\{\big(I-\dpovm_{k}^{(m)} \big)\calW^{B}_{m,k}\big\}\} + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma\ \|\rhotilde^B_{m,k}-\calW^B_{m,k}\|_1\bigg]\\
&\overset{b}{\leq} \frac{(1-\varepsilon)}{(1+\eta)}\ \EE_\PP\bigg[\frac{1}{\Theta}\sum_{m \in [\Theta]}\frac{1}{\bTheta}\sum_{\substack{k\in [\bTheta]}}
\Tr\big\{\big(I-\dpovm_{k}^{(m)} \big)\calW^{B}_{m,k}\big\}\}\bigg]\\
&\hspace{50pt}+ \frac{(1-\varepsilon)}{(1+\eta)}\ \EE_\PP\bigg[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}
(\Theta\bTheta)^{-1}\|\rhotilde^{\refe B}_{m,k}-\calW^{\refe B}_{m,k}\|_1\bigg]\overset{c}{\leq} 2 \frac{(1-\varepsilon)}{(1+\eta)}\epsilon \leq 2\epsilon,
\end{align*}
where $(a)$ follows from the trace inequality: $\Tr\{\Lambda \rho\} \leq \Tr\{\Lambda \sigma\}+\|\rho-\sigma\|_1$ (Lemma \ref{app:lemma:traceinequality} in Appendix \ref{app:useful lemmas}), $(b)$ follows from the definition of  $\gamma = \frac{(1-\varepsilon)}{(1+\eta)}(\Theta\bTheta)^{-1}$ and monotonicity of trace distance, and $(c)$ follows from Proposition \ref{prop:code_dependent_RV} and a weaker version of Proposition \ref{prop:qc_packing}, i.e., 
\begin{equation}\label{eqn:qc_packing}
    \EE_\PP\left[\frac{1}{\Theta}\sum_{m\in [\Theta]}\frac{1}{\bTheta}\sum_{k\in[\bTheta]} \Tr\left\{(I-\dpovm_{k}^{(m)})\calW_{m,k}^B\right\}\right] \leq \epsilon,
\end{equation}
for sufficiently small $\delta>0$ and for all sufficiently large $n$, if $\frac{1}{n}\log(\bTheta)<\chi(\{P_X(x),\calW^B_x\}).$
This completes the proof of Proposition \ref{prop:qc_NP}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:qc_cov}}
\label{app:prop:proof:qc_cov}
Consider the following inequalities:
    \begin{align*}
\EE_\PP[&\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{C}}] \\
&\leq\EE_\PP\Big[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\Big]\\
&\overset{a}{\leq}\EE_\PP\Big[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\omega_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\Big]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{b}{\leq}\EE_\PP\Big[\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma \ \Tr\{\rhotilde_{m,k}^{\refe B}\}\Big\|\frac{\rhotilde_{m,k}^{\refe B}}{\Tr\{\rhotilde_{m,k}^{\refe B}\}} -  \rhotilde_{m,k}^{\refe B}\Big\|_1 + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma \ \Tr\{\rhotilde_{m,k}^{\refe B}\}\|\rhotilde_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\Big]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{c}{\leq}\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma \ (1-\EE_\PP[\Tr\{\rhotilde_{m,k}^{\refe B}\}]) + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\gamma \  \EE_\PP[\|\omega_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1]\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{}{\leq}\frac{(1-\varepsilon)}{(1+\eta)}\Big(1-\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}(\Theta\bTheta)^{-1}\EE_\PP[\Tr\{\rhotilde_{m,k}^{\refe B}\}] + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}(\Theta\bTheta)^{-1} \EE_\PP[\|\omega_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1]\Big)\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\overset{d}{\leq}2\frac{(1-\varepsilon)}{(1+\eta)}\epsilon \leq 2\epsilon,
    \end{align*}
    where $(a)$ follows from the application of the trace inequality: $\Tr{\Lambda \rho} \leq \Tr{\rho}$ for $0\Lambda\leq I$ and positive operator $\rho$, $(b)$ follows by adding and subtracting appropriate terms and then applying triangle inequality, $(c)$ follows from the definition of $\gamma$, and $(d)$ follows from Proposition \ref{prop:code_dependent_RV}. This completes the proof of Proposition \ref{prop:qc_cov}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Proof of Lemma \ref{lemma:sols_linearEqn_close}}\label{app:proof:lemma:sols_linearEqn_close}
We begin the proof by considering the following convex optimization problem:
\begin{align}
    \min_{x} & \frac{1}{2}\|x-x_0\|_2^2 \nonumber \\
    \text{subject to } & \sfA x=\mathsf{b}, \I^\texttt{T}x = 1,x \geq 0. 
\end{align}
Our objective is first to find the unique optimal solution $x^*$ of the above convex optimization problem and then show that for any $x_0 \geq 0$, if $\|\sfA x_0-\mathsf{b}\|_1 \leq \delta$ and $\I^\texttt{T}x_0 = 1$, then  $x^*$ satisfies $\|x^* -x_0\|_1 \leq \tilde{\delta}(\delta)$. 
Toward solving the optimization problem, let, ${\mathbf{A}}_{} := [\sfA^\texttt{T} \; \I]^\texttt{T}$  and ${\mathbf{b}} := [\mathsf{b}^\texttt{T}\; 1]^\texttt{T}$. Assume $\bfb \in \text{col}(\bfA)$, i.e., $\bfb$ belongs to the column space of $\bfA$, otherwise, $\bfA x=\bfb$ has no solution. Moreover, without loss of generality, assume $\bfA$ is full row rank; if not, one can remove the linearly dependent rows to make it full row rank.

The Lagrangian for this problem is 
$$\mathcal{L}(x,\lambda,\mu):= \frac{1}{2}\|x-x_0\|^2_2 + \lambda^\texttt{T}(\mathbf{A}x_0-\mathbf{b}) - \mu^\texttt{T}x,$$
where $\lambda$ and $\mu\geq 0$ are the Lagrange multipliers for the equality constant $\mathbf{A}x=\mathbf{b}$ and non-negativity constraint $x\geq 0$, respectively. The KKT conditions are as follows:
\begin{itemize}
    \item \textit{Stationarity}:  The gradient of the Lagrangian with respect to 
$x$ must vanish, i.e.,
$$\partial_x \mathcal{L}(x,\lambda,\mu) = (x-x_0) + \mathbf{A}^\texttt{T}\lambda-\mu = 0.$$
This simplifies to $x = x_0-\mathbf{A}^\texttt{T}\lambda+\mu.$
\item \textit{Primal Feasibility}: $\mathbf{A}x=\mathbf{b}$. Substituting the value of $x$ from the stationarity condition, we get $(\bfA\bfA^\textT)\lambda = (\bfA x_0-\bfb)+\bfA\mu$.  Thus, we get, $$\lambda = (\bfA\bfA^\textT)^{-1}(\bfA x_0-\bfb)+(\bfA\bfA^\textT)^{-1}\bfA\mu.$$
Once the $\lambda$ is determined, substituting it back into stationary condition, we get $$x = x_0 - \bfA^\textT(\bfA\bfA^\textT)^{-1}(\bfA x_0-\bfb)+(I-\bfA^\textT(\bfA\bfA^\textT)^{-1}\bfA)\mu.$$
\item \textit{Dual Feasibility}: $x\geq 0 \eqand \mu \geq 0$.
\item \textit{Complementary Slackness}: $\mu_i x_i= 0$ for all $i \in [\beta].$
\end{itemize}
We can re-write the above expression of $x$ using the generalized inverse\footnote{Often known as pseudoinverse or Moore–Penrose inverse.}\cite{ben2006generalized} as $$x = x_0 - \bfA^{+}(\bfA x_0-\bfb)+(I-\bfA^{+}\bfA)\mu,$$
where $\bfA^+ = \bfA^\textT(\bfA\bfA^\textT)^{-1}$ when $\bfA$ is full-row rank. The above expression is also valid for arbitrary matrix $\bfA \in \RR^{\alpha\times\beta}$, where the generalized inverse is computed using singular value decomposition (SVD) as follows. Let \(\mathbf{A} = U_{\alpha \times \alpha} \Sigma_{\alpha \times \beta} V^{\textT}_{\beta \times \beta}\) be the SVD of \(\mathbf{A}\), where \(U\) and \(V\) are unitary matrices, and \(\Sigma\) is a rectangular diagonal matrix with non-negative real numbers on the diagonal. The generalized inverse of \(\mathbf{A}\) is given by \(\mathbf{A}^+ = V \Sigma^+ U^{\textT}\), where \(\Sigma^+_{\beta \times \alpha}\) is obtained by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros unchanged, and finally transposing the resulting matrix.

\noindent Now, using complementary slackness condition, we find $x^*$ as follows: Let $w := x_0 - \bfA^+(\bfA x_0-\bfb)$. 
\begin{itemize}
    \item If $w\geq 0$, $x^* = w$. 
    \item If $w_{\sfS} \leq 0,$ for some $\sfS\subseteq [\beta]$. Then, $x^* = x_0 - \bar{\bfA}^+(\bar{\bfA} x_0-\bfb)$, where $\bar{\bfA}$ by setting the entries in the columns corresponding to the set 
$\sfS$ to zero for every row, except for the last row, which consists entirely of ones. 
\end{itemize}
So far, we have constructed a $x^*$, which minimizes the above-stated optimization problem. Now, we find the objective function value corresponding to $x^*$.
% Using the KKT condition \cite{boyd2004convex},$x^* = x_0 - \bfA^{+}(\sfAx_0-\bfb)$.Clearly, $\sfAx_0 = \bar{\mathsf{b}}$, since, $\sfAx=\bfb$ has solutions iff $\bfA\bfA^{+}\bfb = \bfb$, where $\bfA\bfA^{+}$ is the projection onto the column space of $\bfA$.
\begin{align*}
   \|x^*-x_0\|_1 &\overset{a}{\leq} \sqrt{\beta}\|x^*-x_0\|_2 \\
    &= \sqrt{\beta} \|\bfA^{+}(\bfA x-\bfb)\|_2 \\
    &\overset{b}{\leq} \sqrt{\beta} \|\bfA^{+}\|_2 \|(\bfA x-\bfb)\|_2 \\
    % &\overset{}{=} \sqrt{\beta} \|\bfA^{+}\|_2 \|(\bfA x-{\mathsf{b}})\|_2 \\
    &\overset{c}{\leq} \sqrt{\beta} \|\bfA^{+}\|_2 \|(\bfA x-\mathsf{b})\|_1 \\
    &\overset{d}{\leq} \delta \sqrt{\beta} \|\bfA^{+}\|_2 \\
    &\overset{e}{=} \frac{\delta \sqrt{\beta}}{\sigma^{+}_{\min}} \quad \text{ where $\sigma^{+}_{\min}$ is the smallest non-zero singular value of $\bfA$},
\end{align*}
where $(a)$ follows from Cauchy-Schwartz inequality, $(\mathsf{b})$ follows from the sub-multiplicative property of matrix norm, $(c)$ follows from triangle inequality (or $\|\cdot\|_p \leq \|\cdot\|_1$ from Minkowski inequality), $(d)$ follows from the statement of the lemma, and $(e)$ follows from the definition of 2-norm of a matrix, i.e., the largest singular value.  Similarly, for the other case, when $w_\sfS \leq 0$ for some $S\in[\beta]$. We get $\|x^*-x_0\|_1 \leq \frac{\delta\sqrt{\beta}}{\sigmabar^{+}_{\text{min}}}$, where $\sigmabar^{+}_{\min}$ is the smallest non-zero singular value of $\bar{\bfA}$. This completes the proof of Lemma \ref{lemma:sols_linearEqn_close}.
