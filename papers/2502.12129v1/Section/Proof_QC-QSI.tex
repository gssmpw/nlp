% \textit{Overview of the strategy.} Sender simulates the measurement on the 

For a given $(\rho^{AB},\sfX,\calW_{\!\ \sfX \rightarrow AB})$ QC-QSI source coding setup, we choose a $P_X(x) \in \calA(\rho^{AB},\calW_{\!\ \sfX \rightarrow AB})$. From now on, we let $\Theta := 2^{nR}$  and $\Theta := 2^{n\Rbar}$.

\noindent \textbf{Codebook Design}: We generate a codebook $\calC$ consisting of $n$-length codewords by randomly and independently selecting $\Theta\times\bTheta$ sequences $\calC\deq \{\Xn(m,k): m\in [\Theta]\eqand k\in [\bTheta]\}$ according to the following pruned distribution:
 \begin{align}\label{def:qc_distribution}
     &\codeDistribution(\Xn(m) = \xn) = \left\{\!\!\!\!\begin{array}{cc}
          \dfrac{P_X^n(\xn)}{(1-\varepsilon)}  & \mbox{for} \; \xn \in \Txqc\\
           0 &  \mbox{otherwise,}
     \end{array} \right. \!\!
 \end{align} 
  where $ P_X^n(\xn) = \prod_{i=1}^n P_X(x_i)$, $\Txqc$ is the $\delta$-typical set corresponding to the distribution $P_X$ on the set $\sfX$, and $\varepsilon(\delta,n) \triangleq \sum_{\xn \not \in \Txqc} P_X^n(\xn)$. Note that $\varepsilon(\delta,n) \searrow 0$ as $n \rightarrow \infty$ and for all sufficiently small $\delta > 0$. 
  
\vspace{2pt}
\noindent\textbf{Construction of Encoding POVM}:
Let $\Pi_{\rho}^{\refe B}$ and $\Pi_{\xn}^{\refe B}$ denote the $\delta$-typical and conditional $\delta$-typical projectors defined as in \cite[Def. 15.1.3]{wilde_arxivBook} and \cite[Def. 15.2.4]{wilde_arxivBook}, with respect to $\calW^{\refe B} \deq \sum_{x\in\sfX} P_X(x)\calW^{\refe B}_x$ and $\calW_{x}^{\refe B}$, respectively.
For all $\xn \in \Txqc$, define  
\begin{align*}
    \rhotilde_{\xn}^{\refe B} \deq \hat{\Pi} \Pi_{\rho}\Pi_{\xn}\calW_{\xn}^{\refe B}\Pi_{\xn}\Pi_{\rho}\hat{\Pi} \ \! \eqand \ \! \rhotilde^{\refe B} \deq \EE_{\PP}[\rhotilde_{\Xn}^{\refe B}],
\end{align*}
and $\rhotilde_{\xn}^{\refe B} = 0$ for $\xn \notin \Txqc$, where $\hat{\Pi}$ is the cut-off 
 projector onto the subspace spanned by the eigenbasis of $\EE[\Pi_{\rho}\Pi_{\Xn}\calW_{\Xn}^{\refe B}\Pi_{\Xn}\Pi_{\rho}]$ with eigenvalues greater than $\epsilon d$, where $d \!\deq\! 2^{-n(S(\calW^{\refe B})+\delta_1)}$ and $\delta_1$ will be specified later.  
 Using the Average Gentle Measurement Lemma \cite[Lemma 9.4.3]{wilde_arxivBook}, for any given $\epsilon \in (0,1)$, and all sufficiently large $n$ and all sufficiently small $\delta$, we have 
\begin{align} \label{eq:closeness_ref_SI}
    \EE_{\PP}[\|\rhotilde_{\Xn}^{\refe B}  - \calW_{\Xn}^{\refe B} \|_1] \leq \epsilon.
\end{align}
The proof follows from the derivation of \cite[Eq. 35]{wilde_e}. Using the above definitions, for all $\xn \in \sfX^n$, we construct the operators,
\vspace{-0.065in}
\[\epovm_{\xn}^{\refe B} \deq \gamma_{\xn}\ {(\calW^{{\refe B}^{\tensor n}})}^{-1/2} \rhotilde_{\xn}^{\refe B} {(\calW^{{\refe B}^{\tensor n}})}^{-1/2} ,\text{ where }\gamma_{\xn} \!\deq\! \gamma \cdot |\{(m,k)\!:\!\Xn(m,k)\! =\! \xn\}|,\]
 $\gamma \deq (\Theta\bTheta)^{-1}  \frac{(1-\varepsilon)}{(1+\eta)}$ and $\eta \in (0,1)$ is a parameter that determines the probability of not obtaining a sub-POVM. Note that in the above definition operator  $\epovm_{\xn}^{\refe B}$ acts on $(\calH_{\refe^n} \tensor \calH_{\Bn})$, however, we define $\epovm^{A}_{\xn} \in \calL(\calH_{\An})$. To obtain this, we transform $\epovm_{\xn}^{\refe B}$ as 
\[\epovm_{\xn}^{A} = \sum_{\an \abarn} \<\an|\epovm_{\xn}^{\refe B} \ |\abarn\>_{\refe B} |\an\>\<\abarn|_{A},\]
where $\{|a\>_A\}$ is an eigenbasis of $\rho^A$, $\{|a\>_{RB}\}$ is an eigenbasis of $\rho^{\refe B} := \Tr_A\{|\phi_{\refe AB}\>\<\phi_{\refe AB}|\}$, and $|\phi_{\refe AB}\>$ is the canonical purification of $\rho^{AB}$. Furthermore, by using the equivalence of purification \cite[Thm. 5.1.1]{wilde_arxivBook}, it can be easily shown that $\Tr\{\epovm_{\xn}^A\rho^{A^{\tensor n}}\} = \Tr\{\epovm_{\xn}^{\refe B} \ \calW^{\refe B ^{\tensor n}}\}.$

Let $\I_{\{\mbox{sP}\}}$ denote the indicator random variable corresponding to the event that  $\{\epovm_{\xn}^{A} \colon \xn \in  \Txqc\}$ forms a  sub-POVM. We now provide a proposition from \cite{winter}, which will be helpful later in the analysis.
\begin{prop} \label{prop:enc_subpovm}For all $\epsilon, \eta \in (0,1)$, for all sufficiently small $\delta > 0$, and for all sufficiently large $n$, we have
$\EE[{\I_{\{\mbox{\normalfont sP}\}}}] \geq 1-\epsilon$, if $\frac{1}{n}(\log(\Theta)+\log(\bTheta)) > \chi(\{P_X(x),\calW^{\refe B}_x\})$.
\end{prop}
If $\I_{\{\mbox{sP}\}} = 1$, then construct sub-POVM $\Gamma^{(n)}_{\sfA}$ as follows: $\Gamma^{(n)}_{\sfA} \deq \big\{\sum_{k\in[\bTheta]}\epovm_{\xn(m,k)}^{A}\big\}_{m\in[\Theta]}.$ 
We then add an additional operator $\epovm_{0}^A \deq (I\!-\!\sum_{m\in[\Theta]}\sum_{k\in [\bTheta]}\epovm_{\xn(m,k)}^A)$, associated with an arbitrary sequence $\xn_0 \in \sfX^n \backslash\Txqc$, to form a valid POVM $[\Gamma^{(n)}_\sfA]$ with at most $(\Theta \times \bTheta+1)$ elements. The extra element $\epovm_{\xn_0}^A$ corresponds to a failed encoding.
% If $\I_{\{\mbox{sP}\}} = 0$, then we define $\Gamma^{(n)} = \{I\}$ and associate it with $\xn_0$. 

\vspace{2pt}
\noindent\textbf{Construction of Decoding POVM}:
For the ensemble $\{P_X(x),\calW^B_x\}$, we construct a collection of $n$-letter Bob's POVMs, one for each $m \in [\Theta]$, capable of decoding the message $k\in [\bTheta]$. 
Upon receiving the message $m$, Bob performs a sequence of binary measurements $\{\Pi_{\xn(m,k)},(I-\Pi_{\xn(m,k)})\}$ for all sequence $\xn(m,k) \in \calC$, where $\Pi_{\xn(m,k)}$ is a conditional typical projector for the tensor-product state $\calW^B_{\xn(m,k)}$.
% (Bob's system after reconstructing $\xn(m,k')$ and applying the tensor-product CQ channel $\calW_{X_i\rightarrow RB}^{\tensor n}$). 
% Let $a^{(m)}_{1}, a^{(m)}_{2}, \cdots, a^{(m)}_{\Rbar}$ enumerate all the codewords for a fixed $ m\in [2^{nR}]$ and let $a_j^{(m)}$ denote the correct codeword $\xn(m,k)$ produced from the outcomes of Alice's POVM.
% Define the decoding POVM as $$\dpovm_{k}^{(m)} \deq \Pibar_{a^{(m)}_{1}} \cdots \Pibar_{a^{(m)}_{j-1}} \Pi_{a^{(m)}_{j}} \Pibar_{a^{(m)}_{j-1}} \cdots   \Pibar_{a^{(m)}_{1}},$$
Define the decoding POVM element as
$$\dpovm_{k}^{(m)} \deq \Pibar^{(m)}_{1} \cdots \Pibar^{(m)}_{k-1} \
\Pi^{(m)}_{k} \ \Pibar^{(m)}_{k-1} \cdots \Pibar^{(m)}_{1},$$
where $ \Pibar^{(m)}_{k}$ and $ \Pi^{(m)}_{k}$ are the shorthand notation for $(I-\Pi_{\xn(m,k)})$ and $\Pi_{\xn(m,k)}$, respectively.
% Let $\Bar{\rho}_{m,l}^{\refe B} \deq (\Tr\{\epovm_{m,l}^{A^n} \rho_A^{\tensor n}\})^{-1}\Tr_{A^n}\{({I^{\refe B}}^{\tensor n}\!\tensor \epovm_{m,l}^{A^n})({\Phi^{\refe AB}}^{\tensor n})\}$ be the normalized post-measurement state from the Bob's encoding,  where $\epovm_{m,l}^{A^n}$ is used as a shorthand notation for $\epovm_{\xn(m,l)}^{A^n}$.
The following proposition demonstrates that for Bob's POVMs, we can make the average probability of error arbitrarily small by using the non-commutative union bound \cite{sen2012achieving}.
\begin{prop}\label{prop:qc_packing}
    Given the ensemble $\{P_X(x),\calW^B_x\}$ and the collection of POVMs $\{\Xi_{k}^{(m)}\}_{k\in[\bTheta]}$, for any $\epsilon \in (0,1)$
$$\EE_\PP\left[\frac{1}{\bTheta}\sum_{k\in[\bTheta]} \Tr\left\{(I-\dpovm_{k}^{(m)})\calW_{m,k}^B\right\}\right] \leq \epsilon,$$
for sufficiently small $\delta>0$ and for all sufficiently large $n$, and for all $m \in [\Theta]$, if $\frac{1}{n}\log(\bTheta) < \chi(\{P_X(x),\calW^B_x\})$. 
\end{prop}
\begin{proof}
    The proof follows from packing lemma using sequential decoding \cite[Sec. 16.6]{wilde_arxivBook}, while making the following identification. For each $m\in\calM$, identify $\calM$ as $[\bTheta]$, $\calX$ as $\Txqc$, $\{\sigma_{C_m}\}_m$ with $\{\calW_{m,k}^{\refe B}\}_k$, $\Pi_x$ with $\Pi_{k}^{(m)}$, $d$ with $2^{n(S(X|B)_{\tau}-\Bar{\delta})}$, and $D$ with $2^{n(S(B)_\tau-\Bar{\delta})}$, where $\tau^{XB} := \sum_x P_X(x) |x\>\<x|\tensor \calW^B_x$ and $\Bar{\delta} \searrow 0$ as $\delta\searrow 0$. 
    % For more details, please refer to \cite{wilde2013sequential,wilde_e}. 
\end{proof}
In general, the decoding POVM elements satisfy the condition $\sum_{k \in [\bTheta]} \dpovm_{k}^{(m)} \leq I$ for all $m \in [\Theta].$
Under the condition $\{\I_{\{\mbox{sP}\}} = 1\}$,
construct sub-POVM $\Gamma_\sfB^{(n)}$ as follows: $\{\dpovm_{k}^{(m)}\}_{k\in [\bTheta]}$ for each $m\in[\Theta]$. This sub-POVM is completed by adding an additional operator $\dpovm_{0}^{(m)}\deq(I\!-\!\sum_{k\in[\bTheta]}\dpovm_k^{(m)})$ to form a valid POVM $[\Gamma_\sfB^{(n}]$, for each $m\in [\Theta]$.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent\textbf{Decoding Unitary:} Observe that after applying the binary projectors Bob recovers the $(m,k)$, and consequently the sequence $\xn(m,k)$. The post-measured state after the sequential decoding can be expressed as
% $$\frac{1}{\Tr\{\dpovm_k^{(m)}\Bar{\rho}^{B}_{m,k}\}}({I^{\refe}}^{\tensor n}\tensor \Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)})\Bar{\rho}^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \Pibar_{1}^{(m)} \cdots \Pibar_{k-1}^{(m)}\Pi_{k}^{(m)}),$$ where $$\Bar{\rho}_{m,k}^{\refe B} \deq \frac{1}{\Tr\{\epovm_{m,k}^{A^n} \ {\rho^A}^{\tensor n}\}}\Tr_{A^n}\{({I^{\refe B}}^{\tensor n}\!\tensor \epovm_{m,k}^{A^n})({\Phi^{\refe AB}}^{\tensor n})\}$$ is the normalized post-measurement state from Alice's encoding. Here, $\epovm_{m,k}^{A^n}$ is used as a shorthand notation for $\epovm_{\xn(m,k)}^{A^n}$, $\Phi^{\refe A B}:= |\phi\>\<\phi|^{\refe AB}$, and $|\phi\>^{\refe AB} $ is the canonical purification of $\rho^{AB}$.
% Note that the projectors $\Pi_{k}^{(m)}\cdots \Pi_{1}^{(m)}$ and the POVM element ${\Lambda_{k}^{(m)}}$ are related by a polar decomposition, given as
% \begin{align*}
%     \sqrt{\Lambda_{k}^{(m)}} = U_{m,k} \ \Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)},
% \end{align*}
% for some unitary $U_{m,k}$. We make the use of this relation to design the decoding unitary as follows: Bob applies the binary projectors and recovers $\xn(m,k)$. Following this, Bob applies the unitary $U_{m,k}$ and the state becomes as follows:
% \begin{equation}\label{eqn:final_enc_dec}
%     \omega_{m,k}^{\refe B}\deq \frac{1}{\Tr\{\Lambda_{k}^{(m)}\Bar{\rho}^B_{m,k}\}}\left({I^{\refe}}^{\tensor n}\tensor \sqrt{\Lambda_{k}^{(m)}}\right)\Bar{\rho}^{\refe B}_{m,k}\left({I^{\refe}}^{\tensor n}\tensor \sqrt{\Lambda_{k}^{(m)}}\right).
% \end{equation}
% Finally, 

\noindent\textbf{Error Analysis}: We begin by defining the following code-dependent random variables \(E_1\), \(E_2\), and \(E_3\), which will be useful in the error analysis, given as:
\begin{align*}
    E_1 \deq \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]} {(\Theta\bTheta)}^{-1}\ &\Tr\{\rhotilde_{m,k}^{\refe B}\}\eqand E_2 \deq \sum_{m\in [\Theta]}\sum_{k\in [\bTheta]}{(\Theta\bTheta)}^{-1}\ \|\rhotilde_{m,k}^{\refe B} - \calW^{\refe B}_{m,k}\|_1,
    % \\
    % \text{and }E_3&:=\sum_{m\in [\Theta]} \sum_{k\in[\bTheta]}{(\Theta\bTheta)}^{-1} \ \Tr\big\{(I-\dpovm_{k}^{(m)})\calW_{m,k}^B\big\},
\end{align*}
where $\rhotilde_{m,k}^{\refe B}$ and $\calW_{m,k}^{\refe B}$ are the shorthand notation for $\rhotilde_{\xn(m,k)}^{\refe B}$ and $\calW_{\xn(m,k)}^{\refe B},$ respectively. We provide the following proposition that bound these terms under the condition $\I_{\curly{\mbox{\normalfont sP}}} = 1$.
\begin{prop}\label{prop:code_dependent_RV}
For all $\epsilon\in(0,1)$, for all sufficiently small $\eta, \delta>0$, and for all sufficiently large $n$, we have $\EE_\PP[E_1]\geq (1-\epsilon) \eqand  \EE_\PP[E_2]\leq \epsilon$.
\end{prop}
\begin{proof}
    The proof is provided in Appendix \ref{app:prop:code_dependent_RV}.
\end{proof}
Now, Observe that after applying the binary projectors Bob generates $(m,k')$, and consequently the sequence $\xn(m,k')$ using the decoding map $f$. The (unnormalized) post-measured state after the sequential decoding can be expressed as
$$({I^{\refe}}^{\tensor n}\tensor \Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)})\omega^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \Pibar_{1}^{(m)} \cdots \Pibar_{k-1}^{(m)}\Pi_{k}^{(m)}),$$ where 
\vspace{-10pt}\begin{equation}\label{eqn:omegamk}
    \omega_{m,k}^{\refe B} \deq \Tr_{A^n}\{({I^{\refe B}}^{\tensor n}\!\tensor \epovm_{m,k}^{A})({\phi^{\refe AB}}^{\tensor n})\} 
\end{equation}
is the unnormalized post-measured state from Alice's encoding. Here, $\omega_{m,k}^{\refe B} \eqand  \epovm_{m,k}^{A}$ are used as a shorthand notation for $\omega_{\xn(m,k)}^{\refe B} \eqand \epovm_{\xn(m,k)}^{A}$, respectively, and  $\phi^{\refe A B}:= |\phi\>\<\phi|^{\refe AB}$ and $|\phi\>^{\refe AB} $ is the canonical purification of $\rho^{AB}$. Furthermore, note that the projectors $\Pi_{k}^{(m)}\cdots \Pi_{1}^{(m)}$ and the POVM element ${\Lambda_{k}^{(m)}}$ are related by a polar decomposition, given as
\begin{align*}
    \sqrt{\Lambda_{k}^{(m)}} = U_{m,k} \ \Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)},
\end{align*}
for some unitary $U_{m,k}$. Therefore, Bob first applies the binary projectors and constructs $\xn(m,k')$. Following this, Bob applies the unitary $U_{m,k}$ and the (unnormalized) state becomes as follows:
\begin{equation}\label{eqn:lambdamk}
    \lambda_{(m,k),k'}^{\refe B}\deq\Big({I^{\refe}}^{\tensor n}\tensor \sqrt{\Lambda_{k'}^{(m)}}\Big)\omega^{\refe B}_{m,k}\Big({I^{\refe}}^{\tensor n}\tensor \sqrt{\Lambda_{k'}^{(m)}}\Big).
\end{equation}
% \begin{align}
% \widetilde{\Pi}_{k}^{(m)}&\deq\Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)}\\
%     \omega_{m,k}^{\refe B} &\deq \frac{1}{\Tr\{\epovm_{m,k}^{A^n} \ {\rho^A}^{\tensor n}\}}\Tr_{A^n}\{({I^{\refe B}}^{\tensor n}\!\tensor \epovm_{m,k}^{A^n})({\Phi^{\refe AB}}^{\tensor n})\} \\
%     % \lambda_{m,k}^{\refe B} &= \frac{1}{\Tr\{\dpovm_k^{(m)}\omega^{B}_{m,k}\}}({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k}^{(m)})\omega^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k}^{(m)})^{\dagger}\\
%     \lambda_{(m,k),k'}^{\refe B} &= \frac{1}{\Tr\{\dpovm_{k'}^{(m)}\omega^{B}_{m,k}\}}({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k'}^{(m)})\omega^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k'}^{(m)})^{\dagger}, 
%     % \quad \text{for $k\neq k'$},
% \end{align}
% \begin{align}
% \widetilde{\Pi}_{k}^{(m)}&\deq\Pi_{k}^{(m)}\Pibar_{k-1}^{(m)}\cdots \Pibar_{1}^{(m)}\label{eqn:pitilde}\\
%     \omega_{m,k}^{\refe B} &\deq \Tr_{A^n}\{({I^{\refe B}}^{\tensor n}\!\tensor \epovm_{m,k}^{A})({\phi^{\refe AB}}^{\tensor n})\} \label{eqn:omegamk}\\
%     % \lambda_{m,k}^{\refe B} &= \frac{1}{\Tr\{\dpovm_k^{(m)}\omega^{B}_{m,k}\}}({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k}^{(m)})\omega^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k}^{(m)})^{\dagger}\\
%     \lambda_{(m,k),k'}^{\refe B} &= ({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k'}^{(m)})\omega^{\refe B}_{m,k} ({I^{\refe}}^{\tensor n}\tensor \widetilde{\Pi}_{k'}^{(m)})^{\dagger}, \label{eqn:lambdamk}
%     % \quad \text{for $k\neq k'$},
% \end{align}
% where $\omega_{m,k}^{\refe B}, \lambda_{(m,k),k'}^{\refe B} \eqand  \epovm_{m,k}^{A}$ are used as a shorthand notation for $\omega_{\xn(m,k)}^{\refe B}, \lambda_{\xn(m,k')}^{\refe B}$, and $\epovm_{\xn(m,k)}^{A}$, respectively, and  $\phi^{\refe A B}:= |\phi\>\<\phi|^{\refe AB}$ and $|\phi\>^{\refe AB} $ is the canonical purification of $\rho^{AB}$.
% Here, $\omega_{m,k}^{\refe B}$ is the unnormalized post-measurement state from Alice's encoding, and $\lambda_{(m,k),k'}^{\refe B}$ is the unnormalized start after the sequential decoding. 
If $k=k'$ (indicating correct decoding), then $\lambda_{(m,k),k'}^{\refe B} = \lambda_{m,k}^{\refe B}$, i.e., Bob successfully recovers the sequence $\xn(m,k)$.
Now, following Definition \ref{def:qc_qsi_achievability}, our objective is to show that the following term 
\begin{align*}
    \EE_\PP[&\error] \\&= \EE_\PP\Bigg[ \Big\|\sum_{m \in [\Theta]\cup \{0\}}\sum_{\substack{k\in [\bTheta]\cup \{0\}}}\sum_{k'\in [\bTheta]\cup \{0\}} \!\!|\xn(m,k')\>\<\xn(m,k')|\tensor \big(\lambda_{(m,k),k'}^{\refe B} -  \Tr\{\lambda_{(m,k),k'}^{\refe B}\}\calW_{m,k'}^{\refe B}\big)\Big\|_1\Bigg]
\end{align*}
can be made arbitrarily small for sufficiently large $n$ for the code $\codebook$, where $\calW_{m,k'}^{\refe B}\deq \calW_{\xn(m,k')}^{\refe B}.$ 

First, we split the error $ \error $ into two terms using the indicator function $\I_{\curly{\mbox{\normalfont sP}}}$ as 
\begin{align}
\error&=\I_{\curly{\mbox{\normalfont sP}}} 
\error + \round{1- \I_{\curly{\mbox{\normalfont sP}}}}\error\\
&\leq \I_{\curly{\mbox{\normalfont sP}}} 
\error  + 2\round{1- \I_{\curly{\mbox{\normalfont sP}}}} \label{eqn:qcerrorsubpovm},
\end{align}
% Consider the un-normalized post-measured states on $\Bn$: $\rhotilde^B_{(m,k)} = \Tr_{\An}\{(\epovm_{(m,k)}^A\tensor I)(\rho^{{AB}^{\tensor n}})\}.$ Define, $\lambda_{(m,k)} := \Tr\{(\epovm_{(m,k)}^A\tensor I)(\rho^{{AB}^{\tensor n}})\}$, the probability of observing $(m,k)$ for Alice.
% The probability of correct decoding for Bob is $$\mu_{(m,k)}\! \deq \!\Tr\{\Pi_{a^{(m)}_{j}} \Pibar_{a^{(m)}_{j-1}}\!\cdots \!\Pibar_{a^{(m)}_{1}} \rhotilde^B_{a^{(m)}_{j}}\Pibar_{a^{(m)}_{1}} \!\cdots \!\Pibar_{a^{(m)}_{j-1}} \Pi_{a^{(m)}_{j}}\}$$
where \eqref{eqn:qcerrorsubpovm} follows from upper bounding the trace distance between two density operators by its maximum value of two. 
Under the condition $\I_{\curly{\mbox{\normalfont sP}}} = 1$, 
 \begin{align*}
     \error &\overset{}{\leq} \underbrace{\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\|\lambda_{m,k}^{\refe B} -  \Tr\{\lambda_{m,k}^{\refe B}\}\calW_{m,k}^{\refe B}\|_1}_{\zeta_{\text{CP}}} 
     + \ 2 \!\!\!\!\underbrace{\sum_{\substack{k'\in [\bTheta]\cup \{0\}}}\!\!\!\! \Tr\{\lambda_{(0,0),k'}^{\refe B}\}}_{\zeta_{\text{NC}}}
     \\
     &\hspace{50pt}+2\!\!\underbrace{\sum_{m \in [\Theta]}\sum_{\substack{k \neq k' \in [\bTheta]}}  \Tr\{\lambda_{(m,k),k'}^{\refe B}\}}_{\zeta_{\text{NP}_1}}  +   \ 2\underbrace{\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} \Tr\{\lambda_{(m,k),0}^{\refe B}\}}_{\zeta_{\text{NP}_2}},
 \end{align*}
 where the inequality follows from the fact that $\|\calW_{\xn}\|_1=1$ for all $\xn\in \sfX^n$ and  using triangle inequality.


\noindent \textbf{Step 1. Bounding the error induced by not covering, i.e., encoding error.} 

\noindent The error term $\zeta_{\text{NC}}$ captures the error induced by not covering the $n$-tensored posterior reference channel. We provide the following proposition that bounds this term.  
\begin{prop}\label{prop:qc_NC}
    For all $\epsilon\in(0,1)$, for all sufficiently small $\eta, \delta>0$, and for all sufficiently large $n$, we have $\EE_\PP[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{NC}}]\leq \epsilon$.
\end{prop}
\begin{proof}
The proof is provided in Appendix \ref{app:prop:proof:qc_NC}.
\end{proof}
% \textbf{EXTRA}: $1-\frac{(1-\varepsilon)}{(1+\eta)}\sum_{\xn \in \Txqc}\frac{P_\Xn(\xn)}{(1-\varepsilon)}\Tr\{\rhotilde_{\xn}^{\refe B}\}]$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Step 2. Bounding the error induced by not packing, i.e., decoding error.} 

\noindent The error term $\zeta_{\text{NP}} \deq \zeta_{\text{NP}_1} + \zeta_{\text{NP}_2}$ captures the error induced by not packing, i.e., incorrect decoding. Therefore, $\zeta_{\zeta_{\text{NP}}}$ can rewritten as the $(1-$ probability of correct decoding), given as
\begin{align}
\zeta_{\text{NP}} &= \sum_{m \in [\Theta]}\sum_{\substack{k \neq k' \in [\bTheta]}}  \Tr\{\lambda_{(m,k),k'}^{\refe B}\} +  \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} \Tr\{\lambda_{(m,k),0}^{\refe B}\}\nonumber\\
&= \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} \Big(\Tr\big\{\big({I^{\refe}}^{\tensor n} \!\tensor\!\!\!\!\sum_{\substack{k' \neq k \in [\bTheta]}} \!\!\!\!\dpovm_{k'}^{(m)} \big)\omega^{RB}_{m,k}\big\}   +   \Tr\big\{\big({I^{\refe}}^{\tensor n} \!\!\tensor \dpovm_{0}^{(m)} \big)\omega^{RB}_{m,k}\big\}\}\Big)\nonumber\\
& = \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}
\Tr\big\{\big(I-\dpovm_{k}^{(m)} \big)\omega^{B}_{m,k}\big\}\}\label{eqn:zetaNP},
\end{align}
where the second equality follows from \eqref{eqn:lambdamk} and $\omega^B_{m,k} = \Tr_{\refe^n}\{\omega^{\refe B}_{m,k}\}$. We provide the following proposition that bounds $\zeta_{\text{NP}}$.
\begin{prop}\label{prop:qc_NP}
    For all $\epsilon\in(0,1)$, for all sufficiently small $\eta, \delta>0$, and for all sufficiently large $n$, we have $\EE_\PP[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{NP}}]\leq 2\epsilon$.
\end{prop}
\begin{proof}
The proof is provided in Appendix \ref{app:prop:proof:qc_NP}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Step 2. Bounding the error induced by covering and packing.} 

\noindent The error term $\zeta_{\text{CP}}$ captures the error induced by covering and packing. Consider the following inequalities:
\begin{align*}
    \zeta_{\text{CP}} 
    % &\leq \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\|\lambda_{m,k}^{\refe B} -  \Tr\{\lambda_{m,k}^{\refe B}\}\omega^{\refe B}_{m,k}\|_1 + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omega_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\\
    &\overset{a}{\leq} \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} \Tr\{\omega^{\refe B}_{m,k}\}\|(\Tr\{\omega^{\refe B}_{m,k}\})^{-1}\lambda_{m,k}^{\refe B} -  \omegabar^{\refe B}_{m,k}\|_1 \\
    &\hspace{75pt}+ \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} \Tr\{\omega^{\refe B}_{m,k}\} \|\omegabar^{\refe B}_{m,k} - (\Tr\{\omega^{\refe B}_{m,k}\})^{-1}\Tr\{\lambda_{m,k}^{\refe B}\}\omegabar^{\refe B}_{m,k}\|_1
    \\
    &\hspace{150pt}+ \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\\
    %%%%%%%%%%%%
    &\overset{b}{=} \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\omega^{\refe B}_{m,k}\} \Big\|\Big({I^{\refe}}^{\tensor n} \!\tensor \sqrt{\Lambda_{k}^{(m)}}\Big)\omegabar^{\refe B}_{m,k}\Big({I^{\refe}}^{\tensor n} \!\tensor \sqrt{\Lambda_{k}^{(m)}}\Big) -  \omegabar^{\refe B}_{m,k}\Big\|_1 
    \\
    &\hspace{75pt} + \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\big(\Tr\{\omega^{\refe B}_{m,k}\}-\Tr\{({I^{\refe}}^{\tensor n} \!\tensor \Lambda_{k}^{(m)})\omega_{m,k}^{\refe B}\}\big) \|\omegabar^{\refe B}_{m,k}\|_1 + \zeta_{\text{C}}\\
    % &\hspace{150pt}+ \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\\
    %%%%%%%%%%%%%%%%%%%%%
    &\overset{c}{\leq}
    \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}} 2 \ \Tr\{\omega^{\refe B}_{m,k}\} \sqrt{\Tr\Big\{\Big( I-\big({I^{\refe}}^{\tensor n} \!\tensor \Lambda_{k}^{(m)}\big)\Big)\omegabar^{\refe B}_{m,k}\Big\}} 
    \\&\hspace{75pt}+\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\Big\{\Big( I-\big({I^{\refe}}^{\tensor n} \!\tensor \Lambda_{k}^{(m)}\big)\Big) \omega^{\refe B}_{m,k}\Big\}
+ \zeta_{\text{C}}\\
    % &\hspace{150pt}+ \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1\\
     %%%%%%%%%%%%%%%%%%%%%
    &\overset{d}{\leq}
    2 \sqrt{\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}  \Tr\{\omega^{\refe B}_{m,k}\} \Tr\Big\{\Big( I-\big({I^{\refe}}^{\tensor n} \!\tensor \Lambda_{k}^{(m)}\big)\Big)\omegabar^{\refe B}_{m,k}\Big\}} + \zeta_{\text{NP}} + \zeta_{\text{C}}\\
    % \\&\hspace{100pt}+\sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\Big\{\Big( I-\big({I^{\refe}}^{\tensor n} \!\tensor \Lambda_{k}^{(m)}\big)\Big) \omega^{\refe B}_{m,k}\Big\}
    % \\
    % &\hspace{150pt}+ \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1
    &\overset{e}{\leq} 3\sqrt{\zeta_{\text{NP}}}+ \zeta_{\text{C}},
\end{align*}
where $(a)$ follows from adding and subtracting appropriate terms, defining $\omegabar^{\refe B}_{m,k}\deq (\Tr\{\omega^{\refe B}_{m,k}\})^{-1} \omega^{\refe B}_{m,k}$, and applying triangle inequality, $(b)$ follows from \eqref{eqn:lambdamk} and the definition $$\zeta_{\text{C}}\deq \sum_{m \in [\Theta]}\sum_{\substack{k\in [\bTheta]}}\Tr\{\lambda_{m,k}^{\refe B}\}\|\omegabar_{m,k}^{\refe B} -  \calW_{m,k}^{\refe B}\|_1,$$ $(c)$ follows from Gentle Operator Lemma \cite[Lemma 9.4.2]{wilde_arxivBook}, $(d)$ follows from \eqref{eqn:zetaNP} and applying Jensen's inequality for concave functions, and $(e)$ is based on the fact that $x \leq \sqrt{x}$ for all $x\in [0,1]$.
We provide the following proposition that bounds $\zeta_{\text{C}}$.
\begin{prop}\label{prop:qc_cov}
    For all $\epsilon\in(0,1)$, for all sufficiently small $\eta, \delta>0$, and for all sufficiently large $n$, we have $\EE_\PP[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{C}}]\leq 2\epsilon$.
\end{prop} 
\begin{proof}
The proof is provided in Appendix \ref{app:prop:proof:qc_cov}.
\end{proof}
Using Propositions \ref{prop:qc_NP} and \ref{prop:qc_cov}, we now bound the error term $\zeta_{\text{CP}}$. For all $\epsilon\in (0,1)$
\begin{align}
\EE_\PP[&\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{CP}}] = \EE_\PP[3\sqrt{\zeta_{\text{NP}}}+\zeta_{\text{C}}] \leq 3\sqrt{\EE_\PP[\I_{\curly{\mbox{\normalfont sP}}} \zeta_{\text{NP}}}]+\EE_{\PP}[\I_{\curly{\mbox{\normalfont sP}}}\zeta_{\text{C}}] \leq 3\sqrt{2\epsilon}+2\epsilon\label{eqn:zetaCP}
\end{align}
where the first inequality follows from Jensen's inequality for concave functions. Finally, using Propositions \ref{prop:qc_NC} and  \ref{prop:qc_NP}, and \eqref{eqn:zetaCP}, we bound $\error$, for all $\epsilon\in(0,1),$
\begin{align*}
    \EE_\PP[\error]&\leq \EE_\PP[\I_{\curly{\mbox{\normalfont sP}}} 
\error  + 2(1- \I_{\curly{\mbox{\normalfont sP}}})] \\&\leq \EE_\PP[\I_{\curly{\mbox{\normalfont sP}}} 
\error] +2\epsilon \leq 3\sqrt{2\epsilon}+10\epsilon.
\end{align*}
Since $\EE_\PP[\error]\leq 3\sqrt{2\epsilon}+10\epsilon$, there exists a codebook $\codebook$ and the associated POVMs $\Gamma^{(n)}_\sfA$ and $\Gamma^{(n)}_\sfB$ such that $\error\leq 3\sqrt{2\epsilon}+10\epsilon$. This completes the proof of Theorem \ref{thm:QC-QSI}.