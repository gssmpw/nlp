\textbf{Notations.} The set of density operators on Hilbert space $\calH_A$ is denoted by $\calD(\calH_A)$.
We denote the finite alphabet of a source as $\sfX$, and  
the set of probability distributions on the finite alphabet $\sfX$ as $\calP(\calX)$. Let $[\Theta] \deq \{1,2,\cdots,\Theta\}$. 

\begin{definition}\label{def:CQchannel}(Classical-Quantum (CQ) Channel)
Given a finite set $\sfX$ and a probability distribution $P_X$, a CQ channel $\mathcal{W}$ is   
specified by an ensemble of density of operators $\{(P_X(x),\calW_{x})\}_{ x \in \sfX}$. The corresponding average density operator is given as $\calW(P):= \sum_{x\in\sfX}P_X(x)\calW_x$.
\end{definition}
% \begin{definition}\label{def:QCchannel}(Quantum-Classical (QC) Channel or Measurement Channel)
% Given a finite set $\sfX$ and a POVM $\{\Lambda_x\}_{x\in\sfX}$, a QC channel $\calM_{\Lambda}$ acting on an input density operator $\rho$ is   
% specified by an ensemble of pure states $\{(P_X(x),|x\>)\}_{x\in \sfX}$, where $P_X(x) = \Tr\{\Lambda_x \rho\}$. The corresponding density operator is given as $\calM_\Lambda(\rho):= \sum_{x\in\sfX}\Tr\{\Lambda_x \rho\}|x\>\<x|$. 
% \end{definition}

\subsection{Lossy Quantum-Classical Source Coding with Quantum Side Information}
Consider a quantum source $\rho^{AB}\in \calD(\calH_{A}\tensor \calH_{B})$ shared between a sender $A$ and a receiver $B$.
\begin{definition}(QC-QSI Source Coding Setup) A QC-QSI source coding setup is characterized by a triple $(\rho^{AB},\sfX,\calW_{\!\ \sfX\rightarrow \refe B})$, where $\rho^{AB}$ is the bipartite density operator of the source and its side information, $\sfX$ is the reconstruction alphabet, and $\calW:\sfX\rightarrow\calD(\calH_{\refe } \tensor \calH_B)$ is a single-letter posterior CQ channel.
\end{definition}
% \vspace{-10pt}
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.85]{QC-QSI.png}
    \vspace{-5pt}
    \caption{Illustration of Lossy QC-QSI Compression Protocol.}
    \label{fig:QC-QSI}
    \vspace{-5pt}
\end{figure}
\begin{definition}
    (Lossy QC-QSI Compression Protocol) For a given bipartite density operator $\rho^{AB}$ and a reconstruction alphabet $\sfX$, a $(n,\Theta)$ lossy QC-QSI compression protocol is characterized by $(i)$ an encoding POVM $\Gamma^{(n)}_{\sfA} \deq \{\epovm_{m}\}_{m=1}^{\Theta}$ acting on $A^n$, $(ii)$ For each $m\in[\Theta]$, a decoding POVM $\Gamma^{(n,m)}_{\sfB} \deq \{\dpovm^{(m)}_{k}\}_{k=1}^{\bTheta}$ acting on $B^n$, and  $(iii)$ a decoding map $f:[\Theta] \times [\bTheta] \rightarrow \sfX^n$, as shown in Figure \ref{fig:QC-QSI}. 
    % Let $\calR_{\text{QC-QSI}}(\rho^{AB},\sfX,\calW_{\sfX\rightarrow AB})$ denote the set of achievable rates.
\end{definition}
\begin{definition}\label{def:qc_qsi_achievability}
    (Achievability) For a given QC-QSI source coding setup $(\rho^{AB},\sfX,\calW_{\sfX\rightarrow AB})$, a rate $R$ is said to be achievable if for all $\epsilon > 0$ and all sufficiently large $n$, there exists an $(n,\Theta)$ QC-QSI lossy compression protocol such that 
$\frac{1}{n}\log \Theta \leq R + \epsilon$, and $\Xi(\Gamma^{(n)}_\sfA,\Gamma^{(n)}_\sfB,f) \leq \epsilon$, where 
$\Xi(\Gamma^{(n)}_\sfA,\Gamma^{(n)}_\sfB,f) \deq$ 
% \begin{align*}
%     \left\|({I^{\refe B}}^{\tensor n}\tensor f)({I^{R}}^{\tensor n}\tensor \calM_{\Gamma_\sfA\tensor \Gamma_\sfB})({\phi^{\refe A B}}^{\tensor n}) - \Tr\{\calM_{\Gamma_\sfA\tensor \Gamma_\sfB}({\rho^{A B}}^{\tensor n})\}\calW_{}\right\|
% \end{align*}
\begin{align*}
    \Big\| \sum_{m,k} |\xn(m,k)\>\<\xn(m,k)| \tensor \tau^{\refe B}_{\xn(m,k)}   - \sum_{m,k}  Q_{\Xn}(\xn(m,k))|\xn(m,k)\>\<\xn(m,k)| \tensor \calW_{\xn(m,k)}^{RB}\Big\|_1,
\end{align*}
% for all $\xn \!\in \!\sfX^n,$ 
where $\calW_{\xn}^{RB} \deq \bigotimes_{i=1}^n \calW_{x_i}^{RB}$,  $\tau^{R^nB^n}_{\xn}\!\deq \!\Tr_{A^n}\{({I^{\refe}}^{\tensor n}\!\tensor \epovm_{m} \tensor \dpovm^{(m)}_k) (\phi^{RAB}_\rho)^{\tensor n}({I^{\refe A}}^{\tensor n}\! \tensor\!\dpovm^{(m)}_k)\}$ is the unnormalized system-induced density operator on systems $\refe^nB^n$, $, f(m,k) = \xn(m,k), \phi_{\rho}^{RAB}$ is the canonical purification of the state $\rho^{AB}$, and $Q_{\Xn}(\xn(m,k)) \deq \Tr\{(\Omega_{m} \tensor \Xi^{(m)}_k) (\rho^{AB})^{\tensor n}\}$ is the probability of observing the sequence $\xn(m,k)$.
\end{definition}
\begin{theorem}\label{thm:QC-QSI}For a given $(\rho_{AB},\sfX,\calW_{\sfX\rightarrow \refe B})$ QC-QSI source coding setup, 
% such that $\calA(\sourcedo,\calW)$ is non-empty
a rate $R$ is achievable if $\calA(\rho^{AB},\calW_{X\rightarrow \refe B})$ is non-empty and
    $$R\geq I(X;\refe  B)_\sigma-I(X;B)_\sigma = I(X;\refe |B)_\sigma,$$
    where the quantum mutual information is computed with respect to the following quantum-classical state, $$\sigma^{XRB} \deq \sum_x P_X(x) |x\>\<x|^X \tensor \calW_x^{RB} \eqand  P_X(x) \in \calA(\rho^{AB},\calW_{X\rightarrow \refe B}),$$
    $\calA$ is the set of reconstruction distributions defined as 
$$\calA(\rho^{AB},\calW_{X\rightarrow \refe B}) \deq \{P_X\in\calP(\sfX):\sum_{x}P_X(x)\calW^{\refe B}_x = \Tr_{A}\{\phi^{\refe AB}\}\},$$ 
    $\phi^{\refe AB}  \text{ is a purification of $\rho^{AB}$, and } \{|x\>\}_{\{x\in \sfX\}}$ is an orthonormal basis 
    for the Hilbert space $\calH_X$ with $\dim{(\calH_X)}=|\calX|$.
\end{theorem}
\begin{proof}
    The proof is provided in Section \ref{sec:proof:QC-QSI}.
\end{proof}

\subsection{Lossy Classical Source Coding with Classical Side Information}
\begin{definition}(C-CSI Source Coding Setup) A C-CSI source coding setup is characterized by a triple $(\pxz,\sfY,W_{X|YZ})$, where $\pxz$ is the joint source and side-information distribution over a finite alphabet $\sfX\times\sfZ$, $\sfY$ is the reconstruction alphabet, and $W_{X|YZ}:\sfY\times\sfZ \rightarrow\sfX$ is the posterior (backward) channel, i.e., the single-letter conditional distribution of source given the reconstruction and side-information.
\end{definition}
\begin{definition}(Lossy C-CSI Source Compression Protocol) For a given $\pxz$ and reconstruction alphabet $\sfY$, an $(n,\Theta)$ lossy C-CSI source compression protocol consists of $(i)$ a randomized encoding map $\calE^{(n)}:\sfX^n\rightarrow[\Theta]$ and $(ii)$ a randomized decoding map $\calD^{(n)}:\sfZ^n \times[\Theta] \rightarrow\sfY^n,$ as shown in Figure \ref{fig:C-CSI}.  
\end{definition}
\vspace{-3pt}
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.95]{C-CSI.png}
    \vspace{-5pt}
    \caption{Illustration of Lossy C-CSI Compression Protocol.}
    \label{fig:C-CSI}
    \vspace{-3pt}
\end{figure}
\begin{definition}(Achievability) Given C-CSI source coding setup $(\pxz,\sfY,W_{X|YZ})$, a rate R is said to be achievable if for all $\epsilon >0$ and all sufficiently large $n$, there exists an $(n,\Theta)$ lossy compression protocol such that $\frac{1}{n}\log\Theta \leq R+\epsilon$ and $\Xi(\calE^{(n)},\calD^{(n)})\leq \epsilon$, where
\begin{align}
\Xi(\calE^{(n)},\calD^{(n)}) \deq \left\|P_{X^nY^nZ^n} \!-\! P_{Y^nZ^n}W_{X|YZ}^n\right\|_{\text{TV}},\nonumber
\vspace{-10pt}
\end{align}
    $P_{X^nY^nZ^n}(\xn\!,\!\yn\!,\!\zn) \! =\!  {\pxz^n(\xn\!,\!\zn)} \sum_{m \in [\Theta]}\encodern(m|x^n)$ $\decodern(y^n|m,\!\zn),\ \forall(\xn\!,\!\yn\!,\!\zn)\in \sfX^n \times \sfY^n \times \sfZ^n$, is the system-induced distribution,
 $P_{Y^nZ^n}W_{X|YZ}^n$ is the approximating distribution, and $W_{X|YZ}^n(\xn|\yn,\zn) \!\deq\! \prod_{i=1}^nW_{X|YZ}(x_i|y_i,z_i)$. Let $\calR_{\text{C-CSI}}(\pxz,\sfY,W_{X|YZ})$ denote the set of achievable rates.
\end{definition}
\begin{theorem}\label{thm:C-CSI}(Lossy C-CSI Compression Inner Bound)
    $R\in \calR_{\text{C-CSI}}$ if $\calA(P_{XZ},W_{X|YZ})$ is non-empty and there exists a PMF $P_{UXYZ} \in \calA$ such that 
    \begin{itemize}
        \item $P_{XYZ} = \sum_{u\in \sfU}P_{UXYZ}(u,\!x,\!y,\!z)$ for all $(x,\!y,\!z)$
        \item $Z-X-U, \ X-(U,Z)-Y,\text{ and } \ X-(Y,Z)-U$ are Markov chains
        \item $R\geq I(X;U)-I(U;Z),$
    \end{itemize}  
    where $\calA$ is the set of reconstruction distributions defined as 
    $$\calA(P_{XZ},W_{X|YZ})\deq \Big\{P_{U|X}, P_{Y|UZ}:P_{X|Z} \frac{\sum_{u\in\calU}P_{U|X}P_{Y|UZ}}{\sum_{u\in\sfU}P_{U|X}P_{Y|UZ}} = W_{X|YZ} \eqand X-(Y,Z)-U \Big\}.$$
\end{theorem}
\begin{proof}
    The proof is provided in Section \ref{sec:proof:C-CSI}.
\end{proof}


\subsection{Connection Between Rate-Channel Theory and Rate-Distortion Theory}
In \cite{sohail2023unique}, we have developed a new formulation of the lossy source coding problem called \textit{rate-channel theory} which is described below. 
\begin{definition}
    [Achievability]\label{def:clserror_constraint} Given a source coding setup $(\px,\sfY,W_{X|Y})$,
a rate $R$ is said to be achievable if for all $\epsilon >0$ and all sufficiently large $n$, there exists an 
$(n,\Theta)$ lossy source compression protocol consists of  $(i)$ a randomized encoding map $\encodern:\sfX^n \longrightarrow [\Theta]$ and 
$(ii)$ a randomized decoding map $\decodern:[\Theta] \longrightarrow {\sfY}^n$ such that $\frac{1}{n}\log \Theta \leq R + \epsilon$, and $\Xi(\encodern,\decodern) \leq \epsilon$, where 
\begin{equation}\label{def:error_constraint}
  \Xi(\encodern,\decodern)\deq \frac{1}{2}\sum_{\xn \yn}\|{P_{X^n\Yn}(x^n,\yn) -
    P_{\Yn}(\yn)
    % \prevTC^n(x^n|\yn)}
    \\
    \Pi_{i=1}^n\prevTC(x_i|\hat{x}_i)}\|,
\end{equation}
and
$
P_{X^n\Yn}(\xn,\yn) =  {\px^n(x^n)} \sum_{m \in [\Theta]} \encodern(m|x^n) \decodern(\yn|m), 
% P{\curly{\calD(\calE(x^n)) = \Yn}} 
\text{  for all  } (x^n, \yn)\in \sfX^n \times {\sfY}^n,$ is the system-induced distribution,
 and $P_{\Yn}W_{X|Y}^n$ is the approximating distribution.
\end{definition}

\begin{theorem}\label{thm:Csourcecoding}(Rate-Channel Theory \cite[Theorem 2]{sohail2023unique}). For a $(\px,\sfY,W_{X|Y})$ source coding setup, a rate $R$ is said to be achievable if and only if $\calA(\px,W_{X|Y})$ is non-empty, and \begin{equation}\label{eqn:clsratedistortion}R \geq \min_{P_Y \in \calA(\px,W_{X|Y})} I(X;Y), \vspace{-5pt}\end{equation}
where $\calA(\px,W_{X|Y}) \deq \{P_Y \in \calP(\sfY): \!\! \text{for all }  x \in \sfX$, $ 
\sum_{y} P_{Y}(y) W_{X|Y}(x|y) = \px(x)\},$ is the set of reconstruction distributions.
\end{theorem}
Let us consider the case when the side information $Z$ is trivial. 
Given a lossy source coding setup $(\px,\sfY,W_{X|Y})$, 
consider a sequence of 
$(n,\Theta)$
lossy compression protocols 
that achieves the asymptotic performance  $R^\star \deq \min_{P_Y \in \calA(P_X,W_{X|Y})} I(X;Y)$
as given below in Theorem \ref{thm:Csourcecoding}.
Let 
$P_{X^nY^n}$ be the induced $n$-letter joint distribution on the source and the reconstruction vectors. 
Then, we see that 
$\lim_{n \rightarrow \infty} \frac{1}{n} \log \Theta =I(X;Y)$,
$$\lim_{n\rightarrow\infty}\|P_{X^nY^n} -
    P_{Y^n}W_{X|Y}^n\|_{\normalfont \text{TV}} = 0.$$

% and,  by Lemmas \ref{lem:averageSingleletter} and \ref{lemma:sols_linearEqn_close} (stated below and detailed proof provided in \cite{sohail2025WZ}.),
% we have
% \[
% \lim_{n \rightarrow \infty} \| P_{X_QY_Q} - P_{Y}W_{X|Y} \|_{\normalfont \text{TV}} =0,
% \]
% for some distribution $\pxhat$  
% in $\calA(\px,W_{X|Y})$ that achieves the optimality in Theorem \ref{thm:Csourcecoding},
% where $P_{X_QY_Q}=\tfrac{1}{n} \sum_{i=1}^n P_{X_iY_i}.$
% Let $c>0$ and $b(x)$ be an arbitrary constant and a function, respectively.


\begin{theorem}\label{thm:connection}
    Let $c>0$ and $b(x)$ be an arbitrary constant and a function, respectively. Consider a single-letter distortion function given as $d(x,y)= -c\log_2 W_{X|Y}(x|y)+b(x)$,
and distortion level $D={\mathbb{E}[d(X,Y)]}$ with respect to distribution $P_{Y}W_{X|Y}$, where $P_{Y} \in \calA(P_X,W_{X|Y})$ achieves the optimality in Theorem \ref{thm:Csourcecoding}. 
Then, the same sequence of protocols achieves the Shannon rate-distortion function at distortion level $D$ for the source $P_X$, and distortion function $d$, i.e., 
\vspace{-5pt}
\begin{align*}
    \lim_{n \rightarrow \infty} \!\mathbb{E} \bigg[\!\frac{1}{n} \!\sum_{i=1}^n d(X_i,Y_i)\!\bigg] \!=D,
\end{align*}
% \vspace{-5pt}
where the expectation is with respect to the distribution induced by the protocol $P_{X^nY^n}.$
\end{theorem}
\begin{proof}
    The proof is provided in Section \ref{sec:proof:connection}.
\end{proof}










