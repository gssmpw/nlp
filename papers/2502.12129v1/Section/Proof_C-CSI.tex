\noindent For a given $(\pxz,\sfY,W_{X|YZ})$ C-CSI source coding setup, we choose the distributions $(P_{U|X},P_{Y|UZ}) \in \calA(P_{XZ},W_{X|YZ})$. 
% From now on, let $\Theta \deq [2^{nR}]$ and $\bTheta \deq [2^{n\Rbar}]$

\noindent \textbf{Codebook Construction}:
We generate a codebook $\codebook$ consisting of $n$-length codewords by randomly and independently selecting $2^{n\Rbar}$ sequences $\{\Un(1),\Un(2),\cdots, \Un(2^{n\Rbar})\}$ according to the following pruned distribution:
 \begin{align}\label{def:c_distribution}
     &\codeDistribution(\Un(m) = \un) = \left\{\!\!\!\!\begin{array}{cc}
          \dfrac{\targetpu^n(\un)}{(1-\varepsilon)}  & \mbox{for} \; \un \in \Tuqc\\
           0 &  \mbox{otherwise,}
     \end{array} \right. \!\!
 \end{align} 
  where $ \targetpu^n(\un) = \prod_{i=1}^n \targetpu(u_i)$, $\Tuqc$ is the $\delta$-typical set corresponding to the distribution $\pu$ on the set $\calU$, and $\varepsilon(\delta,n) \triangleq \sum_{\un \not \in \Tuqc} \targetpu^n(\un)$. Note that $\varepsilon(\delta,n) \searrow 0$ as $n \rightarrow \infty$ and for all sufficiently small $\delta > 0$. 
 The generated codebook $\calC$ is revealed to the encoder and decoder before the C-CSI protocol begins.


\vspace{5pt}
\noindent \textbf{Encoder Description}:
For an observed source sequence $\xn$, construct a randomized encoder that chooses an index $l \in [2^{n\Rbar}]$ according to a sub-PMF $E_{L|X^n}(l|x^n)$
% \footnote{A non-negative function $q_X(x)$ over a finite alphabet $\calX$ is said to be a sub-PMF if $\sum_{x\in \calX} q_{X}(x) \leq 1$.}
, which is analogous to the likelihood encoders used in  \cite{cuff2013distributed, atif2022source,sohail2023unique}. We now specify $E_{L|X^n}(l|x^n)$ for $x^n\in \Tx$ and $l\in[2^{n\Rbar}]$, where $\hat{\delta} = \delta(|\calX| + |\calU|)$. For a  $\eta \in (0,1)$ (to be specified later), and $\delta>0$, define
\begin{align}
 E_{L|X^n}&(l|x^n) \deq \sum_{\un}   \frac{1}{2^{n\Rbar}}\frac{(1-\varepsilon)}{(1+\eta)}\frac{P^n_{X|U}(x^n|u^n)}{\px^n(x^n)}\I_{\{\xn\in \Tx\}} 
  \I_{\{\un \in \Tucond\}}
  \I_{\{\Un(l) = \un\}},\nonumber
\end{align}
% where $\prevTC^n(x^n|\hat{x}^n)=\prod_{i=1}^n \prevTC(x_i|\hat{x}_i)$ and $\px^n(x^n) = \prod_{i=1}^n\px(x_i)$.
% where $\px^n(x^n) \!\deq \!\prod_{i=1}^n\px(x_i)$. Similar to the encoder specification in \cite{atif2022source}, we also have relaxed the constraint that $E_{M|X^n}(\cdot|x^n)$ is strictly a PMF, i.e, $\sum_{m = 1}^{2^{nR}} E_{M|X^n}(m|x^n) = 1$. 
Let $\Ipmf$ denotes the indicator random variable corresponding to the event 
that $\{E_{L|\Xn}(l|\xn)\}_{l \in [\Theta]}$ forms a sub-PMF for all $\xn\in\Tx $. Once the index $l$ is chosen, it gets mapped to an index $m \in [2^{nR}]$. The mapping is done using a binning map $\calB:[2^{n\Rbar}] \rightarrow [2^{nR}]$. To summarize, on observing $\xn$, the encoder chooses $L \in [2^{n\Rbar}]$ stochastically according the PMF $E_{L|\Xn}(\cdot|\xn)$, and communicate the index $\calB(L)$ to the decoder. 

\vspace{3pt}
\noindent After specifying the PMF $E_{L|\Xn}(\cdot|\xn)$, we now characterize $P_{M|\Xn}$. If $\Ipmf \!\! = \!\! 1$, then construct the sub-PMF $P_{M|\Xn}(m|\xn) \!\deq\! \sum_{l \in [2^{n\Rbar}]}E_{L|\Xn}(l|\xn) \I_{\set{\calB(l) = m}}$, for all $\xn\in\Tx \eqand l\in [2^{n\Rbar}].$ We then append an additional PMF element $P_{L|\Xn}(0|\xn) = E_{L|\Xn}(0|\xn) \deq 1-\sum_{l\in[2^{n\Rbar}]} E_{L|X^n}(l|x^n)$ for all $\xn \in \Tx$, associated with $m=0$, to form a valid PMF $P_{M|\Xn}(m|\xn)$ for all $\xn \in \Tx$ and $ m\in\set{0} \cup [2^{nR}]$. If $\xn \not \in \Tx$, then we define $P_{M|\Xn}(m|\xn) = \I_{\set{m=0}}$. Finally, if $\Ipmf = 0, \text{ then } P_{M|\Xn}(m|\xn) = \I_{\{m=0\}}$, for all $\xn \in \calX^n$. This concludes the encoder description. We provide a proposition from \cite{atif2023lossy}, which will be helpful later in the analysis.

\vspace{3pt}
\begin{proposition}\label{prop:clssubPMF}
    For all $\epsilon,\eta \in (0,1)$, for all sufficiently small $\delta > 0$, and sufficiently large $n$, we have $\EE[{\Ipmf}] \geq 1-\epsilon$,
if $\Rbar > I(X;U)$.
\end{proposition}
% \begin{proof}
%     Proof arguments follow from \cite{atif2022source}. 
%     % For completeness, we provide the proof in Appendix \ref{app:proof:prop:clssubPMF}.
% \end{proof}
% In other words, the above proposition states that if $R > I(X;\hat{X}) + \tilde{\delta}(\delta)$, then $E_{M|X^n}(\cdot|x^n):[2^{nR}] \rightarrow \RR$ is a PMF for each $x^n \in \Tx$ with high probability, where $\tilde{\delta}(\delta) \searrow 0$ as  $\delta\searrow0$. 
% \noindent We now summarize $P_{M|X^n}$ for $m\in \{0\}\cup [2^{nR}]$ and under the condition that $\I_{\curly{\mbox{\normalfont sPMF}}} = 1$,
% \begin{equation*}
%     P_{M|X^n}(m|x^n) \deq \begin{cases}
%     \I_{\{m=0\}} & \text{if } x^n \not \in \Tx,\\
%      E_{M|X^n}(m|x^n) & \text{if } x^n \in \Tx.
%     %  , \\
%     % E_{M|X^n}(0|x^n) &\text{otherwise.}
%     \end{cases}
% \end{equation*}
% In other words, for $x^n \not \in \Tx$ the encoder outputs $0$ with probability 1. For $x^n \in \Tx$, the encoder output an index  from the set $ \calI \deq \{m \in [2^{nR}]:(x^n,\hat{X}^n(m)) \in \Txxhat\}$ according to the distribution $E_{M|X^n}(\cdot|x^n)$, and if $|\calI| = 0$, then encoder outputs $0$ with probability $ (1-\sum_{m'=1}^{2^{nR}}E_{M|X^n}(m'|x^n))$. 
% It can be easily verified that $P_{M|X^n}$ is a valid PMF. 
% \vspace{2pt}

\vspace{3pt}
\noindent \textbf{Decoder Description}: 
For an observed sequence $m \in \set{0} \cup [2^{nR}]$ communicated by the encoder and the sequence $\zn \in \calZ^n$, the decoder constructs the following set:
$\sfL(m,\zn) \deq \set{l \in [2^{n\Rbar}] : \calB(l) = m \eqand (\Un(l),\zn) \in \Tuz}.$
After this, the decoder outputs $\calD(m,\zn) = \Un(l)$ if $\sfL(m,\zn) = \set{l} \eqand m\neq 0$. Otherwise, the decoder outputs a fixed $\un_0 \in \calU^n \backslash \Tu$. At the end, the decoder chooses $\yn$ according to PMF $P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn)$. This implies the PMF $P_{Y^n|MZ^n}(\cdot|m,\zn)$ can be expressed as: 
\[P_{Y^n|MZ^n}(\cdot|m,\zn) = P^n_{Y|UZ}(\cdot|\calD(m,\zn),\zn).\]

\vspace{3pt}
\noindent \textbf{Error Analysis}: 
We show that for the above-mentioned encoder and decoder, $P_{\Xn\Zn\Yn}$ is close to the approximating distribution $P_{\Yn\Zn} W^n_{X|YZ}$. These PMFs can be further expressed as follows:
\begin{align*}
    P_{\Xn\Yn\Zn}(\xn,\yn,\zn) &= \sum_{m \in [\theta] \cup \set{0}} P_{XZ}^n(\xn,\zn) P_{M|\Xn}(m|\xn)P_{\Yn|M\Zn}(\yn|m,\zn)\\
    P_{\Yn\Zn}(\yn,\zn) W^n_{X|YZ}(\xn
|\yn,\zn) &= \sum_{\sfxn}\sum_{\substack{m \in [\theta] \cup \set{0}}}  P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P_{\Yn|M\Zn}(\yn|m,\zn)\\
& \hspace{2.5in}\times W^n_{X|YZ}(\xn
|\yn,\zn),
\end{align*}
where $P_{M|\Xn}(m|\sfxn) \eqand P_{\Yn|M\Zn}(\yn|m,\zn)$ are PMFs induced by encoder and decoder, respectively, and for convenience, we denote $\theta \deq [2^{nR}]$ and  $\bar{\theta} \deq [2^{n\Rbar}]$, for the remaining of the paper. We begin by splitting the error $ \Xi(\encodern,\decodern)$ into two terms using $\Ipmf$ as 
\begin{align}
    \Xi(\encodern,\decodern) 
    &= \Ipmf 
\Xi(\encodern,\decodern) + (1- \Ipmf)\Xi(\encodern,\decodern),\nonumber \\
&\leq \Ipmf 
\Xi(\encodern,\decodern) + ({1- \Ipmf}),\label{eqn:clserrorsubpmf}
\end{align}
where \eqref{eqn:clserrorsubpmf} follows from upper bounding the total variation between two PMFs by one, i.e., the maximum value of the 
total variation between two PMFs. 
% \vspace{10pt}
% \noindent\textbf{Step 1: Isolating the error term induced by not covering}
Using the triangle inequality, we now expand $\Xi(\encodern,\decodern)$. Under the condition $\Ipmf = 1$, as follows:
\begin{align*}
    2 \ \Xi(&\encodern,\decodern) =\sum_{\xn, \yn, \zn} \Big| \sum_{m \in [\theta] \cup \set{0}} P_{XZ}^n(\xn,\zn) P_{M|\Xn}(m|\xn)P_{\Yn|M\Zn}(\yn|m,\zn) \\
    & \hspace{60pt}-\sum_{\sfxn}\sum_{\substack{m \in [\theta] \cup \set{0} }}  P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P_{\Yn|M\Zn}(\yn|m,\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)\Big| \nonumber \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%% not typical 
    &=\sum_{\substack{\xn \notin \Tx \\ \yn,\ \zn}}
    \Big|P_{XZ}^n(\xn,\zn) P^n_{Y|UZ}(\yn|u_0^n,\zn) \\
    & \hspace{50pt}
    - \sum_{\sfxn}\sum_{\substack{m \in [\theta] \cup \set{0} }}   P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P_{\Yn|M\Zn}(\yn|m,\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)\Big|\\
    %%%%%%%%%%%% typical
     &+\sum_{\substack{\xn \in \Tx \\ \yn,\ \zn}}
    \Big|P_{XZ}^n(\xn,\zn) \sum_{m\in [\theta]}P_{M|\Xn}(m|\xn) P_{\Yn|m\Zn}(\yn|m,\zn) \\
    & \hspace{50pt}
    + P_{XZ}^n(\xn,\zn)P_{M|\Xn}(0|\xn)P^n_{Y|UZ}(\yn|u_0^n,\zn)\\
    & \hspace{50pt}
    - \sum_{\sfxn}\sum_{\substack{m \in [\theta] \cup \set{0}}}  P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P_{\Yn|M\Zn}(\yn|m,\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)
    \Big|\\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % using the triangle inequality
    & \overset{a}{\leq} \cpe + \notce +  \er + 3 \!\!\!\!\!\!\sum_{\xn \notin \Tx}P^n_X{(\xn)} \\
    & \hspace{50pt} + \sum_{\substack{\sfxn \in \Tx \\ \xn,\ \yn,\ \zn}} P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(0|\sfxn) P_{\Yn|\Un\Zn}(\yn|u_0^n,\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)\\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    & \overset{b}{\leq} \cpe + 2\notce  + \er + 4 \!\!\!\!\!\!\sum_{\xn \notin \Tx}P^n_X{(\xn)} 
    \overset{c}{\leq}\cpe + 2\notce + \er +  4\epsilon,
\end{align*}
for all sufficiently large $n$ and all $\delta>0$, where $(a)$ follows from triangle inequality and by defining terms $\cpe ,\notce,\eqand \er$ as follows:
\begin{align*}
    \cpe &\deq \sum_{\substack{\xn \in \Tx \\ \yn, \zn}}
    \Big|P_{XZ}^n(\xn,\zn) \sum_{m\in [\theta]}P_{M|\Xn}(m|\xn) P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn) \\
    & \hspace{50pt} - \sum_{\substack{m \in [\theta] \\ \sfxn \in \Tx}}  P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)
    \Big|,\\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \notce &\deq \sum_{\xn \in \Tx} P^n_{X}(\xn) \Big(1 - \sum_{l\in[\btheta]}E_{l|\Xn}(l|\xn)\Big),  \\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % \ee &\deq \sum_{\substack{\sfxn \in \Tx \\ \xn \notin \Tx}} \sum_{\substack{m \in [\theta] \\\yn \\ \zn }}P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P^n_{Y|UZ}(\yn|\un,\zn) W^n_{X|YZ}(\xn|\yn,\zn), \eqand\\
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \er &\deq \sum_{\substack{\sfxn \in \Tx \\ \xn \notin \Tx}} \sum_{\substack{m \in [\theta] \\\yn \\ \zn }}
     P^n_{XZ}(\sfxn,\zn)P_{M|\Xn}(m|\sfxn) P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn) W^n_{X|YZ}(\xn
    |\yn,\zn),
\end{align*}
and $(b)$ follows by writing $P_M(0) = \sum_{\sfxn \notin \Tx} P^n_X(\sfxn) + \sum_{\xn \in \Tx} P^n_X(\xn)E_{l|\Xn}(0|\xn),$ and $(c)$ follows from the standard typically argument for all sufficiently large $n$. 

\noindent \textbf{Step 1: Bounding the error induced by not covering}

\noindent Note that the error term $\notce$ captures the error induced by not covering the $n$-product side-information assisted posterior channel. We bound this term by utilizing the following proposition.
\begin{proposition} For all $\epsilon \in (0,1)$, for all sufficiently small $\eta,\delta >0$, and for all sufficiently large $n$, we have $\EE[\Ipmf \notce] \leq \epsilon$, if $\Rbar > I(X:U)$.
\end{proposition}
\begin{proof}
    The proof follows from the \cite[Proposition 8]{atif2023lossy}.
\end{proof}
Next, we move on to isolating the error component of $\cpe$ caused by binning (packing). 

\noindent \textbf{Step 2: Isolating the term induced by binning}

\noindent We consider the term corresponding to $\cpe$. By adding and subtracting an appropriate term inside the modulus of $\cpe$ and using triangle inequality, we get $\cpe \leq \ce + \peone + \petwo$, where 
\begin{align*}
    \ce &\deq \!\!\!\sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}}
 \Big|P_{Z|X}^n(\zn|\xn) P_{X|U}^n(\xn|\un) P^n_{Y|UZ}(\yn|\un,\zn) \\
    & \hspace{25pt} \times \I_{\set{\xn \in \Txcond}} - \!\!\!\!\!\!\!\sum_{\substack{\sfxn \in \Txcond}} \!\!\!\!\!\!P_{Z|X}^n(\zn|\sfxn) P_{X|U}^n(\sfxn|\un) P^n_{Y|UZ}(\yn|\un,\zn) W^n_{X|YZ}(\xn
    |\yn,\zn)
    \Big|,\\
%%%%%%%%%%%%
    \peone &\deq \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} \I_{\set{\xn \in \Txcond}}
    P_{Z|X}^n(\zn|\xn) P_{X|U}^n(\xn|\un) \\
    & \hspace{100pt}  \times \Big| P^n_{Y|UZ}(\yn|\un,\zn) - P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn)\Big|,\\
%%%%%%%%%%%%
    \petwo &\deq \sum_{\substack{\xn \in \Tx \\ \un \in \Tu \\ \sfxn \in \Tx}} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} 
    \I_{\set{\sfxn \in \Txcond}}
    % \sum_{\sfxn \in \Tx}
    P_{Z|X}^n(\zn|\sfxn) P_{X|U}^n(\sfxn|\un) \\
    & \hspace{100pt}  \times \Big| P^n_{Y|UZ}(\yn|\un,\zn) - P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn)\Big| W^n_{X|YZ}(\xn|\yn,\zn).
\end{align*}
Here, $\peone$ and $\petwo$ captures the error induced by binning. Observe that, we can establish the bound $\petwo \leq \peone$ because $\sum_{\xn \in \Tx} W^n_{X|YZ}(\xn|\yn,\zn) \leq 1$. Consequently, we have $\cpe \leq \ce + 2\peone$. To bound the term $\peone$, we provide the following proposition.
\begin{proposition}\label{prop:binning} For all $\eta,\epsilon \in (0,1),$ for all sufficiently small $\delta >0$, and sufficiently large $n$, we have $\EE[\Ipmf \peone] \leq 3\epsilon$, if $(\Rbar-R) < I(U;Z)$.
\end{proposition}
\begin{proof}
The proof is provided in Appendix \ref{app:proof:prop:binning}.
\end{proof} 
\noindent Next, we proceed to analyze the error term induced by covering. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Step 3: Bounding the covering error}

\noindent Using the Markov chain $U-X-Z$, $X-(U,Z)-Y$, and $X-(Y,Z)-U$ which $P_{UXYZ}$ satisfies, we can rewrite the term $ P_{X|U} P_{Z|UX} P_{Y|UXZ}$ as follows:
\begin{equation}\label{eqn:mc_prob}
    P_{Z|X} P_{X|U} P_{Y|UZ} = P_{Z|UX} P_{X|U} P_{Y|UXZ} = P_{YZ|U} W_{X|YZ}.
\end{equation}
Using \eqref{eqn:mc_prob}, we can simplify the terms inside the modulus of $\ce$ as:
\begin{align*}
    \ce &\deq \sum_{\substack{\xn \in \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} P_{YZ|U}^n(\yn,\zn|\un) W^n_{X|YZ}(\xn|\yn,\zn) \\
    & \hspace{2in} \times \Big|\I_{\set{\xn \in \Txcond}} - \sum_{\substack{\sfxn \in \Txcond}} W^n_{X|YZ}(\sfxn
    |\yn,\zn)
    \Big|.
\end{align*}
To bound the above-simplified term, we provide the following proposition. 
\begin{proposition}\label{prop:covering}
    For all $\eta, \epsilon \in (0,1)$, for all sufficiently small $\delta >0$, and sufficiently large $n$, we have $\EE[\Ipmf \ce] \leq \epsilon$.
\end{proposition}
\begin{proof}
The proof is provided in Appendix \ref{app:proof:prop:covering}.
\end{proof}
Following Propositions \ref{prop:binning} and \ref{prop:covering}, for all $\epsilon \in (0,1)$, for all sufficiently large $n$ and sufficiently small $\delta, \eta> 0$, we obtain, $\EE[\Ipmf \cpe] \leq \EE[\Ipmf \ce] + 2\EE[\Ipmf \peone] \leq 7\epsilon$.  Finally, we are left with the analysis of the error term $\er$.

\noindent \textbf{Step 4: Bounding the error term $\er$}

\noindent By incorporating the addition and subtraction of a suitable term and applying the triangle inequality, we further upper bound the expression of the error term $\er$ as $\er \leq \ee + \epe,$ where
\begin{align*}
     \epe &\deq \sum_{\substack{\xn \notin \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} 
    \sum_{\sfxn \in \Tx}P_{Z|X}^n(\zn|\sfxn) P_{X|U}^n(\sfxn|\un)\\
    & \hspace{0.5in}  \times  \I_{\set{\sfxn \in \Txcond}} \Big| P^n_{Y|UZ}(\yn|\un,\zn) - P^n_{Y|UZ}(\yn|\calD(m,\zn),\zn)\Big| W^n_{X|YZ}(\xn|\yn,\zn)\\
    %%%%%%%%%%%%%%%%%%%
    \eqand \ee &\deq\sum_{\substack{\xn \notin \Tx \\ \un \in \Tu }} \sum_{\substack{\yn \\ \zn}} \sum_{\substack{ l \in [\btheta]\\m\in [\theta] }} 
    \frac{1}{\btheta} \frac{(1-\varepsilon)}{(1+\eta)} 
    \I_{\set{\Un(l) = \un}} \I_{\set{\calB(l) = m}} 
    \sum_{\sfxn \in \Tx}P_{Z|X}^n(\zn|\sfxn) P_{X|U}^n(\sfxn|\un) \\
    & \hspace{0.8in}  \times  \I_{\set{\sfxn \in \Txcond}} P^n_{Y|UZ}(\yn|\un,\zn)W^n_{X|YZ}(\xn|\yn,\zn)
\end{align*}

\noindent Similar to the error $\petwo$, note that the error term $\epe$ can be upper bounded as $\epe \leq \peone$ because $\sum_{\xn \notin \Tx} W^n_{X|YZ}(\xn|\yn,\zn) \leq 1$. Thus, for all $\epsilon \in (0,1)$, for all sufficiently large $n$ and sufficiently small $\delta, \eta > 0$, we have $\EE[\Ipmf \epe] \leq 3\epsilon$. To bound the remaining error term $\ee$, we present a proposition below.
\noindent \begin{proposition}\label{prop:encoding_error}
    For all $\epsilon \in \set{0,1}$, for all sufficiently small $\eta,\delta >0$, and sufficiently large $n$, we have $\EE[\Ipmf \ee] \leq \epsilon$.
\end{proposition}
\begin{proof}
The proof follows from the argument similar to Proposition \ref{prop:covering}. However, for completeness, we provide the proof in Appendix \ref{app:proof:prop:encoding_error}.
\end{proof}
Eventually, using Propositions \ref{prop:binning}, \ref{prop:covering}, and \ref{prop:encoding_error}, we bound the $\EE[\encodern, \decodern].$ For all $\epsilon \in (0,1)$,
\begin{equation*}
    \EE_{\codebook}[\Xi(\encodern, \decodern)] \leq \EE_{\codebook}[\Ipmf \Xi(\encodern,\decodern) + (1-\Ipmf)] \leq 17\epsilon/2,
\end{equation*}
for all sufficiently large $n$. Since $\EE_{\codebook}[\Xi(\encodern, \decodern)] \leq 17\epsilon/2$, there must exists a code $\codebook$ such that the associated error $\Xi(\encodern, \decodern) \leq 17\epsilon/2$. This completes the achievability proof.

