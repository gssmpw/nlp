\subsection{Comparative Analysis}
\label{sec:comparative-analysis}
% \begin{table*}
% \setlength{\tabcolsep}{1mm}
% \small
%   \centering
%   \begin{tabular}{lcccccccccccc}
%     \toprule
%      \multirow{2}{*}{\bf Models} & \multicolumn{4}{c}{\bf BLEU} & \multicolumn{3}{c}{\bf ROUGE} &  \multirow{2}{*}{\bf METEOR} & \multicolumn{3}{c}{\bf BERTScore} & \multirow{2}{*}{\bf SentBERT} \\ \cmidrule{2-5} \cmidrule{6-8} \cmidrule{10-12}
     
%     & \bf B1 & \bf B2 & \bf B3 & \bf B4 & \bf RL & \bf R1 & \bf R2 & & \bf Precision & \bf Recall & \bf F1 & \\
%     \midrule
%     \textbf{PGN} & 17.54 & 6.31 & 2.33 & 1.67 & 16.00 & 17.35 & 6.90 & 15.06 & 84.80 & 85.10 & 84.90 & 49.42\\
    
%     \textbf{Transformer} & 11.44 & 4.79 & 1.68 & 0.73 & 15.90 & 17.78 & 5.83 & 9.74 & 83.40 & 84.90 & 84.10 & 52.55\\
%     % \hline
%     \textbf{MFFG-RNN} & 14.16 & 6.10 & 2.31 & 1.12 & 16.21 & 17.47 & 5.53 & 12.31 & 81.50 & 84.00 & 82.70 & 44.65\\
%     % \hline
%     \textbf{MFFG-Transf} & 13.55 & 4.95 & 2.00 & 0.76 & 15.14 & 16.84 & 4.30 & 10.97 & 81.10 & 83.80 & 82.40 & 41.58\\
%     % \hline
%     \textbf{M-Transf} & 14.37 & 6.48 & 2.94 & 1.57 & 18.77 & 20.99 & 6.98 & 12.84 & 86.30 & 86.20 & 86.20 & 53.85\\
%     % \hline
%     \textbf{ExMore} & 19.26 & 11.21 & 6.56 & 4.26 & 25.23 & 27.55 & 12.49 & 19.16 & 88.30 & 87.50 & 87.90 & 59.12\\ \midrule
%     % \hline
%     \textbf{GPT-4o-Mini-Zeroshot} & 17.51 & 7.71 & 3.98 & 2.04 & 20.68 & 21.77 & 5.20 & 26.45 & 85.47 & 87.41 & 86.42 & 56.23 \\
%     \textbf{LLaVa-Mistral-Zeroshot} & 17.25 & 8.50 & 5.16 & 3.38 & 22.32 & 23.03 & 6.51 & 27.47 & 85.60 & 87.80 & 86.68 & 58.97 \\
%     \textbf{LLaVa-Llama3-Zeroshot} & 21.08 & 11.08 & 7.00 & 4.81 & 25.30 & 25.76 & 8.21 & 28.33 & 86.38 & 87.82 & 87.08 & 60.06 \\ \midrule    
%     \textbf{GPT-4o-Mini-Oneshot} & 19.07 & 8.45 & 4.33 & 2.23 & 22.01 & 22.68 & 5.68 & 25.89 & 86.01 & 87.61 & 86.79 & 57.30 \\
%     \textbf{LLaVa-Mistral-Oneshot} & 20.29 & 10.04 & 6.03 & 3.93 & 24.57 & 24.47 & 6.75 & 26.24 & 86.46 & 87.85 & 87.13 & 59.76 \\
%     \textbf{LLaVa-Llama3-Oneshot} & 24.50 & 13.12 & 8.31 & 5.58 & 27.68 & 27.77 & 8.80 & 27.60 & 87.07 & 87.85 & 87.44 & 61.07 \\ \midrule
%     % \hline
%     \textbf{TEAM [SOTA]} & 55.32 & 45.12 & 38.27 & 33.16 & 50.58 & 51.72 & 34.96 & 50.95 & 91.80 & 91.60 & 91.70 & 72.92 \\
%     \midrule
    
%     \rowcolor{red!20} \textbf{TURBO} & \textbf{57.09} & \textbf{46.93} & \textbf{40.28} & \textbf{35.26} & \textbf{53.12} & \textbf{55.06} & \textbf{38.16} & \textbf{55.17} & \textbf{92.00} & \textbf{91.77} & \textbf{91.86} & \textbf{75.75}\\ \midrule

%     % $+$ \textbf{Only KG} & 51.54 & 39.85 & 32.30 & 26.79 & 47.62 & 49.43 & 30.66 & 48.86 & 91.17 &	90.67 &	90.90 & 72.25\\
%     % % \hline
%     % $+$ \textbf{Only SF} & 54.19 & 44.17 & 37.73 & 32.93 & 49.80 & 50.72 & 34.42 & 51.08 & 91.42 & 91.22 & 91.29 & 73.20\\
%     % % \hline
%     % $+$ \textbf{SF $+$ KG} & 55.37 & 45.09 & 38.45 & 33.53 & 50.98 & 52.25 & 35.41 & 52.08 & 91.55 &	91.51 &	91.51 & 73.62\\
%     % % \hline
%     % $+$ \textbf{SF $+$ TS} & 55.56 & 44.80 & 37.71 & 32.58 & 51.42 & 53.46 & 35.46 & 53.22 & 91.76 &	91.65 &	91.68 & 75.16\\
%     % % \hline
%     % $+$ \textbf{KG $+$ TS} & 54.72 & 42.95 & 35.26 & 29.65 & 50.69 & 52.38 & 32.97 & 52.16 & 91.67 & 91.35 & 91.50 & 74.62\\  \bottomrule
%     \textbf{TURBO} $-$ \textbf{SF} $-$ \textbf{TS} & 51.54 & 39.85 & 32.30 & 26.79 & 47.62 & 49.43 & 30.66 & 48.86 & 91.17 &	90.67 &	90.90 & 72.25\\
%     % \hline
%     \textbf{TURBO} $-$ \textbf{KG} $-$ \textbf{TS} & 54.19 & 44.17 & 37.73 & 32.93 & 49.80 & 50.72 & 34.42 & 51.08 & 91.42 & 91.22 & 91.29 & 73.20\\
%     % \hline
%     \textbf{TURBO} $-$ \textbf{TS} & 55.37 & 45.09 & 38.45 & 33.53 & 50.98 & 52.25 & 35.41 & 52.08 & 91.55 &	91.51 &	91.51 & 73.62\\
%     % \hline
%     \textbf{TURBO} $-$ \textbf{KG} & 55.56 & 44.80 & 37.71 & 32.58 & 51.42 & 53.46 & 35.46 & 53.22 & 91.76 &	91.65 &	91.68 & 75.16\\
%     % \hline
%     \textbf{TURBO} $-$ \textbf{SF} & 54.72 & 42.95 & 35.26 & 29.65 & 50.69 & 52.38 & 32.97 & 52.16 & 91.67 & 91.35 & 91.50 & 74.62\\  \bottomrule
%   \end{tabular}
%   \caption{A comparative analysis of our proposed model with multiple state-of-the-art baselines. This analysis was conducted on the \dataset\ dataset. The best results are in boldface.}
%   \label{comparative-analysis}
% \end{table*}

In line with the existing systems \cite{Desai_Chakraborty_Akhtar_2022,jiang-2023-team}, we employ BLEU, ROUGE, METEOR, BERTScore, and SentBERT for evaluation.
We compare the performance of \model\ with various unimodal and multimodal baselines. 
\begin{itemize}[leftmargin=*, noitemsep, nolistsep]
% \item{\textbf{Pointer Generator Network (PGN)} \citep{see-etal-2017-get} and \textbf{Transformer} \citep{attention-is-all-you-need}. 
% % These are text-generation models and are employed as our text-based baselines.
% }
% \item{\textbf{MFFG-RNN and MFFG-Transf} \citep{liu-etal-2020-multistage}. These are variants of the multimodal video summarization system, MFFG, and utilize the RNN and the Transformer as their decoders respectively.}
% \item{\textbf{M-Transf} \citep{yao-wan-2020-multimodal}. This is the multimodal transformer architecture that utilizes a multimodal attention mechanism.}
% \item{\textbf{ExMore} \citep{Desai_Chakraborty_Akhtar_2022}. This model uses a multimodal transformer-based architecture to generate sarcasm explanations and differs from M-Transf in the way its attention mechanism works.}
\item{
\textbf{Existing Baselines}. We employ all of the baselines employed by \citet{Desai_Chakraborty_Akhtar_2022}. 
These are: 
\textbf{a)} \textit{Pointer Generator Network (PGN)} \citep{see-etal-2017-get};
\textbf{b)} \textit{Transformer} \citep{attention-is-all-you-need};
\textbf{c)} \textit{MFFG-RNN and MFFG-Transf} \citep{liu-etal-2020-multistage};
\textbf{d)} \textit{M-Transf} \citep{yao-wan-2020-multimodal}; and
\textbf{e)} \textit{ExMore} \citep{Desai_Chakraborty_Akhtar_2022}.
}
\item{\textbf{TEAM} \citep{jing-etal-2023-multi}. This model refers to the current state-of-the-art in this task.}   
\end{itemize}
Notably, we also compare our model with multiple state-of-the-art multimodal large-language models (MLLMs) in zero-shot and one-shot settings, such as, \textbf{GPT-4o Mini} \cite{openai2024gpt4technicalreport}, \textbf{LLaVa-Mistral} (\citealp{liu2024improved}; \citealp{jiang2023mistral}), and \textbf{LLaVa-Llama3} (\citealp{liu2024improved}; \citealp{dubey2024llama3herdmodels}). The complexity and size of these LLMs, combined with the vast amount of data that they are trained on, allow them to perform exceptionally well on a diverse set of tasks. Thus, by including this comparison, we aim to demonstrate the effectiveness of \model\ and its superiority at performing this task even when compared to extremely powerful and versatile LLMs.  

% \begin{itemize}[leftmargin=*, itemsep=0pt]
%     \item \textbf{GPT-4o-Mini}. This is the mini variant of the GPT-4o model released by OpenAI. It is a successor to one of the most popular publicly available LLMs, ChatGPT. It significantly outperforms its predecessors such as GPT-3.5 Turbo and gives cut-throat competition to GPT-4o despite containing only a fraction of its number of parameters. 

%     \item \textbf{LLaVa-Mistral} \citep{liu2024improved}. This model is built on top of LLaVa-NeXT \citep{liu2024llavanext}, and uses the large variant of Mistral \citep{jiang2023mistral}, a pre-trained LLM, with a vision encoder to perform multimodal tasks with remarkable performance and efficiency.

%     \item \textbf{LLaVa-Llama3} \citep{liu2024improved}. This model is similar to the LLaVa-Mistral variant except it uses Llama3 as its text processor. Llama3 is shown to perform better than Mistral at various text-related tasks. This serves as our motivation to include this MLLM in our comparative analysis.
% \end{itemize}

\paragraph{Existing Baselines:} We conduct our experiments on the \dataset\ dataset and report our results in Table \ref{comparative-analysis}. We observe a significant disparity in the performance of our model when compared to non-LLM baselines other than TEAM. Moreover, compared to TEAM, we observe that \model\ consistently reports better scores on each evaluation metric -- \model\ outperforms TEAM on B1, B2, B3 and B4 by $+1.77\%$, $+1.81\%$, $+2.01\%$, and $+2.10\%$, respectively, with an average margin of $+1.92\%$. Furthermore, \model\ yields better ROUGE [RL ($+2.54\%$), R1 ($+3.34\%$), R2 ($+3.20\%$)] and METEOR [$+4.22\%$] scores of the two models with an average margin of $+3.33\%$. 
We also compute BERTscore and SentBERT to evaluate the semantic context in the generated explanations. We observe that our model surpasses TEAM by margins of $+0.2\%$, $+0.17\%$, and $0.16\%$ on the mean precision, recall, and F1 components of BERTscore (average margin of $+0.18\%$) and by a margin of $+2.83\%$ on SentBERT. 


\paragraph{Multimodal LLMs:} For completeness, we also evaluate the MLLMs' explanations on the aforementioned evaluation metrics. As evident in Table \ref{comparative-analysis}, \model\ outperforms MLLMs on all evaluation metrics; however, we acknowledge that these metrics are not appropriate for assessing the quality and semantic richness of the generated explanations. Therefore, we also perform a human evaluation to better gauge their quality (\S \ref{sec:human-eval}).

% Our model is also able to beat each of the three MLLMs, clearly proving its efficacy at this task even when compared with state-of-the-art MLLMs capable of excellent performance on a variety of other tasks. 









