% =============================================================
\section{Experiments, Results, and Analyses}\label{sec:experiments}
% =============================================================
% \paragraph{Model Architecture.} 

\begin{table*}
\setlength{\tabcolsep}{1mm}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lcccccccccccc}
    \toprule
     \multirow{2}{*}{\bf Models} & \multicolumn{4}{c}{\bf BLEU} & \multicolumn{3}{c}{\bf ROUGE} &  \multirow{2}{*}{\bf METEOR} & \multicolumn{3}{c}{\bf BERTScore} & \multirow{2}{*}{\bf SentBERT} \\ \cmidrule{2-5} \cmidrule{6-8} \cmidrule{10-12}
     
    & \bf B1 & \bf B2 & \bf B3 & \bf B4 & \bf RL & \bf R1 & \bf R2 & & \bf Precision & \bf Recall & \bf F1 & \\
    \midrule
    \textbf{PGN} & 17.54 & 6.31 & 2.33 & 1.67 & 16.00 & 17.35 & 6.90 & 15.06 & 84.80 & 85.10 & 84.90 & 49.42\\
    
    \textbf{Transformer} & 11.44 & 4.79 & 1.68 & 0.73 & 15.90 & 17.78 & 5.83 & 9.74 & 83.40 & 84.90 & 84.10 & 52.55\\
    % \hline
    \textbf{MFFG-RNN} & 14.16 & 6.10 & 2.31 & 1.12 & 16.21 & 17.47 & 5.53 & 12.31 & 81.50 & 84.00 & 82.70 & 44.65\\
    % \hline
    \textbf{MFFG-Transf} & 13.55 & 4.95 & 2.00 & 0.76 & 15.14 & 16.84 & 4.30 & 10.97 & 81.10 & 83.80 & 82.40 & 41.58\\
    % \hline
    \textbf{M-Transf} & 14.37 & 6.48 & 2.94 & 1.57 & 18.77 & 20.99 & 6.98 & 12.84 & 86.30 & 86.20 & 86.20 & 53.85\\
    % \hline
    \textbf{ExMore} & 19.26 & 11.21 & 6.56 & 4.26 & 25.23 & 27.55 & 12.49 & 19.16 & 88.30 & 87.50 & 87.90 & 59.12\\ \midrule
    % \hline
    \textbf{GPT-4o-Mini-Zeroshot} & 17.51 & 7.71 & 3.98 & 2.04 & 20.68 & 21.77 & 5.20 & 26.45 & 85.47 & 87.41 & 86.42 & 56.23 \\
    \textbf{LLaVa-Mistral-Zeroshot} & 17.25 & 8.50 & 5.16 & 3.38 & 22.32 & 23.03 & 6.51 & 27.47 & 85.60 & 87.80 & 86.68 & 58.97 \\
    \textbf{LLaVa-Llama3-Zeroshot} & 21.08 & 11.08 & 7.00 & 4.81 & 25.30 & 25.76 & 8.21 & 28.33 & 86.38 & 87.82 & 87.08 & 60.06 \\ \midrule    
    \textbf{GPT-4o-Mini-Oneshot} & 19.07 & 8.45 & 4.33 & 2.23 & 22.01 & 22.68 & 5.68 & 25.89 & 86.01 & 87.61 & 86.79 & 57.30 \\
    \textbf{LLaVa-Mistral-Oneshot} & 20.29 & 10.04 & 6.03 & 3.93 & 24.57 & 24.47 & 6.75 & 26.24 & 86.46 & 87.85 & 87.13 & 59.76 \\
    \textbf{LLaVa-Llama3-Oneshot} & 24.50 & 13.12 & 8.31 & 5.58 & 27.68 & 27.77 & 8.80 & 27.60 & 87.07 & 87.85 & 87.44 & 61.07 \\ \midrule
    % \hline
    \textbf{TEAM [SOTA]} & 55.32 & 45.12 & 38.27 & 33.16 & 50.58 & 51.72 & 34.96 & 50.95 & 91.80 & 91.60 & 91.70 & 72.92 \\
    \midrule
    
    \rowcolor{red!20} \textbf{TURBO} & \textbf{57.09} & \textbf{46.93} & \textbf{40.28} & \textbf{35.26} & \textbf{53.12} & \textbf{55.06} & \textbf{38.16} & \textbf{55.17} & \textbf{92.00} & \textbf{91.77} & \textbf{91.86} & \textbf{75.75}\\     \midrule
    \textbf{TURBO $+$ TS Concepts} & 55.65 & 45.38 & 38.55 & 33.36 & 51.64 & 53.63 & 36.50 & 53.80 & 91.72 & 91.60 & 91.64 & 75.15\\ 


    % $+$ \textbf{Only KG} & 51.54 & 39.85 & 32.30 & 26.79 & 47.62 & 49.43 & 30.66 & 48.86 & 91.17 &	90.67 &	90.90 & 72.25\\
    % % \hline
    % $+$ \textbf{Only SF} & 54.19 & 44.17 & 37.73 & 32.93 & 49.80 & 50.72 & 34.42 & 51.08 & 91.42 & 91.22 & 91.29 & 73.20\\
    % % \hline
    % $+$ \textbf{SF $+$ KG} & 55.37 & 45.09 & 38.45 & 33.53 & 50.98 & 52.25 & 35.41 & 52.08 & 91.55 &	91.51 &	91.51 & 73.62\\
    % % \hline
    % $+$ \textbf{SF $+$ TS} & 55.56 & 44.80 & 37.71 & 32.58 & 51.42 & 53.46 & 35.46 & 53.22 & 91.76 &	91.65 &	91.68 & 75.16\\
    % % \hline
    % $+$ \textbf{KG $+$ TS} & 54.72 & 42.95 & 35.26 & 29.65 & 50.69 & 52.38 & 32.97 & 52.16 & 91.67 & 91.35 & 91.50 & 74.62\\  \bottomrule
    \textbf{TURBO} $-$ \textbf{SF} $-$ \textbf{TS} & 51.54 & 39.85 & 32.30 & 26.79 & 47.62 & 49.43 & 30.66 & 48.86 & 91.17 &	90.67 &	90.90 & 72.25\\
    % \hline
    \textbf{TURBO} $-$ \textbf{KG} $-$ \textbf{TS} & 54.19 & 44.17 & 37.73 & 32.93 & 49.80 & 50.72 & 34.42 & 51.08 & 91.42 & 91.22 & 91.29 & 73.20\\
    % \hline
    \textbf{TURBO} $-$ \textbf{TS} & 55.37 & 45.09 & 38.45 & 33.53 & 50.98 & 52.25 & 35.41 & 52.08 & 91.55 &	91.51 &	91.51 & 73.62\\
    % \hline
    \textbf{TURBO} $-$ \textbf{KG} & 55.56 & 44.80 & 37.71 & 32.58 & 51.42 & 53.46 & 35.46 & 53.22 & 91.76 &	91.65 &	91.68 & 75.16\\
    % \hline
    \textbf{TURBO} $-$ \textbf{SF} & 54.72 & 42.95 & 35.26 & 29.65 & 50.69 & 52.38 & 32.97 & 52.16 & 91.67 & 91.35 & 91.50 & 74.62\\  \bottomrule
  \end{tabular}}
  \caption{Results of a comparative analysis of our proposed model with multiple state-of-the-art baselines and an ablation study. These analyses were conducted on the \dataset\ dataset. The best results are in boldface.}
  \label{comparative-analysis}
\end{table*}

We built our model on top of the base variant of the BART model. The feature dimension ($D_f$) of this variant of BART is 768. For the textual modality, the total number of tokens in each sample ($N$) is set to 256 by utilizing padding and truncation operations wherever necessary. The pre-trained visual feature embeddings extracted from vision transformer (\S \ref{sec:visual-semantics-extraction}) are of dimensions $50 \times 768$ which are projected to a $256 \times 768$ embedding space using a learnable linear layer. Additionally, the maximum number of objects that can be extracted from an image, $K$, is set to 36.

% \paragraph{Hyperparameters.} 
We use a learning rate of $10^{-3}$ for the GCN layers and that of $10^{-4}$ for BART. We employ AdamW \cite{loshchilov-etal-adamw} as the optimizer and train our model for 20 epochs using the standard cross entropy loss on a batch size of 16. We train our model on a system running an Ubuntu operating system with a Tesla V100-PCIE-32GB GPU. The model requires approximately 9GB of RAM for the same. 

% \paragraph{Evaluation Metrics.} In order to evaluate the generative performance of our model, we employed the BLEU-1, BLEU-2, BLEU-3, BLEU-4 \citep{papineni-bleu}, ROUGE-L, ROUGE-1, ROUGE-2 \citep{lin-2004-rouge}, METEOR \citep{banerjee-lavie-2005-meteor}, BERT-Score \citep{Zhang-2020BERTScore}, and Sent-BERT metrics \citep{reimers-gurevych-2019-sentence}. 

% In order to compute BERT-Score, we make use of the \verb|bert-score|\footnote{\href{https://pypi.org/project/bert-score/}{https://pypi.org/project/bert-score/}} library for Python which uses the large variant of RoBERTa \citep{liu2019roberta} for the same. For Sent-BERT, we use the \verb|sentence-transformers|\footnote{\href{https://pypi.org/project/sentence-transformers/}{https://pypi.org/project/sentence-transformers/}} Python library and use \verb|bert-large-cased|\footnote{\href{https://huggingface.co/google-bert/bert-large-cased}{https://huggingface.co/google-bert/bert-large-cased}} to extract sentence embeddings and compute cosine similarity scores among them. For the remaining metrics, we utilize the \verb|pycocoevalcap|\footnote{\href{https://pypi.org/project/pycocoevalcap/}{https://pypi.org/project/pycocoevalcap/}} Python library.