\section{Introduction}\label{sec:intro}
Sarcasm is a form of communication usually involving statements meant to insult or mock some targets, such as a person, an entity, or an event. These statements point to one interpretation when taken literally but given the context within which they are uttered, mean something completely different. In other words, there is an incongruity between the explicit and implicit meanings of a sarcastic utterance. Resolving this incongruity is important for properly interpreting a sarcastic message. Existing research suggests significant dependence on cues from multiple sources to interpret sarcastic messages. These can include tone of voice, body language, common sense, etc. Furthermore, the task of identifying and understanding sarcasm is quite relevant in a multimodal scenario where each modality refers to a different source of sarcastic cues.

Recognizing this, \newcite{Desai_Chakraborty_Akhtar_2022} proposed the task of multimodal sarcasm explanation. They utilized multimodal social media posts, consisting of visual and textual information, and generated an explanation to reveal the intended irony in the post. 
% Clues for the underlying sarcastic incongruity can be present in either (or both) of the modalities. 
Along with proposing the novel MORE dataset for this task, they also benchmarked this dataset using their proposed explanation model, ExMORE.
However, \newcite{jing-etal-2023-multi} recognized three limitations in ExMORE. They resolved these limitations by proposing their novel model, TEAM. The authors incorporated external world knowledge to aid sarcasm reasoning with the help of an innovative multi-source semantic graph. This allowed TEAM to perform exceptionally well, beating all baselines by significant margins. 
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{images/sample_post_grayscale.png}
  \caption{A sample in the \dataset\ dataset.}
  \label{fig:example-post}
\end{figure}

\paragraph{Motivation:} Despite its excellent performance, we identify two primary limitations in TEAM: \textbf{a)} \textit{ignoring vital visual cues}; and \textbf{b)} \textit{treating each node in the semantic knowledge relation as equivalent}. For the first case, TEAM converted the visual information into text-based metadata, which was subsequently used as a representation of all of the salient visual features. We argue that such an approach may lead to omitting vital visual features in the textual meta-data, representing the image. We hypothesize that both unimodal features, textual and visual, have their own significance in learning the multimodal semantics, leading to better explanations. For the second case, TEAM \cite{jing-etal-2023-multi} treated each extracted relation equivalent to each other by assigning them unit weights. However, we argue that some relations play relatively more significant roles than other relations in aiding sarcasm understanding.

% combining both unimodal feature spaces into a multimodal feature space will lead to a more comprehensive representation of the salient features present in both modalities. Such a representation will aid the semantic understanding of information from both modalities, leading to better explanations. 

% relevance of two nodes in. \textbf{a)} TEAM utilizes semantic information from both the textual and visual modalities by converting the visual information into text-based metadata. 

% It uses this metadata as a representation of all of the salient visual features. However, by converting the visual feature space into the textual feature space, some vital visual features may get overlooked. 

% We believe that combining both unimodal feature spaces into a multimodal feature space will lead to a more comprehensive representation of the salient features present in both modalities. Such a representation will aid the semantic understanding of information from both modalities, leading to better explanations.
% \textbf{b)} The aforementioned text-based metadata is extracted in the form of attribute-object pairs from images. While this information is useful, it only provides a broad outline of what is happening in an image. Additional visual information that can provide a more detailed view is not utilized by TEAM. We hypothesize that exploiting this unused information can enhance a model's understanding of the underlying sarcasm. 
% \textbf{c)} We identify a limitation in the way that \cite{jing-etal-2023-multi} construct their multi-source semantic graph. This graph has binary edge weights. If a token and an external knowledge concept are related to each other, they are connected by an edge with a weight of one. Otherwise, they are not connected by an edge (which is the same as connecting them with an edge of zero weight). Such a graph is undoubtedly useful in understanding whether two nodes are related or not. However, it does not provide much information about how related they are since two linked nodes will always have an edge of unit weight between them. We hypothesize that such information will lead to a better understanding of the relationships between tokens and external knowledge concepts, leading to better sarcasm reasoning. 

% \end{itemize}
% \begin{enumerate}
%     \item First, we identify a limitation in the way that \cite{jing-etal-2023-multi} construct their multi-source semantic graph. In this graph, an edge between a token and the external knowledge concept related to it can either have a weight of one (if it exists) or zero (if it does not exist). While this is certainly useful in understanding whether two nodes are related or not, it does not provide much information about how related they are since two linked nodes will always have an edge of unit weight between them. We hypothesize that information about the relatedness of nodes will act as a useful heuristic for an explanation model to better weigh these relationships, thereby aiding sarcasm reasoning.
%     \item In order to extract information from the visual modality, TEAM only utilizes object-level metadata extracted from the images in the form of attribute-object pairs. We hypothesize that despite the information present in these pairs being undoubtedly useful, there is additional semantic information present in the visual modality that is not utilized by the authors that can facilitate the understanding of the underlying sarcasm and consequently, lead to better explanations. 
%     \item While TEAM utilizes semantic information from both the textual and visual modalities, it does not attempt to fuse this information into a single multimodal representation. Instead, it essentially converts the visual information into textual information by extracting text-based attribute-object pairs, and uses them with the textual caption tokens to construct a multi-source semantic graph and learn a unified feature representation from it. Even though this feature representation contains information from both modalities and is thus multimodal in that respect, we hypothesize that combining uni-modal feature spaces into a multimodal feature space instead of converting one of the feature spaces into the other will lead to a more adequate incorporation of the salient features of both modalities and thus, a better understanding of the semantic information present in them.
% \end{enumerate}

We also identify the significance of the target of sarcasm in the explanations, as they are primarily intended to reveal the hidden irony towards the target. We observe that existing systems often fail to generate a relevant explanation when the target of sarcasm is highly implicit and needs a higher degree of cognition for comprehension. Therefore, we hypothesize that information about the target would lead to better comprehension by the model; and thus, improve the quality of explanation generation. Figure \ref{fig:example-post} depicts an instance of multimodal sarcasm explanation in the \dataset\ dataset. In the example, the user expresses the concerns towards the traffic in North Carolina (target of sarcasm) as conveyed by the explanation. We observe that the target of sarcasm is an inherent component of the expressed irony and must be exploited appropriately. 

Motivated by these limitations, we propose a novel method, \model\footnote{\textbf{T}arget-a\textbf{U}gmented sha\textbf{R}ed fusion-\textbf{B}ased sarcasm explanati\textbf{O}n}, to mitigate the challenges encountered by existing systems. \model\ comprises of three major components: \textbf{a)} \textit{knowledge-infusion for exploiting the external knowledge concepts considering their relevance}; \textbf{b)} \textit{a novel shared-fusion mechanism to obtain the multimodal fused representation}; and \textbf{c)} \textit{the target of sarcasm to guide the focused explanation generation}. To test our hypothesis, we extend the existing MORE dataset \cite{Desai_Chakraborty_Akhtar_2022} with manually annotated target label for the implied sarcasm. We call this extended dataset, \dataset. Our evaluation demonstrates the superiority of \model\ against the current state-of-the-art model, TEAM \cite{jing-etal-2023-multi}, and other baselines in both comparative analysis and human evaluation. We also explore the application of the Multimodal Large Language Models (MLLMs), such as, GPT-4o Mini, Llava-Mistral, and Llava-Llama-3, for the sarcasm explanation. We conclude the study with a qualitative error analysis and a human evaluation.

% must .  hypothesize that information about the entity being targeted by sarcasm can be quite useful in understanding the underlying sarcastic incongruity. This information has not explicitly been extracted or used by researchers in the past. So, to test our hypothesis, 
% we propose the use of this information to generate sarcasm explanations. To do this, 
% we extend the MORE dataset by manually annotating the entity being targeted by sarcasm for each sample. We call this entity the ``target of sarcasm''.  

% We propose a novel \textbf{T}arget-a\textbf{U}gmented sha\textbf{R}ed fusion-\textbf{B}ased multimodal sarcasm explanation m\textbf{O}del, aka. \model\footnote{Supplementary accompanies the dataset and code.}, and make the following contributions as part of our research. 

\paragraph{Contributions:} We summarize our contribution as follows: 

\begin{itemize}[leftmargin=*, noitemsep, nolistsep]
    \item \textbf{\dataset:} We extend the MORE dataset with target of sarcasm labels for 3,510 sarcastic posts. 
    \item \textbf{\model:} We propose a novel framework for multimodal sarcasm explanation. \model\ exploits the presence of target of sarcasm, external knowledge concepts, and a novel shared-fusion mechanism. 
    \item \textbf{Qualitative Analysis:} We perform extensive qualitative analysis in terms of error analysis and human evaluation.
    % \textbf{Shared Fusion}: We propose a novel method to fuse unimodal feature representations into a single multimodal representation. This mechanism helps the model better understand the intended meaning of a sarcastic post, leading to better explanations. Moreover, it resolves the first limitation of TEAM as identified by us.

    % \item \textbf{Resolution of Limitations of TEAM}: 
    % We resolve the second limitation of TEAM by extracting salient visual features at three different levels of detail. 
    % % Together, these levels of granularity help capture the different degrees of visual detail.
    % Additionally,
    % % , we resolve the third limitation of TEAM by proposing changes to the method used to construct its multi-source semantic graph. 
    % we recognize the advantage of constructing a knowledge graph and learning a feature representation from it. Following this, we borrow the idea of constructing such a knowledge graph from \cite{jing-etal-2023-multi}. However, we propose certain changes to its method of construction to incorporate the additional visual information extracted by us and also to resolve the third limitation. 
    % Additionally, we extract visual semantic information at three different granularity levels in order to resolve the second limitation mentioned earlier.
    
    % \item \textbf{Target of Sarcasm}: We extend the MORE dataset with an annotation, ``target of sarcasm''. This is a short phrase that indicates the entity being targeted by sarcasm in a post. 
    % We identify that exposing the target of sarcasm to the model allows it to better understand where the sarcastic incongruity lies, leading to explanations of a higher quality.
    % We empirically demonstrate that this annotation plays a key role in the superior performance of our model as compared to other state-of-the-art solutions.  
\end{itemize}

\paragraph{Reproducibility:} Code and dataset are available at \url{https://github.com/flamenlp/TURBO}.
% \begin{enumerate}
%     \item \textbf{Shared Fusion}. We propose a novel method to fuse the uni-modal semantic feature representations of both modalities into a single multimodal representation and prove its efficacy at facilitating the understanding of the implicit meaning present in a sarcastic sample, thereby leading to better explanations.
%     \begin{figure}[t]
%       \includegraphics[width=\columnwidth]{images/Frame 2post_example.png}
%       \caption{A sample in the MORE dataset.}
%       \label{fig:example-post}
%     \end{figure}
%     \item \textbf{Cause of Sarcasm}. We propose incorporating the cause of sarcasm, a short phrase that indicates the entity being targeted by the sarcasm in a post, for generating sarcasm explanations (refer to Figure \ref{fig:example-post}). Exposing the aforementioned entity to the model allows it to better understand where the sarcastic incongruity lies and thus, facilitates the generation of more apt explanations.
%     \item \textbf{Resolution of Limitations of TEAM}. As shown by \cite{jing-etal-2023-multi}, we recognize the advantage of utilizing external knowledge concepts by constructing a knowledge graph and learning a feature representation from it. Due to this, while we borrow the concept of creating such a knowledge graph from \cite{jing-etal-2023-multi}, we also propose changes to it in order to resolve some of the aforementioned limitations. Additionally, we extract visual semantic information at three different granularity levels in order to resolve the second limitation mentioned earlier in this section.
    
% \end{enumerate}
% \begin{figure}[!h]
% \begin{center}
% \includegraphics[width=0.45\textwidth]{images/lstm.pdf}     
% \caption{}
% \label{fig:lstm}
% \end{center} 
% \end{figure}
