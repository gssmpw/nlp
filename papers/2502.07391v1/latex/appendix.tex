\appendix
\section{Error Analysis}

In addition to the external knowledge-based errors detailed in Section 5.3, \model\ can exhibit some other types of errors, which can lead to inaccurate explanations. We lay them down as follows.

\paragraph{Missing External Knowledge Concepts:} We observe that for some samples, the model is unable to extract any external knowledge concepts for entities which are vital to the sarcastic incongruity. This can easily lead to the model not being able to resolve the sarcastic incongruity, especially if it is something that requires world knowledge to understand. 

For example, in Figure~\ref{fig:error-analysis-no-knowledge}, the author of the post is sarcastically mocking ``Jimmy'' for bringing a shovel to scare away any rattlesnakes in the grass. In order to understand why the author is being sarcastic, one must be aware of the fact that rattlesnakes are extremely dangerous creatures and using a shovel to deal with such an animal is not safe. The only way our model would be able to know this information is in the form of external knowledge concepts. In the figure, we have provided the knowledge concepts extracted by the model. Clearly, the model was unable to extract any concepts for ``rattlesnakes'' (underlined in the figure). Due to this, \model\ was unaware of how dangerous rattlesnakes are and how ineffective a shovel is against them. Consequently, it was unable to understand the underlying sarcasm in the post and generated an explanation that simply repeated what the author stated in the caption.

\begin{figure}[!t]
\centering
  \includegraphics[width=\linewidth]{images/erroranalysis_C.png}
  \caption{\textbf{Missing External Knowledge Concepts:} A sample where \model\ did not extract any knowledge concepts for the entity relevant to sarcasm. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-no-knowledge}
\end{figure}

\begin{figure}[!t]
\centering
  \includegraphics[width=\linewidth]{images/erroranalysis_B.png}
  \caption{\textbf{Insufficient OCR Features}: A sample containing OCR text where \model\ generated a completely wrong sarcasm explanation. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-OCR}
\end{figure}


\paragraph{Insufficient OCR Features:} A notable source of error in our model is that it does not explicitly extract features from the textual entities present in the visual modality, aka. OCR text. This is not an issue for the samples which contain a minimal amount of OCR text. However, in samples which rely heavily on it, we realize that \model\ often extracts insufficient features from the OCR text due to which it may generate partially or completely incorrect explanations.

For instance, Figure~\ref{fig:error-analysis-OCR} depicts a sample where the image contains text and sarcastically ridicules the new ``Face-ID'' technology introduced by Apple in their iPhone. It does so by suggesting the failure of ``Face-ID'' at a time of emergency, leading to some harm to the user. While the explanation generated by \model\ demonstrates that the model has recognized that the post is related to the ``Face-ID'' technology, it is inadequate at explaining the sarcasm behind the post. We observe that the description generated for this sample refers to the word ``police'' in the image. However, this is not sufficient or relevant enough to allow the model to identify the sarcastic incongruity. 
So, even though \model\ is able to identify OCR features to some extent, there is a definite scope in improving the sufficiency of these extracted features for more apt explanations.  




% We can identify the reason for this by observing the features that \model\ extracts for this sample, notably the image description (via BLIP) and the objects detected in it (via YOLOv9). We observe that while the generated description correctly states that there is text on a white background, it does not say anything about what the text actually says. Additionally, the model is unable to extract any objects from the images. This is problematic since the sarcastic incongruity relies heavily on the content of the OCR text. By not processing the OCR text properly, the model is unable to identify that a sarcastic incongruity even exists in the first place due to which it generates an incorrect explanation.  

\paragraph{Irrelevant Image Description:} We extract low-level detail from the images in the form of image descriptions generated using BLIP. While this model gives high quality and precise results, it may generate a description that is not quite relevant in understanding the underlying sarcasm. 

For instance, in Figure~\ref{fig:error-analysis-irrelevant-desc}, the author of the post sarcastically makes fun of the scarce crowd at a gathering. This is evident by the empty chairs and tables in the background of the corresponding image. However, the image description used by our model describes the bar in the foreground. As a result, even though the description is objectively accurate, it does not contribute to the semantic understanding of sarcasm in this instance. This, in turn, leads to \model\ generating an incorrect explanation that states that there is ``heaving'' (a big crowd) at the mentioned show when the exact opposite is true instead.  

\begin{figure}[!h]
\centering
  \includegraphics[width=\columnwidth]{images/erroranalysis_A.png}
  \caption{\textbf{Irrelevant Image Description}: A sample where \model\ generated a completely wrong sarcasm explanation due to irrelevant image description. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-irrelevant-desc}
\end{figure}