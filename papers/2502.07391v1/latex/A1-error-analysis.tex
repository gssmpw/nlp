\section{Error Analysis}
\label{sec:appendix-error-analysis}

Despite its excellent performance, we observe that \model\ can exhibit different types of errors, which can lead to inaccurate explanations. We lay them down as follows.



\paragraph{Extraction of Irrelevant External Knowledge Concepts:} We recognize that in some cases, the external knowledge concepts extracted by \model\ may not be relevant to the underlying incongruity. This can cause the model to focus on the wrong things when trying to decipher the underlying meaning of sarcasm, leading to inaccurate explanations.

For example, Figure~\ref{fig:error-analysis} shows one such case. The author uses sarcasm to remark that the fast food served at McDonald's is unhealthy and that it would not be a surprise if someone got a heart attack by eating it. We observe that the generated explanation completely misses the sarcasm. It seems to suggest that the author had a heart attack at McDonald's and that it is not a serious concern. 

\begin{figure}[!h]
\centering
  \includegraphics[width=0.8\columnwidth]{images/error_analysis_grayscale.png}
  \caption{\textbf{Extraction of Irrelevant External Knowledge Concepts:} A sample where \model\ extracted irrelevant external knowledge concepts for the entities vital to understanding the sarcastic incongruity.
}
\label{fig:error-analysis}
\end{figure}

Upon extracting the external knowledge concepts for this sample from the model, we see that the knowledge concept identified for the word ``heart''  is ``compassion'' and that for ``attack'' is ``defend''. The knowledge concepts make intuitive sense -- a heart is often used to symbolize compassion and the words ``attack'' and ``defend'' are antonyms of each other. However, these concepts are not relevant in this particular context. Instead, the phrase ``heart attack'' should be linked with a knowledge concept such as ``emergency'' since that is much more semantically relevant for this sample. 

We hypothesize that such errors can be mitigated by developing a mechanism that extracts external knowledge concepts by taking the underlying context into account.

\paragraph{Missing External Knowledge Concepts:} We observe that for some samples, the model is unable to extract any external knowledge concepts for entities which are vital to the sarcastic incongruity. This can easily lead to the model not being able to resolve the sarcastic incongruity, especially if it is something that requires world knowledge to understand. 

For example, in Figure~\ref{fig:error-analysis-no-knowledge}, the author of the post is sarcastically mocking ``Jimmy'' for bringing a shovel to scare away any rattlesnakes in the grass. In order to understand why the author is being sarcastic, one must be aware of the fact that rattlesnakes are extremely dangerous creatures and using a shovel to deal with such an animal is not safe. The only way our model would be able to know this information is in the form of external knowledge concepts. In the figure, we have provided the knowledge concepts extracted by the model. Clearly, the model was unable to extract any concepts for ``rattlesnakes'' (underlined in the figure). Due to this, \model\ was unaware of how dangerous rattlesnakes are and how ineffective a shovel is against them. Consequently, it was unable to understand the underlying sarcasm in the post and generated an explanation that simply repeated what the author stated in the caption.
\begin{figure}[!h]
\centering
  \includegraphics[width=\linewidth]{images/erroranalysis_C.png}
  \caption{\textbf{Missing External Knowledge Concepts:} A sample where \model\ did not extract any knowledge concepts for the entity relevant to sarcasm. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-no-knowledge}
\end{figure}

\paragraph{Insufficient OCR Features:} A notable source of error in our model is that it does not explicitly extract features from the textual entities present in the visual modality, aka. OCR text. This is not an issue for the samples which contain a minimal amount of OCR text. However, in samples which rely heavily on it, we realize that \model\ often extracts insufficient features from the OCR text due to which it may generate partially or completely incorrect explanations.

For instance, Figure~\ref{fig:error-analysis-OCR} depicts a sample where the image contains text and sarcastically ridicules the new ``Face-ID'' technology introduced by Apple in their iPhone. It does so by suggesting the failure of ``Face-ID'' at a time of emergency, leading to some harm to the user. While the explanation generated by \model\ demonstrates that the model has recognized that the post is related to the ``Face-ID'' technology, it is inadequate at explaining the sarcasm behind the post. We observe that the image description generated for this sample refers to the word ``police'' in the image. However, this is not sufficient or relevant enough to allow the model to identify the sarcastic incongruity. 
So, even though \model\ is able to identify OCR features to some extent, there is a definite scope in improving the sufficiency of these extracted features for more apt explanations. 
\begin{figure}[!h]
\centering
  \includegraphics[width=\linewidth]{images/erroranalysis_B.png}
  \caption{\textbf{Insufficient OCR Features}: A sample containing OCR text where \model\ generated a completely wrong sarcasm explanation. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-OCR}
\end{figure}

\begin{figure}[!h]
\centering
  \includegraphics[width=\columnwidth]{images/erroranalysis_A.png}
  \caption{\textbf{Irrelevant Image Description}: A sample where \model\ generated a completely wrong sarcasm explanation due to irrelevant image description. %\hl{Also show explanations for Llava, Team, and ExMore on this example only.}
}
\label{fig:error-analysis-irrelevant-desc}
\end{figure}
\paragraph{Irrelevant Image Description:} We extract low-level detail from the images in the form of image descriptions generated using BLIP. While this model gives high quality and precise results, it may generate a description that is not quite relevant in understanding the underlying sarcasm. 

For instance, in Figure~\ref{fig:error-analysis-irrelevant-desc}, the author of the post sarcastically makes fun of the scarce crowd at a gathering. This is evident by the empty chairs and tables in the background of the corresponding image. However, the image description used by our model describes the bar in the foreground. As a result, even though the description is objectively accurate, it does not contribute to the semantic understanding of sarcasm in this instance. This, in turn, leads to \model\ generating an incorrect explanation that states that there is ``heaving'' (a big crowd) at the mentioned show when the exact opposite is true instead.  
