% =============================================================
\subsection{Human Evaluation}
\label{sec:human-eval}
% =============================================================
\begin{table}
\setlength{\tabcolsep}{1mm}
\small
  \centering
  \begin{tabular}{lcccc}
    \toprule
     % \bf Model & \bf Fluency & \bf Semantic Accuracy & \bf Negative Connotation & \bf Presence of Target of Sarcasm

    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Fluency}} &  \bf Sem. & \multirow{2}{*}{\textbf{Negativity}} & \bf Target \\ 
    % \cline{2-6}
     &  & \bf Acc. &  & \bf Presence \\  
    
     
     % \\
     \midrule
    \textbf{TURBO} & 4.18 & 3.80 & 3.82 & 4.00 \\ \midrule
    \textbf{TEAM} & 3.96 & 3.38 & 3.48 & 3.47\\ \midrule
    \textbf{GPT-4o-Mini} & 4.23 & 4.46 & 4.33 & 4.33\\
    \textbf{LLaVa-Mistral} & 4.26 & 4.09 & 4.22 & 4.03\\
    \textbf{LLaVa-Llama3} & 4.25 & 3.69 & 3.68 & 3.72\\
    % \hline  
    \bottomrule   
  \end{tabular}
  \caption{Human evaluation on \model\ and other baselines. All LLMs are employed in a one-shot setting.}
  \label{table:human-eval}
\end{table}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{images/human_eval_qualitative_samples.png}
  \caption{Two samples where: \textbf{a)} \model\ correctly explained the sarcasm and GPT-4o mini missed it; and \textbf{b)} \model\ generated an inaccurate explanation but GPT-4o mini explained the sarcasm correctly.}
  \label{fig:human-eval-qualitative-samples}
\end{figure*}

We conduct a human evaluation to assess the quality of generated explanations more comprehensively. We perform this evaluation on 20 diverse samples from the test set. These samples are assessed by 20 evaluators. For each sample, we provide the evaluator with five explanations, each generated by a different model. We ask the evaluator to judge each explanation on the following four metrics:
\begin{itemize}[leftmargin=*, noitemsep, nolistsep]
    \item{\textbf{Fluency:} This metric assesses \textit{how easy it is to understand the semantic meaning of a given explanation}. % It does not take into account whether the intended meaning of sarcasm has been captured, instead, it represents the straightforwardness of each generation and its grammar. 
    }
    \item{\textbf{Semantic Accuracy:} This metric is used to determine \textit{how well a generation captures the intended meaning of a sarcastic post}.} 
    \item{\textbf{Negative Connotation:} Sarcasm expresses negative sentiment by default; hence, it must be captured in the explanation as well. We employ this metric to understand \textit{how appropriately the negative connotation is captured in the explanation}.}
    % So, any sentence explaining the intended meaning of sarcasm should explicitly mention the negative connotation around this entity. Using this metric, we capture the degree   
    \item{ \textbf{Presence of Target:} This metric evaluates the presence of the target of sarcasm in the generated explanation. %Since the target of sarcasm is the entity around which the sarcastic incongruity lies, it is important for a sarcasm explanation to refer to this entity clearly to avoid ambiguities.
    }
\end{itemize}
The results for this evaluation are given in Table \ref{table:human-eval} with all metrics rated on a 5-point Likert scale. \model\ outperforms TEAM by 4.40\%, 8.40\%, 6.80\%, and 10.60\% on the above four metrics respectively. It even outperforms LLaVa-Llama3 on every metric except fluency, on which it is only 1.40\% worse. Additionally, we recognize that it performs worse than LLaVa-Mistral and GPT-4o-Mini by an average of 4.00\% and \textasciitilde 7.75\% respectively. However, we note that \model\ has 234 million parameters as compared to GPT-4o-Mini, LLaVa-Mistral, and LLaVa-Llama3 which have 7-8 billion parameters each. Therefore, with an approximately 3000\% reduction in the number of model parameters, \model\ demonstrates, at most, a 7.75\% dip in performance when compared to various state-of-the-art MLLMs. 

% In addition to qualitatively analyzing the performance of \model\ and the existing baselines, we provide two instances in 

Figure \ref{fig:human-eval-qualitative-samples} depicts two samples and the explanations generated by \model\ and GPT-4o mini. In the first sample, the user sarcastically points out that the safety netting at a baseball game does not obstruct one's view and that people are needlessly complaining about the same. While \model\ is able to understand the sarcasm and correctly points out that the netting does not ruin one's view of the game, GPT-4o mini instead incorrectly identifies the user as someone who is concerned more about their view rather than their protection.

In the second sample, the author sarcastically points out how unhealthy fast food is by feigning surprise that someone got a heart attack at McDonald's. Here, \model\ mistakenly concludes that the author is surprised about someone getting a heart attack at McDonald's but does not think it is a serious concern. GPT-4o mini, however, is able to correctly understand the underlying sarcasm and generates an accurate explanation.