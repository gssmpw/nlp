\section{Methodology}\label{sec:method}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{images/TURBO_schematic_grayscale.png}
  \caption{A schematic diagram of \model.}
  \label{fig:CASH}
\end{figure*}

In this section, we describe the proposed model and give a detailed account of its various components.% We utilise BART as the backbone for our proposed model. 

\subsection{Extraction of Visual Semantics} \label{sec:visual-semantics-extraction}
% Extracting relevant visual features is vital for understanding sarcasm.
Considering the complex nature of sarcasm, the extraction of relevant visual information is quite important. 
In some samples, the sarcastic incongruity may be more relevant to a broad detail present in the image. However, in other samples, the sarcasm may become evident only after considering a small visual detail. Considering such diverse cases, we extract information from the visual modality at three different levels of granularity. 

\paragraph{Low-Level Detail:} This includes visual features that only provide an overview of an image. We capture such features by generating a single natural language description for each image. For this, we use the large variant of the BLIP model for image captioning\footnote{\url{https://huggingface.co/Salesforce/blip-image-captioning-large}} \citep{li2022blip}. Formally,
\begin{equation}
    \label{eq: coarse-grained-semantics}
    \begin{aligned}
        BLIP(V_i) = D_i = \{d_1^{(i)}, d_2^{(i)}, \dots d_{N_{D_i}}^{(i)}\}\\
    \end{aligned}
\end{equation}
where $D_i$ is the generated description of $V_i$. Also, $d_j^{(i)}$ is the $j^{th}$ token present in $D_i$.
% $N_{D_i}$ is the total number of tokens in $D_i$.
        
\paragraph{Medium-Level Detail:} Though low-level details provide helpful information to the model and facilitate its understanding of the underlying semantic incongruity, such generated descriptions may not capture the entire semantic meaning present in an image. Consequently, it leads us to capture information about individual entities appearing in an image as medium-level details. Identifying such entities in an image helps the model analyze a more detailed view of it. These entities are extracted by performing object detection on each image using the YOLOv9 object detection model \cite{wang2024yolov9}. We keep the top $K$ objects with the highest confidence to ensure that only the most relevant object-level semantics are retained.
\begin{equation}
    \label{eq: medium-grained-semantics}
    \begin{aligned}
        YOLOv9(V_i) = O_i = \{o_1^{(i)}, o_2^{(i)}, \dots o_K^{(i)}\}\\
    \end{aligned}
\end{equation}
where $o_j^{(i)}$ is a text label describing the $j^{th}$ object extracted from $V_i$ using YOLOv9. 

\paragraph{High-Level Detail:} Finally, we utilize semantic-rich representations of visual features. These are extracted in the form of embedding vectors from a pre-trained vision transformer \cite{dosovitskiy2021image}.
\begin{equation}
    \label{eq: fine-grained-semantics}
    \begin{aligned}
        ViT(V_i) = E_v^{(i)} =
        \begin{bmatrix}
            \textbf{v}_1^{(i)} & \textbf{v}_2^{(i)} & \dots & \textbf{v}_m^{(i)}
        \end{bmatrix}^T
    \end{aligned}
\end{equation}
where $E_v^{(i)} \in \mathbb{R}^{m \times D_f}$ is the feature matrix of $V_i$ containing ``$m$'' $D_f$-dimensional vectors.
% Here, $D_f$ is the length of each vector.
\subsection{External Knowledge Retrieval}
ConceptNet\footnote{\url{https://conceptnet.io/}} \cite{speer2017conceptnet} is a knowledge graph that structures general human knowledge as a directed and weighted graph. We use it to extract relevant external knowledge concepts for our model. 

For the $i^{th}$ sample, we retrieve external knowledge concepts related to caption tokens in $C_i$, $D_i$ (\S Equation~\ref{eq: coarse-grained-semantics}) and $O_i$ (\S Equation~\ref{eq: medium-grained-semantics}). Each token is queried through ConceptNet and we utilize its one-hop neighboring knowledge concept along with its relevance score. 

Note that frequent words\footnote{NLTK stopwords \cite{nltk}} such as `the', `and', `is', etc. are not queried. Formally, 
\begin{equation}
    \label{eq: external-knowledge-retrieval}
    \begin{aligned}
        EK(t_i) & = ConceptNet(t_i)_1 = (T_1^{(i)}, r_1^{(i)})
    \end{aligned}
\end{equation}
where $t_i \notin StopWords$ is a queried token and $T_1^{(i)}$ is its one-hop neighbouring external knowledge concept with a relevance score of $r_1^{(i)}$.

\subsection{Knowledge Enrichment}
\label{sec:knowledge-enrichment}
For each sample, we first 
% structure tokens of the caption, $D_i$ and $O_i$ and their corresponding knowledge concepts into a knowledge graph. This aids our model in learning various semantic relationships by supporting them by general world knowledge. 
perform string concatenation on $C_i$, $D_i$, $O_i$, and their related knowledge concepts. The resultant string is called $T_{knowledge}$. This is a knowledge-enriched sequence of tokens that provides the model with more information than what is present in just $C_i$. Formally,
\begin{equation}
    \label{eq: t-knowledge}
    \begin{aligned}
        T_{knowledge}^{(i)} = C_i + CC_i + D_i + DC_i + O_i + OC_i
    \end{aligned}
\end{equation}
where $CC_i$, $DC_i$ and $OC_i$ are the knowledge concepts corresponding to the tokens of $C_i$, $D_i$ and $O_i$, respectively.

We put a constraint on the ordering of the individual tokens in $CC_i$, $DC_i$, and $OC_i$ to avoid random permutations. If $c_j^{(i)}, c_k^{(i)} \in C_i$ and their respective knowledge concepts are $cc_p^{(i)}, cc_q^{(i)} \in CC_i$. Then,
\begin{equation}
    \label{eq: knowledge-tokens-constraints}
    \begin{aligned}
        j < k \iff p < q
    \end{aligned}
\end{equation}

Similar constraints are placed on $DC_i$ and $OC_i$.

\subsection{Construction of Knowledge Graph}
\label{sec:graph-construction}
$T_{knowledge}$ simply provides the model with additional information in the form of a sequence of tokens. However, the relationships among these tokens are often non-sequential and would be much better represented by a non-linear data structure such as a graph. 

Therefore, we build an undirected, weighted graph $G$ of these tokens. Not only is this a more appropriate representation of the various inter-token relationships, it also facilitates our model's understanding of them. We adopt the following strategy to construct it. 

\begin{enumerate}
    \item We construct an edge of unit weight between every pair of consecutive tokens in $C_i$. 
    \begin{equation}
      \label{eq:connect-input-caption}
      \begin{aligned}
        \forall j: e(c_j^{(i)}, c_{j+1}^{(i)}) = 1 
      \end{aligned}
    \end{equation}
    
    where $c_j^{(i)}$ is the $j^{th}$ token in $C_i$ and $e(a,b)$ denotes the weight of the edge linking nodes $a$ and $b$.
    \item Each $c_j^{(i)}$ is then linked with its corresponding external knowledge concept ($cc_j^{(i)}$) with an edge of a weight equal to the corresponding relevance score ($r_j^{(i)}$). This helps in capturing the strength of the relationship between the nodes.   
    \begin{equation}
      \label{eq:caption-concepts}
      \begin{aligned}
        \forall j: e(c_j^{(i)}, cc_j^{(i)}) = r_j^{(i)}
      \end{aligned}
    \end{equation}
    
    \item A similar strategy is adopted for $D_i$ and $O_i$.
    \begin{equation}
      \label{eq:image-caption-connections}
      \begin{aligned}
        \forall j: e(d_j^{(i)}, d_{j+1}^{(i)}) & = 1 \\
        \forall j: e(d_j^{(i)}, dc_j^{(i)}) &  = r_j^{(i)} \\
        \forall j: e(o_j^{(i)}, oc_j^{(i)}) & = r_j^{(i)}
      \end{aligned}
    \end{equation}
    
    % \item 
 \end{enumerate}   

\noindent Note that the object tokens do not follow a syntactic ordering; therefore, we do not construct edges between consecutive object tokens. 
    
    % tokens are not natural language sentences but rather short labels, they
    
    % \item Lastly, if a token present in the generated image caption is also present in the list of object tokens detected from the image, we connect the two tokens with an edge of unit weight. 
    % \begin{equation}
    %   \label{eq:intramodal-connections-ic-io}
    %   \begin{aligned}
    %     e(o_j^{(i)}, o_{concept_j}^{(i)}) = r_j^{(i)}
    %   \end{aligned}
    % \end{equation}


% The aforementioned connections have all been within pairs of tokens extracted from the same modality. In addition to these intra-modal connections, we also capture inter-modal relations between tokens by forming corresponding edges in our knowledge graph. These are constructed as follows.
% \begin{enumerate}
%     \item For each token of the input caption, if the same token exists in the generated image caption, the two tokens are connected with an edge weight of 1. Specifically,
%     \begin{equation}
%       \label{eq:crossmodal-connections-cc-icc}
%       \begin{aligned}
%         e(o_j^{(i)}, o_{concept_j}^{(i)}) = r_j^{(i)}
%       \end{aligned}
%     \end{equation}
% \end{enumerate}

% \begin{equation}
%   \label{eq:ext-knowledge-concat}
%   \begin{aligned}
%     T_{knowledge}^{(i)} & = \sum_1^{N_{C_i}}c_j^{(i)} + \sum_1^{N_{CC_i}}c_{concept_j}^{(i)}  \\
%     & + \sum_1^{N_{IC_i}}ic_j^{(i)} + \sum_1^{N_{ICC_i}}ic_{concept_j}^{(i)} \\
%     & + \sum_1^{N_{O_i}}o_j^{(i)} + \sum_1^{N_{OC_i}}o_{concept_j}^{(i)}
%   \end{aligned}
% \end{equation}
% where $\sum$ and $+$, both represent the string concatenation operation.

\subsection{Incorporation of Target of Sarcasm}
\label{sec:incorporation-of-target}
The target of sarcasm is incorporated in the model by concatenating it with $T_{knowledge}$ as follows:
\begin{equation}
\label{eq:target-incorporation}
T_{concat}^{(i)} = T_{knowledge}^{(i)} + \verb|</s>| + TS_i
\end{equation}

\noindent where $TS_i$ is the target of sarcasm for the $i^{th}$ sample and \verb|</s>| is the BART separator token. $T_{concat}$ is the final sequence of tokens that is fed to the model as input along with the sample image. We use BART to extract contextual embeddings, $E_t$, for $T_{concat}$ where $E_t \in \mathbb{R}^{N \times D_f}$.

% \subsection{Graph-Based Sarcasm Reasoning and Extraction of Semantically Relevant Features Using Graph Convolution}
\subsection{Sarcasm Reasoning}
\label{sec:graph-convolution}
In order to facilitate sarcasm reasoning and extract the salient features from our knowledge graph, we follow \newcite{jing-etal-2023-multi} and use a Graph Convolution Network (GCN) \cite{kipf2017semisupervised}. We incorporate $L$ GCN layers in our model where the output of layer $l$ is computed as follows. 
\begin{equation}
  \label{eq:gcn}
  \begin{aligned}
    H_l^{(i)} = f(\hat{D_i}^{-\frac{1}{2}}\hat{A_i}\hat{D_i}^{-\frac{1}{2}}H_{l-1}^{(i)}W_l)
  \end{aligned}
\end{equation}

\noindent where, for the $i^{th}$ sample, $H_l^{(i)}$ is the output of layer $l$ and $\hat{A_i}$ is the adjacency matrix corresponding to the knowledge graph. $\hat{D_i}$ is a diagonal matrix that stores the degree of each node in it. $W_l$ is a learnable weight matrix and $f$ is a non-linear activation function. Additionally, $H_0^{(i)} = E_t^{(i)}$.

$H_L^{(i)}$ is the final output of the GCN and provides a representation of the salient semantic features derived from the nodes of the knowledge graph. 

\subsection{Shared Fusion}
In order to facilitate the sharing of information between and within modalities, we propose a novel shared fusion mechanism. We first perform self-attention \citep{attention-is-all-you-need} on the text and image embeddings which allows the model to capture important intra-modal semantic relationships.  
\begin{equation}
  \label{eq:Self Attention}
  \begin{aligned}
  A_v^{(i)} & = Softmax(\frac{Q_v^{(i)} K_v^{(i)^T}}{d_k})V_v^{(i)} \\
  A_t^{(i)} & = Softmax(\frac{Q_t^{(i)} K_t^{(i)^T}}{d_k})V_t^{(i)}
  \end{aligned}
\end{equation}
\noindent where $K_v$, $Q_v$ and $V_v$ are the key, query, and value matrices used while computing self-attention on the visual embeddings. Similarly, $K_t$, $Q_t$, and $V_t$ are used to compute self-attention on text embeddings. 

We use $A_v^{(i)}$ to amplify relevant features represented in $E_t^{(i)}$ and vice versa. This facilitates a cross-modal flow of information and aids multimodal learning as follows:
\begin{equation}
  \label{eq:Fit-Fti}
  \begin{aligned}
  F_{vt}^{(i)} = A_t^{(i)} \odot E_v^{(i)} ; \quad F_{tv}^{(i)} = A_v^{(i)} \odot E_t^{(i)}
  \end{aligned}
\end{equation}

\noindent where $\odot$ denotes element-wise multiplication. 

While $F_{vt}$ and $F_{tv}$ capture cross-modal relationships, we need a mechanism that can dynamically control both the cross-modal and intra-modal flow of information. This is because, for different samples, 
% sarcasm is a complex figure of speech. As a result,  
% Due to the complex nature of sarcasm, 
each modality may contribute differently towards the sarcastic incongruity. In some samples, the image may contribute more towards the sarcastic incongruity while in others, the text caption may be a bigger factor. Therefore, we need a mechanism that ensures that the model does not overly rely on either modality. 

We implement such a mechanism in the form of a gated fusion mechanism. Here, $F_{vt}$ \& $F_{tv}$ serve as the cross-modal feature representations and $E_v$ \& $E_t$ serve as the unimodal feature representations. The gating weights for this mechanism control the flow of information between these sources of information and are computed as follows:
% to learns to weigh the information provided by both modalities dynamically. This mechanism to controls both inter-modal and intra-modal flow of information and allows the model to 
% As a result, the model is able to better decide which unimodal features contribute the most to the sarcastic incongruity. We compute the weights for the gated fusion mechanism as follows.
\begin{equation}
    \label{eq: gated-fusion-weights}
    %Use this for AAAI style
    % \begin{aligned}
    %     G_v^{(i)} = \sigma(W_v^{(i)}E_v^{(i)} + b_v^{(i)}) ; \quad  G_t^{(i)} = \sigma(W_t^{(i)}E_t^{(i)} + b_t^{(i)})
    % \end{aligned} \nonumber
    %Use this for ACL style
    \begin{aligned}
        G_v^{(i)} & = \sigma(W_v^{(i)}E_v^{(i)} + b_v^{(i)}) \\  G_t^{(i)} & = \sigma(W_t^{(i)}E_t^{(i)} + b_t^{(i)})
    \end{aligned} \nonumber
\end{equation}
where $W_v^{(i)}$ and $W_t^{(i)}$ are learnable weight matrices and $b_v^{(i)}$ and $b_t^{(i)}$ are their corresponding biases. $\sigma$ refers to the sigmoid function. 

Using $G_v^{(i)}$ and $G_t^{(i)}$, we compute the final shared fusion matrix as a weighted sum of four individual gated fusions. These are given as follows.
\paragraph{Fusing Two Multimodal Representations.} The semantic relationships and features captured by $F_{tv}$ will be different from those captured by $F_{vt}$. Thus, we use $G_v$ and $G_t$ to separately weigh the semantic information present in them. This allows us to capture the salient features in both multimodal representations.
\begin{equation}
    \label{eq: F1-F2}
    \begin{aligned}
        F_1^{(i)} = (G_v^{(i)} \odot F_{tv}^{(i)}) + [(1-G_v^{(i)}) \odot F_{vt}^{(i)}] \\
        F_2^{(i)} = (G_t^{(i)} \odot F_{tv}^{(i)}) + [(1-G_t^{(i)}) \odot F_{vt}^{(i)}]
    \end{aligned}
\end{equation}

\paragraph{Fusing Multimodal and Unimodal Representations.} It is also possible that by bringing the salient features of both modalities together, the sarcastic incongruity may become less evident than if just one of the modalities was considered. Thus, we compare the semantic information present in an unimodal representation with that present in a multimodal representation. 

Specifically, we first use $G_v$ to weigh the relevant features present in just the visual modality ($E_v$) as well as those present in $F_{tv}$.  
\begin{equation}
    \label{eq: Fv}
    \begin{aligned}
        F_v^{(i)} = (G_v^{(i)} \odot E_v^{(i)}) + [(1-G_v^{(i)}) \odot F_{tv}^{(i)}]
    \end{aligned}
\end{equation}

We then compute $F_t$ by performing a similar computation as above, with the textual modality. 
\begin{equation}
    \label{eq: Ft}
    \begin{aligned}
        F_t^{(i)} = (G_t^{(i)} \odot E_t^{(i)}) + [(1-G_t^{(i)}) \odot F_{vt}^{(i)}]
    \end{aligned}
\end{equation}

We combine $F_1$, $F_2$, $F_v$, and $F_t$ to allow the model to weigh the salient features captured in each of these matrices. This leads to a multimodal representation containing only the most semantically relevant information with respect to the underlying sarcasm. 
\begin{equation}
    \label{eq: sf-out}
    \begin{aligned}
        F_{SF}^{(i)} = \alpha_1F_1^{(i)} + \alpha_2F_2^{(i)} + \beta_1F_v^{(i)} + \beta_2F_t^{(i)}
    \end{aligned}
\end{equation}

where $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta_2$ are learnable parameters allowing the model to dynamically weigh the information present in the four individual matrices. 

\subsection{Sarcasm Explanation Generation}
In order to generate the sarcasm explanations, we sum $H_L$ and $F_{SF}$.
\begin{equation}
    \label{eq: encoder-inp}
    \begin{aligned}
        Z_i = H_L^{(i)} + F_{SF}^{(i)}
    \end{aligned}
\end{equation}

We pass $Z$ to BART to fine-tune it on the \dataset\ dataset for generating sarcasm explanations. The self-attention layer in its encoder captures various semantic relationships and dependencies while the decoder generates the explanations in an auto-regressive manner, that is, by taking the previously generated words into account when predicting the next word in a sequence. 

% We fine-tune BART on the MORE dataset for generating sarcasm explanations by passing $Z$ to the pre-trained BART encoder which captures the various multimodal semantic relationships and dependencies primarily through the use of its self-attention layer. The output of the encoder is then passed to the BART decoder which works in an auto-regressive manner, that is, it generates a sarcasm explanation by taking the previously generated words into account when predicting the next word in the sentence. 
% \begin{equation}
%     \label{eq: decoder}
%     \begin{aligned}
%         Z_{encoder}^{(i)} = Encoder(Z^{(i)}) \\
%         \hat{y}_t^{(i)} = Decoder\Bigl(Z_{encoder}^{(i)}, \hat{y}_{prev}^{(i)}\Bigr)
%     \end{aligned}
% \end{equation}

% where $\hat{y}_t^{(i)}$ is the probability distribution of the $t^{th}$ generated token and $\hat{y}_{prev}^{(i)} = \{ \hat{y}_{t-1}^{(i)}, \hat{y}_{t-2}^{(i)}, \dots, \hat{y}_{1} ^{(i)} \}$ is the sequence of tokens generated previously by the decoder. 

% It is important to note that while training the model, $y_{prev}^{(i)}$ (sequence of previous $t-1$ target tokens) is used instead of $\hat{y}_{prev}^{(i)}$ in order to prevent the accumulation of generation errors.