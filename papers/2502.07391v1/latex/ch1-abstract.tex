\begin{abstract}
% The task of Multimodal Sarcasm Explanation (MuSE) is a challenging one that involves building computational systems that can analyze a sarcastic multimodal social media post, consisting of an image and a corresponding text caption, and generate relevant and appropriate natural language sentences to explain why it is sarcastic. The existing state-of-the-art in this task is called TEAM and utilizes a multi-source semantic graph to capture the relationships among features extracted from each modality and link these features with relevant external knowledge concepts. Note that visual features are extracted in the form of text-based object-level metadata (attribute-object pairs). Despite remarkable performance, TEAM does not take into account the relevance of each external knowledge concept, the sufficiency of object-level metadata in representing all of the salient visual features, as well as the potential advantages of fusing the feature spaces of both modalities to obtain a shared multimodal feature space. We solve these limitations in our proposed Cause-Augmented SHared fusion-based multimodal sarcasm explanation generation model, CASH. Additionally, we incorporate a short annotated phrase known as the cause of sarcasm into our system. This phrase indicates the entity being targeted by a sarcastic post and helps the model understand where the sarcastic incongruity lies, thus facilitating sarcasm reasoning and better explanations. To demonstrate the superiority of our model over other existing baselines (including the current state-of-the-art), we conduct various experiments on the MORE dataset.

Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a \textbf{T}arget-a\textbf{U}gmented sha\textbf{R}ed fusion-\textbf{B}ased sarcasm explanati\textbf{O}n model, aka. \model. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. \model\ assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed \model\ model on the \dataset\ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of \model\ by an average margin of $+3.3\%$. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on \model's generated explanations and find them out to be comparatively better than other systems. 

% Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. 
% Though important, existing systems overlooked the significance of sarcasm target in generating explanations. In this paper, we propose a \textbf{T}arget-a\textbf{U}gmented sha\textbf{R}ed fusion-\textbf{B}ased sarcasm explanation m\textbf{O}del, aka. \model. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. \model\ assumes the target of the sarcasm and guides the multi-modal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed \model\ model on the \dataset\ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of \model\ by an average margin of $+3.3\%$. Moreover, we explore LLMs in a zero/one-shot setting for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuisances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on \model's generated explanations and find them out to be comparatively better than other systems. 



% is a challenging task where AI systems analyze social media posts with images and captions to explain sarcasm through natural language sentences. 
% The current state-of-the-art model employs multi-source semantic knowledge graph to capture the inter-modality relationships and external knowledge concepts. However, it fails to leverage the relevance of each external knowledge concept, as different concepts play different roles for sarcasm. Moreover, 




% In this paper, we 

% , the adequacy of object-level metadata (extracted as text-based attribute-object pairs) in representing all salient visual features, and the potential benefits of integrating feature spaces of both modalities to obtain a shared multimodal feature space. 


% Our proposed model, \textbf{C}ause-\textbf{A}ugmented \textbf{SH}ared fusion-based multimodal sarcasm explanation (CASH), addresses these limitations by enhancing feature fusion and relevance consideration for improved sarcasm explanation. We further enhance our system by incorporating a `cause of sarcasm' phrase. This short annotation identifies the target of the sarcasm, aiding the model in understanding the root of the irony and enabling more accurate reasoning and explanations. We evaluate our proposed model on the MORE dataset, and the proposed model surpasses the state-of-the-art by an average margin of 3.33\%.

% Multimodal Sarcasm Explanation (MuSE) is a challenging task where AI systems analyze social media posts with images and captions to explain sarcasm through natural language sentences. The current state-of-the-art model uses a multi-source semantic graph to capture the relationships between features from each modality and external knowledge concepts. However, it fails to consider the relevance of each external knowledge concept, the adequacy of object-level metadata (extracted as text-based attribute-object pairs) in representing all salient visual features, and the potential benefits of integrating feature spaces of both modalities to obtain a shared multimodal feature space. Our proposed model, \textbf{C}ause-\textbf{A}ugmented \textbf{SH}ared fusion-based multimodal sarcasm explanation (CASH), addresses these limitations by enhancing feature fusion and relevance consideration for improved sarcasm explanation. We further enhance our system by incorporating a `cause of sarcasm' phrase. This short annotation identifies the target of the sarcasm, aiding the model in understanding the root of the irony and enabling more accurate reasoning and explanations. We evaluate our proposed model on the MORE dataset, and the proposed model surpasses the state-of-the-art by an average margin of 3.33\%.



% the experimental results demonstrate its superiority with an average improvement of 2-3\%.


% We evaluate our proposed approach on the MORE dataset to validate the superiority of our model and set a benchmark against existing approaches.

% To validate our model's superiority, we conduct experiments on the MORE dataset, benchmarking it against existing approaches. 


\end{abstract}



