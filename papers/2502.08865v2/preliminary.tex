\section{Preliminaries} \label{sec:prelim}

\subsection{Threat Model}
%We assume that the target of the adversary has two levels: 1) \textbf{Untargeted}: cause wrong pose estimation or Denial-of-Service (DoS); 2) \textbf{Targeted}: lead the pose estimation in a specific way defined by the adversary. 
%JC: we do not discuss targeted/untargeted later, so I am removing

\textbf{Attack Scope.} We assume that the attackers can play acoustic sounds nearby to affect the integrity of sensor data, but cannot directly access the digitized sensor readings or physically touch the sensors on the device. This is because commercial products in XR disable write access to inertial sensors both in physical ways and through software APIs, in an attempt to help improve the security and privacy of users, which are widely explored by researchers~\cite{michalevsky2014gyrophone,aviv2012practicality,owusu2012accessory,dey2014accelprint,marquardt2011sp}.
Attackers may use the attack to either benefit themselves as users of the XR headset (\eg to cheat in a game) or to harm victims who are wearing the XR headset (\eg to inhibit their performance in a game).

\textbf{Sensor Access.} Although we assume that attackers cannot physically access internals of the target devices when they conduct attacks, we do allow the adversary to have access to a device in the same model, which means that the attacker can profile the device's behavior under different acoustic frequencies and amplitudes with a sample device. This assumption is reasonable since attackers can purchase XR headsets on their own to study their behavior. The transferability of attacks between different devices of the same model has been confirmed by previous studies~\cite{trippel2017walnut}. 

\textbf{Speaker Access.} We allow the attacker to generate acoustic signals from any direction around the victim device, at frequencies in the range of human audible sound to ultrasonic (2-30 kHz). This can be done by an attacker playing a sound file while following the user or by speakers in the environment.
%tracking the movements of the device. 
Although sound in the human audible range may be noticeable for users, we allow it in our threat model because its perceptibility depends on the ambient sound volume or music playing in the environment.
Further, users who seek to benefit from the attack by causing themselves advantages in the XR experience would not care about acoustic perceptibility.
%adversaries can utilize any sound to achieve their cheating goals in a multi-user game or APP. 

%\subsection{XR Processing Pipeline}
%\jiasi{explain different layers of processing, starting from sensors to SLAM to HL2SS to Unity app. Maybe need a pic or fold into Fig. 1.} \zijian{I think the pipeline is sensors, SLAM, Unity. hl2ss is just logging the output from SLAM?}

\subsection{Background on Acoustic Attacks on IMU}
\label{sec:background}
%\jiasi{reduce this subsection since we don't use the fancy effects} \zijian{maybe we can shorten it here and move details to appendix?}
Modeling the effects of acoustic signals on MEMS IMUs has been previously studied~\cite{trippel2017walnut}. Here we will briefly describe the model as background for our later simulations and experiments. Because our experimental results later demonstrate successful attacks on the accelerometer, 
%we find similar resonance phenomena in both accelerometers and gyroscopes, 
here we will use the accelerometer for the purposes of explanation, but similar models apply to gyroscopes.

We denote electrical acceleration signals generated by true acceleration $s(t)$ and those generated
by acoustic interference $s_a(t)$. In general, the measured acceleration can be modeled as a linear combination of the true acceleration and acoustic acceleration, which means the measured acceleration $\hat{s}(t)$ by the sensor can be expressed as
\begin{align}
    \hat{s}(t) = s(t) + A_0 s_a(t)
\end{align}
where $A_0$ represents the attenuation of the acoustic signal while in transit to the target device. Because the acoustic acceleration can be modeled as $s_a(t) = A_1 \cos(2\pi F_a t+\phi)$, with frequency $F_a$, amplitude $A_1$, and phase $\phi$, the measured acceleration can be re-written as
\begin{align}
    \hat{s}(t) = s(t) + A_0 A_1  \cos(2\pi F_a t+\phi).
\end{align}
Note that vibrating these systems at their resonant frequencies achieves maximum displacement of the spring mass, \ie $A_0 = 1$.

According to \cite{trippel2017walnut}, there are two kinds of possible attacks: (1) \textbf{Output Biasing Attack} by utilizing sampling deficiencies of the Analog-to-Digital Converter (ADC); and (2) \textbf{Output Control Attack} due to insecure amplifiers, where accelerometers exhibit constant shifted false measurements at their resonant frequencies. 

\subsubsection{Output Biasing Attack}
The output biasing attack consists of two main steps: \textbf{Stablizing} and \textbf{Reshaping}
\begin{enumerate}
    \item \textbf{Stablizing.} The first step is to utilize a DC alias of the acceleration signal at the ADC to generate constant false measurements. This happens when the analog signal’s frequency is an integer multiple of the sampling frequency $F_{\text{samp}}$. We denote the sampling times at discrete intervals $k$ as $t_k=k\cdot\frac{1}{F_{\text{samp}}}$. Because acoustic signals with frequency near the resonant frequency can achieve nearly the same resonant result, and the sampling frequency of IMUs are generally much lower than the resonant frequencies of accelerometer (and gyroscopes), the attacker can find a frequency $F_a = F_{\text{res}} + f_\epsilon = N\cdot F_{\text{samp}}$ where $F_\text{res}$ is the resonant frequency of the accelerometer or the gyroscope, $f_\epsilon$ is the smallest deviation between $F_\text{res}$ and $N\cdot F_{\text{samp}}$, and $N\in\{1,2,3...\}$. Therefore, we have
    \begin{align}
        \hat{s}(t_k) &= s(t_k) + A_0 A_1 \cos(2\pi F_a t_k+\phi) \\
        &= s(t_k) + A_0 A_1 \cos(2\pi Nk+\phi) \\
        &= s(t_k) + A_0 A_1 \cos(\phi),
    \end{align}
    \item \textbf{Reshaping.} To further shape the output signal, the attacker employs either amplitude or phase modulation techniques to tune the parameters $A_1, \phi$ to get the desired output.
\end{enumerate}

\subsubsection{Output Control Attack} 
When the amplifier or the low-pass filter (LPF) is insecure, the attacker can achieve fine-grained control over a sensor’s output using amplitude modulation, which indefinitely controls an accelerometer’s output. 

In conclusion, the accelerometers and the gyroscopes' readings can be manipulated to either have a relative offset or set to a specific constant, depending on whether the ADC or the LPF is  vulnerable.
We will later characterize real XR headset (Hololens 2) in terms of their vulnerabilities to these types of attacks (\Cref{sec:frequency_sweep}).
