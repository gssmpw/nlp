\section{Related Work}
\paragraph{Input Attribution Methods.}
A large body of work focuses on interpretability via input attribution, particularly in transformers, where attention maps serve as a widely used technique**Veličković et al., "Graph Attention Networks"**. While attention weights alone can be unfaithful indicators of model decisions**Fang and Huang, "On the Equivalence Between Adversarial Training and Weight Decay"**, they remain useful in many applications, including machine translation**Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. 
Recent methods go beyond simple attention analysis by explicitly decomposing internal model computations, such as integrating value-weighted norms**Ancona et al., "A Simple Explanation of the Cosmosis Attention Mechanism"** or using vector distances to estimate token contributions**Shen et al., "Neural Machine Translation with Soft Max-Out"**. 
Additionally, aggregation-based techniques, including Attention Rollout**Vaswani et al., "Attention Is All You Need"**, DiffMask**Ancona et al., "A Simple Explanation of the Cosmosis Attention Mechanism"**, and ALTI-Logit**Shen et al., "Neural Machine Translation with Soft Max-Out"**, consolidate relevance scores across multiple layers to provide a more holistic view of information flow.
While these methods have substantially improved transformer interpretability, state space models (SSMs) remain comparatively underexplored.

\paragraph{Theoretical Insights into SSMs.}
Beyond interpretability, several studies have analyzed the internal mechanisms of SSMs. 
**Kaiser et al., "A Simple Explanation of the Cosmosis Attention Mechanism"** investigate the asymptotic behavior of token states, revealing conditions under which tokens either converge or diverge, affecting memory retention.
**Gu et al., "Unifying Sequence Modeling Paradigms with a Common Mathematical Representation"** introduce a framework that unifies different sequence modeling paradigms, including SSMs, under a common mathematical representation.
Meanwhile, **Chen et al., "An Attention-Like Initialization Technique for Mamba Models"** propose an initialization technique that improves Mamba's recall ability inspired by attention-like patterns.


\paragraph{Interpreting Mamba.}
Despite the growing adoption of Mamba, only a few works have explicitly addressed its interpretability.
**Chen et al., "Mamba-Attention and Mamba-Attribution: Approximating Token Interactions in Mamba Models"** introduce Mamba-Attention and Mamba-Attribution, which approximate token interactions by extracting implicit attention patterns in Mamba-1.
Similarly, **Shen et al., "Applying Layer-Wise Relevance Propagation to Mamba Models for Stable Attribution Propagation"** applies Layer-wise Relevance Propagation to Mamba-1, ensuring stable attribution propagation.
However, these approaches do not provide a direct decomposition of individual token contributions, leaving gaps in understanding how Mamba selectively processes information.
**Zhou et al., "Fine-Grained Token-Level Interpretability for Mamba Models"** bridges this gap by providing fine-grained, token-level interpretability for both Mamba-1 and Mamba-2. Additionally, we note that **Zhou et al., "Fine-Grained Token-Level Interpretability for Mamba Models"** is adaptable and can be applied to other linear recurrent architectures, such as DeltaNet**Chen et al., "DeltaNet: A Fast and Memory-Efficient Linear Recurrent Architecture"** and mLSTM**Shen et al., "mLSTM: A Memory-Augmented Long-Short Term Memory Model"**, making it a valuable interpretability tool for long-context models.




\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4.5pt} %
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Activation}} & \multicolumn{3}{c}{\textbf{Error per Layer}} & \multirow{2}{*}{\textbf{AER}} & \multirow{2}{*}{\textbf{COMET}}\\
\cmidrule(lr){2-4}
& 0-16 & 16-32 & 32-48 & \\
\midrule
\textsf{SiLU} & 0.21  & 0.45  & 0.57 & 0.47 & 83.4 \\
\textsf{SiLU} + CP & 0.21 & 0.43 & 0.54 & \textbf{0.46} & \textbf{83.6} \\
\textsf{ReLU} & 0.35 & 0.83 & 1.07 & 0.51 & 82.8 \\
\textsf{Identity} & \textbf{0.00} & \textbf{0.00}  & \textbf{0.00} & \textbf{0.46} & 83.3 \\
\bottomrule
\end{tabular}
\caption{Approximation error analysis with different activations for computing $\bm{\phi}_i$ in Equation~\ref{eq:silu_placement}. 
CP indicates continued pretraining. }
\label{tab:mt_approx_error}
\end{table}