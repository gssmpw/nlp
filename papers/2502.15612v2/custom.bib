% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}

@article{ferrando-etal-2024-primer,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024},
  url={http://arxiv.org/abs/2405.00208},
}

@article{qu2024survey,
  title={A survey of mamba},
  author={Qu, Haohao and Ning, Liangbo and An, Rui and Fan, Wenqi and Derr, Tyler and Liu, Hui and Xu, Xin and Li, Qing},
  journal={arXiv preprint arXiv:2408.01129},
  year={2024}
}

@article{glorioso2024zamba,
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}

@inproceedings{cettolo2017overview,
  title={Overview of the iwslt 2017 evaluation campaign},
  author={Cettolo, Mauro and Federico, Marcello and Bentivogli, Luisa and Jan, Niehues and Sebastian, St{\"u}ker and Katsuitho, Sudoh and Koichiro, Yoshino and Christian, Federmann},
  booktitle={Proceedings of the 14th International Workshop on Spoken Language Translation (IWSLT)},
  pages={2--14},
  year={2017}
}

@inproceedings{
beck2024xlstm,
title={x{LSTM}: Extended Long Short-Term Memory},
author={Maximilian Beck and Korbinian P{\"o}ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael K Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ARAxPPIAhq}
}

@inproceedings{
yang2024parallelizing,
title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y8Rm4VNRPH}
}

@inproceedings{
sieber2024understanding,
title={Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks},
author={Jerome Sieber and Carmen Amo Alonso and Alexandre Didier and Melanie Zeilinger and Antonio Orvieto},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=iF7MnXnxRw}
}

@inproceedings{
vo2025demystifying,
title={Demystifying the Token Dynamics of Deep Selective State Space Models},
author={Thieu Vo and Duy-Tung Pham and Xin T. Tong and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=qtTIP5Gjc5}
}

@article{fantozzi2024explainability,
  title={The Explainability of Transformers: Current Status and Directions},
  author={Fantozzi, Paolo and Naldi, Maurizio},
  journal={Computers},
  volume={13},
  number={4},
  pages={92},
  year={2024},
  publisher={MDPI}
}

@inproceedings{
dong2025hymba,
title={Hymba: A Hybrid-head Architecture for Small Language Models},
author={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Celine Lin and Jan Kautz and Pavlo Molchanov},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=A1ztozypga}
}

@inproceedings{
lenz2025jamba,
title={Jamba: Hybrid Transformer-Mamba Language Models},
author={Barak Lenz and Opher Lieber and Alan Arazi and Amir Bergman and Avshalom Manevich and Barak Peleg and Ben Aviram and Chen Almagor and Clara Fridman and Dan Padnos and Daniel Gissin and Daniel Jannai and Dor Muhlgay and Dor Zimberg and Edden M. Gerber and Elad Dolev and Eran Krakovsky and Erez Safahi and Erez Schwartz and Gal Cohen and Gal Shachaf and Haim Rozenblum and Hofit Bata and Ido Blass and Inbal Magar and Itay Dalmedigos and Jhonathan Osin and Julie Fadlon and Maria Rozman and Matan Danos and Michael Gokhman and Mor Zusman and Naama Gidron and Nir Ratner and Noam Gat and Noam Rozen and Oded Fried and Ohad Leshno and Omer Antverg and Omri Abend and Or Dagan and Orit Cohavi and Raz Alon and Ro'i Belson and Roi Cohen and Rom Gilad and Roman Glozman and Shahar Lev and Shai Shalev-Shwartz and Shaked Haim Meirom and Tal Delbari and Tal Ness and Tomer Asida and Tom Ben Gal and Tom Braude and Uriya Pumerantz and Josh Cohen and Yonatan Belinkov and Yuval Globerson and Yuval Peleg Levy and Yoav Shoham},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=JFPaD7lpBD}
}

@article{xu2024survey,
  title={A survey on vision mamba: Models, applications and challenges},
  author={Xu, Rui and Yang, Shu and Wang, Yihui and Du, Bo and Chen, Hao},
  journal={arXiv preprint arXiv:2404.18861v1},
  year={2024},
  url={https://arxiv.org/abs/2404.18861v1}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}



@inproceedings{alvarez2017causal,
  title={A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  author={Alvarez-Melis, David and Jaakkola, Tommi},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={412--421},
  year={2017}
}

@inproceedings{correia2019adaptively,
  title={Adaptively Sparse Transformers},
  author={Correia, Gon{\c{c}}alo M and Niculae, Vlad and Martins, Andr{\'e} FT},
  booktitle={Proc. EMNLP-IJCNLP},
  pages={2174--2184},
  year={2019}
}

@inproceedings{martins2016softmax,
  title={From softmax to sparsemax: A sparse model of attention and multi-label classification},
  author={Martins, Andre and Astudillo, Ramon},
  booktitle={Proc. ICML},
  pages={1614--1623},
  year={2016}
}

@article{peters2019sparse,
  title={Sparse Sequence-to-Sequence Models},
  author={Peters, Ben and Niculae, Vlad and Martins, Andr{\'e} FT},
  journal={Proc. ACL},
  year={2019}
}

@inproceedings{niculae2017regularized,
  title={A regularized framework for sparse and structured neural attention},
  author={Niculae, Vlad and Blondel, Mathieu},
  booktitle={Advances in neural information processing systems},
  pages={3338--3348},
  year={2017}
}

@article{niculae2018towards,
  title={Towards dynamic computation graphs via sparse latent structure},
  author={Niculae, Vlad and Martins, Andr{\'e} FT and Cardie, Claire},
  journal={arXiv preprint arXiv:1809.00653},
  year={2018}
}

@book{smola1998learning,
  title={Learning with kernels},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  volume={4},
  year={1998},
  publisher={Citeseer}
}


@inproceedings{
arora2023zoology,
title={Zoology: Measuring and Improving  Recall in Efficient Language Models},
author={Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher Re},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LY3ukUANko}
}

@misc{xu2023paradigm,
      title={A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models}, 
      author={Haoran Xu and Young Jin Kim and Amr Sharaf and Hany Hassan Awadalla},
      year={2023},
      eprint={2309.11674},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2019root,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{su2023roformer,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{akyürek2024incontext,
      title={In-Context Language Learning: Arhitectures and Algorithms}, 
      author={Ekin Akyürek and Bailin Wang and Yoon Kim and Jacob Andreas},
      year={2024},
      eprint={2401.12973},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{li2024enhancing,
      title={Enhancing Document-level Translation of Large Language Model via Translation Mixed-instructions}, 
      author={Yachao Li and Junhui Li and Jing Jiang and Min Zhang},
      year={2024},
      eprint={2401.08088},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{sun2023retentive,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024enhancing,
      title={Enhancing Document-level Translation of Large Language Model via Translation Mixed-instructions}, 
      author={Yachao Li and Junhui Li and Jing Jiang and Min Zhang},
      year={2024},
      eprint={2401.08088},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zucchet2024gated,
      title={Gated recurrent neural networks discover attention}, 
      author={Nicolas Zucchet and Seijin Kobayashi and Yassir Akram and Johannes von Oswald and Maxime Larcher and Angelika Steger and João Sacramento},
      year={2024},
      eprint={2309.01775},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{orvieto2023resurrecting,
      title={Resurrecting Recurrent Neural Networks for Long Sequences}, 
      author={Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
      year={2023},
      eprint={2303.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gu2023mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@INBOOK{5311910,
  author={Basar, Tamer},
  booktitle={Control Theory: Twenty-Five Seminal Papers (1932-1981)}, 
  title={A New Approach to Linear Filtering and Prediction Problems}, 
  year={2001},
  volume={},
  number={},
  pages={167-179},
  keywords={},
  doi={10.1109/9780470544334.ch9}}



@inproceedings{
    gu2022efficiently,
    title={Efficiently Modeling Long Sequences with Structured State Spaces},
    author={Albert Gu and Karan Goel and Christopher Re},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=uYLFoz1vlAC}
}

@misc{pang2024salute,
      title={Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models}, 
      author={Jianhui Pang and Fanghua Ye and Longyue Wang and Dian Yu and Derek F. Wong and Shuming Shi and Zhaopeng Tu},
      year={2024},
      eprint={2401.08350},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2024hedgehog,
      title={The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry}, 
      author={Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Ré},
      year={2024},
      eprint={2402.04347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}





@inproceedings{
jelassi2024repeat,
title={Repeat After Me: Transformers are Better than State Space Models at Copying},
author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and eran malach},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=duRRoGeoQT}
}



@book{book,
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
year = {2009},
month = {01},
pages = {},
title = {Natural Language Processing with Python},
isbn = {978-0-596-51649-9}
}

@misc{yang2024efficient,
      title={Do Efficient Transformers Really Save Computation?}, 
      author={Kai Yang and Jan Ackermann and Zhenyu He and Guhao Feng and Bohang Zhang and Yunzhen Feng and Qiwei Ye and Di He and Liwei Wang},
      year={2024},
      eprint={2402.13934},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{park2024mamba,
      title={Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks}, 
      author={Jongho Park and Jaeseung Park and Zheyang Xiong and Nayoung Lee and Jaewoong Cho and Samet Oymak and Kangwook Lee and Dimitris Papailiopoulos},
      year={2024},
      eprint={2402.04248},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wen2024rnns,
      title={RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval}, 
      author={Kaiyue Wen and Xingyu Dang and Kaifeng Lyu},
      year={2024},
      eprint={2402.18510},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@misc{arora2024simple,
      title={Simple linear attention language models balance the recall-throughput tradeoff}, 
      author={Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and Dylan Zinsley and James Zou and Atri Rudra and Christopher Ré},
      year={2024},
      eprint={2402.18668},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{de2024griffin,
      title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}, 
      author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
      year={2024},
      eprint={2402.19427},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
fu2023hungry,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@misc{chung2014empirical,
      title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
      author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1412.3555},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@misc{ainslie2023gqa,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{biderman2023pythia,
author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
title = {Pythia: a suite for analyzing large language models across training and scaling},
year = {2023},
publisher = {JMLR.org},
abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {102},
numpages = {34},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gu2020hippo,
 author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1474--1487},
 publisher = {Curran Associates, Inc.},
 title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{Varis_2021,
   title={Sequence Length is a Domain: Length-based Overfitting in Transformer Models},
   url={http://dx.doi.org/10.18653/v1/2021.emnlp-main.650},
   DOI={10.18653/v1/2021.emnlp-main.650},
   booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
   publisher={Association for Computational Linguistics},
   author={Varis, Dusan and Bojar, Ondřej},
   year={2021} }

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}


@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@inproceedings{katharopoulos2020transformers,
author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran\c{c}ois},
title = {Transformers are RNNs: fast autoregressive transformers with linear attention},
year = {2020},
publisher = {JMLR.org},
abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O(N2) to O(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {478},
numpages = {10},
series = {ICML'20}
}


@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{child2019generating,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
amos2024never,
title={Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors},
author={Ido Amos and Jonathan Berant and Ankit Gupta},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=PdaPky8MUn}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@software{stripedhyena,
  title        = {{StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models}},
  author       = { Poli, Michael and Wang, Jue and Massaroli, Stefano and Quesnelle, Jeffrey and Carlow, Ryan and Nguyen, Eric and Thomas, Armin},
  month        = 12,
  year         = 2023,
  url          = { https://github.com/togethercomputer/stripedhyena },
  doi          = { 10.57967/hf/1595 },
}

@misc{yang2024gated,
      title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
      author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
      year={2024},
      eprint={2312.06635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{hua2022transformer,
  title = 	 {Transformer Quality in Linear Time},
  author =       {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9099--9117},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hua22a/hua22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hua22a.html},
}


@inproceedings{smith2023simplified,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@misc{beck2024xlstmextendedlongshortterm,
      title={xLSTM: Extended Long Short-Term Memory}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2024},
      eprint={2405.04517},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.04517}, 
}



@misc{yang2024parallelizinglineartransformersdelta,
      title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length}, 
      author={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
      year={2024},
      eprint={2406.06484},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06484}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}

@misc{hwang2024hydrabidirectionalstatespace,
      title={Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers}, 
      author={Sukjun Hwang and Aakash Lahoti and Tri Dao and Albert Gu},
      year={2024},
      eprint={2407.09941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.09941}, 
}

@misc{ali2024hiddenattentionmambamodels,
      title={The Hidden Attention of Mamba Models}, 
      author={Ameen Ali and Itamar Zimerman and Lior Wolf},
      year={2024},
      eprint={2403.01590},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.01590}, 
}

@misc{pióro2024moemambaefficientselectivestate,
      title={MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts}, 
      author={Maciej Pióro and Kamil Ciebiera and Krystian Król and Jan Ludziejewski and Michał Krutul and Jakub Krajewski and Szymon Antoniak and Piotr Miłoś and Marek Cygan and Sebastian Jaszczur},
      year={2024},
      eprint={2401.04081},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04081}, 
}

@misc{dai2024deepseekmoeultimateexpertspecialization,
      title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models}, 
      author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
      year={2024},
      eprint={2401.06066},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06066}, 
}


@misc{zhang2022mixtureattentionheadsselecting,
      title={Mixture of Attention Heads: Selecting Attention Heads Per Token}, 
      author={Xiaofeng Zhang and Yikang Shen and Zeyu Huang and Jie Zhou and Wenge Rong and Zhang Xiong},
      year={2022},
      eprint={2210.05144},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.05144}, 
}

@misc{zoph2022stmoedesigningstabletransferable,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.08906}, 
}

@misc{fedus2022switchtransformersscalingtrillion,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03961}, 
}

@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models}, 
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060}, 
}

@misc{zhang2024bamjustlikethat,
      title={BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts}, 
      author={Qizhen Zhang and Nikolas Gritsch and Dwaraknath Gnaneshwar and Simon Guo and David Cairuz and Bharat Venkitesh and Jakob Foerster and Phil Blunsom and Sebastian Ruder and Ahmet Ustun and Acyr Locatelli},
      year={2024},
      eprint={2408.08274},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.08274}, 
}

@misc{abnar2020quantifyingattentionflowtransformers,
      title={Quantifying Attention Flow in Transformers}, 
      author={Samira Abnar and Willem Zuidema},
      year={2020},
      eprint={2005.00928},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.00928}, 
}

@misc{zhong2024loryfullydifferentiablemixtureofexperts,
      title={Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training}, 
      author={Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis},
      year={2024},
      eprint={2405.03133},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.03133}, 
}

@misc{ramachandran2017searchingactivationfunctions,
      title={Searching for Activation Functions}, 
      author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1710.05941},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1710.05941}, 
}


@misc{ferrando2022measuringmixingcontextualinformation,
      title={Measuring the Mixing of Contextual Information in the Transformer}, 
      author={Javier Ferrando and Gerard I. Gállego and Marta R. Costa-jussà},
      year={2022},
      eprint={2203.04212},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.04212}, 
}


@misc{guerreiro2023lookingneedlehaystackcomprehensive,
      title={Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation}, 
      author={Nuno M. Guerreiro and Elena Voita and André F. T. Martins},
      year={2023},
      eprint={2208.05309},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.05309}, 
}

@misc{guerreiro2023hallucinationslargemultilingualtranslation,
      title={Hallucinations in Large Multilingual Translation Models}, 
      author={Nuno M. Guerreiro and Duarte Alves and Jonas Waldendorf and Barry Haddow and Alexandra Birch and Pierre Colombo and André F. T. Martins},
      year={2023},
      eprint={2303.16104},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.16104}, 
}

@misc{ferrando2024informationflowroutesautomatically,
      title={Information Flow Routes: Automatically Interpreting Language Models at Scale}, 
      author={Javier Ferrando and Elena Voita},
      year={2024},
      eprint={2403.00824},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.00824}, 
}

@misc{trockman2024mimeticinitializationhelpsstate,
      title={Mimetic Initialization Helps State Space Models Learn to Recall}, 
      author={Asher Trockman and Hrayr Harutyunyan and J. Zico Kolter and Sanjiv Kumar and Srinadh Bhojanapalli},
      year={2024},
      eprint={2410.11135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.11135}, 
}

@inproceedings{
jafari2024mambalrp,
title={Mamba{LRP}: Explaining Selective State Space Sequence Models},
author={Farnoush Rezaei Jafari and Gr{\'e}goire Montavon and Klaus Robert Muller and Oliver Eberle},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=2n1Ysn1EDl}
}



@misc{ferrando2023explainingtransformersusecontext,
      title={Explaining How Transformers Use Context to Build Predictions}, 
      author={Javier Ferrando and Gerard I. Gállego and Ioannis Tsiamas and Marta R. Costa-jussà},
      year={2023},
      eprint={2305.12535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.12535}, 
}

@inproceedings{
    mohtashami2023landmark,
    title={Landmark Attention: Random-Access Infinite Context Length for Transformers},
    author={Amirkeivan Mohtashami and Martin Jaggi},
    booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
    year={2023},
    url={https://openreview.net/forum?id=PkoGERXS1B}
}

@misc{dauphin2017languagemodelinggatedconvolutional,
      title={Language Modeling with Gated Convolutional Networks}, 
      author={Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
      year={2017},
      eprint={1612.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1612.08083}, 
}

@misc{elfwing2017sigmoidweightedlinearunitsneural,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.03118}, 
}



@inproceedings{rmsnorm,
 author = {Zhang, Biao and Sennrich, Rico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Root Mean Square Layer Normalization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{Wu_2018_ECCV,
author = {Wu, Yuxin and He, Kaiming},
title = {Group Normalization},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}



@inproceedings{
hsieh2024rulerwhatsrealcontext,
title={{RULER}: What{\textquoteright}s the Real Context Size of Your Long-Context Language Models?},
author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=kIoBbc76Sy}
}


@inproceedings{
bick2024transformers,
title={Transformers to {SSM}s: Distilling Quadratic Knowledge to Subquadratic Models},
author={Aviv Bick and Kevin Li and Eric P. Xing and J Zico Kolter and Albert Gu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=FJlrSZBMCD}
}

@article{10.1371/journal.pone.0130140,
    doi = {10.1371/journal.pone.0130140},
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},

}


@inproceedings{
penedo2024the,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}


@misc{hu2024minicpmunveilingpotentialsmall,
      title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
      author={Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zheng Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2404.06395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06395}, 
}


@inproceedings{
loshchilov2019decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}