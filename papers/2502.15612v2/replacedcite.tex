\section{Related Work}
\paragraph{Input Attribution Methods.}
A large body of work focuses on interpretability via input attribution, particularly in transformers, where attention maps serve as a widely used technique____. While attention weights alone can be unfaithful indicators of model decisions____, they remain useful in many applications, including machine translation____. 
Recent methods go beyond simple attention analysis by explicitly decomposing internal model computations, such as integrating value-weighted norms____ or using vector distances to estimate token contributions____. 
Additionally, aggregation-based techniques, including Attention Rollout____, DiffMask____, and ALTI-Logit____, consolidate relevance scores across multiple layers to provide a more holistic view of information flow.
While these methods have substantially improved transformer interpretability, state space models (SSMs) remain comparatively underexplored.

\paragraph{Theoretical Insights into SSMs.}
Beyond interpretability, several studies have analyzed the internal mechanisms of SSMs. 
____ investigate the asymptotic behavior of token states, revealing conditions under which tokens either converge or diverge, affecting memory retention.
____ introduce a framework that unifies different sequence modeling paradigms, including SSMs, under a common mathematical representation.
Meanwhile, ____ propose an initialization technique that improves Mamba's recall ability inspired by attention-like patterns.


\paragraph{Interpreting Mamba.}
Despite the growing adoption of Mamba, only a few works have explicitly addressed its interpretability.
____ introduce Mamba-Attention and Mamba-Attribution, which approximate token interactions by extracting implicit attention patterns in Mamba-1.
Similarly, MambaLRP____ applies Layer-wise Relevance Propagation to Mamba-1, ensuring stable attribution propagation.
However, these approaches do not provide a direct decomposition of individual token contributions, leaving gaps in understanding how Mamba selectively processes information.
\methodname bridges this gap by providing fine-grained, token-level interpretability for both Mamba-1 and Mamba-2. Additionally, we note that \methodname is adaptable and can be applied to other linear recurrent architectures, such as DeltaNet____ and mLSTM____, making it a valuable interpretability tool for long-context models.





\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4.5pt} %
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Activation}} & \multicolumn{3}{c}{\textbf{Error per Layer}} & \multirow{2}{*}{\textbf{AER}} & \multirow{2}{*}{\textbf{COMET}}\\
\cmidrule(lr){2-4}
& 0-16 & 16-32 & 32-48 & \\
\midrule
\textsf{SiLU} & 0.21  & 0.45  & 0.57 & 0.47 & 83.4 \\
\textsf{SiLU} + CP & 0.21 & 0.43 & 0.54 & \textbf{0.46} & \textbf{83.6} \\
\textsf{ReLU} & 0.35 & 0.83 & 1.07 & 0.51 & 82.8 \\
\textsf{Identity} & \textbf{0.00} & \textbf{0.00}  & \textbf{0.00} & \textbf{0.46} & 83.3 \\
\bottomrule
\end{tabular}
\caption{Approximation error analysis with different activations for computing $\bm{\phi}_i$ in Equation~\ref{eq:silu_placement}. 
CP indicates continued pretraining. }
\label{tab:mt_approx_error}
\end{table}