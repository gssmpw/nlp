[
  {
    "index": 0,
    "papers": [
      {
        "key": "fantozzi2024explainability",
        "author": "Fantozzi, Paolo and Naldi, Maurizio",
        "title": "The Explainability of Transformers: Current Status and Directions"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "jain-wallace-2019-attention",
        "author": "Jain, Sarthak and Wallace, Byron C.",
        "title": "{A}ttention is not {E}xplanation"
      },
      {
        "key": "bastings-filippova-2020-elephant",
        "author": "Bastings, Jasmijn and Filippova, Katja",
        "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wiegreffe-pinter-2019-attention",
        "author": "Wiegreffe, Sarah and Pinter, Yuval",
        "title": "Attention is not not Explanation"
      },
      {
        "key": "treviso-martins-2020-explanation",
        "author": "Treviso, Marcos and Martins, Andr{\\'e} F. T.",
        "title": "The Explanation Game: Towards Prediction Explainability through Sparse Communication"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kobayashi-etal-2020-attention",
        "author": "Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro",
        "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ferrando-etal-2022-measuring",
        "author": "Ferrando, Javier and G{\\'a}llego, Gerard I. and Costa-juss{\\`a}, Marta R.",
        "title": "Measuring the Mixing of Contextual Information in the Transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "abnar-zuidema-2020-quantifying",
        "author": "Abnar, Samira and Zuidema, Willem",
        "title": "Quantifying Attention Flow in Transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "de-cao-etal-2020-decisions",
        "author": "De Cao, Nicola and Schlichtkrull, Michael Sejr and Aziz, Wilker and Titov, Ivan",
        "title": "How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ferrando-etal-2023-explaining",
        "author": "Ferrando, Javier and G{\\'a}llego, Gerard I. and Tsiamas, Ioannis and Costa-juss{\\`a}, Marta R.",
        "title": "Explaining How Transformers Use Context to Build Predictions"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "vo2025demystifying",
        "author": "Thieu Vo and Duy-Tung Pham and Xin T. Tong and Tan Minh Nguyen",
        "title": "Demystifying the Token Dynamics of Deep Selective State Space Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "sieber2024understanding",
        "author": "Jerome Sieber and Carmen Amo Alonso and Alexandre Didier and Melanie Zeilinger and Antonio Orvieto",
        "title": "Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "trockman2024mimeticinitializationhelpsstate",
        "author": "Asher Trockman and Hrayr Harutyunyan and J. Zico Kolter and Sanjiv Kumar and Srinadh Bhojanapalli",
        "title": "Mimetic Initialization Helps State Space Models Learn to Recall"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ali2024hiddenattentionmambamodels",
        "author": "Ameen Ali and Itamar Zimerman and Lior Wolf",
        "title": "The Hidden Attention of Mamba Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jafari2024mambalrp",
        "author": "Farnoush Rezaei Jafari and Gr{\\'e}goire Montavon and Klaus Robert Muller and Oliver Eberle",
        "title": "Mamba{LRP}: Explaining Selective State Space Sequence Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yang2024parallelizing",
        "author": "Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim",
        "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "beck2024xlstm",
        "author": "Maximilian Beck and Korbinian P{\\\"o}ppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael K Kopp and G{\\\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter",
        "title": "x{LSTM}: Extended Long Short-Term Memory"
      }
    ]
  }
]