%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage[font=small,skip=2pt]{caption}  % Compact captions
% \usepackage{subcaption}
\raggedbottom % This reduces blank space by allowing text to be placed closer to figures
\usepackage{enumitem} % to reduce space within itemize
\setlist[itemize]{topsep=2pt, partopsep=0pt, itemsep=2pt, parsep=0pt}

% \setlength{\abovecaptionskip}{3pt}  % Reduce space above caption
% \setlength{\belowcaptionskip}{-5pt} % Reduce space below caption
% \setlength{\textfloatsep}{5pt}  % Adjust space between figure and text


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \usepackage{wrapfig}
\usepackage{ulem}
\newcommand{\mingfei}[1]{\textcolor{red}{\{Mingfei: #1\}}}

\newcommand{\beining}[1]{\textcolor{blue}{\{Beining: #1\}}}

\usepackage{soul}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning}

\begin{document}

\twocolumn[
\icmltitle{Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Beining Zhang}{equal,yyy}
\icmlauthor{Aditya Kapoor}{equal,yyy}
\icmlauthor{Mingfei Sun}{yyy}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, University of Manchester, Manchester, United Kingdom}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Beining Zhang}{felix.zbn@gmail.com}
\icmlcorrespondingauthor{Aditya Kapoor}{aditya.kapoor@postgrad.manchester.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Multi-agent reinforcement learning (MARL) often relies on \emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent’s unique specialization, reducing overall performance in heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent’s policy as a specialized “task” fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method’s flexibility and efficiency. Our results suggest LoRASA’s potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

A canonical paradigm in MARL is \textbf{Centralized Training and Decentralized Execution (CTDE)}~\citep{CTDE_1, CTDE_2, CTDE_3, CTDE_4, CTDE_5}, where agents learn with access to global information but execute policies independently. Within CTDE, a standard approach is \textbf{parameter sharing (PS)}~\citep{Gupta2017CooperativeMC, Chu2017PSDPG, terry2020revisiting}, which significantly cuts down on resource requirements by training a single policy network for all agents.

Despite its efficiency, PS can compromise the specialized behaviors needed in heterogeneous or role-based scenarios~\citep{christianos2021SePS, AdaPS}. Simple fixes, such as tagging states with agent identifiers~\citep{terry2020revisiting, CTDE_4, Gupta2017CooperativeMC}, rarely capture deeper skill differences ~\citep{christianos2021SePS, AdaPS}. Merely appending an ID to observations seldom suffices to uncover such diverging policies—an agent must not only “know” it has a particular identity but also adapt its policy to exploit that identity effectively. Furthermore, even in homogeneous scenarios, such as StarCraft with agents of the same unit type, we empirically found that agents also require non-identical behaviors, see Sec ~\ref{subsec:heterogeneous_nature}.

To enable better heterogeneous behaviors in multi-agent systems, researchers have explored approaches like selective parameter sharing (SePS)~\citep{christianos2021SePS} and adaptive parameter sharing (AdaPS)~\citep{AdaPS, PSNetworkPruning}. SePS clusters agents based on behavioral similarities, assigning a shared policy network to each cluster, while AdaPS dynamically selects specialized subnetworks from a shared architecture. However, SePS often struggles in dynamic environments where agent roles evolve or unique edge cases arise, as its static clustering framework cannot adapt to changes. Similarly, AdaPS can over-prune critical parts of the shared network, limiting agents’ ability to leverage common knowledge in unforeseen situations. This lack of adaptability can significantly impact performance in complex environments where flexibility is paramount. For example, in disaster response scenarios, drones performing routine tasks like surveying may be effectively managed by SePS or AdaPS, but these methods often fail to address rare, specialized tasks such as hazardous material containment or rescue operations, where more nuanced specialization is required.

\begin{figure}[!ht]
% \vskip 0.2in
\begin{center}
    \includegraphics[width=\columnwidth]{fig/LoRASA-detailed-overview.png}
    \caption{Overview of LoRASA framework.}
    \label{fig:LoRASA_framework}
\end{center}
\vspace{-15pt}  % Reduce space between figure and next paragraph
\end{figure}


Fully distinct policies, such as those used in Non-Parameter Sharing (NPS), assign unique parameters to each agent~\citep{HAPPO, A2PO}. While this allows for full specialization, it forces each neural network to learn similar representations independently, often with limited data per agent, leading to sample inefficiency. Moreover, the approach is computationally and memory-intensive, making it impractical for large number of agents (see time and memory requirement of NPS in Figure~\ref{fig:resource_requirements}).


In light of these challenges, there is a strong incentive to develop methods that preserve the efficiency and coordination benefits of parameter sharing while allowing agents to specialize effectively. In this paper, we introduce a novel perspective on MARL by framing it as a multi-task learning (MTL) problem, where each agent's policy is treated as a distinct task requiring specialized behavior. Unlike previous multi-task MARL approaches that aim to generalize across similar tasks to facilitate adaptation~\citep{wang2023multitaskmultiagentsharedlayers, omidshafiei2017deepdecentralizedmultitaskmultiagent, MultiTaskMARL, zhang2024hybridtrainingenhancedmultitask}, our approach focuses on fostering diverse agent behaviors within a unified multi-agent objective. For instance, in a disaster-stricken city, rescue robots share the common skill of navigating debris-filled environments but specialize in tasks like clearing rubble, delivering medical supplies, or locating survivors. These distinct roles require specialized policies that cannot be effectively captured by a single shared network or simple agent identifier augmentations.

To address this need for efficient specialization, we propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, refer Figure~\ref{fig:LoRASA_framework}, inspired by \textbf{LoRA}~\citep{hu2021lora}, a parameter-efficient fine-tuning method originally developed for large-scale natural language models. LoRA introduces lightweight, low-rank adaptation matrices that are added to pretrained weights, enabling task-specific refinements while preserving the core shared knowledge. Extending this concept to MARL, LoRASA fine-tunes the shared policy with minimal overhead by constraining adaptations to a low-rank subspace. This induces \emph{parameter-space sparsity} (Sec~\ref{paragraph:sparsity_analysis}), allowing each agent to specialize without the computational and memory burdens of assigning unique, full-rank parameters (see plots in Figure~\ref{fig:resource_requirements} for further evidence). 

Our work makes the following main contributions:
\begin{itemize}
    \item \textbf{LoRASA} (in Sec~\ref{sec:methodology}): We introduce a novel low-rank adaptation mechanism for MARL, positioning the problem as a \emph{multi-task fine-tuning} scenario to achieve both scalability and agent-specific specialization.

    \item \textbf{Comprehensive Empirical Evaluation} (refer Sec~\ref{subsec:results}): On two challenging benchmarks—StarCraft Multi-Agent Challenge (SMAC)~\citep{SMAC} and Multi-Agent MuJoCo (MAMuJoCo)~\citep{HAPPO}—LoRASA consistently matches or outperforms other parameter-sharing variants based on strong baselines (MAPPO, A2PO) while utilizing fewer resources. Our extensive ablation studies on adapter rank, fine-tuning timing, and layer-wise placement provide actionable guidelines, reinforcing LoRASA’s practicality for real-world deployment.

    \item \textbf{Parameter-Space Sparsity and Heterogeneous Behaviors} (see Sec~\ref{subsec:heterogeneous_nature}): LoRASA leverages parameter-space sparsity through low-rank updates, enabling agents to exhibit diverse, specialized behaviors while retaining the efficiency and coordination benefits of a shared policy.
\end{itemize}


Together, these contributions highlight a paradigm shift in MARL from rigid, fully shared or fully distinct policies to a flexible, low-rank adaptation framework. LoRASA enables agent-specific specialization while preserving the coordination benefits of a shared policy, offering a scalable and efficient solution for diverse and large-scale MARL applications that demand both flexibility and resource efficiency.



\section{Methodology}
\label{sec:methodology}

\subsection{Preliminaries}
\label{subsec:preliminaries}

\paragraph{Multi-Agent Reinforcement Learning (MARL).}
We consider cooperative MARL problems modeled as \textbf{Partially Observable Markov Games (POMGs)}~\citep{Kuhn, Shapely, Boutilier}, where each agent $i$ observes $o_{i,t} \in \mathcal{O}_i$, selects an action $a_{i,t} \in \mathcal{A}_i$ according to its policy $\pi_i(a_{i,t}\mid o_{i,t};\theta_i)$, and receives a \emph{shared} reward $r_t$ at time $t$. The objective is to maximize $E\big[\sum_{t=0}^\infty \gamma^t r_t\big]$, focusing on cooperative tasks.
% without delving into complex transition-factorization assumptions.


\paragraph{Centralized Training \& Decentralized Execution (CTDE).}
In this work, we adopt \textbf{CTDE}~\citep{CTDE_1, CTDE_2, CTDE_4}, where $\mathcal{N}$ agents train with access to global information (joint observations, rewards) yet execute independently based on local observations. This setup is a natural fit for real-world multi-agent scenarios demanding high scalability and local autonomy. Under CTDE, the joint policy $\Pi$ factorizes as
\[
\Pi(a\mid o) \;=\;\prod_{i\in \mathcal{N}} \pi_i(a_i \mid o_i;\theta_i).
\]
In PS approaches, $\theta_i=\theta_\text{shared}$ for all $i\in \mathcal{N}$, while in NPS, each agent has distinct parameters. Our method, LoRASA, stands at an \emph{intermediate} point, blending the resource-efficiency of PS with the flexibility of NPS.

\paragraph{Low-Rank Adaptation (LoRA)~\citep{hu2021lora}.}
LoRA was introduced for parameter-efficient fine-tuning in large-scale language models. It adds a low-rank update $\delta W = AB^\top$ to each weight matrix $W$, where $A\in\mathbb{R}^{d\times r}$ and $B\in\mathbb{R}^{k\times r}$ with $r \ll \min(d,k)$. Critically, only $A$ and $B$ are trained, while $W$ remains fixed. In our setting, we treat the shared policy as \emph{pretrained} and each agent’s specialized adaptation as a separate \emph{task}. Thus, LoRA naturally encodes agent-specific deviations from a common baseline \emph{without} replicating entire networks.

\subsection{LoRASA: Low-Rank Adaptation for MARL}
\label{subsec:framework_overview}

% \paragraph{Theoretical and Conceptual Insights.}
% Recent studies in deep reinforcement learning indicate that the effective dimensionality of learned policies can be much lower than the total parameter count~\citep{remman2024discoveringbehavioralmodesdeep, Sun2019RealtimePD, ensemble_policy_distillation, schneider2024identifyingpolicygradientsubspaces}. In a cooperative MARL setting, agents often differ in only a few crucial behavioral directions (e.g., scouting versus attacking), suggesting that these role-specific variations lie in a smaller subspace of the full parameter space ~\citep{wadhwania2019policydistillationvaluematching}. By restricting agent-specific updates to an $r$-rank matrix, LoRA formally encodes each agent's deviations from a shared backbone within this smaller subspace. This design not only preserves the bulk of the pre-trained policy's knowledge but also efficiently captures heterogeneous behaviors across agents. Theoretically, if agent-specific policy variations remain near an $r$-dimensional affine subspace, a rank-$r$ decomposition of each agent’s adjustments will approximate their optimal policies with minimal error~\citep{Eckart1936TheAO, Hiriart-Urruty_Le_2013}\mingfei{is it possible to give some theorems/propositions?}. Empirically, this insight manifests as improved scalability—merging the benefits of PS (low resource cost) and NPS (fine-grained specialization) into a single, low-rank framework.

% \paragraph{Theoretical and Conceptual Insights.}
% Recent studies in deep reinforcement learning suggest that the effective dimensionality of learned policies can be much lower than the total parameter count~\citep{remman2024discoveringbehavioralmodesdeep, Sun2019RealtimePD, ensemble_policy_distillation, schneider2024identifyingpolicygradientsubspaces}. In cooperative MARL, agents often differ along only a few key behavioral directions (e.g., scouting vs.\ attacking), indicating that these variations lie in a smaller subspace of the full parameter space~\citep{wadhwania2019policydistillationvaluematching}. By restricting agent-specific updates to an $r$-rank matrix, LoRA formally encodes each agent’s deviations from a shared backbone within this lower-dimensional subspace. This design not only retains the bulk of the pretrained policy’s knowledge but also efficiently captures heterogeneous behaviors.

% From a matrix approximation standpoint, if agent-specific variations reside in or near an $r$-dimensional affine subspace, a rank-$r$ decomposition can closely approximate optimal updates~\citep{Eckart1936TheAO, Hiriart-Urruty_Le_2013}. Although we do not introduce new theorems specifically for multi-agent learning, these classical results justify that \emph{given} an $r$-dimensional structure of agent adjustments, the truncated (rank-$r$) representation is near-optimal in a least-squares sense. In practice, this translates to improved scalability, merging the benefits of parameter sharing (low overhead) and non-parameter sharing (fine-grained specialization) within a single low-rank framework.

\paragraph{Theoretical and Conceptual Insights.}
Recent studies in deep reinforcement learning suggest that the effective dimensionality of learned policies can be much lower than the total parameter count~\citep{remman2024discoveringbehavioralmodesdeep, Sun2019RealtimePD, ensemble_policy_distillation, schneider2024identifyingpolicygradientsubspaces}. In cooperative MARL, agents often assume distinct roles (e.g., scouting vs.\ attacking), indicating these policy variations lie in a smaller subspace of the full parameter space~\citep{wadhwania2019policydistillationvaluematching}. By restricting agent-specific updates to an \( r \)-rank matrix, LoRA formally encodes each agent’s deviations from a shared backbone within this lower-dimensional subspace. This design not only retains the bulk of the pretrained policy’s knowledge but also efficiently captures heterogeneous behaviors without duplicating entire networks.

\begin{proposition}
Assume that in a cooperative multi-agent reinforcement learning (MARL) setting, the agent-specific parameter deviations lie within or near an \( r \)-dimensional affine subspace of the full parameter space. Then, applying a rank-\( r \) low-rank adaptation (LoRA) to the shared backbone's weights can approximate the optimal agent-specific policies with a bounded error in the least-squares sense.
\end{proposition}

This proposition is supported by the Eckart-Young-Mirsky theorem~\citep{Eckart1936TheAO, Hiriart-Urruty_Le_2013}, which states that the best rank-\( r \) approximation of a matrix minimizes the Frobenius norm of the approximation error. Confining agent-specific offsets to a rank-$r$ subspace thus balances scalability and expressiveness: each agent can specialize sufficiently to capture its role-specific deviations while still sharing the bulk of learned features. In practice, this translates to improved scalability, merging the resource efficiency of parameter sharing with the fine-grained specialization of non-parameter sharing in a single low-rank framework.

\paragraph{Weight Parameterization in the Actor Network.}
Consider a recurrent actor network with fully connected (FC) layers and a recurrent unit (GRU~\citep{GRU} or LSTM~\citep{LSTM}). Let $\theta^\ell \in \mathbb{R}^{d_\ell \times k_\ell}$ be the weight matrix at layer $\ell$. We add a low-rank adaptation \(\delta \theta^\ell = A^\ell B^{\ell\top}\), where \(A^\ell\in\mathbb{R}^{d_\ell\times r}\) and \(B^\ell\in\mathbb{R}^{k_\ell\times r}\). These matrices are trained specifically for each agent, while the shared backbone \(\theta^\ell\) remains frozen. We emphasize linear transformations in the recurrent pathway (input-to-hidden and hidden-to-hidden), leaving biases and layer-norm parameters fixed for simplicity. Nonetheless, even applying LoRA solely to linear transformations gives agents ample capacity to adapt their recurrent dynamics.

\paragraph{Action Spaces and Final Layer Adaptations.}
For continuous and constrained action spaces, the actor network outputs the mean and log-std of a squashed Gaussian distribution~\citep{SAC}. We apply LoRA to the weight matrices of the final fully connected (FC) layers responsible for generating both the mean and the log-std. This allows each agent to tailor its exploration strategy through agent-specific low-rank adaptations without duplicating entire networks. For discrete action spaces, we apply LoRA to the final FC layer that produces action logits, enabling agent-specific adjustments to discrete action probabilities.

By focusing LoRA on these output layers, agents can refine their decision-making to match specialized roles (e.g., scouting vs.\ attacking) while maintaining the efficiency and coordination benefits of a shared policy backbone.

\subsection{Training Procedure}
\label{subsec:training_procedure}

LoRASA consists of two main steps: \textbf{Shared Policy Pretraining} for learning shared knowledge and \textbf{LoRA Fine-Tuning} for agent-specific specialization.

\paragraph{Phase 1: Shared Policy Pretraining.}
We first train a single shared policy \(\theta_\text{shared}\) using a standard multi-agent reinforcement learning (MARL) algorithm (e.g., MAPPO, A2PO). During this phase, the system behaves like a PS method, where all agents rely on the same policy. To evaluate the robustness of the shared policy, we track key performance metrics such as \emph{cumulative returns} and \emph{win rates}. These metrics quantify the policy's ability to exhibit effective behaviors across tasks. Once \(\theta_\text{shared}\) shows consistent improvement and meets predefined performance thresholds, we consider it sufficiently trained for downstream adaptation. At this point, we transition to fine-tuning, allowing agents to specialize their policies while retaining shared knowledge.

\paragraph{Phase 2: LoRA Fine-Tuning.}
In this phase, we introduce LoRA adapters \(\{A_i^\ell, B_i^\ell\}\) for each agent \(i\), while keeping the shared policy \(\theta_\text{shared}\) frozen:
\begin{equation}
    \forall \ell: \quad \theta_i^\ell = \theta_\text{shared}^\ell + A_i^\ell B_i^\ell, \quad A_i^\ell \in \mathbb{R}^{d \times r}, \quad B_i^\ell \in \mathbb{R}^{r \times k}. \nonumber
\end{equation}
The \textbf{low-rank dimension} \( r \) controls the level of specialization:
\begin{itemize}
    \item \textbf{Larger \( r \)} enables more expressive adaptations but increases resource costs.
    \item \textbf{Smaller \( r \)} keeps agents closer to the shared policy, ensuring efficiency.
\end{itemize}

Tuning \( r \) balances \textbf{expressivity vs.\ efficiency}—higher \( r \) approaches NPS-like fine-tuning, while lower \( r \) retains parameter-sharing benefits. During fine-tuning, agents update only \(\delta\theta_i^\ell = A_i^\ell B_i^\ell\) using their own trajectories, allowing specialization atop a shared backbone. This framework \textbf{merges PS’s scalability with NPS’s adaptability}, achieving specialization without excessive overhead.

\subsection{Algorithms for LoRASA}
\label{subsec:algos}

\paragraph{Algorithmic Details.}
Algorithms~\ref{alg:pretraining}, \ref{alg:finetuning}, and \ref{alg:inference} outline our approach. In \textbf{Algorithm~\ref{alg:pretraining}}, we train the shared policy exactly as in standard CTDE-based MARL. \textbf{Algorithm~\ref{alg:finetuning}} then enables agent-specific specialization by updating LoRA adapters for each agent’s actor network. Finally, \textbf{Algorithm~\ref{alg:inference}} merges the LoRA updates into the backbone weights at execution time for efficient inference.

\paragraph{Choice of Baseline Algorithms (MAPPO and A2PO).}
We implement LoRASA on top of two distinct CTDE actor-critic methods: MAPPO~\citep{MAPPO} extends PPO to multi-agent settings with a centralized critic and typically uses shared policy parameters, while A2PO~\citep{A2PO} sequentially updates each agent’s policy, mimicking NPS. By applying LoRASA to both methods, we illustrate how a low-rank adaptation framework bridges these two extremes—offering parameter efficiency and fine-grained specialization within the same architecture.

\paragraph{Rank as a Bridge between PS and NPS.}
Varying $r$ seamlessly interpolates between pure parameter sharing ($r=0$) and fully distinct policies. As $r$ grows, each agent’s policy deviates more from the shared backbone, capturing complex role-specific behaviors without wholly duplicating the network. We show empirically that moderate $r$ values are sufficient for notable performance gains while retaining a low overhead in memory and computation.

\subsection{Computational and Memory Efficiency}
\label{subsec:computational_memory_efficiency}

During pretraining, LoRASA behaves like conventional PS, incurring no extra cost. Once fine-tuning begins, each agent introduces $\sum_{\ell} r \bigl(d_\ell + k_\ell\bigr)$ additional parameters, far fewer than duplicating entire networks (as in NPS). At inference, these adapters can be merged with the shared backbone (Algorithm~\ref{alg:inference}), meaning the final memory footprint remains close to a single policy, scaled only by small, rank-dependent matrices. Figures~\ref{fig:baseline_exps} and \ref{fig:resource_requirements} show that LoRASA achieves higher performance than naive PS at a fraction of NPS’s overhead, confirming its scalability.

Furthermore, by harnessing low-rank subspace adaptation, LoRASA offers an attractive middle ground—yielding heterogeneous agent behaviors with minimal resource demands. Overall, LoRASA expands the design space for cooperative multi-agent RL, moving beyond rigidly shared or fully distinct parameters toward a more adaptive paradigm.


\section{Experimental Setup}
\label{sec:experimental_setup}

We evaluate LoRA-based multi-agent reinforcement learning (MARL) across diverse continuous and discrete environments, detailing our tasks, baselines, metrics, and key findings. Our experiments demonstrate LoRASA’s ability to bridge the gap between purely shared (PS) and fully distinct (NPS) policies, reducing resource overhead while retaining agent-specific specialization. \footnote{Code is available at: \href{https://anonymous.4open.science/r/LoRASA-0D6F}{anonymous.4open.science}.}

\subsection{Environments and Tasks}
\label{subsec:environments}

\paragraph{MAMuJoCo (Continuous Control).}
We first consider MAMuJoCo, where each agent controls specific joints of a multi-limbed robot. Actions are continuous torques, and observations include local joint information (positions, velocities, etc.) plus agent IDs. Episodes run for up to 1000 steps or until the robot becomes inactive (e.g., falls). We benchmark on Half Cheetah 2x3, Walker 3x2 and Ant 4x2 under partial observability. For Humanoid $9|8$, we follow prior work~\citep{HAPPO} in providing global states to avoid degenerate solutions under severe partial observability. These tasks vary significantly in coordination needs, aiming to test LoRASA’s generality in continuous multi-agent control.

\paragraph{SMAC (Discrete Combat).}
For discrete actions, we utilize the StarCraft Multi-Agent Challenge (SMAC)~\citep{SMAC}. Unlike SMAC-v2~\citep{SMACv2}, which randomly samples agents across episodes, SMAC maintains consistent agent assignments. This consistency is crucial for training agent-specific parameters, as random sampling in SMAC-v2 could lead to some agents being trained more frequently than others, introducing unwanted complexity. In SMAC, each agent controls a StarCraft II unit with observations that include local surroundings, partial enemy and ally information, and agent IDs. We evaluate our approach on maps such as 3s5z, 1c3s5z, 3s5z\_vs\_3s6z, and MMM2. These scenarios require specialized roles—for instance, medics versus frontline marines—providing a robust test of LoRASA’s ability to learn heterogeneous behaviors without necessitating fully separate policies.


\subsection{Baselines and Comparisons}
\label{subsec:baselines}

We compare LoRASA to four baselines spanning full sharing, partial sharing, and fully separate parameters:

\begin{itemize}
    \item \textbf{PS + ID.} A single shared policy for all agents, with agent IDs appended to observations~\citep{Gupta2017CooperativeMC, CTDE_3, CTDE_4, terry2020revisiting}. Highly memory-efficient but often fails to capture diverse agent roles.
    \item \textbf{NPS.} Each agent trains an entirely separate network~\citep{HAPPO, A2PO}, allowing maximal specialization at the cost of significant resource overhead.
    \item \textbf{SePS.} Clusters agents by similarity and assigns a shared policy per cluster~\citep{christianos2021SePS}. Reduces the overhead of NPS but may perform suboptimally when agent diversity is high.
    \item \textbf{MTL.} Multi-task learning with partial sharing, typically restricting specialization to the final layer while sharing other layers between agents across tasks~\citep{MTL, MTLSurvey, MTLSurveyDL}.

\end{itemize}

LoRASA comprises two methods, \textbf{PS+LoRA} and \textbf{SePS+LoRA}, which build on top of PS and SePS, respectively. By contrast, \textbf{LoRASA} uses low-rank adaptation matrices on a shared backbone, aiming for near-NPS specialization with far lower parameter and computational demands.


\subsection{Evaluation Metrics and Protocol}
\label{subsec:evaluation_metrics}

\paragraph{Metrics.}
We measure cumulative episode return for MAMuJoCo and episodic win rate for SMAC (and episodic return see Appendix Figure~\ref{fig:smac_rewards}). We also report the total parameter count and wall-clock training and inference time, reflecting LoRASA’s resource efficiency.

\paragraph{Training Protocol.}
All methods train for up to 12 million steps, repeated over 5 random seeds for reliability. LoRA fine-tuning begins after a shared-policy pretraining phase, typically when the shared policy begins to demonstrate improved learning (see ablation studies in Figure~\ref{fig:lora_ablation}). We choose this checkpoint to balance the need for core coordination strategies (captured by the shared policy) against leaving sufficient room for agent-specific refinements. Appendix~\ref{subsec:hyperparameters} provides additional implementation details.

\subsection{Overall Performance and Resource Usage}
\label{subsec:results}

\paragraph{Performance Across Benchmarks.}
Figure~\ref{fig:baseline_exps} compares LoRASA-based approaches (\emph{PS+LoRA}, \emph{SePS+LoRA}) with the baselines on MAMuJoCo and SMAC. LoRASA frequently outperforms naive PS and, in many tasks, matches or surpasses NPS—yet at a fraction of NPS’s parameter overhead. Under A2PO, \emph{SePS+LoRA} and \emph{PS+LoRA} achieves top scores on Walker 3x2 and Ant 4x2, reflecting its ability to adapt efficiently within a shared architecture. MAPPO shows similar trends, with \emph{PS+LoRA} leading in tasks like Walker and Ant. Even in tasks where MTL and NPS performs strongly (e.g., Half Cheetah in MAPPO and A2PO respectively), LoRASA remains competitive but requires significantly less computation than NPS. Notably, all methods trained using MAPPO—including LoRASA—failed to make meaningful progress on the exceptionally challenging Humanoid $9|8$ task. This consistent struggle highlights the extreme complexity of this scenario under the MAPPO framework.

SMAC tasks show a similar pattern: LoRASA-based methods often tie or exceed the strongest baselines in scenarios like 3s5z and MMM2. Although certain maps (e.g., 3s5z\_vs\_3s6z) still favor naive PS, LoRA-based approaches remain highly effective, underscoring LoRASA’s adaptability across different MARL challenges. 

\begin{figure*}[htbp]
\centering
% Subfigure A
\begin{subfigure}
    \centering
    \includegraphics[width=\linewidth]{fig/A2PO-Performance.png}
    \label{fig:a2po_mujoco}
\end{subfigure}

\vspace{0.5em} % Spacing between subfigures

% Subfigure B
\begin{subfigure}
    \centering
    \includegraphics[width=\linewidth]{fig/MAPPO-Performance.png}
    \label{fig:mappo_mujoco}
\end{subfigure}

\caption{Performance comparison of different parameter sharing approaches (PS, NPS, SePS, MTL, PS+LoRA and SePS+LoRA) using A2PO (row1, A--H) and MAPPO (row2, I--P) across four MAMuJoCo and SMAC scenarios: Half Cheetah 2x3, Walker 3x2, Ant 4x2, Humanoid $9|8$, 3s5z, 1c3s5z, 3s5z\_vs\_3s6z, and MMM2. The graphs plot median episode returns and evaluation win rates versus environment steps for each approach for MAMujoco and SMAC respectively. Half Cheetah 2x3 and Humanoid $9|8$ has two agents so we do not have SePS and SePS+LoRA. MAPPO learning style struggles with Humanoid $9|8$ irrespective of the parameter sharing framework.}
\label{fig:baseline_exps}
\vspace{-13pt}  % Reduce space between figure and next paragraph
\end{figure*}


\paragraph{Resource Efficiency.}
Figure~\ref{fig:resource_requirements} compares parameter counts and runtime. While PS is cheapest, it often underperforms in roles requiring specialization. NPS, though powerful, scales poorly in both memory and wall-clock time. LoRASA achieves strong performance similar to (and sometimes exceeding) NPS with far fewer additional parameters. This is especially evident when scaling from 4 to 8 agents, where NPS overhead spikes but LoRASA’s cost grows moderately thanks to its low-rank updates. These findings highlight LoRASA as the “sweet spot” in MARL: it achieves the expressiveness and performance of NPS while retaining the parameter efficiency and scalability of PS, making it a practical, resource-friendly solution for large-scale MARL systems.


\begin{figure*}[!ht]
    \centering
    (1) \includegraphics[width=0.45\linewidth]{fig/num_params.pdf}
    (2) \includegraphics[width=0.45\linewidth]{fig/num_params_agents.pdf}
    \\(3) \includegraphics[width=0.45\linewidth]{fig/time.pdf}
    (4) \includegraphics[width=0.45\linewidth]{fig/smac_time.pdf}
    \caption{
        \textbf{Computational Efficiency of LoRASA Compared to Baselines.} 
        (1) \textbf{Memory footprint across environments}: Total trainable parameters for each baseline in MAMuJoCo and SMAC, highlighting LoRASA’s efficiency over NPS.
        (2) \textbf{Scalability with agent count}: Growth in trainable parameters as the number of agents increases, showing LoRASA scales efficiently while NPS grows linearly.
        (3) \textbf{Training and inference speed in MAMuJoCo}: LoRASA-based approaches significantly reduce computational time compared to NPS while achieving comparable or superior performance.
        (4) \textbf{Training and inference speed in SMAC}: Similar trends observed in SMAC, where LoRASA improves computational efficiency without compromising coordination quality.\footnotemark
    }
    \label{fig:resource_requirements}
    \vspace{-15pt}  % Reduce space between figure and next paragraph
\end{figure*}

\footnotetext{Note: Fig (1) \& (2) Light shades of orange and blue indicate the pretraining stage (Algorithm~\ref{alg:pretraining}), while dark shades indicate fine-tuning (Algorithm~\ref{alg:finetuning}) in LoRA-based methods. For Fig (3) \& (4), MAMuJoCo (Ant 2x4, 4x2, 8x1) and SMAC (2s3z, 3s5z, MMM2)}

\subsection{Ablation Studies}
\label{subsec:ablations}

\begin{figure*}[htbp]
\centering

\includegraphics[width=0.33\linewidth]{fig/ablations/ckpt_ant.pdf}
\includegraphics[width=0.33\linewidth]{fig/ablations/r_ant.pdf}
\includegraphics[width=0.33\linewidth]{fig/ablations/layer_ant.pdf}
\includegraphics[width=0.33\linewidth]{fig/ablations/ckpt_mmm2.pdf}
\includegraphics[width=0.33\linewidth]{fig/ablations/r_mmm2.pdf}
\includegraphics[width=0.33\linewidth]{fig/ablations/layer_mmm2.pdf}
\caption{Ablation studies on A2PO and MAPPO algorithms in Ant4x2 and MMM2 environments. (A--D) \textbf{Timing of LoRA Fine-Tuning}: Evaluates checkpoints starting at different environment steps versus the Parameter Sharing (PS) baseline. (E--H) \textbf{LoRA Rank \( r \)}: Assesses the impact of varying \( r \) values at 4, 8, 16, and 64 (full rank) compared to the PS baseline. (I--L) \textbf{Layer-Wise LoRA}: Compares the effect of applying LoRA selectively to different layers of the policy including applying LoRA to all layers simultaneously. Each subplot displays median episode returns and win rates over environment steps for MAMujoco and SMAC respectively, demonstrating LoRASA’s effectiveness in learning heterogeneous behaviors while balancing efficiency and expressivity.}

\label{fig:lora_ablation}
\vspace{-15pt}  % Reduce space between figure and next paragraph
\end{figure*}

Our ablation experiments highlight three key dimensions—\emph{fine‐tuning checkpoints}, \emph{LoRA rank}, and \emph{adapter placement}—that underscore LoRA’s capacity to systematically unlock agent-specific policies while preserving the advantages of PS. We present ablation results for these factors using A2PO and MAPPO in two challenging scenarios, Ant 4x2 from MAMuJoCo and MMM2 from SMAC (refer Figure~\ref{fig:lora_ablation}).


\paragraph{Early vs. Late Fine-Tuning.}
Our experiments, Figs~\ref{fig:lora_ablation} (A)--(D), show that LoRA typically outperforms pure parameter sharing when introduced at checkpoints where the shared policy begins steady improvement but hasn't fully converged. For instance, switching to LoRA at \(4\times10^6\) steps works best for A2PO and MAPPO on Ant 4x2, while \(2\times10^6\) steps is optimal for A2PO on MMM2. These points likely mark the emergence of useful knowledge that LoRA can specialize more efficiently than continuing with shared weights alone.

However, in the case of MAPPO on MMM2, earlier switches only match PS performance. A later transition, around \(7\times10^6\) steps, yields peak results. This suggests that MAPPO requires more training in complex environments like MMM2 to form a robust foundation, after which LoRA can refine agent-specific behaviors without disrupting stability—consistent with the poor performance of MAPPO NPS in MMM2, see Figure~\ref{fig:baseline_exps}(P).

These findings suggest a practical guideline: \textbf{introduce LoRA updates once the shared policy exhibits competent yet non-plateaued performance}, ensuring an optimal window for effective specialization.


\paragraph{\textbf{Rank \(\mathbf{r}\): Striking a Balance Between Capacity and Efficiency.}}
We evaluate the effects of ranks $4$, $8$, $16$, and $64$ (full rank). Experiments, refer Figs~\ref{fig:lora_ablation} (E)--(H), demonstrate that moderate ranks (e.g., \(\mathbf{r=8}\)) often outperform or match full‐rank updates, reinforcing the idea that agent diversity resides in a smaller subspace than the entire parameter space. Interestingly, higher ranks (\(\mathbf{r=16}\), \(\mathbf{r=64}\)) can lead to slower convergence or overfitting, while extremely low ranks (\(\mathbf{r=4}\)) are sometimes insufficient for capturing nuanced behaviors. The sweet spot around \(\mathbf{r=8}\) suggests that LoRA’s “low‐rank” premise is more than a parameter‐efficiency hack; it’s a targeted mechanism that \emph{regularizes} agent adaptations and harnesses a smaller, behaviorally meaningful subspace. This subspace is powerful enough to drive strong performance and sample efficiency without requiring the overhead of NPS.


\paragraph{\textbf{Adapter Placement: All Layers vs. Specific Layers.}}  
Figs~\ref{fig:lora_ablation} (I)--(L) reveals that distributing LoRA across \emph{multiple} network layers generally performs best, especially when including higher/intermediate layers. By contrast, only adapting the final (output) layer sees strong but not top‐tier performance, indicating that decisions made at earlier layers are also relevant for role differentiation. Meanwhile, the minimal impact of the earliest layers suggests that certain low‐level feature extractions are already well handled by the shared backbone. LoRA’s capacity to adapt \emph{any} layer—rather than just the output or action layer—provides more robust, fine‐grained agent specialization. This finding runs counter to simpler multi‐task learning methods that adapt only the last few layers, underscoring the unique advantage of fully distributing LoRA across relevant modules.

Taken together, these ablation results show that LoRA provides a structured path to specialization while preserving the collaborative benefits of shared policy training. Full-layer LoRA generally matches or outperforms its baseline, except when applied too early or with overly low ranks. For optimal performance, we recommend introducing LoRA at a checkpoint where the shared policy shows steady improvement, using a moderate rank (\(r \approx 8\)), and adapting all layers. In more homogeneous scenarios, applying LoRA after baseline convergence can also be effective. By strategically choosing when (\(t\)) and where (\(\ell\)) to apply LoRA with a moderate rank (\(r\)), it enables near-independent policies while remaining parameter-efficient and simpler to train than fully separate networks.

\subsection{Discussion, Limitations and Future Work}
\label{subsec:discussion_limitations_future_work}

LoRASA significantly boosts performance and efficiency but comes with caveats. It relies on careful hyperparameter tuning—particularly rank selection and fine-tuning checkpoints—and depends on a robust pretrained shared policy. Fixed ranks across layers and agents may not capture the full diversity of highly heterogeneous or dynamic tasks. Moreover, LoRASA currently focuses on actor networks, leaving biases, normalization layers, and critic components unadapted. However, this approach can be extended to value-based methods; for instance, applying LoRASA to the utility functions in QMix enables agent-specific adaptations in value estimation, facilitating more nuanced coordination without requiring entirely separate policies. Future work can address these limitations by exploring dynamic rank adaptation per layer and agent type~\citep{AdaLoRA, DyLoRA}, and by extending LoRA to additional network components such as biases, normalization layers, and critic networks. Specifically, AdaLoRA~\citep{AdaLoRA}  decomposes \(\delta W\) using singular value decomposition to dynamically reduce the rank by removing less important singular values, while DyLoRA~\citep{DyLoRA} introduces adaptive mechanisms that train LoRA blocks across a range of ranks. Additionally, investigating alternating updates between shared and LoRA parameters, and integrating LoRA with hierarchical or adversarial policy architectures, could further generalize low-rank specialization. Extending this framework to competitive and adversarial multi-agent systems remains a promising direction, potentially enabling effective specialization in non-cooperative settings. These avenues promise to enhance LoRASA’s adaptability and robustness in large-scale, complex MARL applications. In challenging scenarios where the underlying algorithm struggles to learn a solid foundation (e.g., MAPPO on Humanoid $9|8$, see Figure~\ref{fig:baseline_exps}(L)), LoRASA’s effectiveness diminishes.

\section{Related Work}
\label{sec:related_work}

\paragraph{Parameter sharing (PS)} is a common strategy in MARL that reduces computational complexity by using a single policy or value network for all agents~\citep{terry2020revisiting}. However, standard PS often fails in heterogeneous environments where agents require diverse behaviors. Dynamic sharing methods~\citep{AdaPS, DynParamSharing} improve adaptability by assigning specific network modules based on predefined clusters, but they increase computational overhead, depend on role assumptions, and can introduce training instability—especially when agent roles change rapidly. In dynamic sharing methods, each agent's parameter subset can be significantly smaller than other policy baselines, making it unclear whether performance gaps stem from suboptimal selection or insufficient capacity. This scale discrepancy complicates direct comparisons with other parameter sharing approaches and is thus left out of the scope of this study. Techniques like agent identification~\citep{terry2020revisiting} or regularization-based methods~\citep{li2021CDS} attempt to differentiate agents within a shared network, but often lack sufficient representational capacity or add complexity and tuning burdens. In contrast, our approach embeds a \emph{low-rank structure} directly into shared parameters, inducing sparsity and enabling agent-specific specialization without requiring dynamic reconfiguration, clustering, or heavy regularization.


\paragraph{Selective Experience Sharing.}
Selective experience-sharing methods improve data efficiency by exchanging only relevant trajectories~\citep{SelectiveSharingExp, SharedExpAC}, reducing communication overhead and accelerating learning. However, they do not address policy expressiveness, as agents may still be constrained by a single shared model. In contrast, LoRASA operates at the parameter level, ensuring that even with fully shared transitions, low-rank offsets allow agents to develop specialized behaviors in an \(r\)-dimensional subspace. Thus, experience sharing enhances sample efficiency, while LoRASA enables representational flexibility, making them complementary rather than conflicting approaches.

\paragraph{Network Pruning} techniques~\citep{PSNetworkPruning} sparsify shared networks to lower resource usage. However, removing parameters outright risks discarding critical features needed by certain agents, especially in tasks requiring rare but crucial skills. Our work takes the opposite route: we \emph{add} low-rank modules to a shared backbone, preserving the base network and preventing irreversible performance degradation from over-pruning. This approach naturally balances expressiveness and efficiency by localizing agent-specific adaptations in small, learnable subspaces.

\paragraph{Non-Parameter Sharing (NPS)} policies (e.g., HAPPO~\citep{HAPPO}, A2PO~\citep{A2PO}) allow maximal specialization, but scale poorly in agent-heavy systems due to their linear growth in parameters and slower training due to re-learning of common behaviors. Despite their strong performance, these methods are often untenable for large populations of agents. In contrast, our low-rank approach approximates the benefits of NPS—i.e., agent-specific customization—while retaining the resource efficiency of a shared framework.

\paragraph{MARL as Multi-Task RL} methods, like ~\citep{wang2023multitaskmultiagentsharedlayers, omidshafiei2017deepdecentralizedmultitaskmultiagent, MultiTaskMARL, zhang2024hybridtrainingenhancedmultitask, PrioritizedTasKMining}, often aim to transfer knowledge such as shared decision-making modules, task representations, or agent-interactions across distinct tasks rather than to accommodate diverse roles within a single shared task. This makes them less suited for MARL scenarios where agents differ significantly but still collaborate on one global objective. In contrast, our work explicitly treats each agent as a separate “task”, applying parameter-efficient fine-tuning via low-rank adapters. Unlike approaches that only adapt output layers~\citep{MTL}, we modify internal layers as needed to capture nuanced agent behaviors without incurring the high overhead of duplicating entire networks.

\section{Conclusion}
\label{sec:conclusion}

We introduce \textbf{LoRASA}, a novel approach in MARL that integrates LoRA into parameter-sharing frameworks. LoRA enables scalable, specialized policies by constraining agent-specific updates to low-dimensional subspaces, effectively combining the efficiency of shared backbones with the expressiveness of near-independent policies. Our extensive experiments on \textbf{MAMuJoCo} and \textbf{SMAC} benchmarks demonstrate that LoRA-based methods consistently outperform or match specialized baselines like NPS, while significantly reducing both parameter and computational overhead. \textbf{Ablation studies} reveal that (1) \textbf{Deeper Network Layers} are essential for performance gains. (2) \textbf{Optimal Transition Timing} occurs when LoRA fine-tuning begins once the shared policy achieves competent, non-plateaued performance. (3) A \textbf{LoRA Rank of 8} effectively balances capacity and efficiency, scaling appropriately with task complexity. These findings provide practical guidelines for integrating LoRA into MARL pipelines. Future work will explore \textbf{dynamic rank adaptation} per layer and agent type, \textbf{alternating updates} between shared and LoRA parameters, and extending LoRA to \textbf{critic networks} and \textbf{adversarial multi-agent systems}, thereby enhancing adaptability and robustness in complex MARL environments. 

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\appendix
\onecolumn
\section{Appendix}

\subsection{Pseudocode}

\begin{algorithm}
\caption{Phase 1: Shared Policy Pretraining}
\label{alg:pretraining}
\textbf{Input:} $N$: number of agents, $\text{Env}$, $\text{Algorithm}$ (e.g., MAPPO/A2PO), $\theta_{\text{shared}}$: shared parameters, $P_\text{steps}$: pretraining steps \\
\textbf{Output:} Pretrained $\theta_{\text{shared}}$
\begin{algorithmic}[1]
\STATE Initialize $\theta_{\text{shared}}$:
\FOR{$\text{step} \gets 1$ to $P_\text{steps}$}
    \STATE Collect joint trajectories $(\text{obs},\text{actions},\text{rewards},\text{next\_obs})$ from $\text{Env}$
    \STATE $\theta_{shared} \gets \text{Algorithm}.\text{update\_shared}(\theta_{\text{shared}}, \text{trajectories})$
\ENDFOR
\OUTPUT $\theta_{\text{shared}}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Phase 2: LoRA-Based Fine-Tuning}
\label{alg:finetuning}
\textbf{Input:} $N$: number of agents, $\text{Env}$, $\text{Algorithm}$, pretrained $\theta_{\text{shared}}$, rank $r$, $F_\text{steps}$ \\
\textbf{Output:} Agent-specific LoRA adapters $\{A_i^\ell, B_i^\ell\}$

\begin{algorithmic}
\small % Optional: reduce font size within the algorithm
\STATE Introduce LoRA adapters $A_i^\ell,B_i^\ell$; freeze $\theta_{\text{shared}}$
\STATE \hspace{1em} $A_i^\ell \gets \mathbf{0}_{d_\ell\times r}, \quad B_i^\ell \gets \text{Random}(k_\ell\times r)$
\FOR{$\text{step} \gets 1$ to $F_\text{steps}$}
    \STATE Collect trajectories $(\text{obs}, a, \text{rewards}, \text{next\_obs})$
    \FOR{$i \gets 1$ to $N$}
        \STATE $(A_i^\ell, B_i^\ell) \gets \text{Algorithm}.\text{update\_agent\_lora}(\theta_{\text{shared}}, A_i^\ell, B_i^\ell,$
        \STATE \hspace{2em} $\text{trajectories}[i], r)$
    \ENDFOR
\ENDFOR
\OUTPUT $\{A_i^\ell, B_i^\ell\}_{i=1}^N$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Inference with LoRA}
\label{alg:inference}
\textbf{Input:} Pretrained $\theta_{\text{shared}}$, LoRA adapters $\{A_i^\ell, B_i^\ell\}$, agent observations $\{o_i\}_{i=1}^N$ \\
\textbf{Output:} Actions $\{a_i\}_{i=1}^N$
\begin{algorithmic}[1]
\FOR{$i \gets 1$ to $N$}
    \FOR{layer $\ell$ in actor network}
        \STATE $\theta_i^\ell \gets \theta_{\text{shared}}^\ell + A_i^\ell B_i^{\ell\top}$
    \ENDFOR
\ENDFOR
\STATE $\{a_i\}_{i=1}^N \gets \text{Actor}.\text{select\_action}(\{o_i\}_{i=1}^N, \{\theta_i^\ell\}_{i=1}^N)$
\OUTPUT $\{a_i\}_{i=1}^N$
\end{algorithmic}
\end{algorithm}

\newpage
\subsection{SMAC Evaluation Rewards}
\begin{figure}[!htbp]
    \centering

    \includegraphics[width=\linewidth]{fig/a2po_smac_rewards.pdf} 

    \includegraphics[width=\linewidth]{fig/mappo_smac_rewards.pdf}

    \caption{Evaluation episode rewards of (A--D) A2PO and (E--H) MAPPO in SMAC scenarios}
    \label{fig:smac_rewards}
\end{figure}

\clearpage

\subsection{Further Analysis}
\label{subsec:analysis}

\paragraph{Heterogeneous nature of agent policies}
\label{subsec:heterogeneous_nature} 
Inspection of the \emph{policy activation heatmaps} (see Figures ~\ref{fig:hm_a2po_3s56z} -- ~\ref{fig:hm_a2po_ant4x2_8M}) indicates that the initial layers of each agent’s policy are comparatively similar—both relative to the shared baseline and among different agents. This aligns closely with our \emph{layer ablation} findings (Figure~\ref{fig:lora_ablation}(I)--(L)), where adapting these early layers alone provides only modest returns. These early, near-identical activations suggest that fundamental state or feature extraction is largely \emph{universal}, capturing environmental signals (e.g., basic positional inputs in MAMuJoCo or unit attributes in SMAC) that all agents need in a shared manner \citep{Gupta2017CooperativeMC, terry2020revisiting}.

In contrast, later layers exhibit far more divergence in the heatmaps, signaling agent-specific computations that reflect distinct strategic roles. For example, in Figure~\ref{fig:lora_ablation}(I)--(L), applying LoRA to mid-to-high layers substantially boosts performance, underscoring that the majority of adaptive capacity is needed where the policy makes higher-level decisions (e.g., unit targeting in SMAC or joint coordination in MAMuJoCo). Further, adapting all layers emerges as the best configuration, indicating that—even though initial layers are mostly similar—some specialized nuance in lower layers can still yield incremental gains when combined with deeper-layer updates, consistent with the agent-specific differences revealed in Figures ~\ref{fig:hm_a2po_3s56z} -- ~\ref{fig:hm_a2po_ant4x2_8M}.

To quantify these visual distinctions, we measure the Wasserstein distance between policy distributions across agents (see Figure~\ref{fig:was_3s5z_vs_3s6z} and ~\ref{fig:was_mmm2}). Two key observations arise:

\textbf{Agents with Similar Roles but Divergent Strategies}.
Units of the same “category” often have smaller pairwise Wasserstein distances, suggesting a shared skill set or baseline. However, even among these nominally similar agents, divergences can arise—particularly in later layers—because each agent may develop a unique strategy. This behavior echoes recent work in multi-task RL that finds role similarity does not preclude agent-specific policy refinements when higher-level decisions are at play \citep{zhang2021multi, MultiTaskMARL}.

\textbf{Different Categories, Greater Distances}.
When comparing agents of distinct roles, the Wasserstein distance grows larger. This supports the notion that LoRA fosters substantial heterogeneity for roles requiring fundamentally different behaviors. Our ablation results Figure~\ref{fig:lora_ablation}(I)--(L) reinforce that focusing LoRA updates on deeper layers, where these role-specific divergences manifest, provides significant performance gains.


\begin{figure}
    \centering
    % Subfigure A
    \begin{subfigure}
        \centering
        \includegraphics[width=0.4\linewidth]{fig/WAS/mmm2_was_dist.pdf} % Replace with your image path
        \caption{Wasserstein distance MMM2}
        \label{fig:was_mmm2}
    \end{subfigure}
    % \hfill
    % Subfigure B
    \begin{subfigure}
        \centering
        \includegraphics[width=0.4\linewidth]{fig/WAS/3s5z_vs_3s6z_was_dist.pdf} % Replace with your image path
        \caption{Wasserstein distance 3s5z\_vs\_3s6z}
        \label{fig:was_3s5z_vs_3s6z}
    \end{subfigure}
\end{figure}


\paragraph{Sparsity analysis}
\label{paragraph:sparsity_analysis}

Figure~\ref{fig:sparsity_comparison} demonstrates the sparsity introduced by LoRASA in the policy parameter space. The percentage of policy parameters above various threshold values is significantly higher for the shared policy (\(\lvert\theta_\text{shared}\rvert\)) compared to LoRASA-adapted parameters (\(\lvert\delta\theta\rvert\)). This suggests that LoRASA fine-tuning effectively prunes the parameter space by focusing updates on a smaller, behaviorally critical subspace.

At lower thresholds, both shared and LoRA-adapted parameters maintain a higher proportion of active values. However, as the threshold increases, the LoRA curve drops off more sharply than the shared policy curve. This indicates that LoRA adaptations primarily influence low-magnitude adjustments, reinforcing its role as a lightweight mechanism for agent-specific fine-tuning without unnecessarily inflating parameter magnitudes.

This analysis ties the sparsity observation to LoRASA's broader benefits, reinforcing its practical and theoretical strengths in MARL.

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.4\linewidth]{fig/pct_above_threshold.pdf}
\end{center}
\caption{Sparsity comparison between LoRA layers vs Shared layers. Percentage of policy parameters above threshold values is computed by flattening all weights into a single array, taking the absolute values, and evaluating 100 evenly spaced threshold values between the minimum and maximum of the array (inclusive). For each threshold, the percentage is calculated as \((\text{array} \geq \text{threshold}).\text{mean()} \times 100\), reflecting the proportion of parameters exceeding the given threshold.}
\label{fig:sparsity_comparison}
\end{figure}

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_3s56z.pdf}
\end{center}
\caption{Layer Activation of the map 3s5z\_vs\_3s6z using A2PO}
\label{fig:hm_a2po_3s56z}
\end{figure}

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_ant4x2_4M.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using A2PO}
\label{fig:hm_a2po_ant4x2}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_mmm2.pdf}
\end{center}
\caption{Layer Activation of the map MMM2 using A2PO}
\label{fig:hm_a2po_mmm2}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.8\linewidth]{fig/heatmaps/hm_a2po_wk3x2.pdf}
\end{center}
\caption{Layer Activation of the map Walker3x2 using A2PO}
\label{fig:hm_a2po_walker3x2}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_mappo_3s56z.pdf}
\end{center}
\caption{Layer Activation of the map 3s5z\_vs\_3s6z using MAPPO}
\label{fig:hm_mappo_3s56z}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_mappo_ant4x2.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using MAPPO}
\label{fig:hm_mappo_ant4x2}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_mappo_mmm2.pdf}
\end{center}
\caption{Layer Activation of the map MMM2 using MAPPO}
\label{fig:hm_mappo_mmm2}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.85\linewidth]{fig/heatmaps/hm_mappo_wk3x2.pdf}
\end{center}
\caption{Layer Activation of the map Walker3x2 using MAPPO}
\label{fig:hm_mappo_walker3x2}
\end{figure} 


% This is the layerwise activation across different checkpoints of LoRA training in Ant4x2 with A2PO

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_ant4x2_2M.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using A2PO at 2M steps}
\label{fig:hm_a2po_ant4x2_2M}
\end{figure} 

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_ant4x2_4M.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using A2PO at 4M steps}
\label{fig:hm_a2po_ant4x2_4M}
\end{figure}

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_ant4x2_6M.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using A2PO at 6M steps}
\label{fig:hm_a2po_ant4x2_6M}
\end{figure}

\begin{figure}
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\linewidth]{fig/heatmaps/hm_a2po_ant4x2_8M.pdf}
\end{center}
\caption{Layer Activation of the map Ant4x2 using A2PO at 8M steps}
\label{fig:hm_a2po_ant4x2_8M}
\end{figure}

\clearpage

\subsection{Hyperparameters}
\label{subsec:hyperparameters}

\begin{table}[h]
\centering
\begin{tabular}{lllcll}
          \toprule      & \multicolumn{2}{c}{A2PO PS+LoRA} && \multicolumn{2}{c}{A2PO SePS+LoRA} \\ \cmidrule{2-3} \cmidrule{5-6} 
Scenario        & r           & Checkpoint    &      & r            & Checkpoint          \\ \midrule
Halfcheetah 2x3 & 8           & 3.00E+06      &      & N/A          & N/A                 \\
Walker 3x2      & 8           & 2.00E+06       &     & 8            & 2.00E+06            \\
Ant 4x2         & 8           & 4.00E+06        &    & 8            & 4.00E+06            \\
Humanoid $9|8$    & 16          & 3.00E+06         &   & N/A          & N/A                 \\ \midrule
3s5z            & 8           & 2.00E+05          &  & 8            & 3.00E+05            \\
1c3s5z          & 8           & 5.00E+05           & & 8            & 1.00E+05            \\
3s5z\_vs\_3s6z  & 16          & 2.00E+06            && 16           & 5.00E+06            \\
MMM2            & 8           & 2.00E+06            && 16           & 2.00E+06 \\      \bottomrule    
\end{tabular}
\caption{Ranks and checkpoints for A2PO PS+LoRA and SePS+LoRA in MAMuJoCo and SMAC}
\label{tab:a2po_lora_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lllcll}
     \toprule           & \multicolumn{2}{c}{MAPPO PS+LoRA} &  & \multicolumn{2}{c}{MAPPO SePS+LoRA} \\ \cmidrule{2-3} \cmidrule{5-6} 
Scenario        & r           & Checkpoint          &  & r             & Checkpoint          \\ \midrule
Halfcheetah 2x3 & 8           & 1.00E+05            &  & N/A           & N/A                 \\
Walker 3x2      & 8           & 2.00E+06            &  & 8             & 7.00E+06            \\
Ant 4x2         & 8           & 4.00E+06            &  & 8             & 4.00E+06            \\
Humanoid $9|8$    & 16          & 1.00E+06            &  & N/A           & N/A                 \\ \midrule
3s5z            & 8           & 2.00E+05            &  & 8             & 2.00E+05            \\
1c3s5z          & 8           & 5.00E+05            &  & 8             & 5.00E+05            \\
3s5z\_vs\_3s6z  & 8           & 4.00E+06            &  & 8             & 4.00E+06            \\
MMM2            & 8           & 7.00E+06            &  & 8             & 6.00E+06   \\ \bottomrule        
\end{tabular}
\caption{Ranks and checkpoints for MAPPO PS+LoRA and SePS+LoRA in MAMuJoCo and SMAC}
\label{tab:mappo_lora_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
gamma                  & 0.99     \\
gain                   & 0.01     \\
activation             & ReLU     \\
optimizer              & Adam     \\
optim eps              & 1.00E-05 \\
MLP hidden layer       & 2        \\
hidden layer after RNN & 1        \\
actor network          & RNN      \\
chunk length           & 10       \\
max grad norm          & 10       \\
hidden layer dim       & 64       \\
num mini-batch         & 1       \\
\bottomrule
\end{tabular}
\caption{Global hyperparameters applicable across all environments, scenarios, algorithms, and parameter sharing methods.}
\label{tab:global_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lllll}\toprule
                & Halfcheetah 2x3 & Walker 3x2 & Ant 4x2 & Humanoid $9|8$ \\ \midrule
episode length  & 4000            & 4000       & 4000    & 4000         \\
eval episode    & 10              & 10         & 10      & 10           \\
rollout threads & 16              & 16         & 25      & 25           \\
gae lambda      & 0.93            & 0.93       & 0.93    & 0.9         \\ \bottomrule
\end{tabular}
\caption{MAMuJoCo scenario-specific hyperparameters common to all algorithms and parameter sharing methods.}
\label{tab:mujoco_common_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lllll}\toprule
                & 3s5z & 1c3s5z & 3s5z\_vs\_3s6z & MMM2 \\ \midrule
episode length  & 3200 & 3200   & 3200           & 3200 \\
eval episode    & 32   & 32     & 32             & 32   \\
rollout threads & 10   & 10     & 10             & 10   \\
gae lambda      & 0.95 & 0.95   & 0.9            & 0.95\\ \bottomrule
\end{tabular}
\caption{SMAC scenario-specific hyperparameters common to all algorithms and parameter sharing methods.}
\label{tab:smac_common_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llllll}
\toprule Scenario        & clip & epoch & actor lr       & critic lr & entropy coefficient \\ \midrule
Halfcheetah 2x3 & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Walker 3x2      & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Ant 4x2         & 0.2  & 8     & 3.00E-04 & 3.00E-04  & 0                   \\
Humanoid $9|8$    & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\ \hline
3s5z            & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
1c3s5z          & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
3s5z\_vs\_3s6z  & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
MMM2            & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01       \\ \bottomrule        
\end{tabular}
\caption{Common hyperparameters for A2PO and MAPPO baselines, including NPS, MTL and SePS for A2PO, and PS, NPS, MTL, and SePS for MAPPO.}
\label{tab:algo_common_hyperparams}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llllll}
\toprule Scenario        & clip & epoch & actor lr & critic lr & entropy coefficient \\ \midrule
Halfcheetah 2x3 & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Walker 3x2      & 0.2  & 3     & 3.00E-04 & 3.00E-04  & 0                   \\
Ant 4x2         & 0.2  & 3     & 3.00E-04 & 3.00E-04  & 0                   \\
Humanoid $9|8$    & 0.2  & 3     & 3.00E-04 & 3.00E-04  & 0                   \\ \midrule
3s5z            & 0.2  & 3     & 5.00E-04 & 5.00E-04  & 0.01                \\
1c3s5z          & 0.2  & 3     & 5.00E-04 & 5.00E-04  & 0.01                \\
3s5z\_vs\_3s6z  & 0.1  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
MMM2            & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01     \\ \bottomrule          
\end{tabular}
\caption{Specific hyperparameters for A2PO PS.}
\label{tab:a2po_hyperparams_ps}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llllll}
\toprule Scenario        & clip & epoch & actor lr & critic lr & entropy coefficient \\ \midrule
Halfcheetah 2x3 & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Walker 3x2      & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Ant 4x2         & 0.2  & 8     & 3.00E-04 & 3.00E-04  & 0                   \\
Humanoid $9|8$    & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\ \midrule
3s5z            & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
1c3s5z          & 0.2  & 3     & 5.00E-04 & 5.00E-04  & 0.01                \\
3s5z\_vs\_3s6z  & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01                \\
MMM2            & 0.2  & 5     & 5.00E-04 & 5.00E-04  & 0.01       \\ \bottomrule        
\end{tabular}
\caption{Specific hyperparameters for A2PO PS+LoRA and SePS+LoRA.}
\label{tab:a2po_hyperparams_pslora_sepslora}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{llllll}
\toprule Scenario        & clip & epoch & actor lr & critic lr & entropy coefficient \\ \midrule
Halfcheetah 2x3 & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Walker 3x2      & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\
Ant 4x2         & 0.2  & 8     & 3.00E-04 & 3.00E-04  & 0                   \\
Humanoid $9|8$    & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0                   \\ \midrule
3s5z            & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0.01                \\
1c3s5z          & 0.2  & 5     & 3.00E-04 & 3.00E-04  & 0.01                \\
3s5z\_vs\_3s6z  & 0.05 & 5     & 3.00E-04 & 3.00E-04  & 0.001               \\
MMM2            & 0.05 & 5     & 3.00E-04 & 3.00E-04  & 0.001    \\ \bottomrule          
\end{tabular}
\caption{Specific hyperparameters for MAPPO PS+LoRA and SePS+LoRA.}
\label{tab:mappo_hyperparams_pslora_sepslora}
\end{table}



\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llclllclllclllclll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{PS} &  & \multicolumn{3}{c}{$2\times10^6$} &  & \multicolumn{3}{c}{$4\times10^6$} &  & \multicolumn{3}{c}{$6\times10^6$} &  & \multicolumn{3}{c}{$8\times10^6$} \\ \cmidrule{2-2} \cmidrule{4-6} \cmidrule{8-10} \cmidrule{12-14} \cmidrule{16-18} 
Layer &
  \multicolumn{1}{c}{$\|\theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} \\ \midrule
Linear 1             & 8.97                   &  & 8.55   & 8.78   & 1.87 &  & 8.70   & 8.82   & 1.48 &  & 8.85   & 8.89   & 1.04 &  & 8.89   & 8.91   & 0.59 \\
Linear 2             & 12.13                  &  & 11.55  & 11.84  & 2.64 &  & 11.80  & 11.94  & 1.90 &  & 11.97  & 12.03  & 1.30 &  & 12.06  & 12.10  & 0.90 \\
Linear 3             & 12.08                  &  & 11.57  & 11.84  & 2.46 &  & 11.82  & 11.99  & 1.96 &  & 11.93  & 12.01  & 1.30 &  & 12.01  & 12.04  & 0.91 \\
GRU x proj 4         & 11.14                  &  & 9.08   & 10.64  & 5.47 &  & 10.10  & 10.81  & 3.78 &  & 10.43  & 10.78  & 2.67 &  & 10.79  & 10.93  & 1.75 \\
GRU h proj 5         & 10.99                  &  & 8.96   & 10.36  & 5.10 &  & 9.81   & 10.59  & 3.92 &  & 10.22  & 10.54  & 2.52 &  & 10.56  & 10.75  & 1.85 \\
Linear 6             & 11.99                  &  & 11.57  & 11.74  & 2.05 &  & 11.80  & 11.88  & 1.56 &  & 11.88  & 11.92  & 0.98 &  & 11.93  & 11.95  & 0.70 \\
Linear 7             & 0.25                   &  & 0.16   & 0.33   & 0.30 &  & 0.21   & 0.32   & 0.27 &  & 0.22   & 0.29   & 0.18 &  & 0.25   & 0.26   & 0.11 \\ \bottomrule
\end{tabular}%
}
\caption{Norms of weights in different layers of the neural network for MAPPO PS and PS+LoRA with different checkpoints in Ant 4x2: including the norms of weights of PS at $12 \times 10^6$ steps ($\theta$), the norms of weights of pretrained PS ($\theta_{shared}$), the average norms of merged weights across agents ($\theta'$), and the average norms of weights for LoRA adapters ($\delta \theta$).}
\label{tab:norms_ckpt_mappo_ant4x2}
\end{table}



\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llclcllllcllll}
\toprule
             &       &  &       &  & \multicolumn{4}{c}{$\|\theta'\|_F$} &  & \multicolumn{4}{c}{$\|\delta \theta\|_F$} \\ \cmidrule{6-9} \cmidrule{11-14} 
Layer &
  \multicolumn{1}{c}{$\|\theta \|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
   &
  \multicolumn{1}{c}{$r=4$} &
  \multicolumn{1}{c}{$r=8$} &
  \multicolumn{1}{c}{$r=16$} &
  \multicolumn{1}{c}{$r=64$} &
   &
  \multicolumn{1}{c}{$r=4$} &
  \multicolumn{1}{c}{$r=8$} &
  \multicolumn{1}{c}{$r=16$} &
  \multicolumn{1}{c}{$r=64$} \\ \midrule
Linear 1     & 8.97  &  & 8.74  &  & 8.86   & 8.82  & 8.85  & 8.98  &  & 1.37    & 1.48    & 1.44    & 1.98   \\
Linear 2     & 12.13 &  & 11.86 &  & 12.00  & 11.94 & 12.02 & 12.28 &  & 1.73    & 1.90    & 1.98    & 2.86   \\
Linear 3     & 12.08 &  & 11.86 &  & 12.03  & 11.99 & 12.05 & 12.27 &  & 1.96    & 1.96    & 2.07    & 2.91   \\
GRU x proj 4 & 11.14 &  & 10.03 &  & 10.76  & 10.81 & 10.97 & 13.01 &  & 3.85    & 3.78    & 4.42    & 7.99   \\
GRU h proj 5 & 10.99 &  & 9.81  &  & 10.61  & 10.59 & 10.85 & 13.79 &  & 3.95    & 3.92    & 4.38    & 9.39   \\
Linear 6     & 11.99 &  & 11.78 &  & 11.94  & 11.88 & 11.88 & 12.13 &  & 1.74    & 1.56    & 1.57    & 2.68   \\
Linear 7     & 0.25  &  & 0.21  &  & 0.31   & 0.32  & 0.30  & 0.25  &  & 0.25    & 0.27    & 0.22    & 0.19   \\ \bottomrule
\end{tabular}%
}
\caption{Norms of weights in different layers of the neural network for MAPPO PS and different ranks of PS+LoRA fine tuning in Ant 4x2: including the norms of weights of PS at $12 \times 10^6$ steps ($\theta$), the norms of weights of pretrained PS at $4 \times 10^6$ steps ($\theta_{shared}$), the average norms of merged weights across agents ($\theta'$), and the average norms of weights for LoRA adapters ($\delta \theta$).}
\label{tab:norms_r_mappo_ant4x2}
\end{table}



\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llclllclllclllclll}\toprule
             & \multicolumn{1}{c}{} &  & \multicolumn{3}{c}{$1\times10^6$} &  & \multicolumn{3}{c}{$3\times10^6$} &  & \multicolumn{3}{c}{$5\times10^6$} &  & \multicolumn{3}{c}{$7\times10^6$} \\ \cmidrule{4-6} \cmidrule{8-10} \cmidrule{12-14} \cmidrule{16-18} 
Layer &
  \multicolumn{1}{c}{$\|\theta \|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
  \multicolumn{1}{c}{$\|\theta'\|_F$} &
  \multicolumn{1}{c}{$\|\delta \theta\|_F$} \\ \midrule
Linear 1     & 17.32                &  & 11.70  & 12.76  & 5.15 &  & 12.95  & 13.79  & 4.70 &  & 13.81  & 14.48  & 4.33 &  & 15.11  & 15.49  & 3.39 \\
Linear 2     & 12.69                &  & 11.38  & 11.57  & 2.02 &  & 11.64  & 11.80  & 1.93 &  & 11.85  & 11.99  & 1.83 &  & 12.18  & 12.26  & 1.37 \\
Linear 3     & 12.82                &  & 11.38  & 11.59  & 2.20 &  & 11.64  & 11.79  & 1.95 &  & 11.85  & 12.01  & 1.98 &  & 12.22  & 12.30  & 1.46 \\
GRU x proj 4 & 13.93                &  & 8.41   & 9.69   & 4.80 &  & 9.61   & 10.56  & 4.30 &  & 10.52  & 11.31  & 4.13 &  & 11.86  & 12.25  & 2.96 \\
GRU h proj 5 & 13.96                &  & 8.45   & 9.83   & 4.96 &  & 9.56   & 10.47  & 4.20 &  & 10.29  & 11.22  & 4.32 &  & 11.73  & 12.17  & 3.15 \\
Linear 6     & 13.02                &  & 11.40  & 11.60  & 2.18 &  & 11.67  & 11.83  & 1.97 &  & 11.92  & 12.10  & 2.02 &  & 12.29  & 12.39  & 1.64 \\
Linear 7     & 6.25                 &  & 4.61   & 6.09   & 2.96 &  & 5.25   & 5.70   & 1.52 &  & 5.50   & 5.87   & 1.34 &  & 5.82   & 5.97   & 0.93 \\ \bottomrule
\end{tabular}%
}
\caption{Norms of weights in different layers of the neural network for MAPPO PS and PS+LoRA with different checkpoints in MMM2: including the norms of weights of PS at $12\times10^6$ steps ($\theta$), the norms of weights of pretrained PS ($\theta_{shared}$), the average norms of merged weights across agents ($\theta'$), and the average norms of weights for LoRA adapters ($\delta \theta$).}
\label{tab:norms_ckpt_mappo_mmm2}
\end{table}


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llclcllllcllll}\toprule
             &       &  &       &  & \multicolumn{4}{c}{$\|\theta'\|_F$} &  & \multicolumn{4}{c}{$\|\delta \theta\|_F$} \\ \cmidrule{6-9} \cmidrule{11-14} 
Layer &
  \multicolumn{1}{c}{$\|\theta \|_F$} &
   &
  \multicolumn{1}{c}{$\|\theta_{shared}\|_F$} &
   &
  \multicolumn{1}{c}{$r=4$} &
  \multicolumn{1}{c}{$r=8$} &
  \multicolumn{1}{c}{$r=16$} &
  \multicolumn{1}{c}{$r=64$} &
   &
  \multicolumn{1}{c}{$r=4$} &
  \multicolumn{1}{c}{$r=8$} &
  \multicolumn{1}{c}{$r=16$} &
  \multicolumn{1}{c}{$r=64$} \\ \midrule
Linear 1     & 17.32 &  & 15.11 &  & 15.48  & 15.49 & 15.49 & 15.85 &  & 3.33    & 3.39    & 3.46    & 4.64   \\
Linear 2     & 12.69 &  & 12.18 &  & 12.26  & 12.26 & 12.25 & 12.36 &  & 1.40    & 1.37    & 1.39    & 1.87   \\
Linear 3     & 12.82 &  & 12.22 &  & 12.32  & 12.30 & 12.31 & 12.41 &  & 1.50    & 1.46    & 1.50    & 1.94   \\
GRU x proj 4 & 13.93 &  & 11.86 &  & 12.27  & 12.25 & 12.29 & 12.71 &  & 3.03    & 2.96    & 3.12    & 4.33   \\
GRU h proj 5 & 13.96 &  & 11.73 &  & 12.16  & 12.17 & 12.21 & 12.72 &  & 3.09    & 3.15    & 3.23    & 4.58   \\
Linear 6     & 13.02 &  & 12.29 &  & 12.38  & 12.39 & 12.40 & 12.50 &  & 1.58    & 1.64    & 1.67    & 2.14   \\
Linear 7     & 6.25  &  & 5.82  &  & 5.96   & 5.97  & 6.01  & 6.00  &  & 0.91    & 0.93    & 1.07    & 1.11   \\ \bottomrule
\end{tabular}%
}
\caption{Norms of weights in different layers of the neural network for MAPPO PS and different ranks of PS+LoRA fine tuning in MMM2: including the norms of weights of PS at $12 \times 10^6$ steps ($\theta$), the norms of weights of pretrained PS at $7 \times 10^6$ steps ($\theta_{shared}$), the average norms of merged weights across agents ($\theta'$), and the average norms of weights for LoRA adapters ($\delta \theta$).}
\label{tab:norms_r_mappo_mmm2}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
