@article{MARL_Survey_1,
title = {A survey on multi-agent reinforcement learning and its application},
journal = {Journal of Automation and Intelligence},
volume = {3},
number = {2},
pages = {73-91},
year = {2024},
issn = {2949-8554},
doi = {https://doi.org/10.1016/j.jai.2024.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2949855424000042},
author = {Zepeng Ning and Lihua Xie},
keywords = {Benchmark environments, Multi-agent reinforcement learning, Multi-agent systems, Stochastic games},
}


@article{MARL_survey_2,
title = "A Comprehensive Survey on Multi-Agent Reinforcement Learning for Connected and Automated Vehicles",
keywords = "autonomous vehicles, connected vehicles, multi-agent reinforcement learning, safety, unseen scenario",
author = "Pamul Yadav and Ashutosh Mishra and Shiho Kim",
note = "Publisher Copyright: {\textcopyright} 2023 by the authors.",
year = "2023",
month = may,
doi = "10.3390/s23104710",
language = "English",
volume = "23",
journal = "Sensors",
issn = "1424-8220",
publisher = "Multidisciplinary Digital Publishing Institute (MDPI)",
number = "10",
}

@article{MARL_Survey_3,
author = {Zhao, Yue and Ju, Lushan and Hernández-Orallo, Josè},
year = {2024},
month = {02},
pages = {},
title = {Team formation through an assessor: choosing MARL agents in pursuit–evasion games},
volume = {10},
journal = {Complex and Intelligent Systems},
doi = {10.1007/s40747-023-01336-5}
}

@misc{MARL_Survey_4,
      title={Multi-agent Reinforcement Learning: A Comprehensive Survey}, 
      author={Dom Huh and Prasant Mohapatra},
      year={2024},
      eprint={2312.10256},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2312.10256}, 
}

@Article{MARL_Survey_5,
AUTHOR = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Spanò, Sergio},
TITLE = {Multi-Agent Reinforcement Learning: A Review of Challenges and Applications},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {4948},
URL = {https://www.mdpi.com/2076-3417/11/11/4948},
ISSN = {2076-3417},
DOI = {10.3390/app11114948}
}

@Article{MARL_Survey_6,
AUTHOR = {Ibrahim, Abdikarim Mohamed and Yau, Kok-Lim Alvin and Chong, Yung-Wey and Wu, Celimuge},
TITLE = {Applications of Multi-Agent Deep Reinforcement Learning: Models and Algorithms},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {10870},
URL = {https://www.mdpi.com/2076-3417/11/22/10870},
ISSN = {2076-3417},
DOI = {10.3390/app112210870}
}

@misc{CTDE_1,
      title={An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning}, 
      author={Christopher Amato},
      year={2024},
      eprint={2409.03052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.03052}, 
}

@misc{CTDE_2,
      title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}, 
      author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
      year={2020},
      eprint={1706.02275},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.02275}, 
}

@misc{CTDE_3,
      title={QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning}, 
      author={Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
      year={2018},
      eprint={1803.11485},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.11485}, 
}

@misc{CTDE_4,
      title={Counterfactual Multi-Agent Policy Gradients}, 
      author={Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
      year={2017},
      eprint={1705.08926},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.08926}, 
}

@misc{CTDE_5,
      title={Value-Decomposition Networks For Cooperative Multi-Agent Learning}, 
      author={Peter Sunehag and Guy Lever and Audrunas Gruslys and Wojciech Marian Czarnecki and Vinicius Zambaldi and Max Jaderberg and Marc Lanctot and Nicolas Sonnerat and Joel Z. Leibo and Karl Tuyls and Thore Graepel},
      year={2017},
      eprint={1706.05296},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1706.05296}, 
}

@inproceedings{scalability_1,
 author = {Qu, Guannan and Lin, Yiheng and Wierman, Adam and Li, Na},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2074--2086},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/168efc366c449fab9c2843e9b54e2a18-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{scalability_2,
  author  = {Siyi Hu and Yifan Zhong and Minquan Gao and Weixun Wang and Hao Dong and Xiaodan Liang and Zhihui Li and Xiaojun Chang and Yaodong Yang},
  title   = {MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {315},
  pages   = {1--23},
  url     = {http://jmlr.org/papers/v24/23-0378.html}
}

@misc{scalability_3,
      title={Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems}, 
      author={Kailash Gogineni and Peng Wei and Tian Lan and Guru Venkataramani},
      year={2023},
      eprint={2302.05007},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2302.05007}, 
}

@inproceedings{Oliehoek2016ACI,
  title={A Concise Introduction to Decentralized POMDPs},
  author={Frans A. Oliehoek and Chris Amato},
  booktitle={SpringerBriefs in Intelligent Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:3263887}
}

@article{amato2024partial,
  title={(A Partial Survey of) Decentralized, Cooperative Multi-Agent Reinforcement Learning},
  author={Amato, Christopher},
  journal={arXiv preprint arXiv:2405.06161},
  year={2024}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@misc{CA_1,
      title={Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization}, 
      author={Aditya Kapoor and Benjamin Freed and Howie Choset and Jeff Schneider},
      year={2024},
      eprint={2408.04295},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2408.04295}, 
}

@article{CA_2,
   title={Learning Cooperative Multi-Agent Policies With Partial Reward Decoupling},
   volume={7},
   ISSN={2377-3774},
   url={http://dx.doi.org/10.1109/LRA.2021.3135930},
   DOI={10.1109/lra.2021.3135930},
   number={2},
   journal={IEEE Robotics and Automation Letters},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Freed, Benjamin and Kapoor, Aditya and Abraham, Ian and Schneider, Jeff and Choset, Howie},
   year={2022},
   month=apr, pages={890–897} }


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{terry2020revisiting,
  title={Revisiting parameter sharing in multi-agent deep reinforcement learning},
  author={Terry, Justin K and Grammel, Nathaniel and Son, Sanghyun and Black, Benjamin and Agrawal, Aakriti},
  journal={arXiv preprint arXiv:2005.13625},
  year={2020}
}

@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24611--24624},
  year={2022}
}

@article{zhong2024heterogeneous,
  title={Heterogeneous-agent reinforcement learning},
  author={Zhong, Yifan and Kuba, Jakub Grudzien and Feng, Xidong and Hu, Siyi and Ji, Jiaming and Yang, Yaodong},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={1-67},
  pages={1},
  year={2024}
}

@inproceedings{christianos2021SePS,
  title={Scaling multi-agent reinforcement learning with selective parameter sharing},
  author={Christianos, Filippos and Papoudakis, Georgios and Rahman, Muhammad A and Albrecht, Stefano V},
  booktitle={International Conference on Machine Learning},
  pages={1989--1998},
  year={2021},
  organization={PMLR}
}

@article{zhang2024pps,
  title={PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning},
  author={Zhang, Ke and Zhu, DanDan and Xu, Qiuhan and Zhou, Hao and Zheng, Ce},
  journal={arXiv preprint arXiv:2403.02635},
  year={2024}
}

@article{rashid2020qmix,
  title={Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={178},
  pages={1--51},
  year={2020}
}

@article{li2021CDS,
  title={Celebrating diversity in shared multi-agent reinforcement learning},
  author={Li, Chenghao and Wang, Tonghan and Wu, Chengjie and Zhao, Qianchuan and Yang, Jun and Zhang, Chongjie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3991--4002},
  year={2021}
}

@article{kim2023SNP_PS,
  title={Parameter sharing with network pruning for scalable multi-agent deep reinforcement learning},
  author={Kim, Woojun and Sung, Youngchul},
  journal={arXiv preprint arXiv:2303.00912},
  year={2023}
}

@inproceedings{li2024AdaPS,
  title={Adaptive parameter sharing for multi-agent reinforcement learning},
  author={Li, Dapeng and Lou, Na and Zhang, Bin and Xu, Zhiwei and Fan, Guoliang},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6035--6039},
  year={2024},
  organization={IEEE}
}

@misc{wang2023multitaskmultiagentsharedlayers,
      title={Multi-Task Multi-Agent Shared Layers are Universal Cognition of Multi-Agent Coordination}, 
      author={Jiawei Wang and Jian Zhao and Zhengtao Cao and Ruili Feng and Rongjun Qin and Yang Yu},
      year={2023},
      eprint={2312.15674},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2312.15674}, 
}


@misc{omidshafiei2017deepdecentralizedmultitaskmultiagent,
      title={Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability}, 
      author={Shayegan Omidshafiei and Jason Pazis and Christopher Amato and Jonathan P. How and John Vian},
      year={2017},
      eprint={1703.06182},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.06182}, 
}

@article{MultiTaskMARL,
author = {Li, Chao and Dong, Shaokang and Yang, Shangdong and Hu, Yujing and Ding, Tianyu and li, Wenbin and Gao, Yang},
year = {2024},
month = {10},
pages = {},
title = {Multi-Task Multi-Agent Reinforcement Learning With Interaction and Task Representations},
volume = {PP},
journal = {IEEE transactions on neural networks and learning systems},
doi = {10.1109/TNNLS.2024.3475216}
}

@misc{zhang2024hybridtrainingenhancedmultitask,
      title={Hybrid Training for Enhanced Multi-task Generalization in Multi-agent Reinforcement Learning}, 
      author={Mingliang Zhang and Sichang Su and Chengyang He and Guillaume Sartoretti},
      year={2024},
      eprint={2408.13567},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.13567}, 
}

@inproceedings{PrioritizedTasKMining,
author = {Yu, Yang and Yin, Qiyue and Zhang, Junge and Huang, Kaiqi},
title = {Prioritized Tasks Mining for Multi-Task Cooperative Multi-Agent Reinforcement Learning},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1615–1623},
numpages = {9},
keywords = {multi-agent cooperation, multi-task learning, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@article{MTLSurvey,
  author       = {Yu Zhang and
                  Qiang Yang},
  title        = {A Survey on Multi-Task Learning},
  journal      = {CoRR},
  volume       = {abs/1707.08114},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.08114},
  eprinttype    = {arXiv},
  eprint       = {1707.08114},
  timestamp    = {Tue, 23 Jun 2020 13:25:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ZhangY17aa.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{HierarchicalMTL,
AUTHOR = {Nisar, Muhammad Adeel and Shirahama, Kimiaki and Irshad, Muhammad Tausif and Huang, Xinyu and Grzegorzek, Marcin},
TITLE = {A Hierarchical Multitask Learning Approach for the Recognition of Activities of Daily Living Using Data from Wearable Sensors},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {19},
ARTICLE-NUMBER = {8234},
URL = {https://www.mdpi.com/1424-8220/23/19/8234},
PubMedID = {37837064},
ISSN = {1424-8220},
DOI = {10.3390/s23198234}
}

@article{MTLSurveyDL,
  author       = {Michael Crawshaw},
  title        = {Multi-Task Learning with Deep Neural Networks: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2009.09796},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.09796},
  eprinttype    = {arXiv},
  eprint       = {2009.09796},
  timestamp    = {Wed, 23 Sep 2020 15:51:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-09796.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{MTL,
author = {Caruana, Rich},
year = {1997},
month = {07},
pages = {},
title = {Multitask Learning},
volume = {28},
isbn = {978-1-4613-7527-2},
journal = {Machine Learning},
doi = {10.1023/A:1007379606734}
}

@ARTICLE{ActorCritic,
  author={Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Neuronlike adaptive elements that can solve difficult learning control problems}, 
  year={1983},
  volume={SMC-13},
  number={5},
  pages={834-846},
  keywords={Adaptive systems;Problem-solving;Training;Pattern recognition;Neurons;Supervised learning;Biological neural networks},
  doi={10.1109/TSMC.1983.6313077}}

@article{TRPO,
  author       = {John Schulman and
                  Sergey Levine and
                  Philipp Moritz and
                  Michael I. Jordan and
                  Pieter Abbeel},
  title        = {Trust Region Policy Optimization},
  journal      = {CoRR},
  volume       = {abs/1502.05477},
  year         = {2015},
  url          = {http://arxiv.org/abs/1502.05477},
  eprinttype    = {arXiv},
  eprint       = {1502.05477},
  timestamp    = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanLMJA15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{HAPPO,
  author       = {Jakub Grudzien Kuba and
                  Ruiqing Chen and
                  Muning Wen and
                  Ying Wen and
                  Fanglei Sun and
                  Jun Wang and
                  Yaodong Yang},
  title        = {Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/2109.11251},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.11251},
  eprinttype    = {arXiv},
  eprint       = {2109.11251},
  timestamp    = {Fri, 19 Aug 2022 14:57:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-11251.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{A2PO,
      title={Order Matters: Agent-by-agent Policy Optimization}, 
      author={Xihuai Wang and Zheng Tian and Ziyu Wan and Ying Wen and Jun Wang and Weinan Zhang},
      year={2023},
      eprint={2302.06205},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.06205}, 
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{SAC,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}

@misc{MAPPO,
      title={The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games}, 
      author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
      year={2022},
      eprint={2103.01955},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.01955}, 
}

@misc{LoRA_Task_1,
      title={Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning}, 
      author={Chongjie Si and Zhiyi Shi and Shifan Zhang and Xiaokang Yang and Hanspeter Pfister and Wei Shen},
      year={2024},
      eprint={2409.01035},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.01035}, 
}

@misc{LoRA_Task_2,
      title={MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning}, 
      author={Ahmed Agiza and Marina Neseem and Sherief Reda},
      year={2024},
      eprint={2403.20320},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.20320}, 
}

@article{Eckart1936TheAO,
  title={The approximation of one matrix by another of lower rank},
  author={Carl Eckart and G. Marion Young},
  journal={Psychometrika},
  year={1936},
  volume={1},
  pages={211-218},
  url={https://api.semanticscholar.org/CorpusID:10163399}
}

@article{Hiriart-Urruty_Le_2013, 
title={From Eckart and Young approximation to Moreau envelopes andvice versa}, 
volume={47}, 
DOI={10.1051/ro/2013040}, 
number={3}, 
journal={RAIRO - Operations Research}, 
author={Hiriart-Urruty, Jean-Baptiste and Le, Hai Yen}, 
year={2013}, 
pages={299–310}
} 

@misc{remman2024discoveringbehavioralmodesdeep,
      title={Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space}, 
      author={Sindre Benjamin Remman and Anastasios M. Lekkas},
      year={2024},
      eprint={2402.12939},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12939}, 
}

@article{Sun2019RealtimePD,
  title={Real-time Policy Distillation in Deep Reinforcement Learning},
  author={Yuxiang Sun and Pooyan Fazli},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.12630},
  url={https://api.semanticscholar.org/CorpusID:209515823}
}

@INPROCEEDINGS{ensemble_policy_distillation,
  author={Sun, Yuxiang and Zhang, Qi},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Ensemble Policy Distillation with Reduced Data Distribution Mismatch}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  keywords={Degradation;Training;Q-learning;Power demand;Supervised learning;Neural networks;Games;reinforcement learning;ensemble learning;distribution mismatch;policy distillation},
  doi={10.1109/IJCNN55064.2022.9892503}}

@misc{schneider2024identifyingpolicygradientsubspaces,
      title={Identifying Policy Gradient Subspaces}, 
      author={Jan Schneider and Pierre Schumacher and Simon Guist and Le Chen and Daniel Häufle and Bernhard Schölkopf and Dieter Büchler},
      year={2024},
      eprint={2401.06604},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.06604}, 
}

@misc{wadhwania2019policydistillationvaluematching,
      title={Policy Distillation and Value Matching in Multiagent Reinforcement Learning}, 
      author={Samir Wadhwania and Dong-Ki Kim and Shayegan Omidshafiei and Jonathan P. How},
      year={2019},
      eprint={1903.06592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1903.06592}, 
}

@article{LSTM,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{GRU,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@misc{SMAC,
      title={The StarCraft Multi-Agent Challenge}, 
      author={Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philip H. S. Torr and Jakob Foerster and Shimon Whiteson},
      year={2019},
      eprint={1902.04043},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.04043}, 
}

@misc{SMACv2,
      title={SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning}, 
      author={Benjamin Ellis and Jonathan Cook and Skander Moalla and Mikayel Samvelyan and Mingfei Sun and Anuj Mahajan and Jakob N. Foerster and Shimon Whiteson},
      year={2023},
      eprint={2212.07489},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07489}, 
}

@misc{QMIX,
      title={QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning}, 
      author={Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
      year={2018},
      eprint={1803.11485},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.11485}, 
}

@misc{AdaLoRA,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@misc{DyLoRA,
      title={DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation}, 
      author={Mojtaba Valipour and Mehdi Rezagholizadeh and Ivan Kobyzev and Ali Ghodsi},
      year={2023},
      eprint={2210.07558},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07558}, 
}

@book{suttonbarto,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
}

@inbook{Kuhn,
url = {https://doi.org/10.1515/9781400881970-012},
title = {11. Extensive Games and the Problem of Information},
booktitle = {Contributions to the Theory of Games, Volume II},
author = {H. W. Kuhn},
publisher = {Princeton University Press},
address = {Princeton},
pages = {193--216},
doi = {doi:10.1515/9781400881970-012},
isbn = {9781400881970},
year = {1953},
lastchecked = {2024-12-09}
}

@article{Shapely,
author = {L. S. Shapley },
title = {Stochastic Games*},
journal = {Proceedings of the National Academy of Sciences},
volume = {39},
number = {10},
pages = {1095-1100},
year = {1953},
doi = {10.1073/pnas.39.10.1095},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.39.10.1095},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.39.10.1095}}

@inproceedings{Boutilier,
author = {Boutilier, Craig},
title = {Planning, learning and coordination in multiagent decision processes},
year = {1996},
isbn = {1558604179},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge},
pages = {195–210},
numpages = {16},
location = {The Netherlands},
series = {TARK '96}
}

@misc{PPO,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@inproceedings{Gupta2017CooperativeMC,
  title={Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  author={Jayesh K. Gupta and Maxim Egorov and Mykel J. Kochenderfer},
  booktitle={AAMAS Workshops},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:9421360}
}

@article{Chu2017PSDPG,
  author       = {Xiangxiang Chu and
                  Hangjun Ye},
  title        = {Parameter Sharing Deep Deterministic Policy Gradient for Cooperative
                  Multi-agent Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1710.00336},
  year         = {2017},
  url          = {http://arxiv.org/abs/1710.00336},
  eprinttype    = {arXiv},
  eprint       = {1710.00336},
  timestamp    = {Mon, 13 Aug 2018 16:47:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1710-00336.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DynParamSharing,
  title     = {ADMN: Agent-Driven Modular Network for Dynamic Parameter Sharing in Cooperative Multi-Agent Reinforcement Learning},
  author    = {Yu, Yang and Yin, Qiyue and Zhang, Junge and Xu, Pei and Huang, Kaiqi},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {302--310},
  year      = {2024},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2024/34},
  url       = {https://doi.org/10.24963/ijcai.2024/34},
}

@misc{AdaPS,
      title={Adaptive parameter sharing for multi-agent reinforcement learning}, 
      author={Dapeng Li and Na Lou and Bin Zhang and Zhiwei Xu and Guoliang Fan},
      year={2023},
      eprint={2312.09009},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.09009}, 
}

@inproceedings{DyPS,
author = {Wang, Jingwei and Hao, Qianyue and Huang, Wenzhen and Fan, Xiaochen and Tang, Zhentao and Wang, Bin and Hao, Jianye and Li, Yong},
title = {DyPS: Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning for Spatio-Temporal Resource Allocation},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3672052},
doi = {10.1145/3637528.3672052},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3128–3139},
numpages = {12},
keywords = {dynamic parameter sharing, multi-agent reinforcement learning, spatio-temporal resource allocation},
location = {Barcelona, Spain},
series = {KDD '24}
}

@misc{SelectiveSharingExp,
      title={Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning}, 
      author={Matthias Gerstgrasser and Tom Danino and Sarah Keren},
      year={2024},
      eprint={2311.00865},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.00865}, 
}

@misc{SharedExpAC,
      title={Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning}, 
      author={Filippos Christianos and Lukas Schäfer and Stefano V. Albrecht},
      year={2021},
      eprint={2006.07169},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2006.07169}, 
}

@misc{PSNetworkPruning,
      title={Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning}, 
      author={Woojun Kim and Youngchul Sung},
      year={2023},
      eprint={2303.00912},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2303.00912}, 
}

% Parameter Sharing:-
% 1) "Agent-Driven Modular Network for Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning": This paper discusses a framework that balances parameter sharing and agent diversity dynamically, addressing the limitations of static parameter sharing methods in MARL environments
% 2) "Adaptive Parameter Sharing for Multi-agent Reinforcement Learning": This work presents a method that maps different types of agents to specific regions within a shared network, enhancing strategy diversity without increasing training parameters
% 3) "Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning": This study formalizes the concept of agent indication to extend parameter sharing to heterogeneous observation and action spaces, demonstrating methods that enable convergence to optimal policies
% 4) "DyPS: Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning for Spatio-Temporal Resource Allocation" by Jingwei Wang et al. (2024): This study proposes a dynamic parameter-sharing method in MARL, focusing on spatio-temporal resource allocation tasks. It aims to improve learning efficiency by dynamically adjusting parameter sharing among agents. 
% 5) "PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning" by Ke Zhang et al. (2024): This paper presents mechanisms for periodically sharing Q-value networks among agents during training to accelerate convergence in MARL, applying these approaches in the QMIX algorithm.
% 6) "Parameter Sharing is Surprisingly Useful for Multi-Agent Deep Reinforcement Learning" by J.K. Terry et al. (2020): This study examines the impact of parameter sharing in cooperative multi-agent reinforcement learning (MARL). The authors identify nonstationarity—where agents must continually adapt to each other's evolving policies—as a significant challenge in MARL. They propose that increasing centralization during training, through methods like parameter sharing, can mitigate this issue. Their experiments demonstrate that parameter sharing enhances performance across various MARL benchmarks. Additionally, they introduce techniques to extend parameter sharing to environments with heterogeneous agents, ensuring convergence to optimal policies.
% 7) "Celebrating Diversity in Shared Multi-Agent Reinforcement Learning" by Chenghao Li et al. (2021): This paper addresses the potential drawback of parameter sharing, where agents may develop similar behaviors, limiting coordination capacity. The authors propose an information-theoretical regularization method to maximize mutual information between agents' identities and their trajectories, promoting diverse and individualized behaviors. They also incorporate agent-specific modules within a shared neural network architecture, regularized by L1-norm, to balance shared learning with necessary diversity. Empirical results indicate that this approach achieves state-of-the-art performance in complex tasks such as Google Research Football and challenging StarCraft II micromanagement scenarios.
% 8) "Parameter Sharing for Heterogeneous Agents in Multi-Agent Reinforcement Learning": This work explores parameter sharing in environments with heterogeneous agents—those with different observation and action spaces. The authors formalize the concept of agent indication, where agent-specific identifiers are added to observations, enabling a shared policy network to differentiate between agents. They provide theoretical proofs that this method allows convergence to optimal policies even with heterogeneous agents. The study also introduces techniques like observation and action space padding to facilitate parameter sharing across diverse agent types, validated through extensive experiments. 

% Selective Experience Sharing:-
% 1) "Selective Multi-Agent Prioritized Experience Relay": This research introduces a selective experience sharing approach, allowing agents to share only relevant experiences, which improves learning efficiency compared to traditional methods 
% 2) "Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning": This research focuses on experience sharing among agents in MARL. The authors propose a framework where agents share a limited number of transitions during training, aiming to enhance learning efficiency. The approach allows for decentralized training with minimal communication overhead. Experiments demonstrate that selective experience sharing outperforms both non-sharing decentralized training and other state-of-the-art MARL algorithms, highlighting the benefits of sharing highly relevant experiences among agents.
% 3) "Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning" by Matthias Gerstgrasser et al. (2023): This paper introduces Selective Multi-Agent Prioritized Experience Relay, a method where agents share a limited number of significant experiences during training. The goal is to improve learning outcomes while maintaining decentralized training with limited communication. The authors show that sharing only a small number of highly relevant experiences leads to better performance compared to sharing all experiences or not sharing at all. The method is robust across various hyperparameters and different Deep Q-Network (DQN) variants.

% Pruning:-
% 1) "Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning": This paper proposes structured pruning techniques to enhance the representational capacity of joint policies in MARL without adding parameters

HAPPO & A2PO