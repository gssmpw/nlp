\section{Related Work}
\label{sec:related_work}

\paragraph{Parameter sharing (PS)} is a common strategy in MARL that reduces computational complexity by using a single policy or value network for all agents~\citep{terry2020revisiting}. However, standard PS often fails in heterogeneous environments where agents require diverse behaviors. Dynamic sharing methods~\citep{AdaPS, DynParamSharing} improve adaptability by assigning specific network modules based on predefined clusters, but they increase computational overhead, depend on role assumptions, and can introduce training instability—especially when agent roles change rapidly. In dynamic sharing methods, each agent's parameter subset can be significantly smaller than other policy baselines, making it unclear whether performance gaps stem from suboptimal selection or insufficient capacity. This scale discrepancy complicates direct comparisons with other parameter sharing approaches and is thus left out of the scope of this study. Techniques like agent identification~\citep{terry2020revisiting} or regularization-based methods~\citep{li2021CDS} attempt to differentiate agents within a shared network, but often lack sufficient representational capacity or add complexity and tuning burdens. In contrast, our approach embeds a \emph{low-rank structure} directly into shared parameters, inducing sparsity and enabling agent-specific specialization without requiring dynamic reconfiguration, clustering, or heavy regularization.


\paragraph{Selective Experience Sharing.}
Selective experience-sharing methods improve data efficiency by exchanging only relevant trajectories~\citep{SelectiveSharingExp, SharedExpAC}, reducing communication overhead and accelerating learning. However, they do not address policy expressiveness, as agents may still be constrained by a single shared model. In contrast, LoRASA operates at the parameter level, ensuring that even with fully shared transitions, low-rank offsets allow agents to develop specialized behaviors in an \(r\)-dimensional subspace. Thus, experience sharing enhances sample efficiency, while LoRASA enables representational flexibility, making them complementary rather than conflicting approaches.

\paragraph{Network Pruning} techniques~\citep{PSNetworkPruning} sparsify shared networks to lower resource usage. However, removing parameters outright risks discarding critical features needed by certain agents, especially in tasks requiring rare but crucial skills. Our work takes the opposite route: we \emph{add} low-rank modules to a shared backbone, preserving the base network and preventing irreversible performance degradation from over-pruning. This approach naturally balances expressiveness and efficiency by localizing agent-specific adaptations in small, learnable subspaces.

\paragraph{Non-Parameter Sharing (NPS)} policies (e.g., HAPPO~\citep{HAPPO}, A2PO~\citep{A2PO}) allow maximal specialization, but scale poorly in agent-heavy systems due to their linear growth in parameters and slower training due to re-learning of common behaviors. Despite their strong performance, these methods are often untenable for large populations of agents. In contrast, our low-rank approach approximates the benefits of NPS—i.e., agent-specific customization—while retaining the resource efficiency of a shared framework.

\paragraph{MARL as Multi-Task RL} methods, like ~\citep{wang2023multitaskmultiagentsharedlayers, omidshafiei2017deepdecentralizedmultitaskmultiagent, MultiTaskMARL, zhang2024hybridtrainingenhancedmultitask, PrioritizedTasKMining}, often aim to transfer knowledge such as shared decision-making modules, task representations, or agent-interactions across distinct tasks rather than to accommodate diverse roles within a single shared task. This makes them less suited for MARL scenarios where agents differ significantly but still collaborate on one global objective. In contrast, our work explicitly treats each agent as a separate “task”, applying parameter-efficient fine-tuning via low-rank adapters. Unlike approaches that only adapt output layers~\citep{MTL}, we modify internal layers as needed to capture nuanced agent behaviors without incurring the high overhead of duplicating entire networks.