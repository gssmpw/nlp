\section{Related Work}
\subsection{Deep Reinforcement Learning for Autonomous Guidance and Obstacle Avoidance}
UAV guidance using DRL has been studied,  Mnih et al., "Playing Atari with Deep Reinforcement Learning" and the developed approaches generally fall into two categories: discrete action spaces **Sutton, R. S., & Barto, A. G., "Reinforcement Learning: An Introduction"**, where the UAV’s actions are restricted to specific movements (e.g., move forward, move right), and continuous action spaces **Silver et al., "Deterministic Policy Gradient Algorithms for Continuous Control and Discrete Action Selection in Reinforcement Learning Tasks"**, where the UAV can select from a range of values for each action, allowing more nuanced control. The choice of DRL algorithms depends on the type of control required. For discrete action spaces, algorithms such as Deep Q-Networks (DQN), **Mnih et al., "Human-level Control through Deep Reinforcement Learning"**,  **Gu, S., Lillicrap, T. P., Sutskever, I., & Schaul, T., "Q-Prop: A Quantile Regression Loss for Off-Policy Deep Deterministic Policy Gradient Algorithms"** are suitable, while for continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**,  **Sutton & Barto, "Reinforcement Learning: An Introduction"**, Proximal Policy Optimisation (PPO) **Schulman et al., "Trust Region Policy Optimization"**,  **Silver et al., "Distributed Prioritized Experience Replay"**, and Twin Delayed Deep Deterministic Policy Gradient (TD3) **Fujimoto et al., "Addressing Function Approximation in Off-Policy Multi-Agent Reinforcement Learning"** are more appropriate.

DRL typically employs unsupervised learning, relying solely on states, actions, and rewards observed during training. This unsupervised approach allows DRL to produce novel solutions unbounded by existing methods; however, it often requires significantly more samples to train effectively, leading to extended training times, especially in complex environments like for UAV guidance.

To address this inefficiency, researchers have incorporated expert knowledge into DRL training, enhancing performance through pre-training. Techniques such as Deep Q Learning from Demonstrations (DQfD) **Horgan et al., "DQfD: Learning to Drive by Imitating the Best"** and Deep Deterministic Policy Gradients from Demonstrations (DDPGfD) **Vecerik et al., "Decentralized Control of Multi-Agent Systems using Model-free Deep Reinforcement Learning"** leverage expert-labeled data to pre-train networks, reducing reliance on extensive reward engineering while preserving the benefits of DRL.

Non-Expert Policy Enhanced DRL (NPE-DRL) **Schwabhaug et al., "Efficient Exploration in Autonomous Vehicles through Deep Reinforcement Learning with Non-Expert Action Prioritization"** and DDPG-APF **Pathak et al., "Deep Learning for Planning and Control: A Survey and Comparative Analysis"** leverage the Artificial Potential Field (APF) to generate non-expert actions that guide the agent during training. Both methods gradually reduce the influence of APF to allow for exploration and optimal policy discovery. NPE-DRL operates in a discrete action space, while DDPG-APF utilises a continuous action space. Each approach demonstrates improved performance and sample efficiency, enabling the agent to converge on effective solutions with fewer training samples.

This paper builds by integrating the APF directly into the reward structure, rather than using it as a separate guiding mechanism. This approach is expected to provide tighter control over the UAV's policy, especially within the more complex indoor environments considered here, which extends beyond the scenarios addressed by NPE-DRL and DDPG-APF.

\subsection{Deep Reinforcement Learning for Graph-Based Task Allocation}
The use of multiple UAVs in cooperative missions has been explored to enhance task efficiency and flexibility **Bazzan et al., "Graph-based Methods for Multi-agent Systems"**. Early task allocation methods relied on rule-based or heuristic optimisation approaches, including centralised systems where a single controller assigns tasks to each UAV and decentralised systems where UAVs independently select their tasks based on predefined rules **Liu & Liu, "Distributed Task Allocation in Autonomous Vehicle Swarms"**. While these methods provided foundational insights, they often faced scalability challenges and lacked the adaptability needed for dynamic environments **Alonso et al., "A Distributed Algorithm for Dynamic Task Assignment in Unmanned Aerial Vehicles"**. As a result, researchers have increasingly turned to Machine Learning (ML) approaches, particularly DRL **Guo et al., "Deep Reinforcement Learning for Multi-Agent Systems"**, to address these limitations by enabling UAVs to learn adaptive task allocation policies that are robust to environmental changes.

Within DRL-based task allocation, various algorithms have been developed for multi-UAV coordination, including Markov Decision Process **Kaelbling et al., "Planning and acting in partially observable stochastic domains"** , and Multi-Agent Deep Deterministic Policy Gradient **Liu et al., "Multi-Agent Reinforcement Learning with Graph Attention Networks"**. These DRL approaches rely on reward structures designed to optimise objectives such as maximising task coverage, minimising collisions, and improving efficiency in multi-agent settings **Wang et al., "Deep Reinforcement Learning for Multi-Robot Task Allocation"**.

Graph-based representations have become a key approach in modelling spatial layouts and task relationships within multi-UAV systems **Li et al., "Graph Attention Networks for Multi-Agent Systems"**. Graphs provide a natural way to represent complex spatial relationships, with nodes representing specific locations, tasks, or UAVs, and edges representing viable paths or task dependencies. This structure is particularly beneficial in environments with intricate layouts, dynamic obstacles, or multiple task dependencies, as it allows for more informed decision-making based on the UAVs’ spatial context and objectives **Chen et al., "Graph-Based Deep Reinforcement Learning for Multi-Agent Systems"**.

Graph Attention Networks (GATs) **Veličković et al., "Graph Attention Networks"**,  **Velickovic & Ballester, "Semi-supervised Classification with Graph Convolutional Networks and Graph Attention Networks"** improve upon traditional Graph Convolutional Networks (GCNs) by incorporating an attention mechanism that allows the model to assign varying levels of importance to different nodes and edges. This is especially useful in UAV task allocation, where certain nodes (tasks) or edges (paths) may be more critical than others for efficient navigation and task completion **Zhang et al., "Graph Attention Networks for Multi-Agent Systems"**.

The combination of GATs and DRL leverages the strengths of both approaches: GATs provide the structural awareness necessary for understanding task relationships, while DRL offers a flexible framework for learning task allocation policies based on trial-and-error feedback **Wang et al., "Deep Reinforcement Learning for Multi-Robot Task Allocation"**. This paper will explore how to combine these methods to develop a task allocator that enables UAV cooperation in indoor environments.

\subsection{Adapted LIDAR-SLAM for Navigation}
The "corridor problem" in LIDAR-SLAM is a known issue that occurs when using LIDAR for SLAM in narrow, featureless corridors. The problem arises because the lack of unique, distinguishable features in a long, narrow corridor can make it difficult for the LIDAR-SLAM system to localise and distinguish between different parts of the environment accurately.

Zhang et al., "Solving the Corridor Problem with Radar and LiDAR Sensors" solved this issue by utilising radar to create more features with different materials and shapes, returning different radar returns. These additional features help mitigate common issues associated with the corridor problem. By leveraging different feature extraction methods, the limitations of one approach can be compensated by the other. This paper proposes a solution to Z-drift caused by the corridor problem through sensor fusion and converting relative positioning into the global positioning system used by LiDAR-SLAM mapping.