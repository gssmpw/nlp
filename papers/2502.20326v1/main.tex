\documentclass[conference]{IEEEtran}







\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{array}


\usepackage{tikz}
\usepackage{url}





% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application}

\author{\IEEEauthorblockN{Thomas Hickling, Maxwell Hogan, Abdulla Tammam, and Nabil Aouf}
\IEEEauthorblockA{School of Science and Technology\\
City St George's University of London\\
London, United Kingdom\\
Email: Tom.Hickling, Maxwell.Hogan, Abdulla.Tammam, Nabil.Aouf@city.ac.uk}
}





\maketitle


\begin{abstract}
This paper proposes a holistic framework for autonomous guidance, navigation, and task distribution among multi-drone systems operating in Global Navigation Satellite System (GNSS)-denied indoor settings. We advocate for a Deep Reinforcement Learning (DRL)-based guidance mechanism, utilising the Twin Delayed Deep Deterministic Policy Gradient algorithm. To improve the efficiency of the training process, we incorporate an Artificial Potential Field (APF)-based reward structure, enabling the agent to refine its movements, thereby promoting smoother paths and enhanced obstacle avoidance in indoor contexts. Furthermore, we tackle the issue of task distribution among cooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). This GCN adeptly represents the interactions between drones and tasks, facilitating dynamic and real-time task allocation that reflects the current environmental conditions and the capabilities of the drones. Such an approach fosters effective coordination and collaboration among multiple drones during search and rescue operations or other exploratory endeavours. Lastly, to ensure precise odometry in environments lacking GNSS, we employ Light Detection And Ranging Simultaneous Localisation and Mapping complemented by a depth camera to mitigate the hallway problem. This integration offers robust localisation and mapping functionalities, thereby enhancing the system’s dependability in indoor navigation. The proposed multi-drone framework not only elevates individual navigation capabilities but also optimises coordinated task allocation in complex, obstacle-laden environments. Experimental evaluations conducted in a setup tailored to meet the requirements of the NATO Sapience Autonomous Cooperative Drone Competition demonstrate the efficacy of the proposed system, yielding outstanding results and culminating in a first-place finish in the 2024 Sapience competition.


{\bf Keywords:} DRL, Cooperative UAVs, DRL Task Allocation, GNSS-Denied, Sim-to-real, LIDAR-SLAM, Search and Rescue.

\end{abstract}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{arena.png}
    \caption{The arena built for the Sapience competition as seen by the eight cameras used for monitoring the UAVs}
    \label{fig:arena}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{floor_layout_2.png}
    \caption{The floor plan for the constructed building}
    \label{fig:layout}
\end{figure}

\section{Introduction}
The adoption of Uncrewed Aerial Vehicles (UAVs) has grown in critical applications such as search and rescue (SAR) operations due to their ability to access hard-to-reach areas and quickly provide real-time situational awareness, \cite{scientific_american_drones}. However, operating in environments denied by the Global Navigation Satellite System (GNSS), such as indoor settings, presents unique challenges for autonomous UAVs, which require the development of intelligent and robust navigation and task management strategies. In recent years, Deep Reinforcement Learning (DRL), has emerged as a promising approach to handling complex decision-making tasks in such scenarios, particularly for cooperative UAV systems \cite{electronics10090999}.

This paper presents the development and deployment of a cooperative drone system that adopts deep learning strategies to achieve autonomous navigation, guidance, and planning. This development is conducted within the NATO-funded Sapience program through an Autonomous Cooperative Drone Competition, \cite{sapience}, which promotes technological advancements as part of NATO’s humanitarian goals. The competition focuses on improving cooperative autonomous UAV capabilities in GNSS-denied environments for search and rescue applications. Teams were challenged with tasks such as mapping, object/person detection and localisation, and the delivery of medical aid. These tasks were to show the benefit of using autonomous systems to perform tasks such as medical aid delivery without subjecting operators to unnecessary risk. The arena created for these tasks can be seen in Fig. \ref{fig:arena} with its layout being shown in Fig. \ref{fig:layout}

The system developed for this competition employs two UAVs, each equipped with 13-inch propellers, two Intel RealSense D455f cameras \cite{IntelRealSenseD455f}, a Velodyne 16-line puck Light Detection and Ranging (LIDAR) sensor \cite{velodyne_vlp16_manual}, and an NVIDIA Jetson Orin Nano \cite{NVIDIAJetsonOrinNano2024}. The DRL-based guidance for each UAV was trained in a simulation environment using the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm \cite{fujimoto2018addressing} with AirSim \cite{airsim2017fsr}, a widely used UAV simulation platform.

A vital component of a cooperative drone system is how the agents cooperate. This paper presents a DRL-trained graph-based task allocation mechanism that optimises real-time coordination between multiple agents. This solution allows the two drones to effectively divide search areas and dynamically adjust their strategies based on environmental feedback, leading to efficient and timely exploration in SAR scenarios. The task allocator is trained alongside the DRL guidance model, enabling the system to adapt to various indoor challenges and obstacles.

A form of odometry is needed to fly in an indoor GNSS-denied environment. Due to the constructed environment, the traditional Visual Simultaneous and Mapping (VSLAM) was unsuitable due to a lack of features for mapping, \cite{tourani2022vslam}. This led to LIDAR being explored in the form of LIDAR Simultaneous and Mapping (LIDAR-SLAM), \cite{grisetti2005improving} and then adapted to deal with narrow corridors, which cause drifting in the vertical coordinates, to allow accurate positioning.

This paper aims to contribute to the development of autonomous drone systems for search and rescue operations in environments denied by GNSS by developing and deploying cooperative drones that integrate advanced navigation, mapping, guidance, and task allocation solutions. The presented system combines LIDAR-SLAM for robust localisation and mapping, DRL-trained guidance for autonomous navigation, and a Graph Attention Network (GAT)-based task allocator for efficient multi-UAV coordination. By demonstrating these capabilities in the context of the NATO SAPIENCE Autonomous Cooperative Drone Competition, this work highlights the potential of UAV technologies to revolutionise search and rescue missions, addressing critical challenges such as agent cooperation, real-time navigation, obstacle avoidance, and efficient resource allocation, while aiming to improve response times, operational efficiency, and safety in life-saving scenarios.

\section{Related Work}
\subsection{Deep Reinforcement Learning for Autonomous Guidance and Obstacle Avoidance}
UAV guidance using DRL has been studied,  \cite{azar2021drone} and the developed approaches generally fall into two categories: discrete action spaces \cite{zhu2024uav}, where the UAV’s actions are restricted to specific movements (e.g., move forward, move right), and continuous action spaces \cite{hu2022obstacle}, where the UAV can select from a range of values for each action, allowing more nuanced control. The choice of DRL algorithms depends on the type of control required. For discrete action spaces, algorithms such as Deep Q-Networks (DQN), \cite{mnih2015human}, \cite{zhu2024uav} are suitable, while for continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) \cite{lillicrap2015continuous}, \cite{hickling2023robust}, Proximal Policy Optimisation (PPO) \cite{schulman2017proximal}, \cite{hu2022obstacle}, and Twin Delayed Deep Deterministic Policy Gradient (TD3) \cite{fujimoto2018addressing}, \cite{he2021explainable} are more appropriate.

DRL typically employs unsupervised learning, relying solely on states, actions, and rewards observed during training. This unsupervised approach allows DRL to produce novel solutions unbounded by existing methods; however, it often requires significantly more samples to train effectively, leading to extended training times, especially in complex environments like for UAV guidance.

To address this inefficiency, researchers have incorporated expert knowledge into DRL training, enhancing performance through pre-training. Techniques such as Deep Q Learning from Demonstrations (DQfD) \cite{hester2018deep} and Deep Deterministic Policy Gradients from Demonstrations (DDPGfD) \cite{vecerik2017leveraging} leverage expert-labeled data to pre-train networks, reducing reliance on extensive reward engineering while preserving the benefits of DRL.

Non-Expert Policy Enhanced DRL (NPE-DRL) \cite{zhang2024npe} and DDPG-APF \cite{hickling2023robust} leverage the Artificial Potential Field (APF) to generate non-expert actions that guide the agent during training. Both methods gradually reduce the influence of APF to allow for exploration and optimal policy discovery. NPE-DRL operates in a discrete action space, while DDPG-APF utilises a continuous action space. Each approach demonstrates improved performance and sample efficiency, enabling the agent to converge on effective solutions with fewer training samples.

This paper builds by integrating the APF directly into the reward structure, rather than using it as a separate guiding mechanism. This approach is expected to provide tighter control over the UAV's policy, especially within the more complex indoor environments considered here, which extends beyond the scenarios addressed by NPE-DRL and DDPG-APF.

\subsection{Deep Reinforcement Learning for Graph-Based Task Allocation}
The use of multiple UAVs in cooperative missions has been explored to enhance task efficiency and flexibility \cite{sargolzaei2020control}. Early task allocation methods relied on rule-based or heuristic optimisation approaches, including centralised systems where a single controller assigns tasks to each UAV and decentralised systems where UAVs independently select their tasks based on predefined rules \cite{zhao2015heuristic}. While these methods provided foundational insights, they often faced scalability challenges and lacked the adaptability needed for dynamic environments \cite{cui2019multi}. As a result, researchers have increasingly turned to Machine Learning (ML) approaches, particularly DRL \cite{mao2024dl}, to address these limitations by enabling UAVs to learn adaptive task allocation policies that are robust to environmental changes.

Within DRL-based task allocation, various algorithms have been developed for multi-UAV coordination, including Markov Decision Process \cite{mao2024dl}, and Multi-Agent Deep Deterministic Policy Gradient \cite{lowe2017multi}. These DRL approaches rely on reward structures designed to optimise objectives such as maximising task coverage, minimising collisions, and improving efficiency in multi-agent settings. The adaptability of DRL allows UAVs to dynamically allocate tasks in response to changes in mission objectives or environmental constraints, providing advantages over classical optimisation techniques in complex, multi-agent scenarios \cite{mao2024dl}.

Graph-based representations have become a key approach in modelling spatial layouts and task relationships within multi-UAV systems \cite{GNN-EnhancedDRL}. Graphs provide a natural way to represent complex spatial relationships, with nodes representing specific locations, tasks, or UAVs, and edges representing viable paths or task dependencies. This structure is particularly beneficial in environments with intricate layouts, dynamic obstacles, or multiple task dependencies, as it allows for more informed decision-making based on the UAVs’ spatial context and objectives.

Graph Attention Networks (GATs) \cite{velickovic2017graph}, \cite{GA-DRL} improve upon traditional Graph Convolutional Networks (GCNs) by incorporating an attention mechanism that allows the model to assign varying levels of importance to different nodes and edges. This is especially useful in UAV task allocation, where certain nodes (tasks) or edges (paths) may be more critical than others for efficient navigation and task completion. By selectively focusing on the most relevant nodes, GATs enable prioritisation of tasks in real-time, enhancing coordination and efficiency in multi-agent systems \cite{shao2021graph}.

The combination of GATs and DRL leverages the strengths of both approaches: GATs provide the structural awareness necessary for understanding task relationships, while DRL offers a flexible framework for learning task allocation policies based on trial-and-error feedback. This paper will explore how to combine these methods to develop a task allocator that enables UAV cooperation in indoor environments.

\subsection{Adapted LIDAR-SLAM for Navigation}
The "corridor problem" in LIDAR-SLAM is a known issue that occurs when using LIDAR for SLAM in narrow, featureless corridors. The problem arises because the lack of unique, distinguishable features in a long, narrow corridor can make it difficult for the LIDAR-SLAM system to localise and distinguish between different parts of the environment accurately.

Zhang et al. \cite{radarlidar} solved this issue by utilising radar to create more features with different materials and shapes, returning different radar returns. These additional features help mitigate common issues associated with the corridor problem. By leveraging different feature extraction methods, the limitations of one approach can be compensated by the other. This paper proposes a solution to Z-drift caused by the corridor problem through sensor fusion and converting relative positioning into the global positioning system used by LiDAR-SLAM mapping.

\section{Autonomous Guidance in UAVs}

Autonomous guidance for UAVs is traditionally facilitated by algorithms such as Dijkstra, A*, and APF. Recent advances in ML, particularly DRL, have enhanced UAVs' capabilities to make complex real-time decisions in search and rescue, mapping, and inspection.

\subsection{Deep Reinforcement Learning}

DRL combines reinforcement learning (RL) principles \cite{sutton2018reinforcement} with Deep Neural Networks (DNNs) to approximate policies or value functions, enabling agents to operate in large state-action spaces. In DRL, an agent interacts with an environment, selects actions, and receives feedback in the form of rewards, learning an optimal policy to maximise cumulative reward. This approach is particularly valuable in complex environments unsuitable for traditional tabular methods as the large state-action spaces take unfeasible amounts of computer memory.

Key components of DRL include:
\begin{itemize}
    \item \textbf{Agent:} The decision-maker in the environment.
    \item \textbf{Environment:} The external system with which the agent interacts.
    \item \textbf{State, Action, Reward:} Indicators of the environment’s current status, possible moves, and feedback for success.
    \item \textbf{Policy:} The strategy the agent uses to decide actions.
\end{itemize}

Popular DRL algorithms, such as DQN \cite{mnih2015human}, PPO \cite{schulman2017proximal}, and TD3 \cite{fujimoto2018addressing}, each address issues related to stability, exploration, and sample efficiency.

\subsubsection{Twin Delayed Deep Deterministic Policy Gradient}

TD3 \cite{fujimoto2018addressing} builds on the DDPG \cite{lillicrap2015continuous} algorithm to improve stability in continuous control tasks, addressing common issues like overestimation bias. TD3 follows an actor-critic structure where the \textit{actor} network outputs actions and the \textit{critic} networks evaluate the expected returns. TD3 introduces three core innovations:

\begin{enumerate}
    \item \textbf{Clipped Double Q-Learning:} Two Q-networks are used, with updates based on the minimum Q-value, reducing overestimation.
    \item \textbf{Delayed Policy Updates:} The actor is updated less frequently than the critics, allowing the critics to stabilise and prevent premature policy updates.
    \item \textbf{Target Policy Smoothing:} This is a regularisation strategy where the target actions have a small noise signal (smoothing) added before being passed to the target critic networks. This variation prevents overfitting of the Q-networks to small peaks in the value estimate. This smoothing of the target actions leads to a lower variation in the Q targets and lowers the chance of functional approximation error.
\end{enumerate}

The training process involves the agent observing the environment, selecting actions through the actor, receiving rewards, and updating the Q-networks and actor based on experiences stored in a replay buffer. These innovations make TD3 effective for precise continuous control in applications like autonomous guidance and cooperative UAV systems.

\subsubsection{Artificial Potential Field Based Reward Structure}

The design of the reward system is vital to successfully training a DNN using DRL techniques, \cite{ng1999policy}. Small changes in how the reward is allocated can significantly change the training result. In DRL training, rewards are typically shaped by specific actions and states. Reward shaping is complicated when applied to training for complex goals and environments. Therefore, looking for novel reward-shaping methods to maximise DRL's promise is crucial.

The Artificial Potential Field (APF) \cite{khatib1986real} is a well-known technique used in robotics for path planning and obstacle avoidance. In the APF approach, the environment is modelled as a potential field where attractive and repulsive forces guide the agent (in this case, a drone) towards the goal while avoiding obstacles. The goal generates the attractive potential, which pulls the agent towards it, while the repulsive potential is generated by obstacles pushing the agent away. 

The potential field is defined as a scalar function over the space, and the agent moves in the direction that minimises this potential. Mathematically, the total potential \( U \) is typically composed of an attractive potential \( U_{\text{att}} \) and a repulsive potential \( U_{\text{rep}} \):

\begin{equation}
U(q) = U_{\text{att}}(q) + U_{\text{rep}}(q),
\end{equation}
where \( q \) represents the position of the drone. The attractive potential is often designed to decrease as the drone approaches the goal, and the repulsive potential increases as the drone approaches obstacles. 

The \textbf{resultant force} acting on the drone is derived from the gradient of the total potential function:

\begin{equation}
F(q) = -\nabla U(q),
\end{equation}
which gives the direction of movement for the drone. This force represents the \textbf{optimal movement direction}, balancing attraction to the goal and repulsion from obstacles.

In a RL context, we can leverage the optimal movement provided by the APF to design an effective reward function for training a drone in a DRL framework. Since the APF provides a clear direction for the drone to move in, we can compare the action chosen by the drone to this optimal movement and use the similarity between the two as a measure of performance.

Let \( a_{\text{opt}} \) represent the \textbf{optimal action} (direction) derived from the APF, and \( a_{\text{agent}} \) represent the action taken by the drone in a given state. The reward function can be designed to encourage the agent to choose actions that align with the optimal direction. One way to measure this alignment is by using the cosine similarity between the two vectors:

\begin{equation}
R = \frac{a_{\text{opt}} \cdot a_{\text{agent}}}{\|a_{\text{opt}}\| \|a_{\text{agent}}\|},
\end{equation}
where \( \cdot \) denotes the dot product and \( \| \cdot \| \) represents the magnitude of the vectors. The reward \( R \) is maximised when the drone's chosen action aligns perfectly with the optimal movement provided by the APF, and it decreases as the angle between the two vectors increases.

Alternatively, a simpler formulation could penalise the agent based on the difference between the actions:

\begin{equation}
R = -\| a_{\text{opt}} - a_{\text{agent}} \|,
\end{equation}
where the reward becomes smaller as the chosen action deviates further from the optimal direction.

\subsection{Advantages of Using APF in Reward Function}

This approach offers several advantages:
\begin{itemize}
    \item \textbf{Guided Learning:} By using the APF as a reference for optimal movement, the agent can learn more efficiently as it is guided towards the correct actions early in training.
    \item \textbf{Smooth Trajectories:} APF typically generates smooth, collision-free paths, ensuring the agent learns to navigate safely in complex environments.
    \item \textbf{Hybrid Approach:} Combining DRL with APF leverages the strengths of classical control (APF) and learning-based approaches (DRL), providing a balance between predefined optimal behaviour and the adaptability of learning.
\end{itemize}

Thus, using APF as a reference for movement and designing a reward function around the similarity of the agent's actions to this reference can accelerate the learning process and improve the overall drone guidance performance, especially in obstacle-rich environments.


\section{Task Allocation}
Task allocation in robotics refers to assigning tasks to individual robots in a multi-robot system to achieve collective goals efficiently, \cite{korsah2013comprehensive}. This is a crucial problem, particularly in complex environments, where robots must collaborate, explore, or cover areas while minimising resource usage and avoiding conflicts. 

\subsection{Deep Reinforcement Learning for Task Allocation}
Neural networks, particularly DRL and other ML techniques, have increasingly been applied to solve task allocation problems in multi-robot systems \cite{gronauer2022multi}. They can learn complex patterns and adapt to dynamic, uncertain environments, making them well-suited for real-time task allocation.

In DRL-based task allocation, the task assignment is framed as a decision-making problem where the agent (drone or central controller) learns to allocate tasks through trial and error. A neural network is trained to predict which task should be assigned to which drone to maximise the system's overall performance, such as reducing task completion time or minimising energy usage.

The state can include the positions and states of drones, the state of the tasks and their positions, and any other sensor data. The action is then the orders for each agent in the system to perform either for the next step or string a series of tasks to be done. The reward structure can be used to guide the network to optimise for tasks to be selected in a way that tasks are not repeated and that the shortest path is taken to do all tasks.

\subsection{Overview of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs)}

\subsubsection{Graph Convolutional Networks (GCNs)}

Graph Convolutional Networks (GCNs) \cite{kipf2016semi} are a type of neural network specifically designed to operate on graph-structured data. GCNs extend the concept of convolution from regular grid data, such as images, to irregular graph data, allowing for the processing of data where relationships between entities are naturally represented as graphs. 

In GCNs, each node's representation is updated by aggregating information from its neighbouring nodes. The layer-wise propagation rule for a GCN can be written as:

\begin{equation}
H^{(l+1)} = \sigma \left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)} \right),
\end{equation}

Where:
\begin{itemize}
    \item \( H^{(l)} \) is the matrix of node features at layer \( l \).
    \item \( \tilde{A} = A + I \) is the adjacency matrix with added self-loops, where \( A \) is the adjacency matrix of the graph, and \( I \) is the identity matrix.
    \item \( \tilde{D} \) is the diagonal degree matrix of \( \tilde{A} \).
    \item \( W^{(l)} \) is the trainable weight matrix at layer \( l \).
    \item \( \sigma \) is an activation function, such as ReLU.
\end{itemize}

The GCN learns to propagate and combine features from neighbouring nodes, effectively capturing the structural information of the graph. However, GCNs apply uniform weighting to the neighbouring nodes, treating all neighbours equally during aggregation. This limitation can reduce the model's ability to focus on the most important nodes in complex graphs, potentially leading to suboptimal performance in certain scenarios.

\subsubsection{Graph Attention Networks (GATs)}

Graph Attention Networks (GATs) \cite{velickovic2017graph} introduce an attention mechanism to address the limitations of GCNs by allowing the model to learn the relative importance of neighbouring nodes. Instead of treating all neighbours equally, GATs compute attention coefficients for each edge, enabling the network to focus on more relevant neighbours when aggregating information.

The attention mechanism in a GAT layer computes attention coefficients \( \alpha_{ij} \) between node \( i \) and its neighboring nodes \( j \) as follows:

\begin{equation}
\alpha_{ij} = \frac{\exp \left( \text{LeakyReLU} \left( \mathbf{a}^T \left[ W \mathbf{h}_i \| W \mathbf{h}_j \right] \right) \right)}{\sum_{k \in \mathcal{N}(i)} \exp \left( \text{LeakyReLU} \left( \mathbf{a}^T \left[ W \mathbf{h}_i \| W \mathbf{h}_k \right] \right) \right)},
\end{equation}

where:
\begin{itemize}
    \item \( \alpha_{ij} \) represents the normalised attention coefficient between nodes \( i \) and \( j \).
    \item \( \mathbf{h}_i \) and \( \mathbf{h}_j \) are the feature vectors of nodes \( i \) and \( j \).
    \item \( W \) is a shared weight matrix.
    \item \( \mathbf{a} \) is a learnable vector for computing attention.
    \item \( \| \) denotes concatenation.
    \item \( \mathcal{N}(i) \) represents the neighbourhood of node \( i \).
\end{itemize}

The resulting attention coefficients \( \alpha_{ij} \) are used to compute a weighted sum of the neighbours' features, allowing the model to focus more on important connections. This attention-based aggregation results in node representations that better capture the varying importance of neighbouring nodes. Unlike GCNs, which
rely on a fixed adjacency matrix for propagation, GATs
do not require a predefined structure and can operate on
dynamically changing graphs. This makes GATs more
versatile for real-time applications where the graph topology may evolve over time.

\section{Global Navigation Satellite System Denied Odometry}

GNSS-denied odometry estimates a vehicle’s position, velocity, and orientation in environments where GNSS signals are unavailable or unreliable, such as indoor, underground, or dense urban areas \cite{mohamed2019survey}. For UAVs, this capability is essential for autonomous navigation, enabling precise manoeuvring and obstacle avoidance in confined spaces. GNSS-denied odometry is particularly valuable in applications requiring real-time localisation and mapping, such as infrastructure inspection, search and rescue, and disaster response.

Several methods are commonly used for GNSS-denied odometry. Visual odometry (VO) \cite{sahiliVSLAM} relies on camera data to track motion by analysing changes in visual features between frames, though its accuracy can diminish in feature-poor or low-light areas. IMU-based odometry \cite{mourikis2007multi} uses accelerometers and gyroscopes to estimate motion through changes in acceleration and rotational velocity, though it accumulates error (drift) over time, making it more suited for short-term estimates and often requires sensor fusion for long-term accuracy.

\subsection{LIDAR-SLAM}

LIDAR-SLAM \cite{grisetti2005improving, magnusson2009three, segal2009generalized} addresses limitations in feature-poor or low-visibility environments by using laser sensors to build a 3D map while simultaneously localising the UAV. This process involves scanning the surroundings to generate a 3D point cloud, which is continuously updated through scan matching as the UAV moves. Loop closure, a crucial aspect of SLAM, helps correct accumulated drift by recognising previously mapped areas, thus improving the consistency of maps and localisation over time. Additionally, graph-based optimisation refines the map and position estimates, reducing errors.

\subsubsection{Challenges in Height (Altitude) Estimation in Enclosed Environments}

Despite its advantages, LIDAR-SLAM faces challenges in enclosed spaces. Limited fields of view can reduce the data quality in tight areas, while feature-poor settings (e.g., long corridors) make it difficult to differentiate between similar-looking sections, leading to localisation errors \cite{li2024laser}. Drift remains a problem without frequent loop closures, and reflective surfaces like glass or metal can distort LIDAR measurements. Operating close to walls or obstacles may also cause incomplete scans, highlighting the need for sensor fusion with IMUs or visual odometry to enhance mapping accuracy in complex environments.


\section{Methodology}
In this work, key technologies such as LIDAR-SLAM for real-time mapping and localisation, a DRL-based guidance AI for autonomous navigation, and a Graph Attention Network (GAT)-based task allocator for multi-UAV coordination were leveraged to develop a cooperative drone system for indoor SAR operations. The SAPIENCE competition looks to improve technologies like those mentioned above to enable UAVs to operate autonomously, avoid obstacles, and collaborate effectively to cover search areas, ensuring efficient exploration and critical aid delivery in environments where human intervention may be hazardous or infeasible.



\subsection{UAV and Sensor Suite}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        % Place the image - UPDATE PATH
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=1.0\linewidth]{IMG_4713.png}};
        
        % Define the coordinates of the rectangles, adjust (x,y) coordinates accordingly
        % First rectangle
        \draw[red, thick] (3.5, 2.3) rectangle (4.7, 2.9); % Example coordinates for a rectangle
        
        % Second rectangle
        \draw[blue, thick] (4.8, 1.2) rectangle (5.75, 1.9); % Another example rectangle

        % Third rectangle
        \draw[green, thick] (4.3, 3.4) rectangle (5.7, 4.1); % Another example rectangle

        % Add more rectangles or shapes if needed
        % \draw[green, thick] (x1,y1) rectangle (x2,y2); % Customize the coordinates

        % Optional: Add labels
        \node[text=red] at (2.3, 2.5) {RGBD camera};
        \node[text=blue] at (5.3, 0.9) {LiDAR};
        \node[text=green] at (2.5, 3.7) {Companion Processor};
        

    \end{tikzpicture}
    \caption{City University's Autonomous Drone.}
    \label{fig:Drone}
\end{figure}

Figure \ref{fig:Drone}, displays the drone developed for this work with annotations to show the additional equipment that was used to provide the platform with additional processing and sensing capabilities. The primary sensors include a Velodyne 16-line puck LiDAR \cite{velodyne_vlp16_manual} and an Intel RealSense D455f RGB-Depth camera \cite{IntelRealSenseD455f}. The companion processor is the NVIDIA Jetson Orin Nano \cite{NVIDIAJetsonOrinNano2024}, equipped with an NVIDIA Ampere GPU featuring 1024 CUDA cores and 32 Tensor Cores. This enables the processor to run advanced deep-learning algorithms and perform GPU-accelerated tasks efficiently. The systems of operation were developed using Isaac ROS \cite{NVIDIAIsaacROS2024}, which enabled the configuration of the sensors, interchange of commands to the flight controller, and embedding of the guidance and navigation algorithms developed in this work. In addition, this package also facilitated inter-process communication that accelerated prototyping and development without compromising real-time performance. 

Real-time positioning and mapping are made by integrating the lidarslam\_ros2 \cite{lidarslam_ros2} into the system. This provided the system with accurate positioning, including a heading, in order to undertake its assigned tasks. The difficulty in deploying the LiDAR-SLAM algorithm that is made available in this package is that it is susceptible to drift in the predicted height, given the absence of features. 

\subsection{LIDAR-SLAM Altitude Fix}

The downward-facing Red, Green, Blue, and Depth (RGBD) camera is utilised to correct for the predicted height drift. First, the median depth was calculated from five equidistance areas in the depth image. One of these was located in the centre of the image, and the other four were in the centre of each quadrant. The final depth was measured as the median of the five values and injected into the control loop. 

The environment in which the drone was expected to operate was mostly flat, with multiple levels. To detect a change in level, a control loop was devised that would compare the vertical velocity measured by the flight controller's Inertial Measurement Unit (IMU) with the calculated velocity from concurrent depth measurements from the depth camera. Any significant deviations between these two values indicate a change in level, and the calculated relative depth is updated to reflect this.

\subsection{Twin Delayed deep Deterministic Policy Gradient}
The TD3 scheme, as previously described, will be utilised to train the two DNNs responsible for providing guidance and task allocation. This scheme enhances the DDPG approach by incorporating three core innovations. The first is an additional critic network for Q-learning. The Q values generated by the two critic networks can be compared, allowing for the utilisation of the minimum Q value during the training process. This methodology effectively constrains the updates applied to the actor network, thereby mitigating the risk of overestimation. The scheme also uses delayed policy updates to stabilise critics before they can update the actor, preventing premature updates. The final core innovation is target policy smoothing, where noise is added to target actions to improve exploration and reduce overfitting.

Additional techniques were used to improve the training process for this implementation. The first is a prioritised experience replay, which ranks the experience's quality, consisting of the state, next state, action, reward, and done. When the mini-batch is selected from the memory buffer, the random selection is weighted to be more likely to pick higher learning potential experiences. By focusing the learning using experiences that the agent found surprising or had significant errors, the agent will converge quicker and more effectively \cite{8122622}. The TD error defines the learning potential, which is calculated using one of the critic networks. Secondly, to calculate the loss of the critic networks, the Huber loss was used rather than the usual mean squared error loss to protect the training from outliers.

The next sections will describe the architecture that will be used to make the decisions needed for the proposed tasks. In the TD3 scheme shown in Fig. \ref{fig:td3}, these networks play the part of the actor networks. The critic networks are the same as the actor networks but include a branch that inputs the actor's output into the concatenation layer. Unlike the actor networks, the final output is just one Q-value. The target networks are copies of the main networks that stabilise the training process.

\subsection{Guidance Deep Neural Network Architecture}
The proposed DNN architecture for the actor network for the UAV perception component of our guidance TD3 DRL scheme integrates three input branches, each processing a different type of perception sensory data—depth images, LIDAR scans, and positional data—before combining them to predict the UAV's motion commands to navigate complex environments.

\subsubsection{Branch 1: Depth Image Processing}

The first branch processes depth images captured from the UAV's onboard camera. The depth image is passed through 8 convolutional layers \cite{lecun1998gradient}, each using a LeakyReLU \cite{maas2013rectifier} activation function and batch normalisation \cite{ioffe2015batch} to stabilise the training process and improve convergence. The convolutional layers are divided into four blocks, each ending with a max-pooling operation to reduce the spatial dimensions and retain important features while providing translational invariance. The output of this branch is then flattened and passed through two fully connected (linear) layers, reducing the feature vector to a size of 128. This vector is further transformed using a tanh \cite{fan2000extended} activation function, preparing it for concatenation with the outputs from the other branches.

\subsubsection{Branch 2: LIDAR Scan Processing}

The second branch processes a 1D LIDAR scan, which provides a flat, radial representation of the surrounding obstacles. This input is passed through 5 one-dimensional convolutional layers, also using LeakyReLU activation and batch normalisation. The layers are organised into three blocks, each reducing the dimensionality of the input data while extracting spatial features relevant to obstacle proximity and distribution around the UAV. A final linear layer compresses the resulting feature map into a 128-dimensional vector, followed by a tanh activation, aligning the output with that of the first branch.

\subsubsection{Branch 3: Positional Data Processing}

The third branch takes a vector containing various positional parameters: the distance and bearing to the goal, the difference in altitude between the UAV and the goal, the relative height of the UAV, and the previous actions in the x, z, and yaw directions. This input is processed through a single fully connected layer with a tanh activation, resulting in a 128-dimensional vector. This approach allows the network to incorporate dynamic task-specific parameters directly, aiding the guidance decision-making process.

\subsubsection{Fusion and Output Layer}

The outputs from the three branches are concatenated into a single 384-dimensional vector. This combined representation is then passed through a series of 5 fully connected layers with LeakyReLU activations. These layers progressively refine the fused features, enabling the network to learn complex relationships between the depth, LIDAR, and positional information. A final linear layer with a tanh activation function outputs the predicted control commands, including the x-velocity, z-velocity, and yaw speed of the UAV. The tanh activation ensures that the output values remain bounded between -1 and 1, which is often desirable for controlling the UAV's speed and orientation in a stable manner.

\subsubsection{Discussion of Architectural Choices}
The use of convolutional layers in the depth image and LIDAR branches allows the network to capture spatial hierarchies and patterns within the sensor data, which is crucial for understanding the environment's structure. Batch normalisation aids in stabilising training by normalising the inputs of each layer, reducing internal covariate shifts. The LeakyReLU activation mitigates the risk of vanishing gradients in the early layers, ensuring more efficient learning. Tanh activation in the final layers helps ensure smooth control outputs, which is critical for the stability of UAV motion.

This multi-branch design enables the UAV to integrate depth-based obstacle detection, LIDAR-based spatial awareness, and task-specific positional data, resulting in a robust perception/guidance strategy suitable for GNSS-denied indoor environments.

The layout of the network can be seen in Fig. \ref{fig:navmodel}. The three separate branches that process the three inputs to the model are shown with the four 2D convolutional blocks, the three 1D convolutional blocks, and the fully connected layers for the data input. The layout of the FCN can be seen as combining and processing all the data together to produce the commands for the UAVs.

This network is trained using the TD3 algorithm shown in Fig. \ref{fig:td3}, where the network architecture shown in Fig. \ref{fig:navmodel} is used as the actor and target actor networks. The critic networks are the same as the actor networks but add the action produced to the concatenation layer and produce a single Q-value. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{navmodel.png}
    \caption{The architecture for the guidance AIs actor network}
    \label{fig:navmodel}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{td3-layout.png}
    \caption{The TD3 Algorithm Architecture used for training the guidance AI}
    \label{fig:td3}
\end{figure}

\subsection{Reward Function for the Guidance DRL}
The APF approach guides the UAV towards the target while avoiding obstacles by generating a set of forces that determine the desired actions for forward movement (\(x\)-velocity), turning (yaw speed), and altitude adjustment (\(z\)-velocity). The optimal actions are defined based on the distance to obstacles and the relative position of the target.

\subsubsection{Attractive Forces Towards the Target}

When the UAV is more than 1 meter away from any obstacles, the optimal action is determined by attractive forces that guide the UAV towards the target position. These forces are calculated as follows:

\begin{align}
\text{x}_{vel} &= \text{clip}(\cos(\theta), 0.0, 1.0), \\
\omega &= \text{clip}\left(\theta \cdot \frac{3}{\pi}, -1.0, 1.0\right), \\
\text{z}_{vel} &= \text{clip}\left(2 \cdot (\text{target}_z - \text{UAV}_z), -1.0, 1.0\right).
\end{align}

\noindent where:
\begin{itemize}
    \item \(\theta\) is the heading angle between the UAV and the target.
    \item \(\text{target}_z\) and \(\text{position}_z\) are the heights of the target and the UAV, respectively.
    \item \(\text{clip}(x, a, b)\) limits \(x\) to the range \([a, b]\).
\end{itemize}

The term \(\cos(\theta)\) ensures that the UAV moves faster when directly facing the target, with a movement restricted to forward directions due to the forward-facing depth camera. The yaw speed adjustment \(\text{force}_2\) scales the heading error, with the multiplier of 3 enhancing responsiveness, allowing for faster alignment with the target direction. The altitude adjustment \(\text{force}_3\) corrects the UAV's height relative to the target, adjusting at twice the rate of the height difference.

\subsubsection{Repulsive Forces for Obstacle Avoidance}

When the UAV is within 1 meter of an obstacle, the behaviour shifts to a repulsive mode, where forces are applied to move the UAV away from nearby obstacles and adjust its orientation. The repulsive forces are calculated based on the relative angle to the obstacle, \(\theta_{\text{obs}}\), which represents the bearing of the obstacle with respect to the UAV's forward direction.

The repulsive force in the forward direction (\(x\)-velocity) is calculated as:
\begin{equation}
\text{x}_{vel} = -0.5 \cdot \cos(\theta_{\text{obs}}),
\end{equation}
where \(\theta_{\text{obs}}\) is the angle to the closest detected obstacle. This formulation ensures that the UAV moves away from obstacles, with the negative sign indicating a backward movement. The cosine term reduces the backward speed when the obstacle is to the sides of the UAV, prioritising movement directly away from obstacles positioned directly in front.

The repulsive adjustment to the yaw speed, \(\omega(\theta_{\text{obs}})\), is designed to turn the UAV away from obstacles, depending on their relative position. It is defined as:
\begin{equation}
\omega(\theta_{\text{obs}}) =
\begin{cases}
0 & \text{if } \theta_{\text{obs}} = -\frac{\pi}{2}, \\
0.5 + \frac{\theta_{\text{obs}}}{\pi} & \text{if } -\frac{\pi}{2} < \theta_{\text{obs}} < 0, \\
-0.5 + \frac{\theta_{\text{obs}}}{\pi} & \text{if } 0 < \theta_{\text{obs}} < \frac{\pi}{2}, \\
0 & \text{if } \theta_{\text{obs}} = \frac{\pi}{2}.
\end{cases}
\label{eq:yaw_repulsive}
\end{equation}

This piecewise function ensures that the UAV adjusts its yaw speed based on the angle of the detected obstacle:
\begin{itemize}
    \item For obstacles directly to the left (\(\theta_{\text{obs}} = -\frac{\pi}{2}\)) or right (\(\theta_{\text{obs}} = \frac{\pi}{2}\)), no yaw adjustment is applied (\(\omega = 0\)).
    \item When the obstacle is slightly to the left (\(-\frac{\pi}{2} < \theta_{\text{obs}} < 0\)), a positive yaw adjustment is applied to turn the UAV right, away from the obstacle.
    \item For obstacles slightly to the right (\(0 < \theta_{\text{obs}} < \frac{\pi}{2}\)), a negative yaw adjustment is applied to turn the UAV left, away from the obstacle.
    \item The constants \(\pm 0.5\) provide a base turning speed, and the term \(\frac{\theta_{\text{obs}}}{\pi}\) adjusts the turning rate based on the angle, creating a smooth transition in yaw speed as the obstacle's relative position changes.
\end{itemize}

The repulsive force in the vertical direction (\(z\)-velocity) is calculated similarly to during the attraction periods of flight, shown in equation 9. These repulsive actions ensure the optimal action for the UAV to safely navigate away from obstacles is biased in the training. Combining backward movement with appropriate turning adjustments to maintain a safe distance from nearby objects.

\subsubsection{Calculating the reward} 
The reward function is designed to incentivise the UAV to follow the optimal actions closely while penalising collisions and rewarding goal achievement. The primary reward is based on the difference between the actual action of the UAV and the optimal action from the APF. The reward \(R\) is calculated as:

\begin{equation}
R = 1 - \sum (\text{action}_i - \text{action}_{\text{opt},i})^2
\end{equation}

where \(\text{action}_i\) represents the UAV’s action in the \(i\)-th dimension (e.g., \(x\)-velocity, yaw speed, and \(z\)-velocity), and \(\text{action}_{\text{opt},i}\) is the corresponding optimal action. Squaring the difference ensures all deviations are positive, with larger deviations penalised more severely. The reward drops off quickly as the UAV moves away from the optimal action, encouraging it to maintain alignment with the optimal path.

The reward is clipped to keep values between \(-1.5\) and \(1\), ensuring consistency with the bounds for task success and failure. Specifically, reaching a goal results in a reward of \(+2\), while colliding with an obstacle incurs a penalty of \(-2\). The final reward function is:

\begin{equation}
R = \text{clip}\left(1 - \sum (\text{action}_i - \text{action}_{\text{opt},i})^2, -1.5, 1\right),
\end{equation}

with additional rewards for task completion or penalties for collisions:
\[
R_{\text{goal}} = 2, \quad R_{\text{collision}} = -2.
\]

This reward structure encourages the UAV to take optimal actions, with significant deviations or collisions strongly penalised to guide effective learning. 

\subsection{Simulated Environment and Guidance Training}
The training of the proposed network architecture was conducted using a custom simulation environment developed in Epic's Unreal Engine 4 \cite{epicgames_unreal4}, integrated with the AirSim plugin developed by Microsoft. This simulation environment provides a realistic 3D setting for training the UAV using the TD3 algorithm which includes the perception branches shown in Fig. 2. The AirSim plugin enabled high-fidelity physics simulation and sensor emulation, allowing for realistic interactions between the UAV and the virtual environment.

The environment was designed with a variety of challenges to ensure robust learning. It featured a series of obstacles arranged along two different paths, with the path selection alternating throughout the training process to encourage exploration and adaptability. The obstacles included narrow corridors that required precise navigation, walls that needed to be circumnavigated, elevated sections that the UAV had to ascend, and barriers that required the UAV to manoeuvre over or under. This diversity of challenges aimed to expose the UAV to a wide range of scenarios that it might encounter in real-world applications.

The training process was conducted on a high-performance workstation equipped with an NVIDIA RTX A4500 GPU \cite{nvidia_rtx_a4500}, allowing for accelerated simulation and network optimisation. The training consisted of 1,000 episodes, during which the agent iteratively improved its guidance policy based on feedback from the environment. The reward system was designed to reinforce goal achievement and penalise collisions, facilitating the development of effective guidance strategies.

Upon completing the training, models that met a predefined threshold of successful goal completions were evaluated in a separate validation environment. This validation course was designed to test the generalisation capabilities of the trained models by presenting scenarios similar to, but distinct from, those encountered during training. Based on its ability to reach goals efficiently while avoiding obstacles, the model that demonstrated the highest performance in the validation environment was selected for deployment onto the physical UAV platform.

\subsection{GAT based Task Allocation Architecture}
The TD3 DRL scheme will be used again for the task allocator, and the following architecture will be used as the actor-network. A similar architecture will be used for the critic networks with a concatenation that adds the actor's action to the linear layers and again produces a single Q-value.

The task allocator for the UAV system is implemented using an GAT architecture known as GATConv \cite{fey2019fast}, designed to determine the optimal node selection in a graph-based environment. The graph structure models the set of nodes representing locations/tasks to be visited by the UAVs, with each node being connected to all other nodes, forming a fully connected graph.

\subsubsection{Graph Structure}

The input graph is defined as follows:
\begin{itemize}
    \item \textbf{Nodes:} Each node represents a location or task that needs to be visited. The node features are encoded as a vector with three elements:
    \begin{itemize}
        \item The first element indicates whether the node has already been visited (\(0\) or \(1\)).
        \item The second element indicates if the current UAV is at that node (\(0\) or \(1\)).
        \item The third element indicates if the other UAV is at that node (\(0\) or \(1\)).
    \end{itemize}
    \item \textbf{Edges:} Each edge between nodes \(i\) and \(j\) carries a weight corresponding to the shortest unobstructed route between the two nodes. This route considers obstacles in the environment, providing a more accurate representation of the actual path that the UAV would need to take to travel between nodes \(i\) and \(j\).
\end{itemize}

\subsubsection{GAT-Based Layer}

The GATConv layer uses an attention mechanism to focus on the most relevant neighbouring nodes when making task allocation decisions. The attention mechanism assigns higher weights to edges representing shorter or more critical paths, based on the features of the nodes and the distance of the shortest unobstructed routes between them.

\subsubsection{GAT-Based Model}
The whole model is wrapped with a pre-processing layer and a post-processing layer. A fully connected layer proceeds the GATConv layers, expanding each node's feature set to 64 with batch normalisation and a leakyReLU activation applied. There are two sequential GATConv layers, each with five attention heads. Following the GATConv layers, the data is fed to two fully connected layers to produce the final output.

The final output is a ten-dimensional vector representing the next node to visit. A softmax activation function is applied to this vector, converting the outputs into a probability distribution over the potential next nodes. The UAV selects the node with the highest probability as its next destination, guiding its movement through the environment. The layout of this model is shown in Fig. \ref{fig:GAT}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{GATNET.png}
    \caption{The network architecture of the GAT-Based model}
    \label{fig:GAT}
\end{figure}

\subsubsection{Decision Process}

The GAT-based model processes the graph and generates an output that balances the need to visit unvisited nodes, the relative shortest paths between nodes, and the current locations of the cooperative UAVs (Two UAVs in our case). By incorporating the spatial and task-specific information directly into the graph, the network can dynamically allocate tasks to each UAV, optimising for efficiency and minimising redundant visits to already completed tasks.

\subsection{Training of the GAT-Based Task Allocator}

The training of the GAT-based task allocator is developed based on a DRL approach with a TD3 architecture. The training process aims to optimise the allocation of tasks among UAVs by learning to select the next node to visit based on the graph representation employed.

\subsubsection{Training Setup}

The training is performed on a workstation equipped with an NVIDIA RTX 3070m GPU \cite{nvidia_rtx_3070m}, allowing for accelerated computation and efficient training. The model is trained over 20,000 episodes, where each episode consists of a complete attempt by the network to visit all nodes in the graph. During each episode, the UAVs interact with the environment by selecting nodes to visit, and the rewards guide the learning process toward optimal task allocation strategies.

\subsubsection{Reward Function Design}

The reward function is designed to encourage the UAVs to visit all nodes while minimising unnecessary travel and avoiding conflicts with other agents. It includes the following components:
\begin{itemize}
    \item \textbf{Positive Rewards:} The UAVs receive positive rewards for visiting previously unvisited nodes and for completing the task of visiting all nodes in the graph.
    \item \textbf{Negative Rewards:} Penalties are applied for visiting nodes that have already been visited, nodes currently occupied by the other UAV, or nodes that are significantly distant from the current position. These penalties discourage inefficient task allocation and unnecessary travel.
    \item \textbf{Distance-Based Reward Adjustment:} The total distance travelled by both UAVs to visit all nodes is also factored into the reward. A baseline distance is calculated by using a simple heuristic that selects the shortest edge to an unvisited node from each current node until the entire graph is visited. The actual distance travelled is measured against this baseline, and additional rewards or penalties are applied based on the relative efficiency of the UAVs' paths.
\end{itemize}

This reward structure encourages the GAT-based task allocator to develop strategies that minimise travel distance while efficiently distributing the task of visiting nodes between the two UAVs. By rewarding the completion of the graph and penalising redundant or inefficient actions, the training process ensures that the network learns an effective policy for multi-agent task allocation.

\subsection{Deploying the Software}
\subsubsection{AI-based Guidance AI}
The guidance AI model was deployed on an NVIDIA Jetson Orin Nano \cite{NVIDIAJetsonOrinNano2024}, selected for its balance of computational power and energy efficiency, making it suitable for real-time inference on UAVs. Due to the model's lightweight nature, it was implemented directly in Python, as the performance gains from converting the code to C++ were deemed unnecessary for this application.

The system utilised ROS 2 Isaac \cite{NVIDIAIsaacROS2024} to manage communication between the UAV's sensors and control algorithms, facilitating seamless integration between hardware components and the guidance AI model. To interface the AI-generated guidance commands with the PX4 flight controller, the $\mu$XRCE-DDS client \cite{px4_uxrce_dds} was employed with the PX4\_msgs ROS 2 package. This setup enabled efficient and reliable transmission of control commands from the AI model to the UAV, allowing for rapid and accurate execution of guidance decisions avoiding obstacles in real-world environments.

\subsubsection{AI-based Task Allocation}
Task allocation is managed by a central server hosted on a laptop, which communicates with the UAVs over a WiFi connection. Each UAV connects to the server and sends requests for guidance instructions, receiving a path to its next target node from the task allocation model. Due to the fully connected nature of the task allocation graph, the server generates a sequence of nodes to guide the UAV to its target when a direct path between nodes is obstructed. For instance, if a wall blocks the direct route between two nodes, an intermediary node along the feasible path will be included in the sequence to ensure the UAV reaches its destination.

Upon reaching its designated target node, the UAV sends a new request to the server, and the task allocator provides the next target and path. The server maintains a separate graph version for each UAV, dynamically updating these graphs as new targets are assigned. This ensures both UAVs have up-to-date task information, allowing them to coordinate their movements efficiently. Once all nodes have been visited, the server calculates and transmits a return path to the take-off point, facilitating the UAVs' recovery.

\section{Test Results and Analysis}
This section presents the results of the developed UAV navigation, guidance and task allocation systems, divided into three main parts: simulation results, real-world results, and the performance in the Sapience Competition. The simulation results focus on validating the guidance and navigation AI, as well as the performance of the task allocation strategy in a controlled environment. The real-world results evaluate the system's practical performance, including the UAV's ability to navigate around obstacles, follow designated paths, and execute task allocation in real-world conditions. Additionally, the effectiveness of LIDAR-SLAM for height correction and the UAV's capabilities in a mapping task are examined, providing a comprehensive assessment of the system's deployment.

\subsection{Simulation Results}


\subsubsection{Validation of UAV AI-based Guidance}
The training process for the AI-based Guidance generated 260 models that met the criteria for further testing, each having achieved the required number of targeted pose goals. The first stage of validation involved running these models through the training environment and evaluating their performance based on the number of goals achieved. Approximately the top 10\% of models were selected, depending on the distribution of performance. Out of the 260 models, 34 surpassed the chosen threshold of achieving 20 goals across four episodes.

These 34 models were then evaluated in a validation simulation designed to more closely resemble the real-world environment in which the UAV would operate. This can be seen in Fig. \ref{fig:droneval}. In this more challenging setting, the best-performing model achieved 304 goals out of a possible 320, with only two crashes recorded during 20 validation runs. The crashes occurred during a particularly difficult manoeuvre, requiring the UAV to drop from an elevated platform and execute a 180-degree turn around a corner.

Notably, the models that reached this second stage of validation emerged from a training window of 77,000 to 170,000 states observed, out of a total of approximately 250,000 states encountered during training. This observation suggests a plateau in training, indicating that further exposure to training data does not necessarily lead to significantly better models. Ultimately, model number 155260 was selected for real-world testing based on its performance in the validation phase.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{drone_val.png}
    \caption{The trained model running through the validation environment}
    \label{fig:droneval}
\end{figure}

\subsubsection{Validation of GAT-based Task Allocation Strategy}
The validation process for the AI-based Task Allocation follows a procedure similar to that used for the AI-based Guidance scheme. During training, models that were able to complete the graph in under six moves were retained for further evaluation. The first stage of validation involved testing these models using the specific graph designed for the real-world scenario, aiming to identify those models that could provide the most efficient solutions.

In the second stage of validation, each selected model was tested on a series of randomly generated graphs. Each model was given five attempts to complete each random graph, and their performance was evaluated based on the total distance travelled across all attempts. The model with the lowest cumulative distance was chosen as the best-performing model, as it demonstrated the most efficient task allocation across diverse scenarios.

\subsection{Testing in a Real-World Environment}
The real-world testing of the UAV systems was conducted in the Autonomous Systems Arena at City, St George’s University of London. For initial tests, the arena was configured as an open space measuring 12 m by 8 m, with large cardboard boxes used as obstacles. These obstacles served to limit potential damage to both the UAV and the testing environment while providing basic challenges for guidance and navigation.

The arena is equipped with Optitrack cameras \cite{optitrack}, which provide precise positional data by tracking reflective markers affixed to the UAV. This setup ensures accurate localisation of the UAV without relying on GNSS, facilitating the evaluation of the adapted LIDAR-SLAM odometry solution adopted in this controlled environment.

Following the initial phase of simple obstacles, a more complex structure was assembled within the arena, featuring a floor plan of 10.8 m by 6 m and walls measuring 2.4 m in height. This structure included a network of 2 m wide corridors and a room measuring 3.6 m by 4.8 m, featuring a 1 m raised floor. This setup was designed to emulate more closely real-world indoor search and rescue challenges. A 2D schematic of the layout is presented in Fig. \ref{fig:layout}. Unlike simulations, where every action is precisely controlled and predictable, real-world environments introduce various sources of uncertainty and unpredictability. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{depth_5_99.png}
    \caption{The depth image from the Intel Realsense D455f camera}
    \label{fig:realsense}
\end{figure}

\subsubsection{Real-World Sensor Output}
Compared to the simulated sensors, the real-world sensors have to deal with shadows and noise. The Intel Realsense, a stereographic depth camera, has to deal with both issues. The shadows caused by the depth camera are worse at lower ranges, so one way to get a depth image is to interpolate the lidar scan for the area that is required. This gets fewer shadows but must deal with a much lower vertical resolution as this is just 16 lines interpolated to have the same number of vertical pixels as the depth camera. This solution was successfully used for this UAV system. The two outputs are in Fig. \ref{fig:realsense} and Fig. \ref{fig:lidardepth}. Here, the much sharper image from the interpolated lidar point cloud can be seen when compared to the noisy image produced by the Intel Realsense. The interpolated image also lacks the blind spots from having stereoscopic sensors like those used in the Intel Realsense. One problem with using the point cloud is the low vertical field of view; where the Intel Realsense can see the whole wall, the point cloud-derived image can only see the middle section of the wall. A lidar with a larger vertical field of view would fix this issue. Fig. \ref{fig:lidarscan} shows the flat lidar scan used in the AI-based Guidance decision-making, allowing for the 360-degree detection of obstacles.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{lidar.png}
    \caption{The depth image extrapolated from the Velodyne 16-Line Puck sensor}
    \label{fig:lidardepth}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{lidarscan.png}
    \caption{The 2D LIDAR scan used as an input to the guidance AI}
    \label{fig:lidarscan}
\end{figure}

\subsubsection{Obstacle Avoidance and Path Following}
Initial UAV control tests were conducted in "position mode," where the UAV maintains or moves to a specified position. During these tests, take-off and landing commands were verified, demonstrating stable flight capabilities. For preliminary testing of the AI-based Guidance, the UAV was tasked with reaching specified targets in an unobstructed environment. The measured loop time for the drone’s control function ranged from 0.05 s to 0.07 s, resulting in a control frequency between 20 H and 14 Hz. This revealed overshooting issues, as the UAV struggled to accurately stop within 20 cm of the target due to momentum. To address this, the UAV’s speed was artificially reduced near the target, and its yaw range was expanded, resulting in more reliable target acquisition.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{tuning_drones.png}
    \caption{The UAV being tested for obstacle avoidance with the cardboard obstacles}
    \label{fig:tunedrone}
\end{figure}

Next, a single cardboard obstacle was introduced to test basic obstacle avoidance, with the UAV successfully travelling to the target and back. Fig. \ref{fig:tunedrone} shows the UAV flying through the cardboard obstacles. However, with multiple obstacles, the UAV experienced “clipping,” where it came too close to obstacles due to unaccounted momentum. Unlike in simulation, where the UAV could stop and change direction instantly, real-world momentum and inertia needed to be managed. To address this, an emergency avoidance system was implemented. When the 2D LIDAR detected an obstacle within 60 cm, the UAV's velocity was overridden with forces in the opposite direction, defined as follows:

\begin{equation}
v_x = \cos\left(\frac{\pi s}{36}\right) \cdot -0.2,
\end{equation}
\begin{equation}
v_y = \sin\left(\frac{\pi s}{36}\right) \cdot -0.2,
\end{equation}

Where \( s \) represents the section of the LIDAR scan from which the closest response is detected (ranging from 0 to 71), \(v_x\) is the \(x\)-velocity, and \(v_y\) is the \(y\)-velocity. This adjustment significantly improved the UAV’s obstacle avoidance performance.

The testing environment was then expanded with the top half of the structure, including a corridor and a raised section. While the AI-based Guidance was able to change heights as needed, direct control of \(z\)-velocity initially caused oscillation due to overshooting. This was resolved by limiting \(z\)-velocity to \(\pm 0.3 \, \text{m/s}\), resulting in stable vertical movements. Additionally, a condition was added to ensure the UAV travelled fully over the raised ledge before descending, preventing rotor interference with the ledge.

Finally, with the complete structure assembled, the UAV was flown through the environment in various patterns, successfully navigating all sections. These tests confirmed the UAV’s readiness for task allocation and further tests of cooperative functionality and the adapted LIDAR-SLAM odometry.




\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{cooperative_drones.png}
    \caption{The UAVs moving through the course cooperatively}
    \label{fig:coopdrone}
\end{figure}


\subsubsection{Real-World Task Allocation Performance}
To evaluate the task allocation system in a real-world environment, the UAVs were positioned at opposite ends of the building, as shown in Fig. \ref{fig:layoutnodes}. For this task allocation scenario, a 2D floorplan of the building was provided as prior knowledge, allowing nodes to be predefined and the graph to be constructed in advance.

The task allocation server was configured to calculate the shortest viable route between any two nodes that do not have a direct connection. Additionally, the server includes a conflict-detection mechanism to prevent potential path conflicts between UAVs, enhancing safety even though the task allocator is designed to avoid conflicts inherently. 

During testing, the UAVs successfully navigated to their assigned nodes, and the paths they took are illustrated in Fig. \ref{fig:layoutpath}. This setup and testing confirmed the effectiveness of the AI-based task allocation system and the UAVs' ability to execute assigned paths reliably in a real-world environment.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{layoutnodes.png}
    \caption{The floor plan for the constructed building with the node locations for the graph}
    \label{fig:layoutnodes}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{layoutpath.png}
    \caption{The floor plan for the constructed building with the node locations for the graph}
    \label{fig:layoutpath}
\end{figure}


\subsubsection{LIDAR-SLAM Altitude Correction Performance}
To evaluate the UAV's self-localisation performance in a GNSS-denied environment, the adapted LIDAR-SLAM solution was tested within the building. Since GNSS positioning is unavailable in the indoor testing arena, an alternative ground truth was established using the OptiTrack system. This system provides positional tracking by using multiple cameras to monitor reflective markers arranged in unique patterns on the UAV.

The high walls of the testing structure, however, can occasionally obstruct the OptiTrack system, degrading its tracking accuracy. This occlusion can lead to small positional errors and, in cases of complete occlusion, a temporary loss of positioning data.

Initial testing showed that the LIDAR-SLAM solution performed accurately in the \(x\) and \(y\) directions but struggled with \(z\)-axis localisation. To address this, the methodology described in the LIDAR-SLAM Altitude Fix portion of the Methodology (VI.B) was implemented to correct the \(z\) positioning. In Fig. \ref{fig:lidarresults}, the positional trace for each axis is shown over time. The raw LIDAR-SLAM data aligns closely with the ground truth in \(x\) and \(y\) positions, demonstrating minimal deviation. However, significant discrepancies are evident in the \(z\)-axis positioning; the raw LIDAR-SLAM data fail to maintain an accurate \(z\) position soon after take-off and do not regain reliable positioning throughout the test.

With the applied corrections, the predicted \(z\)-position from the modified LIDAR-SLAM solution shows a marked improvement, achieving better alignment with the ground truth. Notably, the only visible deviations occur when the ground truth itself displays errors in tracking, which is attributed to temporary occlusions within the OptiTrack system. Evidence of this error can be seen when the UAV returns to land at consistent heights, indicating reliable \(z\)-positioning despite intermittent inaccuracies in the ground truth data.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{2D_results.png}
    \caption{Height corrected LIDAR-SLAM comparison with the ground truth and the raw LIDAR-SLAM}
    \label{fig:lidarresults}
\end{figure}

In Fig. \ref{fig:3dgraph}, the UAV's height transition from the upper platform to floor level and back again is accurately maintained. The UAV successfully adjusts its height relative to the world coordinate system, demonstrating effective height control. This result indicates that the comparison between IMU-derived velocity and calculated velocity enabled the appropriate adjustment of the height offset, allowing for stable vertical positioning during transitions.

\subsection{The Sapience Autonomous Cooperative Drone Competition}
The Sapience competition took place over a week in August 2024 at City, St George’s University of London, with four teams from NATO and NATO partner countries participating. The competition consisted of three tasks designed to simulate a search and rescue operation within a specially constructed indoor arena. Emphasis was placed on the speed of task completion to encourage cooperative use of the two UAVs made available for use in this competition. Teams were additionally evaluated on the innovation in their solutions and the quality of their system outputs.

The first task required each team to produce a legible 3D arena map quickly. In the second task, teams were tasked with detecting and localising three mannequins and five boxes within the arena, with accuracy in detection and localisation serving as key evaluation criteria. The final task involved completing eight deliveries to the mannequins, with the last two deliveries requiring synchronisation between the UAVs. Teams were assessed on the number of successful deliveries and the speed of completion within the allotted time. These tasks simulate essential search and rescue activities, such as those required in the aftermath of an earthquake.

This paper focuses exclusively on the first task and third tasks, as they show the ability of the UAVs to perform autonomous cooperative flight better than the second task. The second task only entailed a short pre-planned flight to collect data for the detection and localisation systems. As these systems are not covered in this paper, this task will be skipped.

\subsubsection{Review of System Performance in 3D Mapping Task}
%These results are not from the actual competition, as data gathering was not running while the competition ran. This data is from the following week when the UAVs were tested more thoroughly than was possible before the competition. 

The first task, generating a 3D map of the environment in the shortest time possible, provides an opportunity to evaluate all system components. This task demonstrates the UAVs' ability to efficiently and safely navigate the environment cooperatively, the task allocator's capacity to guide the UAVs to achieve full coverage, and the GNSS-denied capabilities of the system. Additionally, the UAVs' capability to handle height variations necessary for complete mapping was assessed.

In Fig. \ref{fig:3dgraph}, the positions of the UAVs during the mapping task are shown. Position data was derived from the corrected LIDAR-SLAM, with coordinates relative to the origin point at \((0,0,0)\), located at the lower-level entrance take-off position shown at the bottom of Fig. \ref{fig:layoutpath}. The second take-off position is on the raised platform around \((10.5, 3.2, 1.0)\).

The first graph illustrates the 3D paths both UAVs take throughout the building. UAV 1 navigates around the lower section of the building, effectively completing a circuit around the free-standing wall. UAV 2, starting from the raised platform, descends to floor level, explores the long corridor, and then returns to its starting area.

The second graph provides a top-down view of the UAVs’ flight paths. UAV1 tends to struggle at the junction of the arena as it has to deal with many obstacles from various angles. This often required it to try multiple efforts to get through the gap between the lower inner wall and the eastern outer wall. The flight is not as smooth as a preplanned route, but this method shows an AI-based Guidance ability to control a UAV's navigation in real time in a realistic search and rescue situation.

In Fig. \ref{fig:height_fix}, a comparison of two flights of UAV 1 reveals significant differences in performance. The first flight occurred before the competition and was marked by overheating issues in the electronic speed controllers (ESCs). This resulted in the UAV exhibiting erratic behaviour while completing its tasks. In contrast, the second flight we did for the competition, during which the ESCs were relocated to be secured to the arms of the UAV, illustrates a notable improvement in height control following this modification to the ESC configuration. UAV 2, which retained the older ESC layout, did not experience similar problems, indicating a potential assembly fault in UAV 1. UAV 2 was also upgraded to the new ESC layout to mitigate future complications. This adjustment enhanced the UAV's repairability by disconnecting the ESCs without disassembling the main body.

The resulting 3D point cloud map, shown in Fig. \ref{fig:pointcloud}, illustrates the completed map. The second UAV was provided with an approximate relative position for its odometry to align the two point clouds from each UAV. Final alignment was achieved through registration to produce a coherent map. The mapping task was completed in 134 seconds, whereas the same task using a single UAV took 287 seconds, a time reduction of 53.3\%.

Since this task focused on mapping speed, the point cloud density is relatively low. However, this density could be increased by reducing the UAVs' top speed, increasing the number of nodes, or adjusting their height variations. These adjustments would allow for a denser, more detailed map if necessary.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{3d_graph.png}
    \caption{The path of the two UAVs through the building whilst building the map}
    \label{fig:3dgraph}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{heigh_fix.png}
    \caption{The drone's height during a flight compared to a flight before the overheating was fixed}
    \label{fig:height_fix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{lidarmap2.png}
    \caption{The 3D point cloud gathered from the cooperative UAV flight}
    \label{fig:pointcloud}
\end{figure}

\subsubsection{Delivery Task System Performance}

The delivery task required the UAVs to complete eight deliveries to three designated points within the arena. A delivery was successful when a UAV flew to a target point, landed for a predefined duration, and then returned to its starting position. Additionally, the final two deliveries were required to be completed simultaneously, demonstrating the ability to coordinate the cooperative actions of both UAVs effectively.

Fig. \ref{fig:delivery_path} illustrates the paths taken by the UAVs during the delivery task. To prevent potential conflicts at the first delivery point, UAV 1 was dispatched first to complete its delivery, followed by UAV 2 after UAV 1 had cleared the point. Once UAV 2 completed its first delivery, UAV 1 resumed its remaining deliveries. To improve synchronisation, UAV 1 was delayed by 10 seconds at each take-off, as UAV 2 had longer distances to travel between delivery points.

For the final synchronised deliveries, the UAVs employed a coordination protocol. The first UAV to arrive at its delivery point hovered above it and sent a "ready" signal to the server. The server held the UAV in a hover state until it received a matching "ready" signal from the second UAV. Once both UAVs signalled readiness, the server issued a simultaneous landing command, ensuring synchronised landings.

Fig. \ref{fig:height_delivery} shows the timing of the deliveries, highlighting the UAVs’ landing altitudes of 0.2 m on the floor and 1.2 m on the raised platform. Despite the delayed take-offs for UAV 1, a 10-second loiter period was still necessary to ensure synchronised landings. Compared to the mapping task, the UAVs' speeds were increased to a maximum of 1 m/s, prioritising faster delivery times as it can be critical to the mission's success in a real-world search and rescue scenario.

%Fig. \ref{fig:doubleimg1} and 
Fig. \ref{fig:doubleimg2} shows the outputs from the detection cameras during the delivery task, capturing both UAVs en route to their respective delivery points.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{delivery_graphs.png}
    \caption{The path of the two UAVs through the building whilst doing the delivery task}
    \label{fig:delivery_path}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{synced_delivery_annno.png}
    \caption{The height (z) against time to show the deliveries and the final synced delivery. (i) UAV 2 does not take off until UAV 1 has completed its first delivery. (ii) Once UAV 1 is landed, UAV 2 takes off to complete its task. (iii) As UAV 2 makes the last conflicted delivery, UAV 1 starts again but is delayed by 10s to better sync the deliveries at the end. (iv) The UAV 1 delayed take-off when at base. (v) The sync delivery takes place once the UAVs detect they have both reached the delivery point.}
    \label{fig:height_delivery}
\end{figure*}

%\begin{figure}
%    \centering
%    \includegraphics[width=1.0\linewidth]{drone_di_1.png}
%    \caption{A view in both RGB and Depth of one drone from the other as they navigate around the arena}
%    \label{fig:doubleimg1}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{drone_di_2.png}
    \caption{A head-to-head view of a drone moving from the raised section to the lower section}
    \label{fig:doubleimg2}
\end{figure}


\section{Additional Lessons Learned from the Deployment of the UAVs}

The development and deployment of the UAV system for the SAPIENCE competition revealed several critical areas that required adjustment to ensure improved operation. Key challenges included optimising AI-based guidance, implementing the artificial potential field (APF) to improve collision avoidance, and limiting the training environment.

\subsection{Tuning the AI-based Guidance}

The initial direct embedding of the AI-based guidance highlights the need for tuning specific control outputs, especially in the vertical (z-axis) velocity. Without adjustment, the model's output was too aggressive for the flight controller, causing oscillations around the target altitude. Limiting the z-axis velocity command to ± 0.3 m/s
resolved this issue, allowing the UAV to maintain smoother and more stable flight near target altitudes. Additionally, adjustments in lateral (x-axis) velocity and yaw speed near targets improve the UAV’s accuracy in reaching and stabilising at target points, increasing success rates in approaching waypoints without overshooting.

\subsection{Implementation of the Artificial Potential Field (APF)}

To ensure safe navigation, an APF layer was added to the system as an additional safeguard against collisions. While the guidance AI was trained with an obstacle avoidance objective, real-world dynamics introduced momentum and other factors that required more robust safety measures than initially provided by the reward structure. The APF’s incorporation points to the potential need for refining the reward structure during training so that avoidance behaviours are better aligned with the complexities of real-world flight dynamics.

\subsection{Limitations of the Training Environment}

A significant insight from this deployment was the importance of the quality of the simulation environment. The training and validation environments used in AirSim were simple and often failed to capture the complexities encountered in the real world. Real-world scenes with larger open spaces, varied materials, and complex textures—such as glass and netting—exposed gaps in the depth camera’s data, presenting challenges not accounted for in the training phase. Due to the requirements of AirSim, configuring highly detailed environments with realistic depth textures proved challenging, as each texture must be specifically adapted for depth imaging to function correctly.

\section{Conclusion}
This work presents the development and deployment of a cooperative UAV system designed for SAR operations in GNSS-denied environments. The system integrates LIDAR-SLAM for real-time mapping and localisation, a DRL-based guidance AI for autonomous navigation, and a GAT-based task allocator for efficient multi-UAV coordination. These technologies are tested and validated within the NATO SAPIENCE Autonomous Cooperative Drone Competition, which provides a realistic and challenging platform for evaluating UAV performance in SAR-like scenarios.

The competition tasks—mapping, object detection, and synchronised deliveries—demonstrated the system’s capabilities in addressing critical challenges of SAR operations. The mapping task validates the ability to produce accurate 3D maps quickly, which is crucial for navigating hazardous environments. The delivery task showcases the system's coordination and reliability in executing time-critical missions, such as synchronised aid delivery, emphasising the potential impact of cooperative UAV operations in life-saving applications.

Beyond SAR-specific applications, this work highlights a comprehensive cooperative UAV AI-based navigation, guidance, and task allocation system for GNSS-denied environments, validated through simulation and real-world testing. The integrated LIDAR-SLAM and a GAT-based task allocator demonstrate effective performance across diverse indoor settings, including complex structures with multiple obstacles, varying heights, and confined corridors. Real-world testing enables critical adjustments, such as implementing emergency obstacle avoidance mechanisms, refining height control with IMU-based velocity corrections, and addressing thermal management issues.

This system demonstrates a promising approach to autonomous UAV navigation and multi-agent task allocation, providing valuable insights into designing and deploying robust UAV systems for dynamic and collaborative operations in GNSS-denied indoor settings. Its performance in the SAPIENCE competition underscores the importance of advancing AI-based autonomous technologies for humanitarian applications. This work contributes to the broader goal of enabling faster, safer, and more efficient SAR operations and collaborative UAV missions in critical situations.




% use section* for acknowledgment
\section*{Acknowledgment}
We would like to thank NATO for helping to organise the SAPIENCE competition, the various staff at City St George's University of London for their help in setting it up, and the other competition teams from Delft University of Technology, the University of Alabama in Huntsville, and the University of Klagenfurt for their excellent work and help in making it a success. I would finally like to thank the other members of our RAMI research group for their patience and help while organising the competition.



\bibliographystyle{plain}
\bibliography{refs} 


\end{document}


