\section{Related Work}
\subsection{Deep Reinforcement Learning for Autonomous Guidance and Obstacle Avoidance}
UAV guidance using DRL has been studied,  \cite{azar2021drone} and the developed approaches generally fall into two categories: discrete action spaces \cite{zhu2024uav}, where the UAV’s actions are restricted to specific movements (e.g., move forward, move right), and continuous action spaces \cite{hu2022obstacle}, where the UAV can select from a range of values for each action, allowing more nuanced control. The choice of DRL algorithms depends on the type of control required. For discrete action spaces, algorithms such as Deep Q-Networks (DQN), \cite{mnih2015human}, \cite{zhu2024uav} are suitable, while for continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) \cite{lillicrap2015continuous}, \cite{hickling2023robust}, Proximal Policy Optimisation (PPO) \cite{schulman2017proximal}, \cite{hu2022obstacle}, and Twin Delayed Deep Deterministic Policy Gradient (TD3) \cite{fujimoto2018addressing}, \cite{he2021explainable} are more appropriate.

DRL typically employs unsupervised learning, relying solely on states, actions, and rewards observed during training. This unsupervised approach allows DRL to produce novel solutions unbounded by existing methods; however, it often requires significantly more samples to train effectively, leading to extended training times, especially in complex environments like for UAV guidance.

To address this inefficiency, researchers have incorporated expert knowledge into DRL training, enhancing performance through pre-training. Techniques such as Deep Q Learning from Demonstrations (DQfD) \cite{hester2018deep} and Deep Deterministic Policy Gradients from Demonstrations (DDPGfD) \cite{vecerik2017leveraging} leverage expert-labeled data to pre-train networks, reducing reliance on extensive reward engineering while preserving the benefits of DRL.

Non-Expert Policy Enhanced DRL (NPE-DRL) \cite{zhang2024npe} and DDPG-APF \cite{hickling2023robust} leverage the Artificial Potential Field (APF) to generate non-expert actions that guide the agent during training. Both methods gradually reduce the influence of APF to allow for exploration and optimal policy discovery. NPE-DRL operates in a discrete action space, while DDPG-APF utilises a continuous action space. Each approach demonstrates improved performance and sample efficiency, enabling the agent to converge on effective solutions with fewer training samples.

This paper builds by integrating the APF directly into the reward structure, rather than using it as a separate guiding mechanism. This approach is expected to provide tighter control over the UAV's policy, especially within the more complex indoor environments considered here, which extends beyond the scenarios addressed by NPE-DRL and DDPG-APF.

\subsection{Deep Reinforcement Learning for Graph-Based Task Allocation}
The use of multiple UAVs in cooperative missions has been explored to enhance task efficiency and flexibility \cite{sargolzaei2020control}. Early task allocation methods relied on rule-based or heuristic optimisation approaches, including centralised systems where a single controller assigns tasks to each UAV and decentralised systems where UAVs independently select their tasks based on predefined rules \cite{zhao2015heuristic}. While these methods provided foundational insights, they often faced scalability challenges and lacked the adaptability needed for dynamic environments \cite{cui2019multi}. As a result, researchers have increasingly turned to Machine Learning (ML) approaches, particularly DRL \cite{mao2024dl}, to address these limitations by enabling UAVs to learn adaptive task allocation policies that are robust to environmental changes.

Within DRL-based task allocation, various algorithms have been developed for multi-UAV coordination, including Markov Decision Process \cite{mao2024dl}, and Multi-Agent Deep Deterministic Policy Gradient \cite{lowe2017multi}. These DRL approaches rely on reward structures designed to optimise objectives such as maximising task coverage, minimising collisions, and improving efficiency in multi-agent settings. The adaptability of DRL allows UAVs to dynamically allocate tasks in response to changes in mission objectives or environmental constraints, providing advantages over classical optimisation techniques in complex, multi-agent scenarios \cite{mao2024dl}.

Graph-based representations have become a key approach in modelling spatial layouts and task relationships within multi-UAV systems \cite{GNN-EnhancedDRL}. Graphs provide a natural way to represent complex spatial relationships, with nodes representing specific locations, tasks, or UAVs, and edges representing viable paths or task dependencies. This structure is particularly beneficial in environments with intricate layouts, dynamic obstacles, or multiple task dependencies, as it allows for more informed decision-making based on the UAVs’ spatial context and objectives.

Graph Attention Networks (GATs) \cite{velickovic2017graph}, \cite{GA-DRL} improve upon traditional Graph Convolutional Networks (GCNs) by incorporating an attention mechanism that allows the model to assign varying levels of importance to different nodes and edges. This is especially useful in UAV task allocation, where certain nodes (tasks) or edges (paths) may be more critical than others for efficient navigation and task completion. By selectively focusing on the most relevant nodes, GATs enable prioritisation of tasks in real-time, enhancing coordination and efficiency in multi-agent systems \cite{shao2021graph}.

The combination of GATs and DRL leverages the strengths of both approaches: GATs provide the structural awareness necessary for understanding task relationships, while DRL offers a flexible framework for learning task allocation policies based on trial-and-error feedback. This paper will explore how to combine these methods to develop a task allocator that enables UAV cooperation in indoor environments.

\subsection{Adapted LIDAR-SLAM for Navigation}
The "corridor problem" in LIDAR-SLAM is a known issue that occurs when using LIDAR for SLAM in narrow, featureless corridors. The problem arises because the lack of unique, distinguishable features in a long, narrow corridor can make it difficult for the LIDAR-SLAM system to localise and distinguish between different parts of the environment accurately.

Zhang et al. \cite{radarlidar} solved this issue by utilising radar to create more features with different materials and shapes, returning different radar returns. These additional features help mitigate common issues associated with the corridor problem. By leveraging different feature extraction methods, the limitations of one approach can be compensated by the other. This paper proposes a solution to Z-drift caused by the corridor problem through sensor fusion and converting relative positioning into the global positioning system used by LiDAR-SLAM mapping.