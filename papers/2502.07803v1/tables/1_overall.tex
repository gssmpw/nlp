\begin{table*}[htb]\centering
    \caption{\tool significantly increases final scores with all the three LLM backbones. $\Delta$Gain is the performance gain of \tool compared with the best-performing baseline.}
        \begin{adjustbox}{max width=\columnwidth*2}
        \begin{tabular}{*{11}{c}}
        \toprule
        Dataset & LLM & Direct & CoT & ToT & PoT/SR$^\star$ & SC & SCal & SCheck/SD$^\star$ & \tool & $\Delta$Gain(\%) \\
        \cmidrule{1-11}
        % Dataset & LLM & Direct & CoT & ToT & PoT & SCheck & SC & SCal & \tool & $\Delta$Gain(\%) \\
        \multirow{3}*{GSM8K} 
        & DeepSeek V3  & 0.917 & \underline{0.968} & 0.921 & 0.939 & 0.965 & 0.949 & 0.942 & \textbf{0.971} & +0.310\\
        & Qwen2.5-72B & 0.940 & \underline{0.967} & 0.937 & 0.951 & 0.964 & 0.945 & 0.901 & \textbf{0.980} & +1.344\\
        & Llama3.3-72B & 0.826 & 0.944 & 0.904 & 0.876 & \underline{0.945} & 0.928 & 0.881 & \textbf{0.964} & +2.011\\
        \cmidrule{1-11}
        
        \multirow{3}*{MATH-np} 
        & DeepSeek V3 & 0.691 & 0.723 & 0.643 & 0.751 & 0.703 & \underline{0.814} & 0.601 &  \textbf{0.821} & +0.860\\
        & Qwen2.5-72B & 0.706 & 0.708 & 0.689 & 0.753 & \underline{0.780} & 0.726 & 0.693 & \textbf{0.791} & +1.410\\
        & Llama3.3-72B & 0.347 & 0.541 & 0.489 & 0.551 & \underline{0.607} & 0.539 & 0.533 & \textbf{0.631} & +3.954\\
        
        \cmidrule{1-11}\morecmidrules\cmidrule{1-11}
        \multirow{3}*{HumanEval} 
        & DeepSeek V3 & 0.915 & 0.915 & 0.890 & 0.854 & \underline{0.933} & 0.921 & 0.878 & \textbf{0.939} & +0.643 \\
        & Qwen2.5-72B & 0.841 & \underline{0.872} & 0.841 & 0.707 & 0.738 & 0.787 & 0.774  & \textbf{0.909} & +4.243\\
        & Llama3.3-72B & 0.689 & \underline{0.713} & 0.704 & 0.591 & \underline{0.713} & 0.701 & 0.585 & \textbf{0.811} & +13.745\\ 
        \cmidrule{1-11}
        
        \multirow{3}*{HumanEval+} 
        & DeepSeek V3 & 0.878 & 0.878 & 0.835 & 0.805 & \underline{0.884} & 0.866 & 0.799 & \textbf{0.902} & +2.036 \\
        & Qwen2.5-72B & 0.793 & 0.793 & \underline{0.823} & 0.659 & 0.707 & 0.750 & 0.738 & \textbf{0.860} & +4.496\\
        & Llama3.3-72B & 0.628 & 0.652 & 0.653 & 0.512 & \underline{0.671} & 0.646 & 0.543 & \textbf{0.768} & +14.456\\
        \cmidrule{1-11}
        
        \multirow{3}*{Mbpp} 
        & DeepSeek V3 & 0.923 & \underline{0.926} & 0.910 & 0.892 & 0.921 & 0.915 & 0.899 & \textbf{0.937} & +1.188 \\
        & Qwen2.5-72B & \underline{0.921} & 0.894 & 0.910 & 0.862 & \underline{0.921} & 0.918 & 0.902 & \textbf{0.926} & + 0.543\\
        & Llama3.3-72B & 0.807 & 0.828 & 0.782 & 0.738 & 0.823 & \underline{0.831} & 0.775 & \textbf{0.836} & +0.598\\
        \cmidrule{1-11}
        
        \multirow{3}*{Mbpp+} 
        & DeepSeek V3 & 0.783 & 0.778 & 0.759 & 0.720 & \underline{0.791} & 0.786 & 0.767 & \textbf{0.833} & +5.310 \\
        & Qwen2.5-72B & \underline{0.791} & 0.765 & 0.778 & 0.730 & 0.775 & 0.780 & \underline{0.791} & \textbf{0.828} & +4.678\\
        & Llama3.3-72B & 0.664 & \underline{0.704} & 0.601 & 0.550 & 0.698 & 0.685 & 0.638 & \textbf{0.709} & +0.710\\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \begin{tablenotes}
        \footnotesize
        \item $\star$ Using the former method for math reasoning and using the latter method for code generation. 
        % \item[$\dagger$] The result is obtained from 665 sampled bugs.
        % \item[$\ddagger$] We randomly select 100 plausible patches to check their correctness because of the huge number of plausible patches.
      \end{tablenotes}
    
\vspace{-0.1in}
\label{tab:exp:overall}
\end{table*}
