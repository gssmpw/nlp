\begin{table*}[htb]\centering
    \caption{\tool using open-sourced LLMs (DeepSeek-V3 or Qwen2.5-72B) can even outperform state-of-the-art closed-source LLMs, whose results are reported by previous studies. It achieves SoTA results on both mathematical reasoning benchmarks by making an improvement of 9.46\%, though there are no results on o1 models so we have to omit them. \tool also enhances code generation by 2.61\% on the plus versions of benchmarks compared to o1-preview, despite the slight decline (-2.24\%) on the standard benchmarks.
    }
    \begin{adjustbox}{max width=\columnwidth*2}
    \begin{tabular}{*{8}{c}}
    \toprule
    Dataset & o1 Preview & o1 Mini & GPT-4o & GPT-4-Turbo & Claude-3.5-Sonnet & GPT-3.5-Turbo & Ours (\tool) \\
    \cmidrule{1-8}
    GSM8K & \underline{0.969} & 0.951 & 0.948 & 0.926 & 0.950 & 0.822 & \textbf{0.980}\\
    MATH-np & - & - & 0.697 & 0.590 & 0.623 & 0.347 & \textbf{0.821} \\
    \cmidrule{1-8}\morecmidrules\cmidrule{1-8}
    HumanEval & \textbf{0.963} & \textbf{0.963} & 0.927 & 0.902 & 0.872 & 0.835 & \underline{0.939} \\
    
    HumanEval+ & \underline{0.890} & \underline{0.890} & 0.872 & 0.866 & 0.817 & 0.707 & \textbf{0.902} \\
    
    Mbpp & \textbf{0.955} & 0.931 & 0.876 & 0.857 & 0.894 & 0.825 & \underline{0.937} \\

    
    Mbpp+ & \underline{0.802} & 0.788 & 0.722 & 0.733 & 0.743 & 0.697 & \textbf{0.833} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
      %   \begin{tablenotes}
      %   \footnotesize
      %   \item[$\star$] Only the result on \dfj 1.2 is available. 
      %   \item[$\dagger$] The result is obtained from 665 sampled bugs.
      %   \item[$\ddagger$] We randomly select 100 plausible patches to check their correctness because of the huge number of plausible patches.
      % \end{tablenotes}
    
\vspace{-0.1in}
\label{tab:exp:wLLM}
\end{table*}
