\section{Conclusion}
We present Reasoning-as-Logic-Units (\tool), a pioneering test-time scaling framework designed to tackle the issue of reasoning hallucinations and enhance the reasoning capabilities of LLMs.
Unlike existing methods that often encounter logical inconsistencies between reported reasoning steps and generated programs, \tool effectively extracts logic units from generated programs and aligns them with task requirements using natural language explanations.
This method leverages the strengths of both natural language and program logic, resulting in more reliable, interpretable, and transparent LLM reasoning.
Experimental results demonstrate that \tool consistently outperforms existing baselines across various LLMs, including comparisons with proprietary close-sourced models.
We hope that our work will inspire further research into structural reasoning, advancing LLM problem-solving across diverse domains.