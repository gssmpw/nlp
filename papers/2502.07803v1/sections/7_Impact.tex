% \section*{Acknowledgements}

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of machine learning by improving the reliability and accuracy of large language models (LLMs) in complex reasoning tasks. 
By addressing reasoning hallucinations through logic-aligned hybrid reasoning processes, our framework enhances LLMs' general capabilities to generate coherent and logically consistent solutions, particularly in mathematical and algorithmic domains, without any fine-tuning or re-training. 
Potential societal benefits include more trustworthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here.