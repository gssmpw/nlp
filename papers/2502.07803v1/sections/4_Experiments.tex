\section{Experiments}
\subsection{Experiment Setup}
\noindent\textbf{Benchmarks.} We use four benchmarks: two for mathematical reasoning: GSM8K~\cite{GSM8K} and MATH~\cite{MATH}, and the other two for code reasoning, HumanEval~\cite{HumanEval} and Mbpp~\cite{Mbpp}, along with their extended versions with more test cases~\cite{EvalPlus}. 
See Appendix~\ref{app:datasets} for more details about the benchmarks.
We evaluate \tool on the whole test sets except MATH. 
Due to resource limitation, we follow~\cite{Self-Check} to use a subset of MATH (named by MATH-np) taken from~\cite{DeductiveVeriCoT}~\footnote{\url{https://github.com/lz1oceani/verify_cot/blob/main/results/chatgpt3.5/natural_program/MATH_np.json}}.
We report the answer accuracy and pass@1 score for math- and code-reasoning, respectively.
%We also compare the token consumption using different baselines in Appendix~\ref{app:exp:token}.
Our focus on math- and code-reasoning is due to the availability of well-established benchmarks and the ease of evaluating outputs. \tool can be directly applicable to other domains with minimal adjustments to the prompts.

\noindent\textbf{Baselines.} We compare \tool against three categories of baselines without fine-tuning or external information: 
1) promoting methods for general purposes: Direct Prompting, Zero-Shot CoT~\cite{CoT}, ToT~\cite{ToT}, and Self-Consistency (SC)~\cite{Self-Consistency}.
2) self-correction-based approaches: Self-Calibration (Scal)~\cite{Self-Calibration}, and Self-Refine (SR)~\cite{Self-Refine};
3) techniques specific for either task: PoT~\cite{PoT}, Self-Check (SCheck)~\cite{Self-Check}, and rubber-duck debugging derived from Self-Debug (SD)~\cite{Self-Debug}.
Details of these baselines are provided in Appendix~\ref{app:baselines}.


\noindent\textbf{Implementation.} We deploy \tool on three open-source LLMs: Deepseek-V3 (Dec 2024), Qwen2.5-72B-Instruct (Sep 2024), and Llama3.3-70B-Instruct (April 2024). 
%The default backbone model for \tool is Deepseek-V3.
To prevent breaches of anonymity, we do not deploy \tool on commercial closed-source models such as GPTs and o1. Instead, we compare \tool with the public results of these closed-source LLMs reported on the leaderboard maintained by~\cite{EvalPlus, GSM8KLead, GSM8KSymbolic}, presented in Appendix~\ref{app:exp:close-source}.
We set the maximum number for self-correction turns as 3 and the maximum number of candidate solutions/branches as 10 for Self-Consistency and ToT. The temperature parameter is set to 0.7, and the frequency penalty is 0.3 in all experiments.

\subsection{Results}
\input{tables/1_overall}

Table~\ref{tab:exp:overall} summarizes the performance of \tool on math and code reasoning.
Across all benchmarks and diverse LLM architectures, \tool consistently outperforms existing baselines, demonstrating its generalizability and robustness.
We analyze the advantages through three critical comparisons:

\noindent\textbf{\tool v.s. Single-Path Reasoning.} 
Compared to direct prompting, CoT, and PoT (single reasoning path per query), \tool achieves an average improvement of +12.81\% and +14.85\% for math and code reasoning, respectively, attributed to its structured decomposition of problems into logical units aligned with programmatic constraints, mitigating the inconsistencies inherent in linear reasoning chains, either represented in NL (CoT) or programs (PoT).
We display more cases about how \tool reduces reasoning hallucinations of the combination of CoT and PoT in Appendix~\ref{app:case:single-path}.
\tool enables fuller exploration in the diverse solution subspaces, resulting in optimal solution generation.

\noindent\textbf{\tool v.s. Multi-Path Exploration.}
Multi-path methods like Self-Consistency, Self-Check, and ToT aim to select the optimal reasoning path over multiple samples.
SC and ToT rely on sampling fixed times of independent candidates (up to 10 paths/branches), yet \tool surpasses them by +9.55\% and +10.69\% for math and code reasoning, respectively, with far fewer candidates ($\leq 3$ per unit).
This is because \tool reduces cascading errors by isolating and refining individual units with hybrid reasoning representations. In contrast, SC or ToT might aggregate multiple incorrect paths that share the same flawed premise.

While Self-Check improves robustness through weighted voting—prioritizing solutions with internally consistent steps—it suffers from two critical limitations:
First, its step-wise regeneration and comparison decorrelate errors but fail to propagate corrected logic to subsequent steps.
Second, each re-generation requires 3+ LLM calls with redundant contexts, incurring high costs without guaranteeing holistic consistency.
\tool addresses these via unit-level iterative refinement. By decomposing reasoning into logical units, errors are localized and resolved before subsequent units are processed while reducing LLM calls by about 60+\%. A refined unit $i$ directly informs the context for unit $i+1$, preventing error propagation.
This enables \tool to outperform Self-Check by 15.07\% on average, achieving accuracy and efficiency through context-aware, incremental validation.
% Second, unlike ToT (limited path length and possibly redundant path width), \tool optimizes both dimensions through unit-wise refinement, ensuring thorough yet resource-efficient analysis.


\noindent\textbf{\tool v.s. Self-Correction Methods.}
Many existing self-correction-based methods (e.g., Self-Refine and Self-Debug), often degrade performance by introducing errors into initially correct responses--a flaw exacerbated by their assumption of imperfection existence in the initial response attempt.
\tool mitigates this via a self-judgment stage, where LLMs validate each unit before refinement. This proactive verification yields an average gain of 18.28\% over these baselines. 
Though Self-Calibration applies holistic self-judgment to reduce wrong edits, it still underperforms \tool by 6.13\% and 9.11\% for math and code, respectively, as end-to-end validation fails to isolate localized inconsistencies addressed by \tool’s granular, unit-level verification.

In summary, \tool outperforms baselines by combining formal correctness (via code) and interpretability (via NL). This dual approach enables precise, self-contained error correction, significantly mitigating reasoning hallucinations.






