\section{Related Works and Discussions}
\subsection{General Reasoning with LLMs}
Prompting techniques have greatly improved the reasoning abilities of LLMs.
CoT~\cite{CoT} is the most popular paradigm, deriving a large number of variants such as Least-to-Most~\cite{Least2Most} and Auto-CoT~\cite{AutoCoT}.
The central concept of these approaches is ``divide and conquer"--prompting LLMs to deconstruct complex problems into simpler sub-tasks, systematically address each one by reporting the process and then synthesize a comprehensive final answer.
Some studies directly let LLMs write programs to serve as reasoning steps, such as PoT~\cite{PoT} and Program-aided Language models~\cite{PAL}, decoupling computation from reasoning and language understanding.
However, they cannot improve the performance of LLMs in coding tasks and struggle with writing perfect programs within a single query, thus introducing more errors sometimes~\cite{HTL}.
Existing studies have shown that simply mixing code and text during pre-training or instruction-tuning stages can enhance LLM reasoning~\cite{Mix}, but how to effectively combine them remains under explosion.

\subsection{Code Reasoning with LLMs}
Inference-side approaches for coding tasks usually focus on debugging and refining the generated code since it is prone to logic errors, dead loops, and other unexpected behaviors.
Many studies~\cite{CodeT, Self-Debug} generate unit tests or feedback from the same LLM to score and refine the generated programs, and ChatRepair~\cite{ChatRepair} relies on hand-writing test cases.
Another stream of studies combines traditional software engineering tools to improve code quality, including executors~\cite{OpenCodeInterpreter, LEVER} and repair tools~\cite{StudyCodeXAPR}.
Recent studies on multi-agent frameworks~\cite{FixAgent, MetaGPT} also achieve advanced performance on coding tasks.
They borrow the information provided by software analysis tools and embed such information into prompts to expand the ability bounds of LLMs in code reasoning.

\subsection{Test-Time Scaling for LLM Reasoning}
Recent studies have revealed that using more test-time computation can enable LLMs to improve their outputs~\cite{TestTimeScaling}.
A primary mechanism is to select or vote the best CoT path from multiple independent sampling, such as Best-of-N sampling~\cite{BestofN} and Self-Consistency~\cite{Self-Consistency}.
Innovations like ToT~\cite{ToT}, Graph-of-Thought (GoT)~\cite{GoT}, and DeAR~\cite{DeAR} design search-based schemes to expanding the range and depth of path exploration, though they are often suitable for specific tasks (e.g., the Game of 24) as they require to pre-define a fixed candidate size for each node, leading to redundancy or insufficiency.

Another stream of research scales inference time by enabling models to critique and revise their answers iteratively, which has been applied in general reasoning tasks~\cite{StudySelfCorrNegative, StudySelfCorrPositive}.
Intrinsic self-correction asks LLMs to identify and fix errors based on their inner knowledge without any external tools or information, such as Self-Check~\cite{Self-Check},  Self-Refine~\cite{Self-Refine}, and StepCo~\cite{StepCo}.
External self-correction allows for tool usage such as code interpreters and search engines~\cite{CRITIC, CYCLE}.
Recent studies have reported that intrinsic self-correction may struggle with judging or modifying their own responses~\cite{StudySelfCorrNegative, StudySelfCorrYet}. Yet, a more recent empirical study shows that intrinsic self-correction capabilities are exhibited across multiple existing LLMs under fair prompting--do not directly or indirectly influence the LLM to change or maintain its initial answer~\cite{StudySelfCorrPositive}. 
% Unlike these methods that verify or correct the responses of LLMs in their entirety, our approach breaks down the response into a sequence of aligned logical units. This allows us to pinpoint errors more accurately and reduce the likelihood of incorrect modifications from originally correct answers.




