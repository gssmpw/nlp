\begin{figure*}[htbp]
    \centering
    \vspace{-0.1in}
        {\includegraphics[width=0.8\linewidth]{figures/RaLU.pdf}}
    \vspace{-0.1in}
    \caption{Illustrating the three-stage process of \tool: Logic Unit Extraction, Logic Unit Alignment, and Solution Synthesis for operationalizing synergy in reasoning tasks.}
    \vspace{-0.2in}
    \label{fig:RaLU}
\end{figure*}

\section{Reasoning-as-Logic-Units}
We propose a novel structured test-time scaling framework, \tool, which enforces alignment between NL descriptions and code logic to leverage both sides. Programs ensure rigorous logical consistency through syntax and execution constraints, whereas NL provides intuitive representations with problem semantics and human reasoning patterns.

Specifically, \tool operationalizes this synergy through three iterative stages (as shown in Figure~\ref{fig:RaLU}): \textit{Logic Unit Extraction}, \textit{Logic Unit Alignment}, and \textit{Solution Synthesis}.
The first stage decomposes an initially generated program into atomic logic units via static code analysis. Then, an iterative multi-turn dialogue engages the LLM to 1) explain each unit’s purpose in NL, grounding code operations in problem semantics, 2) validate computational correctness and semantic alignment with task requirements, and 3) correct errors via a rollback-and-revise protocol, where detected inconsistencies trigger localized unit refinement. The validated units form a cohesive, executable reasoning path. The final stage synthesizes this path into a human-readable solution, ensuring the final answer inherits the program’s logical rigor while retaining natural language fluency.

In this way, \tool can significantly mitigate reasoning hallucinations.
%bridges these complementary modalities by decomposing reasoning into atomic logic units.
First, each unit seamlessly pairs executable code with NL explanations to address the type-one hallucination through explicit alignment of local logic.
Second, the LLM focuses on only one unit per response in case of missing a crucial step or introducing an irrelevant step, and iterative verification ensures the LLM to notice all problem constraints
Third, these logic units are interconnected rigorously along the program structure, ensuring logical coherence of the reasoning path.

To sum up, by structurally enforcing bidirectional alignment between code logic and textual justifications, we build a self-consistent reasoning path where computational validity and conceptual clarity mutually reinforce each other. This architecture not only minimizes logical discrepancies but also provides transparent intermediate steps for error diagnosis and refinement.

\subsection{Logic Unit Extraction}
\tool begins with prompting the LLM to generate an initial program that serves as a reasoning scaffold for the task. While possibly imperfect, this program approximates the logical flow required to derive a solution, providing a structured basis for refinement.

We apply static code analysis to construct a Control Flow Graph (CFG), where nodes represent basic blocks (sequential code statements), and edges denote control flow transitions (e.g., branches, loops). 
A CFG explicitly surfaces a program’s decision points and iterative structures, whose details are illustrated in Appendix~\ref{app:example:CFG}.
\tool then partitions the code into atomic units by dissecting the CFG at critical junctions—conditional blocks (if/else), loop boundaries (for/while), and function entries. Each unit encapsulates a self-contained computational intent, such as iterating through a list or evaluating a constraint.


\subsection{Logic Unit Alignment}
The alignment stage iteratively validates and refines logic units through a stateful dialogue governed by:
%
\begin{equation}
\mathcal{V}_i = \text{LLM}\Big(\underbrace{\mathcal{S}} \oplus \underbrace{\bigoplus_{k=0}^{i-1} \mathcal{U}_k} \oplus \underbrace{\mathcal{P}(\mathcal{U}_i)}\Big)
\end{equation}
%
where $\mathcal{U}_i$ denotes the $i$-th unit, $\mathcal{S}$ is the task specification, and the operator $\oplus$ represents contextual concatenation.
$\mathcal{P}(\mathcal{U}_i)$ instructs the LLM to handle the $i$-th unit, where each turn of interaction is responsible for judging the correctness, modifying it upon errors, and explaining it to align with the task specification.
%
Thus, each response $\mathcal{V}_i = \langle \mathcal{J}_i, \widetilde{\mathcal{U}}_i \rangle$ comprises a judgment token $\mathcal{J}_i \in \{\texttt{OK}, \texttt{WRONG}\}$ and a refined unit $\widetilde{\mathcal{U}}_i$.
The refinement adheres to:
%
\begin{equation}
\tilde{\mathcal{U}}_i = \begin{cases}
\mathcal{U}i & \text{if } J_i = \texttt{OK} \\
\text{LLM}_{\text{repair}}\big(\mathcal{S}, \mathcal{U}_i, {\tilde{\mathcal{U}}_k},\, {k < i}\big) & \text{otherwise}
\end{cases}
\end{equation}

To prevent error cascades, corrections trigger a partial rewind: the original unit $\mathcal{U}_i$ is replaced by the refined version $\tilde{\mathcal{U}}_i$ in the interested reasoning path. Then, $\tilde{\mathcal{U}}_i$ will be re-validated based on previous units $\{\mathcal{U}_k|k<i\}$.
This aims to construct a path $\mathcal{P}$ with all nodes able to pass self-judging:
\begin{equation}
\forall \mathcal{U}_k \in \mathcal{P}=\{\mathcal{U}_1, \cdots, \mathcal{U}_{i-1}\}, \quad \mathcal{J}_k = \texttt{OK}.
\end{equation}

The correctness process terminates under two conditions: 1) fixed-point convergence, i.e., all units satisfy $J_i = \texttt{OK} \land \tilde{\mathcal{U}}_i = \mathcal{U}_i$, indicating that no further are refinements needed; and 2) a predefined iteration limit or confidence threshold is reached.
Upon triggering the second condition, multiple candidate units will exist, and we select the optimal version $\tilde{\mathcal{U}}_i^*$ using a normalized confidence metric.
In this case, there are multiple candidates for a unit, and none of them has been judged as correct. 
We select the most confident response. 
The confidence score is calculated as the following equation~\ref{eq:confidence}, based on the log probabilities, which express token likelihoods on a logarithmic scale $(-\infty, 0]$, reported by the LLM.
%
\begin{align}\label{eq:confidence}
 \text{Conf}(\tilde{\mathcal{U}}) = \frac{1}{n}\sum{j=0}^{n-1} \sigma(lp_j) \\
 \sigma(lp_j) = \min\big(e^{lp_j} + 0.005, 1\big) \times 10^{-2}.
\end{align}
%
where $lp_j$ denotes the log probability of the $j$-th token in the LLM’s response, mapped to a [0,1] scale via the clamping function $\sigma$. 
For LLMs lacking log probability outputs, we employ a self-consistency checking process--prompting the same LLM ranks candidates to determine $\tilde{\mathcal{U}}_i^*$.

Herein, we discuss whether $\tilde{\mathcal{U}}$ is more likely to be correct than its original version $\mathcal{U}$ for any unit, that is $P(\mathcal{U} \text{ is correct}) = p < P\big(\tilde{\mathcal{U}}) \text{ is correct}\big) = p'$.
Let's define $\alpha = P(J(\mathcal{U})=\text{OK} | \mathcal{U}\text{ is correct}$) (true positive rate) and
$\beta = P(J(\mathcal{U})=\text{WRONG} | \mathcal{U}\text{ is incorrect}$) (true negative rate).

Thus, we have:
\begin{equation}
p' = \alpha p + \gamma_{repair}[(1-\alpha)p + (1-\beta)(1-p)]
\end{equation}
where $\gamma_{repair} = P(R(\mathcal{U})\text{ is correct} | J(U)=\texttt{WRONG})$ with $R(\cdot)$ representing the LLM's repair action. Then, the condition of $p'> p$ is transformed as:
\begin{equation}
\gamma_{repair} > P(\mathcal{U}\text{ is correct} | J=\texttt{WRONG)}.
\end{equation}
See Appendix~\ref{app:RaLU:repair} for the detailed derivation.
Empirical studies show that modern LLMs can achieve high accuracies when serving as a judge~\cite{JudgeStudy} (where $\alpha$ can reach 0.9+), so the above condition can be easily achieved with intelligent LLMs.
Nevertheless, if the model is almost perfect ($p \approx 1$), then using \tool cannot make significant improvement even though ($p' > p$).

In addition to evaluating and refining the unit, the LLM is tasked with generating explanations that explicitly map the unit’s behavior to the task specification. These explanations serve two critical roles.
First, they help to justify whether the unit aligns with or violates the intended logic.
Second, they demystify the reasoning process, exposing the LLM’s thinking about execution behavior in human-interpretable terms.
By linking concrete code elements to abstract specification requirements, the LLM acts as a translator between implementation and intent. This dual focus on correctness and explainability ensures that both the code and its rationale evolve cohesively during refinement.


\subsection{Solution Synthesis}
Through logic unit alignment, \tool constructs a coherent sequence of verified operations paired with precise NL explanations. This establishes a unified reasoning path that integrates computational logic with interpretive alignment (with problem specifications), ensuring rigorous consistency between code behavior and reasoning steps.
Guided by this aligned reasoning path, the LLM synthesizes the structured units into a final solution using the following prompt: \textit{``Based on the previously verified reasoning path, generate a correct program to solve the given problem."}

This dual-anchoring mechanism--enforcing program-executable logic and specification-aligned reasoning--eliminates ambiguities for response generation. 
% Such a framework guarantees that solutions inherit the reliability of validated logic units, ensuring interoperability between symbolic computation and human-interpretable reasoning.
We formalize the effectiveness of \tool through a Bayesian inference lens, demonstrating how iterative logic unit alignment systematically amplifies the likelihood of generating correct programs.

Let $C$ denote the event where the LLM produces a program correctly solving the task, and $\overline{C}$ its complement. Each logic unit $O_i (1 \leq i \leq n)$ represents a verified reasoning step aligned with both program execution and problem semantics.
By Bayes’ theorem, the posterior probability of correctness, conditioned on validated units, is:
\begin{align}
P(C|O_1, \ldots, O_n) = \frac{P(O_1, \ldots, O_n | C) \cdot P(C)}{P(O_1, \ldots, O_n)} \\
= \frac{P(O_1, \ldots, O_n | C)\cdot P(C)}{P(O_1, \ldots, O_n | C)P(C) + P(O_1, \ldots, O_n | \overline{C})P(\overline{C})}
\end{align}

Note that a correct program inherently exhibits logical coherence, making its reasoning steps more likely to align with human-judged validity. Thus, we have $P(O_1,\cdots, O_n|C) >> P(O_1,\cdots, O_n|\overline{C})$. This asymmetry implies:
\begin{align}
\frac{P(O_1, \ldots, O_n | C)}{P(O_1, \ldots, O_n)} \geq 1 \implies P(C|O_1, \ldots, O_n) > P(C)
\end{align}
Hence, \tool’s rewind-and-correct mechanism—by enforcing consistency across units—statistically elevates the prior correctness probability $P(C)$ (initial program quality) to a higher posterior $P(C|O_1, \cdots, O_n)$. This Bayesian progression quantifies how structured, self-validated reasoning suppresses hallucinations, ensuring solutions inherit rigor from aligned logic units.

Crucially, even if generating incorrect solutions, \tool maintains granular traceability through self-contained logic units. This enables precise identification of defective components responsible for errors, rooted in the framework's transparency. By transforming black-box reasoning into more debuggable processes, \tool accelerates error correction and enhances interpretability for human-AI collaboration.