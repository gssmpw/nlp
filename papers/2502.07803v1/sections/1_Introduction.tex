\section{Introduction}
%% Overall Background
Extensive studies have shown that Chain-of-Thought (CoT) ~\cite{CoT} prompting can improve the reasoning capabilities of large language models (LLMs)~\cite{SurveyBeyondAcc, SurveyCoTHub, SurveyEvalLogicGPT} by requiring LLMs to generate a rationale before its final decision.
Complementary to CoT and its variants, program-aided techniques like Program-of-Thought (PoT)~\cite{PoT} have emerged, which decompose complex reasoning and numerical computation by prompting LLMs to generate programs and use external interpreters to solve mathematical problems.
When combined with test-time scaling, which dynamically allocates more computational resources during inference, these methods optimize LLM performance in reasoning tasks, particularly in mathematical and algorithmic domains~\cite{Evalo1}.

%% Problems of previous methods
Despite these advancements, both CoT and PoT face significant challenges.
The inherent ambiguity and imprecision in natural language (NL) impede precise calculations in CoT-like methods~\cite{SurveyNLReason, PAL}.
Meanwhile, PoTs simply replace NL reasoning with programs, so they cannot improve LLM in code generation, yet solving algorithmic problems is an important aspect of LLM reasoning.
Plus, crafting accurate programs in a single attempt remains challenging~\cite{FunCoder}, so PoT can even introduce more errors than CoT sometimes~\cite{HTL}.

%% Our target challenge: reduce Reasoning Hallucinations
Furthermore, simple combinations of CoT and PoT cannot yield satisfactory outcomes. Research indicates that guiding an LLM to generate step-by-step analysis in NL before deriving programs may not outperform direct prompting~\cite{CodeCoT}.
This underperformance can be traced to inconsistencies between reasoning steps and the logic in generated programs, which we term \textbf{``Reasoning Hallucinations."} 
The hallucinations manifest as: 1) accurate NL step descriptions but logical errors in individual code statements; 2) missing key steps or inclusion of irrelevant ones; and 3) correct steps misordered or improperly connected. Examples of these three types are presented in Figure~\ref{fig:hallucinations}.

\begin{figure*}[htbp]
    \centering
    \vspace{-0.1in}
        {\includegraphics[width=0.98\linewidth]{figures/hallucinations.pdf}}
    \vspace{-0.1in}
    \caption{Motivating examples reflecting the reasoning hallucinations: In Example 1, the LLM attempts to factor out the negative sign as stated yet wrongly neglects to reverse the positions of $x$ and $y$ in the expression. In Example 2, the LLM overlooks the NL step that indicates using the leftover to calculate the money earned; instead, it is misled by the question description to sell all the sets, thus omitting this crucial step. In Example 3, the LLM incorrectly places the loop termination conditions after operating on both odd- and even-numbered rows, which should be positioned individually.}
    \vspace{-0.2in}
    \label{fig:hallucinations}
\end{figure*}

These reasoning hallucinations arise from the statistical nature of LLMs, which generate responses based on token predictions rather than true reasoning.
LLMs mimic reasoning by reproducing patterns linked to logical explanations but are trained on text that often lacks rigorous logical coherence~\cite{Cyc}.
The scarcity of detailed mapping from NL reasoning to precise, logical expressions like code or equations means LLMs struggle to capture the underlying logic patterns under former expressions, not to mention reproducing such patterns, resulting in such inconsistencies.
Moreover, previous findings suggest this misalignment also affects other reasoning tasks, raising doubts about the authenticity of reported reasoning steps~\cite{HTL}. 
% Indeed, we have no way of knowing if the reported steps really belong to the underlying reasoning process.
Unlike factual hallucinations that can be mitigated by introducing external information, reasoning hallucinations are intrinsic to the model's internal processing and pose a unique challenge in reliable LLM reasoning.

%% Our Method
To overcome this challenge, we propose a novel reasoning framework that leverages programs as the logical skeletons and natural language as explanatory content.
Our key insight is that if the two representations of reasoning processes, i.e., NL reasoning steps and generated programs, are aligned in the same fundamental logic, the reasoning path would be more reliable. Each reasoning step can be projected to a series of code statements, and the latter serves as the formalized implementation of the former.
Hence, we introduce our test-time scaling framework, \textbf{Reasoning-as-Logic-Units (\tool)}, whose reasoning path is compared to that of CoT, Self-consistency~\cite{Self-Consistency}, and Tree-of-Thought (ToT)~\cite{ToT} in Figure~\ref{fig:path-cmp} for illustration.
% \tool transforms static program analysis into hybrid logical reasoning processes, marking a paradigm shift in the prompting for LLM reasoning.

\begin{figure*}[htbp]
    \centering
    \vspace{-0.1in}
        {\includegraphics[width=0.8\linewidth]{figures/prompt_cmp.pdf}}
    \vspace{-0.1in}
    \caption{Schematic depicting multiple strategies for test-time scaling frameworks with LLMs. Each rectangular shape symbolizes a distinct thought (\textit{aka.} step/unit), a self-contained text sequence crucial as an intermediate stage in the reasoning process. In previous studies, all the thoughts are natural language-based, while our \tool uses logic units consisting of code statements and NL descriptions.}
    \vspace{-0.2in}
    \label{fig:path-cmp}
\end{figure*}


Specifically, our framework involves four core actions: self-reason, self-judge, self-explain, and self-correct, organized into three primary stages:
\textbf{1) Logic Unit Extraction:} 
The framework begins by directly generating a program to address the given problem. This program serves as a representation of the reasoning process. Using a static analysis tool, we create a control flow graph (CFG) to depict the program's logic.
\tool traverses this CFG, dividing it into logical units based on program branches like conditional and looping statements. 
Each unit comprises several code statements, implementing operations for problem-solving. 
%
\textbf{2) Logic Unit Alignment:}
\tool initiates an iterative dialogue with the same LLM to assess the correctness of each logic unit.
Beyond being a judge, the LLM explains the operations within each unit to ensure alignment with the problem specification. 
Should errors arise, the LLM self-corrects the unit, and the dialogue will rewind to the previous round for re-evaluation on the corrected unit.
The reasoning path branches out until correctness is achieved or a predefined threshold is reached.
%
\textbf{3) Solution Synthesis:}
After processing all logic units, we obtain a reasoning path where each node is verified or contains a confident self-corrected version of code statements and NL explanations. 
Using this hybrid reasoning path as a conversation history, the LLM generates the final solution to the reasoning task.

%% Experiments
We evaluate \tool on four benchmarks, including two for mathematical reasoning (GSM8K~\cite{GSM8K}, MATH~\cite{MATH}) and the other two for code reasoning: HumanEval~\cite{HumanEval}, MbPP~\cite{Mbpp}, and their plus versions~\cite{Evalo1}.
The evaluation involves three LLM backbones: Deepseek-V3, Llama3.3-70B-Instruct and Qwen2.5-72B-Instruct.
Experimental results show that \tool achieves a significant improvement in final answer accuracies or pass@1 compared with best-performing baselines, with specific improvement of 1.22\%, 2.07\%, 6.60\%, and 2.17\% on these four benchmarks, respectively.
It is worth noting that \tool outperforms the best-performing reasoning model family, o1, on HumanEval+ and MbPP+.
We further perform an extensive ablation study to demonstrate the contributions to our key design in \tool.
Our code is available at \url{https://github.com/acceptallgood/RaLU}.

% Our key contributions are:
% 1) We identify challenges in natural language-based reasoning, programming-based reasoning, and their combinations, introducing the concept of reasoning hallucinations to describe inconsistencies between LLM-reported reasoning steps and generated programs.
% 2) We propose a novel test-time scaling framework, \tool, which aligns logic units between natural language reasoning and programs without requiring example crafting or external feedback, applicable to various reasoning tasks.
% 3) We conduct extensive experiments comparing \tool with advanced baselines across different LLMs in mathematical reasoning and algorithmic programming, demonstrating its superiority and robustness. 




