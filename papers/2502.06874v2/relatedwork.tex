\section{Related Work}
\subsection{Machine Learning in Sector Classification}
Machine Learning (ML) methods have been extensively explored for automating sector classification, a task traditionally reliant on expert-based taxonomies (e.g., GICS, NAICS). In a typical setup, each sample \(x_i\) (e.g., firm-level features, textual descriptions, or both) is mapped to a label \(y_i\) from a predefined category set \(\mathcal{Y}\). One seeks a classifier
\[
    f_{\theta}: X \to y,
\]
parameterized by \(\theta\). Minimizing a suitable loss, such as
\[
    \hat{\theta} \;=\; \arg\min_{\theta} \; \frac{1}{N}\sum_{i=1}^{N} \ell\bigl(f_{\theta}(x_i),\, y_i\bigr) \;+\; \lambda\,\Omega(\theta),
\]
lies at the core of traditional supervised learning. However, purely human-assigned labels face key obstacles: inconsistent coding across experts~\cite{sylolypavan2023impact}, limited coverage of new or cross-sector activities, poor scalability, and high annotation costs~\cite{sectorclassify9, sectorclassify10}. These limitations motivate automated, data-driven approaches.

Efforts to automate sector classification have evolved through three main stages. \textbf{Stage~I: Traditional ML on Tabular Data.} Early work leveraged structured firm attributes (e.g., financial statements) with models like Random Forests, K-Nearest Neighbors, and SVMs~\cite{sectorclassify4, sectorclassify5, sectorclassify6, sectorclassify9, sectorclassify12, sectorclassify1}, but often faced small datasets, domain shifts, and limited representational power. \textbf{Stage~II: Text-Based Frequency Models.} With growing availability of unstructured data (e.g., 10-K reports, descriptions), researchers used Bag-of-Words or TF-IDF transformations fed into classifiers like MLPs~\cite{sectorclassify4, sectorclassify5, sectorclassify8, sectorclassify7, sectorclassify11}. However, they still struggled with shallow context and large label spaces. \textbf{Stage~III: Transformer-Based LLMs.} Modern approaches employ pre-trained Transformers such as BERT or Sentence-BERT (SBERT), which encode deeper semantics and demonstrate strong zero-shot or fine-tuned performance~\cite{jain2024empowering, balaji2023flamingo, balaji2023caml, sectorclassify3}. These methods surpass older models but require large-scale computing, careful domain adaptation, and open-source data to maintain reproducibility.

While LLMs offer state-of-the-art accuracy, challenges persist around dataset openness, computational demands, and extending classification beyond narrow taxonomies toward broader tasks like emission estimation. Future advances in flexible, interpretable, and efficient LLMs will be critical for real-world industrial applications.

\subsection{Self-Supervised Contrastive Learning Framework}
Self-supervised learning (SSL) has emerged as a powerful representation learning paradigm that does not require large labeled datasets. Instead, the model learns from inherent data structures, creating \emph{positive} and \emph{negative} instances by various transformations or pairing strategies. SSL shifts away from cross-entropy on labeled samples \(\{(x_i,y_i)\}\) and instead uses contrastive losses to align similar views of the same data point while separating different samples.

Initial advances in SSL stemmed from the image domain, with frameworks like SimCLR~\cite{SimCLR} and MoCo~\cite{MoCo} leveraging an InfoNCE loss to bring positive pairs (augmented views of the same image) closer in latent space relative to a set of negatives. Later works like BYOL~\cite{contractiveBYOL} and SimSiam~\cite{Siamese} showed negative-free designs. In NLP, models such as SBERT~\cite{reimers2019sentence} and SimCSE~\cite{gao2021simcse} adapted contrastive principles to sentence embeddings, enabling robust similarity measures with minimal or no labeled data. Contrastive methods have thus evolved into a general framework for embedding diverse data types (images, text, multimodal) into semantically meaningful spaces.

\subsection{GHG Emission Estimation by Ecological Economic Framework}
Over 70\% of enterprises estimate their carbon footprints using \emph{sector-based carbon intensity factors}, representing GHG emissions produced per unit of economic output in a given sector and region~\cite{dumit2024atlas}. The Environmentally Extended Multi-Regional Input–Output (EE-MRIO) framework provides a structured way to derive these intensities by integrating economic transactions and regional environmental data~\cite{wiedmann2009review, leontief1963multiregional}. The carbon intensity factor is defined as the ratio of a sector’s total emissions to its economic output. While the EE-MRIO framework offers a comprehensive view of inter-sector linkages, its deployment in real industrial applications is hindered by expensive data and domain expertise requirements~\cite{dietzenbacher2013construction, aguiar2016overview}.