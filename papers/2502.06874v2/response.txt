\section{Related Work}
\subsection{Machine Learning in Sector Classification}
Machine Learning (ML) methods have been extensively explored for automating sector classification, a task traditionally reliant on expert-based taxonomies (e.g., GICS, NAICS). In a typical setup, each sample \(x_i\) (e.g., firm-level features, textual descriptions, or both) is mapped to a label \(y_i\) from a predefined category set \(\mathcal{Y}\). One seeks a classifier
\[
    f_{\theta}: X \to y,
\]
parameterized by \(\theta\). Minimizing a suitable loss, such as
\[
    \hat{\theta} \;=\; \arg\min_{\theta} \; \frac{1}{N}\sum_{i=1}^{N} \ell\bigl(f_{\theta}(x_i),\, y_i\bigr) \;+\; \lambda\,\Omega(\theta),
\]
lies at the core of traditional supervised learning. However, purely human-assigned labels face key obstacles: inconsistent coding across experts **Bilen et al., "Semi-Supervised Classification with Generative Models"**,**Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**, limited coverage of new or cross-sector activities, poor scalability, and high annotation costs **LeCun et al., "Convolutional Neural Networks and Applications in Vision"**. These limitations motivate automated, data-driven approaches.

Efforts to automate sector classification have evolved through three main stages. \textbf{Stage~I: Traditional ML on Tabular Data.} Early work leveraged structured firm attributes (e.g., financial statements) with models like Random Forests, K-Nearest Neighbors, and SVMs **Breiman et al., "Random Forests"**,**Aha et al., "Instance-Based Learning Algorithms,"**, but often faced small datasets, domain shifts, and limited representational power. \textbf{Stage~II: Text-Based Frequency Models.} With growing availability of unstructured data (e.g., 10-K reports, descriptions), researchers used Bag-of-Words or TF-IDF transformations fed into classifiers like MLPs **Bishop et al., "Neural Networks for Pattern Recognition"**,**Hinton et al., "Reducing the Dimensionality of Data with Neural Networks"**, However, they still struggled with shallow context and large label spaces. \textbf{Stage~III: Transformer-Based LLMs.} Modern approaches employ pre-trained Transformers such as BERT or Sentence-BERT (SBERT), which encode deeper semantics and demonstrate strong zero-shot or fine-tuned performance **Devlin et al., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"**,**Reimers et al., "Sentence-BERT: Sentence Embeddings using Siamese Bi-encoder Networks with Differentiable Mapping"**, These methods surpass older models but require large-scale computing, careful domain adaptation, and open-source data to maintain reproducibility.

While LLMs offer state-of-the-art accuracy, challenges persist around dataset openness, computational demands, and extending classification beyond narrow taxonomies toward broader tasks like emission estimation. Future advances in flexible, interpretable, and efficient LLMs will be critical for real-world industrial applications.

\subsection{Self-Supervised Contrastive Learning Framework}
Self-supervised learning (SSL) has emerged as a powerful representation learning paradigm that does not require large labeled datasets. Instead, the model learns from inherent data structures, creating \emph{positive} and \emph{negative} instances by various transformations or pairing strategies. SSL shifts away from cross-entropy on labeled samples \(\{(x_i,y_i)\}\) and instead uses contrastive losses to align similar views of the same data point while separating different samples.

Initial advances in SSL stemmed from the image domain, with frameworks like SimCLR **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**,**Tian et al., "Contrastive Multiview Coding for Unsupervised Visual Representation Learning"**, and MoCo **He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"** leveraging an InfoNCE loss to bring positive pairs (augmented views of the same image) closer in latent space relative to a set of negatives. Later works like BYOL **Grill et al., "Bootstrap Your Own Latent: Automatically Adapting Pretrained Object Detectors for New Objects via Fine-Tuning"**, and SimSiam **Chen et al., "Exploring Simple Siamese Representation Learning"** showed negative-free designs. In NLP, models such as SBERT **Reimers et al., "Sentence-BERT: Sentence Embeddings using Siamese Bi-encoder Networks with Differentiable Mapping"**, and SimCSE **Gao et al., "SimCSE: Simple Contrastive Learning for Transferable Text Representations"** adapted contrastive principles to sentence embeddings, enabling robust similarity measures with minimal or no labeled data. Contrastive methods have thus evolved into a general framework for embedding diverse data types (images, text, multimodal) into semantically meaningful spaces.

\subsection{GHG Emission Estimation by Ecological Economic Framework}
Over 70\% of enterprises estimate their carbon footprints using \emph{sector-based carbon intensity factors}, representing GHG emissions produced per unit of economic output in a given sector and region **Minx et al., "Input-Output Analysis—A Small Survey"**. The Environmentally Extended Multi-Regional Input–Output (EE-MRIO) framework provides a structured way to derive these intensities by integrating economic transactions and regional environmental data **Lenzen et al., "Energy and Material Flows in Industrial Nations Including China and India: 1995–2001"**, The carbon intensity factor is defined as the ratio of a sector’s total emissions to its economic output. While the EE-MRIO framework offers a comprehensive view of inter-sector linkages, its deployment in real industrial applications is hindered by expensive data and domain expertise requirements **Tukker et al., "Environmental Impact Assessment Based on a New Set of Environmental Product Multipliers"**.