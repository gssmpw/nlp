\section{Conditions for Generalizaion}

\begin{table}[h!]
\caption{Conditions for Generalization (described in Eq (\ref{eq:optimal}))}
\begin{centering}
\resizebox{0.7\width}{!}{ %
\begin{tabular}{lccl}
\toprule
\textbf{Conditions} & & \textbf{type}  & \textbf{verifiable} \\
\midrule
label-identifiability & & Necessary & No (Data dependency)\\
\midrule
Causal support & & Necessary & No (Data dependency) \\
\midrule
Optimal for all training domains & &Necessary & Yes\\
\midrule
\textbf{Sufficient Representation Function} & & \textbf{Necessary} & \textbf{Yes (Missing part)}\\
\midrule
Sufficient domains & & Sufficient & No \\
\midrule
Invariant Representation Function & & Sufficient & No \\
\bottomrule
\end{tabular}}
\par\end{centering}
\label{tab:pretained}
\end{table}



We examine important assumptions to achieve generalization described in Eq (\ref{eq:optimal}). We start with an assumption so that solution for it exists.
%We note that the following discussion may not be entirely new and exist in some other forms dispersing across the literature. We here make the first contribution to unifying them into a systematic and comprehensive analysis. 

\begin{assumption} (Label-identifiability). We assume that for any pair $z^{'}_c, z^{'}_c\in \mathcal{Z}_c$,  $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c) \text{ if } \psi_x(z_c,z_e,u_x)=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$
\label{as:label_idf}.
\end{assumption}

The causal graph indicates that $Y$ is influenced by $z_c$, making $Y$ identifiable over the distribution $\mathbb{P}(Z_c)$. This assumption implies that different causal factors $z_c$ and $z^{'}_c$ cannot yield the same $x$, unless the condition $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c)$ holds, or in other words, the distribution $\mathbb{P}(Y\mid x)$ is stable. This assumption also can be view as covariate shift setting in OOD 
\cite{shimodaira2000improving}.
%\cite{shimodaira2000improving,bickel2009discriminative}. 

\begin{proposition} (Invariant Representation Function)
Under Assumption.\ref{as:label_idf}, there exists a set of deterministic representation function $\mathcal{G}_c\neq \emptyset$ such that for any $g\in \mathcal{G}_c$, there exists a classifier $h$ for which $h(g(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e) \text{ for some }z_e\}$  (Proof in
Appendix \ref{thm:invariant_correlation_apd}).   
\label{thm:invariant_correlation}
\end{proposition}


\begin{theorem} $\mathcal{F}^*\neq \emptyset$ if and only if Assumption~\ref{as:label_idf} is hold. (Proof in
Appendix \ref{thm:existence_apd}).  
\label{thm:existence}
\end{theorem}


Assumption \ref{as:label_idf} gives rise to a family of invariant representation function $\mathcal{G}_c$, as stated in Proposition \ref{thm:invariant_correlation}. In such works as \cite{ruan2021optimal, zhang2023causal}, the existence of this family is directly assumed, without stating the conditions under which it holds. Assumption \ref{as:label_idf} thus sheds light on this. More importantly, Theorem~\ref{thm:existence} establishes that Assumption~\ref{as:label_idf} is the necessary and sufficient condition for the existence of a global optimal hypothesis.

While Assumption \ref{as:label_idf} establishes the existence of a global optimal hypothesis, it is unclear as to whether it is possible to learn a global optimal hypothesis from a finite number of training domains. The next discussion addresses this question.
% While Assumption \ref{as:label_idf} is necessary for the existence of a global optimal hypothesis, it is not sufficient to establish a method for learning a global optimal hypothesis from a finite number of training domains, which is our focus. To address this, we introduce another assumption that enables the learning of a global optimal hypothesis from a finite number of training domains, as follows:

\begin{assumption} (Causal support). The mixture of training domain distributions is denoted as $\mathbb{P}^{\pi}=\sum_{e\in \mathcal{E}_{tr}}\pi_{e}\mathbb{P}^{e}$, where the mixing coefficients $\pi=\left[\pi_{e}\right]_{e\in \mathcal{E}_{tr}}$ can be conveniently set to $\pi_{e}=\frac{N_{e}}{\sum_{e'\in \mathcal{E}_{tr}}N_{e'}}$ with $N_{e'}$ being the training size of the training domain $\mathbb{P}^{e'}$. We assume support of causal factor $\mathbb{P}^{\pi} \left (Z_c \right )$: $\text{supp}\{\mathbb{P}^{\pi} \left (Z_c \right )\}=\mathcal{Z}_c$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
\label{as:sufficient_causal_support}
\end{assumption}

% \Vy{While previous works assume sufficient causal support across \textit{all} domains \cite{mitrovic2020representation, ruan2021optimal, zhang2023causal}, 
% Assumption~\ref{as:sufficient_causal_support} is a more relaxed condition, which only requires sufficient causal support from a mixture of training domain distributions.}

% \Vy{The following theorem asserts that under Assumption~\ref{as:sufficient_causal_support}, it is feasible to learn a global optimal hypothesis from finite training domain data:}

\begin{theorem} Denote the set of \textbf{domain optimal hypotheses} of $\mathbb{P}^\pi$ induced by $g\in \mathcal{G}$: 
    \begin{equation*}
        \mathcal{F}_{\mathbb{P}^\pi,g}=\left \{h\circ g \mid h\in\underset{h'\in \mathcal{H}}{\rm{argmin }} \mathcal{L}\left ( h'\circ g, {\mathbb{P}^{\pi}} \right )  \right \}.
    \end{equation*} 
(i) If $\mathcal{F}_{\mathbb{P}^\pi,g}  \subseteq \mathcal{F}^{*}$, then Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support} are hold.

(ii) Under Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support} are hold, if $g\in \mathcal{G}_c$, then $\mathcal{F}_{\mathbb{P}^\pi,g}  \subseteq \mathcal{F}^{*}$. 

(Proof in Appendix \ref{thm:single_generalization_apd})
\label{thm:single_generalization}
\end{theorem}

Theorem \ref{thm:single_generalization} (i) shows that Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support} are indeed necessary conditions for learning global optimal hypothesis from finite number of training domains. However, they are inherently derived from the data-generation process and thus often appear as assumptions.

Theorem \ref{thm:single_generalization} (ii) demonstrate that under Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support} $g_c\in \mG_c$ is the \textit{sufficient condition} $f^*\in\mathcal{F}^*$ for learning global optimal hypothesis from finite number of training domains. Therefore, \textit{invariant representation function} becomes the anchor point for DG algorithms.



\paragraph{Remark.} Despite the significance of the above theoretical findings, we argue that they provide little value in evaluating the effectiveness of a DG algorithm. Existing studies predominantly leverage Theorem~\ref{thm:single_generalization} as a motivation for learning invariant representations \Vy{@Long to cite}, which however can only guarantee globally invariant representation in the limit of infinite domains. Thus, the recovery of the true $g_c$ is non-verifiable given finite training domains, and by Theorem 3.5 it means that one cannot guarantee the global optimal hypothesis is attainable. 

On the other hand, the algorithms are assessed and compared based on empirical evidence. We argue that empirical results are not sound to assess the effectiveness. We later show that this is generally of little use since there always exist a bad domain where the algorithm fails (See Corollary 4.3)

In this work, we propose to shift the focus of the analysis from invariant representation towards \textit{sufficient representation function} (See Definition~\ref{def:sufficient}), which gives rise to two additional conditions for generalization based on which we can soundly benchmark DG algorithms on finite domains. In the next section, we show that these conditions are necessary and if an algorithm does not satisfy our two conditions, it is impossible to achieve the global optimal hypothesis. 



\subsection{Necessary Conditions for Global Optimal Hypothesis}



By definition of global optimal hypothesis, $f\in \mathcal{F}^*$ is also the optimal solution across all domains in $\mathcal{E}$\ie, $\mathbb{P}^e \sim \mathcal{P}$. Although this does not inherently imply the reverse, it does lead us to the first \textit{necessary condition} for a global optimal hypothesis: specifically, $f$ must be the optimal hypothesis within each training domain, denoted as $f \in \mathcal{F}_{\mathbb{P}^e}$ for every $e \in \mathcal{E}_{tr}$.


Previous result indicate that under Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support}, given the invariant representation function $g_c$ within the set $\mathcal{G}_c$ it is possible to learn these global optimal hypotheses from finite number of training domains. However this might not be achievable because this would require all domains being observed, therefore, analyzing the set of invariant representation function $\mG$ is also not informative to develop DG algorithm on finite number of training domains. 

While precisely determining the invariant correlation $g_c$ is nearly unattainable, 
this concept can serve as a pivotal reference in defining a second \textit{necessary condition} for $g$ which latter in the next section, being one of two key condition to enable us analyse the generalization ability of DG algorithm using one training domains. This \textit{necessary condition} is described in the subsequent definition.

\begin{definition} (Sufficient Representation Function) $g\in \mathcal{G}_s$ is considered as sufficient representation if there exists a function $\phi\in \Phi$ such that:
$\phi\circ g \in \mathcal{G}_c$. 
\label{def:sufficient}
\end{definition}

The following theorem shows the necessary conditions for $g$ that ensure the potential to capture the global optimal hypothesis.

\begin{theorem}

Considering the training domains $\mathbb{P}^e$ and representation function $g$, let $\mathcal{H}_{\mathbb{P}^e,g}=\underset{h\in \mathcal{H}}{\text{argmin }}\mathbb{E}_{\mathbb{P}^e}\left [ \mathcal{L}\left ( h\circ g(x),y \right ) \right ]$ represent the set of optimal classifiers on $g\#\mathbb{P}^e$ (the push-forward distribution by applying $g$ on $\mathbb{P}^e$), \textbf{the best generalization classifier} from $\mathbb{P}^e$ to $\mathcal{P}$ is defined as 
\begin{equation}
\mathcal{F}^{B}_{\mathbb{P}^e,g}=\left \{ h\circ g \mid h \in \underset{ h'\in \mathcal{H}_{\mathbb{P}^e,g}}{\rm{argmin }} \sup_{e'\in \mathcal{E}} \mathcal{L}\left (  h'\circ g, \mathbb{P}^{e'} \right ) \right \}
\end{equation}  

Give representation function $g: \mathcal{X}\rightarrow \mathcal{Z}$ then $\forall \mathbb{P}^e\sim \mathcal{P}$ we have
$\mathcal{F}^B_{ \mathbb{P}^e,g} \subseteq \mathcal{F}^{*}$ if and only if $g\in \mathcal{G}_s$. (Proof in Appendix~\ref{thm:nacessary_apd})
\label{thm:nacessary}
\end{theorem}

This theorem demonstrates that if $g$ is not a sufficient representation ($g\notin \mathcal{G}_s$), it is impossible to find any hypothesis that would be globally optimal, even if an oracle observed all domains. 

Note that $\exists  \phi$ such that $\phi\circ g \in \mathcal{G}_c$ implies  data processing inequality \cite{blahut1987principles, cover1999elements}
formed the first-order Markov chain $X\rightarrow g(X)\rightarrow g_c(X)$. This induces the necessary condition for the existence of $\phi$, which is grounded in the mutual information constraint $\mathcal{I}(X,g(X))\geq \mathcal{I}(X,g_c(X))$. Given that $g_c$ is unknown, the most effective strategy is to maximize $\mathcal{I}(X,g(X))$. 
In practice, the sufficient representation condition is often overlooked in previous works, possibly because benchmarks use pretrained models that naturally satisfy this condition. However, our ablation study in Section~\ref{abl:sufficient} demonstrates the significant impact of this condition when evaluating DG algorithms without using pretrained models, emphasizing the importance of recognizing its significance.
% \noindent\fbox{\begin{minipage}{0.96\columnwidth}

% \end{minipage}}


\section{Generalization Analysis Based on Necessary Conditions}


Up to this point, we have pinpointed the essential requirements that a representation function and corresponding classifier must meet to potentially secure the global optimal hypothesis. However, meeting these conditions alone does not guarantee that a hypothesis will indeed be the global optimal hypothesis. For example, Corollary~\ref{thm:bad_domain_exist} reveals that even if a hypothesis $f = h \circ g$ meets both necessary conditions, it may still perform poorly in many target domains.

\begin{corollary}
Given $g\in \mathcal{G}_s$, there exists $f = h\circ g\in \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{g, \mathbb{P}^e}$ such that for any $0<\delta<1$, there are many undesirable target domains $\mathbb{P}^T \sim \mathcal{P}$ such that:
\begin{align*}
   \mathbb{E}_{(x,y)\sim\mathbb{P}^T} \left [ f(x) \neq f^*(x)  \right ]  \geq 1-\delta.
\end{align*}  with  $f^* \in \mathcal{F}^*$.
    \label{thm:bad_domain_exist}
(Proof in Appendix~\ref{thm:bad_domain_exist_apd})
\end{corollary}
This implies that benchmarking \textit{an algorithm $\mathcal{A}$} by evaluating the loss $\loss{f,P^T}$ of its learned model $f$ for any specific domain \citep{wiles2021fine, gulrajani2020search} is meaningless. The reason is that there might exist $f\in \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{g, \mathbb{P}^e}$ but $f\notin \mathcal{F}^*$ and if $f\notin \mathcal{F}^*$, there are many "bad" domains $P^T$ for which loss $\loss{f,P^T}$ is arbitrary large.

\begin{figure}[h!]
\begin{centering}
\subfloat{\centering{}\includegraphics[width=1.0\textwidth]{neurips2024/Figures/space_with_text1.png}}
\par\end{centering}

\caption{A schematic illustration of the hypothesis space across three training domains $e_1, e_2, e_3$.\label{fig:space}}
\vspace{-2mm}
\end{figure} 

Rather than evaluating a hypothesis $f$ based on its performance in specific target domains, our objective is to quantify the likelihood that a hypothesis, learned from \textit{an algorithm $\mathcal{A}$} given finite training domains $\mathcal{E}_{tr}$, being the global optimal hypothesis. Denote $\mathcal{F}_{\mathbb{P}^e}=\bigcup_{g\in \mathcal{G}_s}\mathcal{F}_{\mathbb{P}^e,g}$ the set of all training domain optimal hypothesis learned from training domain $\mathbb{P}^e$. 
Reapplying the result from Theorem~\ref{thm:single_generalization}, which states that $f \in \mathcal{F}$ learned from the set of training domains $\mathcal{E}_{tr}$ is a global optimal hypothesis, we deduce that $\mathcal{F}^*\subseteq \mathcal{F}_{\mathbb{P}^e}$, $\forall e\in \mathcal{E}_{tr}$, consequently, $\mathcal{F}^*\subseteq \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{ \mathbb{P}^e}$.This relationship is further illustrated in the Venn diagram in Figure \ref{fig:space}.

\begin{corollary}
 \label{thm:convergence}
Given sequence of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, denote $\mathcal{F}^{\cap}_{k}=\bigcap_{i=1}^{k}\mathcal{F}_{ \mathbb{P}^{e_i}}$, then:
\begin{equation*}
 \mathcal{F}^{\cap}_1\supseteq  \mathcal{F}^{\cap}_2 \supseteq... \supseteq  \mathcal{F}^{\cap}_K   
\end{equation*}
and
\begin{equation*}
        \lim_{K\rightarrow \left | \mathcal{E} \right |}\mathcal{F}^{\cap}_K \rightarrow \mathcal{F}^*.
    \end{equation*}
(Proof in Appendix~\ref{thm:convergence_apd})
\end{corollary}

Corollary \ref{thm:convergence} demonstrate that with a sufficiently large and diverse set of training domains, it becomes feasible to capture the optimal hypothesis. However, 
 
Given a finite number of training domains, our intuition is that an effective algorithm should have a high probability of reaching this optimal hypothesis. With this rationale, an algorithm $\mathcal{A}$ should be designed to minimize the cardinality of $ \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{ \mathbb{P}^e}$ while maintaining cardinality of $\mathcal{F}^*$ as the more overlap between $\mathcal{F}^*$ and $\Funion$, the more chance a element of $\Funion$ will also be a global hypothesis. 

\textbf{Remark:} Note that this observation offer us a criteria for assess DG algorithms using only training domain data. 






    
To deepen our understanding of this concept, we revisit various learning strategies within the domain of DG literature, providing a succinct evaluation of their strengths and weaknesses in Section \ref{sec:discussion_DG}.


    
% \begin{tcolorbox}[width=\columnwidth, 
% colback=white!10,
% left*=2mm,right*=2mm, top*=0mm,bottom*=0mm]
% \textbf{Takeaway-2} 
% \end{tcolorbox}


    
\subsection{Discussion on Domain Generalization Literature} 
\label{sec:discussion_DG}
DG approaches can primarily be categorized into three types: \textit{Representation Alignment}, \textit{Invariant Prediction}, and \textit{Augmentation}. In this section, we discuss their strengths and weaknesses in terms of the two necessary conditions and the hypothesis space that satisfies these conditions.

\textbf{Representation Alignment.} These approaches aim to learn a representation function $g$ for data $X$ such that $g(X)$ is invariant or consistent across different domains. Key studies like \cite{long2017conditional, ganin2016domain, li2018domain, nguyen2021domain, shen2018wasserstein, xie2017controllable, ilse2020diva} have focused on learning such domain-invariant representations by reducing the divergence between latent marginal distributions $\mathbb{E}[g(X) | E]$ where $E$ represents the domain environment. Other methods, on the other hand, align the conditional distributions $\mathbb{E}[g(X) | Y=y, E]$ across domains as seen in \cite{gong2016domain, li2018deep, tachet2020domain}. However, achieving true invariance is challenging and can be excessively limiting. In some instances, improved alignment of features leads to greater joint errors \cite{johansson2019support, zhao2019learning, phung2021learning}. However, theories \cite{johansson2019support, zhao2019learning, phung2021learning}

\begin{theorem}
%%\vspace{+1mm}
\label{theorem:tradeoff} Distance between two marginal distribution $\mathbb{P}^{e}(Y)$ and $\mathbb{P}^{e'}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right) \leq 
D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )
+\mathcal{L}\left ( \hat{f},\mathbb{P}^{e} \right )
+
\mathcal{L}\left ( \hat{f},\mathbb{P}^{e'} \right )
\end{aligned}
\end{equation*}

where $g_{\#}\mathbb{P}(X)$ denotes representation distribution on  representation space $\mathcal{Z}$ induce by applying encoder with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence \citep{zhao2019learning}, Hellinger distance \citep{phung2021learning} or Wasserstein distance \citep{le2021lamda} (cf. Appendix).
\end{theorem}

suggest that a substantial discrepancy in the label marginal distribution $D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right)$ across training domains may result in enforcing \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )$, thereby increasing \textit{domain-losses} $\left ( \mathcal{L}\left ( \hat{f},\mathbb{P}^{e} \right ) + \mathcal{L}\left ( \hat{f},\mathbb{P}^{e'}\right )\right )$. It's important to recognize that while the \textit{Representation Alignment} strategy could challenge \textbf{\textit{Condition-1}}, implementing an alignment constraint helps to reduce the cardinality of $ \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{ \mathbb{P}^e}$. Thus, performance improvements are still achievable with careful adjustment of the alignment weight, which, in essence, somewhat leverages the oracle knowledge of the target domain.

\textbf{Invariant Prediction.}  These methods aim to learn a consistent optimal classifier across domains e.g., the Invariant Risk Minimization (IRM) \cite{arjovsky2020irm} technique learns representation function $g(x)$ with invariant predictors $\mathbb{E}[Y | g(x), E]$. This goal aligns with \textbf{\textit{Condition-1}}, without imposing any restrictions that may negatively or positively affect \textbf{\textit{Condition-2}}. However, IRM may not provide any advantages over ERM, especially if the representations from different domains occupy distinct regions of the representation space, according to findings by \cite{ahuja2020empirical}. REx \cite{krueger2021out} further enforces equal risks across domains.  However, with a limited number of training domains, both IRM and REx may struggle to identify the optimal invariant predictor, as suggested by \cite{rosenfeld2020risks}.  IIB \cite{li2022invariant} aims to minimize invariant risks for nonlinear classifiers while simultaneously mitigating the impact of pseudo-invariant features and geometric skews by minimizing the mutual information between the inputs and their corresponding representations $\mathcal{I}(X,g(x))$. This constraint may adversely affect \textbf{\textit{Condition-2}}. Our ablation study in Section \ref{abl:sufficient} demonstrates that minimizing $\mathcal{I}(X,g(x))$ can impair generalization.



\textbf{Augmentation.} Data augmentation \cite{shankar2018generalizing, zhou2020deep, zhou2021domain, xu2021fourier, zhang2017mixup, wang2020heterogeneous, zhao2020maximum, yao2022improving,carluccidomain, yao2022pcl} have also been applied to DG. This fundamental strategy utilizes predefined or learnable transformations $T$ on the original sample $X$ or its features $g(x)$ to create augmented data $T(X)$ or $T(g(x))$. Applying various transformations during training effectively increases the training dataset, which, according to our principle, should narrow the search space. However, it's crucial that transformation $T$ maintains the integrity of the labels. This implies a \textit{necessity for some knowledge of the target domain} to ensure the transformations do not alter the label information \cite{gao2023out}. If transformation $T$ fails to be label-preserving, it risks violating \textbf{\textit{Condition-2}}.


