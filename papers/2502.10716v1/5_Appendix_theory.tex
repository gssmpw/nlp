\section{Theoretical development}
\label{apd:proof}
In this section, we present all the proofs  of our theoretical development. 

For readers' convenience, we recapitulate our definition and assumptions:

\textit{Domain objective}: Given a domain $\mathbb{P}^e$, let the hypothesis $f:\mathcal{X}\rightarrow\Delta_{\left | \mathcal{Y} \right |}$ is a map from the data space $\mathcal{X}$ to the the $C$-simplex label space $\Delta_{\left | \mathcal{Y} \right |}:=\left\{ \alpha\in\mathbb{R}^{\left | \mathcal{Y} \right |}:\left \| \alpha \right \|_{1}=1\,\land\,\alpha\geq 0\right\}$.
%Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x= \psi_{x}(z_c, z_e, u_{x})\in\mathcal{X}$ and its corresponding label $y\in \mathcal{Y}$ is sampled as $y\sim \mathbb{P}(Y\mid z_c)$. 
Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be a loss function, where $\ell\left(f\left(x\right),y\right)$ with
$f\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
specifies the loss (i.e., cross-entropy) to
assign a data sample $x$ to the class $y$ by the hypothesis $f$. The general 
loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].   
\end{equation}


% The %general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}$ is:
% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}\right)=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}}\left[\ell\left(f\left(x\right),y\right)\right].    
% \end{equation}

\begin{assumption} (Label-identifiability). We assume that for any pair $z_c, z^{'}_c\in \mathcal{Z}_c$,  $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c) \text{ if } \psi_x(z_c,z_e,u_x)=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$
\label{as:label_idf_apd}.
\end{assumption}


\begin{assumption} (Causal support). We assume that the union of the support of causal factors across training domains covers the entire causal factor space $\mathcal{Z}_c$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
\label{as:sufficient_causal_support_apd}
\end{assumption}


% \begin{assumption} (Sufficient causal support).  The mixture of training domain distributions is denoted as $\mathbb{P}^{\pi}=\sum_{e\in \mathcal{E}_{train}}\pi_{e}\mathbb{P}^{e}$,
% where the mixing coefficients $\pi=\left[\pi_{e}\right]_{e\in \mathcal{E}_{train}}$
% can be conveniently set to $\pi_{e}=\frac{N_{e}}{\sum_{e'\in \mathcal{E}_{train}}N_{e'}}$
% with $N_{e'}$ being the training size of the training domain $\mathbb{P}^e'$. The mixture of training domain distributions is said to have a \textit{sufficient causal support} if the support of causal factor $\mathbb{P}^\pi(Z_c)$: $\text{supp}\{\mathbb{P}^\pi(Z_c)\}=\mathcal{Z}_c$ where $\text{supp(.)}$ specifies the support set of a distribution.
% \label{as:sufficient_causal_support_apd}
% \end{assumption}
\begin{corollary}
    $\mathcal{F}\neq \emptyset$ if and only if Assumption~\ref{as:label_idf_apd} holds.
    \label{thm:existence_apd}
\end{corollary}

\begin{proof}

    The \textbf{"if"} direction is directly derived from the Proposition~\ref{thm:invariant_correlation_apd}.  We prove \textbf{"only if"} direction by contraction.

    If Assumption~\ref{as:label_idf_apd} does not hold, there a pair $x=x'$ such that $x= \psi_x(z_c,z_e,u_x)$ $x'=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$ and $\mathbb{P}(Y|Z_c=z_c) \neq \mathbb{P}(Y|Z_c=z^{'}_c)$.

    By definition of $f\in\mathcal{F}^*$, $f(x)=\mathbb{P}(Y|Z_c=z_c)\neq \mathbb{P}(Y|Z_c=z^{'}_c)=f(x')=f(x)$ which is a contradiction. (It is worth noting that a domain containing only one sample $x$ is also valid within our data-generation process depicted in Figure~\ref{fig:graph}.).
\end{proof}



\begin{proposition} (Invariant Representation Function)
Under Assumption.\ref{as:label_idf_apd}, there exists a set of deterministic representation function $(\mathcal{G}_c\neq \emptyset)\in \mathcal{G}$ such that for any $g\in \mathcal{G}_c$, $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ and $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$
\label{thm:invariant_correlation_apd}
\end{proposition}

% \longvt{

% is $h(Y\mid g_c(x)) = p(Y\mid z_c)$ condition is too strong? 

% may we use $h_c\in \underset{h\in \mathcal{H}}{\text{argmin }} \ell\left ( h\circ g_c(x), p(Y\mid z_c) \right )  $ for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e) \text{ for some }z_e\}$

% }
\begin{proof}
Under Assumption.\ref{as:label_idf_apd}, we can always choose a deterministic function $g_c: \mathcal{X}\rightarrow \mathcal{Z}_c$ such that the outcome of $g_c(x)$, can be any $z_c\in\{z_c\mid x= \psi_x(z_c, z_e, u_x)\}$ and $\mathbb{P}(Y\mid g_c(x))=\mathbb{P}(Y\mid z_c)$, will consistently provide an accurate prediction of $Y$. In essence, Y is identifiable over the pushforward measure $g_c\#\mathbb{P}(X)$.  

% if $h(g_c(x))=\mathbb{P}(Y\mid z_c)$ for $x=\psi_x(z_c,z_e,u_x)$ then $h(g_c(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all }z_e,u_x\}$




% There is a set $\mathcal{B}=\{x\mid x=\psi_x\{z^{'}_c, z_e, u_x\} \text{ for some } z^{'}_c \text{ such that } \mathbb{P}(Y\mid_{z_e}=z_e)\neq \mathbb{P}(Y\mid z_c=z^{'}_c)\} \neq \emptyset$. Consequently, $h(\phi(g(x))) \neq h_c(g_c(x))$ for all $x\in \mathcal{B}$


\end{proof}

\begin{corollary} \label{cor:proterties}(Invariant Representation Function Properties) For any \(g \in \mathcal{G}_c\), the following properties hold:
\begin{enumerate}
    \item(Causal representation:) \(g\) is a mapping function directly from the sample space \(\mathcal{X}\) to the causal feature space \(\mathcal{Z}_c\), such that \(g: \mathcal{X} \rightarrow \mathcal{Z}_c\).
    \item (Equivalent causal representation) Given a deterministic equivalent causal transformation mapping \(T: \mathcal{Z}_c \rightarrow \mathcal{Z}_c\), which maps a causal factor \(z_c\) to another equivalent causal factor \(T(z_c)\), such that
 $\mathbb{P}(Y\mid z_c)=\mathbb{P}(Y\mid T(z_c))$, then we have \(g(x) = T(z_c)\) holds for all \(\{x \mid x = \psi_x(z_c, z_e, u_x), \text{ for all } z_e, u_x\}\).

\item Given $\ell$ is the Cross-Entropy Loss i.e., $\ell(h(z_c), y) = -\sum_{y \in \mathcal{Y}} \mathbb{P}(Y = y \mid z_c) \log h(z_c)[y]$, there exists $h^*$ such that:
\begin{equation*}
  h^* \in\bigcap_{z_c\in\mathcal{Z}_c} \underset{h\in \mathcal{H}}{\text{argmin }} \mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \ell\left ( h( z_c), y \right ),  
\end{equation*}
\end{enumerate}
\end{corollary}
\color{black}
\begin{proof}
We prove each property as follows:

% We will further demonstrate that \( g \in \mathcal{G}_c \) must be a function mapping from \( \mathcal{X} \) to \( \mathcal{Z}_c \) in order to $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all }z_e,u_x\}$

\underline{\textit{Proof of property-1:}} Suppose there exists $g: \mathcal{X}\rightarrow \mathcal{Z}$ such that $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all }z_e,u_x\}$.

If \( g \) is not a function from \( \mathcal{X} \) to \( \mathcal{Z}_c \), then \( g(x) \) may include spurious features \( z_e \), or both \( z_c \) and \( z_e \) for $x=\psi(z_c,z_e,u_x)$.

Based on the structural causal model (SCM) depicted in Figure~\ref{fig:graph}, it follows that \( Z_e \not\!\perp\!\!\!\perp Y \), meaning that the environmental feature \( Z_e \) is spuriously correlated with \( Y \). Consequently, 
\[
\mathbb{P}(Y \mid g(x = \psi(z_c, z_e, u_x))) \neq \mathbb{P}(Y \mid g(x = \psi(z_c, z_e', u_x)))
\]
for some \( z_e \neq z_e' \), which is a contradiction.

\underline{\textit{Proof of property-2:}} Since $g: \mathcal{X}\rightarrow \mathcal{Z}_c$ and $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all }z_e,u_x\}$, the outcome of $g(x)$ have to be any $z^{'}_c\in \mathcal{Z}_c$ such that $\mathbb{P}(Y\mid z_c)=\mathbb{P}(Y\mid z^{'})$, which means \(g(x) = T(z_c)\) holds for  \(\{x \mid x = \psi_x(z_c, z_e, u_x)\}\)

This highlights the flexibility of the family of invariant representation functions \(\mathcal{G}_c\), as they allow the model to map a sample \(x = \psi(z_c, z_e, u_x)\) to a set of equivalent causal factors \(\{z'_c \in \mathcal{Z}_c \mid \mathbb{P}(Y \mid z_c) = \mathbb{P}(Y \mid z'_c)\}\), rather than requiring an exact mapping to \(z_c\).

Finally, since $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$, \(g(x) = T(z_c)\) holds for all \(\{x \mid x = \psi_x(z_c, z_e, u_x), \text{ for all } z_e, u_x\}\)

\underline{\textit{Proof of property-3:}}

Given $z_c\in \mathcal{Z}_c$ and $\ell(h(z_c), y) = -\sum_{y \in \mathcal{Y}} \mathbb{P}(Y = y \mid z_c) \log h(z_c)[y]$, it is easy to show that the optimal $$h^*=\underset{h\in \mathcal{H}}{\text{argmin }} \mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \ell\left ( h( z_c), y \right )$$ is the conditional probability distribution $h^*(z_c)=\mathbb{P}(Y\mid z_c)$.

Based on structural causal model (SCM) depicted in Figure~\ref{fig:graph}, $\mathbb{P}(Y\mid z_c)$  remains stable across all domains. Therefore, there exists an optimal function \(h^*\) such that:
\begin{equation*}
  h^* \in\bigcap_{z_c\in\mathcal{Z}_c} \underset{h\in \mathcal{H}}{\text{argmin }} \mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \ell\left ( h( z_c), y \right ),  
\end{equation*}

where $h^*(z_c)=\mathbb{P}(Y\mid z_c)$ for all $z_c\in \mathcal{Z}_c$
\end{proof}


\subsection{Sufficient Conditions for achieving Generalization}

\begin{theorem} (\textbf{Theorem \ref{thm:sufficient_conditions} in the main paper}) Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, given a hypothesis $f=h\circ g$, if $f$ is optimal hypothesis for training domains i.e.,
\begin{equation*}
    f\in \bigcap_{{e}\in \mathcal{E}_{tr}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}
\end{equation*}
and one of the following sub-conditions holds:
\begin{enumerate}
    \item $g$ belongs to the set of \textbf{invariant representation functions} as specified in Proposition~\ref{thm:invariant_correlation}.
    
    \item $\mathcal{E}_{tr}$ is set of \textbf{Sufficient and diverse training domains} i.e., the union of the support of joint causal and spurious factors across training domains covers the entire causal and spurious factor space $\mathcal{Z}_c\times\mathcal{Z}_e$ i.e., $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c, Z_e \right )\}=\mathcal{Z}_c\times\mathcal{Z}_e$.
    
    \item Given $\mathcal{T}$ is set of all \textbf{invariance-preserving transformations} such that for any $T\in \mathcal{T}$ and $g_c\in \mathcal{G}_c$: $(g_c\circ T)(\cdot)=g_c(\cdot)$, $f$ is also an optimal hypothesis on all augmented domains i.e., $$f\in \bigcap_{{e}\in \mathcal{E}_{tr}, T\in \mathcal{T}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,T\#\mathbb{P}^{e}}$$
\end{enumerate}
Then $f\in \mathcal{F}^*$
\label{thm:sufficient_conditions_apd}.
\end{theorem}

\begin{proof}
For clarity, we divide this theorem into three sub-theorems and present their proofs in the following subsections.
\end{proof}

\subsubsection{Proof of the Theorem~\ref{thm:sufficient_conditions_apd}.1}


Theorem~\ref{thm:sufficient_conditions_apd}.1 demonstrates that:

If 
\begin{itemize}
    \item \( f = h \circ g \) is an optimal hypothesis for all training domains, i.e., $f \in \bigcap_{{e} \in \mathcal{E}_{tr}} \underset{f \in \mathcal{F}}{\text{argmin}} \ \mathcal{L}(f, \mathbb{P}^{e}),$
    \item and \( g \) is an invariant representation function, i.e., \( g \in \mathcal{G}_c \),
\end{itemize}
then \( f \in \mathcal{F}^* \).


To prove this, in the following theorem, we show that for any domain \( \mathbb{P}^e \) satisfying causal support (Assumption~\ref{as:sufficient_causal_support}), if \( f = h \circ g \) is optimal for \( \mathbb{P}^e \) and \( g \in \mathcal{G}_c \), then \( f \in \mathcal{F}^* \). Note that in the following theorem, for simplicity, we assume that
$\mathbb{P}^e$ is a mixture of the training domains. Therefore, $\mathcal{E}_{tr}$ satisfying causal support implies \( \mathbb{P}^e \) also satisfying causal support i.e., $\text{supp}\{\mathbb{P}^e(Z_c)\}=\mathcal{Z}_c$.


\begin{theorem} Denote the set of \textbf{domain optimal hypotheses} of $\mathbb{P}^e$ induced by $g\in \mathcal{G}$: 
    \begin{equation*}
        \mathcal{F}_{\mathbb{P}^e,g}=\left \{h\circ g \mid h\in\underset{h'\in \mathcal{H}}{\rm{argmin }} \mathcal{L}\left ( h'\circ g, {\mathbb{P}^{e}} \right )  \right \}.
    \end{equation*} 
If $\text{supp}\{\mathbb{P}^e(Z_c)\}=\mathcal{Z}_c$ and $g\in \mathcal{G}_c$, then $\mathcal{F}_{\mathbb{P}^e,g}  \subseteq \mathcal{F}^{*}$. 
\label{thm:single_generalization_apd}
\end{theorem}


\begin{proof}


% Before diving into the proof, let us recall that, based on structural causal model (SCM) depicted in Figure~\ref{fig:graph} we have a distribution (domain) over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$: \begin{align*}  \mathbb{P}^e(Y,X)&=\int_{z_c}\int_{z_e}\mathbb{P}^{e}(X, Y, Z_c,Z_e, E=e)d_{z_c} d_{z_e}\\
%    &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X,Y, Z_c, Z_e) d_{z_c} d_{z_e}\\
%    &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X\mid z_c, Z_e)\mathbb{P}^{e}(Y\mid z_c)\mathbb{P}^{e}(z_c,z_e) d_{z_c} d_{z_e}\\
% \end{align*}
% and
% \begin{align*}
%    \mathbb{P}^e( X=x)&= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X=x\mid z_c, Z_e)\mathbb{P}^{e}(z_c,z_e) d_{z_c} d_{z_e}\\
% \end{align*}
% and the conditional distribution:
% \begin{align*}
%    \mathbb{P}^e(Y\mid X=x)&=\int_{z_c}\int_{z_e}\mathbb{P}^{e}(X=x, Y, Z_c,Z_e, E=e)d_{z_c} d_{z_e}\\
%    &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X=x\mid z_c, Z_e)\mathbb{P}^{e}(Y\mid z_c)\mathbb{P}^{e}(z_c,z_e) d_{z_c} d_{z_e}\\
% \end{align*}
% and $\mathbb{P}^{e}(Y\mid z_c)=\mathbb{P}^{e'}(Y\mid z_c)=\mathbb{P}(Y\mid z_c)$.


% \textbf{We start the proof the main theorem:}

Given $\text{supp}\{\mathbb{P}^e(Z_c)\}=\mathcal{Z}_c$ and $g_c\in \mathcal{G}_c$, 
it suffices to prove that for any $f_c = h_c \circ g_c \in \mathcal{F}_{\mathbb{P}^e,g_c}$, we have:

\begin{equation}
    f_c \in \bigcap_{\mathbb{P}^{e}\in \mathcal{P}} \underset{f\in \mathcal{F}}{\text{argmin}}\mathcal{L}\left(f, \mathbb{P}^e\right).    
    \label{eq:single_generalization_apd}
\end{equation}

To prove (\ref{eq:single_generalization_apd}), we only need to show that for any $f=h\circ g_c \in\mathcal{F}$ and $\mathbb{P}^{e'} \in \mathcal{P}$:

\begin{equation}
\mathcal{L}\left(f, \mathbb{P}^{e'}\right)
\geq \mathcal{L}\left(f_c, \mathbb{P}^{e'}\right),
%\label{eq:target_apd}
\end{equation}

which is equivalent to:
\begin{equation}
\mathbb{E}_{(x,y)\sim\mathbb{P}^{e'}}\left[\ell\left(f\left(x\right),y\right)\right]
\geq \mathbb{E}_{(x,y)\sim\mathbb{P}^{e'}}\left[\ell\left(f_c\left(x\right),y\right)\right].
\label{eq:target_apd}
\end{equation}

% \begin{equation}
% \mathbb{E}_{x\sim\mathbb{P}^{e'}(X),y\sim \mathbb{P}^{e'}(Y\mid X=x)}\left[\ell\left(f\left(x\right),y\right)\right]
% \geq \mathbb{E}_{x\sim\mathbb{P}^{e'}(X),y\sim \mathbb{P}^{e'}(Y\mid X=x)}\left[\ell\left(f_c\left(x\right),y\right)\right].
% \label{eq:target_apd}
% \end{equation}

\underline{\textit{Step 1: Simplifying the general loss using the invariant representation function \(g_c\).}} 

Based on structural causal model (SCM) depicted in Figure~\ref{fig:graph} we have a distribution (domain) over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$: \begin{align*}  \mathbb{P}^e(X,Y)&=\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X, Y, Z_c=z_c,Z_e=z_e)d_{z_c} d_{z_e}\\
&=\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X, Y, z_c,z_e)d_{z_c} d_{z_e}\\
   &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(X\mid z_c, z_e)\mathbb{P}^{e}(Y\mid z_c)\mathbb{P}^{e}(z_c,z_e) d_{z_c} d_{z_e}\\
   &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\mathbb{P}^{e}(X=x\mid z_c, z_e)\mathbb{P}^{e}(Y\mid z_c)d_{z_c} d_{z_e} d_x\\
   &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\mathbb{P}^{e}(X=x\mid z_c, z_e)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c) d_{z_c} d_{z_e} d_x d_y\\
    &= \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\int_{\mathcal{U}_x}\mathbb{P}^{e}(X=x\mid z_c, z_e,u_x)\mathbb{P}^{e}(u_x)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)  d_{z_c} d_{z_e} d_x d_y d_{u_x}\\
    &\stackrel{(1)}{=} \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\int_{\mathcal{U}_x}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\mathbb{P}^{e}(u_x)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)  d_{z_c} d_{z_e} d_x d_y d_{u_x}\\
\end{align*}

We have $\stackrel{(1)}{=}$ by definition of SCM, $x$ is the deterministic function of $(z_c, z_e,u_x)$.

Therefore we have:

\begin{align}
&\mathbb{E}_{(x,y)\sim\mathbb{P}^{e}(X,Y)}\left[\ell\left(f\left(x\right),y\right)\right]\nonumber\\
% &= 
% \int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\int_{\mathcal{X}}\mathbb{P}^{e}(z_c,z_e)\mathbb{P}^{e}(X=x\mid z_c, z_e)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\ell\left(f\left(x\right),y\right)d_y d_{z_c} d_{z_e} d_x  \nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\int_{\mathcal{U}_x}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\mathbb{P}^{e}(u_x)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\ell\left(f\left(x\right),y\right)  d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\int_{\mathcal{X}}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\ell\left(f\left(x\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\int_{\mathcal{X}}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_y d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right)\right]
 \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e}  d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left((h\circ g_c)\left(\psi_x(z_c, z_e,u_x)\right),y\right)\right]
 \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e}  d_{u_x}\nonumber
\\
&\stackrel{(1)}{=} 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(T(z_c)\right),y\right)\right]
 \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e}  d_{u_x}\nonumber\\
&=
\int_{\mathcal{Z}_c}\mathbb{P}^{e}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(T(z_c)\right),y\right)\right]
 d_{z_c} \nonumber\\
&= 
\int_{\mathcal{Z}_c}\mathbb{P}^{e}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid T(z_c))} \left[ \ell\left(h\left(T(z_c)\right),y\right)\right]
 d_{z_c} \nonumber
\\
&\stackrel{(2)}{=} 
\int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(z_c\right),y\right)\right]
 d_{z_c} \nonumber
\end{align}

We have:
\begin{itemize}
    \item $\stackrel{(1)}{=}$ by property-2 of $g_c$ (Corollary~\ref{cor:proterties});
    \item $\stackrel{(2)}{=}$ because $T: \mathcal{Z}_c\rightarrow \mathcal{Z}_c$ and  $T_{\#}\mathbb{P}^{e}(z_c)=\int_{z^{'}_c\in T^{-1}(z_c)}\mathbb{P}^{e}(z^{'}_c)d_{z^{'}_c}$ 
\end{itemize} 

Now, to prove (\ref{eq:target_apd}), we only need to show:

% \begin{align}
% \int_{\mathcal{Z}_c}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h_c\left(T(z_c)\right),y\right)\right]
%  d_{z_c} \leq \int_{\mathcal{Z}_c}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(T(z_c)\right),y\right)\right]
%  d_{z_c} \nonumber
% \end{align}

\begin{align}
\int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h_c\left(z_c\right),y\right)\right]
 d_{z_c}\leq\int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(z_c\right),y\right)\right]
 d_{z_c}
 \label{eq:target_causal}
\end{align}

% Recall that by proposition.\ref{thm:invariant_correlation_apd}, there exists $h^*$ such that: $h^*(g_c(x)) = \mathbb{P}(Y\mid z_c)$ holds true for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all }z_e,u_x\}$. Therefore,

% \begin{equation*}
%   h^* \in\bigcap_{(x,z_c)\in\mathbb{B}} \underset{h\in \mathcal{H}}{\text{argmin }} \mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \ell\left ( h\circ g_c(x), y \right ),  
% \end{equation*}

% where $\mathbb{B}=\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for all } z_c \in \mathcal{Z}_c \text{ and for all }z_e,u_x\}$.

\underline{\textit{Step 2: Generalization of $h_c$.}} \textit{Step-1} Demonstrate that \(h_c\) only needs to make predictions for the set of causal factors \(z_c \in \mathcal{Z}_c\). Therefore, it is sufficient to show that \(h_c\) is optimal for every \(z \in \mathcal{Z}_c\).


Recall that $f_c=h_c\circ g_c\in \mathcal{F}_{\mathbb{P}^e,g_c}$, 
therefore, 
$$h_c\in \underset{h\in \mathcal{H}}{\text{argmin }} \int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(z_c\right),y\right)\right]
 d_{z_c} $$

By property-3 of $g_c$ (Corollary~\ref{cor:proterties}), there exists an optimal function \(h^*\) such that:
\begin{equation*}
  h^* \in\bigcap_{z_c\in\mathcal{Z}_c} \underset{h\in \mathcal{H}}{\text{argmin }} \mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \ell\left ( h( z_c), y \right ),  
\end{equation*}

% for all $\{(x,z_c)\mid  x= \psi_x(z_c, z_e, u_x) \text{ for some }z_e, u_x \text{ and all } z_c\in \text{supp}\mathbb{P}^e(Z_c)\}$.

Property-3 of \(g_c\) ensures the existence of an optimal \(h^*\) for every causal factor \(z_c \in \mathcal{Z}_c\), it follows that \(h_c\) must also be optimal for every causal feature \(z_c\) within its support, \(\text{supp}\,\mathbb{P}^e(Z_e)\). This implies that \(h_c(z_c) = h^*(z_c)\) for every \(z_c\) where \(\mathbb{P}^e(z_e) > 0\).

Moreover, since \(\text{supp}\,\mathbb{P}^e(Z_e) = \mathcal{Z}_c\), this implies that \(h_c(z_c) = h^*(z_c)\) for every \(z_c \in \mathcal{Z}_c\).

\underline{\textit{Step-3: Proof of (\ref{eq:target_causal})}}.

\begin{align*}
\int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h_c\left(z_c\right),y\right)\right]
 d_{z_c}\leq\int_{\mathcal{Z}_c}T_{\#}\mathbb{P}^{e'}(z_c)\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(z_c\right),y\right)\right]
 d_{z_c}
\end{align*}

From \textit{step-2}, we have 

$$\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h_c\left(z_c\right),y\right)\right]
\leq\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(h\left(z_c\right),y\right)\right]
$$
for all $z_c\in \mathcal{Z}_c$. By taking the expectation and applying the law of iterated expectation, inequality (\ref{eq:target_causal}) follows. This concludes the proof.


\end{proof}

\subsubsection{Proof of the Theorem~\ref{thm:sufficient_conditions_apd}.2}

Similar to the proof of Theorem~\ref{thm:sufficient_conditions_apd}.1, in the following result, we assume for simplicity that \( \mathbb{P}^e \) is a mixture of the training domains.
Then, Theorem~\ref{thm:sufficient_conditions_apd}.2 is stated as follows:

\begin{theorem}Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, 
if \( f\) is an optimal hypothesis for $\mathbb{P}^{e}$ i.e., $ f \in \underset{f \in \mathcal{F}}{\text{argmin}} \ \mathcal{L}(f, \mathbb{P}^{e}),$ and the support of joint causal and spurious factors of $\mathbb{P}^{e}$ covers the entire causal and spurious factor space $\mathcal{Z}_c\times\mathcal{Z}_e$ i.e., $\text{supp}\{\mathbb{P}^{e} \left (Z_c, Z_e \right )\}=\mathcal{Z}_c\times\mathcal{Z}_e$, then then \( f \in \mathcal{F}^* \).
\end{theorem}

\begin{proof}

Based on structural causal model (SCM) depicted in Figure~\ref{fig:graph} we have a distribution (domain) over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$: 

Therefore we have:

\begin{align}
&\mathbb{E}_{(x,y)\sim\mathbb{P}^{e}(X,Y)}\left[\ell\left(f\left(x\right),y\right)\right]\nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{X}}\int_{\mathcal{U}_x}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\mathbb{P}^{e}(u_x)\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\ell\left(f\left(x\right),y\right)  d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\int_{\mathcal{X}}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\ell\left(f\left(x\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\int_{\mathcal{X}}\mathbb{I}_{x= \psi_x(z_c, z_e,u_x)}\ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_x d_y d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\int_{\mathcal{Y}}\mathbb{P}^{e}(Y=y\mid z_c)\ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right) \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e} d_y d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left(f\left(\psi_x(z_c, z_e,u_x)\right),y\right)\right]
 \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e}  d_{u_x}\nonumber
\\
&= 
\int_{\mathcal{Z}_c}\int_{\mathcal{Z}_e}\mathbb{P}^{e}(z_c,z_e)\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left((h\circ g_c)\left(\psi_x(z_c, z_e,u_x)\right),y\right)\right]
 \mathbb{P}^{e}(u_x) d_{z_c} d_{z_e}  d_{u_x}
\end{align}
\end{proof}

Under Assumption \ref{as:label_idf}, 
given \( f\) is an optimal hypothesis for $\mathbb{P}^{e}$ i.e., $ f \in \underset{f \in \mathcal{F}}{\text{argmin}} \ \mathcal{L}(f, \mathbb{P}^{e}),$ then 
\begin{align*}
f\in\int_{\mathcal{U}_x}\mathbb{E}_{y\sim\mathbb{P}(Y\mid z_c)} \left[ \ell\left((h\circ g_c)\left(\psi_x(z_c, z_e,u_x)\right),y\right)\right]
 \mathbb{P}^{e}(u_x)  d_{u_x}
\end{align*}

This holds because, under Assumption~\ref{as:label_idf}, it is guaranteed that there exists an optimal prediction for every $x=\psi_x(z_c, z_e, u_x)$ for all $\{(z_c,z_e)\sim \mathbb{P}^{e}(z_c,z_e),u_x\sim \mathbb{P}^{e}(u_x)\}$.

Furthermore, since the support of the joint causal and spurious factors in \( \mathbb{P}^{e} \) spans the entire causal and spurious factor space, i.e., $\text{supp}\{\mathbb{P}^{e} (Z_c, Z_e)\} = \mathcal{Z}_c \times \mathcal{Z}_e,$ the hypothesis \( f \) remains optimal for all possible configurations of \( x = \psi_x(z_c, z_e, u_x) \) across all \( z_c, z_e, u_x \). This implies that \( f \in \mathcal{F}^* \).

\textbf{Note:} 
\begin{itemize}
    \item It is important to highlight that this theorem aligns with Theorem 3 from \citep{ahuja2021invariance}. However, their analysis is conducted in a linear setting.
    \item We argue that the sub-condition of "sufficient and diverse training domains" is impractical, making it a weak guarantee for ensuring the generalization of algorithms based on this condition.
\end{itemize}



\subsubsection{Proof of the Theorem~\ref{thm:sufficient_conditions_apd}.3}

Similar to the proof of Theorem~\ref{thm:sufficient_conditions_apd}.1, in the following result, we assume for simplicity that \( \mathbb{P}^e \) is a mixture of the training domains.
Then, Theorem~\ref{thm:sufficient_conditions_apd}.3 is stated as follows:

\begin{theorem}Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, given $\mathcal{T}$ is set of all \textbf{invariance-preserving transformations} such that for any $T\in \mathcal{T}$ and $g_c\in \mathcal{G}_c$: $(g_c\circ T)(\cdot)=g_c(\cdot)$,
if \( f\) is an optimal hypothesis for $\mathbb{P}^{e}$ i.e., $ f \in \underset{f \in \mathcal{F}}{\text{argmin}} \ \mathcal{L}(f, \mathbb{P}^{e}),$ and  $f$ is also an optimal hypothesis on all augmented domains i.e., $$f\in \bigcap_{T\in \mathcal{T}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,T\#\mathbb{P}^{e}}$$
 
, then then \( f \in \mathcal{F}^* \).
\end{theorem}

\begin{proof}
We first analyze the characteristics of the set of all invariance-preserving transformations \( \mathcal{T} \).

By the definition of \( \mathcal{T} \) and the set of invariant representations \( \mathcal{G}_c \):

\begin{itemize}
    \item given $T\in \mathcal{T}$ and $g_c\in \mathcal{G}_c$, we have $(g_c\circ T)(x)=g_c(x)$ for all $x=\psi(z_c,z_e,u_x)$ (for all $z_c\in \mathcal{Z}_c, z_c\in \mathcal{Z}_c$, $u_x\in \mathcal{U}_x$).
    \item  $g\in \mathcal{G}_c$, $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ and $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$
\end{itemize}

This implies that for any $ x=\psi_x(z_c, z_e, u_x)$, we have:
\begin{equation*}
    T(x) \in \left \{\psi_x(z'_c, z'_e, u'_x) \text{ where } \left (z'_c\in\mathcal{Z}_c,z'_e\in \mathcal{Z}_e, u'_x\in \mathcal{U}_x \text{ and } P(Y\mid Z_c=z_c)=P(Y\mid Z_c=z'_c)\right ) \right \}
\end{equation*}

In other words, given a sample $\psi_x(z_c, z_e, u_x)$, the transformation $T$ operates as follows:
\begin{itemize}
    \item It augments \( z_c \) to its equivalent \( z'_c \), ensuring that $P(Y\mid Z_c=z_c)=P(Y\mid Z_c=z'_c)$
    \item It modifies the environmental (or spurious) feature $z_e$ to any $z'_e \in \mathcal{Z}_e$.
    \item It applies changes to the noise term $u_x$.
\end{itemize}

Therefore, under Assumption~\ref{as:sufficient_causal_support} (causal support), having access to all \( T \in \mathcal{T} \) is equivalent to having sufficient and diverse training domains. That is, the support of the joint causal and spurious factors in \( \mathbb{P}^{e} \) spans the entire causal and spurious factor space i.e., $\text{supp}\{\mathbb{P}^{e} (Z_c, Z_e)\} = \mathcal{Z}_c \times \mathcal{Z}_e$. This concludes the proof.




\textbf{Note:} In general, accessing all transformations from $\mathcal{T}$ is impractical. However, recently, some works leverage foundation models to generate these augmentations, achieving promising empirical performance \cite{ruan2021optimal}.
\end{proof}

\subsection{Necessary Conditions for achieving Generalization}

% \begin{theorem} \textbf{(Theorem \ref{thm:nacessary} in the main paper)}
% Considering the training domains $\mathbb{P}^e$ and representation function $g$, let $\mathcal{H}_{\mathbb{P}^e,g}=\underset{h\in \mathcal{H}}{\text{argmin }} \mathcal{L}\left ( h\circ g,\mathbb{P}^e \right ) $ represent the set of optimal classifiers on $g\#\mathbb{P}^e$ (the push-forward distribution by applying $g$ on $\mathbb{P}^e$), \textbf{the best generalization classifier} from $\mathbb{P}^e$ to $\mathcal{P}$ is defined as 
% \begin{equation}
% \mathcal{F}^{B}_{\mathbb{P}^e,g}=\left \{ h\circ g \mid h = \underset{ h'\in \mathcal{H}_{\mathbb{P}^e,g}}{\rm{argmin }} \sup_{e'\in \mathcal{E}} \mathcal{L}\left (  h'\circ g, \mathbb{P}^{e'} \right ) \right \}
% \end{equation}  

% Give representation function $g: \mathcal{X}\rightarrow \mathcal{Z}$ then $\forall \mathbb{P}^e\sim \mathcal{P}$ we have
% $\mathcal{F}^B_{ \mathbb{P}^e,g} \subseteq \mathcal{F}^{*}$ if and only if $g\in \mathcal{G}_s$. 
% \label{thm:nacessary_apd}
% \end{theorem}

\begin{theorem} \textbf{(Theorem \ref{thm:nacessary} in the main paper)} Given representation function $g$,
$\exists h: h\circ g\in \mathcal{F}^*$ if and only if $g\in \mathcal{G}_s$.
\label{thm:nacessary_apd}
\end{theorem}

\begin{proof} \textit{\textbf{``if"} direction.} If $g\in \mathcal{G}_s$, we have: 

\begin{enumerate}
    \item By definition of $g\in \mathcal{G}_s$ we have
$I(g(X),g_c(X))=I(X,g_c(X)$ i.e., $g(X)$ retain all information about $g_c(X)$ presented in $X$. Therefore, there exists a function \(\phi\) such that \(\phi \circ g \in \mathcal{G}_c\), which implies the existence of a \(g_c\in \mathcal{G}_c\) such that \(\phi \circ g = g_c\).

    \item By the definition of \(g_c \in \mathcal{G}_c\), we can always find a classifier \(h_c\) such that \(h_c \circ g_c \in \mathcal{F}^*\).

\end{enumerate}

To complete the proof of the \textbf{``if"} direction, we need to demonstrate the existence of a classifier \( h \) on top of the representation induced by \( g \) such that it forms a globally optimal hypothesis, i.e., \( h \circ g \in \mathcal{F}^* \).

From (1) and (2) we have $h_c\circ g_c = h_c\circ \phi \circ g \in \mathcal{F}^*$. Therefore, we can construct classifier $h = h_c \circ \phi$, then $h \circ g =  h_c\circ \phi \circ g =h_c\circ g_c \in \mathcal{F}^*$.
\end{proof}

\begin{proof} \textit{\textbf{``only if"} direction by contraction.} 

Define the set of optimal hypotheses induced by a representation function \( g \) as:

\begin{equation*}
\mathcal{F}_{g,\mathcal{E}_{tr}}=\left\{ h\circ g: \bigcap_{{e} \in \mathcal{E}_{tr}} \underset{h\circ g \in \mathcal{F}}{\text{argmin}} \ \mathcal{L}(h\circ g, \mathbb{P}^{e})\right \}
\end{equation*}  

We show that if $g$ is not sufficient-representation, for any $f\in\mathcal{F}_{g,\mathcal{E}_{tr}}$ there exists multiple target domains where $f$ performs arbitrarily bad.  


By definition of $g\notin \mathcal{G}_s$, we have
$I(g(X),g_c(X))<I(X,g_c(X)$. Therefore, there does not exist a function $\phi$ such that $\phi\circ g \in \mathcal{G}_c$. This implies we can not construct any classifier $h = h_c \circ \phi$, then $h \circ g =  h_c\circ \phi \circ g =h_c\circ g_c \in \mathcal{F}^*$.

Consequently, $h$ has to rely on spurious feature $z_e$ (or both $z_c$ and $z_e$) to make predict for some $\{x\mid x=\psi_x\{z_c,z_e, u_x\} \text{ for some } z_c \text{ such that } \mathbb{P}(Y\mid Z_e=z_e)= \mathbb{P}(Y\mid Z_c=z_c) \}$.  

Note that based on structural causal model (SCM) depicted in Figure~\ref{fig:graph}, we have $Z_e\not\!\perp\!\!\!\perp Y$ i.e., the environmental feature $Z_e$ spuriously correlated with $Y$. Therefore, there is a set $\mathcal{B}=\{x\mid x=\psi_x\{z^{'}_c, z_e, u_x\} \text{ for some } z^{'}_c \text{ such that } \mathbb{P}(Y\mid Z_e = z_e)\neq \mathbb{P}(Y\mid Z_c=z^{'}_c)\} \neq \emptyset$. Consequently, $h(\phi(g(x))) \neq h_c(g_c(x))$ for all $x\in \mathcal{B}$

    We can construct undesirable target domains $\mathbb{P}^{e_i}$ with arbitrary loss $\mathcal{L}(h\circ g, \mathbb{P}^{e_i})$ by giving $(1-\delta)$ percentage mass to that examples in $\mathcal{B}$ and $(\delta)$ percentage mass that examples in $\mathcal{X} \setminus \mathcal{B}$. This is equivalent to 
\begin{equation}
    \mathbb{E}_{(x,y)\sim\mathbb{P}^{e_i}} \left [ h(g(x)) \neq h_c(g_c(x))  \right ]  \geq 1-\delta.\nonumber
\end{equation}
 with $(0\leq\delta\leq 1)$.

This concludes the proof.

\end{proof}

% \begin{corollary} \textbf{(Corollary \ref{thm:bad_domain_exist} in the main paper)}
%      Given $g\in \mathcal{G}_s$, there exists $f = h\circ g\in \bigcap_{e\in \mathcal{E}_{train}}\mathcal{F}_{g, \mathbb{P}^e}$  such that for any $0<\delta<1$, there are many undesirable target domains $\mathbb{P}^T \sim \mathcal{P}$ such that:
%     \begin{align*}
%    \mathbb{E}_{(x,y)\sim\mathbb{P}^T} \left [ f(x) \neq f^*(x)  \right ]  \geq 1-\delta.
%     \end{align*}  with  $f^* \in \mathcal{F}^*$.
%     \label{thm:bad_domain_exist_apd}
% \end{corollary}

% \begin{proof}
% Denote $\mathbb{P}^{\mathcal{E}_{tr}}$ is the mixture of training domains, then $\text{supp}\{\mathbb{P}^{\mathcal{E}_{tr}} \left (Z_c \right )\}=\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$.  Additionally, given $g\in \mathcal{G}_s$, then there exists $\phi$ such that $g_c=\phi\circ g\in \mathcal{G}_c$. 

% Based on structural causal model (SCM) depicted in Figure~\ref{fig:graph}, we have $Z_e\not\!\perp\!\!\!\perp Y$ i.e., the environmental feature $Z_e$ spuriously correlated with $Y$. Hence, there exist $h \not\in \{h_c \circ \phi \mid h_c \circ g_c \in \mathcal{F}_{g_c,\mathbb{P}^{\mathcal{E}_{tr}}}\}$  e.g., $h$ can rely on spurious feature $z_e$ (or both $z_c$ and $z_e$) to make predict for some $\{x\mid x=\psi_x\{z_c,z_e, u_x\} \text{ for some } z_c \text{ such that } \mathbb{P}(Y\mid_{z_e}=z_e)= \mathbb{P}(Y\mid z_c=z_c) \}$. 
    
% There is a set $\mathcal{B}=\{x\mid x=\psi_x\{z^{'}_c, z_e, u_x\} \text{ for some } z^{'}_c \text{ such that } \mathbb{P}(Y\mid_{z_e}=z_e)\neq \mathbb{P}(Y\mid z_c=z^{'}_c)\} \neq \emptyset$. Consequently, $h(\phi(g(x))) \neq h_c(g_c(x))$ for all $x\in \mathcal{B}$

%     We can construct undesirable target domains $\mathbb{P}^{e_i}$ with arbitrary loss $\mathcal{L}(h\circ g, \mathbb{P}^{e_i})$ by giving $(1-\delta)$ percentage mass to that examples in $\mathcal{B}$ and $(\delta)$ percentage mass that examples in $\mathcal{X} \setminus \mathcal{B}$. This is equivalent to 
% \begin{equation}
%     \mathbb{E}_{(x,y)\sim\mathbb{P}^{e_i}} \left [ h(g(x)) \neq h_c(g_c(x))  \right ]  \geq 1-\delta.\nonumber
% \end{equation}
%  with $(0\leq\delta\leq 1)$.

% By Theorem \ref{thm:single_generalization}, $h_c\circ g_c\in \mathcal{F}_{g_c,\mathbb{P}^{\mathcal{E}_{tr}}}$ implies $h_c\circ g_c\in \mathcal{F}^*$. This concludes the proof.

% \end{proof}


% \begin{theorem}
%  \label{thm:convergence_apd} \textbf{(Theorem \ref{thm:convergence} in the main paper)}
% Given sequence of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, denote $\Funion^{k}=\bigcap_{i=1}^{k}\mathcal{F}_{ \mathbb{P}^{e_i}}$. We consider $\mathcal{E}_{tr}$ to be \textbf{diverse} if for domain $e_k$, there exists at least one sample $x=\psi_x(z_c,z_e,u_x)$ such that $\exists f  \in \Funion^{k-1} :f(x)\neq \mathbb{P}(Y\mid z_c)$. Given a set of diverse domains $\mathcal{E}_{tr}$, we have:
% \begin{equation*}
%  \Funion^1\supset  \Funion^2 \supset... \supset  \Funion^K   
% \end{equation*}
% and the number of training domains $\mathcal{E}_{tr}$ is sufficiently large:
% \begin{equation*}
% \lim_{\mathcal{E}_{tr}\rightarrow \mathcal{E}}\Funion^{ \left| \mathcal{E}_{tr} \right|} \rightarrow \mathcal{F}^*.
% \end{equation*}
% \end{theorem}


% \begin{proof}

% We prove the first statement by induction. Consider the case $\mathcal{F}^{\cap}_{k-1}$ and $\mathcal{F}^{\cap}_{k}$, we will show that if $\mathcal{E}_{tr}$ is considered as \textbf{diverse} $\mathcal{F}^{\cap}_{k-1}\supset \mathcal{F}^{\cap}_{k}$. 

% We have $\mathcal{F}^{\cap}_{k-1}\supseteq \mathcal{F}^{\cap}_{k}$ is obvious by definition.
% By definition of "diverse" training domains $\mathcal{E}_{tr}$, there exists at least one sample $x=\psi_x(z_c,z_e,u_x)$ such that $\exists f  \in \Funion^{k-1} :f(x)\neq \mathbb{P}(Y\mid z_c)$. This means $f\notin \mathcal{F}^{\cap}_{k}$, hence, $\mathcal{F}^{\cap}_{k-1}\supset \mathcal{F}^{\cap}_{k}$.


% For the second statement, we need to show that if  $\mathcal{E}_{tr}=\mathcal{E}$ then $\Funion^{ \left| \mathcal{E}_{tr} \right|}= \mathcal{F}^*$. This holds true by the definition of $\mathcal{F}^*$.



% \end{proof}


\begin{corollary} \textbf{(Corollary~\ref{thm:information} in the main paper)} Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, let the minimal representation function $g_{\text{min}}$ be defined as:
\begin{equation}
g_{\text{min}} \in \mathcal{G}_{min}=\left\{\underset{g \in \mathcal{G}}{\text{argmin }} I(g(X); X) \ \text{s.t.} \ f = h \circ g \in \mathcal{F}_{\mathcal{E}_{\text{tr}}} \right\},
\label{eq:minimal}
\end{equation}
where $I$ denotes mutual information. Then, for any $g_c\in \mathcal{G}_c$ the following holds:
\begin{equation}
I(g_{\text{min}}(X), g_c(X)) \leq I(X, g_c(X)),
\end{equation}
and the equality holds if and only if at least one of sufficient conditions is hold.
\label{thm:information_apd}.
\end{corollary}

\begin{proof} We first prove that if one of the sufficient conditions holds, then the following equality holds:
\[
I(g_{\text{min}}(X), g_c(X)) = I(X, g_c(X)).
\]

Define:
\begin{equation*}
\mathcal{G}_{\mathcal{E}_{tr}}=\left \{g: f = h \circ g \in \mathcal{F}_{\mathcal{E}_{\text{tr}}} \right\}.
\end{equation*}
By Theorem~\ref{thm:sufficient_conditions_apd}, if one of the sufficient conditions holds, then \( \mathcal{G}_{\mathcal{E}_{tr}} \subseteq \mathcal{G}_c \).

From the definition in Eq.~(\ref{eq:minimal}), we have:
\[
\mathcal{G}_{\text{min}} \subseteq \mathcal{G}_{\mathcal{E}_{tr}} \subseteq \mathcal{G}_c.
\]
This implies:
\[
I(g_{\text{min}}(X), g_c(X)) = I(g_c(X), g_c(X)) = I(X, g_c(X)).
\]
\end{proof}

\begin{proof} We prove that if the equality holds, then \( g_{\text{min}} \in \mathcal{G}_c \).

If
\[
I(g_{\text{min}}(X), g_c(X)) = I(X, g_c(X)),
\]
then it follows that
\[
I(g_{\text{min}}(X), X) \geq I(X, g_c(X)).
\]
Therefore, by the definition of \( g_{\text{min}} \), we conclude that \( g_{\text{min}} \in \mathcal{G}_c \).

\end{proof}

\subsection{Representation Alignment trade-off}
\label{apd:tradeoff}
As a reminder, $\mathbb{P}$ denotes data distribution on data space $\mathcal{X}$, while $g_{\#}\mathbb{P}$ denotes latent distribution on full latent space $\mathcal{Z}$, with $g: \mathcal{X} \mapsto \mathcal{Z}$ is the encoder. 

In the following, we recap the theoretical results for Hellinger distance as presented by \cite{phung2021learning}. Similar results for $\mathcal{H}$-divergence can be found in Zhao et al. \cite{zhao2019learning}, and for Wasserstein distance in Le et al. \cite{le2021lamda}.

\subsubsection{Upper Bound}

\begin{theorem} 
\label{thm:single_bound_A}Consider the source domain
$\mathbb{P}^{e'}$ and the
target domain $\mathbb{P}^{e}$. Let $\ell$ be any loss function
upper-bounded by a positive constant $L$. For any hypothesis $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
where $f=h\circ g$ with $g:\mathcal{X}\mapsto\mathcal{Z}$
and $h:\mathcal{Z}\mapsto\mathcal{Y}_{\Delta}$, the target
loss on input space is upper bounded 
\begin{equation}
\begin{aligned}\mathcal{L}\left(f,\mathbb{P}^{e}\right)\leq\mathcal{L}\left(f,\mathbb{P}^{e'}\right)+L\sqrt{2}\,d_{1/2}\left(\mathbb{P}_{g}^{e},\mathbb{P}_{g}^{e'}\right)\end{aligned}
,\label{eq:input_bound_1-1}
\end{equation}
\end{theorem}

This Theorem is directly adapted from the result of Trung et al. \cite{phung2021learning}.
The upper bound for target loss above relates source loss, target loss and data shift on feature space, which is different to other bounds in which the data shift is on input space.

\subsubsection{Lower Bound}
\begin{theorem}
\label{theorem:single_lower_bound_A}
\cite{phung2021learning} Consider a hypothesis $f=h\circ g$, the Hellinger distance between two label marginal distributions $\mathbb{P}^{e'}$ and $\mathbb{P}^{e}$ can be upper-bounded as: 
\begin{equation}
d_{1/2}\left(\mathbb{P}^{e'}_\mathcal{Y},\mathbb{P}^{e}_\mathcal{Y}\right) \leq 
\mathcal{L}\left ( f,\mathbb{P}^{e'} \right )^{1/2}+
d_{1/2}\left ( g_{\#}\mathbb{P}^{e'},g_{\#}\mathbb{P}^{e} \right )+
\mathcal{L}\left ( f,\mathbb{P}^{e} \right )^{1/2}
\end{equation}

where the general loss $\mathcal{L}$ is defined based on the Hellinger loss $\ell$ which is define as $\ell\left ( f(x) \right )=D_{1/2}\left ( f(x),\mathbb{P}(Y\mid x) \right )=2\sum_{i=1}^C\left ( \sqrt{f(x,i)}-\sqrt{\mathbb{P}(Y=i\mid x)} \right )^2$.
\end{theorem}

\subsection{Subspace Representation Alignment}

In the following, we prove the theoretical results for Hellinger distance based on the findings of Trung et al. \cite{phung2021learning}. A similar strategy can be directly applied to $\mathcal{H}$-divergence \cite{zhao2019learning} and Wasserstein distance \cite{le2021lamda}.

\begin{theorem}
\label{theorem:multi_bound_A} \textbf{(Theorem~\ref{theorem:multi_bound})} 
\textit{(Subspace Representation Alignment)} Given \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, and a subspace index $m\in \mathcal{M}$, let $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$ and $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$, then $\pi^{e}_m=\frac{\mathbb{P}^{e}\left(A_{m}\right)}{\sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)}$ is mixture co-efficient, if the loss function $\ell$ is upper-bounded by a positive
constant $L$, then:

(i)  The target general loss is upper-bounded: 
\begin{align*}
\left | \mathcal{E}_{tr} \right |\sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
\leq
\sum_{e\in \mathcal{E}_{tr}} \sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left ( f,\mathbb{P}^{e}_{m} \right ) +
L\sum_{e, e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_{m}D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right ),
\end{align*}
(ii) Distance between two label marginal distribution $\mathbb{P}^{e}_{m}(Y)$ and $\mathbb{P}^{e'}_{m}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) \leq 
D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right )
+\mathcal{L}\left ( f,\mathbb{P}^{e}_{m}\right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'}_{m} \right )
\end{aligned}
\end{equation*}
(iii) Construct the subspace projector $\Gamma$ as the optimal hypothesis for the training domains i,e., \(\Gamma \in \mathcal{F}_{\mathcal{E}_{tr}}\), which defines $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=\Gamma(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$, then
$
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$ for all \(m \in \mathcal{M}\).

where $g_{\#}\mathbb{P}$ denotes representation distribution on $\mathcal{Z}$ induce by applying $g$ with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence, Hellinger or Wasserstein distance.
\end{theorem}

\begin{proof}
We consider \textit{sub-space projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a sub-space index $m\in \mathcal{M}$, we denote $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$.
Let $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$ and $\mathbb{P}_{m}^{e}$ as the distribution restricted by $\mathbb{P}^{e}$
over $A_{m}$. Eventually, we define $\mathbb{P}_{m}^{e}\left(y\mid x\right)$ as the probabilistic labeling
distribution on the sub-space $\left(A_{m},\mathbb{P}_{m}^{e}\right)$,
meaning that if $x\sim\mathbb{P}_{m}^{e}$, $\mathbb{P}_{m}^{e}\left(y\mid x\right)=\mathbb{P}_{e}\left(y\mid x\right)$.
Similarly, we define if $x\sim\mathbb{P}_{m}^{e'}$, $\mathbb{P}_{m}^{e'}\left(y\mid x\right)=\mathbb{P}^{e'}\left(y\mid x\right)$. Due to this construction, any data sampled from $\mathbb{P}_{m}^{e}$
or $\mathbb{P}_{m}^{e'}$ have the same index $m=\Gamma(x)$. 
Additionally, since each data point $x \in \mathcal{X}$ corresponds to only a single $\Gamma(x)$, the data space is partitioned into disjoint sets, i.e., $\mathcal{X} = \bigcup_{m=1}^{\mathcal{M}} A_{m}$, where $A_m \cap A_n = \emptyset, \forall m \neq n$. 
Consequently, the general loss of the target domain becomes:
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^{e}\right):=\sum_{m\in\mathcal{M}}\pi^{e}_m\mathcal{L}\left(f,\mathbb{P}_{m}^{e}\right),\label{eq:subspace_loss}
\end{equation}
where $\mathcal{M}$ is the set of all feasible sub-spaces indexing $m$ and  $\pi^{e}_m=\frac{\mathbb{P}^{e}\left(A_{m}\right)}{\sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)}$.

\end{proof}

\begin{proof}(i):

Using the same proof
for a single space in Theorem \ref{thm:single_bound_A}, we obtain:
\begin{equation} \mathcal{L}\left(f,\mathbb{P}^{e}_m\right)
\leq
\mathcal{L}\left(f_m,\mathcal{\mathbb{P}}_{m}^{e'}\right)
+ 
L\sqrt{2} d_{1/2}\left(g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}_{m}^{e'}\right)
\end{equation}

Since $\mathcal{L}\left(f,\mathbb{P}^{e}\right):= \sum_{m}\pi^{e}_m \mathcal{L}\left(f,\mathbb{D}_{m}^{e}\right)$, taking weighted average over $m\in \mathcal{M}$, we reach (ii):
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^{e}\right)
\leq
\sum_{m}\pi^{e}_m
\mathcal{L}\left(f_m,\mathbb{P}_{m}^{e'}\right)
+ 
L\sqrt{2}\sum_{m}\pi^{e}_m d_{1/2}\left(g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}_{m}^{e'}\right)
\end{equation}

By summing over the training domains on the left-hand side, we obtain:

\begin{align} \sum_{e\in\mathcal{E}_{tr}}\mathcal{L}\left(f_{\mathcal{M}},\mathbb{P}^{e}\right)
\leq&
\sum_{e\in\mathcal{E}_{tr}}\sum_{m}\pi^{e}_m
\mathcal{L}\left(f_m,\mathbb{P}_{m}^{e'}\right)
+ 
\sum_{e\in\mathcal{E}_{tr}}L\sqrt{2}\sum_{m}\pi^{e}_m d_{1/2}\left(g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}_{m}^{e'}\right) \nonumber
\end{align}
Summing over the training domains on the left-hand side again:

\begin{align}
\sum_{e'\in\mathcal{E}_{tr}}\sum_{e\in\mathcal{E}_{tr}}\mathcal{L}\left(f_{\mathcal{M}},\mathbb{P}^{e}\right)
\leq&
\sum_{e'\in\mathcal{E}_{tr}}\sum_{e\in\mathcal{E}_{tr}}\sum_{m}\pi^{e}_m
\mathcal{L}\left(f_m,\mathbb{P}_{m}^{e'}\right)\nonumber\\
&+ 
\sum_{e'\in\mathcal{E}_{tr}}\sum_{e\in\mathcal{E}_{tr}}L\sqrt{2}\sum_{m}\pi^{e}_m d_{1/2}\left(g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}_{m}^{e'}\right)\nonumber
\end{align}

Finally, we obtain:
\begin{align}
\left | \mathcal{E}_{tr} \right | \sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left(f,\mathbb{P}^{e}\right)
\leq&
\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left(f,\mathcal{\mathbb{P}}_{m}^{e'}\right)
+
\sum_{e,e'\in \mathcal{E}_{tr}}L\sqrt{2}\sum_{m\in\mathcal{M}}\pi^{e}_{m} d_{1/2}\left(g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}_{m}^{e'}\right)
\end{align}
\end{proof}


\begin{proof}(ii):

We obtain (ii) directly by applying the results from Theorem \ref{theorem:single_lower_bound_A} to each individual sub-space, denoted by the index $m$.
\end{proof}

\begin{proof}(iii):

Within training domains, we anticipate that $f\in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}$ will predict the ground truth label $f(x)=f^*(x)$ where $f^*\in \mathcal{F}^*$.We can define a projector \(\Gamma = f\), which induces a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \Delta_{\left | \mathcal{Y}\right |}$. As a result, given subspace index $m\in\mathcal{M}$, $\forall i \in \mathcal{Y}, \mathbb{P}^{e}_{\mathcal{Y},m}(Y=i) = \mathbb{P}^{e'}_{\mathcal{Y},m}(Y=i) = \sum_{x \in f^{-1}(m)}\mathbb{P}(Y=i\mid x) = m[i]$. Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.
\end{proof}

% \begin{theorem}
% \label{theorem:information_view-1} \textbf{(Theorem 1 in the main paper)} Let $X$ is a random variable of source
% sample (i.e., drawn from $\mathbb{P}^{S}$) and $Y$ is a random
% variable of ground-truth labels. Denote $N=\sum_{m'\in\mathcal{M}}\mathbb{P}^{S}\left(A_{m'}\right)$,
% we then have
% \begin{equation}
% \mathbb{I}\left(\Gamma\left(g\left(X\right)\right)\odot g\left(X\right),Y\right)\geq-\sum_{m\in\mathcal{M}}\frac{\mathbb{P}^{S}\left(A_{m}\right)}{N}\mathcal{L}\left(f,\mathbb{D}_{m}^{S}\right)+\text{const},\label{eq:mutual_information-1}
% \end{equation}
% where the loss $\mathcal{L}\left(f,\mathbb{D}_{m}^{S}\right)$
% is defined based on the cross-entropy loss and $\mathbb{I}$ denotes
% the mutual information.
% \end{theorem}
% \begin{proof}
% Denote $T=\Gamma\left(g\left(X\right)\right)\odot g\left(X\right)$,
% we have
% \begin{align*}
% \mathbb{I}\left(T,Y\right) = & \int p(t,y)\log\frac{p\left(t,y\right)}{p\left(t\right)p\left(y\right)}dtdy\\
% = & \int p(t,y)\log\frac{p\left(y\mid t\right)}{p\left(y\right)}dtdy\\
% = & \int p(t,y)\log p\left(y\mid t\right)dtdy+\mathbb{H}\left(Y\right)\\
% = & \int p(t,y)\log h\left(y\mid t\right)\frac{p\left(y\mid t\right)}{h\left(y\mid t\right)}dtdy+\mathbb{H}\left(Y\right)\\
% = & \int p(t,y)\log h\left(y\mid t\right)dtdy+D_{KL}\left(p\left(y\mid t\right)\Vert h\left(y\mid t\right)\right)+\mathbb{H}\left(Y\right)\\
% \geq & \int p(t,y)\log h\left(y\mid t\right)dtdy+\text{const},
% \end{align*}
% where $\mathbb{H}$ specifies the entropy, $D_{KL}$is Kullback-Leibler
% (KL) divergence, $h\left(y\mid t\right)=h\left(t,y\right)=h\left(\Gamma\left(g\left(x\right)\right)\odot g\left(x\right),y\right)$
% for any $h:\mathcal{Z}\rightarrow\mathcal{Y}_{\simplex}$, and $h\left(t,y\right)$
% returns the $y$-th element of $h\left(t\right)$.

% We further derive
% \begin{align*}
% \int p(t,y)\log h\left(y\mid t\right)dtdy = & \sum_{i=1}^{C}\mathbb{E}_{p(t)}\left[p\left(y=i\mid t\right)\log h\left( y=i \mid t\right)\right]\\
% \overset{(1)}{=} & \sum_{i=1}^{C} \mathbb{E}_{p^{S}(x)}\left[p\left(y=i\mid\Gamma\left(g\left(x\right)\right)\odot g\left(x\right)\right)\log h\left(\Gamma\left(g\left(x\right)\right)\odot g\left(x\right),i\right)\right]\\
% = & \sum_{i=1}^{C}\mathbb{E}_{\mathbb{P}^{S}}\left[p\left(y=i\mid\Gamma\left(g\left(x\right)\right)\odot g\left(x\right)\right)\log h\left(\Gamma\left(g\left(x\right)\right)\odot g\left(x\right),i\right)\right].
% \end{align*}

% Note that we have $\overset{(1)}{=}$ because $\Gamma\left(g\left(x\right)\right)\odot g\left(x\right)$
% pushes forward $X\sim p^{S}(x)$ to $T\sim p\left(t\right)$.
% Moreover, according to our definitions: $\mathbb{P}^{S}=\sum_{m\in\mathcal{M}}\frac{\mathbb{P}^{S}\left(A_{m}\right)}{N}\mathbb{P}_{m}^{S}$,
% we hence obtain
% \begin{align*}
% \int p(t,y)\log h\left(y\mid t\right)dtdy = & \sum_{m\in\mathcal{M}}\frac{\mathbb{P}^{S}\left(A_{m}\right)}{N}\sum_{i=1}^{C}\mathbb{E}_{\mathbb{P}_{m}^{S}}\left[p\left(y=i\mid m\odot g\left(x\right)\right)\log h\left(m\odot g\left(x\right),i\right)\right]\\
% = & -\sum_{m\in\mathcal{M}}\frac{\mathbb{P}^{S}\left(A_{m}\right)}{N}\sum_{i=1}^{C}\mathbb{E}_{\mathbb{P}_{m}^{S}}\left[-p_{m}^{S}\left(y=i\mid x\right)\log f\left(x,i\right)\right]\\
% = & -\sum_{m\in\mathcal{M}}\frac{\mathbb{P}^{S}\left(A_{m}\right)}{N}\mathcal{L}\left(f,\mathbb{D}_{m}^{S}\right).
% \end{align*}
% \end{proof}


% \begin{lemma} Given a deterministic sub-space indicator $\Gamma$, we have:
% \begin{align}
% d_{1/2}\left(g_{\#}\mathbb{P}^{T},g_{\#}\mathbb{P}_{S}^{S}\right) 
% \geq &
% \sum_{m} \frac{S^S_m+S^T_m}{2} d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right)
% \end{align}
% \label{theorem:latent_data_shift_A}
% \end{lemma}


% \begin{proof}


% By definition, $g_{\#}\mathbb{P}^{T} (z) = g_{\#}\mathbb{P}^{T} (z) = \sum_{m}S^T_m  g_{\#}\mathbb{P}^{T}_{m}\left ( z_m \right ) $ and $ g_{\#}\mathbb{P}_{S}^{S} (z)= g_{\#}\mathbb{P}_{S}^{S} (z) = \sum_{m}S^S_m g_{\#}\mathbb{P}^{S}_{m}\left ( z_m \right )$, we have:
% \begin{align}
% &d_{1/2}\left(g_{\#}\mathbb{P}^{T},g_{\#}\mathbb{P}_{S}^{S}\right)\nonumber\\
% &=
% \left [ 2  \int_{z} \left ( \sqrt{g_{\#}\mathbb{P}^{e_i} (z)}-\sqrt{g_{\#}\mathbb{P}^{e_j}_S (z)} \right )^{2} dz  \right ]^{1/2}
% \\
% &=
% \left [ 2 \int_{z} \left (g_{\#}\mathbb{P}^{e_i} (z) + g_{\#}\mathbb{P}^{e_j}_S (z) - 2\sqrt{g_{\#}\mathbb{P}^{e_i} (z) g_{\#}\mathbb{P}^{e_j}_S (z)} \right ) dz  \right ]^{1/2}
% \\
% &\overset{(1)}{=}\left [ 2\sum_{m}\int_{z \in \Gamma^{-1}(m)} S^T_m  g_{\#}\mathbb{P}^{T}_{m}\left ( z \right ) + S^S_m g_{\#}\mathbb{P}^{S}_{m}\left ( z \right )   -  2\sqrt{S^T_m g_{\#}\mathbb{P}^{S}_{m}\left ( z \right )}
% \sqrt{S^S_m g_{\#}\mathbb{P}^{S}_{m}\left ( z \right )} dz \right ]^{1/2}
% \label{eq:global_d_subspace}
% \end{align}

% Here we note that $\overset{(1)}{=}$ is the results of $\Gamma$ partitioning source and target data to disjoint-sets:
% \begin{equation*}
% g_{\#}\mathbb{P}^{e_j}_m(z),g_{\#}\mathbb{P}^{e_i}_m(z):
% \left\{\begin{matrix}
%  >0 & \text{if} & z\in \Gamma^{-1}\left(m\right)\\ 
%  =0 & \text{if} & z\notin \Gamma^{-1}\left(m\right) \\
% \end{matrix}\right.,
% \end{equation*}
% therefore, given sub-space index $m$ and $z \in \Gamma^{-1}(m)$:
% \begin{align*}
% &g_{\#}\mathbb{P}^{e_j}(z)
% = \sum_{m'}S_{m'} g_{\#}\mathbb{P}^{e_j}_{m'}(z)
% = S_m g_{\#}\mathbb{P}^{e_j}_m(z)\\
% &g_{\#}\mathbb{P}^{e_i}(z)
% = \sum_{m'}S_{m'} g_{\#}\mathbb{P}^{e_i}_{m'}(z)
% = S_m g_{\#}\mathbb{P}^{e_i}_m(z)
% \end{align*}


% Consider $B$ as follows:

% \begin{align}
% B=&\sum_{m} \int_{z} \left ( 
% S^T_m g_{\#}\mathbb{P}^{e_i}_m (z)
% +
% S^S_m g_{\#}\mathbb{P}^{e_j}_m (z) - 2\sqrt{
% S^S_m S^T_m g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)} \right ) \1_{z \in \Gamma^{-1}(m)} dz 
% \\
% =&
% \sum_{m} \int_{z} \left (S^T_m \mathbb{P}^{e_i}_m (z)
% +
% S^T_m g_{\#}\mathbb{P}^{e_j}_m (z)
% - 2\sqrt{
% S^T_m S^T_m g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)}\right ) \1_{z \in \Gamma^{-1}(m)} dz \\
% &+
% \sum_{m} \int_{z} \left (
% (S^S_m-S^T_m) g_{\#}\mathbb{P}^{e_j}_m (z) - 2\left ( \sqrt{S^S_m} - \sqrt{S^T_m} \right )
% \sqrt{S^T_m g_{\#}\mathbb{P}^{e_i}_m(z) \mathbb{P}^{e_j}_m (z)}\right ) \1_{z \in \Gamma^{-1}(m)} dz 
% \\
% =&
% \sum_{m} \int_{z} \left( S^T_m \left(\sqrt{g_{\#}\mathbb{P}^{e_i}_m (z)} - \sqrt{g_{\#}\mathbb{P}^{e_j}_m (z)} \right) ^{2} \right ) \1_{z \in \Gamma^{-1}(m)} dz 
% \\
% &+
% \underset{=\sum_{m}(S^S_m-S^T_m) =0}{\underbrace{\sum_{m} \int_{z} \left (
% S^S_m-S^T_m\right ) g_{\#}\mathbb{P}^{e_j}_m (z) \1_{z \in \Gamma^{-1}(m)}  dz}} 
% \\
% &+
% \sum_{m} \int_{z} 2\left ( \sqrt{S^T_m} - \sqrt{S^S_m} \right )
% \sqrt{S^T_m g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)} \1_{z \in \Gamma^{-1}(m)} dz 
% \\
% =&
% \sum_{m}\int_{z \in \Gamma^{-1}(m)} S^T_m \left (\sqrt{g_{\#}\mathbb{P}^{e_i}_m (z)} - \sqrt{g_{\#}\mathbb{P}^{e_j}_m (z)}\right )^{2} +2\left ( S^T_m - \sqrt{S^T_mS^S_m} \right )
% \sqrt{ g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)} dz
% \end{align}

% Similarly, we also have: 
% \begin{equation}
% B =\sum_{m}\int_{z} S^S_m \left (\sqrt{g_{\#}\mathbb{P}^{e_i}_m (z)} - \sqrt{g_{\#}\mathbb{P}^{e_j}_m (z)}\right )^{2} +2\left ( S^S_m - \sqrt{S^T_mS^S_m} \right )
% \sqrt{g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)}\1_{z \in \Gamma^{-1}(m)} dz
% \end{equation}

% Combining (40) and (41), we have:
% \begin{align}
% B &=\sum_{m}\int_{z} \frac{S^S_m+S^T_m}{2} \left (\sqrt{g_{\#}\mathbb{P}^{e_i}_m (z)} - \sqrt{g_{\#}\mathbb{P}^{e_j}_m (z)}\right )^{2} +2\left ( \sqrt{S^S_m} - \sqrt{S^T_m} \right )^2
% \sqrt{ g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)}\1_{z \in \Gamma^{-1}(m)} dz\\
% &=\sum_{m}\frac{S^S_m+S^T_m}{4}\int_{z}  2\left (\sqrt{g_{\#}\mathbb{P}^{e_i}_m (z)} - \sqrt{g_{\#}\mathbb{P}^{e_j}_m (z)}\right )^{2} +2\left ( \sqrt{S^S_m} - \sqrt{S^T_m} \right )^2
% \sqrt{ g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)}\1_{z \in \Gamma^{-1}(m)} dz\\
% &=\sum_{m} \left [ \frac{S^S_m+S^T_m}{4}\left [ d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right) \right ]^2+\int_{z}2\left ( \sqrt{S^S_m} - \sqrt{S^T_m} \right )^2
% \sqrt{ g_{\#}\mathbb{P}^{e_i}_m(z) g_{\#}\mathbb{P}^{e_j}_m (z)}\right ] \1_{z \in \Gamma^{-1}(m)} dz
% \end{align}

% Substituting $B$ to~(\ref{eq:global_d_subspace}) we have:

% \begin{align}
% &d_{1/2}\left(\sum_{m} S^T_m g_{\#}\mathbb{P}^{T}_{m},\sum_{m} S^S_m g_{\#}\mathbb{P}_{m}^{S}\right)\nonumber\\
% &=
% \left [ 2\sum_{m} \left [ \frac{S^S_m+S^T_m}{4}\left [ d_{1/2}\left(g^\Gamma_{\#}\mathbb{P}^{T}_{m},g^\Gamma_{\#}\mathbb{P}_{m}^{S}\right) \right ]^2+\int_{z} 2\left ( \sqrt{S^S_m} - \sqrt{S^T_m} \right )^2
% \sqrt{ g^\Gamma_{\#}\mathbb{P}^{e_i}_m(z) g^\Gamma_{\#}\mathbb{P}^{e_j}_m (z)}\right ] \1_{z \in \Gamma^{-1}(m)} dz \right ]^{1/2}
% \\
% &\geq
% \left [ \sum_{m}  \frac{S^S_m+S^T_m}{2}\left [ d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right) \right ]^2 \right ]^{1/2}
% \\
% &=
% \left [ \left ( \sum_{m} \frac{S^S_m+S^T_m}{2}\right)  \left ( \sum_{m} \frac{S^S_m+S^T_m}{2}   \left [ d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right) \right ]^2 \right) \right]^{1/2}\\
% &\overset{(3)}{\geq}
% \sum_{m} \frac{S^S_m+S^T_m}{2} d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right)\\
% \end{align}
% We obtain $(3)$ by applying CauchySchwarz inequality. 
% \end{proof}


% \begin{lemma} \textbf{(Lemma 3 in the main paper)} Given a \textit{Source-Target Balanced sup-space projection $\Gamma$} i.e., $S_m^T=S^S_m =S_m \forall m$, we have:

% \begin{enumerate}
%   \item Reduce latent data-shift: 
%   \begin{equation}
%       d_{1/2}\left(g_{\#}\mathbb{P}^{T},g_{\#}\mathbb{P}_{S}^{S}\right)\nonumber \geq
% \sum_{m} S^m d_{1/2}\left(g_{\#}\mathbb{P}^{T}_{m},g_{\#}\mathbb{P}_{m}^{S}\right)
%   \end{equation}
  
%   \item Tighter target-risks' upper-bound:
%   \begin{equation}
%       \mathcal{L}\left(f,\mathbb{P}^{e_i}\right)
% \leq B_{S}\leq B_{M}
%   \end{equation}
% \end{enumerate}

% \label{theorem:latent_data_shift_A}
% %%\vspace{-2mm}
% \end{lemma}



% \subsection{Practical Method}

% \begin{corollary}
% \label{cor:appendix_cluster_latent}\textbf{(Corollary \ref{cor:cluster_latent} in the main paper)} Consider minimizing $\min_{g,C}\mathcal{W}_{d_{z}}\left(g\#\mathbb{P}_{x},\mathbb{P}_{c,\pi}\right)$
% given $\pi$ and assume
% $K<N$, its optimal solution $g^{*}$ and $C^{*}$are also the
% optimal solution of the following OP:
% \begin{equation}
% \min_{g,C}\min_{\sigma\in\Sigma_{\pi}}\sum_{n=1}^{N}d_{z}\left(g\left(x_{n}\right),c_{\sigma\left(n\right)}\right),\label{eq:appendix_clus_latent}
% \end{equation}
% where $\Sigma_{\pi}$ is the set of assignment functions $\sigma:\left\{ 1,...,N\right\} \rightarrow\left\{ 1,...,K\right\} $
% such that the cardinalities $\left|\sigma^{-1}\left(k\right)\right|,k=1,...,K$
% are proportional to $\pi_{k},k=1,...,K$.
% \end{corollary}

% \textbf{Proof of Corollary \ref{cor:appendix_cluster_latent}}.

% By the Monge definition, we have
% \begin{align*}
% \mathcal{W}_{d_{z}}\left(g\#\mathbb{P}_{x},\mathbb{P}_{c,\pi}\right) & =\mathcal{W}_{d_{z}}\left(\frac{1}{N}\sum_{n=1}^{N}\delta_{g\left(x_{n}\right)},\sum_{k=1}^{K}\pi_{k}\delta_{c_{k}}\right)=\min_{T:T\#\left(g\#\mathbb{P}_{x}\right)=\mathbb{P}_{c,\pi}}\mathbb{E}_{z\sim g\#\mathbb{P}_{x}}\left[d_{z}\left(z,T\left(z\right)\right)\right]\\
% = & \frac{1}{N}\min_{T:T\#\left(g\#\mathbb{P}_{x}\right)=\mathbb{P}_{c,\pi}}\sum_{n=1}^{N}d_{z}\left(g\left(x_{n}\right),T\left(g\left(x_{n}\right)\right)\right).
% \end{align*}

% Since $T\#\left(g\#\mathbb{P}_{x}\right)=\mathbb{P}_{c,\pi}$,
% $T\left(g\left(x_{n}\right)\right)=c_{k}$ for some $k$. Additionally,
% $\left|T^{-1}\left(c_{k}\right)\right|,k=1,...,K$ are proportional
% to $\pi_{k},k=1,...,K$. Denote $\sigma:\left\{ 1,...,N\right\} \rightarrow\left\{ 1,...,K\right\} $
% such that $T\left(g\left(x_{n}\right)\right)=c_{\sigma\left(n\right)},\forall i=1,...,N$,
% we have $\sigma\in\Sigma_{\pi}$. It also follows that 
% \[
% \mathcal{W}_{d_{z}}\left(\frac{1}{N}\sum_{n=1}^{N}\delta_{g\left(x_{n}\right)},\sum_{k=1}^{K}\pi_{k}\delta_{c_{k}}\right)=\frac{1}{N}\min_{\sigma\in\Sigma_{\pi}}\sum_{n=1}^{N}d_{z}\left(g\left(x_{n}\right),c_{\sigma\left(n\right)}\right).
% \]


\