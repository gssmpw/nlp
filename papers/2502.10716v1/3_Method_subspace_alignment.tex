

\section{Sufficient Invariant Representation Learning}
\label{sec:main_proposed_method}

Section ~\ref{sec:discussion_DG} highlights that existing DG strategies attempt to maximize the likelihood of seeking a global optimal hypothesis from different directions yet with several drawbacks. Furthermore, that they all overlook \textit{Condition}~\ref{def:sufficient} poses a risk of landing in regions with empty solution set. Generally, an effective DG algorithm is one that strives to attain the sufficient conditions while guarantees the necessary conditions. Here we propose a method that exploits the joint effect of the two sets of conditions to boost generalization. 

In the following, we explain how to incorporate the sufficient representation constraint via \textit{ensemble learning} and present a novel \textit{representation alignment} strategy that can enforce the necessary conditions. We particularly do not consider \textit{invariant prediction} since it cannot substantiate its superiority over ERM with a potential of violating both necessary conditions. Meanwhile, \textit{data augmentation} typically provides significant benefits and can be integrated in a plug-and-play fashion. Since it requires prior knowledge, users should apply it carefully based on their expertise. 

% We here do not aim to propose a state-of-the-art method, rather introduce a foundational approach that can be incorporated into any existing DG framework that already targets sufficient conditions for enhancing their generalizability.  

% Here we propose an approach that can leverage the joint advantages of popular DG approaches while mitigating their respective weaknesses.




\subsection{Subspace Representation Alignment}

\textit{Representation Alignment} strategy helps reduce the cardinality of $\Funion$ but may compromise \textit{Condition}~\ref{def:joint_optimal} due to the potential trade-off between alignment constraints and domain losses (Theorem~\ref{theorem:single_tradeoff}). \textbf{However, we now show that with a more careful design, we can address the trade-off effectively}. Our proposed strategy, called \textit{Subspace Representation Alignment} (SRA), involves organizing training domains into distinct subspaces and aligning representations within these subspaces. 
This aims to diminish or completely remove differences in the marginal label distributions across these domains so that the search space can be reduced. 

% \color{blue}
% Merge to theorem, if $\Gamma=f^*$ them $D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$
% \color{black}
% We consider \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, we denote $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$.
% Let $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$. Eventually, we define $\mathbb{P}_{m}^{e}\left(y\mid x\right)$ as the probabilistic labeling
% distribution on the subspace $\left(A_{m},\mathbb{P}_{m}^{e}\right)$,
% meaning that if $x\sim\mathbb{P}_{m}^{e}$, $\mathbb{P}_{m}^{e}\left(y\mid x\right)=\mathbb{P}_{e}\left(y\mid x\right)$. Since each data point $x \in \mathcal{X}$ corresponds to only a single $\Gamma(x)$, the data space is partitioned into disjoint sets, i.e., $\mathcal{X} = \bigcup_{m=1}^{\mathcal{M}} A_{m}$, where $A_m \cap A_n = \emptyset, \forall m \neq n$. Consequently, 
% $\mathbb{P}^{e}:=\sum_{m\in\mathcal{M}}\pi^{e}_m\mathbb{P}_{m}^{e}$
% where $\pi^{e}_m=\mathbb{P}^{e}\left(A_{m}\right) \slash \sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)$.
\begin{theorem}
\label{theorem:multi_bound} Given \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, let $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$ and $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$, if the loss function $\ell$ is upper-bounded by a positive
constant $L$, then:
(i)  The target general loss is upper-bounded: 
\begin{align*}
\left | \mathcal{E}_{tr} \right |\sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
&\leq
\sum_{e\in \mathcal{E}_{tr}} \sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left ( f,\mathbb{P}^{e}_{m} \right ) \\+
&L\sum_{e, e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_{m}D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right ),
\end{align*}
(ii) Distance between two label marginal distribution $\mathbb{P}^{e}_{m}(Y)$ and $\mathbb{P}^{e'}_{m}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) &\leq 
D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right )\\
+&\mathcal{L}\left ( f,\mathbb{P}^{e}_{m}\right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'}_{m} \right )
\end{aligned}
\end{equation*}
(iii) Construct the subspace projector $\Gamma$ as the optimal hypothesis for the source domains, i,e., \(\Gamma \in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}\), which defines a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=\Gamma(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$, then
\begin{equation*}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0
\end{equation*}

where $g_{\#}\mathbb{P}$ denotes representation distribution on $\mathcal{Z}$ induce by applying $g$ with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence, Hellinger or Wasserstein distance. (Proof in Appendix~\ref{theorem:multi_bound_A})
\label{theorem:tradeoff}
\end{theorem}

In Theorem \ref{theorem:multi_bound}, (i) illustrates that \textit{domain-specific losses} can be broken down into \textit{losses} and \textit{representation alignments} within individual subspaces. Optimizing the subspace-specific losses across domains ensures optimizing the overall loss within the original domains are optimized. Meanwhile, (ii) demonstrates that the distance between the marginal label distributions is now grounded within subspaces, denoted as $d_{1/2}\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$. Theorem~\ref{theorem:multi_bound} suggests that appropriately distributing training domains across subspaces can reduce both the upper and lower bounds. Particularly, for a given subspace index $m$, if $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$, we can jointly optimize both \textit{domains losses} $\mathcal{L}\left (f,\mathbb{P}^{e}_m \right ) + \mathcal{L}\left (  f,\mathbb{P}^{e'}_m \right )$ and \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e}_m,g_{\#}\mathbb{P}^{e'}_m \right )$. Consequently, optimizing the RHS of (ii) for all supspaces is equivalent to minimizing the RHS of (i).

Fortunately, Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

% The question now is how we can manage the training distribution into a subspace such that $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$ is reduced, potentially even to zero. Fortunately, working within training domains, we anticipate that $f\in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}$ will predict the ground truth label $f(x)=f^*(x)$ where $f^*\in \mathcal{F}^*$. We can define a projector \(\Gamma = f\), which induces a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$. As a result, given subspace index $m\in\mathcal{M}$, $\forall i \in \mathcal{Y}, \mathbb{P}^{e}_{\mathcal{Y},m}(Y=i) = \mathbb{P}^{e'}_{\mathcal{Y},m}(Y=i) = \sum_{x \in f^{-1}(m)}\mathbb{P}(Y=i\mid x) = m[i]$. Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

The \textbf{final optimization objective}, encapsulating the constraints of optimal hypothesis for all training domain, ensemble for sufficient representation, and subspace representation alignment is given by:
\begin{align}
\min_{f} &\underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}\\
&\text{ s.t. } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}} \nonumber
\label{eq:final_objective}
\end{align}

% \begin{align}
%     &\min_{f} \underset{\text{Sufficient Representation Constraint}}{\underbrace{\sum_{e\in \mathcal{E}_{tr}}-\mathcal{I}\left(X^{e},g(X^{e})\right )}}
% + \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}
% \label{eq:final_objective}
% \\
% &\text{subject to } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}}.\nonumber
% \end{align}

where $\mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$ and $D$ can be $\mathcal{H}$-divergence, Hellinger distance, Wasserstein distance. We provide the details on the practical implementation of the proposed objective in Appendix~\ref{Sec:practical}.

\import{}{4_Experiments.tex}