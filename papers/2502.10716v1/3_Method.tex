
\section{Preliminaries}
% \subsection{Notation}
We first introduce the notations and basic concepts in the paper. We use calligraphic letters (i.e., $\mathcal{X}$) for spaces, upper case letters (i.e. $X$) for random variables, lower case letters (i.e. $x$) for their values, $\mathbb{P}$ for probability distributions and $\text{supp}(\cdot)$ specifies the support set of a distribution.


\subsection{Revisiting Domain Generalization setting}

%\longvt{missing the introduction $\mathbb{P}^e$?}

We consider a standard domain generalization setting with a potentially high-dimensional variable $X$ (e.g., an image), a label variable $Y$ and a discrete environment (or domain)
variable $E$ in the sample spaces $\mathcal{X}, \mathcal{Y}$, and $\mathcal{E}$, respectively. Specifically, we focus on a multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$. Denote $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
be a hypothesis predicting a $C$-tuple $f\left(x\right)=\left[f\left(x\right)[i]\right]_{i=1}^{C}$,
whose element $f\left(x\right)[i]=p\left(y=i\mid x\right)$,  
is the probability to assign a data sample $x\sim\mathbb{P}$
to the class $i$ with $i\in\left\{ 1,...,C\right\} $. Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
a loss function, where $l\left(f\left(x\right),y\right)$ with $f\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
assign a data sample $x$ to the class $y$ by the hypothesis $f$. The general 
loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].   
\end{equation}

\color{black}

% \color{blue}
% Let $\mathcal{X}$ be a data space on which we endow a data distribution
% $\mathbb{P}$ with a corresponding density function $p(x)$. We consider
% the multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
% where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$.
% % represents the set of the first $C$ positive integer numbers.
% Denote
% $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
% as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% be a probabilistic labeling function returning a $C$-tuple $f\left(x\right)=\left[f\left(x,i\right)\right]_{i=1}^{C}$,
% whose element $f\left(x,i\right)=p\left(y=i\mid x\right)$ 
% % can be interpreted as 
% is the probability to assign a data sample $x\sim\mathbb{P}$
% to the class $i$ (i.e., $i\in\left\{ 1,...,C\right\} $). Moreover,
% a domain is denoted compactly as pair of data distribution and labeling
% function $\mathbb{D}:=\left(\mathbb{P},f\right)$. We note that
% given a data sample $x\sim\mathbb{P}$, its categorical label $y\in\mathcal{Y}$
% is sampled as $y\sim Cat\left(f\left(x\right)\right)$ which a categorical
% distribution over $f\left(x\right)\in\mathcal{Y}_{\Delta}$. 

% Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
% a loss function, where $l\left(f\left(x\right),y\right)$ with
% $f\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
% specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
% assign a data sample $x$ to the class $y$ by the hypothesis $f$.
% Moreover, given a prediction probability $\hat{f}\left(x\right)$
% w.r.t. the ground-truth prediction $f\left(x\right)$, we define the
% loss $\ell\left(\hat{f}\left(x\right),f\left(x\right)\right)=\mathbb{E}_{y\sim f\left(x\right)}\left[l\left(\hat{f}\left(x\right),y\right)\right]=\sum_{y=1}^{C}l\left(\hat{f}\left(x\right),y\right)f\left(x,y\right)$.
% We further define the general loss caused by using a classifier $\hat{f}:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% to predict $\mathbb{D}\equiv\left(\mathbb{P},f\right)$ as
% \begin{equation}
% \mathcal{L}\left(\hat{f},f,\mathbb{P}\right)=\mathcal{L}\left(\hat{f},\mathbb{D}\right):=\mathbb{E}_{x\sim\mathbb{P}}\left[\ell\left(\hat{f}(x),f(x)\right)\right].
% \end{equation}

\textit{Objective}: Given a set of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, the objective of DG is to exploit the `commonalities' present in the training domains to improve generalization to any domain of the population $e\in \mathcal{E}$. For supervised classification, the task is equivalent to seeking the set of \textbf{global optimal hypotheses} $\mathcal{F}^{*}$ 
%\footnote{under the assumptions of the data generation process, the set of global optimal hypotheses defined in Eq.~(\ref{eq:optimal}) is equivalent to the one defined by the worst-case domain: $\mathcal{F}^{*} = \underset{f'\in \mathcal{F}}{\text{argmin}}\sup_{{e}\in \mathcal{E}}\loss{f',\mathbb{P}^{e}}$ in the Appendix~\ref{apd:worstcase}.}
where every $f\in \mathcal{F}^*$ is locally optimal for every domain:
    \begin{equation}
    \mathcal{F}^{*} := \bigcap_{{e}\in \mathcal{E}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}
    \label{eq:optimal}
\end{equation}
% where the hypothesis $f:\mathcal{X}\rightarrow\mathcal{Y}_{\Delta}$ is a map from the data space $\mathcal{X}$ to the the $C$-simplex label space $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{\left | \mathcal{Y} \right |}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$.
% Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $y\in \mathcal{Y}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is: 
% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].    
% \end{equation}

% \longvt{Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $\mathbb{P}(Y\mid x)\in \mathcal{Y}_{\Delta}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:

% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),\mathbb{P}(Y\mid x)\right)\right].    
% \end{equation}
% }

\color{black}

We here examine the widely used {\it composite hypothesis} $f = h \circ g \in \mathcal{F}$, where $g : \mathcal{X} \rightarrow \mathcal{Z}$ belongs to a set of representation functions $\mathcal{G}$, mapping the data space $\mathcal{X}$ to a latent space $\mathcal{Z}$, and $h : \mathcal{Z} \rightarrow \mathcal{Y}_{\Delta}$ is the classifier in the space $\mathcal{H}$. %For simplicity, we assume \(\mathcal{Z}_c, \mathcal{Z}_e \subseteq \mathcal{Z}\) in the following analyses. \hiendh{$\mathcal{Z}_c, \mathcal{Z}_e$ have not been defined}


\textbf{Presumption}. While our work considers limited and a finite number of domains, we follow recent theoretical works \citep{wang2022provable, rosenfeld2020risks, kamath2021does, ahuja2021invariance, chen2022iterative} assuming the infinite data setting for every training environment. This assumption distinguishes DG literature from traditional generalization analysis (e.g., PAC-Bayes framework) that focuses on in-distribution generalization where the testing data are drawn from the same distribution.


\subsection{Assumptions on Data Generation Process}
We consider the following family of distributions over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$ where environment space under consideration $\mathcal{E} = \{ e \mid \mathbb{P}^e \in \mathcal{P} \}$:
\vspace{-2mm}
\begin{equation*}
    \mathcal{P}=\left \{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\right \}
\end{equation*}
The data generative process underlying every observed distribution $\mathbb{P}^e(X, Y)$ is characterized by a \textit{structural causal model} (SCM) over a tuple  $\left \langle V, U, \psi \right \rangle$ (See Figure~\ref{fig:graph}). The SCM consists of a set of \textit{endogenous} variables $V = \{X, Y, Z_c, Z_e, E\}$, a set of mutually independent \textit{exogenous} variables $U = \{U_x, U_y, U_{z_c}, U_{z_e}, U_e\}$ associated with each variable in $V$ and a set of deterministic equations $\psi = \{\psi_x, \psi_y, \psi_{z_c}, \psi_{z_e}, \psi_e\}$ representing the generative process for $V$. We note that this generative structure has been widely used and extended in several other studies, including \citep{chang2020invariant, mahajan2021domain, li2022invariant, zhang2023causal, lu2021invariant,liu2021heterogeneous}. 
% % \begin{figure}[h!]
% \begin{wrapfigure}{r!}{0.5\textwidth}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.25\textwidth]{ICLR2025/Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.\label{fig:graph}}
% \vspace{-2mm}
% \end{wrapfigure} 
% % \end{figure} 


The generative process begins with the sampling of an environmental variable $e$ from a prior distribution $\mathbb{P}(U_e)$\footnote{explicitly via the equation $e = \psi_e(u_e), u_e \sim P(U_e)$.}. We assume there exists a causal factor $z_c\in\mathcal{Z}_c$ determining the label $Y$ and a environmental feature $z_e\in\mathcal{Z}_e$ \textit{spuriously} correlated with $Y$. 
These two latent factors are 
generated from an environment $e$ via the mechanisms $z_c = \psi_{z_c}(e, u_{z_c})$ and $z_e = \psi_{z_e}(e, u_{z_e})$ with $u_{z_c} \sim \mathbb{P}(U_{z_c}), u_{z_e} \sim \mathbb{P}(U_{z_e})$. A data sample $x\in \mathcal{X}$ is generated from both the causal feature and the environmental feature i.e., $x = \psi_{x}(z_c, z_e, u_{x})$ with $u_x \sim \mathbb{P}(U_x)$. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/SCM.png}
    \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.}
    \label{fig:graph}
\end{figure}

% \begin{figure}{h!}{0.5\linewidth}
% \vspace{-5mm}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.28\textwidth]{Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. \\Observed variables are shaded.\label{fig:graph}}
% \vspace{-5mm}
% \end{figure} 



% A causal factor $z_c$ determines a label $y\in \mathcal{Y}$ by $y = \psi_{y}(z_c, u_{y})$ with $u_y \sim \mathbb{P}(U_y)$. The presence of intrinsic noise $u_y$ is crucial, implying that a causal factor can be observed with different label categories. Therefore, without loss of generality, we can consider the label variable $y$ lying on the $C$-simplex, that is $\mathcal{Y} := \Delta_{C} = \left\{ \alpha\in\mathbb{R}^{C}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$. We note this is the point of difference of ours from prior works. As the ``soft" representation of $Y$ encapsulates the noise effect, the generative process for $y$ can now be written as $\psi_y(z_c) = y$.

Figure \ref{fig:graph} dictates that the joint distribution over $X$ and $Y$ can vary across domains resulting from the variations in the distributions of $Z_c$ and $Z_e$. Furthermore, \textit{both causal and environmental features are correlated with $Y$, but only $Z_c$ causally influences $Y$}.  However, because $Y \perp\!\!\!\perp E | Z_c$, the conditional distribution of $Y$ given a specific $Z_c = z_c$ remains unchanged across different domains i.e., $\mathbb{P}^e(Y | Z_c = z_c) = \mathbb{P}^{e'}(Y | Z_c = z_c)$ $\forall e, e' \in \mathcal{E}$. For readability, we omit the superscript $e$ and denote this invariant conditional distribution as $\mathbb{P}(Y | Z_c = z_c)$. 
\subsection{Assumptions on Possibility of Generalization}

We first establish crucial assumptions for the feasibility of generalization as described in Eq (\ref{eq:optimal}). These assumptions are essential for understanding the conditions under which generalization can be achieved. 


%We note that the following discussion may not be entirely new and exist in some other forms dispersing across the literature. We here make the first contribution to unifying them into a systematic and comprehensive analysis. 

\begin{assumption} (Label-identifiability). We assume that for any pair $z_c, z^{'}_c\in \mathcal{Z}_c$,  $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c) \text{ if } \psi_x(z_c,z_e,u_x)=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$
\label{as:label_idf}.
\end{assumption}

The causal graph indicates that $Y$ is influenced by $z_c$, making $Y$ identifiable over the distribution $\mathbb{P}(Z_c)$. This assumption implies that different causal factors $z_c$ and $z^{'}_c$ cannot yield the same $x$, unless the condition $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c)$ holds, or  the distribution $\mathbb{P}(Y\mid x)$ is stable. %This assumption also can be view as covariate shift setting in OOD  \citep{shimodaira2000improving}. 

Assumption \ref{as:label_idf} gives rise to a family of invariant representation function $\mathcal{G}_c$, as stated in Proposition \ref{thm:invariant_correlation} below. 

\begin{proposition} (Invariant Representation Function)
Under Assumption.\ref{as:label_idf}, there exists a set of deterministic representation function $(\mathcal{G}_c\neq \emptyset)\in \mathcal{G}$ such that for any $g\in \mathcal{G}_c$, $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ and $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$ (Appendix \ref{thm:invariant_correlation_apd}).   
\label{thm:invariant_correlation}
\end{proposition}

Assumption \ref{as:label_idf} and Proposition~\ref{thm:invariant_correlation} license the existence of global optimal hypotheses as defined in Eq. (\ref{eq:optimal}).% (\textcolor{blue}{??} App \ref{thm:existence_apd}). 

%\cite{shimodaira2000improving,bickel2009discriminative}. 

%\begin{theorem} $\mathcal{F}^*\neq \emptyset$ if and only if Assumption~\ref{as:label_idf} is hold. (Proof in Appendix \ref{thm:existence_apd}).  \label{thm:existence} \end{theorem}


\begin{assumption} (Causal support). We assume that the union of the support of \textit{causal} factors across training domains covers the entire causal factor space $\mathcal{Z}_c$, i.e., $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$. 
\label{as:sufficient_causal_support}
\end{assumption}

% \begin{assumption} (Spurious support). We assume that the union of the support of \textit{spurious} factors across training domains covers the entire causal factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
% \label{as:sufficient_spurious_support}
% \end{assumption}

This assumption holds significance in DG theories \citep{johansson2019support, ruan2021optimal, li2022sparse}, especially when we avoid imposing strict constraints on the target functions. Particularly, \citep{ahuja2021invariance} showed that without the support overlap assumption on the causal features, OOD generalization is impossible for such a simple linear setting. Meanwhile, for more complicated tasks, deep neural networks are typically employed. However, when trained via gradient descent, they cannot effectively approximate a broad spectrum of nonlinear functions beyond their support range \citep{xu2020neural}. It is worth noting that causal support overlap does not imply that the distribution over the causal features is held unchanged.






\section{DG: A view of Necessity and
Sufficiency}\label{sec:main_conds}

In this section, we present the necessary and sufficient conditions for achieving generalization defined in Eq. (\ref{eq:optimal}) (See Table \ref{tab:conditions} for summary). These conditions are critical to our analysis, where we first reveal that the existing DG methods aim to satisfy one or several of these necessary and sufficient conditions to achieve generalization. %Following this section, we theoretically assess whether a method works effectively by to what extent the necessary conditions are met. 


\subsection{Conditions for Generalization}

We begin with the definition of the global optimal hypothesis in Eq. (\ref{eq:optimal}), where $f\in \mathcal{F}^*$
  must also be the optimal solution across all domains. Thus, $f$ must first satisfy the requirement of being the optimal hypothesis for all training domains, which can be considered a \textit{necessary condition} for generalization, formally stated as follows:

\begin{definition} \textit{(Necessary-Condition-1: Optimal hypothesis for training domains) Given  $\mathcal{F}_{\mathbb{P}^e}=\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}$ is set of optimal hypothesis for $\mathbb{P}^{e}$, the optimal hypothesis for all training domains is defined as $f\in\mathcal{F}_{\mathcal{E}_{tr}} = \bigcap_{{e}\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^e}$.}
\label{def:joint_optimal}
\end{definition}


However, $f \in \mathcal{F}_{\mathcal{E}_{tr}}$ is not sufficient to guarantee that $f \in \mathcal{F}^*$ as a global optimal hypothesis. This drives the exploration of analyses and algorithms that identify the conditions, objectives or constraints required to truly achieve generalization. The following theorem highlights that conventional DG algorithms predominantly focus on satisfying sufficient conditions for generalization.% \textcolor{blue}{suggest reason why}.
% \begin{definition} (Invariance-preserving  transformation). Denote $\mathcal{T}$ is set of all invariance-preserving  transformations  $T(\cdot)$ such that for any $T\in \mathcal{T}$: $(g\circ T)(\cdot)=g(\cdot)$.
% \label{def:causal_transformation}
% \end{definition}

% \begin{definition} (Sufficient and diverse training domains). Set of training domains $\mathcal{E}_{tr}$ is consider as sufficient and diverse if  the union of the support of \textit{spurious} factors across training domains covers the entire spurious factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$. 
% \label{as:sufficient_spurious_support}
% \end{definition}

\begin{theorem} (Sufficient conditions) Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, given a hypothesis $f=h\circ g$, if $f$ is optimal hypothesis for training domains i.e.,
\begin{equation*}
    f\in \bigcap_{{e}\in \mathcal{E}_{tr}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}
\end{equation*}
and one of the following sub-conditions holds:
\begin{enumerate}
    \item $g$ belongs to the set of \textbf{invariant representation functions} as specified in Proposition~\ref{thm:invariant_correlation}.
    
    \item $\mathcal{E}_{tr}$ is a set of \textbf{Sufficient and diverse training domains} i.e., the union of the support of joint causal and spurious factors across training domains covers the entire causal and spurious factor space $\mathcal{Z}_c\times\mathcal{Z}_e$ i.e., $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c, Z_e \right )\}=\mathcal{Z}_c\times\mathcal{Z}_e$.
    
    \item Given $\mathcal{T}$ is set of all \textbf{invariance-preserving transformations} such that for any $T\in \mathcal{T}$ and $g_c\in \mathcal{G}_c$: $(g_c\circ T)(\cdot)=g_c(\cdot)$, $f$ is also an optimal hypothesis on all augmented domains i.e., $$f\in \bigcap_{{e}\in \mathcal{E}_{tr}, T\in \mathcal{T}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,T\#\mathbb{P}^{e}}$$
\end{enumerate}
Then $f\in \mathcal{F}^*$ (Proof is in Appendix \ref{thm:sufficient_conditions_apd})
\label{thm:sufficient_conditions}.
\end{theorem}

Theorem \ref{thm:sufficient_conditions} states that the Necessary-Condition-1 ``optimal hypothesis for training domains" condition is the primary objective to achieve generalization and should not be violated when performing DG algorithms. The other three sub-conditions, \textit{Invariant Representation Function, Sufficient and Diverse Training Domains, and Invariance-Preserving Transformations}, are additional properties, or \textit{``constraints"}, to transfer the optimality on training domains to the true generalization on unseen domains. The theorem demonstrates that the main objective, combined with any one of these sub-conditions, forms a sufficient condition for the solution $f$ to generalize. 

Note that the high-level findings corresponding to each sufficient condition in theorem above are not new. However, generalizing these results within our framework in the form of sufficient conditions provides a fresh perspective on the functioning of DG algorithms, enabling a deeper analysis of when and why they succeed or fail.  Particularly, each sub-condition corresponds to a conventional DG strategy, as follows:
\begin{itemize}
    \item \textit{Invariant Representation Function}: This is the core objective of the \textit{representation alignment} methods \cite{ben2010theory, lu2021invariant,zhang2023causal}.
    
    \item \textit{Sufficient and Diverse Training Domains}: This constraint is independent of specific algorithms but appears as the backbone in the analysis of \textit{invariant prediction}-based algorithms \citep{ ahuja2021invariance}.
    
    \item \textit{Invariance-Preserving Transformations}: This ensures generalization for the family of \textit{augmentation}-based DG algorithms \citep{mitrovic2020representation,gao2023out}.
\end{itemize}

However, satisfying these sub-conditions (or constraints) is often impractical in scenarios with a limited number of training domains. In practice, these constraints act as regularization mechanisms, shaping the feasible optimal hypothesis space. To gain deeper insights into the dynamics of the hypothesis space under such limitations, we shift our focus to another necessary condition, \textit{the invariance-preserving representation function}, which is crucial yet frequently overlooked in the DG literature. It is formally defined as follows:

% \begin{definition}\textit{(Necessary condition-2: Invariance-preserving representation function) A set of representation functions $\mathcal{G}_s\subset \mathcal{G}$ is considered as Causal-preserved representation functions if for any $g\in\mathcal{G}_s$, there exists a classifier $h: \mathcal{Z}\rightarrow \mathcal{Y}$ such that $h\circ g \in \mathcal{F}^*$. }
% \label{def:sufficient}
% \end{definition}
% \color{black}

% Recall that an invariant representation function seeks to extract \textit{exact} invariant representations that remain consistent across all environments. In contrast, an invariance-preserving representation function captures representations that \textit{contain} invariant information rather than strictly enforcing invariance. 


% This property is critical for understanding the generalization ability of DG algorithms, as discussed in the following section.

% \begin{definition}\textit{(Necessary condition-2: Invariance-preserving representation function) A set of representation functions $\mathcal{G}_s\subset \mathcal{G}$ is considered as Causal-preserved representation functions if for any $g\in\mathcal{G}_s$, there exists a function $\phi: \mathcal{Z}\rightarrow \mathcal{Z}$ such that
% $(\phi\circ g) \in \mathcal{G}_c$ (i.e., given $g\in \mathcal{G}_s$, $g(x)$ retains all information about causal feature of $x$). }
% \label{def:sufficient}
% \end{definition}

\begin{definition}\textit{(Necessary-Condition-2: Invariance-preserving representation function) A set of representation functions $\mathcal{G}_s\subset \mathcal{G}$ is considered as invariance-preserving representation functions if for any $g\in\mathcal{G}_s$,
$I(g(X),g_c(X))=I(X,g_c(X)$ where $g_c$ is an invariant representation function (i.e., $g(X)$ retains all the information about the invariant features of $g_c(X)$ from $X$). }
\label{def:sufficient}
\end{definition}


Recall that an invariant representation function seeks to extract \textit{exact} invariant representations that remain consistent across all environments. In contrast, an invariance-preserving representation function captures representations that contain invariant information, along with potentially other information, rather than strictly enforcing invariance. 
%This concept is inspired by the data-processing inequality, which states that if a representation $g(X)$ contains information about invariant representation $g_c(X)$, there exists a deterministic mapping $\phi$ that can project $g(X)$ onto $g_c(X)$. The following theorem shows that $g \in \mathcal{G}_s$ is necessary for achieving the global optimal hypothesis. 

\begin{theorem} Given a representation function $g$,
$\exists h: h\circ g\in \mathcal{F}^*$ if and only if $g\in \mathcal{G}_s$. (Proof is in Appendix~\ref{thm:nacessary_apd})
\label{thm:nacessary}
\end{theorem}

This theorem implies that if $g$ is not an invariance-preserving representation function, i.e., $g \notin \mathcal{G}_s$, no classifier $h$ can exist such that $f=h \circ g \in \mathcal{F}^*$. In other words, $g \in \mathcal{G}_s$ is a \textit{necessary} condition for $f \in \mathcal{F}^*$. This property is critical for understanding the generalization ability of DG algorithms, as discussed in the following section.


\subsection{Why conventional DG algorithms fail?}
\label{sec:efficacy_DG}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/space1.png}
    \vspace{-7mm}
    \caption{Venn diagram of the optimal hypothesis spaces induced by a DG algorithm $A$.}%The circles (\textcolor{brown}{brown}, \textcolor{blue}{blue}, \textcolor{red}{red}) denote the spaces of domain-optimal hypotheses \textcolor{brown}{$\mathcal{F}_{\mathbb{P}^{e_1}}$}, \textcolor{blue}{$\mathcal{F}_{\mathbb{P}^{e_2}}$}, \textcolor{red}{$\mathcal{F}_{\mathbb{P}^{e_3}}$} of training domains $e_1, e_2, e_3\in \mathcal{E}_{tr}$ respectively. %The grey area indicates the space of global optimal hypotheses $\mathcal{F}^{*}$ while the green area represents the joint space of domain-optimal hypotheses $\mathcal{F}_{\mathcal{E}_{tr}}$.    }
    \vspace{-7mm}
\label{fig:space}
\end{figure}

In this Section, we illustrate the relationship between sufficient and necessary conditions using Venn diagrams of hypothesis spaces corresponding to each condition to understand how DG algorithms help generalization when sufficient conditions cannot be fully met. Additionally, we denote the feasible hypothesis spaces induced by applying a DG algorithm $A$ as $\mathcal{F}^*_{A},\mathcal{F}_{A,e_i}, \mathcal{F}_{A,\mathcal{E}_{tr}}$.

We start with two necessary conditions. By definition of the first necessary condition, "Optimal Hypothesis for Training Domains," we have 
$\mathcal{F}^*_A \subseteq \mathcal{F}_{A,\mathcal{E}_{tr}}$. Thus, in Figure~\ref{fig:space}, the global optimal hypothesis space $\mathcal{F}^*_A$ (grey area) lies within the joint domain-optimal hypothesis space $\mathcal{F}_{A,\mathcal{E}_{tr}}$ (green area). According to Theorem~\ref{thm:nacessary}, if the "Invariance-Preserving Representation Function" condition is not satisfied, the feasible global optimal hypothesis space becomes empty $\mathcal{F}^*_A=\emptyset$. In other words, satisfying the Necessary-Condition-2 ensures the existence of generalization.

Turning to sufficient conditions, Theorem~\ref{thm:sufficient_conditions} states that if these conditions (or constraints) are satisfied, a global optimal hypothesis is achieved, meaning the green area converges to the grey area. Otherwise, the constraints imposed by conventional DG algorithms act as regularization on the first necessary condition, primarily aiming to shrink the green area. If the grey area remains intact, reducing the green area increases the likelihood of achieving generalization. However, while restricting the set of feasible joint optimal hypotheses for the training domains (green area), these regularizations may inadvertently shrink the grey area. If the constraints are arbitrary or overly restrictive, there is a risk that the grey area reduces to null, ultimately causing the DG algorithms to fail.

% Interestingly, a key insight from Theorem~\ref{thm:nacessary} is that, under the \textit{Condition}~\ref{def:joint_optimal}, as long as the solution of an algorithm fulfills the \textit{Invariance-preserving representation function function} constraint (\textit{Condition}~\ref{def:sufficient}), there exists a non-empty $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$; otherwise $\mathcal{F}^{*} = \emptyset$. In fact, that an algorithm meets a sufficient condition implies the satisfaction of \textit{Condition}~\ref{def:sufficient} by default.

% \textbf{In summary}, an algorithm should be effectively designed to minimize the space $\mathcal{F}_{\mathcal{E}_{tr}}$ while maintaining the coverage of $\mathcal{F}^{*}$. \textit{Condition}~\ref{def:joint_optimal} ensures the \textit{green} area is non-empty i.e., $\mathcal{F}_{\mathcal{E}_{tr}} \ne \emptyset$ while  \textit{Condition}~\ref{def:sufficient} ensures the \textit{grey} one is non-empty i.e., $\mathcal{F}^{*} \ne \emptyset$. Satisfying both conditions further guarantees the existence of the global optimal solutions in $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. In contrast, if both conditions are violated, the algorithm has zero chance of achieving generalization. Despite its significance, existing DG algorithms tends to overlook \textit{Condition}~\ref{def:sufficient}. From finite training domains, they thus cannot guarantee the possibility of searching for global optimal hypotheses. 




\section{Understanding DG Literature via Necessity} 
 \label{sec:discussion_DG}

In the previous section, we examined the role of sufficient conditions and the importance of necessary conditions in ensuring generalization. Building on this, we analyze why existing DG methods fail by highlighting that while they promote sufficient conditions, they violate necessary ones.

\subsection{Representation Alignment} 

Representation Alignment focus on learning domain-invariant representations by reducing the divergence between latent marginal distributions $\mathbb{E}[g(X) | E]$ where $E$ represents a domain environment. Other methods seek to align the conditional distributions $\mathbb{E}[g(X) | Y=y, E]$ across domains. Achieving true invariance is inherently challenging and may impose overly restrictive conditions. In certain cases, better feature alignment can result in higher joint errors, as demonstrated in the following theorem:

\begin{theorem}
\label{theorem:single_tradeoff} \citep{zhao2019learning, phung2021learning, le2021lamda} Distance between two marginal distribution $\mathbb{P}^{e}_\mathcal{Y}$ and $\mathbb{P}^{e'}_\mathcal{Y}$ can be upper-bounded: 
\vspace{-1mm}
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right) \leq 
D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )
+\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'} \right )
\end{aligned}
\end{equation*}
where $g_{\#}\mathbb{P}(X)$ denotes representation distribution on  representation space $\mathcal{Z}$ induced by applying encoder with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$. $D$ can be $\mathcal{H}$-divergence \citep{zhao2019learning}, Hellinger distance \citep{phung2021learning} or Wasserstein distance \citep{le2021lamda} (see Appendix~\ref{apd:tradeoff}).
\end{theorem}

Theorem~\ref{theorem:single_tradeoff} suggests that if there is a substantial discrepancy in the label marginal distribution $D(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'})$ across training domains, strongly enforcing \textit{representation alignment} $D( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'})$ will lead to an increase in \textit{domain-losses} $\left ( \mathcal{L}( f,\mathbb{P}^{e}) + \mathcal{L}( f,\mathbb{P}^{e'})\right )$. In other words, while the representation alignment strategy promotes the development of an invariant representation function, it may also pose a challenge to Necessary-Condition-1. %Thus, performance improvement is still attainable with careful adjustment of the alignment weight by exploiting the oracle knowledge of the target domain.
\label{sec:representation_alignment}


\subsection{Augmentation} 
\begin{figure}[h!]
    \centering
\includegraphics[width=1.0\linewidth]{Figures/aug.png}
\vspace{-7mm}
    \caption{(Left) RandomResizedCrop alters the label-information, whereas ColorJitter does not. (Right) ColorJitter modifies the label-information of traffic light.}
    \label{fig:aug}
    \vspace{-4mm}
\end{figure}
Data augmentation utilizes predefined or learnable transformations $T$ on the original sample $X$ or its features $g(x)$ to create augmented data $T(X)$ or $T(g(x))$. However, it's crucial that transformation $T$ maintains the integrity of the causal factors. For instance, in Figure~\ref{fig:aug}.Left, random-cropping augmentation alters the causal features, whereas color-based augmentation does not. However, in Figure~\ref{fig:aug}.Right, color-based augmentation modifies the causal features. This implies a \textit{necessity for some knowledge of the target domain} to ensure the transformations do not alter the label information \citep{gao2023out,zhang2022rethinking}, otherwise it risks violating Necessary-Condition-2.
\label{sec:augmentation}

\subsection{Invariant Prediction} These methods aim to learn a consistent optimal classifier across domains. For example, Invariant Risk Minimization (IRM) \citep{arjovsky2020irm} seeks to learn a representation function $g(x)$ with invariant predictors $\mathbb{E}[Y | g(x), E]$. This goal aligns with Necessary-Condition-1 without imposing restrictions that could affect Necessary-Condition-2. In fact, in fully informative invariant features setting (i.e., $Y \!\perp\! X\mid g(x)$) or the number of training domains is limited, IRM does not provide significant advantages over ERM \citep{rosenfeld2020risks, ahuja2020empirical, ahuja2021invariance}. VREx \citep{krueger2021out} relaxes the IRM's constraint to enforce equal risks across domains, assuming that the optimal risks are similar across domains. If, however, the optimal solutions exhibit large loss variations, balancing risks could result in suboptimal performance for some domains, violating Necessary-Condition-1.
%IIB \citep{li2022invariant} and 
IB-IRM \citep{ahuja2021invariance} posits that integrating the information bottleneck (minimizing $I(g(X); X)$) principle to invariant prediction strategies aids generalization.

However, information bottleneck approach is beneficial only when there is a sufficient and diverse set of training domains \cite{ahuja2021invariance}. Otherwise, the information bottleneck may lead to a higher risk of violating Necessary-Condition-2. The following corollary demonstrates our arguments.

% The following corollary demonstrates that the information bottleneck approach is beneficial only when there is a sufficient and diverse set of training domains (see Theorem 1 or Assumptions 5 and 6 regarding the overlap between invariant and spurious feature support in \cite{ahuja2021invariance}). Otherwise, the information bottleneck may lead to a higher risk of violating Necessary-Condition-2.

\begin{corollary} Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, let the minimal representation function $g_{\text{min}}$ be defined as:
\begin{equation*}
g_{\text{min}} \in \left\{\underset{g \in \mathcal{G}}{\text{argmin }} I(g(X); X) \ \text{s.t.} \ f = h \circ g \in \mathcal{F}_{\mathcal{E}_{\text{tr}}} \right\},
\end{equation*}
where $I(\cdot,\cdot)$ denotes mutual information. Then, for any $g_c\in \mathcal{G}_c$ the following holds:
\begin{equation}
I(g_{\text{min}}(X), g_c(X)) \leq I(X, g_c(X)),
\label{eq:ineq}
\end{equation}
and the equality holds if and only if at least one of sufficient conditions is hold. (Proof is provided in Appendix \ref{thm:information_apd}.)
\label{thm:information}.
\end{corollary}

This corollary, derived from Theorem~\ref{thm:sufficient_conditions}, shows that the minimal representation \( g_{\text{min}} \) violates Necessary-Condition-2  i.e., $I(g_{\text{min}}(X), g_c(X)) < I(X, g_c(X))$ unless one of the sufficient conditions is met, in that case, the equality holds.

\begin{figure}[h!]
    \centering
\includegraphics[width=1.0\linewidth]{Figures/info.png}
\vspace{-7mm}
    \caption{Information diagrams of \( X, Y \); the invariant representation \( g_c(X) \); the minimal representation \( g_{\text{min}}(X) \); and the representations \( g_i(X), g_j(X) \), where there exist corresponding classifiers on top of these representations that form optimal hypotheses for the training domains.}
    \label{fig:info_min}
    %\vspace{-5mm}
\end{figure}

Particularly, the intuition of this corollary is illustrated in Figure~\ref{fig:info_min}.Mid which depicts the mutual information among the four variables $X,Y,g_c(X)$ and $g_{min}(X)$. By Assumption~\ref{as:sufficient_causal_support} and Proposition~\ref{thm:invariant_correlation}, it follows that $X$ contains all information about $Z_c$. Based on the data generation process, we have $I(X,Y \mid Z_c)=0$, indicating that the causal features $Z_c$ must encapsulate the shared information $I(X;Y)$. 
By Theorem~\ref{thm:sufficient_conditions}, if one sufficient condition is satisfied, generalization is achieved with $g_{min}$, which implies: $I(g_{min}(X), g_c(X)) = I(X, g_c(X))$ (Figure~\ref{fig:info_min}.Left).
Otherwise, \( g_{\text{min}} \) tends to favor simple features, such as textures or backgrounds, rather than more complex and generalizable semantic features, like shapes \citep{geirhos2020shortcut}, leading to violate Necessary-Condition-2 (Figure~\ref{fig:info_min}.Right).


\subsection{Ensemble Learning} 
\label{sec:sufficient_constraint}

In line with the analyses based on the information-theoretic perspective in the previous section, we intuitively discuss a connection between the Necessary-Condition-2 and the recent \textit{ensemble} strategy, demonstrating that \textit{ensemble} methods indeed encourage models to satisfy this. Particularly, it can be observed that any representation \( g_i \) capable of forming an optimal hypothesis for the training domains captures the shared information \( I(X; Y) \) and potentially some additional information about \( g_c(X) \) (Figure~\ref{fig:info_min} (Mid)).

One possible way to increase the likelihood of \( g_i(X) \) capturing information about \( g_c(X) \) is to maximize \( I(X, g_i(X)) \). However, this approach may not be appropriate when combined with domain generalization (DG) algorithms designed to encourage sufficient conditions. For instance, representation alignment approaches aim to retain shared information across domains while removing other information, whereas maximizing \( I(X, g_i(X)) \) seeks to preserve as much information about \( X \) as possible, regardless of its relevance.

In contrast, \textit{Ensemble Learning} can encourage the representation to capture more information from \( g_c(X) \) by learning multiple diverse versions of representations through ensemble methods to capture as much information as possible about \( g_c(X) \) (as illustrated in Figure~\ref{fig:info_min} (Right)). Additionally, each version \( g_i(X), \dots, g_j(X) \) can also align with the constraints necessary to satisfy the sufficient conditions.

% While this discussion is currently informal form, it provides a new insight to the connection of ensemble-based methods to DG literatures can also encourage the learning of \textit{sufficient representation}  (\textit{Condition} \ref{def:sufficient}) to promote generalizability.

% This intuition aligns with the analysis of ensembles for OOD generalization presented in \cite{rame2022diverse}.



\section{Necessity-Preserving DG Algorithm}
\label{sec:main_proposed_method}

Section~\ref{sec:discussion_DG} highlights that while conventional DG algorithms promote sufficient conditions, they often violate necessary conditions, which, as analyzed in Section~\ref{sec:main_conds}, is the primary reason for their failure to generalize. To further validate our analysis, we propose a method that promote sufficient conditions without violate necessary conditions.

To recap, the three conventional strategies are as follows: \textit{Invariant prediction}-based methods require sufficient and diverse domains, which depend heavily on the data;
\textit{Augmentation}-based methods rely on invariance-preserving transformations, which often require access to the target domain;
\textit{Representation alignment}-based methods are more flexible and can be directly manipulated. Therefore, in this section, we present a method that imposes the representation alignment constraint without violating necessary conditions, thereby offering a practical means to verify our analysis.


\subsection{Subspace Representation Alignment (SRA)}

As demonstrated in Theorem~\ref{theorem:single_tradeoff}, \textit{Representation Alignment}-based methods face a trade-off between alignment constraints and domain losses due to discrepancies in label distribution across domains, resulting in a violation of necessary conditions. The following theorem demonstrates that by appropriately organizing training domains into distinct subspaces, where differences in marginal label distributions are removed, we can align representations within these subspaces without violating necessary conditions.

% \color{blue}
% Merge to theorem, if $\Gamma=f^*$ them $D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$
% \color{black}
% We consider \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, we denote $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$.
% Let $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$. Eventually, we define $\mathbb{P}_{m}^{e}\left(y\mid x\right)$ as the probabilistic labeling
% distribution on the subspace $\left(A_{m},\mathbb{P}_{m}^{e}\right)$,
% meaning that if $x\sim\mathbb{P}_{m}^{e}$, $\mathbb{P}_{m}^{e}\left(y\mid x\right)=\mathbb{P}_{e}\left(y\mid x\right)$. Since each data point $x \in \mathcal{X}$ corresponds to only a single $\Gamma(x)$, the data space is partitioned into disjoint sets, i.e., $\mathcal{X} = \bigcup_{m=1}^{\mathcal{M}} A_{m}$, where $A_m \cap A_n = \emptyset, \forall m \neq n$. Consequently, 
% $\mathbb{P}^{e}:=\sum_{m\in\mathcal{M}}\pi^{e}_m\mathbb{P}_{m}^{e}$
% where $\pi^{e}_m=\mathbb{P}^{e}\left(A_{m}\right) \slash \sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)$.
\begin{theorem}
\label{theorem:multi_bound} \textit{(Subspace Representation Alignment)} Given \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, and a subspace index $m\in \mathcal{M}$, let $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$ and $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$, then $\pi^{e}_m=\frac{\mathbb{P}^{e}\left(A_{m}\right)}{\sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)}$ is mixture co-efficient, if the loss function $\ell$ is upper-bounded by a positive
constant $L$, then:

(i)  The target general loss is upper-bounded: 
\begin{align*}
\left | \mathcal{E}_{tr} \right |\sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
&\leq
\sum_{e\in \mathcal{E}_{tr}} \sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left ( f,\mathbb{P}^{e}_{m} \right ) \\+
&L\sum_{e, e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_{m}D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right ),
\end{align*}
(ii) Distance between two label marginal distribution $\mathbb{P}^{e}_{m}(Y)$ and $\mathbb{P}^{e'}_{m}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) &\leq 
D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right )\\
+&\mathcal{L}\left ( f,\mathbb{P}^{e}_{m}\right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'}_{m} \right )
\end{aligned}
\end{equation*}
(iii) Construct the subspace projector $\Gamma$ as the optimal hypothesis for the training domains i,e., \(\Gamma \in \mathcal{F}_{\mathcal{E}_{tr}}\), which defines $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=\Gamma(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$, then
$
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$ for all \(m \in \mathcal{M}\).

where $g_{\#}\mathbb{P}$ denotes representation distribution on $\mathcal{Z}$ induce by applying $g$ with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence, Hellinger or Wasserstein distance. %(Proof in Appendix~\ref{theorem:multi_bound_A})
\end{theorem}

Theorem \ref{theorem:multi_bound}.(i) shows that the \textit{domain losses} on the left-hand side (LHS) are upper-bounded by the \textit{subspace losses} and \textit{subspace alignments} within individual subspaces on the right-hand side (RHS). %Optimizing subspace-losses ensures the optimization of the overall loss within the original domains.
If the RHS is perfectly optimized (i.e., without trade-offs), representation alignment can be imposed without violating the necessary conditions.

Theorem \ref{theorem:multi_bound}.(ii) demonstrates that the distance between the marginal label distributions is now defined within subspaces, denoted as \( D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) \). It is worth noting that \( D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) \) can be adjusted based on how \(\Gamma\) distributes the training domains across subspaces. 

Theorem \ref{theorem:multi_bound}.(iii) shows that by constructing the subspace projector \(\Gamma\) as the optimal hypothesis for the training domains i.e., \(\Gamma \in \mathcal{F}_{\mathcal{E}_{tr}}\), we achieve $D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0$
for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{subspace losses} and \textit{subspace alignments} without trade-off. 


% The question now is how we can manage the training distribution into a subspace such that $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$ is reduced, potentially even to zero. Fortunately, working within training domains, we anticipate that $f\in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}$ will predict the ground truth label $f(x)=f^*(x)$ where $f^*\in \mathcal{F}^*$. We can define a projector \(\Gamma = f\), which induces a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$. As a result, given subspace index $m\in\mathcal{M}$, $\forall i \in \mathcal{Y}, \mathbb{P}^{e}_{\mathcal{Y},m}(Y=i) = \mathbb{P}^{e'}_{\mathcal{Y},m}(Y=i) = \sum_{x \in f^{-1}(m)}\mathbb{P}(Y=i\mid x) = m[i]$. Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

\textbf{SRA objective.} Building on Theorem \ref{theorem:multi_bound}, the objective of the subspace representation alignment algorithm is defined as:
\begin{align}
&\min_{f=h\circ g} \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}\label{eq:final_objective}\\
&\text{ s.t. } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f=h\circ g\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}} \nonumber
\end{align}

% \begin{align}
%     &\min_{f} \underset{\text{Invariance-preserving representation function Constraint}}{\underbrace{\sum_{e\in \mathcal{E}_{tr}}-\mathcal{I}\left(X^{e},g(X^{e})\right )}}
% + \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}
% \label{eq:final_objective}
% \\
% &\text{subject to } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}}.\nonumber
% \end{align}

where $\mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$ and $D$ can be $\mathcal{H}$-divergence, Hellinger distance, Wasserstein distance. We provide the details on the practical implementation of the proposed objective in Appendix~\ref{Sec:practical}.

