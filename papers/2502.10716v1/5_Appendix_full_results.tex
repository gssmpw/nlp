\section{Experimental Settings}
\label{apd:settings}

% \paragraph{Metric.} we adopt the training and evaluation protocol as in DomainBed benchmark \citep{gulrajani2020search}, including dataset splits, hyperparameter (HP) search, model selection on the validation set, and optimizer HP. However, we use a reduced HP search space to reduce computational costs. For training, we choose one domain as the target domain and the remaining domains as the training domain, with 20\% of the samples used for validation and model selection. 

% \paragraph{Datasets.} Following existing benchmark \citep{gulrajani2020search}, we evaluate our method on five datasets: PACS~\citep{li2017deeper} (9,991 images, 7 classes, and 4 domains), VLCS~\citep{torralba2011unbiased} (10,729 images, 5 classes, and 4 domains), OfficeHome~\citep{venkateswara2017deep} (15,588 images, 65 classes, and 4 domains), TerraIncognita~\citep{beery2018recognition} (24,788 images, 10 classes, and 4 domains), and DomainNet~\citep{peng2019moment} (586,575 images, 345 classes, and 6 domains).
\paragraph{Metrics.} We adopt the training and evaluation protocol as in DomainBed benchmark \citep{gulrajani2020search}, including dataset splits, hyperparameter (HP) search, model selection on the validation set, and optimizer HP. To manage computational demands more efficiently, as suggested by \citep{cha2021swad}, we narrow our HP search space. Specifically, we use the Adam optimizer, as detailed in \citep{gulrajani2020search}, setting the learning rate to a default of $5e^{-5}$ and forgoing dropout and weight decay adjustments. The batch size is maintained at 32. For DomainNet, we run a total of 15,000 iterations, while for other datasets, we limit iterations to 5,000, deemed adequate for model convergence. Our method's unique parameters, including the regularization hyperparameters $(\lambda_P, \lambda_D)$, undergo optimization within the range of $[0.01, 0.1, 1.0]$, and the number of prototypes $\left | \mathcal{Z} \right |$ is fixed at 16 times the number of classes. It is worth noting that  while we conduct ablation study on PACS dataset, we utilize the number of prototypes $\left | \mathcal{Z} \right |$ is fixed at $16$ times the number of classes for all datasets.
SWAD-specific hyperparameters remain unaltered from their default settings. The evaluation frequency is set to 300 for all dataset.

Our code is anonymously published at \url{https://anonymous.4open.science/r/submisson-FCF0}.


\subsection{Datasets}
To evaluate the effectiveness of the proposed method, we utilize five
datasets: PACS~\citep{li2017deeper}, VLCS~\citep{torralba2011unbiased},
 Office-Home~\citep{venkateswara2017deep}, Terra Incognita~\citep{beery2018recognition} and DomainNet~\citep{peng2019moment} which are the common DG benchmarks with multi-source domains.
\begin{itemize}
    \item \textbf{PACS}~\citep{li2017deeper}: 9991 images of seven classes in total, over four domains:Art\_painting (A), Cartoon (C), Sketches (S), and Photo (P). 
    
    \item \textbf{VLCS}~\citep{torralba2011unbiased}: five classes over four domains with a total of 10729 samples. The domains are defined by four image origins, i.e., images were taken from the PASCAL VOC 2007 (V), LabelMe (L), Caltech (C) and Sun (S) datasets. 


    \item \textbf{Office-Home}~\citep{venkateswara2017deep}: 65 categories of 15,500 daily objects from 4 domains: Art, Clipart, Product (vendor website with white-background) and Real-World (real-object collected from regular cameras).
    \item \textbf{Terra Incognita}~\citep{beery2018recognition} includes 24,788 wild photographs of dimension (3, 224, 224) with 10
animals, over 4 camera-trap domains L100, L38, L43 and L46. This dataset contains photographs of wild animals taken by camera traps; camera trap locations are different across 
domains. 
    \item  \textbf{DomainNet}~\citep{peng2019moment} contains 596,006 images of dimension (3, 224, 224) and 345 classes, over
6 domains clipart, infograph, painting, quickdraw, real and sketch. This is the biggest
dataset in terms of the number of samples and classes.
\end{itemize}



% \subsection{Results}
% \label{apd:result_details}
% In this section, we present the extended results of Table \ref{tab:Averages} in the main text. The following tables report the domain-specific performance of each method on 5 datasets: VLCS (Table \ref{tab:VLCS}), PACS (Table \ref{tab:PACS}), OfficeHome (Table \ref{tab:OfficeHome}), TerraIncognita (Table \ref{tab:TerraIncognita}) and Domain Net (Table \ref{tab:DomainNet}).

% Standard errors are computed over three trials. Our models are run on 4 RTX 6000 GPU cores of 32GB. One full training routine takes roughly 2 hours. 



% % \subsubsection{VLCS}
% \begin{table}[h!]
% \caption{Classification Accuracy on \textbf{VLCS} using ResNet50}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.7\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{C} & \textbf{L} & \textbf{S} & \textbf{V} & \textbf{Avg}  \\
% \midrule
% ERM~\citep{zhang2020adaptive} & 97.7 $\pm$ 0.4 & 64.3 $\pm$ 0.9 & 73.4 $\pm$ 0.5 & 74.6 $\pm$ 1.3 & 77.5 \\
% DANN~\citep{ganin2016domain}& 99.0 $\pm$ 0.3 & \textbf{65.1} $\pm$ 1.4 & 73.1 $\pm$ 0.3 & 77.2 $\pm$ 0.6 & 78.6 \\
% CDANN~\citep{li2018domain}& 97.1 $\pm$ 0.3 & \textbf{65.1} $\pm$ 1.2 & 70.7 $\pm$ 0.8 & 77.1 $\pm$ 1.5 & 77.5 \\
% \textbf{Ours} (SRA) & {97.1} $\pm$ 1.5 & {63.8} $\pm$ 2.3 & 70.5 $\pm$ 2.2 & 74.1 $\pm$ 1.8 & 76.4 \\
% \midrule


% %MTL~\citep{blanchard2021domain}& 97.8 $\pm$ 0.4 & 64.3 $\pm$ 0.3 & 71.5 $\pm$ 0.7 & 75.3 $\pm$ 1.7 & 77.2 \\
% %SagNet~\citep{nam2021reducing} & 97.9 $\pm$ 0.4 & 64.5 $\pm$ 0.5 & 71.4 $\pm$ 1.3 & {77.5} $\pm$ 0.5 & 77.8 \\
% %ARM~\citep{zhang2020adaptive}& 98.7 $\pm$ 0.2 & 63.6 $\pm$ 0.7 & 71.3 $\pm$ 1.2 & 76.7 $\pm$ 0.6 & 77.6 \\
% %VREx~\citep{krueger2021out}& 98.4 $\pm$ 0.3 & 64.4 $\pm$ 1.4 & {74.1} $\pm$ 0.4 & 76.2 $\pm$ 1.3 & 78.3 \\
% %RSC~\citep{huang2020self}& 97.9 $\pm$ 0.1 & 62.5 $\pm$ 0.7 & 72.3 $\pm$ 1.2 & 75.6 $\pm$ 0.8 & 77.1 \\

% SWAD~\cite{cha2021swad}& {98.8} $\pm$ 0.1 & 63.3 $\pm$ 0.3 & 75.3 $\pm$ 0.5 & 79.2 $\pm$ 0.6 & {79.1}\\
% SWAD + DANN& {99.2} $\pm$ 0.1 & 63.0 $\pm$ 0.8 & 75.3 $\pm$ 1.8 & 79.3 $\pm$ 0.5 & {79.2}\\
% SWAD + CDANN& {99.1} $\pm$ 0.1 & 63.3 $\pm$ 0.7 & 75.1 $\pm$ 0.7 & 80.1 $\pm$ 0.2 & {79.3}\\


% %DNA~\cite{chu2022dna} & {98.8} $\pm$ 0.1 & 63.6 $\pm$ 0.2 & {74.1} $\pm$ 0.1 & \textbf{79.5} $\pm$ 0.4 & 79.0\\


% %DiWA ($M=20$)~\citep{rame2022diverse} & 98.4 $\pm$ 0.1 & 63.4 $\pm$ 0.1 & 75.5 $\pm$ 0.3 & 78.9 $\pm$ 0.6 & 79.1\\
% %DiWA ($M=60$)~\citep{rame2022diverse} & 98.4 & 63.3 & 76.1 & 79.6 & 79.4\\


% \textbf{Ours} (SRA + SWAD) & {98.9} $\pm$ 0.2 & {63.7} $\pm$ 0.3 & 75.6 $\pm$ 0.4 & 79.4 $\pm$ 0.8 & 79.4 \\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{99.1} $\pm$ 0.0 & {63.9} $\pm$ 0.0 & \textbf{76.3} $\pm$ 0.0 & \textbf{79.9} $\pm$ 0.8 & \textbf{79.8} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:VLCS}
% \end{table}

% % \subsubsection{PACS}

% \begin{table}[h!]
% \caption{Classification Accuracy on \textbf{PACS} using ResNet50}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.7\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{A} & \textbf{C} & \textbf{P} & \textbf{S} & \textbf{Avg}  \\
% \midrule
% ERM~\citep{gulrajani2020search}& 84.7 $\pm$ 0.4 & {80.8} $\pm$ 0.6 & 97.2 $\pm$ 0.3 & 79.3 $\pm$ 1.0 & 85.5 \\
% %IRM~\citep{arjovsky2020%IRM}& 84.8 $\pm$ 1.3 & 76.4 $\pm$ 1.1 & 96.7 $\pm$ 0.6 & 76.1 $\pm$ 1.0 & 83.5 \\
% %GroupDRO~\citep{sagawa2019distributionally}& 83.5 $\pm$ 0.9 & 79.1 $\pm$ 0.6 & 96.7 $\pm$ 0.3 & 78.3 $\pm$ 2.0 & 84.4 \\
% %Mixup~\citep{wang2020heterogeneous}& 86.1 $\pm$ 0.5 & 78.9 $\pm$ 0.8 & \textbf{97.6} $\pm$ 0.1 & 75.8 $\pm$ 1.8 & 84.6 \\
% %MLDG~\citep{li2017learning}& 85.5 $\pm$ 1.4 & 80.1 $\pm$ 1.7 & 97.4 $\pm$ 0.3 & 76.6 $\pm$ 1.1 & 84.9 \\
% %%CORAL & \textbf{88.3} $\pm$ 0.2 & 80.0 $\pm$ 0.5 & 97.5 $\pm$ 0.3 & 78.8 $\pm$ 1.3 & 86.2 \\
% %MMD~\citep{li2018domain}& 86.1 $\pm$ 1.4 & 79.4 $\pm$ 0.9 & 96.6 $\pm$ 0.2 & 76.5 $\pm$ 0.5 & 84.6 \\
% DANN~\citep{ganin2016domain}& 86.4 $\pm$ 0.8 & 77.4 $\pm$ 0.8 & 97.3 $\pm$ 0.4 & 73.5 $\pm$ 2.3 & 83.6 \\
% CDANN~\citep{li2018domain}& 84.6 $\pm$ 1.8 & 75.5 $\pm$ 0.9 & 96.8 $\pm$ 0.3 & 73.5 $\pm$ 0.6 & 82.6 \\
% \textbf{Ours} (SRA) & {86.4} $\pm$ 0.2 & {82.0} $\pm$ 0.8 & 96.7 $\pm$ 1.1 & 80.2 $\pm$ 4.4 & 86.3 \\
% \midrule
% %MTL~\citep{blanchard2021domain}& 87.5 $\pm$ 0.8 & 77.1 $\pm$ 0.5 & 96.4 $\pm$ 0.8 & 77.3 $\pm$ 1.8 & 84.6 \\
% %SagNet~\citep{nam2021reducing} & 87.4 $\pm$ 1.0 & 80.7 $\pm$ 0.6 & 97.1 $\pm$ 0.1 & 80.0 $\pm$ 0.4 & {86.3} \\
% %ARM~\citep{zhang2020adaptive}& 86.8 $\pm$ 0.6 & 76.8 $\pm$ 0.5 & 97.4 $\pm$ 0.3 & 79.3 $\pm$ 1.2 & 85.1 \\
% %VREx~\citep{krueger2021out}& 86.0 $\pm$ 1.6 & 79.1 $\pm$ 0.6 & 96.9 $\pm$ 0.5 & 77.7 $\pm$ 1.7 & 84.9 \\
% %RSC~\citep{huang2020self}& 85.4 $\pm$ 0.8 & 79.7 $\pm$ 1.8 & {97.6} $\pm$ 0.3 & 78.2 $\pm$ 1.2 & 85.2 \\
% SWAD~\cite{cha2021swad}& 89.3 $\pm$ 0.2 & 83.4 $\pm$ 0.6 & 97.3 $\pm$ 0.3 & 82.5 $\pm$ 0.5 & 88.1\\
% SWAD + DANN& 90.7 $\pm$ 1.2 & 82.2 $\pm$ 0.4 & 97.3 $\pm$ 0.1 & 81.6 $\pm$ 0.4 & 87.9\\
% SWAD + CDANN & 90.5 $\pm$ 0.3 & 82.4 $\pm$ 1.0 & 97.6 $\pm$ 0.1 & 80.4 $\pm$ 0.3 & 87.7\\

% %DNA~\cite{chu2022dna} & 89.8 $\pm$ 0.2 & 83.4 $\pm$ 0.4 & {97.7} $\pm$ 0.1 & \textbf{82.6} $\pm$ 0.2 & {88.4}\\

% %DiWA ($M=20$)~\citep{rame2022diverse} & 90.1 $\pm$ 0.6 & 83.3 $\pm$ 0.6 & \textbf{98.2} $\pm$ 0.1 & 83.4 $\pm$ 0.4 & 88.8\\
% %DiWA ($M=60$)~\citep{rame2022diverse} & 90.5 & 83.7 & \textbf{98.2} & 83.8 & 89.0\\


% \textbf{Ours} (SRA + SWAD) & 90.5 $\pm$ 0.5 & 83.4 $\pm$ 0.2 & 97.8 $\pm$ 0.1 & 83.2 $\pm$ 0.2 & 88.7 \\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{91.2} $\pm$ 0.0 & \textbf{83.8} $\pm$ 0.0 & 97.8 $\pm$ 0.0 & \textbf{83.9} $\pm$ 0.0 & \textbf{89.2} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:PACS}
% \end{table}

% % \subsubsection{OfficeHome}
% \begin{table}[h!]
% \caption{Classification Accuracy on \textbf{OfficeHome} using ResNet50}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.7\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{A} & \textbf{C} & \textbf{P} & \textbf{R} & \textbf{Avg}  \\
% \midrule
% ERM~\citep{gulrajani2020search}& 61.3 $\pm$ 0.7 & 52.4 $\pm$ 0.3 & 75.8 $\pm$ 0.1 & 76.6 $\pm$ 0.3 & 66.5 \\
% %IRM~\citep{arjovsky2020%IRM}& 58.9 $\pm$ 2.3 & 52.2 $\pm$ 1.6 & 72.1 $\pm$ 2.9 & 74.0 $\pm$ 2.5 & 64.3 \\
% %GroupDRO~\citep{sagawa2019distributionally}& 60.4 $\pm$ 0.7 & 52.7 $\pm$ 1.0 & 75.0 $\pm$ 0.7 & 76.0 $\pm$ 0.7 & 66.0 \\
% %Mixup~\citep{wang2020heterogeneous}& 62.4 $\pm$ 0.8 & 54.8 $\pm$ 0.6 & \textbf{76.9} $\pm$ 0.3 & 78.3 $\pm$ 0.2 & 68.1 \\
% %MLDG~\citep{li2017learning}& 61.5 $\pm$ 0.9 & 53.2 $\pm$ 0.6 & 75.0 $\pm$ 1.2 & 77.5 $\pm$ 0.4 & 66.8 \\
% %%CORAL & \textbf{65.3} $\pm$ 0.4 & 54.4 $\pm$ 0.5 & {76.5} $\pm$ 0.1 & \textbf{78.4} $\pm$ 0.5 & \textbf{68.7} \\
% %MMD~\citep{li2018domain}& 60.4 $\pm$ 0.2 & 53.3 $\pm$ 0.3 & 74.3 $\pm$ 0.1 & 77.4 $\pm$ 0.6 & 66.3 \\
% DANN~\citep{ganin2016domain}& 59.9 $\pm$ 1.3 & 53.0 $\pm$ 0.3 & 73.6 $\pm$ 0.7 & 76.9 $\pm$ 0.5 & 65.9 \\
% CDANN~\citep{li2018domain}& 61.5 $\pm$ 1.4 & 50.4 $\pm$ 2.4 & 74.4 $\pm$ 0.9 & 76.6 $\pm$ 0.8 & 65.8 \\
% %MTL~\citep{blanchard2021domain}& 61.5 $\pm$ 0.7 & 52.4 $\pm$ 0.6 & 74.9 $\pm$ 0.4 & 76.8 $\pm$ 0.4 & 66.4 \\
% %SagNet~\citep{nam2021reducing} & 63.4 $\pm$ 0.2 & \textbf{54.8} $\pm$ 0.4 & 75.8 $\pm$ 0.4 & 78.3 $\pm$ 0.3 & 68.1 \\
% %ARM~\citep{zhang2020adaptive}& 58.9 $\pm$ 0.8 & 51.0 $\pm$ 0.5 & 74.1 $\pm$ 0.1 & 75.2 $\pm$ 0.3 & 64.8 \\
% %VREx~\citep{krueger2021out}& 60.7 $\pm$ 0.9 & 53.0 $\pm$ 0.9 & 75.3 $\pm$ 0.1 & 76.6 $\pm$ 0.5 & 66.4 \\
% %RSC~\citep{huang2020self}& 60.7 $\pm$ 1.4 & 51.4 $\pm$ 0.3 & 74.8 $\pm$ 1.1 & 75.1 $\pm$ 1.3 & 65.5 \\
% \textbf{Ours} (SRA) & {62.2} $\pm$ 1.4 & {52.3} $\pm$ 1.7 & 74.5 $\pm$ 0.8 & 76.6 $\pm$ 1.3 & 66.4 \\

% \midrule
% SWAD~\cite{cha2021swad}& 66.1 $\pm$ 0.4 & 57.7 $\pm$ 0.4 & 78.4 $\pm$0.1 & 80.2 $\pm$ 0.2& 70.6\\
% SWAD + DANN& 67.2 $\pm$ 0.1 & 56.2 $\pm$ 0.1 & 78.6 $\pm$0.2 & 80.0 $\pm$ 0.5& 70.5\\
% SWAD + CDANN& 66.8 $\pm$ 0.4 & 56.4 $\pm$ 0.8 & 78.4 $\pm$0.5 & 80.1 $\pm$ 0.2& 70.4\\

% %DNA~\cite{chu2022dna} & 67.7 $\pm$ 0.2 & {57.7} $\pm$ 0.3 & 78.9 $\pm$ 0.2 & 80.5 $\pm$ 0.2 & 71.2\\

% %DiWA ($M=20$)~\citep{rame2022diverse} & 67.3 $\pm$ 0.2 & 57.9 $\pm$ 0.2 & 79.0 $\pm$ 0.2 & 79.9 $\pm$ 0.1 & 71.0 \\
% %DiWA ($M=60$)~\citep{rame2022diverse} & 67.7 & 58.8 & 79.4 & 80.5 & 71.6\\


% \textbf{Ours} (SRA + SWAD) & {69.1} $\pm$ 0.6 & 58.4 $\pm$ 0.8 &  {79.5} $\pm$ 0.2 &  {81.4} $\pm$ 0.3 & {72.1}\\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{70.5} $\pm$ 0.0 & \textbf{59.5} $\pm$ 0.0 &  \textbf{80.4} $\pm$ 0.0 &  \textbf{82.1} $\pm$ 0.0 & \textbf{73.2}\\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:OfficeHome}
% \end{table}

% % \subsubsection{TerraIncognita}
% \begin{table}[h!]
% \caption{Classification Accuracy on \textbf{TerraIncognita} using ResNet50}
% \begin{centering}
% \resizebox{0.7\columnwidth}{!}{ %
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{L100} & \textbf{L38}  & \textbf{L43}  & \textbf{L46}  & \textbf{Avg}  \\
% \midrule
% ERM~\citep{gulrajani2020search}& 49.8 $\pm$ 4.4 & 42.1 $\pm$ 1.4 & 56.9 $\pm$ 1.8 & 35.7 $\pm$ 3.9 & 46.1 \\
% %IRM~\citep{arjovsky2020%IRM}& 54.6 $\pm$ 1.3 & 39.8 $\pm$ 1.9 & 56.2 $\pm$ 1.8 & 39.6 $\pm$ 0.8 & 47.6 \\
% %GroupDRO~\citep{sagawa2019distributionally}& 41.2 $\pm$ 0.7 & 38.6 $\pm$ 2.1 & 56.7 $\pm$ 0.9 & 36.4 $\pm$ 2.1 & 43.2 \\
% %Mixup~\citep{wang2020heterogeneous}& \textbf{59.6} $\pm$ 2.0 & 42.2 $\pm$ 1.4 & 55.9 $\pm$ 0.8 & 33.9 $\pm$ 1.4 & 47.9 \\
% %MLDG~\citep{li2017learning}& {54.2} $\pm$ 3.0 & \textbf{44.3} $\pm$ 1.1 & 55.6 $\pm$ 0.3 & 36.9 $\pm$ 2.2 & 47.7 \\
% %%CORAL & 51.6 $\pm$ 2.4 & {42.2} $\pm$ 1.0 & 57.0 $\pm$ 1.0 & 39.8 $\pm$ 2.9 & 47.6 \\
% %MMD~\citep{li2018domain}& 41.9 $\pm$ 3.0 & 34.8 $\pm$ 1.0 & 57.0 $\pm$ 1.9 & 35.2 $\pm$ 1.8 & 42.2 \\
% DANN~\citep{ganin2016domain}& 51.1 $\pm$ 3.5 & 40.6 $\pm$ 0.6 & {57.4} $\pm$ 0.5 & 37.7 $\pm$ 1.8 & 46.7 \\
% CDANN~\citep{li2018domain}& 47.0 $\pm$ 1.9 & 41.3 $\pm$ 4.8 & 54.9 $\pm$ 1.7 & 39.8 $\pm$ 2.3 & 45.8 \\
% \textbf{Ours} (SRA) & {52.9} $\pm$ 3.5 & {45.8} $\pm$ 5.1 & 57.2 $\pm$ 4.6 & 42.3 $\pm$ 1.1 & 49.5 \\

% %MTL~\citep{blanchard2021domain}& 49.3 $\pm$ 1.2 & 39.6 $\pm$ 6.3 & 55.6 $\pm$ 1.1 & 37.8 $\pm$ 0.8 & 45.6 \\
% %SagNet~\citep{nam2021reducing} & 53.0 $\pm$ 2.9 & 43.0 $\pm$ 2.5 & \textbf{57.9} $\pm$ 0.6 & {40.4} $\pm$ 1.3 & \textbf{48.6} \\
% %ARM~\citep{zhang2020adaptive}& 49.3 $\pm$ 0.7 & 38.3 $\pm$ 2.4 & 55.8 $\pm$ 0.8 & 38.7 $\pm$ 1.3 & 45.5 \\
% %VREx~\citep{krueger2021out}& 48.2 $\pm$ 4.3 & 41.7 $\pm$ 1.3 & 56.8 $\pm$ 0.8 & 38.7 $\pm$ 3.1 & 46.4 \\
% %RSC~\citep{huang2020self}& 50.2 $\pm$ 2.2 & 39.2 $\pm$ 1.4 & 56.3 $\pm$ 1.4 & 40.8 $\pm$ 0.6 & 46.6 \\

% %DiWA ($M=20$)~\citep{rame2022diverse} & 52.2 $\pm$ 1.8 & 46.2 $\pm$ 0.4 & 59.2 $\pm$ 0.2 & 37.8 $\pm$ 0.6 & 48.9\\
% %DiWA ($M=60$)~\citep{rame2022diverse} & 52.7 & 46.3 & 59.0 & 37.7 & 49.0\\
% \midrule
% SWAD~\cite{cha2021swad}& 55.4 $\pm$ 0.0 & 44.9 $\pm$ 1.1 & 59.7 $\pm$ 0.4 & 39.9 $\pm$ 0.2 & 50.0\\

% SWAD + DANN & 56.3 $\pm$ 2.6 & 44.9 $\pm$ 0.4 & 60.0 $\pm$ 0.7 & 41.4 $\pm$ 0.3 & 50.6\\

% SWAD + CDANN& 55.2 $\pm$ 2.2 & 45.3 $\pm$ 0.2 & 61.4 $\pm$ 0.7 & 40.9 $\pm$ 2.0 & 50.7\\
% %DNA~\cite{chu2022dna} & 56.8 $\pm$ 1.2 & 47.0 $\pm$ 0.9 & \textbf{61.0} $\pm$ 0.5 & \textbf{44.0} $\pm$ 1.0 & 52.2\\
% \textbf{Ours} (SRA + SWAD) & {56.2} $\pm$ 0.8 & {45.5} $\pm$ 2.6 & {60.4} $\pm$ 1.0& {44.4} $\pm$ 0.6 & {51.6} \\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{57.4} $\pm$ 0.0 & \textbf{45.3} $\pm$ 0.0 & \textbf{60.9} $\pm$ 0.0 & \textbf{45.2} $\pm$ 0.0 & \textbf{52.2} \\

% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:TerraIncognita}
% \end{table}

% % \subsubsection{DomainNet}
% \begin{table}[h!]
% \caption{Classification Accuracy on \textbf{DomainNet} using ResNet50}
% %\vspace{-0.5mm}
% \begin{centering}
% \resizebox{0.9\columnwidth}{!}{ %
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{clip} & \textbf{info} & \textbf{paint} & \textbf{quick} & \textbf{real} & \textbf{sketch} & \textbf{Avg}  \\
% \midrule
% ERM~\citep{gulrajani2020search}& 58.1 $\pm$ 0.3 & 18.8 $\pm$ 0.3 & 46.7 $\pm$ 0.3 & 12.2 $\pm$ 0.4 & 59.6 $\pm$ 0.1 & 49.8 $\pm$ 0.4 & 40.9 \\
% %IRM~\citep{arjovsky2020%IRM}& 48.5 $\pm$ 2.8 & 15.0 $\pm$ 1.5 & 38.3 $\pm$ 4.3 & 10.9 $\pm$ 0.5 & 48.2 $\pm$ 5.2 & 42.3 $\pm$ 3.1 & 33.9 \\
% % GroupDRO~\citep{sagawa2019distributionally}& 47.2 $\pm$ 0.5 & 17.5 $\pm$ 0.4 & 33.8 $\pm$ 0.5 & 9.3 $\pm$ 0.3 & 51.6 $\pm$ 0.4 & 40.1 $\pm$ 0.6 & 33.3 \\
% % Mixup~\citep{wang2020heterogeneous}& 55.7 $\pm$ 0.3 & 18.5 $\pm$ 0.5 & 44.3 $\pm$ 0.5 & 12.5 $\pm$ 0.4 & 55.8 $\pm$ 0.3 & 48.2 $\pm$ 0.5 & 39.2 \\
% %CORAL & 59.2 $\pm$ 0.1 & 19.7 $\pm$ 0.2 & 46.6 $\pm$ 0.3 & {13.4} $\pm$ 0.4 & 59.8 $\pm$ 0.2 & 50.1 $\pm$ 0.6 & 41.5 \\
% %MMD~\citep{li2018domain}& 32.1 $\pm$ 13.3 & 11.0 $\pm$ 4.6 & 26.8 $\pm$ 11.3 & 8.7 $\pm$ 2.1 & 32.7 $\pm$ 13.8 & 28.9 $\pm$ 11.9 & 23.4 \\
% DANN~\citep{ganin2016domain}& 53.1 $\pm$ 0.2 & 18.3 $\pm$ 0.1 & 44.2 $\pm$ 0.7 & 11.8 $\pm$ 0.1 & 55.5 $\pm$ 0.4 & 46.8 $\pm$ 0.6 & 38.3 \\
% CDANN~\citep{li2018domain}& 54.6 $\pm$ 0.4 & 17.3 $\pm$ 0.1 & 43.7 $\pm$ 0.9 & 12.1 $\pm$ 0.7 & 56.2 $\pm$ 0.4 & 45.9 $\pm$ 0.5 & 38.3 \\
% \textbf{Ours} (SRA) & {64.2} $\pm$ 0.3 & {21.6} $\pm$ 0.9 & 50.8 $\pm$ 1.1 & 13.3 $\pm$ 0.8 & 64.4 $\pm$ 0.1 & 53.0 $\pm$ 0.4 &  44.5\\
% \midrule
% % MTL~\citep{blanchard2021domain}& 57.9 $\pm$ 0.5 & 18.5 $\pm$ 0.4 & 46.0 $\pm$ 0.1 & 12.5 $\pm$ 0.1 & 59.5 $\pm$ 0.3 & 49.2 $\pm$ 0.1 & 40.6 \\
% % SagNet~\citep{nam2021reducing} & 57.7 $\pm$ 0.3 & 19.0 $\pm$ 0.2 & 45.3 $\pm$ 0.3 & 12.7 $\pm$ 0.5 & 58.1 $\pm$ 0.5 & 48.8 $\pm$ 0.2 & 40.3 \\
% % ARM~\citep{zhang2020adaptive}& 49.7 $\pm$ 0.3 & 16.3 $\pm$ 0.5 & 40.9 $\pm$ 1.1 & 9.4 $\pm$ 0.1 & 53.4 $\pm$ 0.4 & 43.5 $\pm$ 0.4 & 35.5 \\
% %VREx~\citep{krueger2021out}& 47.3 $\pm$ 3.5 & 16.0 $\pm$ 1.5 & 35.8 $\pm$ 4.6 & 10.9 $\pm$ 0.3 & 49.6 $\pm$ 4.9 & 42.0 $\pm$ 3.0 & 33.6 \\
% %RSC~\citep{huang2020self}& 55.0 $\pm$ 1.2 & 18.3 $\pm$ 0.5 & 44.4 $\pm$ 0.6 & 12.2 $\pm$ 0.2 & 55.7 $\pm$ 0.7 & 47.8 $\pm$ 0.9 & 38.9 \\
% SWAD~\cite{cha2021swad}& 66.0 $\pm$ 0.1 & 22.4 $\pm$ 0.3 & 53.5 $\pm$ 0.1 & 16.1 $\pm$ 0.2 & 65.8 $\pm$ 0.4 & {55.5} $\pm$ 0.3 & 46.5\\
% SWAD + DANN& 64.3 $\pm$ 0.1 & 21.9 $\pm$ 0.6 & 52.6 $\pm$ 0.2 & 15.5 $\pm$ 0.2 & 65.3 $\pm$ 0.1 & {54.5} $\pm$ 0.1 & 45.7\\
% SWAD + CDANN& 64.3 $\pm$ 0.2 & 21.9 $\pm$ 0.4 & 52.5 $\pm$ 0.0 & 15.6 $\pm$ 0.0 & 65.3 $\pm$ 0.1 & {54.4} $\pm$ 0.2 & 45.7\\

% %DNA~\cite{chu2022dna} & {66.1} $\pm$ 0.2 & \textbf{23.0} $\pm$ 0.1 & \textbf{54.6} $\pm$ 0.1 & \textbf{16.7} $\pm$ 0.1 & {65.8} $\pm$ 0.2 & \textbf{56.8} $\pm$ 0.1 & \textbf{47.2}\\
% %DiWA ($M=20$)~\citep{rame2022diverse} & 63.4 $\pm$ 0.2 & 23.1 $\pm$ 0.1 & 53.9 $\pm$ 0.2 & 15.4 $\pm$ 0.2 & 65.5 $\pm$ 0.2 & 55.1 $\pm$ 0.2 & 46.1\\
% %DiWA ($M=60$)~\citep{rame2022diverse} & 63.5 & 23.3 & 54.3 & 15.6 & 65.7 & 55.3 & 46.3\\


% \textbf{Ours} (SRA + SWAD) & {67.4} $\pm$ 0.1 & {23.5} $\pm$ 0.2 & {55.0} $\pm$ 0.1 & {15.9} $\pm$ 0.2 &  {67.2} $\pm$ 0.2 & {56.6} $\pm$ 0.1 & {47.6} \\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{68.7} $\pm$ 0.0 & \textbf{24.0} $\pm$ 0.2 & \textbf{56.3} $\pm$ 0.0 & \textbf{16.7} $\pm$ 0.0 &  \textbf{68.5} $\pm$ 0.0 & \textbf{57.8} $\pm$ 0.0 & \textbf{48.7} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:DomainNet}
% \end{table}


