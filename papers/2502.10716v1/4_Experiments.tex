% \begin{table*}[h!]
% \caption{Classification accuracy (\%) for all algorithms and datasets summarization.
% %The \textbf{best} and \underline{second best} results are highlighted in \textbf{bold} and \underline{underline}.
% }
% \begin{centering}
% \resizebox{0.97\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraIncognita}  & \textbf{DomainNet} & \textbf{Avg} \\
% \midrule
% ERM  & 77.5 $\pm$ 0.4 & 85.5 $\pm$ 0.2 & 66.5 $\pm$ 0.3 & 46.1 $\pm$ 1.8 & 40.9 $\pm$ 0.1 & 63.3 \\
% IRM & 78.5 $\pm$ 0.5 & 83.5 $\pm$ 0.8 & 64.3 $\pm$ 2.2 & 47.6 $\pm$ 0.8 & 33.9 $\pm$ 2.8 & 61.6 \\
% MMD & 77.5 $\pm$ 0.9 & 84.6 $\pm$ 0.5 & 66.3 $\pm$ 0.1 & 42.2 $\pm$ 1.6 & 23.4 $\pm$ 9.5 & 58.8 \\
% DANN & 78.6 $\pm$ 0.4 & 83.6 $\pm$ 0.4 & 65.9 $\pm$ 0.6 & 46.7 $\pm$ 0.5 & 38.3 $\pm$ 0.1 & 62.6 \\
% CDANN  & 77.5 $\pm$ 0.1 & 82.6 $\pm$ 0.9 & 65.8 $\pm$ 1.3 & 45.8 $\pm$ 1.6 & 38.3 $\pm$ 0.3 & 62.0 \\
% VREx & 78.3 $\pm$ 0.2 & 84.9 $\pm$ 0.6 & 66.4 $\pm$ 0.6 & 46.4 $\pm$ 0.6 & 33.6 $\pm$ 2.9 & 61.9 \\
% % GroupDRO~\citep{sagawa2019distributionally} & 76.7 $\pm$ 0.6 & 84.4 $\pm$ 0.8 & 66.0 $\pm$ 0.7 & 43.2 $\pm$ 1.1 & 33.3 $\pm$ 0.2 & 60.7 \\
% Mixup  & 77.4 $\pm$ 0.6 & 84.6 $\pm$ 0.6 & 68.1 $\pm$ 0.3 & 47.9 $\pm$ 0.8 & 39.2 $\pm$ 0.1 & 63.4 \\
% % MLDG~\citep{li2017learning} & 77.2 $\pm$ 0.4 & 84.9 $\pm$ 1.0 & 66.8 $\pm$ 0.6 & 47.7 $\pm$ 0.9 & 41.2 $\pm$ 0.1 & 63.6 \\
% %CORAL~\citep{sun2016deep} & \underline{78.8} $\pm$ 0.6 & 86.2 $\pm$ 0.3 & 68.7 $\pm$ 0.3 & 47.6 $\pm$ 1.0 & \underline{41.5} $\pm$ 0.1 & \underline{64.5} \\
% %MTL~\citep{blanchard2021domain} & 77.2 $\pm$ 0.4 & 84.6 $\pm$ 0.5 & 66.4 $\pm$ 0.5 & 45.6 $\pm$ 1.2 & 40.6 $\pm$ 0.1 & 62.9 \\
% %SagNet~\citep{nam2021reducing} & 77.8 $\pm$ 0.5 & \underline{86.3} $\pm$ 0.2 & 68.1 $\pm$ 0.1 & 48.6 $\pm$ 1.0 & 40.3 $\pm$ 0.1 & 64.2 \\
% %ARM~\citep{zhang2021adaptive} & 77.6 $\pm$ 0.3 & 85.1 $\pm$ 0.4 & 64.8 $\pm$ 0.3 & 45.5 $\pm$ 0.3 & 35.5 $\pm$ 0.2 & 61.7 \\
% %RSC~\citep{huang2020self}  & 77.1 $\pm$ 0.5 & 85.2 $\pm$ 0.9 & 65.5 $\pm$ 0.9 & 46.6 $\pm$ 1.0 & 38.9 $\pm$ 0.5 & 62.7 \\
% mDSDI & 79.0 $\pm$ 0.3 & 86.2 $\pm$ 0.2 & 69.2 $\pm$ 0.4 & 48.1 $\pm$ 1.4 & 42.8 $\pm$ 0.1 & 65.1\\
% SWAD & 79.1 $\pm$ 0.4 & 88.1 $\pm$ 0.4 & 70.6 $\pm$ 0.3 & 50.0 $\pm$ 0.4  &46.5 $\pm$ 0.2 & 66.9\\
% DNA & 79.0 $\pm$ 0.1 & 88.4 $\pm$ 0.1 & 71.2 $\pm$ 0.1 & 52.2 $\pm$ 0.4 & 47.2 $\pm$ 0.1 & 67.6\\
% %PCL~\cite{yao2022pcl} & - & 88.7 $\pm$ 0.1 & 71.6 $\pm$ 0.1 & 52.1 $\pm$ 0.4 & 47.7 $\pm$ 0.1 & -\\
% BAIR (ours) & \textbf{79.3} $\pm$ 0.2 & \textbf{88.8} $\pm$ 0.3 &  \textbf{71.3} $\pm$ 0.4 &  \textbf{52.5} $\pm$ 1.2 & 46.9 $\pm$ 0.0  & \textbf{67.7} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:Averages}
% \end{table*}

% \begin{table}[h!]
% \caption{Classification accuracy (\%) across datasets.
% }
% \vspace{-2mm}
% \begin{centering}
% \resizebox{0.45\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraIncognita}  & \textbf{DomainNet} & \textbf{Avg} \\
% \toprule
% ERM~\citep{gulrajani2020search}  & 77.5 $\pm$ 0.4 & 85.5 $\pm$ 0.2 & 66.5 $\pm$ 0.3 & 46.1 $\pm$ 1.8 & 40.9 $\pm$ 0.1 & 63.3 \\
% DANN~\citep{ganin2016domain} & 78.6 $\pm$ 0.4 & 83.6 $\pm$ 0.4 & 65.9 $\pm$ 0.6 & 46.7 $\pm$ 0.5 & 38.3 $\pm$ 0.1 & 62.6 \\
% CDANN~\citep{li2018domain}  & 77.5 $\pm$ 0.1 & 82.6 $\pm$ 0.9 & 65.8 $\pm$ 1.3 & 45.8 $\pm$ 1.6 & 38.3 $\pm$ 0.3 & 62.0 \\
% \textbf{Ours} (SRA)  & 76.4 $\pm$ 0.7 & 86.3 $\pm$ 1.1 & 66.4 $\pm$ 0.7 & 49.5 $\pm$ 1.0 & 44.5 $\pm$ 0.3 & 64.6 \\
% \midrule
% SWAD~\citep{cha2021swad} & 79.1 $\pm$ 0.4 & 88.1 $\pm$ 0.4 & 70.6 $\pm$ 0.3 & 50.0 $\pm$ 0.4  &46.5 $\pm$ 0.2 & 66.9\\
% SWAD + DANN & 79.2 $\pm$ 0.0 & 87.9 $\pm$ 0.5 & 70.5 $\pm$ 0.1 & 50.6 $\pm$ 0.6  &45.7 $\pm$ 0.1 & 66.8\\
% SWAD + CDANN & 79.3 $\pm$ 0.2 & 87.7 $\pm$ 0.3 & 70.4 $\pm$ 0.1 & 50.7 $\pm$ 0.1  &45.7 $\pm$ 0.2 & 66.8\\
% \textbf{Ours} (SRA + SWAD) & \underline{79.4} $\pm$ 0.4 & \underline{88.7} $\pm$ 0.2 &  \underline{72.1} $\pm$ 0.5 &  \underline{51.6} $\pm$ 1.2 & \underline{47.6} $\pm$ 0.1  & \underline{67.9} \\
% \midrule
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{79.8} $\pm$ 0.0 & \textbf{89.2} $\pm$ 0.0 &  \textbf{73.2} $\pm$ 0.0 &  \textbf{52.2} $\pm$ 0.0 & \textbf{48.7} $\pm$ 0.6  & \textbf{68.6} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:Averages}
% \end{table}

\subsection{Experimental results}\label{sec:main_exp}
\begin{table}[h!]
\caption{Classification accuracy (\%) across datasets.
}
\begin{centering}
\resizebox{1.0\columnwidth}{!}{ %
\begin{tabular}{lcccccc}
\toprule
\textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraInc}  & \textbf{DomainNet} & \textbf{Avg} \\
\toprule
ERM~\citep{gulrajani2020search}  & 73.5  & 85.5  & 66.5  & 46.1  & 40.9  & 63.3 \\
IRM~\citep{arjovsky2020irm} & 78.5 & 83.5 & 64.3 & 47.6 & 33.9 & 61.6 \\
VREx~\citep{krueger2021out} & 78.3 & 84.9 & 66.4 & 46.4 & 33.6 & 61.9 \\
%IIB~\citep{li2022invariant} & 77.5 & 84.4 & 67.2 & 47.9 & 45.3 & 64.5  \\
IB-IRM~\citep{ahuja2021invariance} & 77.7 & 85.8 & 70,9  &43.4  & 35.1 & 62.6  \\
DANN~\citep{ganin2016domain} & 78.6  & 83.6  & 65.9  & 46.7  & 38.3  & 62.6 \\
CDANN~\citep{li2018domain}  & 77.5  & 82.6  & 65.8  & 45.8  & 38.3  & 62.0 \\
\textbf{Ours} (SRA)  & 76.4  & 86.3  & 66.4  & 49.5  & 44.5  & 64.6 \\
\midrule
SWAD~\citep{cha2021swad} & 79.1  & 88.1  & 70.6  & 50.0   &46.5  & 66.9\\
SWAD + IRM & 78.8 & 88.1 & 70.4 & 49.6 & 39.9 & 65.4\\
SWAD + VRE & 78.1 & 85.4 & 69.9 & 50.0 & 40.0 &64.9\\
%SWAD + IIB~\citep{li2022invariant} & 77.5 & 88.4 & 70.7 & 50.9 & 47,5 & 67.0 \\
SWAD + IB-IRM~ & 78.8 & 88.1 & 70.9 & 49.3 & 38.4  & 65.1 \\
SWAD + DANN & 79.2  & 87.9  & 70.5  & 50.6   &45.7  & 66.8\\
SWAD + CDANN & 79.3  & 87.7  & 70.4  & 50.7   &45.7  & 66.8\\
\textbf{Ours} (SRA + SWAD) & \underline{79.4}  & \underline{88.7}  &  \underline{72.1}  &  \underline{51.6}  & \underline{47.6}   & \underline{67.9} \\
\midrule
SRA + SWAD + Ensemble & \textbf{79.8}  & \textbf{89.2}  &  \textbf{73.2}  &  \textbf{52.2}  & \textbf{48.7}   & \textbf{68.6} \\
\bottomrule
\end{tabular}}
\par\end{centering}
\label{tab:Averages}
\end{table}
We present empirical evidence from experiments conducted on 5 datasets from the DomainBed benchmark (full experimental results and detailed settings are provided in Appendix~\ref{apd:settings}) to validate our theoretical analyses:

\textit{(i) Conventional DG algorithms fail because they inadvertently violate necessary conditions:} As shown in Table~\ref{tab:Averages}, none of the baseline methods consistently surpass ERM overall, regardless of whether SWAD is applied. In contrast, our proposed approach SRA consistently outperforms all baseline methods across all datasets.  

Additionally, we highlight that SRA is most similar to DANN and CDANN. Like these methods, SRA utilizes $\mathcal{H}$-divergence for alignment; however, the key distinction lies in the alignment strategy:  DANN aligns the entire domain representation,  CDANN aligns class-conditional representations, while SRA employs subspace-conditional alignment.
As discussed in Theorem~\ref{theorem:single_tradeoff}, DANN and CDANN potentially violate necessary conditions, whereas SRA does not (Theorem~\ref{theorem:multi_bound}). The superior performance of SRA compared to DANN and CDANN further confirms this.


\textit{(ii) Ensemble-based approaches promote the "Invariance-Preserving Representation Function" Condition, leading to improve generalization:} 
The results of the baselines with SWAD in Table~\ref{tab:Averages} highlight the crucial benefits of using ensembles to encourage Necessary-Condition-2 for improved generalization. To further demonstrate the advantages of an ensemble strategy, we average the predictions of models trained with different random seeds (SRA + SWAD + Ensemble), leading to a noticeable performance boost. 


\begin{table}[h!]
\vspace{-5mm}
\caption{Classification accuracy on PACS using ResNet-50 with varying information bottleneck coefficient $\lambda$.}
\begin{centering}
\resizebox{0.8\columnwidth}{!}{ %
\begin{tabular}{lccccc}
\toprule
$\lambda$  & \textbf{Art\_painting} & \textbf{Cartoon} & \textbf{Photo} & \textbf{Sketch} & \textbf{Average}  \\
\midrule
1 & 89.8 & 82.4 & 97.7 & 82.6 & 88.1\\
10 & 87.9 & 82.2 & 97.2 & 78.5 & 86.5\\
100 & 85.6 & 75.9 & 97.5 & 44.1 & 75.8\\
\bottomrule
\end{tabular}}
\par\end{centering}
\label{tab:infor}
\vspace{-2mm}
\end{table}
\textit{(iii) The Information-Bottleneck regularization potentially violating Necessary-Condition-2, being ineffective for generalization:}  As shown in Table~\ref{tab:infor}, the results indicate that the information bottleneck strategy performs poorly in practical scenarios, particularly in the IB-IRM baseline. Increasing the information bottleneck regularization strength can lead to significant performance degradation, especially when the source domains (e.g., Photo, Art, Cartoon) contain rich label information while the target domain ("Sketch") has limited label information (e.g., $\lambda=100$, the default value in IB-IRM). In contrast, when the target domain is "Photo" or "Art," the model achieves relatively better performance.
\vspace{-1mm}


% It can be seen that Although SRA does not violate this condition, it also does not actively promote it.


% First of all

% , that is enforcing good sufficient conditions (SRA) while encouraging necessary conditions (Ensemble) can improve generalization. For the ensemble component, we utilize the weight averaging strategy from the SWAD method \citep{cha2021swad} for efficient inference. Importantly, our analysis highlights that using ensembles for targeting the sufficient representation constraint can provide crucial benefits for generalization. This strategy should therefore not be viewed as merely post-processing or an orthogonal technique in DG setting. 

% Table \ref{tab:Averages} compares our method against two popular representation alignment strategies: DANN and CDANN, on $5$ datasets from DomainBed benchmark \cite{gulrajani2020search}. Note that we also use $\mathcal{H}$-divergence for alignment in DANN and CDANN. The only difference is that DANN aligns the whole domain representation, CDANN aligns class-conditional representation, while SRA employs subspace-conditional alignment. First, it is seen that both DANN and CDANN cannot surpass ERM overall with and without SWAD. This supports our analysis in Section~\ref{sec:discussion_DG} that these methods violate the necessary condition. In contrast, our method consistently achieves better performance than the baseline approaches on all datasets. We further demonstrate the benefit of an ensemble approach by averaging the predictions of models trained with different random seeds (SRA + SWAD + Ensemble), resulting in a performance boost.  Full experimental results and detailed settings are provided in Appendix \ref{apd:settings}.



% \begin{table*}[h!]
% \caption{Classification accuracy (\%) for all algorithms across datasets.
% }
% \vspace{-2mm}
% \begin{centering}
% \resizebox{0.65\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm}  & \textbf{VLCS} & \textbf{PACS} & \textbf{OfficeHome} & \textbf{TerraIncognita}  & \textbf{DomainNet} & \textbf{Avg} \\
% \toprule
% ERM~\citep{gulrajani2020search}  & 77.5 $\pm$ 0.4 & 85.5 $\pm$ 0.2 & 66.5 $\pm$ 0.3 & 46.1 $\pm$ 1.8 & 40.9 $\pm$ 0.1 & 63.3 \\
% IRM~\citep{arjovsky2020irm} & 78.5 $\pm$ 0.5 & 83.5 $\pm$ 0.8 & 64.3 $\pm$ 2.2 & 47.6 $\pm$ 0.8 & 33.9 $\pm$ 2.8 & 61.6 \\
% MMD~\citep{li2018domain} & 77.5 $\pm$ 0.9 & 84.6 $\pm$ 0.5 & 66.3 $\pm$ 0.1 & 42.2 $\pm$ 1.6 & 23.4 $\pm$ 9.5 & 58.8 \\
% DANN~\citep{ganin2016domain} & 78.6 $\pm$ 0.4 & 83.6 $\pm$ 0.4 & 65.9 $\pm$ 0.6 & 46.7 $\pm$ 0.5 & 38.3 $\pm$ 0.1 & 62.6 \\
% CDANN~\citep{li2018domain}  & 77.5 $\pm$ 0.1 & 82.6 $\pm$ 0.9 & 65.8 $\pm$ 1.3 & 45.8 $\pm$ 1.6 & 38.3 $\pm$ 0.3 & 62.0 \\
% VREx~\citep{krueger2021out} & 78.3 $\pm$ 0.2 & 84.9 $\pm$ 0.6 & 66.4 $\pm$ 0.6 & 46.4 $\pm$ 0.6 & 33.6 $\pm$ 2.9 & 61.9 \\
% % GroupDRO~\citep{sagawa2019distributionally} & 76.7 $\pm$ 0.6 & 84.4 $\pm$ 0.8 & 66.0 $\pm$ 0.7 & 43.2 $\pm$ 1.1 & 33.3 $\pm$ 0.2 & 60.7 \\
% %Mixup~\citep{wang2020heterogeneous}  & 77.4 $\pm$ 0.6 & 84.6 $\pm$ 0.6 & 68.1 $\pm$ 0.3 & 47.9 $\pm$ 0.8 & 39.2 $\pm$ 0.1 & 63.4 \\
% % MLDG~\citep{li2017learning} & 77.2 $\pm$ 0.4 & 84.9 $\pm$ 1.0 & 66.8 $\pm$ 0.6 & 47.7 $\pm$ 0.9 & 41.2 $\pm$ 0.1 & 63.6 \\
% CORAL~\citep{sun2016deep} & 78.8 $\pm$ 0.6 & 86.2 $\pm$ 0.3 & 68.7 $\pm$ 0.3 & 47.6 $\pm$ 1.0 & 41.5 $\pm$ 0.1 & 64.5 \\
% %MTL~\citep{blanchard2021domain} & 77.2 $\pm$ 0.4 & 84.6 $\pm$ 0.5 & 66.4 $\pm$ 0.5 & 45.6 $\pm$ 1.2 & 40.6 $\pm$ 0.1 & 62.9 \\
% %SagNet~\citep{nam2021reducing} & 77.8 $\pm$ 0.5 & \underline{86.3} $\pm$ 0.2 & 68.1 $\pm$ 0.1 & 48.6 $\pm$ 1.0 & 40.3 $\pm$ 0.1 & 64.2 \\
% %ARM~\citep{zhang2021adaptive} & 77.6 $\pm$ 0.3 & 85.1 $\pm$ 0.4 & 64.8 $\pm$ 0.3 & 45.5 $\pm$ 0.3 & 35.5 $\pm$ 0.2 & 61.7 \\
% %RSC~\citep{huang2020self}  & 77.1 $\pm$ 0.5 & 85.2 $\pm$ 0.9 & 65.5 $\pm$ 0.9 & 46.6 $\pm$ 1.0 & 38.9 $\pm$ 0.5 & 62.7 \\
% %mDSDI~\citep{bui2021exploiting} & 79.0 $\pm$ 0.3 & 86.2 $\pm$ 0.2 & 69.2 $\pm$ 0.4 & 48.1 $\pm$ 1.4 & 42.8 $\pm$ 0.1 & 65.1\\
% SWAD~\citep{cha2021swad} & 79.1 $\pm$ 0.4 & 88.1 $\pm$ 0.4 & 70.6 $\pm$ 0.3 & 50.0 $\pm$ 0.4  &46.5 $\pm$ 0.2 & 66.9\\
% %DNA~\cite{chu2022dna} & 79.0 $\pm$ 0.1 & 88.4 $\pm$ 0.1 & 71.2 $\pm$ 0.1 & 52.2 $\pm$ 0.4 & \textbf{47.2} $\pm$ 0.1 & 67.6\\
% %PCL~\cite{yao2022pcl} & - & 88.7 $\pm$ 0.1 & 71.6 $\pm$ 0.1 & 52.1 $\pm$ 0.4 & 47.7 $\pm$ 0.1 & -\\

% DiWA ($M=20$)~\citep{rame2022diverse} & 79.1 $\pm$ 0.2 & 88.8 $\pm$ 0.4 & 71.0 $\pm$ 0.1 & 48.9 $\pm$ 0.5  & 46.1 $\pm$ 0.1 & 66.8\\
% DiWA ($M=60$)~\citep{rame2022diverse} & 79.4 $\pm$ 0.4 & 89.0 $\pm$ 0.0 & 71.6 $\pm$ 0.3 & 49.0 $\pm$ 0.4  &46.3 $\pm$ 0.2 & 67.1\\
% \textbf{Ours} (SRA + SWAD) & \textbf{79.4} $\pm$ 0.4 & 88.7 $\pm$ 0.2 &  \textbf{72.1} $\pm$ 0.5 &  \textbf{51.6} $\pm$ 1.2 & \textbf{47.6} $\pm$ 0.1  & \textbf{67.9} \\
% \textbf{Ours} (SRA + SWAD + Ensemble) & \textbf{79.8} $\pm$ 0.4 & \textbf{89.2} $\pm$ 0.0 &  \textbf{73.2} $\pm$ 0.0 &  \textbf{52.2} $\pm$ 0.0 & \textbf{48.7} $\pm$ 0.6  & \textbf{68.6} \\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:Averages}
% \end{table*}


% \textbf{Sufficient Representation Condition.}\label{abl:sufficient}
% A pivotal discovery in our study is the critical role of sufficient representation, which led us to recommend enhancing sufficient representation by maximizing the mutual information $\mathcal{I}(X, g(X))$. To validate this condition, we designed experiments that incorporate the mutual information maximization term into various baselines, evaluating its impact. Specifically, we utilized the PACS dataset with a ResNet50 architecture, in conjunction with the SWAD technique. \Vy{must rewrite it a bit due to the max I term}

% \begin{table}[h!]
% \caption{Classification Accuracy on PACS using ResNet50}

% \begin{centering}
% \resizebox{0.8\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm} & \textbf{ERM}  & \textbf{IRM}  & \textbf{DANN} & \textbf{CDANN} & \textbf{IIB}  & \textbf{BAIR}  \\
% \midrule
%  \multicolumn{7}{c}{ResNet50 model pretrained on ImageNet-1k }\\
%  \midrule
%  & 88.1 & 88.2 & 87.4 & 87.8 & 87.8 & 88.2 \\
% \midrule
% $\max \mathcal{I}(X,G(X))$ & 88.2 & 88.5 & 88.6  &  88.1  &- & 88.7\\
% \midrule
%  \multicolumn{7}{c}{ResNet50 model without pretraining on ImageNet-1k}  \\
% \midrule
%  & 44.9 & 39.67 & 42.8 & 42.1 & 33.9 & 37.7\\
% \midrule    
% $\max \mathcal{I}(X,G(X))$ & 46.2 & 40.4 & 44.1 & 45.1 & - & 46.6\\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:pretrained}
% \end{table}
% Our investigation also extended to evaluating the ResNet50 architecture in \textit{scenarios both with and without ImageNet pretraining}. This aspect is crucial due to the significant overlap of labels between ImageNet and datasets used in DG benchmarks. We hypothesize that models pretrained on ImageNet might already possess information relevant to the target domains, which could influence the strategies employed by DG methods, especially in reducing the necessity to optimize for sufficient representation. According to our results, integrating a term to maximize $\mathcal{I}(X, g(X))$ enhances the performance of all baselines. As highlighted in Table \ref{tab:pretrained}, this significantly boosts performance in backbones without pretraining, yet the approach shows limited improvements in pretrained models. In contrast, the IIB \citep{li2022invariant} baseline, which aims at minimizing $\mathcal{I}(X, g(X))$, performs poorly with non-pretrained models, highlighting the importance of sufficient representation function constraint.



% \textbf{Sufficient Representation Condition.}\label{abl:sufficient}
% A pivotal discovery in our study is the critical role of sufficient representation, which led us to recommend enhancing sufficient representation by maximizing the mutual information $\mathcal{I}(X, g(X))$. To validate this condition, we designed experiments that incorporate the mutual information maximization term into various baselines, evaluating its impact. Specifically, we utilized the PACS dataset with a ResNet50 architecture, in conjunction with the SWAD technique.

% \begin{table}[h!]
% \caption{Classification Accuracy on PACS using ResNet50}

% \begin{centering}
% \resizebox{0.7\width}{!}{ %
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Algorithm} & \textbf{ERM}  & \textbf{IRM}  & \textbf{DANN} & \textbf{CDANN} & \textbf{IIB}  & \textbf{BAIR}  \\
% \midrule
%  \multicolumn{7}{c}{ResNet50 model pretrained on ImageNet-1k }\\
%  \midrule
%  & 88.1 & 88.2 & 87.4 & 87.8 & 87.8 & 88.2 \\
% \midrule
% $\max \mathcal{I}(X,G(X))$ & 88.2 & 88.5 & 88.6  &  88.1  &- & 88.7\\
% \midrule
%  \multicolumn{7}{c}{ResNet50 model without pretraining on ImageNet-1k}  \\
% \midrule
%  & 44.9 & 39.67 & 42.8 & 42.1 & 33.9 & 37.7\\
% \midrule    
% $\max \mathcal{I}(X,G(X))$ & 46.2 & 40.4 & 44.1 & 45.1 & - & 46.6\\
% \bottomrule
% \end{tabular}}
% \par\end{centering}
% \label{tab:pretrained}
% \end{table}
% Our investigation also extended to evaluating the ResNet50 architecture in \textit{scenarios both with and without ImageNet pretraining}. This aspect is crucial due to the significant overlap of labels between ImageNet and datasets used in DG benchmarks. We hypothesize that models pretrained on ImageNet might already possess information relevant to the target domains, which could influence the strategies employed by DG methods, especially in reducing the necessity to optimize for sufficient representation. According to our results, integrating a term to maximize $\mathcal{I}(X, g(X))$ enhances the performance of all baselines. As highlighted in Table \ref{tab:pretrained}, this significantly boosts performance in backbones without pretraining, yet the approach shows limited improvements in pretrained models. In contrast, the IIB \cite{li2022invariant} baseline, which aims at minimizing $\mathcal{I}(X, g(X))$, performs poorly with non-pretrained models, highlighting the importance of sufficient representation function constraint.