\section{Preliminaries}
% \subsection{Notation}
We first introduce the notations and basic concepts in the paper. We use calligraphic letters (i.e., $\mathcal{X}$) for spaces, upper case letters (i.e. $X$) for random variables, lower case letters (i.e. $x$) for their values, $\mathbb{P}$ for probability distributions and $\text{supp}(\cdot)$ specifies the support set of a distribution.


\subsection{Revisiting Domain Generalization setting}

\longvt{missing the introduction $\mathbb{P}^e$?}

We consider a standard domain generalization setting with a potentially high-dimensional variable $X$ (e.g., an image), a label variable $Y$ and a discrete environment (or domain)
variable $E$ in the sample spaces $\mathcal{X}, \mathcal{Y}$, and $\mathcal{E}$, respectively. Specifically, we focus on a multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$. Denote $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
be a hypothesis predicting a $C$-tuple $f\left(x\right)=\left[f\left(x\right)[i]\right]_{i=1}^{C}$,
whose element $f\left(x\right)[i]=p\left(y=i\mid x\right)$ 
is the probability to assign a data sample $x\sim\mathbb{P}$
to the class $i$ (i.e., $i\in\left\{ 1,...,C\right\} $). Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
a loss function, where $l\left(f\left(x\right),y\right)$ with
$\hat{f}\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
assign a data sample $x$ to the class $y$ by the hypothesis $f$. The general 
loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:
\begin{equation}
\mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].   
\end{equation}

\color{black}

% \color{blue}
% Let $\mathcal{X}$ be a data space on which we endow a data distribution
% $\mathbb{P}$ with a corresponding density function $p(x)$. We consider
% the multi-class classification problem with the label set $\mathcal{Y}=\left[C\right]$,
% where $C$ is the number of classes and $\left[C\right]:=\{1,\ldots,C\}$.
% % represents the set of the first $C$ positive integer numbers.
% Denote
% $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{C}:\left \| \alpha \right \|_{1}=1\,\wedge\,\alpha\geq 0\right\} $
% as the $C-$simplex, let $f:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% be a probabilistic labeling function returning a $C$-tuple $f\left(x\right)=\left[f\left(x,i\right)\right]_{i=1}^{C}$,
% whose element $f\left(x,i\right)=p\left(y=i\mid x\right)$ 
% % can be interpreted as 
% is the probability to assign a data sample $x\sim\mathbb{P}$
% to the class $i$ (i.e., $i\in\left\{ 1,...,C\right\} $). Moreover,
% a domain is denoted compactly as pair of data distribution and labeling
% function $\mathbb{D}:=\left(\mathbb{P},f\right)$. We note that
% given a data sample $x\sim\mathbb{P}$, its categorical label $y\in\mathcal{Y}$
% is sampled as $y\sim Cat\left(f\left(x\right)\right)$ which a categorical
% distribution over $f\left(x\right)\in\mathcal{Y}_{\Delta}$. 

% Let $l:\mathcal{Y}_{\Delta}\times\mathcal{Y}\mapsto\mathbb{R}$ be
% a loss function, where $l\left(f\left(x\right),y\right)$ with
% $f\left(x\right)\in\mathcal{Y}_{\Delta}$ and $y\in\mathcal{Y}$
% specifies the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to
% assign a data sample $x$ to the class $y$ by the hypothesis $f$.
% Moreover, given a prediction probability $\hat{f}\left(x\right)$
% w.r.t. the ground-truth prediction $f\left(x\right)$, we define the
% loss $\ell\left(\hat{f}\left(x\right),f\left(x\right)\right)=\mathbb{E}_{y\sim f\left(x\right)}\left[l\left(\hat{f}\left(x\right),y\right)\right]=\sum_{y=1}^{C}l\left(\hat{f}\left(x\right),y\right)f\left(x,y\right)$.
% We further define the general loss caused by using a classifier $\hat{f}:\mathcal{X}\mapsto\mathcal{Y}_{\Delta}$
% to predict $\mathbb{D}\equiv\left(\mathbb{P},f\right)$ as
% \begin{equation}
% \mathcal{L}\left(\hat{f},f,\mathbb{P}\right)=\mathcal{L}\left(\hat{f},\mathbb{D}\right):=\mathbb{E}_{x\sim\mathbb{P}}\left[\ell\left(\hat{f}(x),f(x)\right)\right].
% \end{equation}

\textit{Objective}: Given a set of training domains $\mathcal{E}_{tr}=\{e_1,...,e_K\} \subset \mathcal{E}$, the objective of DG is to exploit the `commonalities' present in the training domains to improve generalization to any domain of the population $e\in \mathcal{E}$. For supervised classification, the task is equivalent to seeking the set of \textbf{global optimal hypotheses} $\mathcal{F}^{*}$ 
%\footnote{under the assumptions of the data generation process, the set of global optimal hypotheses defined in Eq.~(\ref{eq:optimal}) is equivalent to the one defined by the worst-case domain: $\mathcal{F}^{*} = \underset{f'\in \mathcal{F}}{\text{argmin}}\sup_{{e}\in \mathcal{E}}\loss{f',\mathbb{P}^{e}}$ in the Appendix~\ref{apd:worstcase}.}
where every $f\in \mathcal{F}^*$ is locally optimal for every domain:
    \begin{equation}
    \mathcal{F}^{*} := \bigcap_{{e}\in \mathcal{E}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}
    \label{eq:optimal}
\end{equation}
% where the hypothesis $f:\mathcal{X}\rightarrow\mathcal{Y}_{\Delta}$ is a map from the data space $\mathcal{X}$ to the the $C$-simplex label space $\mathcal{Y}_{\Delta}:=\left\{ \alpha\in\mathbb{R}^{\left | \mathcal{Y} \right |}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$.
% Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $y\in \mathcal{Y}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is: 
% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x,y\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),y\right)\right].    
% \end{equation}

% \longvt{Let $\ell\left(f\left(x\right),y\right)$ be the loss incurred by using this hypothesis to predict $x\in\mathcal{X}$ with its true label $\mathbb{P}(Y\mid x)\in \mathcal{Y}_{\Delta}$. The general 
% loss of the hypothesis $f$ w.r.t. a given domain $\mathbb{P}^e$ is:

% \begin{equation}
% \mathcal{L}\left(f,\mathbb{P}^e\right):=\mathbb{E}_{\left(x\right)\sim\mathbb{P}^e}\left[\ell\left(f\left(x\right),\mathbb{P}(Y\mid x)\right)\right].    
% \end{equation}
% }

\color{black}

We here examine the widely used {\it composite hypothesis} $f = h \circ g \in \mathcal{F}$, where $g : \mathcal{X} \rightarrow \mathcal{Z}$ belongs to a set of representation functions $\mathcal{G}$, mapping the data space $\mathcal{X}$ to a latent space $\mathcal{Z}$, and $h : \mathcal{Z} \rightarrow \mathcal{Y}_{\Delta}$ is the classifier in the space $\mathcal{H}$. For simplicity, we assume \(\mathcal{Z}_c, \mathcal{Z}_e \subseteq \mathcal{Z}\) in the following analyses.


\textbf{Presumption}. While our work considers limited 
and finite domains, we follow recent theoretical works \citep{wang2022provable, rosenfeld2020risks, kamath2021does, ahuja2021invariance, chen2022iterative} assuming the infinite data setting for every training environment. This assumption distinguishes DG literature from traditional generalization analysis (e.g., PAC-Bayes framework) that focuses on in-distribution generalization where the testing data are drawn from the same distribution.


\subsection{Assumptions on Data Generation Process}
We consider the following family of distributions over the observed variables $(X,Y)$ given the environment $E=e \in \mathcal{E}$ where environment space under consideration $\mathcal{E} = \{ e \mid \mathbb{P}^e \in \mathcal{P} \}$:
\vspace{-2mm}
\begin{equation*}
    \mathcal{P}=\left \{\mathbb{P}^e(X, Y)=\int_{z_c}\int_{z_e}\mathbb{P}(X, Y, Z_c,Z_e, E=e)d z_c d z_e\right \}
\end{equation*}
The data generative process underlying every observed distribution $\mathbb{P}^e(X, Y)$ is characterized by a \textit{structural causal model} (SCM) over a tuple  $\left \langle V, U, \psi \right \rangle$ (See Figure~\ref{fig:graph}). The SCM consists of a set of \textit{endogenous} variables $V = \{X, Y, Z_c, Z_e, E\}$, a set of mutually independent \textit{exogenous} variables $U = \{U_x, U_y, U_{z_c}, U_{z_e}, U_e\}$ associated with each variable in $V$ and a set of deterministic equations $\psi = \{\psi_x, \psi_y, \psi_{z_c}, \psi_{z_e}, \psi_e\}$ representing the generative process for $V$. We note that this generative structure has been widely used and extended in several other studies, including \citep{chang2020invariant, mahajan2021domain, li2022invariant, zhang2023causal, lu2021invariant,liu2021heterogeneous}. 
% % \begin{figure}[h!]
% \begin{wrapfigure}{r!}{0.5\textwidth}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.25\textwidth]{ICLR2025/Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.\label{fig:graph}}
% \vspace{-2mm}
% \end{wrapfigure} 
% % \end{figure} 


The generative process begins with the sampling of an environmental variable $e$ from a prior distribution $\mathbb{P}(U_e)$\footnote{explicitly via the equation $e = \psi_e(u_e), u_e \sim P(U_e)$.}. We assume there exists a causal factor $z_c\in\mathcal{Z}_c$ determining the label $Y$ and a environmental feature $z_e\in\mathcal{Z}_e$ \textit{spuriously} correlated with $Y$. 
These two latent factors are 
generated from an environment $e$ via the mechanisms $z_c = \psi_{z_c}(e, u_{z_c})$ and $z_e = \psi_{z_e}(e, u_{z_e})$ with $u_{z_c} \sim \mathbb{P}(U_{z_c}), u_{z_e} \sim \mathbb{P}(U_{z_e})$. A data sample $x\in \mathcal{X}$ is generated from both the causal feature and the environmental feature i.e., $x = \psi_{x}(z_c, z_e, u_{x})$ with $u_x \sim \mathbb{P}(U_x)$. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/SCM.png}
    \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. Observed variables are shaded.}
    \label{fig:graph}
\end{figure}

% \begin{figure}{h!}{0.5\linewidth}
% \vspace{-5mm}
% \begin{centering}
% \subfloat{\centering{}\includegraphics[width=0.28\textwidth]{Figures/SCM.png}}
% \par\end{centering}
% \caption{A directed acyclic graph (DAG) describing the causal relations among different factors producing data $X$ and label $Y$ in our SCM. \\Observed variables are shaded.\label{fig:graph}}
% \vspace{-5mm}
% \end{figure} 



% A causal factor $z_c$ determines a label $y\in \mathcal{Y}$ by $y = \psi_{y}(z_c, u_{y})$ with $u_y \sim \mathbb{P}(U_y)$. The presence of intrinsic noise $u_y$ is crucial, implying that a causal factor can be observed with different label categories. Therefore, without loss of generality, we can consider the label variable $y$ lying on the $C$-simplex, that is $\mathcal{Y} := \Delta_{C} = \left\{ \alpha\in\mathbb{R}^{C}:\norm\Vert\alpha\Vert_{1}=1\,\land\,\alpha\geq 0\right\}$. We note this is the point of difference of ours from prior works. As the ``soft" representation of $Y$ encapsulates the noise effect, the generative process for $y$ can now be written as $\psi_y(z_c) = y$.

Figure \ref{fig:graph} dictates that the joint distribution over $X$ and $Y$ can vary across domains resulting from the variations in the distributions of $Z_c$ and $Z_e$. Furthermore, \textit{both causal and environmental features are correlated with $Y$, but only $Z_c$ causally influences $Y$}.  However because $Y \perp\!\!\!\perp E | Z_c$, the conditional distribution of $Y$ given a specific $Z_c = z_c$ remains unchanged across different domains i.e., $\mathbb{P}^e(Y | Z_c = z_c) = \mathbb{P}^{e'}(Y | Z_c = z_c)$ $\forall e, e' \in \mathcal{E}$. For readability, we omit the superscript $e$ and denote this invariant conditional distribution as $\mathbb{P}(Y | Z_c = z_c)$. 
\subsection{Assumptions on Possibility of Generalization}

We first establish crucial assumptions for the feasibility of generalization as described in Eq (\ref{eq:optimal}). These assumptions are essential for understanding the conditions under which generalization can be achieved. 


%We note that the following discussion may not be entirely new and exist in some other forms dispersing across the literature. We here make the first contribution to unifying them into a systematic and comprehensive analysis. 

\begin{assumption} (Label-identifiability). We assume that for any pair $z_c, z^{'}_c\in \mathcal{Z}_c$,  $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c) \text{ if } \psi_x(z_c,z_e,u_x)=\psi_x(z_c',z'_e,u'_x)$ for some $z_e, z'_e, u_x, u'_x$
\label{as:label_idf}.
\end{assumption}

The causal graph indicates that $Y$ is influenced by $z_c$, making $Y$ identifiable over the distribution $\mathbb{P}(Z_c)$. This assumption implies that different causal factors $z_c$ and $z^{'}_c$ cannot yield the same $x$, unless the condition $\mathbb{P}(Y|Z_c=z_c) = \mathbb{P}(Y|Z_c=z^{'}_c)$ holds, or  the distribution $\mathbb{P}(Y\mid x)$ is stable. This assumption also can be view as covariate shift setting in OOD 
\citep{shimodaira2000improving}. This assumption licenses the existence of globally optimal hypotheses as defined in Eq. (\ref{eq:optimal}) (Appendix \ref{thm:existence_apd}). 
%\cite{shimodaira2000improving,bickel2009discriminative}. 

%\begin{theorem} $\mathcal{F}^*\neq \emptyset$ if and only if Assumption~\ref{as:label_idf} is hold. (Proof in Appendix \ref{thm:existence_apd}).  \label{thm:existence} \end{theorem}


\begin{assumption} (Causal support). We assume that the union of the support of \textit{causal} factors across training domains covers the entire causal factor space $\mathcal{Z}_c$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_c \right )\}=\mathcal{Z}_c$. 
\label{as:sufficient_causal_support}
\end{assumption}

% \begin{assumption} (Spurious support). We assume that the union of the support of \textit{spurious} factors across training domains covers the entire causal factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$ where $\text{supp}(\cdot)$ specifies the support set of a distribution. 
% \label{as:sufficient_spurious_support}
% \end{assumption}

This assumption holds significance in DG theories \citep{johansson2019support, ruan2021optimal, li2022sparse}, especially when we avoid imposing strict constraints on the target functions. Particularly, \citep{ahuja2021invariance} showed that without the support overlap assumption on the causal features, OOD generalization is impossible for such a simple model as linear classification. Meanwhile, for more complicated tasks, deep neural networks are typically employed, which, when trained via gradient descent however, cannot effectively approximate a broad spectrum of nonlinear functions beyond their support range \citep{xu2020neural}. It is worth noting that causal support overlap does not imply that the distribution over the causal features is held unchanged.






\section{DG: A view of Necessity and
Sufficiency}\label{sec:main_conds}

In this section, we present the the necessary and sufficient conditions for achieving generalization defined in Eq. (\ref{eq:optimal}) (See Table \ref{tab:conditions} for summary). These conditions are critical to our analysis, where we first reveal that the existing DG methods aim to satisfy one or several of these necessary and sufficient conditions to achieve generalization. %Following this section, we theoretically assess whether a method works effectively by to what extent the necessary conditions are met. 


\subsection{Conditions for Generalization}

We begin with the definition of the global optimal hypothesis in Eq. (\ref{eq:optimal}), where $f\in \mathcal{F}^*$
  must also be the optimal solution across all domains. Thus, $f$ must first satisfy the requirement of being the optimal hypothesis for all training domains, which can be considered a \textit{necessary condition} for generalization, formally stated as follows:

\begin{definition} \textit{(Optimal hypothesis for training domains) Given  $\mathcal{F}_{\mathbb{P}^e}=\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}$ is set of optimal hypothesis for $\mathbb{P}^{e}$, the optimal hypothesis for all training domains $f\in\mathcal{F}_{\mathcal{E}_{tr}} = \bigcap_{{e}\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^e}$.}
\label{def:joint_optimal}
\end{definition}


However, $f \in \mathcal{F}_{\mathcal{E}_{tr}}$ is not sufficient to guarantee that $f \in \mathcal{F}^*$ as a global optimal hypothesis. This motivates the study of DG analyses and algorithms, which explore constraints to fulfill sufficient conditions for truly achieving generalization.


Assumption \ref{as:label_idf} gives rise to a family of invariant representation function $\mathcal{G}_c$, as stated in Proposition \ref{thm:invariant_correlation}. 

\begin{proposition} (Invariant Representation Function)
Under Assumption.\ref{as:label_idf}, there exists a set of deterministic representation function $(\mathcal{G}_c\neq \emptyset)\in \mathcal{G}$ such that for any $g\in \mathcal{G}_c$, $\mathbb{P}(Y\mid g(x)) = \mathbb{P}(Y\mid z_c)$ and $g(x)=g(x')$ holds true for all $\{(x,x',z_c)\mid  x= \psi_x(z_c, z_e, u_x), x'= \psi_x(z_c, z^{'}_e, u^{'}_x) \text{ for all }z_e,z^{'}_e, u_x, u^{'}_x\}$ (Proof in
Appendix \ref{thm:invariant_correlation_apd}).   
\label{thm:invariant_correlation}
\end{proposition}


\begin{definition} (Invariance-preserving  transformation). Denote $\mathcal{T}$ is set of all invariance-preserving  transformations  $T(\cdot)$ such that for any $T\in \mathcal{T}$: $(g\circ T)(\cdot)=g(\cdot)$.
\label{def:causal_transformation}
\end{definition}

\begin{definition} (Sufficient and diverse training domains). Set of training domains $\mathcal{E}_{tr}$ is consider as sufficient and diverse if  the union of the support of \textit{spurious} factors across training domains covers the entire spurious factor space $\mathcal{Z}_e$: $\cup_{e\in \mathcal{E}_{tr}}\text{supp}\{\mathbb{P}^{e} \left (Z_e \right )\}=\mathcal{Z}_e$. 
\label{as:sufficient_spurious_support}
\end{definition}

\begin{theorem} Under Assumption \ref{as:label_idf} and Assumption \ref{as:sufficient_causal_support}, given a hypothesis $f=h\circ g$, if $f$ is optimal hypothesis for training domains i.e.,

$$f\in \bigcap_{{e}\in \mathcal{E}_{tr}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}$$

and one of the following equivalent sub-conditions is holds:
\begin{enumerate}
    \item $g$ belongs to the set of invariant representation functions as specified in Definition~\ref{thm:invariant_correlation}.
    
    \item $\mathcal{E}_{tr}$ is the set of sufficient and diverse domains, as described in Definition~\ref{as:sufficient_spurious_support}.
    
    \item $f$ is also an optimal hypothesis on all augmented domains, i.e., $f\in \bigcap_{{e}\in \mathcal{E}_{tr}, T\in \mathcal{T}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,T\#\mathbb{P}^{e}}$ where $\mathcal{T}$ is the set of all invariance-preserving transformations, as defined in Definition~\ref{def:causal_transformation}.
\end{enumerate}

Then $f\in \mathcal{F}^*$ (Proof in Appendix \ref{thm:single_generalization_apd})
\label{thm:single_generalization}
\end{theorem}




Theorem \ref{thm:single_generalization} demonstrates that under Assumption~\ref{as:label_idf} and Assumption~\ref{as:sufficient_causal_support},  
sub-condition optimal hypothesis for all training domains $f\in \bigcap_{{e}\in \mathcal{E}_{tr}}\underset{f\in \mathcal{F}}{\text{argmin}} \ \loss{f,\mathbb{P}^{e}}$ along with:

\begin{itemize}
    \item sub-condition (Theorem \ref{thm:single_generalization}.1): $g \in \mathcal{G}_c$ 
    is the \textit{sufficient condition} for 
    $f^* \in \mathcal{F}^*$ 
    to learn the global optimal hypothesis from a finite number of training domains. This condition is what the family of \textit{representation alignment} methods strives for.

     \item sub-condition (Theorem \ref{thm:single_generalization}.2): having a sufficient and diverse set of training domains is a \textit{sufficient condition} for attaining the global optimal hypothesis:
    $f^* \in \mathcal{F}^*$.
    This condition, in fact, does not depend on the algorithm but appears in analyses of \textit{invariant prediction}-based research.

     \item sub-condition (Theorem \ref{thm:single_generalization}.3): availability of the set of all invariance-preserving transformations is also sufficient for learning the optimal hypothesis:
    $T \in \mathcal{T}$.
    This forms the foundation of the family of \textit{augmentation}-based DG algorithms.
\end{itemize}

It can be seen that conventional DG approaches indeed strive for sufficient condition for generalization. However, achieving the associated sub-conditions, \textit{invariant representation, having a sufficient and diverse set of training domains, or access to the set of all invariance-preserving transformations}, is often impractical with a limited number of training domains. %This raises a natural question: to what extent can DG algorithms generalize when the sufficient conditions are not fully met? To address this, we introduce a set of necessary conditions in Section~\ref{sec:necessary_condition} and explore how DG algorithms operate by examining the interaction between necessary and sufficient conditions in Section~\ref{sec:efficacy_DG}.


% \subsection{Conditions for Generalization}
% \label{sec:necessary_condition}



We therefore shift the attention to studying a new class of
representation function that serves as a necessary condition for global optimal hypothesis, which is
defined as follows:

% Another necessary condition, which is indeed important but often overlooked in the domain generalization (DG) literature, is the Invariance-Preserving Representation Function, defined as follows:

\begin{definition}\textit{(Invariance-preserving representation function) A set of representation functions $\mathcal{G}_s\in \mathcal{G}$ is considered as Causal-preserved representation functions if for any $g\in\mathcal{G}_s$, there exists a function $\phi: \mathcal{Z}\rightarrow \mathcal{Z}$ such that
$(\phi\circ g) \in \mathcal{G}_c$ (i.e., given $g\in \mathcal{G}_s$, $g(x)$ retains all information about causal feature of $x$). }
\label{def:sufficient}
\end{definition}
\color{black}

Remind that the an invariant representation function aims to extract \textit{exact} invariant representations that remain consistent across environments, ensuring that all optimal classifiers trained on top of these representations achieve optimal solutions. In contrast, a Invariance-preserving representation function captures representations \textit{containing} invariant information rather than strictly invariant features.

The following theorem shows that $g \in \mathcal{G}_s$ is necessary for achieving the global optimal hypothesis. 

\begin{theorem} Given representation function $g$,
$\exists h: h\circ g\in \mathcal{F}^*$ if and only if $g\in \mathcal{G}_s$. (Proof in Appendix~\ref{thm:nacessary_apd})
\label{thm:nacessary}
\end{theorem}

This theorem implies that if $g$ is not an invariance-preserving representation function, i.e., $g \notin \mathcal{G}_s$, no classifier $h$ can exist such that $h \circ g \in \mathcal{F}^*$. In other words, $g \in \mathcal{G}_s$ is a \textit{necessary} condition for $f \in \mathcal{F}^*$. This property is critical for understanding the generalization ability of domain generalization (DG) algorithms, as discussed in the following section.




\subsection{DG generalize with limited training domains?}
\label{sec:efficacy_DG}

To answer this question, we illustrate the relationship between sufficient and necessary conditions using Venn diagrams of hypothesis spaces corresponding to each condition, as shown in Figure \ref{fig:space}. 

We start with two necessary conditions. By definition of the first necessary condition, "Optimal Hypothesis for Training Domains," we have 
$\mathcal{F}^* \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. Thus, the global optimal hypothesis space $\mathcal{F}^*$ (grey area) lies within the joint domain-optimal hypothesis space $\mathcal{F}_{\mathcal{E}_{tr}}$ (green area). According to Theorem~\ref{thm:nacessary}, if the second necessary condition, "Invariance-Preserving Representation Function," is not satisfied, the feasible global optimal hypothesis space becomes empty.



% We further define $\mathcal{F}^S_{\mathcal{E}_{tr}}$ as optimal training and sufficient, then $\mathcal{F}^S_{\mathcal{E}_{tr}}\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$

% By theorem~\ref{thm:nacessary},  $\mathcal{F}^*\subseteq \mathcal{F}^S_{\mathcal{E}_{tr}}$

% From theorem $\mathcal{F}^*\subseteq \mathcal{F}^S_{\mathcal{E}_{tr}}\subseteq \mathcal{F}_{\mathcal{E}_{tr}}$

% By theorem~\ref{thm:}, Sufficient play as contraint to $\mathcal{F}_{\mathcal{E}_{tr}}$


% Denote  $\mathcal{F}_{\mathbb{P}^e}=\bigcup_{g\in \mathcal{G}_s}\mathcal{F}_{\mathbb{P}^e,g}$ as the set of hypotheses induced by an algorithm $A$ that satisfies both necessary conditions i.e., optimal for domain $e$ (\textit{Condition}~\ref{def:joint_optimal}) and $g \in \mathcal{G}_s$ is a \textit{Invariance-preserving representation function} (\textit{Condition}~\ref{def:sufficient}).



\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/space1.png}
    \caption{The circles (\textcolor{brown}{brown}, \textcolor{blue}{blue}, \textcolor{red}{red}) denote the spaces of domain-optimal hypotheses \textcolor{brown}{$\mathcal{F}_{\mathbb{P}^{e_1}}$}, \textcolor{blue}{$\mathcal{F}_{\mathbb{P}^{e_2}}$}, \textcolor{red}{$\mathcal{F}_{\mathbb{P}^{e_3}}$} of training domains $e_1, e_2, e_3\in \mathcal{E}_{tr}$ respectively. %The grey area indicates the space of global optimal hypotheses $\mathcal{F}^{*}$ while the green area represents the joint space of domain-optimal hypotheses $\mathcal{F}_{\mathcal{E}_{tr}}$.
    }
\label{fig:space}
\end{figure}


% The Venn diagram reveals that any algorithm achieving \textit{Condition}~\ref{def:joint_optimal} can guarantee that its corresponding global optimal set is bounded by the feasible hypothesis set induced by the algorithm $A$; in visual terms, the green area always cover the grey area. Apparently, "bad" domains occur for any learned hypothesis $f$ that falls outside of the grey area. Therefore, the more the green area collapses to the grey area, the higher chance generalization can be attained.


Now move to sufficient conditions, we can interpret the objective function of Theorem~\ref{thm:single_bound_A} as prioritizing the optimization problem with "optimal hypothesis for training domains" condition as the primary objective. Meanwhile, the other three conditions \textit{Invariant Representation Function, Sufficient and Diverse Training Domains, and Invariance-Preserving Transformations} can be treated as regularization terms to support the main objective.

\begin{itemize}
    \item When one of the sub-conditions is met, according to Theorem~\ref{thm:single_generalization}, we achieve global optimal hypothesis, which means the green area converge to the grey area. 

    \item However, in a limited domain setting where the sufficient conditions cannot be fully met, existing DG approaches are essentially seeking to reduce the green area to increase the likelihood of achieving generalization.
\end{itemize}
 
While attempting to restrict the set of feasible solutions, a DG algorithm, with its extra constraints, may as well reduce the grey area, by restricting the global optimal set to only solutions that also meet the constraints. With arbitrary constraints, there is a possibility that the grey area shrinks to null. 

Interestingly, a key insight from Theorem~\ref{thm:nacessary} is that, under the \textit{Condition}~\ref{def:joint_optimal}, as long as the solution of an algorithm fulfills the \textit{Invariance-preserving representation function function} constraint (\textit{Condition}~\ref{def:sufficient}), there exists a non-empty $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$; otherwise $\mathcal{F}^{*} = \emptyset$. In fact, that an algorithm meets a sufficient condition implies the satisfaction of \textit{Condition}~\ref{def:sufficient} by default.

\textbf{In summary}, an algorithm should be effectively designed to minimize the space $\mathcal{F}_{\mathcal{E}_{tr}}$ while maintaining the coverage of $\mathcal{F}^{*}$. \textit{Condition}~\ref{def:joint_optimal} ensures the \textit{green} area is non-empty i.e., $\mathcal{F}_{\mathcal{E}_{tr}} \ne \emptyset$ while  \textit{Condition}~\ref{def:sufficient} ensures the \textit{grey} one is non-empty i.e., $\mathcal{F}^{*} \ne \emptyset$. Satisfying both conditions further guarantees the existence of the global optimal solutions in $\mathcal{F}^{*} \subseteq \mathcal{F}_{\mathcal{E}_{tr}}$. In contrast, if both conditions are violated, the algorithm has zero chance of achieving generalization. Despite its significance, existing DG algorithms tends to overlook \textit{Condition}~\ref{def:sufficient}. From finite training domains, they thus cannot guarantee the possibility of searching for global optimal hypotheses. 




\section{Why conventional DG algorithms fail?} 
 \label{sec:discussion_DG}

In the previous section, we examined the role of sufficient conditions and the importance of necessary conditions in ensuring generalization. Building on this, we analyze why existing DG methods fail by highlighting that while they promote sufficient conditions, they violate necessary ones.

\subsection{Representation Alignment.} 

Representation Alignment focus on learning domain-invariant representations by reducing the divergence between latent marginal distributions $\mathbb{E}[g(X) | E]$ where $E$ represents a domain environment. Other methods seek to align the conditional distributions $\mathbb{E}[g(X) | Y=y, E]$ across domains. Achieving true invariance is inherently challenging and may impose overly restrictive conditions. In certain cases, better feature alignment can paradoxically result in higher joint errors, as demonstrated in the following theorem:

\begin{theorem}
\label{theorem:single_tradeoff} \citep{zhao2019learning, phung2021learning, le2021lamda} Distance between two marginal distribution $\mathbb{P}^{e}_\mathcal{Y}$ and $\mathbb{P}^{e'}_\mathcal{Y}$ can be upper-bounded: 
\vspace{-1mm}
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right) \leq 
D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )
+\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'} \right )
\end{aligned}
\end{equation*}
where $g_{\#}\mathbb{P}(X)$ denotes representation distribution on  representation space $\mathcal{Z}$ induce by applying encoder with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence \citep{zhao2019learning}, Hellinger distance \citep{phung2021learning} or Wasserstein distance \citep{le2021lamda} (Appendix~\ref{apd:tradeoff}).
\end{theorem}

Theorem~\ref{theorem:single_tradeoff} suggests that a substantial discrepancy in the label marginal distribution $D\left(\mathbb{P}_{\mathcal{Y}}^{e},\mathbb{P}_{\mathcal{Y}}^{e'}\right)$ across training domains may result in strong \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e},g_{\#}\mathbb{P}^{e'} \right )$ while increasing \textit{domain-losses} $\left ( \mathcal{L}\left ( f,\mathbb{P}^{e} \right ) + \mathcal{L}\left ( f,\mathbb{P}^{e'}\right )\right )$. It's important to recognize that while the \textit{representation alignment} strategy could challenge \textit{Condition}~\ref{def:joint_optimal}, this alignment constraint can help reduce the cardinality of $ \bigcap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{ \mathbb{P}^e}$. Thus, performance improvement is still attainable with careful adjustment of the alignment weight by exploiting the oracle knowledge of the target domain.
\label{sec:representation_alignment}

\subsection{Invariant Prediction.} These methods aim to learn a consistent optimal classifier across domains. For example, Invariant Risk Minimization (IRM) \citep{arjovsky2020irm} seeks to learn a representation function $g(x)$ with invariant predictors $\mathbb{E}[Y | g(x), E]$. This goal aligns with \textit{Condition}~\ref{def:sufficient} and encourages using invariant representations, without imposing restrictions that could affect \textit{Condition}~\ref{def:joint_optimal}. In fact, in fully informative invariant features setting (i.e., $Y \!\perp\! X\mid g(x)$) or the number of training domains is limited, IRM does not provide significant advantages over ERM \citep{rosenfeld2020risks, ahuja2020empirical, ahuja2021invariance}. VREx \citep{krueger2021out} relaxes the IRM's constraint to enforce equal risks across domains, assuming that the optimal risks are similar across domains. If, however, the optimal solutions exhibit large loss variations, balancing risks could result in suboptimal performance for some domains, violating \textit{Condition}~\ref{def:joint_optimal}. IIB \citep{li2022invariant} and IB-IRM \citep{ahuja2021invariance} integrate the information bottleneck principle with invariant prediction strategies. However, similar to IRM, these approaches only show benefits with a sufficient and diverse number of training domains. Otherwise, the information bottleneck even makes it susceptible to violating \textit{Condition}~\ref{def:sufficient}. 
\begin{figure}[h!]
    \centering
\includegraphics[width=1.0\linewidth]{Figures/information_min.png}
    \caption{Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.}
    \label{fig:info_min}
\end{figure}

\paragraph{Information Bottleneck Theory.} We here  elucidate our claim that minimizing $I(g(X); X)$ can subject the model to violating \textit{Condition}~\ref{def:sufficient}. Whereas \cite{ahuja2021invariance} posits that information bottleneck aids generalization, such methods in fact assume sufficient and diverse domains (ref. their Theorem 1 or Assumption 5 and 6 about invariant and spurious feature support overlap), that is when a sufficient condition is met. In this case, the information about $Z_c$ is fully covered by the region $I(X;Y)$ and any $g = \underset{g\in \mathcal{G}}{\text{argmin }} I(g(X);X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ could guarantee all spurious features are discarded.



When the training domains are limited, the learned representations is however more likely to contain spurious correlations bad for prediction on unseen domains. Thus, minimizing $I(g(X);X)$ in fact would at most capture the shared information of $X$ and $Y$, thus yielding representations with the least information about $Z_c$. Therefore, such minimal representations are the least likely to meet the Invariance-preserving representation function constraint in practice. Figure~\ref{fig:info_min} illustrates the difference between two learning scenarios.




\subsection{Augmentation} 

\begin{figure}[h!]
    \centering
\includegraphics[width=1.0\linewidth]{Figures/augmentation.png}
    \caption{(a) Aligning
the two cropped views introduces misleading spatial information.
(b) Aligning the two color-augmented views loses the fine-grained
color information of flowers. (Figure from \citep{zhang2022rethinking}, need create our own one and cite)}
    \label{fig:aug}
\end{figure}

Data augmentation utilize predefined or learnable transformations $T$ on the original sample $X$ or its features $g(x)$ to create augmented data $T(X)$ or $T(g(x))$. However, it's crucial that transformation $T$ maintains the integrity of the causal factors. This implies a \textit{necessity for some knowledge of the target domain} to ensure the transformations do not alter the causal/invariant information \citep{gao2023out}, otherwise it risks violating \textit{Condition}~\ref{def:sufficient}, e.g., augmentation possibly introduces misleading information \citep{zhang2022rethinking} Figure~\ref{fig:aug}.
\label{sec:augmentation}

\subsection{Ensemble Learning.} 
%Ensemble learning \citep{zhou2012ensemble} refers to training multiple copies of the same architecture with different initializations or splits of the training data, then ensembling the individual models for prediction. This straightforward technique has been shown to outperform a single model across various applications, including DG \citep{zhou2021domain, ding2017deep, zhou2021domain, wang2020dofe, mancini2018best, cha2021swad, arpit2022ensemble}. Unlike explicit ensemble methods where multiple models (or model components) need to be trained, \citet{cha2021swad,rame2022diverse, wortsman2022robust} demonstrate that averaging model weights (WA) at different time steps during training to form a single model at test time \citep{izmailov2018averaging} can significantly enhance robustness under domain shift.  Different from the previous works, our analysis in Section \ref{sec:sufficient_constraint}) provides a new insight that ensemble-based methods can also encourage the learning of \textit{Invariance-preserving representation function}  (\textit{Condition} \ref{def:sufficient}) to promote generalizability.

% The original explanation of WA is rooted in flatness-based analysis where flat minima is expected to generalize better \citep{cha2021swad}.
% However, \citet{rame2022diverse} argue that this analysis does not fully explain its success in OOD setting and provide an explanation via their proposed bias-variance-covariance-locality decomposition. 


% \section{Invariance-preserving representation function Constraints}
\label{sec:sufficient_constraint}
In the previous section, we introduced a new \textit{necessary condition} called the "Invariance-preserving representation function condition." In this section, we first explain why this condition has often been overlooked in prior research. We then establish a connection between this condition and the recent \textit{ensemble} strategy, demonstrating that \textit{ensemble} methods indeed encourage models to satisfy this condition.


% \begin{figure*}[!h]
% \subfloat[\label{fig:info}  Learning multiple representations $Z_{i},...,Z_{j}$ through ensemble learning where $ Z_i= g_i(X)\text{ s.t }g_i \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ to maximize the shared information with $Z_c$.]
% {\begin{centering}
% \includegraphics[width=0.4\textwidth]{Figures/information_2.png}
% \par\end{centering}
%  }
%  % \hfill{}
% \subfloat[Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.]
% {\begin{centering}
% \label{fig:info_min} 
% \includegraphics[width=0.4\textwidth]{Figures/information_min.png}
% \par\end{centering}
% }
% \caption{
% Information diagrams of $X, Y, Z_c$ and $Z=g(X)$.
% }
% \label{fig:tnse}
% %\vspace{-4mm}
% \end{figure*}

% \begin{figure}[h!]
%     \centering
% \includegraphics[width=1.0\linewidth]{Figures/information_min.png}
%     \caption{Information diagrams of $X,Y,Z_c$ and $Z_{\text{min}} := g(X) \text{ s.t } g \in \{\underset{g\in \mathcal{G}}{\text{argmax }} I(g(X);Y) - I(g(X); X)\}$. In limited training domains, learning such minimal representation $Z_{\text{min}}$ would capture the least information about $Z_c$.}
%     \label{fig:info_min}
% \end{figure}

  
% However, maximizing the lower bound $I(Z; Y)$ only guarantee $Z$ cover share information $I(X; Y)$. As demonstrated in Figure~\ref{fig:info}.(Right)), for a representation $Z_i$, learned by maximizing the lower bound $I(Z_i; Y)$ i.e., $Z_i=g(X) \mid g \in \{\argmax_{g}I(g(X);Y)\}_{i=1}^{M}$,it optimally cover share information $I(X; Y)$ and possibly some additional information about $Z_c$. To encourage representation capture more information from $Z_c$, we can further directly extend the approach to learn multi-version of representations through ensemble learning i.e., $Z^M = \{Z_i=g_i(X) \mid g_i \in \{\argmax_{g_i}I(g_i(X);Y)\}_{i=1}^{M}\}$, to capture as much information as possible about $Z_c$. This intuition aligns with the analysis on ensembles for OOD
%  generalization in \cite{rame2022diverse}. 
 \color{black}



% \cmt{For clarity, some might wonder why we're applying an information theory perspective here. A common question could be: if we extract spurious features that correlate with the label, and thus also carry information about the causal features, doesn't that make information theory irrelevant in this context? However, it's important to remember that in this section, our goal is to achieve \textbf{Invariance-preserving representation function}. This means that spurious features can still be valuable if there exists a mapping that transforms these spurious features into the causal features.}


 
By definition, a representation function $g$ is considered as Invariance-preserving representation function if there exists a function $\phi\in \Phi$ such that:
$\phi\circ g \in \mathcal{G}_c$. Our task can thus be translated into learning the representation $Z=g(X)$ that captures the most information about the causal factor $Z_c$. This motivates us to find $Z$ that maximizes the mutual information $I(Z; Z_c)$.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/information_2.png}
    \caption{Information diagrams of $X, Y, Z_c$ and $Z=g(X)$. Learning multiple representations $Z_{i},...,Z_{j}$ through ensemble learning where $ Z_i= g_i(X)\text{ s.t }g_i \in \{\underset{g\in \mathcal{G}}{\text{argmax }}I(g(X);Y)\}$ to maximize the shared information with $Z_c$.}
    \label{fig:info}
\end{figure}


Given a specific domain, recall our model $Z\leftarrow X \leftarrow Z_c \rightarrow Y$, where $Y$ is influenced by $Z_c$ (the latent cause) and $Z_c$ also affects $X$. Note that $X$ is also under the influence of $Z_e$, which we omit here for simplicity. Since $Z_c$ is unobserved, we cannot directly measure or learn from it. However, we can leverage $Y$, which inherits the causal information of $Z_c$. This intuition can be best understood via an information diagram. 

Let us examine Figure~\ref{fig:info}.(Left) that illustrates the mutual information of the $4$ variables. We have $I(X,Y \mid Z_c)=0$, meaning the causal features $Z_c$ must capture the shared information $I(X;Y)$. By Assumption~\ref{as:sufficient_causal_support} and Proposition~\ref{thm:invariant_correlation}, it follows that $X$ contains all information about $Z_c$.

By the chain rule of mutual information, we have that $I(Z; Z_c) \ge I(Z; Y)$. Thus, we resort to maximizing the lower bound $I(Z; Y)$ to increasing the chance of learning $Z$ that contains causal information $Z_c$. 
Recall that we use the cross-entropy loss \(\ell:\mathcal{Y}_{\Delta} \times \mathcal{Y} \mapsto \mathbb{R}\) to optimize the hypothesis for training domains. It is well-known that minimizing the cross-entropy loss is equivalent to maximizing the lower bound of \(I(Z; Y)\) \citep{qin2019rethinking, colombo2021novel}. In other words, hypotheses that are optimal on training domains (\textit{Condition}~\ref{def:joint_optimal}) also promote the Invariance-preserving representation function function condition (\textit{Condition}~\ref{def:sufficient}). However, maximizing the lower bound \(I(Z; Y)\) only ensures that \(Z\) captures the shared information \(I(X; Y)\) and potentially some additional information about \(Z_c\) (as illustrated in Figure~\ref{fig:info} (Right)).

To encourage the representation \(Z\) to capture more information from \(Z_c\), this approach can be extended to learn multiple versions of representations through ensemble learning. Specifically, we can learn an \(M\)-ensemble of representations \(Z^M\):
\[
Z^M = \left\{Z_i = g_i(X) \mid g_i \in \underset{g\in \mathcal{G}}{\text{argmax }} I(g(X); Y) \right\}_{i=1}^{M},
\]
to capture as much information as possible about \(Z_c\). This intuition aligns with the analysis of ensembles for OOD generalization presented in \cite{rame2022diverse}.



\section{Sufficient Invariant Representation Learning}
\label{sec:main_proposed_method}

Section~\ref{sec:discussion_DG} highlights that while conventional DG algorithms promote sufficient conditions, they often violate necessary conditions, which, as analyzed in Section~\ref{sec:main_conds}, is the primary reason for their failure to generalize. To further validate our analysis, we propose a method that simultaneously addresses both sets of conditions to enhance generalization.

To recap, the three conventional strategies are as follows: Invariant prediction-based methods require sufficient and diverse domains, which depend heavily on the data;
Augmentation-based methods rely on invariance-preserving transformations, which often require access to the target domain;
Representation alignment methods are more flexible and can be directly manipulated. Therefore, in this section, we present a method that promotes the representation alignment constraint without violating sufficient conditions, thereby providing a practical means to verify our analysis.




\subsection{Subspace Representation Alignment}

As demonsrate in Theorem~\ref{theorem:single_tradeoff}, \textit{Representation Alignment} based method potentially violate sufficient conditions due to the  trade-off between alignment constraints and domain losses. \textbf{However, we now show that with a more careful design, we can address the trade-off effectively}. Our proposed strategy, called \textit{Subspace Representation Alignment} (SRA), involves organizing training domains into distinct subspaces and aligning representations within these subspaces. 
This aims to diminish or completely remove differences in the marginal label distributions across these domains so that the search space can be reduced. 

% \color{blue}
% Merge to theorem, if $\Gamma=f^*$ them $D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$
% \color{black}
% We consider \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, we denote $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$.
% Let $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$. Eventually, we define $\mathbb{P}_{m}^{e}\left(y\mid x\right)$ as the probabilistic labeling
% distribution on the subspace $\left(A_{m},\mathbb{P}_{m}^{e}\right)$,
% meaning that if $x\sim\mathbb{P}_{m}^{e}$, $\mathbb{P}_{m}^{e}\left(y\mid x\right)=\mathbb{P}_{e}\left(y\mid x\right)$. Since each data point $x \in \mathcal{X}$ corresponds to only a single $\Gamma(x)$, the data space is partitioned into disjoint sets, i.e., $\mathcal{X} = \bigcup_{m=1}^{\mathcal{M}} A_{m}$, where $A_m \cap A_n = \emptyset, \forall m \neq n$. Consequently, 
% $\mathbb{P}^{e}:=\sum_{m\in\mathcal{M}}\pi^{e}_m\mathbb{P}_{m}^{e}$
% where $\pi^{e}_m=\mathbb{P}^{e}\left(A_{m}\right) \slash \sum_{m'\in\mathcal{M}}\mathbb{P}^{e}\left(A_{m'}\right)$.
\begin{theorem}
\label{theorem:multi_bound} Given \textit{subspace projector} $\Gamma: \mathcal{X}\rightarrow \mathcal{M}$, given a subspace index $m\in \mathcal{M}$, let $A_{m}=\Gamma^{-1}(m)=\left\{ x:\Gamma(x)=m\right\} $ is the region on data space which has the same index $m$ and $\mathbb{P}_{m}^{e}$ be the distribution restricted by $\mathbb{P}^{e}$ over the set $A_{m}$, if the loss function $\ell$ is upper-bounded by a positive
constant $L$, then:
(i)  The target general loss is upper-bounded: 
\begin{align*}
\left | \mathcal{E}_{tr} \right |\sum_{e\in \mathcal{E}_{tr}}\mathcal{L}\left ( f,\mathbb{P}^{e} \right )
&\leq
\sum_{e\in \mathcal{E}_{tr}} \sum_{m\in\mathcal{M}}\pi^{e}_m
\mathcal{L}\left ( f,\mathbb{P}^{e}_{m} \right ) \\+
&L\sum_{e, e'\in \mathcal{E}_{tr}}\sum_{m\in\mathcal{M}}\pi^{e}_{m}D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right ),
\end{align*}
(ii) Distance between two label marginal distribution $\mathbb{P}^{e}_{m}(Y)$ and $\mathbb{P}^{e'}_{m}(Y)$ can be upper-bounded: 
\begin{equation*}
\begin{aligned}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right) &\leq 
D\left ( g_{\#}\mathbb{P}^{e}_{m},g_{\#}\mathbb{P}^{e'}_{m} \right )\\
+&\mathcal{L}\left ( f,\mathbb{P}^{e}_{m}\right )
+
\mathcal{L}\left ( f,\mathbb{P}^{e'}_{m} \right )
\end{aligned}
\end{equation*}
(iii) Construct the subspace projector $\Gamma$ as the optimal hypothesis for the source domains, i,e., \(\Gamma \in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}\), which defines a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=\Gamma(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$, then
\begin{equation*}
D\left(\mathbb{P}^{e}_{\mathcal{Y}, m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0
\end{equation*}

where $g_{\#}\mathbb{P}$ denotes representation distribution on $\mathcal{Z}$ induce by applying $g$ with $g: \mathcal{X} \mapsto \mathcal{Z}$ on data distribution $\mathbb{P}$, $D$ can be $\mathcal{H}$-divergence, Hellinger or Wasserstein distance. (Proof in Appendix~\ref{theorem:multi_bound_A})
\label{theorem:tradeoff}
\end{theorem}

In Theorem \ref{theorem:multi_bound}, (i) illustrates that \textit{domain-specific losses} can be broken down into \textit{losses} and \textit{representation alignments} within individual subspaces. Optimizing the subspace-specific losses across domains ensures optimizing the overall loss within the original domains are optimized. Meanwhile, (ii) demonstrates that the distance between the marginal label distributions is now grounded within subspaces, denoted as $d_{1/2}\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$. Theorem~\ref{theorem:multi_bound} suggests that appropriately distributing training domains across subspaces can reduce both the upper and lower bounds. Particularly, for a given subspace index $m$, if $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)=0$, we can jointly optimize both \textit{domains losses} $\mathcal{L}\left (f,\mathbb{P}^{e}_m \right ) + \mathcal{L}\left (  f,\mathbb{P}^{e'}_m \right )$ and \textit{representation alignment} $D\left ( g_{\#}\mathbb{P}^{e}_m,g_{\#}\mathbb{P}^{e'}_m \right )$. Consequently, optimizing the RHS of (ii) for all supspaces is equivalent to minimizing the RHS of (i).

Fortunately, Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

% The question now is how we can manage the training distribution into a subspace such that $D\left(\mathbb{P}^{e}_{\mathcal{Y},m},\mathbb{P}^{e'}_{\mathcal{Y},m}\right)$ is reduced, potentially even to zero. Fortunately, working within training domains, we anticipate that $f\in \cap_{e\in \mathcal{E}_{tr}}\mathcal{F}_{\mathbb{P}^{e}}$ will predict the ground truth label $f(x)=f^*(x)$ where $f^*\in \mathcal{F}^*$. We can define a projector \(\Gamma = f\), which induces a set of subspace indices $\mathcal{M}=\{m=\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}\subseteq \mathcal{Y}_\Delta$. As a result, given subspace index $m\in\mathcal{M}$, $\forall i \in \mathcal{Y}, \mathbb{P}^{e}_{\mathcal{Y},m}(Y=i) = \mathbb{P}^{e'}_{\mathcal{Y},m}(Y=i) = \sum_{x \in f^{-1}(m)}\mathbb{P}(Y=i\mid x) = m[i]$. Consequently, \(D\left(\mathbb{P}^{e}_{\mathcal{Y},m}, \mathbb{P}^{e'}_{\mathcal{Y},m}\right) = 0\) for all \(m \in \mathcal{M}\), allowing us to jointly optimize both \textit{domain losses} and \textit{representation alignment}.

The \textbf{final optimization objective}, encapsulating the constraints of optimal hypothesis for all training domain, ensemble for Invariance-preserving representation function, and subspace representation alignment is given by:
\begin{align}
\min_{f} &\underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}\\
&\text{ s.t. } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}} \nonumber
\label{eq:final_objective}
\end{align}

% \begin{align}
%     &\min_{f} \underset{\text{Invariance-preserving representation function Constraint}}{\underbrace{\sum_{e\in \mathcal{E}_{tr}}-\mathcal{I}\left(X^{e},g(X^{e})\right )}}
% + \underset{\text{Subspace Representation Alignment}}{\underbrace{\sum_{e,e'\in \mathcal{E}_{tr}}\sum_{m\in \mathcal{M}}D\left( g\#\mathbb{P}_m^{e},g\#\mathbb{P}_m^{e'}\right)}}
% \label{eq:final_objective}
% \\
% &\text{subject to } \underset{\text{Training domain optimal hypothesis}}{\underbrace{f\in \bigcap_{e\in \mathcal{E}_{tr}}\underset{ f}{\text{argmin }} \mathcal{L}\left(f,\mathbb{P}^{e}\right)}}.\nonumber
% \end{align}

where $\mathcal{M}=\{\hat{y}\mid \hat{y}=f(x), x\in\bigcup_{e\in\mathcal{E}_{tr}}\text{supp}\mathbb{P}^{e} \}$ and $D$ can be $\mathcal{H}$-divergence, Hellinger distance, Wasserstein distance. We provide the details on the practical implementation of the proposed objective in Appendix~\ref{Sec:practical}.

\import{}{4_Experiments.tex}